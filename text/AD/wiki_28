<doc id="47525" url="http://en.wikipedia.org/wiki?curid=47525" title="Contrail">
Contrail

Contrails (; short for "condensation trails") or vapor trails are long, thin artificial clouds that sometimes form behind aircraft. Their formation is most often triggered by the water vapor in the exhaust of aircraft engines, but can also be triggered by changes in air pressure in wingtip vortices or in the air over the entire wing surface. Contrails are made of water in the form of a suspension of billions of liquid droplets or ice crystals.
Depending on the temperature and humidity at the altitude the contrails form, they may be visible for only a few seconds or minutes, or may persist for hours and spread to be several miles wide. The resulting cloud forms may resemble cirrus, cirrocumulus, or cirrostratus, and are sometimes called cirrus aviaticus. Persistent spreading contrails are thought by some, without overwhelming scientific proof, to have a significant effect on global climate. According to the FAA, in conjunction with scientific experts at the EPA, NASA and NOAA, "Contrail cloudiness might contribute to human-induced climate change. Climate change may have important impacts on public health and environmental protection ... Changes in cloudiness resulting from human activities are important because they might contribute to long-term changes in the Earth’s climate. Contrails’ possible climate effects are one component of aviation’s expected overall climate effect." 
Condensation from engine exhaust.
The main products of hydrocarbon fuel combustion are carbon dioxide and water vapor. At high altitudes this water vapor emerges into a cold environment, and the local increase in water vapor can raise the relative humidity of the air past saturation point. The vapor then condenses into tiny water droplets which freeze if the temperature is low enough. These millions of tiny water droplets and/or ice crystals form the contrails. The time taken for the vapor to cool enough to condense accounts for the contrail forming some way behind the aircraft's engines. At high altitudes, supercooled water vapor requires a trigger to encourage deposition or condensation. The exhaust particles in the aircraft's exhaust act as this trigger, causing the trapped vapor to 
condense rapidly. Contrails usually form at very high altitudes; usually above 8000 m, where the air temperature is below -36.5 C, and the relative humidity is over 60%. They can also form closer to the ground when the air is very cold and has enough moisture.
Condensation from decreases in pressure.
As a wing generates lift, it causes a vortex to form at each wingtip, and sometimes also at the tip of each wing flap. These wingtip vortices persist in the atmosphere long after the aircraft has passed. The reduction in pressure and temperature across each vortex can cause water to condense and make the cores of the wingtip vortices visible. This effect is more common on humid days. Wingtip vortices can sometimes be seen behind the wing flaps of airliners during takeoff and landing, and during landing of the Space shuttle.
The visible cores of wingtip vortices contrast with the other major type of contrails which are caused by the combustion of fuel. Contrails produced from jet engine exhaust are seen at high altitude, directly behind each engine. By contrast, the visible cores of wingtip vortices are usually seen only at low altitude where the aircraft is travelling slowly after takeoff or before landing, and where the ambient humidity is higher. They trail behind the wingtips and wing flaps rather than behind the engines.
During high-thrust settings the fan blades at the intake of a turbofan engine reach transonic speeds, causing a sudden drop in air pressure. This creates the condensation fog (inside the intake) which is often observed by air travelers during takeoff. For more information see the Prandtl-Glauert singularity effect.
The tips of rotating surfaces (such as propellers and rotors) sometimes produce visible contrails.
Contrails and climate.
Contrails, by affecting the Earth's radiation balance, act as a radiative forcing. Studies have found that contrails trap outgoing longwave radiation emitted by the Earth and atmosphere (positive radiative forcing) at a greater rate than they reflect incoming solar radiation (negative radiative forcing). NASA conducted a great deal of detailed research on atmospheric and climatological effects of contrails, including effects on ozone, ice crystal formation, and particle composition, during the Atmospheric Effects of Aviation Project (AEAP). Global radiative forcing has been calculated from the reanalysis data, climatological models and radiative transfer codes. It is estimated to amount to 0.012 W/m2 (watts per square meter) for 2005, with an uncertainty range of 0.005 to 0.026 W/m2, and with a low level of scientific understanding. Therefore, the overall net effect of contrails is positive, i.e. a "warming" effect. However, the effect varies daily and annually, and overall the magnitude of the forcing is not well known: globally (for 1992 air traffic conditions), values range from 3.5 mW/m2 to 17 mW/m2. Other studies have determined that night flights are mostly responsible for the warming effect: while accounting for only 25% of daily air traffic, they contribute 60 to 80% of contrail radiative forcing. Similarly, winter flights account for only 22% of annual air traffic, but contribute half of the annual mean radiative forcing. 
September 11, 2001 climate impact study.
The grounding of planes for three days in the United States after September 11, 2001 provided a rare opportunity for scientists to study the effects of contrails on climate forcing. Measurements showed that without contrails, the local diurnal temperature range (difference of day and night temperatures) was about 1 °C (1.8 °F) higher than immediately before; however, it has also been suggested that this was due to unusually clear weather during the period.
Condensation trails have been suspected of causing "regional-scale surface temperature" changes for some time. Researcher David J. Travis, an atmospheric scientist at the University of Wisconsin-Whitewater, has published and spoken on the measurable impacts of contrails on climate change in the science journal "Nature" and at the American Meteorological Society's was observed in surface temperature change, measured across over 4,000 reporting stations in the continental United States. Travis' research documented an "anomalous increase in the average diurnal temperature change". The diurnal temperature range (DTR) is the difference in the day's highs and lows at any weather reporting station. Travis observed a 1.8 °C (3.24 °F) departure from the two adjacent three-day periods to the 11th–14th. This increase was the largest recorded in 30 years, more than "2 standard deviations away from the mean DTR".
The September 2001 air closures are deeply unusual in the modern world, but similar effects have provisionally been identified from Second World War records, when flying was more tightly controlled. A 2011 study of climate records in the vicinity of large groups of airbases found a case where contrails appeared to induce a statistically significant change in local climate, with a temperature variance around 0.8 °C, suggesting that examination of historic weather data could help study these effects.
Head-on contrails.
A contrail from an airplane flying towards the observer can appear to be generated by an object moving vertically. On November 8, 2010 in California, U.S., a contrail of this type gained wide media attention as a "mystery missile" that could not be explained by U.S. military and aviation authorities, and its explanation as a contrail took more than 24 hours to become accepted by U.S. media and military institutions.
Distrails.
Where an aircraft passes through a cloud, it can clear a path through it; this is known as a distrail (short for "dissipation trail"). The plane's warm engine exhaust causes existing precipitation to evaporate, leaving a clear wake through an otherwise cloudy sky.
Distrails are created by the elevated temperature of the exhaust gases absorbing the moisture from the cloud. Clouds exist where the relative humidity is 100% but by increasing the temperature the air can hold more moisture and the relative humidity drops below 100%, even for the same absolute moisture density, causing the visible water droplets in the cloud to be converted back into water vapor.

</doc>
<doc id="47526" url="http://en.wikipedia.org/wiki?curid=47526" title="Convection">
Convection

Convection is the concerted, collective movement of groups or aggregates of molecules within fluids (e.g., liquids, gases) and rheids, either through advection or through diffusion or as a combination of both of them. Convection of mass cannot take place in solids, since neither bulk current flows nor significant diffusion can take place in solids. Diffusion of heat can take place in solids, but that is called heat conduction. Convection can be demonstrated by placing a heat source (e.g. a Bunsen burner) at the side of a glass full of a liquid, and observing the changes in temperature in the glass caused by the warmer fluid moving into cooler areas.
Convective heat transfer is one of the major types of heat transfer, and convection is also a major mode of mass transfer in fluids. Convective heat and mass transfer take place both by diffusion – the random Brownian motion of individual particles in the fluid – and by advection, in which matter or heat is transported by the larger-scale motion of currents in the fluid. In the context of heat and mass transfer, the term "convection" is used to refer to the sum of advective and diffusive transfer. In common use the term "convection" may refer loosely to heat transfer by convection, as opposed to mass transfer by convection, or the convection process in general. Sometimes "convection" is even used to refer specifically to "free heat convection" (natural heat convection) as opposed to forced heat convection. However, in mechanics the correct use of the word is the general sense, and different types of convection should be qualified for clarity.
Convection can be qualified in terms of being natural, forced, gravitational, granular, or thermomagnetic. It may also be said to be due to combustion, capillary action, or Marangoni and Weissenberg effects. Heat transfer by natural convection plays a role in the structure of Earth's atmosphere, its oceans, and its mantle. Discrete convective cells in the atmosphere can be seen as clouds, with stronger convection resulting in thunderstorms. Natural convection also plays a role in stellar physics.
Terminology.
The term "convection" may have slightly different but related usages in different scientific or engineering contexts or applications. The broader sense is in fluid mechanics, where "convection" refers to the motion of fluid regardless of cause. However in thermodynamics "convection" often refers specifically to heat transfer by convection.
Additionally, convection includes fluid movement both by bulk motion (advection) and by the motion of individual particles (diffusion). However in some cases, convection is taken to mean only advective phenomena. For instance, in the transport equation, which describes a number of different transport phenomena, terms are separated into "convective" and "diffusive" effects, with "convective" meaning purely advective in context.
Examples and applications of convection.
Convection occurs on a large scale in atmospheres, oceans, planetary mantles, and it provides the mechanism of heat transfer for a large fraction of the outermost interiors of our sun and all stars. Fluid movement during convection may be invisibly slow, or it may be obvious and rapid, as in a hurricane. On astronomical scales, convection of gas and dust is thought to occur in the accretion disks of black holes, at speeds which may closely approach that of light.
Heat transfer.
Convective heat transfer is a mechanism of heat transfer occurring because of bulk motion (observable movement) of fluids. Heat is the entity of interest being advected (carried), and diffused (dispersed). This can be contrasted with conductive heat transfer, which is the transfer of energy by vibrations at a molecular level through a solid or fluid, and radiative heat transfer, the transfer of energy through electromagnetic waves.
Heat is transferred by convection in numerous examples of naturally occurring fluid flow, such as: wind, oceanic currents, and movements within the Earth's mantle. Convection is also used in engineering practices of homes, industrial processes, cooling of equipment, etc.
The rate of convective heat transfer may be improved by the use of a heat sink, often in conjunction with a fan. For instance, a typical computer CPU will have a purpose-made fan to ensure its operating temperature is kept within tolerable limits.
Convection cells.
A convection cell, also known as a Bénard cell is a characteristic fluid flow pattern in many convection systems. A rising body of fluid typically loses heat because it encounters a cold surface. In liquid this occurs because it exchanges heat with colder liquid through direct exchange. In the example of the Earth's atmosphere, this occurs because it radiates heat. Because of this heat loss the fluid becomes denser than the fluid underneath it, which is still rising. Since it cannot descend through the rising fluid, it moves to one side. At some distance, its downward force overcomes the rising force beneath it, and the fluid begins to descend. As it descends, it warms again and the cycle repeats itself.
Atmospheric circulation.
Atmospheric circulation is the large-scale movement of air, and is a means by which thermal energy is distributed on the surface of the Earth, together with the much slower (lagged) ocean circulation system. The large-scale structure of the atmospheric circulation varies from year to year, but the basic climatological structure remains fairly constant.
Latitudinal circulation occurs because incident solar radiation per unit area is highest at the heat equator, and decreases as the latitude increases, reaching minima at the poles. It consists of two primary convection cells, the Hadley cell and the polar vortex, with the Hadley cell experiencing stronger convection due to the release of latent heat energy by condensation of water vapor at higher altitudes during cloud formation.
Longitudinal circulation, on the other hand, comes about because the ocean has a higher specific heat capacity than land (and also thermal conductivity, allowing the heat to penetrate further beneath the surface) and thereby absorbs and releases more heat, but the temperature changes less than land. This brings the sea breeze, air cooled by the water, ashore in the day, and carries the land breeze, air cooled by contact with the ground, out to sea during the night. Longitudinal circulation consists of two cells, the Walker circulation and El Niño / Southern Oscillation.
Weather.
Some more localized phenomena than global atmospheric movement are also due to convection, including wind and some of the hydrologic cycle. For example, a foehn wind is a down-slope wind which occurs on the downwind side of a mountain range. It results from the adiabatic warming of air which has dropped most of its moisture on windward slopes. Because of the different adiabatic lapse rates of moist and dry air, the air on the leeward slopes becomes warmer than at the same height on the windward slopes.
A thermal column (or thermal) is a vertical section of rising air in the lower altitudes of the Earth's atmosphere. Thermals are created by the uneven heating of the Earth's surface from solar radiation. The Sun warms the ground, which in turn warms the air directly above it. The warmer air expands, becoming less dense than the surrounding air mass, and creating a thermal low. The mass of lighter air rises, and as it does, it cools by expansion at lower air pressures. It stops rising when it has cooled to the same temperature as the surrounding air. Associated with a thermal is a downward flow surrounding the thermal column. The downward moving exterior is caused by colder air being displaced at the top of the thermal. Another convection-driven weather effect is the sea breeze.
Warm air has a lower density than cool air, so warm air rises within cooler air, similar to hot air balloons. Clouds form as relatively warmer air carrying moisture rises within cooler air. As the moist air rises, it cools, causing some of the water vapor in the rising packet of air to condense. When the moisture condenses, it releases energy known as latent heat of fusion which allows the rising packet of air to cool less than its surrounding air, continuing the cloud's ascension. If enough instability is present in the atmosphere, this process will continue long enough for cumulonimbus clouds to form, which support lightning and thunder. Generally, thunderstorms require three conditions to form: moisture, an unstable airmass, and a lifting force (heat).
All thunderstorms, regardless of type, go through three stages: the developing stage, the mature stage, and the dissipation stage. The average thunderstorm has a 24 km diameter. Depending on the conditions present in the atmosphere, these three stages take an average of 30 minutes to go through.
Oceanic circulation.
Solar radiation affects the oceans: warm water from the Equator tends to circulate toward the poles, while cold polar water heads towards the Equator. The surface currents are initially dictated by surface wind conditions. The trade winds blow westward in the tropics, and the westerlies blow eastward at mid-latitudes. This wind pattern applies a stress to the subtropical ocean surface with negative curl across the Northern Hemisphere, and the reverse across the Southern Hemisphere. The resulting Sverdrup transport is equatorward. Because of conservation of potential vorticity caused by the poleward-moving winds on the subtropical ridge's western periphery and the increased relative vorticity of poleward moving water, transport is balanced by a narrow, accelerating poleward current, which flows along the western boundary of the ocean basin, outweighing the effects of friction with the cold western boundary current which originates from high latitudes. The overall process, known as western intensification, causes currents on the western boundary of an ocean basin to be stronger than those on the eastern boundary.
As it travels poleward, warm water transported by strong warm water current undergoes evaporative cooling. The cooling is wind driven: wind moving over water cools the water and also causes evaporation, leaving a saltier brine. In this process, the water becomes saltier and denser. and decreases in temperature. Once sea ice forms, salts are left out of the ice, a process known as brine exclusion. These two processes produce water that is denser and colder (or, more precisely, water that is still liquid at a lower temperature). The water across the northern Atlantic ocean becomes so dense that it begins to sink down through less salty and less dense water. (The convective action is not unlike that of a lava lamp.) This downdraft of heavy, cold and dense water becomes a part of the North Atlantic Deep Water, a southgoing stream.
Mantle convection.
Mantle convection is the slow creeping motion of Earth's rocky mantle caused by convection currents carrying heat from the interior of the earth to the surface. It is the driving force that causes tectonic plates to move around the Earth's surface.
The Earth's surface is divided into a number of tectonic plates that are continuously being created and consumed at their opposite plate boundaries. Creation (accretion) occurs as mantle is added to the growing edges of a plate. This hot added material cools down by conduction and convection of heat. At the consumption edges of the plate, the material has thermally contracted to become dense, and it sinks under its own weight in the process of subduction at an ocean trench. This subducted material sinks to some depth in the Earth's interior where it is prohibited from sinking further. The subducted oceanic crust triggers volcanism.
Stack effect.
The Stack effect or chimney effect is the movement of air into and out of buildings, chimneys, flue gas stacks, or other containers due to buoyancy. Buoyancy occurs due to a difference in indoor-to-outdoor air density resulting from temperature and moisture differences. The greater the thermal difference and the height of the structure, the greater the buoyancy force, and thus the stack effect. The stack effect helps drive natural ventilation and infiltration. Some cooling towers operate on this principle; similarly the solar updraft tower is a proposed device to generate electricity based on the stack effect.
Stellar physics.
The convection zone of a star is the range of radii in which energy is transported primarily by convection.
Granules on the photosphere of the Sun are the visible tops of convection cells in the photosphere, caused by convection of plasma in the photosphere. The rising part of the granules is located in the center where the plasma is hotter. The outer edge of the granules is darker due to the cooler descending plasma. A typical granule has a diameter on the order of 1,000 kilometers and each lasts 8 to 20 minutes before dissipating. Below the photosphere is a layer of much larger "supergranules" up to 30,000 kilometers in diameter, with lifespans of up to 24 hours.
Convection mechanisms.
Convection may happen in fluids at all scales larger than a few atoms. There are a variety of circumstances in which the forces required for natural and forced convection arise, leading to different types of convection, described below. In broad terms, convection arises because of body forces acting within the fluid, such as gravity (buoyancy), or surface forces acting at a boundary of the fluid.
The causes of convection are generally described as one of either "natural" ("free") or "forced", although other mechanisms also exist (discussed below). However the distinction between natural and forced convection is particularly important for convective heat transfer.
Natural convection.
Natural convection, or free convection, occurs due to temperature differences which affect the density, and thus relative buoyancy, of the fluid. Heavier (more dense) components will fall, while lighter (less dense) components rise, leading to bulk fluid movement. Natural convection can only occur, therefore, in a gravitational field. A common example of natural convection is the rise of smoke from a fire. It can be seen in a pot of boiling water in which the hot and less-dense water on the bottom layer moves upwards in plumes, and the cool and more dense water near the top of the pot likewise sinks.
Natural convection will be more likely and/or more rapid with a greater variation in density between the two fluids, a larger acceleration due to gravity that drives the convection, and/or a larger distance through the convecting medium. Natural convection will be less likely and/or less rapid with more rapid diffusion (thereby diffusing away the thermal gradient that is causing the convection) and/or a more viscous (sticky) fluid.
The onset of natural convection can be determined by the Rayleigh number (Ra).
Note that differences in buoyancy within a fluid can arise for reasons other than temperature variations, in which case the fluid motion is called gravitational convection (see below). However, all types of buoyant convection, including natural convection, do not occur in microgravity environments. All require the presence of an environment which experiences g-force (proper acceleration).
Forced convection.
In forced convection, also called heat advection, fluid movement results from external surface forces such as a fan or pump. Forced convection is typically used to increase the rate of heat exchange. Many types of mixing also utilize forced convection to distribute one substance within another. Forced convection also occurs as a by-product to other processes, such as the action of a propeller in a fluid or aerodynamic heating. Fluid radiator systems, and also heating and cooling of parts of the body by blood circulation, are other familiar examples of forced convection.
Forced convection may happen by natural means, such as when the heat of a fire causes expansion of air and bulk air flow by this means. In microgravity, such flow (which happens in all directions) along with diffusion is the only means by which fires are able to draw in fresh oxygen to maintain themselves. The shock wave that transfers heat and mass out of explosions is also a type of forced convection.
Although forced convection from thermal gas expansion in zero-g does not fuel a fire as well as natural convection in a gravity field, some types of artificial forced convection are far more efficient than free convection, as they are not limited by natural mechanisms. For instance, a convection oven works by forced convection, as a fan which rapidly circulates hot air forces heat into food faster than would naturally happen due to simple heating without the fan.
Gravitational or buoyant convection.
Gravitational convection is a type of natural convection induced by buoyancy variations resulting from material properties other than temperature. Typically this is caused by a variable composition of the fluid. If the varying property is a concentration gradient, it is known as solutal convection. For example, gravitational convection can be seen in the diffusion of a source of dry salt downward into wet soil due to the buoyancy of fresh water in saline.
Variable salinity in water and variable water content in air masses are frequent causes of convection in the oceans and atmosphere which do not involve heat, or else involve additional compositional density factors other than the density changes from thermal expansion (see "thermohaline circulation"). Similarly, variable composition within the Earth's interior which has not yet achieved maximal stability and minimal energy (in other words, with densest parts deepest) continues to cause a fraction of the convection of fluid rock and molten metal within the Earth's interior (see below).
Gravitational convection, like natural thermal convection, also requires a g-force environment in order to occur.
Granular convection.
Vibration-induced convection occurs in powders and granulated materials in containers subject to vibration where an axis of vibration is parallel to the force of gravity. When the container accelerates upward, the bottom of the container pushes the entire contents upward. In contrast, when the container accelerates downward, the sides of the container push the adjacent material downward by friction, but the material more remote from the sides is less affected. The net result is a slow circulation of particles downward at the sides, and upward in the middle.
If the container contains particles of different sizes, the downward-moving region at the sides is often narrower than the largest particles. Thus, larger particles tend to become sorted to the top of such a mixture. This is one possible explanation of the Brazil nut effect.
Thermomagnetic convection.
Thermomagnetic convection can occur when an external magnetic field is imposed on a ferrofluid with varying magnetic susceptibility. In the presence of a temperature gradient this results in a nonuniform magnetic body force, which leads to fluid movement. A ferrofluid is a liquid which becomes strongly magnetized in the presence of a magnetic field.
This form of heat transfer can be useful for cases where conventional convection fails to provide adequate heat transfer, e.g., in miniature microscale devices or under reduced gravity conditions.
Capillary action.
Capillary action is a phenomenon where liquid spontaneously rises in a narrow space such as a thin tube, or in porous materials. This effect can cause liquids to flow against the force of gravity. It occurs because of inter-molecular attractive forces between the liquid and solid surrounding surfaces; If the diameter of the tube is sufficiently small, then the combination of surface tension and forces of adhesion between the liquid and container act to lift the liquid.
Marangoni effect.
The Marangoni effect is the convection of fluid along an interface between dissimilar substances because of variations in surface tension. Surface tension can vary because of inhomogeneous composition of the substances, and/or the temperature-dependence of surface tension forces. In the latter case the effect is known as thermo-capillary convection.
A well-known phenomenon exhibiting this type of convection is the "tears of wine".
Weissenberg effect.
The Weissenberg effect is a phenomenon that occurs when a spinning rod is placed into a solution of liquid polymer. Entanglements cause the polymer chains to be drawn towards the rod instead of being thrown outward as would happen with an ordinary fluid (i.e., water).
Combustion.
In a zero-gravity environment, there can be no buoyancy forces, and thus no natural (free) convection possible, so flames in many circumstances without gravity smother in their own waste gases. However, flames may be maintained with any type of forced convection (breeze); or (in high oxygen environments in "still" gas environments) entirely from the minimal forced convection that occurs as heat-induced "expansion" (not buoyancy) of gases allows for ventilation of the flame, as waste gases move outward and cool, and fresh high-oxygen gas moves in to take up the low pressure zones created when flame-exhaust water condenses.
Mathematical models of convection.
Mathematically, convection can be described by the convection–diffusion equation, also known as the generic scalar transport equation.
Quantifying natural versus forced convection.
In cases of mixed convection (natural and forced occurring together) one would often like to know how much of the convection is due to external constraints, such as the fluid velocity in the pump, and how much is due to natural convection occurring in the system.
The relative magnitudes of the Grashof and Reynolds number squared determine which form of convection dominates. If formula_1 forced convection may be neglected, whereas if formula_2 natural convection may be neglected. If the ratio is approximately one, then both forced and natural convection need to be taken into account.

</doc>
<doc id="47527" url="http://en.wikipedia.org/wiki?curid=47527" title="Cryosphere">
Cryosphere

The cryosphere (from the Greek κρύος "kryos", "cold", "frost" or "ice" and σφαῖρα "sphaira", "globe, ball") is those portions of Earth's surface where water is in solid form, including sea ice, lake ice, river ice, snow cover, glaciers, ice caps, ice sheets, and frozen ground (which includes permafrost). Thus, there is a wide overlap with the hydrosphere. The cryosphere is an integral part of the global climate system with important linkages and feedbacks generated through its influence on surface energy and moisture fluxes, clouds, precipitation, hydrology, atmospheric and oceanic circulation. Through these feedback processes, the cryosphere plays a significant role in the global climate and in climate model response to global changes. The term deglaciation describes the retreat of cryospheric features.
Structure.
Frozen water is found on the Earth’s surface primarily as snow cover, freshwater ice in lakes and rivers, sea ice, glaciers, ice sheets, and frozen ground and permafrost (permanently frozen ground). The residence time of water in each of these cryospheric sub-systems varies widely. Snow cover and freshwater ice are essentially seasonal, and most sea ice, except for ice in the central Arctic, lasts only a few years if it is not seasonal. A given water particle in glaciers, ice sheets, or ground ice, however, may remain frozen for 10-100,000 years or longer, and deep ice in parts of East Antarctica may have an age approaching 1 million years.
Most of the world’s ice volume is in Antarctica, principally in the East Antarctic Ice Sheet. In terms of areal extent, however, Northern Hemisphere winter snow and ice extent comprise the largest area, amounting to an average 23% of hemispheric surface area in January. The large areal extent and the important climatic roles of snow and ice, related to their unique physical properties, indicate that the ability to observe and model snow and ice-cover extent, thickness, and physical properties (radiative and thermal properties) is of particular significance for climate research.
There are several fundamental physical properties of snow and ice that modulate energy exchanges between the surface and the atmosphere. The most important properties are the surface reflectance (albedo), the ability to transfer heat (thermal diffusivity), and the ability to change state (latent heat). These physical properties, together with surface roughness, emissivity, and dielectric characteristics, have important implications for observing snow and ice from space. For example, surface roughness is often the dominant factor determining the strength of radar backscatter . Physical properties such as crystal structure, density, length, and liquid water content are important factors affecting the transfers of heat and water and the scattering of microwave energy.
The surface reflectance of incoming solar radiation is important for the surface energy balance (SEB). It is the ratio of reflected to incident solar radiation, commonly referred to as albedo. Climatologists are primarily interested in albedo integrated over the shortwave portion of the electromagnetic spectrum (~300 to 3500 nm), which coincides with the main solar energy input. Typically, albedo values for non-melting snow-covered surfaces are high (~80-90%) except in the case of forests. The higher albedos for snow and ice cause rapid shifts in surface reflectivity in autumn and spring in high latitudes, but the overall climatic significance of this increase is spatially and temporally modulated by cloud cover. (Planetary albedo is determined principally by cloud cover, and by the small amount of total solar radiation received in high latitudes during winter months.) Summer and autumn are times of high-average cloudiness over the Arctic Ocean so the albedo feedback associated with the large seasonal changes in sea-ice extent is greatly reduced. Groisman "et al." (1994a) observed that snow cover exhibited the greatest influence on the Earth radiative balance in the spring (April to May) period when incoming solar radiation was greatest over snow-covered areas.
The thermal properties of cryospheric elements also have important climatic consequences. Snow and ice have much lower thermal diffusivities than air. Thermal diffusivity is a measure of the speed at which temperature waves can penetrate a substance. Snow and ice are many orders of magnitude less efficient at diffusing heat than air. Snow cover insulates the ground surface, and sea ice insulates the underlying ocean, decoupling the surface-atmosphere interface with respect to both heat and moisture fluxes. The flux of moisture from a water surface is eliminated by even a thin skin of ice, whereas the flux of heat through thin ice continues to be substantial until it attains a thickness in excess of 30 to 40 cm. However, even a small amount of snow on top of the ice will dramatically reduce the heat flux and slow down the rate of ice growth. The insulating effect of snow also has major implications for the hydrological cycle. In non-permafrost regions, the insulating effect of snow is such that only near-surface ground freezes and deep-water drainage is uninterrupted.
While snow and ice act to insulate the surface from large energy losses in winter, they also act to retard warming in the spring and summer because of the large amount of energy required to melt ice (the latent heat of fusion, 3.34 x 105 J/kg at 0 °C). However, the strong static stability of the atmosphere over areas of extensive snow or ice tends to confine the immediate cooling effect to a relatively shallow layer, so that associated atmospheric anomalies are usually short-lived and local to regional in scale. In some areas of the world such as Eurasia, however, the cooling associated with a heavy snowpack and moist spring soils is known to play a role in modulating the summer monsoon circulation. Gutzler and Preston (1997) recently presented evidence for a similar snow-summer circulation feedback over the southwestern United States.
The role of snow cover in modulating the monsoon is just one example of a short-term cryosphere-climate feedback involving the land surface and the atmosphere. From Figure 1 it can be seen that there are numerous cryosphere-climate feedbacks in the global climate system. These operate over a wide range of spatial and temporal scales from local seasonal cooling of air temperatures to hemispheric-scale variations in ice sheets over time-scales of thousands of years. The feedback mechanisms involved are often complex and incompletely understood. For example, Curry "et al." (1995) showed that the so-called “simple” sea ice-albedo feedback involved complex interactions with lead fraction, melt ponds, ice thickness, snow cover, and sea-ice extent.
Snow.
Snow cover has the second-largest areal extent of any component of the cryosphere, with a mean maximum areal extent of approximately 47 million km². Most of the Earth’s snow-covered area (SCA) is located in the Northern Hemisphere, and temporal variability is dominated by the seasonal cycle; Northern Hemisphere snow-cover extent ranges from 46.5 million km² in January to 3.8 million km² in August. North American winter SCA has exhibited an increasing trend over much of this century (Brown and Goodison 1996; Hughes "et al." 1996) largely in response to an increase in precipitation. However, the available satellite data show that the hemispheric winter snow cover has exhibited little interannual variability over the 1972-1996 period, with a coefficient of variation (COV=s.d./mean) for January Northern Hemisphere snow cover of < 0.04. According to Groisman "et al." (1994a) Northern Hemisphere spring snow cover should exhibit a decreasing trend to explain an observed increase in Northern Hemisphere spring air temperatures this century. Preliminary estimates of SCA from historical and reconstructed in situ snow-cover data suggest this is the case for Eurasia, but not for North America, where spring snow cover has remained close to current levels over most of this century. Because of the close relationship observed between hemispheric air temperature and snow-cover extent over the period of satellite data (IPCC 1996), there is considerable interest in monitoring Northern Hemisphere snow-cover extent for detecting and monitoring climate change.
Snow cover is an extremely important storage component in the water balance, especially seasonal snowpacks in mountainous areas of the world. Though limited in extent, seasonal snowpacks in the Earth’s mountain ranges account for the major source of the runoff for stream flow and groundwater recharge over wide areas of the midlatitudes. For example, over 85% of the annual runoff from the Colorado River basin originates as snowmelt. Snowmelt runoff from the Earth’s mountains fills the rivers and recharges the aquifers that over a billion people depend on for their water resources. Further, over 40% of the world’s protected areas are in mountains, attesting to their value both as unique ecosystems needing protection and as recreation areas for humans. Climate warming is expected to result in major changes to the partitioning of snow and rainfall, and to the timing of snowmelt, which will have important implications for water use and management. These changes also involve potentially important decadal and longer time-scale feedbacks to the climate system through temporal and spatial changes in soil moisture and runoff to the oceans.(Walsh 1995). Freshwater fluxes from the snow cover into the marine environment may be important, as the total flux is probably of the same magnitude as desalinated ridging and rubble areas of sea ice. In addition, there is an associated pulse of precipitated pollutants which accumulate over the Arctic winter in snowfall and are released into the ocean upon ablation of the sea-ice .
Sea ice.
Sea ice covers much of the polar oceans and forms by freezing of sea water. Satellite data since the early 1970s reveal considerable seasonal, regional, and interannual variability in the sea-ice covers of both hemispheres. Seasonally, sea-ice extent in the Southern Hemisphere varies by a factor of 5, from a minimum of 3-4 million km² in February to a maximum of 17-20 million km² in September. The seasonal variation is much less in the Northern Hemisphere where the confined nature and high latitudes of the Arctic Ocean result in a much larger perennial ice cover, and the surrounding land limits the equatorward extent of wintertime ice. Thus, the seasonal variability in Northern Hemisphere ice extent varies by only a factor of 2, from a minimum of 7-9 million km² in September to a maximum of 14-16 million km² in March.
The ice cover exhibits much greater regional-scale interannual variability than it does hemispherical. For instance, in the region of the Sea of Okhotsk and Japan, maximum ice extent decreased from 1.3 million km² in 1983 to 0.85 million km² in 1984, a decrease of 35%, before rebounding the following year to 1.2 million km² . The regional fluctuations in both hemispheres are such that for any several-year period of the satellite record some regions exhibit decreasing ice coverage while others exhibit increasing ice cover. The overall trend indicated in the passive microwave record from 1978 through mid-1995 shows that the extent of Arctic sea ice is decreasing 2.7% per decade. Subsequent work with the satellite passive-microwave data indicates that from late October 1978 through the end of 1996 the extent of Arctic sea ice decreased by 2.9% per decade while the extent of Antarctic sea ice increased by 1.3% per decade.
Lake ice and river ice.
Ice forms on rivers and lakes in response to seasonal cooling. The sizes of the ice bodies involved are too small to exert other than localized climatic effects. However, the freeze-up/break-up processes respond to large-scale and local weather factors, such that considerable interannual variability exists in the dates of appearance and disappearance of the ice. Long series of lake-ice observations can serve as a proxy climate record, and the monitoring of freeze-up and break-up trends may provide a convenient integrated and seasonally specific index of climatic perturbations. Information on river-ice conditions is less useful as a climatic proxy because ice formation is strongly dependent on river-flow regime, which is affected by precipitation, snow melt, and watershed runoff as well as being subject to human interference that directly modifies channel flow, or that indirectly affects the runoff via land-use practices.
Lake freeze-up depends on the heat storage in the lake and therefore on its depth, the rate and temperature of any inflow, and water-air energy fluxes. Information on lake depth is often unavailable, although some indication of the depth of shallow lakes in the Arctic can be obtained from airborne radar imagery during late winter (Sellman "et al." 1975) and spaceborne optical imagery during summer (Duguay and Lafleur 1997). The timing of breakup is modified by snow depth on the ice as well as by ice thickness and freshwater inflow.
Frozen ground and permafrost.
Frozen ground (permafrost and seasonally frozen ground) occupies approximately 54 million km² of the exposed land areas of the Northern Hemisphere (Zhang et al., 2003) and therefore has the largest areal extent of any component of the cryosphere. Permafrost (perennially frozen ground) may occur where mean annual air temperatures (MAAT) are less than -1 or -2 °C and is generally continuous where MAAT are less than -7 °C. In addition, its extent and thickness are affected by ground moisture content, vegetation cover, winter snow depth, and aspect. The global extent of permafrost is still not completely known, but it underlies approximately 20% of Northern Hemisphere land areas. Thicknesses exceed 600 m along the Arctic coast of northeastern Siberia and Alaska, but, toward the margins, permafrost becomes thinner and horizontally discontinuous. The marginal zones will be more immediately subject to any melting caused by a warming trend. Most of the presently existing permafrost formed during previous colder conditions and is therefore relic. However, permafrost may form under present-day polar climates where glaciers retreat or land emergence exposes unfrozen ground. Washburn (1973) concluded that most continuous permafrost is in balance with the present climate at its upper surface, but changes at the base depend on the present climate and geothermal heat flow; in contrast, most discontinuous permafrost is probably unstable or "in such delicate equilibrium that the slightest climatic or surface change will have drastic disequilibrium effects".
Under warming conditions, the increasing depth of the summer active layer has significant impacts on the hydrologic and geomorphic regimes. Thawing and retreat of permafrost have been reported in the upper Mackenzie Valley and along the southern margin of its occurrence in Manitoba, but such observations are not readily quantified and generalized. Based on average latitudinal gradients of air temperature, an average northward displacement of the southern permafrost boundary by 50-to-150 km could be expected, under equilibrium conditions, for a 1 °C warming.
Only a fraction of the permafrost zone consists of actual ground ice. The remainder (dry permafrost) is simply soil or rock at subfreezing temperatures. The ice volume is generally greatest in the uppermost permafrost layers and mainly comprises pore and segregated ice in Earth material. Measurements of bore-hole temperatures in permafrost can be used as indicators of net changes in temperature regime. Gold and Lachenbruch (1973) infer a 2-4 °C warming over 75 to 100 years at Cape Thompson, Alaska, where the upper 25% of the 400-m thick permafrost is unstable with respect to an equilibrium profile of temperature with depth (for the present mean annual surface temperature of -5 °C). Maritime influences may have biased this estimate, however. At Prudhoe Bay similar data imply a 1.8 °C warming over the last 100 years (Lachenbruch "et al." 1982). Further complications may be introduced by changes in snow-cover depths and the natural or artificial disturbance of the surface vegetation.
The potential rates of permafrost thawing have been established by Osterkamp (1984) to be two centuries or less for 25-meter-thick permafrost in the discontinuous zone of interior Alaska, assuming warming from -0.4 to 0 °C in 3–4 years, followed by a further 2.6 °C rise. Although the response of permafrost (depth) to temperature change is typically a very slow process (Osterkamp 1984; Koster 1993), there is ample evidence for the fact that the active layer thickness quickly responds to a temperature change (Kane "et al." 1991). Whether, under a warming or cooling scenario, global climate change will have a significant effect on the duration of frost-free periods in both regions with seasonally and perennially frozen ground.
Glaciers and ice sheets.
Ice sheets and glaciers are flowing ice masses that rest on solid land. They are controlled by snow accumulation, surface and basal melt, calving into surrounding oceans or lakes and internal dynamics. The latter results from gravity-driven creep flow ("glacial flow") within the ice body and sliding on the underlying land, which leads to thinning and horizontal spreading. Any imbalance of this dynamic equilibrium between mass gain, loss and transport due to flow results in either growing or shrinking ice bodies.
Ice sheets are the greatest potential source of global freshwater, holding approximately 77% of the global total. This corresponds to 80 m of world sea-level equivalent, with Antarctica accounting for 90% of this. Greenland accounts for most of the remaining 10%, with other ice bodies and glaciers accounting for less than 0.5%. Because of their size in relation to annual rates of snow accumulation and melt, the residence time of water in ice sheets can extend to 100,000 or 1 million years. Consequently, any climatic perturbations produce slow responses, occurring over glacial and interglacial periods. Valley glaciers respond rapidly to climatic fluctuations with typical response times of 10–50 years. However, the response of individual glaciers may be asynchronous to the same climatic forcing because of differences in glacier length, elevation, slope, and speed of motion. Oerlemans (1994) provided evidence of coherent global glacier retreat which could be explained by a linear warming trend of 0.66 °C per 100 years.
While glacier variations are likely to have minimal effects upon global climate, their recession may have contributed one third to one half of the observed 20th Century rise in sea level (Meier 1984; IPCC 1996). Furthermore, it is extremely likely that such extensive glacier recession as is currently observed in the Western Cordillera of North America, where runoff from glacierized basins is used for irrigation and hydropower, involves significant hydrological and ecosystem impacts. Effective water-resource planning and impact mitigation in such areas depends upon developing a sophisticated knowledge of the status of glacier ice and the mechanisms that cause it to change. Furthermore, a clear understanding of the mechanisms at work is crucial to interpreting the global-change signals that are contained in the time series of glacier mass balance records.
Combined glacier mass balance estimates of the large ice sheets carry an uncertainty of about 20%. Studies based on estimated snowfall and mass output tend to indicate that the ice sheets are near balance or taking some water out of the oceans. Marinebased studies suggest sea-level rise from the Antarctic or rapid ice-shelf basal melting. Some authors (Paterson 1993; Alley 1997) have suggested that the difference between the observed rate of sea-level rise (roughly 2 mm/y) and the explained rate of sea-level rise from melting of mountain glaciers, thermal expansion of the ocean, etc. (roughly 1 mm/y or less) is similar to the modeled imbalance in the Antarctic (roughly 1 mm/y of sea-level rise; Huybrechts 1990), suggesting a contribution of sea-level rise from the Antarctic.
Relationships between global climate and changes in ice extent are complex. The mass balance of land-based glaciers and ice sheets is determined by the accumulation of snow, mostly in winter, and warm-season ablation due primarily to net radiation and turbulent heat fluxes to melting ice and snow from warm-air advection,(Munro 1990). However, most of Antarctica never experiences surface melting. Where ice masses terminate in the ocean, iceberg calving is the major contributor to mass loss. In this situation, the ice margin may extend out into deep water as a floating ice shelf, such as that in the Ross Sea. Despite the possibility that global warming could result in losses to the Greenland ice sheet being offset by gains to the Antarctic ice sheet, there is major concern about the possibility of a West Antarctic Ice Sheet collapse. The West Antarctic Ice Sheet is grounded on bedrock below sea level, and its collapse has the potential of raising the world sea level 6–7 m over a few hundred years.
Most of the discharge of the West Antarctic Ice Sheet is via the five major ice streams (faster flowing ice) entering the Ross Ice Shelf, the Rutford Ice Stream entering Ronne-Filchner shelf of the Weddell Sea, and the Thwaites Glacier and Pine Island Glacier entering the Amundsen Ice Shelf. Opinions differ as to the present mass balance of these systems (Bentley 1983, 1985), principally because of the limited data. The West Antarctic Ice Sheet is stable so long as the Ross Ice Shelf is constrained by drag along its lateral boundaries and pinned by local grounding.

</doc>
<doc id="47530" url="http://en.wikipedia.org/wiki?curid=47530" title="Cumulonimbus cloud">
Cumulonimbus cloud

Cumulonimbus, from the Latin cumulus ("heap") and nimbus ("rainstorm", "storm cloud"), is a dense towering vertical cloud associated with thunderstorms and atmospheric instability, forming from water vapor carried by powerful upward air currents. Cumulonimbus can form alone, in clusters, or along cold front squall lines. These clouds are capable of producing lightning and other dangerous severe weather, such as tornadoes. Cumulonimbus progress from overdeveloped cumulus congestus clouds and may further develop as part of a supercell. Cumulonimbus is abbreviated Cb and are designated in the D2 family.
Appearance.
Towering cumulonimbus clouds are typically accompanied by smaller cumulus clouds. The cumulonimbus base may extend several miles across and occupy low to middle altitudes- formed at altitude from approximately 200 to. Peaks typically reach to as much as 6000 m, with extreme instances as high as 23000 m. Well-developed cumulonimbus clouds are characterized by a flat, anvil-like top (anvil dome), caused by wind shear or inversion near the tropopause. The shelf of the anvil may precede the main cloud's vertical component for many miles, and be accompanied by lightning. Occasionally, rising air parcels surpass the equilibrium level (due to momentum) and form an overshooting top culminating at the maximum parcel level. When vertically developed, this largest of all clouds usually extends through all three cloud regions. Even the smallest cumulonimbus cloud dwarfs its neighbors in comparison.
Effects.
Cumulonimbus storm cells can produce torrential rain of a convective nature and flash flooding, as well as straight-line winds. Most storm cells die after about 20 minutes, when the precipitation causes more downdraft than updraft, causing the energy to dissipate. If there is enough solar energy in the atmosphere, however (on a hot summer's day, for example), the moisture from one storm cell can evaporate rapidly—resulting in a new cell forming just a few miles from the former one. This can cause thunderstorms to last for several hours. Cumulonimbus clouds can also bring dangerous winter storms (called "blizzards") which bring lightning, thunder, and torrential snow. However, cumulonimbus clouds are most common in tropical regions.
Life cycle or stages.
In general, cumulonimbus require moisture, an unstable air mass, and a lifting force (heat) in order to form. Cumulonimbus typically go through three stages: the developing stage, the mature stage (where the main cloud may reach supercell status in favorable conditions), and the dissipation stage. The average thunderstorm has a 24 km diameter. Depending on the conditions present in the atmosphere, these three stages take an average of 30 minutes to go through.
Cloud types.
Clouds form when the dewpoint of water is reached in the presence of condensation nuclei in the troposphere. The atmosphere is a dynamic system, and the local conditions of turbulence, uplift and other parameters give rise to many types of clouds. Various types of cloud occur frequently enough to have been categorized. Furthermore, some atmospheric processes can make the clouds organize in distinct patterns such as wave clouds or actinoform clouds. These are large-scale structures and are not always readily identifiable from single point of view.

</doc>
<doc id="47532" url="http://en.wikipedia.org/wiki?curid=47532" title="Cumulus cloud">
Cumulus cloud

 "Cumulo-" means "heap" or "pile" in Latin. They are often described as "puffy" or "cotton-like" in appearance, and have flat bases. Cumulus clouds, being low-stage clouds, are generally less than 1,000 m (3,300 ft) in altitude unless they are the more vertical cumulus congestus form. Cumulus clouds may appear by themselves, in lines, or in clusters.
Cumulus clouds are often precursors of other types of cloud, such as cumulonimbus, when influenced by weather factors such as instability, moisture, and temperature gradient. Normally, cumulus clouds produce little or no precipitation, but they can grow into the precipitation-bearing congestus or cumulonimbus clouds. Cumulus clouds can be formed from water vapor, supercooled water droplets, or ice crystals, depending upon the ambient temperature. They come in many distinct subforms, and generally cool the earth by reflecting the incoming solar radiation. Cumulus clouds are part of the larger category of free-convective cumuliform clouds, which include cumulonimbus clouds. The latter genus-type is sometimes categorized separately as cumulonimbiform due to its more complex structure that often includes a cirriform or anvil top. There are also cumuliform clouds of limited convection that comprise stratocumulus (low-étage), altocumulus (middle-étage) and cirrocumulus. (high-étage). These last three genus-types are sometimes classified separately as stratocumuliform.
Formation.
Cumulus clouds form via atmospheric convection as air warmed by the surface begins to rise. As the air rises, the temperature drops (following the lapse rate), causing the relative humidity (RH) to rise. If convection reaches a certain level the RH reaches one hundred percent, and the "wet-adiabatic" phase begins. At this point a positive feedback ensues: since the RH is above 100%, water vapour condenses, releasing latent heat, warming the air and spurring further convection.
In this phase, water vapor condenses on various nuclei present in the air, forming the cumulus cloud. This creates the characteristic flat-bottomed puffy shape associated with cumulus clouds. The size of the cloud depends on the temperature profile of the atmosphere and the presence of any inversions. During the convection, surrounding air is entrained (mixed) with the thermal and the total mass of the ascending air increases.
Rain forms in a cumulus cloud via a process involving two non-discrete stages. The first stage occurs after the droplets coalesce onto the various nuclei. Langmuir writes that surface tension in the water droplets provides a slightly higher pressure on the droplet, raising the vapor pressure by a small amount. The increased pressure results in those droplets evaporating and the resulting water vapor condensing on the larger droplets. Due to the extremely small size of the evaporating water droplets, this process becomes largely meaningless after the larger droplets have grown to around 20 to 30 micrometres, and the second stage takes over. In the accretion phase, the raindrop begins to fall, and other droplets collide and combine with it to increase the size of the raindrop. Langmuir was able to develop a formula which predicted that the droplet radius would grow unboundedly within a discrete time period.
Description.
The liquid water density within a cumulus cloud has been found to change with height above the cloud base rather than being approximately constant throughout the cloud. At the cloud base, the concentration was 0 grams of liquid water per kilogram of air. As altitude increased, the concentration rapidly increased to the maximum concentration near the middle of the cloud. The maximum concentration was found to be anything up to 1.25 grams of water per kilogram of air. The concentration slowly dropped off as altitude increased to the height of the top of the cloud, where it immediately dropped to zero again.
Cumulus clouds can form in lines stretching over 480 km long called cloud streets. These cloud streets cover vast areas and may be broken or continuous. They form when wind shear causes horizontal circulation in the atmosphere, producing the long, tubular cloud streets. They generally form during high-pressure systems, such as after a cold front.
The height at which the cloud forms depends on the amount of moisture in the thermal that forms the cloud. Humid air will generally result in a lower cloud base. In temperate areas, the base of the cumulus clouds is usually below 550 m above ground level, but it can range up to 2400 m in altitude. In arid and mountainous areas, the cloud base can be in excess of 6100 m.
Cumulus clouds can be composed of ice crystals, water droplets, supercooled water droplets, or a mixture of them. The water droplets form when water vapor condenses on the nuclei, and they may then coalesce into larger and larger droplets. In temperate regions, the cloud bases studied ranged from 500 to above ground level. These clouds were normally above 25 C, and the concentration of droplets ranged from 23 to 1300 droplets per cubic centimeter (380 to 21,300 droplets per cubic inch). This data was taken from growing isolated cumulus clouds that were not precipitating. The droplets were very small, ranging down to around 5 micrometers in diameter. Although smaller droplets may have been present, the measurements were not sensitive enough to detect them. The smallest droplets were found in the lower portions of the clouds, with the percentage of large droplets (around 20 to 30 micrometers) rising dramatically in the upper regions of the cloud. The droplet size distribution was slightly bimodal in nature, with peaks at the small and large droplet sizes and a slight trough in the intermediate size range. The skew was roughly neutral. Furthermore, large droplet size is roughly inversely proportional to the droplet concentration per unit volume of air. In places, cumulus clouds can have "holes" where there are no water droplets. These can occur when winds tear the cloud and incorporate the environmental air or when strong downdrafts evaporate the water.
Subforms.
Cumulus clouds come in four distinct species, "cumulis humilis", "mediocris", "congestus", and "fractus". These species may be arranged into the variety, "cumulus radiatus"; and may be accompanied by up to seven supplementary features, "cumulus pileus", "velum", "virga", "praecipitatio", "arcus", "pannus", and "tuba".
The species "Cumulus fractus" is ragged in appearance and can form in clear air as a precursor to cumulus humilis and larger cumulus species-types; or it can form in precipitation as the supplementary feature "pannus" (also called scud) which can also include stratus fractus of bad weather. "Cumulus humilis" clouds look like puffy, flattened shapes. "Cumulus mediocris" clouds look similar, except that they have some vertical development. "Cumulus congestus" clouds have a cauliflower-like structure and tower high into the atmosphere, hence their alternate name "towering cumulus". The variety "Cumulus radiatus" forms in radial bands called cloud streets and can comprise any of the four species of cumulus.
Cumulus supplementary features are most commonly seen with the species congestus. "Cumulus virga" clouds are cumulus clouds producing virga (precipitation that evaporates while aloft), and "cumulus praecipitatio" produce precipitation that reaches the Earth's surface. "Cumulus pannus" comprise shredded clouds that normally appear beneath the parent cumulus cloud during precipitation. "Cumulus arcus" clouds have a gust front, and "cumulus tuba" clouds have funnel clouds or tornadoes. "Cumulus pileus" clouds refer to cumulus clouds that have grown so rapidly as to force the formation of pileus over the top of the cloud. "Cumulus velum" clouds have an ice crystal veil over the growing top of the cloud.
Forecast.
Cumulus humilis clouds usually indicate fair weather. Cumulus mediocris clouds are similar, except that they have some vertical development, which implies that they can grow into cumulus congestus or even cumulonimbus clouds, which can produce heavy rain, lightning, severe winds, hail, and even tornadoes. Cumulus congestus clouds, which appear as towers, will often grow into cumulonimbus storm clouds. They can produce precipitation. Glider pilots often pay close attention to cumulus clouds, as they can be indicators of rising air drafts or thermals underneath that can suck the plane high into the sky—a phenomenon known as cloud suck.
Cumulus clouds can also produce acid rain or possibly a tornado. The acidity is largely formed by the oxidation of sulfur dioxide, the most plentiful acidifying gas, into sulfate ions. The main oxidizing compounds are hydrogen peroxide and ozone. Various nitrogen oxides can also react with hydroxide ions to form acids.
Effects on climate.
Due to reflectivity, clouds cool the earth by around 12 C-change, an effect largely caused by stratocumulus clouds. However, at the same time, they heat the earth by around 7 C-change by reflecting emitted radiation, an effect largely caused by cirrus clouds. This averages out to a net loss of 5 C-change. Cumulus clouds, on the other hand, have a variable effect on heating the earth's surface. The more vertical "cumulus congestus" species and cumulonimbus genus of clouds grow high into the atmosphere, carrying moisture with them, which can lead to the formation of cirrus clouds. The researchers speculated that this might even produce a positive feedback, where the increasing upper atmospheric moisture further warms the earth, resulting in an increasing number of "cumulus congestus" clouds carrying more moisture into the upper atmosphere.
Relation to other clouds.
Cumulus clouds are a genus of free-convective low-étage cloud along with the related limited-convective cumuliform or stratocumuliform cloud stratocumulus. These clouds form from ground level to 2000 m at all latitudes. Stratus clouds are also low-étage. In the middle étage are the alto clouds, which consist of the limiited-convective cumuliform or stratocumuliform cloud altocumulus and the stratiform cloud altostratus. Middle-étage clouds form from 2000 m to 7000 m in polar areas, 7000 m in temperate areas, and 7600 m in tropical areas. The high-étage clouds are all cirriform, one of which, cirrocumulus, is also cumuliform of limited convection or stratocumuliform. The other clouds in this étage are cirrus and cirrostratus. High-étage clouds form 3000 to in high latitudes, 5000 to in temperate latitudes, and 6100 to in low, tropical latitudes. Cumulonimbus clouds, like cumulus congestus, extend vertically rather than remaining confined to one étage.
Cirrocumulus clouds.
Cirrocumulus clouds form in patches and cannot cast shadows. They commonly appear in regular, rippling patterns or in rows of clouds with clear areas between. Cirrocumulus are, like other members of the cumuliform and stratocumuliform categories, formed via convective processes. Significant growth of these patches indicates high-altitude instability and can signal the approach of poorer weather. The ice crystals in the bottoms of cirrocumulus clouds tend to be in the form of hexagonal cylinders. They are not solid, but instead tend to have stepped funnels coming in from the ends. Towards the top of the cloud, these crystals have a tendency to clump together. These clouds do not last long, and they tend to change into cirrus because as the water vapor continues to deposit on the ice crystals, they eventually begin to fall, destroying the upward convection. The cloud then dissipates into cirrus. Cirrocumulus clouds come in four species which are common to all three genus-types that have limited-convective or stratocumuliform characteristics: "stratiformis", "lenticularis", "castellanus", and "floccus". They are iridescent when the constituent supercooled water droplets are all about the same size.
Altocumulus clouds.
Altocumulus clouds are a middle-étage cloud that forms from 2000 m high to 4000 m in polar areas, 7000 m in temperate areas, and 7600 m in tropical areas. They can have precipitation and are commonly composed of a mixture of ice crystals, supercooled water droplets, and water droplets in temperate latitudes. However, the liquid water concentration was almost always significantly greater than the concentration of ice crystals, and the maximum concentration of liquid water tended to be at the top of the cloud while the ice concentrated itself at the bottom. The ice crystals in the base of the altocumulus clouds and in the virga were found to be dendrites or conglomerations of dendrites while needles and plates resided more towards the top. Altocumulus clouds can form via convection or via the forced uplift caused by a warm front. Because Altocumulus is a genus-type of limited convection, it is divided into the same four species as cirrocumulus.
Stratocumulus clouds.
A stratocumulus cloud is another type of a cumuliform or stratocumuliform cloud. Like cumulus clouds, they form at low levels and via convection. However, unlike cumulus clouds, their growth is almost completely retarded by a strong inversion. As a result, they flatten out like stratus clouds, giving them a layered appearance. These clouds are extremely common, covering on average around twenty-three percent of the earth's oceans and twelve percent of the earth's continents. They are less common in tropical areas and commonly form after cold fronts. Additionally, stratocumulus clouds reflect a large amount of the incoming sunlight, producing a net cooling effect. Stratocumulus clouds can produce drizzle, which stabilizes the cloud by warming it and reducing turbulent mixing. Being a cloud of limited convection, stratocumulus is divided into three species; stratiformis, lenticularis, and castellanus, that are common to the higher stratocumuliform genus-types.
Cumulonimbus clouds.
Cumulonimbus clouds are the final form of growing cumulus clouds. They form when "cumulus congestus" clouds develop a strong updraft that propels their tops higher and higher into the atmosphere until they reach the tropopause at 18000 m in altitude. Cumulonimbus clouds, commonly called thunderheads, can produce high winds, torrential rain, lightning, gust fronts, waterspouts, funnel clouds, and tornadoes. They commonly have anvil clouds.
Extraterrestrial.
Some cumuliform clouds have been discovered on most other planets in the solar system. On Mars, the Viking Orbiter detected cirrocumulus and stratocumulus clouds forming via convection primarily near the polar icecaps. The Galileo space probe detected massive cumulonimbus clouds near the Great Red Spot on Jupiter. Cumuliform clouds have also been detected on Saturn. In 2008, the Cassini spacecraft determined that cumulus clouds near Saturn's south pole were part of a cyclone over 4000 km in diameter. The Keck Observatory detected whitish cumulus clouds on Uranus. Like Uranus, Neptune has methane cumulus clouds. Venus, however, does not appear to have cumulus clouds.

</doc>
<doc id="47535" url="http://en.wikipedia.org/wiki?curid=47535" title="Haptophyte">
Haptophyte

The haptophytes, classified either as the Prymnesiophyta (named for "Prymnesium") or Haptophyta, are a division of algae.
The names Haptophyceae or Prymnesiophyceae are sometimes used instead. This ending implies classification at the class rank rather than as a division. Although the phylogenetics of this group has become much better understood in recent years, there remains some dispute over which rank is most appropriate.
Characteristics.
The chloroplasts are pigmented similarly to those of the heterokonts, but the structure of the rest of the cell is different, so it may be that they are a separate line whose chloroplasts are derived from similar red algal endosymbionts.
The cells typically have two slightly unequal flagella, both of which are smooth, and a unique organelle called a "haptonema", which is superficially similar to a flagellum but differs in the arrangement of microtubules and in its use. The name comes from the Greek "hapsis", touch, and "nema", thread. The mitochondria have tubular cristae.
Economic importance.
Haptophytes are economically important as "Pavlova lutheri" and "Isochrysis sp." are widely used in the aquaculture industries.
Examples and classification.
The haptophytes were first placed in the class Chrysophyceae (golden algae) but ultrastructural data have provided evidence to classify them separately. The best-known haptophytes are coccolithophores, which have an exoskeleton of calcareous plates called coccoliths. Coccolithophores are some of the most abundant marine phytoplankton, especially in the open ocean and are extremely abundant as microfossils. Other planktonic haptophytes of note include "Chrysochromulina" and "Prymnesium", which periodically form toxic marine algal blooms, and "Phaeocystis" blooms of which can produce unpleasant foam which often accumulates on beaches. Both molecular and morphological evidence supports their division into five orders; coccolithophores make up the Isochrysidales and Coccolithales. Very small (2-3μm) uncultured pico-prymnesiophytes are ecologically important
Haptophytes are closely related to cryptomonads.

</doc>
<doc id="47537" url="http://en.wikipedia.org/wiki?curid=47537" title="Configuration">
Configuration

The term configuration has several meanings.
In computing it may refer to:
Other usages include:

</doc>
<doc id="47541" url="http://en.wikipedia.org/wiki?curid=47541" title="United Nations Security Council Resolution 242">
United Nations Security Council Resolution 242

United Nations Security Council Resolution 242 (S/RES/242) was adopted unanimously by the UN Security Council on November 22, 1967, in the aftermath of the Six-Day War. It was adopted under Chapter VI of the UN Charter. The resolution was sponsored by British ambassador Lord Caradon and was one of five drafts under consideration.
The preamble refers to the "inadmissibility of the acquisition of territory by war and the need to work for a just and lasting peace in the Middle East in which every State in the area can live in security."
Operative Paragraph One "Affirms that the fulfillment of Charter principles requires the establishment of a just and lasting peace in the Middle East which should include the application of both the following principles:
Egypt, Jordan, Israel and Lebanon entered into consultations with the UN Special representative over the implementation of 242. After denouncing it in 1967, Syria "conditionally" accepted the resolution in March 1972. Syria formally accepted UN Security Council Resolution 338, the cease-fire at the end of the Yom Kippur War (in 1973), which embraced resolution 242. 
On 1 May 1968, the Israeli ambassador to the UN expressed Israel's position to the Security Council: "My government has indicated its acceptance of the Security Council resolution for the promotion of agreement on the establishment of a just and lasting peace. I am also authorized to reaffirm that we are willing to seek agreement with each Arab State on all matters included in that resolution."
In a statement to the General Assembly on 15 October 1968, the PLO rejected Resolution 242, saying "the implementation of said resolution will lead to the loss of every hope for the establishment of peace and security in Palestine and the Middle East region." In September 1993, the PLO agreed that Resolutions 242 and 338 should be the basis for negotiations with Israel when it signed the Declaration of Principles.
Resolution 242 is one of the most widely affirmed resolutions on the Arab–Israeli conflict and formed the basis for later negotiations between the parties. These led to Peace Treaties between Israel and Egypt (1979) and Jordan (1994), as well as the 1993 and 1995 agreements with the Palestinians.
Context.
The resolution is the formula proposed by the Security Council for the successful resolution of the Arab-Israeli conflict, in particular, ending the state of belligerency then existing between the 'States concerned', Israel and Egypt, Jordan, Syria and Lebanon. The resolution deals with five principles; withdrawal of Israeli forces, 'peace within secure and recognized boundaries', freedom of navigation, a just settlement of the refugee problem and security measures including demilitarized zones. It also provided for the appointment of a Special Representative to proceed to the Middle East in order to promote agreement on a peaceful and accepted settlement in accordance with the principles outlined in the resolution.
Upon presenting the draft resolution to the Security Council, the U.K. representative Lord Caradon said:
All of us recognize that peace is the prize. None of us wishes a temporary truce or a superficial accommodation. We could never advocate a return to uneasy hostility. As I have said, my Government would never wish to be associated with any so-called settlement which was only a continuation of a false truce, and all of us without any hesitation at all can agree that we seek a settlement within the principles laid down in Article 2 of the Charter. So much for the preamble.
As to the first operative paragraph, and with due respect for fulfillment of Charter principles, we consider it essential that there should be applied the principles of both withdrawal and security, and we have no doubt that the words set out throughout that paragraph are perfectly clear.
As to the second operative paragraph, there is I believe no vestige of disagreement between us all that there must be a guarantee of freedom of navigation through international waterways. There must be a just settlement of the refugee problem. There must be a guarantee and adequate means to ensure the territorial inviolability and political independence of every State in the area.
As to the third operative paragraph, I have said before that I consider that the United Nations special representative should be free to decide himself the exact means and methods by which he pursues his endeavors in contact with the States concerned both to promote agreement and to assist efforts to achieve a peaceful and accepted and final settlement."
Secretary of State Dean Rusk commented on the most significant area of disagreement regarding the resolution:There was much bickering over whether that resolution should say from "the" territories or from "all" territories. In the French version, which is equally authentic, it says withdrawal de territory, with de meaning "the." We wanted that to be left a little vague and subject to future negotiation because we thought the Israeli border along the West Bank could be "rationalized"; certain anomalies could easily be straightened out with some exchanges of territory, making a more sensible border for all parties. We also wanted to leave open demilitarization measures in the Sinai and the Golan Heights and take a fresh look at the old city of Jerusalem. But we never contemplated any significant grant of territory to Israel as a result of the June 1967 war. On that point we and the Israelis to this day remain sharply divided. This situation could lead to real trouble in the future. Although every President since Harry Truman has committed the United States to the security and independence of Israel, I'm not aware of any commitment the United States has made to assist Israel in retaining territories seized in the Six-Day War.
A memorandum from the President's Special Assistant, Walt Rostow, to President Johnson said: "What's on the Arab Ambassadors' minds boils down to one big question: Will we make good on our pledge to support the territorial integrity of all states in the Middle East? Our best answer is that we stand by that pledge, but the only way to make good on it is to have a genuine peace. The tough question is whether we'd force Israel back to 4 June borders if the Arabs accepted terms that amounted to an honest peace settlement. Secretary Rusk told the Yugoslav Foreign Minister: 'The US had no problem with frontiers as they existed before the outbreak of hostilities. If we are talking about national frontiers--in a state of peace--then we will work toward restoring them.' But we all know that could lead to a tangle with the Israelis."
Rusk met with Foreign Minister Nikezic on August 30, 1967. However, according to telegram 30825 to Belgrade, September 1, which summarizes the conversation, Rusk said the key to a settlement was to end the state of war and belligerence and that if a way could be found to deal with this, other things would fall into place; the difference between pre-June 5 positions and secure national boundaries was an important difference.
President Johnson responded to a complaint from President Tito that Israel could change the frontiers without Arab consent: "You note that the Arabs feel the US interprets the draft resolution to imply a change of frontiers to their detriment. We have no preconceptions on frontiers as such. What we believe to be important is that the frontiers be secure. For this the single most vital condition is that they be acceptable to both sides. It is a source of regret to us that the Arabs appear to misunderstand our proposal and misread our motives."
Furthermore, Secretary Rusk's Telegram dated March 2, 1968 to the U.S. Interests Section of the Spanish Embassy in Cairo summarizing Undersecretary of State for Political Affairs Eugene Rostow’s conversation with Soviet Ambassador Anatoly Dobrynin states:
Rostow said ... resolution required agreement on "secure and recognized" boundaries, which, as practical matter, and as matter of interpreting resolution, had to precede withdrawals. Two principles were basic to Article I of resolution. Paragraph from which Dobrynin quoted was linked to others, and he did not see how anyone could seriously argue, in light of history of resolution in Security Council, withdrawal to borders of June 4th was contemplated. These words had been pressed on Council by Indians and others, and had not been accepted. Rusk 
In an address delivered on September 1, 1982 President Ronald Reagan said:
In the pre-1967 borders Israel was barely 10 miles wide at its narrowest point. The bulk of Israel's population lived within artillery range of hostile Arab armies. I am not about to ask Israel to live that way again...
So the United States will not support the establishment of an independent Palestinian state in the West Bank and Gaza, and we will not support annexation or permanent control by Israel.
There is, however, another way to peace. The final status of these lands must, of course, be reached through the give-and-take of negotiations; but it is the firm view of the United States that self-government by the Palestinians of the West Bank and Gaza in association with Jordan offers the best chance for a durable, just and lasting peace.
It is the United States' position that - in return for peace - the withdrawal provision of Resolution 242 applies to all fronts, including the West Bank and Gaza.
When the border is negotiated between Jordan and Israel, our view on the extent to which Israel should be asked to give up territory will be heavily affected by the extent of true peace and normalization and the security arrangements offered in return.
Finally, we remain convinced that Jerusalem must remain undivided, but its final status should be decided through negotiations.
According to Michael Lynk, there are three schools of thought concerning the proper legal interpretation of the withdrawal phrase. Some of the parties involved have suggested that the indefinite language is a “perceptible loophole”, that authorizes “territorial revision” for Israel’s benefit. Some have stated that the indefinite language was used to permit insubstantial and mutually beneficial alterations to the 1949 armistices lines, but that unilateral annexation of the captured territory was never authorized. Other parties have said that no final settlement obtained through force or the threat of force could be considered valid. They insist that the Security Council cannot create loopholes in peremptory norms of international law or the UN Charter, and that any use of indefinite language has to be interpreted in line with the overriding legal principles regarding the “inadmissibility of the acquisition of territory by war” and the prohibitions on mass deportations or displacement in connection with the settlement of the refugee problem.
Alexander Orakhelashvili says that the Security Council manifestly lacks the competence to validate agreements imposed through coercion, not least because the peremptory prohibition of the use of force is a limitation on the Council’s powers and the voidness of coercively imposed treaties is the clear consequence of jus cogens and the conventional law as reflected in the Vienna Convention on the Law of Treaties. A recent South African study concluded that the ultimate status and boundaries will require negotiation between the parties, according to Security Council Resolutions 242 and 338. The same study also found that the provisions of the Fourth Geneva Convention which govern ‘special agreements’ that can adversely affect the rights of protected persons precludes any change in status of the territory obtained through an agreement concluded during a state of belligerent occupation.
Content.
Preamble.
The second preambular reference states: "Emphasizing the inadmissibility of the acquisition of territory by war and the need to work for a just and lasting peace in which every State in the area can live in security."
John McHugo says that by the 1920s, international law no longer recognized that a state could acquire title to territory by conquest. Article 2 of the Charter of the United Nations requires all members to refrain in their international relations from the threat or use of force against the territorial integrity or political independence of any state, or in any other manner inconsistent with the purposes of the United Nations.
Michael Lynk says that article 2 of the Charter embodied a prevailing legal principle that there could be "no title by conquest". He says that principle had been expressed through numerous international conferences, doctrines and treaties since the late 19th Century. Lynk cites the examples of the First International Conference of American States in 1890; the United States Stimson Doctrine of 1932; the 1932 League of Nations resolution on Japanese aggression in China; the Buenos Aires Declaration of 1936; and the Atlantic Charter of 1941. Surya Sharma says that a war in self-defense cannot result in acquisition of title by conquest. He says that even if a war is lawful in origin it cannot exceed the limits of legitimate self-defense.
Land for peace.
The resolution also calls for the implementation of the "land for peace" formula, calling for Israeli withdrawal from "territories" it had occupied in 1967 in exchange for peace with its neighbors. This was an important advance at the time, considering that there were no peace treaties between any Arab state and Israel until the Israel-Egypt Peace Treaty of 1979. "Land for peace" served as the basis of the Israel-Egypt Peace Treaty, in which Israel withdrew from the Sinai peninsula (Egypt withdrew its claims to the Gaza Strip in favor of the Palestine Liberation Organization). Jordan renounced its claims regarding the West Bank in favor of the Palestine Liberation Organization, and has signed the Israel-Jordan Treaty of Peace in 1994, that established the Jordan River as the boundary of Jordan.
Throughout the 1990s, there were Israeli-Syrian negotiations regarding a normalization of relations and an Israeli withdrawal from the Golan Heights. But a peace treaty was not made, mainly due to Syria's desire to recover and retain 25 square kilometers of territory in the Jordan River Valley which it seized in 1948 and occupied until 1967. As the United Nations recognizes only the 1948 borders, there is little support for the Syrian position outside the Arab bloc nor in resolving the Golan Heights issue.
The UN resolution does not specifically mention the Palestinians. The United Kingdom had recognized the union between the West Bank and Transjordan. Lord Caradon said that the parties assumed that withdrawal from occupied territories as provided in the resolution was applicable to East Jerusalem. "Nevertheless so important is the future of Jerusalem that it might be argued that we should have specifically dealt with that issue in the 1967 Resolution. It is easy to say that now, but I am quite sure that if we had attempted to raise or settle the question of Jerusalem as a separate issue at that time our task in attempting to find a unanimous decision would have been far greater if not impossible."
Judge Higgins of the International Court of Justice explained "from Security Council resolution 242 (1967) through to Security Council Resolution 1515 (2003), the key underlying requirements have remained the same - that Israel is entitled to exist, to be recognized, and to security, and that the Palestinian people are entitled to their territory, to exercise self-determination, and to have their own State. Security Council resolution 1515 (2003)
envisages that these long-standing obligations are to be secured (...) by negotiation"
Secretary of State Madeleine Albright told the U.N. Security Council: "We simply do not support the description of the territories occupied by Israel in 1967 as 'Occupied Palestinian Territory'. In the view of my Government, this language could be taken to indicate sovereignty, a matter which both Israel and the PLO have agreed must be decided in negotiations on the final status of the territories. "Had this language appeared in the operative paragraphs of the resolution, let me be clear: we would have exercised our veto. In fact, we are today voting against a resolution in the Commission on the Status of Women precisely because it implies that Jerusalem is "occupied Palestinian territory".
The Palestinians were represented by the Palestine Liberation Organization in negotiations leading to the Oslo Accords. They envisioned a 'permanent settlement based on Security Council Resolution 242'. The main premise of the Oslo Accords was the eventual creation of Palestinian autonomy in some or all of the territories captured during the Six-Day War, in return for Palestinian recognition of Israel. However, the Foreign Minister of the Palestinian Authority, Nabil Shaath, said: "Whether a state is announced now or after liberation, its borders must be those of 4 June 1967. We will not accept a state without borders or with borders based on UN Resolution 242, which we believe is no longer suitable. On the contrary, Resolution 242 has come to be used by Israel as a way to procrastinate."
The Security Council subsequently adopted resolution 1515 (2003), which recalled resolution 242 and endorsed the Middle East Quartet’s Road Map towards a permanent, two-State solution to the Israeli-Palestinian conflict. The Quartet Plan calls for direct, bilateral negotiations as part of a comprehensive resolution of the Arab-Israeli conflict, on the basis of UN Security Council Resolutions 242, 338, 1397, 1515, 1850, and the Madrid principles. The Quartet has reiterated that the only viable solution to the Israeli-Palestinian conflict is an agreement that ends the occupation that began in 1967; resolves all permanent status issues as previously defined by the parties; and fulfils the aspirations of both parties for independent homelands through two states for two peoples, Israel and an independent, contiguous and viable state of Palestine, living side by side in peace and security.
On April 14, 2004, US President George W. Bush said to Israeli Prime Minister Ariel Sharon, "The United States reiterates its steadfast commitment to Israel's security, including secure, defensible borders." Israeli officials argue that the pre-1967 armistice line is not a defensible border, since Israel would be nine miles wide at the thinnest point, subjected to rocket fire from the highlands of the West Bank, and unable to stop smuggling from Jordan across the Jordan Valley. Thus, Israeli officials have been arguing for the final-status borders to be readjusted to reflect security concerns.
Resolution 1860 (2009) recalled resolution 242 and stressed that the Gaza Strip constitutes an integral part of the territory occupied in 1967 that will be a part of the Palestinian state.
Settlement of the refugee problem.
The resolution advocates a "just settlement of the refugee problem". Lord Caradon said "It has been said that in the Resolution we treated Palestinians only as refugees, but this is unjustified. We provided that Israel should withdraw from occupied territories and it was together with that requirement for a restoration of Arab territory that we also called for a settlement of the refugee problem." Upon the adoption of Resolution 242, French President Charles de Gaulle stressed this principle during a press conference on November 27, 1967 and confirmed it in his letter of January 9, 1968 to David Ben-Gurion. De Gaulle cited "the pitiful condition of the Arabs who had sought refuge in Jordan or were relegated to Gaza" and stated that provided Israel withdrew her forces, it appeared it would be possible to reach a solution "within the framework of the United Nations that included the assurance of a dignified and fair future for the refugees and minorities in the Middle East."
Alexander Orakhelashvili said that ‘Just settlement’ can only refer to a settlement guaranteeing
the return of displaced Palestinians. He explained that it must be presumed that the Council did not adopt decisions that validated mass deportation or displacement, since expulsion or deportation are crimes against humanity or an exceptionally serious war crime.
According to M. Avrum Ehrlich, 'Resolution 242 called for "a just solution to the refugee problem," a term covering Jewish refugees from Arab countries as stated by President Carter in 1978 at Camp David'.
According to John Quigley, however, it is clear from the context in which it was adopted, and from the statements recounted by the delegates, that Resolution 242 contemplates the Palestine Arab refugees only.
French version vs. English version of text.
The French version of the clause reads:
Retrait des forces armées israéliennes des territoires occupés lors du récent conflit.
The difference between the two versions lies in the absence of a definite article ("the") in the English version, while the word "des" present in the French version in the expression "des territoires occupés" can only mean "from the occupied territories" (the "des" in front of "territoires occupés" can only be the contraction "from the" because of the use of the word "retrait" which entails an object - "des forces israéliennes" where the "des" is the contraction of "of the" (of the Israeli forces) and a location "des territoires occupés" where the "des" is the contraction of "from the" (from the occupied territories)). If the meaning of "from some occupied territories" were intended, the only way to say so in French would have been "de territoires occupés".
Although some have dismissed the controversy by suggesting that the use of the word "des" in the French version is a translation error and should therefore be ignored in interpreting the document, the debate has retained its force since both versions are of equal legal force, as recognized languages of the United Nations and in international law.
Solicitor John McHugo, a partner at Trowers & Hamlins and a visiting fellow at the Scottish Centre for International Law at Edinburgh University, draws a comparison to phrases such as:
Dogs must be kept on the lead near ponds in the park.
In spite of the lack of definite articles, according to McHugo, it is clear that such an instruction cannot legitimately be taken to imply that some dogs need not be kept on the lead or that the rule applies only near some ponds. Further, McHugo points out a potential consequence of the logic employed by advocates of a "some" reading. Paragraph 2 (a) of the Resolution, which guarantees "freedom of navigation through international waterways in the area," may allow Arab states to interfere with navigation through "some" international waterways of their choosing.
Glenn Perry asserts that because the French version resolves ambiguities in the English text, and is more consistent with the other clauses of the treaty, it is the correct interpretation. He argues that "it is an accepted rule that the various language versions must be considered together, with the ambiguities of one version elucidated by the other". He cites Article 33 of the Vienna Convention on the Law of Treaties, which states that except when a treaty provides that one text shall prevail "the meaning which best reconciles the texts, having regard to the object and purpose of the treaty, shall be adopted". He furthermore argues that the context of the passage, in a treaty that reaffirms "'territorial integrity', 'territorial inviolability,' and 'the inadmissibility of the acquisition of territory by war' - taken together cannot be reconciled with anything less than full withdrawal". He argues that the reference to "secure and recognized borders" can be interpreted in several ways, and only one of them contradicts the principle of full withdrawal.
Shabtai Rosenne, former Permanent Representative of Israel to the United Nations Office at Geneva and member of the UN's International Law Commission, notes that:
It is a historical fact, which nobody has ever attempted to deny, that the negotiations between the members of the Security Council, and with the other interested parties, which preceded the adoption of that resolution, were conducted on the basis of English texts, ultimately consolidated in Security Council document S/8247. [...] Many experts in the French language, including academics with no political axe to grind, have advised that the French translation is an accurate and idiomatic rendering of the original English text, and possibly even the only acceptable rendering into French. [...] [o]n the question of concordance, the French representative [to the 1379th meeting of the Security Council on November 16, 1967] was explicit in stating that the French text was "identical" with the English text.
Only English and French were the Security Council's working languages (Arabic, Russian, Spanish and Chinese were official but not the working languages).
The Committee for Accuracy in Middle East Reporting in America argues the practice at the UN is that the binding version of any resolution is the one voted upon. In the case of 242 that version was in English, so they assert the English version the only binding one.David A. Korn asserts that this was indeed the position held by the United States and United Kingdom:
... both the British and the Americans pointed out that 242 was a British resolution; therefore, the English language text was authoritative and would prevail in any dispute over interpretation.
The French representative to the Security Council, in the debate immediately after the vote, asserted:
the French text, which is equally authentic with the English, leaves no room for any ambiguity, since it speaks of withdrawal "des territoires occupés," which indisputably corresponds to the expression "occupied territories" We were likewise gratified to hear the United Kingdom representative stress the link between this paragraph of his resolution and the principle of inadmissibility of the acquisition of territories by force...
Opponents of the "all territories" reading remind that the UN Security Council declined to adopt a draft resolution, including the definite article, far prior to the adoption of Resolution 242. They argue that, in interpreting a resolution of an international organization, one must look to the process of the negotiation and adoption of the text. This would make the text in English, the language of the discussion, take precedence.
The negotiating and drafting process.
A Congressional Research Service (CRS) Issue Brief quotes policy statements made by President Johnson in a speech delivered on September 10, 1968, and by Secretary of State Rogers in a speech delivered on December 9, 1969: "The United States has stated that boundaries should be negotiated and mutually recognized, 'should not reflect the weight of conquest,' and that adjustments in the pre-1967 boundaries should be 'insubstantial.'"
President Carter asked for a State Department report "to determine if there was any justice to the Israeli position that the resolution did not include all the occupied territories". The State Department report concluded:Support for the concept of total withdrawal was widespread in the Security Council, and it was only through intensive American efforts that a resolution was adopted which employed indefinite language in the withdrawal clause. In the process of obtaining this result, the United States made clear to the Arab states and several other members of the Security Council that the United States envisioned only insubstantial revisions of the 1949 armistice lines. Israel did not protest the approach.
Ruth Lapidoth describes the view, adopted by Israel, which holds that the resolution allowed Israel to retain "some territories". She argues "The provision on the establishment of “secure and recognized boundaries” would have been meaningless if there had been an obligation to withdraw from all the territories.
U.S. Secretary of State Henry Kissinger recalled the first time he heard someone invoke "the sacramental language of United Nations Security Council Resolution 242, mumbling about the need for a just and lasting peace within secure and recognized borders". He said the phrase was so platitudinous that he thought the speaker was pulling his leg. Kissinger said that, at that time, he did not appreciate how the flood of words used to justify the various demands obscured rather than illuminated the fundamental positions. Kissinger said those "clashing perspectives" prevented any real bargaining and explained:
Jordan’s acquiescence in Resolution 242 had been obtained in 1967 by the promise of our United Nations Ambassador Arthur Goldberg that under its terms we would work for the return of the West Bank of Jordan with minor boundary rectifications and that we were prepared to use our influence to obtain a role for Jordan in Jerusalem.
However, speaking to Henry Kissinger, President Richard Nixon said "You and I both know they can’t go back to the other [1967] borders. But we must not, on the other hand, say that because the Israelis win this war, as they won the '67 War, that we just go on with status quo. It can't be done." Kissinger replied "I couldn't agree more" 
Moreover, President Gerald Ford said: "The U.S. further supports the position that a just and lasting peace, which remains our objective, must be acceptable to both sides. The U.S. has not developed a final position on the borders. Should it do so it will give great weight to Israel's position that any peace agreement with Syria must be predicated on Israel remaining on the Golan Heights." 
Furthermore, Secretary of State George Shultz declared: "Israel will never negotiate from, or return to, the lines of partition or to the 1967 borders."
Secretary of State Christopher's letter to Netanyahu states: "I would like to reiterate our position that Israel is entitled to secure and defensible borders, which should be directly negotiated and agreed with its neighbors."
A key part of the case in favour of a "some territories" reading is the claim that British and American officials involved in the drafting of the Resolution omitted the definite article deliberately in order to make it less demanding on the Israelis. As George Brown, British Foreign Secretary in 1967, said:
The Israelis had by now anexed de facto, if not formally, large new areas of Arab land, and there were now very many more Arab refugees. It was clear that what Israel or at least many of her leaders, really wanted was permanently to colonize much of this newly annexed Arab territory, particularly the Jordan valley, Jerusalem, and other sensitive areas. This led me into a flurry of activity at the United Nations, which resulted in the near miracle of getting the famous resolution - Resolution 242 - unanimously adopted by the Security Council. It declares "the inadmissibility of territory by war" and it also affirms the necessity "for guaranteeing the territorial inviolability and political independence of every state in the area". It calls for "withdrawal of Israeli forces from territories occupied during the recent conflict." It does not call for Israeli withdrawal from “the” territories recently occupied, nor does it use the word “all”. It would have been impossible to get the resolution through if either of these words had been included, but it does set out the lines on which negotiations for a settlement must take place. Each side must be prepared to give up something: the resolution doesn’t attempt to say precisely what, because that is what negotiations for a peace-treaty must be about.
Lord Caradon, chief author of the resolution, takes a subtly different slant. His focus seems to be that the lack of a definite article is intended to deny permanence to the "unsatisfactory" pre-1967 border, rather than to allow Israel to retain land taken by force. Such a view would appear to allow for the possibility that the borders could be varied through negotiation:
Knowing as I did the unsatisfactory nature of the 1967 line I was not prepared to use wording in the Resolution which would have made that line permanent. Nevertheless it is necessary to say again that the overriding principle was the "inadmissibility of the acquisition of territory by war" and that meant that there could be no justification for annexation of territory on the Arab side of the 1967 line merely because it had been conquered in the 1967 war. The sensible way to decide permanent "secure and recognized" boundaries would be to set up a Boundary Commission and hear both sides and then to make impartial recommendations for a new frontier line, bearing in mind, of course, the "inadmissibility" principle. The purposes are perfectly clear, the principle is stated in the preamble, the necessity for withdrawal is stated in the operative section. And then the essential phrase which is not sufficiently recognized is that withdrawal should take place to secure and recognized boundaries, and these words were very carefully chosen: they have to be secure and they have to be recognized. They will not be secure unless they are recognized. And that is why one has to work for agreement. This is essential. I would defend absolutely what we did. It was not for us to lay down exactly where the border should be. I know the 1967 border very well. It is not a satisfactory border, it is where troops had to stop in 1948, just where they happened to be that night, that is not a permanent boundary...
Arthur J. Goldberg, another of the resolution's drafters, concurred that Resolution 242 does not dictate the extent of the withdrawal, and added that this matter should be negotiated between the parties:
Does Resolution 242 as unanimously adopted by the UN Security Council require the withdrawal of Israeli armed forces from all of the territories occupied by Israel during the 1967 war? The answer is no. In the resolution, the words the and all are omitted. Resolution 242 calls for the withdrawal of Israeli armed forces from territories occupied in the 1967 conflict, without specifying the extent of the withdrawal. The resolution, therefore, neither commands nor prohibits total withdrawal.
If the resolution is ambiguous, and purposely so, on this crucial issue, how is the withdrawal issue to be settled? By direct negotiations between the concerned parties. Resolution 242 calls for agreement between them to achieve a peaceful and accepted settlement. Agreement and acceptance necessarily require negotiations.
Mr. Michael Stewart, Secretary of State for Foreign and Commonwealth Affairs, in a reply to a question in Parliament, 9 December 1969: "As I have explained before, there is reference, in the vital United Nations Security Council Resolution, both to withdrawal from territories and to secure and recognized boundaries. As I have told the House previously, we believe that these two things should be read concurrently and that the omission of the word 'all' before the word 'territories' is deliberate."
Mr. Joseph J. Sisco, Assistant Secretary of State, 12 July 1970 (NBC "Meet the Press"): "That Resolution did not say 'withdrawal to the pre-June 5 lines'. The Resolution said that the parties must negotiate to achieve agreement on the so-called final secure and recognized borders. In other words, the question of the final borders is a matter of negotiations between the parties." Mr. Sisco was actively involved in drafting the Resolution in his capacity as Assistant Secretary of State for International Organization Affairs in 1967.
President Lyndon B. Johnson:
Fifth, the crisis underlines the importance of respect for political independence and territorial integrity of all the states of the area. We reaffirmed that principle at the height of this crisis. We reaffirm it again today on behalf of all.
This principle can be effective in the Middle East only on the basis of peace between the parties. The nations of the region have had only fragile and violated truce lines for 20 years. What they now need are recognized boundaries and other arrangements that will give them security against terror, destruction, and war.
There are some who have urged, as a single, simple solution, an immediate return to the situation as it was on June 4. As our distinguished and able Ambassador, Mr. Arthur Goldberg, has already said, this is not a prescription for peace but for renewed hostilities. Certainly troops must be withdrawn, but there must also be recognized rights of national life, progress in solving the refugee problem, freedom of innocent maritime passage, limitation of the arms race, and respect for political independence and territorial integrity." 
U.S. position.
On June 19, 1967 President Johnson declared the five principles, including land for peace, that he believed comprised the components of any United Nations settlement of the Middle East crisis. He pledged the U.S. Government would "do its part for peace in every forum, at every level, at every hour". On July 12, 1967, Secretary of State Rusk announced that the U.S. position on the Near East crisis was outlined in the President's statement of June 19 and that it provided the basis for a just and equitable settlement between the Arab states and Israel. On August 16, 1967 the Israeli Foreign Office stated that Israel agreed with the principles set forth by the President on June 19 and indicated that no resolution would be acceptable if it deviated from them.
On June 9, 1967, Israeli Foreign Minister Eban assured Arthur Goldberg, US Ambassador to the UN, that Israel was not seeking territorial aggrandizement and had no "colonial" aspirations. Secretary of State Rusk stressed to the Government of Israel that no settlement with Jordan would be accepted by the world community unless it gave Jordan some special position in the Old City of Jerusalem. The US also assumed Jordan would receive the bulk of the West Bank as that was regarded as Jordanian territory.
On November 3, 1967 Ambassador Goldberg, accompanied by Mr. Sisco and Mr. Pedersen, called on King Hussein of Jordan. Goldberg said the US was committed to the principle of political independence and territorial integrity and was ready to reaffirm it bilaterally and publicly in the Security Council resolution. Goldberg said the US believes in territorial integrity, withdrawal, and recognition of secure boundaries. Goldberg said the principle of territorial integrity has two important sub-principles: there must be a withdrawal to recognized and secure frontiers for all countries, not necessarily the old armistice lines, and there must be mutuality in adjustments.
Walt Rostow advised President Johnson that Secretary Rusk had explained to Mr. Eban that US support for secure permanent frontiers does not mean the US supports territorial changes. The record of a meeting between Under Secretary of State Eugene Rostow and Israeli Ambassador Harmon stated that Rostow made clear the US view that there should be movement from General Armistice Agreements to conditions of peace and that this would involve some adjustments of armistice lines as foreseen in the Armistice Agreements. Rostow told Harmon that he had already stressed to Foreign Minister Eban that the US expected the thrust of the settlement would be toward security and demilitarization arrangements rather than toward major changes in the Armistice lines. Harmon said the Israeli position was that Jerusalem should be an open city under unified administration but that the Jordanian interest in Jerusalem could be met through arrangements including "sovereignty". Rostow said the US government assumed (and Harman confirmed) that despite public statements to the contrary, the Government of Israel position on Jerusalem was that which Eban, Harman, and Evron had given several times, that Jerusalem was negotiable.
Ambassador Goldberg briefed King Hussein on US assurances regarding territorial integrity. Goldberg said the US did not view Jordan as a country that consisted only of the East Bank, and that the US was prepared to support a return of the West Bank to Jordan with minor boundary rectifications. The US would use its influence to obtain compensation to Jordan for any territory it would be required to give up. Finally, although as a matter of policy the US did not agree with Jordan's position on Jerusalem, nor with the Israeli position on Jerusalem, the US was prepared to use its influence to obtain for Jordan a role in Jerusalem. Secretary Rusk advised President Johnson that he confirmed Golberg's pledge regarding territorial integrity to King Hussein.
During a subsequent meeting between President Johnson, King Hussein, and Secretary of State Rusk, Hussein said the phrasing of the resolution calling for withdrawal from occupied territories could be interpreted to mean that the Egyptians should withdraw from Gaza and the Jordanians should withdraw from the West Bank. He said this possibility was evident from a speech given by Prime Minister Eshkol in which it had been claimed that both Gaza and the West Bank had been "occupied territory". The President agreed, and promised he would talk to Ambassador Goldberg about inserting Israel in that clause. Ambassador Goldberg told King Hussein that after taking into account legitimate Arab concerns and suggestions, the US would be willing to add the word "Israeli" before "Armed Forces" in the first operative paragraph.
A State Department study noted that when King Hussein met on 8 November with President Johnson, who had been briefed by Secretary Rusk on the US interpretation, the Jordanian monarch asked how soon the Israeli troops would withdraw from most of the occupied lands. The President replied "In six months."
William Quandt wrote about Johnson's meeting with Eban on October 24, 1967, and noted that Israel had annexed East Jerusalem. He said Johnson forcefully told Eban he thought Israel had been unwise when it went to war and that he still thought they were unwise. The President stressed the need to respect the territorial integrity of the Arab states. Quandt said "'The President wished to caution the Israelis that the further they get from June 5 the further they get from peace.' Meaning the more territory they insisted on holding beyond the 1967 lines, the worse would be the odds of getting a peace agreement with the Arabs."
Interpretations.
Israel interprets Resolution 242 as calling for withdrawal from territories as part of a negotiated peace and full diplomatic recognition. The extent of withdrawal would come as a result of comprehensive negotiations that led to durable peace not before Arabs start to meet their own obligations under Resolution 242.
Initially, the resolution was accepted by Egypt, Jordan and Israel but not by the Palestine Liberation Organization. The Arab position was initially that the Resolution called for Israel to withdraw from all the territory it occupied during the Six-Day War prior to peace agreements.
Israel and the Arab states have negotiated before the Israeli withdrawal. Israel and Jordan made peace without Israel withdrawing from the West Bank, since Jordan had already renounced its claims and recognized the PLO as the sole representative of the Palestinians. Egypt began negotiations before Israel withdrew from the Sinai. Negotiations ended without Egypt ever resuming control of the Gaza Strip, which Egypt held until 1967.
Supporters of the "Palestinian viewpoint" focus on the phrase in the resolution's preamble emphasizing the "inadmissibility of the acquisition of territory by war", and note that the French version called for withdrawal from "des territoires occupés" - ""the" territories occupied". The French UN delegation insisted on this interpretation at the time, but both English and French are the Secretariat's working languages.
Supporters of the "Israeli viewpoint" note that the second part of that same sentence in the preamble explicitly recognizes the need of existing states to live in security.
They focus on the operative phrase calling for "secure and recognized boundaries" and note that the resolution calls for a withdrawal "from territories" rather than "from the territories" or "from all territories," as the Arabs and others proposed; the latter two terms were rejected from the final draft of Resolution 242.
Alexander Orakhelashvili cites a number cases in which international tribunals have ruled that international organizations, including the Security Council, are bound by general international law. He says that inclusion of explicit clauses about the inadmissibility of acquisition of territory by war and requiring respect of territorial integrity and sovereignty of a state demonstrates that the Council does not intend to offend peremptory norms in these specific ways. The resolution also acknowledges that these principles must be part of an accepted settlement. That is confirmed by the Vienna Convention on the Law of Treaties which reiterates the prohibition on the use of force and provides that any settlement obtained by the threat or use of force in violation of the principles of international law embodied in the Charter of the United Nations or conflicting with a peremptory norm of general international law is invalid. According to Hans-Paul Gasser, ‘doubtful’ wording of the Council’s resolutions must always be construed in such a way as to avoid conflict with fundamental international obligations.
The USSR, India, Mali, Nigeria and Arab States all proposed that the resolution be changed to read "all territories" instead of "territories." Their request was discussed by the UN Security Council and "territories" was adopted instead of "all territories", after President Johnson told Premier Alexei Kosygin that the delegates should not try to negotiate the details of a Middle East settlement in the corridors and meeting halls of the United Nations, and Ambassador Goldberg stipulated that the exact wording of the resolution would not affect the position of any of the parties. Per Lord Caradon, the chief author of the resolution:
It was from occupied territories that the Resolution called for withdrawal. The test was which territories were occupied. That was a test not possibly subject to any doubt. As a matter of plain fact East Jerusalem, the West Bank, Gaza, the Golan and Sinai were occupied in the 1967 conflict. It was on withdrawal from occupied territories that the Resolution insisted.
Lord Caradon also maintained,
We didn't say there should be a withdrawal to the '67 line; we did not put the 'the' in, we did not say all the territories, deliberately.. We all knew - that the boundaries of '67 were not drawn as permanent frontiers, they were a cease-fire line of a couple of decades earlier... We did not say that the '67 boundaries must be forever; it would be insanity.
During a symposium on the subject Lord Caradon said that Israel was in clear defiance of resolution 242. He specifically cited the "annexation of East Jerusalem" and "the creeping colonialism on the West Bank and in Gaza and in the Golan."
However, British Foreign Secretary George Brown said:
I have been asked over and over again to clarify, modify or improve the wording, but I do not intend to do that. The phrasing of the Resolution was very carefully worked out, and it was a difficult and complicated exercise to get it accepted by the UN Security Council. I formulated the Security Council Resolution. Before we submitted it to the Council, we showed it to Arab leaders. The proposal said 'Israel will withdraw from territories that were occupied', and not from 'the' territories, which means that Israel will not withdraw from all the territories.
Statements by Security Council representatives.
The representative for India stated to the Security Council:
It is our understanding that the draft resolution, if approved by the Council, will commit it to the application of the principle of total withdrawal of Israel forces from all the territories - I repeat, all the territories - occupied by Israel as a result of the conflict which began on 5 June 1967.
The representatives from Nigeria, France, USSR, Bulgaria, United Arab Republic (Egypt), Ethiopia, Jordan, Argentina and Mali supported this view, as worded by the representative from Mali: "[Mali] wishes its vote today to be interpreted in the light of the clear and unequivocal interpretation which the representative of India gave of the provisions of the United Kingdom text." The Russian representative Vasili Kuznetsov stated:
We understand the decision taken to mean the withdrawal of Israel forces from all, and we repeat, all territories belonging to Arab States and seized by Israel following its attack on those States on 5 June 1967. This is borne out by the preamble to the United Kingdom draft resolution [S/8247] which stresses the "inadmissibility of the acquisition of territory by war". It follows that the provision contained in that draft relating to the right of all States in the Near East "to live in peace within secure and recognized boundaries" cannot serve as a pretext for the maintenance of Israel forces on any part of the Arab territories seized by them as a result of war.
Israel was the only country represented at the Security Council to express a contrary view. The USA, United Kingdom, Canada, Denmark, China and Japan were silent on the matter, but the US and UK did point out that other countries' comments on the meaning of 242 were simply their own views. The Syrian representative was strongly critical of the text's "vague call on Israel to withdraw".
The statement by the Brazilian representative perhaps gives a flavour of the complexities at the heart of the discussions:
I should like to restate...the general principle that no stable international order can be based on the threat or use of force, and that the occupation or acquisition of territories brought about by such means should not be recognized...Its acceptance does not imply that borderlines cannot be rectified as a result of an agreement freely concluded among the interested States. We keep constantly in mind that a just and lasting peace in the Middle East has necessarily to be based on secure permanent boundaries freely agreed upon and negotiated by the neighboring States.
However, the Soviet delegate Vasily Kuznetsov argued: " ... phrases such as 'secure and recognized boundaries'. ... make it possible for Israel itself arbitrarily to establish new boundaries and to withdraw its forces only to those lines it considers appropriate." [1373rd meeting, para. 152.]
U.S. Supreme Court Justice Arthur Goldberg, who represented the US in discussions, later stated: "The notable omissions in regard to withdrawal are the word 'the' or 'all' and 'the June 5, 1967 lines' the resolution speaks of withdrawal from occupied territories, without defining the extent of withdrawal".
Implementation.
On November 23, 1967, the Secretary General appointed Gunnar Jarring as Special Envoy to negotiate the implementation of the resolution with the parties, the so-called Jarring Mission. The governments of Israel, Egypt, Jordan and Lebanon recognized Jarring's appointment and agreed to participate in his shuttle diplomacy, although they differed on key points of interpretation of the resolution. The government of Syria rejected Jarring's mission on grounds that total Israeli withdrawal was a prerequisite for further negotiations. The talks under Jarring's auspices lasted until 1973, but bore no results. After 1973, the Jarring mission was replaced by bilateral and multilateral peace conferences.

</doc>
<doc id="47542" url="http://en.wikipedia.org/wiki?curid=47542" title="Buffy the Vampire Slayer">
Buffy the Vampire Slayer

Buffy the Vampire Slayer is an American television series which aired from March 10, 1997 until May 20, 2003. The series was created in 1997 by writer-director Joss Whedon under his production tag, Mutant Enemy Productions with later co-executive producers being Jane Espenson, David Fury, David Greenwalt, Doug Petrie, Marti Noxon, and David Solomon. The series narrative follows Buffy Summers (played by Sarah Michelle Gellar), the latest in a line of young women known as "Vampire Slayers" or simply "Slayers". In the story, Slayers are "called" (chosen by fate) to battle against vampires, demons, and other forces of darkness. Like previous Slayers, Buffy is aided by a Watcher, who guides, teaches, and trains her. Unlike her predecessors, Buffy surrounds herself with a circle of loyal friends who become known as the "Scooby Gang".
The series received critical and popular acclaim and usually reached between four and six million viewers on original airings. Although such ratings are lower than successful shows on the "big four" networks (ABC, CBS, NBC, and Fox), they were a success for the relatively new and smaller WB Television Network. The show was ranked 41st on "TV Guide"'s list of 50 Greatest TV Shows of All Time, second on "Empire"‍ '​s "50 Greatest TV Shows of All Time", voted third in 2004 and 2007 on "TV Guide"‍ '​s "Top Cult Shows Ever" and listed in "Time" magazine's "100 Best TV Shows of All-"Time"". In 2013, "TV Guide" also included it in its list of The 60 Greatest Dramas of All Time. "Buffy" was also named the third Best School Show of All Time by AOL TV. It was nominated for Emmy and Golden Globe awards, winning a total of three Emmys. However, snubs in lead Emmy categories resulted in outrage among TV critics and the decision by the academy to hold a tribute event in honor of the series after it had gone off the air in 2003.
"Buffy"'s success has led to hundreds of tie-in products, including novels, comics, and video games. The series has received attention in fandom (including fan films), parody, and academia, and has influenced the direction of other television series.
Production.
Origins.
Writer Joss Whedon says that "Rhonda the Immortal Waitress" was really the first incarnation of the "Buffy" concept, "the idea of some woman who seems to be completely insignificant who turns out to be extraordinary." This early, unproduced idea evolved into "Buffy", which Whedon developed to invert the Hollywood formula of "the little blonde girl who goes into a dark alley and gets killed in every horror movie." Whedon wanted "to subvert that idea and create someone who was a hero." He explained, "The very first mission statement of the show was the joy of female power: having it, using it, sharing it."
The idea was first visited through Whedon's script for the 1992 movie "Buffy the Vampire Slayer", which featured Kristy Swanson in the title role. The director, Fran Rubel Kuzui, saw it as a "pop culture comedy about what people think about vampires." Whedon disagreed: "I had written this scary film about an empowered woman, and they turned it into a broad comedy. It was crushing." The script was praised within the industry, but the movie was not.
Several years later, Gail Berman (later a Fox executive, but at that time President and CEO of the production company Sandollar Television, who owned the TV rights to the movie) approached Whedon to develop his "Buffy" concept into a television series. Whedon explained that "They said, 'Do you want to do a show?' And I thought, 'High school as a horror movie.' And so the metaphor became the central concept behind "Buffy", and that's how I sold it." The supernatural elements in the series stood as metaphors for personal anxieties associated with adolescence and young adulthood. Early in its development, the series was going to be simply titled "Slayer". Whedon went on to write and partly fund a 25-minute non-broadcast pilot that was shown to networks and eventually sold to the WB Network. The latter promoted the premiere with a series of "History of the Slayer" clips, and the first episode aired on March 10, 1997.
Executive producers.
Joss Whedon was credited as executive producer throughout the run of the series, and for the first five seasons (1997–2001) he was also the showrunner, supervising the writing and all aspects of production. Marti Noxon took on the role for seasons six and seven (2001–2003), but Whedon continued to be involved with writing and directing "Buffy" alongside projects such as "Angel", "Fray", and "Firefly". Fran Rubel Kuzui and her husband, Kaz Kuzui, were credited as executive producers but were not involved in the show. Their credit, rights, and royalties over the franchise relate to their funding, producing, and directing of the original movie version of "Buffy".
Writing.
Script-writing was done by Mutant Enemy, a production company created by Whedon in 1997. The writers with the most writing credits are Joss Whedon, Steven S. DeKnight, Jane Espenson, David Fury, Drew Goddard, Drew Greenberg, David Greenwalt, Rebecca Rand Kirshner, Marti Noxon and Doug Petrie. Other authors with writing credits include Dean Batali, Carl Ellsworth, Tracey Forbes, Ashley Gable, Howard Gordon, Diego Gutierrez, Elin Hampton, Rob Des Hotel, Matt Kiene, Ty King, Thomas A. Swyden, Joe Reinkemeyer, Dana Reston and Dan Vebber.
Jane Espenson has explained how scripts came together. First, the writers talked about the emotional issues facing Buffy Summers and how she would confront them through her battle against evil supernatural forces. Then the episode's story was "broken" into acts and scenes. Act breaks were designed as key moments to intrigue viewers so that they would stay with the episode following the commercial break. The writers collectively filled in scenes surrounding these act breaks for a more fleshed-out story. A whiteboard marked their progress by mapping brief descriptions of each scene. Once "breaking" was done, the credited author wrote an outline for the episode, which was checked by Whedon or Noxon. The writer then wrote a full script, which went through a series of drafts, and finally a quick rewrite from the show runner. The final article was used as the shooting script.
Broadcast history and syndication.
"Buffy the Vampire Slayer" first aired on March 10, 1997, (as a mid season replacement for the show "Savannah") on the WB network, and played a key role in the growth of the Warner Bros. television network in its early years. After five seasons, it transferred to the United Paramount Network (UPN) for its final two seasons. In 2001, the show went into syndication in the United States on local stations and on cable channel FX; the local airings ended in 2005, and the FX airings lasted until 2008 but returned to the network in 2013. Beginning in January 2010, it began to air in syndication in the United States on Logo. Reruns also briefly aired on MTV. In March 2010, it began to air in Canada on MuchMusic and MuchMore. On November 7, 2010, it began airing on Chiller with a 24-hour marathon; the series airs weekdays. Chiller also aired a 14-hour Thanksgiving Day marathon on November 25, 2010. In 2011, it began airing on Oxygen and TeenNick.
While the seventh season was still being broadcast, Sarah Michelle Gellar told "Entertainment Weekly" she was not going to sign on for an eighth year; "When we started to have such a strong year this year, I thought: 'This is how I want to go out, on top, at our best.'" Whedon and UPN gave some considerations to production of a spin-off series that would not require Gellar, including a rumored Faith series, but nothing came of those plans. The "Buffy" canon continued outside the television medium in the Dark Horse Comics series, "Buffy" Season Eight. This was produced starting March 2007 by Whedon, who also wrote the first story arc, "The Long Way Home".
As of July 15, 2008, "Buffy the Vampire Slayer" episodes are available to download for PlayStation 3 and PlayStation Portable video game consoles via the PlayStation Network.
In the United Kingdom, the entire series aired on Sky1 and BBC Two. After protests from fans about early episodes being edited for their pre-watershed time-slot, from the second run (mid-second season onwards), the BBC gave the show two time slots: the early-evening slot (typically Thursday at 6:45 pm) for a family-friendly version with violence, objectionable language and other stronger material cut out, and a late-night uncut version (initially late-night Sundays, but for most of the run, late-night Fridays; exact times varied). Sky1 aired the show typically at 8:00 pm on Thursdays. From the fourth season onwards, the BBC aired the show in anamorphic 16:9 widescreen format. Whedon later said that "Buffy" was never intended to be viewed this way. Despite his claims, Sky1 and Syfy now air repeat showings in the widescreen format.
In August 2014, Pivot announced that, for the first time, episodes of "Buffy" would be broadcast in high-definition which was remastered by 20th Century Fox. The transfer was poorly received by some fans, owing to a number of technical and format changes that were viewed as detrimental to the show's presentation; various scenes were heavily cropped to fit the 16:9 format, and shots were altered to have a brighter look, often with color levels altered. Other problems included missing filters, editing errors, and poorly re-rendered CGI. Series creator Joss Whedon and other members of the original team also expressed their displeasure.
Music.
"Buffy" features a mix of original, indie, rock and pop music. The composers spent around seven days scoring between fourteen to thirty minutes of music for each episode. Christophe Beck revealed that the "Buffy" composers used computers and synthesizers and were limited to recording one or two "real" samples. Despite this, their goal was to produce "dramatic" orchestration that would stand up to film scores.
Alongside the score, most episodes featured indie rock music, usually at the characters' venue of choice, The Bronze. "Buffy" music supervisor John King explained that "we like to use unsigned bands" that "you would believe would play in this place." For example, the fictional group Dingoes Ate My Baby were portrayed on screen by front group Four Star Mary. Pop songs by famous artists were rarely featured prominently, but several episodes spotlighted the sounds of more famous artists such as Sarah McLachlan, The Brian Jonestown Massacre, Blink-182, Third Eye Blind, Aimee Mann (who also had a line of dialogue), The Dandy Warhols, Cibo Matto, Coldplay, Lisa Loeb, K's Choice and Michelle Branch. The popularity of music used in "Buffy" has led to the release of four soundtrack albums: ', ', the ""Once More, with Feeling" Soundtrack", and "".
Setting.
Setting and filming locations.
The show is set in the fictional California town of Sunnydale, whose suburban Sunnydale High School sits on top of a "Hellmouth," a gateway to demon realms. The Hellmouth, located beneath the school library, is a source of mystical energies as well as a nexus for a wide variety of evil creatures and supernatural phenomena. In addition to being an open-ended plot device, Joss Whedon has cited the Hellmouth and "High school as Hell" as one of the primary metaphors in creating the series.
Most of "Buffy" was shot on location in Los Angeles, California. The main exterior set of the town of Sunnydale, including the "sun sign," was in a lot on Olympic Boulevard in Santa Monica, California. The high school used in the first three seasons is actually Torrance High School, in Torrance, California. In addition to the high school and its library, scenes take place in the town's cemeteries, a local nightclub (The Bronze), and Buffy's home (located in Torrance), where many of the characters live at various points in the series.
Some of the exterior shots of the college Buffy attends, UC Sunnydale, were filmed at UCLA. Several episodes include shots from the Oviatt Library at CSUN.
Format.
"Buffy" is told in a serialized format, with each episode involving a self-contained story while contributing to a larger storyline, which is broken down into season-long narratives marked by the rise and defeat of a powerful antagonist, commonly referred to as the "Big Bad". While the show is mainly a drama with frequent comic relief, most episodes blend different genres, including horror, martial arts, romance, melodrama, farce, science fiction, comedy, and even, in one episode, musical comedy.
The series' narrative revolves around Buffy and her friends, collectively dubbed the "Scooby Gang," who struggle to balance the fight against supernatural evils with their complex social lives. The show mixes complex, season-long storylines with a villain-of-the-week format; a typical episode contains one or more villains, or supernatural phenomena, that are thwarted or defeated by the end of the episode. Though elements and relationships are explored and ongoing subplots are included, the show focuses primarily on Buffy and her role as an archetypal heroine.
In the first few seasons, the most prominent monsters in the "Buffy" bestiary are vampires, which are based on traditional myths, lore, and literary conventions. As the series continues, Buffy and her companions fight an increasing variety of demons, as well as ghosts, werewolves, zombies, and unscrupulous humans. They frequently save the world from annihilation by a combination of physical combat, magic, and detective-style investigation, and are guided by an extensive collection of ancient and mystical reference books. 
Characters.
Main characters.
Buffy Summers (played by Sarah Michelle Gellar) is "the Slayer," one in a long line of young women chosen by fate to battle evil forces. This mystical calling endows her with dramatically increased physical strength, endurance, agility, accelerated healing, intuition, and a limited degree of clairvoyance, usually in the form of prophetic dreams.
Buffy receives guidance from her Watcher, Rupert Giles (Anthony Stewart Head). Giles, rarely referred to by his first name (it is later revealed that in his misspent younger days he was called "Ripper"), is a member of the Watchers' Council, whose job is to train and assist the Slayers. Giles researches the supernatural creatures that Buffy must face, offering insights into their origins and advice on how to defeat them.
Buffy is also helped by friends she meets at Sunnydale High: Willow Rosenberg (Alyson Hannigan) and Xander Harris (Nicholas Brendon). Willow is originally a wallflower who excels at academics, providing a contrast to Buffy's outgoing personality and less-than-stellar educational record. They share the social isolation that comes with being different, and especially from being exceptional young women. As the series progresses, Willow becomes a more assertive character and a powerful witch, and comes out as a lesbian. In contrast, Xander, with no supernatural skills, provides comic relief and a grounded perspective. It is Xander who often provides the heart to the series, and in season six, becomes the hero in place of Buffy who defeats the "Big Bad." Buffy and Willow are the only characters who appear in all 144 episodes; Xander is missing in only one.
Supporting characters.
The cast of characters grew over the course of the series. Buffy first arrives in Sunnydale with her mother, Joyce Summers (portrayed by Kristine Sutherland), who functions as an anchor of normality in the Summers' lives even after she learns of Buffy's role in the supernatural world ("Becoming, Part Two"). Buffy's younger sister Dawn Summers (Michelle Trachtenberg) is introduced in season five.
A vampire with a soul, Angel (portrayed by David Boreanaz), is Buffy's love interest throughout the first three seasons. He leaves Buffy to make amends for his sins and to search for redemption in his own spin-off, "Angel". He makes several guest appearances in the remaining seasons, including the last episode.
At Sunnydale High, Buffy meets several other students besides Willow and Xander willing to join her fight for good, an informal group eventually tagged the "Scooby Gang" or "Scoobies." Cordelia Chase (Charisma Carpenter), the archetypal shallow cheerleader, reluctantly becomes involved, and Daniel "Oz" Osbourne (Seth Green), a fellow student, rock guitarist and werewolf, joins the group through his relationship with Willow. Jenny Calendar (Robia LaMorte), Sunnydale's computer science teacher, joins the group after helping destroy a demon trapped in cyberspace during season 1. She later becomes Giles' love interest. Anya (Emma Caulfield), a former vengeance demon (Anyanka) who specialized in avenging scorned women, becomes Xander's lover after losing her powers and joins the group in season four.
In Buffy's senior year at high school, she meets Faith (Eliza Dushku), the other current Slayer, who was brought forth when Slayer Kendra Young (Bianca Lawson) was killed by vampire Drusilla (Juliet Landau), in season two. Although she initially fights on the side of good with Buffy and the rest of the group, she comes to stand against them and sides with Mayor Richard Wilkins (Harry Groener) after accidentally killing a human in season three. She reappears briefly in the fourth season, looking for vengeance, and moves to "Angel" where she voluntarily goes to jail for her murders. Faith reappears in season seven of "Buffy", having helped Angel and crew, and fights alongside Buffy against The First Evil.
Buffy gathers other allies: Spike (James Marsters), a vampire, is an old companion of Angelus and one of Buffy's major enemies in early seasons, although they later become allies and lovers. At the end of season six, Spike regains his soul. Spike is known for his Billy Idol-style peroxide blond hair and his black leather coat, stolen from a previous Slayer, Nikki Wood; her son, Robin Wood (D. B. Woodside), joined the group in the final season. Tara Maclay (Amber Benson) is a fellow member of Willow's Wicca group during season four, and their friendship eventually turns into a romantic relationship. Buffy became involved personally and professionally with Riley Finn (Marc Blucas), a military operative in "the Initiative," which hunts demons using science and technology. The final season sees geeky wannabe-villain Andrew Wells (Tom Lenk) come to side with the Scoobies, who regard him more as a nuisance than an ally.
"Buffy" featured dozens of recurring characters, both major and minor. For example the "Big Bad" (villain) characters were featured for at least one season (for example, Glorificus was a character who appeared in 12 episodes, spanning much of season five). Similarly, characters who allied themselves to the group and characters which attended the same institutions were sometimes featured in multiple episodes.
Plot.
Plot summary.
Season one exemplifies the "high school is hell" concept. Buffy Summers has just moved to Sunnydale after burning down her old school's gym, and hopes to escape her Slayer duties. Her plans are complicated by Rupert Giles, her new Watcher, who reminds her of the inescapable presence of evil. Sunnydale High is built atop a Hellmouth, a portal to demon dimensions that attracts supernatural phenomena to the area. Buffy befriends two schoolmates, Xander Harris and Willow Rosenberg, who help her fight evil throughout the series, but they must first prevent The Master, an ancient and especially threatening vampire, from opening the Hellmouth and taking over Sunnydale.
The emotional stakes are raised in season two. Vampires Spike and Drusilla (weakened from a mob in Prague, which, it is implied, caused her debilitating injury), come to town along with a new slayer, Kendra Young, who was activated as a result of Buffy's brief death in the season one finale. Xander becomes involved with Cordelia, while Willow becomes involved with witchcraft and Daniel "Oz" Osbourne, who is a werewolf. The romantic relationship between Buffy and the vampire Angel develops over the course of the season, but after they sleep together, Angel's soul, given to him by a Gypsy curse in the past, is lost, and he once more becomes Angelus, a sadistic killer. Kendra is killed by a restored Drusilla. Angelus torments much of the "Scooby Gang" throughout the rest of the season and murders multiple innocents and Giles's new girlfriend Jenny Calendar, a gypsy who was sent to maintain Angel's curse. To avert an apocalypse, Buffy is forced to banish Angelus to a demon dimension just moments after Willow has restored his soul. The ordeal leaves Buffy emotionally shattered, and she leaves Sunnydale.
After attempting to start a new life in Los Angeles, Buffy returns to town in season three. Angel has mysteriously been released from the demon dimension, but is close to insanity due to the torment he suffered there, and is nearly driven to suicide by the First Evil. He and Buffy realize that a relationship between them can never happen; he eventually leaves Sunnydale at the end of the season. A new watcher named Wesley is put in Giles's place when Giles is fired from the Watcher's Council because he has developed a "father's love" for Buffy; and towards the end of the season, Buffy announces that she will no longer be working for the Council. Early in the season, she meets Faith, the Slayer activated after Kendra's death. She also encounters the affable Mayor Richard Wilkins, who secretly has plans to "ascend" (become a "pure" demon) on Sunnydale High's Graduation Day. Although Faith initially works well with Buffy, she becomes increasingly unstable after accidentally killing a human and forms a relationship with the paternal yet manipulative Mayor, eventually landing in a coma after a fight with Buffy. At the end of the season, after the Mayor becomes a huge snake-like demon, Buffy and the entire graduating class destroy him by blowing up Sunnydale High.
Season four sees Buffy and Willow enroll at UC Sunnydale, while Xander joins the workforce and begins dating Anya, a former vengeance demon. Spike returns as a series regular and is abducted by The Initiative, a top-secret military installation based beneath the UC Sunnydale campus. They implant a microchip in his head that punishes him whenever he tries to harm a human. He makes a truce with the Scooby Gang and begins to fight on their side, for the joy of fighting, upon learning that he can still harm other demons. Oz leaves town after realizing that he is too dangerous as a werewolf, and Willow falls in love with Tara Maclay, another witch. Buffy begins dating Riley Finn, a graduate student and member of The Initiative. Although appearing to be a well-meaning anti-demon operation, The Initiative's sinister plans are revealed when Adam, a monster secretly built from parts of humans, demons and machinery, escapes and begins to wreak havoc on the town. Adam is destroyed by a magical composite of Buffy and her three friends, and The Initiative is shut down.
During season five, a younger sister, Dawn, suddenly appears in Buffy's life; although she is new to the series, to the characters it is as if she has always been there. Buffy is confronted by Glory, an exiled Hell God who is searching for a "Key" that will allow her to return to her Hell dimension and in the process blur the lines between dimensions and unleash Hell on Earth. It is later discovered that the Key's protectors have turned the Key into human form – Dawn – concurrently implanting everybody with lifelong memories of her. The Watcher's Council aids in Buffy's research on Glory, and she and Giles are both reinstated on their own terms. Riley leaves early in the season after realizing that Buffy does not love him and joins a military demon-hunting operation. Spike, still implanted with the Initiative chip, realizes he is in love with Buffy and increasingly helps the Scoobies in their fight. Buffy's mother Joyce dies of a brain aneurysm, while at the end of the season, Xander proposes to Anya. Glory finally discovers that Dawn is the key and kidnaps her. To save Dawn, Buffy sacrifices her own life by diving into the portal to the Hell dimension and thus closes it with her death.
At the beginning of season six, Buffy has been dead for many months, but Buffy's friends resurrect her through a powerful spell, believing they have rescued her from the Hell dimension. Buffy returns in a deep depression, explaining that she had been in Heaven and is devastated to be pulled back to earth. Giles returns to England because he has concluded that Buffy has become too reliant on him, while Buffy takes up a fast-food job to support herself and Dawn, and develops a secret, mutually abusive relationship with Spike. Dawn suffers from kleptomania and feelings of alienation, Xander leaves Anya at the altar (after which she once again becomes a vengeance demon), and Willow becomes addicted to magic, causing Tara to temporarily leave her. They also begin to deal with The Trio, a group of nerds led by Warren Mears who use their technological proficiency to attempt to kill Buffy and take over Sunnydale. Warren is shown to be the only competent villain of the group and, after Buffy thwarts his plans multiple times and the Trio breaks apart, he becomes unhinged and attacks Buffy with a gun, killing Tara in the process. This causes Willow to descend into a nihilistic darkness and unleash all of her dark magical powers, killing Warren and attempting to kill his friends. Giles returns to face her in battle and infuses her with light magic, tapping into her remaining humanity. This overwhelms Willow with guilt and pain, whereupon she attempts to destroy the world to end everyone's suffering, although it eventually allows Xander to reach through her pain and end her rampage. Late in the season, after losing control and trying to rape Buffy, Spike leaves Sunnydale and travels to see a demon and asks him to "return him to what he used to be" so that he can "give Buffy what she deserves." After Spike passes a series of brutal tests, the demon restores his soul.
During season seven, it is revealed that Buffy's second resurrection caused an instability that is allowing the First Evil to begin tipping the balance between good and evil. It begins by hunting down and killing inactive Potential Slayers, and soon raises an army of ancient, powerful Turok-Han vampires. After the Watchers' Council is destroyed, a number of Potential Slayers (some brought by Giles) take refuge in Buffy's house. Faith returns to help fight the First Evil, and the new Sunnydale High School's principal, Robin Wood, also joins the cause. The Turok-Han vampires and a sinister, misogynistic preacher known as Caleb begin causing havoc for the Scoobies. As the Hellmouth becomes more active, nearly all of Sunnydale's population – humans and demons alike – flee. In the series finale, Buffy kills Caleb, and Angel returns to Sunnydale with an amulet, which Buffy gives to Spike; the Scoobies then surround the Hellmouth and the Potential Slayers descend into its cavern, while Willow casts a spell that activates their Slayer powers. Anya dies in the fight, as do some of the new Slayers. Spike's amulet channels the power of the sun to destroy the Hellmouth and all the vampires within it, including himself. The collapse of the cavern creates a crater that swallows all of Sunnydale, while the survivors of the battle escape in a school bus. In the final scene, as the survivors survey the crater, Dawn asks, "What are we going to do now?" Buffy slowly begins to enigmatically smile as she contemplates the future ahead of her, ending the series on a hopeful note.
Inspirations and metaphors.
During the first year of the series, Whedon described the show as "My So-Called Life" meets "The X-Files." "My So-Called Life" gave a sympathetic portrayal of teen anxieties; in contrast, "The X-Files" delivered a supernatural "monster of the week" storyline. Alongside these series, Whedon has cited cult film "Night of the Comet" as a "big influence," and credited the "X-Men" character Kitty Pryde as a significant influence on the character of Buffy. The authors of the unofficial guidebook "Dusted" point out that the series was often a pastiche, borrowing elements from previous horror novels, movies, and short stories and from such common literary stock as folklore and mythology. Nevitt and Smith describe "Buffy"'s use of pastiche as "post modern Gothic." For example, the Adam character parallels the "Frankenstein" monster, the episode "Bad Eggs" parallels "Invasion of the Body Snatchers", and so on.
"Buffy" episodes often include a deeper meaning or metaphor as well. Whedon explained, "We think very carefully about what we're trying to say emotionally, politically, and even philosophically while we're writing it... it really is, apart from being a pop-culture phenomenon, something that is deeply layered textually episode by episode." Academics Wilcox and Lavery provide examples of how a few episodes deal with real life issues turned into supernatural metaphors:
In the world of "Buffy" the problems that teenagers face become literal monsters. A mother can take over her daughter's life ("Witch"); a strict stepfather-to-be really is a heartless machine ("Ted"); a young lesbian fears that her nature is demonic ("Goodbye Iowa" and "Family"); a girl who has sex with even the nicest-seeming guy may discover that he afterwards becomes a monster ("Innocence").
The love affair between the vampire Angel and Buffy was fraught with metaphors. For example, their night of passion cost the vampire his soul. Sarah Michelle Gellar said: "That's the ultimate metaphor. You sleep with a guy and he turns bad on you."
Buffy struggles throughout the series with her calling as Slayer and the loss of freedom this entails, frequently sacrificing teenage experiences for her Slayer duties. Her difficulties and eventual empowering realizations are reflections of several dichotomies faced by modern women and echo feminist issues within society.
In the episode "Becoming (Part 2)," when Joyce learns that Buffy is the Slayer, her reaction has strong echoes of a parent discovering their child is gay, including denial, suggesting that she try "not being a Slayer," and ultimately kicking Buffy out of the house.
Casting.
Actresses who auditioned for Buffy Summers and got other roles include Julie Benz (Darla), Elizabeth Anne Allen (Amy Madison), Julia Lee (Chantarelle/Lily Houston), Charisma Carpenter (Cordelia Chase), and Mercedes McNab (Harmony Kendall). Bianca Lawson, who played vampire slayer Kendra Young in season 2 of the show, auditioned for the role of Cordelia Chase before Charisma Carpenter was cast in the role.
The title role went to Sarah Michelle Gellar, who had appeared as Sydney Rutledge on "Swans Crossing" and Kendall Hart on "All My Children". At age 18 in 1995, Gellar had already won a Daytime Emmy Award for Outstanding Younger Leading Actress in a Drama Series. In 1996, she was initially cast as Cordelia Chase during a week of auditioning. She decided to keep trying for the role of Buffy, and after several more auditions, she landed the lead.
Nathan Fillion auditioned for the role of Angel back in early 1996. David Boreanaz had already been cast at the time of the unaired "Buffy" pilot, but did not appear.
Anthony Stewart Head had already led a prolific acting and singing career, but remained best known in the United States for a series of twelve coffee commercials with Sharon Maughan for Nescafé. He accepted the role of Rupert Giles.
Nicholas Brendon, unlike other "Buffy" regulars, had little acting experience, instead working various jobs—including production assistant, plumber's assistant, veterinary janitor, food delivery, script delivery, day care counselor, and waiter—before breaking into acting and overcoming his stutter. He landed his Xander Harris role following only four days of auditioning. Ryan Reynolds and Danny Strong also auditioned for the part. Strong later played the role of Jonathan Levinson, a recurring character for much of the series run.
Alyson Hannigan was the last of the original six to be cast. Following her role in "My Stepmother Is an Alien", she appeared in commercials and supporting roles on television shows throughout the early 1990s. In 1996, the role of Willow Rosenberg was initially played by Riff Regan for the unaired "Buffy" pilot, but Hannigan auditioned when the role was being recast for the series proper. Hannigan described her approach to the character through Willow's reaction to a particular moment: Willow sadly tells Buffy that her Barbie doll was taken from her as a child. Buffy asks her if she ever got it back. Willow's line was to reply "most of it." Hannigan decided on an upbeat and happy delivery of the line "most of it," as opposed to a sad, depressed delivery. Hannigan figured Willow would be happy and proud that she got "most of it" back. That indicated how she was going to play the rest of the scene, and the role, for that matter, and defined the character. Her approach subsequently got her the role.
Opening sequence.
The "Buffy" opening sequence provides credits at the beginning of each episode, with the accompanying music performed by Californian rock band Nerf Herder. In the DVD commentary for the first "Buffy" episode, Whedon said his decision to go with Nerf Herder's theme was influenced by Hannigan, who had urged him to listen to the band's music. Janet Halfyard, in her essay "Music, Gender, and Identity in "Buffy the Vampire Slayer" and "Angel"," describes the opening:
Firstly ... we have the sound of an organ, accompanied by a wolf’s howl, with a visual image of a flickering night sky overlaid with unintelligible archaic script: the associations with both the silent era and films such as "Nosferatu" and with the conventions of the Hammer House of Horror and horror in general are unmistakable.
But the theme quickly changes: "It removes itself from the sphere of 1960s and 70s horror by replaying the same motif, the organ now supplanted by an aggressively strummed electric guitar, relocating itself in modern youth culture ..." Halfyard describes sequences, in which the action and turbulence of adolescence are depicted, as the visual content of the opening credits, and which provide a postmodern twist on the horror genre.
Spin-offs.
"Buffy" has inspired a range of official and unofficial works, including television shows, books, comics and games. This expansion of the series encouraged use of the term "Buffyverse" to describe the fictional universe in which "Buffy" and related stories take place.
The franchise has inspired "Buffy" action figures and merchandise such as official "Buffy/Angel" magazines and "Buffy" companion books. Eden Studios has published a "Buffy" role-playing game, while Score Entertainment has released a "Buffy" Collectible Card Game.
Series continuation.
The storyline is currently being continued in a comic book series produced by Joss Whedon and published by Dark Horse Comics. The series, which began in 2007 with "Buffy the Vampire Slayer Season Eight", followed by "Buffy the Vampire Slayer Season Nine", serve as a canonical continuation of the television series.
Joss Whedon was interested in a film continuation in 1998, but such a film has yet to materialize.
"Angel".
The spin-off "Angel" was introduced in October 1999, at the start of "Buffy" season four. The series was created by "Buffy"'s creator Joss Whedon in collaboration with David Greenwalt. Like "Buffy", it was produced by the production company Mutant Enemy. At times, it performed better in the Nielsen ratings than its parent series did.
The series was given a darker tone focusing on the ongoing trials of Angel in Los Angeles. His character is tormented by guilt following the return of his soul, punishment for more than a century of murder and torture. During the first four seasons of the show, he works as a private detective in a fictionalized version of Los Angeles, California, where he and his associates work to "help the helpless" and to restore the faith and "save the souls" of those who have lost their way. Typically, this mission involves doing battle with evil demons or demonically allied humans (primarily the law firm Wolfram & Hart), while Angel must also contend with his own violent nature. In season five, the Senior Partners of Wolfram and Hart take a bold gamble in their campaign to corrupt Angel, giving him control of their Los Angeles office. Angel accepts the deal as an opportunity to fight evil from the inside.
In addition to Boreanaz, "Angel" inherited "Buffy" series cast regular Charisma Carpenter (Cordelia Chase). When Glenn Quinn (Doyle) left the series during its first season, Alexis Denisof (Wesley Wyndam-Pryce), who had been a recurring character in the last nine episodes of season three of "Buffy", took his place. Carpenter and Denisof were followed later by Mercedes McNab (Harmony Kendall) and James Marsters (Spike). Several actors and actresses who played "Buffy" characters made guest appearances on "Angel", including Seth Green (Daniel "Oz" Osbourne), Sarah Michelle Gellar (Buffy Summers), Eliza Dushku (Faith), Tom Lenk (Andrew Wells), Alyson Hannigan (Willow Rosenberg), Julie Benz (Darla), Mark Metcalf (The Master), Julia Lee (Anne Steele), and Juliet Landau (Drusilla). Angel also continued to appear occasionally on "Buffy".
The storyline has been continued in the comic book series "" published by IDW Publishing and later "Angel and Faith" published by Dark Horse Comics.
Expanded universe.
Outside of the TV series, the Buffyverse has been officially expanded and elaborated on by authors and artists in the so-called "Buffyverse Expanded Universe." The creators of these works may or may not keep to established continuity. Similarly, writers for the TV series were under no obligation to use information which had been established by the Expanded Universe, and sometimes contradicted such continuity.
Dark Horse has published the "Buffy" comics since 1998. In 2003, Whedon wrote an eight-issue miniseries for Dark Horse Comics titled "Fray", about a Slayer in the future. Following the publication of "Tales of the Vampires" in 2004, "Dark Horse Comics" halted publication on Buffyverse-related comics and graphic novels. The company produced Whedon's "Buffy the Vampire Slayer Season Eight" with forty issues from March 2007 to January 2011, picking up where the television show left off—taking the place of an eighth canonical season. The first story arc is also written by Whedon, and is called "The Long Way Home" which has been widely well-received, with circulation rivalling industry leaders DC and Marvel's top-selling titles. Also after "The Long Way Home" came other story arcs like Faith's return in "No Future for You" and a "Fray" cross-over in "Time of Your Life." Dark Horse later followed "Season Eight" with "Buffy the Vampire Slayer Season Nine", starting in 2011, and "Buffy the Vampire Slayer Season Ten", which began in 2014.
Pocket Books hold the license to produce "Buffy" novels, of which they have published more than sixty since 1998. These sometimes flesh out background information on characters; for example, "Go Ask Malice" details the events that lead up to Faith arriving in Sunnydale. The most recent novels include "Carnival of Souls", "Blackout", "Portal Through Time", "Bad Bargain", and "The Deathless".
Five official "Buffy" video games have been released on portable and home consoles. Most notably, "Buffy the Vampire Slayer" for Xbox in 2002 and "" for GameCube, Xbox and PlayStation 2 in 2003.
Undeveloped spinoffs.
The popularity of "Buffy" and "Angel" has led to attempts to develop more on-screen ventures in the fictional 'Buffyverse'. These projects remain undeveloped and may never be greenlit. In 2002, two potential spinoffs were in discussion: "Buffy the Animated Series" and "Ripper". "Buffy the Animated Series" was a proposed animated TV show based on "Buffy"; Whedon and Jeph Loeb were to be executive producers for the show, and most of the cast from "Buffy" were to return to voice their characters. 20th Century Fox showed an interest in developing and selling the show to another network. A three-minute pilot was completed in 2004, but was never picked up. Whedon revealed to "The Hollywood Reporter": "We just could not find a home for it. We had six or seven hilarious scripts from our own staff – and nobody wanted it." Neither the pilot nor the scripts have been seen outside of the entertainment industry, though writer Jane Espenson has teasingly revealed small extracts from some of her scripts for the show.
"Ripper" was originally a proposed television show based upon the character of Rupert Giles portrayed by Anthony Stewart Head. More recent information has suggested that if "Ripper" were ever made, it would be a TV movie or a DVD movie. There was little heard about the series until 2007 when Joss Whedon confirmed that talks were almost completed for a 90 minute "Ripper" special on the BBC with both Head and the BBC completely on board.
In 2003, a year after the first public discussions on "Buffy the Animated Series" and "Ripper", "Buffy" was nearing its end. Espenson has said that during this time spinoffs were discussed, "I think Marti talked with Joss about "Slayer School" and Tim Minear talked with him about Faith on a motorcycle. I assume there was some back-and-forth pitching." Espenson has revealed that "Slayer School" might have used new slayers and potentially included Willow Rosenberg, but Whedon did not think that such a spinoff felt right.
Dushku declined the pitch for a Buffyverse TV series based on Faith and instead agreed to a deal to produce "Tru Calling". Dushku explained to IGN: "It would have been a really hard thing to do, and not that I would not have been up for a challenge, but with it coming on immediately following "Buffy", I think that those would have been really big boots to fill." Tim Minear explained some of the ideas behind the aborted series: "The show was basically going to be Faith meets "Kung Fu". It would have been Faith, probably on a motorcycle, crossing the earth, trying to find her place in the world."
Finally, during the summer of 2004 after the end of "Angel", a movie about Spike was proposed. The movie would have been directed by Tim Minear and starred Marsters and Amy Acker and featured Alyson Hannigan. Outside the 2006 Saturn Awards, Whedon announced that he had pitched the concept to various bodies but had yet to receive any feedback.
In September 2008, "Sci-Fi Wire" ran an interview with Sarah Michelle Gellar in which she said she would not rule out returning to her most iconic role: "Never say never," she said. "One of the reasons the original "Buffy" movie did not really work on the big screen–and people blamed Kristy, but that's not what it was–the story was better told over a long arc," Gellar said. "And I worry about Buffy as a 'beginning, middle and end' so quickly. ... You show me a script; you show me that it works, and you show me that [the] audience can accept that, [and] I'd probably be there. Those are what my hesitations are."
Cultural impact.
Academia.
"Buffy" is notable for attracting the interest of scholars of popular culture, as a subset of popular culture studies, and some academic settings include the show as a topic of literary study and analysis. National Public Radio describes "Buffy" as having a "special following among academics, some of whom have staked a claim in what they call 'Buffy Studies.'" Though not widely recognized as a distinct discipline, the term "Buffy studies" is commonly used amongst the peer-reviewed academic "Buffy"-related writings.
Critics have emerged in response to the academic attention the series has received. For example, Jes Battis, who authored "", admits that study of the Buffyverse "invokes an uneasy combination of enthusiasm and ire" and meets "a certain amount of disdain from within the halls of the academy." Nonetheless, "Buffy" eventually led to the publication of around twenty books and hundreds of articles examining the themes of the show from a wide range of disciplinary perspectives, including sociology, Speech Communication, psychology, philosophy, and women's studies. In a 2012 study by "Slate", "Buffy the Vampire Slayer" was named the most studied pop culture work by academics, with more than 200 papers, essays, and books devoted to the series.
The Whedon Studies Association produces the online academic journal "Slayage" and sponsors a biennial academic conference on the works of Whedon. The sixth "Biennial Slayage Conference", titled "Much Ado About Whedon", was held at California State University-Sacramento in late June 2014.
Fandom and fan films.
The popularity of "Buffy" has led to websites, online discussion forums, works of "Buffy" fan fiction and several unofficial fan-made productions. Since the end of the series, Whedon has stated that his intention was to produce a "cult" television series and has acknowledged a “rabid, almost insane fan base" that the show has created.
"Buffy" in popular culture.
The series, which employed pop culture references as a frequent humorous device, has itself become a frequent pop culture reference in video games, comics and television shows and has been frequently parodied and spoofed. Sarah Michelle Gellar has participated in several parody sketches, including a "Saturday Night Live" sketch in which the Slayer is relocated to the "Seinfeld" universe, and adding her voice to an episode of "Robot Chicken" that parodied a would-be eighth season of "Buffy".
"Buffy" was the code-name used for an early HTC mobile phone which integrated the social networking website Facebook.
U.S. television ratings.
"Buffy" helped put The WB on the ratings map, but by the time the series landed on UPN in 2001, viewing figures had fallen. The series' high came during the third season, with 5.3 million viewers. This was probably due to the fact that both Gellar and Hannigan had hit movies out during the season ("Cruel Intentions" and "American Pie" respectively). The series' low was in season one at 3.7 million. Season seven almost equaled that, with 3.8 million. The show's series finale "Chosen" pulled in a season high of 4.9 million viewers on the UPN network.
"Buffy" did not compete with shows on the main four networks (CBS, ABC, NBC, and Fox), but The WB was impressed with the young audience that the show was bringing in. Because of this, The WB ordered a full season of 22 episodes for the series' second season. Beginning with the episode "Innocence," which was watched by 8.2 million people, "Buffy" was moved from Monday at 9:00 pm to launch The WB's new night of programming on Tuesday. Due to its large success in that time slot, it remained on Tuesdays at 8:00 pm for the remainder of its original run. With its new timeslot on The WB, the show quickly climbed to the top of The WB ratings and became one of their highest-rated shows for the remainder of its time on the network. The show always placed in the top 3, usually only coming in behind "7th Heaven". Between seasons three and five, "Buffy" flip-flopped with "Dawson's Creek" and "Charmed" as the network's second highest-rated show.
In the 2001–2002 season, the show had moved to UPN after a negotiation dispute with The WB. While it was still one of their highest rated shows on their network, The WB felt that the show had already peaked and was not worth giving a salary increase to the cast and crew. UPN on the other hand, had strong faith in the series and picked up it for a two-season renewal. UPN dedicated a two-hour premiere to the series to help re-launch it. The relaunching had effect, as the season premiere attracted the second highest rating of the series, with 7.7 million viewers.
Impact on television.
Commentators of the entertainment industry including "The Village Voice", "PopMatters", "Allmovie", "The Hollywood Reporter", "The Washington Post" have cited "Buffy" as "influential." Some citing it as the ascent of television into its golden age. Stephanie Zacharek, in the "Village Voice", wrote "If we really are in a golden age of television, "Buffy the Vampire Slayer" was a harbinger." Robert Moore of "Popmatters" also expressed these sentiments, writing "TV was not art before "Buffy", but it was afterwards," suggesting that it was responsible for re-popularizing long story arcs on primetime television.
Its effect on programming was quickly evident. Autumn 2003 saw several new shows going into production in the U.S. that featured strong females who are forced to come to terms with supernatural power or destiny while trying to maintain a normal life. These post-"Buffy" shows include "Dead Like Me", "Joan of Arcadia" and "Teen Wolf". Bryan Fuller, the creator of "Dead Like Me", said that ""Buffy" showed that young women could be in situations that were both fantastic and relatable, and instead of shunting women off to the side, it puts them at the center." In the United Kingdom, the lessons learned from the impact of "Buffy" influenced the revived "Doctor Who" series (2005–present), and executive producer Russell T Davies has said:
"Buffy the Vampire Slayer" showed the whole world, and an entire sprawling industry, that writing monsters and demons and end-of-the world is not hack-work, it can challenge the best. Joss Whedon raised the bar for every writer—not just genre/niche writers, but every single one of us.
As well as influencing "Doctor Who", "Buffy" influenced its spinoff series "Torchwood".
Several "Buffy" alumni have gone on to write for or create other shows. Such endeavors include "Tru Calling" (Douglas Petrie, Jane Espenson and lead actress Eliza Dushku), "Wonderfalls" (Tim Minear), "Point Pleasant" (Marti Noxon), "Jake 2.0" (David Greenwalt), "The Inside" (Tim Minear), "Smallville" (Steven S. DeKnight), "Once Upon a Time" (Jane Espenson), and "Lost" (Drew Goddard and David Fury).
Meanwhile, the Parents Television Council complained of efforts to "deluge their young viewing audiences with adult themes." The U.S. Federal Communications Commission (FCC), however, rejected the Council's indecency complaint concerning the violent sex scene between Buffy and Spike in "Smashed." The BBC, however, chose to censor some of the more controversial sexual content when it was shown on the pre-watershed 6:45 pm slot.
Series information.
The first season was introduced as a mid-season replacement for the short-lived night-time soap opera "Savannah", and therefore was made up of only 12 episodes. Each subsequent season was built up of 22 episodes. Discounting the unaired "Buffy" pilot, the seven seasons make up a total of 144 "Buffy" episodes aired between 1997 and 2003.
Awards and nominations.
"Buffy" has gathered a number of awards and nominations which include an Emmy Award nomination for the 1999 episode "Hush", which featured an extended sequence with no character dialogue. The 2001 episode "The Body" revolved around the death of Buffy's mother. It was filmed with no musical score, only diegetic music; it was nominated for a Nebula Award in 2002. The 2001 musical episode "Once More, with Feeling" received plaudits, but was omitted from Emmy nomination ballots by "accident". It since was featured on "Channel 4's "100 Greatest Musicals."" In 2001, Sarah Michelle Gellar received a Golden Globe-nomination for Best Actress in a TV Series-Drama for her role in the show, as well nominations for the Teen Choice Awards and the Saturn Award for Best Genre TV Actress. The series won the Drama Category for Television's Most Memorable Moment at the 60th Primetime Emmy Awards for "The Gift" beating "The X-Files", "Grey's Anatomy", "Brian's Song" and "Dallas", although the sequence for this award was not aired.
External links.
 

</doc>
<doc id="47543" url="http://en.wikipedia.org/wiki?curid=47543" title="Zero Wing">
Zero Wing

Zero Wing (ゼロウィング, Zero Uingu) is a 1989 side-scrolling shoot 'em up arcade game developed by Toaplan and published by Taito. The player is a lone hero who will save the universe from an evil force.
It enjoyed a degree of success in arcades and was subsequently ported to the Mega Drive by Toaplan on May 31, 1991, in Japan, and by Sega during the following year in Europe, followed by a Japan-only release by Naxat Soft on September 18, 1992, for the PC Engine's CD-ROM².
The European version of the Mega Drive port was the source for "All your base are belong to us", an Internet meme which plays off of the poorly translated English in the game's introduction.
Gameplay.
As with other scrolling shooters, the aim of the game is to shoot all enemies that appear on screen and avoid getting obliterated by enemy fire, crashing into enemies or into foreground scenery. There are mid-level and end-of-level boss enemies that stay with the player until they are defeated. The game features eight levels, each a few minutes long and featuring different styles and enemies: Natols, Legrous, Pleades, Aquese, Submarine Tunnel, Barricade Zone, Bellon, Gerbarra.
The player, a "ZIG" star fighter, has several ways to attack:
In the intro scenes, the ZIG's windows are green. In the game itself, the windows change color depending on what weapon the player has.
Soon after starting, the player encounters power-up ships. If destroyed, they leave behind power-ups. These run in the sequence of red weapon, blue weapon, green weapon, and speed-up, and then start with red again. There is also an occasional shield power-up, which attaches to the front of the ship. Once the first weapon power-up is collected, two small ships appear above and below the ZIG, and follow its exact movements. These extra ships are impervious and can be used as shields. As they occasionally move nearer to the ZIG when blocked by large enemies or foreground scenery, they can serve as a warning to the player that they should move carefully to avoid a collision.
Each of the three main weapons has three power levels. Each time the same weapon is collected, the power level increases. If a different weapon is collected, it starts back on level 1 power, unless level 3 power was already attained previously. If a second shield power-up is obtained while one is already held, a special power-up will replace it which increase all weapons to a special, otherwise unattainable, level 4. Level 4 weapons may also be obtained by detonating an object that would yield another bomb power up while already carrying a bomb.
Ports.
After it became fairly successful in the arcades and game centers, "Zero Wing" was ported to the Mega Drive in 1991 by Toaplan themselves and the CD-ROM², an add-on for the PC Engine, by Naxat Soft in 1992. The Mega Drive version was also released in Europe by Sega in 1992. The home console versions of "Zero Wing" were never released in North America due to the release of the arcade version distributed by Williams Electronics. The Japanese release will play fine on American consoles. Like most early titles it had no region protection, nor had the European release been PAL-optimized.
In the Mega Drive version, to expand on the game's plot, Toaplan added an introductory cut scene to the game. This introductory scene was translated by Sega of Europe to English from Japanese rather poorly for the European release (a phenomenon dubbed Engrish), resulting in dialogue such as "Somebody set up us the bomb", "All your base are belong to us", and "You have no chance to survive make your time". The introduction does not appear in the arcade nor CD-ROM² versions, rather, a different intro takes place with a blue-windowed ZIG.
Reception.
GameTrailers listed the Mega Drive version of Zero Wing as the seventh-worst video game in its "10 Best and Worst Video Games", though the focus was on its bad translation. However, in a later ScrewAttack review, it was noted the game was "not that bad". It praised its soundtrack, stating that it contains "some of the best 16-bit rock music you'll ever hear". Retro review site HonestGamers noted that "Much is made of this game, all things considered. And it's funny, because there's not a whole lot to it," before awarding a lackluster score of 4/10.
"All your base are belong to us".
In 1999, "Zero Wing"'s introduction was re-discovered, culminating in the wildly popular "All your base are belong to us" Internet meme.

</doc>
<doc id="47544" url="http://en.wikipedia.org/wiki?curid=47544" title="Carrying capacity">
Carrying capacity

The carrying capacity of a biological species in an environment is the maximum population size of the species that the environment can sustain indefinitely, given the food, habitat, water, and other necessities available in the environment. In population biology, carrying capacity is defined as the environment's maximal load, which is different from the concept of population equilibrium.
For the human population, more complex variables such as sanitation and medical care are sometimes considered as part of the necessary establishment. As population density increases, birth rate often decreases and death rate typically increases. The difference between the birth rate and the death rate is the "natural increase". The carrying capacity could support a positive natural increase, or could require a negative natural increase. Thus, the carrying capacity is the number of individuals an environment can support without significant negative impacts to the given organism and its environment. Below carrying capacity, populations typically increase, while above, they typically decrease. A factor that keeps population size at equilibrium is known as a regulating factor. Population size decreases above carrying capacity due to a range of factors depending on the species concerned, but can include insufficient space, food supply, or sunlight. The carrying capacity of an environment may vary for different species and may change over time due to a variety of factors, including: food availability, water supply, environmental conditions and living space.
The origins of the term "carrying capacity" are uncertain, with researchers variously stating that it was used "in the context of international shipping" or that it was first used during 19th-century laboratory experiments with micro-organisms. A recent review finds the first use of the term in an 1845 report by the US Secretary of State to the Senate.
Humans.
Several estimates of the carrying capacity have been made with a wide range of population numbers. A 2001 UN report said that two-thirds of the estimates fall in the range of 4 billion to 16 billion (with unspecified standard errors), with a median of about 10 billion. More recent estimates are much lower, particularly if resource depletion and increased consumption are considered.
The application of the concept of carrying capacity for the human population has been criticized for not successfully capturing the multi-layered processes between humans and the environment, which have a nature of fluidity and non-equilibrium, and for sometimes being employed in a blame-the-victim framework.
Supporters of the concept argue that the idea of a finite carrying capacity is just as valid when applied to humans as when applied to any other species. Animal population size, living standards, and resource depletion vary, but the concept of carrying capacity still applies. The carrying capacity of Earth has been studied by computer simulation models like World3.
Numbers of people are not the only factor. Waste and over-consumption, especially by wealthy people and nations, is putting more strain on the environment than overpopulation.
Factors that Govern Carrying Capacity.
Some aspects of a system's carrying capacity may involve matters such as available supplies of food, water, and/or other similar "resources." In addition, there are other factors that govern carrying capacity which may be less-instinctive or less-intuitive in nature, such as ever-increasing and/or ever-accumulating levels of wastes, damage, and/or eradication of essential components of any complex functioning system. Eradication of, for example, large or critical portions of any complex system (envision a space vehicle, for instance, or an airplane, or an automobile, or computer code, or the body components of a living vertebrate) can interrupt essential processes and dynamics in ways that induce systems-failures or unexpected collapse. (As an example of these latter factors, the "carrying capacity" of a complex system such an airplane is more than a matter of available food, or water, or available seating, but also reflects total weight carried and presumes that its passengers do not damage, destroy, or eradicate parts, doors, windows, wings, engine parts, fuel, and oil, and so forth.) Thus, on a global scale, food and similar resources may affect planetary carrying capacity to some extent so long as Earth's human passengers do not dismantle, eradicate, or otherwise destroy critical biospheric life-support capacities for essential processes of self-maintenance, self-perpetuation, and self-repair.
Thus, carrying capacity interpretations that focus solely on resource-limitations alone (such as food) may distract attention from wider functional factors. lIf the humans neither gain nor lose weight in the long run, the calculation is fairly accurate. If the quantity of food is invariably equal to the "Y" amount, carrying capacity has been reached. Humans, with the need to enhance their reproductive success (see Richard Dawkins' "The Selfish Gene"), understand that food supply can vary and also that other factors in the environment can alter humans' need for food. A house, for example, might mean that one does not need to eat as much to stay warm as one otherwise would. Over time, monetary transactions have replaced barter and local production, and consequently modified local human carrying capacity. However, purchases also impact regions thousands of miles away. For example, carbon dioxide from an automobile travels to the upper atmosphere. This led Paul R. Ehrlich to develop the I = PAT equation
where:
Technology can play a role in the dynamics of carrying capacity and while this can sometimes be positive, in other cases its influence can be problematic. For example, it has been suggested that in the past that the Neolithic revolution increased the carrying capacity of the world relative to humans through the invention of agriculture. In a similar way, viewed from the perspective of foods, the use of fossil fuels has been alleged to artificially increase the carrying capacity of the world by the use of stored sunlight, even though that food production does not guarantee the capacity of the Earth's climatic and biospheric life-support systems to withstand the damage and wastes arising from such fossil fuels. Again, however, such interpretations presume the continued and uninterrupted functioning of all other critical components of the global system. It has also been suggested that other technological advances that have increased the carrying capacity of the world relative to humans are: polders, fertilizer, composting, greenhouses, land reclamation, and fish farming. In an adverse way, however, many technologies enable economic entities and individual humans to inflict far more damage and eradication, far more quickly, efficiently, and on a wider-scale than ever. (As examples of such problematic outcomes of technology, imagine machine guns, chain saws, earth-movers, and the capacity of industrialized fishing fleets to capture and harvest targeted fish species faster than the fish themselves can reproduce.)
Agricultural capability on Earth expanded in the last quarter of the 20th century. But now there are many projections of a continuation of the decline in world agricultural capability (and hence carrying capacity) which began in the 1990s. Most conspicuously, China's food production is forecast to decline by 37% by the last half of the 21st century, placing a strain on the entire carrying capacity of the world, as China's population could expand to about 1.5 billion people by the year 2050. This reduction in China's agricultural capability (as in other world regions) is largely due to the world water crisis and especially due to mining groundwater beyond sustainable yield, which has been happening in China since the mid-20th century.
Lester Brown of the Earth Policy Institute, has said: "It would take 1.5 Earths to sustain our present level of consumption. Environmentally, the world is in an overshoot mode."
Ecological footprint.
One way to estimate human demand compared to ecosystem's carrying capacity is "Ecological footprint" accounting. Rather than speculating about future possibilities and limitations imposed by carrying capacity constraints, Ecological Footprint accounting provides empirical, non-speculative assessments of the past. It compares historic regeneration rates (biocapacity) against historical human demand (Ecological Footprint) in the same year. One result shows that humanity's demand for 1999 exceeded the planet's biocapacity for 1999 by over 20 percent.
See also.
</dl>

</doc>
<doc id="47546" url="http://en.wikipedia.org/wiki?curid=47546" title="Vardar Macedonia">
Vardar Macedonia

Vardar Macedonia (formerly Yugoslav Macedonia) is an area in the north of the geographical region of Macedonia, corresponding with the area of today's Republic of Macedonia. It covers an area of 25713 km2. It usually refers to the part of Macedonia region attributed to the Kingdom of Serbia by the Treaty of Bucharest in 1913. It is named after the Vardar, the major river in the area.

</doc>
<doc id="47548" url="http://en.wikipedia.org/wiki?curid=47548" title="Daylight saving time">
Daylight saving time

Daylight saving time (DST) or summer time is the practice of advancing clocks during summer months by one hour so that in the evening hours day light is experienced later, while sacrificing normal sunrise times. Typically, users in regions with summer time adjust clocks forward one hour close to the start of spring and adjust them backward in the autumn to standard time.
New Zealander George Vernon Hudson proposed the modern idea of daylight saving in 1895. Germany and Austria-Hungary organized the first implementation, starting on 30 April 1916. Many countries have used it at various times since then, particularly since the energy crisis of the 1970s.
The practice has received both advocacy and criticism. Putting clocks forward benefits retailing, sports, and other activities that exploit sunlight after working hours, but can cause problems for evening entertainment and for other activities tied to sunlight, such as farming. Although some early proponents of DST aimed to reduce evening use of incandescent lighting, which was formerly a primary use of electricity, modern heating and cooling usage patterns differ greatly and research about how DST currently affects energy use is limited or contradictory.
DST clock shifts sometimes complicate timekeeping and can disrupt travel, billing, record keeping, medical devices, heavy equipment, and sleep patterns. Computer software can often adjust clocks automatically, but policy changes by various jurisdictions of the dates and timings of DST may be confusing.
Rationale.
Industrialized societies generally follow a clock-based schedule for daily activities that does not change throughout the course of the year. The time of day that individuals begin and end work or school, and the coordination of mass transit, for example, usually remain constant year-round. In contrast, an agrarian society's daily routines for work and personal conduct are more likely governed by the length of daylight hours and solar time, which change seasonally because of the Earth's axial tilt. North and south of the tropics daylight lasts longer in summer and shorter in winter, the effect becoming greater as one moves away from the tropics.
By synchronously resetting all clocks in a region to one hour ahead of Standard Time (one hour "fast"), individuals who follow such a year-round schedule will wake an hour earlier than they would have otherwise; they will begin and complete daily work routines an hour earlier, and they will have an extra hour of daylight after their workday activities. However, they will have one less hour of daylight at the start of each day, making the policy less practical during winter.
While the times of sunrise and sunset change at roughly equal rates as the seasons change, proponents of Daylight Saving Time argue that most people prefer a greater increase in daylight hours after the typical "nine-to-five" workday. Supporters have also argued that DST decreases energy consumption by reducing the need for lighting and heating, but the actual effect on overall energy use is heavily disputed. (See: "Dispute over benefits and drawbacks section")
The manipulation of time at higher latitudes (for example Iceland, Nunavut or Alaska) has little impact on daily life, because the length of day and night changes more extremely throughout the seasons (in comparison to other latitudes), and thus sunrise and sunset times are significantly out of sync with standard working hours regardless of manipulations of the clock. DST is also of little use for locations near the equator, because these regions see only a small variation in daylight throughout the year.
History.
Although they did not fix their schedules to the clock in the modern sense, ancient civilizations adjusted daily schedules to the sun more flexibly than modern DST does, often dividing daylight into twelve hours regardless of day length, so that each daylight hour was longer during summer. For example, Roman water clocks had different scales for different months of the year: at Rome's latitude the third hour from sunrise, "hora tertia", started by modern standards at 09:02 solar time and lasted 44 minutes at the winter solstice, but at the summer solstice it started at 06:58 and lasted 75 minutes. After ancient times, equal-length civil hours eventually supplanted unequal, so civil time no longer varies by season. Unequal hours are still used in a few traditional settings, such as some Mount Athos monasteries and all Jewish ceremonies.
During his time as an American envoy to France, Benjamin Franklin, publisher of the old English proverb, "Early to bed, and early to rise, makes a man healthy, wealthy and wise", anonymously published a letter suggesting that Parisians economize on candles by rising earlier to use morning sunlight. This 1784 satire proposed taxing shutters, rationing candles, and waking the public by ringing church bells and firing cannons at sunrise. Despite common misconception, Franklin did "not" actually propose DST: like ancient Rome, 18th-century Europe did not even keep precise schedules. However, this soon changed as rail and communication networks came to require a standardization of time unknown in Franklin's day.
Modern DST was first proposed by the New Zealand entomologist George Vernon Hudson, whose shift-work job gave him leisure time to collect insects, and led him to value after-hours daylight. In 1895 he presented a paper to the Wellington Philosophical Society proposing a two-hour daylight-saving shift, and after considerable interest was expressed in Christchurch, he followed up in an 1898 paper. Many publications credit DST's proposal to the prominent English builder and outdoorsman William Willett, who independently conceived DST in 1905 during a pre-breakfast ride, when he observed with dismay how many Londoners slept through a large part of a summer's day. An avid golfer, he also disliked cutting short his round at dusk. His solution was to advance the clock during the summer months, a proposal he published two years later. The proposal was taken up by the Liberal Member of Parliament (MP) Robert Pearce, who introduced the first Daylight Saving Bill to the House of Commons on 12 February 1908. A select committee was set up to examine the issue, but Pearce's bill did not become law, and several other bills failed in the following years. Willett lobbied for the proposal in the UK until his death in 1915.
Starting on 30 April 1916, Germany and its World War I ally Austria-Hungary were the first to use DST (German: "") as a way to conserve coal during wartime. Britain, most of its allies, and many European neutrals soon followed suit. Russia and a few other countries waited until the next year and the United States adopted it in 1918.
Broadly speaking, Daylight Saving Time was abandoned in the years after the war (with some notable exceptions including Canada, the UK, France, and Ireland for example). However, it was brought back for periods of time in many different places during the following decades, and commonly during the Second World War. It became widely adopted, particularly in North America and Europe starting in the 1970s as a result of the 1970s energy crisis.
Since then, the world has seen many enactments, adjustments, and repeals. For specific details, an overview is available at Daylight saving time by country.
Procedure.
In the case of the United States where a one-hour shift occurs at 02:00 local time, in spring the clock jumps forward from the last moment of 01:59 standard time to 03:00 DST and that day has 23 hours, whereas in autumn the clock jumps backward from the last moment of 01:59 DST to 01:00 standard time, repeating that hour, and that day has 25 hours. A digital display of local time does not read 02:00 exactly at the shift to summer time, but instead jumps from 01:59:59.9 forward to 03:00:00.0.
Clock shifts are usually scheduled near a weekend midnight to lessen disruption to weekday schedules. A one-hour shift is customary, but Australia's Lord Howe Island uses a half-hour shift. Twenty-minute and two-hour shifts have been used in the past.
Coordination strategies differ when adjacent time zones shift clocks. The European Union shifts all at once, at 01:00 UTC or 02:00 CET or 03:00 EET; for example, Eastern European Time is always one hour ahead of Central European Time. Most of North America shifts at 02:00 local time, so its zones do not shift at the same time; for example, Mountain Time is temporarily (for one hour) zero hours ahead of Pacific Time, instead of one hour ahead, in the autumn and two hours, instead of one, ahead of Pacific Time in the spring. In the past, Australian districts went even further and did not always agree on start and end dates; for example, in 2008 most DST-observing areas shifted clocks forward on October 5 but Western Australia shifted on October 26. In some cases only part of a country shifts; for example, in the US, Hawaii and most of Arizona do not observe DST.
Start and end dates vary with location and year. Since 1996 European Summer Time has been observed from the last Sunday in March to the last Sunday in October; previously the rules were not uniform across the European Union. Starting in 2007, most of the United States and Canada observe DST from the second Sunday in March to the first Sunday in November, almost two-thirds of the year. The 2007 US change was part of the Energy Policy Act of 2005; previously, from 1987 through 2006, the start and end dates were the first Sunday in April and the last Sunday in October, and Congress retains the right to go back to the previous dates now that an energy-consumption study has been done.
Beginning and ending dates are roughly the reverse in the southern hemisphere. For example, mainland Chile observed DST from the second Saturday in October to the second Saturday in March, with transitions at local time. The time difference between the United Kingdom and mainland Chile could therefore be five hours during the Northern summer, three hours during the Northern winter and four hours a few weeks per year because of mismatch of changing dates.
DST is generally not observed near the equator, where sunrise times do not vary enough to justify it. Some countries observe it only in some regions; for example, southern Brazil observes it while equatorial Brazil does not. Only a minority of the world's population uses DST because Asia and Africa generally do not observe it.
Politics.
Daylight saving has caused controversy since it began. Winston Churchill argued that it enlarges "the opportunities for the pursuit of health and happiness among the millions of people who live in this country" and pundits have dubbed it "Daylight Slaving Time". Historically, retailing, sports, and tourism interests have favored daylight saving, while agricultural and evening entertainment interests have opposed it, and its initial adoption had been prompted by energy crisis and war.
The fate of Willett's 1907 proposal illustrates several political issues involved. The proposal attracted many supporters, including Balfour, Churchill, Lloyd George, MacDonald, Edward VII (who used half-hour DST at Sandringham), the managing director of Harrods, and the manager of the National Bank. However, the opposition was stronger: it included Prime Minister H. H. Asquith, Christie (the Astronomer Royal), George Darwin, Napier Shaw (director of the Meteorological Office), many agricultural organizations, and theater owners. After many hearings the proposal was narrowly defeated in a Parliament committee vote in 1909. Willett's allies introduced similar bills every year from 1911 through 1914, to no avail. The US was even more skeptical: Andrew Peters introduced a DST bill to the US House of Representatives in May 1909, but it soon died in committee.
After Germany led the way with starting DST (German: "") during World War I on 30 April 1916 together with its allies to alleviate hardships from wartime coal shortages and air raid blackouts, the political equation changed in other countries; the United Kingdom used DST first on 21 May 1916. US retailing and manufacturing interests led by Pittsburgh industrialist Robert Garland soon began lobbying for DST, but were opposed by railroads. The US's 1917 entry to the war overcame objections, and DST was established in 1918.
The war's end swung the pendulum back. Farmers continued to dislike DST, and many countries repealed it after the war. Britain was an exception: it retained DST nationwide but over the years adjusted transition dates for several reasons, including special rules during the 1920s and 1930s to avoid clock shifts on Easter mornings. The US was more typical: Congress repealed DST after 1919. President Woodrow Wilson, like Willett an avid golfer, vetoed the repeal twice but his second veto was overridden. Only a few US cities retained DST locally thereafter, including New York so that its financial exchanges could maintain an hour of arbitrage trading with London, and Chicago and Cleveland to keep pace with New York. Wilson's successor Warren G. Harding opposed DST as a "deception". Reasoning that people should instead get up and go to work earlier in the summer, he ordered District of Columbia federal employees to start work at 08:00 rather than 09:00 during summer 1922. Some businesses followed suit though many others did not; the experiment was not repeated.
Since Germany's adoption in 1916 the world has seen many enactments, adjustments, and repeals of DST, with similar politics involved. The history of time in the United States includes DST during both world wars, but no standardization of peacetime DST until 1966. In May 1965, for two weeks, St. Paul and Minneapolis were on different times, when the capital city decided to join most of the nation by starting Daylight Saving Time while Minneapolis opted to follow the later date set by state law. In the mid-1980s, Clorox (parent of Kingsford Charcoal) and 7-Eleven provided the primary funding for the Daylight Saving Time Coalition behind the 1987 extension to US DST, and both Idaho senators voted for it based on the premise that during DST fast-food restaurants sell more French fries, which are made from Idaho potatoes. In 1992 after a three-year trial of daylight saving in Queensland, Australia, a referendum on daylight saving was held and defeated with a 54.5% 'no' vote – with regional and rural areas strongly opposed, while those in the metropolitan south-east were in favor. In 2005, the Sporting Goods Manufacturers Association and the National Association of Convenience Stores successfully lobbied for the 2007 extension to US DST. In December 2008, the Daylight Saving for South East Queensland (DS4SEQ) political party was officially registered in Queensland, advocating the implementation of a dual-time zone arrangement for Daylight Saving in South East Queensland while the rest of the state maintains standard time. DS4SEQ contested the March 2009 Queensland State election with 32 candidates and received one percent of the statewide primary vote, equating to around 2.5% across the 32 electorates contested. After a three-year trial, more than 55% of Western Australians voted against DST in 2009, with rural areas strongly opposed. On 14 April 2010, after being approached by the DS4SEQ political party, Queensland Independent member Peter Wellington, introduced the Daylight Saving for South East Queensland Referendum Bill 2010 into Queensland Parliament, calling for a referendum to be held at the next State election on the introduction of daylight saving into South East Queensland under a dual-time zone arrangement. The Bill was defeated in Queensland Parliament on 15 June 2011.
In the UK the Royal Society for the Prevention of Accidents supports a proposal to observe SDST's additional hour year-round, but is opposed in some industries, such as postal workers and farmers, and particularly by those living in the northern regions of the UK.
In some Muslim countries DST is temporarily abandoned during Ramadan (the month when no food should be eaten between sunrise and sunset), since the DST would delay the evening dinner. Ramadan took place in July and August in 2012. This concerns at least Morocco and Palestine, although Iran keeps DST during Ramadan. Most Muslim countries do not use DST, partially for this reason.
The 2011 declaration by Russia that it would not turn its clocks back and stay in DST all year long was subsequently followed by a similar declaration from Belarus. The plan generated widespread complaints due to the dark of wintertime morning, and thus was abandoned in 2014. The country changed its clocks to Standard Time on 26 October 2014 - and intends to stay there permanently.
Dispute over benefits and drawbacks.
Proponents of DST generally argue that it saves energy, promotes outdoor leisure activity in the evening (in summer), and is therefore good for physical and psychological health, reduces traffic accidents, reduces crime, or is good for business. Groups that tend to support DST are urban workers, retail businesses, outdoor sports enthusiasts and businesses, tourism operators, and others who benefit from increased light during the evening in summer.
Opponents argue that actual energy savings are inconclusive, that DST increases health risks such as heart attack, that DST can disrupt morning activities, and that the act of changing clocks twice a year is economically and socially disruptive and cancels out any benefit. Farmers have tended to oppose DST.
Common agreement about the day's layout or schedule confers so many advantages that a standard DST schedule has generally been chosen over ad hoc efforts to get up earlier. The advantages of coordination are so great that many people ignore whether DST is in effect by altering their nominal work schedules to coordinate with television broadcasts or daylight. DST is commonly not observed during most of winter, because its mornings are darker; workers may have no sunlit leisure time, and children may need to leave for school in the dark. Since DST is applied to many varying communities, its effects may be very different depending on their culture, light levels, geography, and climate; that is why it is hard to make generalized conclusions about the absolute effects of the practice. Some areas may adopt DST simply as a matter of coordination with others rather than for any direct benefits.
Energy use.
DST's potential to save energy comes primarily from its effects on residential lighting, which consumes about 3.5% of electricity in the United States and Canada. Delaying the nominal time of sunset and sunrise reduces the use of artificial light in the evening and increases it in the morning. As Franklin's 1784 satire pointed out, lighting costs are reduced if the evening reduction outweighs the morning increase, as in high-latitude summer when most people wake up well after sunrise. An early goal of DST was to reduce evening usage of incandescent lighting, formerly a primary use of electricity. Although energy conservation remains an important goal, energy usage patterns have greatly changed since then, and recent research is limited and reports contradictory results. Electricity use is greatly affected by geography, climate, and economics, making it hard to generalize from single studies.
Several studies have suggested that DST increases motor fuel consumption. The 2008 DOE report found no significant increase in motor gasoline consumption due to the 2007 United States extension of DST.
Economic effects.
Retailers, sporting goods makers, and other businesses benefit from extra afternoon sunlight, as it induces customers to shop and to participate in outdoor afternoon sports. In 1984, "Fortune" magazine estimated that a seven-week extension of DST would yield an additional $30 million for 7-Eleven stores, and the National Golf Foundation estimated the extension would increase golf industry revenues $200 million to $300 million. A 1999 study estimated that DST increases the revenue of the European Union's leisure sector by about 3%.
Conversely, DST can adversely affect farmers, parents of young children, and others whose hours are set by the sun and they have traditionally opposed the practice, although some farmers are neutral. One reason why farmers oppose DST is that grain is best harvested after dew evaporates, so when field hands arrive and leave earlier in summer their labor is less valuable. Dairy farmers are another group who complain of the change. Their cows are sensitive to the timing of milking, so delivering milk earlier disrupts their systems. Today some farmers' groups are in favor of DST.
DST also hurts prime-time television broadcast ratings, drive-ins and other theaters.
Changing clocks and DST rules has a direct economic cost, entailing extra work to support remote meetings, computer applications and the like. For example, a 2007 North American rule change cost an estimated $500 million to $1 billion, and Utah State University economist William F. Shughart II has estimated the lost opportunity cost at around $1.7 billion USD. Although it has been argued that clock shifts correlate with decreased economic efficiency, and that in 2000 the daylight-saving effect implied an estimated one-day loss of $31 billion on US stock exchanges, the estimated numbers depend on the methodology and the results have been disputed.
Public safety.
In 1975 the US DOT conservatively identified a 0.7% reduction in traffic fatalities during DST, and estimated the real reduction at 1.5% to 2%, but the 1976 NBS review of the DOT study found no differences in traffic fatalities. In 1995 the Insurance Institute for Highway Safety estimated a reduction of 1.2%, including a 5% reduction in crashes fatal to pedestrians. Others have found similar reductions. Single/Double Summer Time (SDST), a variant where clocks are one hour ahead of the sun in winter and two in summer, has been projected to reduce traffic fatalities by 3% to 4% in the UK, compared to ordinary DST. However, accidents do increase by as much as 11% during the two weeks that follow the end of British Summer Time. It is not clear whether sleep disruption contributes to fatal accidents immediately after the spring clock shifts. A correlation between clock shifts and traffic accidents has been observed in North America and the UK but not in Finland or Sweden. If this effect exists, it is far smaller than the overall reduction in traffic fatalities. A 2009 US study found that on Mondays after the switch to DST, workers sleep an average of 40 minutes less, and are injured at work more often and more severely.
In the 1970s the US Law Enforcement Assistance Administration (LEAA) found a reduction of 10% to 13% in Washington, D.C.'s violent crime rate during DST. However, the LEAA did not filter out other factors, and it examined only two cities and found crime reductions only in one and only in some crime categories; the DOT decided it was "impossible to conclude with any confidence that comparable benefits would be found nationwide". Outdoor lighting has a marginal and sometimes even contradictory influence on crime and fear of crime.
In several countries, fire safety officials encourage citizens to use the two annual clock shifts as reminders to replace batteries in smoke and carbon monoxide detectors, particularly in autumn, just before the heating and candle season causes an increase in home fires. Similar twice-yearly tasks include reviewing and practicing fire escape and family disaster plans, inspecting vehicle lights, checking storage areas for hazardous materials, reprogramming thermostats, and seasonal vaccinations. Locations without DST can instead use the first days of spring and autumn as reminders.
Health.
DST has mixed effects on health. In societies with fixed work schedules it provides more afternoon sunlight for outdoor exercise. It alters sunlight exposure; whether this is beneficial depends on one's location and daily schedule, as sunlight triggers vitamin D synthesis in the skin, but overexposure can lead to skin cancer. DST may help in depression by causing individuals to rise earlier, but some argue the reverse. The Retinitis Pigmentosa Foundation Fighting Blindness, chaired by blind sports magnate Gordon Gund, successfully lobbied in 1985 and 2005 for US DST extensions.
Clock shifts were found to increase the risk of heart attack by 10 percent, and to disrupt sleep and reduce its efficiency. Effects on seasonal adaptation of the circadian rhythm can be severe and last for weeks. A 2008 study found that although male suicide rates rise in the weeks after the spring transition, the relationship weakened greatly after adjusting for season. A 2008 Swedish study found that heart attacks were significantly more common the first three weekdays after the spring transition, and significantly less common the first weekday after the autumn transition. The government of Kazakhstan cited health complications due to clock shifts as a reason for abolishing DST in 2005. In March 2011, Dmitri Medvedev, president of Russia, claimed that "stress of changing clocks" was the motivation for Russia to stay in DST all year long. Officials at the time talked about an annual increase in suicides.
An unexpected adverse effect of daylight saving time may lie in the fact that an extra part of morning rush hour traffic occurs before dawn and traffic emissions then cause higher air pollution than during daylight hours.
Complexity.
DST's clock shifts have the obvious disadvantage of complexity. People must remember to change their clocks; this can be time-consuming, particularly for mechanical clocks that cannot be moved backward safely. People who work across time zone boundaries need to keep track of multiple DST rules, as not all locations observe DST or observe it the same way. The length of the calendar day becomes variable; it is no longer always 24 hours. Disruption to meetings, travel, broadcasts, billing systems, and records management is common, and can be expensive. During an autumn transition from 02:00 to 01:00, a clock reads times from 01:00:00 through 01:59:59 twice, possibly leading to confusion.
Damage to a German steel facility occurred during a DST transition in 1993, when a computer timing system linked to a radio time synchronization signal allowed molten steel to cool for one hour less than the required duration, resulting in spattering of molten steel when it was poured. Medical devices may generate adverse events that could harm patients, without being obvious to clinicians responsible for care. These problems are compounded when the DST rules themselves change; software developers must test and perhaps modify many programs, and users must install updates and restart applications. Consumers must update devices such as programmable thermostats with the correct DST rules, or manually adjust the devices' clocks. A common strategy to resolve these problems in computer systems is to express time using the Coordinated Universal Time (UTC) rather than the local time zone. For example, Unix-based computer systems use the UTC-based Unix time internally.
Some clock-shift problems could be avoided by adjusting clocks continuously or at least more gradually—for example, Willett at first suggested weekly 20-minute transitions—but this would add complexity and has never been implemented.
DST inherits and can magnify the disadvantages of standard time. For example, when reading a sundial, one must compensate for it along with time zone and natural discrepancies. Also, sun-exposure guidelines such as avoiding the sun within two hours of noon become less accurate when DST is in effect.
Terminology.
As explained by Richard Meade in the English Journal of the (American) National Council of Teachers of English, the form "daylight savings time" (with an "s") was already in 1978 much more common than the older form "daylight saving time" in American English ("the change has been virtually accomplished"). Nevertheless even dictionaries such as Merriam-Webster's, American Heritage, and Oxford, which describe actual usage instead of prescribing outdated usage (and therefore also list the newer form), still list the older form first. This is because the older form is still very common in print and preferred by many editors. ("Although "daylight saving time" is considered correct, "daylight savings time" (with an "s") is commonly used.") The first two words are sometimes hyphenated ("daylight-saving[s] time"). Merriam-Webster's also lists the forms daylight saving (without "time"), daylight savings (without "time"), and daylight time.
In Britain, Willett's 1907 proposal used the term "daylight saving", but by 1911 the term "summer time" replaced "daylight saving time" in draft legislation. Continental Europe uses similar phrases, such as "Sommerzeit" in Germany, "zomertijd" in Dutch-speaking regions, "kesäaika" in Finland, "horario de verano" or "hora de verano" in Spain and "heure d'été" in France, whereas in Italy the term is "ora legale", that is, legal time (legally enforced time) as opposed to "ora solare", solar time, in winter.
The name of local time typically changes when DST is observed. American English replaces "standard" with "daylight": for example, "Pacific Standard Time" ("PST") becomes "Pacific Daylight Time" ("PDT"). In the United Kingdom, the standard term for UK time when advanced by one hour is "British Summer Time" (BST), and British English typically inserts "summer" into other time zone names, e.g. "Central European Time" ("CET") becomes "Central European Summer Time" ("CEST").
The North American mnemonic "spring forward, fall back" (also "spring ahead ...", "spring up ...", and "... fall behind") helps people remember which direction to shift clocks.
Computing.
Changes to DST rules cause problems in existing computer installations. For example, the 2007 change to DST rules in North America required many computer systems to be upgraded, with the greatest impact on email and calendaring programs; the upgrades consumed a significant effort by corporate information technologists.
Some applications standardize on UTC to avoid problems with clock shifts and time zone differences.
Likewise, most modern operating systems internally handle and store all times as UTC and only convert to local time for display.
However, even if UTC is used internally, the systems still require information on time zones to correctly calculate local time where it is needed. Many systems in use today base their date/time calculations from data derived from the IANA time zone database also known as zoneinfo.
IANA time zone database.
The IANA time zone database maps a name to the named location's historical and predicted clock shifts. This database is used by many computer software systems, including most Unix-like operating systems, Java, and the Oracle RDBMS; HP's "tztab" database is similar but incompatible. When temporal authorities change DST rules, zoneinfo updates are installed as part of ordinary system maintenance. In Unix-like systems the TZ environment variable specifies the location name, as in codice_1. In many of those systems there is also a system-wide setting that is applied if the TZ environment variable isn't set: this setting is controlled by the contents of the /etc/localtime file, which is usually a symbolic link or hard link to one of the zoneinfo files. Internal time is stored in timezone-independent epoch time; the TZ is used by each of potentially many simultaneous users and processes to independently localize time display.
Older or stripped-down systems may support only the TZ values required by POSIX, which specify at most one start and end rule explicitly in the value. For example, codice_2 specifies time for the eastern United States starting in 2007. Such a TZ value must be changed whenever DST rules change, and the new value applies to all years, mishandling some older timestamps.
Microsoft Windows.
As with zoneinfo, a user of Microsoft Windows configures DST by specifying the name of a location, and the operating system then consults a table of rule sets that must be updated when DST rules change. Procedures for specifying the name and updating the table vary with release. Updates are not issued for older versions of Microsoft Windows. Windows Vista supports at most two start and end rules per time zone setting. In a Canadian location observing DST, a single Vista setting supports both 1987–2006 and post-2006 time stamps, but mishandles some older time stamps. Older Microsoft Windows systems usually store only a single start and end rule for each zone, so that the same Canadian setting reliably supports only post-2006 time stamps.
These limitations have caused problems. For example, before 2005, DST in Israel varied each year and was skipped some years. Windows 95 used rules correct for 1995 only, causing problems in later years. In Windows 98, Microsoft marked Israel as not having DST, forcing Israeli users to shift their computer clocks manually twice a year. The 2005 Israeli Daylight Saving Law established predictable rules using the Jewish calendar but Windows zone files could not represent the rules' dates in a year-independent way. Partial workarounds, which mishandled older time stamps, included manually switching zone files every year and a Microsoft tool that switches zones automatically. In 2013, Israel standardized its daylight saving time according to the Gregorian calendar.
Microsoft Windows keeps the system real-time clock in local time. This causes several problems, including compatibility when multi booting with operating systems that set the clock to UTC, and double-adjusting the clock when multi booting different Windows versions, such as with a rescue boot disk. This approach is a problem even in Windows-only systems: there is no support for per-user timezone settings, only a single system-wide setting. In 2008 Microsoft hinted that future versions of Windows will partially support a Windows registry entry RealTimeIsUniversal that had been introduced many years earlier, when Windows NT supported RISC machines with UTC clocks, but had not been maintained. Since then at least two fixes related to this feature have been published by Microsoft.
An interesting effect can be observed with file time properties. The NTFS file system used by recent versions of Windows stores the file with a UTC time stamp, but displays it corrected to local—or seasonal—time. However, the FAT filesystem commonly used on removable devices stores only the local time. Consequently, when a file is copied from the hard disk onto separate media, its time will be set to the current local time. If the time adjustment is changed, perhaps automatically (daylight saving) or if the user selects a different time zone, then when the timestamps of the original file and the copy are compared there will be a difference. This may be verified (without having to wait for the next equinox) by copying a file, removing the media, adjusting the time zone options, reconnecting the media, and viewing the details of the file and its copy. The same effect can be observed when compressing and uncompressing files with some file archivers. It is the NTFS file that changes seen time. One can store a file listing and check it after the DST change. This effect should be kept in mind when trying to determine if a file is a duplicate of another although there are other methods of comparing files for equality like using checksum algorithms.
Permanent daylight saving time.
A move to "permanent daylight saving time" (staying on summer hours all year with no time shifts) is sometimes advocated, and has in fact been implemented in some jurisdictions such as Argentina, Chile, Iceland, Singapore, Uzbekistan and Belarus. Advocates cite the same advantages as normal DST without the problems associated with the twice yearly time shifts. However, many remain unconvinced of the benefits, citing the same problems and the relatively late sunrises, particularly in winter, that year-round DST entails. Russia switched to permanent DST from 2011 to 2014, but the move proved unpopular because of the late sunrises in winter, so the country switched permanently back to "standard" or "winter" time in 2014.
The Xinjiang Uyghur Autonomous Region in western China, Argentina, Chile, Iceland, Russia and other areas skew time zones westward, in effect observing DST year-round without complications from clock shifts. For example, Saskatoon, Saskatchewan, is at 106°39′ W longitude, slightly west of center of the idealized Mountain Time Zone (105° W), but the time in Saskatchewan is Central Standard Time (90° W) year-round, so Saskatoon is always about 67 minutes ahead of mean solar time. Conversely, northeast India and a few other areas skew time zones eastward, in effect observing negative DST. The United Kingdom and Ireland experimented with year-round DST from 1968 to 1971 but abandoned it because of its unpopularity, particularly in northern regions.
Western France, Spain, and other areas skew time zones and shift clocks, in effect observing DST in winter with an extra hour in summer. For example, Nome, Alaska, is at 165°24′ W longitude, which is just west of center of the idealized Samoa Time Zone (165° W), but Nome observes Alaska Time (135° W) with DST, so it is slightly more than two hours ahead of the sun in winter and three in summer. Double daylight saving time has been used on occasion; for example, it was used in some European countries during and shortly after World War II when it was referred to as "Double Summer Time". See British Double Summer Time and Central European Midsummer Time for details.
External links.
Listen to this article ()
This audio file was created from a revision of the "Daylight saving time" article dated 2008-05-20, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="47552" url="http://en.wikipedia.org/wiki?curid=47552" title="Let's roll">
Let's roll

"Let's roll" is a colloquial catchphrase that has been used extensively as a command to move and start an activity, attack, mission or project.
Pre–September 11, 2001 usage.
The phrase may have its origins as early as 1908 in the cadence song now called "The Army Goes Rolling Along", which likely extended into tank usage. "The Roads Must Roll", a science fiction story written in 1940 by Robert A. Heinlein, mentions a re-worded version of "The Roll of the Caissons" called "Road Songs of the Transport Cadets". The protagonist of the 1937 supernatural comedy, "Topper", played by Cary Grant uses the phrase "Let's roll" to his wife, played by Constance Bennett, to indicate they should immediately exit their friend's stuffy office and find a drink. The pair are lighthearted, youthful, irresponsible and impossibly glamorous types, and the line delivery has a decisive insouciance about it. The protagonist of Ernest Hemingway's 1950 novel "Across the River and into the Trees", Colonel Dick Cantwell, based on World War II commander Charles "Buck" Lanham, uses the phrase to his driver. He knows he is facing imminent death, but tries to maintain decency, grace, and a sense of humor. The verb "roll" has been used in both the film and recording industry to signal the beginning of a film or audio recording. "Let's roll" was in common use on 1950s and 1960s police television series such as "Adam-12" and (the original) "Dragnet". It was used at the end of roll call at the beginning of each episode of 1980s TV series "Hill Street Blues". It has appeared, among other places, in "The Transformers" animated series by Optimus Prime before entering battle or embarking on a group journey. The exact phrase was used in Season 1, Episode 3 "More than Meets the Eye" (1984) in preparation for the final showdown with Megatron and the Decepticons (as well as in the 2009 feature film ""). It was used in the 1986 film "Ferris Bueller's Day Off", and the 1987 film "Matewan", where it was used by Baldwin–Felts agents just before a violent attack on striking coal miners. The term was used at the end of the film "Matilda" when the title character was given up for adoption. The toys use the phrase when setting out to rescue Woody in the 1999 animated children's film "Toy Story 2". In the late 1990s, the term "let's roll" was frequently used to initiate a departure from any given place. Hence, the term and true context of the term "let's roll" during this time period was to initiate action from an individual to a group of friends.
Flight 93.
Todd Beamer, a passenger on the hijacked United Airlines Flight 93, tried to place a credit card call through a phone located on the back of a plane seat but was routed to a customer-service representative instead, who passed him on to supervisor Lisa Jefferson. Beamer reported that one passenger was killed and, later, that a flight attendant had told him the pilot and co-pilot had been forced from the cockpit and may have been wounded. He was also on the phone when the plane made its turn in a southeasterly direction, a move that had him briefly panicking. Later, he told the operator that some of the plane's passengers were planning to attack the hijackers and take control of the aircraft. According to Jefferson, Beamer's last audible words were "Are you guys ready? Let's roll."
Cultural impact.
Music.
Several musicians and bands have written songs entitled or including the phrase "Let's Roll", with the songs typically referring to Flight 93 or Todd Beamer. The first song with the name, Neil Young's "Let's Roll", was released as a single in November 2001, and was later included in his album "Are You Passionate?". The following year, three diverse groups released songs: hard rock group L.A. Guns included "OK, Let's Roll" in their album "Waking the Dead", country music duo The Bellamy Brothers's song "Let's Roll, America" was on "Redneck Girls Forever", and Christian rock group dc Talk recorded and released a single entitled "Let's Roll" despite being on hiatus.
Three other 9/11-related songs by the name "Let's Roll" have been released in the following years. Montreal rock band The Stills's song was included on their debut album "Logic Will Break Your Heart" in 2003. Jonny L's song included a sample of President George W. Bush's 2002 State of the Union address which included the phrase. In 2004, Ray Stevens offered up the self-penned "Let's Roll" and referenced Todd Beamer in the lyrics. Stevens' recording appeared on his 2004 "Thank You!" CD. The recording later appeared on his 2005 "Box Set" project and on his 2010 "We the People" project.
Melissa Etheridge's 2004 song "Tuesday Morning", written in honor of Flight 93 passenger Mark Bingham, concluded with the phrase, "Let's roll".
Other.
The catchphrase became especially known and popular after being used by President George W. Bush in a speech to AmeriCorps volunteers and during his 2002 State of the Union address. Even though the phrase was in common use long before September 11, profiteers soon tried to lay claim to it as a trademark. The Todd M. Beamer Foundation was eventually granted a trademark for uses of the phrase relating to "pre-recorded compact discs, audio tapes, digital audio tapes, and phonograph records featuring music."
In the 2002 college football season, the Florida State Seminoles used "Let's Roll" as their official team slogan. After an initial uproar against the team by people who considered its usage in bad taste, the Todd M. Beamer Foundation officially licensed the trademark to the team.
Bobby Labonte drove a 9/11 tribute car with the words "Let's Roll" on the hood of his stock car.
In early 2002, United States Air Force Chief of Staff Gen. John P. Jumper ordered that one airplane in each USAF squadron and all USAF demonstration planes would bear an image of an eagle on an American flag with the words "Let's Roll" and "Spirit of 9-11," to remain until the first anniversary of the attack. It was also used by Lisa Beamer, widow of Todd, in a 2003 book titled "Let's Roll: Ordinary People, Extraordinary Courage".
The phrase was also used in an episode of HBO's "Curb Your Enthusiasm" ("The Survivor", season 4, episode 9). The show's main character, Larry David, says the phrase inadvertently to his rabbi once he and his wife are ready to go out and renew their vows, who then becomes offended because of a relative of his died on September 11, 2001 ("You knew my brother-in-law died on September 11th, how dare you say something like that?!"). Larry takes issue with this, as his rabbi's relative was hit by a bike messenger ("Well, with all due respect, wasn't that just a coincidence?"), in an incident completely unrelated to the 9/11 terrorist attacks.
"Let's Roll!" was the 2004 campaign slogan of the Marijuana Party of Canada. The New Zealand band Fly My Pretties released a song about marijuana titled "Let's Roll" the same year.
The phrase appears in the 2009 British black comedy satire film, "In The Loop". One British character says to another, "Let's roll." To which the other replies; "You can't say that here, they don't like that."
In 2011, American rapper Yelawolf released a song called "Let's Roll", which talks about Southern patriotism.
Detroit Red Wings goaltender Jimmy Howard has the inscription "Lets Roll" on the back of his goalie helmet in reference to Flight 93.

</doc>
<doc id="47556" url="http://en.wikipedia.org/wiki?curid=47556" title="Summertime">
Summertime

Summertime may refer to:

</doc>
<doc id="47563" url="http://en.wikipedia.org/wiki?curid=47563" title="Idiom">
Idiom

An idiom (Latin: "idioma", "special property", from , "special feature, special phrasing, a peculiarity", f. , "one’s own") is a phrase or a fixed expression that has a figurative, or sometimes literal, meaning. An idiom's figurative meaning is different from the literal meaning. There are thousands of idioms, and they occur frequently in all languages. It is estimated that there are at least twenty-five thousand idiomatic expressions in the English language.
Examples.
The following sentences contain idioms. The fixed words constituting the idiom in each case are bolded:
Each of the word combinations in bold has at least two meanings: a literal meaning and a figurative meaning. Such expressions that are typical for a language can appear as words, combinations of words, phrases, entire clauses, and entire sentences.
Expressions such as these have figurative meaning. When one says "The devil is in the details", one is not expressing a belief in demons, but rather one means that things may look good on the surface, but upon scrutiny, undesirable aspects are revealed. Similarly, when one says "The early bird gets the worm", one is not suggesting that there is only one worm, rather one means there are plenty of worms, but for the sake of the idiom one plays along, and imagines that there is only one worm. On the other hand, "Waste not, want not" is completely devoid of a figurative meaning. It counts as an idiom, however, because it has a literal meaning and people keep saying it.
Derivations.
Many idiomatic expressions, in their original use were not figurative but had literal meaning. 
For instance:
"spill the beans" meaning to let out a secret probably originates in a physical spilling of beans which are either being eaten or measured out. The point is that the spiller certainly does not want to lose any beans. Or, alternatively, it may be that a person wants to share a secret, and finally, perhaps after prodding, does so, and when that happens it would be like spilling beans into a bowl or other appropriate container.
"let the cat out of the bag" : has a meaning similar to the former, but the secret revealed in this case will likely cause some problems. A cat was sometimes put in a bag to keep it under control or to pretend that it was a more saleable animal, such as a pig or a rabbit. So, to let the cat out of the bag suggests either that the ruse is revealed or that the situation is out of control. Or alternatively, it may be that the way a cat, once set free, will wander off was a good description of a secret that is revealed. 
"break a leg": meaning good luck in a performance/presentation etc. This common idiom comes from belief in superstitions. The term 'break a leg' appears to come from the belief that one ought not to utter the words 'good luck' to an actor. By wishing someone bad luck, it is supposed that the opposite will occur. 
Compositionality.
In linguistics, idioms are usually presumed to be figures of speech contradicting the principle of compositionality. This principle states that the meaning of a whole should be constructed from the meanings of the parts that make up the whole. In other words, one should be in a position to understand the whole if one understands the meanings of each of the parts that make up the whole. The following example is widely employed to illustrate the point:
Understood compositionally, Fred has literally kicked an actual, physical bucket. The much more likely idiomatic reading, however, is non-compositional: Fred is understood to have died. Arriving at the idiomatic reading from the literal reading is unlikely for most speakers. What this means is that the idiomatic reading is, rather, stored as a single lexical item that is now largely independent of the literal reading.
In phraseology, idioms are defined as a sub-type of , the meaning of which is not the regular sum of the meanings of its component parts. John Saeed defines an idiom as collocated words that became affixed to each other until metamorphosing into a fossilised term. This collocation of words redefines each component word in the word-group and becomes an "idiomatic expression". Idioms usually do not translate well; in some cases, when an idiom is translated directly word-for-word into another language, either its meaning is changed or it is meaningless.
When two or three words are often used together in a particular sequence, the words are said to be irreversible binomials, or Siamese twins. Usage will prevent the words from being displaced or rearranged. For example, a person may be left "high and dry" but never "dry and high". This idiom in turn means that the person is left in their former condition rather than being assisted so that their condition improves. Not all Siamese twins are idioms, however. "Chips and dip" is an irreversible binomial, but it refers to literal food items, not idiomatic ones.
Translating idioms.
Literal translation (word-by-word) of opaque idioms will not convey the same meaning in other languages. Idioms from other languages that are analogous to "kick the bucket" in English are listed next:
Some idioms are transparent. Much of their meaning does get through if they are taken (or translated) literally. For example, "lay one's cards on the table" meaning to reveal previously unknown intentions, or to reveal a secret. Transparency is a matter of degree; "spill the beans" (to let secret information become known) and "leave no stone unturned" (to do everything possible in order to achieve or find something) are not entirely literally interpretable, but only involve a slight metaphorical broadening. Another category of idioms is a word having several meanings, sometimes simultaneously, sometimes discerned from the context of its usage. This is seen in the (mostly uninflected) English language in polysemes, the common use of the same word for an activity, for those engaged in it, for the product used, for the place or time of an activity, and sometimes for a verb.
Idioms tend to confuse those unfamiliar with them; students of a new language must learn its idiomatic expressions as vocabulary. Many natural language words have "idiomatic origins", but are assimilated, so losing their figurative senses, for example, in Portuguese, the expression "saber de coração" 'to know by heart', with the same meaning as in English, was shortened to 'saber de cor', and, later, to the verb "decorar", meaning "memorize".
Dealing with non-compositionality.
The non-compositionality of meaning of idioms challenges theories of syntax. The fixed words of many idioms do not qualify as constituents in any sense, e.g.
The fixed words of this idiom (in bold) do not form a constituent in any theory's analysis of syntactic structure because the object of the preposition (here "this situation") is not part of the idiom (but rather it is an argument of the idiom). One can know that it is not part of the idiom because it is variable, e.g. "How do we get to the bottom of this situation / the claim / the phenomenon / her statement /" etc. What this means is that theories of syntax that take the constituent to be the fundamental unit of syntactic analysis are challenged. The manner in which units of meaning are assigned to units of syntax remains unclear. This problem has motivated a tremendous amount of discussion and debate in linguistics circles and it is a primary motivator behind the Construction Grammar framework.
A relatively recent (1998) development in the syntactic analysis of idioms departs from a constituent-based account of syntactic structure, preferring instead the catena-based account. Any word or any combination of words that are linked together by dependencies qualifies as a catena. The words constituting idioms are stored as catenae in the lexicon, and as such, they are concrete units of syntax. The dependency grammar trees of a few sentences containing non-constituent idioms illustrate the point:
The fixed words of the idiom (in orange) in each case are linked together by dependencies; they form a catena. The material that is outside of the idiom (in normal black script) is not part of the idiom. The following two trees illustrate proverbs:
The fixed words of the proverbs (in orange) again form a catena each time. The adjective "nitty-gritty" and the adverb "always" are not part of the respective proverb and their appearance does not interrupt the fixed words of the proverb. A caveat concerning the catena-based analysis of idioms concerns their status in the lexicon. Idioms are lexical items, which means they are stored as catenae in the lexicon. In the actual syntax, however, some idioms can be broken up by various functional constructions.
The catena-based analysis of idioms provides a basis for an understanding of meaning compositionality. The Principle of Compositionality can in fact be maintained. Units of meaning are being assigned to catenae, whereby many of these catenae are not constituents.

</doc>
<doc id="47565" url="http://en.wikipedia.org/wiki?curid=47565" title="European chub">
European chub

The chub ("Squalius cephalus") is a European species of freshwater fish in the carp family Cyprinidae. It frequents both slow and moderate rivers, as well as canals and still waters of various kinds. In North America, this species is referred to as the European chub. Other names used for the species include round chub, fat chub, chevin, and pollard.
Distribution.
The species is distributed in most of the countries of Europe.
Fishing for chub.
European chub are popular with anglers due to their readiness to feed, and thus to be caught, in almost any conditions. Small chub are freely biting fish which even inexperienced anglers find easy to catch. As they become larger, however, chub become more wary and are easily spooked by noise or visual disturbance. Consequently, large chub (in excess of 2 kg) are keenly sought by anglers who prefer to target specific fish.
The British angling record for chub was broken in May 2007 when Steve White caught a 4.2-kg (9.2-lb) fish from a southern stillwater on a mainline boilie. The chub can reach a maximum length of 60-80 cm (24-31.5 in).
Tackle.
Small chub can be caught readily on light tackle: fly-fishing gear, a lure rod or a float rod, for example. Lines and hooks can be small, but baits are often of a relatively large size due to the chub's "greedy" nature. Larger chub, especially in floodwater conditions, must be fished with more substantial tackle: a stiff to moderately stiff rod, a strong line, strong hooks, and a large bait. Such enhanced equipment is needed due to the chub's predilection for taking cover in underwater snags. They frequently conceal themselves in deep holes or under the roots of trees, etc., and venture out to feed before returning quickly to cover.
Chub will readily take any natural bait. In addition to natural baits, however, they are renowned for their voracious appetites and will often be caught on baits as diverse as cheese, sweetcorn, bread, earthworms, and wasp larvae.
Tactics.
As with many river fish, the best conditions for chub fishing are when the water is "carrying colour", when the clarity of the water has been temporary clouded by mud washing into the river, often following heavy rainfall. Under these conditions, a big, smelly bait is the best bet. Lob worms are a particular favourite, as is breadflake or paste. Another smelly favourite for chub are cheese flavours and a flavour called "scopex" which can be sprayed onto bait. When rivers are clear or swims are heavily fished, big baits are unlikely to work, so the best baits are single or double maggot close to, or under, the far bank features like overhanging trees. In certain waters, chub can become predators and can be caught on spinners or spoons. Chubs have sharp, bone-crushing pharyngeal teeth at the back of their mouths, so care should be used when removing hooks.

</doc>
<doc id="47568" url="http://en.wikipedia.org/wiki?curid=47568" title="Low Earth orbit">
Low Earth orbit

A low Earth orbit (LEO) is an orbit around Earth with an altitude between 160 km (orbital period of about 88 minutes), and 2000 km (about 127 minutes). Objects below approximately 160 km will experience very rapid orbital decay and altitude loss. With the exception of the manned lunar flights of the Apollo program, all human spaceflights have taken place in LEO (or were suborbital). The altitude record for a human spaceflight in LEO was Gemini 11 with an apogee of 1374.1 km. All manned space stations to date, as well as the majority of satellites, have been in LEO.
Orbital characteristics.
Objects in LEO encounter atmospheric drag in the form of gases in the thermosphere (approximately 80–500 km up) or exosphere (approximately 500 km and up), depending on orbit height. Objects in LEO orbit Earth between the atmosphere and below the inner Van Allen radiation belt. The altitude is usually not less than 300 km for satellites, as that would be impractical due to atmospheric drag.
The orbital velocity needed to maintain a stable low Earth orbit is about 7.8 km/s, but reduces with increased orbital altitude. The delta-v needed to achieve low Earth orbit starts around 9.4 km/s. Atmospheric and gravity drag associated with launch typically adds 1.5–2.0 km/s to the delta-v launch vehicle required to reach normal LEO orbital velocity of around 7.8 km/s.
Equatorial low Earth orbits (ELEO) are a subset of LEO. These orbits, with low inclination to the Equator, allow rapid revisit times and have the lowest delta-v requirement (i.e., fuel spend) of any orbit. Orbits with a high inclination angle to the equator are usually called polar orbits.
Higher orbits include medium Earth orbit (MEO), sometimes called intermediate circular orbit (ICO), and further above, geostationary orbit (GEO). Orbits higher than low orbit can lead to early failure of electronic components due to intense radiation and charge accumulation.
Use of LEO.
Although the Earth's pull due to gravity in LEO is not much less than on the surface of the Earth, people and objects in orbit experience weightlessness because they are in free fall.
A low Earth orbit is simplest and cheapest for satellite placement. It provides high bandwidth and low communication time lag (latency), but satellites in LEO will not be visible from any given point on the Earth at all times.
Space debris.
The LEO environment is becoming congested with space debris due to the frequency of object launches. This has caused growing concern in recent years, since collisions at orbital velocities can easily be dangerous, and even deadly. Collisions can produce even more space debris in the process, creating a domino effect, something known as Kessler Syndrome. The Joint Space Operations Center, part of United States Strategic Command (formerly the United States Space Command), currently tracks more than 8,500 objects larger than 10 cm in LEO. However, a limited Arecibo Observatory study suggested there could be approximately one million objects larger than 2 millimeters, which are too small to be visible from Earth-based observatories.
References.
 This article incorporates  from websites or documents of the .

</doc>
<doc id="47569" url="http://en.wikipedia.org/wiki?curid=47569" title="Appian Way">
Appian Way

The Appian Way (Latin and Italian: Via Appia) was one of the earliest and strategically most important Roman roads of the ancient republic. It connected Rome to Brindisi, Apulia, in southeast Italy. Its importance is indicated by its common name, recorded by Statius:
The road is named after Appius Claudius Caecus, the Roman censor who began and completed the first section as a military road to the south in 312 BC during the Samnite Wars.
Origins.
The need for roads.
The Roman army depended for its success on the use of bases in which to prepare for retreat and to refresh and re-equip afterwards. Bases allowed the Romans to keep a large number of soldiers in the field waiting for the opportunity to strike. However, the bases needed to be connected by good roads for easy access and supply from Rome. The Appian Way was used as a main route for military supplies since its construction for that purpose in 312 B.C.
The Appian Way was the first long road built specifically to transport troops outside the smaller region of greater Rome (this was essential to the Romans). The few roads outside the early city were Etruscan and went mainly to Etruria. By the late Republic, the Romans had expanded over most of Italy and were masters of road construction. Their roads began at Rome, where the master "itinerarium", or list of destinations along the roads, was located, and extended to the borders of their domain — hence the expression, "All roads lead to Rome".
The Samnite Wars.
Rome had an affinity for the people of Campania, who, like themselves, traced their backgrounds to the Etruscans. The Samnite Wars were instigated by the Samnites when Rome attempted to ally itself with the city of Capua in Campania. The Italic speakers in Latium had long ago been subdued and incorporated into the Roman state. They were responsible for changing Rome from a primarily Etruscan to a primarily Italic state.
Dense populations of sovereign Samnites remained in the mountains north of Capua, which is just north of the Greek city of Neapolis. Around 343 BC, Rome and Capua attempted to form an alliance, a first step toward a closer unity. The Samnites reacted with military force.
The barrier of the Pontine Marshes.
Between Capua and Rome lay the Pontine Marshes (Pomptinae paludes), a swamp infested with malaria. A tortuous coastal road wound between Ostia at the mouth of the Tiber and Neapolis. The via Latina followed its ancient and scarcely more accessible path along the foothills of Monti Laziali and Monti Lepini, which are visible towering over the former marsh.
In the First Samnite War (343–341 BC) the Romans found they could not support or resupply troops in the field against the Samnites across the marsh. A revolt of the Latin League drained their resources further. They gave up the attempted alliance and settled with Samnium.
Colonization to the southeast.
The Romans were only biding their time while they looked for a solution. The first answer was the colonia, a "cultivation" of settlers from Rome, who would maintain a permanent base of operations. The Second Samnite War (327–304 BC) erupted when Rome attempted to place a colony at Cales in 334 and again at Fregellae in 328 on the other side of the marshes. The Samnites, now a major power after defeating the Greeks of Tarentum, occupied Neapolis to try to ensure its loyalty. The Neapolitans appealed to Rome, which sent an army and expelled the Samnites from Neapolis.
Colonies alone apparently were not the answer. In 321 BC, a Roman army was trapped in the mountain passes north of Capua, at Caudium. At the Battle of the Caudine Forks they were kept penned in without supplies, especially water, until the Senate bought their release in exchange for a half-year treaty the Romans considered humiliating, by which they provided hostages and gave up the colonies. Rome used the time to defeat the Italic tribes around Samnium. In 316, at the end of the treaty, Samnium again opened hostilities against Rome, defeating her at the Battle of Lautulae in 315. By 312  the situation was bleak for Rome and became bleaker when, in 311, the Etruscans in Etruria and Campania defected to the Samnites.
Appius Claudius' beginning of the works.
In 312 BC, Appius Claudius Caecus became censor at Rome. He was of the gens Claudia, who were patricians descended from the Sabines taken into the early Roman state. He had been given the name of the founding ancestor of the gens. He was a populist, i.e., an advocate of the common people. A man of inner perspicacity, in the years of success he was said to have lost his outer vision and thus acquired the name "caecus", "blind".
Without waiting to be told what to do by the Senate, Appius Claudius began bold public works to address the supply problem. An aqueduct (the Aqua Appia) secured the water supply of the city of Rome. By far the best known project was the road, which ran across the Pontine Marshes to the coast northwest of Naples, where it turned north to Capua. On it, any number of fresh troops could be sped to the theatre of operations, and supplies could be moved en masse to Roman bases without hindrance by either enemy or terrain. It is no surprise that, after his term as censor, Appius Claudius became consul twice, subsequently held other offices, and was a respected consultant to the state even during his later years.
The success of the road.
The road achieved its purpose. The outcome of the Second Samnite War was at last favorable to Rome. In a series of blows the Romans reversed their fortunes, bringing Etruria to the table in 311 BC, the very year of their revolt, and Samnium in 304. The road was the main factor that allowed them to concentrate their forces with sufficient rapidity and to keep them adequately supplied, wherein they became a formidable opponent.
Construction of the road.
The main part of the Appian Way was started and finished in 312 BC.
The road began as a leveled dirt road upon which small stones and mortar were laid. Gravel was laid upon this, which was finally topped with tight fitting, interlocking stones to provide a flat surface. The historian Procopius said that the stones fit together so securely and closely that they appeared to have grown together rather than to have been fitted together. The road was cambered in the middle (for water runoff) and had ditches on either side of the road which were protected by retaining walls.
Between Rome and Lake Albano.
The road began in the Forum Romanum, passed through the Servian Wall at the porta Capena, went through a cutting in the clivus Martis, and left the city. For this stretch of the road, the builders used the via Latina. The building of the Aurelian Wall centuries later required the placing of another gate, the Porta Appia. Outside of Rome the new via Appia went through well-to-do suburbs along the via Norba, the ancient track to the Alban hills, where Norba was situated. The road at the time was a via glarea, a gravel road. The Romans built a high-quality road, with layers of cemented stone over a layer of small stones, cambered, drainage ditches on either side, low retaining walls on sunken portions, and dirt pathways for sidewalks. The via Appia is believed to have been the first Roman road to feature the use of lime cement. The materials were volcanic rock. The surface was said to have been so smooth that you could not distinguish the joints. The Roman section still exists and is lined with monuments of all periods, although the cement has eroded out of the joints, leaving a very rough surface.
Across the marsh.
The road concedes nothing to the Alban hills, but goes straight through them over cuts and fills. The gradients are steep. Then it enters the former Pontine Marshes. A stone causeway of about 19 mi led across stagnant and foul-smelling pools blocked from the sea by sand dunes. Appius Claudius planned to drain the marsh, taking up earlier attempts, but he failed. The causeway and its bridges subsequently needed constant repair. No one enjoyed crossing the marsh. In 162 BC, Marcus Cornelius Cathegus had a canal constructed along the road to relieve the traffic and provide an alternative when the road was being repaired. Romans preferred using the canal.
Along the coast.
The via Appia picked up the coastal road at Tarracina. However, the Romans straightened it somewhat with cuttings, which form cliffs today. From there the road swerved north to Capua, where, for the time being, it ended. Caudine Forks was not far to the north. The itinerary was Aricia (Ariccia), Tres Tabernae, Forum Appii, Tarracina (Terracina), Fundi (Fondi), Formiae (Formia), Minturnae (Minturno), Sinuessa (Mondragone), Casilinum and Capua, but some of these were colonies added after the Samnite Wars. The distance was 132 mi. The original road had no milestones, as they were not yet in use. A few survive from later times, including a first milestone near the porta Appia.
Extension to Beneventum.
The Third Samnite War (298–290 BC) is perhaps misnamed. It was an all-out attempt by all the neighbors of Rome: Italics, Etruscans and Gauls, to check the power of Rome. The Samnites were the leading people of the conspiracy. Rome dealt the northerners a crushing blow at the Battle of Sentinum in Umbria in 295. The Samnites fought on alone. Rome now placed 13 colonies in Campania and Samnium. It must have been during this time that they extended the via Appia 35 miles beyond Capua past the Caudine forks to a place the Samnites called Maloenton, "passage of the flocks". The itinerary added Calatia, Caudium and Beneventum (not yet called that). Here also ended the via Latina.
Extension to Apulia and Calabria.
By 290 BC, all was over for the sovereignty of the Samnites. The heel of Italy lay open to the Romans. The dates are somewhat uncertain and there is considerable variation in the sources, but during the Third Samnite War the Romans seem to have extended the road to Venusia, where they placed a colony of 20,000 men. After that they were at Tarentum.
Roman expansion alarmed Tarentum, the leading city of the Greek presence (Magna Graecia) in southern Italy. They hired the mercenary, King Pyrrhus of Epirus, in neighboring Greece to fight the Romans on their behalf. In 280 BC the Romans suffered a defeat at the hands of Pyrrhus at the Battle of Heraclea on the coast west of Tarentum. The battle was costly for both sides, prompting Pyrrhus to remark "One more such victory and I am lost." Making the best of it, the Roman army turned on Greek Rhegium and effected a massacre of Pyrrhian partisans there.
Rather than pursue them, Pyrrhus went straight for Rome along the via Appia and then the via Latina. He knew that if he continued on the via Appia he could be trapped in the marsh. Wary of such entrapment on the via Latina also, he withdrew without fighting after encountering opposition at Anagni. Wintering in Campania, he withdrew to Apulia in 279 BC, where, pursued by the Romans, he won a second costly victory at the Battle of Asculum. Withdrawing from Apulia for a Sicilian interlude, he returned to Apulia in 275 BC and started for Campania up the nice Roman road.
Supplied by that same road, the Romans successfully defended the region against Pyrrhus, crushing his army in a two-day fight at the Battle of Beneventum in 275 BC. The Romans renamed the town from "Maleventum" ("site of bad events") to Beneventum ("site of good events") as a result. Pyrrhus withdrew to Greece, where he died in a street fight in Argos in 272 BC. Tarentum fell to the Romans that same year, who proceeded to consolidate their rule over all of Italy.
The Romans pushed the via Appia to the port of Brundisium in 264 BC. The itinerary from Beneventum was now Venusia, Silvium, Tarentum, Uria and Brundisium. The Roman Republic was the government of Italy, for the time being. Appius Claudius died in 273, but in extending the road a number of times, no one has tried to displace his name upon it.
Extension by Trajan.
The emperor Trajan built the Via Traiana, an extension of the Via Appia from Beneventum, reaching Brundisium via Canusium and Barium rather than via Tarentum. This was commemorated by an arch at Beneventum.
Notable historical events along the road.
The crucifixion of Spartacus' army.
In 73 BC, a slave revolt (known as the Third Servile War) under the ex-gladiator of Capua, Spartacus, began against the Romans. Slavery accounted for roughly every third person in Italy.
Spartacus defeated many Roman armies in a conflict that lasted for over two years. While trying to escape from Italy at Brundisium he unwittingly moved his forces into the historic trap in Apulia/Calabria. The Romans were well acquainted with the region. Legions were brought home from abroad and Spartacus was pinned between armies.
On his defeat the Romans judged that the slaves had forfeited their right to live. In 71 BC, 6,000 slaves were crucified along the 200 km Via Appia from Rome to Capua.
The World War II battle of Anzio.
In 1943, during World War II, the Allies fell into the same trap Pyrrhus had retreated to avoid, in the Pomptine fields, the successor to the Pomptine marshes. The marsh remained, despite many efforts to drain it, until engineers working for Benito Mussolini finally succeeded. (Even so, the fields were infested with malarial mosquitos until the advent of DDT in 1950s.)
Hoping to break a stalemate at Monte Cassino, the Allies landed on the coast of Italy at Nettuno, ancient Antium, which was midway between Ostia and Terracina. They found that the place was undefended. They intended to move along the line of the via Appia to take Rome, outflanking Monte Cassino, but they did not do so quickly enough. The Germans occupied Mounts Laziali and Lepini along the track of the old Via Latina, from which they rained down shells on Anzio. Even though the Allies expanded into all the Pomptine region, they gained no ground. The Germans counterattacked down the via Appia from the Alban hills in a front four miles wide, but could not retake Anzio. The battle lasted for four months, one side being supplied by sea, the other by land through Rome. In May 1944, the Allies broke out of Anzio and took Rome. The German forces escaped to the north of Florence.
1960 Summer Olympics.
For the 1960 Summer Olympics, it served as part of the men's marathon course that was won by Abebe Bikila of Ethiopia.
Main sights.
Via Appia antica.
After the fall of the Western Roman Empire, the road fell out of use; Pope Pius VI ordered its restoration. A new Appian Way was built in parallel with the old one in 1784 as far as the Alban Hills region. The new road is the "Via Appia Nuova" ("New Appian Way") as opposed to the old section, now known as "Via Appia Antica". The old Appian Way close to Rome is now a free tourist attraction. It was extensively restored for Rome's Millennium and Great Jubilee celebrations. The first 3 mi are still heavily used by cars, buses and coaches but from then on traffic is very light and the ruins can be explored on foot in relative safety. The Church of Domine Quo Vadis is in the second mile of the road. Along or close to the part of the road closest to Rome, there are three catacombs of Roman and early Christian origin.
The construction of Rome's ring road, the Grande Raccordo Anulare or GRA, in 1951 caused the Appian Way to be cut in two. More recent improvements to the GRA have rectified this through the construction of a tunnel under the Appia, so that it is now possible to follow the Appia on foot for about 10 mi from its beginning near the Baths of Caracalla.
Many parts of the original road beyond Rome's environs have been preserved, and some are now used by cars (for example, in the area of Velletri). The road inspires the last movement of Ottorino Respighi's "Pini di Roma". To this day the Via Appia contains the longest stretch of straight road in Europe, totaling 62 km.
Monuments along the Via Appia.
Roman bridges along the road.
There are the remains of several Roman bridges along the road, including the Ponte di Tre Ponti, Ponte di Vigna Capoccio, Viadotta di Valle Ariccia, Ponte Alto and Ponte Antico.

</doc>
<doc id="47571" url="http://en.wikipedia.org/wiki?curid=47571" title="American pickerel">
American pickerel

The American pickerels are two subspecies of "Esox americanus", a species of freshwater fish in the pike family (family Esocidae) of order Esociformes: the redfin pickerel, "E. americanus americanus" Gmelin, 1789, and the grass pickerel, "E. americanus vermiculatus" Lesueur, 1846.
Both subspecies are native to North America. They are not to be confused with their aggressive counterpart the Northern pike. The redfin pickerel's range extends from the Saint Lawrence drainage in Quebec down to the Gulf Coast, from Mississippi to Florida, while the grass pickerel's range is further west, extending from the Great Lakes Basin, from Ontario to Michigan, down to the western Gulf Coast, from eastern Texas to Mississippi.
The two subspecies are very similar, but the grass pickerel lacks the redfin's distinctive orange to red fin coloration, its fins having dark leading edges and amber to dusky coloration. In addition, the light areas between the dark bands are generally wider on the grass pickerel and narrower on the redfin pickerel. These pickerels grow to a maximum overall length of 40 cm (16 in) and a maximum weight of 2.25 pounds
The redfin and grass pickerels occur primarily in sluggish, vegetated waters of pools, lakes, and swamps, and are carnivorous, feeding on smaller fish. Larger fishes, such as the striped bass ("Morone saxatilis"), bowfin ("Amia calva"), and gray weakfish ("Cynoscion regalis"), in turn, prey on the pickerels when they venture into larger rivers or estuaries.
These fishes reproduce by scattering spherical, sticky eggs in shallow, heavily vegetated waters. The eggs hatch in 11–15 days; the adults guard neither the eggs nor the young.
The "E. americanus" subspecies are not as highly prized as a game fish as their larger cousins, the northern pike and muskellunge, but they are caught by anglers. "McClane's Standard Fishing Encyclopedia" describes ultralight tackle as a sporty if overlooked method to catch these small but voracious pikes.
Lesueur originally classified the grass pickerel as "E. vermiculatus," but it is now considered a subspecies of "E. americanus."
"E. americanus americanus" is sometimes called the brook pickerel. There is no widely accepted English common collective name for the two "E. americanus" subspecies; "American pickerel" is a translation of the systematic name and the French "brochet d'Amérique."

</doc>
<doc id="47572" url="http://en.wikipedia.org/wiki?curid=47572" title="Redfish">
Redfish

Redfish is a common name for several species of fish. It is most commonly applied to members of the deep-sea genus "Sebastes", or the reef dwelling snappers, "Lutjanus". It is also applied to the slimeheads or roughies (family Trachichthyidae), and the alfonsinos (Berycidae).

</doc>
<doc id="47574" url="http://en.wikipedia.org/wiki?curid=47574" title="Porgy (novel)">
Porgy (novel)

Porgy is a novel written by the American author DuBose Heyward and published by the George H. Doran Company in 1925.
The novel tells the story of Porgy, a crippled street-beggar in the black tenements of Charleston, South Carolina, in the 1920s. The character was based on the real-life Charlestonian Samuel Smalls. Some passages in the novel have the characters speaking in the Gullah language.
The novel was adapted for a 1927 play by Heyward and his wife, playwright Dorothy Heyward. Even before the play had been fully written, Heyward was in discussions with George Gershwin for an operatic version of his novel, which appeared in 1935 as "Porgy and Bess" (renamed to distinguish it from the play).

</doc>
<doc id="47575" url="http://en.wikipedia.org/wiki?curid=47575" title="Weakfish">
Weakfish

The weakfish, "Cynoscion regalis", is a marine fish of the drum family Sciaenidae.
A large, slender, marine fish, it is found along the east coast of North America. The head and back of this fish are dark brown in color with a greenish tinge. The sides have a faint silvery hue with dusky specks, and the belly is white. The origin of its name is based on the weakness of the mouth muscles, which often cause a hook to tear free, allowing the fish to escape. The weakfish grows to 1 m (3 feet) in length and 9 kg (20 pounds) in weight. It is found along the eastern coast of North America from Nova Scotia, Canada to northern Florida, where it is fished both commercially and recreationally. Weakfish are also known by the American Indian name "Squeteague". 
In the mid-Atlantic states, the fish is sometimes referred to by the name sea trout, though it is not related to the fishes properly called trout, which are in the family Salmonidae.
The weakfish is the state fish of Delaware.
Management.
Weakfish stocks have been low in recent years due to fishing and natural mortality increasing. Management of the species includes gear regulations, seasonal fishing, bycatch limitations, minimum size limits, and bycatch reduction gear. It is hoped that these regulations incorporated with others will help weakfish populations come back to a sustainable point.

</doc>
<doc id="47576" url="http://en.wikipedia.org/wiki?curid=47576" title="Striped bass">
Striped bass

The striped bass ("Morone saxatilis"), also called Atlantic striped bass, striper, linesider, pimpfish, rock, or rockfish, is the state fish of Maryland, Rhode Island, South Carolina, and the state saltwater (marine) fish of New York, New Jersey, Virginia, and New Hampshire. They are also found in the Minas Basin, Gaspereau River, and Northumberland Strait in Nova Scotia, Canada and the Miramichi River and Saint John River in New Brunswick, Canada. The history of the striped bass fishery in North America dates all the way back to the Colonial period. There are many written accounts by some of the first European settlers describing the immense abundance of striped bass along with alewives traveling and spawning up most rivers in the coastal Northeast.
Morphology and lifespan.
The striped bass is a typical member of the Moronidae family in shape, having a streamlined, silvery body marked with longitudinal dark stripes running from behind the gills to the base of the tail. The maximum scientifically recorded weight is 57 kg (125 lb). Common mature size is 120 cm (3.9 ft). Striped bass are believed to live for up to 30 years. The maximum length is 1.8 m (6 ft). The average size is about 67–100 cm (2.2-3.3 ft) and 4.5-14.5 kg (10-32 lb).
Distribution.
Natural distribution.
Striped bass are native to the Atlantic coastline of North America from the St. Lawrence River into the Gulf of Mexico to approximately Louisiana. They are anadromous fish that migrate between fresh and salt water. Spawning takes place in fresh water.
Introductions outside their natural range.
Striped bass have been introduced to the Pacific Coast of North America and into many of the large reservoir impoundments across the United States by state game and fish commissions for the purposes of recreational fishing and as a predator to control populations of gizzard shad. These include: Elephant Butte Lake in New Mexico; Lake Ouachita, Lake Norman in North Carolina, Lake Norfork, Beaver Lake and Lake Hamilton in Arkansas; Lake Powell, Putnam Illinois (Lake Thunderbird) Lake Pleasant, and Lake Havasu in Arizona; Castaic Lake and Lake George in Florida, Pyramid Lake, Silverwood Lake, Diamond Valley Lake, Lewis Smith Lake in Alabama , Lake Cumberland in Kentucky
, and Lake Murray in South Carolina; Lake Lanier in Georgia; Watts Bar Lake, Tennessee; and Lake Mead, Nevada; Lake Texoma, Lake Tawakoni, Lake Whitney, Possum Kingdom Lake, and Lake Buchanan in Texas; Raystown Lake in Pennsylvania; and in Virginia's Smith Mountain Lake and Leesville Lake.
Striped bass have also been introduced into waters in Ecuador, Iran, Latvia, Mexico, Russia, South Africa, and Turkey, primarily for sport fishing and aquaculture.
Environmental factors.
The spawning success of striped bass has been studied in the San Francisco Bay-Delta water system, with a finding that high total dissolved solids (TDS) reduce spawning. At levels as low as 200 mg/l TDS, an observable diminution of spawning productivity occurs. They can be found in lakes, ponds, streams, and wetlands.
Even though the population of striped bass was growing and repopulating in the late 1980s and throughout the 1990s, a study executed by the Wildlife & Fisheries Program of the West Virginia University found that the rapid growth of the Striped bass population was exerting a tremendous pressure on its prey(river herring, shad, and blueback herring). This pressure on their food source was putting their own population at risk due to the population of prey naturally not coming back to the same spawning areas.
In the United States, the striped bass was designated as a protected game fish in 2007, and executive agencies were directed to use existing legal authorities to prohibit the sale of striped bass caught in federal waters in the Atlantic Ocean and Gulf of Mexico.
In Canada, the province of Quebec designated the striped bass population of the Saint Lawrence as extirpated in 1996. Analysis of available data implicated overfishing and dredging in the disappearance. In 2002, a successful reintroduction program was introduced.
Life cycle.
Striped bass spawn in fresh water, and although they have been successfully adapted to freshwater habitat, they naturally spend their adult lives in saltwater (i.e., it is anadromous). Four important bodies of water with breeding stocks of striped bass are: Chesapeake Bay, Massachusetts Bay/Cape Cod, Hudson River and Delaware River. It is believed that many of the rivers and tributaries that emptied into the Atlantic, had at one time, breeding stock of striped bass. This was occurred up until the 1860s. One of the largest breeding areas is the Chesapeake Bay, where populations from Chesapeake and Delaware bays have intermingled. The very few successful spawning populations of freshwater striped bass include Lake Texoma, Lake Weiss (Coosa River), the Colorado River and its reservoirs downstream from and including Lake Powell, and the Arkansas River, as well as Lake Marion (South Carolina) that retained a landlocked breeding population when the dam was built; other freshwater fisheries must be restocked with hatchery-produced fish annually. Stocking of striped bass was discontinued at Lake Mead in 1973 once natural reproduction was verified.
Hybrids with other bass.
Striped bass have also been hybridized with white bass to produce hybrid striped bass also known as wiper, whiterock bass, sunshine bass, and Cherokee bass. These hybrids have been stocked in many freshwater areas across the US.
Fishing for striped bass.
Striped bass are of significant value for sport fishing, and have been introduced to many waterways outside their natural range. A variety of angling methods are used, including trolling and surf casting with topwater lures a good pick for surf casting, as well as bait casting with live and deceased bait. Striped bass will take a number of live and fresh baits, including bunker, clams, eels, sandworms, herring, bloodworms, mackerel, and shad, with the last being an excellent bait for freshwater fishing.
The largest striped bass ever taken by angling was an 81.88-lb (37.14-kg) specimen taken from a boat in Long Island Sound, near the Outer Southwest Reef, off the coast of Westbrook, Connecticut. The all-tackle world record fish was taken by Gregory Myerson on the night of August 4, 2011. The fish took a drifted live eel bait, and fought for 20 minutes before being boated by Myerson. A second hook and leader was discovered in the fish's mouth when it was boated, indicating it had been previously hooked by another angler. The fish measured 54 in length and had a girth of 36 in. The International Game Fish Association declared Myerson's catch the new all-tackle world record striped bass on October 19, 2011. In addition to now holding the All-Tackle record, Meyerson's catch also landed him the new IGFA men’s 37-kg (80-lb) line class record for striped bass, which previously stood at 70 lb. The previous all-tackle world record fish was a 78.5-lb (35.6-kg) specimen taken in Atlantic City, New Jersey on September 21, 1982 by Albert McReynolds, who fought the fish from the beach for 1:20 after it took his Rebel artificial lure. McReynolds' all-tackle world record stood for 29 years.
Recreational bag limits vary by state and province.
Landlocked striped bass.
Striped bass are an anadromous fish, so their spawning ritual of traveling up rivers to spawn led some of them to become landlocked during lake dam constructions. The first area where they became landlocked was documented to be in the Santee-Cooper River during the construction of the two dams that impounded Lakes Moultrie and Marion, and because of this, the state game fish of South Carolina is the striped bass.
Recently, biologists came to believe that striped bass stayed in rivers for long periods of time, with some not returning to sea unless temperature changes forced migration. Once fishermen and biologists caught on to rising striped bass populations, many state natural resources departments started stocking striped bass in local lakes. Striped bass still continue the natural spawn run in freshwater lakes, traveling up river and blocked at the next dam, which is why they are landlocked. Landlocked stripers have a hard time reproducing naturally, and one of the few and most successful rivers they have been documented reproducing successfully is the Coosa River in Alabama and Georgia.
A 70.6-lb (32.0-kg) landlocked bass was caught in February 2013 by James Bramlett on the Warrior River in Alabama, a current world record. This fish had a length of 44 inches (112 cm) and a girth of 37.75 inches (96 cm).
One of the only landlocked striped bass populations in Canada is located in the Grand Lake, Nova Scotia. They migrate out in early April into the Shubencadie River to spawn. These bass also spawn in the Stewiacke River (a tributary of the Shubencadie). The Shubencadie River system is one of five known spawning areas in Canada for striped bass, with the others being the St. Lawerence River, Miramichi River, Saint John River, Annapolis River and Shubencadie/Stewiacke Rivers.
Management.
The striped bass population declined to less than 5 million by 1982, but efforts by fishermen and management programs to rebuild the stock proved successful, and in 2007, there were nearly 56 million fish, including all ages. Recreational anglers and commercial fisherman caught an unprecedented 3.8 million fish in 2006. The management of the species includes size limits, commercial quotas, and biological reference points for the health of the species. The Atlantic States Marine Fisheries Commission states that striped bass are "not overfished and overfishing is not occurring." Another way to replenish and help repopulate the striped bass population is to reintroduce the species back to original spawning grounds in coastal rivers and estuaries in the Northeast.
As food.
Striped bass has white meat with a mild flavor and a medium texture. It is extremely versatile in that it can be cooked using numerous methods, including pan-searing, grilling, steaming, poaching, roasting, broiling, sautéing, deep frying (including batter-frying). The flesh can also be eaten raw or pickled.
The primary market forms for fresh bass include headed & gutted (with the head and organs removed) and filets; the primary market forms for frozen bass include headed & gutted and loins. It can also be found in steaks, chunks, or whole. Fresh striped bass is available year-round, and is typically sold in sizes from two to fifteen pounds, and can be sold up to fifty pounds.
Striped bass has firm and flavorful flesh with a large flake. The hybrid striped bass yields more meat, has a more fragile texture, and a blander flavor than wild striped bass. The fish has a mild and distinctive flavor. In recipes, it can be substituted for milder fish like cod as well as for stronger fish like bluefish. Other fish can substitute it, including weakfish, tilefish, blackfish, small bluefish, catfish, salmon, swordfish and shark. Striped bass is easily grilled in fillets, and is therefore popular in beach communities.

</doc>
<doc id="47578" url="http://en.wikipedia.org/wiki?curid=47578" title="Muskellunge">
Muskellunge

The muskellunge "(Esox masquinongy)", also known as muskelunge, muscallonge, milliganong, or maskinonge (and often abbreviated "muskie" or "musky"), is a species of large, relatively uncommon freshwater fish of North America. The muskellunge is the largest member of the pike family, Esocidae. The common name comes from the Ojibwa word "maashkinoozhe", meaning "ugly pike", by way of French "masque allongé" (modified from the Ojibwa word by folk etymology), "elongated face." The French common name is "masquinongé" or "maskinongé".
The muskellunge is known by a wide variety of trivial names including Ohio muskellunge, Great Lakes muskellunge, barred muskellunge, Ohio River pike, Allegheny River pike, jack pike, unspotted muskellunge and the Wisconsin muskellunge.
Description.
Muskellunge closely resemble other esocids such as the northern pike and American pickerel in both appearance and behavior. Like the northern pike and other aggressive pikes, the body plan is typical of ambush predators with an elongated body, flat head, and dorsal, pelvic and anal fins set far back on the body. Muskellunge are typically 28 - long and weigh 5 -, though some have reached up to 6 ft and almost 70 lb. A fish reported at 88 in (224 cm) and 110 lb (50 kg) reportedly caught around 1908 has been identified as a hoax or legend. A fish with a weight of 61.25 lb (27.8 kg) was caught in November 2000 in Georgian Bay, Ontario. The fish are a light silver, brown, or green, with dark vertical stripes on the flank, which may tend to break up into spots. In some cases, markings may be absent altogether, especially in fish from turbid waters. This is in contrast to northern pike, which have dark bodies with light markings. A reliable method to distinguish the two similar species is by counting the sensory pores on the underside of the mandible. A muskie will have seven or more per side, while the northern pike never has more than six. The lobes of the caudal (tail) fin in muskellunge come to a sharper point, while those of northern pike are more generally rounded. In addition, unlike pike, muskies have no scales on the lower half of their opercula.
Habitat.
Muskellunge are found in oligotrophic and mesotrophic lakes and large rivers from northern Michigan, northern Wisconsin, and northern Minnesota through the Great Lakes region, north into Canada, throughout most of the St Lawrence River drainage, and northward throughout the upper Mississippi valley, although the species also extends as far south as Chattanooga in the Tennessee River valley. Also, a small population is found in the Broad River in South Carolina. Several North Georgia reservoirs also have healthy stocked populations of muskie. They are also found in the Red River drainage of the Hudson Bay basin. Muskie were introduced to western Saint John River in the late 1960s and have now spread to many connecting waterways in northern Maine.
They prefer clear waters where they lurk along weed edges, rock outcrops, or other structures to rest. A fish forms two distinct home ranges in summer: a shallow range and a deeper one. The shallow range is generally much smaller than the deeper range due to shallow water heating up. A muskie continually patrols the ranges in search of available food in the appropriate conditions of water temperature.
Diet.
Most of their diets consist of fish, but can also include crayfish, frogs, ducklings, snakes, muskrats, mice, other small mammals, and small birds. The mouth is large with many long, needle-like teeth. Muskies will attempt to take their prey head-first, sometimes in a single gulp. They will take prey items up to 30% of their total length. In the spring, they tend to prefer smaller bait since their metabolism is slower, while large bait are preferred in fall as preparation for winter.
Length and weight.
As muskellunge grow longer they increase in weight, but the relationship between length and weight is not linear. The relationship between them can be expressed by a power-law equation:
The exponent b is close to 3.0 for all species, and c is a constant for each species. For muskellunge, b = 3.325, higher than for many common species, and c = 0.000089 pounds/inch.
This equation implies that a 30-in (76-cm) muskellunge will weigh about 8 lb (3.6 kg), while a 40-in muskellunge will weigh about 18 lb.
Behavior.
Muskellunge are sometimes gregarious, forming small schools in distinct territories. They spawn in mid to late spring, somewhat later than northern pike, over shallow, vegetated areas. A rock or sand bottom is preferred for spawning so the eggs do not sink into the mud and suffocate. The males arrive first and attempt to establish dominance over a territory. Spawning may last from five to 10 days and occurs mainly at night. The eggs are negatively buoyant and slightly adhesive; they adhere to plants and the bottom of the lake. Soon afterward, they are abandoned by the adults. Those embryos which are not eaten by fish, insects, or crayfish hatch within two weeks. The larvae live on yolk until the mouth is fully developed, when they begin to feed on copepods and other zooplankton. They soon begin to prey upon fish. Juveniles generally attain a length of 12 in by November of their first year.
Predators.
Adult muskellunge are apex predators where they occur naturally. Only humans pose a threat to an adult but juveniles are consumed by other muskies, northern pike, bass, and occasionally birds of prey. The musky's low reproductive rate and slow growth render populations highly vulnerable to overfishing. This has prompted some jurisdictions to institute artificial propagation programs in an attempt to maintain otherwise unsustainably high rates of angling effort and habitat destruction.
Angling.
Anglers seek large muskies as trophies or for sport. The fish attain impressive swimming speeds, but are not particularly maneuverable. The highest-speed runs are usually fairly short, but they can be quite intense. The muskie can also do headshaking in an attempt to rid itself of a hook. Muskies are known for their strength and for their tendency to leap from the water in stunning acrobatic displays. A challenging fish to catch, the muskie has been called "the fish of ten thousand casts". Anglers tend to use smaller lures in spring or during cold-front conditions and larger lures in fall or the heat of summer. The average lure is 7.9 - long, but longer lures of 14 - are not uncommon. Many times, live bait is used in the form of "muskie minnows" or 8- to 12-in-long fish strung on treble hooks. Anglers in many areas are strongly encouraged to practice catch and release when fishing for muskellunge due to their low population. In places where muskie are not native, such as in Maine, anglers are encouraged not to release the fish back into the water because of their negative impact on the populations of trout and other smaller fish species. One strategy for securing the fish is called the figure eight.
Subspecies and hybrids.
Though interbreeding with other pike species can complicate the classification of some individuals, zoologists usually recognize up to three subspecies of muskellunge.
The tiger muskellunge ("E. masquinongy" × "lucius" or "E. lucius" × "masquinongy") is a hybrid of the musky and northern pike. Hybrids are sterile, although females sometimes unsuccessfully engage in spawning motions. Some hybrids are artificially produced and planted for anglers to catch. Tiger muskies grow faster than pure muskies, but do not attain the ultimate size of their pure relatives, as the tiger muskie does not live as long. The body is often quite silvery and largely or entirely without spots, but with indistinct longitudinal bands.

</doc>
<doc id="47579" url="http://en.wikipedia.org/wiki?curid=47579" title="Pollock">
Pollock

Pollock (alternatively spelled pollack; pronounced ) is the common name used for either of the two species of North Atlantic marine fish in the Pollachius ("P.") genus. Both "P. pollachius" and "P. virens" are commonly referred to as pollock. Other names for "P. pollachius" include the Atlantic pollock, European pollock, lieu jaune, and lythe; while "P. virens" is sometimes known as Boston blues (distinct from bluefish), coalfish (or coley), silver bills or saithe.
Species.
There are currently two recognized species in this genus:
Description.
Both species can grow to 3 ft 6 in (1.07 m) and can weigh up to 46 lb (21 kg). "P.virens" has a strongly-defined, silvery lateral line running down the sides. Above the lateral line, the color is a greenish black. The belly is white, while "P.pollachius" has a distinctly crocked lateral line, gray-ish to golden belly and a dark brown back. "P. pollachius" also has a strong under-bite. It can be found in water up to 100 fathoms (180 m) deep over rocks, and anywhere in the water column. Pollock are a "whitefish".
Other fish called pollock.
One member of the genus "Gadus" is also commonly referred to as pollock. This is the Alaska pollock or walleye pollock ("Theragra chalcogramma") including the form known as the Norwegian pollock. While related (they are also members of the family "Gadidae") to the above pollock species, they are not members of the "Pollachius" genus. Alaska pollock generally spawn in late winter and early spring in the southeastern Bering Sea. The Alaska pollock is a significant part of the commercial fishery in the Gulf of Alaska.
Parasites.
Pollock and other species of gadids are plagued by parasites, one of which is the cod worm, "Lernaeocera branchialis", a copepod crustacean. At its final stage, the female parasite, with fertilized eggs, clings to the gills of the fish and metamorphoses into a plump, sinusoidal, wormlike body, with a coiled mass of egg strings at the rear.
As food.
Atlantic pollock is largely considered to be a whitefish, although it is a fairly strongly flavored one. Traditionally a popular source of food in some countries, such as Norway, in the United Kingdom it has previously been largely consumed as a cheaper and versatile alternative to cod and haddock. However, in recent years pollock has become more popular due to over-fishing of cod and haddock. It can now be found in most supermarkets as fresh fillets or prepared freezer items. For example it is used minced in fish fingers or as an ingredient in imitation crab meat.
Because of its slightly gray color, pollock is often prepared, as in Norway, as fried fish balls, or if juvenile sized, breaded with oatmeal and fried, as in Shetland. Year-old fish are traditionally split, salted and dried over a peat hearth in Orkney, where their texture becomes wooden. The fish can also be salted and smoked and achieve a salmon-like orange color (although it is not closely related to the salmon), as is the case in Germany where the fish is commonly sold as "Seelachs" or sea salmon. In Korea, pollock may be repeatedly frozen and melted to create "hwangtae", half-dried to create "ko-da-ri", or fully dried and eaten as "book-o".
In 2009, U.K. supermarket Sainsbury's renamed pollock 'Colin' in a bid to boost of the fish as an alternative to cod. Sainsbury's, which said the new name was derived from the French for cooked pollock (), launched the product under the banner "Colin and chips can save British cod."
In the U.S. and worldwide, Alaska pollock is the primary fish used by the McDonald's chain in their Filet-O-Fish sandwich.

</doc>
<doc id="47580" url="http://en.wikipedia.org/wiki?curid=47580" title="Kingfish">
Kingfish

Kingfish may refer to:

</doc>
<doc id="47581" url="http://en.wikipedia.org/wiki?curid=47581" title="Archosargus probatocephalus">
Archosargus probatocephalus

The sheepshead, Archosargus probatocephalus, is a marine fish that grows to 30 in (760 mm), but commonly reaches 10 to 20 in. It is deep and compressed in body shape, with five or six dark bars on the side of the body over a gray background. It has sharp dorsal spines. Its diet consists of oysters, clams, and other bivalves, and barnacles, fiddler crabs, and other crustaceans. It has a hard mouth, with several rows of stubby teeth, which help crush the shells of prey.
Fishing.
Although the Sheepshead Bay section of Brooklyn, in New York City, was named after the fish, it is almost entirely a southern species: its range extends from the mid-Atlantic to Texas. As sheepshead feed on bivalves and crustaceans, successful baits include shrimp, sand fleas (mole crabs), clams, fiddler crabs, and mussels. Sheepshead have a knack for stealing bait, so a small hook is necessary. Locating sheepshead with a boat is not difficult: Fishermen look for rocky bottoms or places with obstructions, jetties, and the pilings of bridges and piers. The average weight of a sheepshead is 3 to 4 lb, but some individuals reach the range of 10 to 15 lb.

</doc>
<doc id="47582" url="http://en.wikipedia.org/wiki?curid=47582" title="Blackfish">
Blackfish

Blackfish may refer to:

</doc>
<doc id="47586" url="http://en.wikipedia.org/wiki?curid=47586" title="Fort Montgomery (Hudson River)">
Fort Montgomery (Hudson River)

Fort Montgomery is the name of a fortification built in 1776 by the Continental Army on West Bank of the Hudson River during the American Revolution. It was one of the first major investments by the Americans in strategic construction projects. Declared a National Historic Landmark, it is owned and operated by the state of New York as the Fort Montgomery State Historic Site.
Fort Montgomery in the American Revolution.
Fort Montgomery was located at the confluence of Popolopen Creek with the Hudson River near Bear Mountain in Orange County, New York. The fortifications included a river battery of six 32-pound cannons, a cable chain supported by a boom across the Hudson River (see Hudson River Chain), and landward redoubts connected by ramparts, all situated on a cliff promontory rising 100 feet (30 m) above the river. The fort was commanded by General George Clinton, also the newly appointed governor of the state. Fort Montgomery and its companion fortification Fort Clinton, on the southern bank of the Popolopen, held a combined garrison of roughly 700 American soldiers. These men were from the 5th NY Regiment, Lamb's Artillery, Orange County Militia, and Ulster County Militia.
The strategic importance of the ability to control navigation along the Hudson River was obvious to both the Americans and the British from the outbreak of open hostilities. The Hudson was the major means for transportation of supplies and troops throughout a large portion of the northeast. The fort was constructed at a site noted as early as the seventeenth century for its strategic advantage in controlling navigation along the river.
A month after the first open armed conflict in Lexington, the Continental Congress resolved on May 25, 1775 to build fortifications in the Hudson highlands for the purpose of protecting and maintaining control of the Hudson River. It noted that "…a post be also taken in the Highlands on each side of Hudson’s River and batteries erected in such a manner as will most effectually prevent any vessels passing that may be sent to harass the inhabitants on the borders of said river…"
James Clinton and Christopher Tappan, both lifetime residents of the area, were sent to scout appropriate locations. The initial site chosen was further to the north at West Point, and construction of the fortifications to be named Fort Constitution began. However, difficulties in construction and management of the original plan of fortifications, together with escalating costs, led to abandonment of that project. The site on the north side of Popolopen Creek across from Anthony's Nose was proposed, and the materials and resources from Fort Constitution were redirected to the construction at the new site. Construction began on the new Fort Montgomery in March 1776.
The strategic importance of the opposite bank of Popolopen Creek was quickly realized, as it was an elevated cliff terrace with a full view of the Fort Montgomery site and could not be left undefended. The Army built the smaller fortification named Fort Clinton at that site. These two forts and their associated cannon batteries effectively controlled this stretch of the Hudson River. The Army also conceived a major engineering project to effectively blockade any naval traffic headed north on the river. In 1776 a chain and boom were built across the river to provide a physical barrier to ships, in addition to the combined firepower of the fortifications, which could be massed against ships.
In July 1776, the New York convention appointed a committee, including John Jay, Robert Livingston, George Clinton and Robert Yates, to "devise and carry into execution" measures for "obstructing the channel of Hudson's river, or annoying the navigation of the said River." Worried about lack of arms, it worked to buy more cannon.
Battle of Fort Montgomery.
On October 6, 1777, a combined force of roughly 2,100 Loyalists, Hessians, and British regulars led by Lieutenant General Sir Henry Clinton attacked forts Montgomery and Clinton from the landward side (where the defenses were only partially completed). They had support from cannon fire from British ships on the Hudson River that had passed through the chevaux de frise on the lower river. The land columns attacking from west of the fort consisted of the New York Volunteers, the Loyal American Regiment, Emmerich's Chasseurs, the 57th and the 52nd Regiments of Foot. By the end of the day, both forts had fallen to the British, who burned the forts and tore down the stonework buildings.
The battle was a pyrrhic victory for the British, however. Their campaign against the forts caused delays in reinforcing General John Burgoyne at Saratoga. Americans gained the upper hand at the Battle of Bemis Heights and forced the surrender of Burgoyne ten days later at the Battle of Saratoga, when the reinforcements were still far to the south.
Historic Site.
The site was declared a National Historic Landmark in 1972. A system of trails and interpretive signs guides visitors through the ruins of the fort.
Designed by the architect Salvatore Cuciti, the 5700 sqft Visitor Center opened in October 2006. The timber frame building is oriented to provide visitors with a "gun sight" view down the Hudson. Operated as a state museum, it contains artifacts from the site, mannequins representing military units and a detailed model of the fort.

</doc>
<doc id="47588" url="http://en.wikipedia.org/wiki?curid=47588" title="Neurosis">
Neurosis

Neurosis is a class of functional mental disorders involving distress but neither delusions nor hallucinations. Neurotic behavior is typically within socially acceptable limits. Neurosis may also be called psychoneurosis or neurotic disorder.
History and etymology.
The term "neurosis" was coined by the Scottish doctor William Cullen in 1769 to refer to "disorders of sense and motion" caused by a "general affection of the nervous system." Cullen used the term to describe various nervous disorders and symptoms that could not be explained physiologically. However, the meaning of the term was redefined by Carl Jung and Sigmund Freud over a century later. It has continued to be used in psychology and philosophy.
The "Diagnostic and Statistical Manual of Mental Disorders" (DSM) has eliminated the category "neurosis" because of a decision by its editors to provide descriptions of behavior rather than descriptions of hidden psychological mechanisms. This change has been controversial.
According to the "American Heritage Medical Dictionary", "neurosis" is "no longer used in psychiatric diagnosis."
The term is derived from the Greek word νεῦρον ("neuron", "nerve") and the suffix -ωσις "-osis" (diseased or abnormal condition).
Symptoms and causes.
There are many different neuroses: obsessive–compulsive disorder, obsessive–compulsive personality disorder, impulse control disorder, anxiety disorder, hysteria, and a great variety of phobias.
According to C. George Boeree, professor emeritus at Shippensburg University, the symptoms of neurosis may involve:
 ... anxiety, sadness or depression, anger, irritability, mental confusion, low sense of self-worth, etc., behavioral symptoms such as phobic avoidance, vigilance, impulsive and compulsive acts, lethargy, etc., cognitive problems such as unpleasant or disturbing thoughts, repetition of thoughts and obsession, habitual fantasizing, negativity and cynicism, etc. Interpersonally, neurosis involves dependency, aggressiveness, perfectionism, schizoid isolation, socio-culturally inappropriate behaviors, etc.
Neurosis may be defined simply as a "poor ability to adapt to one's environment, an inability to change one's life patterns, and the inability to develop a richer, more complex, more satisfying personality."
Jung's theory.
Carl Jung found his approach particularly effective for patients who are well adjusted by social standards but are troubled by existential questions.
 *I have frequently seen people become neurotic when they content themselves with inadequate or wrong answers to the questions of life. (Jung [1961] (1989) p. 140)
Jung found that the unconscious finds expression primarily through an individual's inferior psychological function, whether it is thinking, feeling, sensation, or intuition. The characteristic effects of a neurosis on the dominant and inferior functions are discussed in "Psychological Types".
Jung saw collective neuroses in politics: "Our world is, so to speak, dissociated like a neurotic." (Jung (1964) p. 85)
Psychoanalytical theory.
According to psychoanalytic theory, neuroses may be rooted in ego defense mechanisms, but the two concepts are not synonymous. Defense mechanisms are a normal way of developing and maintaining a consistent sense of self (i.e., an ego). But only those thoughts and behaviors that produce difficulties in one's life should be called neuroses.
A neurotic person experiences emotional distress and unconscious conflict, which are manifested in various physical or mental illnesses. The definitive symptom is anxiety.
Neurotic tendencies are common and may manifest themselves as acute or chronic anxiety, depression, an obsessive–compulsive disorder, a phobia, or a personality disorder.
Neurosis should not be mistaken for psychosis, which refers to a loss of touch with reality. Neither should it be mistaken for neuroticism, which is a fundamental personality trait according to psychological theory.
Horney's theory.
In her final book, "Neurosis and Human Growth", Karen Horney laid out a complete theory of the origin and dynamics of neurosis.
In her theory, neurosis is a distorted way of looking at the world and at oneself, which is determined by compulsive needs rather than by a genuine interest in the world as it is.
Horney proposed that neurosis is transmitted to a child from his or her early environment and that there are many ways in which this can occur:
 When summarized, they all boil down to the fact that the people in the environment are too wrapped up in their own neuroses to be able to love the child, or even to conceive of him as the particular individual he is; their attitudes toward him are determined by their own neurotic needs and responses.
The child's initial reality is then distorted by his or her parents' needs and pretenses. Growing up with neurotic caretakers, the child quickly becomes insecure and develops basic anxiety. To deal with this anxiety, the child's imagination creates an idealized self-image:
 Each person builds up his personal idealized image from the materials of his own special experiences, his earlier fantasies, his particular needs, and also his given faculties. If it were not for the personal character of the image, he would not attain a feeling of identity and unity. He idealizes, to begin with, his particular "solution" of his basic conflict: compliance becomes goodness, love, saintliness; aggressiveness becomes strength, leadership, heroism, omnipotence; aloofness becomes wisdom, self-sufficiency, independence. What—according to his particular solution—appear as shortcomings or flaws are always dimmed out or retouched.
Once he identifies himself with his idealized image, a number of effects follow. He will make claims on others and on life based on the prestige he feels entitled to because of his idealized self-image. He will impose a rigorous set of standards upon himself in order to try to measure up to that image. He will cultivate pride, and with that will come the vulnerabilities associated with pride that lacks any foundation. Finally, he will despise himself for all his limitations. Vicious circles will operate to strengthen all of these effects.
Eventually, as he grows to adulthood, a particular "solution" to all the inner conflicts and vulnerabilities will solidify. He will be expansive and will display symptoms of narcissism, perfectionism, or vindictiveness. Or he will be self-effacing and compulsively compliant; he will display symptoms of neediness or codependence. Or he will be resigned and will display schizoid tendencies.
In Horney's view, mild anxiety disorders and full-blown personality disorders all fall under her basic scheme of neurosis. These are considered to be variations in the degree of severity and in the individual dynamics.
The opposite of neurosis is a condition which Horney calls self-realization, which is a state of being in which the person responds to the world with the full depth of his or her spontaneous feelings, rather than with anxiety-driven compulsion. Thus the person grows to actualize his or her inborn potentialities. Horney compares this process to an acorn that grows and becomes a tree.

</doc>
<doc id="47589" url="http://en.wikipedia.org/wiki?curid=47589" title="480s BC">
480s BC


</doc>
<doc id="47592" url="http://en.wikipedia.org/wiki?curid=47592" title="Waveform">
Waveform

A waveform is the shape and form of a signal such as a wave moving in a physical medium or an abstract representation.
In many cases the medium in which the wave is being propagated does not permit a direct visual image of the form. In these cases, the term "waveform" refers to the shape of a graph of the varying quantity against time or distance. An instrument called an oscilloscope can be used to pictorially represent a wave as a repeating image on a screen. By extension, the term "waveform" also describes the shape of the graph of any varying quantity against time.
Examples of waveforms.
Common periodic waveforms include ("t" is time):
Other waveforms are often called composite waveforms and can often be described as a combination of a number of sinusoidal waves or other basis functions added together.
The Fourier series describes the decomposition of periodic waveforms, such that any periodic waveform can be formed by the sum of a (possibly infinite) set of fundamental and harmonic components. Finite-energy non-periodic waveforms can be analyzed into sinusoids by the Fourier transform.

</doc>
<doc id="47595" url="http://en.wikipedia.org/wiki?curid=47595" title="Manchuria">
Manchuria

Manchuria () is a modern name given to a large geographic region in Northeast Asia. Depending on the definition of its extent, Manchuria usually falls entirely within China, or is sometimes divided between China and Russia. The region is now usually referred to as Northeast China () in China, although "Manchuria" is widely used outside of China to denote the geographical and historical region. This region is the traditional homeland of the Xianbei, Khitan, and Jurchen (later called Manchus, after whom Manchuria is named) peoples, who built several states historically.
Extent of Manchuria.
Manchuria can refer to any one of several regions of various size. These are, from smallest to largest:
Etymology and names.
"“Three centuries and a half must now pass away before entering upon the next act of the Manchu drama. The Nü-chêns had been scotched, but not killed, by their Mongol conquerors, who, one hundred and thirty-four years later (1368), were themselves driven out of China, a pure native dynasty being re-established under the style of Ming, "Bright." During the ensuing two hundred years the Nü-chêns were scarcely heard of, the House of Ming being busily occupied in other directions. Their warlike spirit, however, found scope and nourishment in the expeditions organised against Japan and Tan-lo, or Quelpart, as named by the Dutch, a large island to the south of the Korean peninsula; while on the other hand the various tribes scattered over a portion of the territory known to Europeans as Manchuria, availed themselves of long immunity from attack by the Chinese to advance in civilization and prosperity. It may be noted here that "Manchuria" is unknown to the Chinese or to the Manchus themselves as a geographical expression. The present extensive home of the Manchus is usually spoken of as the Three Eastern Provinces, namely, (1) Shêngking, or Liao-tung, or Kuan-tung, (2) Kirin, and (3) Heilungchiang or Tsitsihar.”" — Herbert A. Giles, "China and the Manchus", 1912
"Manchuria" is a translation of the Japanese word "Manshū", which dates from the 19th century. The name "Manju" (Manzhou) was invented and given to the Jurchen people by Hong Taiji in 1635 as a new name for their ethnic group, however, the name "Manchuria" was never used by the Manchus or the Qing dynasty itself to refer to their homeland. According to the Japanese scholar Junko Miyawaki-Okada, the Japanese geographer Takahashi Kageyasu was the first to use the term 满洲 (Manshū) as a place-name in 1809 in the "Nippon Henkai Ryakuzu", and it was from that work where Westerners adopted the name. According to Mark C. Elliott, Katsuragawa Hoshū's 1794 work, the "Hokusa bunryaku", was where 满洲 (Manshū) first appeared as a place name was in two maps included in the work, "Ashia zenzu" and "Chikyū hankyū sōzu" which were also created by Katsuragawa. 满洲 (Manshū) then began to appear as a place names in more maps created by Japanese like Kondi Jūzō, Takahashi Kageyasu, Baba Sadayoshi and Yamada Ren, and these maps were brought to Europe by the Dutch Philipp von Siebold. According to Nakami Tatsuo, Philip Franz von Siebold was the one who brought the usage of the term Manchuria to Europeans, after borrowing it from the Japanese, who were the first to use it in a geographic manner in the eighteenth century, while neither the Manchu nor Chinese languages had a term in their own language equivalent to "Manchuria" as a geographic place name. According to Bill Sewell, it was Europeans who first started using Manchuria as a name to refer to the location and it is "not a genuine geographic term." The historian Gavan McCormack agreed with Robert H. G. Lee's statement that "The term Manchuria or Man-chou is a modern creation used mainly by westerners and Japanese.", with McCormack writing that the term Manchuria is imperialistic in nature and has no "precise meaning", since the Japanese deliberately promoted the use of "Manchuria" as a geographic name to promote its separation from China while they were setting up their puppet state of Manchukuo. The Japanese had their own motive for deliberately spreading the usage of the term Manchuria. The historian Norman Smith wrote that "The term "Manchuria" is controversial". Professor Mariko Asano Tamanoi said that she "should use the term in quotation marks", when referring to Manchuria. In his 2012 dissertation on the Jurchen people to obtain a Doctor of Philosophy degree in History from the University of Washington, Professor Chad D. Garcia noted that usage of the term "Manchuria" is out of favor in "currently scholarly practice" and he did away with using the term, using instead "the northeast" or referring to specific geographical features.
In the 18th-century Europe, the region later known as "Manchuria" was most commonly referred to as "[Chinese] Tartary". However, the term Manchuria ("Mantchourie", in French) started appearing by the end of the century; French missionaries used it as early as 1800, The French-based geographers Conrad Malte-Brun and Edme Mentelle promoted the use of the term Manchuria ("Mantchourie", in French), along with "Mongolia", "Kalmykia", etc., as more precise terms than Tartary, in their world geography work published in 1804.
During the Qing dynasty, the area of Manchuria was known as the "three eastern provinces" (san dong sheng) 三東省 since 1683 when Jilin and Heilongjiang were separated even though it was not until 1907 that they were turned into actual provinces. The area of Manchuria was then converted into three provinces by the late Qing government in 1907. Since then, the "Three Northeast Provinces" () was officially used by the Qing government in China to refer to this region, and the post of Viceroy of Three Northeast Provinces was established to take charge of these provinces. After the 1911 revolution, which resulted in the collapse of the Manchu-established Qing Dynasty, the name of the region where the Manchus originated was known as "the Northeast" in official documents in the newly founded Republic of China, in addition to the "Three Northeast Provinces".
In current Chinese parlance, an inhabitant of "the Northeast", or Northeast China, is a "Northeasterner" ("Dōngběi rén"). "The Northeast" is a term that expresses the entire region, encompassing its history, culture, traditions, dialects, cuisines and so forth, as well as the "Three East Provinces" or "Three Northeast Provinces". In China, the term Manchuria () is rarely used today and the term is often negatively associated with the Japanese imperial legacy in the puppet state of Manchukuo ().
During the Ming dynasty the area where the Jurchens lived was referred to as Nurgan. Nurgan was the area of modern Jilin province in Manchuria.
Manchuria has historically also been referred to as Guandong (), which literally means "east of the pass", a reference to Shanhai Pass in Qinhuangdao in today's Hebei province, at the eastern end of the Great Wall of China. This usage is seen in the expression "Chuǎng Guāndōng" (literally "Rushing into Guandong") referring to the mass migration of Han Chinese to Manchuria in the 19th and 20th centuries. An alternate name, Guanwai (關外; 关外; "Guānwài"; "outside of the pass"), was also used for the region. The name Guandong later came to be used more narrowly for the area of the Kwantung Leased Territory on the Liaodong Peninsula.
Geography and climate.
Manchuria consists mainly of the northern side of the funnel-shaped North China Craton, a large area of tilled and overlaid Precambrian rocks spanning 100 million hectares. The North China Craton was an independent continent beforeo the Triassic period, and is known to have been the northernmost piece of land in the world during the Carboniferous. The Khingan Mountains in the west are a Jurassic mountain range formed by the collision of the North China Craton with the Siberian Craton, which marked the final stage of the formation of the supercontinent Pangaea.
No part of Manchuria was glaciated during the Quaternary, but the surface geology of most of the lower-lying and more fertile parts of Manchuria consists of very deep layers of loess, which have been formed by wind-borne movement of dust and till particles formed in glaciated parts of the Himalayas, Kunlun Shan and Tien Shan, as well as the Gobi and Taklamakan Deserts. Soils are mostly fertile Mollisols and Fluvents, except in the more mountainous parts where they are poorly developed Orthents, as well as the extreme north where permafrost occurs and Orthels dominate.
The climate of Manchuria has extreme seasonal contrasts, ranging from humid, almost tropical heat in the summer to windy, dry, Arctic cold in the winter. This pattern occurs because the position of Manchuria is on the boundary between the great Eurasian continental landmass and the huge Pacific Ocean causes complete monsoonal wind reversal.
In the summer, when the land heats faster than the ocean, low pressure forms over Asia and warm moist south to southeasterly winds bring heavy thundery rain, yielding annual rainfall ranging from 400 mm (16 in.), or less in the west, to over 1150 mm (45 in.) in the Changbai Mountains. Temperatures in the summer are very warm to hot, with July average maxima ranging from 31 °C (88 °F) in the south to 24 °C (75 °F) in the extreme north. Except in the far north near the Amur River, high humidity causes major discomfort at this time of year.
In the winter, however, the vast Siberian High causes very cold, north to northwesterly winds that bring temperatures as low as −5 °C (23 °F) in the extreme south and −30 °C (−22 °F) in the north, where the zone of discontinuous permafrost reaches northern Heilongjiang. However, because the winds from Siberia are exceedingly dry, snow falls only on a few days every winter and it is never heavy. This explains why, whereas corresponding latitudes of North America were fully glaciated during glacial periods of the Quaternary, Manchuria, though even colder, always remained too dry to form glaciers – a state of affairs enhanced by stronger westerly winds from the surface of the ice sheet in Europe.
History.
Early history.
Manchuria was the homeland of several ethnic groups, including the Manchu, Ulchs and Hezhen. Various ethnic groups and their respective kingdoms, including the Sushen, Donghu, Xianbei, Wuhuan, Mohe, Khitan and Jurchens have risen to power in Manchuria. At various times in this time period, Han Dynasty, Cao Wei Dynasty, Western Jin Dynasty, Tang Dynasty and some other minor kingdoms of China established control in parts of Manchuria and in some cases tributary relations with peoples in the area. Various kingdoms of Korea such as Gojoseon, Buyeo, Goguryeo and Balhae were also established in parts of this area. Finnish linguist Juha Janhunen believes that it was likely that a "Tungusic-speaking elite" ruled Goguryeo and Balhae, describing them as "protohistorical Manchurian states" and that part of their population was Tungusic, and that the area of southern Manchuria was the origin of Tungusic peoples and inhabited continuously by them since ancient times, and Janhunen rejected opposing theories of Goguryeo and Balhae's ethnic composition. With the Song Dynasty to the south, the Khitan people of in Inner Mongolia created the Liao Empire in the region, which went on to control adjacent parts of Northern China as well. The Khitan Empire was the first state to control all of Manchuria.
In the early 12th century the Tungusic Jurchen people, who were Liao's tributaries, overthrew the Liao and formed the Jin Dynasty (1115–1234), which went on to control parts of Northern China and Mongolia after a series of successful military campaigns. During the Yuan Dynasty (1271–1368), Manchuria was administered under the Liaoyang province. In 1375, Nahacu, a Mongol official of the Northern Yuan in Liaoyang province invaded Liaodong, but later surrendered to the Ming Dynasty in 1387. In order to protect the northern border areas the Ming decided to "pacify" the Jurchens in order to deal with its problems with Yuan remnants along its northern border. The Ming solidified control over Manchuria under Yongle Emperor (1402–1424), establishing the Nurgan Regional Military Commission. Starting in the 1580s, a Jianzhou Jurchen chieftain, Nurhaci (1558–1626), started to unify Jurchen tribes of the region. Over the next several decades, the Jurchen took control over most of Manchuria. In 1616, Nurhaci founded the Later Jin Dynasty.
Chinese cultural and religious infleunce such as Chinese New Year, the "Chinese god", Chinese motifs like the dragon, spirals, scrolls, and material goods like agriculture, husbandry, heating, iron cooking pots, silk, and cotton spread among the Amur natives like the Udeghes, Ulchis, and Nanais.
In 1644, after the Ming Dynasty's capital of Beijing was sacked by the peasant rebels, the Jurchens (now called Manchus) allied with Ming general Wu Sangui and seized control of Beijing, overthrowing the short-lived Shun Dynasty and establishing Qing Dynasty rule (1644–1912) over all of China. The Willow Palisade was a system of ditches and embankments intended to restrict the movement of the Han civilians into Jilin and Heilongjiang, built by the Qing Dynasty during the later 17th century. Only bannermen, including Chinese bannermen were allowed to settle in Jilin and Heilongjiang.
After conquering the Ming, the Qing identified their state as "China" (中國, Zhongguo; "Middle Kingdom"), and referred to it as "Dulimbai Gurun" in Manchu. The Qing equated the lands of the Qing state (including present day Manchuria, Xinjiang, Mongolia, Tibet and other areas) as "China" in both the Chinese and Manchu languages, defining China as a multi ethnic state, rejecting the idea that China only meant Han areas, proclaiming that both Han and non-Han peoples were part of "China", using "China" to refer to the Qing in official documents, international treaties, and foreign affairs, and the "Chinese language" (Dulimbai gurun i bithe) referred to Chinese, Manchu, and Mongol languages, and the term "Chinese people" (中國人 Zhongguo ren; Manchu: Dulimbai gurun i niyalma) referred to all Han, Manchus, and Mongol subjects of the Qing. The lands in Manchuria were explicitly stated by the Qing to belong to "China" (Zhongguo, Dulimbai gurun) in Qing edicts and in the Treaty of Nerchinsk.
Han Chinese farmers were resettled from north China by the Qing to the area along the Liao River in order to restore the land to cultivation. Wasteland was reclaimed by Han Chinese squatters in addition to other Han who rented land from Manchu landlords. Despite officially prohibiting Han Chinese settlement on the Manchu and Mongol lands, by the 18th century the Qing decided to settle Han refugees from northern China who were suffering from famine, floods, and drought into Manchuria and Inner Mongolia so that Han Chinese farmed 500,000 hectares in Manchuria and tens of thousands of hectares in Inner Mongolia by the 1780s. Qianlong allowed Han Chinese peasants suffering from drought to move into Manchuria despite him issuing edicts in favor of banning them from 1740-1776. Chinese tenant farmers rented or even claimed title to land from the "imperial estates" and Manchu Bannerlands in the area. Besides moving into the Liao area in southern Manchuria, the path linking Jinzhou, Fengtian, Tieling, Changchun, Hulun, and Ningguta was settled by Han Chinese during Qianlong Emperor's rule, and Han Chinese were the majority in urban areas of Manchuria by 1800. To increase the Imperial Treasury's revenue, the Qing sold formerly Manchu only lands along the Sungari to Han Chinese at the beginning of the Daoguang Emperor's reign, and Han Chinese filled up most of Manchuria's towns by the 1840s according to Abbe Huc.
The Russian conquest of Siberia was accompanied by massacres due to indigenous resistance to colonization by the Russian Cossacks, who savagely crushed the natives. At the hands of people like Vasilii Poyarkov in 1645 and Yerofei Khabarov in 1650 some peoples like the Daur were slaughtered by the Russians to the extent that it is considered genocide. The Daurs initially deserted their villages since they heard about the cruelty of the Russians the first time Khabarov came. The second time he came, the Daurs decided to do battle against the Russians instead but were slaughtered by Russian guns. The indigenous peoples of the Amur region were attacked by Russians who came to be known as "red-beards". The Russian Cossacks were named luocha (羅剎), after Demons found in Buddhist mythology, by the Amur natives because of their cruelty towards the Amur tribes people, who were subjects of the Qing. The Russian proselytization of Orthodox Christianity to the indigenous peoples along the Amur River was viewed as a threat by the Qing.
In 1858, a weakening Qing Empire was forced to cede Manchuria north of the Amur to Russia under the Treaty of Aigun. In 1860, at the Treaty of Peking, the Russians managed to obtain a further large slice of Manchuria, east of the Ussuri River. As a result, Manchuria was divided into a Russian half known as "Outer Manchuria", and a remaining Chinese half known as "Inner Manchuria". In modern literature, "Manchuria" usually refers to Inner (Chinese) Manchuria. As a result of the Treaties of Aigun and Peking, China lost access to the Sea of Japan.
History after 1860.
Inner Manchuria also came under strong Russian influence with the building of the Chinese Eastern Railway through Harbin to Vladivostok. In the "Chuang Guandong" movement, many Han farmers, mostly from the Shandong peninsula moved there. By 1921, Harbin, northern Manchuria's largest city, had a population of 300,000, including 100,000 Russians. Japan replaced Russian influence in the southern half of Inner Manchuria as a result of the Russo-Japanese War in 1904–1905. Most of the southern branch of the Chinese Eastern Railway was transferred from Russia to Japan, and became the South Manchurian Railway. Japanese influence extended into Outer Manchuria in the wake of the Russian Revolution of 1917, but Outer Manchuria had reverted to Soviet control by 1925. Manchuria was an important region for its rich mineral and coal reserves, and its soil is perfect for soy and barley production. For pre–World War II Japan, Manchuria was an essential source of raw materials. Without occupying Manchuria, the Japanese probably could not have carried out their plan for conquest over Southeast Asia or taken the risk to attack Pearl Harbor and the British Empire in 1941.
It was reported that among Banner people, both Manchu and Chinese (Hanjun) in Aihun, Heilongjiang in the 1920s, would seldom marry with Han civilians, but they (Manchu and Chinese Bannermen) would mostly intermarry with each other. Owen Lattimore reported that during his January 1930 visit to Manchuria, he studied a community in Jilin (Kirin), where both Manchu and Chinese bannermen were settled at a town called Wulakai, and eventually the Chinese Bannermen there could not be differentiated from Manchus since they were effectively Manchufied. The Han civilian population was in the process of absorbing and mixing with them when Lattimore wrote his article.
Around the time of World War I, Zhang Zuolin established himself as a powerful warlord with influence over most of Manchuria. During his rule, the Manchurian economy grew tremendously, backed by immigration of Chinese from other parts of China. The Japanese assassinated him on June 2, 1928, in what is known as the Huanggutun Incident. Following the Mukden Incident in 1931 and the subsequent Japanese invasion of Manchuria, the Japanese declared Inner Manchuria an "independent state", and appointed the deposed Qing emperor Puyi as puppet emperor of Manchukuo. Under Japanese control Manchuria was one of the most brutally run regions in the world, with a systematic campaign of terror and intimidation against the local Russian and Chinese populations including arrests, organised riots and other forms of subjugation. Manchukuo was used as a base to invade the rest of China.
After the atomic bombing of Hiroshima, Japan in 1945, the Soviet Union invaded from Soviet Outer Manchuria as part of its declaration of war against Japan. Soon afterwards, the Chinese communists and nationalists started fighting for control over Manchuria. The communists won in the Liaoshen Campaign and took complete control over Manchuria. With the encouragement of the Soviet Union, Manchuria was then used as a staging ground during the Chinese Civil War for the Communist Party of China, which emerged victorious in 1949. Ambiguities in the treaties that ceded Outer Manchuria to Russia led to dispute over the political status of several islands. This led to armed conflict in 1969, called the Sino-Soviet border conflict. In 2004, Russia agreed to transfer Yinlong Island and one half of Heixiazi Island to the PRC, ending an enduring border dispute.

</doc>
<doc id="47596" url="http://en.wikipedia.org/wiki?curid=47596" title="Korean Peninsula Energy Development Organization">
Korean Peninsula Energy Development Organization

The Korean Peninsula Energy Development Organization (KEDO) is an organization founded on March 15, 1995 by the United States, South Korea, and Japan to implement the 1994 U.S.-North Korea Agreed Framework that froze North Korea's indigenous nuclear power plant development centered at the Yongbyon Nuclear Scientific Research Center, that was suspected of being a step in a nuclear weapons program.
KEDO's principal activity is to construct a light water reactor nuclear power plant in North Korea to replace North Korea's Magnox type reactors. The original target year for completion was 2003.
Since then, other members have joined:
KEDO discussions took place at the level of a U.S. Assistant Secretary of State, South Korea's deputy foreign minister, and the head of the Asian bureau of Japan's Foreign Ministry.
The KEDO Secretariat is located in New York.
History.
Formal ground breaking on the site for two light water reactors (LWR) was on August 21, 1997 at Kumho, 30 km north of Sinpo. The Kumho site had been previously selected for two similar sized reactors that had been promised in the 1980s by the Soviet Union, before its collapse.
Soon after the Agreed Framework was signed, U.S. Congress control changed to the Republican Party, who did not support the agreement. Some Republican Senators were strongly against the agreement, regarding it as appeasement. KEDO's first director, Stephen Bosworth, later commented "The Agreed Framework was a political orphan within two weeks after its signature". 
Arranging project financing was not easy, and formal invitations to bid were not issued until 1998, by which time the delays were infuriating North Korea. Significant spending on the LWR project did not commence until 2000, with "First Concrete" pouring at the construction site on August 7, 2002. Construction of both reactors was well behind the original schedule.
In the wake of the breakdown of the Agreed Framework in 2003, KEDO has largely lost its function. KEDO ensured that the nuclear power plant project assets at the construction site at Kumho in North Korea and at manufacturers’ facilities around the world ($1.5 billion invested to date) were preserved and maintained. The project was reported to be about 30% complete. One reactor containment building was about 50% complete and another about 15% finished. No key equipment for the reactors has been moved yet to the site.
In 2005 there were reports indicating that KEDO had agreed in principle to terminate the light-water reactor project. On January 9, 2006, it was announced that the project was over and the workers would be returning to their home countries. North Korea demanded compensation and has refused to return the approximately $45 million worth of equipment left behind.

</doc>
<doc id="47599" url="http://en.wikipedia.org/wiki?curid=47599" title="Unconscious">
Unconscious

Unconscious may refer to:

</doc>
<doc id="47600" url="http://en.wikipedia.org/wiki?curid=47600" title="Simple group">
Simple group

In mathematics, a simple group is a nontrivial group whose only normal subgroups are the trivial group and the group itself. A group that is not simple can be broken into two smaller groups, a normal subgroup and the quotient group, and the process can be repeated. If the group is finite, then eventually one arrives at uniquely determined simple groups by the Jordan–Hölder theorem. The complete classification of finite simple groups, completed in 2008, is a major milestone in the history of mathematics.
Examples.
Finite simple groups.
The cyclic group "G" = Z/3Z of congruence classes modulo 3 (see modular arithmetic) is simple. If "H" is a subgroup of this group, its order (the number of elements) must be a divisor of the order of "G" which is 3. Since 3 is prime, its only divisors are 1 and 3, so either "H" is "G", or "H" is the trivial group. On the other hand, the group "G" = Z/12Z is not simple. The set "H" of congruence classes of 0, 4, and 8 modulo 12 is a subgroup of order 3, and it is a normal subgroup since any subgroup of an abelian group is normal. Similarly, the additive group Z of integers is not simple; the set of even integers is a non-trivial proper normal subgroup.
One may use the same kind of reasoning for any abelian group, to deduce that the only simple abelian groups are the cyclic groups of prime order. The classification of nonabelian simple groups is far less trivial. The smallest nonabelian simple group is the alternating group "A"5 of order 60, and every simple group of order 60 is isomorphic to "A"5. The second smallest nonabelian simple group is the projective special linear group PSL(2,7) of order 168, and it is possible to prove that every simple group of order 168 is isomorphic to PSL(2,7).
Infinite simple groups.
The infinite alternating group, i.e. the group of even permutations of the integers, formula_1 is simple. This group can be defined as the increasing union of the finite simple groups formula_2 with respect to standard embeddings formula_3. Another family of examples of infinite simple groups is given by formula_4, where formula_5 is a field and formula_6.
It is much more difficult to construct "finitely generated" infinite simple groups. The first example is due to Graham Higman and is a quotient of the Higman group. Other examples include the infinite Thompson groups "T" and "V". Finitely presented torsion-free infinite simple groups were constructed by Burger-Mozes.
Classification.
There is as yet no known classification for general simple groups.
Finite simple groups.
The finite simple groups are important because in a certain sense they are the "basic building blocks" of all finite groups, somewhat similar to the way prime numbers are the basic building blocks of the integers. This is expressed by the Jordan–Hölder theorem which states that any two composition series of a given group have the same length and the same factors, up to permutation and isomorphism. In a huge collaborative effort, the classification of finite simple groups was declared accomplished in 1983 by Daniel Gorenstein, though some problems surfaced (specifically in the classification of quasithin groups, which were plugged in 2004).
Briefly, finite simple groups are classified as lying in one of 18 families, or being one of 26 exceptions:
Structure of finite simple groups.
The famous theorem of Feit and Thompson states that every group of odd order is solvable. Therefore every finite simple group has even order unless it is cyclic of prime order.
The Schreier conjecture asserts that the group of outer automorphisms of every finite simple group is solvable. This can be proved using the classification theorem.
History for finite simple groups.
There are two threads in the history of finite simple groups – the discovery and construction of specific simple groups and families, which took place from the work of Galois in the 1820s to the construction of the Monster in 1981; and proof that this list was complete, which began in the 19th century, most significantly took place 1955 through 1983 (when victory was initially declared), but was only generally agreed to be finished in 2004. s of 2010[ [update]], work on improving the proofs and understanding continues; see for 19th century history of simple groups.
Construction.
Simple groups have been studied at least since early Galois theory, where Évariste Galois realized that the fact that the alternating groups on five or more points are simple (and hence not solvable), which he proved in 1831, was the reason that one could not solve the quintic in radicals. Galois also constructed the projective special linear group of a plane over a prime finite field, PSL(2,"p"), and remarked that they were simple for "p" not 2 or 3. This is contained in his last letter to Chevalier, and are the next example of finite simple groups.
The next discoveries were by Camille Jordan in 1870. Jordan had found 4 families of simple matrix groups over finite fields of prime order, which are now known as the classical groups.
At about the same time, it was shown that a family of five groups, called the Mathieu groups and first described by Émile Léonard Mathieu in 1861 and 1873, were also simple. Since these five groups were constructed by methods which did not yield infinitely many possibilities, they were called "sporadic" by William Burnside in his 1897 textbook.
Later Jordan's results on classical groups were generalized to arbitrary finite fields by Leonard Dickson, following the classification of complex simple Lie algebras by Wilhelm Killing. Dickson also constructed exception groups of type G2 and E6 as well, but not of types F4, E7, or E8 . In the 1950s the work on groups of Lie type was continued, with Claude Chevalley giving a uniform construction of the classical groups and the groups of exceptional type in a 1955 paper. This omitted certain known groups (the projective unitary groups), which were obtained by "twisting" the Chevalley construction. The remaining groups of Lie type were produced by Steinberg, Tits, and Herzig (who produced 3"D"4("q") and 2"E"6("q")) and by Suzuki and Ree (the Suzuki–Ree groups).
These groups (the groups of Lie type, together with the cyclic groups, alternating groups, and the five exceptional Mathieu groups) were believed to be a complete list, but after a lull of almost a century since the work of Mathieu, in 1964 the first Janko group was discovered, and the remaining 20 sporadic groups were discovered or conjectured in 1965–1975, culminating in 1981, when Robert Griess announced that he had constructed Bernd Fischer's "Monster group". The Monster is the largest sporadic simple group having order of 808,017,424,794,512,875,886,459,904,961,710,757,005,754,368,000,000,000. The Monster has a faithful 196,883-dimensional representation in the 196,884-dimensional Griess algebra, meaning that each element of the Monster can be expressed as a 196,883 by 196,883 matrix.
Classification.
The full classification is generally accepted as starting with the Feit–Thompson theorem of 1962/63, largely lasting until 1983, but only being finished in 2004.
Soon after the construction of the Monster in 1981, a proof, totaling more than 10,000 pages, was supplied that group theorists had successfully listed all finite simple groups, with victory declared in 1983 by Daniel Gorenstein. This was premature – some gaps were later discovered, notably in the classification of quasithin groups, which were eventually replaced in 2004 by a 1,300 page classification of quasithin groups, which is now generally accepted as complete.
Tests for nonsimplicity.
"Sylows' test": Let "n" be a positive integer that is not prime, and let "p" be a prime divisor of "n". If 1 is the only divisor of "n" that is equal to 1 modulo p, then there does not exist a simple group of order "n".
Proof: If "n" is a prime-power, then a group of order "n" has a nontrivial center and, therefore, is not simple. If "n" is not a prime power, then every Sylow subgroup is proper, and, by Sylow's Third Theorem, we know that the number of Sylow p-subgroups of a group of order "n" is equal to 1 modulo "p" and divides "n". Since 1 is the only such number, the Sylow p-subgroup is unique, and therefore it is normal. Since it is a proper, non-identity subgroup, the group is not simple.
"Burnside": A non-Abelian finite simple group has order divisible by at least three distinct primes. This follows from Burnside's p-q theorem.
References.
Textbooks.
</dl>
Papers.
</dl>

</doc>
<doc id="47604" url="http://en.wikipedia.org/wiki?curid=47604" title="Eleanor of Castile (disambiguation)">
Eleanor of Castile (disambiguation)

Leonora of Castile or Eleanor of Castile may refer to:

</doc>
<doc id="47607" url="http://en.wikipedia.org/wiki?curid=47607" title="Suspension bridge">
Suspension bridge

A suspension bridge is a type of bridge in which the deck (the load-bearing portion) is hung below suspension cables on vertical suspenders. The first modern examples of this type of bridge were built in the early 19th century. Simple suspension bridges, which lack vertical suspenders, have a long history in many mountainous parts of the world.
This type of bridge has cables suspended between towers, plus vertical "suspender cables" that carry the weight of the deck below, upon which traffic crosses. This arrangement allows the deck to be level or to arc upward for additional clearance. Like other suspension bridge types, this type often is constructed without falsework.
The suspension cables must be anchored at each end of the bridge, since any load applied to the bridge is transformed into a tension in these main cables. The main cables continue beyond the pillars to deck-level supports, and further continue to connections with anchors in the ground. The roadway is supported by vertical suspender cables or rods, called hangers. In some circumstances, the towers may sit on a bluff or canyon edge where the road may proceed directly to the main span, otherwise the bridge will usually have two smaller spans, running between either pair of pillars and the highway, which may be supported by suspender cables or may use a truss bridge to make this connection. In the latter case there will be very little arc in the outboard main cables.
History.
The earliest suspension bridges were ropes slung across a chasm, with a deck possibly at the same level or hung below the ropes so that the rope has a catenary shape.
Precursor.
The Tibetan saint and bridge-builder Thangtong Gyalpo originated the use of iron chains in his version of early suspension bridges. In 1433, Gyalpo built eight bridges in eastern Bhutan. The last surviving chain-linked bridge of Gyalpo's was the Thangtong Gyalpo Bridge in Duksum en route to Trashi Yangtse, which was finally washed away in 2004. Gyalpo's iron chain bridges did not include a suspended deck bridge which is the standard on all modern suspension bridges today. Instead, both the railing and the walking layer of Gyalpo's bridges used wires. The stress points that carried the screed were reinforced by the iron chains. Before the use of iron chains it is thought that Gyalpo used ropes from twisted willows or yak skins. He may have also used tightly bound cloth.
First.
The first design for a bridge resembling the modern suspension bridge is attributed to Venetian polymath Fausto Veranzio, whose 1595 book "Machinae Novae" included drawings both for a timber and rope suspension bridge, and a hybrid suspension and cable-stayed bridge using iron chains (see gallery below).
The first American iron chain suspension bridge was the Jacob's Creek Bridge (1801) in Westmoreland County, Pennsylvania, designed by inventor James Finley. Finley's bridge was the first to incorporate all of the necessary components of a modern suspension bridge, including a suspended deck which hung by trusses. Finley patented his design in 1808, and published it in the Philadelphia journal, The Port Folio, in 1810.
Early British chain bridges included the Dryburgh Abbey Bridge (1817) and 137 m Union Bridge (1820), with spans rapidly increasing to 176 m with the Menai Bridge (1826), "the first important modern suspension bridge". The Clifton Suspension Bridge (designed in 1831, completed in 1864 with a 214 m central span) is one of the longest of the parabolic arc chain type.
Wire-cable.
The first wire-cable suspension bridge was the Spider Bridge at Falls of Schuylkill (1816), a modest and temporary footbridge built following the collapse of James Finley's nearby Chain Bridge at Falls of Schuylkill (1808). The footbridge's span was 124 m, although its deck was only 0.45 m wide.
Development of wire-cable suspension bridges dates to the temporary simple suspension bridge at Annonay built by Marc Seguin and his brothers in 1822. It spanned only 18 m. The first permanent wire cable suspension bridge was Guillaume Henri Dufour's Saint Antoine Bridge in Geneva of 1823, with two 40 m spans. The first with cables assembled in mid-air in the modern method was Joseph Chaley's Grand Pont Suspendu in Fribourg, in 1834.
In the United States, the first major wire-cable suspension bridge was the in Philadelphia, Pennsylvania. Designed by Charles Ellet, Jr. and completed in 1842, it had a span of 109 m. Ellet's Niagara Falls Suspension Bridge (1847–48) was abandoned before completion. It was used as scaffolding for John A. Roebling's double decker railroad and carriage bridge (1855).
The Otto Beit Bridge (1938–39) was the first modern suspension bridge outside the United States built with parallel wire cables.
Structural behavior.
Structural analysis.
The main forces in a suspension bridge of any type are tension in the cables and compression in the pillars. Since almost all the force on the pillars is vertically downwards and they are also stabilized by the main cables, the pillars can be made quite slender, as on the Severn Bridge, on the Wales-England border.
The slender lines of the Severn Bridge
In a suspended deck bridge, cables suspended via towers hold up the road deck. The weight is transferred by the cables to the towers, which in turn transfer the weight to the ground.
Assuming a negligible weight as compared to the weight of the deck and vehicles being supported, the main cables of a suspension bridge will form a parabola (very similar to a catenary, the form the unloaded cables take before the deck is added). One can see the shape from the constant increase of the gradient of the cable with linear (deck) distance, this increase in gradient at each connection with the deck providing a net upward support force. Combined with the relatively simple constraints placed upon the actual deck, this makes the suspension bridge much simpler to design and analyze than a cable-stayed bridge, where the deck is in compression.
Variations.
Underspanned.
In an underspanned suspension bridge, the main cables hang entirely below the bridge deck, but are still anchored into the ground in a similar way to the conventional type. Very few bridges of this nature have been built, as the deck is inherently less stable than when suspended below the cables. Examples include the Pont des Bergues of 1834 designed by Guillaume Henri Dufour; James Smith's Micklewood Bridge; and a proposal by Robert Stevenson for a bridge over the River Almond near Edinburgh.
Roebling's Delaware Aqueduct (begun 1847) consists of three sections supported by cables.
The timber structure essentially hides the cables; and from a quick view, it is not immediately apparent that it is even a suspension bridge.
Suspension cable types.
The main suspension cable in older bridges was often made from chain or linked bars, but modern bridge cables are made from multiple strands of wire. This contributes greater redundancy; a few flawed strands in the hundreds used pose very little threat, whereas a single bad link or eyebar can cause failure of the entire bridge. (The failure of a single eyebar was found to be the cause of the collapse of the Silver Bridge over the Ohio River). Another reason is that as spans increased, engineers were unable to lift larger chains into position, whereas wire strand cables can be largely prepared in mid-air from a temporary walkway.
Deck structure types.
Most suspension bridges have open truss structures to support the roadbed, particularly owing to the unfavorable effects of using plate girders, discovered from the Tacoma Narrows Bridge (1940) bridge collapse. Recent developments in bridge aerodynamics have allowed the re-introduction of plate structures. In the picture of the Yichang Bridge, note the very sharp entry edge and sloping undergirders in the suspension bridge shown. This enables this type of construction to be used without the danger of vortex shedding and consequent aeroelastic effects, such as those that destroyed the original Tacoma Narrows bridge.
Forces.
Three kinds of forces operate on any bridge: the dead load, the live load, and the dynamic load. Dead load refers to the weight of the bridge itself. Like any other structure, a bridge has a tendency to collapse simply because of the gravitational forces acting on the materials of which the bridge is made. Live load refers to traffic that moves across the bridge as well as normal environmental factors such as changes in temperature, precipitation, and winds. Dynamic load refers to environmental factors that go beyond normal weather conditions, factors such as sudden gusts of wind and earthquakes. All three factors must be taken into consideration when building a bridge.
Use other than road and rail.
The principles of suspension used on the large scale may also appear in contexts less dramatic than road or rail bridges. Light cable suspension may prove less expensive and seem more elegant for a footbridge than strong girder supports. Where such a bridge spans a gap between two buildings, there is no need to construct special towers, as the buildings can anchor the cables. Cable suspension may also be augmented by the inherent stiffness of a structure that has much in common with a tubular bridge.
Construction sequence (wire strand cable type).
Typical suspension bridges are constructed using a sequence generally described as follows. Depending on length and size, construction may take anywhere between a year and a half (construction on the original Tacoma Narrows Bridge took only 19 months) up to as a decade (the Akashi-Kaikyō Bridge's construction began in May 1986 and was opened in May, 1998 – a total of twelve years).
Longest spans.
Suspension bridges are typically ranked by the length of their main span. These are the ten bridges with the longest spans, followed by the length of the span and the year the bridge opened for traffic:

</doc>
<doc id="47610" url="http://en.wikipedia.org/wiki?curid=47610" title="Great Belt Fixed Link">
Great Belt Fixed Link

The Great Belt Fixed Link (Danish: "Storebæltsforbindelsen") runs between the Danish islands of Zealand and Funen. It consists of three structures: a road suspension bridge and a railway tunnel between Zealand and the small island Sprogø located in the middle of the Great Belt, and a box girder bridge for both road and rail traffic between Sprogø and Funen. The "Great Belt Bridge" (Danish: "Storebæltsbroen") commonly refers to the suspension bridge, although it may also be used to mean the box-girder bridge or the link in its entirety. The suspension bridge, officially known as the East Bridge, has the world's third longest main span (1.6 km), the longest outside of Asia. It was designed by the Danish engineering firm COWI.
The link replaced the ferry service that had been the primary means of crossing the Great Belt. After more than five decades of speculation and debate, the decision to construct the link was made in 1986; the original intent was to complete the railway link three years before opening the road connection, but the link opened to rail traffic in 1997 and road traffic in 1998. At an estimated cost of DKK 21.4 billion (1988 prices), the link is the largest construction project in Danish history.
Operation and maintenance are performed by "A/S Storebælt" under "Sund & Bælt". Construction and maintenance are financed by tolls on vehicles and trains.
The link has reduced travel times significantly; previously taking about an hour by ferry, the Great Belt can now be crossed in about ten minutes. The construction of the link and the Øresund Bridge have together enabled driving from mainland Europe to Sweden and the rest of Scandinavia through Denmark. Cyclists are not permitted to use the bridge, but cycles may be transported by train or bus.
History.
The Great Belt ferries entered service between the coastal towns of Korsør and Nyborg in 1883, connecting the railway lines on either side of the Belt. In 1957, road traffic was moved to the Halsskov–Knudshoved route, about 1.5 kilometres to the north and close to the fixed link.
Construction drafts for a fixed link were presented as early as the 1850s, with several suggestions appearing in the following decades. The Danish State Railways, responsible for the ferry service, presented plans for a bridge in 1934. The concepts of bridges over Øresund (152m DKK) and Storebælt (257m DKK) were calculated around 1936. In 1948, the Ministry for Public Works (now the Ministry of Transport) established a commission to investigate the implications of a fixed link.
The first law concerning a fixed link was enacted in 1973, but the project was put on hold in 1978 as the Venstre (Liberal) party demanded postponing public spending. Political agreement to restart work was reached in 1986, with a construction law (Danish: "anlægslov") being passed in 1987.
The design was carried out by the engineering firm COWI together with Dissing+Weitling architecture practice.
Construction of the link commenced in 1988. In 1991, Finland sued Denmark at the International Court of Justice, on the grounds that Finnish-built mobile offshore drilling units would be unable to pass beneath the bridge. The two countries negotiated a financial compensation of 90 million Danish kroner, and Finland withdrew the lawsuit.
The link is estimated to have created a value of 379 billion DKK after 50 years of use.
Construction.
The construction of the fixed link became the biggest building project in the history of Denmark. In order to connect Halsskov on Zealand with Knudshoved on Funen, 18 kilometres to its west, a two-track railway and a four-lane motorway had to be built, via the small island of Sprogø in the middle of the Great Belt. The project comprised three different tasks: the East Bridge for road transport, the East Tunnel for rail transport and the West Bridge for road and rail transport combined. The construction work was carried out by Sundlink Contractors, a consortium of Skanska, Hochtief, Højgaard & Schultz (which built the West Bridge) and Monberg & Thorsen (which built the eight-kilometre section under the Great Belt). The work of lifting and placing the elements was carried out by Ballast Nedam using a floating crane.
The East Bridge.
Built between 1991 and 1998 at a cost of US$950 million, the East Bridge ("Østbroen") is a suspension bridge between Halsskov and Sprogø. It is 6790 m long with a free span of 1624 m, making it the world's third-longest suspension bridge span, surpassed only by the Akashi Kaikyō Bridge and Xihoumen Bridge. The Akashi-Kaikyo Bridge was opened two months earlier. The East Bridge had been planned to be completed in time to be the longest bridge, but it was delayed. The vertical clearance for ships is 65 m, meaning the world's largest cruise ship, an Oasis-class cruise ship, just fits under with its smokestack folded.
At 254 m above sea level, the two pylons of the East Bridge are the highest points on solid structures in Denmark. Some radio masts, such as Tommerup transmitter, are taller.
To keep the main cables tensioned, an anchorage structure on each side of the span is placed below the road deck. After 15 years, the cables have no rust. They were scheduled for a 15million DKK paint job, but due to corroding cables on other bridges, the decision was made to instead install a 70million DKK sealed de-humidifying system in the cables.
Nineteen concrete pillars (12 on the Zealand side, seven by Sprogø), 193 m apart, carry the road deck outside the span.
The West Bridge.
The West Bridge ("Vestbroen") is a box girder bridge between Sprogø and Knudshoved. It is 6611 m long, and has a vertical clearance for ships of 18 m. It is actually two separate, adjacent bridges: the northern one carries rail traffic and the southern one road traffic. The pillars of the two bridges rest on common foundations below sea level. The West Bridge was built between 1988 and 1994; its road/rail deck comprises 63 sections, supported by 62 pillars.
The East Tunnel.
The twin bored tunnel tubes of the East Tunnel ("Østtunnelen") are each 8024 m long. There are 31 connecting tunnels between the two main tunnels, at 250 m intervals. The equipment that is necessary for train operation in the tunnels is installed in the connecting tunnels, which also serve as emergency escape routes.
There were delays and cost overruns in the tunnel construction. The plan was to open it in 1993, giving the trains a head start of three years over road traffic, but train traffic started in 1997 and road traffic in 1998. During construction the sea bed gave way and one of the tunnels was flooded. The water continued to rise and reached the end at Sprogø, where it continued into the (still dry) other tunnel. The water damaged two of the four tunnel boring machines, but no workers were injured. Only by placing a clay blanket on the sea bed was it possible to dry out the tunnels. The two damaged machines were repaired and the majority of the tunnelling was undertaken from the Sprogø side. The machines on the Zealand side tunnelled through difficult ground and made little progress. A major fire on one of the Zealand machines in June 1994 stopped these drives and the tunnels were completed by the two Sprogø machines.
A total of 320 compressed air workers were involved in 9018 pressure exposures in the four tunnel-boring machines. The project had a decompression sickness incidence of 0.14% with two workers having long-term residual symptoms.
Traffic implications.
Prior to the opening of the link, an average of 8,000 cars used the ferries across the Great Belt every day. This increased 127 percent the first year of opening due to the so-called traffic leap; new traffic generated by the improved ease, facility and lower price of crossing the Great Belt.
In 2008, an average of 30,200 cars used the link each day. The increase in traffic is partly caused by the general growth of traffic, partly diversion of traffic volume from other ferry services and air services.
The fixed link has produced considerable time savings between eastern and western Denmark. Previously, it took approximately 90 minutes on average to cross the Great Belt in a car with transfer by ferry, including the waiting time at the ports. It took considerably longer during peak periods, such as at weekends and holidays. With the opening of the link, the journey is now between ten and 15 minutes.
By train the time savings are significant as well. The journey has been reduced by 60 minutes, and there are many more seats available because more carriages may be added to a train as the train does not have to fit onto a ferry. The seating capacity offered by DSB across the Great Belt on an ordinary Wednesday has risen from 11,060 seats to 37,490 seats. On Fridays the seating capacity exceeds 40,000 seats.
On the following stretches the shortest travel times are as follows: Copenhagen–Odense 1 hour 15 minutes, Copenhagen–Aarhus 2 hours 30 minutes, Copenhagen–Aalborg 3 hours 55 minutes and Copenhagen–Esbjerg 2 hours 35 minutes.
Flights between Copenhagen and Odense, and between Copenhagen and Esbjerg have ceased, and the train now has the largest market share between Copenhagen and Aarhus.
Together with the Øresund Bridge, the link provides a direct fixed connection between western Continental Europe and northern Scandinavia, eventually connecting all parts of the European Union except Ireland, Malta and Cyprus and outlying islands. Most people from Zealand still prefer taking the ferry between Puttgarden and Rødby, as it is a much shorter distance and provides a needed break for those travelling a long distance.
For freight trains, the fixed links are a large improvement between Sweden and Germany, and between Sweden and the UK. The Sweden-to-Germany ferry system is still used to some extent owing to limited rail capacity, with heavy passenger traffic over the bridges and some single track stretches in southern Denmark and northern Germany.
The Great Belt was used by now defunct night passenger trains between Copenhagen and Germany, which were too long to fit on the ferries. Day trains on the Copenhagen-Hamburg route use the Fehmarn Belt ferries, with short diesel trains.
In 2020 the Fehmarn Belt Fixed Link is expected to be complete with much of this international traffic being shifted from the Great Belt Fixed Link. This more direct route will reduce the rail journey from Hamburg to Copenhagen from 4¾ to 3½ hours.
Toll charge.
In 2014 the vehicle tolls were as follows:
Environmental effects.
Environmental considerations have been an integral part of the project, and have been of decisive significance for the choice of alignment and determination of the design. Environmental considerations were the reason why Great Belt A/S established an environmental monitoring programme in 1988, and initiated co-operation with authorities and external consultants on the definition of environmental concerns during the construction work and the professional requirements to the monitoring programme. This co-operation issued in a report published at the beginning of 1997 on the state of the environment in the Great Belt. The conclusion of the report was that the marine environment was at least as good as before construction work began.
As concerns the water flows, the link must comply with the so-called zero-solution. This has been achieved by deepening parts of the Great Belt, so that the water flow cross section has been increased. This excavation compensates for the blocking effect caused by the bridge pylons and approach ramps. The conclusion of the report is that water flows are now almost at the level they were before the bridge was built.
The fixed link has generated increased road traffic volume, which has meant increased air pollution. However, there has been significant savings in the energy consumption by switching from ferries to the fixed link. Train and car ferries consume much energy for propulsion, high-speed ferries consume large amounts of energy at high speeds, and air transport is highly energy consuming. Domestic air travel over the Great Belt was greatly reduced after the opening of the bridge, with the former air travellers now using trains and private cars.
The larger energy consumption by ferries as opposed to via the fixed link is most clearly seen when comparing short driving distances from areas immediately east or west of the link. For more extended driving distances the difference in energy consumption is smaller, but any transport within Denmark across the link shows very clear energy savings.
During 2009, seven large wind turbines, likely Vestas 3MWs totalling 21MW capacity, were erected in the sea north of Sprogø to contribute to the electrical demand of the Great Belt Link. Their hub heights are about the same level as the road deck of the suspension bridge. Part of the project was to showcase sea wind at the December 2009 Copenhagen climate meeting.
Accidents.
During construction 479 work-related accidents were reported, of which 53 resulted in serious injuries or death. At least seven workers died as a result of work-related accidents.
The West Bridge has been struck by sea traffic twice. While the link was still under construction on 14 September 1993, the ferry M/F Romsø drifted off course in bad weather and hit the West Bridge. At 19:17 on 3 March 2005, the 3,500-ton freighter MV Karen Danielsen crashed into the West Bridge 800 metres from Funen. All traffic across the bridge was halted, effectively cutting Denmark in two. The bridge was re-opened shortly after midnight, after the freighter was pulled free and inspectors had found no structural damage to the bridge.
The East Bridge has so far been in the clear, although on 16 May 2001, the bridge was closed for 10 minutes as the Cambodian 27,000-ton bulk carrier "Bella" was heading straight for one of the anchorage structures. The ship was deflected by a swift response from the navy.
On 5 June 2006, a maintenance vehicle burst into flames in the east-bound railway tunnel at about 21:30. Nobody was hurt; its crew of three fled to the other tunnel and escaped. The fire was put out shortly before midnight, and the vehicle was removed from the tunnel the next day. Train service resumed on 6 June at reduced speed, and normal service was restored on 12 June.

</doc>
<doc id="47611" url="http://en.wikipedia.org/wiki?curid=47611" title="Doomsday Clock">
Doomsday Clock

The Doomsday Clock is a symbolic clock face, representing a countdown to possible global catastrophe (e.g. nuclear war or climate change). It has been maintained since 1947 by the members of the Science and Security Board of the "Bulletin of the Atomic Scientists", who are in turn advised by the Governing Board and the Board of Sponsors, including 18 Nobel Laureates. The closer they set the Clock to midnight, the closer the scientists believe the world is to global disaster.
Originally, the Clock, which hangs on a wall in the Bulletin's office in the University of Chicago, represented an analogy for the threat of global nuclear war; however, since 2007 it has also reflected climate change and new developments in the life sciences and technology that could inflict irrevocable harm to humanity. The most recent officially announced setting—three minutes to midnight (11:57 pm)—was made on January 22, 2015 due to climate change, the modernization of nuclear weapons in the United States and Russia, and the problem of nuclear waste.
History.
The origin of the Clock can be traced to the international group of researchers called the Chicago Atomic Scientists who had participated in the Manhattan Project. After the atomic bombing of Hiroshima and Nagasaki, they started to publish a mimeographed newsletter and then a bulletin. Since its inception, the Clock has been depicted on every cover of the "Bulletin of the Atomic Scientists". Its first representation was in 1947, when bulletin co-founder Hyman Goldsmith asked artist Martyl Langsdorf (wife of Manhattan Project research associate and Szilárd petition signatory Alexander Langsdorf, Jr.) to design a cover for the magazine's June 1947 issue. As Eugene Rabinowitch, another co-founder of the Bulletin, explained later, The Bulletin's clock is not a gauge to register the ups and downs of the international power struggle; it is intended to reflect basic changes in the level of continuous danger in which mankind lives in the nuclear age...
In January, 2007, designer Michael Bierut, who was on the "Bulletin's" Governing Board, redesigned the Clock to give it a more modern feel. In 2009, the "Bulletin" ceased its print edition and was one of the first print publications in the US to become entirely digital; the Clock is now found as part of the logo on the "Bulletin's" website. Information about the Doomsday Clock Symposium, a timeline of the Clock's settings, and multimedia shows about the Clock's history and culture can also be found on the "Bulletin"'s website.
The 5th Doomsday Clock Symposium was held on November 14, 2013 in Washington, D.C.; it was a daylong event that was open to the public and featured panelists discussing various issues on the topic "Communicating Catastrophe." There was also an evening event at the Hirshhorn Museum and Sculpture Garden in conjunction with the Hirshhorn's current exhibit, "Damage Control: Art and Destruction Since 1950." The panel discussions, held at the American Association for the Advancement of Science, were streamed live from the "Bulletin's" website, and can still be viewed there. Reflecting international events dangerous to humankind, the Clock's hands have been adjusted twenty times since its inception in 1947, when the Clock was initially set to seven minutes to midnight (11:53pm).
Symbolic timepiece changes.
In 1947, during the Cold War, the Clock was started at seven minutes to midnight and was subsequently advanced or rewound per the state of the world and nuclear war prospects. The Clock's setting is decided by the Science and Security Board of the "Bulletin of the Atomic Scientists" and is an adjunct to the essays in the "Bulletin" on global affairs. The Clock is not set and reset in real time as events occur; rather than respond to each and every crisis as it happens, the Science and Security Board meets twice annually to discuss global events in a deliberative manner. The closest nuclear war threat, the Cuban Missile Crisis in 1962, reached crisis, climax, and resolution before the Clock could be set to reflect that possible doomsday.

</doc>
<doc id="47612" url="http://en.wikipedia.org/wiki?curid=47612" title="Hexactinellid">
Hexactinellid

Hexactinellid sponges are sponges with a skeleton made of four- and/or six-pointed siliceous spicules, often referred to as glass sponges. They are usually classified along with other sponges in the phylum Porifera, but some researchers consider them sufficiently distinct to deserve their own phylum, Symplasma.
Biology.
Glass sponges are relatively uncommon and are mostly found at depths from 450 to although the species "Oopsacas minuta" has been found in shallow water, while others have been found much deeper. They are found in all oceans of the world, although they are particularly common in Antarctic and Northern Pacific waters.
They are more-or-less cup-shaped animals, ranging from 10 to in height, with sturdy lattice-like internal skeletons made up of fused spicules of silica. The body is relatively symmetrical, with a large central cavity that, in many species, opens to the outside through a sieve formed from the skeleton. Some species of glass sponges are capable of fusing together to create reefs or bioherms. They are generally pale in colour, ranging from white to orange.
Much of the body is composed of syncitial tissue, extensive regions of multinucleate cytoplasm. In particular, the epidermal cells of other sponges are absent, being replaced by a syncitial net of amoebocytes, through which the spicules penetrate. Unlike other sponges, they do not possess the ability to contract.
One ability they do possess is a unique system for rapidly conducting electrical impulses across their bodies, making it possible for them to respond quickly to external stimuli. Glass sponges like "Venus' Flower Basket" have a tuft of fibers that extends outward like an inverted crown at the base of their skeleton. These fibers are 50 to long and about the thickness of a human hair.
Glass sponges are different from other sponges in a variety of other ways. For example, most of the cytoplasm is not divided into separate cells by walls but forms a syncytium or continuous mass of cytoplasm with many nuclei (e.g., Reiswig and Mackie, 1983).
These creatures are long lived, but the exact age is hard to measure; one study based on modelling gave an estimated age of a specimen of "Scolymastra joubini" as 23,000 years, which is thought impossible, but is the basis for a listing of ~15,000 years in the AnAge Database.
The shallow water occurrence of hexactinellids is rare world wide. In the Antarctic two species occur as shallow as 33 meters under the ice. In the Mediterranean one species occurs as shallow as 18 m in a cave with deep water upwelling (Boury-Esnault & Vacelet (1994))
Reefs.
The sponges form reefs off the coast of British Columbia and Washington State, which are studied in the Sponge Reef Project.
Classification.
The earliest known hexactinellids are from the earliest Cambrian or late Neoproterozoic. They are fairly common relative to demosponges as fossils, but this is thought to be, at least in part, because their spicules are sturdier than spongin and fossilize better. 
Like almost all sponges, the hexactinellids draw water in through a series of small pores by the whip like beating of a series of hairs or flagella in chambers which in this group line the sponge wall. (Sponge Gardens)
The class is divided into six orders, in two subclasses:
Class Hexactinellida

</doc>
<doc id="47615" url="http://en.wikipedia.org/wiki?curid=47615" title="Rook (chess)">
Rook (chess)

A rook (♖ ♜ borrowed from Persian رخ "rokh", Sanskrit रथ "rath", "chariot") is a piece in the strategy board game of chess. Formerly the piece was called the "tower", "marquess", "rector", and "comes" . The term "castle" is considered informal, incorrect, or old-fashioned.
Each player starts the game with two rooks, one in each of the corner squares on their own side of the board.
Initial placement and movement.
In algebraic notation, the white rooks start on squares "a1" and "h1", while the black rooks start on "a8" and "h8". The rook moves horizontally or vertically, through any number of unoccupied squares (see diagram). As with captures by other pieces, the rook captures by occupying the square on which the enemy piece sits. The rook also participates, with the king, in a special move called castling.
History.
In the medieval shatranj, the rook symbolized a chariot. The Persian word "rukh" means chariot , and the corresponding pieces in Oriental chess games such as xiangqi and shogi have names also meaning chariot (車).
Persian war chariots were heavily armoured, carrying a driver and at least one ranged-weapon bearer, such as an archer. The sides of the chariot were built to resemble fortified stone work, giving the impression of small, mobile buildings, causing terror on the battlefield. However, in the West the rook is almost universally represented as a crenellated turret. One possible explanation is that when the game was imported to Italy, the Persian "rukh" became the Italian word "rocca," meaning fortress, and from there spread in the rest of Europe. Another possible explanation is that rooks represent siege towers – the piece is called "torre", meaning tower, in Italian, Portuguese, and Spanish; "tour" in French; "toren" in Dutch; "Turm" in German; and "Torn" in Swedish. An alternative name in Russian: тура (pronounced as toura). Finally, the chariot was sometimes represented as a silhouette, a square with two points above representing the horse's heads, which may have been seen to resemble a building with arrowports to the medieval imagination. An exception is seen in the British Museum's collection of the medieval Lewis chess pieces in which the rooks appear as stern warders or wild-eyed Berzerker warriors. Rooks usually are similar in appearance to small castles, and as a result a rook is sometimes called a "castle" . This usage was common in the past ("The Rook, or Castle, is next in power to the Queen" – Howard Staunton, 1847) but today it is rarely if ever used in chess literature or among players, except in the expression "castling".
The Russian name for the rook ("ladya") means a sailing boat or longship of Northern cultures such as the Vikings.
Strategy.
Relative value.
In general, rooks are stronger than bishops or knights (which are called "minor pieces") and are considered greater in value than either of those pieces by nearly two pawns but less valuable than two minor pieces. Two rooks are generally considered to be worth slightly more than a queen (see chess piece relative value). Winning a rook for a bishop or knight is referred to as winning "the exchange". Rooks and queens are called "heavy pieces" or "major pieces", as opposed to bishops and knights, the minor pieces.
Placement.
In the opening, the rooks are blocked in by other pieces and cannot immediately participate in the game; so it is usually desirable to "connect" one's rooks on the first rank by clearing all pieces except the king and rooks from the first rank and then castling. In that position, the rooks support each other, and can more easily move to occupy and control the most favorable files.
A common strategic goal is to place a rook on the first rank of an open file (i.e. one unobstructed by pawns of either player), or a half-open file (i.e., one unobstructed by friendly pawns). From this position, the rook is relatively unexposed to risk but can exert control on every square on the file. If one file is particularly important, a player might advance one rook on it, then position the other rook behind – "doubling" the rooks.
A rook on the seventh rank (the opponent's second rank) is typically very powerful, as it threatens the opponent's unadvanced pawns and hems in the enemy king. A rook on the seventh rank is often considered sufficient compensation for a pawn . In the diagrammed position from a game between Lev Polugaevsky and Larry Evans, the rook on the seventh rank enables White to draw, despite being a pawn down .
"Two" rooks on the seventh rank are often enough to force victory, or at least a draw by perpetual check.
Endgame.
Rooks are most powerful towards the end of a game (i.e., the endgame), when they can move unobstructed by pawns and control large numbers of squares. They are somewhat clumsy at restraining enemy pawns from advancing towards promotion, unless they can occupy the file behind the advancing pawn. By the same token, a rook best supports a friendly pawn towards promotion from behind it on the same file (see Tarrasch rule).
In a position with a rook and one or two minor pieces versus two rooks, generally in addition to pawns, and possibly other pieces – Lev Alburt advises that the player with the single rook should avoid exchanging the rook for one of his opponent's rooks .
The rook is a very powerful piece to deliver checkmate. Below are a few examples of rook checkmates that are easy to force.
Heraldry.
 Chess rooks frequently occur as heraldic charges. Heraldic rooks are usually shown as they looked in medieval chess-sets, with the usual battlements replaced by two outward-curving horns. They occur in arms from around the 13th century onwards.
In Canadian heraldry, the chess rook is the cadency mark of a fifth daughter.
Unicode.
Unicode defines two codepoints for rook:
♖ U+2656 White Chess Rook (HTML &#9814;)
♜ U+265C Black Chess Rook (HTML &#9820;)

</doc>
<doc id="47616" url="http://en.wikipedia.org/wiki?curid=47616" title="Bishop (chess)">
Bishop (chess)

A bishop (♗,♝) is a piece in the board game of chess. Each player begins the game with two bishops. One starts between the king's knight and the king, the other between the queen's knight and the queen. In algebraic notation the starting squares are c1 and f1 for White's bishops, and c8 and f8 for Black's bishops.
Movement.
The bishop has no restrictions in distance for each move, but is limited to diagonal movement. Bishops, like all other pieces except the knight, cannot jump over other pieces. A bishop captures by occupying the square on which an enemy piece sits.
The bishops may be differentiated according to which wing they begin on, i.e. the "king's bishop" and "queen's bishop". As a consequence of its diagonal movement, each bishop always remains on either the white or black squares, and so it is also common to refer to them as "light-squared" or "dark-squared" bishops.
History.
The bishop's predecessor in shatranj (medieval chess) was the alfil, meaning "elephant," which could leap two squares along any diagonal, and could jump over an intervening piece. As a consequence, each alfil was restricted to eight squares, and no alfil could attack another. The modern bishop first appeared shortly after 1200 in Courier chess. A piece with this move, called a "cocatriz" or crocodile, is part of the Grande Acedrex in the game book compiled in 1283 for King Alfonso X of Castile. The game is attributed to "India", then a very vague term. About half a century later Muḥammad ibn Maḥmud al-Āmulī, in his "Treasury of the Sciences", describes an expanded form of chess with two pieces moving "like the rook but obliquely".
Derivatives of "alfil" survive in the languages of the two countries where chess were first introduced within Western Europe—Italian ("alfiere") and Spanish ("alfil"). It was known as the "aufin" in French, or the aufin, alphin, or archer in early English.
The term "bishop" first entered the English language in the 16th century, with the first known written example dating back to 1560s. In all other Germanic languages, except for Icelandic, it is called various names, all of which directly translate to English as "runner" or "messenger" (e.g. in Norwegian "Løper", in Danish "Løber", in Swedish "Löpare", in German "Läufer" and in Dutch "loper"; in Finnish, the word is "lähetti", with the same meaning). In Romanian, it is known as "nebun" which refers to a crazy person (similarly to the French name "Fou" (fool) which is most likely derived from "Fou du roi", a jester). In Icelandic, however, it is called "biskup", with the same meaning as in English. Interestingly, the use of the term in Icelandic predates that of the English language, as the first mentioning of "biskup" in Icelandic texts dates back to the early part of the 14th century, while the 12th-century Lewis Chessmen portray the bishop as an unambiguously ecclesiastical figure. In The Saga of Earl Mágus, which was written in Iceland somewhere between 1300–1325, it is described how an emperor was checkmated by a bishop. This has led to some speculations as to the origin of the English use of the term "bishop".
The canonical chessmen date back to the Staunton chess set of 1849. The piece's deep groove symbolizes a bishop's (or abbot's) mitre. Some have written that the groove originated from the original form of the piece, an elephant with the groove representing the elephant's tusks (see photo in the history section). The British chose to call the piece a bishop because the projections at the top resembled a mitre. This groove was interpreted differently in different countries as the game moved to Europe; in France, for example, the groove was taken to be a jester's cap, hence in France the bishop is called "fou" (the jester; the word can also mean madman or gannet).
In some Slavic languages (e.g. Czech/Slovak) the bishop is called "střelec/strelec", which directly translates to English as a "shooter" meaning an archer, while in others it is still known as "elephant" (e.g. Russian "slon"). In South Slavic languages it is usually known as "lovac", meaning "hunter", or "laufer", taken from the German name for the same piece. An alternative name for bishop in Russian is officer (Russian: офицер).
Comparison to other pieces.
Versus rook.
A rook is generally worth about two pawns more than a bishop (see Chess piece relative value and the exchange). The bishop has access to only half of the squares on the board, whereas all squares of the board are accessible to the rook. On an empty board, a rook always attacks fourteen squares, whereas a bishop attacks no more than thirteen and sometimes as few as seven, depending on how near it is to the center. Also, a king and rook can force checkmate against a lone king, while a king and bishop cannot.
Versus knight.
In general bishops are approximately equal in strength to knights, but depending on the game situation either may have a distinct advantage.
Less experienced players tend to underrate the bishop compared to the knight because the knight can reach all squares and is more adept at forking. More experienced players understand the power of the bishop .
Bishops usually gain in relative strength towards the endgame as more pieces are captured and more open lines become available on which they can operate. A bishop can easily influence both wings simultaneously, whereas a knight is less capable of doing so. In an open endgame, a pair of bishops is decidedly superior to either a bishop and a knight, or two knights. A player possessing a pair of bishops has a strategic weapon in the form of a long-term threat to trade down to an advantageous endgame.
Two bishops vs King can force checkmate, whereas two knights cannot. A bishop and knight can force mate, but with far greater difficulty than two bishops. 
In certain positions a bishop can by itself lose a move (see triangulation and tempo), while a knight can never do so. The bishop is capable of skewering or pinning a piece, while the knight can do neither. A bishop can in some situations hinder a knight from moving. In these situations, the bishop is said to be "dominating" the knight.
On the other hand, in the opening and middle game a bishop may be hemmed in by pawns of both players, and thus be inferior to a knight which can hop over them. Furthermore, on a crowded board a knight has many tactical opportunities to fork two enemy pieces. A bishop can fork, but opportunities are more rare. One such example occurs in the position at right, which arises from the Ruy Lopez: 1.e4 e5 2.Nf3 Nc6 3.Bb5 a6 4.Ba4 Nf6 5.0-0 b5 6.Bb3 Be7?! 7.d4 d6 8.c3 Bg4 9.h3!? Bxf3 10.Qxf3 exd4 11.Qg3 g6 12.Bh6!
Game use.
Good bishop and bad bishop.
In the middle game, a player with only one bishop should generally place his pawns on squares of the color that the bishop cannot move to. This allows the player to control squares of both colors, allows the bishop to move freely among the pawns, and helps fix enemy pawns on squares on which they can be attacked by the bishop. Such a bishop is often referred to as a "good" bishop.
Conversely, a bishop which is impeded by friendly pawns is often referred to as a "bad bishop" (or sometimes, disparagingly, a "tall pawn"). However, a "bad" bishop need not always be a weakness, especially if it is outside its own pawn chains. In addition, having a "bad" bishop may be advantageous in an opposite-colored bishops endgame. Even if the bad bishop is passively placed, it may serve a useful defensive function; a well-known quip from GM Mihai Suba is that "Bad bishops protect good pawns."
In the position from the game Krasenkow versus Zvjaginsev, a thicket of black pawns hems in Black's bishop on c8, so Black is effectively playing with one piece fewer than White. Although the black pawns also obstruct the white bishop on e2, it has many more attacking possibilities, and thus is a good bishop vis-à-vis Black's bad bishop. Black resigned after another ten moves.
Fianchetto.
A bishop may be "fianchettoed", for example after moving the g2 pawn to g3 and the bishop on f1 to g2. This can form a strong defense for the castled king on g1 and the bishop can often exert strong pressure on the long diagonal (here h1-a8). A fianchettoed bishop should generally not be given up lightly, since the resulting holes in the pawn formation may prove to be serious weaknesses, particularly if the king has castled on that side of the board.
There are nonetheless some modern opening lines where a fianchettoed bishop is given up for a knight in order to double the opponent's pawns, for example 1.d4 g6 2.c4 Bg7 3.Nc3 c5 4.d5 Bxc3+!? 5.bxc3 f5, a sharp line originated by Roman Dzindzichashvili. Giving up a fianchettoed queen bishop for a knight is usually less problematic. For example, in Karpov–Browne, San Antonio 1972, after 1.c4 c5 2.b3 Nf6 3.Bb2 g6?!, Karpov gave up his fianchettoed bishop with 4.Bxf6! exf6 5.Nc3, doubling Black's pawns and giving him a hole on d5.
Endgame.
An endgame in which each player has only one bishop, one controlling the dark squares and the other the light, will often result in a draw even if one player has a pawn or sometimes two more than the other. The players tend to gain control of squares of opposite colors, and a deadlock results. In endgames with same-colored bishops, however, even a positional advantage may be enough to win .
Bishops on opposite colors.
Endgames in which each player has only one bishop (and no other pieces besides the king) and the bishops are on opposite colors are often drawn, even when one side has an extra pawn or two. Many of these positions would be a win if the bishops were on the same color.
The position from Wolf versus Leonhardt (see diagram), shows an important defensive setup. Black can make no progress, since the white bishop ties the black king to defending the pawn on "g4" and it also prevents the advance  ...f3+ because it would simply capture the pawn – then either the other pawn is exchanged for the bishop (an immediate draw) or the pawn advances (an easily drawn position). Otherwise the bishop alternates between the squares "d1" and "e2" .
If two pawns are connected, they normally win if they reach their sixth rank, otherwise the game may be a draw (as above). If two pawns are separated by one file they usually draw, but win if they are farther apart .
In some cases with more pawns on the board, it is actually advantageous to have the bishops on opposite colors if one side has weak pawns. In the 1925 game of Efim Bogoljubov versus Max Blümich, (see diagram) White wins because of the bishops being on opposite colors making Black weak on the black squares, the weakness of Black's isolated pawns on the queenside, and the weak doubled pawns on the kingside . The game continued
Wrong bishop.
In an endgame with a bishop, in some cases the bishop is the "wrong bishop", meaning that it is on the wrong color of square for some purpose (usually promoting a pawn). For example, with just a bishop and a rook pawn, if the bishop cannot control the promotion square of the pawn, it is said to be the "wrong bishop" or the pawn is said to be the wrong rook pawn. This results in some positions being drawn (by setting up a fortress) which otherwise would be won.
Unicode.
Unicode defines two codepoints for bishop:
♗ U+2657 White Chess Bishop (HTML &#9815;)
♝ U+265D Black Chess Bishop (HTML &#9821;)

</doc>
<doc id="47617" url="http://en.wikipedia.org/wiki?curid=47617" title="Rook">
Rook

Rook or rooks may refer to:

</doc>
<doc id="47618" url="http://en.wikipedia.org/wiki?curid=47618" title="Chess piece">
Chess piece

A chess piece, or chessman, is any of the 32 movable objects deployed on a chessboard used to play the game of chess. In a standard game of chess, each of the two players begins a game with the following 16 pieces:
In playing chess, the players take turns moving one of their own chess pieces. The rules of chess prescribe the types of move a player can make with each type of chess piece.
The pieces that belong to each player are distinguished by color. The lighter colored pieces, and the player that plays them, are referred to as white. The darker colored pieces and their player are referred to as black.
Terminology.
In chess, the word "piece" has three meanings, depending on the context.
The context should make the intended meaning clear
Moves of the pieces.
Each piece type moves in a different way.
Pieces other than pawns capture in the same way that they move. A capturing piece replaces the opponent piece on its square, except for an "en passant" capture. Captured pieces are immediately removed from the game. A square may hold only one piece at any given time. Except for castling and the knight's move, no piece may jump over another piece .
Chess sets.
Table sets.
The variety of designs available is broad, from small cosmetic changes to highly abstract representations, to themed designs such as those that emulate the drawings from the works of Lewis Carroll, or modern treatments such as Star Trek or "The Simpsons". Themed designs are generally intended for display purposes rather than actual play . Some works of art are designs of chess sets, such as the modernist chess set by chess enthusiast and dadaist Man Ray, that is on display in the Museum of Modern Art in New York City.
Chess pieces used for play are usually figurines that are taller than they are wide. For example, a set of pieces designed for a chessboard with 2.25 in squares typically have a king around 3.75 in tall. Chess sets are available in a variety of designs, with the most well-known Staunton design, named after Howard Staunton, a 19th-century English chess player, and designed by Nathaniel Cook. The first Staunton style sets were made in 1849 by Jaques of London (also known as "John Jaques of London" and "Jaques and Son of London") .
Wooden White chess pieces are normally made of a light wood, boxwood, or sometimes maple. Black wooden pieces are made of a dark wood such as rosewood, ebony, red sandalwood, or walnut. Sometimes they are made of boxwood and stained or painted black, brown, or red. Plastic white pieces are made of white or off-white plastic, and plastic black pieces are made of black or red plastic. Sometimes other materials are used, such as bone, ivory, or a composite material .
For actual play, pieces of the Staunton chess set design are standard. The height of the king should be between 3.35 to. United States Chess Federation rules call for a king height between 3.375 and 4.5 inches (86 to 114 mm). A height of about 3.75 to is preferred by most players. The diameter of the king should be 40–50% of its height. The size of the other pieces should be in proportion to the king. The pieces should be well balanced. The length of the sides of the squares of the chessboard should be about 1.25–1.3 times the diameter of the base of the king, or 2 to. Squares of about 2.25 in are normally well suited for pieces with the kings in the preferred size range. These criteria are from the United States Chess Federation's "Official Rules of Chess", which is based on the Fédération Internationale des Échecs rules .
The Grandmaster Larry Evans offered this advice on buying a set :Make sure the one you buy is easy on the eye, felt-based, and heavy (weighted). The men should be constructed so they don't come apart. ... The regulation board used by the U. S. Chess Federation is green and buff—never red and black. However, there are several good inlaid wood boards on the market. ... Avoid cheap equipment. Chess offers a lifetime of enjoyment for just a few dollars well spent at the outset.
Pocket and travel sets.
Some small magnetic sets, designed to be compact and/or for travel, have pieces more like those used in shogi and xiangqi – each piece being a similar flat token, with a symbol printed on it to identify the piece type.
Computer images.
On computers, chess pieces are often 2D symbols on a 2D board, although some programs have 3D graphics engines with more traditional designs of chess pieces.
Unicode contains symbols for chess pieces in both white and black.
Relative value.
The value assigned to a piece attempts to represent the potential strength of the piece in the game. As the game develops, the relative values of the pieces will also change. A bishop positioned to control long, open diagonal spaces is usually more valuable than a knight stuck in a corner. Similar ideas apply to placing rooks on open files and knights on active, central squares. The standard valuation is one point for a pawn, three points for a knight or bishop, five points for a rook, and nine points for a queen . These values are general throughout a game; in specific circumstances the values may be quite different—a knight can be more valuable than a queen in a particular decisive attack.

</doc>
<doc id="47619" url="http://en.wikipedia.org/wiki?curid=47619" title="Knight (chess)">
Knight (chess)

The knight (♘ ♞) is a piece in the game of chess, representing a knight (armored cavalry). It is normally represented by a horse's head and neck. Each player starts with two knights, which begin on the row closest to the player, one square from each corner. 
Movement.
The knight move is unusual among chess pieces. When it moves, it can move to a square that is two squares horizontally and one square vertically, or two squares vertically and one square horizontally. The complete move therefore looks like the letter "L". Unlike all other standard chess pieces, the knight can 'jump over' all other pieces (of either color) to its destination square. It captures an enemy piece by replacing it on its square. The knight's ability to "jump over" other pieces means it tends to be at its most powerful in closed positions, in contrast to that of a bishop. The move is one of the longest-surviving moves in chess, having remained unchanged since before the seventh century. Because of this it also appears in most chess-related regional games. The knight moves alternately to light and dark squares.
A knight should always be close to where the action is, meaning it is best used on areas of the board where the opponent's pieces are clustered or close together. Pieces are generally more powerful if placed near the center of the board, but this is particularly true for a knight. A knight on the edge of the board attacks only three or four squares (depending on its exact location) and a knight in the corner only two. Moreover, it takes more moves for an uncentralized knight to switch operation to the opposite side of the board than an uncentralized bishop, rook, or queen. The mnemonic phrases "A knight on the rim is grim" or "A knight on the rim is dim" are often used in chess instruction to reflect this principle.
The knight is the only piece that can move at the beginning of the game without first moving a pawn. For the reasons above, the best square for the initial move of each knight is usually one towards the center. Knights are usually brought into play slightly sooner than the bishops and much sooner than the rooks and the queen.
Because of its move pattern, the knight is especially well-suited for executing a fork.
In the numbered diagram, the numbers represent how many moves it takes for a knight to reach each square on the chessboard from its location on the f5-square. 
Value.
A knight is approximately equal in strength and value to a bishop. The bishop has longer range, but is restricted to only half the squares on the board. Since the knight can jump over pieces which obstruct other pieces, it is usually more valuable when the board is more crowded (closed positions). A knight is best when it has a 'support point' or outpost – a relatively sheltered square where it can be positioned to exert its strength remotely. On the fourth rank a knight is comparable in power to a bishop, and on the fifth it is often superior to the bishop, and on the sixth rank it can be a decisive advantage. This is assuming the knight is taking part in the action; a knight on the sixth rank which is not doing anything useful is not a well-placed piece.
Properties.
Enemy pawns are very effective at harassing knights because a pawn attacking a knight is not itself attacked by the knight. For this reason, a knight is most effective when placed in a weakness in the opponent's pawn structure, i.e. a square which cannot be attacked by enemy pawns. In the diagram at right, White's knight on d5 is very powerful – more powerful than Black's bishop on g7.
Whereas two bishops cover each other's weaknesses, two knights tend not to cooperate with each other as efficiently. As such, a pair of bishops is usually considered better than a pair of knights . World Champion José Raúl Capablanca considered that a queen and a knight is usually a better combination than a queen and a bishop. However, Glenn Flear found no game of Capablanca's that supported his statement and statistics do not support the statement either . In an endgame without other pieces or pawns, two knights generally have a better chance against a queen than two bishops or a bishop and a knight would (see fortress (chess)).
Compared to a bishop, a knight is often not as good in an endgame. The knight's potential range of movement is more limited, which often makes it less suitable in endgames with pawns on both sides of the board. However, this limitation is less important in endgames with pawns on only one side of the board. Knights are superior to bishops in an endgame if all the pawns are on one side of the board. Furthermore, knights have the advantage of being able to control squares of either color, unlike a lone bishop.
Nonetheless, a disadvantage of the knight (compared to the other pieces) is that by itself it cannot lose a move to put the opponent in zugzwang (see triangulation and tempo), while a bishop can. In this position, if the knight is on a white square and it is White's turn to move, White cannot win. Similarly, if the knight was on a black square and it was Black's turn to move, White cannot win. In the other two cases, White would win. If instead of the knight, White had a bishop on either color of square, White would win with either side to move .
At the end of the game, if one side has only a king and a knight while the other side has only a king, the game is a draw since a checkmate is impossible. When a bare king faces a king and two knights, checkmate can occur only if the opponent commits a blunder by moving his king to a square where it may be checkmated on the next move. Otherwise, a checkmate can never be forced. However checkmate can be forced with a bishop and knight, or with two bishops, even though the bishop and knight are in general about equal in value. Paradoxically, checkmate with two knights sometimes can be forced if the weaker side has a single extra pawn, but this is a curiosity of little practical value (see two knights endgame). Pawnless endings are a rarity, and if the stronger side has even a single pawn, an extra knight should give him an easy win. A bishop can trap (although it cannot then capture) a knight on the rim (diagram), especially in the endgame.
Notation.
In algebraic notation, the usual modern way of recording chess games, the letter "N" stands for the knight ("K" is reserved for the king); in descriptive chess notation, "Kt" is sometimes used instead, mainly in older literature. In chess problems and endgame studies, the letter "S", standing for the German name for the piece, "Springer", is often used (and in some variants of fairy chess "N" is used for the popular fairy chess piece, the nightrider).
Unicode.
Unicode defines two codepoints for knight:
♘ U+2658 White Chess Knight (HTML &#9816;)
♞ U+265E Black Chess Knight (HTML &#9822;)

</doc>
<doc id="47620" url="http://en.wikipedia.org/wiki?curid=47620" title="Henry Clay">
Henry Clay

Henry Clay, Sr. (April 12, 1777 – June 29, 1852) was an American lawyer, politician, and skilled orator who represented Kentucky in both the United States Senate and House of Representatives. He served three different terms as Speaker of the House of Representatives and was also Secretary of State from 1825 to 1829. He lost his campaigns for president in 1824, 1832 and 1844.
Clay was a very
dominant figure in both the First and Second Party systems. As a leading war hawk in 1812, he favored war with Britain and played a significant role in leading the nation to war in the War of 1812. In 1824 he ran for president and lost, but maneuvered House voting in favor of John Quincy Adams, who made him secretary of state as the Jacksonians denounced what they considered a "corrupt bargain." He ran and lost again in 1832 and 1844 as the candidate of the Whig Party, which he founded and usually dominated. Clay was the foremost proponent of the American System, fighting for an increase in tariffs to foster industry in the United States, the use of federal funding to build and maintain infrastructure, and a strong national bank. He opposed the annexation of Texas, fearing it would inject the slavery issue into politics. Clay also opposed the Mexican-American War and the "Manifest Destiny" policy of Democrats, which cost him votes in the close 1844 election.
Dubbed the "Great Pacificator," Clay brokered important compromises during the Nullification Crisis and on the slavery issue. As part of the "Great Triumvirate" or "Immortal Trio," along with his colleagues Daniel Webster and John C. Calhoun, he was instrumental in formulating the Missouri Compromise of 1820 and the Compromise of 1850. He was viewed as the primary representative of Western interests in this group, and was given the names "Henry of the West" and "The Western Star." A plantation owner, Clay held slaves during his lifetime but freed them in his will.
Abraham Lincoln, the Whig leader in Illinois, was a great admirer of Clay, saying he was "my ideal of a great man." Lincoln wholeheartedly supported Clay's economic programs. In 1957, a Senate Committee selected Clay as one of the five greatest U.S. Senators, along with Daniel Webster, John C. Calhoun, Robert La Follette, and Robert A. Taft.
Early life and education.
Childhood.
Henry Clay was born on April 12, 1777, at the Clay homestead in Hanover County, Virginia, in a story-and-a-half frame house. It was an above-average home for a "common" Virginia planter of that time. At the time of his death, Clay's father owned more than 22 slaves, making him part of the planter class in Virginia (those men who owned 20 or more slaves).
Henry was the seventh of nine children of the Reverend John Clay and Elizabeth (née Hudson) Clay. His father, a Baptist minister nicknamed "Sir John," died four years after the boy's birth (1781). The father left Henry and his brothers two slaves each, and his wife 18 slaves and 464 acre of land. Henry Clay was a second cousin of Cassius Marcellus Clay, who became a politician and an abolitionist in Kentucky.
The widow Elizabeth Clay married Capt. Henry Watkins, who was an affectionate stepfather. Henry Watkins moved the family to Richmond, Virginia. Elizabeth had seven more children with Watkins, bearing a total of sixteen.
Education.
His stepfather secured Clay employment in the office of the Virginia Court of Chancery, where the youth displayed an aptitude for law. There he became friends with George Wythe. Hampered by a crippled hand, Wythe chose Clay as his secretary. After Clay was employed as Wythe's amanuensis for four years, the chancellor took an active interest in Clay's future; he arranged a position for him with the Virginia attorney general, Robert Brooke. Clay read law by working and studying with Wythe, Chancellor of the Commonwealth of Virginia (also a mentor to Thomas Jefferson and John Marshall, among others), and Brooke. Clay was admitted to the bar to practice law in 1797.
Marriage and family.
After beginning his law career, on April 11, 1799, Clay married Lucretia Hart at the Hart home in Lexington, Kentucky. She was a sister to Captain Nathaniel G. S. Hart, who died in the Massacre of the River Raisin in the War of 1812.
Clay and his wife had eleven children (six daughters and five sons): Henrietta (1800–1801), Theodore (1802–1870), Thomas (1803–1871), Susan (1805–1825), Anne (1807–1835), Lucretia (1809–1823), Henry, Jr. (1811–1847), Eliza (1813–1825), Laura (1815–1817), James Brown Clay (1817–1864), and John Morrison Clay (1821–1887).
Seven of Clay's children died before him. By 1835 all six daughters had died of varying causes, two when very young, two as children, the other two as young women: from whooping cough, yellow fever, and complications of childbirth. Henry Clay, Jr. was killed at the Battle of Buena Vista during the Mexican-American War.
Lucretia Hart Clay died in 1864 at the age of 83. She is interred with her husband in the vault of his monument at the Lexington Cemetery. Henry and Lucretia Clay were great-grandparents of the suffragette Madeline McDowell Breckinridge.
Early law and political career.
Legal career.
In November 1797, Clay relocated to Lexington, Kentucky, the growing town near where his family then resided in Woodford County. He soon established a reputation for his legal skills and courtroom oratory. Some of his clients paid him with horses and others with land. Clay came to own town lots and the Kentucky Hotel.
By 1812, Clay owned a productive 600 acre plantation, which he called "Ashland," and numerous slaves to work the land. He held 60 slaves at the peak of operations, and likely produced tobacco and hemp, the two chief commodity crops of the Bluegrass Region.
One of Clay's clients was his father-in-law, Colonel Thomas Hart, an early settler of Kentucky and a prominent businessman. Clay's most notable client was Aaron Burr in 1806, after the US District Attorney Joseph Hamilton Daveiss indicted him for planning an expedition into Spanish Territory west of the Mississippi River. Clay and his law partner John Allen successfully defended Burr. Some years later Thomas Jefferson convinced Clay that Daveiss had been right in his charges. Clay was so upset that many years later, when he met Burr again, Clay refused to shake his hand.
State legislator.
In 1803, although not old enough to be elected, Clay was appointed a representative of Fayette County in the Kentucky General Assembly. As a legislator, Clay advocated a liberal interpretation of the state's constitution and initially the gradual emancipation of slavery in Kentucky, although the political realities of the time forced him to abandon that position. Clay also advocated moving the state capitol from Frankfort to Lexington. He defended the Kentucky Insurance Company, which he saved from an attempt in 1804 by Felix Grundy to repeal its monopolistic charter.
First Senate appointment and eligibility.
Clay's influence in Kentucky state politics was such that in 1806 the Kentucky legislature elected him to the Senate seat of John Breckinridge. He had resigned when appointed as US Attorney General. The legislature first chose John Adair to complete Breckinridge's term, but he had to resign over his alleged role in the Burr Conspiracy. On December 29, 1806, Clay was sworn in as senator, serving for slightly more than two months that first time.
When elected by the legislature, Clay was below the constitutionally required age of thirty. His age did not appear to have been noticed by any other Senator, and perhaps not by Clay. His term ended before his thirtieth birthday. Such an age qualification issue has occurred with only two other U.S. Senators, Armistead Thomson Mason (aged 28 in 1816), and John Eaton (aged 28 in 1818). Such an occurrence, however, has not been repeated since. In 1934, Rush D. Holt, Sr. was elected to the Senate at the age of 29; he waited until he turned 30 (on the following June 19) to take the oath of office. In November 1972, Joe Biden was elected to the Senate at the age of 29, but he reached his 30th birthday before the swearing-in ceremony for incoming senators in January 1973.
Speaker of the State House and duel with Humphrey Marshall.
When Clay returned to Kentucky in 1807, he was elected the Speaker of the state House of Representatives. On January 3, 1809, Clay introduced a resolution to require members to wear homespun suits rather than those made of imported British broadcloth. Two members voted against the measure. One was Humphrey Marshall, an "aristocratic lawyer who possessed a sarcastic tongue," who had been hostile toward Clay in 1806 during the trial of Aaron Burr.
On January 4, 1809 Clay and Marshall nearly came to blows on the Assembly floor and Clay challenged Marshall to a duel, which then took place on January 19. Apparently to keep any possible blood from being spilled in their home state of Kentucky, the chosen dueling ground was in Indiana, directly across the Ohio River from what was then Shippingport, Kentucky and also near the mouth of Silver Creek.
They each had three turns. Clay grazed Marshall once, just below the chest. Marshall hit Clay once in the thigh.
Second Senate appointment.
In 1810, United States Senator Buckner Thruston resigned to serve as a judge on the United States Circuit Court, and Clay was again selected to fill his seat.
Speaker of the House.
Early years.
In the summer of 1811, Clay was elected to the United States House of Representatives. He was chosen Speaker of the House on the first day of his first session, something never done before or since (except for the first ever session of congress back in 1789). During the fourteen years following his first election, he was re-elected five times to the House and to the speakership. Like other Southern Congressmen, Clay took slaves to Washington, DC to work in his household. They included Aaron and Charlotte Dupuy, their son Charles and daughter Mary Ann.
Before Clay's election as Speaker of the House, the position had been that of a rule enforcer and mediator. Clay made the position one of political power second only to the President of the United States. He immediately appointed members of the War Hawk faction (of which he was the "guiding spirit") to all the important committees, effectively giving him control of the House. This was a singular achievement for a 34-year-old House freshman. During his early House service, Clay strongly opposed the creation of a National Bank, in part because of his personal ownership in several small banks in his hometown of Lexington. Later he changed his position and, when he was seeking the presidency, gave strong support for the Second Bank of the United States.
The War Hawks, mostly from the South and the West, resented British violations of United States (US) maritime rights and its treatment of US sailors; they feared British designs on US territory in the Old Northwest. They advocated a declaration of war against the British. As the Congressional leader of the Democratic-Republican Party, Clay took charge of the agenda, especially as a "War Hawk" supporting the War of 1812 against the British Empire. Later, as one of the peace commissioners, Clay helped negotiate the Treaty of Ghent and signed it on December 24, 1814. In 1815, while still in Europe, he helped negotiate a commerce treaty with Great Britain.
Henry Clay helped establish and became president in 1816 of the American Colonization Society, a group that wanted to establish a colony for free American blacks in Africa; it founded Monrovia, in what became Liberia, for that purpose. The group was made up of both abolitionists from the North, who wanted to end slavery, and slaveholders, who wanted to deport free blacks to reduce what they considered a threat to the stability of slave society. On the "amalgamation" of the black and white races, Clay said that "The God of Nature, by the differences of color and physical constitution, has decreed against it." Clay presided at the founding meeting of the ACS on December 21, 1816, at the Davis Hotel in Washington, D.C. Attendees included Robert Finley, James Monroe, Bushrod Washington, Andrew Jackson, Francis Scott Key, and Daniel Webster.
The "American System".
Henry Clay and John C. Calhoun helped to pass the Tariff of 1816 as part of the national economic plan Clay called "The American System," rooted in Alexander Hamilton's American School. Described later by Friedrich List, it was designed to allow the fledgling American manufacturing sector, largely centered on the eastern seaboard, to compete with British manufacturing through the creation of tariffs.
After the conclusion of the War of 1812, British factories were overwhelming American ports with inexpensive goods. To persuade voters in the western states to support the tariff, Clay advocated federal government support for internal improvements to infrastructure, principally roads and canals. These internal improvements would be financed by the tariff and by sale of the public lands, prices for which would be kept high to generate revenue. Finally, a national bank would stabilize the currency and serve as the nexus of a truly national financial system.
Clay's American System ran into strong opposition from President Jackson's administration. One of the most important points of contention between the two men was over the Maysville Road. Jackson vetoed a bill which would authorize federal funding for a project to construct a road linking Lexington and the Ohio River, the entirety of which would be in the state of Kentucky, because he felt that it did not constitute interstate commerce, as specified in the Commerce Clause of the United States Constitution.
Foreign policy.
In foreign policy, Clay was the leading American supporter of independence movements and revolutions in Latin America after 1817. Between 1821 and 1826, the U.S. recognized all the new countries, except Uruguay (whose independence was debated and recognized only later). When in 1826 the U.S. was invited to attend the Columbia Conference of new nations, opposition emerged, and the American delegation never arrived. Clay supported the Greek independence revolutionaries in 1824 who wished to separate from the Ottoman Empire, an early move into European affairs.
The Missouri Compromise and 1820s.
In 1820 a dispute erupted over the extension of slavery in Missouri Territory. Clay helped settle this dispute by gaining Congressional approval for a plan called the "Missouri Compromise". It brought in Maine as a free state and Missouri as a slave state (thus maintaining the balance in the Senate, which had included 11 free and 11 slave states), and it forbade slavery north of 36° 30' (the northern boundary of Arkansas and the latitude line) except in Missouri.
Presidential Election of 1824 and Secretary of State.
By 1824, the unparalleled success of the Democratic-Republican Party had driven all other parties from the field. Four major candidates, including Clay, sought the office of president. Because of the unusually large number of candidates receiving electoral votes, no candidate secured a majority of votes in the electoral college. According to the terms of the Twelfth Amendment to the United States Constitution, the top three electoral vote-getters advanced to the runoff in the House of Representatives. Having finished fourth, Clay was eliminated from contention; the top three were Andrew Jackson, John Quincy Adams and William H. Crawford. Clay, who was Speaker of the House, supported Adams, and his endorsement ultimately secured Adams' win in the House.
Clay used his political clout to secure the victory for Adams, who he felt would be both more sympathetic to Clay's political views and more likely to appoint Clay to a cabinet position. When Clay was appointed Secretary of State, his maneuver was called a "corrupt bargain" by many of Jackson's supporters and tarnished Clay's reputation.
Slave freedom suit.
As Secretary of State, Clay lived with his family and slaves in Decatur House on Lafayette Square. As he was preparing to return to Lexington in 1829, his slave Charlotte Dupuy sued Clay for her freedom and that of her two children, based on a promise by an earlier owner. Her legal challenge to slavery preceded the more famous Dred Scott case by 27 years. The "freedom suit" received a fair amount of attention in the press at the time. Dupuy's attorney gained an order from the court for her to remain in DC until the case was settled, and she worked for wages for 18 months for Martin Van Buren, the successor to Secretary of State and the Decatur House. Clay returned to Ashland with Aaron, Charles and Mary Ann Dupuy.
The jury ruled against Dupuy, deciding that any agreement with her previous master Condon did not bear on Clay. Because Dupuy refused to return voluntarily to Kentucky, Clay had his agent arrest her. She was imprisoned in Alexandria, Virginia, before Clay arranged for her transport to New Orleans, where he placed her with his daughter and son-in-law Martin Duralde. Mary Ann Dupuy was sent to join her mother, and they worked as domestic slaves for the Duraldes for another decade.
In 1840 Henry Clay finally gave Charlotte and her daughter Mary Ann Dupuy their freedom. He kept her son Charles Dupuy as a personal servant, frequently citing him as an example of how well he treated his slaves. Clay granted Charles Dupuy his freedom in 1844. While no deed of emancipation has been found for Aaron Dupuy, in 1860 he and Charlotte were living together as free black residents in Fayette County, Kentucky. He may have been freed or "given his time" by one of Clay's sons, as Dupuy continued to work at Ashland, for pay.
Decatur House in Washington, DC, a National Historic Landmark and museum on Lafayette Square near the White House, has exhibits on urban slavery and Charlotte Dupuy's freedom suit against Henry Clay.
Senate career.
The Nullification Crisis.
After the passage of the Tariff of 1828, dubbed the "tariff of abominations" which raised tariffs considerably in an attempt to protect fledgling factories built under previous tariff legislation, South Carolina declared its right to nullify federal tariff legislation and stopped assessing the tariff on imports. It threatened to secede from the Union if the Federal government tried to enforce the tariff laws. Furious, President Jackson threatened to lead an army to South Carolina and hang any man who refused to obey the law.
The crisis worsened until 1833. Clay was by that time a U.S. Senator again, having been re-elected by Kentucky in 1831. His return to the U.S. Senate, after 20 years, 8 months, 7 days out of office, marks the fourth longest gap in service to the chamber in history.
In 1833, Clay helped to broker a deal in Congress to lower the tariff gradually. This measure helped to preserve the supremacy of the Federal government over the states, but the crisis was indicative of the developing conflict between the northern and southern United States over economics and slavery.
Opposition to Jackson and creation of Whig Party.
After the election of Andrew Jackson, Clay led the opposition to Jackson's policies. His supporters included the National Republicans, who were beginning to identify as "Whigs" in honor of ancestors during the Revolutionary War. They opposed the "tyranny" of Jackson, as their ancestors had opposed the tyranny of King George III. Clay strongly opposed Jackson's refusal to renew the charter of the Second Bank of the United States, and advocated passage of a resolution to censure Jackson for his actions.
In 1832 the National Republicans unanimously nominated Clay for the presidency, while the Democrats nominated the sitting President Jackson. The main issue was the policy of continuing the Second Bank of the United States. Clay lost by a wide margin to the highly popular Jackson (55% to 37%).
In 1840, Clay was a candidate for the Whig nomination, but he was defeated at the party convention by supporters of war hero William Henry Harrison. Harrison was chosen because his war record was attractive, and he was seen as more likely to win than Clay.
In 1844, Clay was nominated by the Whigs against James K. Polk, the Democratic candidate. Polk won by 170 to 105 electoral votes, carrying 15 of the 26 states. Polk's populist stances on territorial expansion figured prominently—particularly his opinion on US control over the entire Oregon Country and his support for the annexation of Texas. Clay opposed annexing Texas on the grounds that it would once again bring the issue of slavery to the forefront of the nation's political dialog and would draw the ire of Mexico, from which Texas had declared its independence in 1836. Despite Polk's populism, the election was close; New York's 36 electoral votes proved the difference, and went to Polk by a slim 5,000 vote margin. Liberty Party candidate James G. Birney won slightly more than 15,000 votes in New York and likely attracted votes that might have gone to Clay. His warnings about Texas proved prescient. The US annexation of Texas led to the Mexican-American War (1846–1848) (in which his namesake son died). The North and South came to increased tensions during Polk's Presidency over the extension of slavery into Texas and beyond.
The Compromise of 1850.
After losing the Whig Party nomination to Zachary Taylor in 1848, Clay decided to retire to his Ashland estate in Kentucky. Retired for less than a year, he was in 1849 again elected to the U.S. Senate from Kentucky. During his term, the controversy over the expansion of slavery in new lands had reemerged with the addition of the lands ceded to the United States by Mexico in the Treaty of Guadalupe Hidalgo at the conclusion of the Mexican-American War. David Wilmot, a Northern congressman, had proposed preventing the extension of slavery into any of the new territory in a proposal referred to as the "Wilmot Proviso".
On January 29, 1850, Clay proposed a series of resolutions, which he considered to reconcile Northern and Southern interests, what would widely be called the Compromise of 1850. Clay originally intended the resolutions to be voted on separately, but at the urging of southerners he agreed to the creation of a Committee of Thirteen to consider the measures. The committee was formed on April 17. On May 8, as chair of the committee, Clay presented an omnibus bill linking all of the resolutions. The resolutions included:
The Omnibus bill, despite Clay's efforts, failed in a crucial vote on July 31 with the majority of his Whig Party opposed. He announced on the Senate floor the next day that he intended to persevere and pass each individual part of the bill. Clay was physically exhausted; the tuberculosis that would eventually kill him began to take its toll. Clay left the Senate to recuperate in Newport, Rhode Island. Stephen A. Douglas separated the bills and guided them through the Senate.
Clay was given much of the credit for the Compromise's success. It quieted the controversy between Northerners and Southerners over the expansion of slavery, and delayed secession and civil war for another decade. Senator Henry S. Foote of Mississippi, who had suggested the creation of the Committee of Thirteen, later said, "Had there been one such man in the Congress of the United States as Henry Clay in 1860–'61 there would, I feel sure, have been no civil war."
Death and estate.
Clay continued to serve both the Union he loved and his home state of Kentucky. On June 29, 1852, he died of tuberculosis in Washington, D.C., at the age of 75. Clay was the first person to lie in state in the United States Capitol.
He was buried in Lexington Cemetery, and Theodore Frelinghuysen, Clay's vice-presidential candidate in the election of 1844, gave the eulogy. Clay's headstone reads: "I know no North — no South — no East — no West." Even though the 1852 pro-slavery novel "Life at the South; or, "Uncle Tom's Cabin" As It Is", by W.L.G. Smith, is dedicated to his memory, Clay's Will freed all the slaves he held.
Ashland, named for the many ash trees on the property, was Clay's plantation and mansion for many years. He held as many as 60 slaves at the peak of the plantation operations. It was there he introduced the Hereford livestock breed to the United States.
By the time of his death, his only surviving sons were James Brown Clay and John Morrison Clay, who inherited the estate and took portions for use. For several years (1866–1878), James Clay allowed the mansion to be used as a residence for the regent of Kentucky University, forerunner of the University of Kentucky and present-day Transylvania University. Later the mansion and estate were rebuilt and remodeled by later descendants. John Clay designated his portion of the estate as Ashland Stud, which he devoted to breeding thoroughbred horses.
Maintained and operated as a museum, today Ashland includes 17 acre of the original estate grounds. It is located on Richmond Road (US 25) in Lexington. It is open to the public (admission charged).
Henry Clay is credited with introducing the mint julep drink to Washington, D.C., at the Willard Hotel during his residence as a senator in the city.
External links.
class="wikitable succession-box" style="margin:0.5em auto; font-size:95%;clear:both;"

</doc>
<doc id="47621" url="http://en.wikipedia.org/wiki?curid=47621" title="Isabella of France">
Isabella of France

Isabella of France (1295 – 22 August 1358), sometimes described as the She-Wolf of France, was Queen of England as the wife of Edward II. She was the youngest surviving child and only surviving daughter of Philip IV of France and Joan I of Navarre. Queen Isabella was notable at the time for her beauty, diplomatic skills, and intelligence.
Isabella arrived in England at the age of 12 during a period of growing conflict between the king and the powerful baronial factions. Her new husband was notorious for the patronage he lavished on his favourite, Piers Gaveston, but the queen supported Edward during these early years, forming a working relationship with Piers and using her relationship with the French monarchy to bolster her own authority and power. After the death of Gaveston at the hands of the barons in 1312, however, Edward later turned to a new favourite, Hugh Despenser the younger, and attempted to take revenge on the barons, resulting in the Despenser War and a period of internal repression across England. Isabella could not tolerate Hugh Despenser and by 1325 her marriage to Edward was at a breaking point.
Travelling to France under the guise of a diplomatic mission, Isabella began an affair with Roger Mortimer, and the two agreed to depose Edward and oust the Despenser family. The Queen returned to England with a small mercenary army in 1326, moving rapidly across England. The King's forces deserted him. Isabella deposed Edward, becoming regent on behalf of her son, Edward III. Many have believed that Isabella then arranged the murder of Edward II. Isabella and Mortimer’s regime began to crumble, partly because of her lavish spending, but also because the Queen successfully, but unpopularly, resolved long-running problems such as the wars with Scotland.
In 1330, Isabella’s son Edward III deposed Mortimer in turn, taking back his authority and executing Isabella’s lover. The Queen was not punished, however, and lived for many years in considerable style—although not at Edward III’s court—until her death in 1358. Isabella became a popular "femme fatale" figure in plays and literature over the years, usually portrayed as a beautiful but cruel, manipulative figure.
Early life and marriage: 1295–1308.
Isabella was born in Paris on an uncertain date – on the basis of the chroniclers and the eventual date of her marriage, she was probably born between May and November 1295. She is described as born in 1292 in the Annals of Wigmore, and Piers Langtoft agrees, claiming that she was 7 years old in 1299. The French chroniclers Guillaume de Nangis and Thomas Walsingham describe her as 12 years old at the time of her marriage in January 1308, placing her birth between the January of 1295 and of 1296. A Papal dispensation by Clement V in November 1305 permitted her immediate marriage by proxy, despite the fact that she was probably only 10 years old. Since she had to reach the canonical age of 7 before her betrothal in May 1303, and that of 12 before her marriage in January 1308, the evidence suggests that she was born between May and November 1295. Her parents were King Philip IV of France and Queen Joan I of Navarre; her brothers Louis, Philip and Charles became kings of France.
Isabella was born into a royal family that ruled the most powerful state in Western Europe. Her father, King Philip, known as "le Bel" (the Fair) because of his good looks, was a strangely unemotional man; contemporaries described him as "neither a man nor a beast, but a statue"; modern historians have noted that he "cultivated a reputation for Christian kingship and showed few weaknesses of the flesh". Philip built up centralised royal power in France, engaging in a sequence of conflicts to expand or consolidate French authority across the region, but remained chronically short of money throughout his reign. Indeed, he appeared almost obsessed about building up wealth and lands, something that his daughter was also accused of in later life. Isabella's mother died when Isabella was still quite young; some contemporaries suspected Philip IV of her murder, albeit probably incorrectly.
Isabella was brought up in and around the Château du Louvre and the "Palais de la Cité" in Paris. Isabella was cared for by Théophania de Saint-Pierre, her nurse, given a good education and taught to read, developing a love of books. As was customary for the period, all of Philip's children were married young for political benefit. Isabella was promised in marriage by her father to King Edward II of England whilst she was still an infant, with the intention to resolve the conflicts between France and England over the latter's continental possession of Gascony and claims to Anjou, Normandy and Aquitaine. Pope Boniface VIII had urged the marriage as early as 1298 but was delayed by wrangling over the terms of the marriage contract. The English king, Edward I attempted to break the engagement several times for political advantage, and only after he died in 1307 did the wedding proceed.
Isabella and Edward were finally married at Boulogne-sur-Mer on 25 January 1308. Isabella's wardrobe gives some indications of her wealth and style – she had gowns of baudekyn, velvet, taffeta and cloth, along with numerous furs; she had over 72 headdresses and coifs; she brought with her two gold crowns, gold and silver dinnerware and 419 yards of linen. At the time of her marriage, Isabella was probably about twelve and was described by Geoffrey of Paris as "the beauty of beauties... in the kingdom if not in all Europe." This description was probably not simply flattery by a chronicler, since both Isabella's father and brothers were considered very handsome men by contemporaries, and her husband was to nickname her "Isabella the Fair". Isabella was said to resemble her father, and not her mother, queen regnant of Navarre, a plump, plain woman. This indicates that Isabella was slender and pale-skinned, although the fashion at the time was for blonde, slightly full-faced women, and Isabella may well have followed this stereotype instead. Throughout her career, Isabella was noted as charming and diplomatic, with a particular skill at convincing people to follow her courses of action. Unusual for the medieval period, contemporaries also commented on her high intelligence.
Queenship.
As queen, the young Isabella faced numerous challenges. Edward was handsome, but highly unconventional, possibly forming close romantic attachments to first Piers Gaveston and then Hugh Despenser the younger. Edward found himself at odds with the barons, too, in particular the Lancastrians under Thomas of Lancaster, whilst continuing the war against the Scots that he had inherited from Edward I. Using her own supporters at court, and the patronage of her French family, Isabella attempted to find a political path through these challenges; she successfully formed an alliance with Gaveston, but after his death at the hands of the barons her position grew increasingly precarious. Edward began to take revenge on his enemies, using an ever more brutal alliance with the Despenser family, in particular his new favourite, Hugh Despenser the younger. By 1326 Isabella found herself at increasing odds with both Edward and Hugh, ultimately resulting in Isabella's own bid for power and the invasion of England.
Fall of Gaveston: 1308–12.
Isabella's new husband Edward was an unusual character by medieval standards. Edward looked the part of a Plantagenet king to perfection. He was tall, athletic, and wildly popular at the beginning of his reign. He rejected most of the traditional pursuits of a king for the period – jousting, hunting and warfare – and instead enjoyed music, poetry and many rural crafts. Furthermore, there is the question of Edward's sexuality. He can be considered to have been bisexual, in a period when homosexuality of any sort was considered a very serious crime, but there is no complicit evidence which comments directly on his sexual orientation. Contemporary chroniclers made much of his close affinity with a succession of male favourites; some condemned Edward for loving them "beyond measure" and "uniquely", others explicitly referring to an "illicit and sinful union". Nonetheless, Isabella bore four children by Edward, leading to an opinion amongst some historians that Edward's affairs with his male favourites may have been platonic.
When Isabella first arrived in England following her marriage, her husband was already in the midst of a relationship with Piers Gaveston, an "arrogant, ostentatious" soldier, with a "reckless and headstrong" personality that clearly appealed to Edward. Isabella, then aged twelve, was effectively sidelined by the pair. Edward chose to sit with Gaveston rather than Isabella at their wedding celebration, causing grave offence to her uncles Louis, Count of Évreux, and Charles, Count of Valois, and then refused to grant her either her own lands or her own household. Edward also gave Isabella's own jewelry to Gaveston, which he wore publicly. It took the intervention of Isabella's father, Philip IV, before Edward began to provide for her more appropriately.
Isabella's relationship with Gaveston was a complex one. Baronial opposition to Gaveston, championed by Thomas of Lancaster, was increasing, and Philip IV began to covertly fund this grouping, using Isabella and her household as intermediaries. Edward was forced to exile Gaveston to Ireland for a period, and began to show Isabella much greater respect and assigning her significant lands and patronage; in turn, Philip ceased his support for the barons. Gaveston eventually returned from Ireland, and by 1309–11 the three seemed to be co-existing together relatively comfortably. Indeed, Gaveston's key enemy, Thomas of Lancaster, considered Isabella to be an ally of Gaveston's. Isabella had begun to build up her own supporters at court, principally the de Beaumont family, itself opposed to the Lancastrians; originating, like her, from France, the senior member of the family, Isabella de Vesci, had been a close confidant of Queen Eleanor; supported by her brother, Henry de Beaumont, Isabella de Vesci became a close friend of Isabella in turn.
During 1311, however, Edward conducted a failed campaign against the Scots, during which Isabella and he only just escaped capture. In the aftermath, the barons rose up, signing the Ordinances of 1311, which promised action against Gaveston and expelled Isabella de Vesci and Henry de Beaumont from court. 1312 saw a descent into civil war against the king and his lover – Isabella stood with Edward, sending angry letters to her uncles d'Évreux and de Valois asking for support. Edward left Isabella, rather against her will, at Tynemouth Priory in Northumberland whilst he unsuccessfully attempted to fight the barons. The campaign was a disaster, and whilst Edward escaped, Gaveston found himself stranded at Scarborough Castle, where his baronial enemies first surrounded and captured him. Guy de Beauchamp and Thomas of Lancaster ensured Gaveston's execution as he was being taken south to rejoin Edward. Isabella's rival for Edward's affections was gone, but the situation in England was deeply unstable.
Tensions grow: 1312–21.
Tensions mounted steadily over the decade. In 1312, Isabella gave birth to the future Edward III, but by the end of the year Edward's court was beginning to change. Edward was still relying upon his French relatives – Isabella's uncle, Louis d'Évreux, for example had been sent from Paris to assist him – but Hugh Despenser the elder now formed part of the inner circle, marking the beginning of the Despensers' increased prominence at Edward's court. The Despensers were opposed to both the Lancastrians and their other allies in the Welsh Marches, making an easy alliance with Edward, who sought revenge for the death of Gaveston.
In 1313, Isabella travelled to Paris with Edward to garner further French support, which resulted in the Tour de Nesle Affair. The journey was a pleasant one, with lots of festivities, although Isabella was injured when her tent burned down. During the visit Louis and Charles had had a satirical puppet show put on for their guests, and after this Isabella had given new embroidered purses both to her brothers and to their wives. Isabella and Edward then returned to England with new assurances of French support against the English barons. Later in the year, however, Isabella and Edward held a large dinner in London to celebrate their return and Isabella apparently noticed that the purses she had given to her sisters-in-law were now being carried by two Norman knights, Gautier and Philippe d'Aunay. Isabella concluded that the pair must have been carrying on an illicit affair, and appears to have informed her father of this during her next visit to France in 1314. The consequence of this was the Tour de Nesle Affair in Paris, which led to legal action against all three of Isabella's sisters-in-law; Blanche and Margaret of Burgundy were imprisoned for life for adultery. Joan of Burgundy was imprisoned for a year. Isabella's reputation in France suffered somewhat as a result of her perceived role in the affair.
In the north, however, the situation was turning worse. Edward attempted to quash the Scots in a fresh campaign in 1314, resulting in the disastrous defeat at the battle of Bannockburn. Edward was blamed by the barons for the catastrophic failure of the campaign. Thomas of Lancaster reacted to the defeats in Scotland by taking increased power in England and turning against Isabella, cutting off funds and harassing her household. To make matters worse, the "Great Famine" descended on England during 1316–7, causing widespread loss of life and financial problems.
Despite Isabella giving birth to her second son, John, in 1316, Edward's position was precarious. Indeed, John Deydras, a royal Pretender, appeared in Oxford, claiming to have been switched with Edward at birth, and to be the real king of England himself. Given Edward's unpopularity, the rumours spread considerably before Deydras' eventual execution, and appear to have greatly upset Isabella. Isabella responded by deepening her alliance with Lancaster's enemy Henry de Beaumont and by taking up an increased role in government herself, including attending council meetings and acquiring increased lands. Henry's sister, Isabella de Vesci, continued to remain a close adviser to the Queen. The Scottish general Sir James Douglas, war leader for Robert I of Scotland, made a bid to capture Isabella personally in 1319, almost capturing her at York – Isabella only just escaped. Suspicions fell on Lancaster, and one of Edward's knights, Edmund Darel, was arrested on charges of having betrayed her location, but the charges were essentially unproven. In 1320, Isabella accompanied Edward to France, to try and convince her brother, Charles IV, to provide fresh support to crush the English barons.
Meanwhile, Hugh de Despenser the younger became an increasing favourite of Isabella's husband, and was widely believed to have begun a sexual relationship with him around this time. Hugh was the same age as Edward. His father, Hugh the elder, had supported Edward and Gaveston a few years previously. The Despensers were bitter enemies of Lancaster, and with Edward's support began to increase their power base in the Welsh Marches, in the process making enemies of Roger Mortimer de Chirk and his nephew, Roger Mortimer of Wigmore, their rival Marcher lords. Whilst Isabella had been able to work with Gaveston, Edward's previous favourite, it became increasingly clear that Hugh the younger and Isabella could not work out a similar compromise. Unfortunately for Isabella, she was still estranged from Lancaster's rival faction, giving her little room to manoeuvre. In 1321, Lancaster's alliance moved against the Despensers, sending troops into London and demanding their exile. Aymer de Valence, 2nd Earl of Pembroke, a moderate baron with strong French links, asked Isabella to intervene in an attempt to prevent war; Isabella publicly went down on her knees to appeal to Edward to exile the Despensers, providing him with a face-saving excuse to do so, but Edward intended to arrange their return at the first opportunity.
Return of the Despensers, 1321-6.
Despite the momentary respite delivered by Isabella, by the autumn of 1321, the tensions between the two factions of Edward, Isabella and the Despenser, opposing the baronial opposition led by Thomas of Lancaster, were extremely high, with forces still mobilised across the country. At this point, Isabella undertook a pilgrimage to Canterbury, during which she left the traditional route to stop at Leeds Castle in Kent, a fortification held by Bartholomew de Badlesmere, steward of the King's household who had by 1321 joined the ranks of Edward's opponents. Historians believe that the pilgrimage was a deliberate act by Isabella on Edward's behalf to create a "casus belli". Lord Badlesmere was away at the time, having left his wife Margaret in charge of the castle. When the latter adamantly refused the Queen admittance, fighting broke out outside the castle between Isabella's guards and the garrison, marking the beginning of the Despenser War. Whilst Edward mobilised his own faction and placed Leeds castle under siege, Isabella was given the Great Seal and assumed control of the royal Chancery from the Tower of London. After surrendering to Edward's forces on 31 October 1321, Margaret, Baroness Badlesmere and her children were sent to the Tower, and 13 of the Leeds garrison were hanged. By January 1322, Edward's army, reinforced by the Despensers returning from exile, had forced the surrender of the Mortimers, and by March Lancaster himself had been captured after the battle of Boroughbridge; Lancaster was promptly executed, leaving Edward and the Despensers victorious.
Hugh Despenser the younger was now firmly ensconced as Edward's new favourite and lover, and together over the next four years Edward and the Despensers imposed a harsh rule over England, a "sweeping revenge" characterised by land confiscation, large-scale imprisonment, executions and the punishment of extended family members, including women and the elderly. This was condemned by contemporary chroniclers, and is felt to have caused concern to Isabella as well; some of those widows being persecuted included her friends. Isabella's relationship with Despenser the younger continued to deteriorate; the Despensers refused to pay her monies owed to her, or return her castles at Marlborough and Devizes. Indeed, various authors have suggested that there is evidence that Hugh Despenser the younger may have attempted to assault Isabella herself in some fashion. Certainly, immediately after the battle of Boroughbridge, Edward began to be markedly less generous in his gifts towards Isabella, and none of the spoils of the war were awarded to her. Worse still, later in the year Isabella was caught up in the failure of another of Edward's campaigns in Scotland, in a way that permanently poisoned her relationship with both Edward and the Despensers.
Isabella and Edward had travelled north together at the start of the autumn campaign; before the disastrous battle of Old Byland, Edward had ridden south, apparently to raise more men, sending Isabella east to Tynemouth Priory. With the Scottish army marching south, Isabella expressed considerable concern about her personal safety and requested assistance from Edward. Her husband initially proposed sending Despenser forces to secure her, but Isabella rejected this outright, instead requesting friendly troops. Rapidly retreating south with the Despensers, Edward failed to grip the situation, with the result that Isabella found herself and her household cut off from the south by the Scottish army, with the coastline patrolled by Flemish naval forces allied to the Scots. The situation was precarious and Isabella was forced to use a group of squires from her personal retinue to hold off the advancing army whilst other of her knights commandeered a ship; the fighting continued as Isabella and her household retreated onto the vessel, resulting in the death of two of her ladies-in-waiting. Once aboard, Isabella evaded the Flemish navy, landing further south and making her way to York. Isabella was furious, both with Edward for, from her perspective, abandoning her to the Scots, and with Despensers for convincing Edward to retreat rather than sending help. For his part, Edward blamed Lewis de Beaumont, the Bishop of Durham and an ally of Isabella, for the fiasco.
Isabella effectively separated from Edward from here onwards, leaving him to live with Hugh Despenser. At the end of 1322, Isabella left the court on a ten-month-long pilgrimage around England by herself. On her return in 1323 she visited Edward briefly, but refused to take a loyalty oath to the Despensers and was removed from the process of granting royal patronage. At the end of 1324, as tensions grew with Isabella's homeland of France, Edward and the Despensers confiscated all of Isabella's lands, took over the running of her household and arrested and imprisoned all of her French staff. Isabella's youngest children were removed from her and placed into the custody of the Despensers. At this point, Isabella appears to have realised that any hope of working with Edward was effectively over and begun to consider radical solutions.
The invasion of England.
By 1325, Isabella was facing increasing pressure from Hugh Despenser the Younger, Edward's new royal favourite. With her lands in England seized, her children taken away from her and her household staff arrested, Isabella began to pursue other options. When her brother, King Charles IV of France, seized Edward's French possessions in 1325, she returned to France, initially as a delegate of the King charged with negotiating a peace treaty between the two nations. However, her presence in France became a focal point for the many nobles opposed to Edward's reign. Isabella gathered an army to oppose Edward, in alliance with Roger Mortimer, whom she took as a lover. Isabella and Mortimer returned to England with a mercenary army, seizing the country in a lightning campaign. The Despensers were executed and Edward was forced to abdicate – his eventual fate and possible murder remains of considerable historical debate. Isabella ruled as regent until 1330, when her son, Edward deposed Mortimer in turn and ruled directly in his own right.
Tensions in Gascony, 1323-5.
Isabella's husband Edward, as the Duke of Aquitaine, owed homage to the King of France for his lands in Gascony. Isabella's three brothers each had only short reigns, and Edward had successfully avoided paying homage to Louis X, and had only paid homage to Philip V under great pressure. Once Charles IV took up the throne, Edward had attempted to avoid doing so again, increasing tensions between the two. One of the elements in the disputes was the border province of Agenais, part of Gascony and in turn part of Aquitaine. Tensions had risen in November 1323 after the construction of a bastide, a type of fortified town, in Saint-Sardos, part of the Agenais, by a French vassal. Gascon forces destroyed the bastide, and in turn Charles attacked the English-held Montpezat: the assault was unsuccessful, but in the subsequent War of Saint-Sardos Isabella's uncle, Charles of Valois, successfully wrestled Aquitaine from English control; by 1324, Charles had declared Edward's lands forfeit and had occupied the whole of Aquitaine apart from the coastal areas.
Edward was still unwilling to travel to France to give homage; the situation in England was febrile; there had been an assassination plot against Edward and Hugh Despenser in 1324, there had been allegations that the famous magician John of Nottingham had been hired to kill the pair using necromancy in 1325, and criminal gangs were occupying much of the country. Edward was deeply concerned that should he leave England, even for a short while, the barons would take the chance to rise up and take their revenge on the Despensers. Charles sent a message through Pope John XXII to Edward, suggesting that he was willing to reverse the forfeiture of the lands if Edward ceded the Agenais and paid homage for the rest of the lands: the Pope proposed Isabella as an ambassador. Isabella, however, saw this as a perfect opportunity to resolve her situation with Edward and the Despensers.
Having promised to return to England by the summer, Isabella reached Paris in March 1325, and rapidly agreed a truce in Gascony, under which Prince Edward, then thirteen years old, would come to France to give homage on his father's behalf. Prince Edward arrived in France, and gave homage in September. At this point, however, rather than returning, Isabella remained firmly in France with her son. Edward began to send urgent messages to the Pope and to Charles IV, expressing his concern about his wife's absence, but to no avail. For his part, Charles replied that the, "queen has come of her own will and may freely return if she wishes. But if she prefers to remain here, she is my sister and I refuse to expel her." Charles went on to refuse to return the lands in Aquitaine to Edward, resulting in a provisional agreement under which Edward resumed administration of the remaining English territories in early 1326 whilst France continued to occupy the rest.
Meanwhile, the messages brought back by Edward's agent Walter de Stapledon, Bishop of Exeter and others grew steadily worse: Isabella had publicly snubbed Stapledon; Edward's political enemies were gathering at the French court, and threatening his emissaries; Isabella was dressed as a widow, claiming that Hugh Despenser had destroyed her marriage with Edward; Isabella was assembling a court-in-exile, including Edmund of Kent and John of Brittany, Earl of Richmond. By this stage Isabella had begun a romantic relationship with the English exile Roger Mortimer.
Roger Mortimer, 1325-6.
Roger Mortimer of Wigmore was a powerful Marcher lord, married to the wealthy heiress Joan de Geneville, and the father of twelve children. Mortimer had been imprisoned in the Tower of London in 1322 following his capture by Edward during the Despenser wars. Mortimer's uncle, Roger Mortimer de Chirk finally died in prison, but Mortimer managed to escape the Tower in August 1323, making a hole in the stone wall of his cell and then escaping onto the roof, before using rope ladders provided by an accomplice to get down to the River Thames, across the river and then on eventually to safety in France. Victorian writers suggested that, given later events, Isabella might have helped Mortimer escape and some historians continue to argue that their relationship had already begun at this point, although most believe that there is no hard evidence for their having had a substantial relationship before meeting in Paris.
Isabella was reintroduced to Mortimer in Paris by her cousin, Joan, Countess of Hainault, who appears to have approached Isabella suggesting a marital alliance between their two families, marrying Prince Edward to Joan's daughter, Philippa. Mortimer and Isabella began a passionate relationship from December 1325 onwards; Isabella was taking a huge risk in doing so – female infidelity was a very serious offence in medieval Europe, as shown during the Tour de Nesle Affair – both Isabella's former French sisters-in-law had died by 1326 as a result of their imprisonment for exactly this offence. Isabella's motivation has been the subject of discussion by historians; most agree that there was a strong sexual attraction between the two, that they shared an interest in the Arthurian legends and that they both enjoyed fine art and high living. One historian has described their relationship as one of the "great romances of the Middle Ages". They also shared a common enemy – the regime of Edward II and the Despensers.
Taking Prince Edward with them, Isabella and Mortimer left the French court in summer 1326 and travelled north to William I, Count of Hainaut. As Joan had suggested the previous year, Isabella betrothed Prince Edward to Philippa, the daughter of the Count, in exchange for a substantial dowry. She then used this money plus an earlier loan from Charles to raise a mercenary army, scouring Brabant for men, which were added to a small force of Hainaut troops. William also provided eight men of war ships and various smaller vessels as part of the marriage arrangements. Although Edward was now fearing an invasion, secrecy remained key, and Isabella convinced William to detain envoys from Edward. Isabella also appears to have made a secret agreement with the Scots for the duration of the forthcoming campaign. On 22 September, Isabella, Mortimer and their modest force set sail for England.
Seizure of power, 1326.
Having evaded Edward's fleet, which had been sent to intercept them, Isabella and Mortimer landed at Orwell on the east coast of England on 24 September with a small force; estimates of Isabella's army vary from between 300 to around 2,000 soldiers, with 1,500 being a popular middle figure. After a short period of confusion during which they attempted to work out where they had actually landed, Isabella moved quickly inland, dressed in her widow's clothes. The local levies mobilised to stop them immediately changed sides, and by the following day Isabella was in Bury St Edmunds and shortly afterwards had swept inland to Cambridge. Thomas, Earl of Norfolk, joined Isabella's forces and Henry of Lancaster – the brother of the late Thomas, and Isabella's uncle – also announced he was joining Isabella's faction, marching south to join her.
By the 27th, word of the invasion had reached the King and the Despensers in London. Edward issued orders to local sheriffs to mobilise opposition to Isabella and Mortimer, but London itself was becoming unsafe because of local unrest and Edward made plans to leave. Isabella struck west again, reaching Oxford on 2 October where she was "greeted as a saviour" – Adam Orleton, the bishop of Hereford, emerged from hiding to give a lecture to the university on the evils of the Despensers. Edward fled London on the same day, heading west toward Wales. Isabella and Mortimer now had an effective alliance with the Lancastrian opposition to Edward, bringing all of his opponents into a single coalition.
Isabella now marched south towards London, pausing at Dunstable, outside the city on 7 October. London was now in the hands of the mobs, although broadly allied to Isabella. Bishop Stapledon failed to realise the extent to which royal power had collapsed in the capital and tried to intervene militarily to protect his property against rioters; a hated figure locally, he was promptly attacked and killed – his head was later sent to Isabella by her local supporters. Edward, meanwhile, was still fleeing west, reaching Gloucester by the 9th. Isabella responded by marching swiftly west herself in an attempt to cut him off, reaching Gloucester a week after Edward, who slipped across the border into Wales the same day.
Hugh de Despenser the elder continued to hold Bristol against Isabella and Mortimer, who placed it under siege between 18–26 October; when it fell, Isabella was able to recover her daughters Eleanor and Joan, who had been kept in the Despenser's custody. By now desperate and increasingly deserted by their court, Edward and Hugh Despenser the younger attempted to sail to Lundy, a small island just off the Devon coast, but the weather was against them and after several days they were forced to land back in Wales. With Bristol secure, Isabella moved her base of operations up to the border town of Hereford, from where she ordered Henry of Lancaster to locate and arrest her husband. After a fortnight of evading Isabella's forces in South Wales, Edward and Hugh were finally caught and arrested near Llantrisant on 16 November.
The retribution began immediately. Hugh Despenser the elder had been captured at Bristol, and despite some attempts by Isabella to protect him, was promptly executed by his Lancastrian enemies – his body was hacked to pieces and fed to the local dogs. The remainder of the former regime were brought to Isabella. Edmund Fitzalan, a key supporter of Edward II and who had received many of Mortimer's confiscated lands in 1322, was executed on 17 November. Hugh Despenser the younger was sentenced to be brutally executed on 24 November, and a huge crowd gathered in anticipation at seeing him die. They dragged him from his horse, stripped him, and scrawled Biblical verses against corruption and arrogance on his skin. He was then dragged into the city, presented to Queen Isabella, Roger Mortimer, and the Lancastrians. Despenser was then condemned to hang as a thief, be castrated, and then to be drawn and quartered as a traitor, his quarters to be dispersed throughout England. Simon of Reading, one of the Despensers' supporters, was hanged next to him, on charges of insulting Isabella. Once the core of the Despenser regime had been executed, Isabella and Mortimer began to show restraint. Lesser nobles were pardoned and the clerks at the heart of the government, mostly appointed by the Despensers and Stapleton, were confirmed in office. All that was left now was the question of Edward II, still officially Isabella's legal husband and lawful king.
The death of Edward, 1327.
As an interim measure, Edward II was held in the custody of Henry of Lancaster, who surrendered Edward's Great Seal to Isabella. The situation remained tense, however; Isabella was clearly concerned about Edward's supporters staging a counter-coup, and in November she seized the Tower of London, appointed one of her supporters as mayor and convened a council of nobles and churchmen in Wallingford to discuss the fate of Edward. The council concluded that Edward would be legally deposed and placed under house arrest for the rest of his life. This was then confirmed at the Parliament of England, dominated by Isabella and Mortimer's followers. The session was held in January 1327, with Isabella's case being led by her supporter Adam Orleton, Bishop of Hereford. Isabella's son, Prince Edward, was confirmed as Edward III, with his mother appointed regent. Isabella's position was still precarious, as the legal basis for deposing Edward was minimal and many lawyers of the day maintained that Edward was still the rightful king, regardless of the declaration of the Parliament. The situation could be reversed at any moment and Edward was known to be a vengeful ruler.
Edward II's subsequent fate, and Isabella's role in it, remains hotly contested by historians. The minimally agreed version of events is that Isabella and Mortimer had Edward moved from Kenilworth Castle in the Midlands to the safer location of Berkeley Castle in the Welsh borders, where he was put into the custody of Lord Berkeley. On 23 September, Isabella and Edward III were informed by messenger that Edward had died whilst imprisoned at the castle, because of a "fatal accident". Edward's body was apparently buried at Gloucester Cathedral, with his heart being given in a casket to Isabella. After the funeral, there were rumours for many years that Edward had survived and was really alive somewhere in Europe, some of which were captured in the famous Fieschi Letter written in the 1340s, although no concrete evidence ever emerged to support the allegations. There are, however, various historical interpretations of the events surrounding this basic sequence of events.
According to legend, Isabella and Mortimer famously plotted to murder Edward in such a way as not to draw blame on themselves, sending a famous order (in Latin: "Eduardum occidere nolite timere bonum est") which, depending on where the comma was inserted, could mean either "Do not be afraid to kill Edward; it is good" or "Do not kill Edward; it is good to fear". In actuality, there is little evidence of anyone deciding to have Edward assassinated, and none whatsoever of the note having been written. Similarly, accounts of Edward being killed with a red-hot poker have no strong contemporary sources to support them. The conventional 20th-century view has been that Edward did die at Berkeley Castle, either murdered on Isabella's orders or of ill-health brought on by his captivity, and that subsequent accounts of his survival were simply rumours, similar to those that surrounded Joan of Arc and other near contemporaries after their deaths.
Three recent historians, however, have offered an alternative interpretation of events. Paul Doherty, drawing extensively on the Fieschi Letter of the 1340s, has argued that Edward in fact escaped from Berkeley Castle with the help of William Ockle, a knight whom Doherty argues subsequently pretended to be Edward in disguise around Europe, using the name "William the Welshman" to draw attention away from the real Edward himself. In this interpretation, a look-alike was buried at Gloucester. Ian Mortimer, focusing more on contemporary documents from 1327 itself, argues that Roger de Mortimer engineered a fake "escape" for Edward from Berkeley Castle; after this Edward was kept in Ireland, believing he was really evading Mortimer, before finally finding himself free, but politically unwelcome, after the fall of Isabella and Mortimer. In this version, Edward makes his way to Europe, before subsequently being buried at Gloucester. Finally, Alison Weir, again drawing on the Fieschi Letter, has recently argued that Edward II escaped his captors, killing one in the process, and lived as a hermit for many years; in this interpretation, the body in Gloucester Cathedral is of Edward's dead captor. In all of these versions, it is argued that it suited Isabella and Mortimer to publicly claim that Edward was dead, even if they were aware of the truth. Other historians, however, including David Carpenter, have criticised the methodology behind this revisionist approach and disagree with the conclusions.
Later years.
Isabella and Mortimer ruled together for four years, with Isabella's period as regent marked by the acquisition of huge sums of money and land. When their political alliance with the Lancastrians began to disintegrate, Isabella continued to support Mortimer, her lover. Isabella fell from power when her son, Edward III deposed Mortimer in a coup, taking back royal authority for himself. Unlike Mortimer, Isabella survived the transition of power, however, remaining a wealthy and influential member of the English court, albeit never returning directly to active politics.
As regent, 1326–30.
Isabella's reign as regent lasted only four years, before the fragile political alliance that had brought her and Mortimer to power disintegrated. 1328 saw the marriage of Isabella's son, Edward III to Philippa of Hainault, as agreed before the invasion of 1326; the lavish ceremony was held in London to popular acclaim. Isabella and Mortimer had already begun a trend that continued over the next few years, in starting to accumulate huge wealth. With her lands restored to her, Isabella was already exceptionally rich, but she began to accumulate yet more. Within the first few weeks, Isabella had granted herself almost £12,000; finding that Edward's royal treasury contained £60,000, a rapid period of celebratory spending then ensued. Isabella soon awarded herself another £20,000, allegedly to pay off foreign debts. At Prince Edward's coronation, Isabella then extended her land holdings from a value of £4,400 each year to the huge sum of £13,333, making her one of the largest landowners in the kingdom. Isabella also refused to hand over her dower lands to Philippa after her marriage to Edward III, in contravention of usual custom. Isabella's lavish lifestyle matched her new incomes. Mortimer, as her lover and effective first minister, after a restrained beginning, also began to accumulate lands and titles at a tremendous rate, particularly in the Marcher territories.
The new regime also faced some key foreign policy dilemmas, which Isabella approached from a realist perspective. The first of these was the situation in Scotland, where Edward II's unsuccessful policies had left an unfinished, tremendously expensive war. Isabella was committed to bringing this issue to a conclusion by diplomatic means. Edward III initially opposed this policy, before eventually relenting, leading to the Treaty of Northampton. Under this treaty, Isabella's daughter Joan would marry David Bruce (heir apparent to the Scottish throne) and Edward III would renounce any claims on Scottish lands, in exchange for the promise of Scottish military aid against any enemy except the French, and £20,000 in compensation for the raids across northern England. No compensation would be given to those earls who had lost their Scottish estates, and the compensation would be taken by Isabella. Although strategically successful and, historically at least, "a successful piece of policy making", Isabella's Scottish policy was by no means popular and contributed to the general sense of discontent with the regime. Secondly, the Gascon situation, still unresolved from Edward II's reign, also posed an issue. Isabella reopened negotiations in Paris, resulting in a peace treaty under which the bulk of Gascony, minus the Agenais, would be returned to England in exchange for a 50,000 mark penalty. The treaty was not popular in England because of the Agenais clause.
Henry of Lancaster was amongst the first to break with Isabella and Mortimer. By 1327 Lancaster was irritated by Mortimer's behaviour and Isabella responded by beginning to sideline him from her government. Lancaster was furious over the passing of the Treaty of Northampton, and refused to attend court, mobilising support amongst the commoners of London. Isabella responded to the problems by undertaking a wide reform of royal administration, local law enforcement. In a move guaranteed to appeal to domestic opinion, Isabella also decided to pursue Edward III's claim on the French throne, sending her advisers to France to demand official recognition of his claim. The French nobility were unimpressed and, since Isabella lacked the funds to begin any military campaign, she began to court the opinion of France's neighbours, including proposing the marriage of her son John to the Castilian royal family.
By the end of 1328 the situation had descended into near civil war once again, with Lancaster mobilising his army against Isabella and Mortimer. In January 1329 Isabella's forces under Mortimer's command took Lancaster's stronghold of Leicester, followed by Bedford; Isabella – wearing armour, and mounted on a warhorse – and Edward III marched rapidly north, resulting in Lancaster's surrender. He escaped death but was subjected to a colossal fine, effectively crippling his power. Isabella was merciful to those who had aligned themselves with him, although some – such as her old supporter Henry de Beaumont, whose family had split from Isabella over the peace with Scotland, which had lost them huge land holdings in Scotland – fled to France.
Despite Lancaster's defeat, however, discontent continued to grow. Edmund of Kent had sided with Isabella in 1326, but had since begun to question his decision and was edging back towards Edward II, his half-brother. Edmund of Kent was in conversations with other senior nobles questioning Isabella's rule, including Henry de Beaumont and Isabella de Vesci. Edmund was finally involved in a conspiracy in 1330, allegedly to restore Edward II, whom he claimed was still alive: Isabella and Mortimer broke up the conspiracy, arresting Edmund and other supporters – including Simon Mepeham, Archbishop of Canterbury. Edmund may have expected a pardon, possibly from Edward III, but Isabella was insistent on his execution. The execution itself was a fiasco after the executioner refused to attend and Edmund of Kent had to be killed by a local dung-collector, who had been himself sentenced to death and was pardoned as a bribe to undertake the beheading. Isabella de Vesci escaped punishment, despite have been closely involved in the plot.
Mortimer's fall from power, 1330.
By mid-1330, Isabella and Mortimer's regime was increasingly insecure, and Isabella's son, Edward III, was growing frustrated at Mortimer's grip on power. Various historians, with different levels of confidence, have also suggested that in late 1329 Isabella became pregnant. A child of Mortimer's with royal blood would have proved both politically inconvenient for Isabella, and challenging to Edward's own position.
Edward quietly assembled a body of support from the Church and selected nobles, whilst Isabella and Mortimer moved into Nottingham Castle for safety, surrounding themselves with loyal troops. In the autumn, Mortimer was investigating another plot against him, when he challenged a young noble, Montague, during an interrogation. Mortimer declared that his word had priority over the king's, an alarming statement that Montague reported back to Edward. Edward was convinced that this was the moment to act, and on 19 October, Montague led a force of twenty three armed men into the castle by a secret tunnel. Up in the keep, Isabella, Mortimer and other council members were discussing how to arrest Montague, when Montague and his men appeared. Fighting broke out on the stairs and Mortimer was overwhelmed in his chamber. Isabella threw herself at Edward's feet, famously crying "Fair son, have pity on gentle Mortimer!" Lancastrian troops rapidly took the rest of the castle, leaving Edward in control of his own government for the first time.
Parliament was convened the next month, where Mortimer was put on trial for treason. Isabella was portrayed as an innocent victim during the proceedings, and no mention of her sexual relationship with Mortimer was made public. Isabella's lover was executed at Tyburn, but Edward III showed leniency and he was not quartered or disembowelled.
In retirement, 1330–58.
After the coup, Isabella was initially transferred to Berkhamsted Castle, and then held under house arrest at Windsor Castle until 1332, when she then moved back to her own Castle Rising in Norfolk. Agnes Strickland, a Victorian historian, argued that Isabella suffered from occasional fits of madness during this period but modern interpretations suggest, at worst, a nervous breakdown following the death of her lover. Isabella remained extremely wealthy; despite being required to surrender most of her lands after losing power, in 1331 she was reassigned a yearly income of £3000, which increased to £4000 by 1337. She lived an expensive lifestyle in Norfolk, including minstrels, huntsmen, grooms and other luxuries, and was soon travelling again around England. In 1342, there were suggestions that she might travel to Paris to take part in peace negotiations, but eventually this plan was quashed. She was also appointed to negotiate with France in 1348 and was involved in the negotiations with Charles II of Navarre in 1358.
As the years went by, Isabella became very close to her daughter Joan, especially after Joan left her unfaithful husband, King David II of Scotland. Joan also nursed her just before she died. She doted on her grandchildren, including Edward, the Black Prince. She became increasingly interested in religion as she grew older, visiting a number of shrines. She remained, however, a gregarious member of the court, receiving constant visitors; amongst her particular friends appear to have been Roger Mortimer's daughter Agnes Mortimer, Countess of Pembroke, and Roger Mortimer's grandson, also called Roger Mortimer, whom Edward III restored to the Earldom of March. King Edward and his children often visited her as well. She remained interested in Arthurian legends and jewellery; in 1358 she appeared at the St George's Day celebrations at Windsor wearing a dress made of silk, silver, 300 rubies, 1800 pearls and a circlet of gold. She may also have developed an interest in astrology or geometry towards the end of her life, receiving various presents relating to these disciplines.
Isabella took the habit of the Poor Clares before she died on 22 August 1358, and her body was returned to London for burial at the Franciscan church at Newgate, in a service overseen by Archbishop Simon Islip. She was buried in the mantle she had worn at her wedding and at her request, Edward's heart, placed into a casket thirty years before, was interred with her. Isabella left the bulk of her property, including Castle Rising, to her favourite grandson, the Black Prince, with some personal effects being granted to her daughter Joan
Legacy.
Literature and theatre.
Queen Isabella appeared with a major role in Christopher Marlowe's play "Edward II" (c. 1592) and thereafter has been frequently used as a character in plays, books, and films, often portrayed as beautiful but manipulative or wicked. Thomas Gray, the 18th-century poet, combined Marlowe's depiction of Isabella with William Shakespeare's description of Margaret of Anjou (the wife of Henry VI) as the "She-Wolf of France", to produce the anti-French poem "The Bard", in which Isabella rips apart the bowels of Edward II with her "unrelenting fangs". The "She-Wolf" epithet stuck, and Bertolt Brecht re-used it in "The Life of Edward II of England" (1923).
Film.
In Derek Jarman's film "Edward II" (1991), based on Marlowe's play, Isabella is portrayed (by actress Tilda Swinton) as a "femme fatale" whose thwarted love for Edward causes her to turn against him and steal his throne. In contrast to the negative depictions, Mel Gibson's film "Braveheart" (1995) portrays Isabella (played by the French actress Sophie Marceau) more sympathetically. In the film, an adult Isabella is fictionally depicted as having a romantic affair with the Scottish hero William Wallace. However, in reality, she was 9-years-old at the time of Wallace's death. Additionally, Wallace is incorrectly portrayed as the real father of her son, Edward III, despite Wallace's death many years before Edward's birth.
Issue.
Edward and Isabella had four children, and she suffered at least one miscarriage. Their itineraries demonstrate that they were together 9 months prior to the births of all four surviving offspring. Their children were:
Ancestry.
Isabella is descended from Gytha of Wessex through King Andrew II of Hungary and thus brought the bloodline of the last Saxon King of England, Harold Godwinson, back into the English Royal family.

</doc>
<doc id="47622" url="http://en.wikipedia.org/wiki?curid=47622" title="Caernarfon">
Caernarfon

Caernarfon (; ]) is a royal town, community and port in Gwynedd, Wales, with a population of 9,615; this figure does not include nearby Bontnewydd or Caeathro as they are in separate communities. It lies along the A487 road, on the eastern shore of the Menai Strait, opposite the Isle of Anglesey. The city of Bangor is 8.6 mi to the north-east, while Snowdonia fringes Caernarfon to the east and south-east. Carnarvon and Caernarvon are Anglicised spellings that were superseded in 1926 and 1974 respectively.
Abundant natural resources in and around the Menai Straits enabled human habitation in the area during prehistory. The Ordovices, a Celtic tribe, lived in the region during classical antiquity. The Roman fort Segontium was established around AD 80 to subjugate the Ordovices during the Roman conquest of Britain. The Romans and occupied the region until their departure in the 5th century, after which Caernarfon became part of the Kingdom of Gwynedd. In the late 11th century, William the Conqueror ordered the construction of a motte at Caernarfon, as part of an attempt at conquering the region. He was unsuccessful and most of Wales remained independent until around 1283.
In the 13th century, Llywelyn ap Gruffudd, ruler of Gwynedd, refused to pay homage to Edward I prompting the English conquest of Gwynedd. This was followed by the construction of Caernarfon Castle, one of the largest and most imposing fortifications built by the English in Wales. In 1284, the English-style county of Caernarfonshire was established by the Statute of Rhuddlan; the same year, Caernarfon was made a borough, a county and market town, and the seat of English government in North Wales.
The ascent of the Tudor dynasty to the throne of England eased hostilities between the English and resulted in Caernarfon Castle falling into a state of disrepair. The city has flourished, leading to its status as a major tourist centre and seat of Gwynedd Council, with a thriving harbour and marina. Caernarfon has expanded beyond its medieval walls and experienced heavy suburbanisation. Its population includes the largest percentage of Welsh-speaking citizens anywhere in Wales. The status of Royal Borough was granted by Queen Elizabeth II in 1963 and emended to Royal Town in 1974.
History.
The present city of Caernarfon grew up around and owes its name to its Norman and Edwardian fortifications. The earlier British and Roman settlement at Segontium was named for the nearby River Seiont. After the Roman withdrawal from Britain around AD 410, the settlement continued to be known as "Cair Segeint" ("Fort Seiont") and as "Cair Custoient" ("Fort Constantius or Constantine"), both of which names appear among the 28 cities of Britain in the "History of the Britons" traditionally ascribed to Nennius. The work stated that the inscribed tomb of "Constantius the Emperor" (presumably Constantius Chlorus, father of Constantine the Great) was still present in the 9th century. (Constantius actually died at York; Ford credited the monument to a different Constantine, the supposed son of Saint Elen and Magnus Maximus, who was said to have ruled northern Wales before being removed by the Irish.) The medieval romance about Maximus and Elen, "Macsen's Dream", calls her home "Caer Aber Sein" ("Fort Seointmouth" or "the caer at the mouth of the Seoint") and other pre-conquest poets such as Hywel ab Owain Gwynedd also used the name "Caer Gystennin".<Ref>William, Ifor. "Breuddwyd Maxen". (Bangor), 1920.</ref>
The Norman motte was erected apart from the existing settlement and came to be known as "y gaer yn Arfon", "the fortress in Arfon". (The region of Arfon itself derived its name from its position opposite Anglesey island, known as "Môn" in Welsh.) A 1221 charter by Llywelyn the Great to the canons of Penmon priory on Anglesey mentions "Kaerinarfon"; the "Brut" mentions both "Kaerenarvon" and "Caerenarvon". Caernarfon was the county town of the historic county of Caernarfonshire. It is best known for the great stone-built Caernarfon Castle, built by Edward I, King of England and consequently sometimes seen as a symbol of English domination. Edward's architect, James of St. George, may well have modelled the castle on the walls of Constantinople, possibly being aware of the town's legendary associations. In addition, Edward was a supporter of the Crusader cause.
Caernarfon was constituted a borough in 1284 by charter of Edward I. The charter, which was confirmed on a number of occasions, appointed the mayor of the borough Constable of the Castle ex officio. The former municipal borough was designated a royal borough in 1963. The borough was abolished by the Local Government Act 1972 in 1974, and the status of "royal town" was granted to the community which succeeded it.
In 1911, David Lloyd George, then Member of Parliament for Caernarfon boroughs, which included various towns from Llŷn to Conwy, agreed to the British Royal family's idea of holding the investiture of the Prince of Wales at Caernarfon Castle. The ceremony took place on 13 July, with the royal family paying a rare visit to Wales, and the future King Edward VIII was duly invested.
In 1955 Caernarfon was in the running for the title of Capital of Wales on historical grounds but the town's campaign was heavily defeated in a ballot of Welsh local authorities, with 11 votes compared to Cardiff's 136. Cardiff therefore became the Welsh capital.
On 1 July 1969 the investiture ceremony for Charles, Prince of Wales was again held at Caernarfon Castle. The ceremony itself went ahead without incident despite terrorist threats and protests, which culminated in the death of two members of Mudiad Amddiffyn Cymru (Welsh Defence Movement), Alwyn Jones and George Taylor, who were killed when their bomb – intended for the railway line at Abergele in order to stop the British Royal Train – exploded prematurely. The bomb campaign (one in Abergele, two in Caernarfon and finally one on Llandudno Pier) was organised by the leader of Mudiad Amddiffyn Cymru, John Jenkins. He was later arrested after a tip-off and was sentenced to ten years' imprisonment.
The history of Caernarfon as an example where the rise and fall of different civilizations can be seen from one hilltop, are discussed in John Michael Greer's book 'The Long Descent'. He writes the "Welsh town of Caernarfon.
Spread out below us in an unexpected glory of sunlight was the
whole recorded history of that little corner of the world.
The ground beneath us still rippled with earthworks from the
Celtic hill fort that guarded the Menai Strait more than two and
a half millennia ago. The Roman fort that replaced it was now the
dim brown mark of an old archeological site on low hills off to the
left. Edward I’s great gray castle rose up in the middle foreground,
and the high contrails of RAF jets on a training exercise out over
the Irish Sea showed that the town’s current overlords still maintained
the old watch. Houses and shops from more than half a
dozen centuries spread eastward as they rose through the waters
of time, from the cramped medieval buildings of the old castle
town straight ahead to the gaudy sign and sprawling parking lot of
the supermarket back behind us".
Geography.
Caernarfon is situated on the eastern bank of the Menai Strait facing the Isle of Anglesey. It is situated 8.6 mi south-west of Bangor, 19.4 mi north of Porthmadog and approximately 8.0 mi west of Llanberis and Snowdonia National Park. The mouth of the River Seiont is in the town, creating a natural harbour where it flows into the Menai Strait. Caernarfon Castle stands at the mouth of the river. The A487 passes directly through Caernarfon, with Bangor to the north and Porthmadog to the south. Llanberis at the foot of Snowdon can be reached via the A4086, which heads east out of the town to Capel Curig. Heading north out of the town is the Lôn Las Menai cycle path to nearby Y Felinheli. Heading south out of the town is the Lôn Eifion cycle path, which leads to Bryncir, near Criccieth. The route provides views into the Snowdonia mountains, down along the Llŷn Peninsula and across to the Isle of Anglesey. The Welsh Highland Railway or Rheilffordd Eryri, a narrow gauge heritage railway, was restored in 2011 and runs from Caernarfon to Porthmadog where it connects with the Festiniog Railway.
Economy.
Caernarfon's historical prominence and landmarks have made it a major tourist centre. As a result many of the local businesses cater for the tourist trade. Caernarfon is home to numerous guest houses, inns and pubs, hotels, restaurants and shops. The majority of shops in the town are located either in the centre of town around Pool Street and Castle Square (Maes), or on Doc Fictoria. A number of shops are also located within the Town Walls.
The majority of the retail and residential section of Doc Fictoria (Victoria Dock) was opened in 2008. The retail and residential section of Doc Fictoria is built directly beside a Blue Flag beach marina. It contains numerous homes, bars and bistros, cafés and restaurants, an award- winning arts centre, a maritime museum and a range of shops and stores.
Pool Street and Castle Square (Maes) contain a number of large, national retail shops and smaller independent stores. Pool Street is a pedestrianised street and, as such, serves as the town's main shopping street. Castle Square, commonly referred to as the 'Maes' by both Welsh and English speakers, is the market square of the town. A market is held every Saturday throughout the year and also on Mondays in the Summer. The square was revamped at a cost of £2.4 million in 2009. However, since its revamp the square has caused controversy due to traffic and parking difficulties. During the revamp, it was decided to remove barriers between traffic and pedestrians creating a 'shared space', to try and force road users to be more considerate of pedestrians and other vehicles. This is the first use of this kind of arrangement in Wales, but it has been described by councillor Bob Anderson as being 'too ambiguous' for road users. Another controversy caused by the revamp of the Maes was that a historic feature of the town was taken down, namely a very old oak tree, situated outside the HSBC bank. When the Maes was re-opened in July 2009 by the local politician and Heritage Minister of Wales, Alun Ffred Jones AM, he said, "the use of beautiful local slate is very prominent in the new Maes."
There are many old public houses serving the town, including The Four Alls, The Anglesey Arms Hotel, The Castle Hotel, The Crown, Morgan Lloyd, Pen Deitch and The Twthill Vaults. The oldest public house in Caernarfon is the Black Boy Inn, which remained in the same family for over 40 years until sold in 2003 to a local independent family business. The pub has stood inside Caernarfon's Town Walls since the 16th century, and many ghosts have been sighted within the building.
In and around the Town Walls are numerous restaurants, public houses and inns, and guest houses and hostels.
Local government.
Gwynedd Council's head offices are situated in the town. The local court serves the town and the rest of north-west Wales, and in 2009 moved to a multi-million pound court complex on Llanberis Road. The Caernarfon UK Parliament constituency was a former electoral area centred on Caernarfon. Caernarfon is now part of the Arfon constituency for both the UK Parliament and the Welsh Assembly. The town is twinned with Landerneau in Brittany.
Demography.
Demographically Gwynedd has the highest proportion of people in Wales who can speak Welsh. The highest proportion of these Welsh speakers are to be found in and around Caernarfon. According to the 2001 Census, 86.1% of the population could speak the Welsh language, with the largest majority of Welsh speakers in the 10-14 age group, where 97.7% could speak it fluently. The town is nowadays a rallying-point for the Welsh nationalist cause.
The population of Caernarfon Community Parish in 2001 was 9,611.
Caernarfon residents are known colloquially as "Cofis". The word "Cofi" is also used locally in Caernarfon to describe the local Welsh dialect, notable for a number of words not in use elsewhere.
Landmarks.
One of the oldest buildings in the town is The Market Hall, which is situated on Hole In The Wall street, or Stryd Twll Yn Wal as it is referred to by Welsh speakers.
The old court buildings, replaced in 2009 by a new complex designed by HOK on the former Segontium School site in Llanberis Road, are situated inside the castle walls, next door to the Anglesey Arms Hotel and to the Gwynedd County Council buildings in Pendeitch. They are grand buildings, especially the exterior of the former magistrates' court, which features a gothic architecture style of decoration. The old buildings adjoin what used to be Caernarfon gaol, which has been closed since about the early 20th century and has now been converted into further council offices.
Caernarfon is also home to the regimental museum of the Royal Welch Fusiliers.
The location of the town creates a lovely view across the Menai Strait towards the south of Anglesey.
There is a small hospital in the town, 'Ysbyty Eryri' (i.e. "Snowdon Hospital"). The nearest large regional hospital is Ysbyty Gwynedd, in Bangor.
Previously, Caernarfon had been chosen as the location of a new prison. HMP Caernarfon would have held up to 800 adult males when constructed, and would have taken prisoners from all over the North Wales area. However, in September 2009 the UK Government withdrew plans to construct the prison.
Transport.
Caernarfon was at one time an important port, exporting slate from the Dyffryn Nantlle quarries.
Caernarvon railway station served the town from 1852 to 1970 and was one of the last passenger services to be closed under the Beeching Axe; it is now the site of a Morrisons supermarket. The site served as the terminus of the Bangor and Carnarvon Railway, and an end-on junction with the Carnarvonshire Railway and the Carnarvon and Llanberis Railway. All three companies were operated by and absorbed into the London and North Western Railway by 1871.
The route of the line southwards passed through a tunnel under central Caernarfon that was converted in 1995 for road traffic. The new Caernarfon railway station in St. Helen's Road is the northern terminus of the narrow gauge Rheilffordd Eryri / Welsh Highland Railway.
Bus services in the town are provided by Arriva Buses Wales, GHA Coaches, Express Motors and Padarn Bus.
Caernarfon Airport is 4.5 mi to the south west, and offers pleasure flights and an aviation museum.
Education.
There are four primary schools in Caernarfon, Ysgol yr Hendre being the largest. The others are Ysgol y Gelli, Ysgol Santes Helen and Ysgol Maesincla.
The single secondary school serving Caernarfon and the surrounding areas – Ysgol Syr Hugh Owen – currently has between 900 and 1000 pupils from ages 11 to 18.
Ysgol Pendalar, a school for children with special needs, serves all of Arfon.
Coleg Menai is a further education college for adult learners.
Sport.
Caernarfon Town F.C. is a football team that plays at The Oval, in Division One of the Welsh Alliance League. Caernarfon Wanderers play in Division Two of the Welsh Alliance.
There is a rugby union club, Clwb Rygbi Caernarfon, which plays in Division One North of the Swalec League. The club's home ground is Y Morfa.
Culture.
Caernarfon hosted the National Eisteddfod in 1862, 1894, 1906, 1921, 1935, 1959 and 1979. Unofficial National Eisteddfod events were also held there in 1877 and 1880. Caernarfon also hosted the 30th annual Celtic Media Festival in March 2009.
Cultural destinations include Galeri, Bocs and Oriel Pendeitsh.
Galeri is a creative enterprise centre that houses a gallery, a concert hall, cinema, a number of companies, and a range of other creative and cultural spaces.
Bocs is a young artists' co-operative and an arts centre that holds exhibitions and a range of cultural and creative events.
Oriel Pendeitsh is a ground-floor exhibition space adjoining the Tourist Information Centre opposite Caernarfon Castle. The gallery has a varied and changing programme of exhibitions throughout the year.
References.
</dl>

</doc>
<doc id="47623" url="http://en.wikipedia.org/wiki?curid=47623" title="Free Trade Area of the Americas">
Free Trade Area of the Americas

The Free Trade Area of the Americas (FTAA) (Spanish: "Área de Libre Comercio de las Américas" [ALCA], French: "Zone de libre-échange des Amériques" [ZLÉA], Portuguese: "Área de Livre Comércio das Américas" [ALCA], Dutch: "Vrijhandelszone van Amerika") was a proposed agreement to eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba.
History.
In the latest round of negotiations, trade ministers from 34 countries met in Miami, Florida, in the United States, in November 2003 to discuss the proposal. The proposed agreement was an extension of the North American Free Trade Agreement (NAFTA) between Canada, Mexico, and the United States. Opposing the proposal were Cuba, Venezuela, Bolivia, Ecuador, Dominica, and Nicaragua (all of which entered the Bolivarian Alternative for the Americas in response), and Mercosur member states.
Discussions have faltered over similar points as the Doha Development Round of World Trade Organization (WTO) talks; developed nations seek expanded trade in services and increased intellectual property rights, while less developed nations seek an end to agricultural subsidies and free trade in agricultural goods. Similar to the WTO talks, Brazil has taken a leadership role among the less developed nations, while the United States has taken a similar role for the developed nations.
Beginning.
Free Trade Area of the Americas began with the Summit of the Americas in Miami, Florida, on December 11, 1994, but the FTAA came to public attention during the Quebec City Summit of the Americas, held in Canada in 2001, a meeting targeted by massive anti-corporatization and anti-globalization protests. The Miami negotiations in 2003 met similar protests, though perhaps not as large.
Disagreements.
In previous negotiations, the United States had pushed for a single comprehensive agreement to reduce trade barriers for goods, while increasing intellectual property protection. Specific intellectual property protections could include Digital Millennium Copyright Act-style copyright protections similar to the U.S.-Australia Free Trade Agreement. Another protection would likely restrict the reimportation or cross-importation of pharmaceuticals, similar to the proposed agreement between the United States and Canada. Brazil proposed a three-track approach that calls for a series of bilateral agreements to reduce specific tariffs on goods, a hemispheric pact on rules of origin, and a dispute resolution process; Brazil proposed to omit the more controversial issues from the FTAA, leaving them to the WTO.
The location of the FTAA Secretariat was to have been determined in 2005. The contending cities are: Atlanta, Chicago, Galveston, Houston, San Juan, and Miami in the United States; Cancún and Puebla in Mexico; Panama City, Panama; and Port of Spain, Trinidad and Tobago. The U.S. city of Colorado Springs also submitted its candidacy in the early days but subsequently withdrew. Miami, Panama City and Puebla served successively as interim secretariat headquarters during the negotiation process.
The last summit was held at Mar del Plata, Argentina, in November 2005, but no agreement on FTAA was reached. Of the 34 countries present at the negotiations, 26 pledged to meet again in 2006 to resume negotiations, but no such meeting took place. The failure of the Mar del Plata summit to establish a comprehensive FTAA agenda augured poorly.
Current status.
The FTAA missed the targeted deadline of 2005, which followed the stalling of useful negotiations of the World Trade Organization Ministerial Conference of 2005. Over the next few years, some governments, most notably the United States, not wanting to lose any chance of hemispheric trade expansion moved in the direction of establishing a series of bilateral trade deals. The leaders however, planned further discussions at the Sixth Summit of the Americas in Cartagena, Colombia, in 2012.
Membership.
The following countries are in the plans of the Free Trade Area of the Americas:
Current support and opposition.
A vocal critic of the FTAA was Venezuelan president Hugo Chávez, who has described it as an "annexation plan" and a "tool of imperialism" for the exploitation of Latin America. As a counterproposal to this initiative, Chávez promoted the Bolivarian Alternative for the Americas ("Alternativa Bolivariana para las Américas", ALBA), based mostly on the model of the Eurasian Economic Community, which makes emphasis on energy and infrastructure agreements that are gradually extended to other areas finally to include the total economic, political and military integration of the member states. Also, Evo Morales of Bolivia has referred to the U.S.-backed Free Trade Area of the Americas, as "an agreement to legalize the colonization of the Americas".
On the other hand, the then presidents of Brazil, Luiz Inácio Lula da Silva, and Argentina, Néstor Kirchner, have stated that they do not oppose the FTAA but they do demand that the agreement provide for the elimination of U.S. agriculture subsidies, the provision of effective access to foreign markets and further consideration towards the needs and sensibilities of its members.
One of the most contentious issues of the treaty proposed by the United States is with concerns to patents and copyrights. Critics claim that if the measures proposed by the United States were implemented and applied this would reduce scientific research in Latin America. On the left-wing Council of Canadians web site, Barlow wrote: "This agreement sets enforceable global rules on patents, copyrights and trademark. It has gone far beyond its initial scope of protecting original inventions or cultural products and now permits the practice of patenting plants and animal forms as well as seeds. It promotes the private rights of corporations over local communities and their genetic heritage and traditional medicines".
On the weekend of April 20, 2001, the 3rd Summit of the Americas was a summit held in Quebec City, Canada. This international meeting was a round of negotiations regarding a proposed FTAA.
Agreements.
There are currently 34 countries in the Western Hemisphere, stretching from Canada to Chile that still have the FTAA as a long term goal. The Implementation of a full multilateral FTAA between all parties could be made possible by enlargement of existing agreements. North America, with the exception of Cuba and Haiti (which has participated in economic integration with the Caricom since 2002) has come close to setting up a subcontinental free trade area. At this point Agreements within the Area of the Americas include:

</doc>
<doc id="47624" url="http://en.wikipedia.org/wiki?curid=47624" title="Pawn (chess)">
Pawn (chess)

The pawn (♙♟) is the most numerous piece in the game of chess, and in most circumstances, also the weakest. It historically represents infantry, or more particularly, armed peasants or pikemen. Each player begins a game of chess with eight pawns, one on each square of the rank immediately in front of the other pieces. (In algebraic notation, the white pawns start on a2, b2, c2, ..., h2, while black pawns start on a7, b7, c7, ..., h7.)
Individual pawns are referred to by the file on which they stand. For example, one speaks of "White's f-pawn" or "Black's b-pawn", or less commonly (using descriptive notation), "White's king bishop pawn" or "Black's queen knight pawn". It is also common to refer to a "rook pawn", meaning any pawn on the a- or h-file, a "knight pawn" (on the b- or g-file), a "bishop pawn" (on the c- or f-file), a "queen pawn" (on the d-file), a "king pawn" (on the e-file), and a "central pawn" (on either the d- or e-file).
 
Movement.
Unlike the other pieces, pawns may not move backwards. Normally a pawn moves by advancing a single square, but the first time a pawn is moved, it has the option of advancing two squares. Pawns may not use the initial two-square advance to jump over an occupied square, or to capture. Any piece directly in front of a pawn, friend or foe, blocks its advance. In the diagram at right, the pawn on c4 may move to c5, while the pawn on e2 may move to either e3 or e4.
Capturing.
Unlike other pieces, the pawn does not capture in the same direction as it otherwise moves. A pawn captures diagonally, one square forward and to the left or right.
In the diagram to the left, the white pawn may capture either the black rook or the black knight.
Another unusual move is the "en passant" capture.
This arises when a pawn uses its initial move option to advance two squares instead of one, and in so doing passes over a square that is attacked by an enemy pawn. That enemy pawn, which would have been able to capture the moving pawn had it advanced only one square, is entitled to capture the moving pawn "in passing" "as if" it had advanced only one square. The capturing pawn moves into the empty square over which the moving pawn passed, and the moving pawn is removed from the board. In the diagram at right, the black pawn has just moved c7 to c5, so the white pawn may capture it by going from d5 to c6. The option to capture "en passant" must be exercised on the move immediately following the double-square pawn advance, or it is lost for the rest of the game. The "en passant" move was added to the pawn's repertoire in the 15th century to compensate for the then newly added two-square initial move rule . Without "en passant", a pawn could simply march past squares guarded by opposing pawns; "en passant" preserves the restrictive ability of pawns that have reached the fifth rank.
Promotion.
A pawn that advances all the way to the opposite side of the board (the opposing player's first rank) is "promoted" to another piece of that player's choice: a queen, rook, bishop, or knight of the same color. The pawn is immediately (before the opposing player's next move) replaced by the new piece. Since it is uncommon for a piece other than a queen to be chosen, promotion is often called "queening". When some other piece is chosen it is known as "underpromotion", and the piece selected is most often a knight, used to execute a checkmate or a fork giving the player a net increase in material compared to promoting to a queen. Underpromotion is also used in situations where promoting to a queen would give immediate stalemate.
The choice of promotion is not limited to pieces that have been captured. Thus a player could in theory have as many as ten knights, ten bishops, ten rooks or nine queens on the board at the same time. While this extreme would almost never occur in practice, in game 11 of their 1927 world championship match, José Raúl Capablanca and Alexander Alekhine each had two queens in play at once (from move 65 through the end on move 66). While some finer sets do include an extra queen of each color, most standard chess sets do not come with additional pieces, so the physical piece used to replace a promoted pawn on the board is usually one that was previously captured. When the correct piece is not available, some substitute is used: a second queen is often indicated by inverting a previously captured rook, or a piece is borrowed from another set.
Strategy.
The "pawn structure", the configuration of pawns on the chessboard, mostly determines the strategic flavor of a game. While other pieces can usually be moved to a more favorable position if they are temporarily badly placed, a poorly positioned pawn is limited in its movement and often cannot be so relocated.
Because pawns capture diagonally and can be blocked from moving straight forward, opposing pawns can become locked in diagonal <dfn id="">pawn chains</dfn> of two or more pawns of each color, where each player controls squares of one color. In the diagram, Black and White have locked their d- and e-pawns.
Here, White has a long-term space advantage. White will have an easier time than Black in finding good squares for his pieces, particularly with an eye to the kingside. Black, in contrast, suffers from a <dfn id=">bad bishop</dfn> on c8, which is prevented by the black pawns from finding a good square or helping out on the kingside. On the other hand, White's central pawns are somewhat <dfn id=">overextended</dfn> and vulnerable to attack. Black can undermine the white pawn chain with an immediate c7–c5 and perhaps a later f7–f6.
Isolated pawn.
Pawns on adjacent files can support each other in attack and defense. A pawn which has no friendly pawns in adjacent files is an "isolated pawn". The square in front of an isolated pawn may become an enduring weakness. Any piece placed directly in front not only blocks the advance of that pawn, but cannot be driven away by other pawns.
In the diagram at right, Black has an isolated pawn on d5. If all the pieces except the kings and pawns were removed, the weakness of that pawn might prove fatal to Black in the endgame. In the middlegame, however, Black has slightly more freedom of movement than White, and may be able to trade off the isolated pawn before an endgame ensues.
Passed pawn.
A pawn which cannot be blocked or captured by enemy pawns in its advance to promotion is a "passed pawn". In the diagram at right, White has a "protected" passed pawn on c5 and Black has an "outside" passed pawn on h5. Because endgames are often won by the player who can promote a pawn first, having a passed pawn in an endgame can be decisive – especially a protected passed pawn (a passed pawn that is protected by a pawn). In this vein, a "pawn majority", a greater number of pawns belonging to one player on one side of the chessboard, is strategically important because it can often be converted into a passed pawn.
The diagrammed position might appear roughly equal, because each side has a king and three pawns, and the positions of the kings are about equal. In truth, White wins this endgame on the strength of the protected passed pawn, regardless which player moves first. The black king cannot be on both sides of the board at once – to defend the isolated h-pawn and to stop White's c-pawn from advancing to promotion. Thus White can capture the h-pawn and then win the game .
Doubled pawn.
After a capture with a pawn, a player may end up with two pawns on the same file, called "doubled pawns". Doubled pawns are substantially weaker than pawns which are side by side, because they cannot defend each other, they usually cannot both be defended by adjacent pawns, and the front pawn blocks the advance of the back one. In the diagram at right, Black is playing at a strategic disadvantage due to the doubled c-pawns.
There are situations where doubled pawns confer some advantage, typically when the guarding of consecutive squares in a file by the pawns prevents an invasion by the opponent's pieces.
Pawns which are both doubled and isolated are typically a tangible weakness. A single piece or pawn in front of doubled isolated pawns blocks both of them, and cannot be easily dislodged. It is rare for a player to have three pawns in a file, i.e. "tripled" pawns. Depending on the position, tripled pawns may be more or less valuable than two pawns which are side by side.
Wrong rook pawn.
In chess endgames with a bishop, a rook pawn may be the "wrong rook pawn", depending on the square-color of the bishop. This causes some positions to be draws which would otherwise be wins.
History.
The pawn has its origins in the oldest version of chess, chaturanga, and it is present in all other significant versions of the game as well. In chaturanga, this piece moved directly forward, capturing to the sides (one square diagonally forward to the left or right).
In medieval chess, an attempt was made to make the pieces more interesting, each file's pawn being given the name of a commoner's occupation. On the board, from left to right, those titles were:
The most famous example of this is found in the second book ever printed in the English language, The Game and Playe of the Chesse. Purportedly, this book was viewed to be as much a political commentary on society as a chess book, and was printed by William Caxton. It was, like the Bible, among the most popular books of its day.
The ability to move two spaces, and the related ability to capture "en passant", were only introduced in 15th-century Europe (see En-passant (Historical Context)). The rule for promotion has changed through history, see History of the Promotion rule (Chess).
Etymology and word usage.
Although the name origin of most other chess pieces is obvious, the etymology of "pawn" is fairly obscure. It is derived from the Old French word "paon," which comes from the Medieval Latin term for "foot soldier" and is cognate with "peon".
"Pawn" is often taken to mean "one who is easily manipulated" or "one who is sacrificed for a larger purpose". Because the pawn is the weakest piece, it is often used metaphorically to indicate unimportance or outright disposability, for example, "She's only a pawn in their game."
In most other languages, the word for pawn is similarly derived from "paon", its Latin ancestor or some other word for foot soldier. Exceptions include the Irish "fichillín", which means "little chess", and the German "Bauer", meaning "farmer" or "peasant".
Unicode.
Unicode defines two codepoints for pawn:
♙ U+2659 White Chess Pawn (HTML &#9817;)
♟ U+265F Black Chess Pawn (HTML &#9823;)
References.
Notes
References
</dl>

</doc>
<doc id="47625" url="http://en.wikipedia.org/wiki?curid=47625" title="Yarkovsky effect">
Yarkovsky effect

The Yarkovsky effect is a force acting on a rotating body in space caused by the anisotropic emission of thermal photons, which carry momentum. It is usually considered in relation to meteoroids or small asteroids (about 10 cm to 10 km in diameter), as its influence is most significant for these bodies.
History of discovery.
The effect was discovered by the Russian civil engineer Ivan Osipovich Yarkovsky (1844–1902), who worked on scientific problems in his spare time. Writing in a pamphlet around the year 1900, Yarkovsky noted that the diurnal heating of a rotating object in space would cause it to experience a force that, while tiny, could lead to large long-term effects in the orbits of small bodies, especially meteoroids and small asteroids. Yarkovsky's insight would have been forgotten had it not been for the Estonian astronomer Ernst J. Öpik (1893–1985), who read Yarkovsky's pamphlet sometime around 1909. Decades later, Öpik, recalling the pamphlet from memory, discussed the possible importance of the Yarkovsky effect on movement of meteoroids about the Solar System.
Mechanism.
The Yarkovsky effect is a consequence of the fact that change in the temperature of an object warmed by radiation (and therefore the intensity of thermal radiation from the object) lags behind changes in the incoming radiation. That is, the surface of the object takes time to become warm when first illuminated; and takes time to cool down when illumination stops. In general there are two components to the effect:
In general, the effect is size dependent, and will affect the semi-major axis of smaller asteroids, while leaving large asteroids practically unaffected. For kilometre-sized asteroids, the Yarkovsky effect is minuscule over short periods: the force on asteroid 6489 Golevka has been estimated at about 0.25 newton, for a net acceleration of 10−10 m/s². But it is steady; over millions of years an asteroid's orbit can be perturbed enough to transport it from the asteroid belt to the inner Solar System.
The above details can become more complicated for bodies in strongly eccentric orbits. 
Measurement.
The effect was first measured in 1991–2003 on the asteroid 6489 Golevka. The asteroid drifted 15 km from its predicted position over twelve years (the orbit was established with great precision by a series of radar observations in 1991, 1995 and 1999) from the Arecibo radio telescope.
Without direct measurement, it is very hard to predict the exact impact of the Yarkovsky effect on a given asteroid's orbit. This is because the magnitude of the effect depends on many variables that are hard to determine from the limited observational information that is available. These include the exact shape of the asteroid, its orientation, and its albedo. Calculations are further complicated by the effects of shadowing and thermal "reillumination", whether caused by local craters or a possible overall concave shape. The Yarkovsky effect also competes with radiation pressure, whose net effect may cause similar small long-term forces for bodies with albedo variations and/or non-spherical shapes.
As an example, even for the simple case of the pure seasonal Yarkovsky effect on a spherical body in a circular orbit with 90° obliquity, semi-major axis changes could differ by as much as a factor of two between the case of a uniform albedo and the case of a strong north/south albedo asymmetry. Depending on the object's orbit and spin axis, the Yarkovsky change of the semi-major axis may be reversed simply by changing from a spherical to a non-spherical shape.
Despite these difficulties, utilizing the Yarkovsky effect is one scenario under investigation to alter the course of potentially Earth-impacting near-Earth asteroids. Possible asteroid deflection strategies include "painting" the surface of the asteroid or focusing solar radiation onto the asteroid to alter the intensity of the Yarkovsky effect and so alter the orbit of the asteroid away from a collision with Earth.

</doc>
<doc id="47627" url="http://en.wikipedia.org/wiki?curid=47627" title="Llywelyn ap Gruffudd">
Llywelyn ap Gruffudd

Llywelyn ap Gruffudd (c. 1223 – 11 December 1282), sometimes written as Llywelyn ap Gruffydd, also known as Llywelyn the Last, or, in Welsh, Llywelyn Ein Llyw Olaf ("Llywelyn, Our Last Leader"), was King of Wales from 1258, until his death at Cilmeri, in 1282. The son of Gruffudd ap Llywelyn Fawr and grandson of Llywelyn the Great, he was the last sovereign prince and king of Wales before its conquest by Edward I of England.
Genealogy and early life.
Llywelyn was the second of the four sons of Gruffudd, the eldest son of Llywelyn the Great, and Senana ferch Caradog, the daughter of Caradoc ap Thomas ap Rhodri, Lord of Anglesey. The eldest was Owain Goch ap Gruffudd and Llywelyn had two younger brothers, Dafydd ap Gruffudd and Rhodri ap Gruffudd. Llywelyn is thought to have been born around 1222 or 1223. He is first heard of holding lands in the Vale of Clwyd around 1244.
Following his grandfather's death in 1240, Llywelyn's uncle, Dafydd ap Llywelyn (who was Llywelyn the Great's eldest legitimate son), succeeded him as ruler of Gwynedd. Llywelyn's father, Gruffudd (who was Llywelyn's eldest son but illegitimate), and his brother, Owain, were initially kept prisoner by Dafydd, then transferred into the custody of King Henry III of England. Gruffudd died in 1244, from a fall while trying to escape from his cell at the top of the Tower of London. The window from which he attempted to escape the Tower was bricked up and can still be seen to this day.
This freed Dafydd ap Llywelyn's hand as King Henry could no longer use Gruffudd against him, and war broke out between him and King Henry in 1245. Llywelyn supported his uncle in the savage fighting that followed. Owain, meanwhile, was freed by Henry after his father's death in the hope that he would start a civil war in Gwynedd, but stayed in Chester, so when Dafydd died in February 1246 without leaving an heir, Llywelyn had the advantage of being on the spot.
Early reign.
Llywelyn and Owain came to terms with King Henry and in 1247, signed the Treaty of Woodstock at Woodstock Palace. The terms they were forced to accept restricted them to Gwynedd Uwch Conwy, the part of Gwynedd west of the River Conwy, which was divided between them. Gwynedd Is Conwy, east of the river, was taken over by King Henry.
When Dafydd ap Gruffudd came of age, King Henry accepted his homage and announced his intention to give him part of the already reduced Gwynedd. Llywelyn refused to accept this, and Owain and Dafydd formed an alliance against him. This led to the Battle of Bryn Derwin in June 1255. Llywelyn defeated Owain and Dafydd and captured them, thereby becoming sole ruler of Gwynedd Uwch Conwy. Llywelyn now looked to expand his area of control. The population of Gwynedd Is Conwy resented English rule. This area, also known as "Perfeddwlad"(meaning 'middle land') had been given by King Henry to his son Edward and during the summer of 1256, he visited the area, but failed to deal with grievances against the rule of his officers. An appeal was made to Llywelyn, who, that November, crossed the River Conwy with an army, accompanied by his brother, Dafydd, whom he had released from prison. By early December, Llywelyn controlled all of Gwynedd Is Conwy apart from the royal castles at Dyserth and Dnoredudd as a reward for his support and dispossessing his brother-in-law, Rhys Fychan, who supported the king. An English army led by Stephen Bauzan invaded to try to restore Rhys Fychan but was decisively defeated by Welsh forces at the Battle of Cadfan in June 1257, with Rhys having previously slipped away to make his peace with Llywelyn.
Rhys Fychan now accepted Llywelyn as overlord, but this caused problems for Llywelyn, as Rhys's lands had already been given to Maredudd. Llywelyn restored his lands to Rhys, but the king's envoys approached Maredudd and offered him Rhys's lands if he would change sides. Maredudd paid homage to Henry in late 1257. By early 1258, Llywelyn was using the title Prince of Wales, first used in an agreement between Llywelyn and his supporters and the Scottish nobility associated with the Comyn family. The English Crown refused to recognise this title however, and in 1263, Llywelyn's brother, Dafydd, went over to King Henry.
On 12 December 1263 in the commote of Ystumanner, Gruffydd ap Gwenwynwyn did homage and swore fealty to Llywelyn. In return he was made a vassal lord and the lands taken from him by Llywelyn about six years earlier were restored to him.
In England, Simon de Montfort (the Younger) defeated the king's supporters at the Battle of Lewes in 1264, capturing the king and Prince Edward. Llywelyn began negotiations with de Montfort, and in 1265, offered him 30,000 marks in exchange for a permanent peace, in which Llywelyn's right to rule Wales would be acknowledged. The Treaty of Pipton, 22 June 1265, established an alliance between Llywelyn and de Montfort, but the very favourable terms given to Llywelyn in this treaty were an indication of de Montfort's weakening position. De Montfort was to die at the Battle of Evesham in 1265, a battle in which Llywelyn took no part.
Supremacy in Wales.
After Simon de Montfort's death, Llywelyn launched a campaign in order to rapidly gain a bargaining position before King Henry had fully recovered. In 1265, Llywelyn captured Hawarden Castle and routed the combined armies of Hamo Lestrange and Maurice fitz Gerald in north Wales. Llywelyn then moved on to Brycheiniog, and in 1266, he routed Roger Mortimer's army. With these victories and the backing of the papal legate, Ottobuono, Llywelyn opened negotiations with the king, and was eventually recognised as Prince of Wales by King Henry in the Treaty of Montgomery in 1267. In return for the title, the retention of the lands he had conquered and the homage of almost all the native rulers of Wales, he was to pay a tribute of 25,000 marks in yearly instalments of 3,000 marks, and could if he wished, purchase the homage of the one outstanding native prince - Maredudd ap Rhys of Deheubarth - for another 5,000 marks. However, Llywelyn's territorial ambitions gradually made him unpopular with some minor Welsh leaders, particularly the princes of south Wales.
The Treaty of Montgomery marked the high point of Llywelyn's power. Problems began arising soon afterwards, initially a dispute with Gilbert de Clare concerning the allegiance of a Welsh nobleman holding lands in Glamorgan. Gilbert built Caerphilly Castle in response to this. King Henry sent a bishop to take possession of the castle while the dispute was resolved but when Gilbert regained the castle by trickery, the king was unable to do anything about it.
Following the death of King Henry in late 1272, with the new King Edward I of England away from the kingdom, the rule fell to three men. One of them, Roger Mortimer was one of Llywelyn's rivals in the marches. When Humphrey de Bohun tried to take back Brycheiniog, which was granted to Llywelyn by the Treaty of Montgomery, Mortimer supported de Bohun. Llywelyn was also finding it difficult to raise the annual sums required under the terms of this treaty, and ceased making payments.
In early 1274, there was a plot by Llywelyn's brother, Dafydd, and Gruffudd ap Gwenwynwyn of Powys Wenwynwyn and his son, Owain, to kill Llywelyn. Dafydd was with Llywelyn at the time, and it was arranged that Owain would come with armed men on 2 February to carry out the assassination; however, he was prevented by a snowstorm. Llywelyn did not discover the full details of the plot until Owain confessed to the Bishop of Bangor. He said that the intention had been to make Dafydd prince of Gwynedd, and that Dafydd would reward Gruffudd with lands. Dafydd and Gruffudd fled to England where they were maintained by the king and carried out raids on Llywelyn's lands, increasing Llywelyn's resentment. When Edward called Llywelyn to Chester in 1275 to pay homage, Llywelyn refused to attend.
Llywelyn also made an enemy of King Edward by continuing to ally himself with the family of Simon de Montfort, even though their power was now greatly reduced. Llywelyn sought to marry Eleanor de Montfort, born in 1252, Simon de Montfort's daughter. They were married by proxy in 1275, but King Edward took exception to the marriage, in part because Eleanor was his first cousin: her mother was Eleanor of England, daughter of King John and princess of the House of Plantagenet. When Eleanor sailed from France to meet Llywelyn, Edward hired pirates to seize her ship and she was imprisoned at Windsor Castle until Llywelyn made certain concessions.
In 1276, Edward declared Llywelyn a rebel and in 1277, gathered an enormous army to march against him. Edward's intention was to disinherit Llywelyn completely and take over Gwynedd Is Conwy himself. He was considering two options for Gwynedd Uwch Conwy: either to divide it between Llywelyn's brothers, Dafydd and Owain, or to annex Anglesey and divide only the mainland between the two brothers. Edward was supported by Dafydd ap Gruffudd and Gruffudd ap Gwenwynwyn. Many of the lesser Welsh princes who had supported Llywelyn now hastened to make peace with Edward. By the summer of 1277, Edward's forces had reached the River Conwy and encamped at Deganwy, while another force had captured Anglesey and took possession of the harvest there. This deprived Llywelyn and his men of food, forcing them to seek terms.
Treaty of Aberconwy.
What resulted was the Treaty of Aberconwy, which guaranteed peace in Gwynedd in return for several difficult concessions from Llywelyn, including confining his authority to Gwynedd Uwch Conwy once again. Part of Gwynedd Is Conwy was given to Dafydd ap Gruffudd, with a promise that if Llywelyn died without an heir, he would be given a share of Gwynedd Uwch Conwy instead.
Llywelyn was forced to acknowledge the English king as his sovereign; initially he had refused, but after the events of 1276, Llywelyn was stripped of all but a small portion of his lands. He went to meet Edward, and found Eleanor lodged with the royal family at Worcester; after Llywelyn agreed to Edward's demands, Edward gave them permission to be married at Worcester Cathedral. A stained glass window exists to this day depicting the wedding of the Prince of Wales and Lady Eleanor. By all accounts, the marriage was a genuine love match; Llywelyn is not known to have fathered any illegitimate children, which is extremely unusual for the Welsh royalty. (In medieval Wales, illegitimate children were as entitled to their father's property as legitimate children.)
Last campaign and death.
By early 1282, many of the lesser princes who had supported Edward against Llywelyn in 1277 were becoming disillusioned with the exactions of the royal officers. On Palm Sunday that year, Dafydd ap Gruffudd attacked the English at Hawarden Castle and then laid siege to Rhuddlan. The revolt quickly spread to other parts of Wales, with Aberystwyth castle captured and burnt and rebellion in Ystrad Tywi in south Wales, also inspired by Dafydd according to the annals, where Carreg Cennen castle was captured.
Llywelyn, according to a letter he sent to the Archbishop of Canterbury John Peckham, was not involved in the planning of the revolt. He felt obliged, however, to support his brother and a war began for which the Welsh were ill-prepared. Personal tragedy also struck him at this time when, on or about 19 June 1282, his wife Eleanor de Montfort, died shortly after giving birth to their daughter Gwenllian.
Events followed a similar pattern to 1277, with Edward's forces capturing Gwynedd Is Conwy, Anglesey and taking the harvest. The force occupying Anglesey were defeated, however, when trying to cross to the mainland in the Battle of Moel-y-don. The Archbishop of Canterbury tried mediating between Llywelyn and Edward, and Llywelyn was offered a large estate in England if he would surrender Wales to Edward, while Dafydd was to go on crusade and not return without the king's permission. In an emotional reply, which has been compared to the Declaration of Arbroath, Llywelyn said he would not abandon the people whom his ancestors had protected since "the days of Kamber son of Brutus". The offer was refused.
Llywelyn now left Dafydd to lead the defence of Gwynedd and took a force south, trying to rally support in mid and south Wales and open up an important second front. On 11 December at the Battle of Orewin Bridge at Builth Wells, he was killed while separated from his army. The exact circumstances are unclear and there are two conflicting accounts of his death. Both accounts agree that Llywelyn was tricked into leaving the bulk of his army and was then attacked and killed. The first account says that Llywelyn and his chief minister approached the forces of Edmund Mortimer and Hugh Le Strange after crossing a bridge. They then heard the sound of battle as the main body of his army was met in battle by the forces of Roger Despenser and Gruffudd ap Gwenwynwyn. Llywelyn turned to rejoin his forces and was pursued by a lone lancer who struck him down. It was not until some time later that an English knight recognised the body as that of the prince. This version of events was written in the north of England some fifty years later and has suspicious similarities with details about the Battle of Stirling Bridge in Scotland. An alternative version of events written in the east of England by monks in contact with Llywelyn's exiled daughter, Gwenllian ferch Llywelyn, and niece, Gwladys ferch Dafydd, states that Llywelyn, at the front of his army, approached the combined forces of Edmund and Roger Mortimer, Hugo Le Strange and Gruffudd ap Gwenwynwyn on the promise that he would receive their homage. This was a deception. His army was immediately engaged in fierce battle during which a significant section of it was routed, causing Llywelyn and his eighteen retainers to become separated. At around dusk, Llywelyn and a small group of his retainers (which included clergy), were ambushed and chased into a wood at Aberedw. Llywelyn was surrounded and struck down. As he lay dying, he asked for a priest and gave away his identity. He was then killed and his head hewn from his body. His person was searched and various items recovered, including a list of "conspirators", (which may well have been faked), and his privy seal. He may have been slain by an Anthony Tipton, later known as Sir Anthony De Tipton.
"If the king wishes to have the copy "[of the list]" found in the breeches of Llywelyn, he can have it from Edmund Mortimer, who has custody of it and also of Llywelyn’s privy seal and certain other things found in the same place." Archbishop Peckham, in his first letter to Robert Bishop of Bath and Wells, dated 17 December 1282 (Lambeth Palace Archives)
There are legends surrounding the fate of Llywelyn's severed head. It is known that it was sent to Edward at Rhuddlan and after being shown to the English troops based in Anglesey, Edward sent the head on to London. In London, it was set up in the city pillory for a day, and crowned with ivy (i.e. to show he was a "king" of Outlaws and in mockery of the ancient Welsh prophecy, which said that a Welshman would be crowned in London as king of the whole of Britain). Then it was carried by a horseman on the point of his lance to the Tower of London and set up over the gate. It was still on the Tower of London 15 years later.
The last resting place of Llywelyn's body is not known for certain, however it has always been tradition that it was interred at the Cistercian Abbey at Abbeycwmhir. On 28 December 1282 Archbishop Peckham wrote a letter to the Archdeacon of Brecon at Brecon Priory, in order to;
"...inquire and clarify if the body of Llywelyn has been buried in the church of Cwmhir, and he was bound to clarify the latter before the feast of Epiphany, because he had another mandate on this matter, and ought to have certified the lord Archbishop before Christmas, and has not done so."
There is further supporting evidence for this hypothesis in the Chronicle of Florence of Worcester;
"As for the body of the Prince, his mangled trunk, it was interred in the Abbey of Cwm Hir, belonging to the Cistercian Order."
Another theory is that his body was transferred to Llanrumney Hall in Cardiff.
The poet Gruffudd ab yr Ynad Coch wrote in an elegy on Llywelyn:
There is an enigmatic reference in the Welsh annals Brut y Tywysogion, "…and then Llywelyn was betrayed in the belfry at Bangor by his own men". No further explanation is given.
Annexation.
With the loss of Llywelyn, Welsh morale and the will to resist diminished, Dafydd was Llywelyn's named successor. He carried on the struggle for several months, but in June 1283 was captured in the uplands above Abergwyngregyn at Bera Mountain, together with his family, brought before Edward, then taken to Shrewsbury where a special session of Parliament condemned him to death. He was dragged through the streets, hanged, drawn and quartered.
After the final defeat of 1283, Gwynedd was stripped of all royal insignia, relics, and regalia. Edward took particular delight in appropriating the royal home of the Gwynedd dynasty. In August 1284, he set up his court at Abergwyngregyn, Gwynedd. With equal deliberateness, he removed all the insignia of majesty from Gwynedd; a coronet was solemnly presented to the shrine of St. Edward at Westminster; the matrices of the seals of Llywelyn, of his wife, and his brother Dafydd were melted down to make a chalice which was given by the king to Vale Royal Abbey where it remained until the dissolution of that institution in 1538 (after which it came into the possession of the family of the final abbot) The most precious religious relic in Gwynedd, the fragment of the True Cross known as Cross of Neith, was paraded through London in May 1285 in a solemn procession on foot led by the king, the queen, the archbishop of Canterbury and fourteen bishops, and the magnates of the realm. Edward was thereby appropriating the historical and religious regalia of the house of Gwynedd and placarding to the world the extinction of its dynasty and the annexation of the principality to his Crown. Commenting on this a contemporary chronicler is said to have declared "and then all Wales was cast to the ground."
Most of Llywelyn's relatives ended their lives in captivity — with the notable exceptions of his younger brother Rhodri, who had long since sold his claim to the crown and endeavoured to keep a very low profile, and a distant cousin, Madog ap Llywelyn, who led a future revolt and claimed the title Prince of Wales in 1294. Llywelyn and Eleanor's baby daughter Gwenllian of Wales was captured by Edward's troops in 1283. She was interned at Sempringham Priory in England for the rest of her life, becoming a nun in 1317 and dying without issue in 1337, probably knowing little of her heritage and speaking none of her language.
Dafydd's two surviving sons were captured and incarcerated at Bristol Gaol, where they eventually died many years later. Llywelyn's elder brother Owain Goch disappears from the record in 1282 and the presumption is that he was murdered. Llywelyn's surviving brother Rhodri (who had been exiled from Wales since 1272) survived and held manors in Gloucestershire, Cheshire, Surrey, and Powys and died around 1315. His grandson, Owain Lawgoch, later claimed the title Prince of Wales.
Historical fiction.
The life of Llywelyn the Last is the subject of Edith Pargeter's "Brothers of Gwynedd Quartet": 'Sunrise in the West' (1974); 'The Dragon at Noonday' (1975); 'The Hounds of Sunset' (1976); and 'Afterglow and Nightfall' (1977).
The stories of Llywelyn Fawr, Llywelyn ap Gryffydd and Dafydd ap Gryffydd are depicted in Sharon Penman's "Welsh Trilogy": 'Here be Dragons' (1985); 'Falls the Shadow' (1988); and 'The Reckoning' (1991). Also, "A Memory of Love" by Bertrice Small.
An alternate history/time travel series, "After Cilmeri" by Sarah Woodbury, explores what might have happened if Llywelyn had survived the ambush at Cilmeri, and had a son and assistance from people from the future.
Llywelyn the Last is the subject of the New Riders of the Purple Sage song "Llewellyn". The song focuses on the Conquest of Wales by Edward I, but specifically on the Campaign of 1282-83. In the song, the band claims "In September, Edward [Edward I] moved up to the baird/ His forces stronger every day/ Llewellyn then turned southward bound/ His forces lay upon the ground." It also claims that the message of Llywelyn's death came "soon thereafter."

</doc>
<doc id="47628" url="http://en.wikipedia.org/wiki?curid=47628" title="Bomb">
Bomb

A bomb is one of a range of explosive weapons that only rely on the exothermic reaction of an explosive material to provide an extremely sudden and violent release of energy (an explosive device). Detonations inflict damage principally through ground- and atmosphere-transmitted mechanical stress, the impact and penetration of pressure-driven projectiles, pressure damage, and explosion-generated effects. Bombs have been in use since the 11th century in Song Dynasty China.
The term bomb is not usually applied to explosive devices used for civilian purposes such as construction or mining, although the people using the devices may sometimes refer to them as a "bomb". The military use of the term "bomb", or more specifically aerial bomb action, typically refers to airdropped, unpowered explosive weapons most commonly used by air forces and naval aviation. Other military explosive weapons not classified as "bombs" include grenades, shells, depth charges (used in water), warheads when in missiles, or land mines. In unconventional warfare, "bomb" can refer to a range of offensive weaponry. For instance, in recent conflicts, "bombs" known as improvised explosive devices (IEDS) have been employed by insurgent fighters to great effectiveness.
The word comes from the Latin "bombus", which in turn comes from the Greek βόμβος ("bombos"), an onomatopoetic term meaning "booming", "buzzing".
History.
Explosive bombs were used in China in 1221, by a Jin Dynasty army against a Song Dynasty city. Bombs built using bamboo tubes appear in the 11th century. Bombs made of cast iron shells packed with explosive gunpowder date to 13th century China. The term was coined for this bomb (i.e. "thunder-crash bomb") during a Jin Dynasty (1115–1234) naval battle of 1231 against the Mongols. The "History of Jin" 《金史》 (compiled by 1345) states that in 1232, as the Mongol general Subutai (1176–1248) descended on the Jin stronghold of Kaifeng, the defenders had a "thunder-crash bomb" which "consisted of gunpowder put into an iron container ... then when the fuse was lit (and the projectile shot off) there was a great explosion the noise whereof was like thunder, audible for more than a hundred "li", and the vegetation was scorched and blasted by the heat over an area of more than half a "mou". When hit, even iron armour was quite pierced through." The Song Dynasty (960–1279) official Li Zengbo wrote in 1257 that arsenals should have several hundred thousand iron bomb shells available and that when he was in Jingzhou, about one to two thousand were produced each month for dispatch of ten to twenty thousand at a time to Xiangyang and Yingzhou. The Ming Dynasty text "Huolongjing" describes the use of poisonous gunpowder bombs, including the "wind-and-dust" bomb.
During the Mongol invasions of Japan, the Mongols used the explosive "thunder-crash bombs" against the Japanese. Archaeological evidence of the "thunder-crash bombs" has been discovered in an underwater shipwreck off the shore of Japan by the Kyushu Okinawa Society for Underwater Archaeology. X-rays by Japanese scientists of the excavated shells confirmed that they contained gunpowder.
Shock.
Explosive shock waves can cause situations such as body displacement (i.e., people being thrown through the air), dismemberment, internal bleeding and ruptured eardrums.
Shock waves produced by explosive events have two distinct components, the positive and negative wave. The positive wave shoves outward from the point of detonation, followed by the trailing vacuum space "sucking back" towards the point of origin as the shock bubble collapses.
The greatest defense against shock injuries is distance from the source of shock. As a point of reference, the overpressure at the Oklahoma City bombing was estimated in the range of 28 MPa.
Heat.
A thermal wave is created by the sudden release of heat caused by an explosion. Military bomb tests have documented temperatures of up to 2,480 °C (4,500 °F). While capable of inflicting severe to catastrophic burns and causing secondary fires, thermal wave effects are considered very limited in range compared to shock and fragmentation. This rule has been challenged, however, by military development of thermobaric weapons, which employ a combination of negative shock wave effects and extreme temperature to incinerate objects within the blast radius. This would be fatal to humans, as bomb tests have proven.
Fragmentation.
Fragmentation is produced by the acceleration of shattered pieces of bomb casing and adjacent physical objects. The use of fragmentation in bombs dates to the 14th century, and appears in the Ming Dynasty text "Huolongjing". The fragmentation bombs were filled with iron pellets and pieces of broken porcelain. Once the bomb explodes, the resulting shrapnel is capable of piercing the skin and blinding enemy soldiers.
While conventionally viewed as small metal shards moving at super-supersonic and hypersonic speeds, fragmentation can occur in epic proportions and travel for extensive distances. When the S.S. Grandcamp exploded in the Texas City Disaster on April 16, 1947, one fragment of that blast was a two-ton anchor which was hurled nearly two miles inland to embed itself in the parking lot of the Pan American refinery. Fragmentation should not be confused with shrapnel, which relies on the momentum of a shell to cause damage.
Effects on living things.
To people who are close to a blast incident, such as bomb disposal technicians, soldiers wearing body armor, deminers or individuals wearing little to no protection, there are four types of blast effects on the human body: overpressure (shock), fragmentation, impact and heat. Overpressure refers to the sudden and drastic rise in ambient pressure that can damage the internal organs, possibly leading to permanent damage or death. Fragmentation includes the shrapnel described above but can also include sand, debris and vegetation from the area surrounding the blast source. This is very common in anti-personnel mine blasts. The projection of materials poses a potentially lethal threat caused by cuts in soft tissues, as well as infections, and injuries to the internal organs. When the overpressure wave impacts the body it can induce violent levels of blast-induced acceleration. Resulting injuries range from minor to unsurvivable. Immediately following this initial acceleration, deceleration injuries can occur when a person impacts directly against a rigid surface or obstacle after being set in motion by the force of the blast. Finally, injury and fatality can result from the explosive fireball as well as incendiary agents projected onto the body. Personal protective equipment, such as a bomb suit or demining ensemble, as well as helmets, visors and foot protection, can dramatically reduce the four effects, depending upon the charge, proximity and other variables.
Types.
Experts commonly distinguish between civilian and military bombs. The latter are almost always mass-produced weapons, developed and constructed to a standard design out of standard components and intended to be deployed in a standard explosive device. IEDs are divided into three basic categories by basic size and delivery. Type 76, IEDs are hand-carried parcel or suitcase bombs, type 80, are "suicide vests" worn by a bomber, and type 3 devices are vehicles laden with explosives to act as large-scale stationary or self-propelled bombs, also known as VBIED (vehicle-borne IEDs).
Improvised explosive materials are typically very unstable and subject to spontaneous, unintentional detonation triggered by a wide range of environmental effects ranging from impact and friction to electrostatic shock. Even subtle motion, change in temperature, or the nearby use of cellphones or radios, can trigger an unstable or remote-controlled device. Any interaction with explosive materials or devices by unqualified personnel should be considered a grave and immediate risk of death or dire injury. The safest response to finding an object believed to be an explosive device is to get as far away from it as possible.
Atomic bombs are based on the theory of nuclear fission, that when a large atom splits it releases a massive amount of energy. Hydrogen bombs use the energy from an initial fission explosion to create an even more powerful fusion explosion.
The term dirty bomb refers to a specialized device that relies on a comparatively low explosive yield to scatter harmful material over a wide area. Most commonly associated with radiological or chemical materials, dirty bombs seek to kill or injure and then to deny access to a contaminated area until a thorough clean-up can be accomplished. In the case of urban settings, this clean-up may take extensive time, rendering the contaminated zone virtually uninhabitable in the interim.
The power of large bombs is typically measured in kilotons (kt) or megatons of TNT (Mt). The most powerful bombs ever used in combat were the two atomic bombs dropped by the United States to attack Hiroshima and Nagasaki, and the most powerful ever tested was the Tsar Bomba. The most powerful non-nuclear bomb is Russian "Father of All Bombs" (officially Aviation Thermobaric Bomb of Increased Power (ATBIP)) followed by the United States Air Force's MOAB (officially Massive Ordnance Air Blast, or more commonly known as the "Mother of All Bombs").
Below is a list of five different types of bombs based on the fundamental explosive mechanism they employ.
Compressed gas.
Relatively small explosions can be produced by pressurizing a container until catastrophic failure such as with a dry ice bomb. Technically, devices that create explosions of this type can not be classified as "bombs" by the definition presented at the top of this article. However, the explosions created by these devices can cause property damage, injury, or death. Flammable liquids, gasses and gas mixtures dispersed in these explosions may also ignite if exposed to a spark or flame.
Low explosive.
The simplest and oldest type of bombs store energy in the form of a low explosive. Black powder is an example of a low explosive. Low explosives typically consist of a composition of an oxidizing salt, such as potassium nitrate, and solid fuel, such as charcoal or aluminum powder. These compositions deflagrate upon ignition producing hot gas. Under normal circumstances deflagration occurs too slowly to produce a significant pressure wave. Low explosives must, therefore, be used in large quantities or confined in a container with a high burst pressure to be used as a bomb.
High explosive.
A high explosive bomb is one that employs a process called "detonation" to rapidly release its chemical energy. Detonation is distinct from deflagration in that the chemical reaction propagates faster than the speed of sound (often many times faster) in an intense shock wave. Therefore, the pressure wave produced by a high explosive is not significantly increased by confinement as detonation occurs so quickly that the resulting plasma does not expand much before all the explosive material has reacted. This has led to the development of plastic explosive. A casing is still employed in some high explosive bombs, but with the purpose of fragmentation. Most high explosive bombs consist of an insensitive secondary explosive that must be detonated with a blasting cap containing a more sensitive primary explosive.
Nuclear fission.
Nuclear fission type atomic bombs utilize the energy present in very heavy atomic nuclei, such as U-235 or Pu-239. In order to release this energy rapidly, a certain amount of the fissile material must be very rapidly consolidated while being exposed to a neutron source. If consolidation occurs slowly, repulsive forces drive the material apart before a significant explosion can occur. Under the right circumstances, rapid consolidation can provoke a chain reaction that can proliferate and intensify by many orders of magnitude within microseconds. The energy released by a nuclear fission bomb may be tens of thousands of times greater than a chemical bomb of the same mass.
Nuclear fusion.
Nuclear fusion type atomic bombs release energy through the fusion of the light atomic nuclei of deuterium and tritium. With this type of bomb, a thermonuclear detonation is triggered by the detonation of a fission type nuclear bomb contained within a material containing high concentrations of deuterium and tritium. Weapon yield is typically increased with a tamper that increases the duration and intensity of the reaction through inertial confinement and neutron reflection. Nuclear fusion bombs can have arbitrarily high yields making them hundreds or thousands of times more powerful than nuclear fission.
Other.
Concrete bomb.
A concrete bomb is an aerial bomb which contains dense, inert material (typically concrete) instead of explosive. The target is destroyed using the kinetic energy of the falling bomb.
Inert bomb.
An inert munition is one whose inner energetic material has been removed or otherwise rendered harmless. Inert munitions are used in military and naval training, and they are also collected and displayed by public museums, or by private parties.
Typically, NATO inert munitions are painted entirely in light blue and/or have the word "INERT" stenciled on them in prominent locations.
Delivery.
The first air-dropped bombs were used by the Austrians in the 1849 siege of Venice. Two hundred unmanned balloons carried small bombs although few bombs actually hit the city.
The first bombing from a fixed-wing aircraft took place in 1911 when the Italians dropped bombs by hand on the Turkish lines in what is now Libya, during the Italo-Turkish War. The first large scale dropping of bombs took place during World War I starting in 1915 with the German Zeppelin airship raids on London, England, and the same war saw the invention of the first heavy bombers. One Zeppelin raid on 8 September 1915 dropped 4000 lb of high explosives and incendiary bombs, including one bomb that weighed 600 lb.
During World War II bombing became a major military feature, and a number of novel delivery methods were introduced. These included Barnes Wallis's bouncing bomb, designed to bounce across water, avoiding torpedo nets and other underwater defenses, until it reached a dam, ship or other destination where it would sink and explode. By the end of the war, planes such as the allied forces' Avro Lancaster were delivering with 50 yard accuracy from 20000 feet, ten ton earthquake bombs (also invented by Barnes Wallis) named "Grand Slam", which unusually for the time were delivered from high altitude in order to gain high speed, and would upon impact penetrate and explode deep underground ("camouflet"), causing massive caverns or craters, and affecting targets too large or difficult to be affected by other types of bomb.
Modern military bomber aircraft are designed around a large-capacity internal bomb bay while fighter bombers usually carry bombs externally on pylons or bomb racks, or on multiple ejection racks which enable mounting several bombs on a single pylon. Some bombs are equipped with a parachute, such as the World War II "parafrag", which was an 11 kg fragmentation bomb, the Vietnam war-era daisy cutters, and the bomblets of some modern cluster bombs. Parachutes slow the bomb's descent, giving the dropping aircraft time to get to a safe distance from the explosion. This is especially important with airburst nuclear weapons, and in situations where the aircraft releases a bomb at low altitude. A number of modern bombs are also precision-guided munitions, and may be guided after they leave an aircraft by remote control, or by autonomous guidance.
A hand grenade is delivered by being thrown. Grenades can also be projected by other means, such as being launched from the muzzle of a rifle, as in the rifle grenade or using a grenade launcher such as the M203 or by attaching a rocket to the explosive grenade as in a rocket-propelled grenade (RPG).
A bomb may also be positioned in advance and concealed.
A bomb destroying a rail track just before a train arrives causes a train to derail. Apart from the damage to vehicles and people, a bomb exploding in a transport network often also damages, and is sometimes mainly intended to damage that network. This applies for railways, bridges, runways, and ports, and to a lesser extent, depending on circumstances, to roads.
In the case of suicide bombing the bomb is often carried by the attacker on his or her body, or in a vehicle driven to the target.
The Blue Peacock nuclear mines, which were also termed "bombs", were planned to be positioned during wartime and be constructed such that, if they were disturbed, they would explode within ten seconds.
The explosion of a bomb may be triggered by a detonator or a fuse. Detonators are triggered by clocks, remote controls like cell phones or some kind of sensor, such as pressure (altitude), radar, vibration or contact. Detonators vary in ways they work, they can be electrical, fire fuze or blast initiated detonators and others,
Blast seat.
In forensic science, the point of detonation of a bomb is referred to as its blast seat, seat of explosion, blast hole or epicenter. Depending on the type, quantity and placement of explosives, the blast seat may be either spread out or concentrated (i.e., an explosion crater).
Other types of explosions, such as dust or vapor explosions, do not cause craters or even have definitive blast seats.

</doc>
<doc id="47629" url="http://en.wikipedia.org/wiki?curid=47629" title="Atil">
Atil

Atil (Turkish: "İtil"; cf. "A-de Shui"), literally meaning "Big River", was the capital of Khazaria from the middle of the 8th century until the end of the 10th century. The word is also a Turkic name for the Volga River.
History.
Atil was located along the Volga delta at the northwestern corner of the Caspian Sea. Following the defeat of the Khazars in the Second Arab-Khazar War, Atil became the capital of Khazaria. The city is referred to as Khamlij in 9th-century Arab sources, and the name Atil appears in the 10th century. At its height, the city was a major center of trade, and consisted of three parts separated by the Volga. The western part contained the administrative center of the city, with a court house and a large military garrison. The eastern part of the city was built later and acted as the commercial center of the Atil, and had many public baths and shops. Between them was an island on which stood the palaces of the Khazar Khagan and Bek. The island was connected to one of the other parts of the city by a pontoon bridge. According to Arab sources, one half of the city was referred to as Atil, while the other was named Khazaran.
Atil was a multi-ethnic and religiously diverse city, inhabited by Jews, Christians, Muslims, Shamanists, and Pagans, many of them traders from foreign countries. All of the religious groups had their own places of worship in the city, and there were 7 judges appointed to settle disputes (two Christian, two Jewish, and two Muslim judges, with a single judge for all of the Shamanists and other Pagans).
Svyatoslav I of Kiev sacked Atil in 968 or 969 CE. Ibn Hawqal and al-Muqaddasi refer to Atil after 969, indicating that it may have been rebuilt. Al-Biruni (mid-11th century) reported that Atil was again in ruins, and did not mention the later city of Saqsin which was built nearby, so it is possible that this new Atil was only destroyed in the middle of the 11th century.
The archaeological remains of Atil have never been positively identified. It has been hypothesized that they were washed away by the rising level of the Caspian Sea. However, beginning in 2003 Dmitri Vasilyev of Astrakhan State University led a series of excavations at the Samosdelskoye site near the village of Samosdelka (Russian: Самосделка) in the Volga Delta. Vasilyev connected artifacts from the site with Khazar, Oghuz and Bulgar culture, leading him to believe that he had discovered the site of Saqsin. The matter is still unresolved. In 2006 Vasilyev announced his belief that the lowest stratum at the Samosdelka site was identical with the site of Atil. In 2008, this team of Russian archaeologists announced that they had discovered the ruins of Itil.

</doc>
<doc id="47630" url="http://en.wikipedia.org/wiki?curid=47630" title="Paella">
Paella

Paella (: [paˈeʎa] or [pəˈeʎə], : [paˈeʎa], , or ) is a Valencian rice dish with ancient roots that originated in its modern form in the mid-nineteenth century near Albufera lagoon, a coastal lagoon in Valencia, on the east coast of Spain. The dish is widely regarded as Spain's national dish, as well as a regional Valencian dish; Valencians regard paella as one of their identifying symbols. Spanish food historian Lourdes March notes that the dish "symbolizes the union and heritage of two important cultures, the Roman, which gives us the utensil and the Arab which brought us the basic food of humanity for centuries."
The three best known types of paella are Valencian paella (Spanish: "paella valenciana"), seafood paella (Spanish: "paella de marisco"), and mixed paella (Spanish: "paella mixta"), but there are many others as well. Valencian paella is believed to be the original recipe and consists of white rice, green beans ('bajoqueta' and 'tavella'), meat (chicken and rabbit), white beans, snails, and seasoning such as saffron and rosemary. Another very common but seasonal ingredient is artichoke. Seafood paella replaces meats with seafood and omits beans and green vegetables. Mixed paella is a free-style combination of land animals, seafood, vegetables, and sometimes beans. Most paella chefs use calasparra or bomba rices. All types of paellas use olive oil.
Etymology.
Paella is a Valencian-Catalan word which derives from the Old French word "paelle" for pan, which in turn comes from the Latin word "patella" for pan as well. "Patella" is also akin to the modern French "poêle", the Italian "padella" and the Old Spanish "padilla".
Valencians use the word "paella" for all pans, including the specialized shallow pan used for cooking paellas. However, in most other parts of Spain and throughout Latin America, the term "paellera" is more commonly used for this pan, though both terms are correct, as stated by the Royal Spanish Academy, the body responsible for regulating the Spanish language in Spain. "Paelleras" are traditionally round, shallow and made of polished steel with two handles.
A popular but inaccurate belief in Arabic-speaking countries is that the word paella derives from the Arabic word for leftovers, "baqiyah", (Arabic script: بقية) because it was customary among the servants of Moorish kings to combine the leftovers of a banquet for royal guests, purportedly leading to a paella-like creation in Moorish Spain.
History.
Moorish influence.
In Moorish Spain, farmers improved the old Roman irrigation systems along the Mediterranean coast. This led to greater yields in rice production. Consequently, residents of the Valencian region often made casseroles of rice, fish and spices for family gatherings and religious feasts, thus establishing the custom of eating rice in Spain. This led to rice becoming a staple by the 15th century. Afterwards, it became customary for cooks to combine rice with vegetables, beans and dry cod, providing an acceptable meal for Lent. Along Spain's eastern coast, rice was predominantly eaten with fish.
Valencian paella.
On special occasions, 18th century Valencians used "calderos" to cook rice in the open air of their orchards near lake Albufera. Water vole meat was one of the main ingredients of early paellas, along with eel and butter beans. Novelist Vicente Blasco Ibáñez described the Valencian custom of eating water voles in "Cañas y Barro" (1902), a realistic novel about life among the fishermen and peasants near lake Albufera.
Living standards rose with the sociological changes of the late 19th century in Spain, giving rise to gatherings and outings in the countryside. This led to a change in paella's ingredients as well, using instead rabbit, chicken, duck and sometimes snails. This dish became so popular that in 1840 a local Spanish newspaper first used the word "paella" to refer to the recipe rather than the pan.
The most widely used, complete ingredient list of this era was as follows: short-grain white rice, chicken, rabbit, snails (optional), duck (optional), butter beans, great northern beans, runner beans, artichoke (a substitute for runner beans in the winter), tomatoes, fresh rosemary, sweet paprika, saffron, garlic (optional), salt, olive oil and water. Poorer Valencians, however, sometimes used nothing more than snails for meat. Valencians insist that only these ingredients should go into making modern Valencian paella.
Seafood and mixed paella.
On the Mediterranean coast, Valencians used seafood instead of meat and beans to make paella. Valencians regard this recipe as authentic as well. In this recipe, the seafood is served in the shell. A variant on this is "paella del senyoret" which utilizes seafood without shells. Later, however, Spaniards living outside of Valencia combined seafood with meat from land animals and mixed paella was born. This paella is sometimes called "preparación barroca" (baroque preparation) due to the variety of ingredients and its final presentation. 
During the 20th century, paella's popularity spread past Spain's borders. As other cultures set out to make paella, the dish invariably acquired regional influences. Consequently, paella recipes went from being relatively simple to including a wide variety of seafood, meat, sausage, (even chorizo) vegetables and many different seasonings. However, the most globally popular recipe is seafood paella.
Throughout non-Valencian Spain, mixed paella is very popular. Some restaurants in Spain (and many in the United States) that serve this mixed version, refer to it as "Valencian" paella. However, Valencians insist only the original two Valencian recipes are authentic. They generally view all others as inferior, not genuine or even grotesque.
Basic cooking methods.
According to tradition in Valencia, paella is cooked over an open fire, fueled by orange and pine branches along with pine cones. This produces an aromatic smoke which infuses the paella. Also, dinner guests traditionally eat directly out of the "paellera".
Some recipes call for paella to be covered and left to settle for five or ten minutes after cooking.
Valencian paella.
This recipe is standardized because Valencians consider it traditional and very much part of their culture. Rice in Valencian paella is never braised in oil, as pilaf, though the paella made further southwest of Valencia often is.
Seafood paella.
Recipes for this dish vary somewhat, even in Valencia. Below is a recipe by Juanry Segui, a prominent Valencian chef.
Mixed paella.
There are countless mixed paella recipes. The following method is common to most of these. Seasoning depends greatly on individual preferences and regional influences. However, salt, saffron and garlic are almost always included.
For all recipes.
After cooking paella, there is usually a layer of toasted rice at the bottom of the pan, called "socarrat" in Spain. This is considered a delicacy among Spaniards and is essential to a good paella. The toasted rice develops on its own if the paella is cooked over a burner or open fire. If cooked in an oven, however, it will not. To correct this, place the "paellera" over a high flame while listening to the rice toast at the bottom of the pan. Once the aroma of toasted rice wafts upwards, remove it from the heat. The paella must then sit for about five minutes (most recipes recommend the paella be covered with a tea-towel at this point) to absorb the remaining broth.
Competitions and records.
It has become a custom at mass gatherings in the Valencian Community (festivals, political campaigns, protests, etc.) to prepare enormous paellas, sometimes to win mention in the "Guinness Book of World Records". Chefs use gargantuan "paelleras" for these events.
Valencian restaurateur Juan Galbis claims to have made the world's largest paella with help from a team of workers on 2 October 2001. This paella fed about 110,000 people according to Galbis' former website. Galbis says this paella was even larger than his earlier world-record paella made on 8 March 1992 which fed about 100,000 people. Galbis's record-breaking 1992 paella is listed in "Guinness World Records".
Similar dishes.
Traditional Valencian cuisine offers recipes similar to "paella valenciana" and "paella de marisco" such as "arròs negre", "arròs al forn", "arròs a banda" and "arròs amb fesols i naps". Fideuà is a noodle dish variation of the paella cooked in a similar fashion, though it may be served with allioli sauce.
The following is a list of other similar rice dishes:

</doc>
<doc id="47631" url="http://en.wikipedia.org/wiki?curid=47631" title="Khazaran">
Khazaran

Khazaran was a city in the Khazar kingdom, located on the eastern bank of the lower Volga River. It was connected to Atil by a pontoon bridge.
Khazaran was later inhabited primarily by Muslims and featured numerous mosques, minarets, and madrasas. It was a bustling trade center easily reachable by ship from the Caspian Sea and Volga River, and many of its inhabitants were crafters, fishers, and traders. The leader of the city was a Muslim official known as the Khazz; in Arab sources the title given is sometimes vizier.

</doc>
<doc id="47632" url="http://en.wikipedia.org/wiki?curid=47632" title="Falles">
Falles

The Falles (], sing. "Falla"), or Fallas (]), is a traditional celebration held in commemoration of Saint Joseph in the city of Valencia, in Spain. The term "Falles" refers to both the celebration and the monuments created during the celebration. A number of towns in the Valencian Community have similar celebrations inspired by the original Fallas de Valencia celebration.
Each neighbourhood of the city has an organized group of people, the "Casal faller", that works all year long holding fundraising parties and dinners, usually featuring the famous dish, paella, a specialty of the region. Each "casal faller" produces a construction known as a "falla" which is eventually burnt. A "casal faller" is also known as a "comissió fallera".
Etymology.
The name of the festival is the plural of the Valencian word "falla". The word's derivation is as follows:
"Falles" and "ninots".
Formerly, much time would be spent by the "Casal faller" preparing the "ninots" (Valencian for puppets or dolls). During the four days leading up to 19 March, each group takes its "ninot" out for a grand parade, and then mounts it, each on its own elaborate firecracker-filled cardboard and paper-mâché artistic monument in a street of the given neighbourhood. This whole assembly is a "falla".
The "ninots" and their "falles" are constructed according to an agreed upon theme that has traditionally been, and continues to be, a satirical jab at anything or anyone who draws the attention of the critical eyes of the "falleros"—the celebrants themselves. In modern times, the whole two-week-long festival has spawned a huge local industry, to the point that an entire suburban area has been designated the City of Falles – "Ciutat fallera". Here, crews of artists and artisans, sculptors, painters, and many others all spend months producing elaborate constructions of paper and wax, wood and styrofoam tableaux towering up to five stories, composed of fanciful figures in outrageous poses arranged in gravity-defying architecture. Each of them is produced at the direction of one of the many individual neighbourhood "Casals fallers" who vie with each other to attract the best artists, and then to create the most outrageous monument to their target. There are more than 500 different "falles" in Valencia, including those of other towns in the Valencian Community.
During Falles, many people wear their "casal faller" dress in regional and historical costumes from different eras of Valencia's history; the dolçaina and tabalet (a kind of Valencian drum) are frequently heard, as most of the different casals fallers have their own traditional bands.
Although the "Falles" is a very traditional event and many participants dress in medieval clothing, the "ninots" for 2005 included such modern characters as Shrek and George W. Bush, and the 2012 "Falles" included characters like Barack Obama and Lady Gaga.
Events during "Falles".
The five days and nights of "Falles" are a continuous party. There are a multitude of processions: historical, religious, and comedic. Crowds in the restaurants spill out into the streets. Explosions can be heard all day long and sporadically through the night. Foreigners may be surprised to see everyone from small children to elderly gentlemen throwing fireworks and noisemakers in the streets, which are littered with pyrotechnical debris. The timing of the events is fixed and they fall on the same date every year, though there has been discussion about holding some events on the weekend preceding the Falles, to take greater advantage of the tourist potential of the festival or changing the end date in years where it is due to occur in midweek.
La Despertà.
Each day of Falles begins at 8:00 am with "La Despertà" ("the wake-up call"). Brass bands appear from the casals and begin to march down every street playing lively music. Close behind them are the "fallers", throwing large firecrackers in the street as they go.
La Mascletà.
The "Mascletà", an explosive barrage of coordinated firecracker and fireworks displays, takes place in each neighbourhood at 2:00 pm every day of the festival; the main event is the municipal Mascletà in the "Plaça de l'Ajuntament" where the pyrotechnicians compete for the honor of providing the final Mascletà of the fiestas (on 19 March). At 2:00 pm the clock chimes and the Fallera Mayor (dressed in her "fallera" finery) will call from the balcony of City Hall, "Senyor pirotècnic, pot començar la mascletà!" ("Mr. Pyrotechnic, you may commence the Mascletà!"), and the Mascletà begins.
The Mascletà is almost unique to the Valencian Community, hugely popular with the Valencian people and found in very few other places in the world. Smaller neighbourhoods often hold their own mascletà for saint's days, weddings and other celebrations.
La Plantà.
The day of the 15th all of the "falles" infantils are to be finished being constructed and later that night all of the "falles majors" (major Falles) are to be completed. If not, they face disqualification.
L'Ofrena de flors.
In this event, the flower offering, each falla casal takes an offering of flowers to the Virgin Mary as Our Lady of the Abandoned. This occurs all day during 17–18 March. A statue of the Virgin Mary and its large pedestal are then covered with all the flowers.
Els Castells and La Nit del Foc.
On the nights of the 15, 16, 17, and 18th there are firework displays in the old riverbed in Valencia. Each night is progressively grander and the last is called "La Nit del Foc" (the Night of Fire).
Cabalgata del Fuego.
On the final evening of Falles, at 7pm on March 19, a parade known in Spanish as the "Cabalgata del Fuego" (the Fire Parade) takes place along Colon street and Porta de la Mar square. This spectacular celebration of fire, the symbol of the fiesta’s spirit, is the "grand finale" of Falles and a colourful, noisy event featuring exhibitions of the varied rites and displays from around the world which use fire; it incorporates floats, giant mechanisms, people in costumes, rockets, gunpowder, street performances and music.
La Cremà.
On the final night of Falles, around midnight on March 19, these "falles" are burnt as huge bonfires. This is known as "La Cremà" (the Burning), the climax of the whole event, and the reason why the constructions are called "falles" ("torches"). Traditionally, the falla in the "Plaça de l'Ajuntament" is burned last.
Many neighbourhoods have a "falla infantil" (a children's "falla", smaller and without satirical themes), which is held a few metres away from the main one. This is burnt first, at 10:00 pm. The main neighbourhood "falles" are burnt closer to midnight; the burning of the "falles" in the city centre often starts later. For example, in 2005, the fire brigade delayed the burning of the Egyptian funeral "falla" in "Carrer del Convent de Jerusalem" until 1:30 am, when they were sure all safety concerns were addressed.
Each "falla" is laden with fireworks which are lit first. The construction itself is lit either after or during the explosion of these fireworks. "Falles" burn quite quickly, and the heat given off is felt by all around. The heat from the larger ones often drives the crowd back a couple of metres, even though they are already behind barriers that the fire brigade has set several metres from the construction. In narrower streets, the heat scorches the surrounding buildings, and the firemen douse the façades, window blinds, street signs, etc. with their hoses to stop them catching fire or melting, from the beginning of the "cremà" until it cools down.
Away from the "falles", people frolic in the streets, the whole city resembling an open-air dance party, except that instead of music there is the incessant (and occasionally deafening) sound of people throwing fireworks around randomly. There are stalls selling products such as the typical fried snacks "porres", "xurros" and "bunyols", as well as roasted chestnuts or trinkets.
While the smaller fallas dotted around the streets are burned at approximately the same time as each other, the last falla to be burned is the main one, which is saved until last so that everybody can watch it. This main falla is found outside the ayuntamiento - the town hall. People arrive a few hours before the scheduled burning time to get a front row view.
History.
There are different speculations regarding the origin of the Falles festival. One suggests that the Falles started in the Middle Ages, when artisans disposed of the broken artifacts and pieces of wood they saved during the winter by burning them to celebrate the spring equinox. Valencian carpenters used planks of wood called "parots" to hang their candles on during the winter, as these were needed to provide light to work by. With the coming of the spring, they were no longer necessary, so they were burned. Over time, and with the intervention of the Church, the date of the burning of these "parots" was made to coincide with the celebration of the festival of Saint Joseph, the patron saint of carpenters.
This tradition continued to evolve. The "parot" was dressed with clothing so that it looked like a person; features identifiable with some well-known person from the neighbourhood were often added as well. To collect these materials, children went from house to house asking for "una estoreta velleta" (an old rug) to add to the "parot". This became a popular song that the children sang as they gathered all sorts of old flammable furniture and utensils to burn in the bonfire with the "parot". These "parots" were the first "ninots". With time, people of the neighbourhoods organized the building of the falles and the typically intricate constructions, including their various figures, were born.
Until the beginning of the twentieth century, the "falles" were tall boxes with three or four wax dolls dressed in fabric clothing. This changed when the creators began to use cardboard. The fabrication of the "falles" continues to evolve in modern times, when the largest displays are made of polystyrene and soft cork easily molded with hot saws. These techniques have allowed the creation of "falles" over 30 metres high.
The origin of the pagan festival is similar to that of the Bonfires of Saint John celebrated in the Alicante region, in the sense that both came from the Latin habit of lighting fires to welcome spring. But in Valencia, this ancient tradition led to the burning of accumulated waste at the end of winter, particular wood, on the day of Saint Joseph, as was fitting. Given the reputed humorous character of Valencians, it was natural that they began to burn figurines depicting people and events of the past year. The burning symbolised liberation from servitude to the memory of these events or else represented humorous and often critical commentary on them. The festival thus evolved a more satirical and ironic character, and the wooden castoffs gradually came to be assembled into progressively more elaborate 'monuments' that were designed and painted in advance.
During the early 20th century and especially during the Spanish Civil War, the monuments became more anti-clerical in nature and were often highly critical of the local or national governments, which in fact tried to ban the Falles many times, without success. Under the dictatorship of Franco the celebration lost much of its satirical nature because of government censorship, but the monuments were among the few fervent public expressions allowed then, and they could be made freely in Valencia. During this period, many religious customs such as the offering of flowers to Our Lady of the Forsaken were taken up, which today are essential parts of the festival, even though unrelated to the original purpose of the celebration, and somewhat antithetical in spirit.
With the restoration of democracy and the end of government censorship, the critical "falles" reappeared, and obscene satirical ones with them. Despite thirty years of freedom of expression, the world view of the "fallero" can still be socially conservative, is often sexist and may involve some of the amoralism of Valencian politics. This has sometimes led to criticism by certain cultural critics, environmentalists, and progressives. Yet there are celebrants of all ideologies and factions, and they have different interpretations of the spirit of the celebration. In fact, recent initiatives such as the pilota championships, literary competitions and other events show a culturally vibrant city that yet relies on its ancient traditions to express its singular identity, even those as seemingly frivolous as the Falles festival.
Secció Especial.
The Secció Especial is a group of the largest and most prestigious "falles" commissions in the city of Valencia. In 2007, the group consisted of 14 commissions. This class of "falles" was first started in 1942 and originally included the "falles" of Barques, Reina-Pau and Plaça del Mercat. Currently, none of these are still in the group. The commission that has most often participated in this group as of 2015 was Na Jordana, with 62 times. Winning the first prize in the Secció Especial is the most prestigious prize any falla can win. All other "falles" fall into different classes that also award prizes with the exception of the one erected by the town hall.

</doc>
<doc id="47634" url="http://en.wikipedia.org/wiki?curid=47634" title="Samandar (city)">
Samandar (city)

Samandar (also Semender) was a city in Khazaria on the western shore of the Caspian Sea, south of the city of Atil, in the North Caucasus. The exact location of the city is unknown, but most likely, it was situated on the Terek river near the present-day city of Kizlyar, which, like Samandar, is noted for its vineyards. According to the Soviet archeologist Mikhail Artamonov, remains of a large town have been found deep in the woods along the lower Terek.
The name of the city may derive from the name of a Hunnish tribe "Zabender". The Greek writer Theophylact Simocatta refers to a migration of Zabender from Asia to Europe in about 598; in addition, an Armenian book on geography attributed to Moses of Chorene mentions a town "M-s-n-d-r" in the land of Huns located to the north of Derbent.
Samandar was inhabited by Jews, Christians, Muslims, and members of other religious faiths, each of which had its houses of worship. The city served as the capital of Khazaria from the 720s to about 750, when the capital was moved northwards to Atil, which was less vulnerable to Arab attacks. Both cities were destroyed by Kievan Rus' prince Sviatoslav in the 960s, leading to a decline and disappearance of Khazaria.
According to Istakhri, Samandar was famous for its fertile gardens and vineyards, and large quantities of wine were made there.

</doc>
<doc id="47635" url="http://en.wikipedia.org/wiki?curid=47635" title="Murphy's law">
Murphy's law

Murphy's law is an adage or epigram that is typically stated as: "Anything that can go wrong, will go wrong".
History.
The perceived perversity of the universe has long been a subject of comment, and precursors to the modern version of Murphy's law are not hard to find. Recent significant research in this area has been conducted by members of the American Dialect Society. ADS member Stephen Goranson has found a version of the law, not yet generalized or bearing that name, in a report by Alfred Holt at an 1877 meeting of an engineering society.
It is found that anything that can go wrong at sea generally does go wrong sooner or later, so it is not to be wondered that owners prefer the safe to the scientific ... Sufficient stress can hardly be laid on the advantages of simplicity. The human factor cannot be safely neglected in planning machinery. If attention is to be obtained, the engine must be such that the engineer will be disposed to attend to it.
Mathematician Augustus De Morgan wrote on June 23, 1866:
"The first experiment already illustrates a truth of the theory, well confirmed by practice, what-ever can happen will happen if we make trials enough." In later publications "whatever can happen will happen" occasionally is termed "Murphy's law," which raises the possibility—if something went wrong—that "Murphy" is "De Morgan" misremembered (an option, among others, raised by Goranson on American Dialect Society list).
American Dialect Society member Bill Mullins has found a slightly broader version of the aphorism in reference to stage magic. The British stage magician Nevil Maskelyne wrote in 1908:
It is an experience common to all men to find that, on any special occasion, such as the production of a magical effect for the first time in public, everything that "can" go wrong "will" go wrong. Whether we must attribute this to the malignity of matter or to the total depravity of inanimate things, whether the exciting cause is hurry, worry, or what not, the fact remains.
The contemporary form of Murphy's law goes back as far as 1952, as an epigraph to a mountaineering book by John Sack, who described it as an "ancient mountaineering adage":
Anything that can possibly go wrong, does.
Fred R. Shapiro, the editor of the "Yale Book of Quotations", has shown that in 1952 the adage was called "Murphy's law" in a book by Anne Roe, quoting an unnamed physicist:
he described [it] as "Murphy's law or the fourth law of thermodynamics" (actually there were only three last I heard) which states: "If anything can go wrong, it will."
In May 1951, Anne Roe gives a transcript of an interview (part of a Thematic Apperception Test, asking impressions on a photograph) with Theoretical Physicist number 3: "...As for himself he realized that this was the inexorable working of the second law of the thermodynamics which stated Murphy's law ‘If anything can go wrong it will’." Anne Roe's papers are in the American Philosophical Society archives in Philadelphia; those records (as noted by Stephen Goranson on the American Dialect Society list 12/31/2008) identify the interviewed physicist as Howard Percy "Bob" Robertson (1903–1961). Robertson's papers are at the Caltech archives; there, in a letter Robertson offers Roe an interview within the first three months of 1949 (as noted by Goranson on American Dialect Society list 5/9/2009). The Robertson interview apparently predated the Muroc scenario said by Nick Spark ("American Aviation Historical Society Journal" 48 (2003) p. 169) to have occurred in or after June, 1949.
The name "Murphy's law" was not immediately secure. A story by Lee Correy in the February 1955 issue of "Astounding Science Fiction" referred to "Reilly's law," which "states that in any scientific or engineering endeavor, anything that can go wrong "will" go wrong". Atomic Energy Commission Chairman Lewis Strauss was quoted in the "Chicago Daily Tribune" on February 12, 1955, saying "I hope it will be known as Strauss' law. It could be stated about like this: If anything bad can happen, it probably will."
Arthur Bloch, in the first volume (1977) of his "Murphy's Law, and Other Reasons Why Things Go WRONG" series, prints a letter that he received from George E. Nichols, a quality assurance manager with the Jet Propulsion Laboratory. Nichols recalled an event that occurred in 1949 at Edwards Air Force Base, Muroc, California that, according to him, is the origination of Murphy's law, and first publicly recounted by USAF Col. John Paul Stapp. An excerpt from the letter reads:
The law's namesake was Capt. Ed Murphy, a development engineer from Wright Field Aircraft Lab. Frustration with a strap transducer which was malfunctioning due to an error in wiring the strain gage bridges caused him to remark – "If there is any way to do it wrong, he will" – referring to the technician who had wired the bridges at the Lab. I assigned Murphy's law to the statement and the associated variations.
Association with Murphy.
According to the book "A History of Murphy's Law" by author Nick T. Spark, differing recollections years later by various participants make it impossible to pinpoint who first coined the saying "Murphy's law". The law's name supposedly stems from an attempt to use new measurement devices developed by the eponymous Edward Murphy. The phrase was coined in adverse reaction to something Murphy said when his devices failed to perform and was eventually cast into its present form prior to a press conference some months later — the first ever (of many) given by Dr. John Stapp, a U.S. Air Force colonel and Flight Surgeon in the 1950s. These conflicts (a long running interpersonal feud) were unreported until Spark researched the matter. His book expands upon and documents an original four part article published in 2003 (Annals of Improbable Research (AIR)) on the controversy: "Why Everything You Know About Murphy's Law is Wrong".
From 1948 to 1949, Stapp headed research project MX981 at Muroc Army Air Field (later renamed Edwards Air Force Base) for the purpose of testing the human tolerance for g-forces during rapid deceleration. The tests used a rocket sled mounted on a railroad track with a series of hydraulic brakes at the end. Initial tests used a humanoid crash test dummy strapped to a seat on the sled, but subsequent tests were performed by Stapp, at that time an Air Force captain. During the tests, questions were raised about the accuracy of the instrumentation used to measure the g-forces Captain Stapp was experiencing. Edward Murphy proposed using electronic strain gauges attached to the restraining clamps of Stapp's harness to measure the force exerted on them by his rapid deceleration. Murphy was engaged in supporting similar research using high speed centrifuges to generate g-forces. Murphy's assistant wired the harness, and a trial was run using a chimpanzee.
The sensors provided a zero reading; however, it became apparent that they had been installed incorrectly, with each sensor wired backwards. It was at this point that a disgusted Murphy made his pronouncement, despite being offered the time and chance to calibrate and test the sensor installation prior to the test proper, which he declined somewhat irritably, getting off on the wrong foot with the MX981 team. In an interview conducted by Nick Spark, George Nichols, another engineer who was present, stated that Murphy blamed the failure on his assistant after the failed test, saying, "If that guy has any way of making a mistake, he will." Nichols' account is that "Murphy's law" came about through conversation among the other members of the team; it was condensed to "If it can happen, it will happen," and named for Murphy in mockery of what Nichols perceived as arrogance on Murphy's part. Others, including Edward Murphy's surviving son Robert Murphy, deny Nichols' account (which is supported by Hill, both interviewed by Spark), and claim that the phrase did originate with Edward Murphy. According to Robert Murphy's account, his father's statement was along the lines of "If there's more than one way to do a job, and one of those ways will result in disaster, then he will do it that way."
The phrase first received public attention during a press conference in which Stapp was asked how it was that nobody had been severely injured during the rocket sled tests. Stapp replied that it was because they always took "Murphy's law" under consideration; he then summarized the law and said that in general, it meant that it was important to consider all the possibilities (possible things that could go wrong) before doing a test and act to counter them. Thus Stapp's usage and Murphy's alleged usage are very different in outlook and attitude. One is sour, the other an affirmation of the predictable being surmountable, usually by sufficient planning and redundancy. Hill and Nichols believe Murphy was unwilling to take the responsibility for the device's initial failure (by itself a blip of no large significance) and is to be doubly damned for not allowing the MX981 team time to validate the sensor's operability and for trying to blame an underling when doing so in the embarrassing aftermath.
The association with the 1948 incident is by no means secure. Despite extensive research, no trace of documentation of the saying as "Murphy's law" has been found before 1951 (see above). The next citations are not found until 1955, when the May–June issue of "Aviation Mechanics Bulletin" included the line "Murphy's law: If an aircraft part can be installed incorrectly, someone will install it that way," and Lloyd Mallan's book, "Men, Rockets and Space Rats", referred to: "Colonel Stapp's favorite takeoff on sober scientific laws—Murphy's law, Stapp calls it—'Everything that can possibly go wrong will go wrong'." The Mercury astronauts in 1962 attributed Murphy's law to U.S. Navy training films.
Other variations on Murphy's law.
From its initial public announcement, Murphy's law quickly spread to various technical cultures connected to aerospace engineering. Before long, variants had passed into the popular imagination, changing as they went.
Author Arthur Bloch has compiled a number of books full of corollaries to Murphy's law and variations thereof. The first of these was "Murphy's law and other reasons why things go wrong!",
There have been persistent references to Murphy's law associating it with the laws of thermodynamics right from the very beginning (see the quotation from Anne Roe's book above). In particular, Murphy's law is often cited as a form of the second law of thermodynamics (the law of entropy) because both are predicting a tendency to a more disorganised state.
In Analog Devices' technical seminars on precision analog circuit techniques Murphy's law is interpreted as "The Laws of Physics always work, even when you're not paying attention".
The law of truly large numbers is similar to Murphy's Law. It states that with a sample size large enough, any outrageous thing is likely to happen.
Yhprum's law, where the name is spelled backwards, is "anything that can go right, will go right" — the optimistic application of Murphy's law in reverse.

</doc>
<doc id="47636" url="http://en.wikipedia.org/wiki?curid=47636" title="International Sign">
International Sign

International Sign (IS) is a contact variety of sign language used in a variety of different contexts, particularly at international meetings such as the World Federation of the Deaf (WFD) congress, events such as the Deaflympics, in video clips produced by Deaf people and watched by other Deaf people from around the world, and informally when travelling and socialising. It can be seen as a pidgin form of sign language, which is not as conventionalised or complex as natural sign languages and has a limited lexicon.
Naming.
While the more commonly used term is either International Sign, it is sometimes referred to as Gestuno, or erroneously as International Sign Language (ISL), International Sign Pidgin and International Gesture (IG). International Sign is a term used by the World Federation of the Deaf and other international organisations.
History.
Deaf people in the Western and Middle Eastern world have gathered together using sign language for 2,000 years When Deaf people from different sign language backgrounds get together, a contact variety of sign language arises from this contact, whether it is in an informal personal context or in a formal international context. Deaf people have therefore used a kind of auxiliary gestural system for international communication at sporting or cultural events since the early 19th century. The need to standardise an international sign system was discussed at the first World Deaf Congress in 1951, when the WFD was formed. In the following years, a pidgin developed as the delegates from different language backgrounds communicated with each other, and in 1973, a WFD committee ("the Commission of Unification of Signs") published a standardized vocabulary. They selected "naturally spontaneous and easy signs in common use by deaf people of different countries" to make the language easy to learn. A book published by the commission in the early 1970s, "Gestuno: International Sign Language of the Deaf", contains a vocabulary list of about 1500 signs. The name "Gestuno" was chosen, referencing gesture and oneness.
However, when Gestuno was first used, at the WFD congress in Bulgaria in 1976, it was incomprehensible to deaf participants. Subsequently, it was developed informally by deaf and hearing interpreters, and came to include more grammar — especially linguistic features that are thought to be universal among sign languages, such as role shifting and the use of classifiers. Additionally, the vocabulary was gradually replaced by more iconic signs and loan signs from different sign languages.
The name Gestuno has fallen out of use, and the phrase "International Sign" is now more commonly used in English to identify this sign variety. Indeed, current IS has little in common with the signs published under the name 'Gestuno'.
A parallel development has been occurring in Europe in recent years, where increasing interaction between Europe's deaf communities has led to the emergence of a pan-European pidgin or creole sign. It is referred to by some sign linguists as "Eurosigns". Influence in Euro-Signs can be seen from British Sign Language, French Sign Language and Scandinavian signs.
Vocabulary.
The lexicon of International Sign is limited, and varies between signers. IS interpreter Bill Moody noted in a 1994 paper that the vocabulary used in conference settings is largely derived from the sign languages of the Western world and is less comprehensible to those from African or Asian sign language backgrounds. A 1999 study by Bencie Woll suggested that IS signers often use a large amount of vocabulary from their native language, choosing sign variants that would be more easily understood by a foreigner. In contrast, Rachel Rosenstock notes that the vocabulary exhibited in her study of International Sign was largely made up of highly iconic signs common to many sign languages:
"Over 60% of the signs occurred in the same form in more than eight SLs as well as in IS. This suggests that the majority of IS signs are not signs borrowed from a specific SL, as other studies found, but rather are common to many natural SLs. Only 2% of IS signs were found to be unique to IS. The remaining 38% were borrowed (or "loan") signs that could be traced back to one SL or a group of related SLs." 
Grammar.
People communicating in International Sign tend to make heavy use of role play, as well as a feature common to most sign languages researched to date: an extensive formal system of classifiers. Classifiers are used to describe things, and they transfer well across linguistic barriers. It has been noted that signers are generally better at interlingual communication than non-signers, even without a lingua franca. Perhaps, along with deaf people's experience with bridging communication barriers, the use of classifiers is a key reason.
A paper presented in 1994 suggested that IS signers "combine a relatively rich and structured grammar with a severely impoverished lexicon". Supalla and Webb (1995) describe IS as a kind of a pidgin, but conclude that it is "more complex than a typical pidgin and indeed is more like that of a full sign language".
Letters and numbers.
A manual alphabet is used for fingerspelling names, which is based on the one-handed systems used in Europe and America for representing the Roman alphabet. In a two-way conversation, any manual alphabet known may be used; often one speaker will fingerspell using the alphabet of the other party, as it is often easier to spell quickly in an unfamiliar alphabet than to read quickly. ISL also has a standardised system of numbers as these signs vary greatly between sign languages.
Use of indigenous signs.
Each region's own sign is preferred for country and city names. This may be used in conjunction with spelling and classifying for the first instance, and the indigenous sign used alone from then on.

</doc>
<doc id="47638" url="http://en.wikipedia.org/wiki?curid=47638" title="Kerch">
Kerch

Kerch (Russian: Керчь, Ukrainian: Керч, Crimean Tatar: "Keriç", Old East Slavic: Кърчевъ, Ancient Greek: Παντικάπαιον "Pantikapaion", Turkish: "Kerç") is a city of regional significance on the Kerch Peninsula in the east of the Crimea.
Population:  (2013 est.).
Founded 2,600 years ago as an ancient Greek colony, Kerch is considered to be one of the most ancient cities in Crimea. The city experienced rapid growth starting in the 1920s and was the site of a major battle during World War II.
Today, it is one of the largest cities in Crimea and is among the republic's most important industrial, transport and tourist centres.
History.
Ancient times.
Archeological digs at Mayak village near the city ascertained that the area had already been inhabited in 17th–15th centuries BC.
Kerch as a city starts its history in 7th century BC, when Greek colonists from Miletus founded a city-state named Panticapaeum on Mount Mithridat near the mouth of the Melek-Chesme river. Panticapaeum subdued nearby cities and by 480 BC became a capital of the Kingdom of Bosporus. Later, during the rule of Mithradates VI Eupator, Panticapaeum for a short period of time became the capital of the much more powerful and extensive Kingdom of Pontus.
The city was located at the intersection of trade routes between the steppe and Europe. This caused it to grow rapidly. The city's main exports were grain and salted fish, wine-making was also common. Panticapaeum minted its own coins. According to extant documents the Melek-Chesme river (small and shallow nowadays) was navigable in Bosporan times, and sea galleys were able to enter the river. A large portion of the city's population was ethnically Scythian, later Sarmatian, as the large royal barrow at Kul-Oba testifies.
In the 1st century AD Panticapaeum and the Kingdom of Bosporus suffered from Ostrogoth raids, then the city was devastated by the Huns in AD 375.
Middle Ages.
From the 6th century the city was under the control of the Byzantine Empire. By order of Emperor Justinian I a citadel named Bospor was built there. Bospor was the centre of a diocese and developed under the influence of Greek Christianity. In 576, it withstood a siege by the Göktürks under Bokhan, aided by Anagai, the last khan of the Uturgur Huns.
In the 7th century, the Turkic Khazars took control of Bospor, and the city was named Karcha from Turkic "karşı" meaning 'opposite, facing.' The main local government official during Khazar times was the tudun. Christianity was a major religion in Kerch during the period of Khazar rule. Kerch's Church of St. John the Baptist was founded in 717, thus, it is the oldest church in Ukraine. The "Church of the Apostles" existed during the late 8th and early 9th centuries, according to the "Life of the Apostle Andrew" by Epiphanus.
Following the fall of Khazaria to Kievan Rus' in the late 10th century, Kerch became the centre of a Khazar successor-state. Its ruler, Georgius Tzul, was deposed by a Byzantine-Rus expedition in 1016.
From the 10th century, the city was a Slavic settlement named Korchev, which belonged to the Tmutarakan principality. Kerch was a center of trade between Russia', Crimea, Caucasus, and the Orient.
In the 13th century, Crimea including Korchev was invaded by Mongols. After Mongols, the city became the Genoese colony of Cerco (Cherkio) in 1318 and served as a sea harbour, townspeople worked at salt-works and fishery.
In 1475, city was passed to the Ottoman Empire. During the Turkish rule Kerch fell into decay and served as a slave-market. It repeatedly suffered from raids of Zaporizhian Cossacks.
18th - 20th centuries.
In response to strengthening of Russian military forces in Azov area, the Turks built a fortress, named Yenikale, near Kerch on the shore of Kerch Strait. The fortress was completed by 1706. In 1771 the Imperial Russian Army invaded Crimea and approached Yenikale. The Turks decided to abandon the fortress, though reinforcements from the Ottoman Empire had arrived a few days earlier. By the Peace Treaty of Kuchuk-Kainarji in 1774, Kerch and Yenikale were ceded to Russia. As a result, the Turkish heritage has been almost completely wiped out.
In 1790 Russian naval forces under the command of admiral Fyodor Ushakov defeated the Turkish fleet at the Battle of Kerch Strait.
Because of its location, from 1821 Kerch developed into an important trade and fishing port. The state museum of ancient times and a number of educational institutions were opened in the city. The ironwork factory was built in 1846 based on a huge iron ore deposit found on Kerch Peninsula.
During the Crimean War the city was devastated by British forces in 1855.
In the late 19th century, mechanical and cement factories were built, and tinned food and tobacco factories were established.
By 1900, Kerch was connected to a railroad system, and the fairway of Kerch Strait was deepened and widened. At this time, the population had reached 33,000.
After suffering a decline during the First World War and the Russian Civil War, the city resumed its growth in the late 1920s, with the expansion of various industries, iron ore and metallurgy in particular, and by 1939 its population had reached 104,500.
Kerch in World War II.
On the Eastern Front of World War II from 1941 to 1945, Kerch was the site of heavy fighting between Soviet Army and Axis forces.
After fierce fighting, the city was taken by the Germans in November 1941. On 30 December 1941 the Soviets recaptured the city in a naval landing operation. In 1942 the Germans occupied the city again. The Red Army lost over 160,000 men, either killed or taken POW at the Battle of the Kerch Peninsula. On 31 October 1943 another Soviet naval landing operation was launched. Kerch returned to Soviet control on 11 April 1944.
The German invaders killed about 15,000 citizens and deported another 14,000 during their occupation.
Evidence of German atrocities in Kerch was presented in the Nuremberg trials. After the war, the city was awarded the title Hero City.
The Adzhimushkay catacombs (mines) in the city's suburbs were the site of guerrilla warfare against the occupation. Thousands of soldiers and refugees found shelter inside, and were involved in counterattacks. Many of them died underground, including those who died of numerous alleged poison gas attacks. Later, a memorial was established on the site.
Modern Kerch.
Administrative divisions.
The city municipality stretches over a substantial area and includes several separate neighborhoods that are part of the Kerch city: Eltigen (Heroyevskoe), Kamysh-Burun (Arshyntsevo), Port Krym, Adzhimushkai, and Tuzla Island.
Industry.
Today Kerch is considered as a city of metallurgists, shipbuilders and fishermen.
The largest enterprises in the city are:
Construction-materials, food processing, and light industries play a significant role in the city's economy. Kerch is also a fishing fleet base and an important processing centre for numerous fish products.
Transport.
Kerch has a harbour on the Kerch Strait, which makes it a key to the Sea of Azov, several railroad terminals and a small airport. The Kerch Strait ferry line across the Kerch Strait was established in 1953, connecting Crimea and the Krasnodar Krai (Port Krym - Port Kavkaz line); (as of November 2009) there are also plans for a Kerch-Poti ferry route.
There are several ports in Kerch, including Kerch Maritime Trading Port, Kerch Maritime Fishing Port, Port Krym (ferry crossing), Kamysh-Burun Port.
The railroad terminals include: Kerch, Kerch I, Kerch Factory, Arshyntsevo, and Krym.
Bus network connects Kerch to other cities in Crimea and Krasnodar Krai.
Education.
Kerch hosts (2004):
Archaeology.
Archaeological digs in Kerch were launched under Russian auspices in the middle of the 19th century. Since then the site of ancient Panticapaeum city on Mount Mithridat has been systematically excavated. Located nearby are several ancient burial mounds (kurgans) and excavated cities. Kerch takes part in UNESCO's "Silk Road" programme.
Treasures and historical findings of Kerch adorn the collections of major museums around the world. Such as: the Hermitage, the Louvre, the British Museum, the Berlin Museum, the Moscow State Museum of fine arts and many others.
Currently, excavations at ancient fortresses of Kerch are led by scientists from Russia, Ukraine, and Poland.
Tourism.
Because of its location on shores of Azov and Black seas, Kerch became a popular summer resort among people of former USSR. Also, several mud-cure sources are located near the city. Despite the seaside location, the tourist appeal of Kerch today is limited because of the industrial character of the city and associated pollution.
Despite the lack of beaches in the town's area, there are a lot of them at a distance of 20 minutes' travel by bus, train or taxi.
Kerch has a number of impressive architectural and historical monuments. Ancient historical heritage of the city makes it attractive for scientific tourism. The most notable of Kerch's sights are:
Honours.
A minor planet 2216 Kerch discovered in 1971 by Soviet astronomer Tamara Mikhailovna Smirnova is named after the city.
Climate.
Kerch has a humid subtropical climate (Köppen climate classification "Cfa") with cool to cold winters and warm to hot summers.
Recent events.
Autumn storm of 2007.
On 11 November 2007 there was a great storm that passed through the city, causing much damage and an ecological disaster as a few ships, including an oil tanker, were shipwrecked and blocked the Kerch Strait.
Kerch Strait Bridge.
On 25 April 2010, Ukrainian President Viktor Yanukovych and Russian President Dmitry Medvedev signed an agreement to build a bridge across the Kerch Strait.

</doc>
<doc id="47639" url="http://en.wikipedia.org/wiki?curid=47639" title="Liane Gabora">
Liane Gabora

Liane Gabora is a professor of psychology at the University of British Columbia - Okanagan. She is best known for her theory of the "Origin of the modern mind through conceptual closure." This built on her earlier work on "Autocatalytic closure in a cognitive system: A tentative scenario for the origin of culture."
Career.
Gabora has contributed to the study of cultural evolution and evolution of societies, focusing strongly on the role of personal creativity, as opposed to memetic imitation or instruction, in differentiating modern human from prior hominid or modern ape culture. In particular, she seems to follow feminist economists and green economists in making a very strong, indeed pivotal, distinction between creative "enterprise", invention, art or "individual capital" and imitative "meme", rule, social category or "instructional capital". 
Gabora's views contrasts with that of memetics and of the strongest social capital theorists (e.g. Karl Marx or Paul Adler) in that she seems to see, as do theorists of intellectual capital, social signals or labels as markers of trust already invested in individual and instructional complexes - rather than as first class actors in themselves. She puts special emphasis on quantifiable archaeological data, such as the number of different arrow points styles, than on contemporary observations to minimize cultural bias and notational bias.
Some of her recent work raises extremely controversial themes in philosophy of science and strongly challenges the particle physics foundation ontology (e.g. studying the "violation of Bell inequalities in the macroworld"). She is also known for her contributions to the subtle technology field.
Sources.
Gabora, L. (2010). Revenge of the 'neurds': Characterizing creative thought in terms of the structure and dynamics of human memory. Creativity Research Journal, 22(1), 1-13.

</doc>
<doc id="47640" url="http://en.wikipedia.org/wiki?curid=47640" title="British Sign Language">
British Sign Language

British Sign Language (BSL) is the sign language used in the United Kingdom (UK), and is the first or preferred language of some deaf people in the UK; there are 125,000 deaf adults in the UK who use BSL plus an estimated 20,000 children. In 2011, 15,000 people, living in England and Wales, reported themselves using BSL as their main language. The language makes use of space and involves movement of the hands, body, face and head. Many thousands of people who are not deaf also use BSL, as hearing relatives of deaf people, sign language interpreters or as a result of other contact with the British deaf community.
History.
Records exist of a sign language existing within deaf communities in Britain as far back as 1570. British sign language has evolved, as all languages do, from these origins by modification, invention and importation. Thomas Braidwood, an Edinburgh teacher, founded 'Braidwood's Academy for the Deaf and Dumb' in 1760 which is recognised as the first school for the deaf in Britain. His pupils were the sons of the well-to-do. His early use of a form of sign language, "the combined system", was the first codification of what was to become British Sign Language. Joseph Watson was trained as a teacher of the Deaf under Thomas Braidwood and he eventually left in 1792 to become the headmaster of the first public school for the Deaf in Britain, the London Asylum for the Deaf and Dumb in Bermondsey.
In 1815, an American Protestant minister, Thomas Hopkins Gallaudet, travelled to Europe to research teaching of the deaf. He was rebuffed by both the Braidwood schools who refused to teach him their methods. Gallaudet then travelled to Paris and learned the educational methods of the French Royal Institution for the Deaf, a combination of Old French Sign Language and the signs developed by Abbé de l’Épée. As a consequence American sign language today has a 60% similarity to modern French Sign Language but is almost unintelligible to users of British Sign Language.
Until the 1940s sign language skills were passed on unofficially between deaf people often living in residential institutions. Signing was actively discouraged in schools by punishment and the emphasis in education was on forcing deaf children to learn to lip read and finger spell. From the 1970s there has been an increasing tolerance and instruction in BSL in schools. The language continues to evolve as older signs such as "alms" and "pawnbroker" have fallen out of use and new signs such as "internet" and "laser" have been coined. The evolution of the language and its changing level of acceptance means that older users tend to rely on finger spelling while younger ones make use of a wider range of gestures.
On March 18, 2003 the UK government formally recognized that BSL is a language in its own right.
Linguistics.
Phonology.
Like many other sign languages, BSL phonology is defined by elements such as hand shape, orientation, location, and motion.
Grammar.
BSL uses a topic–comment structure. Canonical word order outside of topic–comment structure is OSV, and noun phrases are head-initial.
Relationships with other sign languages.
Although the United Kingdom and the United States share English as the predominant oral language, British Sign Language is quite distinct from American Sign Language (ASL) - having only 31% signs identical, or 44% cognate. BSL is also distinct from Irish Sign Language (ISL) (ISG in the ISO system) which is more closely related to French Sign Language (LSF) and ASL.
It is also distinct from Signed English, a manually coded method expressed to represent the English language.
The sign languages used in Australia and New Zealand, Auslan and New Zealand Sign Language, respectively, evolved largely from 19th century BSL, and all retain the same manual alphabet and grammar and possess similar lexicons. These three languages may technically be considered dialects of a single language (BANZSL) due to their use of the same grammar and manual alphabet and the high degree of lexical sharing (overlap of signs). The term BANZSL was coined by Trevor Johnston and Adam Schembri.
In Australia deaf schools were established by educated deaf people from London, Edinburgh and Dublin. This introduced the London and Edinburgh dialects of BSL to Melbourne and Sydney respectively and Irish Sign Language to Sydney in Roman Catholic schools for the deaf. The language contact post secondary education between Australian ISL users and 'Australian BSL' users accounts for some of the dialectal differences we see between modern BSL and Auslan. Tertiary education in the US for some deaf Australian adults also accounts for some ASL borrowings found in modern Auslan.
Auslan, BSL and NZSL have 82% of signs identical (using concepts from a Swadesh list). When considering similar or related signs as well as identical, they are 98% cognate. Further information will be available after the completion of the is completed and allows for comparison with the and the . There continues to be language contact between BSL, Auslan and NZSL through migration (deaf people and interpreters), the media (television programmes such as See Hear, Switch, Rush and SignPost are often recorded and shared informally in all three countries) and conferences (the World Federation of the Deaf Conference – WFD – in Brisbane 1999 saw many British deaf people travelling to Australia).
Makaton, a communication system for people with cognitive impairments or other communication difficulties, was originally developed with signs borrowed from British Sign Language. The sign language used in Sri Lanka is also closely related to BSL despite the oral language not being English, demonstrating the distance between sign languages and spoken ones.
BSL users campaigned to have BSL recognised on a similar level to Welsh, Scottish Gaelic, and Irish. BSL was recognised as a language in its own right by the UK government on 18 March 2003, but it has no legal protection. There is however legislation requiring the provision of interpreters such as the Police and Criminal Evidence Act 1984.
Usage.
BSL has many regional dialects. Signs used in Scotland, for example, may not be used, and may not be understood immediately by those in Southern England, and vice versa. Some signs are even more local, occurring only in certain towns or cities (such as the Manchester system of number signs). Likewise, some may go in or out of fashion, or evolve over time, just as terms in oral languages do.
Many British television channels broadcast programmes with in-vision signing, using BSL, as well as specially made programmes aimed mainly at deaf people such as the BBC's "See Hear" and Channel 4's "VEE-TV".
BBC News broadcasts in-vision signing at 07:00-07:45, 08:00-08:20 and 13:00-13:45 GMT/BST each weekday. BBC One also broadcasts in-vision signed repeats of the channel's primetime programmes between 00:30 and 04:00 each weekday. All BBC channels (excluding BBC Alba and BBC Parliament) provide in-vision signing for some of their programmes.
BSL is used in some educational establishments, but is not always the policy for deaf children in some local authority areas. The Let's Sign BSL and fingerspelling graphics are being developed for use in education by deaf educators and tutors and include many of the regional signs referred to above.
Learning British Sign Language.
British Sign Language can be learnt throughout the UK and four examination systems exist. Courses are provided by community colleges, local centres for deaf people and private organisations. Most tutors are native users of sign language and hold a relevant teaching qualification.
 is an awarding body accredited by the Qualifications and Curriculum Authority (QCA) who provide the following qualifications:
The British Deaf Association has formed the British Sign Language Academy to provide an official British Sign Language curriculum and tutor training.
 also award language qualifications: a Level 1 Award and Level 2, 3, 4 and 6 Certificates.
In Scotland, there is a Scottish Qualifications Authority (SQA) system for students learning British Sign Language. Currently there are 5 levels in the SQA system (continuing assessments):
Becoming a BSL / English interpreter.
There are two qualification routes: via post-graduate studies, or via National Vocational Qualifications. Deaf Studies undergraduate courses with specific streams for sign language interpreting exist at several British universities; post-graduate level interpreting diplomas are also on offer from universities and one private company. Course entry requirements vary from no previous knowledge of BSL to NVQ level 6 BSL (or equivalent). The alternative to university studies are either NVQ language and interpreting courses on offer from or language qualifications followed by an interpreting qualification which is mapped against the .
The qualification process allows interpreters to register with the National Registers of Communication Professionals with Deaf and Deafblind People (), a voluntary regulator. Registrants are asked to self-certify that they have both cleared a DBS (Disclosure and Barring Service) check and are covered by professional indemnity insurance. Completing a level 3 BSL language assessment and enrolling on an approved interpreting course allows applications to register as a TSLI (Trainee Sign Language Interpreter). After completing an approved interpreting course, trainees can then apply to achieve RSLI (Registered Sign Language Interpreter) status. RSLIs are currently required by NRCPD to log Continuous Professional Development activities. Post-qualification, specialist training is still considered necessary to work in specific critical domains.
Both the and provide a network of regional groups, professional development opportunities and mentoring. These membership organisations represent the sign language interpreting profession in England, Wales and Northern Ireland and provide interpreters with professional indemnity insurance.
Communication Support Workers.
Communication Support Workers (CSWs) are professionals who support the communication of deaf students in education at all ages, and deaf people in many areas of work, using British Sign Language and other communication methods such as Sign Supported English. The (ADEPT) is a national association, formed from a merger of ACSW and NATED in 2014, that supports and represents the interests and views of CSWs, encourages good practice and aims to improve the training standards and opportunities for current and future CSWs, among other things. The Association provides a professional network, improving information exchange, professional standards and support. The qualifications and experience of CSWs varies: some are fully qualified interpreters, others are not. There is a Level 3 Certificate in Communication Support for Deaf Learners available from Signature; this qualification is modelled on standards for learning support in Further Education only and is not required by all employers.

</doc>
<doc id="47641" url="http://en.wikipedia.org/wiki?curid=47641" title="Standard Model">
Standard Model

The Standard Model of particle physics is a theory concerning the electromagnetic, weak, and strong nuclear interactions, as well as classifying all the subatomic particles known. It was developed throughout the latter half of the 20th century, as a collaborative effort of scientists around the world. The current formulation was finalized in the mid-1970s upon experimental confirmation of the existence of quarks. Since then, discoveries of the top quark (1995), the tau neutrino (2000), and more recently the Higgs boson (2013), have given further credence to the Standard Model. Because of its success in explaining a wide variety of experimental results, the Standard Model is sometimes regarded as a "theory of almost everything".
Although the Standard Model is believed to be theoretically self-consistent and has demonstrated huge and continued successes in providing experimental predictions, it does leave some phenomena unexplained and it falls short of being a complete theory of fundamental interactions. It does not incorporate the full theory of gravitation as described by general relativity, or account for the accelerating expansion of the universe (as possibly described by dark energy). The model does not contain any viable dark matter particle that possesses all of the required properties deduced from observational cosmology. It also does not incorporate neutrino oscillations (and their non-zero masses).
The development of the Standard Model was driven by theoretical and experimental particle physicists alike. For theorists, the Standard Model is a paradigm of a quantum field theory, which exhibits a wide range of physics including spontaneous symmetry breaking, anomalies, non-perturbative behavior, etc. It is used as a basis for building more exotic models that incorporate hypothetical particles, extra dimensions, and elaborate symmetries (such as supersymmetry) in an attempt to explain experimental results at variance with the Standard Model, such as the existence of dark matter and neutrino oscillations.
Historical background.
The first step towards the Standard Model was Sheldon Glashow's discovery in 1961 of a way to combine the electromagnetic and weak interactions. In 1967 Steven Weinberg and Abdus Salam incorporated the Higgs mechanism into Glashow's electroweak theory, giving it its modern form.
The Higgs mechanism is believed to give rise to the masses of all the elementary particles in the Standard Model. This includes the masses of the W and Z bosons, and the masses of the fermions, i.e. the quarks and leptons.
After the neutral weak currents caused by Z boson exchange were discovered at CERN in 1973, the electroweak theory became widely accepted and Glashow, Salam, and Weinberg shared the 1979 Nobel Prize in Physics for discovering it. The W and Z bosons were discovered experimentally in 1981, and their masses were found to be as the Standard Model predicted.
The theory of the strong interaction, to which many contributed, acquired its modern form around 1973–74, when experiments confirmed that the hadrons were composed of fractionally charged quarks.
Overview.
At present, matter and energy are best understood in terms of the kinematics and interactions of elementary particles. To date, physics has reduced the laws governing the behavior and interaction of all known forms of matter and energy to a small set of fundamental laws and theories. A major goal of physics is to find the "common ground" that would unite all of these theories into one integrated theory of everything, of which all the other known laws would be special cases, and from which the behavior of all matter and energy could be derived (at least in principle).
Particle content.
The Standard Model includes members of several classes of elementary particles (fermions, gauge bosons, and the Higgs boson), which in turn can be distinguished by other characteristics, such as color charge.
Fermions.
The Standard Model includes 12 elementary particles of spin-½ known as fermions. According to the spin-statistics theorem, fermions respect the Pauli exclusion principle. Each fermion has a corresponding antiparticle.
The fermions of the Standard Model are classified according to how they interact (or equivalently, by what charges they carry). There are six quarks (up, down, charm, strange, top, bottom), and six leptons (electron, electron neutrino, muon, muon neutrino, tau, tau neutrino). Pairs from each classification are grouped together to form a generation, with corresponding particles exhibiting similar physical behavior (see table).
The defining property of the quarks is that they carry color charge, and hence, interact via the strong interaction. A phenomenon called color confinement results in quarks being very strongly bound to one another, forming color-neutral composite particles (hadrons) containing either a quark and an antiquark (mesons) or three quarks (baryons). The familiar proton and the neutron are the two baryons having the smallest mass. Quarks also carry electric charge and weak isospin. Hence they interact with other fermions both electromagnetically and via the weak interaction.
The remaining six fermions do not carry colour charge and are called leptons. The three neutrinos do not carry electric charge either, so their motion is directly influenced only by the weak nuclear force, which makes them notoriously difficult to detect. However, by virtue of carrying an electric charge, the electron, muon, and tau all interact electromagnetically.
Each member of a generation has greater mass than the corresponding particles of lower generations. The first generation charged particles do not decay; hence all ordinary (baryonic) matter is made of such particles. Specifically, all atoms consist of electrons orbiting atomic nuclei ultimately constituted of up and down quarks. Second and third generations charged particles, on the other hand, decay with very short half lives, and are observed only in very high-energy environments. Neutrinos of all generations also do not decay, and pervade the universe, but rarely interact with baryonic matter.
Gauge bosons.
In the Standard Model, gauge bosons are defined as force carriers that mediate the strong, weak, and electromagnetic fundamental interactions.
Interactions in physics are the ways that particles influence other particles. At a macroscopic level, electromagnetism allows particles to interact with one another via electric and magnetic fields, and gravitation allows particles with mass to attract one another in accordance with Einstein's theory of general relativity. The Standard Model explains such forces as resulting from matter particles exchanging other particles, generally referred to as "force mediating particles". When a force-mediating particle is exchanged, at a macroscopic level the effect is equivalent to a force influencing both of them, and the particle is therefore said to have "mediated" (i.e., been the agent of) that force. The Feynman diagram calculations, which are a graphical representation of the perturbation theory approximation, invoke "force mediating particles", and when applied to analyze high-energy scattering experiments are in reasonable agreement with the data. However, perturbation theory (and with it the concept of a "force-mediating particle") fails in other situations. These include low-energy quantum chromodynamics, bound states, and solitons.
The gauge bosons of the Standard Model all have spin (as do matter particles). The value of the spin is 1, making them bosons. As a result, they do not follow the Pauli exclusion principle that constrains fermions: thus bosons (e.g. photons) do not have a theoretical limit on their spatial density (number per volume). The different types of gauge bosons are described below.
The interactions between all the particles described by the Standard Model are summarized by the diagrams on the right of this section.
Higgs boson.
The Higgs particle is a massive scalar elementary particle theorized by Robert Brout, François Englert, Peter Higgs, Gerald Guralnik, C. R. Hagen, and Tom Kibble in 1964 (see 1964 PRL symmetry breaking papers) and is a key building block in the Standard Model. It has no intrinsic spin, and for that reason is classified as a boson (like the gauge bosons, which have integer spin).
The Higgs boson plays a unique role in the Standard Model, by explaining why the other elementary particles, except the photon and gluon, are massive. In particular, the Higgs boson explains why the photon has no mass, while the W and Z bosons are very heavy. Elementary particle masses, and the differences between electromagnetism (mediated by the photon) and the weak force (mediated by the W and Z bosons), are critical to many aspects of the structure of microscopic (and hence macroscopic) matter. In electroweak theory, the Higgs boson generates the masses of the leptons (electron, muon, and tau) and quarks. As the Higgs boson is massive, it must interact with itself.
Because the Higgs boson is a very massive particle and also decays almost immediately when created, only a very high-energy particle accelerator can observe and record it. Experiments to confirm and determine the nature of the Higgs boson using the Large Hadron Collider (LHC) at CERN began in early 2010, and were performed at Fermilab's Tevatron until its closure in late 2011. Mathematical consistency of the Standard Model requires that any mechanism capable of generating the masses of elementary particles become visible at energies above ; therefore, the LHC (designed to collide two 7 to 8 TeV proton beams) was built to answer the question of whether the Higgs boson actually exists.
On 4 July 2012, the two main experiments at the LHC (ATLAS and CMS) both reported independently that they found a new particle with a mass of about (about 133 proton masses, on the order of 10−25 kg), which is "consistent with the Higgs boson." Although it has several properties similar to the predicted "simplest" Higgs, they acknowledged that further work would be needed to conclude that it is indeed the Higgs boson, and exactly which version of the Standard Model Higgs is best supported if confirmed.
On 14 March 2013 the Higgs Boson was tentatively confirmed to exist.
Full particle count.
Counting particles by a rule that distinguishes between particles and their corresponding antiparticles, and among the many color states of quarks and gluons, gives a total of 61 elementary particles.
Theoretical aspects.
Construction of the Standard Model Lagrangian.
Technically, quantum field theory provides the mathematical framework for the Standard Model, in which a Lagrangian controls the dynamics and kinematics of the theory. Each kind of particle is described in terms of a dynamical field that pervades space-time. The construction of the Standard Model proceeds following the modern method of constructing most field theories: by first postulating a set of symmetries of the system, and then by writing down the most general renormalizable Lagrangian from its particle (field) content that observes these symmetries.
The global Poincaré symmetry is postulated for all relativistic quantum field theories. It consists of the familiar translational symmetry, rotational symmetry and the inertial reference frame invariance central to the theory of special relativity. The local SU(3)×SU(2)×U(1) gauge symmetry is an internal symmetry that essentially defines the Standard Model. Roughly, the three factors of the gauge symmetry give rise to the three fundamental interactions. The fields fall into different representations of the various symmetry groups of the Standard Model (see table). Upon writing the most general Lagrangian, one finds that the dynamics depend on 19 parameters, whose numerical values are established by experiment. The parameters are summarized in the table above (note: with the Higgs mass is at 125 GeV, the Higgs self-coupling strength "λ" ~ 1/8).
Quantum chromodynamics sector.
The quantum chromodynamics (QCD) sector defines the interactions between quarks and gluons, with SU(3) symmetry, generated by Ta. Since leptons do not interact with gluons, they are not affected by this sector. The Dirac Lagrangian of the quarks coupled to the gluon fields is given by
formula_2 is the SU(3) gauge field containing the gluons, formula_3 are the Dirac matrices, D and U are the Dirac spinors associated with up- and down-type quarks, and gs is the strong coupling constant.
Electroweak sector.
The electroweak sector is a Yang–Mills gauge theory with the simple symmetry group U(1)×SU(2)L,
where "B""μ" is the U(1) gauge field; "Y"W is the weak hypercharge—the generator of the U(1) group; formula_5 is the
three-component SU(2) gauge field; formula_6 are the Pauli matrices—infinitesimal generators of the SU(2) group. The subscript L indicates that they only act on left fermions; "g"′ and "g" are coupling constants.
Higgs sector.
In the Standard Model, the Higgs field is a complex scalar of the group SU(2)L:
where the indices + and 0 indicate the electric charge ("Q") of the components. The weak isospin ("Y"W) of both components is 1.
Before symmetry breaking, the Higgs Lagrangian is:
which can also be written as:
Fundamental forces.
The Standard Model classified all four fundamental forces in nature. In the Standard Model, a force is described as an exchange of bosons between the objects affected, such as a photon for the electromagnetic force and a gluon for the strong interaction. Those particles are called force carriers.
Tests and predictions.
The Standard Model (SM) predicted the existence of the W and Z bosons, gluon, and the top and charm quarks before these particles were observed. Their predicted properties were experimentally confirmed with good precision. To give an idea of the success of the SM, the following table compares the measured masses of the W and Z bosons with the masses predicted by the SM:
The SM also makes several predictions about the decay of Z bosons, which have been experimentally confirmed by the Large Electron-Positron Collider at CERN.
In May 2012 BaBar Collaboration reported that their recently analyzed data may suggest possible flaws in the Standard Model of particle physics. These data show that a particular type of particle decay called "B to D-star-tau-nu" happens more often than the Standard Model says it should. In this type of decay, a particle called the B-bar meson decays into a D meson, an antineutrino and a tau-lepton.
While the level of certainty of the excess (3.4 sigma) is not enough to claim a break from the Standard Model, the results are a potential sign of something amiss and are likely to impact existing theories, including those attempting to deduce the properties of Higgs bosons.
On December 13, 2012, physicists reported the constancy, over space and time, of a basic physical constant of nature that supports the "standard model of physics". The scientists, studying methanol molecules in a distant galaxy, found the change (∆μ/μ) in the proton-to-electron mass ratio μ to be equal to "(0.0 ± 1.0) × 10−7 at redshift z = 0.89" and consistent with "a null result".
Challenges.
Self-consistency of the Standard Model (currently formulated as a non-abelian gauge theory quantized through path-integrals) has not been mathematically proven. While regularized versions useful for approximate computations (for example lattice gauge theory) exist, it is not known whether they converge (in the sense of S-matrix elements) in the limit that the regulator is removed. A key question related to the consistency is the Yang–Mills existence and mass gap problem.
Experiments indicate that neutrinos have mass, which the classic Standard Model did not allow. To accommodate this finding, the classic Standard Model can be modified to include neutrino mass.
If one insists on using only Standard Model particles, this can be achieved by adding a non-renormalizable interaction of leptons with the Higgs boson. On a fundamental level, such an interaction emerges in the seesaw mechanism where heavy right-handed neutrinos are added to the theory.
This is natural in the left-right symmetric extension of the Standard Model and in certain grand unified theories. As long as new physics appears below or around 1014 GeV, the neutrino masses can be of the right order of magnitude.
Theoretical and experimental research has attempted to extend the Standard Model into a Unified field theory or a Theory of everything, a complete theory explaining all physical phenomena including constants. Inadequacies of the Standard Model that motivate such research include:
Currently, no proposed Theory of Everything has been widely accepted or verified.

</doc>
<doc id="47642" url="http://en.wikipedia.org/wiki?curid=47642" title="Castling">
Castling

Castling is a move in the game of chess involving a player's king and either of the player's original rooks. It is the only move in chess in which a player moves two pieces in the same move, and it is the only move aside from the knight's move where a piece can be said to "jump over" another.
Castling consists of moving the king two squares towards a rook on the player's first rank, then moving the rook to the square over which the king crossed. Castling may only be done if the king has never moved, the rook involved has never moved, the squares between the king and the rook involved are unoccupied, the king is not in check, and the king does not cross over or end on a square in which it would be in check. Castling is one of the rules of chess and is technically a king move .
The notation for castling, in both the descriptive and the algebraic systems, is 0-0 with the kingside rook and 0-0-0 with the queenside rook. (In PGN, O-O and O-O-O are used instead.) Castling on the kingside is sometimes called "castling short" and castling on the queenside is called "castling long" – the difference based on whether the rook moves a short distance (two squares) or a long distance (three squares) .
Castling was added to European chess in the 14th or 15th century and did not develop into its present form until the 17th century. The Asian versions of chess do not have such a move.
 
Requirements.
Castling is permissible if and only if all of the following conditions hold :
Conditions 4 through 6 can be summarized with the more memorable phrase: "One may not castle out of, through, or into check."
It is a common misperception that the requirements for castling are even more stringent than the above. To clarify:
In handicap games where odds of a rook are given, the player giving odds may still castle with the absent rook, moving only the king.
Strategy.
Castling is an important goal in the opening, because it serves two valuable purposes: it moves the king into a safer position away from the center of the board, and it moves the rook to a more active position in the center of the board (it is even possible to checkmate with castling).
The choice as to which side to castle often hinges on an assessment of the trade-off between king safety and activity of the rook. Kingside castling is generally slightly safer, because the king ends up closer to the edge of the board and all the pawns on the castled side are defended by the king. In queenside castling, the king is placed closer to the center and the pawn on the a-file is undefended; the king is thus often moved to the b-file to defend the a-pawn and to move the king away from the center of the board. In addition, queenside castling requires moving the queen; therefore, it may take slightly longer to achieve than kingside castling. On the other hand, queenside castling places the rook more effectively – on the central d-file. It is often immediately active, whereas with kingside castling a tempo may be required to move the rook to a more effective square.
It is common for both players to castle kingside, and rare for both players to castle queenside. If one player castles kingside and the other queenside, it is called "opposite" (or "opposite-side") "castling". Castling on opposite sides usually results in a fierce fight as both players' pawns are free to advance to attack the opposing king's castled position without exposing the player's own castled king. An example is the Yugoslav Attack, in the Dragon Variation of the Sicilian Defence.
If the king is forced to move before it has the opportunity to castle, the player may still wish to maneuver the king towards the edge of the board and the corresponding rook towards the center. When a player takes three or four moves to accomplish what castling would have accomplished in one move, it is sometimes called "artificial castling", or "castling by hand".
Tournament rules.
Under the strict touch-move rules enforced in most tournaments, castling is considered a king move. But under current US Chess Federation rules, a player who intends to castle and touches the rook first would suffer no penalty, and would be permitted to perform castling, provided castling is legal in the position. Still, the correct way to castle is to first move the king. As usual, the player's mind may change between all legal destination squares for the king until it is released. When the two-square king move is completed, however, the player has formally chosen to castle (if it is legal), and the rook must be moved accordingly. A player who performs a forbidden castling must return the king and the rook to their original places and then move the king, if there is another legal king move, including castling on the other side. If there is no legal king move, the touch-move rule does "not" apply to the rook .
It is also required by the official rules that the entire move be completed using only a single hand. Neither of these rules is commonly enforced in casual play, nor commonly known by non-competitive players .
The right to castle must be the same in all three positions for a valid draw claim under the threefold repetition rule.
Chess variants and problems.
Some chess variants, for example Chess960, have modified castling rules to handle modified starting positions. Castling can also be adapted to large chess variants, like Capablanca chess, which is played on a 10×8 board.
In chess problems, castling is assumed to be allowed if it appears possible, unless it can be proved by retrograde analysis that either the king or chosen rook has previously moved.
Notable examples.
Averbakh game.
In this game between Yuri Averbakh and Cecil Purdy, Black castled queenside. Averbakh pointed out that the rook passed over a square controlled by White and thought it was illegal. Purdy proved that the castling was legal since this applies only to the king, to which Averbakh replied "Only the king? Not the rook?" , .
Lasker game.
In this game between Edward Lasker and Sir George Thomas (London 1912), Black had just played 17...Kg1. White might have checkmated by 18.0-0-0# but instead played 18.Kd2#. (See Edward Lasker's notable games.)
Prins versus Day.
This game between Lodewijk Prins and Lawrence Day ended in a checkmate by castling: 31...0-0-0#. (See Lawrence Day' notable chess games.)
Feuer versus O'Kelly.
In the game , Feuer perpetrated what later became known as a famous opening trap against O'Kelly when he castled queenside with check, simultaneously attacking and winning O'Kelly's rook on b2, which had captured Feuer's pawn on that square.
History.
Castling has its roots in the "king's leap". There were two forms of the leap: (1) the king would move once like a knight, and (2) the king would move two squares on his first move. The knight-move might be used early in the game to get the king to safety or later in the game to escape a threat. This second form was played in Europe as early as the 13th Century. In North Africa, the king was moved to a safe square by a two-step procedure: (1) the king moved to the second rank and (2) the rook moved to the king's original square and the king moved to the rook's original square .
Before the bishop and queen acquired their current moves in the 16th Century they were weak pieces and the king was relatively safe in the middle of the board. When the bishop and queen got their current moves they became very powerful and the king was no longer safe on its original square, since it can be attacked from a distance and from both sides. Castling was added to allow the king to get to a safer location and to allow rooks to get into the game earlier .
The rule of castling has varied by location and time. In medieval England, Spain, and France, the white king was allowed to jump to c1, c2, d3, e3, f3, or g1, if no capture was made, the king was not in check, and did not move over check. (The black king might move similarly.) In Lombardy, the white king might jump an additional square to b1 or h1 or to a2 (and equivalent squares for the black king). Later in Germany and Italy, the king move was combined with a pawn move.
In Rome from the early 17th century until the late 19th century, the rook might be placed on any square up to and including the king's square, and the king might be moved to any square on the other side of the rook. This was called "free castling".
In the Göttingen manuscript (c. 1500) and a game published by Luis Ramírez de Lucena in 1498, castling consisted of two moves: first the rook and then the king.
The current version of castling was established in France in 1620 and England in 1640 .
In the 1811 edition of his chess treatise, Johann Allgaier introduced the 0-0 notation. He differentiated between "0-0r" (r=right) and "0-0l" (l=left). The 0-0-0 notation for queenside castling was added in 1837 by Aaron Alexandre. The practice was then accepted in the first edition (1843) of the "Handbuch des Schachspiels".
Nomenclature.
Castling is in most European languages other than English known as "roszada", "rochieren", "rochada", "rocada" - Romanian, "enroque", "rošáda", "arrocco", "rokada", "рокировка", or some other derivative of the same root (from which also the English word "rook" is derived), while the local adjectives meaning "long" and "short" (or "big" and "small") are used in those countries to refer to queenside and kingside castling, respectively.
References.
Bibliography
</dl>

</doc>
<doc id="47643" url="http://en.wikipedia.org/wiki?curid=47643" title="Michel Foucault">
Michel Foucault

Michel Foucault (]; born Paul-Michel Foucault) (15 October 1926 – 25 June 1984) was a French philosopher, historian of ideas, social theorist, philologist and literary critic. His theories addressed the relationship between power and knowledge, and how they are used as a form of social control through societal institutions. Though often cited as a post-structuralist and postmodernist, Foucault rejected these labels, preferring to present his thought as a critical history of modernity. His thought has been highly influential for both academic and activist groups.
Born in Poitiers, France, to an upper-middle-class family, Foucault was educated at the Lycée Henri-IV and then the École Normale Supérieure, where he developed an interest in philosophy and came under the influence of his tutors Jean Hyppolite and Louis Althusser. After several years as a cultural diplomat abroad, he returned to France and published his first major book, "The History of Madness". After obtaining work between 1960 and 1966 at the University of Clermont-Ferrand, he produced two more significant publications, "The Birth of the Clinic" and "The Order of Things", which displayed his increasing involvement with structuralism, a theoretical movement in social anthropology from which he later distanced himself. These first three histories were examples of a historiographical technique Foucault was developing which he called "archaeology".
From 1966 to 1968, Foucault lectured at the University of Tunis, Tunisia, before returning to France, where he became head of the philosophy department at the new experimental university of Paris VIII. In 1970 he was admitted to the Collège de France, membership of which he retained until his death. He also became active in a number of left-wing groups involved in anti-racist campaigns, anti-human rights abuses movements, and the struggle for penal reform. He went on to publish "The Archaeology of Knowledge", "Discipline and Punish", and "The History of Sexuality". In these books, he developed archaeological and genealogical methods which emphasized the role power plays in the evolution of discourse in society. Foucault died in Paris of neurological problems compounded by HIV/AIDS; he was the first public figure in France to have died from the disease, and his partner Daniel Defert founded the AIDES charity in his memory.
Early life.
Youth: 1926–1946.
Paul-Michel Foucault was born on 15 October 1926 in the city of Poitiers, west-central France, as the second of three children to a prosperous and socially conservative upper-middle-class family.
He had been named after his father, Dr. Paul Foucault, as was the family tradition, but his mother insisted on the addition of the double-barrelled "Michel"; referred to as "Paul" at school, throughout his life he always expressed a preference for "Michel".
His father (1893–1959) was a successful local surgeon, having been born in Fontainebleau before moving to Poitiers, where he set up his own practice and married local woman Anne Malapert.
She was the daughter of prosperous surgeon Dr. Prosper Malapert, who owned a private practice and taught anatomy at the University of Poitiers' School of Medicine. Paul Foucault eventually took over his father-in-law's medical practice, while his wife took charge of their large mid-19th-century house, Le Piroir, in the village of Vendeuvre-du-Poitou. Together the couple had three children, a girl named Francine and two boys, Paul-Michel and Denys, all of whom shared the same fair hair and bright blue eyes. The children were raised to be nominal Roman Catholics, attending mass at the Church of Saint-Porchair, and while Michel briefly became an altar boy, none of the family were devout.
"I wasn't always smart, I was actually very stupid in school ... [T]here was a boy who was very attractive who was even stupider than I was. And in order to ingratiate myself with this boy who was very beautiful, I began to do his homework for him—and that's how I became smart, I had to do all this work to just keep ahead of him a little bit, in order to help him. In a sense, all the rest of my life I've been trying to do intellectual things that would attract beautiful boys."
Michel Foucault, 1983
In later life, Foucault would reveal very little about his childhood. Describing himself as a "juvenile delinquent", he claimed his father was a "bully" who would sternly punish him. In 1930, Foucault began his schooling two years early at the local Lycée Henry-IV. Here he undertook two years of elementary education before entering the main "lycée", where he stayed until 1936. He then undertook his first four years of secondary education at the same establishment, excelling in French, Greek, Latin and history but doing poorly at arithmetic and mathematics. In 1939, the Second World War broke out and France was occupied by Nazi Germany until 1945; his parents opposed the occupation and the Vichy regime, but did not join the Resistance. In 1940, Foucault's mother enrolled him in the Collège Saint-Stanislas, a strict Roman Catholic institution run by the Jesuits. Lonely, he described his years there as the "ordeal", but excelled academically, particularly in philosophy, history and literature. In 1942, he entered his final year, the "terminale", where he focused on the study of philosophy, earning his "baccalauréat" in 1943.
Returning to the local Lycée Henry-IV, he studied history and philosophy for a year, aided by a personal tutor, the philosopher Louis Girard. Rejecting his father's wishes that he become a surgeon, in 1945 Foucault traveled to Paris, where he enrolled in one of the country's most prestigious secondary schools, which was also known as the Lycée Henri-IV. Here, he studied under the philosopher Jean Hyppolite, an existentialist and expert on the work of 19th-century German philosopher Hegel who had devoted himself to uniting existentialist theories with the dialectical theories of Hegel and Karl Marx. These ideas influenced Foucault, who adopted Hyppolite's conviction that philosophy must be developed through a study of history.
École Normale Supérieure: 1946–1951.
Attaining excellent results, in autumn 1946 Foucault was admitted to the elite École Normale Supérieure (ENS); to gain entry, he undertook exams and an oral interrogation by Georges Canguilhem and Pierre-Maxime Schuhl. Of the hundred students entering the ENS, Foucault was ranked fourth based on his entry results, and encountered the highly competitive nature of the institution. Like most of his classmates, he was housed in the school's communal dormitories on the Parisian Rue d'Ulm.
He remained largely unpopular, spending much time alone, reading voraciously. His fellow students noted his love of violence and the macabre; he decorated his bedroom with images of torture and war drawn during the Napoleonic Wars by Spanish artist Francisco Goya, and on one occasion chased a classmate with a dagger. Prone to self-harm, in 1948 Foucault allegedly undertook a failed suicide attempt, for which his father sent him to see the psychiatrist Jean Delay at the Hôpital Sainte-Anne. Obsessed with the idea of self-mutilation and suicide, Foucault attempted the latter several times in ensuing years, praising suicide in later writings. The ENS's doctor examined Foucault's state of mind, suggesting that his suicidal tendencies emerged from the distress surrounding his homosexuality, because same-sex sexual activity was socially taboo in France. At the time, Foucault engaged in homosexual activity with men whom he encountered in the underground Parisian gay scene, also indulging in drug use; according to biographer James Miller, he enjoyed the thrill and sense of danger that these activities offered him.
Although studying various subjects, Foucault's particular interest was soon drawn to philosophy, reading not only Hegel and Marx but also Immanuel Kant, Edmund Husserl and most significantly, Martin Heidegger. He began reading the publications of philosopher Gaston Bachelard, taking a particular interest in his work exploring the history of science. In 1948, the philosopher Louis Althusser became a tutor at the ENS. A Marxist, he proved to be an influence both on Foucault and a number of other students, encouraging them to join the French Communist Party ("Parti communiste français" – PCF). Foucault did so in 1950, but never became particularly active in its activities, and never adopted an orthodox Marxist viewpoint, refuting core Marxist tenets such as class struggle. He soon became dissatisfied with the bigotry that he experienced within the party's ranks; he personally faced homophobia and was appalled by the anti-semitism exhibited during the Doctors' plot in the Soviet Union. He left the Communist Party in 1953, but remained Althusser's friend and defender for the rest of his life. Although failing at the first attempt in 1950, he passed his "agrégation" in philosophy on the second try, in 1951. Excused from national service on medical grounds, he decided to study for a doctorate at the Fondation Thiers, focusing on the philosophy of psychology.
Early career: 1951–1955.
Over the following few years, Foucault embarked on a variety of research and teaching jobs. From 1951 to 1955, he worked as a psychology instructor at the ENS at Althusser's invitation. In Paris, he shared a flat with his brother, who was training to become a surgeon, but for three days in the week commuted to the northern town of Lille, teaching psychology at the Université Lille Nord de France from 1953 to 1954. Many of his students liked his lecturing style. Meanwhile, he continued working on his thesis, visiting the Bibliothèque Nationale every day to read the work of psychologists like Ivan Pavlov, Jean Piaget and Karl Jaspers. Undertaking research at the psychiatric institute of the Hôpital Sainte-Anne, he became an unofficial intern, studying the relationship between doctor and patient and aiding experiments in the electroencephalographic laboratory. Foucault adopted many of the theories of the psychoanalyst Sigmund Freud, undertaking psychoanalytical interpretation of his dreams and making friends undergo Rorschach tests.
Embracing the Parisian "avant-garde", Foucault entered into a romantic relationship with the serialist composer Jean Barraqué. Together, they pushed the boundaries of the human mind, trying to produce their greatest work; heavily used recreational drugs and engaged in sado-masochistic sexual activity. In August 1953, Foucault and Barraqué holidayed in Italy, where the philosopher immersed himself in "Untimely Meditations" (1873–1876), a set of four essays by philosopher Friedrich Nietzsche. Later describing Nietzsche's work as "a revelation", he felt that reading the book deeply affected him, being a watershed moment in his life. Foucault subsequently experienced another groundbreaking self-revelation when watching a Parisian performance of Samuel Beckett's new play, "Waiting for Godot", in 1953.
Interested in literature, Foucault was an avid reader of the philosopher Maurice Blanchot's book reviews published in "Nouvelle Revue Française". Enamoured with Blanchot's literary style and critical theories, in later works he adopted Blanchot's technique of "interviewing" himself. Foucault also came across Hermann Broch's 1945 novel "The Death of Virgil", a work that obsessed both him and Barraqué. While the latter attempted to convert the work into an epic opera, Foucault admired Broch's text for its portrayal of death as an affirmation of life. The couple took a mutual interest in the work of such authors as the Marquis de Sade, Fyodor Dostoyevsky, Franz Kafka and Jean Genet, all of whose works explored the themes of sex and violence.
"I belong to that generation who, as students, had before their eyes, and were limited by, a horizon consisting of Marxism, phenomenology and existentialism. For me the break was first Beckett's "Waiting for Godot", a breathtaking performance."
Michel Foucault, 1983
Interested in the work of Swiss psychologist Ludwig Binswanger, Foucault aided family friend Jacqueline Verdeaux in translating his works into French. Foucault was particularly interested in Binswanger's studies of Ellen West who, like himself, had a deep obsession with suicide, eventually killing herself. In 1954, Foucault authored an introduction to Binswanger's paper "Dream and Existence", in which he argued that dreams constituted "the birth of the world" or "the heart laid bare", expressing the mind's deepest desires. That same, year Foucault published his first book, "Mental Illness and Personality" ("Maladie mentale et personnalité"), in which he exhibited his influence from both Marxist and Heideggerian thought, covering a wide range of subject matter from the reflex psychology of Pavlov to the classic psychoanalysis of Freud. Referencing the work of sociologists and anthropologists such as Émile Durkheim and Margaret Mead, he presented his theory that illness was culturally relative. Biographer James Miller noted that while the book exhibited "erudition and evident intelligence", it lacked the "kind of fire and flair" which Foucault exhibited in subsequent works. It was largely critically ignored, receiving only one review at the time. Foucault grew to despise it, unsuccessfully attempting to prevent its republication and translation into English.
Sweden, Poland, and West Germany: 1955–1960.
Foucault spent the next five years abroad, first in Sweden, working as cultural diplomat at the University of Uppsala, a job obtained through his acquaintance with historian of religion Georges Dumézil. At Uppsala he was appointed a Reader in French language and literature, while simultaneously working as director of the Maison de France, thus opening the possibility of a cultural-diplomatic career. Although finding it difficult to adjust to the "Nordic gloom" and long winters, he developed close friendships with two Frenchmen, biochemist Jean-François Miquel and physicist Jacques Papet-Lépine, and entered into romantic and sexual relationships with various men. In Uppsala, he became known for his heavy alcohol consumption and reckless driving in his new Jaguar car. In spring 1956, Barraqué broke from his relationship with Foucault, announcing that he wanted to leave the "vertigo of madness". In Uppsala, Foucault spent much of his spare time in the university's "Carolina Rediviva" library, making use of their Bibliotheca Walleriana collection of texts on the history of medicine for his ongoing research. Finishing his doctoral thesis, Foucault hoped it would be accepted by Uppsala University, but Sten Lindroth, a historian of science there, was unimpressed, asserting that it was full of speculative generalisations and was a poor work of history; he refused to allow Foucault to be awarded a doctorate at Uppsala. In part because of this rejection, Foucault left Sweden.
Again at Dumézil's recognition, in October 1958 Foucault arrived in the Polish city of Warsaw, placed in charge of the University of Warsaw's Centre Français. Foucault found life in Poland difficult due to the lack of material goods and services following the destruction of the Second World War. Witnessing the aftermath of the Polish October in which students had protested against the governing communist Polish United Workers' Party, he felt that most Poles despised their government as a puppet regime of the Soviet Union, and thought that the system ran "badly". Considering the university a liberal enclave, he traveled the country giving lectures; proving popular, he adopted the position of "de facto" cultural attaché. Like France and Sweden, homosexual activity was legal but socially frowned upon in Poland, and he undertook relationships with a number of men; one was a Polish security agent who hoped to trap Foucault in an embarrassing situation, which would therefore reflect badly on the French embassy. Wracked in diplomatic scandal, he was ordered to leave Poland for a new destination. Various positions were available in West Germany, and so Foucault relocated to Hamburg, teaching the same courses he had given in Uppsala and Warsaw. Spending much time in the Reeperbahn red light district, he entered into a relationship with a transvestite.
Growing career.
"Madness and Civilization": 1960.
""Histoire de la folie" is not an easy text to read, and it defies attempts to summarise its contents. Foucault refers to a bewildering variety of sources, ranging from well-known authors such as Erasmus and Molière to archival documents and forgotten figures in the history of medicine and psychiatry. His erudition derives from years pondering, to cite Poe, 'over many a quaint and curious volume of forgotten lore', and his learning is not always worn lightly."
Foucault biographer David Macey, 1993
In West Germany Foucault completed his doctoral thesis, "Folie et déraison: Histoire de la folie à l'âge classique" ("Madness and Insanity: History of Madness in the Classical Age"), a philosophical work based upon his studies into the history of medicine. The book discussed how West European society had dealt with madness, arguing that it was a social construct distinct from mental illness. Foucault traces the evolution of the concept of madness through three phases: the Renaissance, the later 17th and 18th centuries, and the modern experience. The work alludes to the work of French poet and playwright Antonin Artaud, who exerted a strong influence over Foucault's thought at the time.
"Histoire de la folie" was an expansive work, consisting of 943 pages of text, followed by appendices and a bibliography. Foucault submitted it at the University of Paris, although the university's regulations for awarding a doctorate required the submission of both his main thesis and a shorter complementary thesis. Obtaining a doctorate in France at the period was a multi-step process. The first step was to obtain a "rapporteur", or sponsor for the work: Foucault chose Georges Canguilhem. The second was to find a publisher, and as a result "Folie et déraison" would be published in French in May 1961 by the company Plon, whom Foucault chose over Presses Universitaires de France after being rejected by Gallimard. In 1964, a heavily abridged version was published as a mass market paperback, then translated into English for publication the following year as "Madness and Civilization".
"Folie et déraison" received a mixed reception in France and in foreign journals focusing on French affairs. Although it was critically acclaimed by Blochot, Michel Serres, Roland Barthes, Gaston Bachelard, and Fernand Braudel, it was largely ignored by the leftist press, much to Foucault's disappointment. It was notably criticised for advocating metaphysics by young philosopher Jacques Derrida in a March 1963 lecture at the University of Paris. Responding with a vicious retort, Foucault criticised Derrida's interpretation of René Descartes. The two remained bitter rivals until reconciling in 1981. In the English-speaking world, the work became a significant influence on the anti-psychiatry movement during the 1960s; Foucault took a mixed approach to this, associating with a number of anti-psychiatrists but arguing that most of them misunderstood his work.
Foucault's secondary thesis was a translation and commentary on German philosopher Immanuel Kant's 1798 work "Anthropology from a Pragmatic Point of View". Largely consisting of Foucault's discussion of textual dating—an "archaeology of the Kantian text"—he rounded off the thesis with an evocation of Nietzsche, his biggest philosophical influence. This work's "rapporteur" was his old tutor and then director of the ENS, Hyppolite, who was well acquainted with German philosophy. After both theses were championed and reviewed, he underwent his public defense, the "soutenance de thèse", on 20 May 1961. The academics responsible for reviewing his work were concerned about the unconventional nature of his major thesis; reviewer Henri Gouhier noted that it was not a conventional work of history, making sweeping generalisations without sufficient particular argument, and that Foucault clearly "thinks in allegories". They all agreed however that the overall project was of merit, awarding Foucault his doctorate "despite reservations".
University of Clermont-Ferrand, "The Birth of the Clinic", and "The Order of Things": 1960–1966.
In October 1960, Foucault took a tenured post in philosophy at the University of Clermont-Ferrand, commuting to the city every week from Paris, where he lived in a high-rise block on the rue du Dr Finlay. Responsible for teaching psychology, which was subsumed within the philosophy department, he was considered a "fascinating" but "rather traditional" teacher at Clermont. The department was run by Jules Vuillemin, who soon developed a friendship with Foucault. Foucault then took Vuillemin's job when the latter was elected to the Collège de France in 1962. In this position, Foucault took a dislike to another staff member whom he considered stupid: Roger Garaudy, a senior figure in the Communist Party. Foucault made life at the university difficult for Garaudy, leading the latter to transfer to Poitiers. Foucault also caused controversy by securing a university job for his lover, the philosopher Daniel Defert, with whom he retained a non-monogamous relationship for the rest of his life.
Foucault maintained a keen interest in literature, publishing reviews in amongst others the literary journals "Tel Quel" and "Nouvelle Revue Française", and sitting on the editorial board of "Critique". In May 1963, he published a book devoted to poet, novelist, and playwright Raymond Roussel. It was written in under two months, published by Gallimard, and would be described by biographer David Macey as "a very personal book" that resulted from a "love affair" with Roussel's work. It would be published in English in 1983 as "Death and the Labyrinth: The World of Raymond Roussel". Receiving few reviews, it was largely ignored. That same year he published a sequel to " Folie et déraison", entitled "Naissance de la Clinique", subsequently translated as "Birth of the Clinic: An Archaeology of Medical Perception". Shorter than its predecessor, it focused on the changes that the medical establishment underwent in the late 18th and early 19th centuries. Like his preceding work, "Naissance de la Clinique" was largely critically ignored, but later gained a cult following. Foucault was also selected to be among the "Eighteen Man Commission" that assembled between November 1963 and March 1964 to discuss university reforms that were to be implemented by Christian Fouchet, the Gaullist Minister of National Education; implemented in 1967, they brought staff strikes and student protests.
In April 1966, Gallimard published Foucault's "Les Mots et les choses" ("The words and the things"), later translated as "The Order of Things: An Archaeology of the Human Sciences". Exploring how man came to be an object of knowledge, it argued that all periods of history have possessed certain underlying conditions of truth that constituted what was acceptable as scientific discourse. Foucault argues that these conditions of discourse have changed over time, from one period's episteme to another. Although designed for a specialist audience, the work gained media attention, becoming a surprise bestseller in France. Appearing at the height of interest in structuralism, Foucault was quickly grouped with scholars Jacques Lacan, Claude Lévi-Strauss, and Roland Barthes, as the latest wave of thinkers set to topple the existentialism popularized by Jean-Paul Sartre. Although initially accepting this description, Foucault soon vehemently rejected it. Foucault and Sartre regularly criticised one another in the press; both Sartre and Simone de Beauvoir attacked Foucault's ideas as "bourgeois", while Foucault retaliated against their Marxist beliefs by proclaiming that "Marxism exists in nineteenth-century thought as a fish exists in water; that is, it ceases to breathe anywhere else."
University of Tunis and Vincennes: 1966–1970.
"I lived [in Tunisia] for two and a half years. It made a real impression. I was present for large, violent student riots that preceded by several weeks what happened in May in France. This was March 1968. The unrest lasted a whole year: strikes, courses suspended, arrests. And in March, a general strike by the students. The police came into the university, beat up the students, wounded several of them seriously, and started making arrests ... I have to say that I was tremendously impressed by those young men and women who took terrible risks by writing or distributing tracts or calling for strikes, the ones who really risked losing their freedom! It was a political experience for me."
Michel Foucault, 1983
In September 1966, Foucault took a position teaching psychology at the University of Tunis in Tunisia. His decision to do so was largely because his lover, Defert, had been posted to the country as part of his national service. Foucault moved a few kilometres from Tunis, to the village of Sidi Bou Saïd, where fellow academic Gérard Deledalle lived with his wife. Soon after his arrival, Foucault announced that Tunisia was "blessed by history", a nation which "deserves to live forever because it was where Hannibal and St. Augustine lived." His lectures at the university proved very popular, and were well attended. Although many young students were enthusiastic about his teaching, they were critical of what they believed to be his right-wing political views, viewing him as a "representative of Gaullist technocracy", even though he considered himself a leftist.
Foucault was in Tunis during the anti-government and pro-Palestinian riots that rocked the city in June 1967, and which continued for a year. Although highly critical of the violent, ultra-nationalistic and anti-semitic nature of many protesters, he used his status to try to prevent some of his militant leftist students from being arrested and tortured for their role in the agitation. He hid their printing press in his garden, and tried to testify on their behalf at their trials, but was prevented when the trials became closed-door events. While in Tunis, Foucault continued to write. Inspired by a correspondence with the surrealist artist René Magritte, Foucault started to write a book about the impressionist artist Eduard Manet, but never completed it.
In 1968, Foucault returned to Paris, moving into an apartment on the Rue de Vaugirard. After the May 1968 student protests, Minister of Education Edgar Faure responded by founding new universities with greater autonomy. Most prominent of these was the Centre Expérimental de Vincennes in Vincennes on the outskirts of Paris. A group of prominent academics were asked to select teachers to run the Centre's departments, and Canguilheim recommended Foucault as head of the Philosophy Department. Becoming a tenured professor of Vincennes, Foucault's desire was to obtain "the best in French philosophy today" for his department, employing Michel Serres, Judith Miller, Alain Badiou, Jacques Rancière, François Regnault, Henri Weber, Etienne Balibar, and François Châtelet; most of them were Marxists or ultra-left activists.
Lectures began at the university in January 1969, and straight away its students and staff, including Foucault, were involved in occupations and clashes with police, resulting in arrests. In February, Foucault gave a speech denouncing police provocation to protesters at the Latin Quarter of the Mutualité. Such actions marked Foucault's embrace of the ultra-left, undoubtedly influenced by Defert, who had gained a job at Vincennes' sociology department and who had become a Maoist. Most of the courses at Foucualt's philosophy department were Marxist-Leninist oriented, although Foucault himself gave courses on Nietzsche, "The end of Metaphysics", and "The Discourse of Sexuality"; they were highly popular and over-subscribed. While the right-wing press was heavily critical of this new institution, new Minister of Education Olivier Guichard was angered by its ideological bent and the lack of exams, with students being awarded degrees in a haphazard manner. He refused national accreditation of the department's degrees, resulting in a public rebuttal from Foucault.
Later life.
Collège de France and "Discipline and Punish": 1970–1975.
Foucault desired to leave Vincennes and become a fellow of the prestigious Collège de France. He requested to join, taking up a chair in what he called the "history of systems of thought," and his request was championed by members Dumézil, Hyppolite, and Vuillemin. In November 1969, when an opening became available, Foucault was elected to the Collège, though with opposition by a large minority. He gave his inaugural lecture in December 1970, which was subsequently published as "L'Ordre du discours" ("The Discourse of Language"). He was obliged to give 12 weekly lectures a year—and did so for the rest of his life—covering the topics that he was researching at the time; these became "one of the events of Parisian intellectual life" and were repeatedly packed out events. On Mondays, he also gave seminars to a group of students; many of them became a "Foulcauldian tribe" who worked with him on his research. He enjoyed this teamwork and collective research, and together they would publish a number of short books. Working at the Collège allowed him to travel widely, giving lectures in Brazil, Japan, Canada, and the United States over the next 14 years.
In May 1971, Foucault co-founded the Group d'Information sur les Prisons (GIP) along with historian Pierre Vidal-Naquet and journalist Jean-Marie Domenach. The GIP aimed to investigate and expose poor conditions in prisons and give prisoners and ex-prisoners a voice in French society. It was highly critical of the penal system, believing that it converted petty criminals into hardened delinquents. The GIP gave press conferences and staged protests surrounding the events of the Toul prison riot in December 1971, alongside other prison riots that it sparked off; in doing so it faced police crack down and repeated arrest. The group became active across France, with 2,000 to 3,000, members, but disbanded before 1974. Also campaigning against the death penalty, Foucault co-authored a short book on the case of the executed murderer Pierre Rivière. After his research into the penal system, Foucault published "Surveiller et punir: Naissance de la prison" ("Discipline and Punish") in 1975, offering a history of the system in western Europe. Biographer Didier Eribon described it as "perhaps the finest" of Foucault's works, and it was well received.
Foucault was also active in anti-racist campaigns; in November 1971, he was a leading figure in protests following the perceived racist killing of Arab migrant Dejellali Ben Ali. In this he worked alongside his old rival Sartre, the journalist Claude Mauriac, and one of his literary heroes, Jean Genet. This campaign was formalised as the Committee for the Defence of the Rights of Immigrants, but there was tension at their meetings as Foucault opposed the anti-Israeli sentiment of many Arab workers and Maoist activists. At a December 1972 protest against the police killing of Algerian worker Mohammad Diab, both Foucault and Genet were arrested, resulting in widespread publicity. Foucault was also involved in founding the Agence de Press-Libération (APL), a group of leftist journalists who intended to cover news stories neglected by the mainstream press. In 1973, they established the daily newspaper "Libération", and Foucault suggested that they establish committees across France to collect news and distribute the paper, and advocated a column known as the "Chronicle of the Workers' Memory" to allow workers' to express their opinions. Foucault wanted an active journalistic role in the paper, but this proved untenable, and he soon became disillusioned with "Libération", believing that it distorted the facts; he would not publish in it until 1980.
"The History of Sexuality" and Iranian Revolution: 1976–1979.
In 1976 Gallimard published Foucault's "Histoire de la sexualité: la volonté de savoir" ("The History of Sexuality: The Will to Knowledge"), a short book exploring what Foucault called the "repressive hypothesis". It revolved largely around the concept of power, rejecting Marxist theories of power and rejecting psychoanalysis. Foucault intended it as the first in a seven-volume exploration of the subject. "Histoire de la sexualité" was a best seller and gained a positive press reception, but lukewarm intellectual interest, something that upset Foucault, who felt that many misunderstood his hypothesis. He soon became dissatisfied with Gallimard after being offended by senior staff member Pierre Nora. Along with Paul Veyne and François Wahl, Foucault launched a new series of academic books, known as "Dex Travaux" ("Some Works"), through the company Seuil, which he hoped would improve the state of academic research in France. He also produced introductions for the memoirs of Herculine Barbin and "My Secret Life".
"There exists an international citizenry that has its rights, and has its duties, and that is committed to rise up against every abuse of power, no matter who the author, no matter who the victims. After all, we are all ruled, and as such, we are in solidarity."
Michel Foucault, 1981
Foucault remained active as a political activist, focusing on protesting government abuses of human rights across the world. He was a key player in the 1975 protests against the Spanish government to execute 11 militants sentenced to death without fair trial. It was his idea to travel to Madrid with 6 others to give their press conference there; they were subsequently arrested and deported back to Paris. In 1977, he protested the extradition of Klaus Croissant to West Germany, and his rib was fractured during clashes with riot police. In July that year, he organised an assembly of Eastern Bloc dissidents to mark the visit of Soviet Premier Leonid Brezhnev to Paris. In 1979, he campaigned for Vietnamese political dissidents to be granted asylum in France.
In 1977, Italian newspaper "Corriere della sera" asked Foucault to write a column for them. In doing so, in 1978 he travelled to Tehran in Iran, days after the Black Friday massacre. Documenting the developing Iranian Revolution, he met with opposition leaders such as Mohammad Kazem Shariatmadari and Mehdi Bazargan, and discovered the popular support for Islamism. Returning to France, he was one of the journalists who visited the Ayatollah Khomeini, before he visited Tehran again. His articles expressed awe of Khomeini's Islamist movement, for which he was widely criticised in the French press, including by Iranian liberal dissidents. Foucault's response was that Islamism was to become a major political force in the region, and that the West must treat it with respect rather than hostility. In April 1978, Foucault traveled to Japan, where he studied Zen Buddhism under Omori Sogen at the Seionji temple in Uenohara.
Final years: 1980–1984.
Although remaining critical of power relations, Foucault expressed cautious support for the Socialist Party government of François Mitterrand following its electoral victory in 1981. But his support soon deteriorated when that party refused to condemn the Polish government's crackdown on the 1982 demonstrations in Poland orchestrated by the Solidarity trade union. He and sociologist Pierre Bourdieu authored a document condemning Mitterrand's inaction that was published in "Libération", and they also took part in large public protests on the issue. Foucault continued to support Solidarity, and with his friend Simone Signoret traveled Poland as part of a Médecins du Monde expedition, taking time out to visit the Auschwitz concentration camp. He continued his academic research, and in June 1984 Gallimard published the second and third volumes of "Histoire de la sexualité". Volume two, "L'Usage des plaisirs", dealt with the "techniques of self" prescribed by ancient Greek pagan morality in relation to sexual ethics, while volume three, "Le Souci de soi" explored the same theme in the Greek and Latin texts of the first two centuries CE. A fourth volume, "Les Aveux de la chair", examined it in early Christianity, but it remained unfinished at Foucault's death.
In October 1980, Foucault became a visiting professor at the University of California, Berkeley, giving the Howison Lectures on "Truth and Subjectivity", while in November he lectured at the Humanities Institute at the New York University. His growing popularity in American intellectual circles was noted by "Time" magazine, while Foucault went on to lecture at UCLA in 1981, the University of Vermont in 1982, and Berkeley again in 1983, where his lectures drew huge crowds. When in California, Foucault spent many evenings in the gay scene of the San Francisco Bay Area, frequenting sado-masochistic bathhouses, engaging in sexual intercourse with other patrons. He would praise sado-masochistic activity in interviews with the gay press, describing it as "the real creation of new possibilities of pleasure, which people had no idea about previously." Through this sexual activity, Foucault contracted HIV, which eventually developed into AIDS. Little was known of the virus at the time; the first cases had only been identified in 1980. In summer 1983, he developed a persistent dry cough, which concerned friends in Paris, but Foucault insisted it was just a pulmonary infection. Only when hospitalized was Foucault correctly diagnosed; treated with antibiotics, he delivered a final set of lectures at the Collège de France. Foucault entered Paris' Hôpital de la Salpêtrière – the same institution that he had studied in "Madness and Civilisation" – on 9 June 1984, with neurological symptoms complicated by septicemia. He died in the hospital on 25 June.
On 26 June, "Libération" announced his death, mentioning the rumour that it had been brought on by AIDS. The following day, "Le Monde" issued a medical bulletin cleared by his family which made no reference to HIV/AIDS. On 29 June, Foucault's "la levée du corps" ceremony was held, in which the coffin was carried from the hospital morgue. Hundreds attended, including activist and academic friends, while Gilles Deleuze gave a speech using text from "The History of Sexuality". His body was then buried at Vendeuvre in a small ceremony. Soon after his death, Foucault's partner Daniel Defert founded the first national HIV/AIDS organisation in France, AIDES; a pun on the French language word for "help" ("aide") and the English language acronym for the disease. On the second anniversary of Foucault's death, Defert publicly revealed that Foucault's death was AIDS-related in California-based gay magazine, "The Advocate".
Personal life.
Foucault's first biographer, Didier Eribon, described the philosopher as "a complex, many-sided character", and that "under one mask there is always another". He also noted that he exhibited an "enormous capacity for work". At the ENS, Foucault's classmates unanimously summed him up as a figure who was both "disconcerting and strange" and "a passionate worker". His personality would change as he aged however; Eribon noted that while he was a "tortured adolescent", post-1960, he had become "a radiant man, relaxed and cheerful", even being described by those who worked with him as a dandy. He noted that in 1969, Foucault embodied the idea of "the militant intellectual".
Foucault was a fan of classical music, particularly enjoying the work of Johann Sebastian Bach and Wolfgang Amadeus Mozart. Foucault became known for wearing turtleneck jumpers. After his death, Foucault's friend Georges Dumézil described him as having possessed "a profound kindness and goodness", also exhibiting an "intelligence [that] literally knew no bounds."
Politically, Foucault remained a leftist throughout his life, but his particular stance within the left often changed. In the early 1950s he had been a member of the French Communist Party, although never adopted an orthodox Marxist viewpoint and left the party after three years, disgusted by the prejudice against Jews and homosexuals within its ranks. After spending some time working in Poland, then governed as a socialist state by the Polish United Workers' Party, he became further disillusioned with communist ideology. As a result, in the early 1960s he was considered to be "violently anticommunist" by some of his detractors, even though totally involved in Marxist campaigns along with most of his students and colleagues.
Thought.
Foucault's colleague Pierre Bourdieu summarised the philosopher's thought as "a long exploration of transgression, of going beyond social limits, always inseparably linked to knowledge and power."
"The theme that underlies all Foucault's work is the relationship between power and knowledge, and how the former is used to control and define the latter. What authorities claim as 'scientific knowledge' are really just means of social control. Foucault shows how, for instance, in the eighteenth century 'madness' was used to categorise and stigmatise not just the mentally ill but the poor, the sick, the homeless and, indeed, anyone whose expressions of individuality were unwelcome."
Philip Stokes, "Philosophy: 100 Essential Thinkers" (2004)
Philosopher Philip Stokes of the University of Reading noted that overall, Foucault's work was "dark and pessimistic", but that it did leave some room for optimism, in that it illustrates how the discipline of philosophy can be used to highlight areas of domination. In doing so, Stokes claimed, we are able to understand how we are being dominated and strive to build social structures that minimise this risk of domination. In all of this development there had to be close attention to detail; it is the detail which eventually individualises people.
Later in his life, Foucault explained that his work was less about analysing power as a phenomenon than about trying to characterise the different ways in which contemporary society has expressed the use of power to "objectivise subjects." These have taken three broad forms: one involving scientific authority to classify and 'order' knowledge about human populations. A second, and related form, has been to categorise and 'normalise' human subjects (by identifying madness, illness, physical features, and so on). The third relates to the manner in which the impulse to fashion sexual identities and train one's own body to engage in routines and practices ends up reproducing certain patterns within a given society.
Literature.
In addition to his philosophical work, Foucault also wrote on literature. "Death and the Labyrinth: The World of Raymond Roussel" was published in 1963, and translated into English in 1986. It is Foucault's only book-length work on literature. Foucault described it as "by far the book I wrote most easily, with the greatest pleasure, and most rapidly." Foucault explores theory, criticism, and psychology with reference to the texts of Raymond Roussel, one of the first notable experimental writers.
Influence.
Foucault's discussions on power and discourse have inspired many critical theorists, who believe that Foucault's analysis of power structures could aid the struggle against inequality. They claim that through discourse analysis, hierarchies may be uncovered and questioned by way of analyzing the corresponding fields of knowledge through which they are legitimated. This is one of the ways that Foucault's work is linked to critical theory.
In 2007, Foucault was listed as the most cited scholar in the humanities by the "ISI Web of Science" among a large quantity of French philosophers, the compilation's author commenting that "What this says of modern scholarship is for the reader to decide – and it is imagined that judgments will vary from admiration to despair, depending on one’s view".
Critiques and engagements.
Crypto-normativity.
A prominent critique of Foucault's thought concerns his refusal to propose positive solutions to the social and political issues that he critiques. Since no human relation is devoid of power, freedom becomes elusive - even as an ideal. This stance which critiques normativity as socially constructed and contingent, but which relies on an implicit norm in order to mount the critique led philosopher Jürgen Habermas to describe Foucault's thinking as "crypto-normativist", covertly reliant on the very Enlightenment principles he attempts to deconstruct. A similar critique has been advanced by Diana Taylor, and by Nancy Fraser who argues that "Foucault's critique encompasses traditional moral systems, he denies himself recourse to concepts such as "freedom" and "justice", and therefore lacks the ability to generate positive alternatives ".
Genealogy as historical method.
Philosopher Richard Rorty has argued that Foucault's 'archaeology of knowledge' is fundamentally negative, and thus fails to adequately establish any 'new' theory of knowledge "per se". Rather, Foucault simply provides a few valuable maxims regarding the reading of history. Says Rorty:
 As far as I can see, all he has to offer are brilliant redescriptions of the past, supplemented by helpful hints on how to avoid being trapped by old historiographical assumptions. These hints consist largely of saying: "do not look for progress or meaning in history; do not see the history of a given activity, of any segment of culture, as the development of rationality or of freedom; do not use any philosophical vocabulary to characterize the essence of such activity or the goal it serves; do not assume that the way this activity is presently conducted gives any clue to the goals it served in the past."
Foucault has frequently been criticized by historians for what they consider to be a lack of rigor in his analyses. For example, Hans-Ulrich Wehler harshly criticized Foucault in 1998. Wehler regards Foucault as a bad philosopher who wrongfully received a good response by the humanities and by social sciences. According to Wehler, Foucault's works are not only insufficient in their empiric historical aspects, but also often contradictory and lacking in clarity. For example, Foucault's concept of power is "desperatingly undifferentiated", and Foucault's thesis of a "disciplinary society" is, according to Wehler, only possible because Foucault does not properly differentiate between authority, force, power, violence and legitimacy. In addition, his thesis is based on a one-sided choice of sources (prisons and psychiatric institutions) and neglects other types of organizations as e.g. factories. Also, Wehler criticizes Foucault's "francocentrism" because he did not take into consideration major German-speaking theorists of social sciences like Max Weber and Norbert Elias. In all, Wehler concludes that Foucault is "because of the endless series of flaws in his so-called empirical studies ... an intellectually dishonest, empirically absolutely unreliable, crypto-normativist seducer of Postmodernism".
Feminist critiques.
Though American feminists have built on Foucault's critiques of the historical construction of gender roles and sexuality, some feminists have accused him of androcentrism, adopting exclusively male perspectives on subjectivity and ethics.
Queer theory.
Foucault's approach to sexuality, in which he understands sexualities as socially constructed concepts that are ascribed onto bodies has become widely influential for example through the work of Judith Butler and Eve Sedgwick. Nonetheless Foucault's resistance to identity politics and the rejection of sexual object choice as fixed foundation for sexual behavior, stands at odds with some formulations of queer or gay identity.
Social Constructionism and Human nature.
Foucault is sometimes criticized for his prominent formulation of principles of social constructionism, which some see as an affront to the concept of truth. In Foucault's 1971 televised debate with Noam Chomsky, Foucault argued against the possibility of any fixed human nature, as posited by Chomsky's concept of innate human faculties. Chomsky argued that concepts of justice were rooted in human reason, whereas Foucault rejected the universal basis for a concept of justice. Following the debate, Chomsky was stricken with Foucault's total rejection of the possibility of a universal morality, stating "He struck me as completely amoral, I’d never met anyone who was so totally amoral" ... "I mean, I liked him personally, it's just that I couldn't make sense of him. It's as if he was from a different species, or something."
External links.
General sites (updated regularly):
Biographies:
Bibliographies:
Journals:

</doc>
<doc id="47645" url="http://en.wikipedia.org/wiki?curid=47645" title="Value">
Value

Value or values may refer to:

</doc>
<doc id="47646" url="http://en.wikipedia.org/wiki?curid=47646" title="Hippie">
Hippie

A hippie (or hippy) is a member of a subculture that was originally a youth movement that emerged in the United States during the mid-1960s and spread to other countries around the world. The word 'hippie' came from "hipster", and was initially used to describe beatniks who had moved into New York City's Greenwich Village and San Francisco's Haight-Ashbury district. The origins of the terms "hip" and "hep" are uncertain, although by the 1940s both had become part of African American jive slang and meant "sophisticated; currently fashionable; fully up-to-date". The Beats adopted the term "hip", and early hippies inherited the language and countercultural values of the Beat Generation. Hippies created their own communities, listened to psychedelic music, embraced the sexual revolution, and used drugs such as cannabis, LSD, peyote and psilocybin mushrooms to explore altered states of consciousness.
In January 1967, the Human Be-In in Golden Gate Park in San Francisco popularized hippie culture, leading to the Summer of Love on the West Coast of the United States, and the 1969 Woodstock Festival on the East Coast. Hippies in Mexico, known as "jipitecas", formed "La Onda" and gathered at Avándaro, while in New Zealand, nomadic housetruckers practiced alternative lifestyles and promoted sustainable energy at Nambassa. In the United Kingdom in 1970 many gathered at the gigantic Isle of Wight Festival with a crowd of around 400,000 people. In later years, mobile "peace convoys" of New age travellers made summer pilgrimages to free music festivals at Stonehenge and elsewhere. In Australia, hippies gathered at Nimbin for the 1973 Aquarius Festival and the annual Cannabis Law Reform Rally or MardiGrass. ""Piedra Roja" Festival", a major hippie event in Chile, was held in 1970.
Hippie fashion and values had a major effect on culture, influencing popular music, television, film, literature, and the arts. Since the 1960s, many aspects of hippie culture have been assimilated by mainstream society. The religious and cultural diversity espoused by the hippies has gained widespread acceptance, and Eastern philosophy and spiritual concepts have reached a larger audience. The hippie legacy can be observed in contemporary culture in myriad forms, including health food, music festivals, contemporary sexual mores, and even the cyberspace revolution.
Etymology.
Lexicographer Jesse Sheidlower, the principal American editor of the "Oxford English Dictionary", argues that the terms "hipster" and "hippie" derive from the word "hip", whose origins are unknown. The word "hip" in the sense of "aware, in the know" is first attested in a 1902 cartoon by Tad Dorgan, and first appeared in print in a 1904 novel by George Vere Hobart, "Jim Hickey, A Story of the One-Night Stands", where a black American character uses the slang phrase "Are you hip?"
The term "hipster" was coined by Harry Gibson in 1944. By the 1940s, the terms "hip", "hep" and "hepcat" were popular in Harlem jazz slang, although "hep" eventually came to denote an inferior status to "hip". 
In Greenwich Village in the early 1960s, New York City, young counterculture advocates were named "hips" because they were considered "in the know" or "cool", as opposed to being "square". In a 1961 essay, Kenneth Rexroth used both the terms "hipster" and "hippies" to refer to young people participating in black American or Beatnik nightlife. According to Malcolm X's 1964 autobiography, the word "hippie" in 1940s Harlem had been used to describe a specific type of white man who "acted more Negro than Negroes". Andrew Loog Oldham refers to "all the Chicago hippies," seemingly in reference to black blues/R&B musicians, in his rear sleeve notes to the 1965 LP "The Rolling Stones, Now!"
Although the word "hippies" made other isolated appearances in print during the early 1960s, the first use of the term on the West Coast appeared on September 5, 1965, in the article, "A New Haven for Beatniks", by San Francisco journalist Michael Fallon. In that article, Fallon wrote about the Blue Unicorn coffeehouse, using the term "hippie" to refer to the new generation of beatniks who had moved from North Beach into the Haight-Ashbury district. "New York Times" editor and usage writer Theodore M. Bernstein said the paper changed the spelling from "hippy" to "hippie" to avoid the ambiguous description of clothing as "hippy fashions".
History.
Origins.
A July 1968 "Time Magazine" study on hippie philosophy credited the foundation of the hippie movement with historical precedent as far back as the counterculture of the Ancient Greeks, espoused by philosophers like Diogenes of Sinope and the Cynics also as early forms of hippie culture. It also named as notable influences the religious and spiritual teachings of Henry David Thoreau, Hillel the Elder, Jesus, Buddha, St. Francis of Assisi, Gandhi, and J.R.R. Tolkien.
The first signs of modern "proto-hippies" emerged in fin de siècle Europe. Between 1896 and 1908, a German youth movement arose as a countercultural reaction to the organized social and cultural clubs that centered around German folk music. Known as "Der Wandervogel" ("migratory bird"), the hippie movement opposed the formality of traditional German clubs, instead emphasizing amateur music and singing, creative dress, and communal outings involving hiking and camping. Inspired by the works of Friedrich Nietzsche, Goethe, Hermann Hesse, and Eduard Baltzer, Wandervogel attracted thousands of young Germans who rejected the rapid trend toward urbanization and yearned for the pagan, back-to-nature spiritual life of their ancestors. During the first several decades of the 20th century, Germans settled around the United States, bringing the values of the Wandervogel with them. Some opened the first health food stores, and many moved to southern California where they could practice an alternative lifestyle in a warm climate. Over time, young Americans adopted the beliefs and practices of the new immigrants. One group, called the "Nature Boys", took to the California desert and raised organic food, espousing a back-to-nature lifestyle like the Wandervogel. Songwriter eden ahbez wrote a hit song called "Nature Boy" inspired by Robert Bootzin (Gypsy Boots), who helped popularize health-consciousness, yoga, and organic food in the United States.
Like Wandervogel, the hippie movement in the United States began as a youth movement. Composed mostly of white teenagers and young adults between 15 and 25 years old, hippies inherited a tradition of cultural dissent from bohemians and beatniks of the Beat Generation in the late 1950s. Beats like Allen Ginsberg crossed over from the beat movement and became fixtures of the burgeoning hippie and anti-war movements. By 1965, hippies had become an established social group in the U.S., and the movement eventually expanded to other countries, extending as far as the United Kingdom and Europe, Australia, Canada, New Zealand, Japan, Mexico, and Brazil. The hippie ethos influenced The Beatles and others in the United Kingdom and other parts of Europe, and they in turn influenced their American counterparts. Hippie culture spread worldwide through a fusion of rock music, folk, blues, and psychedelic rock; it also found expression in literature, the dramatic arts, fashion, and the visual arts, including film, posters advertising rock concerts, and album covers. Self-described hippies had become a significant minority by 1968, representing just under 0.2% of the U.S. population before declining in the mid-1970s.
Along with the New Left and the American Civil Rights Movement, the hippie movement was one of three dissenting groups of the 1960s counterculture. Hippies rejected established institutions, criticized middle class values, opposed nuclear weapons and the Vietnam War, embraced aspects of Eastern philosophy, championed sexual liberation, were often vegetarian and eco-friendly, promoted the use of psychedelic drugs which they believed expanded one's consciousness, and created intentional communities or communes. They used alternative arts, street theatre, folk music, and psychedelic rock as a part of their lifestyle and as a way of expressing their feelings, their protests and their vision of the world and life. Hippies opposed political and social orthodoxy, choosing a gentle and nondoctrinaire ideology that favored peace, love and personal freedom, expressed for example in The Beatles' song "All You Need is Love". Hippies perceived the dominant culture as a corrupt, monolithic entity that exercised undue power over their lives, calling this culture "The Establishment", "Big Brother", or "The Man". Noting that they were "seekers of meaning and value", scholars like Timothy Miller have described hippies as a new religious movement.
Early hippies (1960–1966).
During the early 1960s, novelist Ken Kesey and the Merry Pranksters lived communally in California. Members included Beat Generation hero Neal Cassady, Ken Babbs, Carolyn Adams (aka Mountain Girl/Carolyn Garcia), Stewart Brand, Del Close, Paul Foster, George Walker, Sandy Lehmann-Haupt and others. Their early escapades were documented in Tom Wolfe's book "The Electric Kool-Aid Acid Test". With Cassady at the wheel of a school bus named Further, the Merry Pranksters traveled across the United States to celebrate the publication of Kesey's novel "Sometimes a Great Notion" and to visit the 1964 World's Fair in New York City. The Merry Pranksters were known for using cannabis, amphetamine, and LSD, and during their journey they "turned on" many people to these drugs. The Merry Pranksters filmed and audio taped their bus trips, creating an immersive multimedia experience that would later be presented to the public in the form of festivals and concerts. The Grateful Dead wrote a song about the Merry Pranksters' bus trips called "That's It for the Other One".
During this period Greenwich Village in New York City and Berkeley, California anchored the American folk music circuit. Berkeley's two coffee houses, the Cabale Creamery and the Jabberwock, sponsored performances by folk music artists in a beat setting. In April 1963, Chandler A. Laughlin III, co-founder of the Cabale Creamery, established a kind of tribal, family identity among approximately fifty people who attended a traditional, all-night Native American peyote ceremony in a rural setting. This ceremony combined a psychedelic experience with traditional Native American spiritual values; these people went on to sponsor a unique genre of musical expression and performance at the Red Dog Saloon in the isolated, old-time mining town of Virginia City, Nevada.
During the summer of 1965, Laughlin recruited much of the original talent that led to a unique amalgam of traditional folk music and the developing psychedelic rock scene. He and his cohorts created what became known as "The Red Dog Experience", featuring previously unknown musical acts — Grateful Dead, Jefferson Airplane, Big Brother and the Holding Company, Quicksilver Messenger Service, The Charlatans, and others — who played in the completely refurbished, intimate setting of Virginia City's Red Dog Saloon. There was no clear delineation between "performers" and "audience" in "The Red Dog Experience", during which music, psychedelic experimentation, a unique sense of personal style and Bill Ham's first primitive light shows combined to create a new sense of community. Laughlin and George Hunter of the Charlatans were true "proto-hippies", with their long hair, boots and outrageous clothing of 19th-century American (and Native American) heritage. LSD manufacturer Owsley Stanley lived in Berkeley during 1965 and provided much of the LSD that became a seminal part of the "Red Dog Experience", the early evolution of psychedelic rock and budding hippie culture. At the Red Dog Saloon, The Charlatans were the first psychedelic rock band to play live (albeit unintentionally) loaded on LSD.
When they returned to San Francisco, Red Dog participants Luria Castell, Ellen Harman and Alton Kelley created a collective called "The Family Dog." Modeled on their Red Dog experiences, on October 16, 1965, the Family Dog hosted "A Tribute to Dr. Strange" at Longshoreman's Hall. Attended by approximately 1,000 of the Bay Area's original "hippies", this was San Francisco's first psychedelic rock performance, costumed dance and light show, featuring Jefferson Airplane, The Great Society and The Marbles. Two other events followed before year's end, one at California Hall and one at the Matrix. After the first three Family Dog events, a much larger psychedelic event occurred at San Francisco's Longshoreman's Hall. Called "The Trips Festival", it took place on January 21–January 23, 1966, and was organized by Stewart Brand, Ken Kesey, Owsley Stanley and others. Ten thousand people attended this sold-out event, with a thousand more turned away each night. On Saturday January 22, the Grateful Dead and Big Brother and the Holding Company came on stage, and 6,000 people arrived to imbibe punch spiked with LSD and to witness one of the first fully developed light shows of the era.
By February 1966, the Family Dog became Family Dog Productions under organizer Chet Helms, promoting happenings at the Avalon Ballroom and the Fillmore Auditorium in initial cooperation with Bill Graham. The Avalon Ballroom, the Fillmore Auditorium and other venues provided settings where participants could partake of the full psychedelic music experience. Bill Ham, who had pioneered the original Red Dog light shows, perfected his art of liquid light projection, which combined light shows and film projection and became with the San Francisco ballroom experience. The sense of style and costume that began at the Red Dog Saloon flourished when San Francisco's Fox Theater went out of business and hippies bought up its costume stock, reveling in the freedom to dress up for weekly musical performances at their favorite ballrooms. As "San Francisco Chronicle" music columnist Ralph J. Gleason put it, "They danced all night long, orgiastic, spontaneous and completely free form."
Some of the earliest San Francisco hippies were former students at San Francisco State College who became intrigued by the developing psychedelic hippie music scene. These students joined the bands they loved, living communally in the large, inexpensive Victorian apartments in the Haight-Ashbury. Young Americans around the country began moving to San Francisco, and by June 1966, around 15,000 hippies had moved into the Haight. The Charlatans, Jefferson Airplane, Big Brother and the Holding Company, and the Grateful Dead all moved to San Francisco's Haight-Ashbury neighborhood during this period. Activity centered around the Diggers, a guerrilla street theatre group that combined spontaneous street theatre, anarchistic action, and art happenings in their agenda to create a "free city". By late 1966, the Diggers opened free stores which simply gave away their stock, provided free food, distributed free drugs, gave away money, organized free music concerts, and performed works of political art.
On October 6, 1966, the state of California declared LSD a controlled substance, which made the drug illegal. In response to the criminalization of psychedelics, San Francisco hippies staged a gathering in the Golden Gate Park panhandle, called the Love Pageant Rally, attracting an estimated 700–800 people. As explained by Allan Cohen, co-founder of the "San Francisco Oracle", the purpose of the rally was twofold: to draw attention to the fact that LSD had just been made illegal — and to demonstrate that people who used LSD were not criminals, nor were they mentally ill. The Grateful Dead played, and some sources claim that LSD was consumed at the rally. According to Cohen, those who took LSD "were not guilty of using illegal substances...We were celebrating transcendental consciousness, the beauty of the universe, the beauty of being."
Summer of Love (1967).
On January 14, 1967, the outdoor Human Be-In organized by Michael Bowen helped to popularize hippie culture across the United States, with 20,000 hippies gathering in San Francisco's Golden Gate Park. On March 26, Lou Reed, Edie Sedgwick and 10,000 hippies came together in Manhattan for the Central Park Be-In on Easter Sunday. The Monterey Pop Festival from June 16 to June 18 introduced the rock music of the counterculture to a wide audience and marked the start of the "Summer of Love". Scott McKenzie's rendition of John Phillips' song, "San Francisco", became a hit in the United States and Europe. The lyrics, "If you're going to San Francisco, be sure to wear some flowers in your hair", inspired thousands of young people from all over the world to travel to San Francisco, sometimes wearing flowers in their hair and distributing flowers to passersby, earning them the name, "Flower Children". Bands like the Grateful Dead, Big Brother and the Holding Company (with Janis Joplin), and Jefferson Airplane lived in the Haight.
In June 1967, Herb Caen was approached by "a distinguished magazine" to write about why hippies were attracted to San Francisco. He declined the assignment but interviewed hippies in the Haight for his own newspaper column in the "San Francisco Chronicle". Caen determined that, "Except in their music, they couldn't care less about the approval of the straight world." Caen himself felt that the city of San Francisco was so straight that it provided a visible contrast with hippie culture. On July 7, "Time" magazine featured a cover story entitled, "The Hippies: The Philosophy of a Subculture." The article described the guidelines of the hippie code: "Do your own thing, wherever you have to do it and whenever you want. Drop out. Leave society as you have known it. Leave it utterly. Blow the mind of every straight person you can reach. Turn them on, if not to drugs, then to beauty, love, honesty, fun." It is estimated that around 100,000 people traveled to San Francisco in the summer of 1967. The media was right behind them, casting a spotlight on the Haight-Ashbury district and popularizing the "hippie" label. With this increased attention, hippies found support for their ideals of love and peace but were also criticized for their anti-work, pro-drug, and permissive ethos.
At this point, The Beatles had released their groundbreaking album "Sgt. Pepper's Lonely Hearts Club Band" which was quickly embraced by the hippie movement with its colorful psychedelic sonic imagery.
By the end of the summer, the Haight-Ashbury scene had deteriorated. The incessant media coverage led the Diggers to declare the "death" of the hippie with a parade. According to poet Susan 'Stormi' Chambless, the hippies buried an effigy of a hippie in the Panhandle to demonstrate the end of his/her reign. Haight-Ashbury could not accommodate the influx of crowds (mostly naive youngsters) with no place to live. Many took to living on the street, panhandling and drug-dealing. There were problems with malnourishment, disease, and drug addiction. Crime and violence skyrocketed. None of these trends reflected what the hippies had envisioned. By the end of 1967, many of the hippies and musicians who initiated the Summer of Love had moved on. Beatle George Harrison had once visited Haight-Ashbury and found it to be just a haven for dropouts, inspiring him to give up LSD. Misgivings about the hippie culture, particularly with regard to drug abuse and lenient morality, fueled the moral panics of the late 1960s.
Revolution (1967–1969).
By 1968, hippie-influenced fashions were beginning to take off in the mainstream, especially for youths and younger adults of the populous "Baby Boomer" generation, many of whom may have aspired to emulate the hardcore movements now living in tribalistic communes, but had no overt connections to them. This was noticed not only in terms of clothes and also longer hair for men, but also in music, film, art, and literature, and not just in the US, but around the world. Eugene McCarthy's brief presidential campaign successfully persuaded a significant minority of young adults to "get clean for Gene" by shaving their beards or wearing longer skirts; however the "Clean Genes" had little impact on the popular image in the media spotlight, of the hirsute hippy adorned in beads, feathers, flowers and bells.
 A sign of this was the visibility that the hippie subculture gained in various mainstream and underground media. Hippie exploitation films are 1960s exploitation films about the hippie counterculture with stereotypical situations associated with the movement such as cannabis and LSD use, sex and wild psychedelic parties. Examples include "The Love-ins", "Psych-Out", "The Trip", and "Wild in the Streets". Other more serious and more critically acclaimed films about the hippie counterculture also appeared such as "Easy Rider" and "Alice's Restaurant" (for more information on hippie related films see List of films related to the hippie subculture). Documentaries and television programs have also been produced until today as well as fiction and nonfiction books. Also the popular broadway musical "Hair" was presented in 1967.
People commonly label other cultural movements of that period as Hippie, however it is important to know the difference. For example, Hippies were often not directly engaged in politics, as opposed to their activist counterparts known as “Yippies” (Youth International Party).The Yippies came to national attention during their celebration of the 1968 spring equinox, when some 3,000 of them took over Grand Central Terminal in New York — eventually resulting in 61 arrests. The Yippies, especially their leaders Abbie Hoffman and Jerry Rubin, became notorious for their theatrics, such as trying to levitate the Pentagon at the October 1967 war protest, and such slogans as "Rise up and abandon the creeping meatball!" Their stated intention to protest the 1968 Democratic National Convention in Chicago in August, including nominating their own candidate, "Lyndon Pigasus Pig" (an actual pig), was also widely publicized in the media at this time. In Cambridge, hippies congregated each Sunday for a large "be-in" at Cambridge Park with swarms of drummers and those beginning the Women's Movement. In the US the Hippie movement started to be seen as part of the "New Left" which was associated with anti-war college campus protest movements. The New Left was a term used mainly in the United Kingdom and United States in reference to activists, educators, agitators and others in the 1960s and 1970s who sought to implement a broad range of reforms on issues such as gay rights, abortion, gender roles and drugs in contrast to earlier leftist or Marxist movements that had taken a more vanguardist approach to social justice and focused mostly on labor unionization and questions of social class.
In April 1969, the building of People's Park in Berkeley, California received international attention. The University of California, Berkeley had demolished all the buildings on a 2.8 acre parcel near campus, intending to use the land to build playing fields and a parking lot. After a long delay, during which the site became a dangerous eyesore, thousands of ordinary Berkeley citizens, merchants, students, and hippies took matters into their own hands, planting trees, shrubs, flowers and grass to convert the land into a park. A major confrontation ensued on May 15, 1969, when Governor Ronald Reagan ordered the park destroyed, which led to a two-week occupation of the city of Berkeley by the California National Guard. Flower power came into its own during this occupation as hippies engaged in acts of civil disobedience to plant flowers in empty lots all over Berkeley under the slogan "Let a Thousand Parks Bloom".
In August 1969, the Woodstock Music and Art Fair took place in Bethel, New York, which for many, exemplified the best of hippie counterculture. Over 500,000 people arrived to hear some of the most notable musicians and bands of the era, among them Canned Heat, Richie Havens, Joan Baez, Janis Joplin, The Grateful Dead, Creedence Clearwater Revival, Crosby, Stills, Nash & Young, Carlos Santana, Sly & The Family Stone, The Who, Jefferson Airplane, and Jimi Hendrix. Wavy Gravy's Hog Farm provided security and attended to practical needs, and the hippie ideals of love and human fellowship seemed to have gained real-world expression. Similar rock festivals occurred in other parts of the country, which played a significant role in spreading hippie ideals throughout America.
In December 1969, a rock festival took place in Altamont, California, about 30 miles (45 km) east of San Francisco. Initially billed as "Woodstock West", its official name was The Altamont Free Concert. About 300,000 people gathered to hear The Rolling Stones; Crosby, Stills, Nash and Young; Jefferson Airplane and other bands. The Hells Angels provided security that proved far less benevolent than the security provided at the Woodstock event: 18-year-old Meredith Hunter was stabbed and killed during The Rolling Stones' performance after he brandished a gun and waved it toward the stage.
Aftershocks (1970–present).
By the 1970s, the 1960s zeitgeist that had spawned hippie culture seemed to be on the wane. The events at Altamont Free Concert shocked many Americans, including those who had strongly identified with hippie culture. Another shock came in the form of the Sharon Tate and Leno and Rosemary LaBianca murders committed in August 1969 by Charles Manson and his "family" of followers. Nevertheless, the turbulent political atmosphere that featured the bombing of Cambodia and shootings by National Guardsmen at Jackson State University and Kent State University still brought people together. These shootings inspired the May 1970 song by Quicksilver Messenger Service "What About Me?", where they sang, "You keep adding to my numbers as you shoot my people down", as well as Neil Young's "Ohio", recorded by Crosby, Stills, Nash and Young.
Much of hippie style had been integrated into mainstream American society by the early 1970s. Large rock concerts that originated with the 1967 KFRC Fantasy Fair and Magic Mountain Music Festival and Monterey Pop Festival and the 1968 Isle of Wight Festival became the norm, evolving into stadium rock in the process. The anti-war movement reached its peak at the 1971 May Day Protests as over 12,000 protesters were arrested in Washington DC. President Nixon himself actually ventured out of the White House and chatted with a group of the 'hippie' protesters. The draft was ended soon thereafter, in 1973. During the mid 1970s, with the end of the draft and the Vietnam War, a renewal of patriotic sentiment associated with the approach of the United States Bicentennial and the emergence of punk in London, Manchester, New York and Los Angeles, the mainstream media lost interest in the hippie counterculture. At the same time there was a revival of the Mod subculture, skinheads, teddy boys and the emergence of new youth cultures, like the goths (an arty offshoot of punk) and football casuals. Acid rock gave way to prog rock, heavy metal, disco, and punk rock.
Starting in the late 1960s, hippies began to come under attack by skinheads. Hippies were also vilified and sometimes attacked by punks, revivalist mods, greasers, football casuals, Teddy boys, and members of other youth subcultures of the 1970s and 1980s. The countercultural movement was also under covert assault by J. Edgar Hoover's infamous "Counter Intelligence Program" (COINTELPRO), but in some countries it was other youth groups that were a threat. Hippie ideals had a marked influence on anarcho-punk and some post-punk youth subcultures, especially during the Second Summer of Love.
Hippie communes, where members tried to live the ideals of the hippie movement continued to flourish. On the west coast, Oregon had quite a few. Some faded away. Some are still around.
While many hippies made a long-term commitment to the lifestyle, some people argue that hippies "sold out" during the 1980s and became part of the materialist, consumer culture. Although not as visible as it once was, hippie culture has never died out completely: hippies and neo-hippies can still be found on college campuses, on communes, and at gatherings and festivals. Many embrace the hippie values of peace, love, and community, and hippies may still be found in bohemian enclaves around the world.
Towards the end of the 20th century, a trend of "cyber hippies" emerged, that embraced some of the qualities of the 1960s psychedelic counterculture. The hippie subculture is also linked to the psychedelic trance or psytrance scene, born out of the Goa scene in India.
Ethos and characteristics.
Hippies sought to free themselves from societal restrictions, choose their own way, and find new meaning in life. One expression of hippie independence from societal norms was found in their standard of dress and grooming, which made hippies instantly recognizable to one another, and served as a visual symbol of their respect for individual rights. Through their appearance, hippies declared their willingness to question authority, and distanced themselves from the "straight" and "square" (i.e., conformist) segments of society. Personality traits and values that hippies tend to be associated with are "altruism and mysticism, honesty, joy and nonviolence".
At the same time, many thoughtful hippies distanced themselves from the very idea that the way a person dresses could be a reliable signal of who he was — especially after outright criminals such as Charles Manson began to adopt superficial hippie characteristics, and also after plainclothes policemen started to "dress like hippies" to divide and conquer legitimate members of the counterculture. Frank Zappa, known for lampooning hippie ethos, particularly with songs like "Who Needs the Peace Corps?" (1968), admonished his audience that "we all wear a uniform". The San Francisco clown/hippie Wavy Gravy said in 1987 that he could still see fellow-feeling in the eyes of Market Street businessmen who had dressed conventionally to survive.
Art and fashion.
Leading proponents of the 1960s Psychedelic Art movement were San Francisco poster artists such as: Rick Griffin, Victor Moscoso, Bonnie MacLean, Stanley Mouse & Alton Kelley, and Wes Wilson. Their Psychedelic Rock concert posters were inspired by Art Nouveau, Victoriana, Dada, and Pop Art. The "Fillmore Posters" were among the most notable of the time. Richly saturated colors in glaring contrast, elaborately ornate lettering, strongly symmetrical composition, collage elements, rubber-like distortions, and bizarre iconography are all hallmarks of the San Francisco psychedelic poster art style. The style flourished from roughly the years 1966 to 1972. Their work was immediately influential to album cover art, and indeed all of the aforementioned artists also created album covers. Psychedelic light-shows were a new art-form developed for rock concerts. Using oil and dye in an emulsion that was set between large convex lenses upon overhead projectors, the lightshow artists created bubbling liquid visuals that pulsed in rhythm to the music. This was mixed with slideshows and film loops to create an improvisational motion picture art form, and to give visual representation to the improvisational jams of the rock bands and create a completely "trippy" atmosphere for the audience. The Brotherhood of Light were responsible for many of the light-shows in San Francisco psychedelic rock concerts.
Out of the psychedelic counterculture there also arose a new genre of comic books: underground comix. "Zap Comix" was among the original underground comics, and featured the work of Robert Crumb, S. Clay Wilson, Victor Moscoso, Rick Griffin, and Robert Williams among others. Underground Comix were ribald, intensely satirical, and seemed to pursue weirdness for the sake of weirdness. Gilbert Shelton created perhaps the most enduring of underground cartoon characters, "The Fabulous Furry Freak Brothers", whose drugged-out exploits held a hilarious mirror up to the hippy lifestyle of the 1960s.
As in the beat movement preceding them, and the punk movement that followed soon after, hippie symbols and iconography were purposely borrowed from either "low" or "primitive" cultures, with hippie fashion reflecting a disorderly, often vagrant style. As with other adolescent, white middle-class movements, deviant behavior of the hippies involved challenging the prevailing gender differences of their time: both men and women in the hippie movement wore jeans and maintained long hair, and both genders wore sandals or went barefoot. Men often wore beards, while women wore little or no makeup, with many going braless. Hippies often chose brightly colored clothing and wore unusual styles, such as bell-bottom pants, vests, tie-dyed garments, dashikis, peasant blouses, and long, full skirts; non-Western inspired clothing with Native American, Asian, Indian, African and Latin American motifs were also popular. Much hippie clothing was self-made in defiance of corporate culture, and hippies often purchased their clothes from flea markets and second-hand shops. Favored accessories for both men and women included Native American jewelry, head scarves, headbands and long beaded necklaces. Hippie homes, vehicles and other possessions were often decorated with psychedelic art. The bold colors, hand-made clothing and loose fitting clothes opposed the tight and uniform clothing of the 1940s and 1950s. It also rejected consumerism in that the hand-production of clothing called for self-efficiency and individuality.
Love and sex.
 The common stereotype on the issues of love and sex had it that the hippies were "promiscuous, having wild sex orgies, seducing innocent teenagers and every manner of sexual perversion." The hippie movement appeared concurrently in the midst of a rising Sexual Revolution, in which many views of the "status quo" on this subject were being challenged.
The clinical study "Human Sexual Response" was published by Masters and Johnson in 1966, and the topic suddenly became more commonplace in America. The 1969 book "Everything You Always Wanted to Know About Sex (But Were Afraid to Ask)" by Dr. David Reuben was a more popular attempt at answering the public's curiosity regarding such matters. Then in 1972 appeared "The Joy of Sex" by Alex Comfort, reflecting an even more candid perception of love-making. By this time, the recreational or 'fun' aspects of sexual behavior were being discussed more openly than ever before, and this more 'enlightened' outlook resulted not just from the publication of such new books as these, but from a more pervasive Sexual Revolution that had already been well underway for some time.
The hippies inherited various countercultural views and practices regarding sex and love from the Beat Generation; "their writings influenced the hippies to open up when it came to sex, and to experiment without guilt or jealousy." One popular hippie slogan that appeared was "If it feels good, do it!" which for many "meant you were free to love whomever you pleased, whenever you pleased, however you pleased. This encouraged spontaneous sexual activity and experimentation. Group sex, public sex... homosexuality, all the taboos went out the window. This doesn't mean that straight sex... or monogamy were unknown, quite the contrary. Nevertheless, the open relationship became an accepted part of the hippy lifestyle. This meant that you might have a primary relationship with one person, but if another attracted you, you could explore that relationship without rancor or jealousy."
Hippies embraced the old slogan of free love of the radical social reformers of other eras; it was accordingly observed that "Free love made the whole love, marriage, sex, baby package obsolete. Love was no longer limited to one person, you could love anyone you chose. In fact love was something you shared with everyone, not just your sex partners. Love exists to be shared freely. We also discovered the more you share, the more you get! So why reserve your love for a select few? This profound truth was one of the great hippie revelations." Sexual experimentation alongside psychedelics also occurred, due to the perception of their being uninhibitors. Others explored the spiritual aspects of sex.
Travel.
Hippies tended to travel light, and could pick up and go wherever the action was at any time. Whether at a "love-in" on Mount Tamalpais near San Francisco, a demonstration against the Vietnam War in Berkeley, or one of Ken Kesey's "Acid Tests", if the "vibe" wasn't right and a change of scene was desired, hippies were mobile at a moment's notice. Planning was eschewed, as hippies were happy to put a few clothes in a backpack, stick out their thumbs and hitchhike anywhere. Hippies seldom worried whether they had money, hotel reservations or any of the other standard accoutrements of travel. Hippie households welcomed overnight guests on an "impromptu" basis, and the reciprocal nature of the lifestyle permitted greater freedom of movement. People generally cooperated to meet each other's needs in ways that became less common after the early 1970s. This way of life is still seen among Rainbow Family groups, new age travellers and New Zealand's housetruckers.
A derivative of this free-flow style of travel were the hippie trucks and buses, hand-crafted mobile houses built on a truck or bus chassis to facilitate a nomadic lifestyle, as documented in the 1974 book "Roll Your Own". Some of these mobile gypsy houses were quite elaborate, with beds, toilets, showers and cooking facilities.
On the West Coast, a unique lifestyle developed around the Renaissance Faires that Phyllis and Ron Patterson first organized in 1963. During the summer and fall months, entire families traveled together in their trucks and buses, parked at Renaissance Pleasure Faire sites in Southern and Northern California, worked their crafts during the week, and donned Elizabethan costume for weekend performances, and to attend booths where handmade goods were sold to the public. The sheer number of young people living at the time made for unprecedented travel opportunities to special happenings. The peak experience of this type was the Woodstock Festival near Bethel, New York, from August 15 to 18, 1969, which drew between 400,000 to 500,000 people.
Hippie trail.
One travel experience, undertaken by hundreds of thousands of hippies between 1969 and 1971, was the Hippie trail overland route to India. Carrying little or no luggage, and with small amounts of cash, almost all followed the same route, hitch-hiking across Europe to Athens and on to Istanbul, then by train through central Turkey via Erzurum, continuing by bus into Iran, via Tabriz and Tehran to Mashhad, across the Afghan border into Herat, through southern Afghanistan via Kandahar to Kabul, over the Khyber Pass into Pakistan, via Rawalpindi and Lahore to the Indian frontier. Once in India, hippies went to many different destinations, but gathered in large numbers on the beaches of Goa and Kovalam in Trivandrum (Kerala), or crossed the border into Nepal to spend months in Kathmandu. In Kathmandu, most of the hippies hung out in the tranquil surroundings of a place called Freak Street, (Nepal Bhasa: Jhoo Chhen) which still exists near Kathmandu Durbar Square.
Spirituality and religion.
Many hippies rejected mainstream organized religion in favor of a more personal spiritual experience, often drawing on indigenous and folk beliefs. If they adhered to mainstream faiths, hippies were likely to embrace Buddhism, Unitarian Universalism, Hinduism and the restorationist Christianity of the Jesus Movement. Some hippies embraced neo-paganism, especially Wicca.
In his 1991 book, "Hippies and American Values", Timothy Miller described the hippie ethos as essentially a "religious movement" whose goal was to transcend the limitations of mainstream religious institutions. "Like many dissenting religions, the hippies were enormously hostile to the religious institutions of the dominant culture, and they tried to find new and adequate ways to do the tasks the dominant religions failed to perform." In his seminal, contemporaneous work, "The Hippie Trip", author Lewis Yablonsky notes that those who were most respected in hippie settings were the spiritual leaders, the so-called "high priests" who emerged during that era.
One such hippie "high priest" was San Francisco State University Professor Stephen Gaskin. Beginning in 1966, Gaskin's "Monday Night Class" eventually outgrew the lecture hall, and attracted 1,500 hippie followers in an open discussion of spiritual values, drawing from Christian, Buddhist, and Hindu teachings. In 1970 Gaskin founded a Tennessee community called The Farm, and he still lists his religion as "Hippie."
Timothy Leary was an American psychologist and writer, known for his advocacy of psychedelic drugs. On September 19, 1966, Leary founded the League for Spiritual Discovery, a religion declaring LSD as its holy sacrament, in part as an unsuccessful attempt to maintain legal status for the use of LSD and other psychedelics for the religion's adherents based on a "freedom of religion" argument. "The Psychedelic Experience" was the inspiration for John Lennon's song "Tomorrow Never Knows" in The Beatles' album "Revolver". He published a pamphlet in 1967 called "Start Your Own Religion" to encourage just that and was invited to attend the January 14, 1967 Human Be-In a gathering of 30,000 hippies in San Francisco's Golden Gate Park In speaking to the group, he coined the famous phrase "Turn on, tune in, drop out". The English magician Aleister Crowley became an influential icon to the new alternative spiritual movements of the decade as well as for rock musicians. The Beatles included him as one of the many figures on the cover sleeve of their 1967 album "Sgt. Pepper's Lonely Hearts Club Band" while Jimmy Page, the guitarist of The Yardbirds and co-founder of 1970s rock band Led Zeppelin was fascinated by Crowley, and owned some of his clothing, manuscripts and ritual objects, and during the 1970s bought Boleskine House, which also appears in the band's movie "The Song Remains the Same". On the back cover of the Doors "13" album, Jim Morrison and the other members of the Doors are shown posing with a bust of Aleister Crowley. Timothy Leary also openly acknowledged Crowley's inspiration.
Politics.
For the historian of the anarchist movement Ronald Creagh, the hippie movement could be considered as the last spectacular resurgence of utopian socialism. For Creagh, a characteristic of this is the desire for the transformation of society not through political revolution, or through reformist action pushed forward by the state, but through the creation of a counter-society of a socialist character in the midst of the current system, which will be made up of ideal communities of a more or less libertarian social form.
The peace symbol was developed in the UK as a logo for the Campaign for Nuclear Disarmament, and was embraced by U.S. anti-war protesters during the 1960s.
Hippies were often pacifists, and participated in non-violent political demonstrations, such as civil rights marches, the marches on Washington D.C., and anti–Vietnam War demonstrations, including draft-card burnings and the 1968 Democratic National Convention protests. The degree of political involvement varied widely among hippies, from those who were active in peace demonstrations, to the more anti-authority street theater and demonstrations of the Yippies, the most politically active hippie sub-group. Bobby Seale discussed the differences between Yippies and hippies with Jerry Rubin, who told him that Yippies were the political wing of the hippie movement, as hippies have not "necessarily become political yet". Regarding the political activity of hippies, Rubin said, "They mostly prefer to be stoned, but most of them want peace, and they want an end to this stuff."
In addition to non-violent political demonstrations, hippie opposition to the Vietnam War included organizing political action groups to oppose the war, refusal to serve in the military and conducting "teach-ins" on college campuses that covered Vietnamese history and the larger political context of the war.
Scott McKenzie's 1967 rendition of John Phillips' song "San Francisco (Be Sure to Wear Flowers in Your Hair)", which helped to inspire the hippie Summer of Love, became a homecoming song for all Vietnam veterans arriving in San Francisco from 1967 onward. McKenzie has dedicated every American performance of "San Francisco" to Vietnam veterans, and he sang in 2002 at the 20th anniversary of the dedication of the Vietnam Veterans Memorial. Hippie political expression often took the form of "dropping out" of society to implement the changes they sought.
Politically motivated movements aided by hippies include the back to the land movement of the 1960s, cooperative business enterprises, alternative energy, the free press movement, and organic farming. The San Francisco group known as the Diggers articulated an influential radical criticism of contemporary mass consumer society, and so they opened free stores which simply gave away their stock, provided free food, distributed free drugs, gave away money, organized free music concerts, and performed works of political art. The Diggers took their name from the original English Diggers (1649–50) led by Gerrard Winstanley, and they sought to create a mini-society free of money and capitalism.
Such activism was ideally carried through anti-authoritarian and non-violent means; thus it was observed that "The way of the hippie is antithetical to all repressive hierarchical power structures since they are adverse to the hippie goals of peace, love and freedom... Hippies don't impose their beliefs on others. Instead, hippies seek to change the world through reason and by living what they believe."
The political ideals of hippies influenced other movements, such as anarcho-punk, rave culture, green politics, stoner culture and the New Age movement. Penny Rimbaud of the English anarcho-punk band Crass said in interviews, and in an essay called "The Last Of The Hippies", that Crass was formed in memory of his friend, Wally Hope. Crass had its roots in Dial House, which was established in 1967 as a commune. Some punks were often critical of Crass for their involvement in the hippie movement. Like Crass, Jello Biafra was influenced by the hippie movement, and cited the yippies as a key influence on his political activism and thinking, though he also wrote songs critical of hippies.
Drugs.
Following in the footsteps of the Beats, many hippies used cannabis (marijuana), considering it pleasurable and benign. They enlarged their spiritual pharmacopeia to include hallucinogens such as peyote, LSD, psilocybin mushrooms and DMT, while often renouncing the use of alcohol. On the East Coast of the United States, Harvard University professors Timothy Leary, Ralph Metzner and Richard Alpert (Ram Dass) advocated psychotropic drugs for psychotherapy, self-exploration, religious and spiritual use. Regarding LSD, Leary said, "Expand your consciousness and find ecstasy and revelation within."
On the West Coast of the United States, Ken Kesey was an important figure in promoting the recreational use of psychotropic drugs, especially LSD, also known as "acid." By holding what he called "Acid Tests", and touring the country with his band of Merry Pranksters, Kesey became a magnet for media attention that drew many young people to the fledgling movement. The Grateful Dead (originally billed as "The Warlocks") played some of their first shows at the Acid Tests, often as high on LSD as their audiences. Kesey and the Pranksters had a "vision of turning on the world." Harder drugs, such as cocaine, amphetamines and heroin, were also sometimes used in hippie settings; however, these drugs were often disdained, even among those who used them, because they were recognized as harmful and addictive.
Legacy.
The legacy of the hippie movement continues to permeate Western society. In general, unmarried couples of all ages feel free to travel and live together without societal disapproval. Frankness regarding sexual matters has become more common, and the rights of homosexual, bisexual and transsexual people, as well as people who choose not to categorize themselves at all, have expanded. Religious and cultural diversity has gained greater acceptance. Co-operative business enterprises and creative community living arrangements are more accepted than before. Some of the little hippie health food stores of the 1960s and 1970s are now large-scale, profitable businesses, due to greater interest in natural foods, herbal remedies, vitamins and other nutritional supplements. Authors Stewart Brand and John Markoff argue that the development and popularization of personal computers and the Internet find one of their primary roots in the anti-authoritarian ethos promoted by hippie culture.
Distinct appearance and clothing was one of the immediate legacies of hippies worldwide. During the 1960s and 1970s, mustaches, beards and long hair became more commonplace and colorful, while multi-ethnic clothing dominated the fashion world. Since that time, a wide range of personal appearance options and clothing styles, including nudity, have become more widely acceptable, all of which was uncommon before the hippie era. Hippies also inspired the decline in popularity of the necktie and other "business" clothing, which had been unavoidable for men during the 1950s and early 1960s. Additionally, hippie fashion itself has been commonplace in the years since the 1960s in clothing and accessories, particularly the peace symbol. Astrology, including everything from serious study to whimsical amusement regarding personal traits, was integral to hippie culture. The generation of the 1970s became influenced by the hippie and the 60s countercultural legacy. As such in New York City musicians and audiences from the female, homosexual, black, and Latino communities adopted several traits from the hippies and psychedelia. They included overwhelming sound, free-form dancing, weird lighting, colorful costumes, and hallucinogens. Psychedelic soul groups like the Chambers Brothers and especially Sly and The Family Stone influenced proto-disco acts such as Isaac Hayes, Willie Hutch and the Philadelphia Sound. In addition, the perceived positivity, lack of irony, and earnestness of the hippies informed proto-disco music like M.F.S.B.'s album "Love Is the Message".
The hippie legacy in literature includes the lasting popularity of books reflecting the hippie experience, such as "The Electric Kool-Aid Acid Test". In music, the folk rock and psychedelic rock popular among hippies evolved into genres such as acid rock, world beat and heavy metal music. Psychedelic trance (also known as psytrance) is a type of electronic music music influenced by 1960s psychedelic rock. The tradition of hippie music festivals began in the United States in 1965 with Ken Kesey's Acid Tests, where the Grateful Dead played tripping on LSD and initiated psychedelic jamming. For the next several decades, many hippies and neo-hippies became part of the Deadhead community, attending music and art festivals held around the country. The Grateful Dead toured continuously, with few interruptions between 1965 and 1995. Phish and their fans (called "Phish Heads") operated in the same manner, with the band touring continuously between 1983 and 2004. Many contemporary bands performing at hippie festivals and their derivatives are called jam bands, since they play songs that contain long instrumentals similar to the original hippie bands of the 1960s.
With the demise of Grateful Dead and Phish, nomadic touring hippies attend a growing series of summer festivals, the largest of which is called the Bonnaroo Music & Arts Festival, which premiered in 2002. The Oregon Country Fair is a three-day festival featuring handmade crafts, educational displays and costumed entertainment. The annual Starwood Festival, founded in 1981, is a seven-day event indicative of the spiritual quest of hippies through an exploration of non-mainstream religions and world-views, and has offered performances and classes by a variety of hippie and counter-culture icons.
"The '60s were a leap in human consciousness. Mahatma Gandhi, Malcolm X, Martin Luther King, Che Guevara, they led a revolution of conscience. The Beatles, The Doors, Jimi Hendrix created revolution and evolution themes. The music was like Dalí, with many colors and revolutionary ways. The youth of today must go there to find themselves."
 — Carlos Santana
The Burning Man festival began in 1986 at a San Francisco beach party and is now held in the Black Rock Desert northeast of Reno, Nevada. Although few participants would accept the "hippie" label, Burning Man is a contemporary expression of alternative community in the same spirit as early hippie events. The gathering becomes a temporary city (36,500 occupants in 2005, 50,000+ in 2011), with elaborate encampments, displays, and many art cars. Other events that enjoy a large attendance include the Rainbow Family Gatherings, The Gathering of the Vibes, Community Peace Festivals, and the Woodstock Festivals.
In the UK, there are many new age travellers who are known as hippies to outsiders, but prefer to call themselves the Peace Convoy. They started the Stonehenge Free Festival in 1974, but English Heritage later banned the festival in 1985, resulting in the Battle of the Beanfield. With Stonehenge banned as a festival site, new age travellers gather at the annual Glastonbury Festival. Today, hippies in the UK can be found in parts of South West England, such as Bristol (particularly the neighborhoods of Montpelier, Stokes Croft, St Werburghs, Bishopston, Easton and Totterdown), Glastonbury in Somerset, Totnes in Devon, and Stroud in Gloucestershire, as well as areas of London and Brighton. In the summer, many hippies and those of similar subcultures gather at numerous outdoor festivals in the countryside.
In New Zealand between 1976 and 1981 tens of thousands of hippies gathered from around the world on large farms around Waihi and Waikino for music and alternatives festivals. Named "Nambassa", the festivals focused on peace, love, and a balanced lifestyle. The events featured practical workshops and displays advocating alternative lifestyles, self sufficiency, clean and sustainable energy and sustainable living.
In the UK and Europe, the years 1987 to 1989 were marked by a large-scale revival of many characteristics of the hippie movement. This later movement, composed mostly of people aged 18 to 25, adopted much of the original hippie philosophy of love, peace and freedom. The summer of 1988 became known as the Second Summer of Love. Although the music favored by this movement was modern electronic music, especially house music and acid house, one could often hear songs from the original hippie era in the "chill out rooms" at raves. In the UK, many of the well-known figures of this movement first lived communally in Stroud Green, an area of north London located in Finsbury Park. In 1995, The Sekhmet Hypothesis attempted to link both hippie and rave culture together in relation to transactional analysis, suggesting that rave culture was a social archetype based on the mood of friendly strength, compared to the gentle hippie archetype, based on friendly weakness. The later electronic dance genres known as goa trance and psychedelic trance and its related events and culture have important hippie legacies and neo hippie elements. The popular DJ of the genre Goa Gil, like other hippies from the 1960s, decided to leave the US and Western Europe to travel on the hippie trail and later developing psychedelic parties and music in the Indian island of Goa in which the goa and psytrance genres were born and exported around the world in the 1990s and 2000s.
Popular films depicting the hippie ethos and lifestyle include "Woodstock", "Easy Rider", "Hair", "The Doors", "Across the Universe", "Taking Woodstock", and "Crumb".
In 2002, photojournalist John Bassett McCleary published a 650-page, 6,000-entry unabridged slang dictionary devoted to the language of the hippies titled "The Hippie Dictionary: A Cultural Encyclopedia of the 1960s and 1970s". The book was revised and expanded to 700 pages in 2004. McCleary believes that the hippie counterculture added a significant number of words to the English language by borrowing from the lexicon of the Beat Generation, through the hippies' shortening of beatnik words and then popularizing their usage. 
In 2005, journalist Oliver Benjamin founded , a website-philosophy and mock religion inspired by the character "the Dude", a former hippie, in the 1998 movie "The Big Lebowski". Dudeism, as it is known, holds many connections to the hippie ethos, from its “take it easy” attitude and rebel shrug, to its come-as-you-are sense of individual freedom and expression. Dudeism is very much influenced by the hippie movement, maintaining that the "revolution is not over", that it actually began a very long time ago, and will continue far into the future. Dudeist literature even claims that Dudeism has provided a contemporary spiritual home for the hippie philosophy.
Further reading and resources.
</dl>

</doc>
<doc id="47651" url="http://en.wikipedia.org/wiki?curid=47651" title="Reproducibility">
Reproducibility

Reproducibility is the ability of an entire experiment or study to be reproduced, either by the researcher or by someone else working independently. It is one of the main principles of the scientific method and relies on "ceteris paribus" (other things being equal). The result values of distinct experimental trials are said to be "commensurate" if they are obtained according to the same reproducible experimental description and procedure. The basic idea can be seen in Aristotle's dictum that there is no scientific knowledge of the individual, where the word used for "individual" in Greek had the connotation of the "idiosyncratic", or wholly isolated occurrence. Thus all knowledge, all science, necessarily involves the formation of general concepts and the invocation of their corresponding symbols in language (cf. Turner).
Aristotle′s conception about the knowledge of the individual being considered unscientific is due to lack of the field of statistics in his time, so he could not appeal to statistical averaging of the individual.
Reproducibility also refers to the degree of agreement between measurements or observations conducted on replicate specimens in different locations by different people, as part of the precision of a test method.
History.
The first to stress the importance of reproducibility in science was the Irish chemist Robert Boyle, in England in the 17th century. Boyle's air pump was designed to generate and study vacuum, which at the time was a very controversial concept. Indeed, distinguished philosophers such as René Descartes and Thomas Hobbes denied the very possibility of vacuum existence. Historians of science Steven Shapin and Simon Schaffer, in their 1985 book "Leviathan and the Air-Pump", describe the debate between Boyle and Hobbes, ostensibly over the nature of vacuum, as fundamentally an argument about how useful knowledge should be gained. Boyle, a pioneer of the experimental method, maintained that the foundations of knowledge should be constituted by experimentally produced facts, which can be made believable to a scientific community by their reproducibility. By repeating the same experiment over and over again, Boyle argued, the certainty of fact will emerge.
The air pump, which in the 17th century was a complicated and expensive apparatus to build, also led to one of the first documented disputes over the reproducibility of a particular scientific phenomenon. In the 1660s, the Dutch scientist Christiaan Huygens built his own air pump in Amsterdam, the first one outside the direct management of Boyle and his assistant at the time Robert Hooke. Huygens reported an effect he termed "anomalous suspension", in which water appeared to levitate in a glass jar inside his air pump (in fact suspended over an air bubble), but Boyle and Hooke could not replicate this phenomenon in their own pumps. As Shapin and Schaffer describe, “it became clear that unless the phenomenon could be produced in England with one of the two pumps available, then no one in England would accept the claims Huygens had made, or his competence in working the pump”. Huygens was finally invited to England in 1663, and under his personal guidance Hooke was able to replicate anomalous suspension of water. Following this Huygens was elected a Foreign Member of the Royal Society. However, Shapin and Schaffer also note that “the accomplishment of replication was dependent on contingent acts of judgment. One cannot write down a formula saying when replication was or was not achieved”.
The philosopher of science Karl Popper noted briefly in his famous 1934 book "The Logic of Scientific Discovery" that “non-reproducible single occurrences are of no significance to science”. The Statistician Ronald Fisher wrote in his 1935 book "The Design of Experiments", which set the foundations for the modern scientific practice of hypothesis testing and statistical significance, that “we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us statistically significant results”. Such assertions express a common dogma in modern science that reproducibility is a necessary condition (although not necessarily sufficient) for establishing a scientific fact, and in practice for establishing scientific authority in any field of knowledge. However, as noted above by Shapin and Schaffer, this dogma is not well-formulated quantitatively, such as statistical significance for instance, and therefore it is not explicitly established how many times must a fact be replicated to be considered reproducible.
Reproducible data.
Reproducibility is one component of the precision of a measurement or test method. The other component is repeatability which is the degree of agreement of tests or measurements on replicate specimens by the same observer in the same laboratory. Both repeatability and reproducibility are usually reported as a standard deviation. A reproducibility limit is the value below which the difference between two test results obtained under reproducibility conditions may be expected to occur with a probability of approximately 0.95 (95%).
Reproducibility is determined from controlled interlaboratory test programs or a Measurement systems analysis.
Although they are often confused, there is an important distinction between replicates, versus an independent repeat of an experiment. Replicates are performed within an experiment. They are not and cannot provide independent evidence of reproducibility. Rather they serve as an internal "check" on an experiment and should not be shown as part of the experimental results within a scientific publication. It is the independent repeat of an experiment that serves to underpin its reproducibility 
Reproducible research.
The term "reproducible research" refers to the idea that the ultimate product of academic research is the paper along with the full computational environment used to produce the results in the paper such as the code, data, etc. that can be used to reproduce the results and create new work based on the research.
Psychology has recently seen a renewal of internal concerns about irreproducible results. Researchers explained in a 2006 study that, of 249 data sets from American Psychology Association (APA) empirical articles, 73% of contacted authors did not respond with their data over a 6-month period. The first author published a paper in 2012 suggesting researchers should publish data along with their works, releasing a dataset alongside as a demonstration.
Reproducible research is key to new discoveries in pharmacology. A Phase I discovery will be followed by Phase II reproductions as a drug develops towards commercial production. In recent decades Phase II success has fallen from 28% to 18%. A 2011 study found that 65% of medical studies were inconsistent when re-tested, and only 6% were completely reproducible.
In 2012, a study by Begley and Ellis was published in "Nature" that reviewed a decade of research. That study found that 47 out of 53 medical research papers focused on cancer research were irreproducible. The irreproducible studies had a number of features in common, including that studies were not performed by investigators blinded to the experimental versus the control arms, there was a failure to repeat experiments, a lack of positive and negative controls, failure to show all the data, inappropriate use of statistical tests and use of reagents that were not appropriately validated. John P. A. Ioannidis writes, "While currently there is unilateral emphasis on 'first' discoveries, there should be as much emphasis on replication of discoveries." The "Nature" study was itself reproduced in the journal PLOS ONE, which confirmed that a majority of cancer researchers surveyed had been unable to reproduce a result. Attempts to reproduce studies often strained relationships with the laboratories that were first to publish.
Noteworthy irreproducible results.
Hideyo Noguchi became famous for correctly identifying the bacterial agent of syphilis, but also claimed that he could culture this agent in his laboratory. Nobody else has been able to produce this latter result.
In March 1989, University of Utah chemists Stanley Pons and Martin Fleischmann reported the production of excess heat that could only be explained by a nuclear process ("cold fusion"). The report was astounding given the simplicity of the equipment: it was essentially an electrolysis cell containing heavy water and a palladium cathode which rapidly absorbed the deuterium produced during electrolysis. The news media reported on the experiments widely, and it was a front-page item on many newspapers around the world (see science by press conference). Over the next several months others tried to replicate the experiment, but were unsuccessful.
Nikola Tesla claimed as early as 1899 to have used a high frequency current to light gas-filled lamps from over 25 mi away without using wires. In 1904 he built Wardenclyffe Tower on Long Island to demonstrate means to send and receive power without connecting wires. The facility was never fully operational and was not completed due to economic problems, so no attempt to reproduce his first result was ever carried out.

</doc>
<doc id="47653" url="http://en.wikipedia.org/wiki?curid=47653" title="Game clock">
Game clock

A game clock consists of two adjacent clocks with buttons to stop one clock while starting the other, so that the two clocks never run simultaneously. Game clocks are used in two-player games where the players move in turn. The purpose is to keep track of the total time each player takes for his or her own moves, and ensure that neither player overly delays the game.
Game clocks were first used extensively in tournament chess, and are often called chess clocks. In a tournament, the arbiter typically places all clocks in the same orientation, so that he can easily assess games that need attention at later stages. Their use has since spread to tournament Scrabble, shogi, go, and nearly every competitive two-player board game, as well as other types of games. The first time that game clocks were used in a chess tournament was in the London 1883 tournament.
The simplest time control is "sudden death", in which players must make a predetermined number of moves in a certain amount of time or forfeit the game immediately.
A particularly popular variant in informal play is blitz chess, in which each player is given a short time (e.g. five minutes) on the clock in which to play the entire game.
The players may take more or less time over any individual move. The opening moves in chess are often played quickly due to their familiarity, which leaves the players more time to consider more complex and unfamiliar positions later. It is not unusual in slow chess games for a player to leave the table, but the clock of the absent player continues to run if it is his turn, or starts to run if his opponent makes a move.
Analog game clocks.
Analog clocks are equipped with a "flag" (a Dutch invention) that falls to indicate the exact moment the player's time has expired. Analog clocks use mechanical buttons. Pressing the button on one player's side physically stops the movement of that player's clock and releases the hold on the opponent's.
The drawbacks of the mechanical clocks include accuracy and matching of the two clocks, and matching of the indicators (flags) of time expiration. Additional time cannot easily be added for more complex time controls, especially those that call for an increment or delay on every move, such as some forms of byoyomi. However, a malfunctioning analog clock is a less serious event than a malfunctioning digital clock.
Early development of digital game clocks.
In 1973, to address the issues with analog clocks, Bruce Cheney, a Cornell University Electrical Engineering student and chess player, created the first digital chess clock as a project for an undergraduate EE course. Typical of most inventions, it was crude compared to the products on the market many years later and was limited by the technology that existed at the time. For example, the display was done with red LEDs. LEDs require significant power, and as a result, the clock had to be plugged into a wall outlet. The high cost of LEDs at the time meant that only one set of digits could be displayed, that of the player whose turn it was to move. This meant that each player's time had to be multiplexed to the display when their time was running. In 1973, LSI chips were not readily or cheaply available, so all the multiplexing and logic were done using chips that consisted of four two-input TTL NAND gates, which resulted in excessive power consumption. Being plugged into the wall is obviously a major drawback, but had one advantage: the timebase for the clock was driven off of a rectified version of 60 cycle AC current. Each player had a separate counter, and, in a parallel to the original mechanical architecture, one player's counter was disabled while the other's was running. The clock only had one mode: time ran forward. It could be reset, but not set. It did not count the number of moves. But it successfully addressed the original goals of the project (accurate and matched timing). 
A chess clock that was patented in 1975 was developed by Joseph Meshi and became the first commercially available digital chess clock. The patent numbers are 4,062,180 (filed on July 1975) & 4,247,925 (filed on July 1978). Mr. Meshi published his MBA Thesis on this subject titled-"Demand Analysis for a New Product (The Digital Chess Clock)", at San Diego State University in 1978. Mr. Meshi designed and built two versions of the clock-the Micromate-80 (based on the original patent) and the Micromate-180, which incorporated the latest innovations. The Micromate 180 was truly revolutionary and clearly ahead of its time.
Recent developments of digital clocks and current usage.
Digital clocks and Internet gaming have spurred a wave of experimentation with more varied and complex time controls than the traditional standards. Time control is commonly used in modern chess in many different methodologies. One particularly notable development, which has gained quite wide acceptance in chess, was proposed by former world champion Bobby Fischer, who in 1988 filed for U.S. Patent (awarded in 1989) for a new type of digital chess clock. Fischer's digital clock gave each player a fixed period of time at the start of the game and then added a small amount after each move. Joseph Meshi called this "Accumulation" as it was a main feature of his patented Micromate-180 (US Patent 4,247,925 1978). This became the linchpin of Fischer's clock patented 10 years later. In this way, the players would never be desperately short of time, but games could also be completed more quickly, doing away with the need for adjournments (in which a game is left incomplete to be finished at a later date). Although it was slow to catch on, as of 2004 a very large number of top class tournaments use Fischer's system, though usually in combination with the more traditional clocks (at lower levels, more traditional clocks are still employed, as they are cheaper). Other aspects of Fischer's patent, such as a synthesized voice announcing how much time the players have, thus eliminating the need for them to keep looking at the clock, have not been adopted.
On March 10, 1994, a patent application was filed by inventors Frank A. Camaratta, jr. of Huntsville, AL, and William Goichberg of Salisbury Mills, NY for a game timer, especially suitable for playing the game of chess, which employed a "Delay" feature. The game timer provides, among other features, a user definable delay between the time the activation button is pressed and the time that the activated clock actually begins to count down. United States Patent 5,420,830 was issued on May 10, 1995 and subsequently assigned to the United States Chess Federation by the inventors. The benefit of the delay clock is to reduce the likelihood that a player with positional and/or material superiority will lose a match solely because of the expiration of time on that player's time clock. The "Delay" mode is still a popular feature for both Standard and sudden-death time controls in major tournaments throughout the U.S.
Time controls.
There are five main types of "Time Controls": (1) "Fischer" (invented by Bobby Fischer), (2) "Bronstein" (invented by David Bronstein), (3) "Simple Delay", (4) "Overtime Penalty" and (5) "Hour Glass". The first three time controls implement some sort of delay clock, a small amount of time that is added for each move. The reason is that with a sudden-death time limit, all moves must be completed in the specified time, or the player loses. With a small delay added at each move, the player always has at least that much time to make a move. The three types of delay clocks differ in how the delay is implemented. The last two time controls are somewhat different, as they do not rely on time delay, as explained below.

</doc>
<doc id="47656" url="http://en.wikipedia.org/wiki?curid=47656" title="Kamov">
Kamov

Kamov is a Russian rotorcraft manufacturing company, founded by Nikolai Ilyich Kamov, who started building his first rotary-winged aircraft in 1929, together with N. K. Skrzhinskii. Up to the 1940s, they created many autogyros, including the TsAGI A-7-3, the only armed autogyro to see (limited) combat action.
The Kamov Design Bureau (design office prefix Ka) has more recently specialised in compact helicopters with coaxial rotors, suitable for naval service and high-speed operations.
Kamov merged with Mil and Rostvertol to form Oboronprom Corp. in 2006. The Kamov brand name was retained, though the new company dropped overlapping product lines.
Other design consultancy work.
In the 1990s Kamov drew up an attack helicopter design for the Chinese government under a secret contract which was later revealed. This preliminary design was used as the basis of the CAIC Z-10.

</doc>
<doc id="47658" url="http://en.wikipedia.org/wiki?curid=47658" title="Aérospatiale">
Aérospatiale

Aérospatiale (]) was a French state-owned aerospace manufacturer that built both civilian and military aircraft, rockets and satellites. It was originally known as Société nationale industrielle aérospatiale (SNIAS). Its head office was in the 16th arrondissement of Paris.
The former assets of Aerospatiale are now part of Airbus Group, except the Satellites activities which merged with Alcatel and became Alcatel Space, in 1999, now Thales Alenia Space.
History.
The company (as SNIAS) was created in 1970 by the merger of the state-owned companies Sud Aviation, Nord Aviation and "Société d'études et de réalisation d'engins balistiques" (SEREB). Starting in 1971 it was directed by Henri Ziegler. Its North American Marketing arm French Aerospace Corporation was renamed to European Aerospace Corporation in 1971.
In 1991 the company helped construct the revolutionary chassis of the Bugatti EB110 Supercar. The chassis was built completely of carbon fibre, and was very lightweight.
In 1992, DaimlerBenz Aerospace AG (DASA) and Aerospatiale combined their helicopter divisions to form the Eurocopter Group.
In 1999, Aerospatiale, except for the satellites activities, merged with Matra Haute Technologie to form Aerospatiale-Matra. In 2001, Aerospatiale-Matra's missile group was merged with Matra BAe Dynamics and the missile division of Alenia Marconi Systems to form MBDA. Lionel Jospin's Plural Left government initiated the privatization of Aerospatiale.
On July 10, 2000, Aerospatiale-Matra merged with Construcciones Aeronáuticas SA (CASA) of Spain and DaimlerChrysler Aerospace AG (DASA) of Germany to form the European Aeronautic Defence and Space Company (EADS).

</doc>
<doc id="47660" url="http://en.wikipedia.org/wiki?curid=47660" title="Espresso">
Espresso

Espresso (, ]), or less frequently expresso, is coffee brewed by forcing a small amount of nearly boiling water under pressure through finely ground coffee beans. Espresso is generally thicker than coffee brewed by other methods, has a higher concentration of suspended and dissolved solids, and has crema on top (a foam with a creamy consistency). As a result of the pressurized brewing process, the flavors and chemicals in a typical cup of espresso are very concentrated. Espresso is the base for other drinks, such as a caffè latte, cappuccino, caffè macchiato, cafe mocha, or caffè Americano. Espresso has more caffeine per unit volume than most coffee beverages, but because the usual serving size is much smaller, the total caffeine content is less. Although the actual caffeine content of any coffee drink varies by size, bean origin, roast method and other factors, the caffeine content of "typical" servings of espresso vs. drip brew are 120 to 170 mg vs. 150 to 200 mg.
Brewing process.
Espresso is made by forcing very hot water under high pressure through finely ground, compacted coffee. Tamping down the coffee promotes the water's even penetration of the grounds. This process produces an almost syrupy beverage by extracting both solid and dissolved components. The "crema" is produced by emulsifying the oils in the ground coffee into a colloid, which does not occur in other brewing methods. There is no universal standard defining the process of extracting espresso, but there are several published definitions which attempt to place constraints on the amount and type of ground coffee used, the temperature and pressure of the water, and the rate of extraction. Generally, one uses an espresso machine to make espresso. The act of producing a shot of espresso is often termed "pulling" a shot, originating from lever espresso machines, which require pulling down a handle attached to a spring-loaded piston, forcing hot water through the coffee at high pressure. Today, however, it is more common for the pressure to be generated by an electric pump.
The technical parameters outlined by the Italian Espresso National Institute for making 
a "certified Italian espresso" are:
Espresso roast.
Espresso is both a coffee beverage and a brewing method. It is not a specific bean, bean blend, or roast level. Any bean or roasting level can be used to produce authentic espresso. For example, in southern Italy, a darker roast is generally preferred. Farther north, the trend moves toward slightly lighter roasts, while outside Italy, a wide range is popular.
Popularity.
Espresso has risen in popularity worldwide since the 1980s. In the United States, cafés serve many variations by adding syrup, whipped cream, flavour extracts, soy milk, and spices to their drinks. The American Pacific Northwest has been viewed as the driver behind this trend. The popularity later spread to shops in other regions and into homes as kitchen-friendly machines became available at moderate cost. In other parts of the world, espresso has long been the customary method of coffee preparation in restaurants, bars and coffee shops.
History.
Angelo Moriondo’s Italian patent for a steam-driven "instantaneous" coffee beverage making device, which was registered in Turin in 1884 (No. 33/256), is notable. Author Ian Bersten, whose history of coffee brewers is cited below, claims to have been the first to discover Moriondo’s patent. Bersten describes the device as “… almost certainly the first Italian bar machine that controlled the supply of steam and water separately through the coffee” and Moriondo as “... certainly one of the earliest discoverers of the expresso ["sic"] machine, if not the earliest.” 
Seventeen years later, in 1901, Milanese Luigi Bezzera came up with a number of improvements to the espresso machine. He patented a number of these, the first of which was applied for on the 19th of December 1901. It was titled “Innovations in the machinery to prepare and immediately serve coffee beverage” (Patent No. 153/94, 61707, granted on the 5th of June 1902).
In 1905, the patent was bought by Desiderio Pavoni, who founded the “La Pavoni” company and began to produce the machine industrially (one a day) in a small workshop in Via Parini in Milan.
The popularity of espresso developed in various ways; a detailed discussion of the spread of espresso is given in , which is a source of various statements below.
In Italy, the rise of espresso consumption was associated with urbanization, espresso bars providing a place for socializing. Further, coffee prices were controlled by local authorities, provided the coffee was consumed standing up, encouraging the "stand at a bar" culture.
In the English-speaking world, espresso became popular, particularly in the form of cappuccino, due to the tradition of drinking coffee with milk and the exotic appeal of the foam; in the United States, this was more often in the form of lattes, with or without flavored syrups added. The latte is claimed to have been invented in the 1950s by Italian American Lino Meiorin of Caffe Mediterraneum in Berkeley, California, as a long cappuccino, and was then popularized in Seattle, and then nationally and internationally by Seattle-based Starbucks in the late 1980s and 1990s.
In the United Kingdom, espresso grew in popularity among youth in the 1950s, who felt more welcome in the coffee shops than in public houses (pubs).
Espresso was initially popular, particularly within the Italian diaspora, growing in popularity with tourism to Italy exposing others to espresso, as developed by Eiscafès established by Italians in Germany.
Initially, expatriate Italian espresso bars were downmarket venues, serving the working class Italian diaspora – and thus providing appeal to the alternative subculture / counterculture; this can still be seen in the United States in Italian American neighborhoods, such as Boston's North End, New York's Little Italy, and San Francisco's North Beach. As specialty coffee developed in the 1980s (following earlier developments in the 1970s and even 1960s), an indigenous artisanal coffee culture developed, with espresso instead positioned as an upmarket drink.
Today, coffee culture commentators distinguish large chain, midmarket coffee as "Second Wave Coffee", and upmarket, artisanal coffee as "Third Wave Coffee".
In the Middle East, espresso is growing in popularity, with the opening of Western coffee shop chains.
Café vs. home preparation.
A distinctive feature of espresso, as opposed to brewed coffee, is espresso's association with cafés, due both to the specialized equipment and skill required, thus making the enjoyment of espresso a social experience.
Home espresso machines have increased in popularity with the general rise of interest in espresso. Today, a wide range of home espresso equipment can be found in kitchen and appliance stores, online vendors, and department stores. The first espresso machine for home use was the Gaggia Gilda. Soon afterwards, similar machines such as the Faema Faemina, FE-AR La Peppina and VAM Caravel followed suit in similar form factor and operational principles. These machines still have a small but dedicated share of fans. Until the advent of the first small electrical pump-based espresso machines such as the Gaggia Baby and Quickmill 810, home espresso machines were not widely adopted. In recent years, the increased availability of convenient counter-top fully automatic home espresso makers and pod-based espresso serving systems has increased the quantity of espresso consumed at home.
The popularity of home espresso making parallels the increase of home coffee roasting. Some amateurs pursue both home roasting coffee and making espresso.
Etymology and usage of the term.
The origin of the term "espresso" is the subject of considerable debate. Although some Anglo-American dictionaries simply refer to "pressed-out", "espresso," much like the English word "express", conveys the senses of "just for you" and "quickly," which can be related to the method of espresso preparation.
The words "express", "expres" and "espresso" each have several meanings in English, French and Italian. The first meaning is to do with the idea of "expressing" or squeezing the flavour from the coffee using the pressure of the steam. The second meaning is to do with speed, as in a train. Finally there is the notion of doing something "expressly" for a person ... The first Bezzera and Pavoni espresso machines in 1906 took 45 seconds to make a cup of coffee, one at a time, expressly for you.
The spelling "espresso" is widely considered correct while "expresso" appears as a less common variant. Italy uses the term "espresso", substituting most "x" letters in Latin root words with "s"; x is not considered part of the standard Italian alphabet. Italian people commonly refer to it simply as "caffè" (coffee), espresso being the ordinary coffee to order; in Spain, while "café expreso" is seen as the more "formal" denomination, "café solo" (alone, without milk) is the usual way to ask for it when at an espresso bar.
Modern espresso, using hot water under pressure, as pioneered by Gaggia in the 1940s, was originally called "crema caffè", in English "cream coffee", as can be seen on old Gaggia machines, due to the crema. This term is no longer used, though "crema caffè" and variants ("caffè crema, café crema") find occasional use in branding.
While the 'expresso' spelling is recognized as mainstream usage in some American dictionaries, its inclusion is controversial, with many outright calling the 'x' variant illegitimate. Oxford Dictionaries online states "The spelling expresso is not used in the original Italian and is strictly incorrect, although it is common."
Shot variables.
The main variables in a shot of espresso are the "size" and "length". This terminology is standardized, but the precise sizes and proportions vary substantially.
Cafés may have a standardized shot (size and length), such as "triple "ristretto"", only varying the number of shots in espresso-based drinks such as lattes, but not changing the extraction – changing between a double and a triple requires changing the filter basket size, while changing between "ristretto", "normale", and "lungo" may require changing the grind, which is less easily accommodated in a busy café, as fine tweaking of the grind is a central aspect to consistent quality espresso-making.
Size.
The size can be a single, double, or triple, which corresponds roughly to a 25, 50, and 75 ml (approximately 0.8, 1.7, or 2.5 US fluid ounce) standard ("normale") shot, and use a proportional amount of ground coffee, roughly 7, 14, and 21 grams; correspondingly sized filter baskets are used. The Italian multiplier term "doppio" is often used for a double, with "solo" and "triplo" being more rarely used for singles and triples. The single shot is the traditional shot size, being the maximum that could easily be pulled on a lever machine, while the double is the standard shot today.
Single baskets are sharply tapered or stepped down in diameter to provide comparable depth to the double baskets and, therefore, comparable resistance to water pressure. Most double baskets are gently tapered (the "Faema model"), while others, such as the La Marzocco, have straight sides. Triple baskets are normally straight-sided.
Portafilters will often come with two spouts, usually closely spaced, and a double-size basket – each spout can optionally dispense into a separate cup, yielding two solo-size (but doppio-brewed) shots, or into a single cup (hence the close spacing). True "solo" shots are rare, with a single shot in a café generally being half of a "doppio" shot.
In espresso-based drinks, particularly larger milk-based drinks, a drink with three or four shots of espresso will be called a "triple" or "quad", respectively.
Length.
The length of the shot can be "ristretto" (or "stretto") (reduced), "normale"/standard (normal), or "lungo" (long): these may correspond to a smaller or larger drink with the same amount of ground coffee and same level of extraction, or to different length of extraction. Proportions vary, and the volume (and low density) of crema make volume-based comparisons difficult (precise measurement uses the mass of the drink), but proportions of 1:1, 1:2, and 1:3–4 are common for "ristretto", "normale", and "lungo", corresponding to 30 ml, 60 ml, and 120–170 ml (1, 2, and 4–6 US fl oz) for a double shot. "Ristretto" is the most commonly used of these terms, and double or triple "ristrettos" are particularly associated with artisanal espresso.
"Ristretto, normale", and "lungo" may not simply be the same shot, stopped at different times – which may result in an underextracted shot (if run too short a time) or an overextracted shot (if run too long a time). Rather, the grind is adjusted (finer for "ristretto", coarser for "lungo") so the target volume is achieved by the time extraction finishes.
A significantly longer shot is the "caffè crema", which is longer than a "lungo", ranging in size from 120–240 ml (4–8 US fl oz), and brewed in the same way, with a coarser grind.
The method of adding hot water produces a milder version of original flavor, while passing more water through the load of ground coffee will add other flavors to the espresso, which might be unpleasant for some people.
Espresso-based drinks.
In addition to being served alone, espresso is frequently blended, notably with milk - either steamed (without significant foam), wet foamed ("microfoam"), or dry foamed, and with hot water. Notable milk-based espresso drinks, in order of size, include: macchiato, cappuccino, flat white, and latte; other milk and espresso combinations include latte macchiato, cortado and galão, which are made primarily with steamed milk with little or no foam. Espresso and water combinations include Americano and long black. Other combinations include coffee with espresso, sometimes called "red eye" or "shot in the dark".
In order of size, these may be organized as follows:
Some common combinations may be organized graphically as follows:
Methods of preparation differ between drinks and between baristas. For macchiatos, cappuccino, flat white, and smaller lattes and Americanos, the espresso is brewed into the cup, then the milk or water is poured in. For larger drinks, where a tall glass will not fit under the brew head, the espresso is brewed into a small cup, then poured into the larger cup; for this purpose a demitasse or specialized espresso brew pitcher may be used. This "pouring into an existing glass" is a defining characteristic of the latte macchiato and classic renditions of the red eye. Alternatively, a glass with "existing" water may have espresso brewed into it – to preserve the crema – in the long black. Brewing onto milk is not generally done.
References.
</dl>

</doc>
<doc id="47661" url="http://en.wikipedia.org/wiki?curid=47661" title="Perkele">
Perkele

Perkele is an alternative name of Ukko, the chief god of the Finnish pagan pantheon. In modern Finnish, the interjection "perkele!" is a common profanity, approximately equivalent to "the Devil!" in meaning and "fuck!" in intensity.
Origins.
The name is of Indo-European origin. Related gods from other areas are Perkūnas (Lithuania), Pērkons (Latvia), Percunis (Prussia), Piarun (Belarus), Peko or Pekolasõ (Estonia) and Perun or Piorun (Bulgaria, Croatia, Czech Republic, Poland, Russia, Ukraine, Serbia, Slovakia, Slovenia).
Use.
It has a history of being used as a curse: a cry for the god for strength. It still is a common curse word in vernacular Finnish. To a Finn, the word entails seriousness and potency that more lightly used curses lack. Also, when the Research Institute for the Languages of Finland held a popular contest to nominate the "most energizing" word in the Finnish language, one of the suggestions was Perkele because "it is the curse word that gave the most strength for the reconstruction of Finland after the wars." However, the popular vote was won by the word "Aurinko", sun.
For comparison, "Parom" a corrupted form of the name "Perun", is used as a mild curse in Slovak language - "Do Paroma!" is roughly equivalent to perkele in Finnish.
Introduction of Christianity.
As Finland was Christianized, the church started demonizing the Finnish gods. This led to the use of "Perkele" as a translation for "devil" in the Finnish translation of the Bible, thus making the use of the word a sin. Later, in a 1992 translation, the word is switched to "paholainen".
Uses in popular culture.
Many Finnish heavy metal bands like Impaled Nazarene and Norther use the word perkele for emphasis and to reference Finnishness. The Swedish Oi! band Perkele is also named after the Finnish swear word.
In a Finnish whaler can be heard exclaiming 'Perkele!' after the Klingon Bird of Prey decloaks ahead of the whaling vessel.
Perkele is a popular trend among "Scandinavia and the World" comics, often used by the commonly grouchy Finland to express anger, sadness, happiness, or any kind of emotion that replaces having to deal with conversation.

</doc>
<doc id="47662" url="http://en.wikipedia.org/wiki?curid=47662" title="Antonov">
Antonov

Antonov State Company (Ukrainian: Державне підприємство "Антонов"), formerly the Antonov Aeronautical Scientific-Technical Complex (Antonov ASTC) (Ukrainian: Авіаційний науково-технічний комплекс імені Антонова, АНТК ім. Антонова), and earlier the Antonov Design Bureau, is a Ukrainian aircraft manufacturing and services company. Antonov's particular expertise is in the fields of very large aeroplanes and aeroplanes using unprepared runways. Antonov (model prefix An-) has built a total of approximately 22,000 aircraft, and thousands of planes are currently operating in the former Soviet Union and in developing countries.
Antonov StC is a state-owned commercial company. Its headquarters and main industrial ground are located in and adjacent to Kiev. On 12 May 2015 it was transferred from the Ministry of Economic Development and Trade to the Ukroboronprom (Ukrainian Defense Industry).
History.
Soviet era.
Foundation and relocation.
The company was established in 1946 in Novosibirsk as a top-secret Soviet Research and Design Bureau No. 153, headed by Oleg Antonov and specialised in turboprop military transport aircraft. The An-2 biplane is a major achievement of this period with hundreds of aircraft still operated as of 2013. In 1952, the Bureau was relocated to Kiev, a city with rich aviation history where aircraft-manufacturing infrastructure was being restored after the World War II destruction.
First serial aircraft and expansion.
In 1957, the bureau successfully introduced the An-10/An-12 family of mid-range turboprop aeroplanes into mass production (thousands of aircraft were manufactured). The model have been seeing heavy combat and civil use around the globe to the present day, most notably in the Vietnam War, Soviet war in Afghanistan and the Chernobyl disaster relief megaoperation.
In 1959, the bureau began construction of the separate Flight Testing and Improvement Base in suburban Hostomel (now the Antonov Airport).
In 1965, the Antonov An-22 heavy military transport enters serial production, supplementing the An-12 in major military and humanitarian airlifts of the Soviet Union. The model became the first Soviet wide-body aircraft and remains the world's largest turboprop-powered aircraft to date. Antonov designed and presented a nuclear-powered version of the An-22 which, however, never entered flight testing phase.
In 1966, after major expansion in the Sviatoshyn neighbourhood of the city, the company was renamed to another disguise name "Kiev Mechanical Plant". Two independent aircraft production and repair facilities, under engineering supervision of the Antonov Bureau, also appeared in Kiev during this period.
Prominence and Antonov's retirement.
In the 1970s and early 1980s, the company established itself as USSR's main designer of military transport aircraft with dozens of new modifications in development and production. After Oleg Antonov's death in 1984, the company is officially renamed as the Research and Design Bureau named after O.K. Antonov (Russian: Опытно-конструкторское бюро имени О.К. Антонова) while continuing the use of "Kiev Mechanical Plant" alias for some purposes.
Late Soviet-era: superlarge projects and first commercialisation.
In the late 1980s, the Antonov Bureau achieved global prominence after introduction of its extra large aeroplanes. The An-124 "Ruslan" (1982) became Soviet Union's serial-produced strategic airlifter. The Bureau enlarged the "Ruslan" design even more for the Soviet space shuttle programme logistics, creating the An-225 "Mriya" in 1989. "Mriya" has since been the world's largest and heaviest aeroplane.
End of the Cold War and perestroyka allowed the Antonov's first step to commercialisation and foreign expansion. In 1989, the Antonov Airlines subsidiary was created for its own aircraft maintenance and cargo projects.
Independent Ukraine.
Antonov Design Bureau remained a state-owned company after Ukraine achieved its independence in 1991 and is since regarded as a strategic national asset.
Expansion to free market.
Since independence, Antonov is busy with certifying and marketing of its models (both Soviet-era and newly developed) to free commercial aeroplanes' markets. New models introduced to serial production and delivered to customers include the Antonov An-140, Antonov An-148 and Antonov An-158 regional airliners.
Among several modernisation projects, Antonov received orders for upgrading "hundreds" of its legendary An-2 utility planes still in operation in Azerbaijan, Cuba and Russia to the An-2-100 upgrade version.
Production facilities' consolidation.
During the Soviet period, not all Antonov-designed aircraft were manufactured by the company itself. This was a result of Soviet industrial strategy that split military production between different regions of the USSR to minimise potential war loss risks. As a result, Antonov aeroplanes are often assembled by the specialist contract manufacturers.
In 2009, the once-independent "Aviant" aeroplane-assembling plant in Kyiv became part of the Antonov State Company, facilitating a full serial manufacturing cycle of the company. However, the old tradition of co-manufacturing with contractors is continued, both with Soviet-time partners and with new licensees like Iran's HESA.
Products and activities.
Fields of commercial activity of Antonov ASTC include:
Aircraft.
Antonov's aeroplanes (design office prefix An) range from the rugged An-2 biplane (which itself is comparatively large for a biplane) through the An-28 reconnaissance aircraft to the massive An-124 Ruslan and An-225 Mriya strategic airlifters (the latter being the world's heaviest aircraft with only one currently in service). Whilst less famous, the An-24, An-26, An-30 and An-32 family of twin turboprop, high winged, passenger/cargo/troop transport aircraft are important for domestic/short-haul air services particularly in parts of the world once led by communist governments. The An-72/An-74 series of small jetliners is slowly replacing that fleet, and a larger An-70 freighter is under certification.
The Antonov An-148 is a new regional airliner of twin-turbofan configuration. Over 150 aircraft have been ordered since 2007. A stretched version is in development, the An-158 (from 60–70 to 90–100 passengers).

</doc>
<doc id="47663" url="http://en.wikipedia.org/wiki?curid=47663" title="Breguet">
Breguet

Breguet may refer to: 

</doc>
<doc id="47665" url="http://en.wikipedia.org/wiki?curid=47665" title="British Aerospace">
British Aerospace

British Aerospace plc (BAe) was a British aircraft, munitions and defence-systems manufacturer. Its head office was at Warwick House in the Farnborough Aerospace Centre in Farnborough, Hampshire. In 1999 it purchased Marconi Electronic Systems, the defence electronics and naval shipbuilding subsidiary of the General Electric Company plc, to form BAE Systems.
History.
1977 to 1997.
The company was formed in the United Kingdom as a statutory corporation on 29 April 1977 as a result of the Aircraft and Shipbuilding Industries Act. This called for the nationalisation and merger of the British Aircraft Corporation, Hawker Siddeley Aviation, Hawker Siddeley Dynamics and Scottish Aviation. In 1979 BAe officially joined Airbus, the UK having previously withdrawn support for the consortium in April 1969.
In accordance with the provisions of the British Aerospace Act 1980 the statutory corporation was changed to a public limited company (plc), British Aerospace Public Limited Company, on 1 January 1981. On 4 February 1981 the government sold 51.57% of its shares. The British government sold its remaining shares in 1985, maintaining a £1 golden share which allows it veto foreign control of the board or company.
On 26 September 1985, the UK and Saudi Arabian governments signed the Al Yamamah contract, with BAe as prime contractor. The contracts, extended in the 1990s and never fully detailed, involved the supply of Panavia Tornado strike and air defence aircraft, Hawk trainer jets, Rapier missile systems, infrastructure works and naval vessels. The Al Yamamah deals are valued at anything up to £20 billion and still continue to provide a large percentage of BAE Systems' profits.
In 1986, With Alenia Aeronautica, CASA and DASA, BAe formed Eurofighter GmbH for the development of the Eurofighter Typhoon.
On 22 April 1987 BAe acquired Royal Ordnance, the British armaments manufacturer, for £190 million. Heckler & Koch GmbH was folded into this division when BAe acquired it in 1991.
In 1988 BAe purchased the Rover Group which then was privatised by the British government of Margaret Thatcher.
In 1991 BAe acquired a 30% interest in Hutchison Telecommunications through a stock swap deal, where Hutchison was given a controlling stake of 65% in BAe’s wholly owned subsidiary - Microtel Communications Ltd. In August 1991, BAe formed a naval systems joint venture, BAeSEMA, with the Sema Group. BAe acquired Sema's 50% share in 1998. 1991 also saw BAe begin to experience major difficulties. BAe saw its share price fall below 100p for the first time. On 9 September 1991, the company issued a profits warning and later that week "bungled" the launch of a £432 million rights issue. On 25 September 1991 BAe directors led by CEO Richard Evans ousted the Chairman Professor Sir Roland Smith in a move described by "The Independent" as "one of the most spectacular and brutal boardroom coups witnessed in many years." Evans described the troubles as a confluence of events:
 "our property company [Arlington Securities] was hit with a lousy market. Sales of the Rover Group sank by about a fifth and losses mounted. The government's defence spending volumes underwent a major review. Losses in our commercial aerospace division increased dramatically with the recession in the airline industry."
In 1992 BAe formed Avro RJ Regional Jets to produce the Avro RJ series, an evolution of the BAe 146. In mid-1992 BAe wrote off £1 billion of assets, largely as part of redundancies and restructuring of its regional aircraft division. This was largest asset write-off in UK corporate history. The General Electric Company (GEC), later to sell its defence interests to BAe, came close to acquiring BAe at this time. BAe cut 47% of its workforce (60,000 out of 127,000), 40,000 of which were from the regional aircraft division.
Evans decided to sell non-core business activities which included The Rover Group, Arlington Securities, BAe Corporate Jets, BAe Communications and Ballast Nedam. Although the rationale of diversification was sound (to shield the company from cyclical aerospace and defence markets) the struggling company could not afford to continue the position: "We simply could not afford to carry two core businesses, cars and aerospace. At one point Rover was eating up about £2 billion of our banking capacity." BAe Corporate Jets Ltd and Arkansas Aerospace Inc were sold to Raytheon in 1993. In 1994 the Rover Group was sold to BMW and British Aerospace Space Systems was sold to Matra Marconi Space. In 1998 BAe's shareholding of Orange plc was reduced to 5%. The Orange shareholding was a legacy of the 30% stake in Hutchison Telecommunications (UK) Ltd when Hutchison exchanged its own shares for a mobile phone company (Microtel Communications Ltd) from BAe.
BAeSEMA, Siemens Plessey and GEC-Marconi formed UKAMS Ltd in 1994 as part of the Principal Anti-Air Missile System (PAAMS) consortium. UKAMS would become a wholly owned subsidiary of BAe Dynamics in 1998. In 1995 Saab Military Aircraft and BAe signed an agreement for the joint development and marketing of the export version of the JAS 39 Gripen. In 1996 BAe and Matra Defense agreed to merge their missile businesses into a joint venture called Matra BAe Dynamics. In 1997 BAe joined the Lockheed Martin X-35 Joint Strike Fighter team. The company acquired the UK operations of Siemens Plessey Systems (SPS) in 1998 from Siemens AG. DASA purchased SPS' German assets.
Transition to BAE Systems: 1997 to 1999.
Defence consolidation became a major issue in 1998, with numerous reports linking various European defence groups – mainly with each other but also with American defence contractors. It was widely anticipated that BAe would merge with Germany’s DASA to form a pan-European aerospace giant. A merger deal was negotiated between Richard Evans and DASA CEO Jürgen Schrempp. However when it became clear that GEC was selling its defence electronics business Marconi Electronic Systems, Evans put the DASA merger on hold in favour of purchasing Marconi. Evans stated in 2004 that his fear was that an American defence contractor would acquire Marconi and challenge both BAe and DASA. Schrempp was angered by Evans' actions and chose instead to merge DASA with Aerospatiale to create the European Aeronautic Defence and Space Company (EADS). This group was joined by Spain’s CASA following an agreement in December 1999.
The GEC merger to create a UK company compared to what would have been an Anglo-German firm, made the possibility of further penetration of the United States (US) defence market more likely. The company, initially called "New British Aerospace", was officially formed on 30 November 1999 and known as BAE Systems.
Products.
BAe was the UK's largest exporter, a Competition Commission report gives a ten-year aggregate figure of £45 billion, with defence sales accounting for approximately 80%.
Corruption investigation and criticisms.
There have been allegations that the Al Yamamah contracts were a result of bribes ("douceurs") to members of the Saudi royal family and government officials. Some allegations suggested that the former Prime Minister's son Mark Thatcher may have been involved; he has strongly denied receiving payments or exploiting his mother's connections in his business dealings. The UK National Audit Office investigated the contracts and has so far never released its conclusions – the only NAO report ever to be withheld. The BBC's "Newsnight" observed that it is ironic that the once classified report analysing the construction of MI5's Thames House and MI6's Vauxhall Cross headquarters has been released, but the Al Yamamah report is still deemed too sensitive.
The 2007 documentary film Welcome Aboard Toxic Airlines contained evidence that vital data was withheld from a 1999-2000 Australian Senate Inquiry into the health and flight safety issues relating to oil fumes on the BAe 146. The film also contains an Australian Senator speech about money being paid by BAe for silence on the fumes issue.

</doc>
<doc id="47667" url="http://en.wikipedia.org/wiki?curid=47667" title="Marconi Electronic Systems">
Marconi Electronic Systems

Marconi Electronic Systems (MES), or GEC-Marconi as it was until 1998, was the defence arm of The General Electric Company (GEC). It was demerged from GEC and acquired by British Aerospace (BAe) on 30 November 1999 to form BAE Systems. GEC then renamed itself Marconi plc.
MES exists today as BAE Systems Electronics Limited, a subsidiary of BAE Systems, but the assets were rearranged elsewhere within that company. MES-related businesses include BAE Systems Submarine Solutions, BAE Systems Surface Ships, BAE Systems Insyte and Selex ES (now a part of Finmeccanica).
History.
MES represented the pinnacle of GEC's defence businesses which had a heritage of almost 100 years. Following GEC's acquisition of Marconi as part of English Electric in 1968 the Marconi brand was used for its defence businesses e.g. Marconi Space & Defence Systems (MSDS), Marconi Underwater Systems Ltd (MUSL). GEC's history of military products dates back to World War I with its contribution to the war effort then including radios and bulbs. World War II consolidated this position with the company involved in many important technological advances, most notably radar.
Between 1945 and GEC's demerger of its defence business in 1999, the company became one of the world's most important defence contractors. GEC's major defence related acquisitions included Associated Electrical Industries in 1967, English Electric Company (including its Marconi subsidiary) in 1968, Yarrow Shipbuilders in 1985, parts of Ferranti's defence business in 1990, Vickers Shipbuilding and Engineering in 1995 and Kvaerner Govan in 1999. In June 1998, MES acquired Tracor, a major American defence contractor, for $1.4bn.
Demerger.
The 1997 merger of American corporations Boeing and McDonnell Douglas, which followed the forming of Lockheed Martin, the world's largest defence contractor in 1995, increased the pressure on European defence companies to consolidate. In June 1997 British Aerospace Defence Managing Director John Weston commented "Europe... is supporting three times the number of contractors on less than half the budget of the U.S.". European governments wished to see the merger of their defence manufacturers into a single entity, a European Aerospace and Defence Company.
As early as 1995 British Aerospace and the German aerospace and defence company DaimlerChrysler Aerospace (DASA) were said to be keen to create a transnational aerospace and defence company. Merger discussions began between British Aerospace and DASA in July 1998. A merger was agreed between British Aerospace Chairman Richard Evans and DASA CEO Jürgen Schrempp in December 1998.
GEC was also under pressure to participate in defence industry consolidation. Reporting the appointment of George Simpson as GEC managing director in 1996, "The Independent" had said "some analysts believe that Mr Simpson's inside knowledge of BAe, a long-rumoured GEC bid target, was a key to his appointment. GEC favours forging a national 'champion' defence group with BAe to compete with the giant US organisations." When GEC put MES up for sale on 22 December 1998, BAE abandoned the DASA merger in favour of purchasing its British rival. The merger of British Aerospace and MES was announced on 19 January 1999. Evans stated that in 2004 that his fear was that an American defence contractor would acquire MES and challenge both British Aerospace and DASA. The merger created a vertically integrated company which "The Scotsman" described as "[a combination of British Aerospace's] contracting and platform-building skills with Marconi's coveted electronics systems capability". for example combining the manufacturer of the Eurofighter with the company that provided many of the aircraft's electronic systems; British Aerospace was MES' largest customer. In contrast, DASA's response to the breakdown of the merger discussion was to merge with Aérospatiale to create the European Aeronautic Defence and Space Company (EADS), a horizontal integration. EADS has since considered a merger with Thales to create a "fully rounded" company.
While MES was responsible for the majority of GEC's defence sales other GEC companies achieved defence related sales, principally GEC Alsthom, GEC-Plessey Telecommunications (GPT) and GEC Plessey Semiconductors.
Major projects.
"This is a partial list:"

</doc>
<doc id="47668" url="http://en.wikipedia.org/wiki?curid=47668" title="AviaBellanca Aircraft">
AviaBellanca Aircraft

AviaBellanca Aircraft Corporation is an American aircraft design and manufacturing company. Prior to 1983 it was known as the Bellanca Aircraft Company. The company was founded in 1927 by Giuseppe Mario Bellanca.
History.
After Giuseppe Mario Bellanca, the designer and builder of Italy's first aircraft, came to the United States in 1911, he began to design aircraft for a number of firms, including the Maryland Pressed Steel Company, Wright Aeronautical Corporation and the Columbia Aircraft Corporation. Bellanca founded his own company, Bellanca Aircraft Corporation of America, in 1927, sited first in Richmond Hill, New York and moving in 1928 to New Castle (Wilmington), Delaware. In the 1920s and 1930s, Bellanca's aircraft of his own design were known for their efficiency and low operating cost, gaining fame for world record endurance and distance flights. Lindbergh's first choice for his New York to Paris flight was a Bellanca WB-2. The company's insistence on selecting the crew drove Lindbergh to Ryan.
Bellanca remained President and Chairman of the Board from the corporation's inception on the last day of 1927 until he sold the company to L. Albert and Sons in 1954. From that time on, the Bellanca line was part of a succession of companies that maintained the lineage of the original aircraft produced by Bellanca.
Aircraft.
First Flight - Model / Military number - Name
References.
</dl>

</doc>
<doc id="47669" url="http://en.wikipedia.org/wiki?curid=47669" title="Church of Domine Quo Vadis">
Church of Domine Quo Vadis

The Church of St Mary in Palmis (Italian: "Chiesa di Santa Maria delle Piante", Latin: "Sanctae Mariae in Palmis"), better known as Chiesa del Domine Quo Vadis, is a small church southeast of Rome, central Italy. It is located about some 800 m from Porta San Sebastiano, where the Via Ardeatina branches off the Appian Way, on the site where, according to the legend, Saint Peter met Jesus while the former was fleeing persecution in Rome. According to the apocryphal "Acts of Peter", Peter asked Jesus, "Lord, where are you going?" (Latin: "Domine, quo vadis?"). Jesus answered, "I am going to Rome to be crucified again" (Latin: "Eo Romam iterum crucifigi").
History.
There has been a sanctuary on the spot since the ninth century, but the current church is from 1637. The current façade was added in the 17th century.
It has been supposed that the sanctuary might have been even more ancient, perhaps a Christian adaption of some already existing temple: the church is in fact located just in front of the sacred "campus" dedicated to Rediculus, the Roman "God of the Return". This "campus" hosted a sanctuary for the cult of the deity that received devotion by travellers before their departure, especially by those who were going to face long and dangerous journeys to far places like Egypt, Greece or the East. Those travellers who returned also stopped to thank the god for the happy outcome of their journey.
The presence of the Apostle Peter in this area, where he is supposed to have lived, appears to be confirmed in an epigraph in the Catacombs of Saint Sebastian that reads "Domus Petri" (English: House of Peter). An epigram by Pope Damasus I (366–384) in honor of Peter and Paul reads: "You that are looking for the names of Peter and Paul, you must know that the saints have lived here."
The two footprints on a marble slab at the center of the church — nowadays a copy of the original, which is kept in the nearby Basilica of San Sebastiano fuori le mura — are popularly held to be a miraculous sign left by Jesus. It is to these footprints that the official name of the church alludes: "palmis" refers to the soles of Jesus' feet. It is likely that these footprints are actually the draft of an ancient Roman "ex voto", a tribute paid to the gods for the good outcome of a journey.
There was an inscription above the front door on the church's façade which used to say: "Stop your walking, traveller, and enter this sacred temple in which you will find the footprint of our Lord Jesus Christ when He met with St. Peter who escaped from the prison. An alms for the wax and the oil is recommended in order to free some spirits from Purgatory." Pope Gregory XVI found the advertising tone of this inscription so inappropriate that he ordered its removal in 1845.
The church is currently administered by priests of the Congregation of Saint Michael the Archangel.

</doc>
<doc id="47671" url="http://en.wikipedia.org/wiki?curid=47671" title="Toxic heavy metal">
Toxic heavy metal

A toxic heavy metal is any relatively dense metal or metalloid that is noted for its potential toxicity, especially in environmental contexts. The term has particular application to cadmium, mercury, lead and arsenic, all of which appear in the World Health Organisation's list of 10 chemicals of major public concern. Other examples include chromium, cobalt, nickel, copper, zinc, selenium, silver, antimony and thallium.
Toxic heavy metals are found naturally in the earth, and become concentrated as a result of human caused activities. They enter plant, animal and human tissues via inhalation, diet and manual handling, and can bind to, and interfere with the functioning of vital cellular components. The toxic effects of arsenic, mercury and lead were known to the ancients but methodical studies of the toxicity of some heavy metals appear to date from only 1868. In humans, heavy metal poisoning is generally treated by the administration of chelating agents. Some elements regarded as toxic heavy metals are essential, in small quantities, for human health.
Contamination sources.
Toxic heavy metals are found naturally in the earth, and become concentrated as a result of human caused activities. Common sources are from mining and industrial wastes; vehicle emissions; lead-acid batteries; fertilisers; paints; treated woods; plastics floating on the world's oceans; and aging water supply infrastructure. Lead is the most prevalent toxic heavy metal contaminant. As a component of tetra-ethyl lead it was used extensively in gasoline during the 1930s-1970s. Lead levels in the aquatic environments of industrialised societies have been estimated to be two to three times those of pre-industrial levels. Although the use of leaded gasoline was largely phased out in North America by 1996, soils next to roads built before this time retain high lead concentrations. Lead (from lead azide or lead styphnate used in firearms) gradually accumulates at firearms training grounds, contaminating the local environment and exposing range employees to a risk of lead poisoning.
Entry routes.
Toxic heavy metals enter plant, animal and human tissues via air inhalation, diet and manual handling. Motor vehicle emissions are a major source of airborne contaminants including arsenic, cadmium, cobalt, nickel, lead, antimony, vanadium, zinc, platinum, palladium and rhodium. Water sources (groundwater, lakes, streams and rivers) can be polluted by toxic heavy metals leaching from industrial and consumer waste; acid rain can exacerbate this process by releasing toxic heavy metals trapped in soils. Plants are exposed to toxic heavy metals through the uptake of water; animals eat these plants; ingestion of plant- and animal-based foods are the largest sources of toxic heavy metals in humans. Absorption through skin contact, for example from contact with soil, is another potential source of toxic heavy metal contamination. Toxic heavy metals can bioaccumulate in organisms as they are hard to metabolize (process and eliminate).
Detrimental effects.
Toxic heavy metals "can bind to vital cellular components, such as structural proteins, enzymes, and nucleic acids, and interfere with their functioning." Symptoms and effects can vary according to the metal or metal compound, and the dose involved. Broadly, long-term exposure to toxic heavy metals can have carcinogenic, central and peripheral nervous system and circulatory effects. For humans, typical presentations associated with exposure to any of the "classical" toxic heavy metals, or chromium (another toxic heavy metal) or arsenic (a metalloid), are shown in the table.
History.
The toxic effects of arsenic, mercury and lead were known to the ancients but methodical studies of the overall toxicity of heavy metals appear to date from only 1868. In that year, Wanklyn and Chapman speculated on the adverse effects of the heavy metals "arsenic, lead, copper, zinc, iron and manganese" in drinking water. They noted an "absence of investigation" and were reduced to "the necessity of pleading for the collection of data." In 1884, Blake described an apparent connection between toxicity and the atomic weight of an element. The following sections provide historical thumbnails for the "classical" toxic heavy metals (arsenic, mercury and lead) and some more recent examples (chromium and cadmium).
Arsenic.
Arsenic, as realgar (As4S4) and orpiment (As2S3), was known in ancient times. Strabo (64–50 BCE – c. AD 24?), a Greek geographer and historian, wrote that only slaves were employed in realgar and orpiment mines since they would inevitably die from the toxic effects of the fumes given off from the ores. Arsenic contaminated beer poisoned over 6,000 people in the Manchester area of England in 1900, and is thought to have killed at least 70 victims. Clare Luce, American ambassador to Italy from 1953 to 1956, suffered from arsenic poisoning. Its source was traced to flaking arsenic-laden paint on the ceiling of her bedroom. She may also have eaten food contaminated by arsenic in flaking ceiling paint in the embassy dining room. Ground water contaminated by arsenic, as of 2014, "is still poisoning millions of people in Asia."
Mercury.
The first emperor of unified China, Qin Shi Huang, it is reported, died of ingesting mercury pills that were intended to give him eternal life. The phrase "mad as a hatter" is likely a reference to mercury poisoning among milliners (so-called "mad hatter disease"), as mercury-based compounds were once used in the manufacture of felt hats in the 18th and 19th century. Historically, gold amalgam (an alloy with mercury) was widely used in gilding, leading to numerous casualties among the workers. It is estimated that during the construction of Saint Isaac's Cathedral alone, 60 workers died from the gilding of the main dome. Outbreaks of methylmercury poisoning occurred in several places in Japan during the 1950s due to industrial discharges of mercury into rivers and coastal waters. The best-known instances were in Minamata and Niigata. In Minamata alone, more than 600 people died due to what became known as Minamata disease. More than 21,000 people filed claims with the Japanese government, of which almost 3000 became certified as having the disease. In 22 documented cases, pregnant women who consumed contaminated fish showed mild or no symptoms but gave birth to infants with severe developmental disabilities. Since the industrial Revolution, mercury levels have tripled in many near-surface seawaters, especially around Iceland and Antarctica.
Lead.
The adverse effects of lead were known to the ancients. In the 2nd century BC the Greek botanist Nicander described the colic and paralysis seen in lead-poisoned people. Dioscorides, a Greek physician who is thought to have lived in the 1st century CE, wrote that lead "makes the mind give way". Lead was used extensively in Roman aqueducts from about 500 BC to 300 AD. Julius Caesar's engineer, Vitruvius, reported, "water is much more wholesome from earthenware pipes than from lead pipes. For it seems to be made injurious by lead, because white lead is produced by it, and this is said to be harmful to the human body." During the 17th and 18th centuries, people in Devon were afflicted by a condition referred to as Devon colic; this was discovered to be due to the imbibing of lead-contaminated cider. In 2013, the World Health Organization estimated that lead poisoning resulted in 143,000 deaths, and "contribute[d] to 600,000 new cases of children with intellectual disabilities", each year. In 2015, drinking water lead levels in north-eastern Tasmania, Australia, were reported to reach over 50 times national drinking water guidelines. The source of the contamination was attributed to “a combination of dilapidated drinking water infrastructure, including lead jointed pipelines, end-of-life polyvinyl chloride pipes and household plumbing.”
Chromium.
Chromium(III) compounds and chromium metal are not considered a health hazard, while the toxicity and carcinogenic properties of chromium(VI) have been known since at least the late 19th century. In 1890, Newman described the elevated cancer risk of workers in a chromate dye company. Chromate-induced dermatitis was reported in aircraft workers during World War II. In 1963, an outbreak of dermatitis, ranging from erythema to exudative eczema, occurred amongst 60 automobile factory workers in England. The workers had been wet-sanding chromate-based primer paint that had been applied to car bodies. In Australia, chromium was released from the Newcastle Orica explosives plant on August 8, 2011. Up to 20 workers at the plant were exposed as were 70 nearby homes in Stockton. The town was only notified three days after the release and the accident sparked a major public controversy, with Orica criticised for playing down the extent and possible risks of the leak, and the state Government attacked for their slow response to the incident.
Cadmium.
Cadmium exposure is a phenomenon of the early 20th century, and onwards. In Japan in 1910, the Mitsui Mining and Smelting Company began discharging cadmium into the Jinzugawa river, as a byproduct of mining operations. Residents in the surrounding area subsequently consumed rice grown in cadmium contaminated irrigation water. They experienced softening of the bones and kidney failure. The origin of these symptoms was not clear; possibilities raised at the time included "a regional or bacterial disease or lead poisoning." In 1955, cadmium was identified as the likely cause and in 1961 the source was directly linked to mining operations in the area. In February 2010, cadmium was found in Wal-Mart exclusive Miley Cyrus jewelry. Wal-Mart continued to sell the jewelry until May, when covert testing organised by Associated Press confirmed the original results. In June 2010 cadmium was detected in the paint used on promotional drinking glasses for the movie Shrek Forever After, sold by McDonald's Restaurants, triggering a recall of 12 million glasses.
Remediation.
In humans, heavy metal poisoning is generally treated by the administration of chelating agents.
These are chemical compounds, such as CaNa2 EDTA (calcium disodium ethylenediaminetetraacetate) that convert heavy metals to chemically inert forms that can be excreted without further interaction with the body. Chelates are not without side effects and can also remove beneficial metals from the body. Vitamin and mineral supplements are sometimes co-administered for this reason.
Soils contaminated by toxic heavy metals can be re-mediated by one or more of the following technologies: isolation; immobilization; toxicity reduction; physical separation; or extraction. "Isolation" involves the use of caps, membranes or below-ground barriers in an attempt to quarantine the contaminated soil. "Immobilization" aims to alter the properties of the soil so as to hinder the mobility of the heavy contaminants. "Toxicity reduction" attempts to oxidise or reduce the toxic heavy metal ions, via chemical or biological means into less toxic or mobile forms. "Physical separation" involves the removal of the contaminated soil and the separation of the metal contaminants by mechanical means. "Extraction" is an on or off-site process that uses chemicals, high-temperature volatization, or electrolysis to extract contaminants from soils. The process or processes used will vary according to contaminant and the characteristics of the site.
Benefits.
Some elements regarded as toxic heavy metals are essential, in small quantities, for human health. These elements include vanadium, manganese, iron, cobalt, copper, zinc, selenium, strontium and molybdenum. A deficiency of these essential metals may increase susceptibility to heavy metal poisoning.
References.
</dl>

</doc>
<doc id="47672" url="http://en.wikipedia.org/wiki?curid=47672" title="Beriev">
Beriev

The Beriev Aircraft Company, formerly Beriev Design Bureau, is a Russian aircraft manufacturer (design office prefix Be), specializing in amphibious aircraft. The company was founded in Taganrog in the 1934 as OKB-49 by Georgy Mikhailovich Beriev (born February 13, 1903), and since that time has designed and produced more than 20 different models of aircraft for civilian and military purposes, as well as customized models. Today the Company employs some 3000 specialists and is developing and manufacturing amphibious aircraft.
Pilots flying Beriev seaplanes have broken 228 world aviation records. The records are registered and acknowledged by the Fédération Aéronautique Internationale. In November 1989 BERIEV Aircraft Company became the only defense industry enterprise to win the Prize for Quality awarded by the Government of Russia.
In mid-2002, Irkut raised its 40 percent holding in the Beriev Design Bureau to a controlling stake.

</doc>
<doc id="47673" url="http://en.wikipedia.org/wiki?curid=47673" title="Aermacchi">
Aermacchi

Aermacchi was an Italian aircraft manufacturer. Formerly known as Aeronautica Macchi, the company was founded in 1912 by Giulio Macchi at Varese in north-western Lombardy. With a factory located on the shores of Lake Varese, the firm originally manufactured seaplanes.
After the Second World War, the company began producing motorcycles as a way to fill the post-war need for cheap, efficient transportation. In 1960, US business motorcycles purchased 50% of Aermacchi's motorcycle division.
The remaining motorcycle holdings were sold in 1974 to AMF-Harley Davidson, with motorcycles continuing to be made at Varese. The business was sold to Cagiva in 1978.
The company then specialised in civil and military pilot training aircraft. In July 2003, Aermacchi was integrated into the Finmeccanica Group as Alenia Aermacchi, which increased its shareholding to 99%.
Military trainers.
Since the beginning, the design and production of military trainers have been Alenia Aermacchi's core business.
The products include: 
Military collaboration.
Alenia Aermacchi has cooperated in international military programs:
Alenia Aermacchi takes part in the AMX program with Alenia Aeronautica and Embraer of Brazil with a total share of 24%. Alenia Aermacchi develops and manufactures the fuselage forward and rear sections and installs some avionic equipment in the aircraft. A Mid-Life Updating program is required by the Italian Air Force to upgrade the aircraft capabilities.
Alenia Aermacchi designs and produces wing pylons and wing tips, roots, trailing edges and flaps, which represents a 5% share in the overall program.
Alenia Aermacchi has a share of more than 4% in the Eurofighter program, for the design and development of wing pylons, twin missile and twin store carriers, ECM pods, carbon fiber structures and titanium engine cowlings.
After participating in the G-222 transport aircraft program, the company is involved in the new Military Transport Aircraft C-27J Spartan, for the production of outer wings.
Civil programs.
Since the mid-1990s, Alenia Aermacchi has participated in programs for the supply of engine nacelles for civil aircraft. It produces cold parts for engine nacelles: inlets, fan cowls and EBU, the systems-to-engine interface. 
In 1999, the company established a joint venture (MHD) with Hurel-Dubois (presently Hurel-Hispano, of SNECMA group), a French company specializing in the development and manufacture of thrust reversers, to obtain the full responsibility for the development of nacelles installed on maximum 100-seat aircraft.

</doc>
<doc id="47674" url="http://en.wikipedia.org/wiki?curid=47674" title="Blackburn Aircraft">
Blackburn Aircraft

Blackburn Aircraft Limited was a British aircraft manufacturer that concentrated mainly on naval and maritime aircraft during the first part of the 20th century.
History.
Blackburn Aircraft was founded by Robert Blackburn, who built his first aircraft in Leeds in 1908.
The Blackburn Aeroplane & Motor Company was created in 1914, established in a new factory built at Brough, East Riding of Yorkshire in 1916, where Robert's brother Norman Blackburn was later Managing Director. By acquiring the Cirrus-Hermes company in 1937, Blackburn started producing aircraft engines, the Blackburn Cirrus range.
By 1937, pressure to re-arm was growing and the Yorkshire factory was approaching capacity. A fortuitous friendship between Maurice Denny, managing director of Denny Bros., the Dumbarton ship building company, and Robert Blackburn resulted in the building of a new Blackburn factory at Barge Park, Dumbarton where production of the Blackburn Botha commenced in 1939.
The company's name was changed to Blackburn Aircraft Limited in 1939, and the company amalgamated with General Aircraft Limited in 1949 as Blackburn and General Aircraft Limited, reverting to Blackburn Aircraft Limited by 1958.
As part of the rationalisation of British aircraft manufacturers, its aircraft production and engine operations were absorbed into Hawker Siddeley and Bristol Siddeley respectively. The Blackburn name was dropped completely in 1963.
An American company, Blackburn Aircraft Corp., was incorporated in Detroit on 20 May 1929 to acquire design and patent rights of the aircraft of Blackburn Airplane & Motor Co., Ltd. in the USA. It was owned 90% by Detroit Aircraft Corp. and 10% by Blackburn Airplane & Motor Co., Ltd. Agreements covered such rights in North and South America, excepting Brazil and certain rights in Canada and provided that all special tools and patterns were to be supplied by the UK company at cost.

</doc>
<doc id="47676" url="http://en.wikipedia.org/wiki?curid=47676" title="Data (Star Trek)">
Data (Star Trek)

Lieutenant Commander Data ( ) is a character in the fictional "Star Trek" universe portrayed by actor Brent Spiner. He appears in the television series ' and the feature films "Star Trek Generations", ', ' and '.
An artificial intelligence and synthetic life form designed and built by Doctor Noonien Soong, Data is a self-aware, sapient, sentient, and anatomically fully functional android who serves as the second officer and chief operations officer aboard the Federation starships USS "Enterprise"-D and USS "Enterprise"-E. His positronic brain allows him impressive computational capabilities. Data experienced ongoing difficulties during the early years of his life with understanding various aspects of human behavior and was unable to feel emotion or understand certain human idiosyncrasies, inspiring him to strive for his own humanity. This goal eventually led to the addition of an "emotion chip", also created by Soong, to Data's positronic net. Although Data's endeavor to increase his humanity and desire for human emotional experience is a significant plot point (and source of humor) throughout the series, he consistently shows a nuanced sense of wisdom, sensitivity, and curiosity, garnering immense respect from his peers and colleagues.
Data is in many ways a successor to the original ""‍ '​s Spock (Leonard Nimoy), in that the character offers an "outsider's" perspective on humanity, even briefly working with Spock in the two-part "Next Generation" episode, .
Development.
Gene Roddenberry told Brent Spiner that over the course of the series, Data was to become "more and more like a human until the end of the show, when he would be very close, but still not quite there. That was the idea and that's the way that the writers took it." Spiner felt that Data exhibited the Chaplinesque characteristics of a sad, tragic clown. To get into his role as Data, Spiner used the character of Robby the Robot from the film "Forbidden Planet" as a role model.
Commenting on Data's perpetual albino-like appearance, he said: "I spent more hours of the day in make-up than out of make-up", so much so that he even called it a way of method acting. Spiner also portrayed Data's manipulative and malignant brother Lore (a role he found much easier to play, because the character was "more like me"), and Data's creator, Dr. Noonien Soong. Additionally, he portrayed another Soong-type android, B-4, in the film "Star Trek Nemesis", and also one of Soong's ancestors in three episodes of ". Spiner said his favorite Data scene takes place in ", when Data plays poker on the holodeck with a re-creation of the famous physicist Stephen Hawking, played by Hawking himself.
Spiner reprised his role of Data in the "" series finale "These Are the Voyages..." in an off-screen speaking part. Spiner felt that he had visibly aged out of the role and that Data was best presented as a youthful figure.
Depiction.
Television series and films.
Dialog in "Datalore" establishes some of Data's backstory. It is stated that he was deactivated in 2336 on Omicron Theta before an attack by the Crystalline Entity, a spaceborne creature which converts life forms to energy for sustenance. He was found and reactivated by Starfleet personnel two years later. Data went to Starfleet Academy from 2341–45 (he describes himself as "Class of '78" to Riker in "Encounter at Farpoint", but that may refer to the stardate and not the year that he graduated) and then served in Starfleet aboard the USS "Trieste". He was assigned to the "Enterprise" under Captain Jean-Luc Picard in 2364. In "Datalore", Data discovers his amoral brother, Lore, and learns that Lore, not he, was the first android constructed by Soong. Lore fails in an attempt to betray the "Enterprise" to the Crystalline Entity, and Wesley Crusher beams Data's brother into space at the episode's conclusion.
In ", Data reunites with Dr. Soong (also portrayed by Spiner). There he meets again with Lore, who steals the emotion chip Soong meant for Data to receive. Lore then fatally wounds Soong. Lore returns in the two-part episode ", using the emotion chip to control Data and make him help with Lore's attempt to make the Borg entirely artificial lifeforms. Data eventually deactivates Lore, and recovers, but does not install the damaged emotion chip.
In "", a Starfleet judge rules that Data is not Starfleet property. The episode establishes that Data has a storage capacity of 800 quadrillion bits, (88.81784197 PiB) and a total linear computational speed of 60 trillion operations per second.
Data's family is expanded in ", which introduces Lal, a robot based on Data's neural interface and who Data refers to as his daughter. Lal dies shortly after activation. Later, his mother Julianna appears in the episode " and reunites with Data, though the crew discovers she was an android duplicate built by Soong after the real Julianna's death, programmed to die after a long life, and to believe she is the true Julianna, unaware of the fact she is an android. Faced with the decision, Data chooses not to disclose this to her and allow her the chance to continue on with her normal life.
In ", the two-hour concluding episode of "The Next Generation", Captain Picard travels between three different time periods. The Picard of 25 years into the future goes with La Forge to seek advice from Professor Data, a luminary physicist who holds the Lucasian Chair at Cambridge University.
Although several androids, robots, and artificial intelligences were seen in the original "Star Trek" series, Data was often referred to as being unique in the galaxy as being the only sentient android known to exist (save the other androids created by Soong).
In the film "Star Trek Generations", Data finally installs the emotion chip he retrieved from Lore, and experiences the full scope of emotions. However, those emotions proved difficult to control and Data struggled to master them. However, by the events of ", Data managed to gain complete control of the chip, which includes deactivating it to maintain his performance efficiency.
In the film ", Data beams Picard off an enemy ship before destroying it, sacrificing himself, saving the captain and crew of the Enterprise. However, Data previously copied his core memories into B-4, Data's lost brother who is introduced in the movie. This was done with the reluctant help of Geordi LaForge who voiced concerns about how this could cause B-4 to be nothing more than an exact duplicate of Data.
Spin-off works.
In the comic book miniseries "" (the official prequel to the reboot "Star Trek" film), Data, having successfully transferred his positronic pathways and memories into B-4, now commands the "Enterprise"-E in 2387 in its mission to stop the Romulan Nero. Spock compares Data's "resurrection" with his own and years earlier.
In the novels published by Pocket Books and set after "Nemesis", Data returned in 2384 by having his memories and neural net transferred from B-4 into a new body which contained the memory engrams of Data's creator Doctor Noonian Soong after he was dying and being attacked by Lore years earlier. Data then takes control of the body after Soong deletes himself. After a tearful reunion with his old shipmates, Picard offers to reactivate Data's commission and to rejoin the crew but Data declines as he says he requires time. Several months later, with the help of the "Enterprise" crew, he is able to obtain the help necessary to resurrect his daughter, Lal. 
Data also appeared in the crossover graphic novel series "", set in 2368, in which the Borg Collective joins forces with the Cybermen when the latter invade their universe. Data and the crew of the "Enterprise"-D form an alliance of their own with the Eleventh Doctor-who immediately recognizes Data as an android upon seeing him-and his companions, Amy Pond and Rory Williams. The group later forms a reluctant truce with the Borg, who have been betrayed by the Cybermen and are in danger of falling to them. Data and the others manage to restore the Borg Collective and destroy the Cybermen, but their Borg liaison then attempts to seize control of the Doctor's TARDIS. The time machine's intelligence then briefly transfers itself into Data to escape the Borg's control, and the empowered Data overpowers the Borg and throws him out into the Time Vortex.
Characteristics.
Data is immune to nearly all biological diseases and other weaknesses that can affect humans and other carbon-based lifeforms. This benefits the "Enterprise" many times, such as when Data is the and . One exception however was in the episode "The Naked Now" where Data was also a victim of the Tsiolkovsky polywater virus. Data does not require life support to function and does not register a bio-signature. The crew of the "Enterprise"-D must modify their scanners to detect positronic signals in order to locate and keep track of him on away-missions. Another unique feature of Data's construction is the ability to be dismantled and then re-assembled for later use. This is used as a plot element in the episode " where Data's head (an artifact excavated on Earth from the late 19th century) is reattached to his body after nearly 500 years. Another example is in the episode ", where Data intentionally damages his body to break a high-current electrical arc, and then Riker taking his head to engineering to solve an engine problem.
Data is vulnerable to technological hazards such as computer viruses, certain levels of energy discharges, ship malfunctions (when connected to the "Enterprise" main computer for experiments), remote control shutdown devices, or through use of his "off switch" located in-between his shoulder blades. Data has also been "possessed" through technological means such as: Ira Graves's transfer of consciousness into his neural net; Dr. Soong's "calling" him; and an alien library that placed several different personalities into him. Data cannot swim unless aided by his built in flotation device, yet he is waterproof and can perform tasks underwater without the need to surface. Data is also impervious to sensory tactile emotion such as pain or pleasure. In "" the Borg Queen grafted artificial skin to his forearm. Data was then able to feel pain when a Borg drone slashed at his arm, and pleasure when the Borg Queen blew on the skin's hair follicles. Despite being mechanical in nature, Data is treated as an equal member of the "Enterprise" crew. Being a mechanical construct, technicians such as Chief Engineer LaForge prove to be more appropriate to treat his mechanical or cognitive function failures than the ship's doctor. His positronic brain becomes deactivated, and then repaired and reactivated by Geordi on several occasions.
Data is physically the strongest member of the "Enterprise" crew and also is, in ability to process and calculate information rapidly, the most intelligent member. He is able to survive in atmospheres that most carbon-based life forms would consider inhospitable, including the lack of an atmosphere or the vacuum of space; however, as an android, he is the most emotionally challenged and, with the addition of Dr. Soong's emotions chip, the most emotionally unstable member of the crew. Before the emotions chip, Data was unable to grasp basic emotion and imagination, leading him to download personality subroutines into his programming when participating in holographic recreational activities (most notably during Dixon Hill and Sherlock Holmes holoprograms) and during romantic encounters (most notably with Tasha Yar and Jenna D'Sora). Yet none of those personalities are his own and are immediately put away at the conclusion of their usefulness. Also, the abilities of Data's hearing are explained in the episodes " and " where his hearing is more sensitive than a dog's and that he can identify several hundred different distinct sound patterns simultaneously, but for aesthetics purposes limits it to about ten. Throughout the series, Data develops a frequently humorous affinity for theatrical acting and singing. This is most definitively demonstrated in "" where Picard and Worf distract an erratically behaving Data by singing two parts of "A British Tar", compelling Data to sing the third part.
Because of Julianna Soong's inability to conceive children, Data has at least five robotic siblings (two of which are Lore and B-4). Later on, his "mother" is revealed also to be his positronic sister as the real Julianna Soong died and was replaced with an identical Soong-type android, the most advanced one that Dr. Soong was known to have built. Data constructed a daughter, which he named "Lal" in the episode "". This particular android exceeded her father in basic human emotion when she felt fear toward Starfleet's scientific interests in her. Eventually, this was the cause of a cascade failure in her neural net and she died as a result.
Spot.
Spot is Data's pet cat and a recurring character in the show. Spot appears in several episodes during "TNG"‍ '​s last four seasons, as well as in the feature films "Star Trek Generations" and "". She first appears in the episode "Data's Day".
Despite her name, Spot is not actually patterned with spots. Spot originally appears as a male Somali cat, but later appears as a female orange tabby cat, eventually giving birth to kittens ("TNG": ""). The authors of "The Star Trek Encyclopedia" jokingly speculate that these inconsistencies can be explained by the idea that Spot is a shape-shifter or victim of a transporter accident (depending on which edition of the "Encyclopedia" one reads).
Data creates several hundred food supplement variations for Spot and composes the poem "Ode to Spot" in the cat's honor (""). (The poem was actually written by Clay Dale, the visual effects artist.) A computer error which occurs later in the series (in the episode "A Fistful of Datas") causes some of the ship's food replicators to create only Spot's supplements and replaces portions of a play with the ode's text.
In "Genesis", the morphogenetic virus "Barclay's protomorphosis disease" temporarily mutates Spot into an iguana-like reptile. Spot's kittens are not affected, leading to the discovery of the mechanism and a cure for the virus.
Spot is notoriously unfriendly to most people other than Data. Commander William Riker once received serious scratches while trying to feed Spot ("). Geordi La Forge borrowed her to experience taking care of a cat, but she knocked over a vase and teapot and damaged his furniture ("). When Data asked Worf to take care of Spot, Worf proved to be allergic to her and sneezed in her face, angering her (""). However, she did get along with Lieutenant Reginald Barclay, so when Data had to leave on a mission at the same time Spot's kittens were due, he persuaded Barclay to take care of her ("Genesis"). After Data died, it was mentioned in a deleted scene of "" that Worf is now taking care of her on board the "Enterprise". In later "Star Trek" novels, Worf describes Spot as a "Great Warrior" who sees what she wants and takes it.
Reception.
Like Spock, Data became a sex symbol and Spiner's fan mail came mostly from women. He described the letters as "romantic mail" that was "really written to Data; he's a really accessible personality".
Robotics engineers regard Data (along with the Droids from the "Star Wars" movies) as the pre-eminent face of robots in the public's perception of their field. On April 9, 2008, Data was inducted into Carnegie Mellon University's Robot Hall of Fame during a ceremony at the Carnegie Science Center in Pittsburgh, Pennsylvania.
The Beat Fleet, a Croatian hip hop band, wrote a song called "Data" for their album "Galerija Tutnplok" dedicated to Data. The release of this album coincided with reruns of "Star Trek: The Next Generation" being shown on Croatian Radiotelevision. In 2005, the nerdcore group The Futuristic Sex Robotz released a song about Data entitled "The Positronic Pimp." Cuban-American musician Voltaire has also performed a song about Data entitled "The Sexy Data Tango" on his LP Banned on Vulcan and later his album BiTrektual.

</doc>
<doc id="47679" url="http://en.wikipedia.org/wiki?curid=47679" title="Hawker Pacific Aerospace">
Hawker Pacific Aerospace

Hawker Pacific Aerospace (HPA) is a MRO-Service (Maintenance, Repair and Overhaul) company which offers landing gear and hydraulic MRO services for all major aircraft types. These include all current Airbus, Boeing, Bombardier and Embraer models as well as Helicopters. 
Lufthansa Technik (LHT), a subsidiary of Lufthansa German Airlines, acquired Hawker Pacific in 2002. Within the LHT network Hawker is able to provide an around the world landing gear service.
HPA’s corporate headquarters located in Sun Valley, Los Angeles, California near the Bob Hope Burbank International Airport.
History.
Hawker Pacific Aerospace was formed in 1980. In 1991, Hawker Siddeley was absorbed by BTR Aerospace Group. In 1994, Hawker Pacific merged with Dunlop Aviation Inc and in 1996, Hawker Pacific Aerospace was sold by BTR and became a stand-alone company.
In 1998, Hawker Pacific completed its initial public offering of common stock and used the proceeds to acquire the landing gear, flap track and flap carriage operation from British Airways.
In 2002, Lufthansa Technik acquired 100% ownership of Hawker Pacific Aerospace including its UK location. Hawker was added to Lufthansa Technik's Landing Gear Division to form a global network for Landing Gear MRO services.
In January 2011, Hawker Pacific Aerospace UK was sold to Lufthansa Technik and renamed into Lufthansa Technik Landing Gear Services UK.
Business divisions.
Hawker Pacific Aerospace has three business divisions:

</doc>
<doc id="47682" url="http://en.wikipedia.org/wiki?curid=47682" title="599">
599

Year 599 (DXCIX) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar. The denomination 599 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="47683" url="http://en.wikipedia.org/wiki?curid=47683" title="598">
598

Year 598 (DXCVIII) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar. The denomination 598 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="47684" url="http://en.wikipedia.org/wiki?curid=47684" title="596">
596

Year 596 (DXCVI) was a leap year starting on Sunday (link will display the full calendar) of the Julian calendar. The denomination 596 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="47685" url="http://en.wikipedia.org/wiki?curid=47685" title="595">
595

Year 595 (DXCV) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar. The denomination 595 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="47687" url="http://en.wikipedia.org/wiki?curid=47687" title="Cosmic ray">
Cosmic ray

Cosmic rays are immensely high-energy radiation, mainly originating outside the Solar System. They may produce showers of secondary particles that penetrate and impact the Earth's atmosphere and sometimes even reach the surface. Composed primarily of high-energy protons and atomic nuclei, they are of mysterious origin. Data from the "Fermi" space telescope (2013) have been interpreted as evidence that a significant fraction of primary cosmic rays originate from the supernovae of massive stars. However, this is not thought to be their only source. Active galactic nuclei probably also produce cosmic rays.
The term "ray" is a historical accident, as cosmic rays were at first, and wrongly, thought to be mostly electromagnetic radiation. In common scientific usage high-energy particles with intrinsic mass are known as "cosmic" rays, and photons, which are quanta of electromagnetic radiation (and so have no intrinsic mass) are known by their common names, such as "gamma rays" or "X-rays", depending on their origin.
Cosmic rays attract great interest practically, due to the damage they inflict on microelectronics and life outside the protection of an atmosphere and magnetic field, and scientifically, because the energies of the most energetic ultra-high-energy cosmic rays (UHECRs) have been observed to approach 3 × 1020 eV, about 40 million times the energy of particles accelerated by the Large Hadron Collider. At 50 J, the highest-energy ultra-high-energy cosmic rays have energies comparable to the kinetic energy of a 90 kph baseball. As a result of these discoveries, there has been interest in investigating cosmic rays of even greater energies. Most cosmic rays, however, do not have such extreme energies; the energy distribution of cosmic rays peaks at 0.3 GeV.
Of primary cosmic rays, which originate outside of Earth's atmosphere, about 99% are the nuclei (stripped of their electron shells) of well-known atoms, and about 1% are solitary electrons (similar to beta particles). Of the nuclei, about 90% are simple protons, i. e. hydrogen nuclei; 9% are alpha particles, and 1% are the nuclei of heavier elements, called HZE ions. A very small fraction are stable particles of antimatter, such as positrons or antiprotons. The precise nature of this remaining fraction is an area of active research. An active search from Earth orbit for anti-alpha particles has failed to detect them.
History.
After the discovery of radioactivity by Henri Becquerel and Marie Curie in 1896, it was generally believed that atmospheric electricity, ionization of the air, was caused only by radiation from radioactive elements in the ground or the radioactive gases or isotopes of radon they produce. Measurements of ionization rates at increasing heights above the ground during the decade from 1900 to 1910 showed a decrease that could be explained as due to absorption of the ionizing radiation by the intervening air.
Discovery.
In 1909 Theodor Wulf developed an electrometer, a device to measure the rate of ion production inside a hermetically sealed container, and used it to show higher levels of radiation at the top of the Eiffel Tower than at its base. However, his paper published in "Physikalische Zeitschrift" was not widely accepted. In 1911 Domenico Pacini observed simultaneous variations of the rate of ionization over a lake, over the sea, and at a depth of 3 meters from the surface. Pacini concluded from the decrease of radioactivity underwater that a certain part of the ionization must be due to sources other than the radioactivity of the Earth. 
In 1912, Victor Hess carried three enhanced-accuracy Wulf electrometers to an altitude of 5300 meters in a free balloon flight. He found the ionization rate increased approximately fourfold over the rate at ground level. Hess ruled out the Sun as the radiation's source by making a balloon ascent during a near-total eclipse. With the moon blocking much of the Sun's visible radiation, Hess still measured rising radiation at rising altitudes. He concluded "The results of my observation are best explained by the assumption that a radiation of very great penetrating power enters our atmosphere from above." In 1913–1914, Werner Kolhörster confirmed Victor Hess' earlier results by measuring the increased ionization rate at an altitude of 9 km. Hess received the Nobel Prize in Physics in 1936 for his discovery.
The Hess balloon flight took place on 7 August 1912. By sheer coincidence, exactly 100 years later on 7 August 2012, the Mars Science Laboratory rover used its Radiation Assessment Detector (RAD) instrument to begin measuring the radiation levels on another planet for the first time. On 31 May 2013, NASA scientists reported that a possible manned mission to Mars may involve a greater radiation risk than previously believed, based on the amount of energetic particle radiation detected by the RAD on the Mars Science Laboratory while traveling from the Earth to Mars in 2011–2012.
Identification.
In the 1920s the term "cosmic rays" was coined by Robert Millikan who made measurements of ionization due to cosmic rays from deep under water to high altitudes and around the globe. Millikan believed that his measurements proved that the primary cosmic rays were gamma rays, i.e., energetic photons. And he proposed a theory that they were produced in interstellar space as by-products of the fusion of hydrogen atoms into the heavier elements, and that secondary electrons were produced in the atmosphere by Compton scattering of gamma rays. But then, in 1927, J. Clay found evidence, later confirmed in many experiments, of a variation of cosmic ray intensity with latitude, which indicated that the primary cosmic rays are deflected by the geomagnetic field and must therefore be charged particles, not photons. In 1929, Bothe and Kolhörster discovered charged cosmic-ray particles that could penetrate 4.1 cm of gold. Charged particles of such high energy could not possibly be produced by photons from Millikan's proposed interstellar fusion process.
In 1930, Bruno Rossi predicted a difference between the intensities of cosmic rays arriving from the east and the west that depends upon the charge of the primary particles – the so-called "east-west effect." Three independent experiments found that the intensity is, in fact, greater from the west, proving that most primaries are positive. During the years from 1930 to 1945, a wide variety of investigations confirmed that the primary cosmic rays are mostly protons, and the secondary radiation produced in the atmosphere is primarily electrons, photons and muons. In 1948, observations with nuclear emulsions carried by balloons to near the top of the atmosphere showed that approximately 10% of the primaries are helium nuclei (alpha particles) and 1% are heavier nuclei of the elements such as carbon, iron, and lead.
During a test of his equipment for measuring the east-west effect, Rossi observed that the rate of near-simultaneous discharges of two widely separated Geiger counters was larger than the expected accidental rate. In his report on the experiment, Rossi wrote "... it seems that once in a while the recording equipment is struck by very extensive showers of particles, which causes coincidences between the counters, even placed at large distances from one another." In 1937 Pierre Auger, unaware of Rossi's earlier report, detected the same phenomenon and investigated it in some detail. He concluded that high-energy primary cosmic-ray particles interact with air nuclei high in the atmosphere, initiating a cascade of secondary interactions that ultimately yield a shower of electrons, and photons that reach ground level.
Soviet physicist Sergey Vernov was the first to use radiosondes to perform cosmic ray readings with an instrument carried to high altitude by a balloon. On 1 April 1935, he took measurements at heights up to 13.6 kilometers using a pair of Geiger counters in an anti-coincidence circuit to avoid counting secondary ray showers.
Homi J. Bhabha derived an expression for the probability of scattering positrons by electrons, a process now known as Bhabha scattering. His classic paper, jointly with Walter Heitler, published in 1937 described how primary cosmic rays from space interact with the upper atmosphere to produce particles observed at the ground level. Bhabha and Heitler explained the cosmic ray shower formation by the cascade production of gamma rays and positive and negative electron pairs.
Energy distribution.
Measurements of the energy and arrival directions of the ultra-high energy primary cosmic rays by the techniques of "density sampling" and "fast timing" of extensive air showers were first carried out in 1954 by members of the Rossi Cosmic Ray Group at the Massachusetts Institute of Technology. The experiment employed eleven scintillation detectors arranged within a circle 460 meters in diameter on the grounds of the Agassiz Station of the Harvard College Observatory. From that work, and from many other experiments carried out all over the world, the energy spectrum of the primary cosmic rays is now known to extend beyond 1020 eV. A huge air shower experiment called the Auger Project is currently operated at a site on the pampas of Argentina by an international consortium of physicists, led by James Cronin, winner of the 1980 Nobel Prize in Physics from the University of Chicago, and Alan Watson of the University of Leeds. Their aim is to explore the properties and arrival directions of the very highest-energy primary cosmic rays. The results are expected to have important implications for particle physics and cosmology, due to a theoretical Greisen–Zatsepin–Kuzmin limit to the energies of cosmic rays from long distances (about 160 million light years) which occurs above 1020 eV because of interactions with the remnant photons from the big bang origin of the universe.
High-energy gamma rays (>50 MeV photons) were finally discovered in the primary cosmic radiation by an MIT experiment carried on the OSO-3 satellite in 1967. Components of both galactic and extra-galactic origins were separately identified at intensities much less than 1% of the primary charged particles. Since then, numerous satellite gamma-ray observatories have mapped the gamma-ray sky. The most recent is the Fermi Observatory, which has produced a map showing a narrow band of gamma ray intensity produced in discrete and diffuse sources in our galaxy, and numerous point-like extra-galactic sources distributed over the celestial sphere.
Sources of cosmic rays.
Early speculation on the sources of cosmic rays included a 1934 proposal by Baade and Zwicky suggesting cosmic rays originating from supernovae. A 1948 proposal by Horace W. Babcock suggested that magnetic variable stars could be a source of cosmic rays. Subsequently in 1951, Y. Sekido "et al." identified the Crab Nebula as a source of cosmic rays. Since then, a wide variety of potential sources for cosmic rays began to surface, including supernovae, active galactic nuclei, quasars, and gamma-ray bursts.
Later experiments have helped to identify the sources of cosmic rays with greater certainty. In 2009, a paper presented at the International Cosmic Ray Conference (ICRC) by scientists at the Pierre Auger Observatory showed ultra-high energy cosmic rays (UHECRs) originating from a location in the sky very close to the radio galaxy Centaurus A, although the authors specifically stated that further investigation would be required to confirm Cen A as a source of cosmic rays. However, no correlation was found between the incidence of gamma-ray bursts and cosmic rays, causing the authors to set upper limits as low as 3.4 × 10−6 erg cm−2 on the flux of 1 GeV-1 TeV cosmic rays from gamma-ray bursts.
In 2009, supernovae were said to have been "pinned down" as a source of cosmic rays, a discovery made by a group using data from the Very Large Telescope. This analysis, however, was disputed in 2011 with data from PAMELA, which revealed that "spectral shapes of [hydrogen and helium nuclei] are different and cannot be described well by a single power law", suggesting a more complex process of cosmic ray formation. In February 2013, though, research analyzing data from "Fermi" revealed through an observation of neutral pion decay that supernovae were indeed a source of cosmic rays, with each explosion producing roughly 3 × 1042 - 3 × 1043 J of cosmic rays. However, supernovae do not produce all cosmic rays, and the proportion of cosmic rays that they do produce is a question which cannot be answered without further study.
Types.
Cosmic rays originate as primary cosmic rays, which are those originally produced in various astrophysical processes. Primary cosmic rays are composed primarily of protons and alpha particles (99%), with a small amount of heavier nuclei (~1%) and an extremely minute proportion of positrons and antiprotons. Secondary cosmic rays, caused by a decay of primary cosmic rays as they impact an atmosphere, include neutrons, pions, positrons, and muons. Of these four, the latter three were first detected in cosmic rays.
Primary cosmic rays.
Primary cosmic rays primarily originate from outside the Solar System and sometimes even the Milky Way. When they interact with Earth's atmosphere, they are converted to secondary particles. The mass ratio of helium to hydrogen nuclei, 28%, is similar to the primordial elemental abundance ratio of these elements, 24%. The remaining fraction is made up of the other heavier nuclei that are nuclear synthesis end products, products of the Big Bang, primarily lithium, beryllium, and boron. These nuclei appear in cosmic rays in much greater abundance (~1%) than in the solar atmosphere, where they are only about 10−11 as abundant as helium. Cosmic rays made up of charged nuclei heavier than helium are called HZE ions. Due to the high charge and heavy nature of HZE ions, their contribution to an astronaut's radiation dose in space is significant even though they are relatively scarce.
This abundance difference is a result of the way secondary cosmic rays are formed. Carbon and oxygen nuclei collide with interstellar matter to form lithium, beryllium and boron in a process termed cosmic ray spallation. Spallation is also responsible for the abundances of scandium, titanium, vanadium, and manganese ions in cosmic rays produced by collisions of iron and nickel nuclei with interstellar matter.
Primary cosmic ray antimatter.
Satellite experiments have found evidence of positrons and a few antiprotons in primary cosmic rays, amounting to less than 1% of the particles in primary cosmic rays. These do not appear to be the products of large amounts of antimatter from the Big Bang, or indeed complex antimatter in the universe. Rather, they appear to consist of only these two elementary particles, newly made in energetic processes.
Preliminary results from the presently operating Alpha Magnetic Spectrometer ("AMS-02") on board the International Space Station show that positrons in the cosmic rays arrive with no directionality, and with energies that range from 10 to 250 GeV. In September, 2014, new results with almost twice as much data were presented in a talk at CERN and published in Physical Review Letters. A new measurement of positron fraction up to 500 GeV was reported, showing that positron fraction peaks at a maximum of about 16% of total electron+positron events, around an energy of 275 ± 32 GeV. At higher energies, up to 500 GeV, the ratio of positrons to electrons begins to fall again. The absolute flux of positrons also begins to fall before 500 GeV, but peaks at energies far higher than electron energies, which peak about 10 GeV. These results on interpretation have been suggested to be due to positron production in annihilation events of massive dark matter particles.
Cosmic ray antiprotons also have a much higher energy than their normal-matter counterparts (protons). They arrive at Earth with a characteristic energy maximum of 2 GeV, indicating their production in a fundamentally different process from cosmic ray protons, which on average have only one-sixth of the energy.
There is no evidence of complex antimatter atomic nuclei, such as antihelium nuclei (i.e., anti-alpha particles), in cosmic rays. These are actively being searched for. A prototype of the "AMS-02" designated "AMS-01", was flown into space aboard the Space Shuttle "Discovery" on STS-91 in June 1998. By not detecting any antihelium at all, the "AMS-01" established an upper limit of 1.1×10−6 for the antihelium to helium flux ratio.
Secondary cosmic rays.
When cosmic rays enter the Earth's atmosphere they collide with atoms and molecules, mainly oxygen and nitrogen. The interaction produces a cascade of lighter particles, a so-called air shower secondary radiation that rains down, including x-rays, muons, protons, alpha particles, pions, electrons, and neutrons. All of the produced particles stay within about one degree of the primary particle's path.
Typical particles produced in such collisions are neutrons and charged mesons such as positive or negative pions and kaons. Some of these subsequently decay into muons, which are able to reach the surface of the Earth, and even penetrate for some distance into shallow mines. The muons can be easily detected by many types of particle detectors, such as cloud chambers, bubble chambers or scintillation detectors. The observation of a secondary shower of particles in multiple detectors at the same time is an indication that all of the particles came from that event.
Cosmic rays impacting other planetary bodies in the Solar System are detected indirectly by observing high energy gamma ray emissions by gamma-ray telescope. These are distinguished from radioactive decay processes by their higher energies above  about 10 MeV.
Cosmic-ray flux.
The flux of incoming cosmic rays at the upper atmosphere is dependent on the solar wind, the Earth's magnetic field, and the energy of the cosmic rays. At distances of ~94 AU from the Sun, the solar wind undergoes a transition, called the termination shock, from supersonic to subsonic speeds. The region between the termination shock and the heliopause acts as a barrier to cosmic rays, decreasing the flux at lower energies (≤ 1 GeV) by about 90%. However, the strength of the solar wind is not constant, and hence it has been observed that cosmic ray flux is correlated with solar activity.
In addition, the Earth's magnetic field acts to deflect cosmic rays from its surface, giving rise to the observation that the flux is apparently dependent on latitude, longitude, and azimuth angle. The magnetic field lines deflect the cosmic rays towards the poles, giving rise to the aurorae.
The combined effects of all of the factors mentioned contribute to the flux of cosmic rays at Earth's surface. For 1 GeV particles, the rate of arrival is about 10,000 per square meter per second. At 1 TeV the rate is 1 particle per square meter per second. At 10 PeV there are only a few particles per square meter per year. Particles above 10 EeV arrive only at a rate of about one particle per square kilometer per year, and above 100 EeV at a rate of about one particle per square kilometer per century.
In the past, it was believed that the cosmic ray flux remained fairly constant over time. However, recent research suggests 1.5 to 2-fold millennium-timescale changes in the cosmic ray flux in the past forty thousand years.
The magnitude of the energy of cosmic ray flux in interstellar space is very comparable to that of other deep space energies: cosmic ray energy density averages about one electron-volt per cubic centimeter of interstellar space, or ~1 eV/cm3, which is comparable to the energy density of visible starlight at 0.3 eV/cm3, the galactic magnetic field energy density (assumed 3 microgauss) which is ~0.25 eV/cm3, or the cosmic microwave background (CMB) radiation energy density at ~ 0.25 eV/cm3.
Detection methods.
There are several ground-based methods of detecting cosmic rays currently in use. The first detection method is called the air Cherenkov telescope, designed to detect low-energy (<200 GeV) cosmic rays by means of analyzing their Cherenkov radiation, which for cosmic rays are gamma rays emitted as they travel faster than the speed of light in their medium, the atmosphere. While these telescopes are extremely good at distinguishing between background radiation and that of cosmic-ray origin, they can only function well on clear nights without the Moon shining, and have very small fields of view and are only active for a few percent of the time. Another Cherenkov telescope uses water as a medium through which particles pass and produce Cherenkov radiation to make them detectable.
Extensive air shower (EAS) arrays, a second detection method, measure the charged particles which pass through them. EAS arrays measure much higher-energy cosmic rays than air Cherenkov telescopes, and can observe a broad area of the sky and can be active about 90% of the time. However, they are less able to segregate background effects from cosmic rays than can air Cherenkov telescopes. EAS arrays employ plastic scintillators in order to detect particles.
Another method was developed by Robert Fleischer, P. Buford Price, and Robert M. Walker for use in high-altitude balloons. In this method, sheets of clear plastic, like 0.25 mm Lexan polycarbonate, are stacked together and exposed directly to cosmic rays in space or high altitude. The nuclear charge causes chemical bond breaking or ionization in the plastic. At the top of the plastic stack the ionization is less, due to the high cosmic ray speed. As the cosmic ray speed decreases due to deceleration in the stack, the ionization increases along the path. The resulting plastic sheets are "etched" or slowly dissolved in warm caustic sodium hydroxide solution, that removes the surface material at a slow, known rate. The caustic sodium hydroxide dissolves the plastic at a faster rate along the path of the ionized plastic. The net result is a conical etch pit in the plastic. The etch pits are measured under a high-power microscope (typically 1600x oil-immersion), and the etch rate is plotted as a function of the depth in the stacked plastic.
This technique yields a unique curve for each atomic nucleus from 1 to 92, allowing identification of both the charge and energy of the cosmic ray that traverses the plastic stack. The more extensive the ionization along the path, the higher the charge. In addition to its uses for cosmic-ray detection, the technique is also used to detect nuclei created as products of nuclear fission.
A fourth method involves the use of cloud chambers to detect the secondary muons created when a pion decays. Cloud chambers in particular can be built from widely available materials and can be constructed even in a high-school laboratory. A fifth method, involving bubble chambers, can be used to detect cosmic ray particles.
Another method detects the light from nitrogen fluorescence caused by the excitation of nitrogen in the atmosphere by the shower of particles moving through the atmosphere. This method allows for accurate detection of the direction from which the cosmic ray came.
Finally, the CMOS devices in pervasive smartphone cameras have been proposed as a practical distributed network to detect air showers from ultra-high energy cosmic rays (UHECRs) which is at least comparable with that of conventional cosmic ray detectors. The app, which is currently in beta and accepting applications, is CRAYFIS (Cosmic RAYs Found In Smartphones).
Effects.
Changes in atmospheric chemistry.
Cosmic rays ionize the nitrogen and oxygen molecules in the atmosphere, which leads to a number of chemical reactions. One of the reactions results in ozone depletion. Cosmic rays are also responsible for the continuous production of a number of unstable isotopes in the Earth's atmosphere, such as carbon-14, via the reaction:
Cosmic rays kept the level of carbon-14 in the atmosphere roughly constant (70 tons) for at least the past 100,000 years, until the beginning of above-ground nuclear weapons testing in the early 1950s. This is an important fact used in radiocarbon dating used in archaeology.
Role in ambient radiation.
Cosmic rays constitute a fraction of the annual radiation exposure of human beings on the Earth, averaging 0.39 mSv out of a total of 3 mSv per year (13% of total background) for the Earth's population. However, the background radiation from cosmic rays increases with altitude, from 0.3 mSv per year for sea-level areas to 1.0 mSv per year for higher-altitude cities, raising cosmic radiation exposure to a quarter of total background radiation exposure for populations of said cities. Airline crews flying long distance high-altitude routes can be exposed to 2.2 mSv of extra radiation each year due to cosmic rays, nearly doubling their total ionizing radiation exposure.
Effect on electronics.
Cosmic rays have sufficient energy to alter the states of circuit components in electronic integrated circuits, causing transient errors to occur, such as corrupted data in electronic memory devices, or incorrect performance of CPUs, often referred to as "soft errors" (not to be confused with software errors caused by programming mistakes/bugs). This has been a problem in electronics at extremely high-altitude, such as in satellites, but with transistors becoming smaller and smaller, this is becoming an increasing concern in ground-level electronics as well. Studies by IBM in the 1990s suggest that computers typically experience about one cosmic-ray-induced error per 256 megabytes of RAM per month. To alleviate this problem, the Intel Corporation has proposed a cosmic ray detector that could be integrated into future high-density microprocessors, allowing the processor to repeat the last command following a cosmic-ray event.
Cosmic rays are suspected as a possible cause of an in-flight incident in 2008 where an Airbus A330 airliner of Qantas twice plunged hundreds of feet after an unexplained malfunction in its flight control system. Many passengers and crew members were injured, some seriously. After this incident, the accident investigators determined that the airliner's flight control system had received a data spike that could not be explained, and that all systems were in perfect working order. This has prompted a software upgrade to all A330 and A340 airliners, worldwide, so that any data spikes in this system are filtered out electronically.
Significance to space travel.
Galactic cosmic rays are one of the most important barriers standing in the way of plans for interplanetary travel by crewed spacecraft.
Cosmic rays also pose a threat to electronics placed aboard outgoing probes. In 2010, a malfunction aboard the Voyager 2 space probe was credited to a single flipped bit, probably caused by a cosmic ray. Strategies such as physical or magnetic shielding for spacecraft have been considered in order to minimize the damage to electronics and human beings caused by cosmic rays.
Role in lightning.
Cosmic rays have been implicated in the triggering of electrical breakdown in lightning. It has been proposed that essentially all lightning is triggered through a relativistic process, "runaway breakdown", seeded by cosmic ray secondaries. Subsequent development of the lightning discharge then occurs through "conventional breakdown" mechanisms.
Postulated role in climate change.
A role of cosmic rays directly or via solar-induced modulations in climate change was suggested by Edward P. Ney in 1959 and by Robert E. Dickinson in 1975. Despite the opinion of over 97% of climate scientists against this notion, the idea has been revived in recent years, most notably by Henrik Svensmark, who has argued that because solar variations modulate the cosmic ray flux on Earth, they would consequently affect the rate of cloud formation and hence the climate. Nevertheless, it has been noted by climate scientists actively publishing in the field that Svensmark has inconsistently altered data on most of his published work on the subject, an example being adjustment of cloud data that understates error in lower cloud data, but not in high cloud data.
The 2007 IPCC synthesis report, however, strongly attributes a major role in the ongoing global warming to human-produced gases such as carbon dioxide, nitrous oxide, and halocarbons, and has stated that models including natural forcings only (including aerosol forcings, which cosmic rays are considered by some to contribute to) would result in far less warming than has actually been observed or predicted in models including anthropogenic forcings.
Svensmark, being one of several scientists outspokenly opposed to the mainstream scientific assessment of global warming, has found eminence among the popular culture movement that denies the scientific consensus. Despite this, Svensmark's work exaggerating the magnitude of the effect of GCR on global warming continues to be refuted in the mainstream science. For instance, a November 2013 study showed that less than 14 percent of global warming since the 1950s could be attributed to cosmic ray rate, and while the models showed a small correlation every 22 years, the cosmic ray rate did not match the changes in temperature, indicating that it was not a causal relationship.
Research and experiments.
There are a number of cosmic-ray research initiatives.
Further references.
</dl>

</doc>
<doc id="47691" url="http://en.wikipedia.org/wiki?curid=47691" title="Fork (chess)">
Fork (chess)

In chess, a fork is a tactic whereby a single piece makes two or more direct attacks simultaneously. Most commonly two pieces are threatened, which is also sometimes called a "double attack". The attacker usually aims to gain material by capturing one of the opponent's pieces. The defender often finds it difficult to counter two or more threats in a single move. The attacking piece is called the "forking" piece; the pieces attacked are said to be "forked". A piece that is defended can still be forked, if the forking piece has a lower value.
Besides attacking pieces, a target of a fork can be a direct mating threat (for example, attacking an unprotected knight while simultaneously setting up a battery of queen and bishop to threaten mate). Or a target can be an implied threat (for example, a knight may attack an unprotected piece while simultaneously threaten to fork queen and rook).
Forks are often used as part of a combination which may involve other types of chess tactics as well.
 
Forking piece.
The type of fork is named after the type of forking piece. For example, a "knight fork" is a knight move that attacks two or more opponent's pieces simultaneously. Any type of piece can perform a fork—including the king—and any type of piece can be forked. A fork is most effective when it is forcing, such as when the king is put in check.
Knights are often used for forks. Their unique L-shaped move means that they can attack any other type of piece, including the powerful Queen, without being attacked by their targets.
The queen is also often used to fork, but since the queen is usually more valuable than the pieces it attacks, this typically gains material only when the pieces attacked are undefended or if one is undefended and the opposing king is checked. The possibility of a "queen fork" is a very real threat when the queen is in the open, as is often the case in endgames. If a player wants to force an exchange of queens, forking the opposing queen and king (or an undefended piece) with a protected queen can be useful.
Pawns other than rook pawns (those on the a- and h-files) can also be used to fork by attacking two enemy pieces diagonally—one to the left, the other to the right.
Example from a game.
This example is from the first round of the FIDE World Chess Championship 2004 between Mohamed Tissir and Alexey Dreev. After 33... Nf2+ 34. Kg1 Nd3, White resigned. In the final position the black knight forks the white queen and rook; after the queen moves away, Black will win the exchange.
Example from an opening.
In the Two Knights Defense (1.e4 e5 2.Nf3 Nc6 3.Bc4 Nf6) after 4. Nc3, Black can eliminate White's e4-pawn immediately with 4... Nxe4! due to the "fork trick" 5. Nxe4 d5—regaining either the bishop or the knight. 
Escaping forks.
Forks can possibly be escaped. A forked piece such as the queen might check the enemy king, a zwischenzug, giving time to move the second forked piece to safety on the next move.
Other terms.
A fork of the king and queen, the highest material-gaining fork possible, is sometimes called a "royal fork". A fork of the opponent's king, queen, and one (or both) rooks is sometimes called a "grand fork". A knight fork of the opponent's king, queen, and possibly other pieces is sometimes called a "family fork" or "family check".

</doc>
<doc id="47692" url="http://en.wikipedia.org/wiki?curid=47692" title="Backyard Blitz">
Backyard Blitz

Backyard Blitz was a Logie Award winning Australian lifestyle and DIY television program that aired on the Nine Network between 2000 through to 2007 before its cancellation. It was hosted by Jamie Durie and was produced by Don Burke.
The show featured a very similar premise to the show "Ground Force", in which a team of gardeners employed by the show descend on a supposedly worthy individual's place and improve the garden for the cameras within a specified time limit. This similarity in fact led to legal action being taken by the rival Seven Network who at the time was set to debut an Australian version of "Ground Force".
The show like many of its other lifestyle brethren was mainly watched by older viewers and was widely derided by younger viewers and television critics. However it was a strong ratings performer.
On 14 November 2006 "Backyard Blitz" was axed by the Nine Network after seven years on air. Don Burke, whose own show "Burke's Backyard" was broadcast by Nine for nearly 18 years before it was axed in 2004, said his production company was "quite shocked by this decision". In mid-2007 Nine aired the six remaining unaired episodes that were filmed before the show was cancelled. In 2008, it aired a spin-off show Domestic Blitz hosted by Shelley Craft and Scott Cam
Presenters.
The four regular presenters on the show were landscaper Jamie Durie (the main host and the shows lead landscaper), Scott Cam (builder/carpenter), Nigel Ruck (landscaper) and Jody Rigby (horticulturist).
Awards.
During the shows run, it has won (and been nominated) for several Logie Awards. The show won six consecutive 'Most Popular Lifestyle Program' awards (2002–2006). It was nominated in the same category in 2007 but lost. Jamie Durie won the 'Most Popular New Male Talent' in 2001 and was nominated for 'Most Popular TV Presenter' in 2005 and 2006 for his role on the show.

</doc>
<doc id="47693" url="http://en.wikipedia.org/wiki?curid=47693" title="Jamie Durie">
Jamie Durie

Jamie Paul Durie OAM (born 3 June 1970) is an Australian-born award-winning international horticulturalist and landscape designer. He is founder and Director of Durie Design, a television host and producer, the author of 9 best-selling books and a passionate environmentalist and humanitarian.
Durie previously was the host of the Seven Network's "The Outdoor Room" and the US PBS series "The Victory Garden".
Early life.
Durie was born in Manly, an oceanside suburb in Sydney, to a Sri Lankan mother and Australian father. He spent most of his early childhood in the mining town of Tom Price in north Western Australia. During the 80's he was also working at night club Jamison Street in Sydney with the male review boys. A former model, Durie performed with the international cabaret group Manpower Australia in the early 1990s. It was during his tours with Manpower, and his creative roles within the production, that Durie began to develop his interest in garden design and architecture. He then formalised his passion by returning to study design and horticulture in Sydney for four years at the age of 26. Durie has received 33 International Design Awards since.
Television career.
Durie has hosted and featured in over 35 prime-time lifestyle shows and appeared on television shows all around the world. The top-rating television shows he has hosted or featured in include "The Outdoor Room", "Australia's Best Backyards", "The Block", "Backyard Blitz", "Torvil and Dean's Dancing on Ice", "Disney on Ice", "Cirque Du Soleil Specials", "White Room Challenge" and many more.
He has received seven Australian TV Logie awards including the Logie award for Most Popular New Male Talent and six consecutive Logie awards for hosting the Most Popular Australian Lifestyle Program for "Backyard Blitz".
Durie also appeared regularly on "The Oprah Winfrey Show" following his first appearance in November 2006. Durie appeared on a number of Winfrey's shows during his five-year contract to Harpo and publicly gives credit to Oprah for his USA career.
In the US, Durie's show is now in its fourth season. He has also hosted nine independent shows on HGTV including "HGTV Showdown", "HGTV Dream Home", "HGTV Green Home", "The Rose Parade" and currently hosts America's longest running gardening program on PBS, "The Victory Garden". Durie has also appeared in other top brand name programs in Australia and the USA including "Dancing with the Stars", "60 minutes", "Celebrity Millionaire", "The Price is Right", "Better Homes and Gardens", "Hard Copy", "Entertainment Tonight", "E news", "NBC Today Show", "Donahue", "Lifestyles of the Rich and Famous", and most recently was host and judge on the popular US series "Top Design" in Australia.
Books and online publishing.
Durie is the author of nine best selling books: (in order of release) "Patio—Garden Design and Inspiration", "The Outdoor Room", "Outside", "The Source Book" (Editions 1 and 2), "Outdoor Kids", "Inspired", "Jamie Durie's The Outdoor Room" and "100 Gardens".
Durie and his team produce and publish jamiedurie.com, jamieduriedesign.com and in January 2012 Durie released his own iPhone app, .
He is also founder and Editorial Director of "The Outdoor Room" magazine.
Landscape design.
After completing four years of study in horticulture and design, Durie founded the award-winning landscape design company Patio Landscape Architecture and Design in 1998. Patio was renamed in 2010.
With over 34 design awards to his credit, Durie has been invited to exhibit and compete at international garden shows around the world, winning gold medals in Australia, Singapore, Japan, New Zealand and the prestigious Chelsea Flower Show in the UK. He continues to focus on large-scale resort and hotel designs globally with designs in Australia, the US, Canada, Spain, Thailand, Indonesia, Singapore, Dubai, Bahrain, Hong Kong and The Caribbean. His latest large scale resort project involved designing and rejuvenating the landscape, furniture, sculpture and gardens at Hayman Island and its private estates on Australia's Great Barrier Reef, after they were devastated by tropical cyclones Anthony and Yasi.
Furniture and product design.
In 2003 Durie designed and launched a range of outdoor furniture and gardening products under the brand name "Patio by Jamie Durie". The range includes furniture, décor, gardening tools, lighting, textiles, BBQs, ornaments and accessories. All the timber products in the range are environmentally responsible FSC certified timber. Previously sold through Kmart it is now sold exclusively through Big W Australia.
Durie also designed the Jamie Durie Signature range of products in collaboration with other artists and designers, including a range of paints with Porters Paints, textiles with Cloth, Durie Design outdoor furniture range with , A Studio Design with Modern-Shed and a range of rugs with The Rug Collection.
Environment and charity.
Durie is a passionate advocate for environmental conservation. He was invited and then trained with former US Vice-President Al Gore as a Climate Change Presenter (now named Climate Reality), launched Clean up the World Day with The UN in New York, and is an Ambassador for Greenpeace, Australian Conservation Foundation, Planet Ark and National Tree Day. He also hosted the Australian Conservation Foundation's Spirituality and Sustainability Forum with His Holiness the Dalai Lama, and is an ambassador and former board member of the Royal Botanic Gardens Foundation, Sydney. Jamie is also an ambassador for the Forest Stewardship Council and Earth Hour.
Humanitarian work.
Durie has donated his time and resources to several charities, including PLAN International, The Children's Cancer Institute, FSHD, Sydney Children's Hospital and The Children's Hospital at Westmead, The Variety Club, Rotary, The Children First Foundation, Westmead Millenium Institute among others. He was executive producer and host of "Jamie's Journey – Hope for Uganda's Children" and "Jamie's Journey with the Children of India", both produced in conjunction with Plan International to raise awareness of HIV Aids in Uganda and highlight the challenges and successes of Plan's early childhood care programs in India..
Personal.
Durie's parents, Ron "Dave" and Joy (who was born in Sri Lanka), divorced when he was aged 10. He and his brother Chris moved to the Gold Coast, Queensland with their mother.
Durie has a daughter, Taylor (born 1997), from a relationship with Michelle Gennock, a Las Vegas showgirl. He has since been engaged to Terasa Livingstone and Siobhan Way.

</doc>
<doc id="47694" url="http://en.wikipedia.org/wiki?curid=47694" title="Pin (chess)">
Pin (chess)

In chess, a pin is a situation brought on by an attacking piece in which a defending piece cannot move without exposing a more valuable defending piece on its other side to capture by the attacking piece. "To pin" refers to the action of the attacking piece inducing the pin, and the defending piece so restricted is described as pinned.
Only pieces that can move an indefinite number of squares in a horizontal, vertical, or diagonal line, "i.e.", bishops, rooks and queens, can pin opposing pieces. Kings, knights, and pawns cannot pin. Any piece may be pinned except the king, as the king must be immediately removed from check under all circumstances.
 
Types.
An "absolute pin" is one where the piece shielded by the pinned piece is the king. In this case it is illegal to move the pinned piece out of the line of attack, as that would place one's king in check. A "relative pin" is one where the piece shielded by the pinned piece is a piece other than the king, but typically more valuable than the pinned piece. Moving such a pinned piece is legal, but may not be prudent as the shielded piece would then be vulnerable to capture. (See diagram at right.)
If a rook or queen is pinned along a file or rank, or a bishop or queen is pinned along a diagonal, the pin is a "partial pin": the pinned unit can still move along its line but cannot leave that line. A partially pinned unit may break its own pin by capturing the pinning piece; however, a partial pin can still be advantageous to the pinning player, for instance if the queen is pinned by a rook or bishop, and the pinning piece is defended, so that capturing it with the queen would lose material. Note that a queen can only ever be partially pinned, as it can move in any linear direction.
It is possible for two opposing pieces to be partially pinning each other. It is also possible for one piece to be pinned in one direction (line of attack) and partially pinned in another, or otherwise pinned in two or more directions.
The act of breaking a pin is "unpinning". This can be executed in a number of ways: the piece creating the pin can be captured; another unit can be moved onto the line of the pin; or the unit to which a piece is pinned can be moved.
Although a pin is not a tactic in itself, it can be useful in tactical situations. One tactic which takes advantage of a pin can be called "working the pin". In this tactic, other pieces from the pinning piece's side attack the opposing pinned piece. Since the pinned piece cannot move out of the line of attack, the pinned piece's player may move other pieces to defend the pinned piece, but the pinning player may yet attack with even more pieces, etc. Pinning can also be used in combination with other tactics. For example, a piece can be pinned to prevent it from moving to attack, or a defending piece can be pinned as part of tactic undermining an opponent's defense. A pinned piece can usually no longer be counted on as a defender of another friendly piece (that is out of the pinning line of attack) or as an attacker of an opposing piece (out of the pinning line). However, a pinned piece can still check the opposing king - and therefore still can defend friendly pieces against captures made by the enemy king.
A pin that often occurs in openings is the move Bb5 which, if Black has moved ...Nc6 and ...d6 or ...d5, pins the knight on c6, because moving the knight would expose the king on e8 to check. (The same may, of course, occur on the other flank, with a bishop on g5, or by Black on White, with a bishop on b4 or g4.) A common way to win the queen is to pin her to the king with a rook: for instance with the black queen on e5 and the black king on e8 and no other pieces on the e-file, the move Re1 by White would pin Black's queen.
Example of pin in real game.
In the diagram at right with white to move next, Black is threatening the following rook sacrifice leading to mate. 
The pawn on g2 cannot take the rook on h3 because the queen on g3 is pinning the pawn with a vertical line of attack. The only move to prevent the above moves is 27.Nf4 which temporarily blocks black's bishop from protecting his queen, but to no avail. The black bishop can take the knight by 27...Bxf4 renewing the same threat of mate in 2, or Black can respond as follows to mate anyway: 
In this case, white could not take the mating rook now on f3 with the g2 pawn because the queen on h2 would now be pinning the pawn with a horizontal line of attack. With mate against him being inevitable, white resigned after move 26. 
Sometimes in a chess game position, a piece may be considered to be in a "situational pin". In a situational pin, moving the pinned piece out of the line of attack will result in a situation detrimental to the player of the pinned piece, such as a checkmate. Although a situational pin is not an absolute pin and the pinned piece can still be moved according to the rules, moving out of line of attack can result in a bad situation or even immediate loss of the game.
Consider the chess position shown at right. White has not castled or moved the king or rook yet. The black bishop has just moved from e6 to d5, making itself unprotected and available for capture by the white knight on b4. It is now white's turn to move. White should not capture the black bishop because after 1.Nxd5, 1...Rb1+ wins white's rook, because the king is forced to move away from the check, thereby exposing the rook to attack (a skewer).

</doc>
<doc id="47696" url="http://en.wikipedia.org/wiki?curid=47696" title="Sport utility vehicle">
Sport utility vehicle

A sport utility vehicle or suburban utility vehicle (SUV) is a vehicle similar to a station wagon or estate car, usually equipped with four-wheel drive for on- or off-road ability. Some SUVs include the towing capacity of a pickup truck with the passenger-carrying space of a minivan or large sedan.
The original 1984 Jeep Cherokee (XJ) by American Motors combined a passenger car with truck chassis for ease of driving in difficult conditions, as well as established the modern SUV market segment and its popularity in the late-1980s and early 1990s. The compact-sized XJ Cherokee was one of the most best selling SUV models ever made, with over 2.8 million built between 1984 and 2001.
Definitions.
According to the Merriam-Webster dictionary, a "sport utility vehicle" is "a rugged automotive vehicle similar to a station wagon but built on a light-truck chassis". The "SUV" term is defined as "a large vehicle that is designed to be used on rough surfaces but that is often used on city roads or highways." The "SUV" acronym "is still used to describe nearly anything with available all-wheel drive and raised ground clearance."
North America.
There are a number of definitions for an SUV. Most government regulations simply have categories for "off-highway vehicles," which in turn are lumped in with pickup trucks and minivans as light trucks." The auto industry has not settled on one definition.
Nevertheless, four-wheel-drive SUVs are considered light trucks in North America (and two-wheel-drive SUVs up to the 2011 model year) where they were regulated less strictly than passenger cars under two laws in the United States, the Energy Policy and Conservation Act for fuel economy, and the Clean Air Act for emissions. Starting in 2004, the United States Environmental Protection Agency (EPA) began to hold sport utility vehicles to the same tailpipe emissions standards as cars.
Many people question "how can an SUV be called a truck?" Although the original definition of the "light truck" classification included pickups and delivery vans, usually SUVs and minivans are included in this category because these vehicles are designed to "permit greater cargo-carying capacity than passenger carrying volume. Manufacturing, emissions, and safety regulations in the U.S. classify "an SUV is a truck"; however, for local licensing and traffic enforcement, "an SUV may be a truck or a car" because the classification of these vehicles varies from state to state. For industry production statistics, SUVs are counted in the light truck product segment.
Other markets.
The term is not used in all countries, and outside North America the terms "off-road vehicle", "four-wheel drive" or "four-by-four" (abbreviated to "4WD" or "4×4") or simply use of the brand name to describe the vehicle like "Jeep" or "Land Rover" are more common.
In Europe, the term SUV has a similar meaning, but being newer than in the U.S. it only applies to the newer street oriented one, where-as "Jeep", "Land Rover" or 4x4 are used for the off-roader oriented ones. Not all SUVs have four-wheel drive capabilities, and not all four-wheel-drive passenger vehicles are SUVs. Although some SUVs have off-road capabilities, they often play only a secondary role, and SUVs often do not have the ability to switch among two-wheel and four-wheel-drive high gearing and four-wheel-drive low gearing. While automakers tout an SUV's off-road prowess with advertising and naming, the daily use of SUVs is largely on paved roads.
In India, all SUVs are classified in the "Utility Vehicle" category per the Society of Indian Automobile Manufacturers (SIAM) definitions and carry a 27% excise tax. Those that are 4 m long, have a 1500 cc engine or larger, along with 170 mm of ground clearance, are subject to a 30% excise duty.
Designs.
Although designs vary, SUVs have historically been mid-size passenger vehicles with a body-on-frame chassis similar to that found on light trucks. Early SUVs were mostly two-door models, and were available with removable tops. However, consumer demand pushed the SUV market towards four doors, by 2002 all full-size two-door SUVs were gone from the market. Two-door SUVs were mostly carry-over models, and their sales were not viable enough to warrant a redesign at the end of their design cycle. The Jeep Wrangler remained as a compact two-door body style, although it was also joined by a four-door variant starting with the 2007 model year, the Wrangler Unlimited. The number of two-door SUV models increased in the 2010s with the release of the Range Rover Evoque and the Nissan Murano convertible, although both vehicles are unibody designs.
Most SUVs are designed with an engine compartment, a combined passenger and cargo compartment, and no dedicated trunk such as in a station wagon body. Most mid-size and full-size SUVs have three rows of seats with a cargo area directly behind the last row of seats. Cargo barriers are often fitted to the cargo area to protect the vehicles occupants from injury from unsecured cargo in the event of sudden deceleration or collision.
SUVs are known for high ground clearance, upright, boxy body, and high H-point. This can make them more likely to roll over due to their high center of gravity. Bodies of SUVs have recently become more aerodynamic, but the sheer size and weight keeps their fuel economy poor.
A mini SUV (also called subcompact SUV or subcompact crossover) is a class of small sport utility vehicles. The term usually applies to crossovers based on a supermini (B-segment cars in Europe) platform.
A compact SUV is a class of smaller SUVs that are commonly built with less cargo and passenger space, and often with smaller engines resulting in better fuel economy, the term is often interchangeable with crossover SUV.
A mid-size SUV is a class of medium-size SUVs whose size typically falls between that of a full-size and a compact SUV. This term is not commonly used outside North America, where fullsize and midsize SUVs are considered similar.
A full-size SUV is a class of large-size SUVs that are most often larger than midsize SUVs. They have greater cargo and passenger space than midsize SUVs. Full Size SUVs are usually given higher safety ratings than their smaller counterparts.
An extended length SUV, also sometimes called a long-wheel based SUV, are vehicles that are similar to a full-size SUV, except that these vehicles have a larger cargo area (around 130 in) and passenger space that can seat up to 8 or 9 people (with the available third row seating that when folded or removed adds more cargo space). Although these extended length SUVs are mostly sold in North America because of their size and the roads are made and designed differently, they can also be found in other countries, exported to such places like The Philippines and The Middle East. The vehicles are 221 in to 223 in in length and can be distinguished by the rear wheel area not touching the rear doors.
History.
Origins.
Early SUVs were descendants from commercial and military vehicles as the World War II Jeep and Land Rover.
The earliest examples of longer-wheelbase wagon-type SUVs were the Chevrolet Carryall Suburban (1935, RWD only), GAZ-61 (1938, 4×4), Willys Jeep Station Wagon (1948), Pobeda M-72 (GAZ-M20/1955), which Russian references credit as possibly being the first modern SUV (with unitary body rather than body-on-frame), International Harvester Travelall (1953), Land Rover Series II 109 (1958), and the International Harvester Scout 80 (1961). These were followed by the more 'modern' Jeep Wagoneer (1963), International Harvester Scout II (1971), Ford Bronco (1966), Toyota Land Cruiser FJ-55 (1968), the Chevrolet Blazer / GMC Jimmy (1969), and the Land Rover Range Rover (1970). The actual term "sport utility vehicle" did not come into wide popular usage until the late 1980s; many of these vehicles were marketed during their era as station wagons.
According to Robert Casey, the transportation curator at the Henry Ford Museum, the Jeep Cherokee (XJ) was the first true sport utility vehicle in the modern understanding of the term. Developed under the leadership of AMC's François Castaing and marketed to urban families as a substitute for a traditional car (and especially station wagons, which were still fairly popular at the time), the Cherokee had four-wheel drive in a more manageable size (compared to the full-size Wagoneer), as well as a plush interior resembling a station wagon. With the introduction of more luxurious models and a much more powerful 4-liter engine, sales of the Cherokee increased even higher as the price of gasoline fell, and the term "sport utility vehicle" began to be used in the national press for the first time. "The advent and immediate success of AMC/Jeep's compact four-door Cherokee turned the truck industry upside down."
The corporate average fuel economy (CAFE) standard was ratified in the 1970s to regulate the fuel economy of passenger vehicles. Car manufacturers evaded the regulation by selling SUVs as work vehicles. The popularity of SUV increased among urban drivers in the last 25 years, and particularly in the last decade. Consequently, modern SUVs are available with luxury vehicle features, and some crossover models adopt lower ride heights to accommodate on-road driving.
Keith Bradsher explained the rise of the SUV with American Motors' (AMC) lobbying the United States Environmental Protection Agency (EPA) for a waiver of the United States Clean Air Act. The EPA subsequently designated AMC's compact Cherokee as a "light truck", and the company marketed the vehicle to everyday drivers. AMC's effort to affect rulemaking changing the official definition of their new model then led to the SUV boom when other auto makers marketed their own models in response to the Cherokee taking sales from their regular cars.
Popularity.
SUVs became popular in the United States, Canada, India and Australia in the 1990s and early-2000s. U.S. automakers could enjoy profit margins of $10,000 per SUV, while losing a few hundred dollars on a compact car. For example, the Ford Excursion could net the company $18,000, while they could not break even with the Ford Focus unless the buyer chose options, leading Detroit's big three automakers to focus on SUVs over small cars.
The higher cost of union labor in the U.S. and Canada compared to the lower wages of non-union workers at non-U.S. companies like Toyota, made it unprofitable for American auto makers to build small cars in the U.S. For example, the General Motors factory in Arlington, Texas where rear-wheel-drive cars were built, such as the Chevrolet Caprice, Buick Roadmaster, and Cadillac Fleetwood Brougham was converted to truck and SUV production, putting an end to full-size family station wagon and overall terminating production of rear-wheel drive full-size cars. Due to the shift in the Big Three's strategy, many long-running cars like the Ford Taurus, Buick Century and Pontiac Grand Prix fell behind their Japanese competitors in features and image (relying more on fleet sales instead of retail and/or heavy incentive discounts); some were discontinued.
Buyers were drawn to SUVs' large cabins, higher ride height, and perceived safety. Full-size SUVs often offered features such as three-row seating, to effectively replace full-size station wagons and minivans. Wagons were seen as old-fashioned. Additionally, full-size SUVs have greater towing capabilities than conventional cars, and can haul trailers, travel trailers (caravans) and boats. Increased ground clearance is useful in climates with heavy snowfall. The very low oil prices of the 1990s helped to keep down running costs. The SUV was one of the most popular choices of vehicle for female drivers in the U.S. The 1990 Ford Explorer was also popular despite it being one of several Ford SUV models described as "dangerous vehicles" through the 2000s. By 1994, SUVs outsold minivans in the United States although they were also not ranked high in safety. Peaking in the late-1990s and early to mid-2000s, SUVs sales temporarily declined due to high oil prices and a declining economy. The traditional truck-based SUV is gradually being supplanted by the crossover SUV, which uses an automobile platform for lighter weight and better fuel efficiency.
Social scientists have drawn on popular folklore such as urban legends to illustrate how marketers have been able to capitalize on the feelings of strength and security offered by SUVs. Popular tales include narratives where mothers save the family from armed robbery and other incidents by taking the automobile off road, for example.
In Australia, SUV sales were helped by the fact that SUVs had much lower import duty than passenger cars did, so that they cost less than similarly equipped imported sedans. However, this gap was gradually narrowed, and in January 2010 the import duty on cars was lowered to match the 5 percent duty on SUVs.
Sales of SUVs and other light trucks fell in the mid-2000s because of high oil prices and declining economy. In June 2008, General Motors announced plans to close four truck and SUV plants, including the Oshawa Truck Assembly. The company cited decreased sales of large vehicles in the wake of rising fuel prices. The business model of focusing on SUVs and light trucks, at the expense of more fuel-efficient compact and midsized cars, is blamed for declining sales and profits among Detroit's Big Three automakers since the mid–late-2000s. The Big Three were slower to adapt than their Japanese rivals in producing small cars to meet growing demand due to inflexible manufacturing facilities, which made it unprofitable to build small cars. However, starting in 2010 SUV and light truck sales have started an upward trend due to lower gas prices and a revival of the North American economy. In 2013, General Motors saw its sales for its large SUVs increased by 74%, making them the largest producer of SUVs in the United States. However, the "small and compact SUVs, when compared with other vehicles in the light truck segment, has made this vehicle segment the third highest selling vehicle segment in the automotive market in 2013." With the redesigned GM and Ford large SUVs being introduced in 2014 (for the 2015 model year), it has seen a slight resurgence among consumers due to better fuel economy and new engines, along with updated and newer features.
Use in remote areas.
SUVs are sometimes driven off-road on farms and in remote areas of such places as the Australian Outback, Africa, the Middle East, Alaska, Canada, Iceland, South America, Russia and parts of Asia which have limited paved roads and require a vehicle to have all-terrain handling, increased range, and storage capacity. The scarcity of spare parts and the need to carry out repairs quickly resulted in the popularity of vehicles with the bare minimum of electric and hydraulic systems, such as the basic versions of the Land Rover, Jeep Wrangler, Nissan Patrol and Toyota Land Cruiser. SUVs for urban driving have traditionally been developed from their more rugged all-terrain counterparts. For example, the Hummer H1 was developed from the HMMWV, originally developed for the military of the United States.
As many SUV owners never used the off-road capabilities of their vehicle, newer SUVs (including crossovers) now have lower ground clearance and suspension designed primarily for paved roads.
Some buyers choose SUVs because they have more interior space than sedans of similar sizes. In areas with gravel roads in summer and snow and ice in winter, four-wheel drives offer a safety advantage due to their traction advantages under these conditions.
The sport utility vehicles have also gained popularity in some areas of Mexico, especially in desert areas or in cities where drivers frequently encounter potholes, detours, high water and rough roads. Increasing use is also attributed to the high number of dirt roads outside major population centers, resulting in washboard and mud in the rainy seasons.
Use in recreation and motorsport.
Some highly modified SUVs, together with their more rugged off-road counterparts, are also used to explore places otherwise unreachable by other vehicles. In Australia, China, Europe, South Africa, South America and the United States at least, 4WD clubs have been formed for this purpose. Modified SUVs also take part in races, including the Paris-Dakar Rally, the Baja racing series, TREC events, King of the Hammers in California and the Australian Outback.
The Trophee Andros ice-racing series is another competition where SUVs participate as well.
Many 4×4 mud racing events and other activities take place throughout the US organized by clubs and associations.
Luxury SUV.
Numerous luxury vehicles in the form of SUVs and pickup trucks are being produced. Luxury SUV is principally a marketing term to sell fancier vehicles that may have higher performance, comfort, technology, or brand image. The term lacks both measurability and verifiability, and it is applied to a broad range of SUV sizes and types.
Nevertheless, the marketing category was created in 1966 with Kaiser Jeep's luxurious Super Wagoneer. It was the first SUV to offer a V8 engine, automatic transmission, and luxury car trim and equipment in a serious off-road model. It came with bucket seating, air conditioning, sun roof, and even a vinyl roof. Land Rover followed suit in 1970 by introducing the Range Rover. The trend continued with other competitors adding comfort features to their rudimentary and truck-based models.
The production of luxury models increased in the late-1990s with vehicles such as the Lincoln Navigator and Cadillac Escalade. These luxury SUVs generated higher profit margins than non-luxury SUVs did. For some auto makers, luxury SUVs were the first SUV models they produced. Some of these models are not traditional SUVs based on light truck as they are classified as crossovers.
The luxury SUV class encompasses both smaller 5-passenger SUVs and larger 7-passenger SUVs, with luxury features both inside of the cabin but also in the outside. Buyers looking for a luxury vehicle that offers more cargo capacity than a sedan may prefer a luxury SUV. This is also a vehicle aimed for those who prefer an SUV with a little more style.
Luxury SUVs typically offer the most expected safety features including side airbags, ABS and traction control, and many of them also come with electronic stability control, crash resistant door pillars, dynamic head restraints and back-up sensing systems.
The U.S. News & World Report Rankings and Reviews ranks premium midsize SUVs and crossovers based on an in-depth analysis by its editors of published auto ratings, reviews and test drives. Ranking is based on the score on performance, exterior, interior, safety, and reliability obtained by the vehicles.
Other names.
In Australia and New Zealand, the term "SUV" is not widely used, except by motoring organizations, the press, and industry bodies. Passenger class vehicles designed for off-road use are known as 'four-wheel drives', '4WDs', or '4×4s'. Some manufacturers do refer to their products as SUVs, but others invented names such as XUV, (HSV Avalanche XUV or GMC Envoy XUV) or action utility vehicles (AUVs). The term 'AWD', or all-wheel drive, is used for any vehicle which drives on all four wheels, but may not be designed for off-road use. 'Crossover' is a marketing term for a vehicle that is both four-wheel-drive and primarily a road car.
The pejorative term "Toorak Tractor" is used in Australia to describe vehicles such as Range Rovers used in wealthy urban areas with fine roads, fine dining, and exclusive designer shopping precincts where off-road ability is not required. The term alludes to the affluent Melbourne suburb of Toorak and was used at least as early as the late 1980s. The equivalent term "Chelsea Tractor" became prominent in the United Kingdom around 2004 to describe vehicles such luxury SUVs used in urban areas such as Chelsea, London, where their four-wheel-drive capabilities are not required and the car is believed to be a status symbol rather than a necessity. The term "4×4" (four-by-four) is also common even for vehicles not used in urban areas. "AWD" is not commonly used in the UK. The less capable SUVs also pick up the name "soft-roader" because while they appear designed to go off road, in many cases they're not actually capable of it.
In Norway the term "Børstraktor" (Stock Exchange Tractor) serves a similar purpose.
In Finland the term "katumaasturi" is commonly used to designate SUVs. It roughly translates to street-off-roader, or street-4×4. This marks the difference with what is called "maasturi" which is a vehicle with off-road capability.

</doc>
<doc id="47697" url="http://en.wikipedia.org/wiki?curid=47697" title="Timecode">
Timecode

A timecode (alternatively, time code) is a sequence of numeric codes generated at regular intervals by a timing synchronization system.
Video and film timecode.
In video production and filmmaking, SMPTE timecode is used extensively for synchronization, and for logging and identifying material in recorded media. During filmmaking or video production shoot, the camera assistant will typically log the start and end timecodes of shots, and the data generated will be sent on to the editorial department for use in referencing those shots. This shot-logging process was traditionally done by hand using pen and paper, but is now typically done using shot-logging software running on a laptop computer that is connected to the time code generator or the camera itself.
The SMPTE family of timecodes are almost universally used in film, video and audio production, and can be encoded in many different formats, including:
Keykode, while not a timecode, is used to identify specific film frames in film post-production that uses physical film stock. Keykode data is normally used in conjunction with SMPTE time code.
Rewritable consumer timecode is a proprietary consumer video timecode system that is not frame-accurate, and is therefore not used in professional post-production.
Other time code formats.
Time codes for purposes other than video and audio production include:

</doc>
<doc id="47698" url="http://en.wikipedia.org/wiki?curid=47698" title="Victoria Adams">
Victoria Adams

Victoria Adams is the name of:

</doc>
<doc id="47700" url="http://en.wikipedia.org/wiki?curid=47700" title="Coral">
Coral

Corals are marine invertebrates in the class Anthozoa of phylum Cnidaria. They typically live in compact colonies of many identical individual polyps. The group includes the important reef builders that inhabit tropical oceans and secrete calcium carbonate to form a hard skeleton.
A coral "head" is a colony of myriad genetically identical polyps. Each polyp is a sac-like animal typically only a few millimeters in diameter and a few centimeters in length. A set of tentacles surround a central mouth opening. An exoskeleton is excreted near the base. Over many generations, the colony thus creates a large skeleton that is characteristic of the species. Individual heads grow by asexual reproduction of polyps. Corals also breed sexually by spawning: polyps of the same species release gametes simultaneously over a period of one to several nights around a full moon.
Although some corals can catch small fish and plankton, using stinging cells on their tentacles, most corals obtain the majority of their energy and nutrients from photosynthetic unicellular dinoflagellates in the genus "Symbiodinium" that live within their tissues. These are commonly known as zooxanthellae and the corals that contain them are zooxanthellate corals. Such corals require sunlight and grow in clear, shallow water, typically at depths shallower than 60 m. Corals are major contributors to the physical structure of the coral reefs that develop in tropical and subtropical waters, such as the enormous Great Barrier Reef off the coast of Queensland, Australia.
Other corals do not rely on zooxanthellae and can live in much deeper water, with the cold-water genus "Lophelia" surviving as deep as 3000 m. Some have been found on the Darwin Mounds, north-west of Cape Wrath, Scotland. Corals have also been found as far north as off the coast of Washington State and the Aleutian Islands.
Taxonomy.
The Muslim polymath Al-Biruni (d. 1048) classified sponges and corals as animals, arguing that they respond to touch. Nevertheless, people believed corals to be plants until the eighteenth century, when William Herschel used a microscope to establish that coral had the characteristic thin cell membranes of an animal.
The phylogeny of Anthozoans is not clearly understood and a number of different models have been proposed. Within the Hexacorallia, the sea anemones, coral anemones and stony corals may constitute a monophyletic grouping united by their eight-fold symmetry and cnidocyte trait. The Octocorallia appears to be monophyletic, and primitive members of this group may have been stolonate. The cladogram presented here comes from a 2014 study by Stampar et al. which was based on the divergence of mitochondrial DNA within the group and on nuclear markers.
Corals are classified in the class Anthozoa of the phylum Cnidaria. They are divided into three subclasses, Hexacorallia, Octocorallia, and Ceriantharia. The Hexacorallia include the stony corals, the sea anemones and the zoanthids. These groups have polyps that generally have 6-fold symmetry. The Octocorallia include blue coral, soft corals, sea pens, and gorgonians (sea fans and sea whips). These groups have polyps with 8-fold symmetry, each polyp having eight tentacles and eight mesenteries. Ceriantharia are the tube-dwelling anemones.
Fire corals are not true corals, being in the order Anthomedusa (sometimes known as Anthoathecata) of the class Hydrozoa.
Anatomy.
Corals are sessile animals in the class Anthozoa and differ from most other cnidarians in not having a medusa stage in their life cycle. The body unit of the animal is a polyp. Most corals are colonial, the initial polyp budding to produce another and the colony gradually developing from this small start. In stony corals, also known as hard corals, the polyps produce a skeleton composed of calcium carbonate to strengthen and protect the organism. This is deposited by the polyps and by the coenosarc, the living tissue that connects them. The polyps sit in cup-shaped depressions in the skeleton known as corallites. Colonies of stony coral are very variable in appearance; a single species may adopt an encrusting, plate-like, bushy, columnar or massive solid structure, the various forms often being linked to different types of habitat, with variations in light level and water movement being significant.
In soft corals, there is no stony skeleton but the tissues are often toughened by the presence of tiny skeletal elements known as sclerites, which are made from calcium carbonate. Soft corals are very variable in form and most are colonial. A few soft corals are stolonate, but the polyps of most are connected by sheets of coenosarc. In some species this is thick and the polyps are deeply embedded. Some soft corals are encrusting or form lobes. Others are tree-like or whip-like and have a central axial skeleton embedded in the tissue matrix. This is composed either of a fibrous protein called gorgonin or of a calcified material. In both stony and soft corals, the polyps can be retracted, with stony corals relying on their hard skeleton and cnidocytes for defence against predators, with soft corals generally relying on chemical defences in the form of toxic substances present in the tissues known as terpenoids.
The polyps of stony corals have six-fold symmetry while those of soft corals have eight. The mouth of each polyp is surrounded by a ring of tentacles. In stony corals these are cylindrical and taper to a point, but in soft corals they are pinnate with side branches known as pinnules. In some tropical species these are reduced to mere stubs and in some they are fused to give a paddle-like appearance. In most corals, the tentacles are retracted by day and spread out at night to catch plankton and other small organisms. Shallow water species of both stony and soft corals can be zooxanthellate, the corals supplementing their plankton diet with the products of photosynthesis produced by these symbionts. The polyps interconnect by a complex and well-developed system of gastrovascular canals, allowing significant sharing of nutrients and symbionts.
Ecology.
Feeding.
Polyps feed on a variety of small organisms, from microscopic zooplankton to small fish. The polyp's tentacles immobilize or kill prey using their nematocysts. These cells carry venom which they rapidly release in response to contact with another organism. A dormant nematocyst discharges in response to nearby prey touching the trigger (cnidocil). A flap (operculum) opens, and its stinging apparatus fires the barb into the prey. The venom is injected through the hollow filament to immobilise the prey; the tentacles then manoeuvre the prey to the mouth.
The tentacles then contract to bring the prey into the stomach. Once the prey is digested, the stomach reopens, allowing the elimination of waste products and the beginning of the next hunting cycle. They can scavenge drifting organic molecules and dissolved organic molecules.:24
Intracellular symbionts.
Many corals, as well as other cnidarian groups such as "Aiptasia" (a sea anemone) form a symbiotic relationship with a class of dinoflagellate algae, zooxanthellae of the genus "Symbiodinium".:24 "Aiptasia", a familiar pest among coral reef aquarium hobbyists, serves as a valuable model organism in the study of cnidarian-algal symbiosis. Typically, each polyp harbors one species of algae. Via photosynthesis, these provide energy for the coral, and aid in calcification. As much as 30% of the tissue of a polyp may be plant material.:23
The algae benefit from a safe place to live and consume the polyp's carbon dioxide and nitrogenous waste. Due to the strain the algae can put on the polyp, stress on the coral often drives them to eject the algae. Mass ejections are known as coral bleaching, because the algae contribute to coral's brown coloration; other colors, however, are due to host coral pigments, such as green fluorescent proteins (GFPs). Ejection increases the polyp's chance of surviving short-term stress—they can regain algae, possibly of a different species at a later time. If the stressful conditions persist, the polyp eventually dies.
Reproduction.
Corals can be both gonochoristic (unisexual) and hermaphroditic, each of which can reproduce sexually and asexually. Reproduction also allows coral to settle in new areas.
Sexual.
Corals predominantly reproduce sexually. About 25% of hermatypic corals (stony corals) form single sex (gonochoristic) colonies, while the rest are hermaphroditic.
Broadcasters.
About 75% of all hermatypic corals "broadcast spawn" by releasing gametes—eggs and sperm—into the water to spread offspring. The gametes fuse during fertilization to form a microscopic larva called a planula, typically pink and elliptical in shape. A typical coral colony forms several thousand larvae per year to overcome the odds against formation of a new colony.
Synchronous spawning is very typical on the coral reef, and often, even when multiple species are present, all corals spawn on the same night. This synchrony is essential so male and female gametes can meet. Corals rely on environmental cues, varying from species to species, to determine the proper time to release gametes into the water. The cues involve temperature change, lunar cycle, day length, and possibly chemical signalling. Synchronous spawning may form hybrids and is perhaps involved in coral speciation. The immediate cue is most often sunset, which cues the release. The spawning event can be visually dramatic, clouding the usually clear water with gametes.
Brooders.
Brooding species are most often ahermatypic (not reef-building) in areas of high current or wave action. Brooders release only sperm, which is negatively buoyant, sinking on to the waiting egg carriers who harbor unfertilized eggs for weeks. Synchronous spawning events sometimes occurs even with these species. After fertilization, the corals release planula that are ready to settle.
Planulae.
Planulae exhibit positive phototaxis, swimming towards light to reach surface waters, where they drift and grow before descending to seek a hard surface to which they can attach and begin a new colony. They also exhibit positive sonotaxis, moving towards sounds that emanate from the reef and away from open water. High failure rates afflict many stages of this process, and even though millions of gametes are released by each colony, few new colonies form. The time from spawning to settling is usually two to three days, but can be up to two months. The larva grows into a polyp and eventually becomes a coral head by asexual budding and growth.
Asexual.
Within a coral head, the genetically identical polyps reproduce asexually, either by budding (gemmation) or by dividing, whether longitudinally or transversely.
Budding involves splitting a smaller polyp from an adult. As the new polyp grows, it forms its body parts. The distance between the new and adult polyps grows, and with it, the coenosarc (the common body of the colony; see ). Budding can be intratentacular, from its oral discs, producing same-sized polyps within the ring of tentacles, or extratentacular, from its base, producing a smaller polyp.
Division forms two polyps that each become as large as the original. Longitudinal division begins when a polyp broadens and then divides its coelenteron (body), effectively splitting along its length. The mouth divides and new tentacles form. The two polyps thus created then generate their missing body parts and exoskeleton. Transversal division occurs when polyps and the exoskeleton divide transversally into two parts. This means one has the basal disc (bottom) and the other has the oral disc (top); the new polyps must separately generate the missing pieces.
Asexual reproduction offers the benefts of high reproductive rate, delaying senescence, and replacement of dead modules, as well as geographical distribution.
Colony division.
Whole colonies can reproduce asexually, forming two colonies with the same genotype. The possible mechanisms include fission, bailout and fragmentation. Fission occurs in some corals, especially among the family Fungiidae, where the colony splits into two or more colonies during early developmental stages. Bailout occurs when a single polyp abandons the colony and settles on a different substrate to create a new colony. Fragmentation involves individuals broken from the colony during storms or other disruptions. The separated individuals can start new colonies.
Reefs.
Many corals in the order Scleractinia are hermatypic, meaning that they are involved in building reefs. Most such corals obtain some of their energy from zooxanthellae in the genus "Symbiodinium". These are symbiotic photosynthetic dinoflagellates which require sunlight; reef-forming corals are therefore found mainly in shallow water. They secrete calcium carbonate to form hard skeletons that become the framework of the reef. However, not all reef-building corals in shallow water contain zooxanthellae, and some deep water species, living at depths to which light cannot penetrate, form reefs but do not harbour the symbionts.
There are various types of shallow-water coral reef, including fringing reefs, barrier reefs and atolls; most occur in tropical and subtropical seas. They are very slow-growing, adding perhaps one centimetre (0.4 in) in height each year. The Great Barrier Reef is thought to have been laid down about two million years ago. Over time, corals fragment and die, sand and rubble accumulates between the corals, and the shells of clams and other molluscs decay to form a gradually evolving calcium carbonate structure. Coral reefs are extremely diverse marine ecosystems hosting over 4,000 species of fish, massive numbers of cnidarians, molluscs, crustaceans, and many other animals.
Evolutionary history.
Although corals first appeared in the Cambrian period, some million years ago, fossils are extremely rare until the Ordovician period, 100 million years later, when rugose and tabulate corals became widespread. Paleozoic corals often contained numerous endobiotic symbionts.
Tabulate corals occur in limestones and calcareous shales of the Ordovician and Silurian periods, and often form low cushions or branching masses of calcite alongside rugose corals. Their numbers began to decline during the middle of the Silurian period, and they became extinct at the end of the Permian period, million years ago.
Rugose or horn corals became dominant by the middle of the Silurian period, and became extinct early in the Triassic period. The rugose corals existed in solitary and colonial forms, and were also composed of calcite.
The scleractinian corals filled the niche vacated by the extinct rugose and tabulate species. Their fossils may be found in small numbers in rocks from the Triassic period, and became common in the Jurassic and later periods. Scleractinian skeletons are composed of a form of calcium carbonate known as aragonite. Although they are geologically younger than the tabulate and rugose corals, the aragonite of their skeletons is less readily preserved, and their fossil record is accordingly less complete.
At certain times in the geological past, corals were very abundant. Like modern corals, these ancestors built reefs, some of which ended as great structures in sedimentary rocks. Fossils of fellow reef-dwellers algae, sponges, and the remains of many echinoids, brachiopods, bivalves, gastropods, and trilobites appear along with coral fossils. This makes some corals useful index fossils. Coral fossils are not restricted to reef remnants, and many solitary fossils may be found elsewhere, such as "Cyclocyathus", which occurs in England's Gault clay formation.
Status.
Threats.
Coral reefs are under stress around the world. In particular, coral mining, agricultural and urban runoff, pollution (organic and inorganic), overfishing, blast fishing, disease, and the digging of canals and access into islands and bays are localized threats to coral ecosystems. Broader threats are sea temperature rise, sea level rise and pH changes from ocean acidification, all associated with greenhouse gas emissions. In 1998, 16% of the world's reefs died as a result of increased water temperature.
Approximately 10% of the world's coral reefs are dead. About 60% of the world's reefs are at risk due to human-related activities. The threat to reef health is particularly strong in Southeast Asia, where 80% of reefs are endangered. Over 50% of the world's coral reefs may be destroyed by 2030; as a result, most nations protect them through environmental laws.
In the Caribbean and tropical Pacific, direct contact between ~40–70% of common seaweeds and coral causes bleaching and death to the coral via transfer of lipid-soluble metabolites. Seaweed and algae proliferate given adequate nutrients and limited grazing by herbivores such as parrotfish.
Water temperature changes of more than 1–2 °C (1.8–3.6 °F) or salinity changes can kill some species of coral. Under such environmental stresses, corals expel their Symbiodinium; without them coral tissues reveal the white of their skeletons, an event known as coral bleaching.
Submarine springs found along the coast of Mexico's Yucatán Peninsula produce water with a naturally low pH (relatively high acidity) providing conditions similar to those expected to become widespread as the oceans absorb carbon dioxide. Surveys discovered multiple species of live coral that appeared to tolerate the acidity. The colonies were small and patchily distributed, and had not formed structurally complex reefs such as those that compose the nearby Mesoamerican Barrier Reef System.
Protection.
Marine Protected Areas (MPAs), Biosphere reserves, marine parks, national monuments world heritage status, fishery management and habitat protection can protect reefs from anthropogenic damage.
Many governments now prohibit removal of coral from reefs, and inform coastal residents about reef protection and ecology. While local action such as habitat restoration and herbivore protection can reduce local damage, the longer-term threats of acidification, temperature change and sea-level rise remain a challenge.
To eliminate destruction of corals in their indigenous regions, projects have been started to grow corals in non-tropical countries.
Relation to humans.
Local economies near major coral reefs benefit from an abundance of fish and other marine creatures as a food source. Reefs also provide recreational scuba diving and snorkeling tourism. These activities can damage coral but international projects such as Green Fins that encourage dive and snorkel centres to follow a Code of Conduct has been proven to mitigate these risks.
Live coral is highly sought after for aquaria. Soft corals are easier to maintain in captivity than hard corals.
Jewelry.
Corals' many colors give it appeal for necklaces and other jewelry. Intensely red coral is prized as a gemstone. Sometimes called fire coral, it is not the same as fire coral. Red coral is very rare because of overharvesting.
Medicine.
In medicine, chemical compounds from corals are used for cancer, AIDS, pain, and other uses. Coral skeletons, e.g. "Isididae" are also used for bone grafting in humans.
Coral Calx, known as Praval Bhasma in Sanskrit, is widely used in traditional system of Indian medicine as a supplement in the treatment of a variety of bone metabolic disorders associated with calcium deficiency.
Construction.
Coral reefs in places such as the East African coast are used as a source of building material. Ancient (fossil) coral limestone, notably including the Coral Rag Formation of the hills around Oxford (England), was once used as a building stone, and can be seen in some of the oldest buildings in that city including the Saxon tower of St Michael at the Northgate, St. George's Tower of Oxford Castle, and the mediaeval walls of the city.
Climate research.
Annual growth bands in some corals, such as the deep sea bamboo corals ("Isididae"), may be among the first signs of the effects of ocean acidification on marine life. The growth rings allow geologists to construct year-by-year chronologies, a form of incremental dating, which underlie high-resolution records of past climatic and environmental changes using geochemical techniques.
Certain species form communities called microatolls, which are colonies whose top is dead and mostly above the water line, but whose perimeter is mostly submerged and alive. Average tide level limits their height. By analyzing the various growth morphologies, microatolls offer a low resolution record of sea level change. Fossilized microatolls can also be dated using Radiocarbon dating. Such methods can help to reconstruct Holocene sea levels.
Increasing sea temperatures in tropical regions (~1 degree C) the last century have caused major coral bleaching, death, and therefore shrinking coral populations since although they are able to adapt and acclimate, it is uncertain if this evolutionary process will happen quickly enough to prevent major reduction of their numbers.
Though coral have large sexually-reproducing populations, their evolution can be slowed by abundant asexual reproduction. Gene flow is variable among coral species. According to the biogeography of coral species gene flow cannot be counted on as a dependable source of adaptation as they are very stationary organisms. Also, coral longevity might factor into their adaptivity.
However, adaptation to climate changes has been demonstrated in many cases. These are usually due to a shift in coral and zooxanthellae genotypes. These shifts in allelic frequencies have progressed toward more tolerant types of zooxanthellae. Scientists found that a certain scleractinian zooxanthella is becoming more common where sea temperature is high. Symbionts able to tolerate warmer water seem to photosynthesise more slowly, implying an evolutionary trade-off.
In the Gulf of Mexico, where sea temperatures are rising, cold-sensitive staghorn and elkhorn coral have shifted in location.
Not only have the symbionts and specific species been shown to shift, but there seems to be a certain growth rate favorable to selection. Slower-growing but more heat-tolerant corals have become more common. The changes in temperature and acclimation are complex. Some reefs in current shadows represent a refugium location that will help them adjust to the disparity in the environment even if eventually the temperatures may rise more quickly there than in other locations. This separation of populations by climatic barriers causes a realized niche to shrink greatly in comparison to the old fundamental niche.
Aquaria.
The saltwater fishkeeping hobby has increasingly expanded, over recent years, to include reef tanks, fish tanks that include large amounts of live rock on which coral is allowed to grow and spread. These tanks are either kept in a natural-like state, with algae (sometimes in the form of an algae scrubber) and a deep sand bed providing filtration, or as "show tanks", with the rock kept largely bare of the algae and microfauna that would normally populate it, in order to appear neat and clean.
The most popular kind of coral kept is soft coral, especially zoanthids and mushroom corals, which are especially easy to grow and propagate in a wide variety of conditions, because they originate in enclosed parts of reefs where water conditions vary and lighting may be less reliable and direct. More serious fishkeepers may keep small polyp stony coral, which is from open, brightly lit reef conditions and therefore much more demanding, while large polyp stony coral is a sort of compromise between the two.
Aquaculture.
Coral aquaculture, also known as "coral farming" or "coral gardening", is the cultivation of corals for commercial purposes or coral reef restoration. Aquaculture is showing promise as a potentially effective tool for restoring coral reefs, which have been declining around the world. The process bypasses the early growth stages of corals when they are most at risk of dying. Coral fragments known as "seeds" are grown in nurseries then replanted on the reef. Coral is farmed by coral farmers who live locally to the reefs and farm for reef conservation or for income. It is also farmed by scientists for research, by businesses for the supply of the live and ornamental coral trade and by private aquarium hobbyists.
Gallery.
"Further images: and "

</doc>
<doc id="47702" url="http://en.wikipedia.org/wiki?curid=47702" title="Torture">
Torture

Torture is the act of deliberately inflicting severe physical or psychological pain and possibly injury to an organism, usually to one who is physically restrained or otherwise under the torturer's control or custody and unable to defend against what is being done to him or her. Torture has been carried out or sanctioned by individuals, groups, and states throughout history from ancient times to modern day, and forms of torture can vary greatly in duration from only a few minutes to several days or even longer. Reasons for torture can include punishment, revenge, political re-education, deterrence, interrogation or coercion of the victim or a third party, or simply the sadistic gratification of those carrying out or observing the torture. The need to torture another is thought to be the result of internal psychological pressure in the psyche of the torturer. The torturer may or may not intend to kill or injure the victim, but sometimes torture is deliberately fatal and can precede a murder or serve as a cruel form of capital punishment. In other cases, the torturer may be indifferent to the condition of the victim. Alternatively, some forms of torture are designed to inflict psychological pain or leave as little physical injury or evidence as possible while achieving the same psychological devastation. Depending on the aim, even a form of torture that is intentionally fatal may be prolonged to allow the victim to suffer as long as possible (such as half-hanging).
Although torture was sanctioned by some states historically, it is prohibited under international law and the domestic laws of most countries, as developed in the mid-20th century. It is considered to be a violation of human rights, and is declared to be unacceptable by Article 5 of the UN Universal Declaration of Human Rights. Signatories of the Geneva Conventions of 1949 and the Additional Protocols I and II of 8 June 1977 officially agree not to torture captured persons in armed conflicts, whether international or internal. Torture is also prohibited by the United Nations Convention Against Torture, which has been ratified by 157 countries.
National and international legal prohibitions on torture derive from a consensus that torture and similar ill-treatment are immoral, as well as impractical. Despite these international conventions, organizations that monitor abuses of human rights (e.g., Amnesty International, the International Rehabilitation Council for Torture Victims, etc.) report widespread use condoned by states in many regions of the world. Amnesty International estimates that at least 81 world governments currently practice torture, some of them openly. Historically, in those countries where torture was legally supported and officially condoned, wealthy patrons sponsored the creation of extraordinarily ingenious devices and techniques of torture.
Definitions.
International level.
UN Convention Against Torture.
The United Nations Convention against Torture and Other Cruel, Inhuman or Degrading Treatment or Punishment, which is currently ratified by 157 countries, and in force since June 26, 1987, provides a broad definition of torture. Article 1.1 of the UN Convention Against Torture reads:
For the purpose of this Convention, the term "torture" means any act by which severe pain or suffering, whether physical or mental, is intentionally inflicted on a person for such purposes as obtaining from him, or a third person, information or a confession, punishing him for an act he or a third person has committed or is suspected of having committed, or intimidating or coercing him or a third person, or for any reason based on discrimination of any kind, when such pain or suffering is inflicted by or at the instigation of or with the consent or acquiescence of a public official or other person acting in an official capacity. It does not include pain or suffering arising only from, inherent in, or incidental to, lawful sanctions.
This definition was restricted to apply only to nations and to government-sponsored torture and clearly limits the torture to that perpetrated, directly or indirectly, by those acting in an official capacity, such as government personnel, law enforcement personnel, medical personnel, military personnel, or politicians. It appears to exclude:
Declaration of Tokyo.
An even broader definition was used in the 1975 Declaration of Tokyo regarding the participation of medical professionals in acts of torture:
This definition includes torture as part of domestic violence or ritualistic abuse, as well as in criminal activities.
Rome Statute of the International Criminal Court.
The Rome Statute is the treaty that set up the International Criminal Court (ICC). The treaty was adopted at a diplomatic conference in Rome on 17 July 1998 and went into effect on 1 July 2002, and is currently ratified by 123 nations. The Rome Statute provides a simplest definition of torture regarding the prosecution of war criminals by the International Criminal Court. Paragraph 1 under Article 7(e) of the Rome Statute provides that:
"Torture" means the intentional infliction of severe pain or suffering, whether physical or mental, upon a person in the custody or under the control of the accused; except that torture shall not include pain or suffering arising only from, inherent in or incidental to, lawful sanctions;
Inter-American Convention to Prevent and Punish Torture.
The Inter-American Convention to Prevent and Punish Torture, currently ratified by 18 nations of the Americas and in force since 28 February 1987, defines torture more expansively than the United Nations Convention Against Torture. Article 2 of the Inter-American Convention reads:
For the purposes of this Convention, torture shall be understood to be any act intentionally performed whereby physical or mental pain or suffering is inflicted on a person for purposes of criminal investigation, as a means of intimidation, as personal punishment, as a preventive measure, as a penalty, or for any other purpose. Torture shall also be understood to be the use of methods upon a person intended to obliterate the personality of the victim or to diminish his physical or mental capacities, even if they do not cause physical pain or mental anguish.
The concept of torture shall not include physical or mental pain or suffering that is inherent in or solely the consequence of lawful measures, provided that they do not include the performance of the acts or use of the methods referred to in this article.
Amnesty International.
Since 1973, Amnesty International has adopted the simplest, broadest definition of torture. It reads:
Municipal level.
United States.
U.S. Code § 2340.
Title 18 of the United States Code contains the definition of torture in  , which is only applicable to persons committing or attempting to commit torture outside of the United States. It reads:
As used in this chapter—
In order for the United States to assume control over this jurisdiction, the alleged offender must be a U.S. national or the alleged offender must be present in the United States, irrespective of the nationality of the victim or alleged offender. Any person who conspires to commit an offense shall be subject to the same penalties (other than the penalty of death) as the penalties prescribed for an actual act or attempting to commit an act, the commission of which was the object of the conspiracy.
Torture Victim Protection Act of 1991.
The Torture Victim Protection Act of 1991 provides remedies to individuals who are victims of torture by persons acting in an official capacity of any foreign nation. The definition is similar to the U.S. Code § 2340, which reads:
(b) TORTURE.—For the purposes of this Act—
History.
In the study of the history of torture, some authorities rigidly divide the history of torture per se from the history of capital punishment, while noting that most forms of capital punishment are extremely painful. Torture grew into an ornate discipline, where calibrated violence served two functions: to investigate and produce confessions and to attack the body as a form of punishment. Entire populaces of towns would show up to witness an execution by torture in the public square. Those who had been "spared" torture were commonly locked barefooted into the stocks, where children took delight in rubbing feces into their hair and mouths and between their toes.
Deliberately painful methods of torture and execution for severe crimes were taken for granted as part of justice until the development of Humanism in 17th century philosophy, and "cruel and unusual punishment" came to be denounced in the English Bill of Rights of 1689. The Age of Enlightenment in the western world further developed the idea of universal human rights. The adoption of the Universal Declaration of Human Rights in 1948 marks the recognition at least nominally of a general ban of torture by all UN member states.
Its effect in practice is limited, however, as the Declaration is not ratified officially and does not have legally binding character in international law, but is rather considered part of customary international law. Hundreds of countries still practice torture today. Some countries have legally codified it, and others have claimed that it is not practiced, while maintaining the use of torture in secret.
Since the days when Roman law prevailed throughout Europe, torture has been regarded as subtending three classes or degrees of suffering. First-degree torture typically took the forms of whipping and beating but did not mutilate the body. The most prevalent modern example is bastinado, a technique of beating or whipping the soles of the bare feet. Second-degree torture consisted almost entirely of crushing devices and procedures, including exceptionally clever screw presses or "bone vises" that crushed thumbs, toes, knees, feet, even teeth and skulls in a wide variety of ways. A wide array of "boots"—machines variously, ingeniously designed to slowly squeeze feet until their bones shattered—are quite representative. Finally, third-degree tortures savagely mutilated the body in numerous dreadful ways, incorporating spikes, blades, boiling oil, and extremely carefully controlled fire. The serrated iron tongue shredder; the red-hot copper basin for destroying eyesight (abacination, "q.v."); and the stocks that forcibly held the prisoner's naked feet, glistening with lard, directly over red-hot coals (foot roasting, "q.v.") until the skin and foot muscles were burnt black and the bones fell to ashes are examples of torture in the third degree.
Antiquity.
Over time torture has been used as a means of reform, inducing public terror, interrogation, spectacle, and sadistic pleasure. The ancient Greeks and Romans used torture for interrogation. Until the 2nd century AD, torture was used only on slaves (with a few exceptions). After this point it began to be extended to all members of the lower classes. A slave's testimony was admissible "only" if extracted by torture, on the assumption that slaves could not be trusted to reveal the truth voluntarily. This torture occurred to break the bond between a master and his slave. Slaves were thought to be incapable of lying under torture.
Modern scholars find the concept of torture to be compatible with society's concept of Justice during the time of the Roman Empire. Romans, Jews, Egyptians and many other cultures during that time included torture as part of their justice system. Romans had crucifixion, Jews had stoning and Egyptians had desert sun death. All these acts of torture were considered necessary (to deter others) or good (to punish the immoral).
Middle Ages.
Medieval and early modern European courts used torture, depending on the crime of the accused and his or her social status. Torture was deemed a legitimate means to extract confessions or to obtain the names of accomplices or other information about a crime, although many confessions were greatly invalid due to the victim being forced to confess under great agony and pressure. It was permitted by law only if there was already half-proof against the accused. Torture was used in continental Europe to obtain corroborating evidence in the form of a confession when other evidence already existed. Often, defendants already sentenced to death would be tortured to force them to disclose the names of accomplices. Torture in the Medieval Inquisition began in 1252 with a papal bull Ad Extirpanda and ended in 1816 when another papal bull forbade its use.
A highly esteemed torture in the times of the Inquisition as a good means of interrogating "taciturn" heretics and wizards was the interrogation chair.
Torture was usually conducted in secret, in underground dungeons. By contrast, torturous executions were typically public, and woodcuts of English prisoners being hanged, drawn and quartered show large crowds of spectators, as do paintings of Spanish auto-da-fé executions, in which heretics were burned at the stake. Torture was also used during this time period as a means of reform, spectacle, to induce fear into the public, and most popularly as a punishment for treason.
Medieval torture devices were varied. "They hanged them by the thumbs, or by the head, and hung fires on their feet; they put knotted strings about their heads, and writhed them so that it went to the brain ... Some they put in a chest that was short, and narrow, and shallow, and put sharp stones therein, and pressed the man therein, so that they broke all his limbs ... I neither can nor may tell all the wounds or all the tortures which they inflicted on wretched men in this land." 
Being hung upside down, burning, crushing, breaking of limbs, and drowning were all popular medieval tortures. Specific devices were also created and used during this time, including the rack, the Pear (also mentioned in Grose's Dictionary of the Vulgar Tongue (1811) as "Choak ["sic."] Pears," and described as being "formerly used in Holland."), thumbscrews, animals like rats, iron chair, and the cat o nine tails.
Early modern period.
During the early modern period, the torture of witches took place. In 1613, Anton Praetorius described the situation of the prisoners in the dungeons in his book "Gründlicher Bericht Von Zauberey und Zauberern" ("Thorough Report about Sorcery and Sorcerers"). He was one of the first to protest against all means of torture.
While secular courts often treated suspects ferociously, Will and Ariel Durant argued in "The Age of Faith" that many of the most vicious procedures were inflicted upon pious heretics by even more pious friars. The Dominicans gained a reputation as some of the most fearsomely innovative torturers in medieval Spain.
Torture was continued by Protestants during the Renaissance against teachers who they viewed as heretics. In 1547 John Calvin had Jacques Gruet arrested in Geneva, Switzerland. Under torture he confessed to several crimes including writing an anonymous letter left in the pulpit which threatened death to Calvin and his associates. The Council of Geneva had him beheaded with Calvin's approval. Suspected witches were also tortured and burnt by Protestant leaders, though more often they were banished from the city, as well as suspected spreaders of the plague, which was considered a more serious crime.
In England the trial by jury developed considerable freedom in evaluating evidence and condemning on circumstantial evidence, making torture to extort confessions unnecessary. For this reason in England a regularized system of judicial torture never existed and its use was limited to political cases. Torture was in theory not permitted under English law, but in Tudor and early Stuart times, under certain conditions, torture was used in England. For example the confession of Marc Smeaton at the trial of Anne Boleyn was presented in written form only, either to hide from the court that Smeaton had been tortured on the rack for four hours, or because Thomas Cromwell was worried that he would recant his confession if cross-examined. When Guy Fawkes was arrested for his role in the Gunpowder Plot of 1605 he was tortured until he revealed all he knew about the plot. This was not so much to extract a confession, which was not needed to prove his guilt, but to extract from him the names of his fellow conspirators. By this time torture was not routine in England and a special warrant from King James I was needed before he could be tortured. The wording of the warrant shows some concerns for humanitarian considerations, the severity of the methods of interrogation were to be increased gradually until the interrogators were sure that Fawkes had told all he knew. In the end this did not help Fawkes much as he was broken on the only rack in England, which was in the Tower of London.
The privy council attempted to have John Felton who stabbed George Villiers, 1st Duke of Buckingham to death in 1628 questioned under torture on the rack, but the judges resisted, unanimously declaring its use to be contrary to the laws of England. Torture was abolished in England around 1640 (except "peine forte et dure", which was abolished in 1772).
In Colonial America, women were sentenced to the stocks with wooden clips on their tongues or subjected to the "dunking stool" for the gender-specific crime of talking too much. Certain Native American peoples, especially in the area that later became the eastern half of the United States, engaged in the sacrificial torture of war captives.
In the 17th century the number of incidents of judicial torture decreased in many European regions. Johann Graefe in 1624 published "Tribunal Reformation", a case against torture. Cesare Beccaria, an Italian lawyer, published in 1764 "An Essay on Crimes and Punishments", in which he argued that torture unjustly punished the innocent and should be unnecessary in proving guilt. Voltaire (1694–1778) also fiercely condemned torture in some of his essays.
While in Egypt in 1798, Napoleon Bonaparte wrote to Major-General Berthier regarding the validity of torture as an interrogation tool:
The barbarous custom of whipping men suspected of having important secrets to reveal must be abolished. It has always been recognized that this method of interrogation, by putting men to the torture, is useless. The wretches say whatever comes into their heads and whatever they think one wants to believe. Consequently, the Commander-in-Chief forbids the use of a method which is contrary to reason and humanity.
European states abolished torture from their statutory law in the late 18th and early 19th centuries. England abolished torture in about 1640 (except peine forte et dure, which England only abolished in 1772), Scotland in 1708, Prussia in 1740, Denmark around 1770, Russia in 1774, Austria and Polish-Lithuanian Commonwealth in 1776, Italy in 1786, France in 1789, and Baden in 1831. Sweden was the first to do so in 1722 and the Netherlands did the same in 1798. Bavaria abolished torture in 1806 and Württemberg in 1809. In Spain, the Napoleonic conquest put an end to torture in 1808. Norway abolished it in 1819 and Portugal in 1826. The last European jurisdictions to abolish legal torture were Portugal (1828) and the canton of Glarus in Switzerland (1851).
Tortures included the chevalet, in which an accused witch sat on a pointed metal horse with weights strung from her feet. Sexual humiliation torture included forced sitting on red-hot stools. Gresillons, also called pennywinkis in Scotland, or pilliwinks, crushed the tips of fingers and toes in a vise-like device. The Spanish Boot, or "leg-screw", used mostly in Germany and Scotland, was a steel boot that was placed over the leg of the accused and was tightened. The pressure from the squeezing of the boot would break the shin bone in pieces. An anonymous Scotsman called it "The most severe and cruel pain in the world". Ingenious variants of the Spanish boot were also designed to slowly crush feet between iron plates armed with terrifying spikes. The echelle more commonly known as the "ladder" or "rack" was a long table that the accused would lie upon and be stretched violently. The torture was used so intensely that on many occasions the victim's limbs would be pulled out of the socket and, at times, the limbs would even be torn from the body entirely. On some special occasions a tortillon was used in conjunction with the ladder which would severely squeeze and mutilate the genitals at the same time as the stretching was occurring. Similar to the ladder was the "lift". It too stretched the limbs of the accused, this time however the victim's feet were strapped to the ground and their arms were tied behind their back before a rope was tied to their hands and lifted upwards. This caused the arms to break before the horrific portion of the stretching began. Finally, the judicial system of King James favored the use of the turkas, an ingenious iron instrument for destroying fingernails and toenails. The sharp point of the instrument was first pushed under the nail to the root, splitting the nail down the centerline. Pincers then grabbed either edge of the destroyed nail and slowly, very slowly, tore it from the finger or toe.
Since 1948.
Modern sensibilities have been shaped by a profound reaction to the war crimes and crimes against humanity committed by the Axis Powers and Allied Powers in the Second World War, which have led to a sweeping international rejection of most if not all aspects of the practice. Even as many states engage in torture, few wish to be described as doing so, either to their own citizens or to the international community. A variety of devices bridge this gap, including state denial, "secret police", "need to know", a denial that given treatments are torturous in nature, appeal to various laws (national or international), the use of jurisdictional argument and the claim of "overriding need". Throughout history and today, many states have engaged in torture, albeit unofficially. Torture ranges from physical, psychological, political, interrogations techniques, and also includes rape of anyone outside of law enforcement.
According to scholar Ervand Abrahamian, although there were several decades of prohibition of torture that spread from Europe to most parts of the world, by the 1980s, the taboo against torture was broken and torture "returned with a vengeance," propelled in part by television and an opportunity to break political prisoners and broadcast the resulting public recantations of their political beliefs for "ideological warfare, political mobilization, and the need to win 'hearts and minds.'"
In the years 2004 and 2005, over 16 countries were documented using torture. In an attempt to bring global awareness, Human Rights Watch, has created an internet site to alert people to news and multimedia publications about torture occurring world wide. The International Rehabilitation Council for Torture Victims [IRCT] made a global analysis of torture based on [Amnesty International, 2001], [Human Rights Watch, 2003], [United Nations, 2002], [U.S. Department of State, 2002] yearly human rights reports. These reports showed that torture and ill treatment are consistently report based on all four sources in 32 countries. At least two reports the use of torture and ill treatment in at least 80 countries. These reports confirm the assumption that torture occurs in a quarter of the world's countries on a regular basis. This global prevalence of torture is estimated on the magnitude of particular high-risk groups and the amount of torture used by these groups. "Such groups comprise refugees and persons who are or have been under torture." 
According to professor Darius Rejali, although dictatorships may have used tortured "more, and more indiscriminately", it was modern democracies, "the United States, Britain, and France" who "pioneered and exported techniques that have become the lingua franca of modern torture: methods that leave no marks." The practice of torture used as the oppression against political opponents or could be a part of criminal investigation or interrogation techniques in order to obtain the desired information and keep law enforcement empowered over everyday citizens.
The modern concept of torture methods that leave no physical evidence is noted in 1995 by the Diagnostic and Statistical Manual of Mental Disorders DSM-IV within the changing definition of Post-traumatic Stress Disorder PTSD. This revised definition included psychological torture stating: "Expresses concern that the Diagnostic and Statistical Manual of Mental Disorders definition of posttraumatic stress disorder does not include those forms of psychological torture in which the physical integrity of a person is not threatened. It is suggested that any diagnostic criterion that characterizes the traumatic stressors leading to PTSD should be expressed in such a way that psychological forms of torture are included."
After 1995, the sweeping definition of changes from 'any act by which severe pain or suffering, whether "mental" or "physical",is intentionally inflicted on a person' to including the terms "psychological torture" and including examples such as,interrogation techniques range from sleep deprivation, solitary confinement, fear and humiliation to severe sexual and cultural humiliation and use of threats and phobias to induce fear of death or injury.
Torture still occurs in liberal democracies despite several international treaties such as the International Covenant on Civil and Political Rights and the UN Convention Against Torture making torture illegal. Despite such international conventions, torture cases continue to arise such as the 2004 Abu Ghraib torture and prisoner abuse scandal committed by personnel of the United States Army. The U.S. Constitution and U.S. law prohibits the use of torture, yet such human rights violations occurred during the War on Terror under the euphemism Enhanced interrogation. The United States has revised the previous torture policy in 2009 under the Obama Administration. This revision revokes the Executive Order 13440 of July 20, 2007, under which the incident at Abu Ghraib and prisoner abuse occurred. Executive Order 13491 of January 22, 2009 further defines United States policy on torture and interrogation techniques in an attempt to further prevent another torture incident.
According to the findings of Dr. Christian Davenport of the University of Notre Dame, Professor William Moore of Florida State University, and David Armstrong of Oxford University during their torture research, evidence suggests that non-governmental organizations have played the most determinant factor for stopping torture once it gets started. Preliminary research suggests that it is civil society, not government institutions, that can stop torture once it has begun. Due to this inability to control abuse and torture in society creates an imperfect Democracy and compliance with internationally agreed upon standards for civil and political rights. Organizations such as Amnesty International serve to expose widespread human rights violations and hold the individuals accountable to the international community.
Historical methods of execution and capital punishment.
For most of recorded history, capital punishments were often cruel and inhumane. Severe historical penalties include breaking wheel, boiling to death, flaying, slow slicing, disembowelment, crucifixion, impalement, crushing, stoning, execution by burning, dismemberment, sawing, decapitation, scaphism, or necklacing.
"Slow slicing", or "death by/of a thousand cuts", was a form of execution used in China from roughly 900 AD to its abolition in 1905. According to apocryphal lore, "líng che" began when the torturer, wielding an extremely sharp knife, began by putting out the eyes, rendering the condemned incapable of seeing the remainder of the torture and, presumably, adding considerably to the psychological terror of the procedure. Successive rather minor cuts chopped off ears, nose, tongue, fingers, toes, and such before proceeding to grosser cuts that removed large collops of flesh from more sizable parts, e.g., thighs and shoulders. The entire process was said to last three days, and to total 3,600 cuts. The heavily carved bodies of the deceased were then put on a parade for a show in the public.
"Impalement" was a method of torture and execution whereby a person is pierced with a long stake. The penetration can be through the sides, from the rectum, or through the mouth. This method would lead to slow, painful, death. Often, the victim was hoisted into the air after partial impalement. Gravity and the victim's own struggles would cause him to slide down the pole. Death could take many days. Impalement was frequently practiced in Asia and Europe throughout the Middle Ages. Vlad III Dracula and Ivan the Terrible have passed into legend as major users of the method.
The "breaking wheel" was a torturous capital punishment device used in the Middle Ages and early modern times for public execution by cudgeling to death, especially in France and Germany. In France the condemned were placed on a cart-wheel with their limbs stretched out along the spokes over two sturdy wooden beams. The wheel was made to slowly revolve. Through the openings between the spokes, the executioner hit the victim with an iron hammer that could easily break the victim's bones. This process was repeated several times per limb. Once his bones were broken, he was left on the wheel to die. It could take hours, even days, before shock and dehydration caused death. The punishment was abolished in Germany as late as 1827.
Etymology.
The word 'torture' comes from the French "torture", originating in the Late Latin "tortura" and ultimately deriving the past participle of "torquere" meaning 'to twist'. The word is also used loosely to describe more ordinary discomforts that would be accurately described as tedious rather than painful; for example, "making this spreadsheet was torture!"
According to Diderot's "Encyclopédie", torture was also referred to as "the question" in seventeenth century France. This term is derived from torture's use in criminal cases: as the accused is tortured, the torturers would typically ask questions to the accused in an effort to learn more about the crime.
Religious perspectives.
Roman Catholic Church.
The Catholic Church enjoined secular rulers to exterminate the heretics (lest the ruler’s Catholic subjects be absolved from their allegiance), and in order to coerce heretics or witnesses "into confessing their errors and accusing others," decided to sanction governments to use in the medieval inquisitions the very methods of torture which they utilized in other criminal procedures. However, Pope Innocent IV, in the Bull Ad exstirpanda (May 15, 1252), stipulated that the inquisitors were to "stop short of danger to life or limb". The modern Church's views regarding torture have changed drastically which are generally associated with the Enlightenment. Thus, the "Catechism of the Catholic Church" (published in 1994) condemns the use of torture as a grave violation of Human Rights. In No. 2297-2298 it states:
 Torture, which uses physical or moral violence to extract confessions, punish the guilty, frighten opponents, or satisfy hatred is contrary to respect for the person and for human dignity... In times past, cruel practices were commonly used by legitimate governments to maintain law and order, often without protest from the Pastors of the Church, who themselves adopted in their own tribunals the prescriptions of Roman law concerning torture. Regrettable as these facts are, the Church always taught the duty of clemency and mercy. She forbade clerics to shed blood. In recent times it has become evident that these cruel practices were neither necessary for public order, nor in conformity with the legitimate rights of the human person. On the contrary, these practices led to ones even more degrading. It is necessary to work for their abolition. We must pray for the victims and their tormentors.
Sharia law.
The prevalent view among jurists of sharia law is that torture is permissible only for the maintenance of law and order. Violations in some stricter jurisdictions include acts of public indecency or immorality, which can result in floggings. Other sharia-derived punishments that might overlap with widely held notions of torture include the various forms of hudud. The Shiite practise of self-flaggelation has been described as torture to onself, something that is viewed as unquranic by most other Muslims.
Laws against torture.
On 10 December 1948, the United Nations General Assembly adopted the Universal Declaration of Human Rights (UDHR). Article 5 states, "No one shall be subjected to torture or to cruel, inhuman or degrading treatment or punishment." Since that time, a number of other international treaties have been adopted to prevent the use of torture. The most notable treaties relating to torture are the United Nations Convention Against Torture and the Geneva Conventions of 1949 and their Additional Protocols I and II of 8 June 1977.
United Nations Convention Against Torture.
The United Nations Convention against Torture and Other Cruel, Inhuman or Degrading Treatment or Punishment came into force in June 1987. The most relevant articles are Articles 1, 2, 3, and 16.
 Article 1<br>
1. For the purposes of this Convention, the term "torture" means any act by which severe pain or suffering, whether physical or mental, is intentionally inflicted on a person for such purposes as obtaining from him or a third person information or a confession, punishing him for an act he or a third person has committed or is suspected of having committed, or intimidating or coercing him or a third person, or for any reason based on discrimination of any kind, when such pain or suffering is inflicted by or at the instigation of or with the consent or acquiescence of a public official or other person acting in an official capacity. It does not include pain or suffering arising only from, inherent in or incidental to lawful sanctions.<br>
2. This article is without prejudice to any international instrument or national legislation which does or may contain provisions of wider application.
 Article 2<br>
1. Each State Party shall take effective legislative, administrative, judicial or other measures to prevent acts of torture in any territory under its jurisdiction.<br>
2. No exceptional circumstances whatsoever, whether a state of war or a threat of war, internal political instability or any other public emergency, may be invoked as a justification of torture.<br>
3. An order from a superior officer or a public authority may not be invoked as a justification of torture.
 Article 3<br>
1. No State Party shall expel, return ("refouler") or extradite a person to another State where there are substantial grounds for believing that he would be in danger of being subjected to torture.<br>
2. For the purpose of determining whether there are such grounds, the competent authorities shall take into account all relevant considerations including, where applicable, the existence in the State concerned of a consistent pattern of gross, flagrant or mass violations of human rights.
 Article 16<br>
1. Each State Party shall undertake to prevent in any territory under its jurisdiction other acts of cruel, inhuman or degrading treatment or punishment which do not amount to torture as defined in article I, when such acts are committed by or at the instigation of or with the consent or acquiescence of a public official or other person acting in an official capacity. In particular, the obligations contained in articles 10, 11, 12 and 13 shall apply with the substitution for references to torture of references to other forms of cruel, inhuman or degrading treatment or punishment.<br>
2. The provisions of this Convention are without prejudice to the provisions of any other international instrument or national law which prohibits cruel, inhuman or degrading treatment or punishment or which relates to extradition or expulsion.
Note several points:
As of February 7, 2015, 157 states are parties to the Convention against Torture.
Optional Protocol to the UN Convention Against Torture.
The Optional Protocol to the Convention Against Torture (OPCAT) entered into force on 22 June 2006 as an important addition to the UNCAT. As stated in Article 1, the purpose of the protocol is to "establish a system of regular visits undertaken by independent international and national bodies to places where people are deprived of their liberty, in order to prevent torture and other cruel, inhuman or degrading treatment or punishment." Each state ratifying the OPCAT, according to Article 17, is responsible for creating or maintaining at least one independent national preventive mechanism for torture prevention at the domestic level.
Rome Statute of the International Criminal Court.
The Rome Statute, which established the International Criminal Court (ICC), provides for criminal prosecution of individuals responsible for genocide, war crimes, and crimes against humanity. The statute defines torture as "intentional infliction of severe pain or suffering, whether physical or mental, upon a person in the custody or under the control of the accused; except that torture shall not include pain or suffering arising only from, inherent in or incidental to, lawful sanctions". Under Article 7 of the statute, torture may be considered a crime against humanity "when committed as part of a widespread or systematic attack directed against any civilian population, with knowledge of the attack". Article 8 of the statute provides that torture may also, under certain circumstances, be prosecuted as a war crime.
The ICC came into existence on 1 July 2002 and can only prosecute crimes committed on or after that date. The court can generally exercise jurisdiction only in cases where the accused is a national of a state party to the Rome Statute, the alleged crime took place on the territory of a state party, or a situation is referred to the court by the United Nations Security Council. The court is designed to complement existing national judicial systems: it can exercise its jurisdiction only when national courts are unwilling or unable to investigate or prosecute such crimes. Primary responsibility to investigate and punish crimes is therefore reserved to individual states.
Geneva Conventions.
The four Geneva Conventions provide protection for people who fall into enemy hands.
The conventions do not clearly divide people into combatant and non-combatant roles. The conventions refer to:
The first (GCI), second (GCII), third (GCIII), and fourth (GCIV) Geneva Conventions are the four most relevant for the treatment of the victims of conflicts. All treaties states in Article 3, in similar wording, that in a non-international armed conflict, "Persons taking no active part in the hostilities, including members of armed forces who have laid down their arms... shall in all circumstances be treated humanely." The treaty also states that there must not be any "violence to life and person, in particular murder of all kinds, mutilation, cruel treatment and torture" or "outrages upon personal dignity, in particular humiliating and degrading treatment".
GCI covers wounded combatants in an international armed conflict. Under Article 12, members of the armed forces who are sick or wounded "shall be respected and protected in all circumstances. They shall be treated humanely and cared for by the Party to the conflict in whose power they may be, without any adverse distinction founded on sex, race, nationality, religion, political opinions, or any other similar criteria. Any attempts upon their lives, or violence to their persons, shall be strictly prohibited; in particular, they shall not be murdered or exterminated, subjected to torture or to biological experiments".
GCII covers shipwreck survivors at sea in an international armed conflict. Under Article 12, persons "who are at sea and who are wounded, sick or shipwrecked, shall be respected and protected in all circumstances, it being understood that the term "shipwreck" means shipwreck from any cause and includes forced landings at sea by or from aircraft. Such persons shall be treated humanely and cared for by the Parties to the conflict in whose power they may be, without any adverse distinction founded on sex, race, nationality, religion, political opinions, or any other similar criteria. Any attempts upon their lives, or violence to their persons, shall be strictly prohibited; in particular, they shall not be murdered or exterminated, subjected to torture or to biological experiments".
GCIII covers the treatment of prisoners of war (POWs) in an international armed conflict. In particular, Article 17 says that "No physical or mental torture, nor any other form of coercion, may be inflicted on prisoners of war to secure from them information of any kind whatever. Prisoners of war who refuse to answer may not be threatened, insulted or exposed to unpleasant or disadvantageous treatment of any kind." POW status under GCIII has far fewer exemptions than "Protected Person" status under GCIV. Captured combatants in an international armed conflict automatically have the protection of GCIII and are POWs under GCIII unless they are determined by a competent tribunal to not be a POW (GCIII Article 5).
GCIV covers most civilians in an international armed conflict, and says they are usually "Protected Persons" (see exemptions section immediately after this for those who are not). Under Article 32, civilians have the right to protection from "murder, torture, corporal punishments, mutilation and medical or scientific experiments...but also to any other measures of brutality whether applied by civilian or military agents."
Geneva Convention IV exemptions.
GCIV provides an important exemption:
 Where in the territory of a Party to the conflict, the latter is satisfied that an individual protected person is definitely suspected of or engaged in activities hostile to the security of the State, such individual person shall not be entitled to claim such rights and privileges under the present Convention [ie GCIV] as would ... be prejudicial to the security of such State ... In each case, such persons shall nevertheless be treated with humanity (GCIV Article 5)
Also, nationals of a State not bound by the Convention are not protected by it, and nationals of a neutral State in the territory of a combatant State, and nationals of a co-belligerent State, cannot claim the protection of GCIV if their home state has normal diplomatic representation in the State that holds them (Article 4), as their diplomatic representatives can take steps to protect them. The requirement to treat persons with "humanity" implies that it is still prohibited to torture individuals not protected by the Convention.
The George W. Bush administration afforded fewer protections, under GCIII, to detainees in the "War on Terror" by codifying the legal status of an "unlawful combatant". If there is a question of whether a person is a lawful combatant, he (or she) must be treated as a POW "until their status has been determined by a competent tribunal" (GCIII Article 5). If the tribunal decides that he is an unlawful combatant, he is not considered a protected person under GCIII. However, if he is a protected person under GCIV he still has some protection under GCIV, and must be "treated with humanity and, in case of trial, shall not be deprived of the rights of fair and regular trial prescribed by the present Convention" (GCIV Article 5).
Additional Protocols to the Geneva Conventions.
There are two additional protocols to the Geneva Convention: Protocol I (1977), relating to the protection of victims of international armed conflicts and Protocol II (1977), relating to the protection of victims of non-international armed conflicts. These clarify and extend the definitions in some areas, but to date many countries, including the United States, have either not signed them or have not ratified them.
Protocol I does not mention torture but it does affect the treatment of POWs and Protected Persons. In Article 5, the protocol explicitly involves "the appointment of Protecting Powers and of their substitute" to monitor that the Parties to the conflict are enforcing the Conventions. The protocol also broadens the definition of a lawful combatant in wars against "alien occupation, colonial domination and racist regimes" to include those who carry arms openly but are not wearing uniforms, so that they are now lawful combatants and protected by the Geneva Conventions—although only if the Occupying Power has ratified Protocol I. Under the original conventions, combatants without a recognizable insignia could be treated as war criminals, and potentially be executed. It also mentions spies, and defines who is a mercenary. Mercenaries and spies are considered an unlawful combatant, and not protected by the same conventions.
Protocol II "develops and supplements Article 3 [relating to the protection of victims of non-international armed conflicts] common to the Geneva Conventions of 12 August 1949 without modifying its existing conditions of application" (Article 1). Any person who does not take part in or ceased to take part in hostilities is entitled to humane treatment. Among the acts prohibited against these persons are, "Violence to the life, health and physical or mental well-being of persons, in particular murder as well as cruel treatment such as torture, mutilation or any form of corporal punishment" (Article 4.a), "Outrages upon personal dignity, in particular humiliating and degrading treatment, rape, enforced prostitution and any form of indecent assault" (Article 4.e), and "Threats to commit any of the foregoing acts" (Article 4.h). Clauses in other articles implore humane treatment of enemy personnel in an internal conflict. These have a bearing on torture, but no other clauses explicitly mention torture.
Other conventions.
In accordance with the optional UN Standard Minimum Rules for the Treatment of Prisoners (1955), "corporal punishment, punishment by placing in a dark cell, and all cruel, inhuman or degrading punishments shall be completely prohibited as punishments for disciplinary offences." The International Covenant on Civil and Political Rights, (16 December 1966), explicitly prohibits torture and "cruel, inhuman or degrading treatment or punishment" by signatories.
In 1950 during the Cold War, the participating member states of the Council of Europe signed the European Convention on Human Rights. The treaty was based on the UDHR. It included the provision for a court to interpret the treaty, and Article 3 "Prohibition of torture" stated; "No one shall be subjected to torture or to inhuman or degrading treatment or punishment."
In 1978, the European Court of Human Rights ruled that the five techniques of "sensory deprivation" were not torture as laid out in Article 3 of the European Convention on Human Rights, but were "inhuman or degrading treatment" (see Accusations of use of torture by United Kingdom for details). This case occurred nine years before the United Nations Convention Against Torture came into force and had an influence on thinking about what constitutes torture ever since.
On 26 November 1987, the member states of the Council of Europe, meeting at Strasbourg, adopted the European Convention for the Prevention of Torture and Inhuman or Degrading Treatment or Punishment (ECPT). Two additional Protocols amended the Convention, which entered into force on 1 March 2002. The Convention set up the Committee for the Prevention of Torture to oversee compliance with its provisions.
The Inter-American Convention to Prevent and Punish Torture, currently ratified by 18 nations of the Americas and in force since 28 February 1987, defines torture more expansively than the United Nations Convention Against Torture.
 For the purposes of this Convention, torture shall be understood to be any act intentionally performed whereby physical or mental pain or suffering is inflicted on a person for purposes of criminal investigation, as a means of intimidation, as personal punishment, as a preventive measure, as a penalty, or for any other purpose. Torture shall also be understood to be the use of methods upon a person intended to obliterate the personality of the victim or to diminish his physical or mental capacities, even if they do not cause physical pain or mental anguish.
The concept of torture shall not include physical or mental pain or suffering that is inherent in or solely the consequence of lawful measures, provided that they do not include the performance of the acts or use of the methods referred to in this article.
Supervision of anti-torture treaties.
The Istanbul Protocol, an official UN document, is the first set of international guidelines for documentation of torture and its consequences. It became a United Nations official document in 1999.
Under the provisions of OPCAT that entered into force on 22 June 2006 independent international and national bodies regularly visit places where people are deprived of their liberty, to prevent torture and other cruel, inhuman or degrading treatment or punishment. Each state that ratified the OPCAT, according to Article 17, is responsible for creating or maintaining at least one independent national preventative mechanism for torture prevention at the domestic level.
The European Committee for the Prevention of Torture, citing Article 1 of the European Convention for the Prevention of Torture, states that it will, "by means of visits, examine the treatment of persons deprived of their liberty with a view to strengthening, if necessary, the protection of such persons from torture and from inhuman or degrading treatment or punishment".
In times of armed conflict between a signatory of the Geneva Conventions and another party, delegates of the International Committee of the Red Cross (ICRC) monitor the compliance of signatories to the Geneva Conventions, which includes monitoring the use of torture. Human rights organizations, such as Amnesty International, the World Organization Against Torture, and Association for the Prevention of Torture work actively to stop the use of torture throughout the world and publish reports on any activities they consider to be torture.
Municipal law.
States that ratified the United Nations Convention Against Torture have a treaty obligation to include the provisions into municipal law. The laws of many states therefore formally prohibit torture. However, such "de jure" legal provisions are by no means a proof that, "de facto", the signatory country does not use torture. To prevent torture, many legal systems have a right against self-incrimination or explicitly prohibit undue force when dealing with suspects.
The French 1789 Declaration of the Rights of Man and of the Citizen, of constitutional value, prohibits submitting suspects to any hardship not necessary to secure his or her person.
The U.S. Constitution and U.S. law prohibits the use of unwarranted force or coercion against any person who is subject to interrogation, detention, or arrest. The Fifth Amendment to the United States Constitution includes protection against self-incrimination, which states that "[n]o person...shall be compelled in any criminal case to be a witness against himself". This serves as the basis of the Miranda warning, which U.S. law enforcement personnel issue to individuals upon their arrest. Additionally, the U.S. Constitution's Eighth Amendment forbids the use of "cruel and unusual punishments," which is widely interpreted as prohibiting torture. Finally, 18 U.S.C. § 2340 "et seq." define and forbid torture committed by U.S. nationals outside the United States or non-U.S. nationals who are present in the United States. As the United States recognizes customary international law, or the law of nations, the U.S. Alien Tort Claims Act and the Torture Victim Protection Act also provides legal remedies for victims of torture outside of the United States. Specifically, the status of torturers under the law of the United States, as determined by a famous legal decision in 1980, Filártiga v. Peña-Irala, 630 F.2d 876 (1980), is that, "the torturer has become, like the pirate and the slave trader before him, hostis humani generis, an enemy of all mankind."
Exclusion of evidence obtained under torture.
International law.
Article 15 of the 1984 United Nations Convention Against Torture specify that:
 Each State Party shall ensure that any statement which is established to have been made as a result of torture shall not be invoked as evidence in any proceedings, except against a person accused of torture as evidence that the statement was made.
A similar provision is also found in Article 10 of the 1985 Inter-American Convention to Prevent and Punish Torture:
 No statement that is verified as having been obtained through torture shall be admissible as evidence in a legal proceeding, except in a legal action taken against a person or persons accused of having elicited it through acts of torture, and only as evidence that the accused obtained such statement by such means.
These provisions have the double dissuasive effect of nullifying any utility in using torture with the purpose of eliciting confession, as well as confirming that should a person extract statements by torture, this can be used against him or her in criminal proceedings. The reason for this because experience has shown that under torture, or even under a threat of torture, a person will say or do anything solely to avoid the pain. As a result, there is no way to know whether or not the resulting statement is actually correct. If any court relies on any evidence obtained from torture regardless of validity, it provides an incentive for state officials to force a confession, creating a marketplace for torture, both domestically and overseas.
Within national borders.
Most states have prohibit their legal systems from accepting evidence that is extracted by torture. The question of the use of evidence obtained under torture has arisen in connection with prosecutions during the War on Terror in the United Kingdom and the United States.
United Kingdom.
In September 2011, United Kingdom involvement in torture overseas was brought to light with the unearthing of top secret documents by Human Rights Watch in Libya. Chief Executive of Freedom from Torture Keith Best stated: 'If verified, they show the head of counter-terrorism at MI6 engaged in fawning dialogue with Gaddafi’s former intelligence chief, Musa Kusa, about how "glad" Britain was to help deliver into his hands the Libyan dissident Abdel Hakim Belhadj' on .
During a House of Commons debate on 7 July 2009, MP David Davis accused the UK government of outsourcing torture, by allowing Rangzieb Ahmed to leave the country (even though they had evidence against him upon which he was later convicted for terrorism) to Pakistan, where it is said the Inter-Services Intelligence was given the go ahead by the British intelligence agencies to torture Ahmed. Davis further accused the government of trying to gag Ahmed, stopping him coming forward with his accusations, after he had been imprisoned back in the UK. He said, there was "an alleged request to drop his allegations of torture: if he did that, they could get his sentence cut and possibly give him some money. If this request to drop the torture case is true, it is frankly monstrous. It would at the very least be a criminal misuse of the powers and funds under the Government's Contest strategy, and at worst a conspiracy to pervert the course of justice."
In 2003, the United Kingdom's Ambassador to Uzbekistan, Craig Murray, suggested that it was "wrong to use information gleaned from torture".
The unanimous Law Lords judgment on 8 December 2005 confirmed this position. They ruled that, under English law tradition, "torture and its fruits" could not be used in court. But the information thus obtained could be used by the British police and security services as "it would be ludicrous for them to disregard information about a ticking bomb if it had been procured by torture." The Law Lords thus dismissed concerns about the validity of information obtained under torture, which have been expressed by various security agents and human rights activists.
Murray's accusations did not lead to any investigation by his employer, the FCO, and he resigned after disciplinary action was taken against him in 2004. The Foreign and Commonwealth Office itself is being investigated by the National Audit Office because of accusations that it has victimized, bullied and intimidated its own staff.
Murray later stated that he felt that he had unwittingly stumbled upon what has been called "torture by proxy". He thought that Western countries moved people to regimes and nations where it was known that information would be extracted by torture, and made available to them.
Murray states that he was aware from August 2002 "that the CIA were bringing in detainees to Tashkent from Bagram airport Afghanistan, who were handed over to the Uzbek security services (SNB). I presumed at the time that these were all Uzbek nationals — that may have been a false presumption. I knew that the CIA were obtaining intelligence from their subsequent interrogation by the SNB." He goes on to say that he did not know at the time that any non-Uzbek nationals were flown to Uzbekistan and although he has studied the reports by several journalists and finds their reports credible he is not a firsthand authority on this issue.
United States.
In May 2008, Susan J. Crawford, the official overseeing prosecutions before the Guantanamo military commissions, declined to refer for trial the case of Mohammed al-Qahtani because she said, "we tortured [him]." Crawford said that a combination of techniques with clear medical consequences amounted to the legal definition of torture, and that torture "tainted everything going forward."
On October 28, 2008, Guantanamo military judge Stephen R. Henley ruled that the government cannot use statements made as a result of torture in the military commission case against Afghan national Mohammed Jawad. The judge held that Jawad's alleged confession to throwing a grenade at two U.S. service members and an Afghan interpreter was obtained after armed Afghan officials on December 17, 2002, threatened to kill Jawad and his family. The government had previously told the judge that Jawad's alleged confession while in Afghan custody was central to the case against him. Hina Shamsi, staff attorney with the American Civil Liberties Union National Security Project stated: "We welcome the judge's decision that death threats constitute torture and that evidence obtained as a result must be excluded from trial. Unfortunately, evidence obtained through torture and coercion is pervasive in military commission cases that, by design, disregard the most fundamental due process rights, and no single decision can cure that." A month later, on November 19, the judge again rejected evidence gathered through coercive interrogations in the military commission case against Afghan national Mohammed Jawad, holding that the evidence collected while Jawad was in U.S. custody on December 17–18, 2002, cannot be admitted in his trial, mainly because the U.S. interrogator had blindfolded and hooded Jawad in order to frighten him.
In the 2010 New York trial of Ahmed Khalfan Ghailani who was accused of complicity in the 1998 bombings of U.S. embassies in Tanzania and Kenya, Judge Lewis A. Kaplan ruled evidence obtained under coercion inadmissible. The ruling excluded an important witness, whose name had been extracted from the defendant under duress. The jury acquitted him of 280 charges and convicted on only one charge of conspiracy.
Aspects.
Ethical arguments.
Torture has been criticized on humanitarian and moral grounds, on the grounds that evidence extracted by torture is unreliable, and because torture corrupts institutions that tolerate it. Besides degrading the victim, torture debases the torturer: American advisors alarmed at torture by their South Vietnamese allies early in the Vietnam War concluded that "if a commander allowed his officers and men to fall in to these vices [they] would pursue them for their own sake, for the perverse pleasure they drew from them." The consequent degeneracy destroyed discipline and morale: "[a] soldier had to learn that he existed to uphold law and order, not to undermine it."
Organizations like Amnesty International argue that the universal legal prohibition is based on a universal philosophical consensus that torture and ill-treatment are repugnant, abhorrent, and immoral. But since shortly after the September 11, 2001 attacks there has been a debate in the United States about whether torture is justified in some circumstances. Some people, such as Alan M. Dershowitz and Mirko Bagaric, have argued the need for information outweighs the moral and ethical arguments against torture. However, after coercive practices were banned, interrogators in Iraq saw an increase of 50 percent more high-value intelligence. Maj. Gen. Geoffrey D. Miller, the American commander in charge of detentions and interrogations, stated "a rapport-based interrogation that recognizes respect and dignity, and having very well-trained interrogators, is the basis by which you develop intelligence rapidly and increase the validity of that intelligence."" Others including Robert Mueller, FBI Director since 5 July 2001, have pointed out that despite former Bush Administration claims that waterboarding has "disrupted a number of attacks, maybe dozens of attacks", they do not believe that evidence gained by the U.S. government through what supporters of the techniques call "enhanced interrogation" has disrupted a single attack and no one has come up with a documented example of lives saved thanks to these techniques. On 19 June 2009, the US government announced that it was delaying the scheduled release of declassified portions of a report by the CIA Inspector General that reportedly cast doubt on the effectiveness of the "enhanced interrogation" techniques employed by CIA interrogators, according to references to the report contained in several Bush-era Justice Department memos declassified in the Spring of 2009 by the US Justice Department.
The ticking time bomb scenario, a thought experiment, asks what to do to a captured terrorist who has placed a nuclear bomb in a populated area. If the terrorist is tortured, he may explain how to defuse the bomb. The scenario asks if it is ethical to torture the terrorist. A 2006 BBC poll held in 25 nations gauged support for each of the following positions:
An average of 59% of people worldwide rejected torture. However there was a clear divide between those countries with strong rejection of torture (such as Italy, where only 14% supported torture) and nations where rejection was less strong. Often this lessened rejection is found in countries severely and frequently threatened by terrorist attacks. E.g., Israel, despite its Supreme Court outlawing torture in 1999, showed 43% supporting torture, but 48% opposing, India showed 37% supporting torture and only 23% opposing.
Within nations there is a clear divide between the positions of members of different ethnic groups, religions, and political affiliations, sometimes reflecting distinctions between groups considering themselves threatened or victimized by terror acts and those from the alleged perpetrator groups. For example, the study found that among Jews in Israel 53% favored some degree of torture and only 39% wanted strong rules against torture while Muslims in Israel were overwhelmingly against any use of torture, unlike Muslims polled elsewhere. Differences in general political views also can matter. In one 2006 survey by the Scripps Center at Ohio University, 66% of Americans who identified themselves as strongly Republican supported torture, compared to 24% of those who identified themselves as strongly Democratic. In a 2005 U.S. survey 72% of American Catholics supported the use of torture in some circumstances compared to 51% of American secularists. A Pew survey in 2009 similarly found that the religiously unaffiliated are the least likely (40 percent) to support torture, and that the more a person claims to attend church, the more likely he or she is to condone torture; among racial/religious groups, white evangelical Protestants were far and away the most likely (62 percent) to support inflicting pain as a tool of interrogation.
A "Today/Gallup poll" "found that sizable majorities of Americans disagree with tactics ranging from leaving prisoners naked and chained in uncomfortable positions for hours, to trying to make a prisoner think he was being drowned".
There are also different attitudes as to what constitutes torture, as revealed in an ABC News/Washington Post poll, where more than half of the Americans polled thought that techniques such as sleep deprivation were not torture.
In practice, so-called "enhanced interrogation" techniques were employed by the CIA in situations that did not involve the "ticking time bomb" scenario that has been the subject of opinion polls and public debate. In April 2009 a former senior U.S. intelligence official and a former Army psychiatrist stated that the Bush administration applied pressure on interrogators to use the "enhanced interrogation" techniques on detainees to find evidence of cooperation between al Qaida and the late Iraqi dictator Saddam Hussein's regime. The purported link between al Qaida and Hussein's regime, which has been disproven, was a key political justification for the Iraq War. On 13 May 2009, former NBC News investigative producer Robert Windrem reported, as confirmed by former Iraq Survey Group leader Charles Duelfer, that the Vice President's Office suggested that an interrogation team led by Duelfer waterboard an Iraqi prisoner suspected of knowing about a relationship between al Qaeda and Saddam.
On 14 February 2010, in an appearance on ABC's This Week, Vice-President Dick Cheney reiterated his support of waterboarding and "enhanced interrogation" techniques for captured terrorist suspects, saying, "I was and remain a strong proponent of our enhanced interrogation program."
Pressed by the BBC in 2010 on his personal view of waterboarding, Presidential Advisor Karl Rove said: "I'm proud that we kept the world safer than it was, by the use of these techniques. They’re appropriate, they're in conformity with our international requirements and with US law."
A 15-month investigation by the Guardian and BBC Arabic, published on March 2013, disclosed that "the US sent a veteran of the "dirty wars" in Central America to oversee Iraqi commando units involved in acts of torture during the American-led occupation. These American citizens could theoretically be tried by the International Criminal Court even though the US is not a signatory. But it would have to be referred by the UN security council and, given that the US has a veto on the council, this hypothesis is very improbable." Reprieve Legal Director Kat Craig said: "This latest exposé of human rights abuses shows that torture is endemic to US foreign policy; these are considered and deliberate acts, not only sanctioned but developed by the highest echelons of US security service."
Utilitarian arguments against torture.
There is a strong utilitarian argument against torture; namely, that there is simply no scientific evidence supporting its effectiveness.
The lack of scientific basis for the effectiveness of torture as an interrogation techniques is summarized in a 2006 Intelligence Science Board report titled "EDUCING INFORMATION, Interrogation: Science and Art, Foundations for the Future".
On the other hand, some have pointed to some specific cases where torture has elicited true information.
Rejection.
A famous example of rejection of the use of torture was cited by the Argentine National Commission on the Disappearance of Persons in whose report, Italian general Carlo Alberto Dalla Chiesa was reputed to have said in connection with the investigation of the disappearance of prime minister Aldo Moro, "Italy can survive the loss of Aldo Moro. It would not survive the introduction of torture."
Secrecy.
Before the emergence of modern policing, torture was an important aspect of policing and the use of it was openly sanctioned and acknowledged by the authority. The Economist magazine proposed that one of the reasons torture endures is that torture does indeed work in some instances to extract information/confession, if those who are being tortured are indeed guilty.
Depending on the culture, torture has at times been carried on in silence (official silence ), semi-silence (known but not spoken about), or openly acknowledged in public (to instill fear and obedience).
In the 21st century, even when states sanction their interrogation methods, torturers often work outside the law. For this reason, some prefer methods that, while unpleasant, leave victims alive and unmarked. A victim with no visible damage may lack credibility when telling tales of torture, whereas a person missing fingernails or eyes can easily prove claims of torture. Mental torture, however can leave scars just as deep and long-lasting as physical torture. Professional torturers in some countries have used techniques such as electrical shock, asphyxiation, heat, cold, noise, and sleep deprivation, which leave little evidence, although in other contexts torture frequently results in horrific mutilation or death. However the most common and prevalent form of torture worldwide in both developed and under-developed countries is beating.
Methods and devices.
Psychological torture uses non-physical methods that cause psychological suffering. Its effects are not immediately apparent unless they alter the behavior of the tortured person. Since there is no international political consensus on what constitutes psychological torture, it is often overlooked, denied, and referred to by different names.
Psychological torture is less well known than physical torture and tends to be subtle and much easier to conceal. In practice the distinctions between physical and psychological torture are often blurred. Physical torture is the inflicting of severe pain or suffering on a person. In contrast, psychological torture is directed at the psyche with calculated violations of psychological needs, along with deep damage to psychological structures and the breakage of beliefs underpinning normal sanity. Torturers often inflict both types of torture in combination to compound the associated effects.
Psychological torture also includes deliberate use of extreme stressors and situations such as mock execution, shunning, violation of deep-seated social or sexual norms and taboos, or extended solitary confinement. Because psychological torture needs no physical violence to be effective, it is possible to induce severe psychological pain, suffering, and trauma with no externally visible effects.
Rape and other forms of sexual abuse are often used as methods of torture for interrogative or punitive purposes.
In medical torture, medical practitioners use torture to judge what victims can endure, to apply treatments that enhance torture, or act as torturers in their own right. Josef Mengele and Shirō Ishii were infamous during and after World War II for their involvement in medical torture and murder.
Pharmacological torture is the use of drugs to produce psychological or physical pain or discomfort. Tickle torture is an unusual form of torture which nevertheless has been documented, and can be both physically and psychologically painful.
Murder.
Torture murder involves torture to the point of murder as for punishment in law enforcement agencies of countries that allow torture. Murderers might also torture their victims to death for sadistic reasons. Some terrorist groups tortures—typically commencing with the forcible extraction of all ten fingernails, all ten toenails, and all thirty-two teeth—before executing them by such barbaric techniques as slow decapitation via butcher knife. Ancient conceptual forerunners of torture murder include drawing and quartering and flaying.
Effects.
The consequences of torture reach far beyond immediate pain. Many victims suffer from post-traumatic stress disorder (PTSD), which includes symptoms such as flashbacks (or intrusive thoughts), severe anxiety, insomnia, nightmares, depression and memory lapses. Torture victims often feel guilt and shame, triggered by the humiliation they have endured. Many feel that they have betrayed themselves or their friends and family. All such symptoms are normal human responses to abnormal and inhuman treatment.
Organizations like Freedom from Torture and the Center for Victims of Torture try to help survivors of torture obtain medical treatment and to gain forensic medical evidence to obtain political asylum in a safe country and/or to prosecute the perpetrators.
Torture is often difficult to prove, particularly when some time has passed between the event and a medical examination, or when the torturers are immune from prosecution. Many torturers around the world use methods designed to have a maximum psychological impact while leaving only minimal physical traces. Medical and Human Rights Organizations worldwide have collaborated to produce the Istanbul Protocol, a document designed to outline common torture methods, consequences of torture, and medico-legal examination techniques. Typically deaths due to torture are shown in an autopsy as being due to "natural causes" like heart attack, inflammation, or embolism due to extreme stress.
For survivors, torture often leads to lasting mental and physical health problems.
Physical problems can be wide-ranging, e.g. sexually transmitted diseases, musculo-skeletal problems, brain injury, post-traumatic epilepsy and dementia or chronic pain syndromes.
Mental health problems are equally wide-ranging; common are post-traumatic stress disorder, depression and anxiety disorder.
Psychic deadness, erasure of intersubjectivity, refusal of meaning-making, perversion of agency, and an inability to bear desire constitute the core features of the post-traumatic psychic landscape of torture.
 The most terrible, intractable, legacy of torture is the killing of desire - that is, of curiosity, of the impulse for connection and meaning-making, of the capacity for mutuality, of the tolerance for ambiguity and ambivalence. For these patients, to know another mind is unbearable. To connect with another is irrelevant. They are entrapped in what was born(e) during their trauma, as they perpetuate the erasure of meaning, re-enact the dynamics of annihilation through sadomasochistic, narcissistic, paranoid, or self-deadening modes of relating, and mobilize their agency toward warding off mutuality, goodness, hope and connection. In brief, they live to prove death. And it is this perversion of agency and desire that constitutes the deepest post-traumatic injury, and the most invisible and pernicious of human-rights violations.
On 19 August 2007, the American Psychology Association (APA) voted to bar participation, to intervene to stop, and to report involvement in a wide variety of interrogation techniques as torture, including "using mock executions, simulated drowning, sexual and religious humiliation, stress positions or sleep deprivation", as well as "the exploitation of prisoners' phobias, the use of mind-altering drugs, hooding, forced nakedness, the use of dogs to frighten detainees, exposing prisoners to extreme heat and cold, physical assault and threatening the use of such techniques against a prisoner or a prisoner's family."
However, the APA rejected a stronger resolution that sought to prohibit "all psychologist involvement, either direct or indirect, in any interrogations at U.S. detention centers for foreign detainees or citizens detained outside normal legal channels." That resolution would have placed the APA alongside the American Medical Association and the American Psychiatric Association in limiting professional involvement in such settings to direct patient care. The APA echoed the Bush administration by condemning isolation, sleep deprivation, and sensory deprivation or over-stimulation only when they are likely to cause lasting harm.
Psychiatric treatment of torture-related medical problems might require a wide range of expertise and often specialized experience. Common treatments are psychotropic medication, e.g. SSRI antidepressants, counseling, Cognitive Behavioural Therapy, family systems therapy and physiotherapy.
Rehabilitation.
The aim of rehabilitation is to empower the torture victim to resume as full a life as possible. Rebuilding the life of someone whose dignity has been destroyed takes time and as a result long-term material, medical, psychological and social support is needed.
Treatment must be a coordinated effort that covers both physical and psychological aspects. It is important to take into consideration the patients' needs, problems, expectations, views and cultural references.
The consequences of torture are likely to be influenced by many internal and external factors. Therefore, rehabilitation needs to employ different treatment approaches, taking into account the victims' individual needs, as well as the cultural, social and political environment.
Rehabilitation centres around the world, notably the members of the International Rehabilitation Council for Torture Victims, commonly offer multi-disciplinary support and counselling, including:
In the case of asylum seekers and refugees, the services may also include assisting in documentation of torture for the asylum decision, language classes and help in finding somewhere to live and work.
Rehabilitation of secondary survivors.
In the worst case, torture can affect several generations. The physical and mental after-effects of torture often place great strain on the entire family and society. Children are particularly vulnerable. They often suffer from feelings of guilt or personal responsibility for what has happened. Therefore, other members of the survivor’s family – in particular the spouse and children – are also offered treatment and counselling.
Broken societies.
In some instances, whole societies can be more or less traumatized where torture has been used in a systematic and widespread manner. In general, after years of repression, conflict and war, regular support networks and structures have often been broken or destroyed.
Providing psychosocial support and redress to survivors of torture and trauma can help reconstruct broken societies. "Rehabilitation centres therefore play a key role in promoting democracy, co-existence and respect for human rights. They provide support and hope, and act as a symbol of triumph over the manmade terror of torture which can hold back the development of democracy of entire societies."
Further reading.
</dl>

</doc>
