<doc id="45660" url="http://en.wikipedia.org/wiki?curid=45660" title="Swami Vivekananda">
Swami Vivekananda

Swami Vivekananda (], "Shāmi Bibekānondo"; 12 January 1863 – 4 July 1902), born Narendra Nath Datta (]), was an Indian Hindu monk and chief disciple of the 19th-century saint Ramakrishna. He was a key figure in the introduction of the Indian philosophies of Vedanta and Yoga to the Western world and is credited with raising interfaith awareness, bringing Hinduism to the status of a major world religion during the late 19th century. He was a major force in the revival of Hinduism in India, and contributed to the concept of nationalism in colonial India. Vivekananda founded the Ramakrishna Math and the Ramakrishna Mission. He is perhaps best known for his speech which began, "Sisters and brothers of America ...," in which he introduced Hinduism at the Parliament of the World's Religions in Chicago in 1893.
Born into an aristocratic Bengali family of Calcutta, Vivekananda was inclined towards spirituality. He was influenced by his guru, Ramakrishna, from whom he learnt that all living beings were an embodiment of the divine self; therefore, service to God could be rendered by service to mankind. After Ramakrishna's death, Vivekananda toured the Indian subcontinent extensively and acquired first-hand knowledge of the conditions prevailing in British India. He later travelled to the United States, representing India at the 1893 Parliament of the World Religions. Vivekananda conducted hundreds of public and private lectures and classes, disseminating tenets of Hindu philosophy in the United States, England and Europe. In India, Vivekananda is regarded as a patriotic saint and his birthday is celebrated there as National Youth Day.
Early life (1863–88).
Birth and childhood.
Vivekananda was born Narendranath Datta (shortened to Narendra or Naren) at his ancestral home at 3 Gourmohan Mukherjee Street in Calcutta, the capital of British India, on 12 January 1863 during the Makar Sankranti festival. He belonged to a traditional Bengali Kayastha family and was one of nine siblings. His father, Vishwanath Datta, was an attorney at the Calcutta High Court. Durgacharan Datta, Narendra's grandfather, was a Sanskrit and Persian scholar who left his family and became a monk at age twenty-five. His mother, Bhuvaneswari Devi, was a devout housewife. The progressive, rational attitude of Narendra's father and the religious temperament of his mother helped shape his thinking and personality.
Narendranath was interested spiritually from a young age, and used to meditate before the images of deities such as Shiva, Rama, and Sita. He was fascinated by wandering ascetics and monks. Narendra was naughty and restless as a child, and his parents often had difficulty controlling him. His mother said, "I prayed to Shiva for a son and he has sent me one of his demons".
Education.
In 1871 Narendra enrolled at Ishwar Chandra Vidyasagar's Metropolitan Institution, where he studied until his family moved to Raipur in 1877. In 1879, after his family's return to Calcutta, he was the only student to receive first-division marks in the Presidency College entrance examination. He was an avid reader in a wide range of subjects, including philosophy, religion, history, social science, art and literature. He was also interested in Hindu scriptures, including the Vedas, the Upanishads, the Bhagavad Gita, the "Ramayana", the "Mahabharata" and the Puranas. Narendra was trained in Indian classical music, and regularly participated in physical exercise, sports and organised activities.
Narendra studied Western logic, Western philosophy and European history at the General Assembly's Institution (now known as the Scottish Church College). In 1881 he passed the Fine Arts examination, and completed a Bachelor of Arts degree in 1884. Narendra studied the works of David Hume, Immanuel Kant, Johann Gottlieb Fichte, Baruch Spinoza, Georg W. F. Hegel, Arthur Schopenhauer, Auguste Comte, John Stuart Mill and Charles Darwin. He became fascinated with the evolutionism of Herbert Spencer and corresponded with him, translating Spencer's book "Education" (1861) into Bengali. While studying Western philosophers, he also learned Sanskrit scriptures and Bengali literature. William Hastie (principal of General Assembly's Institution) wrote, "Narendra is really a genius. I have travelled far and wide but I have never come across a lad of his talents and possibilities, even in German universities, among philosophical students". Some accounts have called Narendra a "shrutidhara" (a person with a prodigious memory).
Spiritual apprenticeship.
Narendra became a member of a Freemasonry lodge and a breakaway faction of the Brahmo Samaj led by Keshub Chandra Sen and Debendranath Tagore. His initial beliefs were shaped by Brahmo concepts, which included belief in a formless God and the deprecation of idolatry.
At this time, Narendra met Debendranath Tagore (the leader of Brahmo Samaj) and asked if he had seen God. Instead of answering his question, Tagore said "My boy, you have the "Yogi"‍ '​s eyes." Not satisfied with his knowledge of philosophy, Narendra wondered if God and religion could be made a part of one's growing experiences and deeply internalised. He asked several prominent Calcutta residents if they had come "face to face with God", but none of their answers satisfied him.
With Ramakrishna.
Narendra's first introduction to Ramakrishna occurred in a literature class at General Assembly's Institution when he heard Professor William Hastie lecturing on William Wordsworth's poem, "The Excursion". While explaining the word "trance" in the poem, Hastie suggested that his students visit Ramakrishna of Dakshineswar to understand the true meaning of trance. This prompted some of his students (including Narendra) to visit Ramakrishna.
In November 1881. when Narendra was preparing for his upcoming F. A. examination, Ram Chandra Datta accompanied him to Surendra Nath Mitra's, house where Ramakrishna was invited to deliver a lecture. At this meeting, Ramakrishna asked young Narendra to sing. Impressed by his singing talent, he asked Narendra to come to Dakshineshwar. Narendra did not consider this their first meeting, and neither man mentioned this meeting later.
In late 1881 or early 1882, Narendra went to Dakshineswar with two friends and met Ramakrishna. This meeting proved to be a turning point in his life. Although he did not initially accept Ramakrishna as his teacher and rebelled against his ideas, he was attracted by his personality and began to frequently visit him at Dakshineswar. He initially saw Ramakrishna's ecstasies and visions as "mere figments of imagination" and "hallucinations". As a member of Brahmo Samaj, he opposed idol worship, polytheism and Ramakrishna's worship of Kali. He even rejected the Advaita Vedanta of "identity with the absolute" as blasphemy and madness, and often ridiculed the idea. Narendra tested Ramakrishna, who faced his arguments patiently: "Try to see the truth from all angles", he replied.
Narendra's father's sudden death in 1884 left the family bankrupt; creditors began demanding the repayment of loans, and relatives threatened to evict the family from their ancestral home. Narendra, once a son of a well-to-do family, became one of the poorest students in his college. He unsuccessfully tried to find work and questioned God's existence, but found solace in Ramakrishna and his visits to Dakshineswar increased.
One day Narendra requested Ramakrishna to pray to goddess Kali for their family's financial welfare. Ramakrishna suggested him to go to the temple himself and pray. Following Ramakrishna's suggestion, he went to the temple thrice, but failed to pray for any kind of worldly necessities and ultimately prayed for true knowledge and devotion from the goddess. Narendra gradually grew ready to renounce everything for the sake of realising God, and accepted Ramakrishna as his Guru.
In 1885, Ramakrishna developed throat cancer, and was transferred to Calcutta and (later) to a garden house in Cossipore. Narendra and Ramakrishna's other disciples took care of him during his last days, and Narendra's spiritual education continued. At Cossipore, he experienced "Nirvikalpa samadhi". Narendra and several other disciples received ochre robes from Ramakrishna, forming his first monastic order. He was taught that service to men was the most effective worship of God. Ramakrishna asked him to care for the other monastic disciples, and in turn asked them to see Narendra as their leader. Ramakrishna died in the early-morning hours of 16 August 1886 in Cossipore.
Founding of first Ramakrishna Math at Baranagar.
After Ramakrishna's death, his devotees and admirers stopped supporting his disciples. Unpaid rent accumulated, and Narendra and the other disciples had to find a new place to live. Many returned home, adopting a "Grihastha" (family-oriented) way of life. Narendra decided to convert a dilapidated house at Baranagar into a new "math" (monastery) for the remaining disciples. Rent for the Baranagar Math was low, raised by "holy begging" ("mādhukarī"). The math became the first building of the Ramakrishna Math: the monastery of the monastic order of Ramakrishna. Narendra and other disciples used to spend many hours in practising meditation and religious austerities every day. Narendra later reminisced about the early days of the monastery:
We underwent a lot of religious practice at the Baranagar Math. We used to get up at 3:00 am and become absorbed in "japa" and meditation. What a strong spirit of detachment we had in those days! We had no thought even as to whether the world existed or not.
In 1887, Narendra compiled a Bengali song anthology named "Sangeet Kalpataru" with Vaishnav Charan Basak. Narendra collected and arranged most of the songs of this compilation, but could not finish the work of the book for unfavourable circumstances.
Monastic vows.
In December 1886, the mother of Baburam invited Narendra and his other brother monks to Antpur village. Narendra and the other aspiring monks accepted the invitation and went to Antpur to spend few days. In Antpur, in the Christmas Eve of 1886, Narendra and eight other disciples took formal monastic vows. They decided to live their lives as their master lived. Narendranath took the name "Swami Vivekananda".
Travels in India (1888–93).
In 1888, Narendra left the monastery as a "Parivrâjaka"— the Hindu religious life of a wandering monk, "without fixed abode, without ties, independent and strangers wherever they go". His sole possessions were a kamandalu (water pot), staff and his two favourite books: the "Bhagavad Gita" and "The Imitation of Christ". Narendra travelled extensively in India for five years, visiting centres of learning and acquainting himself with diverse religious traditions and social patterns. He developed sympathy for the suffering and poverty of the people, and resolved to uplift the nation. Living primarily on bhiksha (alms), Narendra travelled on foot and by railway (with tickets bought by admirers). During his travels he met, and stayed with Indians from all religions and walks of life: scholars, "dewans", rajas, Hindus, Muslims, Christians, "paraiyars" (low-caste workers) and government officials.
North.
In 1888 Narendra's first destination was Varanasi, where he visited the places where Gautama Buddha and Adi Shankara preached and met Bengali writer Bhudev Mukhopadhyay and Hindu saint Trailanga Swami. After meeting Vivekananda, Mukhopadhyay said "Such vast experience and insight at such an early age! I am sure he will be a great man". Narendra also met Sanskrit and Vedic scholar Babu Pramadadas Mitra, with whom he corresponded on the interpretation of Hindu scriptures. After leaving Varanasi, he visited Ayodhya, Lucknow, Agra, Vrindavan, Hathras and Rishikesh. When he was staying in Vrindavan, one day, he saw a man smoking a "hookah". He asked to the man to give him a tobacco bowl, but the man refused to do so explaining he was a man of lower caste. Narendra initially accepted his point and started walking, but within few minutes, he started feeling ashamed, as he had been practising "non-duality of soul" for a long time. He returned to the man, once again requested him to give him tobacco boil and despite the man's reluctance, he took the "hookah" from him and started smoking.
While on the way to Haridwar, in September 1888, Narendra stayed at Hathras. There in the railway waiting room Narendra met Sharat Chandra Gupta, a railway station master. Gupta went to Narendra and asked if he was hungry, to which he got a reply in positive. He took Narendra to his home. When Narendra asked him what food he was going to offer, Gupta quoted a Persian poem in reply: "Oh beloved, I shall prepare the most delicious dish with the flesh of my heart". Narendra told Gupta that he had a great mission in life — he wanted to serve his motherland where starvation and poverty stalk millions of people. He narrated his dream of seeing India regaining her old glory. During the conversations Gupta asked Narendra if he could help him anyhow. Narendra immediately replied— "Yes, take up the "kamandalu" and go begging". Gupta understood that he was being asked to renounce his personal interest for the welfare of many. He decided to renounce the world and became a disciple of Narendranath. Narendra and Gupta left Hathras together.
After leaving Hathras Narendra and Gupta first went to Haridwar, and from there travelled to Rishikesh, on foot. Here Narendra initiated Gupta into "Sannyasa" and was named Swami Sadananda. Gupta was the directly initiated monastic disciple of Vivekananda. Vivekananda called him "the child of my spirit".
Meeting with Pavhari Baba.
Between 1888 and 1890, Narendra visited Vaidyanath and Allahabad. In January 1890 he went from Allahabad to Ghazipur and met Pavhari Baba, an "Advaita Vedanta" ascetic who spent much of his time in meditation. At the time he suffered from lumbago, and it was becoming impossible for him to move or sit in meditation. After meeting Baba, Narendra wanted to become his disciple and Baba asked him to stay a few more days at Ghazipur. However, the night before his initiation Narendra had a dream in which Ramakrishna looked at him with a melancholy face. This dream convinced Narendra that no one other than Ramakrishna could be his teacher, and he abandoned the idea of becoming Baba's disciple.
Return to Baranagar Math and Himalayan journey (1890–91).
During the first half of 1890, after the deaths of fellow Ramakrishna disciples Balaram Bose and Suresh Chandra Mitra, Narendra returned to Baranagar Math because of ill health and to arrange for the math's financial support. After finishing his work in July, he left the math (accompanied by fellow monk Swami Akhandananda) for the Himalayas.
This constituted the first phase of a journey which would bring Narendra to the West. He visited the sacred sites of Nainital, Almora, Srinagar, Dehradun, Rishikesh and Haridwar. During these travels, he met Swami Brahmananda, Saradananda, Turiyananda and Advaitananda. They stayed at Meerut for several days engaged in meditation, prayer and study of the scriptures. At the end of January 1891, Narendra left his colleagues and travelled to Delhi.
Rajputana (1891).
After visiting historical sites at Delhi, Narendra started his travel towards Rajputana. In these days, he drew inspiration from the words of the Gautama Buddha— 
<poem>Go forward without a path,
Fearing nothing, caring for nothing!
Wandering alone, like the rhinoceros!
Even as a lion, not trembling at noises,
Even as the wind, not caught in the net,
Even as the lotus leaf, untainted by water,
Do thou wander alone, like the rhinoceros!</poem>
In February 1891, he first went to Alwar, where he was warmly welcomed by the Hindus and the Muslims. There he told a Muslim religion scholar that one significant feature of the Quran is, though it was written a thousand years ago, the book was free from "interpolation" and retained its original purity. When Narendra met the Mangal Singh, king of Alwar, whose outlook was Westernised, Singh challenged Narendra and ridiculed Hindu idol worship. Narendra attempted to explain to him that Hindu worship is symbolic worship, but failed to make the king understand. Then Narendra saw a painting hanging on the wall, it was the a painting of the Singh's deceased father and asked him to spit on it. Singh became angry and retorted how he could spit on his father. Narendra explained, though it was just a painting, not the king himself, it reminds everybody about the king, similarly an idol worshipped by a Hindu is actually a symbolic worship of the Supreme.
From Alwar, Narendra went to Jaipur, where he studied Panini's "Ashtadhyayi" with a Sanskrit scholar. Narendra then went to Ajmer, where he visited the palace of Akbar and the Dargah Sharif. At Mount Abu he met Raja Ajit Singh of Khetri, who became an ardent devotee and supporter. Swami Tathagatananda, a senior monk in the Ramakrishna Order, wrote of their relationship:
... Vivekananda's friendship with Maharaja Ajit Singh of Khetri was enacted against the backdrop of Khetri, a sanctified town in Northern Rajasthan, characterized by its long heroic history and independent spirit. Destiny brought Swamiji and Ajit Singh together on 4 June 1891 at Mount Abu, where their friendship gradually developed through their mutual interest in significant spiritual and secular topics. The friendship intensified when they travelled to Khetri and it became clear that theirs was the most sacred friendship, that of a Guru and his disciple.
At Khetri Narendra delivered discourses to the Raja, became acquainted with pandit Ajjada Adibhatla Narayana Dasu and studied the "Mahābhāṣya" on the sutras of Panini. After two-and-a-half months there, in October 1891 he left for Maharastra.
West (1891–92).
Narendra visited Ahmedabad, Wadhwan and Limbdi; at the former, he completed his studies of Islamic and Jain cultures. At Limbdi he met Thakur Saheb Jaswant Singh, who had been to England and America. From him, Narendra first got the idea of going to the West to preach Vedanta. He visited Junagadh and was the guest of Haridas Viharidas Desai, diwan of the state, who was so charmed by his company that every evening he and all the state officials conversed with Narendra until late at night. Narendra also visited Girnar, Kutch, Porbander, Dwaraka, Palitana, Nadiad, Nadiad ni haveli and Baroda. He remained for nine months at Porbander, furthering his philosophical and Sanskrit studies with learned pandits.
Narendra's next destinations included Mahabaleshwar, Pune, Khandwa and Indore. At Kathiawar he heard of the 1893 Parliament of the World's Religions, and was urged by his followers to attend it. After a brief stay in Bombay in July 1892, he met Bal Gangadhar Tilak during a train journey. After staying with Tilak for a few days in Pune, Narendra travelled to Belgaum in October 1892 and to Panaji and Margao in Goa, spending three days at Rachol Seminary (the oldest convent in Goa, with rare religious manuscripts and printed works in Latin) studying Christian theological works.
South (1892–93).
Narendra later travelled to Bangalore, where he became acquainted with K. Seshadri Iyer (diwan of the Mysore state). Iyer described Narendra as "a magnetic personality and a divine force which were destined to leave their mark on the history of his country". Iyer introduced him to the Maharaja (king) of Mysore Chamaraja Wodeyar. Wodeyar invited Narendra to stay in his palace as a guest. The maharaja gave Narendra a letter of introduction to the diwan of Cochin and a railway ticket.
From Bangalore, Narendra visited Trissur, Kodungalloor and Ernakulam. At Ernakulam he met Chattampi Swamikal, a contemporary of Narayana Guru, in early December 1892. From Ernakulam, Narendra travelled to Trivandrum, Nagercoil and reached Kanyakumari on foot on Christmas Eve 1892. At Kanyakumari, Narendra meditated on the "last bit of Indian rock" (later known as the Narendra Rock Memorial). At Kanyakumari, Narendra had a "vision of one India" (the "Kanyakumari resolve of 1892"). He wrote: "At Cape Camorin sitting in Mother Kumari's temple, sitting on the last bit of Indian rock—I hit upon a plan: We are so many sanyasis wandering about, and teaching the people metaphysics—it is all madness. Did not our "Gurudeva" use to say, 'An empty stomach is no good for religion?' We as a nation have lost our individuality and that is the cause of all mischief in India. We have to raise the masses."
From Kanyakumari, Narendra visited Madurai and had meetings with the Raja of Ramnad Bhaskara Sethupathi. During his meetings, he had extensive discussions on Hindu philosophy with eminent scholars like Mahavidwan R. Raghava Iyengar. The raja became his disciple, urging him to attend the Parliament of Religions in Chicago. From Madurai, Narendra visited Rameswaram, Pondicherry and Madras; there, he met some of his disciples, specially Alasinga Perumal (who played important roles in collecting funds for his voyage to America and later establishing the Ramakrishna Mission in Madras). Perumal went door to door in hopes of getting money for Narendra's travel. With funds collected by his Madras disciples, the kings of Mysore, Ramnad, Khetri, diwans and other followers, Narendra left Bombay for Chicago on 31 May 1893 with the name "Vivekananda" (as suggested by Ajit Singh of Khetri), which means "the bliss of discerning wisdom".
First visit to the West (1893–97).
Vivekananda started his journey to the West on 31 May 1893 and visited several cities in Japan (including Nagasaki, Kobe, Yokohama, Osaka, Kyoto and Tokyo), China and Canada en route to the United States, reaching Chicago on 30 July 1893. However, he was disappointed to learn that no one without credentials from a "bona fide" organisation would be accepted as a delegate. Vivekananda contacted Professor John Henry Wright of Harvard University, who invited him to speak at Harvard. On learning that Vivekananda lacked credentials to speak at the Chicago Parliament, Wright said "To ask for your credentials is like asking the sun to state its right to shine in the heavens". Vivekananda wrote of the professor, "He urged upon me the necessity of going to the Parliament of Religions, which he thought would give an introduction to the nation".
Parliament of the World's Religions.
The Parliament of the World's Religions opened on 11 September 1893 at the Art Institute of Chicago as part of the World's Columbian Exposition. On this day, Vivekananda gave a brief speech representing India and Hinduism. He was initially nervous, bowed to Saraswati (the Hindu goddess of learning) and began his with "Sisters and brothers of America!". At these words, Vivekananda received a two-minute standing ovation from the crowd of seven thousand. When silence was restored he began his address, greeting the youngest of the nations on behalf of "the most ancient order of monks in the world, the Vedic order of sannyasins, a religion which has taught the world both tolerance and universal acceptance". Vivekananda quoted two illustrative passages from the "Shiva mahimna stotram": "As the different streams having their sources in different places all mingle their water in the sea, so, O Lord, the different paths which men take, through different tendencies, various though they appear, crooked or straight, all lead to Thee!" and "Whosoever comes to Me, through whatsoever form, I reach him; all men are struggling through paths that in the end lead to Me." Despite the brevity of his speech, it voiced the spirit and sense of universality of the parliament.
Parliament President John Henry Barrows said, "India, the Mother of religions was represented by Swami Vivekananda, the Orange-monk who exercised the most wonderful influence over his auditors". Vivekananda attracted widespread attention in the press, which called him the "cyclonic monk from India". The "New York Critique" wrote, "He is an orator by divine right, and his strong, intelligent face in its picturesque setting of yellow and orange was hardly less interesting than those earnest words, and the rich, rhythmical utterance he gave them". The "New York Herald" noted, "Vivekananda is undoubtedly the greatest figure in the Parliament of Religions. After hearing him we feel how foolish it is to send missionaries to this learned nation". American newspapers reported Vivekananda as "the greatest figure in the parliament of religions" and "the most popular and influential man in the parliament".
The "Boston Evening Transcript" reported that Vivekananda was "a great favourite at the parliament... if he merely crosses the platform, he is applauded". He spoke at the Parliament on topics related to Hinduism, Buddhism and harmony among religions until the parliament ended on 27 September 1893. Vivekananda's speeches at the Parliament had the common theme of universality, emphasising religious tolerance. He soon became known as a "handsome oriental" and made a huge impression as an orator.
Lecture tours in the U.S. and England.
"I do not come", said Swamiji on one occasion in America, "to convert you to a new belief. I want you to keep your own belief; I want to make the Methodist a better Methodist; the Presbyterian a better Presbyterian; the Unitarian a better Unitarian. I want to teach you to live the truth, to reveal the light within your own soul."
 
After the Parliament of Religions, he toured many parts of the US as a guest. His popularity opened up new views for expanding on "life and religion to thousands". During a question-answer session at Brooklyn Ethical Society, he remarked, "I have a message to the West as Buddha had a message to the East."
Vivekananda spent nearly two years lecturing in the eastern and central United States, primarily in Chicago, Detroit, Boston, and New York. He founded the Vedanta Society of New York in 1894. By spring 1895 his busy, tiring schedule had affected his health. He ended his lecture tours and began giving free, private classes in Vedanta and yoga. Beginning in June 1895, Vivekananda gave private lectures to a dozen of his disciples at Thousand Island Park in New York for two months.
During his first visit to the West he travelled to England twice, in 1895 and 1896, lecturing successfully there. In November 1895 he met Margaret Elizabeth Noble an Irish woman who would become Sister Nivedita. During his second visit to England in May 1896 Vivekananda met Max Müller, a noted Indologist from Oxford University who wrote Ramakrishna's first biography in the West. From England, Vivekananda visited other European countries. In Germany he met Paul Deussen, another Indologist. Vivekananda was offered academic positions in two American universities (one the chair in Eastern Philosophy at Harvard University and a similar position at Columbia University); he declined both, since his duties would conflict with his commitment as a monk.
Vivekananda attracted followers and admirers in the U.S. and Europe, including Josephine MacLeod, William James, Josiah Royce, Robert G. Ingersoll, Nikola Tesla, Lord Kelvin, Harriet Monroe, Ella Wheeler Wilcox, Sarah Bernhardt, Emma Calvé and Hermann Ludwig Ferdinand von Helmholtz. He initiated several followers : Marie Louise (a French woman) became Swami Abhayananda, and Leon Landsberg became Swami Kripananda, so that they could continue the work of the mission of the Vedanta Society. This society even to this day is filled with foreign nationals and is also located in Los Angeles. During his stay in Los Angeles, Vivekananda built a retreat to house Vedanta students. He called it Peace retreat or "Santi Asrama". The American headquarters of the Vedanta Society (one of the twelve) in USA is located in Los Angeles. There is also a Vedantha Press in Hollywood which publishes Hindu scriptures and texts in English. Christina Greenstidel of Detroit was also initiated by Vivekananda with a mantra and she became Sister Christine, and they established a close father–daughter relationship.
From the West, Vivekananda revived his work in India. He regularly corresponded with his followers and brother monks, offering advice and financial support. His letters from this period reflect his campaign of social service, and were strongly worded. He wrote to Swami Akhandananda, "Go from door to door amongst the poor and lower classes of the town of Khetri and teach them religion. Also, let them have oral lessons on geography and such other subjects. No good will come of sitting idle and having princely dishes, and saying "Ramakrishna, O Lord!"—unless you can do some good to the poor". In 1895, Vivekananda founded the periodical "Brahmavadin" to teach the Vedanta. Later, Vivekananda's translation of the first six chapters of "The Imitation of Christ" was published in "Brahmavadin" in 1889. Vivekananda left for India on 16 December 1896 from England with his disciples, Captain and Mrs. Sevier and J.J. Goodwin. On the way they visited France and Italy, and set sail for India from Naples on 30 December 1896. He was later followed to India by Sister Nivedita, who devoted the rest of her life to the education of Indian women and India's independence.
Back in India (1897–99).
The ship from Europe arrived in Colombo, Sri Lanka on 15 January 1897, and Vivekananda received a warm welcome. In Colombo he gave his first public speech in the East, "". From there on, his journey to Calcutta was triumphant. Vivekananda travelled from Colombo to Pamban, Rameswaram, Ramnad, Madurai, Kumbakonam and Madras, delivering lectures. Common people and rajas gave him an enthusiastic reception. During his train travels, people often sat on the rails to force the train to stop so they could hear him. From Madras, he continued his journey to Calcutta and Almora. While in the West, Vivekananda spoke about India's great spiritual heritage; in India, he repeatedly addressed social issues: uplifting the people, eliminating the caste system, promoting science and industrialisation, addressing widespread poverty and ending colonial rule. These lectures, published as "Lectures from Colombo to Almora", demonstrate his nationalistic fervour and spiritual ideology.
On 1 May 1897 in Calcutta, Vivekananda founded the Ramakrishna Mission for social service. Its ideals are based on "", and its governing body consists of the trustees of the Ramakrishna Math (which conducts religious work). Both Ramakrishna Math and Ramakrishna Mission have their headquarters at Belur Math. Vivekananda founded two other monasteries: one in Mayavati in the Himalayas (near Almora), the "Advaita Ashrama" and another in Madras. Two journals were founded: "Prabuddha Bharata" in English and "Udbhodan" in Bengali. That year, famine-relief work was begun by Swami Akhandananda in the Murshidabad district.
Vivekananda earlier inspired Jamsetji Tata to set up a research and educational institution when they travelled together from Yokohama to Chicago on Vivekananda's first visit to the West in 1893. Tata now asked him to head his Research Institute of Science; Vivekananda declined the offer, citing a conflict with his "spiritual interests". He visited Punjab, attempting to mediate an ideological conflict between Arya Samaj (a reformist Hindu movement) and "sanatan" (orthodox Hindus). After brief visits to Lahore, Delhi and Khetri, Vivekananda returned to Calcutta in January 1898. He consolidated the work of the math and trained disciples for several months. Vivekananda composed "Khandana Bhava–Bandhana", a prayer song dedicated to Ramakrishna, in 1898.
Second visit to the West and final years (1899–1902).
Despite declining health, Vivekananda left for the West for a second time in June 1899 accompanied by Sister Nivedita and Swami Turiyananda. Following a brief stay in England, he went to the United States. During this visit, Vivekananda established Vedanta Societies in San Francisco and New York and founded a "shanti ashrama" (peace retreat) in California. He then went to Paris for the Congress of Religions in 1900. His lectures in Paris concerned the worship of the "lingam" and the authenticity of the Bhagavad Gita. Vivekananda then visited Brittany, Vienna, Istanbul, Athens and Egypt. The French philosopher Jules Bois was his host for most of this period, until he returned to Calcutta on 9 December 1900.
After a brief visit to the Advaita Ashrama in Mayavati Vivekananda settled at Belur Math, where he continued co-ordinating the works of Ramakrishna Mission, the math and the work in England and the U.S. He had many visitors, including royalty and politicians. Although Vivekananda was unable to attend the Congress of Religions in 1901 in Japan due to deteriorating health, he made pilgrimages to Bodhgaya and Varanasi. Declining health (including asthma, diabetes and chronic insomnia) restricted his activity.
Death.
On 4 July 1902 (the day of his death) Vivekananda awoke early, went to the chapel at Belur Math and meditated for three hours. He taught "Shukla-Yajur-Veda", Sanskrit grammar and the philosophy of yoga to pupils, later discussing with colleagues a planned Vedic college in the Ramakrishna Math. At seven p.m. Vivekananda went to his room, asking not to be disturbed; he died at 9:10 p.m. while meditating. According to his disciples, Vivekananda attained mahasamādhi; the rupture of a blood vessel in his brain was reported as a possible cause of death. His disciples believed that the rupture was due to his "brahmarandhra" (an opening in the crown of his head) being pierced when he attained "mahasamādhi". Vivekananda fulfilled his prophecy that he would not live forty years. He was cremated on a sandalwood funeral pyre on the bank of the Ganga in Belur, opposite where Ramakrishna was cremated sixteen years earlier.
Teachings and philosophy.
Vivekananda believed that a country's future depends on its people, and his teachings focused on human development. He wanted "to set in motion a machinery which will bring noblest ideas to the doorstep of even the poorest and the meanest". Vivekananda believed that the essence of Hinduism was best expressed in the Vedanta philosophy, based on Adi Shankara's interpretation. He summarised the Vedanta as follows:
Each soul is potentially divine. The goal is to manifest this Divinity within by controlling nature, external and internal. Do this either by work, or worship, or mental discipline, or philosophy—by one, or more, or all of these—and be free.
This is the whole of religion. Doctrines, or dogmas, or rituals, or books, or temples, or forms, are but secondary details.
Vivekananda linked morality with control of the mind, seeing truth, purity and unselfishness as traits which strengthened it. He advised his followers to be holy, unselfish and to have "śraddhā" (faith). Vivekananda supported "brahmacharya" (celibacy), believing it the source of his physical and mental stamina and eloquence. He emphasised that success was an outcome of focused thought and action; in his lectures on Raja Yoga he said, "Take up one idea. Make that one idea your life – think of it, dream of it, live on that idea. Let the brain, muscles, nerves, every part of your body, be full of that idea, and just leave every other idea alone. This is the way to success, that is way great spiritual giants are produced".
Influence and legacy.
Vivekananda revitalised Hinduism within and outside India, and was the principal reason for the enthusiastic reception of yoga, transcendental meditation and other forms of Indian spiritual self-improvement in the West. Agehananda Bharati explained, "...modern Hindus derive their knowledge of Hinduism from Vivekananda, directly or indirectly". Vivekananda espoused the idea that all sects within Hinduism (and all religions) are different paths to the same goal. However, this view has been criticised as an oversimplification of Hinduism.
In the background of emerging nationalism in British-ruled India, Vivekananda crystallised the nationalistic ideal. In the words of social reformer Charles Freer Andrews, "The Swami's intrepid patriotism gave a new colour to the national movement throughout India. More than any other single individual of that period Vivekananda had made his contribution to the new awakening of India". Vivekananda drew attention to the extent of poverty in the country, and maintained that addressing such poverty was a prerequisite for national awakening. His nationalistic ideas influenced many Indian thinkers and leaders. Sri Aurobindo regarded Vivekananda as the one who awakened India spiritually. Mahatma Gandhi counted him among the few Hindu reformers "who have maintained this Hindu religion in a state of splendor by cutting down the dead wood of tradition".
The first governor-general of independent India, Chakravarti Rajagopalachari, said "Vivekananda saved Hinduism, saved India". According to Subhas Chandra Bose, a proponent of armed struggle for Indian independence, Vivekananda was "the maker of modern India"; for Gandhi, Vivekananda's influence increased Gandhi's "love for his country a thousandfold". Vivekananda influenced India's independence movement; his writings inspired freedom fighters such as Netaji Subhas Chandra Bose, Aurobindo Ghose, Bal Gangadhar Tilak and Bagha Jatin and intellectuals such as Aldous Huxley, Christopher Isherwood, Romain Rolland. Many years after Vivekananda's death Rabindranath Tagore told French Nobel laureate Romain Rolland, "If you want to know India, study Vivekananda. In him everything is positive and nothing negative". Rolland wrote, "His words are great music, phrases in the style of Beethoven, stirring rhythms like the march of Händel choruses. I cannot touch these sayings of his, scattered as they are through the pages of books, at thirty years' distance, without receiving a thrill through my body like an electric shock. And what shocks, what transports, must have been produced when in burning words they issued from the lips of the hero!"
Jamsetji Tata was inspired by Vivekananda to establish the Indian Institute of Science, one of India's best-known research universities. Abroad, Vivekananda communicated with orientalist Max Müller, and scientist Nikola Tesla was one of those influenced by his Vedic teachings. While National Youth Day in India is observed on his birthday, 12 January, the day he delivered his masterful speech at the Parliament of Religions, 11 September 1893 is “World Brotherhood Day”. In September 2010, India's Finance Ministry highlighted the relevance of Vivekananda's teachings and values to the modern economic environment. The Union Finance Minister Pranab Mukherjee approved in principle the Swami Vivekananda Values Education Project at a cost of ₹1 billion (), with objectives including involving youth with competitions, essays, discussions and study circles and publishing Vivekananda's works in a number of languages. In 2011, the West Bengal Police Training College was renamed the Swami Vivekananda State Police Academy, West Bengal.
The state technical university in Chhattisgarh has been named the Chhattisgarh Swami Vivekananda Technical University. In 2012, the Raipur airport was renamed Swami Vivekananda Airport.
The 150th birth anniversary of Swami Vivekananda was celebrated in India and abroad. The Ministry of Youth Affairs and Sports in India officially observed 2013 as the occasion in a declaration. Year-long events and programs were organised by branches of the Ramakrishna Math, the Ramakrishna Mission, the central and state governments in India, educational institutions and youth groups. Bengali film director Tutu (Utpal) Sinha made a film, "" as a tribute for his 150th birth anniversary.
Literary works.
Vivekananda was a powerful orator and writer in English and Bengali; most of his published works were compiled from lectures given around the world. He was a singer and a poet, "A singer, a painter, a wonderful master of language and a poet, Vivekananda was a complete artist." composing many songs and poems (including his favourite, "Kali the Mother"). Vivekananda blended humour with his teachings, and his language was lucid. His Bengali writings testify to his belief that words (spoken or written) should clarify ideas, rather than demonstrating the speaker (or writer's) knowledge.
"Bartaman Bharat" meaning "Present Day India" is an erudite Bengali language essay written by him, which was first published in the March 1899 issue of Udbodhan, the only Bengali language magazine of Ramakrishna Math and Ramakrishna Mission. The essay was reprinted as a book in 1905 and later compiled into the fourth volume of The Complete Works of Swami Vivekananda. In this essay his refrain to the readers was to honour and treat every Indian as a brother irrespective of whether he was born poor or in lower caste.
Here a list of selected books of Vivekananda published after his death (1902)
</dl>
References.
Sources.
</dl>

</doc>
<doc id="45667" url="http://en.wikipedia.org/wiki?curid=45667" title="A. C. Bhaktivedanta Swami Prabhupada">
A. C. Bhaktivedanta Swami Prabhupada

Abhay Charanaravinda Bhaktivedanta Swami Prabhupada (Bengali: "Abhoy Charonarobindo Bhoktibedanto Swamy Probhupad"; Sanskrit: अभय चरणारविन्द भक्तिवेदान्त स्वामी प्रभुपाद, IAST: "abhaya-caraṇāravinda bhakti-vedānta svāmī prabhupāda"; 1 September 1896 – 14 November 1977) was a Gaudiya Vaishnava spiritual teacher (guru) and the founder preceptor (acharya) of the International Society for Krishna Consciousness (ISKCON), commonly known as the "Hare Krishna Movement". His mission was to propagate Gaudiya Vaishnavism, a school of Vaishnavite Hinduism that had been taught to him by his guru, Bhaktisiddhanta Sarasvati, throughout the world. Born Abhay Charan De in Calcutta, he was educated at the prestigious local Scottish Church College. Before adopting the life of a pious renunciant (vanaprastha) in 1950, he was married with children and owned a small pharmaceutical business. In 1959 he took a vow of renunciation (sannyasa) and started writing commentaries on Vaishnava scriptures. In his later years, as a traveling Vaishnava monk, he became an influential communicator of Gaudiya Vaishnava theology to India and specifically to the West through his leadership of ISKCON, founded in 1966. As the founder of ISKCON, he "emerged as a major figure of the Western counterculture, initiating thousands of young Americans." Despite attacks from anti-cult groups, he received a favorable welcome from many religious scholars, such as J. Stillson Judah, Harvey Cox, Larry Shinn and Thomas Hopkins, who praised Bhaktivedanta Swami's translations and defended the group against distorted media images and misinterpretations. In respect to his achievements, religious leaders from other Gaudiya Vaishnava movements have also given him credit. 
He has been described as a charismatic leader, in the sense used by sociologist Max Weber, as he was successful in acquiring followers in the United States, Europe, India and elsewhere. CNN cites him on their Top-10 list of "wildly successful people". After his death in 1977, ISKCON, the society he founded based on a type of Hindu Krishnaism using the "Bhagavata Purana" as a central scripture, continued to grow and is respected in India, though there have been disputes about leadership among his followers. In February 2014, ISKCON's news agency reported to have reached a milestone of distributing over half a billion books authored by Bhaktivedanta Swami Prabhupada, since 1965.
Biography.
Early life.
Born on 1 September 1896, the day after Janmastami, one of the most important Vaishnava holidays, in a humble house in the Tollygunge suburb of Calcutta, he was named Abhay Charan, ""one who is fearless", "having taken shelter at Lord Krishna's feet"." Since he was born on the day of Nandotsava ("the celebration of Nanda," Krishna's father, a traditional festival in honor of Krishna's birth) he was also called Nandulal. His parents, Sriman Gour Mohan De and Srimati Rajani De, were devout Vaishnavas (devotees of Vishnu). In accordance with Bengali tradition, his mother had gone to the home of her parents for the delivery, and only a few days later Abhay returned with parents to his home at 151 Harrison Road in Calcutta, where he was brought up and educated.
He received a European led education in the Scottish Church College. This school was well reputed among Bengalis; many Vaishnava families sent their sons there. The professors, most of whom were Europeans, were known as sober, moral men, and it is believed that the students received a good education. The college was located in north Calcutta, not far from Harrison Road where Abhay's family lived. During his years in the college, Abhay Charan De (অভয় চরণ দে) was a member of the English Society as well as that of the Sanskrit Society, and it has been suggested that his education provided him a foundation for his future leadership. He graduated in 1920 with majors in English, philosophy and economics. However he rejected his diploma in response to Gandhi's independence movement. His refusal to accept the diploma he had earned was in protest of the British. He also wore the homespun cotton cloth the followers of Gandhi wore in support of home industry and protest of British rule in India.
At 22 years old Abhay married Radharani Devi, who was 11 years old, in a marriage arranged by their parents. At 14 years old Radharani gave birth to Abhay's first son.
Religious career.
In 1922, when he first met his spiritual master, Bhaktisiddhanta Sarasvati Thakura, he was requested to spread the message of Chaitanya Mahaprabhu in the English language. In 1932 he became a formally initiated disciple of Bhaktisiddhanta. In 1944, (from his front room at Sita Kanta Banerjee, Calcutta), he started the publication called "Back to Godhead", for which he acted as writer, designer, publisher, editor, copy editor and distributor. He personally designed the logo, an effulgent figure of Caitanya Mahaprabhu in the upper left corner, with the motto: "Godhead is Light, Nescience is darkness" greeting the readers. In his first magazine he wrote:
In 1947, the Gaudiya Vaishnava Society recognised his scholarship with the title "Bhaktivedanta," ("bhakti-vedānta") meaning "one who has realised that devotional service to the Supreme Lord is the end of all knowledge" (with the words Bhakti, indicating devotion and Vedanta indicating conclusive knowledge).
His later well known name, "Prabhupāda", is a Sanskrit title, literally meaning "he who has taken the shelter of the lotus feet of the Lord" where" prabhu" denotes "Lord", and "pāda" means "taking shelter." Also, "at whose feet masters sit". This name was used as a respectful form of address by his disciples from late 1967 early 1968 onwards. Previous to this, as with his early disciples, followers used to call him "Swamiji".
From 1950 onwards, he lived at the medieval Radha-Damodar mandir in the holy town of Vrindavan, where he began his commentary and translation work of the Sanskrit work Bhagavata Purana. Of all notable Vrindavana's temples, the Radha-Damodara mandir had at the time the largest collection of various copies of the original writings of the Six Gosvamis and their followers – more than two thousand separate manuscripts, many of them three hundred, some even four hundred years old. His guru, Bhaktisiddhanta Sarasvati, had always encouraged him that "If you ever get money, print books", referring to the need of literary presentation of the Vaishnava culture.
Renunciation.
Keshavaji Gaudiya Matha was the place where Bhaktivedanta used to live, he had written and studied in the library of this building, here he edited the "Gauḍīya Patrikā" magazine and this is the place where he donated the murti of Lord Chaitanya who stands on the altar beside the Deities of Radha Krishna (named "Śrī Śrī Rādhā Vinodavihārījī"). During his visit in September 1959 he entered the doors of this "matha" dressed in white, as Abhoy Babu, but would be leaving dressed in saffron, a sannyasi. He received the sannyasa name "Swami" (स्वामी Svāmi), not to be confused with the title Swami. In this "matha", in Mathura Vrndavana, Abhoy Charan Bhaktivedanta took Vaishnava renunciate vows,"sannyasa", from his friend and godbrother Bhakti Prajnana Keshava, and following this he single-handedly published the first three volumes covering seventeen chapters of the first book of "Bhagavata Purana", filling three volumes of four hundred pages each with a detailed commentary. Introduction to the first volume was a biographical sketch of Caitanya Mahāprabhu. He then left India, obtaining free passage on a freight ship called the "Jaladuta", with the aim and a hope of fulfilling his spiritual master's instruction to spread the message of Caitanya Mahaprabhu around the world. In his possession were a suitcase, an umbrella, a supply of dry cereal, about eight dollars worth of Indian currency, and several boxes of books.
Mission to the West.
A. C. Bhaktivedanta Swami sailed to USA in 1965. His trip to the United States was not sponsored by any religious organization, nor was he met upon arrival by a group of loyal followers. As he neared his destination on the ship, the Indian freighter "Jaladuta", the enormity of his intended task weighed on him. On 13 September he wrote in his diary, "Today I have disclosed my mind to my companion, Lord Sri Krishna." On this occasion and on a number of others, he called on Krishna for help in his native Bengali. Examining these compositions, academics regard them as "intimate records of his prayerful preparation for what lay ahead" and a view on "how Bhaktivedanta Swami understood his own identity and mission."
By journeying to USA, he was attempting to fulfill the wish of his guru, possible only by the grace of "his dear Lord Krishna". It is in July 1966 "global missionary Vaishnavism" was brought to the West by Bhaktivedanta Swami, "the soul agent", founding the International Society for Krishna Consciousness in New York City. Bhaktivedanta Swami spent much of the last decade of his life setting up the institution of ISKCON. Since he was the Society's leader, his personality and management were responsible for much of ISKCON's growth and the reach of his mission.
When it was suggested to him at the time of founding the ISKCON in 1966 that a broader term "God Consciousness" would be preferable to "Krishna Consciousness" in the title, he rejected this recommendation, suggesting that the name "Krishna" includes all other forms and concepts of God.
After a group of devotees and a temple had been established in New York another center was started in San Francisco in 1967. From there he traveled throughout America with his disciples, popularizing the movement through street chanting ("sankirtana"), book distribution and public speeches.
Once ISKCON was more established in the USA a small number of devotees from the San Francisco temple were sent to London, England. After a short time in London they came into contact with The Beatles, of whom George Harrison took the greatest interest, spending a significant time speaking with Bhaktivedanta Swami and producing a record with members of the later London Radha Krsna Temple. Over the following years Bhaktivedanta Swami's continuing leadership role took him around the world some several times setting up temples and communities in all of the major continents. By the time of his death in Vrindavan in 1977, ISKCON had become an internationally known expression of Vaishnavism.
In the twelve years from his arrival in New York until his final days, he:
Through his mission, Bhaktivedanta Swami followed and communicated the teachings of Chaitanya Mahaprabhu and introduced bhakti yoga to an international audience. Within Gaudiya Vaishnavism this was viewed as the fulfillment of a long time mission to introduce Caitanya Mahaprabhu's teachings to the world.
In his discussion with historian Arnold J. Toynbee in London, Bhaktivedanta Swami is quoted as saying: "I have started this Krishna Conscious Movement among the Indians and Americans and for the next ten thousand years it will increase."
Books and publishing.
It is believed that Bhaktivedanta Swami's most significant contribution are his books. Within the final twenty years of his life Bhaktivedanta Swami translated over sixty volumes of classic Vedic scriptures (such as the "Bhagavad Gita" and the "Srimad Bhagavatam") into the English language. For their authority, depth, and clarity, his books have won praise from professors at colleges and universities like Harvard, Oxford, Cornell, Columbia, Syracuse, Oberlin, and Edinburgh, and his "Bhagavad-Gītā As It Is" was published by Macmillan Publishers, in 1968 and unabridged edition in 1972, and is now available in over sixty languages around the world and some other books by Bhaktivedanta Swami are available in over eighty different languages. In February 2014, ISKCON's news agency reported to have reached a milestone of distributing over half a billion books authored by Bhaktivedanta Swami Prabhupada, since 1965.
The Bhaktivedanta Book Trust was established in 1972 to publish his works, and it has also published his multi-volume biography, "Srila Prabhupada-lilamrta", that according to Larry Shinn will "certainly be one of the most complete records of the life and work of any modern religious figure". Prabhupada reminded his devotees before his death that he would live forever in his books, and through them would remain present as a spiritual master or guru. Bhaktivedanta Swami had instilled in his followers an understanding of the importance of writing and publishing not only with regard to his works, but also their own initiatives. His early disciples felt Prabhupada had given them "Back To Godhead" for their own writings from the very start.
A prominent Gaudiya Vaishnava figure, Shrivatsa Goswami, who as a young man had met Bhaktivedanta Swami in 1972, affirmed the significance of book publishing and distribution in spreading the message of Caitanya in an interview with Steven Gelberg:
Views on other religious traditions.
Bhaktivedanta Swami considered Moses, Jesus, and Mohamed to be empowered representatives of God, describing them within his writings as pioneers of the same essential message of dedication to God with love and devotion.
Other typical expressions present a different perspective, where he would point out that "today I may be a Hindu, but tomorrow I may become a Christian or Muslim. In this way faiths can be changed, but dharma" is a natural sequence, a natural occupation or a connection and it can not be changed, because it is permanent, according to him. While the ISKCON theology of personal god is close to Christian theology, both personal and monotheistic, being a preacher of bhakti and a missionary he sometimes would add, that "already many Christians have tasted the nectar of divine love of the holy name and are dancing with "karatalas" (hand-cymbals) and "mridangas" (drums)."
His approach to modern knowledge is also seen in sectarian Orthodox Judaism, where the skills and technical knowledge of modernity are encouraged, but the values rejected. Bhaktivedanta Swami stated "devotees should not be lazy, idle...we are not afraid to work. Whatever our engagement is, by offering the result to Krishna we become Krishna conscious". Some of his representations are believed to affect women adversely and are male-centred, others are tender and celebratory. Bhaktivedanta Swami himself taught a dualism of body and soul and that of the genders. Similar to many traditional religions he considered sexuality and spirituality as conflicting opposites. Among some liberal male followers there is a positive recognition of his example in applying the spirit of the law according to time, place, person and circumstance, rather than literal tracing of the tradition.
In line with traditional Vaishnava theology, he was critical of the monist philosophies of Hinduism representing the Gaudiya Vaishnava point of view . In the view of some, the Gaudiya-Vaishnava philosophy he followed is neither fully dualistic nor monist (this is known as Achintya Bheda Abheda). As a school of thought, Gaudiya Vaishnavism has much more in common with the Dvaita, as opposed to the Advaita schools.
Within India.
Beginning his public preaching mission in India, he founded the League of Devotees in Jhansi in 1953.
Following the establishment of temples and centres in the United States and Europe, he returned to India in 1971, holding many public programs which were well attended. From 1971 onwards, the movement became increasingly popular and spread throughout India. He was particularly eager to see the progress at "the impressive temple project in" Mumbai which he and his disciples had fought very hard to establish, with large temples in Mayapur and Vrindavan to follow in the mid-1970s. To promote the vedic education system within the modern Indian education structure, he introduced a chain of Gurukul in various part of India. The Bhaktivedanta Gurukula and International School is one of the most successful schools in the list.
In 1996, the Government of India recognized his accomplishments by issuing a commemorative stamp in his honour as a part of Prabhupada Centennial celebrations.
Speaking at the inauguration of ISKCON's cultural center in New Delhi on 5 April on the occasion of Ramnavmi in 1998, Atal Bihari Vajpayee, then India's prime minister, said:
Monuments.
A number of memorial samadhis or shrines to Bhaktivedanta Swami were constructed by the members of ISKCON in his remembrance, the largest of which are in Mayapur, Vrindavan and at the larger sized temples in America. Prabhupada's Palace of Gold was designed and constructed by devotees of the New Vrindavan community and dedicated on 2 September 1979. Back in 1972 it was intended to be simply a residence for Bhaktivedanta Swami, but over time the plans evolved into an ornate marble and gold palace which is now visited by thousands of Hindu pilgrims each year, visiting this centerpiece of the community strongly relying upon tourist trade.
Bibliography.
Published posthumously.
 ISBN 0-89213-100-4
 (1981)
 ISBN 0-89213-118-7
 (1985)
 ISBN 0-89213-103-9
 ISBN 0-89213-275-2
 (1991)
 (1991)
 ISBN 0-89213-272-8
References.
</dl>

</doc>
<doc id="45675" url="http://en.wikipedia.org/wiki?curid=45675" title="Primary Chronicle">
Primary Chronicle

The Tale of Bygone Years (Old Church Slavonic: Повѣсть времѧньныхъ лѣтъ, "Povest Vremyannykh Let") or Primary Chronicle is a history of Kievan Rus' from about 850 to 1110, originally compiled in Kiev about 1113. The work is considered to be a fundamental source in the interpretation of the history of the Eastern Slavs.
Three editions.
Primary edition.
Tradition long regarded the original compilation as the work of a monk named Nestor (c. 1056 – c. 1114); hence scholars spoke of "Nestor's Chronicle" or of "Nestor's manuscript". His compilation has not survived. Nestor's many sources included earlier (now-lost) Slavonic chronicles, the Byzantine annals of John Malalas and of George Hamartolus, native legends and Norse sagas, several Greek religious texts, Rus'-Byzantine treaties, and oral accounts of Yan Vyshatich and of other military leaders. Nestor worked at the court of Sviatopolk II of Kiev (ruled 1093–1113) and probably shared Sviatopolk's pro-Scandinavian policies.
The early part of the "Chronicle" features many anecdotal stories, among them those of the arrival of the three Varangian brothers, the founding of Kiev, the murder of Askold and Dir (ca. 882), the death (912) of Oleg (fatally bitten by a serpent concealed in the skeleton of his horse), and the vengeance taken by Olga, the wife of Igor, on the Drevlians, who had murdered her husband. The account of the labors of Saints Cyril and Methodius among the Slavic peoples also makes a very interesting tale, and to Nestor we owe the story of the summary way in which Vladimir the Great (rules 980 to 1015) suppressed the worship of Perun and other traditional gods at Kiev.
Second edition.
In the year 1116, Nestor's text was extensively edited by hegumen Sylvester who appended his name at the end of the chronicle. As Vladimir Monomakh was the patron of the village of Vydubychi where his monastery is situated, the new edition glorified that prince and made him the central figure of later narrative. This second version of Nestor's work is preserved in the Laurentian codex (see below).
Third edition.
A third edition followed two years later and centered on the person of Vladimir's son and heir, Mstislav the Great. The author of this revision could have been Greek, for he corrected and updated much data on Byzantine affairs. This latest revision of Nestor's work is preserved in the Hypatian codex (see below).
Two manuscripts.
Because the original of the chronicle as well as the earliest known copies are lost, it is difficult to establish the original content of the chronicle. The two main sources for the chronicle's text as it is known presently are the Laurentian codex and the Hypatian codex.
The Laurentian codex was compiled in what are today Russian lands by the Nizhegorod monk Laurentius for the Prince Dmitry Konstantinovich in 1377. The original text he used was a lost codex compiled for the Grand Duke Mikhail of Tver in 1305. The account continues until 1305, but the years 898–922, 1263–83 and 1288–94 are missing for reasons unknown. The manuscript was acquired by the famous Count Musin-Pushkin in 1792 and subsequently presented to the National Library of Russia in Saint Petersburg.
The Hypatian Codex dates to the 15th century. It was written in what are today Ukrainian lands and incorporates much information from the lost 12th-century Kievan and 13th-century Halychian chronicles. The language of this work is the East Slavic version of Church Slavonic language with many additional irregular east-slavisms (like other east-Slavic codices of the time). Whereas the Laurentian (Muscovite) text traces the Kievan legacy through to the Muscovite princes, the Hypatian text traces the Kievan legacy through the rulers of the Halych principality.The Hypatian codex was rediscovered in Kiev in the 1620s and copy was made for Prince Kostiantyn Ostrozhsky. A copy was found in Russia in the 18th century at the Ipatiev Monastery of Kostroma by the Russian historian Nikolai Karamzin.
Numerous monographs and published versions of the chronicle have been made, the earliest known being in 1767. Aleksey Shakhmatov published a pioneering textological analysis of the narrative in 1908. Dmitry Likhachev and other Soviet scholars partly revisited his findings. Their versions attempted to reconstruct the pre-Nestorian chronicle, compiled at the court of Yaroslav the Wise in the mid-11th century.
Assessment.
Unlike many other medieval chronicles written by European monks, the "Tale of Bygone Years" is unique as the only written testimony on the earliest history of East Slavic people. Its comprehensive account of the history of Rus' is unmatched in other sources, although important correctives are provided by the Novgorod First Chronicle. It is also valuable as a prime example of the Old East Slavonic literature.

</doc>
<doc id="45678" url="http://en.wikipedia.org/wiki?curid=45678" title="Atonality">
Atonality

Atonality in its broadest sense is music that lacks a tonal center, or key. "Atonality", in this sense, usually describes compositions written from about 1908 to the present day where a hierarchy of pitches focusing on a single, central tone is not used, and the notes of the chromatic scale function independently of one another . More narrowly, the term "atonality" describes music that does not conform to the system of tonal hierarchies that characterized classical European music between the seventeenth and nineteenth centuries . "The repertory of atonal music is characterized by the occurrence of pitches in novel combinations, as well as by the occurrence of familiar pitch combinations in unfamiliar environments" .
More narrowly still, the term is sometimes used to describe music that is neither tonal nor serial, especially the pre-twelve-tone music of the Second Viennese School, principally Alban Berg, Arnold Schoenberg, and Anton Webern . However, "[a]s a categorical label, 'atonal' generally means only that the piece is in the Western tradition and is not 'tonal'" , although there are longer periods, e.g., medieval, renaissance, and modern modal musics to which this definition does not apply. "[S]erialism arose partly as a means of organizing more coherently the relations used in the preserial 'free atonal' music. ... Thus many useful and crucial insights about even strictly serial music depend only on such basic atonal theory" .
Late 19th- and early 20th-century composers such as Alexander Scriabin, Claude Debussy, Béla Bartók, Paul Hindemith, Sergei Prokofiev, Igor Stravinsky, and Edgard Varèse have written music that has been described, in full or in part, as atonal (; ; ; ; ; ; ; ; ; ; ; ).
History.
While music without a tonal center had been written previously, for example Franz Liszt's "Bagatelle sans tonalité" of 1885, it is with the twentieth century that the term "atonality" began to be applied to pieces, particularly those written by Arnold Schoenberg and The Second Viennese School.
The term "atonality" was coined in 1907 by Joseph Marx in a scholarly study of tonality, which was later expanded into his doctoral thesis .
Their music arose from what was described as the "crisis of tonality" between the late nineteenth century and early twentieth century in classical music. This situation had come about historically through the increasing use over the course of the nineteenth century of
ambiguous chords, less probable harmonic inflections, and the more unusual melodic and rhythmic inflections possible within the style[s] of tonal music. The distinction between the exceptional and the normal became more and more blurred; and, as a result, there was a concomitant loosening of the syntactical bonds through which tones and harmonies had been related to one another. The connections between harmonies were uncertain even on the lowest—chord-to-chord—level. On higher levels, long-range harmonic relationships and implications became so tenuous that they hardly functioned at all. At best, the felt probabilities of the style system had become obscure; at worst, they were approaching a uniformity which provided few guides for either composition or listening. 
The first phase, known as "free atonality" or "free chromaticism", involved a conscious attempt to avoid traditional diatonic harmony. Works of this period include the opera "Wozzeck" (1917–1922) by Alban Berg and "Pierrot Lunaire" (1912) by Schoenberg.
The second phase, begun after World War I, was exemplified by attempts to create a systematic means of composing without tonality, most famously the method of composing with 12 tones or the twelve-tone technique. This period included Berg's "Lulu" and "Lyric Suite", Schoenberg's "Piano Concerto", his oratorio "Die Jakobsleiter" and numerous smaller pieces, as well as his last two string quartets. Schoenberg was the major innovator of the system, but his student, Anton Webern, is anecdotally claimed to have begun linking dynamics and tone color to the primary row, making rows not only of pitches but of other aspects of music as well . However, actual analysis of Webern's twelve-tone works has so far failed to demonstrate the truth of this assertion. One analyst concluded, following a minute examination of the Piano Variations, op. 27, that while the texture of this music may superficially resemble that of some serial music ... its structure does not. None of the patterns within separate nonpitch characteristics makes audible (or even numerical) sense "in itself". The point is that these characteristics are still playing their traditional role of differentiation. Twelve-tone technique, combined with the parametrization (separate organization of four aspects of music: pitch, attack character, intensity, and duration) of Olivier Messiaen, would be taken as the inspiration for serialism .
Atonality emerged as a pejorative term to condemn music in which chords were organized seemingly with no apparent coherence. In Nazi Germany, atonal music was attacked as "Bolshevik" and labeled as degenerate ("Entartete Musik") along with other music produced by enemies of the Nazi regime. Many composers had their works banned by the regime, not to be played until after its collapse after World War II.
After Schoenberg's death, Igor Stravinsky used the twelve-tone technique . Iannis Xenakis generated pitch sets from mathematical formulae, and also saw the expansion of tonal possibilities as part of a synthesis between the hierarchical principle and the theory of numbers, principles which have dominated music since at least the time of Parmenides .
Free atonality.
The twelve-tone technique was preceded by Schoenberg's freely atonal pieces of 1908–1923, which, though free, often have as an "integrative element...a minute intervallic cell" that in addition to expansion may be transformed as with a tone row, and in which individual notes may "function as pivotal elements, to permit overlapping statements of a basic cell or the linking of two or more basic cells" .
The twelve-tone technique was also preceded by nondodecaphonic serial composition used independently in the works of Alexander Scriabin, Igor Stravinsky, Béla Bartók, Carl Ruggles, and others . "Essentially, Schoenberg and Hauer systematized and defined for their own dodecaphonic purposes a pervasive technical feature of 'modern' musical practice, the ostinato" 
Composing atonal music.
Setting out to compose atonal music may seem complicated because of both the vagueness and generality of the term. Additionally George Perle explains that, "the 'free' atonality that preceded dodecaphony precludes by definition the possibility of self-consistent, generally applicable compositional procedures" . However, he provides one example as a way to compose atonal pieces, a pre-twelve-tone technique piece by Anton Webern, which rigorously avoids anything that suggests tonality, to choose pitches that do not imply tonality. In other words, reverse the rules of the common practice period so that what was not allowed is required and what was required is not allowed. This is what was done by Charles Seeger in his explanation of dissonant counterpoint, which is a way to write atonal counterpoint .
Kostka and Payne list four procedures as operational in the atonal music of Schoenberg, all of which may be taken as negative rules. Avoidance of melodic or harmonic octaves, avoidance of traditional pitch collections such as major or minor triads, avoidance of more than three successive pitches from the same diatonic scale, and use of disjunct melodies (avoidance of conjunct melodies) .
Further, Perle agrees with and that, "the abandonment of the concept of a root-generator of the individual chord is a radical development that renders futile any attempt at a systematic formulation of chord structure and progression in atonal music along the lines of traditional harmonic theory" . Atonal compositional techniques and results "are not reducible to a set of foundational assumptions in terms of which the compositions that are collectively designated by the expression 'atonal music' can be said to represent 'a system' of composition" . Equal-interval chords are often of indeterminate root, mixed-interval chords are often best characterized by their interval content, while both lend themselves to atonal contexts .
Perle also points out that structural coherence is most often achieved through operations on intervallic cells. A cell "may operate as a kind of microcosmic set of fixed intervallic content, statable either as a chord or as a melodic figure or as a combination of both. Its components may be fixed with regard to order, in which event it may be employed, like the twelve-tone set, in its literal transformations. … Individual tones may function as pivotal elements, to permit overlapping statements of a basic cell or the linking of two or more basic cells" .
Regarding the post-tonal music of Perle, one theorist wrote: "While ... montages of discrete-seeming elements tend to accumulate global rhythms other than those of tonal progressions and their rhythms, there is a similarity between the two sorts of accumulates spatial and temporal relationships: a similarity consisting of generalized arching tone-centers linked together by shared background referential materials" .
Another approach of composition techniques for atonal music is given by Allen Forte who developed the theory behind atonal music . Forte describes two main operations: transposition an inversion. Transposition can be seen as a rotation of "t" either clockwise or anti-clockwise on a circle, where each note of the chord is rotated equally. For example if "t" = 2 and the chord is [0 3 6], transposition (clockwise) will be [2 5 8]. Inversion can be seen as a symmetry with respect to the axis formed by 0 and 6. If we carry on with our example [0 3 6] becomes [0 9 6]. 
An important characteristic are the invariants which are the notes which stay identical after a transformation. It should be noted that no difference is made between the octave in which the note is played so that, for example, all C♯s are equivalent, no matter the octave in which they actually occur. This is why the 12-note scale is represented by a circle. This leads us to the definition of the similarity between two chords which considers the subsets and the interval content of each chord .
Reception.
Controversy over the term itself.
The term "atonality" itself has been controversial. Arnold Schoenberg, whose music is generally used to define the term, was vehemently opposed to it, arguing that "The word 'atonal' could only signify something entirely inconsistent with the nature of tone... to call any relation of tones atonal is just as farfetched as it would be to designate a relation of colors aspectral or acomplementary. There is no such antithesis" .
Composer and theorist Milton Babbitt also disparaged the term, saying "The works that followed, many of them now familiar, include the Five Pieces for Orchestra, Erwartung, Pierrot Lunaire, and they and a few yet to follow soon were termed 'atonal,' by I know not whom, and I prefer not to know, for in no sense does the term make sense. Not only does the music employ 'tones,' but it employs precisely the same 'tones,' the same physical materials, that music had employed for some two centuries. In all generosity, 'atonal' may have been intended as a mildly analytically derived term to suggest 'atonic' or to signify 'a-triadic tonality,' but, even so there were infinitely many things the music was not" .
"Atonal" developed a certain vagueness in meaning as a result of its use to describe a wide variety of compositional approaches that deviated from traditional chords and chord progressions. Attempts to solve these problems by using terms such as "pan-tonal", "non-tonal", "multi-tonal", "free-tonal" and "without tonal center" instead of "atonal" have not gained broad acceptance.
Criticism of the concept of atonality.
Composer Anton Webern held that "new laws asserted themselves that made it impossible to designate a piece as being in one key or another" . Composer Walter Piston, on the other hand, said that, out of long habit, whenever performers "play any little phrase they will hear it in some key—it may not be the right one, but the point is they will play it with a tonal sense. ... [T]he more I feel I know Schoenberg's music the more I believe he thought that way himself. ... And it isn't only the players; it's also the listeners. They will hear tonality in everything" .
Donald Jay Grout similarly doubted whether atonality is really possible, because "any combination of sounds can be referred to a fundamental root". He defined it as a fundamentally subjective category: "atonal music is music in which the person who is using the word cannot hear tonal centers" .
One difficulty is that even an otherwise "atonal" work, tonality "by assertion" is normally heard on the thematic or linear level. That is, centricity may be established through the repetition of a central pitch or from emphasis by means of instrumentation, register, rhythmic elongation, or metric accent . In tonal music, however, centricity is established through hierarchical relationships of chords functions and scale degrees, and is not directly related to instrumentation or temporal aspects.
Criticism of atonal music.
Swiss conductor, composer, and musical philosopher Ernest Ansermet, a critic of atonal music, wrote extensively on this in the book "Les fondements de la musique dans la conscience humaine" (French for The foundations of music in human consciousness) ), where he argued that the classical musical language was a precondition for musical expression with its clear, harmonious structures. Ansermet argued that a tone system can only lead to a uniform perception of music if it is deduced from just a single interval. For Ansermet this interval is the fifth .

</doc>
<doc id="45705" url="http://en.wikipedia.org/wiki?curid=45705" title="Inverse transform sampling">
Inverse transform sampling

Inverse transform sampling (also known as inversion sampling, the inverse probability integral transform, the inverse transformation method, Smirnov transform, golden rule,) is a basic method for pseudo-random number sampling, i.e. for generating sample numbers at random from any probability distribution given its cumulative distribution function (cdf).
The basic idea is to uniformly sample a number formula_1 between 0 and 1, interpreted as a probability, and then return the largest number formula_2 from the domain of the distribution formula_3 such that formula_4. For example, imagine that formula_3 is the standard normal distribution (i.e. with mean 0, standard deviation 1). Then if we choose formula_6, we would return 0, because 50% of the probability of a normal distribution occurs in the region where formula_7. Similarly, if we choose formula_8, we would return 1.95996...; if we choose formula_9, we would return 2.5758...; if we choose formula_10, we would return 4.891638...; etc. Essentially, we are randomly choosing a proportion of the area under the curve and returning the number in the domain such that exactly this proportion of the area occurs to the left of that number. Intuitively, we are unlikely to choose a number in the tails because there is very little area in them: We'd have to pick a number very close to 0 or 1.
Computationally, this method involves computing the quantile function of the distribution — in other words, computing the cumulative distribution function (CDF) of the distribution (which maps a number in the domain to a probability between 0 and 1) and then inverting that function. This is the source of the term "inverse" or "inversion" in most of the names for this method. Note that for a discrete distribution, computing the CDF is not in general too difficult: We simply add up the individual probabilities for the various points of the distribution. For a continuous distribution, however, we need to integrate the probability density function (PDF) of the distribution, which is impossible to do analytically for most distributions (including the normal distribution). As a result, this method may be computationally inefficient for many distributions and other methods are preferred; however, it is a useful method for building more generally applicable samplers such as those based on rejection sampling.
For the normal distribution, the lack of an analytical expression for the corresponding quantile function means that other methods (e.g. the Box–Muller transform) may be preferred computationally. It is often the case that, even for simple distributions, the inverse transform sampling method can be improved on: see, for example, the ziggurat algorithm and rejection sampling. On the other hand, it is possible to approximate the quantile function of the normal distribution extremely accurately using moderate-degree polynomials, and in fact the method of doing this is fast enough that inversion sampling is now the default method for sampling from a normal distribution in the statistical package R.
Definition.
The probability integral transform states that if formula_11 is a continuous random variable with cumulative distribution function formula_12, then the random variable formula_13 has a uniform distribution on [0, 1]. The inverse probability integral transform is just the inverse of this: specifically, if formula_14 has a uniform distribution on [0, 1] and if formula_11 has a cumulative distribution formula_12, then the cumulative distribution function of the random variable formula_17 is formula_11 .
The method.
The problem that the inverse transform sampling method solves is as follows:
The inverse transform sampling method works as follows:
Expressed differently, given a continuous uniform variable "U" in [0, 1] and an invertible cumulative distribution function "F", the random variable "X" = "F" −1("U") has distribution "F" (or, "X" is distributed "F").
A treatment of such inverse functions as objects satisfying differential equations can be given. Some such differential equations admit explicit power series solutions, despite their non-linearity.
Proof of correctness.
Let "F" be a continuous cumulative distribution function, and let "F"−1 be its inverse function (using the infimum because CDFs are weakly monotonic and right-continuous):
"Claim:" If "U" is a uniform random variable on (0, 1) then formula_20 follows the distribution "F".
"Proof:"

</doc>
<doc id="45707" url="http://en.wikipedia.org/wiki?curid=45707" title="Nevi'im">
Nevi'im

Nevi'im (; Hebrew: נְבִיאִים "Nəḇî'îm"‎, "Prophets") is the second main division of the Hebrew Bible (the "Tanakh"), between the Torah (instruction) and Ketuvim (writings). It contains two sub-groups, the Former Prophets ("Nevi'im Rishonim" נביאים ראשונים, the narrative books of Joshua, Judges, Samuel and Kings) and the Latter Prophets ("Nevi'im Aharonim" נביאים אחרונים, the books of Isaiah, Jeremiah and Ezekiel and the Twelve Minor Prophets).
Many of the writings of the Latter Prophets are thought by scholars to be older than the narratives of the Former Prophets which precede them in the canon, and were profoundly influential on the direction and development of Hebrew religion. The Latter Prophets have also had a wide influence on literature and on political and social activism in cultures outside of Judaism.
Synopsis.
In Judaism, "Samuel" and "Kings" are each counted as one book. In addition, twelve relatively short prophetic books are counted as one in a single collection called "Trei Asar" or "The Twelve Minor Prophets". The Jewish tradition thus counts a total of eight books in "Nevi'im" out of a total of 24 books in the entire Tanakh. In the Jewish liturgy, selections from the books of "Nevi'im" known as the "Haftarah" are read publicly in the synagogue after the reading of the Torah on each Shabbat, as well as on Jewish festivals and fast days. The Book of Daniel is part of the Writings, or "Ketuvim", in the Tanakh.
Former Prophets.
The Former Prophets are the books Joshua, Judges, 1st & 2nd Samuel, 1st & 2nd Kings. They contain historical narratives that begin immediately after the death of Moses with the divine appointment of Joshua as his successor, who then leads the people of Israel into the Promised Land, and end with the release from imprisonment of the last king of Judah. Treating Samuel and Kings as single books, they cover:
Joshua.
The Book of Joshua ("Yehoshua" יהושע) contains a history of the Israelites from the death of Moses to that of Joshua. After Moses' death, Joshua, by virtue of his previous appointment as Moses' successor, receives from God the command to cross the Jordan. In execution of this order Joshua issues the requisite instructions to the stewards of the people for the crossing of the Jordan; and he reminds the Reubenites, Gadites, and the half of Manasseh of their pledge given to Moses to help their brethren.
The book essentially consists of three parts:
Judges.
The Book of Judges ("Shoftim" שופטים) consists of three distinct parts:
Samuel.
The Books of Samuel ("Shmu'el" שמואל) consists of five parts:
A conclusion of sorts appears at 1 Kings 1-2, concerning Solomon enacting a final revenge on those who did what David perceived as wrongdoing, and having a similar narrative style. While the subject matter in the Book(s) of Samuel is also covered by the narrative in Chronicles, it is noticeable that the section (2 Sam. 11:2–12:29) containing an account of the matter of Bathsheba is omitted in the corresponding passage in 1 Chr. 20.
Kings.
The Books of Kings ("Melakhim" מלכים) contains accounts of the kings of the ancient Kingdom of Israel and the Kingdom of Judah, and the annals of the Jewish commonwealth from the accession of Solomon until the subjugation of the kingdom by Nebuchadnezzar and the Babylonians.
Latter Prophets.
The Latter Prophets are divided into two groups, the Major prophets (Isaiah, Jeremiah and Ezekiel) and the Twelve Minor Prophets (Hosea, Joel, Amos, Obadiah, Jonah, Micah, Nahum, Habakkuk, Zephaniah, Haggai, Zechariah and Malachi) collected into a single book.
Isaiah.
The 66 chapters of Isaiah ("Yeshayahu" [ישעיהו]) consist primarily of prophecies of the judgments awaiting nations that are persecuting Judah. These nations include Babylon, Assyria, Philistia, Moab, Syria, Israel (the northern kingdom), Ethiopia, Egypt, Arabia, and Phoenicia. The prophecies concerning them can be summarized as saying that God is the God of the whole earth, and that nations which think of themselves as secure in their own power might well be conquered by other nations, at God's command.
Chapter 6 describes Isaiah's call to be a prophet of God. Chapters 35–39 provide historical material about King Hezekiah and his triumph of faith in God. Chapters 24–34, while too complex to characterize easily, are primarily concerned with prophecies of a Messiah, a person anointed or given power by God, and of the Messiah's kingdom, where justice and righteousness will reign. This section is seen by Jews as describing an actual king, a descendant of their great king, David, who will make Judah a great kingdom and Jerusalem a truly holy city.
The prophecy continues with what some scholars have called "The Book of Comfort" which begins in chapter 40 and completes the writing. In the first eight chapters of this book of comfort, Isaiah prophesies the deliverance of the Jews from the hands of the Babylonians and restoration of Israel as a unified nation in the land promised to them by God. Isaiah reaffirms that the Jews are indeed the chosen people of God in chapter 44 and that Hashem is the only God for the Jews (and only the God of the Jews) as he will show his power over the gods of Babylon in due time in chapter 46. In chapter 45:1 the Persian ruler Cyrus is named as the messiah who will overthrow the Babylonians and allow the return of Israel to their original land. The remaining chapters of the book contain prophecies of the future glory of Zion under the rule of a righteous servant (52 & 54). Chapter 53 contains a very poetic prophecy about this servant which is generally considered by Christians to refer to the crucifixion of Jesus, though Jews generally interpret it as a reference to God's people. Although there is still the mention of judgment of false worshippers and idolaters (65 & 66), the book ends with a message of hope of a righteous ruler who extends salvation to his righteous subjects living in the Lord's kingdom on earth.
Jeremiah.
The Book of Jeremiah ("Yirmiyahu" [ירמיהו]) can be divided into twenty-three subsections, and its contents organized into five sub-sections or 'books'.
In Egypt, after an interval, Jeremiah is supposed to have added three sections, viz., ch. 37–39; 40–43; and 44. The main Messianic prophecies are found in 23:1–8; 31:31–40; and 33:14–26.
Jeremiah's prophecies are noted for the frequent repetitions found in them of the same words, phrases, and imagery. They cover the period of about 30 years. They are not in chronological order. Modern scholars do not believe they have reliable theories as to when, where, and how the text was edited into its present form.
Ezekiel.
The Book of Ezekiel ("Yehezq'el" [יחזקאל]) contains three distinct sections.
Twelve Minor Prophets.
The Twelve Minor Prophets are:
Liturgical use.
The Haftarah is a text selected from the books of "Nevi'im" that is read publicly in the synagogue after the reading of the Torah on each Shabbat, as well as on Jewish festivals and fast days.
Cantillation.
There is a special cantillation melody for the haftarah, distinct from that of the Torah portion. In some earlier authorities there are references to a tune for the "prophets" generally, distinct from that for the haftarah: this may have been a simplified melody for learning purposes.
Certain cantillation marks and combinations appear in Nevi'im but not within any of the Haftarah selections, and most communities therefore do not have a musical tradition for those marks. J.L. Neeman suggested that "those who recite Nevi'im privately with the cantillation melody may read the words accented by those rare notes by using a "metaphor" based on the melody of those notes in the five books of the Torah, while adhering to the musical scale of the melody for Nevi'im." Neeman includes a reconstruction of the musical scale for the lost melodies of the rare cantillation notes. In the Ashkenazi tradition, the resemblance between the Torah and Haftarah melodies is obvious and it is easy to transpose motifs between the two as suggested by Neeman. In the Sephardi traditions the haftarah melody is considerably more florid than the Torah melody, and usually in a different musical mode, and there are only isolated points of contact between the two.
Extraliturgical public reading.
In some Near and Middle Eastern Jewish traditions, the whole of Nevi'im (as well as the rest of the Tanakh and the Mishnah) is read each year on a weekly rota, usually on Shabbat afternoons. These reading sessions often take place in the synagogue courtyard but are not considered to be synagogue services.
Aramaic translation.
A "targum" is an Aramaic translation of the Hebrew Scriptures that was compiled or written in the Land of Israel or in Babylonia from the Second Temple period until the early Middle Ages (late first millennium). According to the Talmud, the targum on Nevi'im was composed by Jonathan ben Uzziel. Like Targum Onkelos on the Torah, Targum Jonathan is an eastern (Babylonian) targum with early origins in the west (Land of Israel).
Like the targum to the Torah, Targum Jonathan to Nevi'im served a formal liturgical purpose: it was read alternately, verse by verse, or in blocks of up to three verses, in the public reading of the Haftarah and in the study of Nevi'im. Yemenite Jews continue the above tradition to this day, and have thus preserved a living tradition of the Babylonian vocalization for the Targum to Nevi'im.

</doc>
<doc id="45708" url="http://en.wikipedia.org/wiki?curid=45708" title="Dipolar bond">
Dipolar bond

A dipolar bond, more commonly known as a dative covalent bond or coordinate bond is a kind of 2-center, 2-electron covalent bond in which the two electrons derive from the same atom.
Examples.
The term dipolar bond is used in organic chemistry for compounds such as amine oxides for which the electronic structure can be described in terms of the basic amine donating two electrons to an oxygen atom.
The arrow → indicates that both electrons in the bond originate from the amine moiety. In a standard covalent bond each atom contributes one electron. Therefore, an alternative description is that the amine gives away one electron to the oxygen atom, which is then used, with the remaining unpaired electron on the nitrogen atom, to form a standard covalent bond. The process of transferring the electron from nitrogen to oxygen creates formal charges, so the electronic structure may also be depicted as
This electronic structure has an electric dipole, hence the name dipolar bond. In reality, the atoms carry fractional charges; the more electronegative atom of the two involved in the bond will usually carry a fractional negative charge. One exception to this is carbon monoxide. In this case, the carbon atom carries the fractional negative charge despite its being less electronegative than oxygen.
An example of a dative covalent bond is provided by the interaction between a molecule of ammonia, a Lewis base with a lone pair of electrons on the nitrogen atom, and boron trifluoride, a Lewis acid by virtue of the boron atom having an incomplete octet of electrons. In forming the adduct, the boron atom attains an octet configuration.
The electronic structure of a coordination complex can be described in terms of the set of ligands each donating a pair of electrons to a metal centre. For example, in Hexaamminecobalt(III) chloride, each ammonia ligand donates its lone pair of electrons to the cobalt(III) ion. In this case, the bonds formed are described as coordinate bonds.
In all cases the bond is a covalent bond. The prefix dipolar, dative or coordinate merely serves to indicate the origin of the electrons used in creating the bond.

</doc>
<doc id="45710" url="http://en.wikipedia.org/wiki?curid=45710" title="Forensic science">
Forensic science

Forensic science is the scientific method of gathering and examining information about the past which is then used in a court of law. The word "forensic" comes from the Latin "forēnsis", meaning "of or before the forum." The history of the term originates from Roman times, during which a criminal charge meant presenting the case before a group of public individuals in the forum. Both the person accused of the crime and the accuser would give speeches based on their sides of the story. The case would be decided in favor of the individual with the best argument and delivery. This origin is the source of the two modern usages of the word "forensic" – as a form of legal evidence and as a category of public presentation. In modern use, the term "forensics" in the place of "forensic science" can be considered correct, as the term "forensic" is effectively a synonym for "legal" or "related to courts". However, the term is now so closely associated with the scientific field that many dictionaries include the meaning that equates the word "forensics" with "forensic science".
History.
Early methods.
The ancient world lacked standardized forensic practices, which aided criminals in escaping punishment. Criminal investigations and trials heavily relied on forced confessions and witness testimony. However, ancient sources do contain several accounts of techniques that foreshadow concepts in forensic science that were developed centuries later.
For instance, Archimedes (287–212 BC) invented a method for determining the volume of an object with an irregular shape. According to Vitruvius, a votive crown for a temple had been made for King Hiero II, who had supplied the pure gold to be used, and Archimedes was asked to determine whether some silver had been substituted by the dishonest goldsmith. Archimedes had to solve the problem without damaging the crown, so he could not melt it down into a regularly shaped body in order to calculate its density. Instead he used the law of displacement to prove that the goldsmith had taken some of the gold and substituted silver instead.
The first written account of using medicine and entomology to solve criminal cases is attributed to the book of "Xi Yuan Lu" (translated as "Washing Away of Wrongs"), written in China by Song Ci (宋慈, 1186–1249) in 1248, during the Song Dynasty. In one of the accounts, the case of a person murdered with a sickle was solved by an investigator who instructed everyone to bring his sickle to one location. (He realized it was a sickle by testing various blades on an animal carcass and comparing the wound.) Flies, attracted by the smell of blood, eventually gathered on a single sickle. In light of this, the murderer confessed. The book also offered advice on how to distinguish between a drowning (water in the lungs) and strangulation (broken neck cartilage), along with other evidence from examining corpses on determining if a death was caused by murder, suicide or an accident.
Methods from around the world involved saliva and examination of the mouth and tongue to determine innocence or guilt. In ancient Chinese and Indian cultures, sometimes suspects were made to fill their mouths with dried rice and spit it back out. In ancient middle-eastern cultures the accused were made to lick hot metal rods briefly. Both of these tests had some validity since a guilty person would produce less saliva and thus have a drier mouth. The accused were considered guilty if rice was sticking to their mouths in abundance or if their tongues were severely burned due to lack of shielding from saliva.
Origins of forensic science.
In 16th-century Europe, medical practitioners in army and university settings began to gather information on the cause and manner of death. Ambroise Paré, a French army surgeon, systematically studied the effects of violent death on internal organs. Two Italian surgeons, Fortunato Fidelis and Paolo Zacchia, laid the foundation of modern pathology by studying changes that occurred in the structure of the body as the result of disease. In the late 18th century, writings on these topics began to appear. These included "A Treatise on Forensic Medicine and Public Health" by the French physician Francois Immanuele Fodéré and "The Complete System of Police Medicine" by the German medical expert Johann Peter Frank.
As the rational values of the Enlightenment era increasingly permeated society in the 18th century, criminal investigation became a more evidence-based, rational procedure − the use of torture to force confessions was curtailed, and belief in witchcraft and other powers of the occult largely ceased to influence the court's decisions. Two examples of English forensic science in individual legal proceedings demonstrate the increasing use of logic and procedure in criminal investigations at the time. In 1784, in Lancaster, John Toms was tried and convicted for murdering Edward Culshaw with a pistol. When the dead body of Culshaw was examined, a pistol wad (crushed paper used to secure powder and balls in the muzzle) found in his head wound matched perfectly with a torn newspaper found in Toms's pocket, leading to the latter's conviction.
In Warwick in 1816, a farm labourer was tried and convicted of the murder of a young maidservant. She had been drowned in a shallow pool and bore the marks of violent assault. The police found footprints and an impression from corduroy cloth with a sewn patch in the damp earth near the pool. There were also scattered grains of wheat and chaff. The breeches of a farm labourer who had been threshing wheat nearby were examined and corresponded exactly to the impression in the earth near the pool.
Toxicology and ballistics.
A method for detecting arsenious oxide, simple arsenic, in corpses was devised in 1773 by the Swedish chemist Carl Wilhelm Scheele. His work was expanded, in 1806, by German chemist Valentin Ross, who learned to detect the poison in the walls of a victim's stomach.
James Marsh was the first to apply this new science to the art of forensics. He was called by the prosecution in a murder trial to give evidence as a chemist in 1832. The defendant, John Bodle, was accused of poisoning his grandfather with arsenic-laced coffee. Marsh performed the standard test by mixing a suspected sample with hydrogen sulfide and hydrochloric acid. While he was able to detect arsenic as yellow arsenic trisulfide, when it was shown to the jury it had deteriorated, allowing the suspect to be acquitted due to reasonable doubt.
Annoyed by this, Marsh developed a much better test. He combined a sample containing arsenic with sulfuric acid and arsenic-free zinc, resulting in arsine gas. The gas was ignited, and it decomposed to pure metallic arsenic, which, when passed to a cold surface, would appear as a silvery-black deposit. So sensitive was the test that it could detect as little as one-fiftieth of a milligram of arsenic. He first described this test in "The Edinburgh Philosophical Journal" in 1836.
Henry Goddard at Scotland Yard pioneered the use of bullet comparison in 1835. He noticed a flaw in the bullet that killed the victim and was able to trace this back to the mold that was used in the manufacturing process.
Anthropometry.
The French police officer Alphonse Bertillon was the first to apply the anthropological technique of anthropometry to law enforcement, thereby creating an identification system based on physical measurements. Before that time, criminals could only be identified by name or photograph. Dissatisfied with the "ad hoc" methods used to identify captured criminals in France in the 1870s, he began his work on developing a reliable system of anthropometrics for human classification.
Bertillon created many other forensics techniques, including forensic document examination, the use of galvanoplastic compounds to preserve footprints, ballistics, and the dynamometer, used to determine the degree of force used in breaking and entering. Although his central methods were soon to be supplanted by fingerprinting, "his other contributions like the mug shot and the systematization of crime-scene photography remain in place to this day."
Fingerprints.
Sir William Herschel was one of the first to advocate the use of fingerprinting in the identification of criminal suspects. While working for the Indian Civil Service, he began to use thumbprints on documents as a security measure to prevent the then-rampant repudiation of signatures in 1858.
In 1877 at Hooghly (near Calcutta), he instituted the use of fingerprints on contracts and deeds, and he registered government pensioners' fingerprints to prevent the collection of money by relatives after a pensioner's death. Herschel also fingerprinted prisoners upon sentencing to prevent various frauds that were attempted in order to avoid serving a prison sentence.
In 1880, Dr. Henry Faulds, a Scottish surgeon in a Tokyo hospital, published his first paper on the subject in the scientific journal "Nature", discussing the usefulness of fingerprints for identification and proposing a method to record them with printing ink. He established their first classification and was also the first to identify fingerprints left on a vial. Returning to the UK in 1886, he offered the concept to the Metropolitan Police in London, but it was dismissed at that time.
Faulds wrote to Charles Darwin with a description of his method, but, too old and ill to work on it, Darwin gave the information to his cousin, Francis Galton, who was interested in anthropology. Having been thus inspired to study fingerprints for ten years, Galton published a detailed statistical model of fingerprint analysis and identification and encouraged its use in forensic science in his book "Finger Prints". He had calculated that the chance of a "false positive" (two different individuals having the same fingerprints) was about 1 in 64 billion.
Juan Vucetich, an Argentine chief police officer, created the first method of recording the fingerprints of individuals on file. In 1892, after studying Galton's pattern types, Vucetich set up the world's first fingerprint bureau. In that same year, Francisca Rojas of Necochea was found in a house with neck injuries whilst her two sons were found dead with their throats cut. Rojas accused a neighbour, but despite brutal interrogation, this neighbour would not confess to the crimes. Inspector Alvarez, a colleague of Vucetich, went to the scene and found a bloody thumb mark on a door. When it was compared with Rojas' prints, it was found to be identical with her right thumb. She then confessed to the murder of her sons.
A Fingerprint Bureau was established in Calcutta (Kolkata), India, in 1897, after the Council of the Governor General approved a committee report that fingerprints should be used for the classification of criminal records. Working in the Calcutta Anthropometric Bureau, before it became the Fingerprint Bureau, were Azizul Haque and Hem Chandra Bose. Haque and Bose were Indian fingerprint experts who have been credited with the primary development of a fingerprint classification system eventually named after their supervisor, Sir Edward Richard Henry. The Henry Classification System, co-devised by Haque and Bose, was accepted in England and Wales when the first United Kingdom Fingerprint Bureau was founded in Scotland Yard, the Metropolitan Police headquarters, London, in 1901. Sir Edward Richard Henry subsequently achieved improvements in dactyloscopy.
In the United States, Dr. Henry P. DeForrest used fingerprinting in the New York Civil Service in 1902, and by 1906, New York City Police Department Deputy Commissioner Joseph A. Faurot, an expert in the Bertillon system and a fingerprint advocate at Police Headquarters, introduced the fingerprinting of criminals to the United States.
Maturation.
By the turn of the 20th century, the science of forensics had become largely established in the sphere of criminal investigation. Scientific and surgical investigation was widely employed by the Metropolitan Police during their pursuit of the mysterious Jack the Ripper, who had killed a series of prostitutes in the 1880s. This case is a watershed in the application of forensic science. Large teams of policemen conducted house-to-house inquiries throughout Whitechapel. Forensic material was collected and examined. Suspects were identified, traced and either examined more closely or eliminated from the inquiry. Police work follows the same pattern today. Over 2000 people were interviewed, "upwards of 300" people were investigated, and 80 people were detained.
The investigation was initially conducted by the Criminal Investigation Department (CID), headed by Detective Inspector Edmund Reid. Later, Detective Inspectors Frederick Abberline, Henry Moore, and Walter Andrews were sent from Central Office at Scotland Yard to assist. Initially, butchers, surgeons and physicians were suspected because of the manner of the mutilations. The alibis of local butchers and slaughterers were investigated, with the result that they were eliminated from the inquiry. Some contemporary figures thought the pattern of the murders indicated that the culprit was a butcher or cattle drover on one of the cattle boats that plied between London and mainland Europe. Whitechapel was close to the London Docks, and usually such boats docked on Thursday or Friday and departed on Saturday or Sunday. The cattle boats were examined, but the dates of the murders did not coincide with a single boat's movements, and the transfer of a crewman between boats was also ruled out.
At the end of October, Robert Anderson asked police surgeon Thomas Bond to give his opinion on the extent of the murderer's surgical skill and knowledge. The opinion offered by Bond on the character of the "Whitechapel murderer" is the earliest surviving offender profile. Bond's assessment was based on his own examination of the most extensively mutilated victim and the post mortem notes from the four previous canonical murders. In his opinion the killer must have been a man of solitary habits, subject to "periodical attacks of homicidal and erotic mania", with the character of the mutilations possibly indicating "satyriasis". Bond also stated that "the homicidal impulse may have developed from a revengeful or brooding condition of the mind, or that religious mania may have been the original disease but I do not think either hypothesis is likely".
"Handbook for Coroners, police officials, military policemen" was written by the Austrian criminal jurist Hans Gross in 1893, and is generally acknowledged as the birth of the field of criminalistics. The work combined in one system fields of knowledge that had not been previously integrated, such as psychology and science, and which could be successfully used against crime. Gross adapted some fields to the needs of criminal investigation, such as crime scene photography. He went on to found the Institute of Criminalistics in 1912, as part of the University of Graz' Law School. This Institute was followed by many similar institutes all over the world.
In 1909, Archibald Reiss founded the "Institut de police scientifique" of the University of Lausanne (UNIL), the first school of forensic science in the world. Dr. Edmond Locard, became known as the "Sherlock Holmes of France". He formulated the basic principle of forensic science: "Every contact leaves a trace", which became known as Locard's exchange principle. In 1910, he founded what may have been the first criminal laboratory in the world, after persuading the Police Department of Lyon (France) to give him two attic rooms and two assistants.
Symbolic of the new found prestige of forensics and the use of reasoning in detective work was the popularity of the fictional character Sherlock Holmes, written by Arthur Conan Doyle in the late 19th century. He remains a great inspiration for forensic science, especially for the way his acute study of a crime scene yielded small clues as to the precise sequence of events. He made great use of trace evidence such as shoe and tire impressions, as well as fingerprints, ballistics and handwriting analysis, now known as questioned document examination. Such evidence is used to test theories conceived by the police, for example, or by the investigator himself. All of the techniques advocated by Holmes later became reality, but were generally in their infancy at the time Conan Doyle was writing. In many of his reported cases, Holmes frequently complains of the way the crime scene has been contaminated by others, especially by the police, emphasising the critical importance of maintaining its integrity, a now well-known feature of crime scene examination. He used analytical chemistry for blood residue analysis as well as toxicology examination and determination for poisons. He used ballistics by measuring bullet calibres and matching them with a suspected murder weapon.
20th century.
Later in the 20th century several British pathologists, Mikey Rochman, Francis Camps, Sydney Smith and Keith Simpson pioneered new forensic science methods. Alec Jeffreys pioneered the use of DNA profiling in forensic science in 1984. He realized the scope of DNA fingerprinting, which uses variations in the genetic code to identify individuals. The method has since become important in forensic science to assist police detective work, and it has also proved useful in resolving paternity and immigration disputes. DNA fingerprinting was first used as a police forensic test to identify the rapist and killer of two teenagers, Lynda Mann and Dawn Ashworth, who were both murdered in Narborough, Leicestershire, in 1983 and 1986 respectively. Colin Pitchfork was identified and convicted of murder after samples taken from him matched semen samples taken from the two dead girls.
Forensic science has been fostered by a number of national forensic science learned bodies including the American Academy of Forensic Sciences (founded 1948), publishers of the "Journal of Forensic Sciences"; the Canadian Society of Forensic Science (founded 1953), publishers of the "Journal of the Canadian Society of Forensic Science"; the British Academy of Forensic Sciences (founded 1960), publishers of "Medicine, science and the law", and the Australian Academy of Forensic Sciences (founded 1967), publishers of the "Australian Journal of Forensic Sciences".
Education and research.
Academic centre of education and research in forensic sciences:
Questionable techniques.
Some forensic techniques, believed to be scientifically sound at the time they were used, have turned out later to have much less scientific merit or none. Some such techniques include:
Litigation science.
Litigation science describes analysis or data developed or produced "expressly" for use in a trial versus those produced in the course of independent research. This distinction was made by the US 9th Circuit Court of Appeals when evaluating the admissibility of experts.
This uses demonstrative evidence, which is evidence created in preparation of trial by attorneys or paralegals.
International demographics.
In the United States there are over 12,000 forensic science technicians, as of 2010.
Examples in popular culture.
The Argentinean writer Jorge Luis Borges claims that the police novel genre is inaugurated with Edgar Allan Poe's short story, "The Murders in the Rue Morgue". But it is first Sherlock Holmes, the fictional character created by Sir Arthur Conan Doyle in works produced from 1887 to 1915, who used forensic science as one of his investigating methods. Conan Doyle credited the inspiration for Holmes on his teacher at the medical school of the University of Edinburgh, the gifted surgeon and forensic detective Joseph Bell.
Agatha Christie's Hercule Poirot and Miss Marple books and television series glorify too a similar prototype.
The comic strip "Dick Tracy" also featured a detective using a considerable number of forensic methods, although sometimes the methods were more fanciful than actually possible.
In comic books published by DC Comics, Barry Allen (alter ego of The Flash) is a forensic scientist for the Central City police department.
Defense attorney Perry Mason occasionally used forensic techniques, both in the novels and television series.
One of the earliest television series to focus on the scientific analysis of evidence was "Quincy, M.E." (1976–83, and based loosely on an even earlier Canadian series titled "Wojeck"), with the title character, a medical examiner working in Los Angeles solving crimes through careful study. The opening theme of each episode featured a clip of the title character, played by Jack Klugman, beginning a lecture to a group of police officers with "Gentlemen, you are about to enter the most fascinating sphere of police work, the world of forensic medicine." Later series with similar premises include "Dexter", "The Mentalist", "", "Hawaii Five-0", "Cold Case", "Bones", "Law & Order", "Body of Proof", "NCIS", "Criminal Minds", "Silent Witness", "Case Closed", "Midsomer Murders" and "Waking the Dead", depict glamorized versions of the activities of 21st-century forensic scientists. Some claim these TV shows have changed individuals' expectations of forensic science, an influence termed the "CSI effect".
Non-fiction TV shows such as "Forensic Files", "The New Detectives", "American Justice", and Dayle Hinman's "" have also popularized forensic science.
The Ace Attorney series features forensic science, mainly in ' and the DS-only case in '.
Controversies.
Questions about certain areas of forensic science, such as fingerprint evidence and the assumptions behind these disciplines have been brought to light in some publications including the "New York Post". The article stated that "No one has proved even the basic assumption: That everyone's fingerprint is unique." The article also stated that "Now such assumptions are being questioned - and with it may come a radical change in how forensic science is used by police departments and prosecutors." Law professor Jessica Gabel said on NOVA that forensic science "lacks the rigors, the standards, the quality controls and procedures that we find, usually, in science."
In America, on 25 June 2009, the Supreme Court issued a 5-to-4 decision in "Melendez-Diaz v. Massachusetts" stating that crime laboratory reports may not be used against criminal defendants at trial unless the analysts responsible for creating them give testimony and subject themselves to cross-examination. The Supreme Court cited the National Academies report "Strengthening Forensic Science in the United States" in their decision. Writing for the majority, Justice Antonin Scalia referred to the National Research Council report in his assertion that "Forensic evidence is not uniquely immune from the risk of manipulation."
In 2009, scientists indicated that it is possible to fabricate DNA evidence, therefore suggesting it is possible to falsely accuse or acquit a person or persons using forged evidence.
In America, another area of forensic science that has come under question in recent years is the lack of laws requiring the accreditation of forensic labs. Some states require accreditation, but some states do not. Because of this, many labs have been caught performing very poor work resulting in false convictions or acquittals. For example, it was discovered after an audit of the Houston Police Department in 2002 that the lab had fabricated evidence which led George Rodriguez being convicted of raping a fourteen-year-old girl. The former director of the lab, when asked, said that the total number of cases that could have been contaminated by improper work could be in the range of 5,000 to 10,000. This could have been avoided if the lab had been accredited by organizations such as ASCLD/Lab, which require crime labs to undergo rigorous assessments to show that they are able to perform multiple tests accurately. Once they become accredited, they are periodically re-evaluated to ensure that the lab is still functioning at its best. Periodic evaluations of a lab's performance by an independent organization will help to prevent scandals from occurring in forensic science laboratories.
Although forensic science has greatly enhanced the investigator's ability to solve crimes, it has limitations and must be scrutinized in and out of the courtroom to avoid the occurrence of wrongful convictions.
Forensic science and humanitarian work.
The International Committee of the Red Cross (ICRC) uses forensic science for humanitarian purposes to clarify the fate of missing persons after armed conflict, disasters or migration, and is one of the services related to Restoring Family Links and Missing Persons. Knowing what has happened to a missing relative can often make it easier to proceed with the grieving process and move on with life for families of missing persons.
Forensic science is used by various other organizations to clarify the fate and whereabouts of persons who have gone missing. Examples include the NGO Argentine Forensic Anthropology Team, working to clarify the fate of people who disappeared during the period of the 1976–1983 military dictatorship. The International Commission on Missing Persons (ICMP) uses forensic science to find missing persons, for example after the conflicts in the Balkans.

</doc>
<doc id="45712" url="http://en.wikipedia.org/wiki?curid=45712" title="Eggplant">
Eggplant

Eggplant ("Solanum melongena") or aubergine is a species of nightshade grown for its edible fruit. It is known in South Asia, Southeast Asia and South Africa as brinjal. While "eggplant" is the common name in American, Canadian, and Australian English, "aubergine" is much more common in British English. Other common names are melongene, garden egg, or guinea squash. The fruit is widely used in cooking, most notably as an important ingredient in dishes such as moussaka and ratatouille. As a member of the genus "Solanum", it is related to both the tomato and the potato. It was originally domesticated from the wild nightshade species, the thorn or bitter apple, "S. incanum", probably with two independent domestications, one in the region of South Asia, and one in East Asia.
Description.
The eggplant is a delicate, tropical perennial often cultivated as a tender or half-hardy annual in temperate climates. It grows 40 to 150 cm (16 to 57 in) tall, with large, coarsely lobed leaves that are 10 to 20 cm (4–8 in) long and 5 to 10 cm (2–4 in) broad. Semiwild types can grow much larger, to 225 cm (7 ft) with large leaves over 30 cm (12 in) long and 15 cm (6 in) broad. The stem is often spiny. The flower is white to purple, with a five-lobed corolla and yellow stamens. The egg-shaped glossy purple fruit has white flesh with a meaty texture. The cut surface of the flesh rapidly turns brown when the fruit is cut open. On wild plants, the fruit is less than 3 cm in diameter, but very much larger in cultivated forms, reaching 30 cm or more in length.
Botanically classified as a berry, the fruit contains numerous small, soft seeds which, though edible, taste bitter because, as a relative of tobacco, they contain nicotinoid alkaloids.
Names and etymology.
Some 18th-century European cultivars were yellow or white and resembled goose or hen's eggs, hence the name "eggplant".
Many other names, some of which are superficially quite different, all derive ultimately from a Dravidian word, with modern reflexes in Kannada "badanekāyi", Telugu "Vangakaya", Malayalam "vaṟutina", Tamil "kathirikkai". This was borrowed into Sanskrit and Pali as "vātiṅgaṇa", "vātigama", which in turn was borrowed by Persian as "bādinjān" بادنجان, then by Arabic as "(al-)bāḏinjān" باذنجان. In Albanian it is known as patrixhan or patellxhan, both derived from Arabic.
The Arabic name is the common source of almost all European names for this plant, but through two distinct paths of transmission, with the "melongene" family coming through the eastern Mediterranean, and the "aubergine" family through the western Mediterranean.
In the eastern Mediterranean, Byzantine Greek borrowed "bāḏinjān" as μελιτζάνα "melitzána", influenced by Greek μελανο- 'black'. That form came into medieval Latin as "melongena", which was used in the botanical works of Tournefort and Linnaeus. Though "melongene" has become obsolete in the standard English, as has the French "melanjan", it persists in the Caribbean English "melongene" or "meloongen". The usual word in Italian remains "melanzana". An alternative Italian etymology is "mela insana", insane apple.
Even the archaic English name "mad-apple" comes from the "melongena" family: in Italian, the word "melanzana" was reinterpreted in Italian as "mela insana", and translated into English as "mad apple".
In the western Mediterranean, "(al)-bāḏinjān" became Spanish "berenjena", Catalan as "albergínia", and Portuguese "beringela". The Catalan form was borrowed by French as "aubergine", which was then borrowed into British English.
In Eastern Slavic languages, such as Russian and Ukrainian, the word "baklazhan" is used, while Turkish has "patlıcan". The Hungarian name of the plant, "padlizsán", comes from Bulgarian "патладжан" or "патлиджан", which is in turn from Ottoman Turkish.
In Indian, South African, Malaysian, Singaporean, and West Indian English, the fruit is called "brinjal", from the Portuguese. The Indic name "baingan" or "baigan" is also sometimes used in South Asian English and in Trinidad.
In Kiswahili, it is called "biringanya".
History.
The plant species originated in cultivation. It has been cultivated in southern and eastern Asia since prehistory. The first known written record of the plant is found in "Qí mín yào shù" (齊民要術), an ancient Chinese agricultural treatise completed in 544. The numerous Arabic and North African names for it, along with the lack of the ancient Greek and Roman names, indicate it was introduced throughout the Mediterranean area by the Arabs in the early Middle Ages. A book on agriculture by Ibn Al-Awwam in 12th century Arabic Spain described how to grow aubergines. There are records from later medieval Catalan and Spanish.
The aubergine is unrecorded in England until the 16th century. An English botany book in 1597 stated:
Because of the plant's relationship with the Solanaceae (nightshade) family, the fruit was at one time believed to be extremely poisonous. The flowers and leaves can be poisonous if consumed in large quantities due to the presence of solanine.
Cultivated varieties.
Different varieties of the plant produce fruit of different size, shape, and color, though typically purple. The most widely cultivated varieties (cultivars) in Europe and North America today are elongated ovoid, 12–25 cm long (4 1⁄2 to 9 in) and 6–9 cm broad (2 to 4 in) in a dark purple skin.
A much wider range of shapes, sizes and colors is grown in India and elsewhere in Asia. Larger varieties weighing up to a kilogram (2.2 pounds) grow in the region between the Ganges and Yamuna rivers, while smaller varieties are found elsewhere. Colors vary from white to yellow or green, as well as reddish-purple and dark purple. Some cultivars have a color gradient, from white at the stem to bright pink to deep purple or even black. Green or purple cultivars in white striping also exist. Chinese varieties are commonly shaped like a narrower, slightly pendulous cucumber, and are sometimes called Japanese eggplants in North America.
Oval or elongated oval-shaped and black-skinned cultivars include 'Harris Special Hibush', 'Burpee Hybrid', 'Black Magic', 'Classic', 'Dusky', and 'Black Beauty'. Slim cultivars in purple-black skin include 'Little Fingers', 'Ichiban', 'Pingtung Long', and 'Tycoon'; in green skin, 'Louisiana Long Green' and 'Thai (Long) Green'; in white skin, 'Dourga'. Traditional, white-skinned, egg-shaped cultivars include 'Casper' and 'Easter Egg'. Bicolored cultivars with color gradient include 'Rosa Bianca', 'Violetta di Firenze', 'Bianca Smufata di Rosa' (heirloom), and 'Prosperosa' (heirloom). Bicolored cultivars with striping include 'Listada de Gandia' and 'Udumalapet'. In some parts of India, miniature varieties (most commonly called "vengan") are popular. A particular variety of green brinjal known as "Matti gulla" is grown in Matti, a village of the Udupi district in Karnataka state.
Cooking.
The raw fruit can have a somewhat bitter taste, but becomes tender when cooked and develops a rich, complex flavor. Many recipes advise salting, rinsing and draining of the sliced fruit (known as "degorging"), to soften it and to reduce the amount of fat absorbed during cooking, but mainly to remove the bitterness of the earlier cultivars. Some modern varieties—including large, purple varieties commonly imported into western Europe—do not need this treatment. The fruit is capable of absorbing large amounts of cooking fats and sauces, making for very rich dishes, but salting reduces the amount of oil absorbed. Eggplant, due to its texture and bulk, can be used as a meat substitute in vegan and vegetarian cuisine.
The fruit flesh is smooth, as in the related tomato. The numerous seeds are soft and edible along with the rest of the fruit. The thin skin is also edible.
Eggplant is used in the cuisine of many countries. Eggplant is widely used in its native Indian cuisine, for example in "sambhar", "dalma" (a "dal" preparation with vegetables, native to Odisha), chutney, curry, and "achaar". Owing to its versatile nature and wide use in both everyday and festive Indian food, it is often described (under the name "baingan" or "Brinjal") as the "king of vegetables". Roasted, skinned, mashed, mixed with onions, tomatoes and spices and then slow cooked make the famous Indian and Pakistani dish "Baingan ka Bhartha" or "gojju", similar to "salată de vinete" in Romania. Another version of the dish, "begun-pora" (eggplant charred or burnt), is very popular in Bangladesh and the east Indian states of Odisha and West Bengal where the pulp of the vegetable is mixed with raw chopped shallot, green chilies, salt, fresh coriander and mustard oil. Sometimes fried tomatoes and deep-fried potatoes are also added, creating a dish called "begun bhorta". In a dish called "bharli vangi", brinjal is stuffed with ground coconut, peanuts, and masala, and then cooked in oil.
It is often stewed, as in the French ratatouille, or deep fried as in the Italian "parmigiana di melanzane", the Turkish "karnıyarık" or Turkish and Greek "musakka/moussaka", and Middle-Eastern and South Asian dishes. Eggplants can also be battered before deep-frying and served with a sauce made of tahini and tamarind. In Iranian cuisine, it is blended with whey as "kashk e-bademjan", tomatoes as "mirza ghasemi" or made into stew as "khoresh-e-bademjan". It can be sliced and deep-fried, then served with plain yogurt, (optionally) topped with a tomato and garlic sauce, such as in the Turkish dish "patlıcan kızartması" (meaning fried aubergines) or without yogurt as in "patlıcan şakşuka". Perhaps the best-known Turkish eggplant dishes are "imam bayıldı" (vegetarian) and "karnıyarık" (with minced meat).
It may also be roasted in its skin until charred, so the pulp can be removed and blended with other ingredients, such as lemon, tahini, and garlic, as in the Arab "baba ghanoush" and the similar Greek "melitzanosalata". In Romania a mix of roasted eggplant, roasted red peppers, chopped onions, tomatoes, mushrooms, carrots, celery and spices is called "zacuscă" in Romania or "ajvar" in Croatia and the Balkans. A Spanish dish called "escalivada" calls for strips of roasted aubergine, sweet pepper, onion and tomato. In the La Mancha region of central Spain a small eggplant is pickled in vinegar, paprika, olive oil and red peppers. The result is berenjena de Almagro, Ciudad Real. A Levantine specialty is Makdous, another pickling of eggplants, stuffed with red peppers and walnuts in olive oil.
Eggplant can be hollowed out and stuffed with meat, rice, or other fillings, and then baked. In the Caucasus, for example, it is fried and stuffed with walnut paste to make "nigvziani badrijani". It can also be found in Chinese cuisine, braised (紅燒茄子), stewed (魚香茄子), steamed (凉拌茄子), or stuffed (釀茄子).
Cultivation.
In tropical and subtropical climates, eggplant can be sown directly into the garden. Eggplant grown in temperate climates fares better when transplanted into the garden after all danger of frost is passed. Seeds are typically started eight to ten weeks prior to the anticipated frost-free date.
Many of the pests and diseases that afflict other solanaceous plants, such as tomato, pepper (capsicum), and potato, are also troublesome to eggplants. For this reason, it should not be planted in areas previously occupied by its close relatives. Four years should separate successive crops of eggplants. Common North American pests include the potato beetles, flea beetles, aphids, and spider mites. (Adults can be removed by hand, though flea beetles can be especially difficult to control.) Good sanitation and crop rotation practices are extremely important for controlling fungal disease, the most serious of which is "Verticillium". A herbicide that is commonly used for eggplant is Dimethyl tetrachloroterephthalate. 
Spacing should be 45 cm to 60 cm between plants, depending on cultivar, and 60 to 90 cm (24 to 36 in) between rows, depending on the type of cultivation equipment being used. Mulching helps conserve moisture and prevent weeds and fungal diseases. The flowers are relatively unattractive to bees and the first blossoms often do not set fruit. Hand pollination improves the set of the first blossoms. Growers typically cut fruits from the vine just above the calyx owing to the somewhat woody stems. Flowers are complete, containing both female and male structures, and may be self-pollinated or cross-pollinated.
"Solanum melongena" is included in the Tasmanian Fire Service's list of low flammability plants, indicating that it is suitable for growing within a building protection zone.
Statistics.
According to FAO in 2012, production of eggplant is highly concentrated, with 90% of output coming from five countries. China is the top producer (58% of world output) and India is second (25%), followed by Iran, Egypt and Turkey. More than 4000000 acres are devoted to the cultivation of eggplant in the world. 
Health properties.
Nutritionally, eggplant is low in fat, protein, and carbohydrates. It also contains relatively low amounts of most important vitamins and minerals.
A 1998 study at the Institute of Biology of São Paulo State University, Brazil, found eggplant juice to significantly reduce weight, plasma cholesterol levels, and aortic cholesterol content in hypercholesterolemic rabbits.
The results of a 2000 study on humans suggested "S. melongena" infusion had a modest and transitory effect, no different from diet and exercise.
A 2004 study at the Heart Institute of the University of São Paulo found that, "Eggplant extract with orange juice is not to be considered an alternative to statins in reducing serum levels of cholesterol."
The nicotine content of aubergines, a concentration of 0.01 mg per 100g, is low in absolute terms, but is higher than any other edible plant. The amount of nicotine consumed by eating eggplant may be comparable to being in the presence of a smoker, depending on the cooking method. On average, 9 kg (20 lbs) of eggplant contains about the same amount of nicotine as a cigarette.
Allergies.
Case reports of itchy skin or mouth, mild headache, and stomach upset after handling or eating eggplant have been reported anecdotally and published in medical journals (see also oral allergy syndrome). A 2008 study of a sample of 741 people in India, where eggplant is commonly consumed, found nearly 10% reported some allergic symptoms after consuming eggplant, with 1.4% showing symptoms within less than two hours.
Contact dermatitis from eggplant leaves and allergy to eggplant flower pollen have also been reported. Individuals who are atopic (genetically predisposed to developing certain allergic hypersensitivity reactions) are more likely to have a reaction to eggplant, which may be because eggplant is high in histamines. A few proteins and at least one secondary metabolite have been identified as potential allergens. Cooking eggplant thoroughly seems to preclude reactions in some individuals, but at least one of the allergenic proteins survives the cooking process.
Varieties.
Genetically engineered variety.
Bt brinjal is a transgenic eggplant that contains a gene from the soil bacterium "Bacillus thuringiensis". This variety was designed to give the plant resistance to lepidopteran insects like the brinjal fruit and shoot borer ("Leucinodes orbonalis") and fruit borer ("Helicoverpa armigera").
On 9 February 2010, the Indian Environment Minister, Jairam Ramesh, imposed a moratorium on the cultivation of Bt brinjal. His decision was made after protest from several groups responding to regulatory approval of the cultivation of Bt brinjal in October, 2009. Ramesh stated the moratorium will last "for as long as it is needed to establish public trust and confidence".
Synonyms.
The eggplant is quite often featured in the older scientific literature under the junior synonyms "S. ovigerum" and "S. trongum". Several other now-invalid names have been uniquely applied to it:
A number of subspecies and varieties have been named, mainly by Dikii, Dunal, and (invalidly) by Sweet. Names for various eggplant types, such as "agreste, album, divaricatum, esculentum, giganteum, globosi, inerme, insanum, leucoum, luteum, multifidum, oblongo-cylindricum, ovigera, racemiflorum, racemosum, ruber, rumphii, sinuatorepandum, stenoleucum, subrepandum, tongdongense, variegatum, violaceum" and "viride", are not considered to refer to anything more than cultivar groups at best. On the other hand, "Solanum incanum" and cockroach berry ("S. capsicoides"), other eggplant-like nightshades described by Linnaeus and Allioni, respectively, were occasionally considered eggplant varieties, but this is not correct.
The eggplant has a long history of taxonomic confusion with the scarlet and Ethiopian eggplants, known as "gilo" and "nakati", and described by Linnaeus as "S. aethiopicum". The eggplant was sometimes considered a variety "violaceum" of that species. "S. violaceum" of de Candolle applies to Linnaeus' "S. aethiopicum". There is an actual "S. violaceum", an unrelated plant described by Ortega, which used to include Dunal's "S. amblymerum" and was often confused with the same author's "S. brownii".
Like the potato and "Solanum lichtensteinii", but unlike the tomato, which then was generally put in a different genus, the eggplant was also described as "S. esculentum", in this case once more in the course of Dunal's work. He also recognized varieties "aculeatum", "inerme" and "subinerme" at that time. Similarly, H.C.F. Schuhmacher and Peter Thonning named the eggplant as "S. edule", which is also a junior synonym of sticky nightshade ("S. sisymbriifolium"). Scopoli's "S. zeylanicum" refers to the eggplant, and that of Blanco to "S. lasiocarpum".
Gallery.
The following are eggplant fruit and plants from various parts of the world.

</doc>
<doc id="45714" url="http://en.wikipedia.org/wiki?curid=45714" title="Horseradish">
Horseradish

Horseradish ("Armoracia rusticana", syn. "Cochlearia armoracia") is a perennial plant of the Brassicaceae family (which also includes mustard, wasabi, broccoli,
and cabbage). The plant is probably native to southeastern Europe and western Asia. It is now popular around the world. It grows up to 1.5 m tall, and is cultivated primarily for its large, white, tapered root.
The intact horseradish root has hardly any aroma. When cut or grated, however, enzymes from the now-broken plant cells break down sinigrin (a glucosinolate) to produce allyl isothiocyanate (mustard oil), which irritates the mucous membranes of the sinuses and eyes. Grated mash should be used immediately or preserved in vinegar for best flavor. Once exposed to air or heat it will begin to lose its pungency, darken in color, and become unpleasantly bitter tasting over time.
History.
Horseradish, probably indigenous in temperate Eastern Europe, where its Slavic name "chren" seemed to Augustin Pyramus de Candolle more primitive than any Western synonym. Horseradish has been cultivated since antiquity. According to Greek mythology, the Delphic Oracle told Apollo that the horseradish was worth its weight in gold. Horseradish was known in Egypt in 1500 BC. Dioscorides listed horseradish equally as "Persicon sinapi" ("Diosc." 2.186) or "Sinapi persicum" ("Diosc." 2.168), which Pliny's Natural History reported as "Persicon napy"; Cato discusses the plant in his treatises on agriculture, and a mural in Pompeii shows the plant. Horseradish is probably the plant mentioned by Pliny the Elder in his "Natural History" under the name of "Amoracia", and recommended by him for its medicinal qualities, and possibly the Wild Radish, or "raphanos agrios" of the Greeks. The early Renaissance herbalists Pietro Andrea Mattioli and John Gerard showed it under "Raphanus". Though its modern Linnaean genus "Armoracia" was first applied to it by Heinrich Bernhard Ruppius, in his "Flora Jenensis", 1745, Linnaeus called it "Coclearia armoracia".
Both root and leaves were used as a medicine during the Middle Ages and the root was used as a condiment on meats in Germany, Scandinavia, and Britain. It was introduced to North America during European colonialization; both George Washington and Thomas Jefferson mention horseradish in garden accounts.
William Turner mentions horseradish as "Red Cole" in his "Herbal" (1551–1568), but not as a condiment. In "The Herball, or Generall Historie of Plantes" (1597), John Gerard describes it under the name of "raphanus rusticanus", stating that it occurs wild in several parts of England. After referring to its medicinal uses, he says:
[T]he Horse Radish stamped with a little vinegar put thereto, is commonly used among the Germans for sauce to eat fish with and such like meats as we do mustard.
The word "horseradish" is attested in English from the 1590s. It combines the word "horse" (formerly used as an adjective meaning "strong, large, or coarse") and the word "radish". Despite the name, this plant is poisonous to horses.
Cultivation.
Horseradish is perennial in hardiness zones 2–9 and can be grown as an annual in other zones, although not as successfully as in zones with both a long growing season and winter temperatures cold enough to ensure plant dormancy. After the first frost in the autumn kills the leaves, the root is dug and divided. The main root is harvested and one or more large offshoots of the main root are replanted to produce next year's crop. Horseradish left undisturbed in the garden spreads via underground shoots and can become invasive. Older roots left in the ground become woody, after which they are no longer culinarily useful, although older plants can be dug and re-divided to start new plants. The early season leaves can be distinctively different, asymmetric spiky, before the mature typical flat broad leaves start to be developed.
Pests and diseases.
Widely introduced by accident, "cabbageworms", the larvae of "Pieris rapae", the Small White Butterfly, are a common caterpillar pest in horseradish. The adults are white butterflies with black spots on the forewings that are commonly seen flying around plants during the day. The caterpillars are velvety green with faint yellow stripes running lengthwise down the back and sides. Full grown caterpillars are about 1 in in length. They move sluggishly when prodded. They overwinter in green pupal cases. Adults start appearing in gardens after the last frost and are a problem through the remainder of the growing season. There are three to five overlapping generations a year. Mature caterpillars chew large, ragged holes in the leaves leaving the large veins intact. Handpicking is an effective control strategy in home gardens.
Culinary uses.
Cooks use the terms "horseradish" or "prepared horseradish" to refer to the grated root of the horseradish plant mixed with vinegar. Prepared horseradish is white to creamy-beige in color. It will keep for months refrigerated but eventually will darken, indicating it is losing flavour and should be replaced. The leaves of the plant, while edible, are not commonly eaten, and are referred to as "horseradish greens", which have a flavor similar to that of the roots.
Horseradish sauce.
Horseradish sauce made from grated horseradish root and vinegar is a popular condiment in the United Kingdom and in Poland. In the UK it is usually served with roast beef, often as part of a traditional Sunday roast, but can be used in a number of other dishes also, including sandwiches or salads. A variation of horseradish sauce, which in some cases may substitute the vinegar with other products like lemon juice or citric acid, is known in Germany as "Tafelmeerrettich". Also popular in the UK is Tewkesbury mustard, a blend of mustard and grated horseradish originating in medieval times and mentioned by Shakespeare (Falstaff says: "his wit's as thick as Tewkesbury Mustard" in Henry IV Part II). A very similar mustard, called "Krensenf" or "Meerrettichsenf", is popular in Austria and parts of Eastern Germany. In France, "sauce au raifort" is popular in Alsatian cuisine. 
In the U.S., the term "horseradish sauce" refers to grated horseradish combined with mayonnaise or salad dressing. Prepared horseradish is a common ingredient in Bloody Mary cocktails and in cocktail sauce, and is used as a sauce or sandwich spread. Horseradish cream is a mixture of horseradish and sour cream and is served alongside au jus for a prime rib dinner.
The distinctive pungent taste of horseradish is from the compound allyl isothiocyanate. Upon crushing the flesh of horseradish, the enzyme myrosinase is released and acts on the glucosinolates sinigrin and gluconasturtiin, which are precursors to the allyl isothiocyanate. The allyl isothiocyanate serves the plant as a natural defense against herbivores. Since allyl isothiocyanate is harmful to the plant itself, it is stored in the harmless form of the glucosinolate, separate from the myrosinase enzyme. When an animal chews the plant, the allyl isothiocyanate is released, repelling the animal. Allyl isothiocyanate is an unstable compound, degrading over the course of days at 37 °C. Because of this instability, horseradish sauces lack the pungency of the freshly crushed roots.
Vegetable.
In Central and Eastern Europe horseradish is called "khren" (in various spellings like "kren") in many Slavic languages, in Austria, in parts of Germany (where the other German name "Meerrettich" isn't used), in North-East Italy, and in Yiddish ("כריין" translitered as "khreyn").
There are two varieties of khreyn. "Red" khreyn is mixed with red beet (beetroot) and "white" khreyn contains no beet. It is popular in Ukraine (under the name of "хрін", "khrin"), in Poland (under the name of "chrzan"), in Lithuania (krienai) in the Czech Republic ("křen"), in Russia ("хрен", "khren"), in Hungary ("torma"), in Romania ("hrean"), in Lithuania ("krienai"), in Bulgaria ("хрян", "khryan"), and in Slovakia (under the name of "chren"). Having this on the table is a part of Christian Easter and Jewish Passover tradition in Eastern and Central Europe.
Relation to wasabi.
The Japanese condiment wasabi, although traditionally prepared from the wasabi plant, is now usually made with horseradish due to the scarcity of the wasabi plant. The Japanese botanical name for horseradish is "seiyōwasabi" (セイヨウワサビ, 西洋山葵), or "Western wasabi". Both plants are members of the family Brassicaceae.
Nutritional and biomedical uses.
Compounds found in horseradish have been widely studied for a plethora of health benefits. Horseradish contains volatile oils, notably mustard oil, which has antibacterial properties due to the presence of allyl isothiocyanate. Fresh, the plant also contains average 79.31 mg of vitamin C per 100 g of raw horseradish.
The enzyme horseradish peroxidase (HRP), found in the plant, is used extensively in molecular biology and biochemistry.

</doc>
<doc id="45715" url="http://en.wikipedia.org/wiki?curid=45715" title="Arecaceae">
Arecaceae

The Arecaceae are a botanical family of perennial lianas, shrubs, and trees commonly known as palm trees. (Owing to historical usage, the family is alternatively called Palmae or Palmaceae.) They are flowering plants, the only family in the monocot order Arecales. Roughly 202 genera with around 2600 species are currently known, most of them restricted to tropical, subtropical, and warm temperate climates. Most palms are distinguished by their large, compound, evergreen leaves arranged at the top of an unbranched stem. However, many palms are exceptions, and in fact exhibit an enormous diversity in physical characteristics. As well as being morphologically diverse, palms also inhabit nearly every type of habitat within their range, from rainforests to deserts.
Palms are among the best known and most extensively cultivated plant families. They have been important to humans throughout much of history. Many common products and foods are derived from palms, and palms are also widely used in landscaping for their exotic appearance, making them one of the most economically important plants. In many historical cultures, palms were symbols for such ideas as victory, peace, and fertility. Today, palms remain a popular symbol for the tropics and vacations.
Morphology.
Whether as shrubs, trees, or vines, palms have two methods of growth: solitary or clustered. The common representation is that of a solitary shoot ending in a crown of leaves. This monopodial character may be exhibited by prostrate, trunkless, and trunk-forming members. Some common palms restricted to solitary growth include "Washingtonia" and "Roystonea". Palms may instead grow in sparse though dense clusters. The trunk develops an axillary bud at a leaf node, usually near the base, from which a new shoot emerges. The new shoot, in turn, produces an axillary bud and a clustering habit results. Exclusively sympodial genera include many of the rattans, "Guihaia", and "Rhapis". Several palm genera have both solitary and clustering members. Palms which are usually solitary may grow in clusters, and "vice versa". These aberrations suggest the habit operates on a single gene.
Palms have large, evergreen leaves that are either palmately ('fan-leaved') or pinnately ('feather-leaved') compound and spirally arranged at the top of the stem. The leaves have a tubular sheath at the base that usually splits open on one side at maturity. The inflorescence is a spadix or spike surrounded by one or more bracts or spathes that become woody at maturity. The flowers are generally small and white, radially symmetric, and can be either uni- or bisexual. The sepals and petals usually number three each, and may be distinct or joined at the base. The stamens generally number six, with filaments that may be separate, attached to each other, or attached to the pistil at the base. The fruit is usually a single-seeded drupe (sometimes berry-like) but some genera (e.g. "Salacca") may contain two or more seeds in each fruit.
The Arecaceae are notable among monocots for their height and for the size of their seeds, leaves, and inflorescences. "Ceroxylon quindiuense", Colombia's national tree, is the tallest monocot in the world, reaching up to 60 meters tall. The "coco de mer" ("Lodoicea maldivica") has the largest seeds of any plant, 40–50 cm in diameter and weighing 15–30 kg each. Raffia palms ("Raphia" spp.) have the largest leaves of any plant, up to 25 m long and 3 m wide. The "Corypha" species have the largest inflorescence of any plant, up to 7.5 m tall and containing millions of small flowers. "Calamus" stems can reach 200 m in length.
Range and habitat.
Most palms grow in the tropics. They are abundant throughout the tropics, and thrive in almost every habitat therein. Their diversity is highest in wet, lowland tropical forests, especially in ecological "hotspots" such as Madagascar, which has more endemic palms than all of Africa. Colombia may have the highest number of palm species in one country. Palms are most commonly seen throughout Africa, South America, the Arabian peninsula, southern and south-east Asia, northern Australia, the islands of tropical and sub-tropical parts of the Pacific Ocean, Mexico, and areas of the United States, Puerto Rico and Hawaii.
Only an estimated 130 palm species grow naturally beyond the tropics, mostly in the subtropics. The northernmost native palm is "Chamaerops humilis", which reaches 44°N latitude in southern France. The southernmost palm is the "Rhopalostylis sapida", which reaches 44°S on the Chatham Islands where an oceanic climate prevails. Some palms, such as the "Trachycarpus fortunei", grow well under cultivation in temperate climates, some as far north as 50°N in oceanic climates (Ireland, Scotland, England, and the Pacific Northwest, from Oregon to Vancouver).
Palms inhabit a variety of ecosystems. More than two-thirds of palm species live in tropical forests, where some species grow tall enough to form part of the canopy and shorter ones form part of the understory. Some species form pure stands in areas with poor drainage or regular flooding, including "Raphia hookeri" which is common in coastal freshwater swamps in West Africa. Other palms live in tropical mountain habitats above 1000 m, such as those in the genus "Ceroxylon" native to the Andes. Palms may also live in grasslands and scrublands, usually associated with a water source, and in desert oases such as the date palm. A few palms are adapted to extremely basic lime soils, while others are similarly adapted to extreme potassium deficiency and toxicity of heavy metals in serpentine soils.
Taxonomy.
Palms are a monophyletic group of plants, meaning the group consists of a common ancestor and all its descendants. Extensive taxonomic research on palms began with botanist H.E. Moore, who organized palms into 15 major groups based mostly on general morphological characteristics. The following classification, proposed by N.W. Uhl and J. Dransfield in 1987, is a revision of Moore's classification that organizes palms into six subfamilies.
A few general traits of each subfamily are listed below.
The Coryphoideae are the most diverse subfamily, and are a paraphyletic group, meaning all members of the group share a common ancestor, but the group does not include all the ancestor's descendants. Most palms in this subfamily have palmately lobed leaves and solitary flowers with three, or sometimes four carpels. The fruit normally develops from only one carpel.
Subfamily Calamoideae includes the climbing palms, such as rattans. The leaves are usually pinnate; derived characters (synapomorphies) include spines on various organs, organs specialized for climbing, an extension of the main stem of the leaf-bearing reflexed spines, and overlapping scales covering the fruit and ovary.
Subfamily Nypoideae contains only one species, "Nypa fruticans", which has large, pinnate leaves. The fruit is unusual in that it floats, and the stem is dichotomously branched, also unusual in palms.
Subfamily Ceroxyloideae has small to medium-sized flowers, spirally arranged, with a gynoecium of three joined carpels.
The Arecoideae are the largest subfamily, with six diverse tribes (Areceae - Caryoteae - Cocoeae - Geonomeae - Iriarteeae - Podococceae) containing over 100 genera. All tribes have pinnate or bipinnate leaves and flowers arranged in groups of three, with a central pistillate and two staminate flowers.
The Phytelephantoideae are a monoecious subfamily. Members of this group have distinct monopodial flower clusters. Other distinct features include a gynoecium with five to 10 joined carpels, and flowers with more than three parts per whorl. Fruits are multiple-seeded and have multiple parts.
Currently, few extensive phylogenetic studies of Arecaceae exist. In 1997, Baker" et al." explored subfamily and tribe relationships using chloroplast DNA from 60 genera from all subfamilies and tribes. The results strongly showed the Calamoideae are monophyletic, and Ceroxyloideae and Coryphoideae are paraphyletic. The relationships of Arecoideae are uncertain, but they are possibly related to Ceroxyloideae and Phytelephantoideae. Studies have suggested the lack of a fully resolved hypothesis for the relationships within the family is due to a variety of factors, including difficulties in selecting appropriate outgroups, homoplasy in morphological character states, slow rates of molecular evolution important for the use of standard DNA markers, and character polarization. However, hybridization has been observed among "Orbignya" and "Phoenix" species, and using chloroplast DNA in cladistic studies may produce inaccurate results due to maternal inheritance of the chloroplast DNA. Chemical and molecular data from non-organelle DNA, for example, could be more effective for studying palm phylogeny.
Selected genera.
See list of Arecaceae genera arranged by taxonomic groups or by alphabetical order for a complete listing of genera.
Evolution.
The Arecaceae are the first modern family of monocots clearly represented in the fossil record. Palms first appear in the fossil record around 80 million years ago, during the late Cretaceous period. The first modern species, such as "Nypa fruticans" and "Acrocomia aculeata", appeared 94 million years ago, confirmed by fossil "Nypa" pollen dated to 94 million years ago. Palms appear to have undergone an early period of adaptive radiation. By 60 million years ago, many of the modern, specialized genera of palms appeared and became widespread and common, much more widespread than their range today. Because palms separated from the monocots earlier than other families, they developed more intrafamilial specialization and diversity. By tracing back these diverse characteristics of palms to the basic structures of monocots, palms may be valuable in studying monocot evolution. Several species of palms have been identified from flowers preserved in amber, including "Palaeoraphe dominicana" and "Roystonea palaea".
Evidence can also be found in samples of petrified palmwood.
Uses.
Human use of palms is as old or older than human civilization itself, starting with the cultivation of the date palm by Mesopotamians and other Middle Eastern peoples 5000 years or more ago. Date wood, pits for storing dates, and other remains of the date palm have been found in Mesopotamian sites. The date palm had a tremendous effect on the history of the Middle East. W.H. Barreveld wrote:
An indication of the importance of palms in ancient times is that they are mentioned more than 30 times in the Bible, and at least 22 times in the Quran.
Arecaceae have great economic importance, including coconut products, oils, dates, palm syrup, ivory nuts, carnauba wax, rattan cane, raffia and palm wood.
Along with dates mentioned above, members of the palm family with human uses are numerous.
The southeastern U.S. state of South Carolina is nicknamed the Palmetto State after the sabal palmetto (cabbage palmetto), logs from which were used to build the fort at Fort Moultrie. During the American Revolutionary War, they were invaluable to those defending the fort, because their spongy wood absorbed or deflected the British cannonballs. The sabal palmetto is also the state tree of Florida.
Some palms can be grown as far north as the United States' Mid-Atlantic, such as the National Arboretum in Washington, D.C., southern Midwest, and even north along the Pacific coast to Oregon, Washington and British Columbia, where ocean winds have a warming effect. Species of transplanted palms have even been known to have survived as far north as Devon. The Chinese "Trachycarpus fortunei" is being grown experimentally on the Faroe Islands at 62°N, with young plants doing well so far.
Endangered species.
Like many other plants, palms have been threatened by human intervention and exploitation. The greatest risk to palms is destruction of habitat, especially in the tropical forests, due to urbanization, wood-chipping, mining, and conversion to farmland. Palms rarely reproduce after such great changes in the habitat, and those with small habitat ranges are most vulnerable to them. The harvesting of heart of palm, a delicacy in salads, also poses a threat because it is derived from the palm's apical meristem, a vital part of the palm that cannot be regrown. The use of rattan palms in furniture has caused a major population decrease in these species that has negatively affected local and international markets, as well as biodiversity in the area. The sale of seeds to nurseries and collectors is another threat, as the seeds of popular palms are sometimes harvested directly from the wild. At least 100 palm species are currently endangered, and nine species have reportedly recently become extinct.
However, several factors make palm conservation more difficult. Palms live in almost every type of warm habitat and have tremendous morphological diversity. Most palm seeds lose viability quickly, and they cannot be preserved in low temperatures because the cold kills the embryo. Using botanical gardens for conservation also presents problems, since they can only house a few plants of any species or truly imitate the natural setting. Also, the risk of cross-pollination can lead to hybrid species.
The Palm Specialist Group of the World Conservation Union (IUCN) began in 1984, and has performed a series of three studies to find basic information on the status of palms in the wild, use of wild palms, and palms under cultivation. Two projects on palm conservation and use supported by the World Wildlife Fund took place from 1985 to 1990 and 1986–1991, in the American tropics and southeast Asia, respectively. Both studies produced copious new data and publications on palms. Preparation of a global action plan for palm conservation began in 1991, supported by the IUCN, and was published in 1996.
The rarest palm known is "Hyophorbe amaricaulis". The only living individual remains at the Botanic Gardens of Curepipe in Mauritius.
Pest species.
Pests that attack a variety of species of palm trees include:
Symbolism.
The palm branch was a symbol of triumph and victory in pre-Christian times. The Romans rewarded champions of the games and celebrated military successes with palm branches. Early Christians used the palm branch to symbolize the victory of the faithful over enemies of the soul, as in the Palm Sunday festival celebrating the triumphal entry of Jesus into Jerusalem. In Judaism, the palm represents peace and plenty, and is one of the Four Species of Sukkot; the palm may also symbolize the Tree of Life in Kabbalah.
Panaiveriyamman was an ancient Tamil tree deity related to fertility. Named after "panai", the Tamil name for the Palmyra palm, she was also known as Taalavaasini, a name that further related her to all types of palms.
Today, the palm, especially the coconut palm, remains a symbol of the tropical island paradise.
Palms appear on the flags and seals of several places where they are native, including those of Haiti, Guam, Saudi Arabia, Florida and South Carolina.
Other plants.
Some species commonly called palms, though they are not true palms, include:
References.
</dl>

</doc>
<doc id="45716" url="http://en.wikipedia.org/wiki?curid=45716" title="Transporter (Star Trek)">
Transporter (Star Trek)

A transporter is a fictional teleportation machine used in the Star Trek universe. Transporters convert a person or object into an energy pattern (a process called "dematerialization"), then "beam" it to a target, where it is reconverted into matter ("rematerialization"). The term "transporter accident" is a catch-all term for when a person or object does not rematerialize correctly.
According to "The Making of Star Trek", "Star Trek" creator Gene Roddenberry's original plan did not include transporters, instead calling for characters to land the starship itself. However, this would have required unfeasible and unaffordable sets and model filming, as well as episode running time spent while landing, taking off, etc. The shuttlecraft was the next idea, but when filming began, the full-sized shooting model was not ready. Transporters were devised as a less expensive alternative, achieved by a simple fade-out/fade-in of the subject. Transporters first appear in the original pilot episode "". The transporter special effect, before being done using computer animation, was created by turning a slow-motion camera upside down and photographing some backlit shiny grains of aluminium powder that were dropped between the camera and a black background.
Gene Roddenberry in 1964 had not seen "The Fly" upon his first draft of "The Cage", but it was brought to his attention, and this is how the transporter was considered. The later Doctor Who series 'The Seeds of Death' in 1969 also had teleport device called "T-Mat" for a Teleport Matter transfer.
According to the "", the three touch-sensitive light-up bars on the Enterprise-D's transporter console were an homage to the three sliders used on the duotronic transporter console on the original Enterprise in The Original Series.
In August 2008, physicist Michio Kaku predicted in Discovery Channel Magazine that a teleportation device similar to those in Star Trek would be invented within 100 years.
Depiction.
History.
According to dialogue in the ' ("ENT") episode "Daedalus", the transporter was invented in the early 22nd century by Dr. Emory Erickson, who also became the first human to be successfully transported. Although the "Enterprise" (NX-01) has a transporter, the crew does not routinely use it for moving biological organisms. (Captain Jonathan Archer once said that he wouldn't even put his dog through it.) Instead, they generally prefer using shuttlepods or other means of transportation unless no other means of transportation are possible or feasible. The capability is rare; in "The Andorian Incident", the Andorians, technologically far superior to Starfleet in many regards, are explicitly stated not to possess the technology, and in "Chosen Realm", a group of alien religious extremists who hijack the ship is unaware of it to the point that when Archer, choosing himself after their leader insists on sacrificing a crew member, claims that the device disintegrates matter rather than teleporting it, he is unhesitatingly taken at his word. The crew aboard the 23rd century USS "Enterprise" frequently use the transporter. By the 24th century, transporter travel was reliable and "the safest way to travel" according to dialogue in the ' ("TNG") episode "Realm of Fear".
According to the "" episode "Homefront", Starfleet Academy cadets receive transporter rations, and the Sisko family once used a transporter to move furniture into a new home.
Despite its frequent use, characters such as Leonard McCoy and Katherine Pulaski are reluctant to use the transporter, as the characters express in the "Next Generation" episodes "Encounter at Farpoint" and "", respectively. Reginald Barclay expresses his outright fear of transporting in "Realm of Fear".
Capabilities and limitations.
The television series and films do not go into great detail about transporter technology. The "" claims that the devices transport objects in real time, accurate to the quantum level. The episode "Realm of Fear" specifies the length of a transport under unusual circumstances would last "... four or five seconds; about twice the normal time". This calculates the length of a typical transport as between 2 and 2.5 seconds and possibly less. Heisenberg compensators remove uncertainty from the subatomic measurements, making transporter travel feasible. Further technology involved in transportation include a computer pattern buffer to enable a degree of leeway in the process. When asked "How does the Heisenberg compensator work?" by "Time" magazine, "Star Trek" technical adviser Michael Okuda responded: "It works very well, thank you."
According to "The Original Series" ("TOS") writers' guide, the effective range of a transporter is 40,000 kilometers, although thick layers of rock can reduce this range ("TNG": ""). The "TOS" episode "" however, appears to indicate that the transporters' maximum range, during that time period in "Star Trek" history, is actually around 30,000 kilometers. Transporter operations have been disrupted or prevented by dense metals ("TNG": ""), solar flares ("TNG": ""), and other forms of radiation, including electromagnetic ("TNG": ""; "TNG": "") and nucleonic ("TNG": ""), and affected by ion storms ("TOS": ""). Transporting, in progress, has also been stopped by telekinetic powers ("TNG": "") and by brute strength ("TNG": ""). The "TNG" episode "" features a dangerous and experimental "subspace transporter" capable of interstellar distances and the Dominion had the ability to transport over great distances ("DS9": ""). The 40,000-kilometer limit is also referenced in "ENT": "".
Starfleet transporters from the "TNG" era onward include a device that can detect and disable an active weapon ("TNG": ""), and a bio-filter to remove contagious microbes or viruses from an individual in transport ("TNG": ""). The transporter can also serve a tactical purpose, such as beaming a photon grenade or photon torpedo to detonate at remote locations ("TNG": "", "": ""), or to outright destroy objects ("TNG": "Captain's Holiday"). The "TOS" episode "A Taste of Armageddon" mentions Vendikar materializing fusion bombs over targets of enemy planet Eminiar VII in the course of theoretical computer warfare.
Whenever a person or object is transported, the machine creates a memory file of the pattern. This has been used at least once in every "Star Trek" series to revert people adversely affected by a transport to their original state.
Various episodes of "Deep Space Nine" ("DS9") and "Voyager" ("VOY") have introduced two anti-transporter devices: transport inhibitors and transporter scramblers. Inhibitors prevent a transporter beam from "locking on" to whatever the device is attached. Scramblers distort the pattern that is in transit, literally scrambling the atoms upon rematerialization, resulting in the destruction of inanimate objects and killing living beings by rematerializing them as masses of random tissue; this was gruesomely demonstrated in the "DS9" episode "".
Transporter operations can also be curtailed when either the point of origin and/or the intended target site is moving at warp velocities. In the "TNG" episode, "", a "long-range" or "near-warp" transport was required as a transporter beam cannot penetrate a warp field. (In the 2009 "Star Trek" film Kirk and Scotty beam aboard while the "Enterprise" is traveling at warp, however, the movie takes place in an alternate continuity, thus not affecting the Prime Continuity used in all previous media and the Star Trek Online computer game.) To deposit an away team on the planet Gravesworld while at the same time responding to a distress signal, the "Enterprise" would only drop out of warp drive just long enough to energize the transporter beam. Geordi La Forge personally performed the delicate operation, which involved compensating for the ship's relativistic motion. After materializing, Deanna Troi commented that for a moment she thought she was trapped in a nearby wall, to which Worf replied, "For a moment, you were." In later stories (" and "), it was confirmed that the transporter would work at warp only if the sending and receiving sites were moving at equal velocities.
In his book, "The Physics of Star Trek", after explaining the difference between transporting information and transporting the actual atoms, Krauss notes that "The Star Trek writers seem never to have got it exactly clear what they want the transporter to do. Does the transporter send the atoms and the bits, or just the bits?" He notes that according to the canon definition of the transporter the former seems to be the case, but that that definition is inconsistent with a number of applications, particularly incidents, involving the transporter, which appear to involve only a transport of information, for example the way in which it splits Kirk into two version in the episode " or the way in which Riker is similarly split in the episode ". Krauss elaborates that: "If the transporter carries both the matter stream and the information signal, this splitting phenomenon is impossible. The number of atoms you end up with has to be the same as the number you began 
with. There is no possible way to replicate people in this manner. On the other hand, if only the information were beamed up, one could imagine combining it with atoms that might be stored aboard a starship and making as many copies as you wanted of an individual."
Transporter accidents.
Aside from external influences causing disruptions in the normal operations of transporters, the technology itself has been known to fail on occasion, causing serious injury or usually death to those being transported. This was demonstrated in "" when a malfunction in the transporter sensor circuits resulted in insufficient signal being present at the "Enterprise" end to successfully rematerialize the two subjects, and Starfleet was unable to pull them back to where they had dematerialized from. The transporter system attempted to rematerialize what little signal was available, and despite the efforts of Kirk and Scotty, the system failed and both subjects vanished from the transporter pad. Kirk, visibly shaken by what he had witnessed asked, "Starfleet, do you have them?", to which the response was made ""Enterprise", what we got back didn't live long, fortunately".
By the time of "", transporter technology has advanced considerably, meaning that accidents are now remote, if not near impossible. In fact, in the episode "Realm of Fear", Geordi La Forge states that there have been no more than 2 or 3 transporter accidents in the preceding 10 years. Reference is also made to the advancement of transporter technology in the same episode, where Chief O'Brien states that each individual transporter pad has four redundant scanners whereby in the event a scanner fails the other three will take over, and that he has never lost anyone having been a transporter operator for over twenty years.
In the "" episode "Tuvix", a transporter accident combines both the physical and behavioral aspects of Lt. Tuvok and Neelix into a single being.
Technological and Scientific Restrictions.
While several characters have asserted that transporters cannot transport through a ship's shields or planetary defense shields, there are instances of this "rule" being broken through a technobabble solution ("TNG": "", "DS9": "Trials and Tribble-ations") or disregarded by the show's writers ("Voyager": "").
In ', Vice Admiral James T. Kirk and Lieutenant Saavik carry on a conversation during rematerialization. In ', Dr. Gillian Taylor jumps into Kirk's transporter beam during dematerialization, and rematerializes without any apparent ill effects. This is probably due to the "annular confinement beam", a component of the transporter mentioned in the various television episodes which serves to keep patterns separate from one another. In the same film, Mr. Spock is beamed into a cloaked ship while walking.
According to the "TNG Technical Manual", the transporter cannot move antimatter, but in the "Voyager" episode "Dark Frontier" "Voyager" transported a live photon torpedo equipped with antimatter onto a Borg ship. Also in "TOS" episode "" Kirk and a fellow crewman beam down to the surface of a planet with an antimatter 'bomb'. The "TAS" episode "One of Our Planets Is Missing" has the Enterprise beaming a chunk of antimatter into a stasis box.
In the original series, beaming to and from the transporter chamber was a necessity. This is explained in the "TOS" episode "Day of the Dove". Spock and Scotty had said that doing a site-to-site transport, as they are referred to on the show, on board the ship could be risky. They could beam into a deck or other inanimate object and get stuck there. However, there are apparently safeguards in place to prevent people from being beamed into hostile environments such as under water and into lava pits, although it is possible to override this safety feature; for example, in the "TOS" episode "And the Children Shall Lead", two security guards are beamed into open space. In the following series, however, the transporter room seems to become mostly obsolete, the actual equipment notwithstanding. Characters are shown activating the transporter from ordinary consoles and beaming from place to place without apparent trouble. The main operator can likewise send those in transport anywhere with ease (for example, in the "Voyager" episode ", a medical console is used to transport a body from the morgue to the surgical bay). A possible explanation for this is put forward in the ", where such site-to-site transports would probably use twice as much energy as would be required for transport to or from the transporter room itself, since the subject would have to be beamed to the transporter, stored, then shunted to their destination. In addition, the six circles on the platform are generally used as targets for the subjects to stand on, but they do not appear to represent any limitation of the hardware to six or fewer people. People have been transported carrying others, in a coffin style transport, as well as animals, hay, and various inanimate objects.
Dialogue in "Deep Space Nine" indicates the existence of portable transporters, but these are never seen. The "Next Generation" episode " features emergency transporter armbands, although these may have served only to activate a remote transporter. To confuse things more, " featured the prototype "emergency transport unit". Tom Paris uses a portable transporter in the "Voyager" episode "". However, in "Star Trek: Into Darkness", Khan Noonien Singh uses a portable transporter to beam from a helicopter to the Klingon home world.
For special effects reasons, in "TOS", people generally appear immobilized during transport, with the exception of Kirk in the episode "That Which Survives". However, by "TNG", characters can move within the confines of the transporter beam while being transported, although this is rarely shown. Persons being transported are at least sometimes able to perceive the functioning of the transporter while they are in transit. In the "TOS" episode "", the "Enterprise" transporter malfunctions while transporting Scotty from the disabled USS "Constellation" to the "Enterprise" due to a power drain, and Scotty's pattern is nearly lost in transit. As soon as he successfully materializes, Scotty asks the transporter operator with concern, "What's the matter with that thing?" and orders the transporter to be taken off-line for emergency repair. This incident does not necessarily suggest that such malfunctions would have had strong effects on the person being transported, however, for Scotty's expertise might have allowed him to perceive and diagnose subtle effects during transit that most people would not.
Some species do not use transporter technology for a variety of reasons. In the first appearance of Trill in the "TNG" episode "", Trill were unable to be transported, once joined with a symbiont. It seems that was due to the symbiont being detected and removed by the transporter technology as an infestation in the host. Odan, the Trill host in this episode, is reluctant to say why he will not travel this way, and it only becomes apparent that he is carrying a symbiont when he is later injured. All the crew of the "Enterprise" react as if they have had no contact with this species before. It later becomes apparent that joined Trill have been working in the Federation for some time.
When Trill became a regularly used race in later series, the inability to use transporter technology was dropped. No explanation of the change is ever given in any series where they appear, but according to an article on startrek.com, Trill are actually more than one host race.
In popular culture.
The famous catchphrase "Beam me up, Scotty" refers to the transporter device, which was often operated by chief engineer Montgomery Scott during the original series. The phrase was never uttered by anyone in the original series, although the lines "Scotty, beam us up" and "Beam me up" were spoken by Captain Kirk in that series. "Scotty, beam me up" was spoken by Admiral Kirk in "Star Trek IV: The Voyage Home". On the special edition DVD of "Star Trek IV", the text commentary provided by Michael and Denise Okuda (co-authors of "The Star Trek Encyclopedia" and "The Star Trek Chronology: The History of the Future") indicates that this was the closest anyone came to using that catchphrase in an official "Star Trek" production.
References.
</dl>

</doc>
<doc id="45720" url="http://en.wikipedia.org/wiki?curid=45720" title="Schleswig, Schleswig-Holstein">
Schleswig, Schleswig-Holstein

 
Schleswig (]; Danish: "Slesvig"; South Jutlandic: "Sljasvig"; archaic English: "Sleswick"; Low German: "Sleswig") is a town in the northeastern part of Schleswig-Holstein, Germany. It is the capital of the "Kreis" (district) Schleswig-Flensburg. It has a population of about 27,000, the main industries being leather and food processing. It takes its name from the Schlei, an inlet of the Baltic sea at the end of which it sits, and "vik" or "vig" which means bay in Old Norse and Danish. Schleswig or Slesvig therefore means "bay of the Schlei". (There is also a suggestion that the state's namesake and the term Slesvig originated from a tribe of West Slavs who lived in "Slavsvick" between the 5th century and 10th century AD.) 
Geography.
The city lies at the western end of the Schlei Förde, which separates the two peninsulas of Angeln and Schwansen and is on the western edge of the Schleswig-Holstein Uplands on the transition to the Geest country. The urban area ranges from 0 to 20 m above sea level. Brautsee (lake) is in the town.
The nearest major cities are Flensburg, Husum and Kiel. Autobahn 7 runs immediately west of the city. Highways 76 and 77 end in Schleswig and B 201 runs to the north of the town. Schleswig station is a stop for InterCity and Intercity-Express trains and is on the Hamburg–Neumünster–Flensburg and Husum–Kiel lines.
Climate.
The climate is humid and maritime. The annual mean temperature is 8 °C and precipitation averages 925 mm.
History.
The Viking settlement of Hedeby, located south of the modern town, was first mentioned in 804. It was a powerful settlement in the Baltic region, dominating the area for more than 200 years. In 1050, following several destructions, the population was moved to the opposite shore of the Schlei, becoming the city of Schleswig. In 1066 Hedeby was finally destroyed, and Schleswig remained as a part of the Danish kingdom.
In 1544 Gottorf Castle became the residence of the local rulers. The dukes of Gottorf were vassals of the Danish kings and ruled over much of present day Schleswig-Holstein. In 1721, when the Great Northern War ended, the dukes of Gottorf lost their power and their land became Danish crown land. After the Second Schleswig War (1864), Schleswig was annexed by the Kingdom of Prussia.

</doc>
<doc id="45728" url="http://en.wikipedia.org/wiki?curid=45728" title="Hamburger">
Hamburger

A hamburger (also called a beef burger, hamburger sandwich, burger or hamburg) is a sandwich consisting of one or more cooked patties of ground meat, usually beef, placed inside a sliced bun. Hamburgers are often served with lettuce, bacon, tomato, onion, pickles, cheese and condiments such as mustard, mayonnaise, ketchup, relish, and green chile.
The term "burger" can also be applied to the meat patty on its own, especially in the UK where the term "patty" is rarely used. The term may be prefixed with the type of meat used, as in "turkey burger".
Etymology.
The term "hamburger" originally derives from Hamburg, Germany's second largest city. In High German, "Burg" means "fortified settlement" or "fortified refuge" and is a widespread component of place names. "Hamburger" in German is the demonym of Hamburg, similar to "frankfurter" and "wiener", names for other meat-based foods and demonyms of the cities of Frankfurt and Vienna (Wien), respectively.
The term "burger", a back-formation, is associated with many different types of sandwiches similar to a (ground meat) hamburger, sometimes of different meats like the buffalo burger, venison, kangaroo, turkey, elk, lamb, salmon burger or veggie burger.
History.
There have been many claims about the origin of the hamburger. The earliest known report in a newspaper is from July 5, 1896, when the "Chicago Daily Tribune" made a highly specific claim regarding a "hamburger sandwich" in an article about a "Sandwich Car:" "A distinguished favorite, only five cents, is Hamburger steak sandwich, the meat for which is kept ready in small patties and 'cooked while you wait' on the gasoline range." According to Congresswoman Rosa DeLauro, the hamburger, a ground meat patty between two slices of bread, was first created in America in 1900 by Louis Lassen, a Danish immigrant, owner of Louis' Lunch in New Haven, Connecticut. There have been rival claims by Charlie Nagreen, Frank and Charles Menches, Oscar Weber Bilby, and Fletcher David. White Castle traces the origin of the hamburger to Hamburg, Germany with its invention by Otto Kuase. However, it gained national recognition at the 1904 St. Louis World's Fair when the "New York Tribune" referred to the hamburger as "the innovation of a food vendor on the pike". No conclusive argument has ever ended the dispute over invention. An article from ABC News sums up: "One problem is that there is little written history. Another issue is that the spread of the burger happened largely at the World's Fair, from tiny vendors that came and went in an instant. And it is entirely possible that more than one person came up with the idea at the same time in different parts of the country."
Claims of invention.
Louis Lassen.
Louis Lassen of Louis' Lunch, a small lunch wagon in New Haven, Connecticut, is said to have sold the first hamburger and steak sandwich in the U.S. in 1900. "New York" magazine states that "The dish actually had no name until some rowdy sailors from Hamburg named the meat on a bun after themselves years later", noting also that this claim is subject to dispute. A customer ordered a quick hot meal and Louis was out of steaks. Taking ground beef trimmings, Louis made a patty and grilled it, putting it between two slices of toast. Some critics like Josh Ozersky, a food editor for "New York Magazine", claim that this sandwich was not a hamburger because the bread was toasted.
Charlie Nagreen.
One of the earliest claims comes from Charlie Nagreen, who in 1885 sold a meatball between two slices of bread at the Seymour Fair now sometimes called the Outagamie County Fair. The Seymour Community Historical Society of Seymour, Wisconsin, credits Nagreen, now known as "Hamburger Charlie", with the invention. Nagreen was fifteen when he was reportedly selling pork sandwiches at the 1885 Seymour Fair, made so customers could eat while walking. The Historical Society explains that Nagreen named the hamburger after the Hamburg steak with which local German immigrants were familiar.
Otto Kuase.
According to White Castle, Otto Kuase was the inventor of the hamburger. In 1891 he created a beef patty cooked in butter and topped with a fried egg. German sailors would later omit the fried egg.
Oscar Weber Bilby.
The family of Oscar Weber Bilby claim the first-known hamburger on a bun was served on July 4, 1891 on Grandpa Oscar's farm. The bun was a yeast bun. In 1995, Governor Frank Keating proclaimed that the first true hamburger on a bun was created and consumed in Tulsa, Oklahoma in 1891, calling Tulsa, "The Real Birthplace of the Hamburger."
Frank and Charles Menches.
Frank and Charles Menches claim to have sold a ground beef sandwich at the Erie County Fair in 1885 in Hamburg, New York. During the fair, they ran out of pork sausage for their sandwiches and substituted beef. Kunzog, who spoke to Frank Menches, says they exhausted their supply of sausage, so purchased chopped up beef from a butcher, Andrew Klein. Historian Joseph Streamer wrote that the meat was from Stein's market not Klein's, despite Stein's having sold the market in 1874. The story notes that the name of the hamburger comes from Hamburg, New York not Hamburg Germany. Frank Menches's obituary in "The New York Times" states that these events took place at the 1892 Summit County Fair in Akron, Ohio.
Fletcher Davis.
Fletcher Davis of Athens, Texas claimed to have invented the hamburger. According to oral histories, in the 1880s he opened a lunch counter in Athens and served a 'burger' of fried ground beef patties with mustard and Bermuda onion between two slices of bread, with a pickle on the side. The story is that in 1904, Davis and his wife Ciddy ran a sandwich stand at the St. Louis World's Fair. Historian Frank X. Tolbert, noted that Athen's resident Clint Murchison said his grandfather dated the hamburger to the 1880s with 'Old Dave' a.k.a. Fletcher Davis. A photo of "Old Dave's Hamburger Stand" from 1904 was sent to Tolbert as evidence of the claim. Also the "New York Tribune," without giving names, attributed the innovation of the hamburger to the stand on the pike.
Other hamburger-steak claims.
Various non-specific claims of invention relate to the term "hamburger steak" without mention of its being a sandwich. The first printed American menu which listed hamburger is said to be an 1834 menu from Delmonico's in New York. However, the printer of the original menu was not in business in 1834. In 1889, a menu from Walla Walla Union in Washington offered hamburger steak as a menu item.
Between 1871 and 1884, "Hamburg Beefsteak" was on the "Breakfast and Supper Menu" of the Clipper Restaurant at 311/313 Pacific Street in San Fernando, California. It cost 10 cents—the same price as mutton chops, pig's feet in batter, and stewed veal. It was not, however, on the dinner menu. Only "Pig's Head," "Calf Tongue," and "Stewed Kidneys" were listed. Another claim ties the hamburger to Summit County, New York or Ohio. Summit County, Ohio exists, but Summit County, New York does not.
Today.
Hamburgers are usually a feature of fast food restaurants. The hamburgers served in major fast food establishments are usually mass-produced in factories and frozen for delivery to the site. These hamburgers are thin and of uniform thickness, differing from the traditional American hamburger prepared in homes and conventional restaurants, which is thicker and prepared by hand from ground beef. Most American hamburgers are round, but some fast-food chains, such as Wendy's, sell square-cut hamburgers. Hamburgers in fast food restaurants are usually grilled on a flat-top, but some firms, such as Burger King, use a gas flame grilling process. At conventional American restaurants, hamburgers may be ordered "rare", but normally are served medium-well or well-done for food safety reasons. Fast food restaurants do not usually offer this option.
The McDonald's fast-food chain sells the Big Mac, one of the world's top selling hamburgers, with an estimated 550 million sold annually in the United States. Other major fast-food chains, including Burger King (also known as Hungry Jack's in Australia), A&W, Culver's, Whataburger, Carl's Jr./Hardee's chain, Wendy's (known for their square patties), Jack in the Box, Cook Out, Harvey's, Shake Shack, In-N-Out Burger, Five Guys, Fatburger, Vera's, Burgerville, Back Yard Burgers, Lick's Homeburger, Roy Rogers, Smashburger, and Sonic also rely heavily on hamburger sales. Fuddruckers and Red Robin are hamburger chains that specialize in the mid-tier "restaurant-style" variety of hamburgers.
Some North American establishments offer a unique take on the hamburger beyond what is offered in fast food restaurants, using upscale ingredients such as sirloin or other steak along with a variety of different cheeses, toppings, and sauces. One example is the Bobby's Burger Palace chain founded by well-known chef and Food Network star Bobby Flay.
Hamburgers are often served as a fast dinner, picnic or party food and are usually cooked outdoors on barbecue grills.
A high-quality hamburger patty is made entirely of ground (minced) beef and seasonings; these may be described as "all-beef hamburger" or "all-beef patties" to distinguish them from inexpensive hamburgers made with cost-savers like added flour, textured vegetable protein, ammonia treated defatted beef trimmings (which the company Beef Products Inc, calls "lean finely textured beef"), advanced meat recovery, or other fillers. In the 1930s ground liver was sometimes added. Some cooks prepare their patties with binders like eggs or breadcrumbs. Seasonings may include salt and pepper and others like as parsley, onions, soy sauce, Thousand Island dressing, onion soup mix, or Worcestershire sauce. Many name brand seasoned salt products are also used.
Safety.
Raw hamburger may contain harmful bacteria that can produce food-borne illness such as , due to the occasional initial improper preparation of the meat, so caution is needed during handling and cooking. Because of the potential for food-borne illness, the USDA recommends hamburgers be cooked to an internal temperature of 160 F. If cooked to this temperature, they are considered well-done.
Variations.
Burgers can also be made with patties made from ingredients other than beef. For example, a "turkey burger" uses ground turkey meat, a "chicken burger" uses ground chicken meat. A "buffalo burger" uses ground meat from a bison, and an "ostrich burger" is made from ground seasoned ostrich meat. A deer burger uses ground venison from deer.
A "veggie burger", "garden burger", or "tofu burger" uses a meat analogue, a meat substitute such as tofu, TVP, seitan (wheat gluten), quorn, beans, grains or an assortment of vegetables, ground up and mashed into patties.
United States and Canada.
In the United States and Canada, burgers may be classified as two main types: fast food hamburgers and individually prepared burgers made in homes and restaurants. The latter are often prepared with a variety of toppings, including lettuce, tomato, onion, and often sliced pickles (or pickle relish). French fries often accompany the burger. Cheese (usually processed cheese slices but often Cheddar, Swiss, pepper jack, or blue), either melted on the meat patty or crumbled on top, is generally an option.
Condiments might be added to a hamburger or may be offered separately on the side including mustard, mayonnaise, ketchup, salad dressings and barbecue sauce.
Other toppings include bacon, avocado or guacamole, sliced sautéed mushrooms, cheese sauce and/or chili (usually without beans), fried egg, scrambled egg, feta cheese, blue cheese, salsa, pineapple, jalapeños and other kinds of chili peppers, anchovies, slices of ham or bologna, pastrami or teriyaki-seasoned beef, tartar sauce, french fries, onion rings or potato chips.
Mexico.
In Mexico, burgers (called "hamburguesas") are served with ham and slices of American cheese (locally called "queso americano") fried on top of the meat patty. The toppings include avocado, jalapeño slices, shredded lettuce, onion and tomato. The bun has mayonnaise, ketchup and mustard. In certain parts are served with bacon, which can be fried or grilled along with the meat patty. A slice of pineapple is also a usual option, and the variation is known as a "Hawaiian hamburger".
Some restaurants' burgers also have barbecue sauce, and others also replace the ground patty with sirloin, Al pastor meat, barbacoa or a fried chicken breast. Many burger chains from the United States can be found all over Mexico, including Carl's Jr., Sonic, as well as global chains such as McDonald's and Burger King.
United Kingdom and Ireland.
Hamburgers in the UK and Ireland are very similar to those in the US, and the High Street is dominated by the same big two chains as in the U.S. — McDonald's and Burger King. The menus offered to both countries are virtually identical, although portion sizes tend to be smaller in the UK. In Ireland the food outlet Supermacs is widespread throughout the country serving burgers as part of its menu. In Ireland, Abrakebabra (started out selling kebabs) and Eddie Rocket's are also major chains.
An original and indigenous rival to the big two U.S. giants was the quintessentially British fast-food chain Wimpy, originally known as Wimpy Bar (opened 1954 at the Lyon's Corner House in Coventry Street London), which served its hamburgers on a plate with British-style chips, accompanied by cutlery and delivered to the customer's table. In the late 1970s, to compete with McDonald's, Wimpy began to open American-style counter-service restaurants and the brand disappeared from many UK high streets when those restaurants were re-branded as Burger Kings between 1989–90 by the then-owner of both brands, Grand Metropolitan. A management buyout in 1990 split the brands again and now Wimpy table-service restaurants can still be found in many town centers whilst new counter-service Wimpys are now often found at motorway service stations.
Hamburgers are also available from mobile kiosks, particularly at outdoor events such as football matches. Burgers from this type of outlet are usually served without any form of salad — only fried onions and a choice of tomato ketchup, mustard or brown sauce.
Chip shops, particularly in the West Midlands, North-East, Scotland and Ireland, serve battered hamburgers called batter burgers. This is where the burger patty, by itself, is deep-fat-fried in batter and is usually served with chips.
Hamburgers and veggie burgers served with chips and salad, are standard pub grub menu items. Many pubs specialize in "gourmet" burgers. These are usually high quality minced steak patties, topped with items such as blue cheese, brie, avocado et cetera. Some British pubs serve burger patties made from more exotic meats including venison burgers (sometimes nicknamed Bambi Burgers), bison burgers, ostrich burgers and in some Australian themed pubs even kangaroo burgers can be purchased. These burgers are served in a similar way to the traditional hamburger but are sometimes served with a different sauce including redcurrant sauce, mint sauce and plum sauce.
In the early 21st century "premium" hamburger chain and independent restaurants have arisen, selling burgers produced from meat stated to be of high quality and often organic, usually served to eat on the premises rather than to take away. Chains include Gourmet Burger Kitchen, Ultimate Burger, Hamburger Union and Byron Hamburgers in London. Independent restaurants such as Meatmarket and Dirty Burger developed a style of rich, juicy burger in 2012 which is known as a "dirty burger" or "third-wave burger".
In recent years Rustlers has sold pre-cooked hamburgers re-heat able in a microwave oven in the United Kingdom.
In the UK, as in North America and Japan, the term "burger" can refer simply to the patty, be it beef, some other kind of meat, or vegetarian.
Australia and New Zealand.
Fast food franchises sell American style fast food hamburgers in Australia and New Zealand. The traditional Australasian hamburgers are usually bought from fish and chip shops or milk bars. The hamburger meat is almost always ground beef, or "mince" as it is more commonly referred to in Australia and New Zealand. They commonly include tomato, lettuce, grilled onion and meat as minimum, and can optionally include cheese, beetroot, pineapple, a fried egg and bacon. If all these optional ingredients are included it is known in Australia as "burger with the lot".
The only variance between the two countries' hamburgers is that New Zealand's equivalent to the "The Lot" often contains a steak (beef) as well. The condiments regularly used are barbecue sauce and tomato sauce. The McDonald's "McOz" Burger is partway between American and Australian style burgers, having beetroot and tomato in an otherwise typical American burger, however it is no longer a part of the menu. Likewise McDonald's in New Zealand created a Kiwiburger, similar to a Quarter Pounder, but features salad, beetroot and a fried egg. The Hungry Jack's (Burger King) "Aussie Burger" has tomato, lettuce, onion, cheese, bacon, beetroot, egg, ketchup and a meat patty.
China.
In China, restaurants such as McDonald's and KFC have been proliferating all across the country. In many parts of China, small hamburger chains have opened up.
In supermarkets and corner stores, customers can buy unrefrigerated "hamburgers" ("hanbao") off the bread shelf. These are ultra-sweet buns cut open with a thin slice of pork or ham placed inside without any condiments or vegetables. These hanbao are a half-westernised form of the traditional Cantonese buns called "char siu bao" (BBQ Pork Bun). The Chinese word for hamburger (hanbao) often refers to all sandwiches containing hamburger buns and cooked meat, regardless of the meat's origin including chicken burgers.
Japan.
In Japan, hamburgers can be served in a bun, called "hanbāgā" (ハンバーガー), or just the patties served without a bun, known as "hanbāgu" (ハンバーグ) or "hamburg", short for "hamburg steak".
"Hamburg steaks" (served without buns) are similar to what are known as Salisbury steaks in the USA. They are made from minced beef, pork or a blend of the two mixed with minced onions, egg, breadcrumbs and spices. They are served with brown sauce (or demi-glace in restaurants) with vegetable or salad sides, or occasionally in Japanese curries. Hamburgers may be served in casual, western style suburban restaurant chains known in Japan as "family restaurants".
Hamburgers in buns, on the other hand, are predominantly the domain of fast food chains such as American chains known as McDonald's and Wendy's. Japan has home grown hamburger chain restaurants such as MOS Burger, First Kitchen and Freshness Burger. Local varieties of burgers served in Japan include teriyaki burgers, "katsu" burgers (containing tonkatsu) and burgers containing shrimp korokke. Some of the more unusual examples include the rice burger, where the bun is made of rice, and the luxury 1000-yen (US$10) "Takumi Burger" (meaning "artisan taste"), featuring avocados, freshly grated wasabi, and other rare seasonal ingredients. In terms of the actual patty, there are burgers made with Kobe beef, butchered from cows that are fed with beer and massaged daily. McDonald's Japan also recently launched a McPork burger, made with U.S. pork. McDonald's has been gradually losing market share in Japan to these local hamburger chains, due in part to the preference of Japanese diners for fresh ingredients and more refined, "upscale" hamburger offerings. Burger King once retreated from Japan, but re-entered the market in Summer 2007 in cooperation with the Korean owned Japanese fast-food chain Lotteria.
Other countries.
Rice burgers, mentioned above, are also available in several East Asian countries such as Taiwan and South Korea. Lotteria is a big hamburger franchise in Japan owned by the South Korean Lotte group, with outlets also in China, South Korea, Vietnam, and Taiwan. In addition to selling beef hamburgers, they also have hamburgers made from squid, pork, tofu, and shrimp. Variations available in South Korea include Bulgogi burgers and Kimchi burgers.
In the Philippines a wide range of major U.S. fast-food franchises are well represented, together with local imitators, often amended to the local palate. The chain McDonald's (locally nicknamed "McDo") have a range of burger and chicken dishes often accompanied by plain steamed rice and/or French fries. The Philippines boasts its own burger-chain called Jollibee, which offers burger meals and chicken, including a signature burger called "Champ". Jollibee now has a number of outlets in the United States, the Middle East and East Asia.
In India, burgers are usually made from chicken or vegetable patties due to cultural beliefs against eating beef (which stem from Hindu religious practice) and pork (which stems from Islamic religious practice). Because of this, the majority of fast food chains and restaurants in India do not serve beef. McDonald's in India, for instance, do not serve beef, offering the "Maharaja Mac" instead of the Big Mac, substituting the beef patties with chicken. Another version of the Indian vegetarian burger is the Wada Pav consisting deep-fried potato patty dipped in gramflour batter. It is usually served with mint chutney and fried green chili.
In Pakistan, apart from American fast food chains, burgers can be found in stalls near shopping areas, the best known being the "shami burger". This is made from "shami kebab", made by mixing lentil and minced lamb. Onions, scrambled egg and ketchup are the most may be toppings.
In Malaysia there are 300 McDonald's restaurants. The menu in Malaysia also includes eggs and fried chicken on top of the regular burgers. Burgers are also easily found at nearby mobile kiosks, especially Ramly Burger.
In Mongolia, a recent fast food craze due to the sudden influx of foreign influence has led to the prominence of the hamburger. Specialized fast food restaurants serving to Mongolian tastes have sprung up and seen great success.
In Turkey, in addition to the internationally-familiar offerings, numerous localized variants of the hamburger may be found, such as the Islak Burger (lit. "Wet-Burger"), which a beef slider doused in seasoned tomato sauce and steamed inside a special glass chamber, and has its origins in the Turkish fast food retailer Kizilkayalar. Other variations include lamb-burgers and offal-burgers, which are offered by local fast food businesses and global chains alike, such as McDonald's and Burger King. Most burger shops have also adopted a pizzaria-like approach when it comes to home delivery, and almost all major fast food chains deliver.
In the former Yugoslavia, and originally in Serbia, there is a local version of the hamburger known as the "pljeskavica". It is often served as a patty, but may have a bun as well.

</doc>
<doc id="45729" url="http://en.wikipedia.org/wiki?curid=45729" title="Panthera">
Panthera

Panthera is a genus within the Felidae family that was named and first described by the German naturalist Oken in 1816. The British taxonomist Pocock revised the classification of this genus in 1916 as comprising the species tiger, lion, jaguar and leopard on the basis of cranial features. Results of genetic analysis indicate that the snow leopard also belongs to the "Panthera", a classification that was accepted by IUCN assessors in 2008.
Only the tiger, lion, leopard and jaguar have the anatomical structure that enables them to roar. The primary reason for this was formerly assumed to be the incomplete ossification of the hyoid bone. However, new studies show the ability to roar is due to other morphological features, especially of the larynx. The snow leopard does not roar. Although it has an incomplete ossification of the hyoid bone, it lacks the special morphology of the larynx.
Name.
The word "panther" derives from classical Latin "panthēra", itself from the ancient Greek "pánthēr" (πάνθηρ). The Greek "pan-" (πάν), meaning "all", and "thēr" (θήρ), meaning "prey" bears the meaning of "predator of all animals".
Characteristics.
In "Panthera" species the dorsal profile of the skull is flattish or evenly convex. The frontal interorbital area is not noticeably elevated, and the area behind the elevation less steeply sloped. The basicranial axis is nearly horizontal. The inner chamber of the bullae is large, the outer small. The partition between them is close to the external auditory meatus. The convexly rounded chin is sloping.
Evolution.
"Panthera" probably evolved in Asia, but the roots of the genus remain unclear. Genetic studies indicate that pantherine cats diverged from the subfamily Felinae between six and ten million years ago. Fossil records that appear to belong within the "Panthera" genus reach only 2.0 to 3.8 million years back.
The snow leopard was initially seen at the base of "Panthera", but newer molecular studies suggest that it is nestled within "Panthera" and is a sister species of the tiger. Many place the snow leopard within the genus "Panthera", but there is currently no consensus as to whether the snow leopard should retain its own genus "Uncia" or be moved to "Panthera uncia". Since 2008, the IUCN Red List lists it as "Panthera uncia" using "Uncia uncia" as a synonym.
The genus "Neofelis" is generally placed at the base of the "Panthera" group, but is not included in the genus itself.
Results of a mitogenomic study suggest the phylogeny can be represented as "Neofelis nebulosa" ("Panthera tigris" ("Panthera onca" ("Panthera pardus", ("Panthera leo", "Panthera uncia")))). About million years ago "Panthera" separated from other felid species and then evolved into the several species of the genus. "N. nebulosa" appears to have diverged about million years ago, "P. tigris" about million years ago, "P. uncia" about million years ago and "P. pardus" about million years ago. Mitochondrial sequence data from fossils suggest that American lions ("P. atrox") are a sister lineage to Eurasian cave lions ("P. l. spelaea"), diverging about million years ago.
The prehistoric cat "Panthera onca gombaszogensis", often called European jaguar is probably closely related to the modern jaguar. The earliest evidence of the species was obtained at Olivola in Italy, and dates 1.6 million years.
Classification.
During the 19th and 20th centuries, various explorers and staff of natural history museums suggested numerous subspecies, or at times called races, for all "Panthera" species. The taxonomist Pocock reviewed skins and skulls in the zoological collection of the Natural History Museum, London and grouped subspecies described, thus shortening the lists considerably.
Since the mid 1980s, several "Panthera" species became subject of genetic research, mostly using blood samples of captive individuals. Study results indicate that many of the lion and leopard subspecies are questionable because of insufficient genetic distinction between them.
Subsequently, it was proposed to group all African leopard populations to "P. p. pardus" and retain eight subspecific names for Asian leopard populations.
Based on genetic research, it was suggested, to group all living sub-Saharan lion populations into "P. l. leo". More recent genetic research, however, indicates that the Western and Central African lions form a different clade of lions and are perhaps more related to Asian lions than to lions from southern or eastern Africa. These populations have been largely ignored in previous studies. The black panther is not a distinct species, but is the common name for melanistic specimens of the genus, most often encountered in leopard and jaguar.
Phylogeny.
The cladogram below follows Mazák, Christiansen and Kitchener (2011).
Species.
The genus "Panthera" comprises:
Taxonomic placing is uncertain for the extinct fossil "Panthera" species:

</doc>
<doc id="45732" url="http://en.wikipedia.org/wiki?curid=45732" title="Hieronymus Bosch">
Hieronymus Bosch

Hieronymus Bosch (; ]; born Jheronimus van Aken ]; 1450 – 9 August 1516) was an Early Netherlandish painter. His work is known for its fantastic imagery, detailed landscapes and illustrations of moral and religious concepts and narratives. Within his lifetime his work was collected in the Netherlands, Austria, and Spain, and widely copied, especially his macabre and nightmarish depictions of hell. 
Little is known of Bosch's life, though there are some records. He spent most of it in the town of 's-Hertogenbosch, though his roots are from Aachen, Germany. His pessimistic and fantastical style cast a wide influence on northern art of the 16th century, with Pieter Bruegel the Elder his best known follower. His paintings have been difficult to translate from a modern point of view; attempts to associate instances of modern sexual imagery with fringe sects or the occult have largely failed. Today he is seen as a hugely individualistic painter with deep insight into man's desires and deepest fears. Attribution has been especially difficult; today only 35 to 40 paintings are confidently given to his hand. His most acclaimed works consist of a few triptych altarpieces, the most outstanding of which is the Garden of Earthly Delights. His best surviving panels make innovative use of oil paint and especially glazed finish.
Life.
Hieronymus Bosch was born Jheronimus (or Joen, respectively the Latin and Middle Dutch form of the name "Jerome") van Aken (meaning "from Aachen"). He signed a number of his paintings as "Jheronimus Bosch" (pronounced "Yeronimus Bos"[] in Middle Dutch). The name derives from his birthplace, 's-Hertogenbosch, which is commonly called "Den Bosch" ('the forest').
Little is known of Bosch’s life or training. He left behind no letters or diaries, and what has been identified has been taken from brief references to him in the municipal records of 's-Hertogenbosch, and in the account books of the local order of the Illustrious Brotherhood of Our Blessed Lady. Nothing is known of his personality or his thoughts on the meaning of his art. Bosch’s date of birth has not been determined with certainty. It is estimated at 1450 on the basis of a hand drawn portrait (which may be a self-portrait) made shortly before his death in 1516. The drawing shows the artist at an advanced age, probably in his late sixties.
Bosch was born and lived all his life in and near ‘s-Hertogenbosch, a city in the Duchy of Brabant. His grandfather, Jan van Aken (died 1454), was a painter and is first mentioned in the records in 1430. It is known that Jan had five sons, four of whom were also painters. Bosch’s father, Anthonius van Aken (died 1478), acted as artistic adviser to the Illustrious Brotherhood of Our Blessed Lady. It is generally assumed that either Bosch’s father or one of his uncles taught the artist to paint, but none of their works survive. Bosch first appears in the municipal record on 5 April 1474, when he is named along with two brothers and a sister.
's-Hertogenbosch was a flourishing city in 15th-century Brabant, in the south of the present-day Netherlands, at the time part of the Burgundian Netherlands, and during its lifetime passing through marriage to the Habsburgs. In 1463, 4,000 houses in the town were destroyed by a catastrophic fire, which the then (approximately) 13-year-old Bosch presumably witnessed. He became a popular painter in his lifetime and often received commissions from abroad. In 1488 he joined the highly respected Brotherhood of Our Lady, an arch-conservative religious group of some 40 influential citizens of 's-Hertogenbosch, and 7,000 'outer-members' from around Europe. 
Sometime between 1479 and 1481, Bosch married Aleyt Goyaerts van den Meerveen, who was a few years his senior. The couple moved to the nearby town of Oirschot, where his wife had inherited a house and land from her wealthy family. An entry in the accounts of the Brotherhood of Our Lady records Bosch’s death in 1516. A funeral mass served in his memory was held in the church of Saint John on 9 August of that year.
Works.
Bosch produced at least sixteen triptychs, of which eight are fully intact, and another five in fragments. His most famous triptych is the "The Garden of Earthly Delights" whose inner panels are intended to be read chronologically from left to right. In the left hand panel God presents Eve to Adam; innovatively He is given a youthful appearance. The figures are set in a landscape populated by exotic animals and unusual semi-organic hut-shaped forms. The central panel is a broad panorama teeming with socially engaged nude figures seemingly engaged in innocent, self-absorbed joy, as well as fantastical animals, oversized fruit and hybrid stone formations. 
The right panel presents a hellscape; a world in which humankind has succumbed to the temptations of evil and are reaping eternal damnation. Set at night, the panel features cold colours, tortured figures and frozen waterways. The nakedness of the human figures has lost any eroticism suggested in the central panel, as large explosions in the background throw light through the city gate and spill onto the water in the panel's midground.
Bosch painted in a comparatively sketchy manner, contrasting with the traditional Flemish style of painting in which the smooth surface—achieved by the application of multiple transparent glazes—conceals the brushwork. 
Bosch's paintings with their rough surfaces, so called "impasto" painting, differed from the tradition of the great Netherlandish painters of the end of the 15th, and beginning of the 16th centuries, who wished to hide the work done and so suggest their paintings as more nearly divine creations.
Bosch did not date his paintings. But—unusual for the time—he seems to have signed several of them, although some signatures purporting to be his are certainly not. Fewer than 25 paintings remain today that can be attributed to him. In the late sixteenth-century, Philip II of Spain acquired many of Bosch's paintings, including some probably commissioned and collected by Spaniards active in Bosch's hometown; as a result, the Prado Museum in Madrid now owns "The Adoration of the Magi", "The Garden of Earthly Delights", the tabletop painting of "The Seven Deadly Sins and the Four Last Things", the "The Haywain Triptych" and "The Stone Operation".
Interpretation.
In the 20th century, when changing artistic tastes made artists like Bosch more palatable to the European imagination, it was sometimes argued that Bosch’s art was inspired by heretical points of view (e.g., the ideas of the Cathars and putative Adamites) as well as by obscure hermetic practices. Again, since Erasmus had been educated at one of the houses of the Brethren of the Common Life in 's-Hertogenbosch, and the town was religiously progressive, some writers have found it unsurprising that strong parallels exist between the caustic writing of Erasmus and the often bold painting of Bosch. "Although the Brethren remained loyal to the Pope, they still saw it as their duty to denounce the abuses and scandalous behaviour of many priests: the corruption which both Erasmus and Bosch satirised in their work".
Others, following a strain of Bosch-interpretation datable already to the 16th century, continued to think his work was created merely to titillate and amuse, much like the "grotteschi" of the Italian Renaissance. While the art of the older masters was based in the physical world of everyday experience, Bosch confronts his viewer with, in the words of the art historian Walter Gibson, "a world of dreams [and] nightmares in which forms seem to flicker and change before our eyes". In one of the first known accounts of Bosch’s paintings, in 1560 the Spaniard Felipe de Guevara wrote that Bosch was regarded merely as "the inventor of monsters and chimeras". In the early seventeenth century, the artist biographer Karel van Mander described Bosch’s work as comprising "wondrous and strange fantasies"; however, he concluded that the paintings are "often less pleasant than gruesome to look at".
In recent decades, scholars have come to view Bosch's vision as less fantastic, and accepted that his art reflects the orthodox religious belief systems of his age. His depictions of sinful humanity and his conceptions of Heaven and Hell are now seen as consistent with those of late medieval didactic literature and sermons. Most writers attach a more profound significance to his paintings than had previously been supposed, and attempt to interpret it in terms of a late medieval morality. It is generally accepted that Bosch’s art was created to teach specific moral and spiritual truths in the manner of other Northern Renaissance figures, such as the poet Robert Henryson, and that the images rendered have precise and premeditated significance. According to Dirk Bax, Bosch's paintings often represent visual translations of verbal metaphors and puns drawn from both biblical and folkloric sources. However, the conflict of interpretations that his works still elicit raises profound questions about the nature of "ambiguity" in art of his period.
In recent years, art historians have added a further dimension again to the subject of ambiguity in Bosch’s work. They emphasized his ironic tendencies, which are fairly obvious, for example, in the "The Garden of Earthly Delights", both in the central panel (delights), and the right panel (hell). By adding irony to his morality arenas, Bosch offers the option of detachment, both from the real world and from the painted fantasy world. By doing so he could gain acceptance among both conservative and progressive viewers. Perhaps it was just this ambiguity that enabled the survival of a considerable part of this provocative work through five centuries of religious and political upheaval.
A recent study on Bosch's paintings alleges that they actually conceal a strong nationalist consciousness, censuring the foreign imperial government of the Burgundian Netherlands, especially Maximilian Habsburg. By systematically superimposing images and concepts, the study asserts that Bosch also made his expiatory self-punishment, for he was accepting well-paid commissions from the Habsburgs and their deputies, and therefore betraying the memory of Charles the Bold.
Bosch's work "Ascent of the Blessed" resembles imagery typical with a Near-death experience (NDE), in particular angels escorting people down a tunnel of white light. One NDE researcher in particular had a subject write to him specifically referencing the painting, saying that it very much resembled what he saw during his NDE experience. It is unknown if Bosch himself experienced an NDE or had someone relate to him such an experience.
Debates on attribution.
The exact number of Bosch's surviving works has been a subject of considerable debate. He signed only seven of his paintings, and there is uncertainty whether all the paintings once ascribed to him were actually from his hand. It is known that from the early 16th century onwards numerous copies and variations of his paintings began to circulate. In addition, his style was highly influential, and was widely imitated by his numerous followers.
Over the years, scholars have attributed to him fewer and fewer of the works once thought to be his, and today only 25 are definitively attributed to him.
References.
</dl>

</doc>
<doc id="45738" url="http://en.wikipedia.org/wiki?curid=45738" title="Visual DialogScript">
Visual DialogScript

Visual DialogScript (VDS) is an interpreted programming language for Microsoft Windows. It can be used to create small, fast programs. VDS has a large number of dialog and graphical elements available to create professional looking programs. VDS programs have access to the Windows API; therefore, it is possible to write applications that can perform the same advanced tasks as other programming languages such as Visual Basic, C++, or Delphi.
Language.
Unlike other programming languages, the syntax of VDS is very simple. Each command occupies one line, and has a plain English name that clearly describes its purpose. Variables are typeless, and can hold many kinds of information, for example, numbers or text. Functions are clearly distinguishable with names that start with '@', just like a spreadsheet.
The DialogScript language has a simple syntax not unlike MS-DOS batch language. It is designed for ease of use and efficiency when being interpreted by the run-time engine. There are 10 system variables, %0 to %9, which initially have the script file name in %0 and command line parameters in %1 through %9, just as in a batch file. There are also a further 26 user variables, %A to %Z. The contents of all variables (including system ones) can be changed once the script is running. There are now also 4032 global variables. These variables begin with %%, a letter, then alphanumerics plus underscores (e.g. %%my_variable_1.) There is no limit on the length of these user-defined variable names.
Syntax Examples.
Comments:
Simple Information Message Box:
Simple Warning Message Box:
Create a custom dialog box:
Write to the Windows Registry:
Display an input prompt dialog box, storing the result in the variable %A:
History.
Visual DialogScript was originally created by Julian Moss of JM-Tech. Eventually, S.A.D.E. s.a.r.l., a French company, took over ownership and development of VDS, altering and improving upon its syntax. Currently, VDS is owned and developed by the British company Commercial Research Ltd.
Several versions of VDS have been released over time:
Currently Available Versions.
There are several versions available for download:

</doc>
<doc id="45742" url="http://en.wikipedia.org/wiki?curid=45742" title="Mazara del Vallo">
Mazara del Vallo

Mazara del Vallo is a town and "comune" in southwestern Sicily, Italy, which lies mainly on the left bank at the mouth of the Mazaro river, administratively part of the province of Trapani.
It is an agricultural and fishing centre and its port gives shelter to the largest fishing fleet in Italy.
History.
Ancient town.
Mazara was founded by the Phoenicians in the 9th century BC, with the name of "Mazar" (the "Rock"). It then passed under the control of Greeks, Carthaginians, Romans, Vandals, Ostrogoths, Byzantines, before being occupied by the Arabs in the year 827 AD. During the Arab period, Sicily was divided into three different administrative regions, "Val di Noto", "Val Demone" and "Val di Mazara", making the city an important commercial harbour and centre of learning. The city centre, known as the "Kasbah", retains Arab architectural influences.
In 1072, Mazara was conquered by Normans, headed by Roger I. During that period - in 1093, the Roman Catholic Diocese of Mazara del Vallo was instituted.
After the death of Emperor Frederick II, Sicily passed to the Angevins, then followed by the Spaniards of Aragon. The Aragon period (1282–1409) is characterized by a political, economic and demographic decline of Mazara. The city passed under the control of the House of Savoy in 1713, a reign which lasted only five years, being replaced by the Habsburg Empire (for 16 years) followed by the Bourbons. In 1860 the city was finally conquered by Giuseppe Garibaldi and the Mille, thus joining the then newly formed Kingdom of Italy.
The city was known as "Mazzara del Vallo" until the World War II period, following which the spelling was changed to "Mazara del Vallo".
Today.
Today Mazara is widely considered to be one of the most important fishing centres of Italy; tussles about fishing rights, especially with the North-African countries, figure large in the town's recent history, boat sequestrations being a common event. Currently the fishing business in the city seems to be withering, mainly because of the increasing lack of people willing to work on boats.
Mazara del Vallo is among the Italian cities with the highest percentages of immigrants; it is estimated that the city hosts at least 3,500 registered immigrants, mainly from nearby Tunisia but also the other countries of the Maghreb. They tend to live principally around the old Arab city centre (the "Casbah"). There exists a local school, managed by the Tunisian government, at which only Arabic and French are taught as languages. This has led to some controversy. Most of the local schools show openness to Arab culture, even providing Arabic language classes for both Italians and Arabs, and encouraging integration with the autochthonous students. The local city council also provides a seat reserved for a representative of Mazara's immigrant community.
Main sights.
Mazara made national news in March 1998, when a bronze statue called the "Dancing Satyr" ("Satiro Danzante") was found off the port, at a depth of 500 m in the Strait of Sicily by a local fishing boat. The statue is believed to have been sculpted by Greek artist Praxiteles and is now on display to the public in a dedicated museum in the city, after having been on show at the Chamber of Deputies of Rome, and in Aichi, Japan. After this event, the city quickly gained in terms of visiting tourists and a national advertising campaign was mounted with the slogan "Mazara del Satiro".
Other attractions include the "Norman Arc", that is the remains of the old Norman Castle built in 1073 and demolished in 1880, and a number of churches, including the Royal Saint Nicholas ("San Nicolò Regale") Church, a rare example of Norman architecture built in 1124, the Seminary, built in 1710, which surrounds the main local piazza, "Piazza della Repubblica", and St. Vitus on the Sea ("San Vito a Mare") Church. In honour of St. Vitus, the official patronal saint as well as a native of Mazara del Vallo, the St. Vitus Feast ("Lu Fistinu di Santu Vitu") is held every year.
Transportation.
Mazara del Vallo is connected to the rest of Sicily by a regional train service (run by Trenitalia), a private bus service (only to Palermo), and by car, via the A29 highway (also known as "Palermo-Mazara del Vallo"). It's reachable from the nearby airports of Trapani and Palermo only by car, or by taxi (€20 per person for Trapani).
During the summer period, Mazara is also connected via ferry to the island of Pantelleria and Hammamet, in Tunisia.
Foreign residents.
 - Demographic Statistics
External links.
<br>

</doc>
<doc id="45743" url="http://en.wikipedia.org/wiki?curid=45743" title="Asti">
Asti

Asti    is a city and comune of about 75,000 inhabitants located in the Piedmont region of northwestern Italy, about 55 km east of Turin in the plain of the Tanaro River. It is the capital of the province of Asti and it is deemed to be the modern capital of Monferrato ("Montferrat" in English).
History.
Ancient times and early Middle Ages.
People have lived in and around what is now Asti since the Neolithic period. Before their defeat in 174 BC by the Romans, tribes of Ligures, the Statielli, dominated the area and the toponym probably derives from "Ast" which means "hill" in the ancient Celtic language.
In 124 BC the Romans built a "castrum", or fortified camp, which eventually evolved into a full city named Hasta. In
89 BC the city received the status of "colonia", and in 49 BC that of "municipium". Asti become an important city of the Augustan Regio IX, favoured by its strategic position on the Tanaro river and on the Via Fulvia, which linked Derthona (Tortona) to Augusta Taurinorum (Turin). Other roads connected the city to the main passes for what are today Switzerland and France.
The city was crucial during the early stages of the barbarian invasions which stormed Italy during the fall of the Western Roman Empire. In early 402 AD the Visigoths had invaded northern Italy and were advancing on Mediolanum (modern Milan) which was the imperial capital at that time. Honorius, the young emperor and a resident in that city, unable to wait for promised reinforcements any longer, was compelled to flee from Milan for safety in the city of Arles in Gaul. However, just after his convoy had left Milan and crossed the River Po his escape route through the Alps was cut off by the Gothic cavalry. This forced him to take emergency refuge in the city of Hasta until more Roman troops could be assembled in Italy. The Goths placed Hasta under siege until March when General Stilicho, bringing reinforcements from the Rhine, fought and defeated them at the Battle of Pollentia. After this first victorious defence, thanks to a massive line of walls, Hasta suffered from the barbarian invasions which stormed Italy after the fall of the Western Empire, and declined economically.
In the second half of the 6th century it was chosen as seat for one of the 36 Duchies in which the Lombards divided Italy. The territory of Asti comprised a wide area, stretching out to Albenga and the Maritime Alps. This remained when northern Italy was conquered by the Franks in 774, with the title of County.
In the late Carolingian age Asti was ruled directly by his bishops, who were the main landlords of the area. Most important are Audax (904-926) and Bruningus (937-966), who moved the episcopal seat to the Castel Vecchio ("Old Castle"), where it remained until 1409. The bishopric of Asti remained a powerful entity well into the 11th century, when Pietro II received huge privileges by emperor Henry II. In the second half of the century, Bishop Otto tried to resist the aims of the powerful countess Adelaide of Susa, who damaged the city several times. During Otto's reign, a commune and the consul magistrates are mentioned for the first time (1095) and make this City-State the first republic of Europe.
Local power.
Asti was one of the first free communes of Italy, and in 1140 received the right to mint coins of its own by Conrad II. As the commune, however, had begun to erode the lands of the bishop and other local faudataries, the latter sued for help to Frederick Barbarossa, who presented under the city walls with a huge army in February of 1155. After a short siege, Asti was stormed and burnt. Subsequently Asti adhered to the Lombard League (1169) against the German emperor, but was again defeated in 1174. Despite this, after the Peace of Constance (1183), the city gained further privileges.
The 13th century saw the peak of the Astigiani economic and cultural splendour, only momentarily hindered by wars against Alba, Alessandria, Savoy, Milan (which besieged the city in 1230) and the Marquesses of Montferrat and Saluzzo. In particular, the commune aimed to gain control over the lucrative trade routes leading northwards from the Ligurian ports. In this period, the rise of the Casane Astigiane resulted in contrasting political familial alliances of Guelph and Ghibelline supporters. During the wars led by Emperor Frederick II in northern Italy, the city chose his side: Asti was defeated by the Guelphs of Alessandria at Quattordio and Clamandrana, but thanks to Genoese help, it recovered easily. After Frederick's death, the struggle against Thomas II of Savoy became fierce: the Astigiani defeated him on February 23, 1255, at the Battle of Montebruno, but Thomas (who had been taken prisoner) replied ordering all traders from Asti to be arrested in Savoy and France. This move showed worry on the part of Asti's neighbouring states over the excessive power gained by the city, which had captured Alba and controlled both Chieri and Turin.
This state of affairs led to the intervention of Charles I of Anjou, then King of Naples and the most powerful man in Italy. After some guerrilla actions, Asti signed a pact of alliance with Pavia, Genoa and William VII of Montferrat. In 1274 the Astigiani troops were defeated at the Battle of Cassano, but, on December 12, 1275, were victorious over the Angevins at the Battle of Roccavione, ending Charles' attempt to expand in Piedmont. In the 1290s, after William VII had also been defeated, Asti was the most powerful city in Piedmont. However, internal struggles for the control of trading and banking enterprises soon divided the city into factions. The most prominent faction were the powerful bankers of the Solari family, who, in 1314, gave the city to king Robert of Naples. The free Republic of Asti ceased to exist. In 1339 the Ghibelline exiles recaptured the city, expelling the Solari and their allies. In 1342 however, the menace of the Solari counteroffensive led the new rulers to submit to Luchino Visconti of Milan. Visconti built a citadel and a second ring of walls to protect the new burgs of the city. In 1345, at the Battle of Gamenario, the Ghibelline Astigiani and John II of Montferrat again defeated the Neapolitan troops. John ruled over Asti until 1372, but seven years later the city council submitted to Galeazzo II Visconti's authority. Galeazzo in turn assigned it to Louis of Valois, Duke of Orléans.
French and Savoyard domination.
With the exception of several brief periods under Visconti, Montferrat and Sforza rule, Asti remained under Valois control; it eventually became a direct subject of the French Crown. The situation changed in the early 16th century, during the wars between Charles V and Francis I of France. In 1526 it was besieged in vain by Charles' condottiero Fabrizio Maramaldo. Three years later, the Treaty of Cambrai assigned Asti to the German emperor, who in turn gave it to the viceroy of Naples Charles de Lannoy. After the death of the latter, Charles included it in Beatrice of Portugal's dowry: when she married Charles III of Savoy. Asti became part of the dominion of Savoy in 1575.
Asti was one of the main Savoyard strongholds in later wars. In 1616, besieged by the Spanish governor of Milan, it was defended by Duke Charles Emmanuel I himself. In 1630–1631, the city suffered a high mortality rate from an outbreak of the plague. Some years later Asti was conquered by the Spanish, although Savoy regained the city in 1643. Another unsuccessful Spanish siege occurred in 1650. In November 1703, during the War of Spanish Succession, Asti fell to France again; it was reconquered in 1705 by Victor Amadeus II. In 1745 French troops invaded the city once more, but it was liberated the following year.
In 1797 the Astigiani, enraged by the continuous military campaigns and by their resulting poor economic situation, revolted against the Savoyard government. On July 28 the Repubblica Astese was declared. However, it was suppressed only two days later. The revolutionary chiefs were arrested and executed. The following year the Savoyards were expelled from Piedmont by the French revolutionary army, and Asti was occupied by general Montrichard. After a short reversal, the French returned after the victory at Marengo (1800) near to Alessandra. Napoleon himself visited Asti on April 29, 1805, but was received rather coldly by the citizens. The city was demoted and incorporated with Alessandra under the department of Marengo. After the end of the French empire, Asti returned to Piedmont in 1814; the city followed Piedmontese history until the unification of Italy in 1861.
Climate.
Asti has a continental climate which is moderated by the proximity of the Mediterranean sea: its winters are warmer, and its summers cooler than Turin. Rain falls mostly during the spring and autumn; during the hottest months rain is less common, but stronger when it does occur, usually in thunderstorms. During November and December in particular, the town of Asti can be prone to fog, which is less common in the higher-altitude areas that surround it.
Main sights.
Some sections of the ancient city walls remain on the North side of the city and in the late 20th-century building work uncovered a section of Roman wall in the center of the city.
The area to the NW of the city, between the centre and the Cathedral, is very rich in medieval palaces and merchants houses, many with monumental towers. Asti was known as the city of 100 towers (although there were 120 in total) of which several still remain today within the old city walls. The most known are the Tower of the Comentini (13th century), the octagonal "Torre de Regibus" and "Torre Troyana" (13th century), as well as the ancient "Rossa di San Secondo", built during the reign of the Roman Emperor Augustus. Location 44°53'53.67"N 8°11'45.08"E
Asti is the home to several old churches. These include:
There is a Synagogue and a museum depicting the history of Asti's Jewish community whose presence is documented since 812.
Events.
One of the most famous events held in Asti is the famous Palio di Asti, in which all the old town wards, called "Rioni" and "Borghi" plus nearby towns compete in a bare-back horse race. This event recalls a victory in battle versus the rival city Alba, during the Middle Ages after the victorious battle a race was held around Alba's walls, from then on every year in Asti. Asti's Palio is the oldest recorded one in Italy, and in modern times is held in the triangular Piazza Alfieri preceded by a medieval pageant through the old town on the 3rd Sunday of September.
Wine.
The three neighbouring Provinces of Asti, Cuneo, and Alessandria incorporate the Langhe and Monferrato hill region in the centre of Piedmont, limestone and sandstone deposits laid down by the retreating Adriatic some 5 million years ago, and are home to some of Italy's finest red wines, plus some famous whites. Asti city is in the centre of this area and is the major city of this notable wine district.
Part of Monferrato lies in the Province of Asti, and is an important area for the production of fine wines. Perhaps the wine most famously associated with Asti worldwide is the sparkling Asti (DOCG). The name today is usually shortened to "Asti" in order to avoid associations with the many wines of dubious quality which are labelled as "Spumante". Asti is typically sweet and low in alcohol (often below 8%). It is made solely from the "moscato bianco" white muscat grape. A premium version known as Moscato d'Asti (DOCG) is seen outside Italy. Besides Asti Spumante being the most known wine abroad, the most renowned wine made in Asti and Monferrato is the red wine called Barbera.
While Asti province became famous around the world thanks to Martini and Rossi, Gancia and Riccadonna which made commercial wines like Asti Spumante, it is now also becoming famous internationally for its classic red wines such as Barbera d'Asti, Freisa d'Asti, Grignolino d'Asti, Bonarda and Ruché di Castagnole Monferrato. These wines and many others can be sampled during the week-long Douja d'Or wine exhibition which is held at the same time as the Palio and Sagre.
Food.
Asti is also famous for its Asti's Festival of Festivals, held in September a week before the Palio. During the festival most of the towns in Asti's province meet in a great square called "Campo del Palio", here they offer typical food and wine for which they are known. On the Sunday of the Sagre all the towns involved stage a parade with floats depicting traditional farming with everyone in costume along Asti's roads to reach "Campo del Palio" square.
Asti province becomes a gourmands delight from October to December in the white truffle or "tartufo bianco" season. Although neighbouring Alba is better known for its October truffle fair, some of the best truffles are found around Asti's hills, and every weekend there is a local truffle festival.
Transport.
Asti railway station, opened in 1849, forms part of the Turin–Genoa and Castagnole–Asti–Mortara railways. It is also a junction for two other lines, to Genoa and Chivasso, respectively.
Notable people.
People from Asti includes:
Frazioni.
Bramairate, Bricco Fassio, Bricco Roasio, Ca' dei Coppi, Caniglie, Carretti, Casabianca, Castiglione, Madonna di Viatosto, Mombarone, Mongardino Stazione, Montemarzo, Migliandolo, Poggio d'Asti, Portacomaro Stazione, Quarto, Quarto superiore, Revignano, Rioscone, San Grato di Sessant, San Marzanotto, San Marzanotto Piana, San Vito - Poggio, Santo Spirito, Serravalle, Sessant, Torrazzo, Trincere, Vaglierano Alto, Vaglierano Basso, Valenzani, Valfea, Valgera, Valle Tanaro, Valle Andona, Valmaggiore, Valmairone, Valmanera, Variglie.
International relations.
Asti is twinned with:
 Valence, France, since 1966

</doc>
<doc id="45744" url="http://en.wikipedia.org/wiki?curid=45744" title="Villanova d'Asti">
Villanova d'Asti

Villanova d'Asti is a town and "comune" in the province of Asti, Piedmont, northern Italy. It has around 5,000 inhabitants. The economy is based on a mixture of agriculture and industry.
Villanova d'Asti was founded in the Middle Ages.
The main sight is the sanctuary of the "Beata Vergine delle Grazie"
Twin towns — Sister cities.
Villanova d'Asti is twinned with:
References.
<br>

</doc>
<doc id="45745" url="http://en.wikipedia.org/wiki?curid=45745" title="Dacia">
Dacia

In ancient geography, especially in Roman sources, Dacia () was the land inhabited by the Dacians. The Greeks referred to them as the Getae, which were specifically a branch of the Thracians north of the Haemus range.
Dacia was bounded in the south approximately by the Danubius river (Danube), in Greek sources the "Istros", or at its greatest extent, by the Haemus Mons (the Balkan Mountains). Moesia (Dobrogea), a region south of the Danube, was a core area where the Getae lived and interacted with the Ancient Greeks. In the east it was bounded by the Pontus Euxinus (Black Sea) and the river "Danastris" (Dniester), in Greek sources the "Tyras". But several Dacian settlements are recorded between the rivers Dniester and "Hypanis" (Southern Bug), and the Tisia (Tisza) to the west.
At times Dacia included areas between the Tisa and the Middle Danube. The Carpathian Mountains were located in the middle of Dacia. It thus corresponds to the present day countries of Romania and Moldova, as well as smaller parts of Bulgaria, Serbia, Hungary, and Ukraine.
Dacians (or Getae) were North Thracian tribes. Dacian tribes had both peaceful and military encounters with other neighboring tribes, such as Sarmatians, Scythians, and Celts.
A Dacian Kingdom of variable size existed between 82 BC until the Roman conquest in AD 106. The capital of Dacia, Sarmizegetusa, located in modern Romania, was destroyed by the Romans, but its name was added to that of the new city ("Ulpia Traiana Sarmizegetusa") built by the latter to serve as the capital of the Roman province of Dacia.
Nomenclature.
Classical era.
The Dacians are first mentioned in the writings of the Ancient Greeks, in Herodotus ("Histories" Book IV XCIII: "[Getae] the noblest as well as the most just of all the Thracian tribes") and Thucydides ("Peloponnesian Wars", Book II: "[Getae] border on the Scythians and are armed in the same manner, being all mounted archers").
Geography.
The extent and location of Dacia varied in its three distinct historical periods (see "History", below):
Periods.
1st century BC.
The Dacia of King Burebista (82–44 BC), stretched from the Black Sea to the river Tisa and from the Balkan Mountains to Bohemia. During that period, the Geto-Dacians conquered a wider territory and Dacia extended from the Middle Danube to the Black Sea littoral (between Apollonia and Olbia) and from present-day Slovakia's mountains to the Balkan mountains. In 53 BC, Julius Caesar stated that the lands of the Dacians started on the eastern edge of the Hercynian Forest (Black Forest). After Burebista's death, his kingdom split in four states, later five.
Around 20 AD, Strabo wrote "Geographica", which delineates the regions inhabited by Dacians at that time. On its basis, Lengyel and Radan (1980), Hoddinott (1981) and Mountain (1998) consider that the Geto-Dacians inhabited both sides of the Tisza river prior to the rise of the Celtic Boii, and again after the latter were defeated by the Dacians. The hold of the Dacians between the Danube and Tisza was tenuous. However, the archaeologist Parducz argued a Dacian presence west of the Tisa dating from the time of Burebista. According to Tacitus (AD 56 – AD 117) Dacians bordered Germania in the south-east, while Sarmatians bordered it in the east.
In the 1st century AD, the Iazyges settled West of Dacia, on the plain between the Danube and the Tisa rivers, according to the scholars' interpretation of Pliny's text: “The higher parts between the Danube and the Hercynian Forest (Black Forest) as far as the winter quarters of Pannonia at Carnutum and the plains and level country of the German frontiers there are occupied by the Sarmatian Iazyges, while the Dacians whom they have driven out hold the mountains and forests as far as the river Theiss”.
1st century AD.
Strabo, in his Geography written between 20 BC – AD 23, says:
″As for the southern part of Germany beyond the Albis, the portion which is just contiguous to that river is occupied by the Suevi; then immediately adjoining this is the land of the Getae, which, though narrow at first, stretching as it does along the Ister on its southern side and on the opposite side along the mountain-side of the Hercynian Forest (for the land of the Getae also embraces a part of the mountains), afterwards broadens out towards the north as far as the Tyregetae; but I cannot tell the precise boundaries″
Towards the west Dacia may originally have extended as far as the Danube, where it runs from north to south at Vác. In the 1st century BC, at the time of the Dacian Kingdom of Burebista, Julius Caesar in his "De Bello Gallico" (book 6) speaks of the Hercynian forest extending along the Danube to the territory of the Dacians.
2nd century AD.
Written a few decades after the Roman conquest of Dacia in AD 105–106, Ptolemy's "Geographia" included the boundaries of Dacia. According to the scholars' interpretation of Ptolemy (Hrushevskyi 1997, Bunbury 1879, Mocsy 1974, Barbulescu and Nagler 2005) Dacia was the region between the rivers Tisza, Danube, upper Dniester, and Siret. Mainstream historians accept this interpretation: Avery (1972) Berenger (1994) Fol (1996) Mountain (1998), Waldman Mason (2006).
Ptolemy also provided a couple of Dacian toponyms in south Poland in the Upper Vistula (Polish: Wisla) river basin: Susudava and Setidava (with a manuscript variant Getidava). This could have been an “echo” of Burebista’s expansion. It seems that this northern expansion of the Dacian language, as far as the Vistula river, lasted until AD 170–180 when the migration of the Vandal Hasdingi pushed out this northern Dacian group. This Dacian group, possibly the Costoboci/Lipiţa culture, is associated by Gudmund Schütte with towns having the specific Dacian language ending "dava" i.e. Setidava.
The Roman province "Dacia Traiana", established by the victors of the Dacian Wars during AD 101–106, initially comprised only the regions known today as Banat, Oltenia, Transylvania, and was subsequently gradually extended to parts of Moldavia, while Dobruja and Budjak belonged the Roman province of Moesia.
In the 2nd century AD, after the Roman conquest, Ptolemy puts the eastern boundary of "Dacia Traiana" (the Roman province) as far east as the "Hierasus" (Siret) river, in modern Romania. Roman rule extended to include the south-western area of the Dacian Kingdom, but not to what later became known as Maramureş), to parts of the later Principality of Moldavia east of the Siret and north of the Upper Trajan Wall, and to areas in modern Muntenia and Ukraine, except the Black Sea shore.
After the Marcomannic Wars (AD 166–180), Dacian groups from outside Roman Dacia had been set in motion. So were the 12,000 Dacians 'from the neighbourhood of Roman Dacia sent away from their own country'. Their native country could have been the Upper Tisa region, but other places cannot be excluded.
The later Roman province "Dacia Aureliana", was organized inside former Moesia Superior after the retreat of the Roman army from Dacia, during the reign of emperor Aurelian during AD 271–275. It was reorganised as Dacia Ripensis (as a military province) and Dacia Mediterranea (as a civil province).
Cities.
Ptolemy gives a list of 43 names of towns in Dacia, out of which arguably 33 were of Dacian origin. Most of the latter included the added suffix ‘dava’ (meaning settlement, village). But, other Dacian names from his list lack the suffix (e.g. Zarmisegethusa regia = Zermizirga) In addition, nine other names of Dacian origin seem to have been Latinised.
The cities of the Dacians were known as "-dava", "-deva", -δαυα ("-dawa" or "-dava", Anc. Gk.), -δεβα ("-deva", Byz. Gk.) or -δαβα ("-dava", Byz. Gk.), etc. . There is a list of Dacian davas and, more actual, :
"Gil-doba", a village in Thracia, of unknown location.
"Thermi-daua", a town in Dalmatia. Probably a Grecized form of "*Germidava".
"Pulpu-deva", (Phillipopolis) today Plovdiv in Bulgaria.
Political entities.
Rubobostes.
Geto-Dacians inhabited both sides of the Tisa river prior to the rise of the Celtic Boii and again after the latter were defeated by the Dacians under the king Burebista. It seems likely that the Dacian state arose as an unstable tribal confederacy, which was united only fitfully by charismatic leadership in both military-political and ideological-religious domains. At the beginning of the 2nd century BC, under the rule of Rubobostes, a Dacian king in present-day Transylvania, the Dacians' power in the Carpathian basin increased after they defeated the Celts, who previously held power in the region.
Oroles.
A kingdom of Dacia also existed as early as the first half of the 2nd century BC under King Oroles. Conflicts with the Bastarnae and the Romans (112–109 BC, 74 BC), against whom they had assisted the Scordisci and Dardani, greatly weakened the resources of the Dacians.
Burebista.
Burebista (Boerebista), a contemporary of Julius Caesar, ruled Geto-Dacian tribes between 82 BC and 44 BC. He thoroughly reorganised the army and attempted to raise the moral standard and obedience of the people by persuading them to cut their vines and give up drinking wine. During his reign, the limits of the Dacian Kingdom were extended to their maximum. The Bastarnae and Boii were conquered, and even the Greek towns of Olbia and Apollonia on the Black Sea ("Pontus Euxinus") recognized Burebista's authority. In 53 BC, Caesar stated that the Dacian territory was on the eastern border of the Hercynian Forest.
Burebista suppressed the indigenous minting of coinages by four major tribal groups, adopting imported or copied Roman denarii as a monetary standard During his reign, Burebista transferred Geto-Dacians capital from Argedava to Sarmizegetusa Regia. For at least one and a half centuries, Sarmizegetusa was the Dacians' capital and reached its peak under King Decebalus. The Dacians appeared so formidable that Caesar contemplated an expedition against them, which his death in 44 BC prevented. In the same year Burebista was murdered, and the kingdom was divided into four (later five) parts under separate rulers.
Cotiso.
One of these entities was Cotiso's state, to whom Augustus betrothed his own five-year-old daughter Julia. He is well known from the line in Horace ("Occidit Daci Cotisonis agmen", Odes, III. 8. 18).
The Dacians are often mentioned under Augustus, according to whom they were compelled to recognize Roman supremacy. However they were by no means subdued, and in later times to maintain their independence they seized every opportunity to cross the frozen Danube during the winter and ravaging the Roman cities in the province of Moesia.
Strabo testified: "although the Getae and Daci once attained to very great power, so that they actually could send forth an expedition of two hundred thousand men, they now find themselves reduced to as few as forty thousand, and they have come close to the point of yielding obedience to the Romans, though as yet they are not absolutely submissive, because of the hopes which they base on the Germans, who are enemies to the Romans" [17]
In fact, this occurred because Burebista's empire split after his death into four and later five smaller states, as Strabo explains, "only recently, when Augustus Caesar sent an expedition against them, the number of parts into which the empire had been divided was five, though at the time of the insurrection it had been four. Such divisions, to be sure, are only temporary and vary with the times".
Decebalus.
Decebalus ruled the Dacians between AD 87 and 106. The frontiers of Decebal's Dacia were marked by the Tisa River to the west, by the Carpathians to the north and by the Dniester River to the east.
His name translates into "strong as ten men".
Roman conquest.
Trajan turned his attention to Dacia, an area north of Macedonia and Greece and east of the Danube that had been on the Roman agenda since before the days of Julius Caesar when a Roman army had been beaten at the Battle of Histria. In AD 85, the Dacians had swarmed over the Danube and pillaged Moesia and initially defeated an army the Emperor Domitian sent against them, but the Romans were victorious in the Battle of Tapae in AD 88 and a truce was drawn up.
From AD 85 to 89, the Dacians under Decebalus were engaged in two wars with the Romans.
In AD 87, the Roman troops under Cornelius Fuscus were defeated, and Cornelius Fuscus was killed by the Dacians by authority of their ruler, Diurpaneus. After this victory, Diurpaneus took the name of "Decebalus". The next year, AD 88, new Roman troops under Tettius Iullianus, gained a significant advantage, but were obliged to make peace following the defeat of Domitian by the Marcomanni, leaving the Dacians effectively independent. Decebalus was given the status of "king client to Rome", receiving military instructors, craftsmen and money from Rome.
Emperor Trajan recommenced hostilities against Dacia and, following an uncertain number of battles, and with Trajan's troops pressing towards the Dacian capital Sarmizegethusa, Decebalus once more sought terms. Decebalus rebuilt his power over the following years and attacked Roman garrisons again in AD 105. In response Trajan again marched into Dacia, attacking the Dacian capital in the Siege of Sarmizegethusa, and razing it to the ground; the defeated Dacian general Decebalus committed suicide to avoid capture. With Dacia quelled, Trajan subsequently invaded the Parthian empire to the east. His conquests brought the Roman Empire to its greatest extent. Rome's borders in the east were governed indirectly in this period, through a system of client states, which led to less direct campaigning than in the west.
To increase the glory of his reign, restore the finances of Rome, and end a treaty perceived as humiliating, Trajan resolved on the conquest of Dacia, the capture of the famous Treasure of Decebalus, and control over the Dacian gold mines of Transylvania. The result of his first campaign (101–102) was the siege of the Dacian capital Sarmizegethusa and the occupation of part of the country. The second campaign (105–106) ended with the suicide of Decebalus, and the conquest of the territory that was to form the Roman province Dacia Traiana. The history of the war is given by Cassius Dio, but the best commentary upon it is the famous Column of Trajan in Rome.
Although the Romans conquered and destroyed the ancient Kingdom of Dacia, a large remainder of the land remained outside of Roman Imperial authority. Additionally, the conquest changed the balance of power in the region and was the catalyst for a renewed alliance of Germanic and Celtic tribes and kingdoms against the Roman Empire. However, the material advantages of the Roman Imperial system was attractive to the surviving aristocracy. Afterwards, many of the Dacians became Romanised (see also Origin of Romanians). In AD 183, war broke out in Dacia: few details are available, but it appears two future contenders for the throne of emperor Commodus, Clodius Albinus and Pescennius Niger, both distinguished themselves in the campaign.
According to Lactantius, the Roman emperor Decius (AD 249–251) had to restore Roman Dacia from the "Carpo-Dacians" of Zosimus "having undertaken an expedition against the Carpi, who had then possessed themselves of Dacia and Moesia".
Even so, the Germanic and Celtic kingdoms, particularly the Gothic tribes, slowly moved toward the Dacian borders, and within a generation were making assaults on the province. Ultimately, the Goths succeeded in dislodging the Romans and restoring the "independence" of Dacia following Emperor Aurelian's withdrawal, in 275.
In AD 268–269, at Naissus, Claudius II (Gothicus Maximus) obtained a decisive victory over the Goths. Since at that time Romans were still occupying Roman Dacia it is assumed that the Goths didn't cross the Danube from the Roman province. The Goths who survived their defeat didn't even attempt to escape through Dacia, but through Thrace. At the boundaries of Roman Dacia, Carpi (Free Dacians) were still strong enough to sustain five battles in eight years against the Romans from AD 301–308. Roman Dacia was left in AD 275 by the Romans, to the Carpi again, and not to the Goths. There were still Dacians in AD 336, against whom Constantine the Great fought.
The province was abandoned by Roman troops, and, according to the "Breviarium historiae Romanae" by Eutropius, Roman citizens "from the towns and lands of Dacia" were resettled to the interior of Moesia. Under Diocletian, c. AD 296, in order to defend the Roman border, fortifications were erected by the Romans on both banks of the Danube. By AD 336, Constantine the Great had reconquered the lost province. He took the title "Dacicus Maximus" ("The great Victor over the Dacians") when he restored Dacia back to the Roman Empire in 336 AD. However, following his death, the Romans abandoned Dacia permanently.
Roman Empire as the Dacian Empire.
According to Lactantius, emperor Galerius (c. 260 – April or May 311) affirmed his Dacian identity and avowed himself the enemy of the Roman name once made emperor, even proposing that the empire should be called, not the Roman, but the Dacian Empire, much to the horror of the patricians and senators. He exhibited anti-Roman attitude as soon as he had attained the highest power, treating the Roman citizens with ruthless cruelty, like the conquerors treated the conquered, all in the name of the same treatment that the victorious Trajan had applied to the conquered Dacians, forefathers of Galerius, two centuries before.
Dacia after the Romans.
Victohali, Taifals and Thervingians are tribes mentioned for inhabiting Dacia in 350, after the Romans left. Archeological evidence suggests that Gepids were disputing Transylvania with Taifals and Tervingians. Taifals, once independent from Gothia became federati of the Romans, from whom they obtained the right to settle Oltenia.
In 376 the region was conquered by Huns, who kept it until the death of Attila in 453. The Gepid tribe, ruled by Ardaric, used it as their base, until in 566 it was destroyed by Lombards. Lombards abandoned the country and the Avars (second half of the 6th century), dominating the region for 230 years, until their kingdom was destroyed by Charlemagne in 791. At the same time Slavic people arrived peacefully. They were considered lower class and servants and they were permitted to enter and work on the land.
References.
</dl>

</doc>
<doc id="45746" url="http://en.wikipedia.org/wiki?curid=45746" title="Carpathian Mountains">
Carpathian Mountains

The Carpathian Mountains or Carpathians are a range of mountains forming an arc roughly 1500 km long across Central and Eastern Europe, making them the second-longest mountain range in Europe (after the Scandinavian Mountains, 1700 km). They provide the habitat for the largest European populations of brown bears, wolves, chamois and lynxes, with the highest concentration in Romania, as well as over one third of all European plant species. The Carpathians and their foothills also have many thermal and mineral waters, with Romania having one-third of the European total. Romania is likewise home to the largest surface of virgin forests in Europe (excluding Russia), totaling 250,000 hectares (65%), most of them in the Carpathians, with the Southern Carpathians constituting Europe’s largest unfragmented forested area.
The Carpathians consist of a chain of mountain ranges that stretch in an arc from the Czech Republic (3%) in the northwest through Slovakia (17%), Poland (10%), Hungary (4%) and Ukraine (11%) to Romania (53%) in the east and on to the Iron Gates on the River Danube between Romania and Serbia (2%) in the south. The highest range within the Carpathians is the Tatras, on the border of Slovakia and Poland, where the highest peaks exceed 2600 m. The second-highest range is the Southern Carpathians in Romania, where the highest peaks exceed 2500 m.
The Carpathians are usually divided into three major parts: the Western Carpathians (Czech Republic, Poland, Slovakia), the Eastern Carpathians (southeastern Poland, eastern Slovakia, Ukraine, Romania), and the Southern Carpathians (Romania, Serbia).
The most important cities in or near the Carpathians are: Bratislava and Košice in Slovakia; Kraków in Poland; Cluj-Napoca, Sibiu and Braşov in Romania; and Miskolc in Hungary.
Name.
The name "Carpathian" may have been derived from Carpi, a Dacian tribe. According to Zosimus, this tribe lived until 381 on the eastern Carpathian slopes. The word could come from an Indo-European word meaning "rock". In Thracian Greek "Καρπάτῆς όρος" ("Karpates oros") means "rocky mountain". The Carpi tribe may be similar to or identical to another tribe which lived in the area called the Carpodaces, literally "the Carpi Dacians".
The range is called "Karpaty" in Czech, Polish, Slovak and "Карпати" in Ukrainian, "Carpați" ] in Romanian, "Karpaten" in German and Dutch, "Kárpátok" in Hungarian (kárpátok means "Carpaths", kárpáti means "Carpathian"), "Karpati" in Serbian and "Карпати" in Bulgarian .
The name "Carpates" may ultimately be from the Proto Indo-European root *sker-"/"*ker-, from which comes the Albanian word karpë" (rock), and the Slavic word skála (rock, cliff), perhaps via a Dacian cognate which meant "mountain," "rock", or "rugged" (cf. Germanic root "*skerp-", Old Norse "harfr" "harrow", Middle Low German "scharf" "potsherd" and Modern High German "Scherbe" "shard", Old English "scearp" and English "sharp", Lithuanian "kar~pas" "cut, hack, notch", Latvian "cìrpt" "to shear, clip"). The archaic Polish word karpa" meant "rugged irregularities, underwater obstacles/rocks, rugged roots or trunks". The more common word "skarpa" means a sharp cliff or other vertical terrain. The name may instead come from Indo-European *"kwerp" "to turn", akin to Old English "hweorfan" "to turn, change" (English "warp") and Greek καρπός "karpós" "wrist", perhaps referring to the way the mountain range bends or veers in an L-shape.
In late Roman documents, the Eastern Carpathian Mountains were referred to as "Montes Sarmatici" (meaning "Sarmatian Mountains"). The Western Carpathians were called "Carpates", a name that is first recorded in Ptolemy's "Geographia" (2nd century AD).
In the Scandinavian "Hervarar saga", which relates ancient Germanic legends about battles between Goths and Huns, the name "Karpates" appears in the predictable Germanic form as "Harvaða fjöllum" (see Grimm's law).
"Inter Alpes Huniae et Oceanum est Polonia" by Gervase of Tilbury, has described in his Otia Imperialia ("Recreation for an Emperor") in 1211. Thirteenth to 15th century Hungarian documents named the mountains "Thorchal", "Tarczal" or less frequently "Montes Nivium".
Geography.
The Carpathians begin on the Góra Świętego Marcina 384 m. in Tarnów - northern edge of Pogórze Ciężkowickie. They surround Transcarpathia and Transylvania in a large semicircle, sweeping towards the southeast, and end on the Danube near Orşova in Romania. The total length of the Carpathians is over 1500 km and the mountain chain's width varies between 12 and. The highest altitudes of the Carpathians occur where they are widest. The system attains its greatest breadth in the Transylvanian plateau and in the south of the Tatra group – the highest range, in which Gerlachovský štít in Slovakia is the highest peak at 2655 m above sea level. The Carpathians cover an area of 190000 km2 and, after the Alps, form the next most extensive mountain system in Europe.
Although commonly referred to as a mountain chain, the Carpathians do not actually form an uninterrupted chain of mountains. Rather, they consist of several orographically and geologically distinctive groups, presenting as great a structural variety as the Alps. The Carpathians, which attain an altitude of over 2500 m in only a few places, lack the bold peaks, extensive snowfields, large glaciers, high waterfalls, and numerous large lakes that are common in the Alps. It was believed that no area of the Carpathian range was covered in snow all year round and there were no glaciers, but recent research by Polish scientists discovered one permafrost and glacial area in the Tatra Mountains. The Carpathians at their highest altitude are only as high as the middle region of the Alps, with which they share a common appearance, climate, and flora.
The Carpathians are separated from the Alps by the Danube. The two ranges meet at only one point: the Leitha Mountains at Bratislava. The river also separates them from the Balkan Mountains at Orşova in Romania. The valley of the March and Oder separates the Carpathians from the Silesian and Moravian chains, which belong to the middle wing of the great Central Mountain System of Europe. Unlike the other wings of the system, the Carpathians, which form the watershed between the northern seas and the Black Sea, are surrounded on all sides by plains, namely the Pannonian plain to the southwest, the plain of the Lower Danube (Romania) to the south, and the Galician plain to the northeast.
Cities and towns.
Important cities and towns in or near the Carpathians are, in approximate descending order of population:
Highest peaks.
This is an (incomplete) list of the highest peaks of the Carpathians (limited to summits over 2,500 m), their heights, geologic divisions and locations.
Highest peaks by country.
This is a list of the highest national peaks of the Carpathians, their heights, geologic divisions, and locations.
Mountain passes.
In the Romanian part of the main chain of the Carpathians, the most important mountain passes are (starting from the Ukrainian border): the Prislop Pass, Rodna Pass, Tihuţa Pass (also known as Borgo Pass), Tulgheş Pass, Bicaz Canyon, Ghimeş Pass, Uz Pass and Oituz Pass, Buzău Pass, Predeal Pass (crossed by the railway from Braşov to Bucharest), Turnu Roşu Pass (1,115 ft., running through the narrow gorge of the Olt River and crossed by the railway from Sibiu to Bucharest), Vulcan Pass, Teregova Pass and the Iron Gate (both crossed by the railway from Timișoara to Craiova).
Geology.
The area now occupied by the Carpathians was once occupied by smaller ocean basins. The Carpathian mountains were formed during the Alpine orogeny in the Mesozoic and Tertiary by moving the ALCAPA, Tisza and Dacia plates over subducting oceanic crust ().
The mountains take the form of a fold and thrust belt with generally north vergence in the western segment, northeast to east vergence in the eastern portion and southeast vergence in the southern portion.
The external, generally northern, portion of the orogenic belt is a Tertiary accretionary prism of a so-called Flysch belt created by rocks scraped off the sea bottom and thrust over the North-European plate. The Carpathian accretionary wedge is made of several thin skinned nappes composed of Cretaceous to Paleogene turbidites. Thrusting of the Flysch nappes over the Carpathian foreland caused the formation of the Carpathian foreland basin. The boundary between the Flysch belt and internal zones of the orogenic belt in the western segment of the mountain range is marked by the Pieniny Klippen Belt, a narrow complicated zone of polyphase compressional deformation, later involved in a supposed strike-slip zone. Internal zones in western and eastern segments contain older Variscan igneous massifs reworked in Mesozoic thick and thin-skinned nappes. During the Middle Miocene this zone was affected by intensive calc-alkaline arc volcanism that developed over the subduction zone of the flysch basins. At the same time, the internal zones of the orogenic belt were affected by large extensional structure of the back-arc Pannonian Basin.
Iron, gold and silver were found in great quantities in the Western Carpathians. After the Roman emperor Trajan's conquest of Dacia, he brought back to Rome over 165 tons of gold and 330 tons of silver.
Divisions of the Carpathians.
The largest range is the Tatras.
A major part of the western and northeastern Outer Carpathians in Poland, Ukraine and Slovakia is traditionally called the Beskids.
The geological border between the Western and Eastern Carpathians runs approximately along the line (south to north) between the towns of Michalovce, Bardejov, Nowy Sącz and Tarnów. In older systems the border runs more in the east, along the line (north to south) along the rivers San and Osława (Poland), the town of Snina (Slovakia) and river Tur'ia (Ukraine). Biologists, however, shift the border even further to the east.
The border between the eastern and southern Carpathians is formed by the Predeal Pass, south of Braşov and the Prahova Valley.
Ukrainians sometimes denote as "Eastern Carpathians" only the Ukrainian Carpathians (or Wooded Carpathians), meaning the part situated largely on their territory (i.e., to the north of the Prislop Pass), while Romanians sometimes denote as "Eastern (Oriental) Carpathians" only the part which lies on their territory (i.e., from the Ukrainian border or from the Prislop Pass to the south), which they subdivide into three simplified geographical groups (north, center, south), instead of Outer and Inner Eastern Carpathians. These are:
Tourism.
Bukovel is one of the largest ski resort in Carpathians.
Gallery.
<br>
<br>

</doc>
<doc id="45747" url="http://en.wikipedia.org/wiki?curid=45747" title="Weapons in Star Trek">
Weapons in Star Trek

The "Star Trek" fictional universe contains a variety of weapons, ranging from missiles (the classic photon torpedo) to melee (primarily used by the Klingons, a race of aliens in the Star Trek universe).
Energy weapons.
A directed-energy weapon emits energy in an aimed direction without the means of a projectile. It transfers energy to a target for a desired effect. Intended effects may be non-lethal or lethal. For example, in "Star Trek", a hand phaser can be set to "stun" or "kill".
Laser.
Lasers are a sidearm in the original "Star Trek" pilot "", and laser pistols appear in several "Original Series" episodes, although later episodes in "The Next Generation" seemed to indicate that the laser's use as a weapon was outdated. In one instance, the ship-mounted lasers of two spacecraft were incapable of overcoming even the navigational shields of the USS "Enterprise"-D, though on at least two other occasions it was threatened with destruction by laser-armed spacecraft. The Borg cutter weapon is a laser, as mentioned in the "" episode "Q Who" and is apparently incapable of directly penetrating Federation shielding. (Although it is effective at making 'surgical' incisions into a ship's hull.)
According to "The Making of Star Trek", Gene Roddenberry claimed that production staff realized that using laser technology would cause problems in the future as people came to understand what lasers could and could not do; this resulted in the move to phasers on-screen, while letting lasers be known as a more primitive weapon style.
Pulse cannon.
When the laser had reached its upper limits of power, a new weapon was invented at Jupiter Station for the newly designed NX-class. This new weapon could fire much greater energies at longer range but the drawback was that continuous fire would cause damage to the emitter. To solve this problem, Starfleet scientists devised a way to fire the weapon in short pulsed burst streams to help regulate the temperatures of the emitters in a state of constant fire. The pulse cannon is a rather simple particle beam weapon and Starfleet Command ultimately decided on using the more powerful plasma cannon on the NX-class "Enterprise". The pulse cannon was nevertheless considered an effective energy weapon at the time and was adopted by many private craft such as freighters and privateers due to its lower cost of implementation.
Plasma cannon.
Plasma cannons are a form of directed energy weaponry used by both Earth Starfleet and the early Romulan Star Empire. On Starfleet vessels, they were the precursors to phase cannons. Plasma cannons fired a plasma discharge in the form of a beam or a burst similar to the plasma bullets fired by hand-held plasma weapons, but much bigger in size. The NX-class was initially armed with plasma cannons.
Phase cannon.
Phase cannons are 22nd century weapons, several of which first appear mounted to the "Enterprise" in the "Star Trek: Enterprise" episode "". Phase cannons have a variable yield, with the cannons on the "Enterprise" being rated for a maximum output of 500 gigajoules. Phase cannons are generally more powerful than spatial torpedoes. They are the 22nd century precursor to phaser technology. Phase pistols are hand-carried Phase cannons, and are also the 22nd century precursor to phaser technology. However, unlike phasers, they do not have the normal sixteen power settings or a variable beam width—only stun and kill.
Phaser.
Phasers are common directed-energy weapons first seen in the and later seen or referenced in almost all subsequent films and TV spin-offs.
Phasers come in a wide range of sizes, ranging from hand-held versions to starship-mounted ones. Personal phasers can be made small enough to fit in the user's palm and still be deadly. Larger and more powerful phaser rifles are commonly issued to security personnel. Phaser beams can be adjusted in both width and output. A typical hand phaser has different power settings so that it can merely stun living organisms, or completely disintegrate them. The beam can be adjusted to strike multiple targets at once or evenly destroy large amounts of material. A starship's phasers can be used as an 'anti-missile' defense to destroy incoming projectiles. Also witnessed in the TOS episode "For The World is Hollow and I Have Touched the Sky". They can be used as welding torches or cutting tools, and can create heat sources by firing at a large, solid object (like a rock). Phasers can be set to overload, whereby they build up a force-chamber explosion by continuously generating energy without releasing it; the resulting blast can destroy most natural objects within a 50-meter radius. The overload process is marked by a distinctive sound that increases in volume and frequency until it is deactivated or it detonates. Ship-mounted phasers have a similar range of functions on a larger scale: The phasers on the USS "Enterprise" could stun entire city blocks full of people, destroy cities, and even destroy entire asteroids up to a given size. The ship's phaser system was also said to be capable of destroying continents. There are several types of phasers used by Starfleet and the United Federation of planets.
Originally (from the production notes to ), the phaser was a PHoton mASER, since at the time of writing the laser was a relatively unknown. Masers, on the other hand, were already established and produced long range coherent beams of electromagnetic radiation. The term "phaser" has since been revised as a backronym for "PHASed-Energy Rectifier," though from a physics standpoint even this is of equal semantic content; ordinary incoherent light is "not" "rectified," or synchronous, whereas lasing and masing emissions "are" rectified, or synchronous. Phasers release a beam of fictional subatomic particles called "rapid ", which are then refracted ("rectified") through superconducting crystals. Given the nature of photons, the first acronym seems more accurate. The "" indicates that the superconducting crystals used in phasers are called "fushigi no umi," which is Japanese for "sea of mystery," and the phrase is written ふしぎの海 in the original glyphs. This was an homage to the 1990 anime series "Fushigi no Umi no Nadia," known in North America as "." The phasers that appeared in the 2009 reboot "Star Trek" appear similar to classic phasers, but fire "bolts" of energy instead of sustained beams, and seem to only have two settings, stun and kill. The barrel of the weapon is two-sided, and the user must literally rotate to a different output to use a different setting. It is not known whether or not that type of phaser has adjustable power output. A similar change was seen in the starship-mounted phaser banks, which also fire bolts instead of beams.
A phaser rifle prop from "Where No Man Has Gone Before" sold at auction for $231,000. A phaser pistol prop from "Star Trek" will be sold at auction starting at $60,000.
Disruptor.
Disruptors are employed by several alien species in this series, including Romulans, Klingons, Breen, Cardassians, Iridians and Orions in their personal and military small arms as well as being mounted as cannon, emitters, turrets, and banks. Only the first three species are known to have type-3 disruptors, the most advanced type developed so far, by the 24th Century. Disruptors cause damage by exciting the molecular bonds of targets to such great extents that those bonds are weakened, and/or broken, by the energies emitted. According to Last Unicorn's "," disruptors are considered less "elegant" than phaser-based weapons; their effects there are described as thermal shock and blunt force, as opposed to the "rapid nadion effect." As a result, disruptors inflict more damage to matter, but less damage to shields, than phasers inflict.
Phased polaron cannon.
Phased polaron cannons are the primary armament of the Dominion, the main antagonist faction in the later seasons of ". The cannon emits a beam of polaron particles, the fictional in-universe antimatter counterpart of the muon (not to be confused with the actual polaron or the actual antimuon). When first introduced, Dominion polaron cannons easily penetrate the shielding systems of most Alpha Quadrant races. The Alpha Quadrant races eventually learn to modify their shields to resist polaron weaponry, evoking surprise from the Vorta advisor Weyoun (").
Tetryon Cannon.
Tetryon cannons are the primary armament of the Hirogen and are similar in application to phasers and disruptors. Tetryon cannons are unique in that they are designed primarily to damage energy and force fields such as starship shields. Tetryon cannons do damage matter but not to the same extent as phasers or disruptors. This is in line with the Hirogen philosophy of the hunt. The Hirogen ship would therefore knock out its opponent's shields and beam over hunters to engage in face-to-face ranged or melee combat. This allows the Hirogen hunters to collect items that they would consider to be hunting trophies.
Varon-T Disruptor.
Varon-T disruptors were featured in the "" episode "The Most Toys", and were mentioned to be a rare type of disruptor made illegal in the Federation because of their slow, excruciating method of killing. The weapons tear the body apart from the inside. Kivas Fajo, a Zibalian trader in that episode, owned four of the five Varon-T disruptors ever manufactured (he slept with one under his pillow) and even used one on his own crew before his collection of rare items was confiscated after his capture and arrest for kidnapping and theft (among other crimes).
Ferengi Energy Whip.
The Ferengi energy whip, as seen in the " episode ", looks and handles like a typical Earth bullwhip and discharges a powerful phaser-like energy pulse.
Projectile Weapons.
Spatial torpedo.
Also referred to as conventional torpedoes, spatial torpedoes are 22nd century weapons used by "Enterprise". Spatial torpedoes are the ship's most powerful and primary ship-to-ship weapon before the installation of phase cannons. Spatial torpedoes are themselves superseded by more powerful photonic torpedoes. Unlike photonic torpedoes or any of the warhead's successors, spatial torpedoes are launched at sub-light velocity and can be used much in the manner of a missile, having the warhead on a fly-by-wire.
Photon torpedo.
Photon torpedoes are a standard ship-based weapon armed with an antimatter warhead. They are present in every version of the "Star Trek" series and are a standard weapon on almost every Federation ship, though in "" the titular ship uses less powerful spatial torpedoes (guided, rocket propelled missiles) until receiving the more powerful "photonic" (as the characters describe them) variant, though this does not square with established canon because it took place before the "Romulan Wars" in which, to quote Spock in "Balance of Terror", "This conflict was fought by our standards today with primitive atomic weapons". 
Photon torpedoes first appear on a Starfleet ship in the original series' episode "" as part of the USS "Enterprise"‍ '​s armament—in the "Star Trek: Enterprise" episode "The Expanse", the "Enterprise" (NX-01) first receives photonic torpedoes. Smaller Starfleet craft such as shuttlecraft and runabouts can be armed with "micro-torpedoes", a scaled-down version of photon torpedoes designed for use on craft too small to accommodate the full-sized weapon.
When fired, photon torpedoes usually appear as a spiky orb of energy of varying colours, such as red, orange, yellow, blue, or green, or in the case of "The Original Series", red bolts (However "Enterprise" ignores this). According to the original notes to ' and "The Making of Star Trek", Photon torpedoes are energy shielded to allow armor-penetration. Several episodes seem to suggest this (reference: ' ""). The energy output of a photon torpedo, according to the Technical Manuals is a maximum theoretical yield of 25 Isotons and a maximum rated yield of 18.5 Isotons. According to the TNG Technical Manual, photon torpedoes use 1.5 kg of matter and 1.5 kg of antimatter. However, these amounts would only produce an explosive yield of 64.4 megatons, so a secondary effect must occur to achieve the incredible yields seen on screen. In the ' episode, "Balance of Terror", Photon Torpedoes apparently replaced nuclear warheads as the primary wartime weapons on Federation Starships. The photon torpedo was seen to explode with great force. Later, in ', the explosive effects of the photon torpedo were significantly reduced.
Torpedoes are often depicted as being easy to modify to suit specific situations. Despite the stated maximum yield, torpedoes can apparently be made far more destructive with relatively little effort. In "Star Trek: Voyager", Tuvok and Kim modify a normal photon torpedo with a gravimetric charge, a Borg technology, to increase its destructive yield to 54 isotons. Kim comments that 50 isotons would have been sufficient to destroy a small planet. Janeway later instructs them to increase its yield even further, to 80 isotons. It is not specified exactly how they modified the warhead, but they only required a few hours to complete the work using materials readily available on "Voyager". In "", Spock and Dr. McCoy modify a photon torpedo to track the plasma emissions from a cloaked Klingon bird of prey as it attacks the "Enterprise"-A and the "Excelsior", similar to the principal function of the heat-seeking missile.
Photon torpedo launchers aboard ships are shown to be versatile enough to fire probes, which in-universe are designed with this functionality in mind. In "", a torpedo casing is used as a makeshift coffin for burial in space and as scientific probes as in the "Next Generation" episode "" and "Star Trek Generations". Also in the "Next Generation" episode a photon torpedo casing was used to convey Klingon ambassador K'Ehleyr to the U.S.S. Enterprise when no other long range transport was available.
Plasma torpedo.
Plasma torpedoes are used by the Romulans, Cardassians, and (according to "Star Fleet Battles", "Klingon Academy" and "") the Gorn. The damage of a plasma torpedo spreads out over several ship systems at once, but the torpedo loses its effectiveness after only a few minutes of travel. Romulan plasma torpedoes use trilithium isotopes in their warheads.
Gravimetric Torpedoes.
Gravimetric torpedoes are torpedoes used by the Borg. The weapon emits a complex phase variance of gravitons to create a gravimetric distortion capable of tearing starships apart.
Quantum Torpedo.
Quantum torpedoes first appear in the "Deep Space Nine" episode "" as a weapon aboard the USS "Defiant". Additionally, the USS "Enterprise"-E is equipped with quantum torpedoes in ' and "Star Trek Nemesis". The ' states that quantum torpedoes derive their destructive power from zero-point energy.
In the "DS9" Episode "" the USS "Lakota" was also stated to be carrying quantum torpedoes, although they were never used.
Four of the USS "Enterprise"-E's quantum torpedoes destroyed an unshielded Borg sphere. The launcher appears on the 1701-E in "Star Trek: Insurrection" but is never fired. In "Star Trek Nemesis", nine of the "Enterprise"-E's quantum torpedoes disabled "Scimitar"‍ '​s cloaking function.
Quantum torpedoes are normally shown in a shade of blue. As of "Nemesis" the "Enterprise", the "Lakota", the "Valiant", the "Defiant", and the unmanned spacecraft "" were the only ships known to be equipped with quantum torpedoes.
Polaron Torpedo.
Polaron torpedoes, like the Dominion weapon, are capable of penetrating normal shielding with ease. They appear in various "Star Trek" games. In ', it is one of the Klingon's three heavy weapon options, the others being the photon torpedo and the ion cannon. It also appears in ', and "", as a researchable weapon for the Klingon Empire exclusive to the "Vor'cha"-class cruiser which takes out one of the targeted ship's systems at random.
Transphasic Torpedo.
Transphasic torpedoes appear only once, in the "Voyager" series finale, "Endgame". They are high-yield torpedoes that are designed specifically to fight the Borg. The future Admiral Janeway brought them back in time in a Federation shuttlecraft and had them installed onboard "Voyager" in 2378. They are among the most powerful weapons used in the "Star Trek" universe; just one is capable of obliterating an entire Borg cube, a feat normally requiring an almost impossible amount of punishment using standard Federation weapons. They work by phasing out of normal space, traveling "through" a target and then re-phasing back into normal space. Although they did not appear in the film "Nemesis", according to the non-canon "Destiny" book trilogy these were in fact kept by Starfleet as the weapon of last resort to be deployed to starships only when all else had failed against the Borg. They were the one and only thing Starfleet knew the Borg had not yet adapted to and for that reason wanted to keep this ace in the hole for as long as possible. Eventually the situation became dire enough that the specifications were released to Federation and Klingon ships; the Borg eventually learned to adapt to them.
Isokinetic Cannon.
The Isokinetic Cannon was seen in only one episode of "Star Trek: Voyager", "". Voyager met with a weapons trader and designer known as Kovin. It destroyed a target buoy composed of 10 m thick solid monotanium with a chromoelectric forcefield in one shot, coring it cleanly through. However, Kovin was killed before the installation of the weapon. Though the weapon was never known to have been removed from Voyager, it was never seen in use nor referred to again.
TR-116 Projectile Rifle.
The TR-116 Projectile Rifle is a prototype weapon developed by the Federation for situations where conventional energy weapons might be rendered useless by damping fields or other countermeasures. It is essentially a conventional rifle, but with a rather futuristic visual style. It is introduced in the "Star Trek: Deep Space Nine" episode ", where it is used in conjunction with a micro-transporter and a visual scanner headpiece to create an extremely potent sniper rifle. With the scanner, the shooter can precisely target people hundreds of meters away and through solid matter with no difficulty. Using the transporter attached to the barrel, the slug can then be transported at full velocity, materializing at point-blank range.
Phased Plasma Torpedo.
Phased plasma torpedoes can phase out of normal space-time to bypass shields, then phase back in to detonate on a ship's hull, thus making shields worthless against them. They only appeared in the PC game ". Shortly after the recovery of the Pegasus device, the phasing properties used in the design were seen as a delivery system for torpedoes. Since Borg ships are almost impossible to destroy by Starfleet's current technology, it made sense to their engineers to design a torpedo that could phase itself and enter the body of a Borg cube, causing devastating damage. However, reducing the size of the phasing coils used to accomplish an intangible state proved difficult. Further, the antimatter within the warhead had a destabilizing effect on the phasing coil. A new kind of explosive material was needed, and it was found using the principles behind the first observed Romulan plasma weapons. The installation of a high-energy plasma infuser would allow a torpedo casing to be filled with a warhead charged with high-energy plasma from the ship's warp nacelles. Warp plasma is highly unstable and can be easily detonated. Until recently, it was considered an undeliverable medium that could not be controlled. However, using a nanite controlled trigger for reactant release now allows vessels to deliver a high-energy plasma warhead payload within a Mark IV torpedo casing.
Chroniton Torpedo.
Chroniton torpedoes are a unique form of weapon employed by the Krenim. The weapons phase in and out of normal time, allowing them to pass through ordinary shields and directly damage a vessel's hull. Though quite dangerous, their reliability is not absolute, as Seven of Nine and Tuvok (as well as Kes in the alternate timeline presented in ) find an undetonated chroniton torpedo lodged in "Voyager"‍ '​s hull, which in turn allowed the crew to adapt the shields to withstand further attacks.
Positron Torpedo.
The Kessok are a highly intelligent race that allied themselves with the Cardassians, albeit through deceit, in the video game "". They utilize positron torpedoes: powerful, slow-moving projectiles able to inflict nearly twice as much damage as quantum torpedoes.
Biological, radioactive, and chemical weapons.
Thalaron radiation.
Thalaron radiation was first used in the feature film "Star Trek Nemesis" by the villain Shinzon to assassinate the entire Romulan senate. Later in the movie, Shinzon attempts to kill the crew of the USS "Enterprise"-E using a ship-mounted version. Thalaron radiation, even in small amounts, petrifies living tissue almost instantly. Its properties also allow its range and area of effect to be precisely controlled, from encompassing a single room to engulfing an entire planet. Its massive destructive potential leads the Federation to consider it a biogenic weapon, and an extremely illegal one at that. It later features prominently in the plot of "Homecoming", the "" short story in the 2008 Mirror Universe anthology "Shards and Shadows", in which the rebels manage to steal a Romulan thalaron bomb intended for use by the Alliance, to strike a balance of power against them.
Metreon cascade.
The metreon cascade was designed by Dr. Ma'Bor Jetrel of the Haakonian Order. Unstable metreon isotopes were used to create a devastating explosion, with radiation effects similar to those of the 20th-century atomic bomb. Those not killed or vaporized in the initial blast suffered terrible radiation poisoning and death in the aftermath. It was used only once, on the Talaxian moon Rinax in 2355.
Trilithium resin.
Trilithium resin is a byproduct of a starship's warp engines that is lethal to humans, but harmless to Cardassians. A team of terrorists attempted to steal Trilithium resin from the warp core of the "Enterprise"-D when it was docked at Arkaria Station to receive a baryon sweep.
Captain Benjamin Sisko would later use a Trilithium resin torpedo to render a Maquis planet uninhabitable to all human life for fifty years by detonating it in the atmosphere.
Cobalt diselenide.
Cobalt diselenide is a biogenic weapon that affects the nervous system. It is composed of selenium and rhodium nitrates. It is the counterpart to trilithium resin, being lethal to Cardassians but harmless to most other humanoids.
Aceton assimilators.
Aceton assimilators are used to absorb energy from other sources and then redirect it back as hazardous radiation.
Melee weapons.
Federation.
KaBar combat knife.
The KaBar combat knife is the Federation's standard-issue combat and survival knife. It is 32.5 cm (12.8 in) and is standard equipment in survival gear and in emergency weapons caches aboard starships. Captain Kathryn Janeway uses one in the "Star Trek: Voyager" episode "".
In the real world, the KA-BAR is an official combat knife of the United States Marine Corps.
Katana.
A katana is a Federation sword of Japanese origin. The only major difference compared to the old sword of today is that the Star Trek version is foldable, thus occupying a minimum space when carried and stored. In the 2009 "Star Trek XI" film, Lieutenant Hikaru Sulu produces a folding katana with which to cut the lines of his parachute, having stated just before this that his hand-to-hand combat expertise is in fencing.
Jem'Hadar.
Bayonet.
The Jem'Hadar often have bayonets attached to their plasma rifles. They employ these in close combat or to execute prisoners.
Kar'takin.
These straight bladed polearms are used by the Jem'Hadar in close combat. They were used by both Starfleet officers and Jem'Hadar in the "" episode "To The Death". The Kar'takin bear a resemblance to the Bardiche axe.
Klingon.
Bat'leth.
The bat'leth is the Klingon longsword, designed by martial arts enthusiast and "" effects producer Dan Curry. The bat'leth is a curved blade with spiked protrusions and handholds along the middle of the blade's back. In battle, the handholds are used to twirl and spin the blade rapidly.
Klingon oral history holds that the first bat'leth was forged around 625 A.D. by Kahless, who dropped a lock of his hair into the lava from the Kri'stak Volcano, then plunged the fiery lock into the lake of Lursor and twisted it to form a blade. After forging the weapon, he used it to defeat the tyrant Molor, and in doing so united the Klingon homeworld. This first bat'leth was known as the Sword of Kahless' and was stolen by the invading Hur'q; an episode of "Deep Space Nine" revolves around an effort to recover the Sword of Kahless. The name bat'leth itself is a slight corruption of batlh 'etlh, which means "Sword of Honor" in Klingon.
A replica bat'leth was among the blades surrendered to British police as part of the 2006 knife amnesty. A "Valdris" blade was used in Colorado Springs, Colorado, in two 7-Eleven armed-robberies in 2009.
Qutluch.
Similar to the d'k tahg, the qutluch is "the ceremonial weapon of an assassin." A qutluch is designed to do considerable damage to internal organs, by Klingon standards thus making it an extremely lethal weapon. The qutluch is featured in the "Star Trek: The Next Generation" episode "", when Worf's brother, Kurn, is stabbed; and in the "Star Trek: Voyager" episode "", where the Doctor's simulated "son" prepares for the Qutluch ceremony. It is known in the real world as the "Phoenix" type knife.
Mek'leth.
A mek'leth is the Klingon short sword that appears in several episodes of ' and in the film '. Designed by Dan Curry, it consists of a short, thick, curved blade with a metal guard extending back parallel with the grip to protect the hand. Worf is the most commonly seen user of the mek'leth, owning one and using it several times, including in hand-to-hand combat against Borg drones in "First Contact".
Romulan.
Teral'n.
A Romulan polearm, similar to a trident with retractable blades. It appears in the 2009 "Star Trek" film and is used by the renegade miner Nero. In the comic, Nero's weapon is revealed to be the "Debrune teral'n", an ancient Romulan artifact that symbolized the empire's power; it is traditionally held by the presiding Praetor.
A similar weapon, resembling an axe was also used by a member of Nero's crew. While widely considered to also be a Teral'n, the weapon has yet to be confirmed as one by an official source.
Vulcan.
Lirpa.
A lirpa is a Vulcan weapon consisting of a wooden staff a little over a meter in length, with a semicircular blade at one end and a metal bludgeon on the other. It is similar to the monk's spade and the pugil stick. Captain James T. Kirk and Spock used lirpas when they fought for possession of T'Pring during Spock's Pon farr ritual in "Amok Time". Soldiers sent after Jonathan Archer and T'Pol fought with lirpas because Vulcan's "Forge" region makes conventional energy weapons useless.
Ahn'woon.
An ahn'woon is a Vulcan catch-strangle weapon, similar in principle to the Earth Roman gladiator's cast net. The multi-strapped weapon (approximately 1.1 meters long) uses weights on the ends of the straps, like bolas, to entangle, stun, or cut the target, and the application of tying action and wrapping can restrict the breathing of the target, asphyxiating the victim.
Other.
Ushaan-tor.
In the "Enterprise" episode "United", Andorian commander Thy'lek Shran and the NX-01's captain Jonathan Archer, as a second for a Tellarite officer who kills Tallas, Shran's chief tactical officer and lover, engage in an Andorian "ushaan" duel. The weapon used is the ushaan-tor, an Andorian ice mining blade. The handheld blade of the "ushaan-tor" is about 20 cm from end-to end, and resembles an Inupiat Ulu blade from Alaska, but in a one-piece all-metal design instead of having a separate wooden handle.
Mortaes and Thongs.
In the "TOS" episode "The Cloud Minders," mortaes and thongs are mining tools used as martial weapons by the "troglyte" (a corruption of "troglodyte") miners, and apparently the ruling class is also trained with these weapons as well, as Plasus challenges Kirk to hand-to-hand combat, asking, "Are you as brave with a mortae as with a phaser?" Kirk responds, "Both will kill."
Glavin.
In the "TNG" episode "Code of Honor", the Ligonians have deep traditions of fighting with a poison-tipped hand weapon called a Glavin, It is a large glove with a recurved claw at the end, and covered with dozens of spines, strongly resembling (and most likely based on, in the real world) a scorpion's tail sting. In several episodes Worf is seen displaying one in his quarters, most likely the same one used by Lt. Yar.
Subspace weapons.
Subspace weapons are a class of directed energy weapons that directly affect subspace. The weapons can produce actual tears in subspace, and are extremely unpredictable. These weapons were banned under the second Khitomer Accord. The Son'a equipped their vessels with these types of weapons.
Isolytic burst.
Son'a vessels carried and used isolytic burst weapons, a type of subspace weapon. They were seen using this weapon against the "Enterprise"-E in "Star Trek: Insurrection". The "Enterprise" was only able to escape the weapon's effect by ejecting its warp core and detonating it to seal a subspace rift.
Tricobalt devices.
The tricobalt warhead is a subspace weapon whose high-yield detonations can tear holes in subspace. Tricobalt devices are not a standard armament of Federation vessels and yields are calculated in Tera-Cochranes, indicating that its mechanism is somewhat similar to the general reaction in a warp field.
USS "Voyager" uses a pair of tricobalt devices to destroy the Caretaker array in the ' pilot episode, "", and such a device was used against "Voyager" in the episode "". A tricobalt warhead was also used by the Tholians in the ' episode "In a Mirror, Darkly". They detonated a tricobalt warhead inside the gravity well of a dead star. The explosion created an interphasic rift, which they used to lure the Federation starship USS "Defiant" from another universe. In "TOS": "A Taste of Armageddon", the Eminian Union classified the USS "Enterprise" as 'destroyed' when it was hit by virtual tricobalt satellites. In "DS9": "Trials and Tribble-ations", Arne Darvin plants a tricobalt explosive in a dead Tribble in an attempt to kill Kirk.
The games ' and ' have ships armed with Tricobalt devices for artillery support. The Federation "Steamrunner" class, the Klingon "Chuq'Beh"-class Bird of Prey, the Romulan "Raptor" class Warbird, and the Borg Harbinger are all capable of using them. The workings of the weapon is unknown but theorised is the use of Cobalt-60.
Other weapons.
Magnetometric guided charges.
Around Stardate 43995, the Borg used this weapon to drive the USS "Enterprise", NCC-1701-D, from the Paulson Nebula. This shortly leads to the abduction of Captain Jean Luc Picard.
Multikinetic neutronic mines.
During Season 4, Episode 1 of "", Captain Janeway consults with Borg representative Seven of Nine on how to destroy Species 8472. Janeway calls Seven of Nine's "multikinetic neutronic mine" a "weapon of mass destruction," following up on a statement from Tuvok that it would affect an entire starsystem, destroying innocent worlds. The mine's five-million isoton yield can disperse Borg nanoprobes across a five-light year range.
Dreadnought.
Dreadnought was a Cardassian self-guided missile, containing one thousand kilograms of matter, and another thousand of antimatter. Tuvok describes this as enough to destroy a small moon. Although described as a self-guided missile, in practice Dreadnought functions much like an autonomous starship, and it even had life support capability on board. It possesses shields, phasers, a complement of quantum torpedoes, a Thoron shock emitter, a plasma wave weapon, engines capable of reaching at least Warp 9, and a sophisticated computer AI. It appears in the "Voyager" . It had been captured by the Maquis due to a failed detonator and reprogrammed to attack its original creators. It was dragged into the Delta Quadrant in much the same manner as "Voyager", and when unable to resolve the unforeseen situation it locked on to a planet that was similar to the one it was programmed to target, but which was inhabited by innocents. Dreadnought was equipped with an exceptionally sophisticated artificial intelligence, capable of "paranoia" to a certain degree, as when re-programmer B'Elanna Torres attempted to prevent it from destroying the innocent planet, it came to the conclusion that she had been captured by her Cardassian enemies and forced to make up a story to prevent the attack; it then "pretended" to follow her commands and shut down, only to re-activate and continue its mission once she was no longer aboard.
Series 5 long range tactical armor unit.
Similar in purpose to the Cardassian Dreadnought, the Tactical Armor Units are self-guided missiles with sophisticated artificial intelligence. They are much smaller than Dreadnought, being only a few feet in length, and while nowhere near as powerful, they are nonetheless classified as weapons of mass destruction, capable of destroying everything in a 200-kilometer radius with a highly focused antimatter explosion. Their coordination and control is done through a "Strategic Command Matrix", analogous to a nuclear control network of the type used by the United States. Each one possesses shielding, warp drive of indeterminate speed, and a sentient, genius-level artificial intelligence programmed to do whatever is necessary to reach their targets and detonate. They can detect and prevent tampering, are intelligent enough to find a way past almost any obstacle, and can win engagements even when outnumbered. They were created by a Delta Quadrant race called the Druoda, and the devices were greatly feared for their endurance and tenacity.
Q firearms.
Q firearms were used in the Q Civil War by the "Voyager" crew to compensate against the infinite power of the Q in "The Q and the Grey". They are depicted as front-loading muskets, to fit with the American Civil War-theme used by the Q Continuum as a concession to the human characters' limited perceptions. Presumably, their actual form would be as incomprehensible to non-Q as the Continuum itself. The use of the weapons caused supernovae as a side effect in normal reality. They are arguably the most powerful weapons ever wielded by any humanoid species, as indicated by their ability to injure the otherwise-invulnerable Q.
Red matter.
In the 2009 Star Trek film, red matter was developed on Vulcan before 2387. When even a droplet is ignited an unstable singularity is formed, as a result it had to be stored in a protective chamber. The red matter was originally to be used to save the Romulan homeworld from a volatile supernova, but the design was finished too late to prevent Romulus' destruction. Upon capture in the "past" (2258) by the Romulan Nero, it was used as a planet-destroying doomsday weapon; used in conjunction with a plasma drill which bored a hole almost to the core of a planet, a small amount of red matter would be activated at the bottom of the drilling site, creating a black hole in the heart of the planet that would tear it apart from within. Red matter was thus used to destroy an alternate Vulcan ("alternate" due to temporal disruption, from Nero's haphazard method of time travel), then ultimately destroyed Nero's ship, the "Narada", along with all remaining technology from his ship, and all of the remaining red matter.

</doc>
<doc id="45748" url="http://en.wikipedia.org/wiki?curid=45748" title="Chariot">
Chariot

A chariot is a type of carriage using animals (almost always horses) to provide rapid motive power. Chariots were used for war as "battle taxis" and mobile archery platforms, as well as other pursuits such as hunting or racing for sport, and as a chief vehicle of many ancient peoples, when speed of travel was desired rather than how much weight could be carried. Ox carts, proto-chariots, were built by the Proto-Indo-Europeans, and also in Mesopotamia as early as 3000 BC. The original horse chariot was a fast, light, open, two-wheeled conveyance drawn by two or more horses that were hitched side by side. The car was little more than a floor with a waist-high semicircular guard in front. The chariot, driven by a charioteer, was used for ancient warfare during the Bronze and the Iron Ages. Armor was limited to a shield. After it had been superseded by other vehicles for military purposes, the chariot was used for travel and in processions, games, and races.
The word "chariot" comes from Latin "carrus", which was a loan from Gaulish. A chariot of war or of triumph was called a "car". In ancient Rome and other ancient Mediterranean countries a "biga" required two horses, a "triga" three, and a "quadriga" required four horses abreast. Obsolete terms for chariot include "chair", "charet" and "wain".
The critical invention that allowed the construction of light, horse-drawn chariots was the spoked wheel.
The earliest spoke-wheeled chariots date to ca. 2000 BC and their use peaked around 1300 BC (see Battle of Kadesh). Chariots ceased to have military importance in the 1st century AD, but chariot races continued to be popular in Constantinople until the 6th century.
Early wheeled vehicles in Sumer.
The horse drawn wheeled vehicle probably originated in Mesopotamia about 3000 BC. The earliest depiction of vehicles in the context of warfare is on the Standard of Ur in southern Mesopotamia, c. 2500 BC. These are more properly called wagons or carts and were still double-axled and pulled by oxen, or a hybrid of a donkey and a female onager, named Kunga in the city of Nagar which was famous for breeding them. The hybrids were used by the Eblaite, early Sumerian, Akkadian and Ur III armies. Although sometimes carrying a spearman along with the charioteer (driver), such heavy wagons, borne on solid wooden wheels and covered with skins, may have been part of the baggage train (e.g., during royal funeral processions) rather than vehicles of battle in themselves. The Sumerians had also a lighter, two-wheeled type of cart, pulled by four asses, but still with solid wheels. The spoked wheel did not appear in Mesopotamia until the mid-2000s BC.
Early Indo-Iranians.
The earliest fully developed true chariots known are from the chariot burials of the Andronovo (Timber-Grave) sites of the Sintashta-Petrovka Proto-Indo-Iranian culture in modern Russia and Kazakhstan from around 2000 BC. This culture is at least partially derived from the earlier Yamna culture. It built heavily fortified settlements, engaged in bronze metallurgy on an industrial scale and practiced complex burial rituals reminiscent of Hindu rituals known from the "Rigveda" and the "Avesta". The Sintashta-Petrovka chariot burials yield the earliest spoke-wheeled true chariots. The Andronovo culture over the next few centuries spread across the steppes from the Urals to the Tien Shan, likely corresponding to the time of early Indo-Iranian cultures.
Chariots figure prominently in Indo-Iranian mythology. Chariots are also an important part of both Hindu and Persian mythology, with most of the gods in their pantheon portrayed as riding them. The Sanskrit word for a chariot is "rátha-" (m.), which is cognate with Avestan "raθa-" (also m.), and in origin a substantivisation of the adjective Proto-Indo-European "*rot-h₂-ó-" meaning "having wheels", with the characteristic accent shift found in Indo-Iranian substantivisations. This adjective is in turn derived from the collective noun "*rot-eh₂-" "wheels", continued in Latin "rota", which belongs to the noun "*rót-o-" for "wheel" (from "*ret-" "to run") that is also found in Germanic, Celtic and Baltic (Old High German "rad" n., Old Irish "roth" m., Lithuanian "rãtas" m.).
Ancient Near East.
Some scholars argue that the chariot was most likely a product of the ancient Near East early in the 2nd millennium BC. Archaeologist Joost Crouwel writes that "Chariots were not sudden inventions, but developed out of earlier vehicles that were mounted on disk or cross-bar wheels. This development can best be traced in the Near East, where spoke-wheeled and horse-drawn ‘true’ chariots are first attested in the earlier part of the second millennium BC..." and were illustrated on a Syrian cylinder seal dated to the 18th or 17th century BC.
Hittites.
The oldest testimony of chariot warfare in the ancient Near East is the Old Hittite Anitta text (18th century BC), which mentions 40 teams of horses (in the original cuneiform spelling: 40 "ṢÍ-IM-TI" ANŠE.KUR.RAḪI.A) at the siege of Salatiwara. Since the text mentions "teams" rather than "chariots", the existence of chariots in the 18th century BC is uncertain. The first certain attestation of chariots in the Hittite empire dates to the late 17th century BC (Hattusili I). A Hittite horse-training text is attributed to Kikkuli the Mitanni (15th century BC).
The Hittites were renowned charioteers. They developed a new chariot design that had lighter wheels, with four spokes rather than eight, and that held three rather than two warriors. It could hold three warriors because the wheel was placed in the middle of the chariot and not at the back as in the Egyptian chariots. Hittite prosperity largely depended on Hittite control of trade routes and natural resources, specifically metals. As the Hittites gained dominion over Mesopotamia, tensions
flared among the neighboring Assyrians, Hurrians, and Egyptians. Under Suppiluliuma I, the Hittites conquered Kadesh and, eventually, the whole of Syria. The Battle of Kadesh in 1274 BC is likely to have been the largest chariot battle ever fought, involving some over five thousand chariots.
Egypt.
The chariot and horse were introduced to Egypt by the Hyksos invaders in the 16th century BC and undoubtedly contributed to the military success of the Egyptians. In the remains of Egyptian and Assyrian art, there are numerous representations of chariots, which display rich ornamentation. The chariots of the Egyptians and Assyrians, with whom the bow was the principal arm of attack, were richly mounted with quivers full of arrows. The Egyptians invented the yoke saddle for their chariot horses in c. 1500 BC. The best preserved examples of Egyptian chariots are the four specimens from the tomb of Tutankhamun. Chariots can be carried by two or more horses.
Persia.
The Persians succeeded Elam in the mid 1st millennium. They may have been the first to yoke four horses (rather than two) to their chariots. They also used scythed chariots. Cyrus the Younger employed these chariots in large numbers.
Herodotus mentions that the Libyans and the Indus satrapy supplied cavalry and chariots to Xerxes the Great's army. However, by this time cavalry was far more effective and agile than the chariot, and the defeat of Darius III at the Battle of Gaugamela (331 BC), where the army of Alexander simply opened their lines and let the chariots pass and attacked them from behind, marked the end of the era of chariot warfare.
Chariots in the Bible.
Chariots are frequently mentioned in the Old Testament, particularly by the prophets, as instruments of war or as symbols of power or glory. First mentioned in the story of Joseph (Genesis 50:9), "Iron chariots" are mentioned also in Joshua (17:16,18) and Judges (1:19,4:3,13) as weapons of the Canaanites. 1 Samuel 13:5 mentions chariots of the Philistines, who are sometimes identified with the Sea Peoples or early Greeks. Such examples from the KJV here include:
Jezreel (city) has been identified as the chariot base of King Ahab. And the decorated lynchpin of Sisera's chariot was identified at a site identified as his fortress Harosheth Haggoyim.
India.
Chariots figure prominently in the Rigveda, evidencing their presence in India in the 2nd millennium BC. Among Rigvedic deities, notably Ushas (the dawn) rides in a chariot, as well as Agni in his function as a messenger between gods and men.
There are some depictions of chariots among the petroglyphs in the sandstone of the Vindhya range. Two depictions of chariots are found in Morhana Pahar, Mirzapur district. One depicts a biga and the head of the driver. The second depicts a quadriga, with six-spoked wheels, and a driver standing up in a large chariot box. This chariot is being attacked. One figure, who is armed with a shield and a mace, stands in the chariot's path; another figure, who is armed with bow and arrow, threatens the right flank. It has been suggested (speculated) that the drawings record a story, most probably dating to the early centuries BC, from some center in the area of the Ganges–Yamuna plain into the territory of still Neolithic hunting tribes. The very realistic chariots carved into the Sanchi stupas are dated to roughly the 1st century.
The scythed chariot was invented by the King of Magadha, Ajatashatru around 475 BC. He used these chariots against the Licchavis. A scythed war chariot had a sharp, sickle-shaped blade or blades mounted on each end of the axle. The blades, used as weapons, extended horizontally for a metre on the sides of the chariot.
There is a chariot displayed at the AP State Archaeology Museum, Hyderabad, Telangana.
China.
The earliest archaeological evidence of chariots in China, a chariot burial site discovered in 1933 at Hougang, Anyang in Henan province, dates to the rule of King Wu Ding of the late Shang Dynasty (c. 1200 BC). Oracle bone inscriptions suggest that the western enemies of the Shang used limited numbers of chariots in battle, but the Shang themselves used them only as mobile command vehicles and in royal hunts.
During the Shang Dynasty, members of the royal family were buried with a complete household and servants, including a chariot, horses, and a charioteer. A Shang chariot was often drawn by two horses, but four-horse variants are occasionally found in burials.
Jacques Gernet claims that the Zhou dynasty, which conquered the Shang, made more use of the chariot than did the Shang and "invented a new kind of harness with four horses abreast". The crew consisted of an archer, a driver, and sometimes a third warrior who was armed with a spear or dagger-axe. From the 8th to 5th centuries BC, the Chinese use of chariots reached its peak. Although chariots appeared in greater numbers, infantry often defeated charioteers in battle.
Massed chariot warfare became all but obsolete after the Warring-States Period (476–221 BC). The main reasons were increased use of the crossbow, the adoption of standard cavalry units, and the adaptation of mounted archery from nomadic cavalry, which were more effective. Chariots would continue to serve as command posts for officers during the Qin and Han dynasties, while armored chariots were also used during the Han Dynasty against the Xiongnu Confederation in the Han–Xiongnu War, specifically at the Battle of Mobei.
Europe.
Eastern Europe.
The domestication of the horse was an important step toward civilization, and the clearest evidence of early use of the horse as a means of transport is from chariot burials dated c. 2000 BC. An increasing amount of evidence supports the hypothesis that horses were domesticated in the Eurasian Steppes (Dereivka centered in Ukraine) approximately 4000-3500 BC.
The invention of the wheel most likely took place in Europe. Evidence of wheeled vehicles appears from the mid 4th millennium BC, near-simultaneously in the Northern Caucasus (Maykop culture), Central Europe and Mesopotamia.
The earliest well-dated depiction of a wheeled vehicle (here, a wagon with two axles and four wheels), is on the Bronocice pot, a ca. 3500–3350 BC clay pot excavated in a funnelbeaker settlement in southern Poland.
In Urartu, the chariot was used by both the nobility and the military. In Erebuni (Yerevan), King Argishti of Urartu is depicted riding on a chariot which is dragged by two horses. The chariot has two wheels and each wheel has about eight spokes. This type of chariot was used around 800 BC.
Northern Europe.
The Trundholm sun chariot is dated to c. 1400 BC (see Nordic Bronze Age). The horse drawing the solar disk runs on four wheels, and the Sun itself on two. All wheels have four spokes. The "chariot" comprises the solar disk, the axle, and the wheels, and it is unclear whether the sun is depicted as the chariot or as the passenger. Nevertheless, the presence of a model of a horse-drawn vehicle on two spoked wheels in Northern Europe at such an early time is astonishing.
In addition to the Trundholm chariot, there are numerous petroglyphs from the Nordic Bronze Age that depict chariots. One petroglyph, drawn on a stone slab in a double burial from c. 1000 BC, depicts a biga with two four-spoked wheels.
The use of the composite bow in chariot warfare is not attested in northern Europe.
Central Europe and British Isles.
The Celts were famous chariot makers, and the English word "car" is believed to be derived, via Latin "carrum", from Gaulish "karros" (the English "chariot" is derived from the 13th century French "charriote"). Some 20 iron-aged chariot burials have been excavated in Britain, roughly dating from between 500 BC and 100 BC. Virtually all of them were found in East Yorkshire, with the exception of one find in 2001 in Newbridge, 10 km west of Edinburgh.
The Celtic chariot, which may have been called "karbantos" in Gaulish (compare Latin "carpentum"), was a "biga" that measured approximately 2 m (6.56 ft) in width and 4 m (13 ft) in length. The one-piece iron rim was probably a Celtic innovation. Apart from the iron rims and iron hub fittings, the chariot was constructed from wood and wicker. In some instances, iron rings reinforced the joints. Another Celtic innovation was the free-hanging axle, suspended from the platform with rope. This resulted in a much more comfortable ride on bumpy terrain. Gallic coins offer evidence of a leather 'suspension' system for the central box and a complex, knotted-cord system for the box's attachment; this has informed recent working reconstructions by archaeologists.
British chariots were open in front. Julius Caesar provides the only significant eyewitness report of British chariot warfare:
Their mode of fighting with their chariots is this: firstly, they drive about in all directions and throw their weapons and generally break the ranks of the enemy with the very dread of their horses and the noise of their wheels; and when they have worked themselves in between the troops of horse, leap from their chariots and engage on foot. The charioteers in the meantime withdraw some little distance from the battle, and so place themselves with the chariots that, if their masters are overpowered by the number of the enemy, they may have a ready retreat to their own troops. Thus they display in battle the speed of horse, [together with] the firmness of infantry; and by daily practice and exercise attain to such expertness that they are accustomed, even on a declining and steep place, to check their horses at full speed, and manage and turn them in an instant and run along the pole, and stand on the yoke, and thence betake themselves with the greatest celerity to their chariots again.
Chariots play an important role in Irish mythology surrounding the hero Cú Chulainn. The Celts in the Bronze Age used an ancient four-spoked wheel design called a sun cross or wheel cross to represent the chariot of the sun.
Chariots could also be used for ceremonial purposes. According to Tacitus ("Annals" 14.35), Boudica, queen of the Iceni and a number of other tribes in a formidable uprising against the occupying Roman forces, addressed her troops from a chariot in AD 61:
The last mention of chariot use in battle seems to be at the Battle of Mons Graupius, somewhere in modern Scotland, in AD 84. From Tacitus ("Agricola" 1.35 -36) "The plain between resounded with the noise and with the rapid movements of chariots and cavalry." The chariots did not win even their initial engagement with the Roman auxiliaries: "Meantime the enemy's cavalry had fled, and the charioteers had mingled in the engagement of the infantry."
Later through the centuries, the chariot, became commonly known as the "war wagon". The "war wagon" was a medieval development used to attack rebel or enemy forces in battle fields. The wagon was given slits for archers to shoot enemy targets, supported by infantry using pikes and flails and later for the invention of gunfire by hand-gunners; side walls were use for protection against archers, crossbowmen, the early use of gunpowder and cannon fire.
It was especially useful during the Hussite Wars, ca. 1420, by Hussite forces rebelling in Bohemia. Groups of them could form defensive works, but they also were used as hardpoints for Hussite formations or as firepower in pincer movements. This early use of gunpowder and innovative tactics helped a largely peasant infantry stave off attacks by the Holy Roman Empire's larger forces of mounted knights.
Southern Europe.
The earliest records of chariots are the arsenal inventories of the Mycenaean palaces, as described in Linear B tablets from the 15th-14th centuries BC. The tablets distinguish between "assembled" and "disassembled" chariots.
Herodotus reports that chariots were widely used in the Pontic–Caspian steppe by the Sigynnae.
The only intact Etruscan chariot dates to c. 530 BC and was uncovered as part of a chariot burial at Monteleone di Spoleto. Currently in the collection of the Metropolitan Museum of Art, it is decorated with bronze plates decorated with detailed low-relief scenes, commonly interpreted as depicting episodes from the life of Achilles. Possibly unique to Etruscan chariots, the Monteleone chariot's wheels have nine spokes. As part of a chariot burial, the Monteleone chariot may have been intended primarily for ceremonial use and may not be representative of Etruscan chariots in general.
Greece.
The classical Greeks had a (still not very effective) cavalry arm, and the rocky terrain of the Greek mainland was unsuited for wheeled vehicles. Consequently, in historical Greece the chariot was never used to any extent in war. Nevertheless, the chariot retained a high status and memories of its era were handed down in epic poetry. Linear B tablets from Mycenaean palaces record large inventories of chariots, sometimes with specific details as to how many chariots were assembled or not (i.e. stored in modular form). Later the vehicles were used in games and processions, notably for races at the Olympic and Panathenaic Games and other public festivals in ancient Greece, in "hippodromes" and in contests called "agons". They were also used in ceremonial functions, as when a "paranymph", or friend of a bridegroom, went with him in a chariot to fetch the bride home.
Greek chariots were made to be drawn by two horses attached to a central pole. If two additional horses were added, they were attached on each side of the main pair by a single bar or "trace" fastened to the front or "prow" of the chariot, as may be seen on two prize vases in the British Museum from the Panathenaic Games at Athens, Greece, in which the driver is seated with feet resting on a board hanging down in front close to the legs of the horses. The biga itself consists of a seat resting on the axle, with a rail at each side to protect the driver from the wheels. Greek chariots appear to have lacked any other attachment for the horses, which would have made turning difficult.
The body or "basket" of the chariot rested directly on the axle (called "beam") connecting the two wheels. There was no suspension, making this an uncomfortable form of transport. At the front and sides of the basket was a semicircular guard about 3 ft (1 m) high, to give some protection from enemy attack. At the back the basket was open, making it easy to mount and dismount. There was no seat, and generally only enough room for the driver and one passenger.
The central pole was probably attached to the middle of the axle, though it appears to spring from the front of the basket. At the end of the pole was the yoke, which consisted of two small saddles fitting the necks of the horses, and fastened by broad bands round the chest. Besides this the harness of each horse consisted of a bridle and a pair of reins.
The reins were mostly the same as those in use in the 19th century, and were made of leather and ornamented with studs of ivory or metal. The reins were passed through rings attached to the collar bands or yoke, and were long enough to be tied round the waist of the charioteer to allow for defence.
The wheels and basket of the chariot were usually of wood, strengthened in places with bronze or iron. They had from four to eight spokes and tires of bronze or iron. Due to the widely spaced spokes, the rim of the chariot wheel was held in tension over comparatively large spans. Whilst this provided a small measure of shock absorption, it also necessitated the removal of the wheels when the chariot was not in use, to prevent warping from continued weight bearing. Most other nations of this time had chariots of similar design to the Greeks, the chief differences being the mountings.
According to Greek mythology the chariot was invented by Erichthonius of Athens to conceal his feet, which were those of a dragon.
The most notable appearance of the chariot in Greek mythology occurs when Phaëton, the son of Helios, in an attempt to drive the chariot of the sun, managed to set the earth on fire. This story led to the archaic meaning of a "phaeton" as one who drives a chariot or coach, especially at a reckless or dangerous speed. Plato, in his "Chariot Allegory", depicted a chariot drawn by two horses, one well behaved and the other troublesome, representing opposite impulses of human nature; the task of the charioteer, representing reason, was to stop the horses from going different ways and to guide them towards enlightenment.
The Greek word for chariot, ἅρμα, "hárma", is also used nowadays to denote a tank, properly called άρμα μάχης, "árma mákhēs", literally a "combat chariot".
Rome.
The Romans probably borrowed chariot racing from the Etruscans, who would themselves have borrowed it either from the Celts or from the Greeks, but the Romans were also influenced directly by the Greeks especially after they conquered mainland Greece in 146 BC. In the Roman Empire, chariots were not used for warfare, but for chariot racing, especially in circuses, or for triumphal processions, when they could be drawn by as many as ten horses or even by dogs, tigers, or ostriches. There were four divisions, or "factiones", of charioteers, distinguished by the colour of their costumes: the red, blue, green and white teams. The main centre of chariot racing was the Circus Maximus, situated in the valley between the Palatine and Aventine Hills in Rome. The track could hold 12 chariots, and the two sides of the track were separated by a raised median termed the "spina". Chariot races continued to enjoy great popularity in Byzantine times, in the Hippodrome of Constantinople, even after the Olympic Games had been disbanded, until their decline after the Nika riots in the 6th century. The starting gates were known as the Carceres.
An ancient Roman car or chariot drawn by four horses abreast together with the horses drawing it was called a "Quadriga", from the Latin "quadrijugi" (of a team of four). The term sometimes meant instead the four horses without the chariot or the chariot alone. A three-horse chariot, or the three-horse team drawing it, was a "triga", from "trijugi" (of a team of three).
Gauge.
A popular legend that has been around since at least 1937 traces the origin of the 4 ft 8 1⁄2 in standard railroad gauge to Roman times, suggesting that it was based on the distance between the ruts of rutted roads marked by chariot wheels dating from the Roman Empire. Snopes categorized this legend as false but commented that "... it is perhaps more fairly labelled as 'True, but for trivial and unremarkable reasons.'" The historical tendency to place the wheels of horse-drawn vehicles approximately 5 ft apart probably derives from the width needed to fit a carthorse in between the shafts. In addition, while road-traveling vehicles are typically measured from the outermost portions of the wheel rims (and there is some evidence that the first railroads were measured in this way as well), it became apparent that for vehicles travelling on rails it was better to have the wheel flanges located "inside" the rails, and thus the distance measured on the inside of the wheels (and, by extension, the inside faces of the rail heads), was the important one.

</doc>
<doc id="45749" url="http://en.wikipedia.org/wiki?curid=45749" title="Peloponnese">
Peloponnese

The Peloponnese () or Peloponnesus (; Greek: Πελοπόννησος, "Pelopónnēsos"; see also list of Greek place names) is a peninsula and geographic region in southern Greece. It is separated from the central part of the country by the Gulf of Corinth. During the late Middle Ages and the Ottoman era, the peninsula was known as the Morea (Greek: Μωρέας), a name still in colloquial use in its demotic form (Μωριάς).
The peninsula is divided among three administrative regions: most belongs to the Peloponnese region, with smaller parts belonging to the West Greece and Attica regions.
It was here that the Greek War of Independence began in 1821. The Peloponnesians have almost totally dominated politics and government in Greece since then.
Geography.
The Peloponnese is a peninsula that covers an area of some 21,549.6 sqkm and constitutes the southernmost part of mainland Greece. While technically it may be considered an island since the construction of the Corinth Canal in 1893, like other peninsulas that have been separated from their mainland by man-made bodies of waters, it is rarely, if ever, referred to as an "island". It has two land connections with the rest of Greece, a natural one at the Isthmus of Corinth, and an artificial one by the Rio-Antirio bridge (completed 2004).
The peninsula has a mountainous interior and deeply indented coasts. Mount Taygetus is its highest point, at 2407 m. It possesses four south-pointing peninsulas, the Messenian, the Mani, the Cape Malea (also known as Epidaurus Limera), and the Argolid in the far northeast of the Peloponnese.
Two groups of islands lie off the Peloponnesian coast: the Argo-Saronic Islands to the east, and the Ionian to the west. The island of Kythera, off the Epidaurus Limera peninsula to the south of the Peloponnese, is considered to be part of the Ionian Islands.
History.
The peninsula has been inhabited since prehistoric times. Its modern name derives from ancient Greek mythology, specifically the legend of the hero Pelops, who was said to have conquered the entire region. The name "Peloponnesos" means "Island of Pelops".
The Mycenaean civilization, mainland Greece's (and Europe's) first major civilization, dominated the Peloponnese in the Bronze Age from its stronghold at Mycenae in the north-east of the peninsula. The Mycenean civilization collapsed suddenly at the end of the 2nd millennium BC. Archeological research has found that many of its cities and palaces show signs of destruction. The subsequent period, known as the Greek Dark Ages, is marked by an absence of written records.
In 776 BC, the first Olympic Games were held at Olympia, and this date is sometimes used to denote the beginning of the classical period of Greek antiquity. During classical antiquity, the Peloponnese was at the heart of the affairs of ancient Greece, possessed some of its most powerful city-states, and was the location of some of its bloodiest battles. The major cities of Sparta, Corinth, Argos and Megalopolis were here, and was the homeland of the Peloponnesian League. Soldiers from the peninsula fought in the Persian Wars and was the scene of the Peloponnesian War of 431–404 BC. It fell to the expanding Roman Republic in 146 BC and became the province of Achaea. During the Roman period, the peninsula remained prosperous but became a provincial backwater, relatively cut off from the affairs of the wider Roman world.
Middle Ages.
Byzantine rule and Slavic settlement.
After the partition of the Empire in 395, the Peloponnese became a part of the East Roman or Byzantine Empire. The devastation of Alaric's raid in 396–397 led to the construction of the Hexamilion wall across the Isthmus of Corinth. Through most of Late Antiquity, the peninsula retained its urbanized character: in the 6th century, Hierocles counted 26 cities in his "Synecdemus". By the latter part of that century, however, building activity seems to have stopped virtually everywhere except Constantinople, Thessalonica, Corinth and Athens. This has traditionally been attributed to calamities such as plague, earthquakes and Slavic invasions. However, more recent analysis suggests that urban decline was closely linked with the collapse of long-distance and regional commercial networks that underpinned and supported late antique urbanism in Greece, as well as with the generalized withdrawal of imperial troops and administration from the Balkans.
The scale of the Slavic incursions and settlement in the 7th and 8th centuries remains a matter of dispute. The Slavs did occupy most of the peninsula, as evidenced by the abundance of Slavic toponyms, but these toponyms accumulated over centuries rather than as a result of an initial "flood" of Slavic invasions; and many appeared to have been mediated by speakers of Greek, or in mixed Slavic-Greek compounds. Fewer Slavic toponyms appear in the eastern coast, which remained in Byzantine hands and was included in the "thema" of Hellas, established by Justinian II ca. 690. While traditional historiography has dated the arrival of Slavs to southern Greece to the late 6th century, there is no evidence for a Slavic presence in the Peloponnese until after c. 700 AD, who might have settled an otherwise depopulated landscape.
Relations between the Slavs and Greeks were probably peaceful apart from intermittent uprisings. There was a continuity of the Peloponnesian Greek population; this is especially true in Mani and Tsakonia, where Slavic incursions were minimal, or non-existent. Being agriculturalists, the Slavs probably traded with the Greeks, who remained in the towns, while Greek villages continued to exist in the interior, probably governing themselves, possibly paying tribute to the Slavs. The first attempt by the Byzantine imperial government to re-assert its control over the independent Slavic tribes of the Peloponnese occurred in 783, with the logothete Staurakios' overland campaign from Constantinople into Greece and the Peloponnese, which according to Theophanes the Confessor made many prisoners and forced the Slavs to pay tribute. From the mid-800s, following a Slavic revolt and attack on Patras, a determined Hellenization process was carried out. According to the (not always reliable) "Chronicle of Monemvasia", in 805 the Byzantine governor of Corinth went to war with the Slavs, exterminated them, and allowed the original inhabitants to claim their own lands. They regained control of the city of Patras and the region was re-settled with Greeks. Many Slavs were transported to Asia Minor, and many Asian, Sicilian and Calabrian Greeks were resettled in the Peloponnese. The entire peninsula was formed into the new "thema" of Peloponnesos, with its capital at Corinth.
The imposition of Byzantine rule over the Slavic enclaves may have largely been a process of Christianization and accommodating Slavic chieftains into the Imperial fold, as literary, epigraphic and sigillographic evidence testify to Slavic "archontes" participating in Imperial affairs. By the end of the 9th century, the Peloponnese was culturally and administratively Greek again, with the exception of a few small Slavic tribes in the mountains such as the Melingoi and Ezeritai. Although they were to remain relatively autonomous until Ottoman times, such tribes were the exception rather than the rule. Even the Melingoi and Ezeritai, however, could speak Greek and appear to have been Christian.
Apart from the troubled relations with the Slavs, the coastal regions of the Peloponnese suffered greatly from repeated Arab raids following the Arab capture of Crete in the 820s and the establishment of a corsair emirate there. After the island was recovered by Byzantium in 961 however, the region entered a period of renewed prosperity, where agriculture, commerce and urban industry flourished.
Frankish rule and Byzantine reconquest.
In 1205, following the dissolution of the Byzantine Empire by the forces of the Fourth Crusade, the Crusaders under William of Champlitte and Geoffrey of Villehardouin marched south through mainland Greece and conquered the Peloponnese against sporadic local Greek resistance. The Franks then founded the Principality of Achaea, nominally a vassal of the Latin Empire, while the Venetians occupied a number of strategically important ports around the coast such as Navarino and Coron, which they retained into the 15th century. The Franks popularized the name "Morea" for the peninsula, which first appears as the name of a small bishopric in Elis during the 10th century. Its etymology is disputed, but it is most commonly held to be derived from the mulberry tree ("morea"), whose leaves are similar in shape to the peninsula.
Frankish supremacy in the peninsula however received a critical blow after the Battle of Pelagonia, when William II of Villehardouin was forced to cede the newly constructed fortress and palace at Mystras near ancient Sparta to a resurgent Byzantium. This Greek province (and later a semi-autonomous Despotate) staged a gradual reconquest, eventually conquering the Frankish principality by 1430. The same period was also marked by the influx of Albanian settlers to Central Greece and the Peloponnese, who became the ancestors of the Arvanites.
The Ottoman Turks began raiding the Peloponnese from ca. 1358, but their raids intensified only after 1387, when the energetic Evrenos Bey took control. Exploiting the quarrels between Byzantines and Franks, he plundered across the peninsula and forced both the Byzantine despots and the remaining Frankish rulers to acknowledge Ottoman suzerainty and pay tribute. This situation lasted until the Ottoman defeat at the Battle of Ankara in 1402, after which Ottoman power was for a time checked. Ottoman incursions into the Morea resumed under Turahan Bey after 1423. Despite the reconstruction of the Hexamilion wall at the Isthmus of Corinth, the Ottomans under Murad II breached it in 1446, forcing the Despots of the Morea to re-acknowledge Ottoman suzerainty, and again under Turahan in 1452 and 1456. Following the occupation of the Duchy of Athens in 1456, the Ottomans occupied a third of the Peloponnese in 1458, and Sultan Mehmed II extinguished the remnants of the Despotate in 1460. The last Byzantine stronghold, Salmeniko Castle, under its commander Graitzas Palaiologos, held out until July 1461. Only the Venetian fortresses of Modon, Coron, Navarino, Monemvasia, Argos and Nauplion escaped Ottoman control.
Ottoman conquest, Venetian interlude and Ottoman reconquest.
The Venetian fortresses were conquered in a series of Ottoman–Venetian Wars: the first war, lasting from 1463 to 1479, saw much fighting in the Peloponnese, resulting in the loss of Argos, while Modon and Coron fell in 1500 during the second war. Coron and Patras were captured in a crusading expedition in 1532, led by the Genoese admiral Andrea Doria, but this provoked another war in which the last Venetian possessions on the Greek mainland were lost.
Following the Ottoman conquest, the peninsula was made into a province ("sanjak"), with 109 "ziamets" and 342 "timars". During the first period of Ottoman rule (1460–1687), the capital was first in Corinth (Turk. "Gördes"), later in Leontari ("Londari"), Mystras ("Misistire") and finally in Nauplion (Tr. "Anaboli"). Sometime in the mid-17th century, the Morea became the centre of a separate "eyalet", with Patras ("Ballibadra") as its capital. Until the death of Suleyman the Magnificent in 1570, the Christian population (counted at some 42,000 families ca. 1550) managed to retain some privileges and Islamization was slow, mostly among the Albanians or the estate owners who were integrated into the Ottoman feudal system. Although they quickly came to control most of the fertile lands, Muslims remained a distinct minority. Christian communities retained a large measure of self-government, but the entire Ottoman period was marked by a flight of the Christian population from the plains to the mountains. This occasioned the rise of the "klepht"s, armed brigands and rebels, in the mountains, as well as the corresponding institution of the government-funded "armatoloi" to check the "klephts"‍ '​ activities.
With the outbreak of the "Great Turkish War" in 1683, the Venetians under Francesco Morosini occupied the entire peninsula by 1687, and received recognition by the Ottomans in the Treaty of Karlowitz (1699). The Venetians established their province as the "Kingdom of the Morea" (It. "Regno di Morea"), but their rule proved unpopular, and when the Ottomans invaded the peninsula in 1715, most local Greeks welcomed them. The Ottoman reconquest was easy and swift, and was recognized by Venice in the Treaty of Passarowitz in 1718.
The Peloponnese now became the core of the Morea Eyalet, headed by the "Mora valesi", who until 1780 was a pasha of the first rank (with three horsetails) and held the title of vizier. After 1780 and until the Greek War of Independence, the province was headed by a "muhassil". The pasha of the Morea was aided by a number of subordinate officials, including a Christian translator ("dragoman"), who was the senior Christian official of the province. As during the first Ottoman period, the Morea was divided into 22 districts or beyliks. The capital was first at Nauplion, but after 1786 at Tripolitza (Tr. "Trabliçe").
The Moreot Christians rose against the Ottomans with Russian aid during the so-called "Orlov Revolt" of 1770, but it was swiftly and brutally suppressed. As a result, the total population decreased during this time, while the Muslim element in it increased. Nevertheless, through the privileges granted with the Treaty of Kuchuk-Kainarji, especially the right for the Christians to trade under the Russian flag, led to a considerable economic flowering of the local Greeks, which, coupled with the increased cultural contacts with Western Europe (Modern Greek Enlightenment) and the inspiring ideals of the French Revolution, laid the groundwork for the Greek War of Independence.
Modern Greece.
The Peloponnesians played a major role in the Greek War of Independence – the war actually began in the Peloponnese, when rebels took control of Kalamata on March 23, 1821. Greek control over the peninsula, with the exception of a few coastal forts, was established with the capture of Tripolitsa in September 1821. The peninsula was the scene of fierce fighting and extensive devastation following the arrival of Egyptian troops under Ibrahim pasha in 1825. The decisive naval Battle of Navarino was fought off Pylos on the west coast of the Peloponnese, and a French expeditionary corps cleared the last Turko-Egyptian forces from the peninsula in 1828. The city of Nafplion, on the east coast of the peninsula, became the first capital of the independent Greek state.
During the 19th and early 20th century, the region became relatively poor and economically isolated. A significant part of its population emigrated to the larger cities of Greece, especially Athens, and other countries such as the United States and Australia. It was badly affected by the Second World War and Greek Civil War, experiencing some of the worst atrocities committed in Greece during those conflicts. Living standards improved dramatically throughout Greece after the country's accession to the European Union in 1981. The rural Peloponnese is renowned for being among the most traditionalist and conservative regions of Greece and is a stronghold of the right-wing New Democracy party, while the larger urban centres like Kalamata and especially Patras are dominated by the left-wing Panhellenic Socialist Movement. Villages still continue to see a population decline due the lack of economic opportunities, industrial farming, and the aging population. Despite the relative poverty of the region itself however, the Peloponnesians have always had an almost total dominance of politics and government in Greece; since Greek independence in the 1820s, the vast majority of Prime Ministers have been of Peloponnesian origin, and the most powerful political families (Zaimis, Mavromichalis, Varvitsiotis, Stephanopoulos and Papandreou) hail from the region. The former Prime Minister Antonis Samaras is a Peloponnesian; the business elite of Greece is also mostly Peloponnesian, with the Angelopoulos and Latsis families being a typical example, while the Maniots of Southern Peloponnese traditionally dominate the Armed Forces. All this has gained the Peloponnesians a reputation for cunning and political connections in Greek popular culture.
In late August 2007, large parts of Peloponnese suffered from wildfires, which caused severe damage in villages and forests and the death of 77 people. The impact of the fires to the environment and economy of the region are still unknown. It is thought to be the largest environmental disaster in modern Greek history.
Cities.
The principal modern cities of the Peloponnese are (2011 census):
Archaeological sites.
The Peloponnese possesses many important archaeological sites dating from the Bronze Age through to the Middle Ages. Among the most notable are:
Cuisine.
Specialities of the region:

</doc>
<doc id="45752" url="http://en.wikipedia.org/wiki?curid=45752" title="Topological vector space">
Topological vector space

In mathematics, a topological vector space (also called a linear topological space) is one of the basic structures investigated in functional analysis. As the name suggests the space blends a topological structure (a uniform structure to be precise) with the algebraic concept of a vector space.
The elements of topological vector spaces are typically functions or linear operators acting on topological vector spaces, and the topology is often defined so as to capture a particular notion of convergence of sequences of functions.
Hilbert spaces and Banach spaces are well-known examples.
Unless stated otherwise, the underlying field of a topological vector space is assumed to be either the complex numbers C or the real numbers R.
Definition.
A topological vector space "X" is a vector space over a topological field K (most often the real or complex numbers with their standard topologies) that is endowed with a topology such that vector addition "X" × "X" → "X" and scalar multiplication K × "X" → "X" are continuous functions (where the domains of these functions are endowed with product topologies).
Some authors (e.g., Rudin) require the topology on "X" to be T1; it then follows that the space is Hausdorff, and even Tychonoff. The topological and linear algebraic structures can be tied together even more closely with additional assumptions, the most common of which are listed below.
The category of topological vector spaces over a given topological field K is commonly denoted TVSK or TVectK. The objects are the topological vector spaces over K and the morphisms are the continuous K-linear maps from one object to another.
Examples.
Every normed vector space has a natural topological structure: the norm induces a metric and the metric induces a topology. This is a topological vector space because:
Therefore, all Banach spaces and Hilbert spaces, are examples of topological vector spaces.
There are topological vector spaces whose topology is not induced by a norm, but are still of interest in analysis. Examples of such spaces are spaces of holomorphic functions on an open domain, spaces of infinitely differentiable functions, the Schwartz spaces, and spaces of test functions and the spaces of distributions on them. These are all examples of Montel spaces. On the other hand, infinite-dimensional Montel spaces are never normable.
A topological field is a topological vector space over each of its subfields.
Product vector spaces.
A cartesian product of a family of topological vector spaces, when endowed with the product topology, is a topological vector space. For instance, the set "X" of all functions "f" : R → R: this set "X" can be identified with the product space RR and carries a natural product topology. With this topology, "X" becomes a topological vector space, called the "space of pointwise convergence". The reason for this name is the following: if ("f""n") is a sequence of elements in "X", then "f""n" has limit "f" in "X" if and only if "f""n"("x") has limit "f"("x") for every real number "x". This space is complete, but not normable: indeed, every neighborhood of 0 in the product topology contains lines, "i.e.", sets K "f" for "f" ≠ 0.
Topological structure.
A vector space is an abelian group with respect to the operation of addition, and in a topological vector space the inverse operation is always continuous (since it is the same as multiplication by −1). Hence, every topological vector space is an abelian topological group.
Let "X" be a topological vector space. Given a subspace "M" ⊂ "X", the quotient space "X/M" with the usual quotient topology is a Hausdorff topological vector space if and only if "M" is closed. This permits the following construction: given a topological vector space "X" (that is probably not Hausdorff), form the quotient space "X / M" where "M" is the closure of {0}. "X / M" is then a Hausdorff vector topological space that can be studied instead of "X".
In particular, topological vector spaces are uniform spaces and one can thus talk about completeness, uniform convergence and uniform continuity. (This implies that every Hausdorff topological vector space is completely regular.) The vector space operations of addition and scalar multiplication are actually uniformly continuous. Because of this, every topological vector space can be completed and is thus a dense linear subspace of a complete topological vector space.
A topological vector space is said to be "normable" if its topology can be induced by a norm. A topological vector space is normable if and only if it is Hausdorff and has a convex bounded neighbourhood of 0.
If a topological vector space is semi-metrizable, that is the topology can be given by a semi-metric, then the semi-metric can be chosen to be translation invariant. Also, a topological vector space is metrizable if and only if it is Hausdorff and has a countable local base (i.e., a neighborhood base at the origin).
A linear operator between two topological vector spaces which is continuous at one point is continuous on the whole domain. Moreover, a linear operator "f" is continuous if "f(V)" is bounded for some neighborhood "V" of 0.
A hyperplane on a topological vector space "X" is either dense or closed. A linear functional "f" on a topological vector space "X" has either dense or closed kernel. Moreover, "f" is continuous if and only if its kernel is closed.
Every Hausdorff finite-dimensional topological vector space is isomorphic to K"n" for some topological field K. In particular, a Hausdorff topological vector space is finite-dimensional if and only if it is locally compact.
Local notions.
A subset "E"  of a topological vector space "X"  is said to be
The definition of boundedness can be weakened a bit; "E" is bounded if and only if every countable subset of it is bounded. Also, "E" is bounded if and only if for every balanced neighborhood "V" of 0, there exists "t" such that "E" ⊂ "tV". Moreover, when "X" is locally convex, the boundedness can be characterized by seminorms: the subset "E" is bounded iff every continuous semi-norm "p" is bounded on "E".
Every topological vector space has a local base of absorbing and balanced sets.
A sequence {"xn"} is said to be Cauchy if for every neighborhood "V" of 0, the difference "xm" − "xn" belongs to "V" when "m" and "n" are sufficiently large. Every Cauchy sequence is bounded, although Cauchy nets or Cauchy filters may not be bounded. A topological vector space where every Cauchy sequence converges is sequentially complete but may not be complete (in the sense Cauchy filters converge). Every compact set is bounded.
Types.
Depending on the application additional constraints are usually enforced on the topological structure of the space. In fact, several principal results in functional analysis fail to hold in general for topological vector spaces: the closed graph theorem, the open mapping theorem, and the fact that the dual space of the space separates points in the space.
Below are some common topological vector spaces, roughly ordered by their "niceness".
Dual space.
Every topological vector space has a continuous dual space—the set "V*" of all continuous linear functionals, i.e. continuous linear maps from the space into the base field K. A topology on the dual can be defined to be the coarsest topology such that the dual pairing each point evaluation "V*" → K is continuous. This turns the dual into a locally convex topological vector space. This topology is called the weak-* topology. This may not be the only natural topology on the dual space; for instance, the dual of a normed space has a natural norm defined on it. However, it is very important in applications because of its compactness properties (see Banach–Alaoglu theorem).
Caution: Whenever "V" is a not-normable locally convex space, then the pairing map "V*" × "V" → K is never continuous, no matter which vector space topology one chooses on "V*".

</doc>
<doc id="45754" url="http://en.wikipedia.org/wiki?curid=45754" title="Where Mathematics Comes From">
Where Mathematics Comes From

Where Mathematics Comes From: How the Embodied Mind Brings Mathematics into Being (hereinafter "WMCF") is a book by George Lakoff, a cognitive linguist, and Rafael E. Núñez, a psychologist. Published in 2000, "WMCF" seeks to found a cognitive science of mathematics, a theory of embodied mathematics based on conceptual metaphor.
"WMCF" definition of mathematics.
Mathematics makes up that part of the human conceptual system that is special in the following way:
Nikolay Lobachevsky said "There is no branch of mathematics, however abstract, which may not some day be applied to phenomena of the real world." A common type of conceptual blending process would seem to apply to the entire mathematical procession. Pythagoras is alleged to have said "All is number."
Human cognition and mathematics.
Lakoff and Núñez's avowed purpose is to begin laying the foundations for a truly scientific understanding of mathematics, one grounded in processes common to all human cognition. They find that four distinct but related processes metaphorically structure basic arithmetic: object collection, object construction, using a measuring stick, and moving along a path.
"WMCF" builds on earlier books by Lakoff (1987) and Lakoff and Johnson (1980, 1999), which analyze such concepts of metaphor and image schemata from second-generation cognitive science. Some of the concepts in these earlier books, such as the interesting technical ideas in Lakoff (1987), are absent from "WMCF".
Lakoff and Núñez hold that mathematics results from the human cognitive apparatus and must therefore be understood in cognitive terms. "WMCF" advocates (and includes some examples of) a "cognitive idea analysis" of mathematics which analyzes mathematical ideas in terms of the human experiences, metaphors, generalizations, and other cognitive mechanisms giving rise to them. A standard mathematical education does not develop such idea analysis techniques because it does not pursue considerations of A) what structures of the mind allow it to do mathematics or B) the philosophy of mathematics.
Lakoff and Núñez start by reviewing the psychological literature, concluding that human beings appear to have an innate ability, called subitizing, to count, add, and subtract up to about 4 or 5. They document this conclusion by reviewing the literature, published in recent decades, describing experiments with infant subjects. For example, infants quickly become excited or curious when presented with "impossible" situations, such as having three toys appear when only two were initially present.
The authors argue that mathematics goes far beyond this very elementary level due to a large number of metaphorical constructions. For example, they argue that the Pythagorean position that all is number, and the associated crisis of confidence that came about with the discovery of the irrationality of the square root of two, arises solely from a metaphorical relation between the length of the diagonal of a square, and the possible numbers of objects.
Much of "WMCF" deals with the important concepts of infinity and of limit processes, seeking to explain how finite humans living in a finite world could ultimately conceive of the actual infinite. Thus much of "WMCF" is, in effect, a study of the epistemological foundations of the calculus. Lakoff and Núñez conclude that while the potential infinite is not metaphorical, the actual infinite is. Moreover, they deem all manifestations of actual infinity to be instances of what they call the "Basic Metaphor of Infinity", as represented by the ever-increasing sequence 1, 2, 3, ...
"WMCF" emphatically rejects the Platonistic philosophy of mathematics. They emphasize that all we know and can ever know is "human mathematics", the mathematics arising from the human intellect. The question of whether there is a "transcendent" mathematics independent of human thought is a meaningless question. That is like asking if colors are transcendent of human thought- colors are only varying wavelengths of light, it is our interpretation of physical stimuli that make them colors.
"WMCF" (p. 81) likewise criticizes the emphasis mathematicians place on the concept of closure. Lakoff and Núñez argue that the expectation of closure is an artifact of the human mind's ability to relate fundamentally different concepts via metaphor.
"WMCF" concerns itself mainly with proposing and establishing an alternative view of mathematics, one grounding the field in the realities of human biology and experience. It is not a work of technical mathematics or philosophy. Lakoff and Núñez are not the first to argue that conventional approaches to the philosophy of mathematics are flawed. For example, they do not seem all that familiar with the content of Davis and Hersh (1981), even though "WMCF" warmly acknowledges Reuben Hersh's support.
Lakoff and Núñez cite Saunders Mac Lane (the inventor, with Samuel Eilenberg, of category theory) in support of their position. Mac Lane (1986), an overview of mathematics intended for philosophers, proposes that mathematical concepts are ultimately grounded in ordinary human activities, mostly interactions with the physical world. See From Action to Mathematics per Mac Lane.
Educators have taken some interest in what "WMCF" suggests about how mathematics is learned, and why students find some elementary concepts more difficult than others.
Examples of mathematical metaphors.
Conceptual metaphors described in "WMCF", in addition to the Basic Metaphor of Infinity, include:
Mathematical reasoning requires variables ranging over some universe of discourse, so that we can reason about generalities rather than merely about particulars. "WMCF" argues that reasoning with such variables implicitly relies on what it terms the Fundamental Metonymy of Algebra.
Example of metaphorical ambiguity.
"WMCF" (p. 151) includes the following example of what the authors term "metaphorical ambiguity." Take the set formula_1. Then recall two bits of standard terminology from elementary set theory:
By (1), "A" is the set {1,2}. But (1) and (2) together say that "A" is also the ordered pair (0,1). Both statements cannot be correct; the ordered pair (0,1) and the unordered pair {1,2} are fully distinct concepts. Lakoff and Johnson (1999) term this situation "metaphorically ambiguous." This simple example calls into question any Platonistic foundations for mathematics.
While (1) and (2) above are admittedly canonical, especially within the consensus set theory known as the Zermelo–Fraenkel axiomatization, "WMCF" does not let on that they are but one of several definitions that have been proposed since the dawning of set theory. For example, Frege, "Principia Mathematica", and New Foundations (a body of axiomatic set theory begun by Quine in 1937) define cardinals and ordinals as equivalence classes under the relations of equinumerosity and similarity, so that this conundrum does not arise. In Quinian set theory, "A" is simply an instance of the number 2. For technical reasons, defining the ordered pair as in (2) above is awkward in Quinian set theory. Two solutions have been proposed:
Criticism.
In set theories such as Zermelo–Fraenkel one can indeed have {1,2} = (0,1), as these are two different symbols denoting the same object. The claim that there is an anomaly because these are "fully distinct concepts" is on the one hand not a clear scientific statement, and on the other hand, is on par with such statements as: ""The positive real solution of formula_5" and "formula_6" cannot be equal because they are fully distinct concepts.".
The apparent anomaly stems from the fact that Lakoff and Núñez identify mathematical objects with their various particular realizations. There are several equivalent definitions of ordered pair, and most mathematicians do not identify the ordered pair with just one of these definitions (since this would be an arbitrary and artificial choice), but view the definitions as equivalent models or realizations of the same underlying object. The existence of several different but equivalent constructions of certain mathematical objects supports the platonistic view that the mathematical objects exist beyond their various linguistical, symbolical, or conceptual representations.
As an example, many mathematicians would favour a definition of ordered pair in terms of category theory where the object in question is defined in terms of a characteristic universal property and then shown to be unique up to isomorphism (this was recently mentioned in an article on mathematical platonism by David Mumford).
The above discussion is meant to explain that the most natural and fruitful approach in mathematics is to view a mathematical object as having potentially several different but equivalent realizations. On the other hand, the object is not identified with just one of these realizations. This suggests that the intuitionistic idea that mathematical objects exist only as specific mental constructions, or the idea of Lakoff and Núñez that mathematical objects exist only as "particular instances" of concepts/metaphors in our embodied brains, is an inadequate philosophical basis to account for the experience and de facto research methods of working mathematicians. Perhaps this is a reason why these ideas have been met with comparatively little interest by the mathematical community.
The Romance of Mathematics.
The "Romance of Mathematics" is "WMCF"'s light-hearted term for a perennial philosophical viewpoint about mathematics which the authors describe and then dismiss as an intellectual myth:
It is very much an open question whether "WMCF" will eventually prove to be the start of a new school in the philosophy of mathematics. Hence the main value of "WMCF" so far may be a critical one: its critique of Platonism in mathematics, and the Romance of Mathematics.
Critical response.
Many working mathematicians resist the approach and conclusions of Lakoff and Núñez. by mathematicians of "WMCF" in professional journals, while often respectful of its focus on conceptual strategies and metaphors as paths for understanding mathematics, have taken exception to some of the "WMCF"'s philosophical arguments on the grounds that mathematical statements have lasting 'objective' meanings. For example, Fermat's last theorem means exactly what it meant when Fermat initially proposed it 1664. Other reviewers have pointed out that multiple conceptual strategies can be employed in connection with the same mathematically defined term, often by the same person (a point that is compatible with the view that we routinely understand the 'same' concept with different metaphors). The metaphor and the conceptual strategy are not the same as the formal definition which mathematicians employ. However, "WMCF" points out that formal definitions are built using words and symbols that have meaning only in terms of human experience.
Critiques of "WMCF" include the humorous:
and the physically informed:
Lakoff made his reputation by linking linguistics to cognitive science and the analysis of metaphor. Núñez, educated in Switzerland, is a product of Jean Piaget's school of cognitive psychology as a basis for logic and mathematics. Núñez has thought much about the foundations of real analysis, the real and complex numbers, and the Basic Metaphor of Infinity. These topics, however, worthy though they be, form part of the superstructure of mathematics. Cognitive science should take more interest in the foundations of mathematics. And indeed, the authors do pay a fair bit of attention early on to logic, Boolean algebra and the Zermelo–Fraenkel axioms, even lingering a bit over group theory. But neither author is well-trained in logic (there is no index entry for "quantifier" or "quantification"), the philosophy of set theory, the axiomatic method, metamathematics, and model theory. Nor does "WMCF" say enough about the derivation of number systems (the Peano axioms go unmentioned), abstract algebra, equivalence and order relations, mereology, topology, and geometry.
Lakoff and Núñez tend to dismiss the negative opinions mathematicians have expressed about "WMCF", because their critics do not appreciate the insights of cognitive science. Lakoff and Núñez maintain that their argument can only be understood using the discoveries of recent decades about the way human brains process language and meaning. They argue that any arguments or criticisms that are not grounded in this understanding cannot address the content of the book.
It has been pointed out that it is not at all clear that "WMCF" establishes that the claim "intelligent alien life would have mathematical ability" is a myth. To do this, it would be required to show that intelligence and mathematical ability are separable, and this has not been done. On Earth, intelligence and mathematical ability seem to go hand in hand in all life-forms, as pointed out by Keith Devlin among others. The authors of "WMCF" have not explained how this situation would (or even could) be different anywhere else.
Lakoff and Núñez also appear not to appreciate the extent to which intuitionists and constructivists have anticipated their attack on the Romance of (Platonic) Mathematics. Brouwer, the founder of the intuitionist/constructivist point of view, in his dissertation, "On the Foundation of Mathematics", argued that mathematics was a mental construction, a free creation of the mind and totally independent of logic and language. He goes on to upbraid the formalists for building verbal structures that are studied without intuitive interpretation. Symbolic language should not be confused with mathematics; it reflects, but does not contain, mathematical reality.
Summing up.
"WMCF" (pp. 378–79) concludes with some key points, a number of which follow. Mathematics arises from our bodies and brains, our everyday experiences, and the concerns of human societies and cultures. It is:
The cognitive approach to formal systems, as described and implemented in "WMCF", need not be confined to mathematics, but should also prove fruitful when applied to formal logic, and to formal philosophy such as Edward Zalta's . Lakoff and Johnson (1999) fruitfully employ the cognitive approach to rethink a good deal of the philosophy of mind, epistemology, metaphysics, and the history of ideas.

</doc>
<doc id="45756" url="http://en.wikipedia.org/wiki?curid=45756" title="Pyrite">
Pyrite

The mineral pyrite, or iron pyrite, also known as fool's gold, is an iron sulfide with the chemical formula FeS2. This mineral's metallic luster and pale brass-yellow hue give it a superficial resemblance to gold, hence the well-known nickname of "fool's gold". The color has also led to the nicknames "brass", "brazzle", and "Brazil", primarily used to refer to pyrite found in coal.
Pyrite is the most common of the sulfide minerals. The name pyrite is derived from the Greek πυρίτης ("pyritēs"), "of fire" or "in fire", in turn from πύρ ("pyr"), "fire". In ancient Roman times, this name was applied to several types of stone that would create sparks when struck against steel; Pliny the Elder described one of them as being brassy, almost certainly a reference to what we now call pyrite.
By Georgius Agricola's time, ca. 1550, the term had become a generic term for all of the sulfide minerals.
Pyrite is usually found associated with other sulfides or oxides in quartz veins, sedimentary rock, and metamorphic rock, as well as in coal beds and as a replacement mineral in fossils. Despite being nicknamed fool's gold, pyrite is sometimes found in association with small quantities of gold. Gold and arsenic occur as a coupled substitution in the pyrite structure. In the Carlin–type gold deposits, arsenian pyrite contains up to 0.37 wt% gold.
Uses.
Pyrite enjoyed brief popularity in the 16th and 17th centuries as a source of ignition in early firearms, most notably the wheellock, where the cock held a lump of pyrite against a circular file to strike the sparks needed to fire the gun.
Pyrite has been used since classical times to manufacture "copperas", that is, iron(II) sulfate. Iron pyrite was heaped up and allowed to weather (an example of an early form of heap leaching). The acidic runoff from the heap was then boiled with iron to produce iron sulfate. In the 15th century, such leaching began to replace the burning of sulfur as a source of sulfuric acid. By the 19th century, it had become the dominant method.
Pyrite remains in commercial use for the production of sulfur dioxide, for use in such applications as the paper industry, and in the manufacture of sulfuric acid. Thermal decomposition of pyrite into FeS (iron(II) sulfide) and elemental sulfur starts at 550 °C; at around 700 °C pS2 is about 1 atm.
A newer commercial use for pyrite is as the cathode material in Energizer brand non-rechargeable lithium batteries.
Pyrite is a semiconductor material with a band gap of 0.95 eV.
During the early years of the 20th century, pyrite was used as a mineral detector in radio receivers, and is still used by 'crystal radio' hobbyists. Until the vacuum tube matured, the crystal detector was the most sensitive and dependable detector available – with considerable variation between mineral types and even individual samples within a particular type of mineral. Pyrite detectors occupied a midway point between galena detectors and the more mechanically complicated perikon mineral pairs. Pyrite detectors can be as sensitive as a modern 1N34A germanium diode detector.
Pyrite has been proposed as an abundant, inexpensive material in low cost photovoltaic solar panels. Synthetic iron sulfide was used with copper sulfide to create the photovoltaic material.
Pyrite is used to make marcasite jewelry. Marcasite jewelry, made from small faceted pieces of pyrite, often set in silver, was known since ancient times and was popular in the Victorian era. At the time when the term became common in jewelry making, "marcasite" referred to all iron sulfides including pyrite, and not to the orthorhombic FeS2 mineral marcasite which is lighter in color, brittle and chemically unstable, and thus not suitable for jewelry making. Marcasite jewelry does not actually contain the mineral marcasite.
Formal oxidation states for pyrite, marcasite, and arsenopyrite.
From the perspective of classical inorganic chemistry, which assigns formal oxidation states to each atom, pyrite is probably best described as Fe2+S22−. This formalism recognizes that the sulfur atoms in pyrite occur in pairs with clear S–S bonds. These persulfide units can be viewed as derived from hydrogen disulfide, H2S2. Thus pyrite would be more descriptively called iron persulfide, not iron disulfide. In contrast, molybdenite, MoS2, features isolated sulfide (S2−) centers and the oxidation state of molybdenum is Mo4+. The mineral arsenopyrite has the formula FeAsS. Whereas pyrite has S2 subunits, arsenopyrite has [AsS] units, formally derived from deprotonation of H2AsSH. Analysis of classical oxidation states would recommend the description of arsenopyrite as Fe3+[AsS]3−.
Crystallography.
Iron-pyrite FeS2 represents the prototype compound of the crystallographic pyrite structure. The structure is simple cubic and was among the first crystal structures solved by X-ray diffraction. It belongs to the crystallographic space group "Pa"3 and is denoted by the Strukturbericht notation C2. Under thermodynamic standard conditions the lattice constant formula_1 of stoichiometric iron pyrite FeS2 amounts to 541.87 pm. The unit cell is composed of a Fe face-centered cubic sublattice into which the S ions are embedded. The pyrite structure is also used by other compounds "MX"2 of transition metals "M" and chalcogens "X" = O, S, Se and Te. Also certain dipnictides with "X" standing for P, As and Sb etc. are known to adopt the pyrite structure.
In the first bonding sphere, the Fe atoms are surrounded by six S nearest neighbours, in a distorted octahedral arrangement. The material is a diamagnetic semiconductor and the Fe ions should be considered to be in a "low spin" divalent state (as shown by Mössbauer spectroscopy as well as XPS), rather than a tetravalent state as the stoichiometry would suggest.
The positions of "X" ions in the pyrite structure may be derived from the fluorite structure, starting from a hypothetical Fe2+(S−)2 structure. Whereas F− ions in CaF2 occupy the centre positions of the eight subcubes of the cubic unit cell (¼ ¼ ¼) etc., the S− ions in FeS2 are shifted from these high symmetry positions along <111> axes to reside on ("uuu") and symmetry-equivalent positions. Here, the parameter "u" should be regarded as a free atomic parameter that takes different values in different pyrite-structure compounds (iron pyrite FeS2: "u"(S) = 0.385 ). The shift from fluorite "u" = 0.25 to pyrite "u" = 0.385 is rather large and creates a S-S distance that is clearly a binding one. This is not surprising as in contrast to F− an ion S− is not a closed shell species. It is isoelectronic with a chlorine "atom", also undergoing pairing to form Cl2 molecules. Both low spin Fe2+ and the disulfide S22− moeties are closed shell entities, explaining the diamagnetic and semiconducting properties.
The S atoms have bonds with three Fe and one other S atom. The site symmetry at Fe and S positions is accounted for by point symmetry groups "C"3"i" and "C"3, respectively. The missing center of inversion at S lattice sites has important consequences for the crystallographic and physical properties of iron pyrite. These consequences derive from the crystal electric field active at the sulfur lattice site, which causes a polarisation of S ions in the pyrite lattice. The polarisation can be calculated on the basis of higher-order Madelung constants and has to be included in the calculation of the lattice energy by using a generalised Born–Haber cycle. This reflects the fact that the covalent bond in the sulfur pair is inadequately accounted for by a strictly ionic treatment.
Arsenopyrite has a related structure with heteroatomic As-S pairs rather than homoatomic ones. Marcasite also possesses homoatomic anion pairs, but the arrangement of the metal and diatomic anions is different from that of pyrite. Despite its name a chalcopyrite does not contain dianion pairs, but single S2− sulfide anions.
Crystal habit.
Pyrite usually forms cuboid crystals, sometimes forming in close association to form raspberry-like framboids. However, under certain circumstances, it can form anastamozing filaments or T-shaped crystals.
Pyrite can also form dodecahedral crystals and this suggests an explanation for the artificial geometrical models found in Europe as early as the 5th century BC.
Varieties.
Cattierite (CoS2) and vaesite (NiS2) are similar in their structure and belong also to the pyrite group.
Bravoite is a nickel-cobalt bearing variety of pyrite, with > 50% substitution of Ni2+ for Fe2+ within pyrite. Bravoite is not a formally recognised mineral, and is named after Peruvian scientist Jose J. Bravo (1874–1928).
Distinguishing similar minerals.
It is distinguishable from native gold by its hardness, brittleness and crystal form. Natural gold tends to be anhedral (irregularly shaped), whereas pyrite comes as either cubes or multifaceted crystals. Chalcopyrite is brighter yellow with a greenish hue when wet and is softer (3.5–4 on Mohs' scale). Arsenopyrite is silver white and does not become more yellow when wet.
Hazards.
Iron pyrite is unstable in the natural environment: in nature it is always being created or being destroyed. Iron pyrite exposed to air and water decomposes into iron oxides and sulfate. This process is hastened by the action of "Acidithiobacillus" bacteria which oxidize the pyrite to produce ferrous iron and sulfate. These reactions occur more rapidly when the pyrite is in fine crystals and dust, which is the form it takes in most mining operations.
Acid drainage.
Sulfate released from decomposing pyrite combines with water, producing sulfuric acid, leading to acid rock drainage and potentially acid rain.
Dust explosions.
Pyrite oxidation is sufficiently exothermic that underground coal mines in high-sulfur coal seams have occasionally had serious problems with spontaneous combustion in the mined-out areas of the mine. The solution is to hermetically seal the mined-out areas to exclude oxygen.
In modern coal mines, limestone dust is sprayed onto the exposed coal surfaces to reduce the hazard of dust explosions. This has the secondary benefit of neutralizing the acid released by pyrite oxidation and therefore slowing the oxidation cycle described above, thus reducing the likelihood of spontaneous combustion. In the long term, however, oxidation continues, and the hydrated sulfates formed may exert crystallization pressure that can expand cracks in the rock and lead eventually to roof fall.
Weakened building materials.
Building stone containing pyrite tends to stain brown as the pyrite oxidizes. This problem appears to be significantly worse if any marcasite is present. The presence of pyrite in the aggregate used to make concrete can lead to severe deterioration as the pyrite oxidizes. In early 2009, problems with Chinese drywall imported into the United States after Hurricane Katrina were attributed to oxidation of pyrite. In the United States, in Canada, and more recently in Ireland, where it was used as underfloor infill, pyrite contamination has caused major structural damage. Modern tests for aggregate materials certify such materials as free of pyrite.
Pyritised fossils.
Pyrite and marcasite commonly occur as replacement pseudomorphs after fossils in black shale and other sedimentary rocks formed under reducing environmental conditions. 
However, "pyrite dollars" or "pyrite suns" which have an appearance similar to sand dollars are pseudofossils and lack the pentagonal symmetry of the animal.

</doc>
<doc id="45761" url="http://en.wikipedia.org/wiki?curid=45761" title="Fushimi">
Fushimi

Fushimi may refer to:

</doc>
<doc id="45762" url="http://en.wikipedia.org/wiki?curid=45762" title="Emperor Go-Fushimi">
Emperor Go-Fushimi

Emperor Go-Fushimi (後伏見天皇 "Go-Fushimi-tennō") (April 5, 1288 – May 17, 1336) was the 93rd emperor of Japan, according to the traditional order of succession. His reign spanned the years from 1298 to 1301.
This 13th-century sovereign was named after his father, Emperor Fushimi and "go-" (後), translates literally as "later"; and thus, he is sometimes called the "Later Emperor Fushimi". The Japanese word "go" has also been translated to mean the "second one;" and in some older sources, this emperor may be identified as "Fushimi, the second," or as "Fushimi II."
Genealogy.
Before his ascension to the Chrysanthemum Throne, his personal name (his "imina") was Tanehito"-shinnō" (胤仁親王).
He was the eldest son of the 92nd Emperor Emperor Fushimi. They belonged to the "Jimyōin-tō" branch of the Imperial Family.
Events of Go-Fushimi's life.
Tanehito-shinnō was named Crown Prince or heir in 1289.
Fushimi acted as cloistered emperor for a period, but after a while, from 1313 to 1318, Go-Fushimi acted in that function.
During Hanazono's reign, negotiations between the Bakufu and the two lines resulted in an agreement to alternate the throne between the two lines every 10 years (the Bumpō Agreement). This agreement did not last long, as it was broken by Emperor Go-Daigo.
Go-Fushimi was the author of a famous plea to the god of the Kamo Shrine for help in gaining the throne for his son. This plea was ultimately successful, but it was not until thirty-three years after his abdication that Go-Fushimi's son, Emperor Kōgon became emperor. Kōgon was the first of the northern court emperors backed by the Muromachi Bakufu.
Emperor Go-Fushimi is enshrined with other emperors at the imperial tomb called "Fukakusa no kita no misasagi" (深草北陵) in Fushimi-ku, Kyoto.
Kugyō.
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras. Even during those years in which the court's actual influence outside the palace walls was minimal, the hierarchic organization persisted.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Fo-Fushimi's reign, this apex of the "Daijō-kan included:
Eras of Go-Fushimi's reign.
The years of Go-Fushimi's reign are more specifically identified by more than one era name or "nengō".

</doc>
<doc id="45763" url="http://en.wikipedia.org/wiki?curid=45763" title="Emperor Go-Nijō">
Emperor Go-Nijō

Emperor Go-Nijō (後二条天皇 "Go-Nijō-tennō") (March 9, 1285 – September 10, 1308) was the 94th emperor of Japan, according to the traditional order of succession. He reigned from March 3, 1301 until September 10, 1308.
This 14th-century sovereign was named after the 12th-century Emperor Nijō, and "go-" (後), translates literally as "later;" and thus, he is sometimes called the "Later Emperor Nijō". The Japanese word "go" has also been translated to mean the "second one;" and in some older sources, this emperor may be identified as "Nijō, the second," or as "Nijo II."
Genealogy.
Before his ascension to the Chrysanthemum Throne, his personal name (his "imina") was Kuniharu"-shinnō" (邦治親王).
Go-Nijō was the eldest son of the 91st Emperor Emperor Go-Uda. He belonged to the Daikakuji-tō branch of the Imperial Family.
Events of Go-Nijō's life.
Kuniharu"-shinnō" was made an imperial prince by Imperial proclamation in 1286.
In 1296, he became crown prince (heir) to the "Jimyōin-tō" Emperor Go-Fushimi, his second cousin.
Go-Nijō's father, the Emperor Go-Uda reigned as cloistered emperor during his reign.
The succession dispute between the Daikakuji and Jimyōin branches of the Imperial Family continued during his reign. His grandfather, the retired Emperor Emperor Kameyama was said to have acted through the Bakufu to ensure Go-Nijō's enthronement.
On September 10, 1308, Go-Nijō died of illness.
Go-Nijō is buried at "Kitashirakawa no misasagi" (北白河陵) in Sakyō-ku, Kyoto.
Kugyō.
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras. Even during those years in which the court's actual influence outside the palace walls was minimal, the hierarchic organization persisted.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Nijō's reign, this apex of the "Daijō-kan" included:
Eras of Go-Nijō's reign.
The years of Go-Nijō's reign are more specifically identified by more than one era name or "nengō".

</doc>
<doc id="45764" url="http://en.wikipedia.org/wiki?curid=45764" title="Hanazono">
Hanazono

Hanazono is a Japanese surname, which can refer to:

</doc>
<doc id="45766" url="http://en.wikipedia.org/wiki?curid=45766" title="Emperor Go-Daigo">
Emperor Go-Daigo

Emperor Go-Daigo (後醍醐天皇 "Go-Daigo-tennō") (November 26, 1288 – September 19, 1339) was the 96th emperor of Japan, according to the traditional order of succession.
Post-Meiji historians construe Go-Daigo's reign to span 1318–1339; however, pre-Meiji accounts of his reign considered the years of his reign to encompass only between 1318–1332. Pre-Meiji scholars also considered Go-Daigo a pretender Emperor in the years from 1336 through 1339.
This 14th-century sovereign was named after the 9th-century Emperor Daigo and "go-" (後), translates literally as "later;" and thus, he is sometimes called the 'Later Emperor Daigo'. The Japanese word "go" has also been translated to mean the "second one;" and in some older sources, this emperor may be identified as 'Daigo, the second,' or as 'Daigo II.'
Biography.
Before his ascension to the Chrysanthemum Throne, his personal name (his "imina") was Takaharu"-shinnō" (尊治親王).
He was the second son of the Daikakuji-tō emperor, Emperor Go-Uda. His mother was Fujiwara no "Chūshi"/Tadako ("藤原忠子"), daughter of Fujiwara no Tadatsugu (Itsutsuji Tadatsugu) ("藤原忠継/五辻忠継"). She became Nyoin called Dantenmon-in (談天門院).
Emperor Go-Daigo's ideal was the Engi era (901–923) during the reign of Emperor Daigo, a period of direct imperial rule. An emperor's posthumous name was normally chosen after his death, but Emperor Go-Daigo chose his personally during his lifetime, to share it with Emperor Daigo.
Events of Go-Daigo's life.
Emperor Go-Daigo became emperor at the age of 31, in the prime of his life.
In 1324, with the discovery of Emperor Go-Daigo's plans to overthrow the Kamakura Shogunate, the Rokuhara Tandai disposed of his close associate Hino Suketomo in the Shōchū Incident.
In the Genkō Incident of 1331, Emperor Go-Daigo's plans were again discovered, this time by a betrayal by his close associate Yoshida Sadafusa. He quickly hid the Sacred Treasures in a secluded castle in Kasagiyama (the modern town of Kasagi, Sōraku district, Kyōto Prefecture) and raised an army, but the castle fell to the Bakufu's army the following year, and they enthroned Emperor Kōgon, exiling Emperor Go-Daigo to Oki Province (the Oki Islands in modern-day Shimane Prefecture), the same place to which Emperor Go-Toba had been exiled after the Jōkyū War of 1221.
In 1333, Emperor Go-Daigo escaped from Oki with the help of Nawa Nagatoshi and his family, raising an army at Funagami Mountain in Hōki Province (the modern town of Kotoura in Tōhaku District, Tottori Prefecture). Ashikaga Takauji, who had been sent by the Bakufu to find and destroy this army, sided with the Emperor and captured the Rokuhara Tandai. Immediately following this, Nitta Yoshisada, who had raised an army in the East, destroyed the Hōjō clan and captured the Bakufu.
Returning to Kyōto, Emperor Go-Daigo took the throne from Emperor Kōgon and began the Kenmu Restoration. The Restoration was ostensibly a revival of the older ways, but, in fact, the emperor had his eye set on an imperial dictatorship like that of the emperor of China. He wanted to imitate the Chinese in all their ways and become the most powerful ruler in the East. Impatient reforms, litigation over land rights, rewards, and the exclusion of the samurai from the political order caused much complaining, and his political order began to fall apart. In 1335, Ashikaga Takauji, who had travelled to eastern Japan without obtaining an imperial edict in order to suppress the Nakasendai Rebellion, became disaffected with the Restoration. Emperor Go-Daigo ordered Nitta Yoshisada to track down and destroy Ashikaga. Ashikaga defeated Nitta Yoshisada at the Battle of Takenoshita, Hakone. Kusunoki Masashige and Kitabatake Akiie, in communication with Kyoto, smashed the Ashikaga army. Takauji fled to Kyūshū, but the following year, after restructuring his army in Kyūshū, he again approached Kyōto. Kusunoki Masashige proposed a reconciliation with Ashikaga Takauji to the emperor, but Go-Daigo rejected this. He ordered Masashige and Yoshisada to destroy Takauji. Kusunoki's army was defeated at the Battle of Minatogawa (湊川の戦い).
When Ashikaga's army entered Kyōto, Emperor Go-Daigo resisted, fleeing to Mount Hiei, but seeking reconciliation, he sent the Sacred Treasures to the Ashikaga side. Takauji enthroned the Jimyōin-tō emperor, Kōmyō, and officially began his shogunate with the enactment of the Kenmu Law Code.
Go-Daigo escaped from the capital, the Sacred Treasures that he had handed over to the Ashikaga being counterfeit, and set up the Southern Court among the mountains of Yoshino, beginning the Period of Northern and Southern Courts in which the Northern Dynasty in Kyōto and the Southern Dynasty in Yoshino faced off against each other.
Emperor Go-Daigo ordered Imperial Prince Kaneyoshi to Kyūshū and Nitta Yoshisada and Imperial Prince Tsuneyoshi to Hokuriku, and so forth, dispatching his sons all over, so that they could oppose the Northern Court.
The actual site of Go-Daigo's grave is settled. This emperor is traditionally venerated at a memorial Shinto shrine ("misasagi") at Nara.
The Imperial Household Agency designates this location as Go-Daigo's mausoleum. It is formally named "Tō-no-o no misasagi".
Genealogy.
Consorts and children.
Empress ("Chūgū"): Saionji "Kishi" ("西園寺禧子") (Go-Kyōgoku-in, 後京極院) (1303–1333), daughter of Saionji Sanekane (西園寺実兼)
Empress ("Chūgū"): Imperial Princess Junshi (珣子内親王) (Shin-Muromachi-in, 新室町院) (1311–1337), daughter of Emperor Go-Fushimi
"Nyōgo": Fujiwara no Eishi (藤原栄子), daughter of Nijō Michihira
Court lady: Minamoto no Chikako (源親子), daughter of Kitabatake Morochika (北畠師親)
Court lady: Fujiwara no "Ishi"/Tameko (藤原為子) (?–1311/2), daughter of Nijō Tameyo (二条為世)
Court lady: "Ichijō no Tsubone" (一条局), daughter of Saionji Sanetoshi (西園寺実俊)
Court lady: Fujiwara no "Renshi" (Ano "Renshi") (藤原廉子/阿野廉子) (Shin-Taikenmon-in, 新待賢門院) (1301–1359), daughter of Ano Kinkado (阿野公廉)
Court lady: "Gon-no-Dainagon no Sammi no Tsubone" (権大納言三位局) (?–1351), daughter of Nijō Tamemichi (二条為道)
Princess: a daughter of Emperor Kameyama
Court lady: "Shōshō no Naishi" (少将内侍), daughter of Sugawara no Arinaka (菅原在仲)
Court lady: Fujiwara no Chikako (藤原親子), daughter of Kazan'in Munechika (花山院宗親)
Court lady: Fujiwara no "Shushi"/Moriko (藤原守子), daughter of Tōin Saneyasu (洞院実泰)
Court lady: "Konoe no Tsubone" (近衛局)
Court lady: "Shōnagon no Naishi" (少納言内侍), daughter of Shijō Takasuke (四条隆資)
Court lady: "Gon-no-Chūnagon no Tsubone" (権中納言局), daughter of Sanjō Kinyasu (三条公泰)
Court lady: "Mimbu-kyō no Tsubone" (民部卿局)
Go-Daigo had some other princesses from some court ladies.
Kugyō.
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras. Even during those years in which the court's actual influence outside the palace walls was minimal, the hierarchic organization persisted.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Go-Daigo's reign, this apex of the "Daijō-kan included:
Eras of Go-Daigo's reign.
The years of Go-Diago's reign are more specifically identified by more than one era name or "nengō". Emperor Go-Daigo's eight era name changes are mirrored in number only in the reign of Emperor Go-Hanazono, who also reigned through eight era name changes.
In popular culture.
Emperor Go-Daigo appears in the alternate history novel "Romanitas" by Sophia McDougall.

</doc>
<doc id="45768" url="http://en.wikipedia.org/wiki?curid=45768" title="Emperor Chōkei">
Emperor Chōkei

Emperor Chōkei (長慶天皇 "Chōkei-tennō") (1343 – August 27, 1394) was the 98th emperor of Japan, according to the traditional order of succession. He reigned from 1368 through 1383. His personal name was Yutanari (寛成).
Genealogy.
His father was Emperor Go-Murakami and his mother was Fujiwara Masako (藤原勝子)
Biography.
On March 29, 1368 ("Shōhei 23, 11th day of the 3rd month"), following the death of Emperor Go-Murakami, he was enthroned in the house of the Chief Priest at the Sumiyoshi Grand Shrine in Sumiyoshi, Osaka, where the Southern Court had made its capital. However, because the Southern Court's influence was declining, the enthronement remained in some doubt until the Taishō period. In 1926, the enthronement was officially recognized and inserted into the Imperial Line.
Emperor Chōkei insisted throughout his reign on fighting the Northern Dynasty, but it was already too late. In 1383 or 1384, he abdicated to Emperor Go-Kameyama, who supported the peace faction.
After the Reunification of the rival courts, he went into retirement and eventually returned to Yoshino where he died on August 27, 1394. The "kami" of Emperor Chōkei is venerated at Shishō jinja in Totsugawa, Yamato province.
Kugyō.
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras. Even during those years in which the court's actual influence outside the palace walls was minimal, the hierarchic organization persisted.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Chōkei's reign, this apex of the "Daijō-kan included:
Eras of Chōkei's reign.
The years of Chōkei's reign are more specifically identified by more than one era name or "nengō".

</doc>
<doc id="45769" url="http://en.wikipedia.org/wiki?curid=45769" title="Emperor Go-Kameyama">
Emperor Go-Kameyama

Emperor Go-Kameyama (後亀山天皇, Go-Kameyama Tennō) (c. 1347 – May 10, 1424) was the 99th emperor of Japan, according to the traditional order of succession. He ruled from 1383 to October 21, 1392, becoming the last Emperor of the Southern Court. His personal name was Hironari (熙成).
This Nanboku-chō "sovereign" was named after the 13th century Emperor Kameyama and "go-" (後), translates literally as "later;" and thus, he may be called the "Later Emperor Kameyama". The Japanese word "go" has also been translated to mean the "second one;" and in some older sources, this would-be emperor may be identified as "Kameyama, the second," or as "Kameyama II."
Genealogy.
He was the second son of Emperor Go-Murakami. His mother was Fujiwara Katsuko (藤原勝子).
Little is known of his empress or other consorts. Imperial Prince Tsuneatsu (恒敦) is believed to be his son.
Life.
Go-Kameyama acceeded to the throne during the turbulent "Nanboku-chō" period during which rival claimants to the Chrysanthemum Throne gathered supporters around them in what were known as the Northern court and the Southern Court. Go-Kameyama became Emperor in what was called the Southern court when Emperor Chōkei abdicated in 1383. On October 15, 1392, at the insistence of the peace faction amongst his own courtiers, he applied to Ashikaga Yoshimitsu for peace; and he subsequently returned to the capital where he did hand over the Sacred Treasures to his Northern Court rival. In doing so, Go-Kameyama was understood to have abdicated.
By the conditions of the peace treaty, the Northern Court and the Southern Court were supposed to alternate control of the throne. However, this was thrown out in 1412 as Emperor Go-Komatsu renaged on the treaty by abdicating in favour of his own son. Henceforth, no Southern Court claimant ever sat on the Chrysthansemum Throne again. Still, since 1911, the Japanese government has declared the southern claimants were actually the rightful emperors despite the fact that all subsequent emperors including the then-Emperor Meiji were descended from the Northern Court, reasoning the Southern Court retained possession of the three sacred treasures, thus converting the emperors of the former Northern court into mere pretenders.
Following his abdication, he went into seclusion; but, in 1410, he returned to Yoshino.
The Imperial Household Agency recognizes "Saga no ogura no misasagi" (嵯峨小倉陵) in Ukyō-ku, Kyoto as his tomb.
Kugyō.
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras. Even during those years in which the court's actual influence outside the palace walls was minimal, the hierarchic organization persisted.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Go-Kameyama's reign, this apex of the "Daijō-kan included:
Eras of Go-Kameyama's reign.
The years of Go-Kameyama's reign are more specifically identified by more than one era name or "nengō".

</doc>
<doc id="45770" url="http://en.wikipedia.org/wiki?curid=45770" title="Pylos">
Pylos

Pylos (Greek: Πύλος), historically also known under its Italian name Navarino, is a town and a former municipality in Messenia, Peloponnese, Greece. Since the 2011 local government reform it is part of the municipality Pylos-Nestoras, of which it is the seat and a municipal unit. It was the capital of the former Pylia Province. It is the main harbour on the Bay of Navarino. Nearby villages include Gialova, Pyla, Elaiofyto, Schinolakka, and Palaionero. The town of Pylos has 2,767 inhabitants, the municipal unit of Pylos 5,287 (2011).
Pylos has a long history, being inhabited since Neolithic times. It was a significant kingdom in Mycenean Greece, with remains of the so-called "Palace of Nestor" excavated nearby, named after Nestor, the king of Pylos in Homer's "Iliad". In Classical times, the site was uninhabited, but became the site of the Battle of Pylos in 425 BC, during the Peloponnesian War. Pylos is scarcely mentioned thereafter until the 13th century, when it became part of the Frankish Principality of Achaea. Increasingly known by its French name of Port-de-Jonc or its Italian name Navarino, in the 1280s the Franks built the Old Navarino castle on the site. Pylos came under the control of the Republic of Venice from 1417 until 1500, when it was conquered by the Ottoman Empire. The Ottomans used Pylos and its bay as a naval base, and built the New Navarino fortress there. The area remained under Ottoman control, with the exception of a brief period of renewed Venetian rule in 1685–1715 and a Russian occupation in 1770–71, until the outbreak of the Greek War of Independence in 1821. Ibrahim Pasha of Egypt recovered it for the Ottomans in 1825, but the defeat of the Turco-Egyptian fleet in the 1827 Battle of Navarino forced Ibrahim to withdraw from the Peloponnese and confirmed Greek independence.
The name of Navarino.
Pylos retained its ancient name down to Byzantine times, but appears after the Frankish conquest in the early 13th century under two names:
In the late 14th/early 15th centuries, when it was held by the Navarrese Company, it was also known as "Château Navarres", and called "Spanochori" (Σπανοχώρι, "village of the Spaniards") by the local Greeks.
Under Ottoman rule (1498–1685, 1715–1821), the Turkish name was "Anavarin[o]". After the construction of the new Ottoman fortress ("Anavarin kalesi") in 1571/2, it became known as "Neokastro" (Νεόκαστρο or Νιόκαστρο, "new castle") among the local Greeks, while the old Frankish castle became known as "Palaiokastro" (Παλαιόκαστρο or Παλιόκαστρο, "old castle").
Geography.
The soil about Navarino is of a red colour, and is remarkable for the production of an abundance of squills, which are used in medicine. The rocks, which show themselves in every direction through a scanty but rich soil, are limestone, and present a general appearance of unproductiveness round the castle of Navarino; and the absence of trees is ill compensated by the profusion of sage, brooms, cistus, and other shrubs which start from the innumerable cavities of the limestone.
The remains of Navarino, consist of a fort, covering the summit of a hill sloping quickly to the south, but falling in abrupt precipices to the north and east. The town was built on the southern declivity, and was surrounded by a wall, which, allowing for the natural irregularities of the soil, represented a triangle, with the castle at the summit—a form observable in many of the ancient cities of Greece.
Bay of Pylos.
Pylos' bay is formed by a deep indenture in the Morea, shut in by a long island, anciently called Sphacteria or Sphagia (modern name Sfaktiria), famous for the defeat and capture of the Spartans, in the Battle of Pylos during the Peloponnesian War, and still showing the ruins of walls which perhaps formed their last refuge. This island has been divided into three or four separate sections by the violence of the waves, and boats could pass from the open sea into the port, in calm weather, using the channels so formed. One such section contains the tomb of a Turkish saint, or santon, called the Delikli Baba. This same section also contains a monument to the French sailors who died at the Battle of Navarino; the monument to the Russian dead of the same battle is on the island of Sphacteria, while the monument to the British dead is on another very small island near the centre of the bay. Monuments and tombs from the Greek War of Independence are on the island of Sphacteria, the most important being the monument to the Italian philhellene Santorre di Santa Rosa.
Flora and fauna.
The Gialova wetland is a regional blessing of nature. It is one of 10 major lagoons in Greece. and has been classified as one of the important bird areas in Europe. It has also been listed as a 1500-acre archaeological site, lying between Gialova and the bay of Voidokilia. Its alternative name of Vivari is Latin, meaning 'fishponds'. With a depth, at its deepest point, of no more than four meters, it is the southernmost stopover of birds migrating from the Balkans to Africa, giving shelter to no fewer than 225 bird species, among them heron, cormorant, lesser kestrel, Audouin's gull, flamingo, osprey and imperial eagle. It is Gialova, too, which plays host to a vary rare species, nearing extinction throughout Europe, the African chameleon. The observation post of the Greek Ornithological Society allows visitors to find out more and to watch the shallow brackish waters of the lake, they can walk the paths that circumscribe Gialova's different ecosystems.
History.
Prehistoric and Mycenaean Pylos.
Pylos has evidence of continuous human presence dating back to the Neolithic Age. In Mycenaean times, it was an important centre, Nestor's kingdom of "sandy Pylos", as recalled by Homer in Book 17 of the "Odyssey":
<poem>
We left for Pylos, Nestor too
the shepherd of the peoples,
And He, receiving me the king,
within his halls so lofty,
Embraced me with all
eagerness as father does
his youngling
His son back from long time abroad.
Homer, "Odyssey"
XVII 108-112
</poem>
The Mycenaean state of Pylos (1600–1100 BC) covered an area of 2000 square km and had a minimum population of 50,000 according to Linear B tablets, or even perhaps as large as 80,000 - 120,000.
Bronze Age Pylos was excavated by Carl Blegen between 1939 and 1952. It is located at modern Ano Englianos, about 9 km north-east of the bay . Blegen called the remains of a large Mycenean palace dating from 1300 BC. found there the "Palace of Nestor", after the Homeric poems. Linear B tablets found by Blegen clearly demonstrate that the site itself was called Pylos ("Pulos" in Mycenaean Greek; attested in Linear B as 𐀢𐀫, "pu-ro") by its Mycenaean inhabitants. This site was abandoned sometime after the 8th century BC and burned to the ground. The ruins of a crude stone fortress on nearby Sphacteria Island, apparently of Mycenaean origin, were used by the Spartans during the Peloponnesian War. (Thucydides iv. 31)
Classical Pylos.
According to the Greek historian Thucydides in his "History of the Peloponnesian War", the area was "together with most of the country round, unpopulated" (iv, 3). In 425 BC the Athenian politician Cleon sent an expedition to Pylos where the Athenians fortified the rocky promontory now known as Koryphasion or Old Pylos at the northern edge of the bay, and after a conflict with Spartan ships in the Battle of Pylos, seized and occupied the bay. A little later the Athenians captured a number of Spartan troops besieged on the adjacent island of Sphacteria (see Battle of Sphacteria). Spartan anxiety over the return of the prisoners, who were taken to Athens as hostages, contributed to their acceptance of the Peace of Nicias in 421 BC.
Middle Ages.
Little is known of Pylos under Byzantine rule, except for a mention of raids by Cretan Saracens in the area c. 872/3. In the 12th century, the Muslim geographer al-Idrisi mentioned it as the "commodious port" of "Irūda" in his "Nuzhat al-Mushtaq".
In 1204, following the Fourth Crusade, the Peloponnese became the Principality of Achaea, a Crusader state. Pylos fell quickly to the Crusaders according to a brief reference in the "Chronicle of the Morea", but it is not until the 1280s that it is mentioned again. According to the French and Greek versions of the "Chronicle", Nicholas II of Saint Omer, the lord of Thebes, who in c. 1281 received extensive lands in Messenia in exchange for his wife's possessions of Kalamata and Chlemoutsi, erected a castle at Navarino. According to the Greek version, he intended this as a future fief for his nephew, Nicholas III, although the Aragonese version attributes the construction to Nicholas III himself, a few years later. According to A. Bon, a construction under Nicholas II in the 1280s is more likely, possibly in the period 1287–89 when he served as the viceroy ("bailli") of Achaea. Despite Nicholas II's intentions, however, it is unclear whether his nephew did indeed inherit Navarino. If he did, it remained his until his death in 1317, when it and all the Messenian lands of the family reverted to the princely domain, as Nicholas III had no children.
The fortress remained relatively unimportant thereafter, except for the naval battle in 1354 between Venice and Genoa, and an episode in 1364, during the conflict between Mary of Bourbon and the Prince Philip of Taranto, due to Mary's attempt to claim the Principality following the death of her husband, Robert of Taranto. Mary had been given possession of Navarino (along with Kalamata and Mani) by Robert in 1358, and the local castellan, loyal to Mary, briefly imprisoned the new Prince's "bailli", Simon del Poggio. Mary retained control of Navarino until her death in 1377. At about this time, Albanians settled in the area, while in 1381/2, Navarrese, Gascon and Italian mercenaries were active there. From the early years of the 15th century, Venice set its eyes on the fortress of Navarino, fearing lest its rivals the Genoese seize it and use it as a base for attacks against the Venetian outposts of Modon and Coron. In the event, the Venetians seized the fortress themselves in 1417 and, after prolonged diplomatic manoeuvring, succeeded in legitimizing their new possession in 1423.
First Venetian and first Ottoman periods.
In 1423, Navarino, like the rest of the Peloponnese, suffered its first Ottoman raid, led by Turakhan Bey, which was repeated in 1452. It was also at Navarino that Emperor John VIII Palaiologos embarked in 1437, heading for the Council of Ferrara, and where the last Despot of the Morea, Thomas Palaiologos, embarked with his family in 1460, following the Ottoman conquest of the Despotate of the Morea. After 1460, the fortress, along with the other Venetian outposts and Monemvasia and the Mani Peninsula, were the only Christian-held areas in the peninsula. Venetian control over Navarino survived the First Ottoman–Venetian War (1463–79), but not the Second (1499–1503): following the Venetian defeat in the Battle of Modon in 1500, the 3,000-strong garrison surrendered, although it was well provisioned for a siege. The Venetians nevertheless recaptured it shortly after, on 3/4 December, but on 20 May 1501, a joint Ottoman land and sea attack under Kemal Reis and Hadım Ali Pasha retook it.
The Ottomans used Navarino (which they called "Anavarin" or "Avarna") as a naval base, either for piratical raids or for major fleet operations in the Ionian and Adriatic seas. In 1572/3, the Ottoman chief admiral ("Kapudan Pasha") Uluç Ali Reis built a new fortress at Navarino ("Anavarin-i Cedid", "New Navarino", or "Neokastro" in Greek), to replace the outdated Frankish castle.
The Venetians briefly captured Navarino in the 1650s during the Cretan War.
In 1668, Evliya Çelebi described the city in his "Seyahatname":
Anavarin-i Atik is an unequalled castle... the harbor is a safe
anchorage...
<br>
in most streets of Anavarin-i Cedid there are many fountains of
running water... The city is embellished with trees and vines so
that the sun does not beat into the fine marketplace at all, and
all the city notables sit here, playing backgammon, chess, various
kinds of draughts, and other board games...
Second Venetian period, Ottoman reconquest and Greek Independence.
In 1685, during the early stages of the Morean War, the Venetians under Francesco Morosini and Otto Wilhelm Königsmarck invaded the Peloponnese and captured most of it, successfully storming the two fortresses of Navarino in the process. With the peninsula safely in Venetian hands, Navarino became an administrative centre in the new "Kingdom of the Morea", as the Venetian province was called, until 1715, when the Ottomans recovered the Peloponnese. The Venetian census of 1689 gave the population as 1,413, while twenty years later it had risen to 1,797 inhabitants.
After the Ottoman reconquest, Navarino became the centre of a "kaza" in the Sanjak of the Morea. On 10 April 1770, after a six-day siege, the fortress of New Navarino surrendered to the Russians during the Orlov Revolt. The Ottoman garrison was allowed to depart for Crete, while the Russians repaired the fortress to make it their base. On 1 June, however, the Russians left, and the Ottomans re-entered the fort and burned and partially demolished it.
After the outbreak of the Greek War of Independence in mid-March 1821, Navarino was besieged by the local Greeks on 29 March. The garrison, augmented by the local Muslim population of Kyparissia, held out until the first week of August, when they were forced to capitulate. Despite their promise for safe conduct, the Greeks massacred them all.
The Turks under Ibrahim Pasha of Egypt retook most of the Peloponnese in 1825, including the Pylos area, overcoming the Greek defenders at the battles of Sphacteria (29 April) and Neokastro (11 May). The fortress remained in Ottoman hands until the spring 1828, but in October 1827, the combined Ottoman–Egyptian fleets were defeated in the bay at the Battle of Navarino by the allied navies of the United Kingdom, France and Russia. This event negated Ibrahim's successes, and in autumn 1828 his troops withdrew from the Peloponnese.
The modern town.
The framework of the modern town of Pylos, outside the walls of Neokastro, was built by the troops of General Maison during the subsequent French Morea expedition of 1828–1833.
The western end of Greek National Road 82 begins in downtown Pylos. The highway runs west to east and links Pylos with Kalamata and Sparta. The area enjoys a famously favorable climate, with especially mild winters.
Subdivisions.
The municipal unit Pylos is subdivided into the following communities:

</doc>
<doc id="45772" url="http://en.wikipedia.org/wiki?curid=45772" title="Rocky">
Rocky

Rocky is a 1976 American sports drama film directed by John G. Avildsen and both written by and starring Sylvester Stallone. It tells the rags to riches American Dream story of Rocky Balboa, an uneducated but kind-hearted working class Italian-American boxer working as a debt collector for a loan shark in the slums of Philadelphia. Rocky starts out as a small-time club fighter who later gets a shot at the world heavyweight championship. It also stars Talia Shire as Adrian, Burt Young as Adrian's brother Paulie, Burgess Meredith as Rocky's trainer Mickey Goldmill, and Carl Weathers as the champion, Apollo Creed.
The film, made on a budget of just over $1 million and shot in 28 days, was a sleeper hit; it earned $225 million in global box office receipts becoming the highest grossing film of 1976 and went on to win three Oscars, including Best Picture. The film received many positive reviews and turned Stallone into a major star. It spawned five sequels: "Rocky II", "III", "IV", "V", and "Rocky Balboa", all written by and starring Stallone, who also directed all sequels except for "Rocky V" (which was directed again by Avildsen).
Plot.
In 1975, Rocky "The Italian Stallion" Balboa is a hard-living professional boxer in the Italian neighborhoods of Philadelphia. Between fights, he works as an enforcer for loan shark Anthony Gazzo, and is regarded by many of his neighbors as a bum. The World Heavyweight Champion, Apollo Creed, announces plans to hold an exhibition match in Philadelphia during the upcoming 1976 U.S. Bicentennial. However, he is informed at the last second that his scheduled opponent, Mac Lee Green, is unable to compete due to an injured hand. With all other potential replacements booked up or otherwise unavailable, Creed decides to spice things up and give a local amateur a chance to face him. He settles on Rocky, believing that he will be an easy challenge.
Rocky meets with promoter Miles Jergens and agrees to the match. After several weeks of training using whatever he can find, including meat carcasses as punching bags, Rocky accepts an offer of assistance from Mickey Goldmill, a respected trainer who always criticized Rocky for wasting his potential.
At the same time, Rocky begins a relationship with Adrian, a clerk at the local pet store. He gradually gains the shy Adrian's trust, culminating in a kiss. Her alcoholic brother, Paulie, becomes jealous of Rocky's success, but Rocky calms him by agreeing to advertise his meatpacking business at the fight. The night before the match, Rocky becomes depressed after touring the arena. He confesses to Adrian that he has no desire to win, but instead wants to go the distance and prove himself to everyone.
On New Year's Day, the climactic boxing match begins, with Creed making a dramatic entrance dressed as George Washington and then Uncle Sam. Taking advantage of his overconfidence, Rocky knocks him down in the first round. Enraged, Creed makes a quick comeback and starts pounding Rocky. The fight goes on for 15 rounds, with both fighters sustaining many injuries; Rocky suffers his first broken nose and debilitating trauma around the eye, and Creed sustains brutal blows to his ribs with substantial internal bleeding. As the match progresses, Creed's superior skill is countered by Rocky's apparently unlimited ability to absorb punishment, and his dogged refusal to be knocked out. As the final round bell sounds, with both fighters locked in each other's arms, they promise to each other that there will be no rematch.
After the fight, multiple layers of drama are played out: the sportscasters and the audience go wild, Jergens announces over the loudspeaker that the match was "the greatest exhibition of guts and stamina in the history of the ring", and Rocky calls out repeatedly for Adrian, who runs down and comes into the ring as Paulie distracts arena security. As Jergens declares Apollo Creed the winner by virtue of a split decision (8:7, 7:8, 9:6), Adrian and Rocky embrace while they profess their love to each other.
Cast.
Cameo appearances.
Boxer Joe Frazier has a cameo appearance in the film. The character of Apollo Creed was influenced by outspoken boxer Muhammad Ali who fought Frazier three times. During the Academy Awards ceremony, Ali and Stallone staged a brief comic confrontation to show Ali was not offended by the film. Some of the plot's most memorable moments—Rocky's carcass-punching scenes and Rocky running up the steps of the Philadelphia Museum of Art, as part of his training regime—are taken from the real-life exploits of Joe Frazier, for which he received no credit.
Due to the film's comparatively low budget, members of Stallone's family played minor roles. His father rings the bell to signal the start and end of a round, his brother Frank plays a street corner singer, and his first wife, Sasha, was stills photographer. Other cameos include former Philadelphia and then-current Los Angeles television sportscaster Stu Nahan playing himself, alongside radio and TV broadcaster Bill Baldwin; and Lloyd Kaufman, founder of the independent film company Troma, appearing as a drunk. Diana Lewis, then a news anchor in Los Angeles and later in Detroit, has a small scene as a TV news reporter. Tony Burton appeared as Apollo Creed's trainer, Tony "Duke" Evers, a role he would reprise in the entire "Rocky" series, though he is not given an official name until "Rocky II". Though uncredited, Michael Dorn who would later gain fame as the Klingon Worf in ' and ', made his acting debut as Creed's bodyguard.
Production.
United Artists liked Stallone's script, and viewed it as a possible vehicle for a well-established star such as Robert Redford, Ryan O'Neal, Burt Reynolds, or James Caan. Stallone appealed to the producers to be given a chance to star in the film. He later said that he would never have forgiven himself if the film became a success with someone else in the lead. He also knew that producers Irwin Winkler's and Robert Chartoff's contract with the studio enabled them to "greenlight" a project if the budget was kept low enough. The producers also collateralized any possible losses with their big-budget entry, "New York, New York" (whose eventual losses were ironically covered by "Rocky"'s success).
Certain elements of the story were altered during filming. The original script had a darker tone: Mickey was portrayed as racist and the script ended with Rocky throwing the fight after realizing he did not want to be part of the professional boxing world after all.
Although Chartoff and Winkler were enthusiastic about the script and the idea of Stallone playing the lead character, they were hesitant about having an unknown headline the film. The producers also had trouble casting other major characters in the story, with Adrian and Apollo Creed cast unusually late by production standards (both were ultimately cast on the same day). Real-life boxer Ken Norton was initially sought for the role of Apollo Creed, but he pulled out and the role was ultimately given to Carl Weathers. Norton had had three fights with Muhammad Ali, upon whom Creed was loosely based. According to "The Rocky Scrapbook", Carrie Snodgress was originally chosen to play Adrian, but a money dispute forced the producers to look elsewhere. Susan Sarandon auditioned for the role but was deemed too pretty for the character. After Talia Shire's ensuing audition, Chartoff and Winkler, along with Avildsen, insisted that she play the part.
Inventor/operator Garrett Brown's new Steadicam was used to accomplish smooth photography while running alongside Rocky during the film's Philadelphia street jogging/training sequences and the run up the Art Museum's flight of stairs. It was also used for some of the shots in the fight scenes and can be openly seen at the ringside during some wide shots of the final fight. ("Rocky" is often erroneously cited as the first film to use the Steadicam, although it was actually the third, after "Bound for Glory" and "Marathon Man".)
While filming "Rocky", both Stallone and Weathers suffered injuries during the shooting of the final fight; Stallone suffered bruised ribs and Weathers suffered a damaged nose, the opposite injuries of what their characters had.
The poster seen above the ring before Rocky fights Apollo Creed shows Rocky wearing red shorts with a white stripe when he actually wears white shorts with a red stripe. When Rocky points this out he is told that "it doesn't really matter does it?". According to director Avildsen's DVD commentary, this was an actual mistake made by the props department that they could not afford to rectify, so Stallone wrote the brief scene to ensure the audience didn't see it as a goof (Carl Weathers would, ironically, wear white-striped red shorts for the Creed-Balboa rematch in "Rocky II"). Avildsen said that the same situation arose with Rocky's robe. When it came back from the costume department, it was far too baggy for Stallone. And because the robe arrived on the day of filming the scene and there was no chance of replacing or altering it, instead of ignoring this and risk the audience laughing at it, Stallone wrote the dialogue where Rocky himself points out the robe is too big.
The first date between Rocky and Adrian, in which Rocky bribes a janitor to allow them to skate after closing hours in a deserted ice skating rink, was shot that way only because of budgetary pressures. This scene was originally scheduled to be shot in a skating rink during regular business hours. However, the producers ultimately decided that they couldn't afford to hire the hundreds of extras that would have been necessary for that scene.
The production budget for "Rocky" was $1,075,000, with a further $100,000 spent on producer's fees and $4.2 million of advertising costs. It eventually earned worldwide box-office receipts exceeding $225 million with $117 million coming from North America.
Stallone's inspiration.
The film draws inspiration from the careers of at least three boxers.
The character's name and ethnicity harken to Rocky Marciano.
The main plot of the film may have been based on the match between Muhammad Ali and Chuck Wepner at Richfield Coliseum in Richfield, Ohio on March 24, 1975. Wepner was TKO'd in the 15th round by Ali, but nobody ever expected him to last as long as he did. Stallone watched the Wepner-Ali fight and shortly afterwards wrote the script for Rocky, but Stallone subsequently denied that Wepner provided any inspiration for the script. Other possible inspirations for the film may have included Rocky Graziano's autobiography "Somebody Up There Likes Me," and the movie of the same name. Wepner filed a lawsuit which was eventually settled with Stallone for an undisclosed amount.
The Philadelphia setting and details of training (including using sides of beef) come from the life of Joe Frazier.
Rocky Steps.
The famous scene of Rocky running up the steps of the Philadelphia Museum of Art has become a cultural icon. In 1982, a statue of Rocky, commissioned by Stallone for "Rocky III", was placed at the top of the Rocky Steps. City Commerce Director Dick Doran claimed that Stallone and Rocky had done more for the city's image than "anyone since Ben Franklin."
Differing opinions of the statue and its placement led to a relocation to the sidewalk outside the Spectrum Arena, although the statue was temporarily returned to the top of the steps in 1990 for "Rocky V", and again in 2006 for the 30th anniversary of the original "Rocky" (although this time it was placed at the bottom of the steps). Later that year, it was permanently moved to a spot next to the steps.
The scene is frequently parodied in the media. In "You Don't Mess with the Zohan", Zohan's nemesis, Phantom, goes through a parodied training sequence finishing with him running up a desert dune and raising his hands in victory. In the fourth season's finale of "The Fresh Prince of Bel-Air", as the credits roll at the end of the episode, Will is seen running up the same steps of the Philadelphia Museum of Art; however, as he celebrates after finishing his climb, he passes out in exhaustion, and while he lies unconscious on the ground, a pickpocket steals his wallet and his wool hat. Also in "The Nutty Professor", there is a scene where Eddie Murphy is running up the stairs and throwing punches at the top.
In 2006, E! named the "Rocky Steps" scene #13 in its "101 Most Awesome Moments in Entertainment".
During the 1996 Summer Olympics torch relay, Philadelphia native Dawn Staley was chosen to run up the museum steps. In 2004, Presidential candidate John Kerry ended his pre-convention campaign at the foot of the steps before going to Boston to accept his party's nomination for President.
Critical reception.
"Rocky" received mixed to positive reviews at the time of its release. Roger Ebert of the "Chicago Sun-Times" gave it 4 out of 4 stars and said that Stallone reminded him of "the young Marlon Brando. " "Box Office Magazine" claimed that audiences would be "touting Sylvester 'Sly' Stallone as a new star". The film, however, did not escape criticism. Vincent Canby, of "The New York Times", called it "pure '30s make believe" and dismissed both Stallone's acting and Avildsen's directing, calling the latter "none too decisive". Frank Rich liked the film, calling it "almost 100 per cent schmaltz," but favoring it over the cynicism that was prevalent in movies at that time, although he referred to the plot as "gimmicky" and the script "heavy-handed". He attributed all of the film's weaknesses to Avildsen, describing him as responsible for some of the "most tawdry movies of recent years", and who "has an instinct for making serious emotions look tawdry" and said of "Rocky", "He'll go for a cheap touch whenever he can" and "tries to falsify material that was suspect from the beginning. ... Even by the standards of fairy tales, it strains logic." Rich also criticised the film's "stupid song with couplets like 'feeling strong now/won't be long now.'"
Several reviews, including Richard Eder's (as well as Canby's negative review), compared the work to that of Frank Capra. Andrew Sarris found the Capra comparisons disingenuous: "Capra's movies projected more despair deep down than a movie like "Rocky" could envisage, and most previous ring movies have been much more cynical about the fight scene," and, commenting on Rocky's work as a loan shark, says that the film "teeters on the edge of sentimentalizing gangsters." Sarris also found Meredith "oddly cast in the kind of part the late James Gleason used to pick his teeth." Sarris also took issue with Avildsen's direction, which he described as having been done with "an insidious smirk" with "condescension toward everything and everybody," specifically finding fault, for example, with Avildsen's multiple shots of a chintzy lamp in Rocky's apartment. Sarris also found Stallone's acting style "a bit mystifying" and his character "all rough" as opposed to "a diamond in the rough" like Terry Malloy.
More than 30 years later, the film enjoys a reputation as a classic and still receives positive reviews; "Rocky" holds a 92% rating on Rotten Tomatoes, with the consensus stating: "This story of a down-on-his-luck boxer is thoroughly predictable, but Sylvester Stallone's script and stunning performance in the title role brush aside complaints." Another positive online review came from the BBC Films website, with both reviewer Almar Haflidason and BBC online users giving it 5/5 stars. In Steven J. Schneider's "1001 Movies You Must See Before You Die", Schneider says the film is "often overlooked as schmaltz."
In 2006, "Rocky" was selected for preservation in the United States National Film Registry by the Library of Congress as being "culturally, historically, or aesthetically significant".
In June 2008, AFI revealed its "Ten top Ten"—the best ten films in ten "classic" American film genres—after polling over 1,500 people from the creative community. "Rocky" was acknowledged as the second-best film in the sports genre, after "Raging Bull".
In 2008, "Rocky" was chosen by British film magazine "Empire" as one of "The 500 Greatest Movies of All Time". In contrast, in a 2005 poll by "Empire", Rocky was No. 9 on their list of "The Top 10 Worst Pictures to Win Best Picture Oscar".
Academy Awards – 1976.
"Rocky" received ten Academy Awards nominations in nine categories, winning three:
"Rocky" has also appeared on several of the American Film Institute's "100 Years" lists.
The Directors Guild of America awarded "Rocky" its annual award for best film of the year in 1976, and in 2006, Sylvester Stallone's original screenplay for "Rocky" was selected for the Writers Guild of America Award as the 78th best screenplay of all time.
Other media.
Soundtrack.
All music by Bill Conti.
"Rocky"‍ '​s soundtrack was composed by Bill Conti. The main theme song, "Gonna Fly Now", made it to number one on the "Billboard" magazine's Hot 100 list for one week (from July 2 to July 8, 1977) and the American Film Institute placed it 58th on its AFI's 100 Years...100 Songs. The complete soundtrack was re-released in 1988 by EMI on CD and cassette. Conti was also the composer for "Rockys": "II", "III", "V", and "Rocky Balboa".
The version of "Gonna Fly Now" used in the film is different from the versions released on later CDs and records. The vocals and guitars are much more emphasized than the versions released. The "movie version" has yet to be released.
Although the Conti version of "Gonna Fly Now" is the most recognizable arrangement, a cover of the song performed by legendary trumpeter Maynard Ferguson on his "Conquistador" album prior to the release of the motion picture soundtrack actually outsold the soundtrack itself.
Novelization.
A paperback novelization of the screenplay was written by Rosalyn Drexler and published by Ballantine Books in 1976.
Video games.
Several video games have been made based on the film. The first "Rocky" video game was released by Coleco for ColecoVision in August 1983 titled "Rocky Super Action Boxing"; the principal designer was Coleco staffer B. Dennis Sustare. Another was released in 1987 for the Sega Master System. More recently, a "Rocky" video game was released in 2002 for the Nintendo GameCube, Game Boy Advance, PlayStation 2, and Xbox, and a sequel, "Rocky Legends", was released in 2004 for the PlayStation 2 and Xbox. In 2007, a video game called "Rocky Balboa" was released for PSP. In 1985, Dinamic Software released a boxing game for the Sinclair ZX Spectrum (also advertised for and/or published on the Sega Master System, Amstrad CPC and MSX) called "Rocky". Due to copyright reasons it was quickly renamed "Rocco".
Theatre.
A musical has been written by Stephen Flaherty and Lynn Ahrens (lyrics and music) with the book by Thomas Meehan based on the film. The musical premiered in Hamburg, Germany in October 2012 and began performances at the Winter Garden Theater on Broadway on February 11, 2014 and officially opened on March 13, 2014.

</doc>
<doc id="45773" url="http://en.wikipedia.org/wiki?curid=45773" title="Kinshasa">
Kinshasa

Kinshasa ( or ; formerly Leopoldville (French: "Léopoldville" or Dutch   )) is the capital and the largest city of the Democratic Republic of the Congo. It is located on the Congo River.
Once a site of fishing villages, Kinshasa is now an urban area with a 2013 population of over 9 million. It faces the capital of the neighbouring Republic of Congo, Brazzaville, which can be seen in the distance across the wide Congo River. The city of Kinshasa is also one of the DRC's 11 provinces. Because the administrative boundaries of the city-province cover a vast area, over 90% of the city-province's land is rural in nature, and the urban area only occupies a small section in the far western end of the city-province.
Kinshasa is the third largest urban area in Africa after Cairo and Lagos. It is also the second largest "francophone" urban area in the world after Paris, French being the language of government, schools, newspapers, public services and high-end commerce in the city, while Lingala is used as a lingua franca in the street. If current demographic trends continue, Kinshasa should surpass Paris in population around 2020. Kinshasa hosted the 14th Francophonie Summit in October 2012.
Residents of Kinshasa are known as "Kinois" (in French and sometimes in English) or Kinshasans (English).
History.
The city was founded as a trading post by Henry Morton Stanley in 1881. It was named Léopoldville in honor of King Leopold II of Belgium, who controlled the vast territory that is now the Democratic Republic of the Congo, not as a colony but as a private property. The post flourished as the first navigable port on the Congo River above Livingstone Falls, a series of rapids over 300 km below Leopoldville. At first, all goods arriving by sea or being sent by sea had to be carried by porters between Léopoldville and Matadi, the port below the rapids and 150 km from the coast. The completion of the Matadi-Kinshasa portage railway in 1898 provided an alternative route around the rapids and sparked the rapid development of Léopoldville. In 1914 a pipeline was installed so that crude oil could be transported from Matadi to the upriver steamers in Leopoldville. By 1923, the city was elevated to capital of the Belgian Congo, replacing the town of Boma in the Congo estuary. The town, nicknamed "Léo" or "Leopold", became a commercial centre and grew rapidly during the colonial period. The origin of the HIV virus traces back to 1920s Léopoldville as discovered in 2014.
In 1965, Joseph-Désiré Mobutu seized power in the Congo in his second coup and initiated a policy of "Africanizing" the names of people and places in the country. In 1966, Léopoldville was renamed "Kinshasa", for a village named Kinchassa that once stood near the site. The city grew rapidly under Mobutu, drawing people from across the country who came in search of their fortunes or to escape ethnic strife elsewhere. This inevitably brought a change to the city's ethnic and linguistic composition. Although it is situated in territory that traditionally belongs to the Bateke and Bahumbu people, the "lingua franca" among African languages in Kinshasa today is Lingala, while the administrative and main written language is French (see further Languages of the Democratic Republic of the Congo). In 1974 Kinshasa hosted The Rumble in the Jungle boxing match between Muhammad Ali and George Foreman, in which Ali defeated Foreman to regain the World Heavyweight title.
In the 1990s a rebel uprising began, which by 1997 had brought down the regime of Mobutu Sese Seko. Kinshasa suffered greatly due to Mobutu's excesses, mass corruption, nepotism and the civil war that led to his downfall. Nevertheless, it is still a major cultural and intellectual center for Central Africa, with a flourishing community of musicians and artists. It is also the country's major industrial center, processing many of the natural products brought from the interior. The city has recently had to fend off rioting soldiers who were protesting the government's failure to pay them.
Administration.
Kinshasa is both a city ("ville" in French) and a province ("province" in French), one of the 11 provinces of the Democratic Republic of the Congo. Its status is thus similar to Paris which is both a city and one of the 101 departments of France.
Administrative divisions.
The "ville-province" of Kinshasa is divided into four districts which are further divided into 24 "communes" (municipalities). 
Geography.
Kinshasa is a city of sharp contrasts, with affluent residential and commercial areas and three universities alongside sprawling slums. It is located along the south bank of the Congo River, directly opposite the city of Brazzaville, capital of the Republic of the Congo. This is the only place in the world where two national capital cities face one another, and are in sight of each other on opposite banks of a river.
The Congo river is the second longest river in Africa after the Nile, and has the continent's greatest discharge. As a waterway it provides a means of transport for much of the Congo basin, being navigable for large river barges between Kinshasa and Kisangani, and many of its tributaries are also navigable. The river is an important source of hydroelectric power, and downstream of Kinshasa it has the potential to generate power equivalent to the usage of roughly half of Africa's population.
Climate.
Under the Köppen climate classification, Kinshasa has a Tropical wet and dry climate. Its lengthy rainy season spans from October through May, with a relatively short dry season, between June and September. Kinshasa lies south of the equator, so its dry season begins around its "winter" solstice, which is in June. This is in contrast to African cities further north featuring this climate where the dry season typically begins around January. Kinshasa's dry season is slightly cooler than its wet season, though temperatures remain relatively constant throughout the year.
Buildings and institutions.
Major areas of the city include the Cité de l'OUA, home to the Government of the Democratic Republic of the Congo, quartier Matonge, known regionally for its nightlife, L'ONATRA, the impressive building of the Ministry of Transport and the residential area of Gombe.
Notable features of the city include the Gecamines Commercial Building (formerly SOZACOM) and Hotel Memling skyscrapers, the central market, the Kinshasa Museum and the Kinshasa Fine Arts Academy. The face of Kinshasa is changing as buildings are being built on the Boulvard du 30 Juin: Crown Tower (on Batetela) and Congofutur Tower.
The Boulevard du 30 Juin (Boulevard of the 30 June) links the main areas of the central district of the city. Kinshasa is home to the country's national stadium, the Stade des Martyrs (Stadium of the Martyrs).
Industries.
Marsavco Sarl Biggest FMCG Manufacturing Company located in center of town (Gombe) in Kinshasa.
There are many other industries such as Trust Merchant Bank located in the heart of the city as well as court reporting. Food processing is a major industry and construction and other service industries also play a significant role in the economy.
Social issues.
Crime.
In 2004, Kinshasa was rated as one of Africa's most dangerous cities in terms of crime. Since the Second Congo War, the city has been striving to recover from disorder, with many gangs hailing from Kinshasa's slums. Muggings, robberies, rape, kidnapping and gang violence are relatively common. Kinshasa's homicide rate is estimated to be as high as 112 homicides per 100,000.
Street children.
Street children, often orphaned, are subject to abuse by the police and military. Of the estimated 20,000 children – up to the age of eighteen – living on Kinshasa's streets, almost a quarter are beggars, some are street vendors and about a third have some kind of employment. Some are there as fallout from the times of war; others are accused of witchcraft and have become outcasts.
Police regularly round up street children, to an uncertain fate. In 2001, a street child was killed by a policeman, allegedly while fleeing after stealing flour.
Education.
Kinshasa is home to several higher-level education institutes, covering a wide range of specialities, from civil engineering to nursing and journalism. The city is also home to three large universities and an arts school:
Medicine.
There are twenty hospitals in Kinshasa, plus various medical centres and polyclinics. In 1997, Dikembe Mutombo built a 300-bed hospital near his home town of Kinshasa.
Since 1991, Monkole Hospital is operating as a non-profit health institution collaborating with the Health Department as district hospital in Kinshasa. Directed by Pr Léon Tshilolo, paediatrician and haematologist, Monkole Hospital opened a 150-bed building in 2012 with improved clinical services as laboratory, diagnostic radiology, intensive care, neonatal unit, family medicine, emergencies unit and a larger surgical area.
Media.
Kinshasa is home to a large number of media outlets, including multiple radio and television stations that broadcast to nearly the entire country, including state-run Radio-Television Nationale Congolaise (RTNC) and privately run Digital Congo and Raga TV. The private channel RTGA is also based in Kinshasa.
Several national radio stations, including La Voix du Congo, which is operated by RTNC, MONUC-backed Radio Okapi and Raga FM are based in Kinshasa, as well as numerous local stations. The BBC is also available in Kinshasa on 92.6 FM.
The state-controlled Agence Congolaise de Presse news agency is based in Kinshasa, as well as several daily and weekly newspapers and news websites, including "L'Avenir" (daily), "La Conscience" "L'Observateur" (daily), "Le Phare", "Le Potentiel", and "Le Soft".
Most of the media uses French and Lingala to a large extent; very few use the other national languages.
Language.
The official language of the Democratic Republic of the Congo, of which Kinshasa is the capital, is French (See: Kinshasa French vocabulary). Kinshasa is the second largest officially Francophone city in the world although Lingala is widely used as a spoken language. French is the language of street signs, posters, newspapers, government documents, schools; it dominates plays, television, and the press, and it is used in vertical relationships among people of uneven rank; people of equal rank, however, speak the Congolese languages (Kikongo, Lingala, Tshiluba or Swahili) among themselves. Thus, while the culture is dominated by the Francophonie, a complex multilingualism is present in Kinshasa.
Transport.
Several private companies whose Urban Transport Company (STUC) and the Public City train (12 cars in 2002 ) serves the city. The bus lines are:
Other companies also provide public transport: Urbaco, Tshatu Trans, Socogetra, Gesac and MB Sprl. The city bus carries up to 67,000 passengers per day. Several companies operate taxis and taxi-buses. Also available are fula-fula (trucks adapted to carry passengers). The majority (95.8%) of transport is provided by individuals.
During the early years of the 21st century, the city's planners considered creating a tramway in collaboration with public transport in Brussels (STIB), whose work would start in 2009. That work has not moved beyond the planning stage, partly due to lack of a sufficient electrical supply.
Air.
Several airlines serve Kinshasa (Ndjili) International Airport (FIH) in Kinshasa, including Kenya Airways, Air Gabon, Cameroon Airlines, Bravo Air Congo, Air Zimbabwe, South African Airways, Ethiopian Airlines, Brussels Airlines, Air France and Turkish Airlines.
As of April 2014 DR Congo has two major national airlines (CAA and Korongo Airlines) which offered flights inside DR Congo and to a small number of international locations. Korongo Airlines is based in Lumbumbashi and CAA was based in Kinshasa but both airlines operated many of the same flight routes.
Rail.
ONATRA operates three lines of urban railways linking the town centre, which goes to Bas-Congo.
In 2007 Belgium assisted in a renovation of the country's internal rail network. This improved service to Kintambo, Ndolo, Limete, Lemba, Kasangulu, Gombe, Ndjili and Masina.
External transport.
Kinshasa is the major river port of the Congo. The port, called 'Le Beach Ngobila' extends for about 7 km along the river, comprising scores of quays and jetties with hundreds of boats and barges tied up. Ferries cross the river to Brazzaville, a distance of about 4 km. River transport also connects to dozens of ports upstream, such as Kisangani and Bangui.
There are road and rail links to Matadi, the sea port in the Congo estuary 150 km from the Atlantic Ocean.
There are no rail links from Kinshasa further inland, and road connections to much of the rest of the country are few and in poor condition.
The city has two airports: N'djili Airport is the main airport with connections to other African countries as well as to Brussels, Paris and some other destinations. N'Dolo Airport, located close to the city center, is used for domestic flights only with small turboprop aircraft.

</doc>
<doc id="45775" url="http://en.wikipedia.org/wiki?curid=45775" title="Colonization">
Colonization

Colonization (or colonisation) occurs whenever there is a large-scale migration of any one or more groups of people to a colonial area. The migrants, who can also be called colonizers, keep "strong links" with their previous country, and thus obtain tremendous "privileges" over other people living in the area being colonized. Of course, the prior country of such settlers were often big Imperial powers. Even though large-scale migration is usually a component of colonization, it is not a requirement; a strong enough military force is sometimes sufficient.
The term is derived from the Latin word "colere", which means to "to inhabit". Also, colonization refers strictly to migration, for example, to settler colonies in America or Australia, trading posts, and plantations, while colonialism deals with this, along with ruling the existing indigenous peoples of styled "new territories". So, colonization is a narrower category than the related concept of colonialism.
Colonization was linked to the spread of tens of millions of Europeans all over the world. In many settled colonies, European settlers formed a huge majority of the population. Examples include the Americas, Australia and New Zealand. These colonies were occasionally called 'neo-Europes'. In other places, European settlers formed minority groups, who were often dominant and privileged in their places of settlement.
When European settlers started to settle land such as Australia, they regarded such landmasses as "terra nullius". "Terra nullius" means 'empty land' in Latin. In other words, the settlers treated the land as uninhabited and a "clean slate" for colonization and colonial rule. However, these ideas were clearly not true, as such landmasses were often inhabited by indigenous populations. For example, it was estimated that there were 350,000 native people in Australia during the time when Europeans tried to conquer Australia. A similar process of appropriating land by colonizers can be observed in the late nineteenth century during the colonization of West Africa by Europe.The accepted practice among cartographers at the time was to display unexplored landscapes as "blank spaces". Instead of interpreting "blank spaces" as limited geographical knowledge imperialists saw them as vacant spaces awaiting colonists. Public perception of "blank spaces" was consistent with that of the colonizers; the illusion of "blank spaces" proved to be a successful trick.
Historical colonizations.
Classical period.
In ancient times, maritime nations such as the city-states of Greece and Phoenicia often established colonies so as to farm what they believed as uninhabited land. Land suitable for farming was often occupied by migratory 'barbarian tribes' who lived by hunting and gathering. To ancient Greeks and Phoenicians, these lands were regarded as simply vacant. However, this did not mean that conflict did not exist between the colonizers and native peoples. Greeks and Phoenicians also established colonies with the intent of regulating and expanding trade throughout the Mediterranean and Middle East.
Another period of colonization in ancient times was during the Roman Empire. The Roman Empire conquered large parts of Western Europe, North Africa and West Asia. In North Africa and West Asia, the Romans were often conquering what they regarded as 'civilized' peoples. As they moved north into Europe, they mostly encountered rural tribes with very little in the way of cities. In these areas, waves of Roman colonization often followed the conquest of the areas.
Many of the current cities around Europe began as Roman colonies, such as the German city Köln (Cologne), which was originally called "Colonia Claudia" by the Romans; and the British capital city of London which the Romans founded as "Londinium".
Middle Ages.
The decline and collapse of the Roman Empire saw (and was partly caused by) the large-scale movement of people in Eastern Europe and Asia. This is largely seen as beginning with nomadic horsemen from Asia (specifically the Huns) moving into the richer pasture land to the west and so forcing the people there to move further west and so on until eventually the Goths were forced to cross into the Roman Empire, resulting in continuous war with Rome which played a major role in the fall of the Roman Empire. It was this period that saw the large-scale movement of peoples establishing new colonies all over western Europe, the events of this time saw the development of many of the modern day nations of Europe, the Franks in France and Germany and the Anglo-Saxons in England.
In West Asia, during Sassanid Empire, some Persians established colonies in Yemen and Oman.
The Vikings of Scandinavia also carried out a large-scale colonization. The Vikings are best known as raiders, setting out from their original homelands in Denmark, southern Norway and southern Sweden, to pillage the coastlines of northern Europe. In time, the Vikings began trading, rather than raiding, and established colonies. The Vikings discovered Iceland and established colonies before moving onto Greenland, where they briefly held some colonies. The Vikings also launched an unsuccessful attempt at colonizing an area they called Vinland, which is probably at a site now known as L'Anse aux Meadows, Newfoundland and Labrador, on the eastern coastline of Canada.
Modern "Colonial Era" colonialism.
"Colonialism" in this context refers mostly to Western European countries' colonization of lands mainly in the Americas, Africa, Asia and Oceania; the main European countries active in this form of colonization included Spain, Portugal, France, the Kingdom of England, the Netherlands, and (from the 18th century) Great Britain. Each of these countries had a period of almost complete power in world trade at some stage in the era from roughly 1500 to 1900. Some reports characterize Chinese activities in Tibet as colonization.
While many colonization schemes focused on shorter-term exploitation of economic opportunities (Newfoundland, for example, or Siberia) or addressed specific goals (Massachusetts or New South Wales), a tradition also developed of careful long-term social planning based on elaborate theory-building (note James Oglethorpe's Colony of Georgia in the 1730s and Edward Gibbon Wakefield's New Zealand in the 1840s).
Colonization of Europe.
A number of scholars and analysts describe contemporary Muslim immigration to Europe as a process of colonization. Rauf Ceylan describes the Turkish communities of Germany as "ethnic colonies". Robert S. Leiken describes Muslim immigrant communities in Europe as "something like a Muslim internal colony," in which the immigrant becomes "not so much a member of British society as a colonial of his clan and village". Hans Magnus Enzensberger also uses the language of colonization. Christopher Caldwell writes that "'colonization' well describes the influx of the past half-century". First, because of the scale of the phenomenon, and, more significantly according to Caldwell, because the "terms" of the transformation are "set by the immigrants".
Modern colonization.
Colonization may be used as a method of absorbing and assimilating foreign people into the culture of the imperial country, and thus destroying any remnant of the foreign cultures that might threaten the imperial territory over the long term by inspiring rebellion. During the Russian Empire, a policy of Russification was followed, in order to impose the Russian language and culture on conquered people in territory adjacent to Russia itself. In this way, the Russian Empire aimed to gradually, and permanently, expand its territory by erasing foreign cultures. Foreign languages within its territory were banned, as were foreign religions. The policy of Russification was pursued during the Communist era as well. Under the Union of Soviet Socialist Republics, ethnic Russians were sent to colonize captured territory such as Lithuania, Latvia and Estonia, while local languages, religions and customs were banned or suppressed. Population transfer in the Soviet Union was also used both as a military strategy to extinguish opposition to Soviet expansion, and as a continuation of the Russification policy of assimilating, or failing that, eliminating ethnic minorities through exile to a distant territory such as Siberia.
In some cases, expatriate niches do set up permanently in target countries but whether this can be rightly called colonization is debatable precisely because of the ambiguity of intentions behind the movement and settling of expatriates and in many cases (especially when not gathered into a niche "per se") expatriates do not necessarily seek to "expand their native civilization", but rather to integrate into the population of the new civilization. It must be recognized that expatriates are different from exiles and often there is very little if no relationship between them. Exiles are more often than not diasporic or displaced communities or persons who have fled their native territory or homeland to somewhere else and are usually in this position due to the ramifications of war or other major political upheavals and sometimes this includes the influence of colonization.
Many nations also have large numbers of guest workers who are brought in to do seasonal work such as harvesting or to do low-paid manual labor. Guest workers or contractors have a lower status than workers with visas, because guest workers can be removed at any time for any reason. Many human colonists came to colonies as slaves, so the legal power to leave or remain may not be the issue so much as the actual presence of the people in the new country.
During the mid 20th century, there was the most dramatic and devastating attempt at colonization, and that was pursued with Nazism. Hitler and Heinrich Himmler schemed for a mass migration of Germans to Eastern Europe, where the Germans were to become colonists, having control over the native people. These indigenous people were planned 
to be reduced to slaves or wholly annihilated.
Science policy colonization.
There are suggestions of "science policy colonization", which argues that science policy is increasingly being dominated by scientific experts from developed, industrialized democracies. Scientists from poorer, emerging or developing democracies may mainly be given the role of collecting raw data. Experts from developed, industrialized democracies may have biases unchallenged that run counter to the best interests of emerging democracies such as South Africa. There are also concerns (UNESCO 1999) that the accountability mechanisms imposed on knowledge experts are inadequate.
Hypothetical or fictional types of colonization.
Ocean colonization.
The hypothetical permanent habitation of locations in Earth's oceans is called "ocean colonization". Related ideas such as the floating city are much less hypothetical - funds are presently being sought to build several large ships that would have permanent populations of up to 50,000 people each.
Space colonization.
In science fiction, space colonization is sometimes more benign. Humans find an uninhabited planet, and inhabit it. The colonization of Mars is an often-used example of this type of space colonization. In more recent science fiction, humans may create habitable space (by terraforming or constructing a space habitat) and call that a "colony".
On the other hand, if the planet is already inhabited, much less benign consequences ensue: indeed, some science fiction authors have used the colonization of alien planets by humans, or the colonization of Earth by aliens, to explore the real-world issues surrounding the phenomenon. Such works include those of Mary Doria Russell, "The Sparrow" and "Children of God".
The ultimate form of space colonization is the Kardashev scale which assumes that a single dominant civilisation will take over all energy on one planet, then one star, then a whole galaxy full of stars. However, this would not necessarily be so if other species were to be discovered during a galactic expansion. This may require more than one species to share the galactic space with each other as they both develop.

</doc>
<doc id="45777" url="http://en.wikipedia.org/wiki?curid=45777" title="Reform Act 1832">
Reform Act 1832

The Representation of the People Act 1832 (known informally as the 1832 Reform Act, Great Reform Act or First Reform Act to distinguish it from subsequent Reform Acts) was an Act of Parliament (indexed as 2 & 3 Will. IV c. 45) which introduced wide-ranging changes to the electoral system of England and Wales. According to its preamble, the Act was designed to "take effectual Measures for correcting divers Abuses that have long prevailed in the Choice of Members to serve in the Commons House of Parliament". Before the reform, most members nominally represented boroughs. The number of electors in a borough varied widely, from a dozen or so up to 12,000. Frequently the selection of MPs was effectively controlled by one powerful patron: for example Charles Howard, 11th Duke of Norfolk controlled eleven boroughs. Criteria for qualification for the franchise varied greatly between boroughs, from the requirement to own land, to merely living in a house with a hearth sufficient to boil a pot.
There had been calls for reform long before 1832, but without success. The Act which finally succeeded was proposed by the Whigs, led by the Prime Minister Charles Grey, 2nd Earl Grey. It met with significant opposition from the Pittite factions in Parliament which had long governed the country; opposition was especially pronounced in the House of Lords. Nevertheless the bill was eventually passed, mainly due to public pressure. The Act granted seats in the House of Commons to large cities that had sprung up during the Industrial Revolution, and removed seats from the "rotten boroughs": those with very small electorates and usually dominated by a wealthy patron. The Act also increased the electorate from about 500,000 to 813,000 which allowed about one out of five adult males to vote, from a total population (including women and children) of some 14 million.
The full title is "An Act to amend the representation of the people in England and Wales". Its formal short title and citation is "Representation of the People Act 1832 (2 & 3 Wm. IV, c. 45)". The Act applied only in England and Wales; the separate Scottish Reform Act 1832 and Irish Reform Act 1832 enacted similar legislation in those two countries.
The unreformed House of Commons.
Composition.
After the Act of Union 1800, sometimes referred to as the Act of Union 1801, the unreformed House of Commons was composed of 658 members, of whom 513 represented England and Wales. There were two types of constituencies; counties and boroughs. County members were supposed to represent landholders, while borough members were supposed to represent the mercantile and trading interests of the kingdom. Counties were historical national subdivisions established between the 8th and 16th centuries. They were not merely parliamentary constituencies: many components of government (including courts and the militia) were organised along county lines. The members of Parliament chosen by the counties were known as Knights of the Shire. In Wales each county elected one member, while in England each county elected two members until 1826, when Yorkshire's representation was increased to four, following the disenfranchisement of the Cornish borough of Grampound.
Parliamentary boroughs in England ranged wildly in size from small hamlets to large cities, partly because they had evolved haphazardly. The earliest boroughs were chosen in the Middle Ages by county sheriffs, and even a village might be deemed a borough. Many of these early boroughs (such as Winchelsea and Dunwich) were substantial settlements at the time of their original enfranchisement, but later went into decline, and by the early 19th century some only had a few electors, but still elected two MPs; they were often known as rotten boroughs. In later centuries the reigning monarch decided which settlements to enfranchise. The monarchs seem mostly to have done so capriciously, often with little regard for the merits of the place they were enfranchising. Of the 70 English boroughs that Tudor monarchs enfranchised, 31 were later disenfranchised. Finally, the parliamentarians of the 17th century compounded the inconsistencies by re-enfranchising 15 boroughs whose representation had lapsed for centuries, seven of which were later disenfranchised by the Reform Act. After Newark was enfranchised in 1661, no additional boroughs were enfranchised, and the unfair system remained unchanged until the Reform Act of 1832. Grampound's disenfranchisement in 1821 was the sole exception. Most English boroughs elected two MPs; but five boroughs elected only one MP: Abingdon, Banbury, Bewdley, Higham Ferrers and Monmouth. The City of London and the joint borough of Weymouth and Melcombe Regis each elected four members. The Welsh boroughs each returned a single member.
The franchise.
Statutes passed in 1430 and 1432, during the reign of Henry VI, standardised property qualifications for county voters. Under these Acts, all (male) owners of freehold property or land worth at least forty shillings in a particular county were entitled to vote in that county. This requirement, known as the forty shilling freehold, was never adjusted for inflation; thus the amount of land one had to own in order to vote gradually diminished over time. Nevertheless, the vast majority of people were not entitled to vote; the size of the English county electorate in 1831 has been estimated at only 200,000. Furthermore, the sizes of the individual county constituencies varied significantly. The smallest counties, Rutland and Anglesey, had fewer than 1,000 voters each, while the largest county, Yorkshire, had more than 20,000. Those who owned property in multiple constituencies could vote multiple times; there was usually no need to live in a constituency in order to vote there.
In boroughs the franchise was far more varied. There were broadly six types of parliamentary boroughs, as defined by their franchise:
Some boroughs had a combination of these varying types of franchise, and most had special rules and exceptions, so many boroughs had a form of franchise that was unique to themselves.
The largest borough, Westminster, had about 12,000 voters, while many of the smallest, usually known as "rotten boroughs", had fewer than 100 each. The most famous rotten borough was Old Sarum, which had 13 burgage plots that could be used to "manufacture" electors if necessary—usually around half a dozen was thought sufficient. Other examples were Dunwich (32 voters), Camelford (25), and Gatton (7).
Women's suffrage.
The claim for the women's vote appears to have been first made by Jeremy Bentham in 1817 when he published his "Plan of Parliamentary Reform in the form of a Catechism", and was taken up by William Thompson in 1825, when he published, with Anna Wheeler, "An Appeal of One Half the Human Race, Women, Against the Pretensions of the Other Half, Men, to Retain Them in Political, and Thence in Civil and Domestic Slavery: In Reply to Mr. Mill's Celebrated Article on Government". In the "celebrated article on Government", James Mill had stated:
...all those individuals whose interests are indisputably included in those of other individuals may be struck off without any inconvenience ... In this light also women may be regarded, the interests of almost all of whom are involved in that of their fathers or in that of their husbands.
The passing of the Act seven years later enfranchising "male persons" was, however, a more significant event; it has been argued that it was the inclusion of the word "male", thus providing the first explicit statutory bar to women voting, which provided a focus of attack and a source of resentment from which, in time, the women's suffrage movement grew.
Pocket boroughs, bribery.
Many constituencies, especially those with small electorates, were under the control of rich landowners, and were known as nomination boroughs or pocket boroughs, because they were said to be in the pockets of their patrons. Most patrons were noblemen or landed gentry who could use their local influence, prestige, and wealth to sway the voters. This was particularly true in rural counties, and in small boroughs situated near a large landed estate. Some noblemen even controlled multiple constituencies: for example, the Duke of Norfolk controlled eleven, while the Earl of Lonsdale controlled nine. Writing in 1821, Sydney Smith proclaimed that "The country belongs to the Duke of Rutland, Lord Lonsdale, the Duke of Newcastle, and about twenty other holders of boroughs. They are our masters!" T. H. B. Oldfield claimed in his "Representative History of Great Britain and Ireland" that, out of the 514 members representing England and Wales, about 370 were selected by nearly 180 patrons. A member who represented a pocket borough was expected to vote as his patron ordered, or else lose his seat at the next election.
Voters in some constituencies resisted outright domination by powerful landlords, but were often open to corruption. Electors were bribed individually in some boroughs, and collectively in others. In 1771, for example, it was revealed that 81 voters in New Shoreham (who constituted a majority of the electorate) formed a corrupt organisation that called itself the "Christian Club", and regularly sold the borough to the highest bidder. Especially notorious for their corruption were the "nabobs", or individuals who had amassed fortunes in the British colonies in Asia and the West Indies. The nabobs, in some cases, even managed to wrest control of boroughs from the nobility and the gentry. Lord Chatham, Prime Minister of Great Britain during the 1760s, casting an eye on the fortunes made in India commented that "the importers of foreign gold have forced their way into Parliament, by such a torrent of corruption as no private hereditary fortune could resist".
Movement for reform.
Early attempts at reform.
During the 1640s, England endured a civil war that pitted King Charles I and the Royalists against the Parliamentarians. In 1647, different factions of the victorious parliamentary army held a series of discussions, the Putney Debates, on reforming the structure of English government. The most radical elements proposed universal manhood suffrage and the reorganisation of parliamentary constituencies. Their leader Thomas Rainsborough declared, "I think it's clear, that every man that is to live under a government ought first by his own consent to put himself under that government." More conservative members disagreed, arguing instead that only individuals who owned land in the country should be allowed to vote. For example, Henry Ireton stated, "no man hath a right to an interest or share in the disposing of the affairs of the kingdom ... that hath not a permanent fixed interest in this kingdom." The views of the conservative "Grandees" eventually won out. Oliver Cromwell, who became the leader of England after the abolition of the monarchy in 1649, refused to adopt universal suffrage; individuals were required to own property (real or personal) worth at least £200 in order to vote. He did nonetheless agree to some electoral reform; he disfranchised several small boroughs, granted representation to large towns such as Manchester and Leeds, and increased the number of members elected by populous counties. These reforms were all reversed, however, after Cromwell's death and the last parliament to be elected in the Commonwealth period in 1659 reverted to the electoral system as it had existed under Charles I.
Following Restoration of the monarchy in 1660 the issue of parliamentary reform lay dormant until it was revived in the 1760s by the Whig Prime Minister William Pitt, 1st Earl of Chatham ("Pitt the Elder"), who called borough representation "the rotten part of our Constitution" (hence the term "rotten borough"). Nevertheless, he did not advocate an immediate disfranchisement of rotten boroughs. He instead proposed that a third member be added to each county, to countervail the borough influence. The Whigs failed to unite behind the expansion of county representation; some objected to the idea because they felt that it would give too much power to the aristocracy and gentry in rural areas. Ultimately, despite Chatham's exertions, Parliament took no action on his proposals. The cause of parliamentary reform was next taken up by Lord Chatham's son, William Pitt the Younger (variously described as a Tory and as an "independent Whig"). Like his father, he shrank from proposing the wholesale abolition of the rotten boroughs, advocating instead an increase in county representation. The House of Commons rejected Pitt's resolution by over 140 votes, despite receiving petitions for reform bearing over twenty thousand signatures. In 1783, Pitt became Prime Minister but was still unable to achieve reform. King George III was averse to the idea, as were many members of Pitt's own cabinet. In 1786, the Prime Minister proposed a reform bill, but the House of Commons rejected it on a 174–248 vote. Pitt did not raise the issue again for the remainder of his term.
Aftermath of the French Revolution.
Support for parliamentary reform plummeted after the launch of the French Revolution in 1789. Reacting to the excesses of the revolution, many English politicians became steadfastly opposed to any major political change. Despite this reaction, several Radical Movement groups were established to agitate for reform. A group of Whigs led by James Maitland, 8th Earl of Lauderdale and Charles Grey founded an organisation advocating parliamentary reform in 1792. This group, known as the Society of the Friends of the People, included 28 MPs. In 1793, Grey presented to the House of Commons a petition from the Friends of the People, outlining abuses of the system and demanding change. He did not propose any specific scheme of reform, but merely a motion that the House inquire into possible improvements. Parliament's reaction to the French Revolution was so negative, that even this request for an inquiry was rejected by a margin of almost 200 votes. Grey tried to raise the subject again in 1797, but the House again rebuffed him by a majority of over 150.
Other notable pro-reform organisations included the Hampden Clubs (named after John Hampden, an English politician who opposed the Crown during the English Civil War) and the London Corresponding Society (which consisted of workers and artisans). But the "Radical" reforms supported by these organisations (for example, universal suffrage) found even less support in Parliament. For example, when Sir Francis Burdett, chairman of the London Hampden Club, proposed a resolution in favour of universal suffrage, equally sized electoral districts, and voting by secret ballot to the House of Commons, his motion found only one other supporter (Lord Cochrane) in the entire House.
Despite such setbacks, popular pressure for reform remained strong. In 1819, a large pro-reform rally was held in Birmingham. Although the city was not entitled to any seats in the Commons, those gathered decided to elect Sir Charles Wolseley as Birmingham's "legislatorial representative". Following their example, reformers in Manchester held a similar meeting to elect a "legislatorial attorney". Between 20,000 and 60,000 (by different estimates) attended the event, many of them bearing signs such as "Equal Representation or Death". The protesters were ordered to disband; when they did not, the Manchester Yeomenry suppressed the meeting by force. Eleven people were killed and several hundred injured, the event later to become known as the Peterloo Massacre. In response, the government passed the Six Acts, measures designed to quell further political agitation. In particular, the Seditious Meetings Act prohibited groups of more than 50 people from assembling to discuss any political subject without prior permission from the sheriff or magistrate.
Reform during the 1820s.
Since the House of Commons regularly rejected direct challenges to the system of representation by large majorities, supporters of reform had to content themselves with more modest measures. The Whig Lord John Russell brought forward one such measure in 1820, proposing the disfranchisement of the notoriously corrupt borough of Grampound in Cornwall. He suggested that the borough's two seats be transferred to the city of Leeds. Tories in the House of Lords agreed to the disfranchisement of the borough, but refused to accept the precedent of directly transferring its seats to an industrial city. Instead, they modified the proposal so that two further seats were given to Yorkshire, the county in which Leeds is situated. In this form, the bill passed both houses and became law. In 1828, Lord John Russell suggested that Parliament repeat the idea by abolishing the corrupt boroughs of Penryn and East Retford, and by transferring their seats to Manchester and Birmingham. This time, however, the House of Lords rejected his proposals. In 1830, Russell proposed another, similar scheme: the enfranchisement of Leeds, Manchester, and Birmingham, and the disfranchisement of the next three boroughs found guilty of corruption; again, the proposal was rejected.
Support for reform came from an unexpected source—a faction of the Tory Party—in 1829. The Tory government under Arthur Wellesley, 1st Duke of Wellington, responding to the danger of civil strife in largely Roman Catholic Ireland, drew up the Catholic Relief Act 1829. This legislation repealed various laws that imposed political disabilities on Roman Catholics, in particular laws that prevented them from becoming members of Parliament. In response, disenchanted Tories who perceived a danger to the established religion came to favour parliamentary reform, in particular the enfranchisement of Manchester, Leeds, and other heavily Noncomformist cities in northern England.
Passage of the Reform Act.
First Reform Bill.
The death of King George IV on 26 June 1830 dissolved Parliament by law, and a general election was held. Electoral reform, which had been frequently discussed during the preceding parliamentary session, became a major campaign issue. Across the country, several pro-reform "political unions" were formed, made up of both middle and working class individuals. The most influential of these was the Birmingham Political Union, led by Thomas Attwood. These groups confined themselves to lawful means of supporting reform, such as petitioning and public oratory, and achieved a high level of public support.
The Tories won a majority in the election, but the party remained divided, and support for the Prime Minister (the Duke of Wellington) was weak. When the Opposition raised the issue of reform in one of the first debates of the year, the Duke made a controversial defence of the existing system of government, recorded in the formal "third-party" language of the time:
The Prime Minister's absolutist views proved extremely unpopular, even within his own party. Less than two weeks after Wellington made these remarks, he was forced to resign after he was defeated in a motion of no confidence. Sydney Smith wrote, "Never was any administration so completely and so suddenly destroyed; and, I believe, entirely by the Duke's declaration, made, I suspect, in perfect ignorance of the state of public feeling and opinion." Wellington was replaced by the Whig reformer Charles Grey, who had by this time the title of Earl Grey.
Lord Grey's first announcement as Prime Minister was a pledge to carry out parliamentary reform. On 1 March 1831, Lord John Russell brought forward the Reform Bill in the House of Commons on the government's behalf. The bill disfranchised 60 of the smallest boroughs, and reduced the representation of 47 others. Some seats were completely abolished, while others were redistributed to the London suburbs, to large cities, to the counties, and to Scotland and Ireland. Furthermore, the bill standardised and expanded the borough franchise, increasing the size of the electorate (according to one estimate) by half a million voters.
On 22 March, the vote on the second reading attracted a record 608 members, including the non-voting Speaker (the previous record was 530 members). Despite the high attendance, the second reading was approved by only one vote, and further progress on the Reform Bill was difficult. During the committee stage, Isaac Gascoyne put forward a motion objecting to provisions of the bill that reduced the total number of seats in the House of Commons. This motion was carried, against the government's wishes, by nine votes. Thereafter, the ministry lost a vote on a procedural motion by 22 votes. As these divisions indicated that Parliament was against the Reform Bill, the ministry decided to request a dissolution and take its appeal to the people.
Second Reform Bill.
The political and popular pressure for reform had grown so great that pro-reform Whigs won an overwhelming House of Commons majority in the general election of 1831. The Whig party won almost all constituencies with genuine electorates, leaving the Tories with little more than the rotten boroughs. The Reform Bill was again brought before the House of Commons, which agreed to the second reading by a large majority in July. During the committee stage, opponents of the bill slowed its progress through tedious discussions of its details, but it was finally passed in September, by a margin of more than 100 votes.
The Bill was then sent up to the House of Lords, a majority in which was known to be hostile to it. After the Whigs' decisive victory in the 1831 election, some speculated that opponents would abstain, rather than openly defy the public will. Indeed, when the Lords voted on the second reading of the bill after a memorable series of debates, many Tory peers did refrain from voting. However, the Lords Spiritual mustered in unusually large numbers, and of 22 present, 21 voted against the Bill. It failed by 41 votes.
When the Lords rejected the Reform Bill, public violence ensued. That very evening, riots broke out in Derby, where a mob attacked the city jail and freed several prisoners. In Nottingham, rioters set fire to Nottingham Castle (the home of the Duke of Newcastle) and attacked Wollaton Hall (the estate of Lord Middleton). The most significant disturbances occurred at Bristol, where rioters controlled the city for three days. The mob broke into prisons and destroyed several buildings, including the palace of the Bishop of Bristol, the mansion of the Lord Mayor of Bristol, and several private homes. Other places that saw violence included Dorset, Leicestershire, and Somerset.
Meanwhile, the political unions, which had hitherto been separate groups united only by a common goal, decided to form the National Political Union. Perceiving this group as a threat, the government issued a proclamation pursuant to the Corresponding Societies Act 1799 declaring such an association "unconstitutional and illegal", and commanding all loyal subjects to shun it. The leaders of the National Political Union ignored this proclamation, but leaders of the influential Birmingham branch decided to co-operate with the government by discouraging activities on a national level.
Third Reform Bill.
After the Reform Bill was rejected in the Lords, the House of Commons immediately passed a motion of confidence affirming their support for Lord Grey's administration. Because parliamentary rules prohibited the introduction of the same bill twice during the same session, the ministry advised the King to prorogue Parliament. As soon as the new session began in December 1831, the Third Reform Bill was brought forward. The bill was in a few respects different from its predecessors; it no longer proposed a reduction in the total membership of the House of Commons, and it reflected data collected during the census that had just been completed. The new version passed in the House of Commons by even larger majorities in March 1832; it was once again sent up to the House of Lords.
Realizing that another rejection would not be politically feasible, opponents of reform decided to use amendments to change the bill's essential character: for example, they voted to delay consideration of clauses in the bill that disfranchised the rotten boroughs. The ministers believed that they were left with only one alternative: to create a large number of new peerages, swamping the House of Lords with pro-reform votes. But the prerogative of creating peerages rested with King William IV, who recoiled from so drastic a step and rejected the unanimous advice of his cabinet. Lord Grey then resigned, and the King invited the Duke of Wellington to form a new government.
The ensuing period became known as the "Days of May", with so great a level of political agitation that some feared revolution. Some protesters advocated non-payment of taxes, and urged a run on the banks, and one day signs appeared across London reading "Stop the Duke; go for gold!" £1.8 million was withdrawn from the Bank of England in the first days of the run (out of about £7 million total gold in the Bank's possession). The National Political Union and other organisations sent petitions to the House of Commons, demanding that they withhold supply (cut off funding to the government) until the House of Lords should acquiesce. Some demonstrations called for the abolition of the nobility, and some even of the monarchy. In these circumstances, the Duke of Wellington had great difficulty in building support for his premiership, despite promising moderate reform. He was unable to form a government, leaving William IV with no choice but to recall Lord Grey. Eventually the King consented to fill the House of Lords with Whigs; however, without the knowledge of his cabinet, he circulated a letter among Tory peers, encouraging them to desist from further opposition, and warning them of the consequences of continuing. At this, enough opposition peers relented. By abstaining from further votes, they allowed the legislation to pass in the House of Lords, and the Crown was not forced to create new peers. The bill finally received the Royal Assent on 7 June 1832, thereby becoming law.
Results.
Provisions.
Abolition of seats.
The Reform Act's chief objective was the reduction of the number of nomination boroughs. There were 203 boroughs in England before the Act. The 56 smallest of these, as measured by their housing stock and tax assessments, were completely abolished. The next 30 smallest boroughs each lost one of their two MPs. In addition Weymouth and Melcombe Regis's four members were reduced to two. Thus in total the Act abolished 143 borough seats in England (one of the boroughs to be completely abolished, Higham Ferrers, had only a single representative).
Creation of new seats.
In their place the Act created 130 new seats in England and Wales:
Thus 65 new county seats and 65 new borough seats were created in England and Wales. The total number of English members fell by 17 and the number in Wales increased by four. The boundaries of the new divisions and parliamentary boroughs were defined in a separate Act, the Parliamentary Boundaries Act 1832.
Extension of the franchise.
The Act also extended the franchise. In county constituencies, in addition to forty-shilling freeholders, franchise rights were extended to owners of land in copyhold worth £10 and holders of long-term leases (more than sixty years) on land worth £10 and holders of medium-term leases (between twenty and sixty years) on land worth £50 and to tenants-at-will paying an annual rent of £50. In borough constituencies all male householders living in properties worth at least £10 a year were given the right to vote – a measure which introduced to all boroughs a standardised form of franchise for the first time. Existing borough electors retained a lifetime right to vote, however they had qualified, provided they were resident in the boroughs in which they were electors. In those boroughs which had freemen electors, voting rights were to be enjoyed by future freemen as well provided their freemanship was acquired through birth or apprenticeship and they too were resident.
The Act also introduced a system of voter registration, to be administered by the overseers of the poor in every parish and township. It instituted a system of special courts to review disputes relating to voter qualifications. It also authorised the use of multiple polling places within the same constituency, and limited the duration of polling to two days. (Formerly, polls could remain open for up to forty days.)
The Reform Act itself did not affect constituencies in Scotland or Ireland. However, reforms there were carried out by the Scottish Reform Act and the Irish Reform Act. Scotland received eight additional seats, and Ireland received five; thus keeping the total number of seats in the House of Commons the same as it had been before the Act. While no constituencies were disfranchised in either of those countries, voter qualifications were standardised and the size of the electorate was expanded in both.
Effects.
Local Conservative Associations began to educate citizens about the Party's platform and encouraged them to register to vote annually, as mandated by the Act. Press coverage of national politics in the local press was joined by in-depth reports on provincial politics in the national press. Grassroots Conservatives therefore saw themselves as part of a national political movement during the 1830s.
The size of the pre-Reform electorate is difficult to estimate. Voter registration was lacking, and many boroughs were rarely contested in elections. It is estimated that immediately before the 1832 Reform Act, 400,000 English subjects were entitled to vote, and that after passage, the number rose to 650,000, an increase of more than 60%.
Tradesmen, such as shoemakers, believed that the Reform Act had given them the vote. One example is the shoemakers of Duns, Berwickshire. They created a banner celebrating the Reform Act which declared ""The battle's won. Britannia's sons are free"." This banner is on display at People's History Museum in Manchester.
Many major commercial and industrial cities became separate parliamentary boroughs under the Act. The new constituencies saw party conflicts inside the middle-class, and between the middle-class and working-class. Iwami looked at elections in the medium-sized borough of Halifax, 1832–1852, and reports that the party organizations, and the voters themselves, depended heavily on local social relationships and localized institutions. Having the vote encouraged many men to become much more active in the political, economic and social sphere.
Tenant voters.
Most of the pocket boroughs abolished by the Reform Act belonged to the Tory Party. These losses were somewhat offset by the extension of the vote to tenants-at-will paying an annual rent of £50. This clause, proposed by the Tory Marquess of Chandos, was adopted in the House of Commons despite opposition from the Government. The tenants-at-will thereby enfranchised typically voted as instructed by their landlords, who in turn normally supported the Tory party. This concession, together with the Whig Party's internal divisions and the difficulties faced by the nation's economy, allowed the Tories under Sir Robert Peel to make gains in the elections of 1835 and 1837, and to retake the House of Commons in 1841.
Krein examines the votes in the House and reports that the traditional landed interest "suffered very little" by the terms of the 1832 Act. They continued to dominate Commons, while losing a bit of their power to enact laws that focused on their more parochial interests. By contrast, Krein argues, the 1867 Reform Act caused serious erosion of their legislative power and the 1874 elections saw great landowners losing their county seats to the votes of tenant farmers in England and especially in Ireland.
Limitations.
The Reform Act did very little to appease the working class by enfranchising them, since voters were required to possess property worth £10, a substantial sum at the time. This split the alliance between the working class and the middle class, giving rise to the Chartist Movement.
Although it did disenfranchise most rotten boroughs, a few remained, such as Totnes in Devon and Midhurst in Sussex. Also, bribery of voters remained a problem. As Sir Thomas Erskine May observed, "it was too soon evident, that as more votes had been created, more votes were to be sold".
The Reform Act strengthened the House of Commons by reducing the number of nomination boroughs controlled by peers. Some aristocrats complained that, in the future, the government could compel them to pass any bill, simply by threatening to swamp the House of Lords with new peerages. The Duke of Wellington lamented: "If such projects can be carried into execution by a minister of the Crown with impunity, there is no doubt that the constitution of this House, and of this country, is at an end. [...] [T]here is absolutely an end put to the power and objects of deliberation in this House, and an end to all just and proper means of decision." The subsequent history of Parliament, however, shows that the influence of the Lords was largely undiminished. They compelled the Commons to accept significant amendments to the Municipal Reform Bill in 1835, forced compromises on Jewish emancipation, and successfully resisted several other bills supported by the public.
Further reform.
During the ensuing years, Parliament adopted several more minor reforms. Acts of Parliament passed in 1835 and 1836 increased the number of polling places in each constituency, and reduced polling to a single day. Parliament also passed several laws aimed at combatting corruption, including the Corrupt Practices Act 1854, though these measures proved largely ineffectual. Neither party strove for further major reform; leading statesmen on both sides regarded the Reform Act as a final settlement.
There was considerable public agitation for further expansion of the electorate, however. In particular, the Chartist movement, which demanded universal suffrage for men, equally sized electoral districts, and voting by secret ballot, gained a widespread following. But the Tories were united against further reform, and the Liberal Party (successor to the Whigs) did not seek a general revision of the electoral system until 1852. The 1850s saw Lord John Russell introduce a number of reform bills to correct defects the first act had left unaddressed. However, no proposal was successful until 1867, when Parliament adopted the Second Reform Act.
Assessment.
Several historians credit the Reform Act 1832 with launching modern democracy in Britain. G. M. Trevelyan hails 1832 as the watershed moment at which "'the sovereignty of the people' had been established in fact, if not in law". Sir Erskine May notes that "[the] reformed Parliament was, unquestionably, more liberal and progressive in its policy than the Parliaments of old; more vigorous and active; more susceptible to the influence of public opinion; and more secure in the confidence of the people", but admitted that "grave defects still remained to be considered". Other historians have taken a far less laudatory view, arguing that genuine democracy began to arise only with the Second Reform Act in 1867, or perhaps even later. Norman Gash states that "it would be wrong to assume that the political scene in the succeeding generation differed essentially from that of the preceding one". E. A. Smith proposes, in a similar vein, that "when the dust had settled, the political landscape looked much as it had done before.
Historians have long pointed out that, in 1829–31, it was the Ultra-Tories or "Country Party" which pressed most strongly for Reform, regarding it as a means of weakening Wellington's ministry, which had disappointed them by granting Catholic emancipation and by its economic policies.
Evans (1996) emphasises that the Reform Act "opened a door on a new political world". Although Grey's intentions were conservative, Evans says, and the 1832 Act gave the aristocracy an additional half-century's control of Parliament, the Act nevertheless did open constitutional questions for further development. Evans argues it was the 1832 Act, not the later reforms of 1867, 1884, or 1918, that were decisive in bringing representative democracy to Britain. Evans concludes the Reform Act marked the true beginning of the development of a recognisably modern political system.
References.
Bibliography.
</dl>
Further reading.
</dl>

</doc>
<doc id="45780" url="http://en.wikipedia.org/wiki?curid=45780" title="List of sculptors">
List of sculptors

This is a list of sculptors - people who are known for their three-dimensional artistic creations (this can include artists who use sound and light).
 :

</doc>
<doc id="45782" url="http://en.wikipedia.org/wiki?curid=45782" title="Spencer Tracy">
Spencer Tracy

Spencer Bonaventure Tracy (April 5, 1900 – June 10, 1967) was an American actor, noted for his natural style and versatility. One of the major stars of Hollywood's Golden Age, Tracy was nominated for nine Academy Awards for Best Actor and won two, sharing the record for nominations in that category with Laurence Olivier.
Tracy discovered his talent for acting while attending Ripon College, and later received a scholarship for the American Academy of Dramatic Arts. He spent seven years in the theatre, working in a succession of stock companies and intermittently on Broadway. Tracy's breakthrough came in 1930, when his lead performance in "The Last Mile" caught the attention of Hollywood. After a successful film debut in "Up the River", Tracy was signed to a contract with Fox Film Corporation. His five years with Fox were unremarkable, and he remained largely unknown to audiences after 25 films. In 1935, Tracy joined Metro-Goldwyn-Mayer, Hollywood's most prestigious studio. His career flourished with a series of hit films, and in 1937 and 1938 he won consecutive Oscars for "Captains Courageous" and "Boys Town". By the 1940s, Tracy was one of the studio's top stars. In 1942 he appeared with Katharine Hepburn in "Woman of the Year", beginning a popular partnership that produced nine movies over 25 years.
Tracy left MGM in 1955 and continued to work regularly as a freelance star, despite an increasing weariness as he aged. His personal life was troubled, with a lifelong struggle against alcoholism and guilt over his son's deafness. Tracy became estranged from his wife in the 1930s but never divorced, conducting a long-term relationship with Katharine Hepburn in private. Towards the end of his life, Tracy worked almost exclusively for director Stanley Kramer. It was for Kramer that he made his last film, "Guess Who's Coming to Dinner" (1967), completed 17 days before Tracy's death.
During his career, Tracy appeared in 75 films and developed a reputation among his peers as one of the screen's greatest actors. In 1999, the American Film Institute ranked Tracy as one of the top ten Hollywood legends.
Early life.
Tracy was born on April 5, 1900 in Milwaukee, Wisconsin. He was the second son of Caroline Brown (1874-1942) and John Edward Tracy (1873-1928), a truck salesman. His mother was a Presbyterian from a wealthy Midwestern family and his father was of Irish Catholic background. His one brother, Carroll, was four years older.
Spencer was a difficult and hyperactive child  with poor school attendance. Raised as a Catholic, at nine years old he was placed in the hands of Dominican nuns in the hope of transforming his behavior. Later in life he remarked, "I never would have gone back to school if there had been any other way of learning to read the subtitles in the movies." He became fascinated with motion pictures, watching the same ones repeatedly and then re-enacting scenes to his friends and neighbors. Tracy attended several Jesuit academies in his teenage years, which he claimed took the "badness" out of him and helped him improve his grades. At Marquette Academy he met future actor Pat O'Brien, and the pair began attending plays together, awakening Tracy's interest in the theatre.
With little care for his studies and "itching for a chance to go and see some excitement", Tracy enlisted in the United States Navy when he turned 18. He was sent to the Naval Training Station in North Chicago, where he was still a student when World War I came to an end. He achieved the rank of seaman second class, but never went to sea and was discharged in February 1919. John Tracy's desire to see one of his sons gain a college degree drove Tracy back to high school to finish his diploma. Studies at two more institutions plus the additional allowance of "war credits" won Tracy a place at Ripon College. He entered Ripon in February 1921, declaring his intention to major in medicine.
"It helped me develop memory for lines that has been a godsend since I started stage work; it gave me something of a stage presence; and it helped get rid of my awkwardness. Also, I gradually developed the ability to speak extemporaneously".
—Tracy was a key member of his college debating team, which he later said helped with his acting career.
Tracy was a popular student at Ripon, where he served as president of his hall and was involved in a number of college activities. He made his stage debut in June 1921, playing the male lead in "The Truth". Tracy was very well received in this role  and he quickly developed a passion for the stage. He formed an acting company with friends, which they called "The Campus Players" and took on tour. As a member of the college debate team, Tracy excelled in arguing and public speaking. It was during a tour with his debate team that Tracy auditioned for the American Academy of Dramatic Arts (AADA) in New York City. He was offered a scholarship to attend the school after performing a scene from one of his earlier roles.
Tracy left Ripon, and began classes at AADA in April 1922. He was deemed fit to progress to the senior class, allowing him to join the academy stock company. Tracy made his New York debut in October 1922, in a play called "The Wedding Guests", and then his Broadway debut three months later playing a wordless robot in "R.U.R." He graduated from AADA in March 1923.
Career.
Stock theatre and Broadway (1923–1930).
Immediately following graduation, Tracy joined a new stock company based in White Plains, New York where he was given periphery roles. Unhappy there, he moved to a company in Cincinnati, but failed to make an impact. In November 1923 he landed a small part on Broadway in the comedy "A Royal Fandango", starring Ethel Barrymore. Reviews for the show were poor and it closed after 25 performances; Tracy later said of the failure, "My ego took an awful beating." When he took a position with a struggling company in New Jersey, Tracy was living on an allowance of 35 cents a day. In January 1924 he played his first leading role with a company in Winnipeg, but the organization soon closed.
Tracy finally achieved some success by joining forces with the notable stock manager William H. Wright in the spring of 1924. A stage partnership was formed with the young actress Selena Royle, who had already made her name on Broadway. It proved a popular draw and their productions were favorably received. One of these shows brought Tracy to the attention of a Broadway producer, who offered him the lead in a new play. "The Sheepman" previewed in October 1925, but it received poor reviews and closed after its trial run in Connecticut. Dejected, Tracy was forced back to Wright and the stock circuit.
In the fall of 1926, Tracy was offered his third shot at Broadway: a role in a new George M. Cohan play called "Yellow". Tracy swore that if the play failed to be a hit he would leave stock and work in a "regular" business instead. Tracy was nervous about working with Cohan—one of the most important figures in American theatre—but during rehearsals Cohan announced, "Tracy, you're the best goddamned actor I've ever seen!" "Yellow" opened on September 21; reviews were mixed but it ran for 135 performances. It was the beginning of an important collaboration for Tracy: "I'd have quit the stage completely," he later commented, "if it hadn't been for George M. Cohan." Cohan wrote a part specifically for Tracy in his next play, "The Baby Cyclone". It opened on Broadway in September 1927 and proved to be a hit.
Tracy followed this success with another Cohan play, "Whispering Friends", and in 1929 took over from Clark Gable in "Conflict", a Broadway drama . A variety of other roles followed, but it was the lead in "Dread", written by Pulitzer Prize-winning dramatist Owen Davis that gave Tracy high hopes for success. The story of a man's descent into madness, "Dread" previewed in Brooklyn to an excellent reception, but the very next day—October 29—the New York stock market crashed. Unable to attain funding, "Dread" did not open on Broadway. Following this disappointment, Tracy considered leaving the theatre and returning to Milwaukee for a more stable life.
In January 1930, Tracy was approached about a new play called "The Last Mile". Looking to cast the lead role of a murderer on death row, producer Herman Shumlin met with Tracy, and later recounted: "beneath the surface, here was a man of passion, violence, sensitivity and desperation: no ordinary man, and just the man for the part." "The Last Mile" opened on Broadway in February, where Tracy's performance was met by a standing ovation that lasted 14 curtain calls. The "Commonweal" described him as "one of our best and most versatile young actors". The play was a hit with critics, and ran for 289 performances.
Fox (1930–1935).
In 1930, Broadway was being heavily scouted for actors to work in the "talkies", the new medium of sound film. Tracy was cast in two Vitaphone short movies ("Taxi Talks" and "The Hard Guy"), but he had not considered becoming a film actor: "I had no ambition in that direction and I was perfectly happy on the stage", he later explained in an interview. One of the scouts who saw Tracy in "The Last Mile" was director John Ford. Ford wanted Tracy for the lead role in his next picture, a prison movie. Production company Fox Film Corporation were unsure about Tracy, saying that he did not photograph well, but Ford convinced them that he was right for the role. "Up the River" (1930) marked the film debut of both Tracy and Humphrey Bogart. After seeing the rushes, Fox immediately offered Tracy a long-term contract. Knowing that he needed the money for his family—his young son was deaf and recovering from polio—Tracy signed with Fox and moved to California. He appeared on the stage again only once more in his life.
Winfield Sheehan, the head of Fox, committed to making Tracy a bankable commodity. The studio went to efforts to promote the actor, releasing adverts for his second film "Quick Millions" (1931) with the headline "A New Star Shines." Three films were made in quick succession, all of which were unsuccessful at the box office. Tracy found himself typecast in comedies, usually playing a crook or a con man. The mold was broken with his seventh picture, "Disorderly Conduct" (1932), and it was the first of his films since "Up the River" to make a profit.
In mid-1932, after nine pictures, Tracy remained virtually unknown to the public. He considered leaving Fox once his contract was up for renewal, but a rise in his weekly rate to $1,500 convinced him to stay. He continued to appear in unpopular films, with "Me and My Gal" (1932) setting an all-time low attendance record for the Roxy Theatre in New York City. He was loaned to Warner Bros. for "20,000 Years in Sing Sing" (1932), a prison drama co-starring Bette Davis. Tracy was hopeful that it would be his break-out role, but despite good reviews this failed to materialize.
Critics began to notice Tracy with "The Power and the Glory" (1933). The story of a man's rise to prosperity, written by Preston Sturges, Tracy's performance as railroad tycoon Tom Garner received uniformly strong reviews. William Wilkerson of "The Hollywood Reporter" wrote: "This sterling performer has finally been given an opportunity to show an ability that has been boxed in by gangster roles ... [the film] has introduced Mr. Tracy as one of the screen's best performers". Mordaunt Hall of "The New York Times" stated: "No more convincing performance has been given on the screen than Spencer Tracy's impersonation of Tom Garner." "Shanghai Madness" (1933), meanwhile, gave Tracy a previously unseen sex appeal and served to advance his standing. Despite this attention, Tracy's next two movies went largely unnoticed. "Man's Castle" (1933) with Loretta Young was anticipated to be a hit, but made only a small profit. "The Show Off" (1934), for which he was lent to Metro-Goldwyn-Mayer, proved popular, but his subsequent outings continued to be unsuccessful.
Tracy drank heavily during his years with Fox, and gained a reputation as an alcoholic. He failed to report for filming on "Marie Galante" in June 1934, and was found in his hotel room, virtually unconscious after a two-week binge. Tracy was removed from the Fox payroll while he recovered in a hospital, and then sued for $125,000 for delaying the production. He completed only two more pictures with the studio.
The details on how Tracy's relationship with Fox ended are unclear: later in life Tracy maintained that he was fired for his drunken behavior, but the Fox records do not support such an account. He was still under contract with the studio when MGM expressed their interest in the actor. They were in need of a new male star, and contacted Tracy on April 2, 1935, offering him a seven-year deal. That afternoon, the contract between Tracy and Fox was terminated "by mutual consent". Tracy made a total of 25 pictures in the five years he was with Fox Film Corporation, most of which lost money at the box office.
Metro-Goldwyn-Mayer (1935–1955).
Growing reputation.
In the 1930s, Metro-Goldwyn-Mayer was the most respected movie production studio in Hollywood. When Tracy arrived there, his own reputation was not strong. Biographer James Curtis writes: "Tracy was scarcely a blip on the box office barometer in 1935, a critics' darling and little more". He was, however, well known for being a troublemaker. Producer Irving G. Thalberg was nevertheless enthusiastic about working with the actor, telling journalist Louella Parsons: "Spencer Tracy will become one of MGM's most valuable stars."
Curtis notes that the studio managed Tracy with care, a welcome change from the ineptitude he had known at Fox, which was like "a shot of adrenaline" for the actor. His first film under the new contract was the quickly produced "The Murder Man" (1935), which included the feature film debut of James Stewart. Thalberg then began a strategy of pairing Tracy with the studio's top actresses: "Whipsaw" (1935) co-starred Myrna Loy and was a commercial success. "Riffraff" (1936) put Tracy opposite Jean Harlow. Both films were, however, designed and promoted to showcase their leading ladies, thus continuing Tracy's reputation as a secondary star.
"Fury" (1936) was the first film to prove that Tracy could make a success on his own merit. Directed by Fritz Lang, Tracy played a man who swears revenge after narrowly escaping death by a lynch mob. The film and performance received excellent reviews. It was popular with the public, going on to make $1.3 million worldwide. Curtis writes: "audiences who, just a year earlier, had no clear handle on him, were suddenly turning out to see him. It was a transition that was nothing short of miraculous ... [and showed] a willingness on the part of the public to embrace a leading man who was not textbook handsome nor bigger than life."
"Fury" was followed one month later with the release of the big-budget disaster movie "San Francisco" (1936). Tracy played a supporting role alongside Clark Gable in the film, allowing audiences to see him with the top male star in Hollywood. Taking on the role of a priest, Tracy reportedly felt a heavy responsibility in representing the church. Despite having only 17 minutes of screen time, Tracy was highly praised for his performance and received an Oscar nomination for Best Actor. "San Francisco" became the highest grossing picture of 1936. Donald Deschner, in his book on Tracy, credits "Fury" and "San Francisco" as the "two films that changed his career and gave him the status of a major star."
By this point, Tracy entered a period of self-imposed sobriety and MGM expressed pleasure with Tracy's professionalism. His public reputation continued to grow with "Libeled Lady" (1936), a screwball comedy that cast him with William Powell, Loy and Harlow. According to Curtis, "Powell, Harlow and Loy were among the biggest draws in the industry, and equal billing in such a powerhouse company could only serve to advance Tracy's standing". "Libeled Lady" was his third hit picture in the space of six months.
Oscar wins.
Tracy appeared in four movies in 1937. "They Gave Him a Gun" went largely unnoticed, but "Captains Courageous" was one of the major film events of the year. Tracy played a Portuguese fisherman in the adventure movie, based on the novel by Rudyard Kipling. He was uncomfortable feigning a foreign accent, and resented having his hair curled, but the role was a hit with audiences and Tracy won the Academy Award for Best Actor. "Captains Courageous" was followed by "Big City" with Luise Rainer and "Mannequin" with Joan Crawford, the latter of which took good billings at the box office. With two years of hit movies and industry recognition, Tracy became a star in the United States. A 1937 poll of 20 million people to find the "King and Queen of Hollywood" ranked Tracy sixth among males. Tracy was reunited with Gable and Loy for 1938's "Test Pilot". The film was another commercial and critical success, permanently cementing the notion of Gable and Tracy as a team.
Based on the positive response he had received in "San Francisco", MGM again cast Tracy as a priest in "Boys Town" (1938). Portraying Edward J. Flanagan, a Catholic priest and founder of Boys Town, was a role Tracy took seriously: "I'm so anxious to do a good job as Father Flanagan that it worries me, keeps me awake at night." Tracy received strong reviews for his performance, and the movie grossed $4 million worldwide. For the second year running, Tracy received an Academy Award for Best Actor. He was humble about the recognition, saying in his acceptance speech: "I honestly do not feel that I can accept this award ... I can accept it only as it was meant to be for a great man—Father Flanagan". He immediately sent the Academy Award statuette to Flanagan. Tracy was listed as the fifth biggest money-making star of 1938.
Tracy was absent from screens for almost a year before returning to Twentieth Century-Fox on loan and appearing as Henry M. Stanley in "Stanley and Livingstone", his only film of 1939. Curtis maintains that Tracy's non-visibility did little to affect his standing with the public or exhibitors. In October of that year, a "Fortune" magazine survey to find the nation's favorite movie actor listed Tracy in first place.
Established star.
MGM capitalized on Tracy's popularity, casting him in four movies for 1940. "I Take This Woman" with Hedy Lamarr was a critical and commercial failure, but the historical drama "Northwest Passage"—Tracy's first film in Technicolor—proved popular. He then portrayed Thomas Edison in "Edison, the Man". Howard Barnes of the "New York Herald Tribune" was not charmed by the story, but wrote that Tracy, "by sheer persuasion of his acting", made the film worthy. "Boom Town" was the third and final Gable-Tracy picture, also featuring Claudette Colbert and Hedy Lamarr, making it one of the most anticipated films of the year. The film opened to the biggest crowd since "Gone With the Wind".
Tracy signed a new contract with MGM in April 1941, which paid $5,000 a week and limited him to three pictures a year (Tracy had previously expressed a need to reduce his workload). The contract also stated for the first time that his billing was to be "that of a star". Contrary to popular belief, the contract did not include a clause that he receive top billing, but from this point onwards, every film Tracy appeared in featured his name in pole position.
In 1941, Tracy returned to the role of Father Flanagan in "Men of Boys Town". It was followed later that year by Tracy's only venture into the horror genre, an adaptation of "Dr. Jekyll and Mr. Hyde", co-starring Ingrid Bergman and Lana Turner. Tracy was unhappy with the film, disliking the heavy make-up he needed to portray Hyde. Critical response to the film was mixed. Theodore Strauss of "The New York Times" wrote that "Mr. Tracy's portrait of Hyde is not so much evil incarnate as it is the ham rampant." The film was popular with audiences, however, taking in more than $2 million at the box office.
Tracy was set to star in a film version of "The Yearling" for 1942, but on-set difficulties and bad weather forced the production to close. With the end of that project, he became available for the new Katharine Hepburn movie, "Woman of the Year" (1942). Hepburn greatly admired Tracy, calling him "the best movie actor there was". She had wanted him for her comeback vehicle, "The Philadelphia Story" (1940). Hepburn was delighted that Tracy was available for "Woman of the Year", saying "I was just damned grateful he was willing to work with me." The romantic comedy performed well at the box office and received strong reviews. William Boehnel wrote in the "New York World-Telegram", "To begin with, it has Katharine Hepburn and Spencer Tracy in the leading roles. This in itself would be enough to make any film memorable. But when you get Tracy and Hepburn turning in brilliant performances to boot, you've got something to cheer about."
"Woman of the Year" was followed by an adaptation of John Steinbeck's "Tortilla Flat" (1942) which met with a tepid response. MGM did not hesitate to repeat the teaming of Tracy and Hepburn and cast them in the dark mystery "Keeper of the Flame" (1942). Despite a weak critical reception the film was a popular success, outgrossing its predecessor and confirming the strength of the partnership.
Tracy's next three appearances were all war-based. "A Guy Named Joe" (1943) with Irene Dunne surpassed "San Francisco" to become his highest-grossing film to date. "The Seventh Cross" (1944), about an escape from a Nazi concentration camp, met with critical acclaim. It was followed by the aviation film "Thirty Seconds Over Tokyo" (1944). On the strength of these three releases, the annual Quigley poll revealed Tracy was MGM's biggest money-making star of 1944. His only film the following year was "Without Love" (1945), a third film with Hepburn that performed well at the box office despite muted enthusiasm from critics.
Stage and screen.
In 1945, Tracy returned to the stage for the first time in 15 years. He had been through a dark patch personally—culminating with a stay in hospital—and Hepburn felt that a play would help restore his focus. Tracy told a journalist in April, "I'm coming back to Broadway to see if I can still act." The play was "The Rugged Path" by Robert E. Sherwood. It first previewed in Providence on September 28, to a sold out crowd and tepid response. It was a difficult production; director Garson Kanin later wrote: "In the ten days prior to the New York opening all the important relationships had deteriorated. Spencer was tense and unbending, could not, or would not, take direction". Tracy considered leaving the show before it even opened on Broadway, and lasted there just six weeks before announcing his intention to close the show. It closed on January 19, 1946, after 81 performances. Tracy later explained to a friend: "I couldn't say those goddamn lines over and over and over again every night ... At least every day is a new day for me in films ... But this thing—every day, every day, over and over again."
Tracy was absent from screens in 1946, the first year since his motion picture debut that there was no Spencer Tracy release. His next film was "The Sea of Grass" (1947) a drama set in the American Old West with Hepburn. Similarly to "Keeper of the Flame" and "Without Love", a lukewarm response from critics did not stop it from being a financial success both at home and abroad. He followed it later that year with "Cass Timberlane", in which he played a judge. It was a commercial success, but Curtis notes that co-star Lana Turner overshadowed Tracy in most of the reviews.
A fifth film with Hepburn came in 1948, Frank Capra's political drama "State of the Union". Tracy played a presidential candidate in the movie, which was warmly received. He then appeared in "Edward, My Son" (1949) with Deborah Kerr. Tracy disliked the role, and told director George Cukor, "It's rather disconcerting to me to find how easily I play a heel." Upon its release, "The New Yorker" wrote of the "hopeless miscasting of Mr. Tracy". The film became Tracy's biggest money-loser at MGM.
Tracy finished off the 1940s with "Malaya" (1949), an adventure film with James Stewart, and "Adam's Rib" (1949), a comedy with Tracy and Hepburn playing married lawyers who oppose each other in court. Tracy and Hepburn's friends, Garson Kanin and Ruth Gordon, wrote the parts specifically for the duo. The film received strong reviews and became the highest grossing Tracy-Hepburn picture to date. Film critic Bosley Crowther wrote, "Mr. Tracy and Miss Hepburn are the stellar performers in this show and their perfect compatibility in comic capers is delightful to see."
Final MGM years.
Tracy received his first Academy Award nomination in 12 years for playing the role of Stanley Banks in "Father of the Bride" (1950). In the comedy film, Banks attempts to handle preparations for his daughter's (played by Elizabeth Taylor) upcoming wedding. "It's the second strong comedy in a row for Spencer Tracy, doing the title role, and he socks it", "Variety" noted. The film was the biggest commercial success of Tracy's career to date, earning $6 million worldwide. MGM wanted a sequel, and while Tracy was unsure, he accepted. "Father's Little Dividend" (1951) was released ten months later and performed well at the box office. On the strength of the two movies, Tracy polled as one of the nation's top stars once again.
In 1951, Tracy portrayed a lawyer in "The People Against O'Hara". The next year he re-teamed with Hepburn for the sports comedy "Pat and Mike" (1952), the second feature written expressly for the pair by Kanin and Gordon. "Pat and Mike" became one of the duo's most popular and critically acclaimed films. Tracy followed it with "Plymouth Adventure" (1952), a historical drama set abroad the "Mayflower", co-starring Gene Tierney. It met with a poor response and posted a loss of $1.8 million. In 1953, Tracy returned to the role of a concerned father in "The Actress". "That film ... got more [acclaim] from the critics than any film I ever made in all the years, and we didn't make enough to pay for the ushers in the theatre," recalled producer Lawrence Weingarten. For his performance in "The Actress", Tracy won a Golden Globe Award and received a nomination for the British Academy Film Award (BAFTA) .
MGM lent Tracy to Twentieth Century-Fox for the Western film "Broken Lance", his only appearance of 1954. The picture was well received. In 1955 Tracy turned down William Wyler's "The Desperate Hours" because he refused to take second-billing to Humphrey Bogart. Instead, Tracy appeared as a one-armed protagonist who faces the hostility of a small town in "Bad Day at Black Rock" (1955), a film directed by John Sturges. For his work, Tracy received a fifth Oscar nomination and was awarded the Best Actor prize at the Cannes Film Festival. He had personally been unhappy with the picture, and threatened to leave it during production. This type of petulant behavior became a regular occurrence for the aging Tracy, who was increasingly lethargic and cynical. He began production on "Tribute to a Bad Man" in the summer of 1955, but pulled out when the location shooting in Colorado gave him altitude sickness. The trouble caused by the picture fractured Tracy's relationship with MGM. In June 1955 he was the last remaining star of the studio's heyday, but with his contract up for renewal—Tracy opted to go independent for the first time in his movie career.
Independent player (1956–1967).
Tracy's first post-MGM appearance was in "The Mountain" (1956) with Robert Wagner, who played his much younger brother (Wagner had earlier played his son in "Broken Lance"). The location filming in the French Alps proved a difficult experience, and he threatened to leave the project. His performance earned a BAFTA nomination for Best Foreign Actor. Tracy and Hepburn then paired together for the eighth time in the office-based comedy "Desk Set" (1957). He again had to be convinced to stay with the film which met with a weak response.
In 1958, Tracy appeared in "The Old Man and the Sea", a project that had been in development for five years. An adaptation of Ernest Hemingway's novella of the same name, Hemingway's agent, Leland Hayward, had previously written to the author: "Of all Hollywood people, the one that comes the closest to me in quality, in personality and voice, in personal dignity and ability, is Spencer Tracy." Tracy was delighted to be offered the role. He was told to lose some of his 210 pounds before filming began, but failed to do so. Hemingway thus reported that Tracy was a "terrible liability to the picture", and had to be reassured that the star was being carefully photographed to disguise his weight. Appearing alone on screen for the majority of the film, Tracy considered "The Old Man and the Sea" the toughest part he ever played. In reviewing the performance, Jack Moffitt of the "Hollywood Reporter" said it was "so intimate and revealing of universal human experience that, to me, it almost transcended acting and became reality." Tracy received Oscar and BAFTA Award nominations for the work.
After abandoning two projects, including a proposed remake of "The Blue Angel" with Marilyn Monroe, Tracy's next feature was "The Last Hurrah" (1958). It reunited him with his debut director, John Ford, after 28 years. Tracy took a year to commit to the project, in which he played an Irish-American mayor seeking re-election. The movie was favorably reviewed, but not commercially successful. At the end of 1958, the National Board of Review named Tracy the year's Best Actor. He nevertheless began to ponder retirement, with Curtis writing that he was "chronically tired, unhappy, ill, and uninterested in work."
Stanley Kramer partnership.
Tracy did not appear on the screen again until October 1960, with the release of "Inherit the Wind", a film based on the 1925 Scopes "Monkey Trial" which debated the right to teach evolution in schools. Director Stanley Kramer sought Tracy for the role of lawyer Clarence Darrow from the outset. Starring opposite Tracy was Fredric March, a pairing "Variety" described as "a stroke of casting genius ... Both men are spellbinders in the most laudatory sense of the word." The film garnered Tracy some of the strongest reviews of his career—he was nominated for an Academy Award, BAFTA Award and Golden Globe Award for the performance—but it was not a commercial hit.
In the volcano disaster movie "The Devil at 4 O'Clock" (1961), Tracy played a priest for the fourth time in his career. His co-star, Frank Sinatra, ceded top-billing to guarantee Tracy for the picture. Continuing his pattern of indecisiveness, Tracy briefly pulled out of the production before recommitting. Critics were unenthusiastic about the film, which was nevertheless Tracy's most successful box office outing since "Father of the Bride".
"Inherit the Wind" began an enduring collaboration between Stanley Kramer and Tracy—Kramer directed Tracy's three final films. "Judgment at Nuremberg", released at the end of 1961, was their second feature together. The film depicts the "Judges' Trial", the trial of Nazi judges for their role in the Holocaust. Abby Mann wrote the role of Judge Haywood with Tracy in mind; Tracy called it the best script he had ever read. At the end of the film, Tracy delivered a 13-minute speech. He recorded it in one take, and received a round of applause from the cast and crew. Upon seeing the film, Mann wrote to Tracy: "Every writer ought to have the experience of having Spencer Tracy do his lines. There is nothing in the world quite like it." The film met with positive reviews and a large audience; Tracy received an eighth Oscar nomination for his performance.
Tracy turned down roles in "Long Days Journey Into Night" (1962) and "The Leopard" (1963), and had to pull out of MGM's all-star "How the West Was Won" (1962) when it clashed with "Judgment at Nuremberg". He was, however, able to record the film's narration track. Tracy was in very poor health by this time, and working became a challenge. He took the role of Captain T. G. Culpeper in Kramer's comedy "It's a Mad, Mad, Mad, Mad World" (1963), a small but key part that he was able to complete in nine days. Tracy's name topped the list of performers, and the comedy became the highest grossing American film of the year. As his health worsened he had to cancel commitments to "Cheyenne Autumn" (1964) and "The Cincinnati Kid" (1965). Offers continued to come, but Tracy did not work again until Kramer's "Guess Who's Coming to Dinner" (1967), Tracy's ninth and final film with Hepburn.
"Guess Who's Coming to Dinner" explored the topic of interracial marriage, with Tracy playing a liberal-minded newspaper publisher whose values are challenged when his daughter wishes to marry a black man, played by Sidney Poitier. Tracy was happy to be working again, but told the press the movie would be his last. To commence filming, Tracy had to be insured for the high premium of $71,000; Hepburn and Kramer both put their salaries in escrow until Tracy completed his scenes. In poor health, Tracy could only work for two or three hours each day. He completed his last scene on May 24, 1967. Tracy died 17 days later from a heart attack on June 10.
The film was released in December, and although reviews were mixed, Curtis notes that "Tracy's performance was singled out for praise in nearly every instance." Brendan Gill of "The New Yorker" wrote that Tracy gave "a faultless and, under the circumstances, heartbreaking performance." The movie became Tracy's highest grossing picture. He received a posthumous nomination for Best Actor—his ninth—at the 40th Academy Awards, along with a Golden Globe Award nomination and a BAFTA win for Best Actor.
Personal life.
Marriage, family and Hepburn.
Tracy met actress Louise Treadwell while they were both members of the Wood Players in White Plains, New York—the first stock company Tracy joined after graduating. The couple were engaged in May 1923 and married on September 10 of that year between the matinee and evening performances of his show. Their son, John Ten Broeck Tracy, was born in June 1924. When John was 10 months old, Louise discovered that the boy was deaf. She resisted telling Tracy for three months. Tracy was devastated by the news and felt a lifelong guilt over his son's deafness. He was convinced that John's hearing impairment was a punishment for his own sins, e.g. adultery. As a result, Tracy had trouble connecting with his son and distanced himself from his family. Joseph L. Mankiewicz, a friend of Tracy's, later theorized: "[Tracy] didn't leave Louise. He left the scene of his guilt." A second child, Louise "Susie" Treadwell Tracy, was born in July 1932. The children were raised in their mother's Episcopalian faith.
Tracy left the family home in 1933. He and Louise openly discussed the separation with the media, maintaining that they were still friends and had not taken divorce action. From September 1933 to June 1934, Tracy had a public affair with Loretta Young, his co-star in "Man's Castle". He reconciled with Louise in 1935. There was never again an official separation between Tracy and his wife, but the marriage continued to be troubled. Tracy increasingly lived in hotels and by the 1940s, the two were effectively living separate lives. Tracy frequently engaged in extramarital affairs, including with co-stars Joan Crawford in 1937, and Ingrid Bergman in 1941.
While making "Woman of the Year" in September 1941, Tracy began a relationship with Katharine Hepburn. The actress became devoted to him and their relationship lasted until his death 26 years later. Tracy never returned to live in the family home, although he visited regularly. The MGM moguls were careful to protect their contract big stars from controversy, and Tracy wished to conceal his relationship with Hepburn from his wife, so it was hidden from the public. The couple did not live together until the final years of Tracy's life. In Hollywood, the intimate nature of the Tracy-Hepburn partnership was an open secret. Angela Lansbury, who worked with the pair on "State of the Union", later said: "We all knew, but nobody ever said anything. In those days it wasn't discussed." Tracy was not someone to express his emotions, but friend Betsy Drake believed he "was "utterly" dependent upon [Hepburn]." The infidelity continued, including an affair with Gene Tierney during the making of "Plymouth Adventure" in 1952.
Neither Tracy nor his wife ever pursued a divorce, despite their estrangement. He told Joan Fontaine, "I can get a divorce whenever I want to, but my wife and Kate like things just as they are." Louise, meanwhile, reportedly commented: "I will be Mrs. Spencer Tracy until the day I die." Hepburn did not interfere and never fought for marriage. In the 21st century, a man named Scotty Bowers claimed he was Tracy's occasional lover in the 1940s and 1950s. Biographer James Curtis has refuted these claims as "unverifiable", and said there is no evidence of Bowers in any of Tracy's personal documents.
Character.
Tracy was an avowed Catholic, but his cousin, Jane Feely, said that he did not devoutly follow the religion: "he was often not a practical Catholic either. I would call him a spiritual Catholic." Garson Kanin, a friend of Tracy's for 25 years, described him as "a true believer" who respected his religion. At periods in his life, Tracy attended Mass regularly. Tracy did not believe actors should publicize their political views, but in 1940 lent his name to the "Hollywood for Roosevelt" committee and personally identified as a Democrat.
Tracy struggled with alcoholism throughout his adult life, an ailment that ran in his father's side of the family. Rather than being a steady drinker, as commonly thought, he was prone to periods of binging on alcohol. Loretta Young remarked that Tracy was "awful" when he was drunk, and he was twice arrested for his behavior while intoxicated. Because of this bad reaction to alcohol, Tracy regularly embarked on prolonged periods of sobriety, and developed an all-or-nothing routine. Hepburn commented that he could stop drinking for "months, even years at a time".
Tracy was prone to bouts of depression and anxiety: he was described by Mrs. Tracy as having "the most volatile disposition I've ever seen—up in the clouds one minute and down in the depths the next. And when he's low, he's very, very low." He was plagued by insomnia throughout his life. As a result, Tracy became dependent on barbiturates to sleep, followed by dexedrine to function. Hepburn, who adopted a nursing role towards Tracy, was unable to understand her partner's unhappiness. She wrote in her autobiography: "What was it? ... Never at peace ... Tortured by some sort of guilt. Some terrible misery."
Illness and death.
Years of drinking, smoking, taking pills and being overweight left Tracy in poor health. He aged rapidly during the 1950s and appeared considerably older than he actually was. On July 21, 1963, he was hospitalized after a severe attack of breathlessness. Doctors found that he was suffering from pulmonary edema, where fluid accumulates in the lungs due to an inability of the heart to pump properly. They also declared his blood pressure as dangerously high. From this point on Tracy remained very weak, and Hepburn moved into his home to provide constant care. In January 1965, he was diagnosed with hypertensive heart disease, and began treatment for a previously ignored diagnosis of diabetes. Tracy almost died in September 1965: a stay in the hospital following a prostatectomy resulted in his kidneys failing, and he spent the night in a coma. His recovery was described by his doctor as "a kind of miracle".
Tracy spent the majority of the next two years at home with Hepburn, living what she described as a quiet life: reading, painting and listening to music. On June 10, 1967, Tracy awakened at 3:00 am to make himself a cup of tea in his apartment in Beverly Hills, California. Hepburn described in her autobiography how she followed him to the kitchen: "Just as I was about to give [the door] a push, there was a sound of a cup smashing to the floor—then clump—a loud clump." She entered the room to find Tracy dead from a heart attack. Hepburn recalled, "He looked so happy to be done with living, which for all his accomplishments had been a frightful burden for him." MGM publicist Howard Strickling told the media that Tracy had been alone when he died, and was found by his housekeeper.
A Requiem Mass was held for Tracy on June 12 at the Immaculate Heart of Mary Catholic Church in East Hollywood. Active pallbearers included George Cukor, Stanley Kramer, Frank Sinatra, James Stewart and John Ford. Out of consideration for Tracy's family, Hepburn did not attend the funeral. Tracy was interred in Forest Lawn Memorial Park in Glendale, California.
Reputation and acting style.
Tracy had a high reputation among his peers and received considerable praise from the film industry. After his death, Dore Schary, head of Metro-Goldwyn-Mayer in the 1950s, said, "There can be no question that he was the best and most protean actor of our screen." Humphrey Bogart, Clark Gable, James Cagney, John Ford, Garson Kanin and Katharine Hepburn also called Tracy the greatest actor of his generation. Richard Widmark, who idolized Tracy, said: "He's the greatest movie actor there ever was ... I've learned more about acting from watching Tracy than in any other way."
Tracy was particularly respected for his naturalism on screen. Hume Cronyn, who worked with Tracy on "The Seventh Cross", admired his co-star's screen presence: "His method appeared to be as simple as it is difficult to achieve. He appeared to do nothing. He listened, he felt, he said the words without forcing anything." Joan Crawford likewise expressed her admiration for Tracy's seemingly effortless performances. His four-time co-star Joan Bennett said, "One never had the feeling he was 'acting' in a scene, but the truth of the situation was actually happening, spontaneously, at the moment he spoke his lines." James Cagney noted that Tracy was rarely the target of impressionists, because "You can't mimic reserve and control very well" and "there's nothing to imitate except his genius and that can't be mimicked." Specifically, Tracy was praised for his listening and reacting skills. Barry Nelson said that Tracy "brought the art of reacting to a new height"; Stanley Kramer declared that Tracy "thought and listened better than anyone in the history of motion pictures". Millard Kaufman noted that "[Tracy] listened with every fiber of his entire body".
"I've never known what acting is. Who can honestly say what it is? ... I wonder what actors are supposed to be, if not themselves ... I've finally narrowed it down to where, when I begin a part, I say to myself, this is Spencer Tracy as a judge, or this is Spencer Tracy as a priest or as a lawyer, and let it go at that. Look, the only thing an actor has to offer a director and finally an audience is his instinct. That's all."
–Spencer Tracy, giving his opinion on acting.
Despite the perception of being able to perform effortlessly, acquaintances of Tracy said that he would carefully prepare for each role. Joseph L. Mankiewicz lived with Tracy during the production of "Test Pilot", and recounted that the actor would lock himself in his bedroom "working extremely hard" each night. Many co-workers commented on his strong work-ethic and professionalism. Tracy did not like to rehearse, however, and would lose his effectiveness after two or three takes. Kanin described him as "an instinctive player, who trusted the moment of creation." Tracy's close friend Chester Erskine pinpointed his acting style as one of "selection"—he strove to give as little as was needed to be effective—reaching "a minimum to make the maximum."
Tracy disliked when he was asked about his technique, or about what advice he would give to others. He often belittled the profession, for instance saying to Kanin, "Why do actors think they're so God damn important? They're not. Acting is not an important job in the scheme of things. Plumbing is." Tracy was humble about his abilities, telling a journalist, "it's just that I try no tricks. No profile. No 'great lover' act ... I just project myself as I am—plain, trying to be honest." He was known to have enjoyed the quip once made by Alfred Lunt: "The art of acting is—learn your lines!" Katharine Hepburn, in an interview six years after Tracy's death, suggested that Tracy wished he had held a different profession.
Assessment and legacy.
In the 21st century, Tracy is best known to general audiences for his association with Katharine Hepburn. He continues to receive praise from film scholars: critic Leonard Maltin calls Tracy "one of the 20th century’s finest actors", while film historian Jeanine Basinger describes his career as a "golden record of movie achievement". Charles Matthews, writing for "The Washington Post", argues that "Tracy deserves to be remembered for himself, as a master of acting technique".
Preserving Tracy's legacy, an award for excellence in film acting is bestowed in his name at the University of California, Los Angeles. Past recipients of the UCLA Spencer Tracy Award include James Stewart, Michael Douglas, Denzel Washington, Tom Hanks, Anthony Hopkins, Kirk Douglas and Morgan Freeman.
A documentary about Tracy was made in 1986, entitled "The Spencer Tracy Legacy". First broadcast by PBS, and hosted by Katharine Hepburn, it includes footage of Tracy's career and interviews with his former co-stars. In 2009, Tracy provided inspiration for the character Carl in Pixar's Oscar-winning film "Up". Director Pete Docter explained that there is "something sweet about these grumpy old guys". In 2014, a film about Tracy's relationship with Katharine Hepburn was announced to be in development.
Several of Tracy's films, particularly his later comedies, are regarded as classics of American cinema. He starred in four of the titles on the American Film Institute's list of "100 Years ... 100 Laughs": "Adam's Rib", "It's a Mad, Mad, Mad, Mad World", "Father of the Bride" and "Woman of the Year". "Guess Who's Coming to Dinner" was included on AFI's list of the 100 greatest American movies, while "Captains Courageous" featured on their list of America's most inspiring movies.
Awards and nominations.
Tracy was nominated for nine Academy Awards for Best Actor, a category record he holds with Laurence Olivier. He was the first of nine actors to win the award twice, and is one of two actors to receive it consecutively, the other being Tom Hanks. Tracy was also nominated for five British Academy Film Awards, of which he won two, and four Golden Globe Awards, winning once. In addition, he received the Cannes Film Festival award for Best Actor and was once named Best Actor by the National Board of Review.
Tracy was recognized by the Academy of Motion Picture Arts and Sciences for the following performances:
Filmography.
Selected filmography:

</doc>
<doc id="45783" url="http://en.wikipedia.org/wiki?curid=45783" title="Genotype-phenotype distinction">
Genotype-phenotype distinction

The genotype–phenotype distinction is drawn in genetics. "Genotype" is an organism's full hereditary information. "Phenotype" is an organism's actual observed properties, such as morphology, development, or behavior. This distinction is fundamental in the study of inheritance of traits and their evolution.
It is the organism's physical properties which directly determine its chances of survival and reproductive output, while the inheritance of physical properties occurs only as a secondary consequence of the inheritance of genes. Therefore, to properly understand the theory of evolution via natural selection, one must understand the genotype–phenotype distinction. The genes contribute to a trait, and the phenotype is the observable expression of the genes (and therefore the genotype that affects the trait). Say a white mouse had the recessive genes that caused the genes that cause the color of the mouse to be inactive (so "cc"). Its genotype would be responsible for its phenotype (the white color)..
The mapping of a set of genotypes to a set of phenotypes is sometimes referred to as the genotype–phenotype map. 
An organism's genotype is a major (the largest by far for morphology) influencing factor in the development of its phenotype, but it is not the only one. Even two organisms with identical genotypes normally differ in their phenotypes. One experiences this in everyday life with monozygous (i.e. identical) twins. Identical twins share the same genotype, since their genomes are identical; but they never have the same phenotype, although their phenotypes may be very similar. This is apparent in the fact that their mothers and close friends can always tell them apart, even though others might not be able to see the subtle differences. Further, identical twins can be distinguished by their fingerprints, which are never completely identical.
The concept of phenotypic plasticity defines the degree to which an organism's phenotype is determined by its genotype. A high level of plasticity means that environmental factors have a strong influence on the particular phenotype that develops. If there is little plasticity, the phenotype of an organism can be reliably predicted from knowledge of the genotype, regardless of environmental peculiarities during development. An example of high plasticity can be observed in larval newts1: when these larvae sense the presence of predators such as dragonflies, they develop larger heads and tails relative to their body size and display darker pigmentation. Larvae with these traits have a higher chance of survival when exposed to the predators, but grow more slowly than other phenotypes.
In contrast to phenotypic plasticity, the concept of genetic canalization addresses the extent to which an organism's phenotype allows conclusions about its genotype. A phenotype is said to be canalized if mutations (changes in the genome) do not noticeably affect the physical properties of the organism. This means that a canalized phenotype may form from a large variety of different genotypes, in which case it is not possible to exactly predict the genotype from knowledge of the phenotype (i.e. the genotype-phenotype map is not invertible). If canalization is not present, small changes in the genome have an immediate effect on the phenotype that develops.
The terms "genotype" and "phenotype" were created by Wilhelm Johannsen in 1911.

</doc>
<doc id="45784" url="http://en.wikipedia.org/wiki?curid=45784" title="Biomimetics">
Biomimetics

Biomimetics or biomimicry is the imitation of the models, systems, and elements of nature for the purpose of solving complex human problems. The terms biomimetics and biomimicry come from Ancient Greek: βίος ("bios"), life, and μίμησις ("mīmēsis"), imitation, from μιμεῖσθαι ("mīmeisthai"), to imitate, from μῖμος ("mimos"), actor. A closely related field is bionics.
Living organisms have evolved well-adapted structures and materials over geological time through natural selection. Biomimetics has given rise to new technologies inspired by biological solutions at macro and nanoscales. Humans have looked at nature for answers to problems throughout our existence. Nature has solved engineering problems such as self-healing abilities, environmental exposure tolerance and resistance, hydrophobicity, self-assembly, and harnessing solar energy.
Possible applications.
Biomimetics could in principle be applied in many fields. Because of the complexity of biological systems, the number of features that might be imitated is large. Some examples of biomimetic applications at various stages of development from prototypes to technologies that might become commercially usable include:
History.
One of the early examples of biomimicry was the study of birds to enable human flight. Although never successful in creating a "flying machine", Leonardo da Vinci (1452–1519) was a keen observer of the anatomy and flight of birds, and made numerous notes and sketches on his observations as well as sketches of "flying machines". The Wright Brothers, who succeeded in flying the first heavier-than-air aircraft in 1903, derived inspiration from observations of pigeons in flight.
Biomimetics was coined by the American biophysicist and polymath Otto Schmitt during the 1950s. It was during his doctoral research that he developed the Schmitt trigger by studying the nerves in squid, attempting to engineer a device that replicated the biological system of nerve propagation. He continued to focus on devices that mimic natural systems and by 1957 he had perceived a converse to the standard view of biophysics at that time, a view he would come to call biomimetics.
Biophysics is not so much a subject matter as it is a point of view. It is an approach to problems of biological science utilizing the theory and technology of the physical sciences. Conversely, biophysics is also a biologist's approach to problems of physical science and engineering, although this aspect has largely been neglected.—Otto Herbert Schmitt, In Appreciation, A Lifetime of Connections: Otto Herbert Schmitt, 1913 - 1998
A similar term, "Bionics" was coined by Jack Steele in 1960 at Wright-Patterson Air Force Base in Dayton, Ohio where Otto Schmitt also worked. Steele defined bionics as "the science of systems which have some function copied from nature, or which represent characteristics of natural systems or their analogues". During a later meeting in 1963 Schmitt stated,
Let us consider what bionics has come to mean operationally and what it or some word like it (I prefer biomimetics) ought to mean in order to make good use of the technical skills of scientists specializing, or rather, I should say, despecializing into this area of research—Otto Herbert Schmitt, In Appreciation, A Lifetime of Connections: Otto Herbert Schmitt, 1913 - 1998
In 1969 the term biomimetics was used by Schmitt to title one of his papers, and by 1974 it had found its way into Webster's Dictionary, bionics entered the same dictionary earlier in 1960 as "a science concerned with the application of data about the functioning of biological systems to the solution of engineering problems". Bionic took on a different connotation when Martin Caidin referenced Jack Steele and his work in the novel "Cyborg" which later resulted in the 1974 television series "The Six Million Dollar Man" and its spin-offs. The term bionic then became associated with "the use of electronically operated artificial body parts" and "having ordinary human powers increased by or as if by the aid of such devices". Because the term "bionic" took on the implication of supernatural strength, the scientific community in English speaking countries largely abandoned it.
The term "biomimicry" appeared as early as 1982. Biomimicry was popularized by scientist and author Janine Benyus in her 1997 book "Biomimicry: Innovation Inspired by Nature". Biomimicry is defined in the book as a "new science that studies nature's models and then imitates or takes inspiration from these designs and processes to solve human problems". Benyus suggests looking to Nature as a "Model, Measure, and Mentor" and emphasizes sustainability as an objective of biomimicry.
Nanobiomimetics or Nanobiomimicry.
Fabrication.
Biomorphic mineralization is a technique that produces materials with morphologies and structures resembling those of natural living organisms by using bio-structures as templates for mineralization. Compared to other methods of material production, biomorphic mineralization is facile, environmentally benign and economic.
Biomedicine.
Mimicking the diving behavior of animals, researchers have recently discovered that humans have a similar capacity to lower brain temperature and suppress metabolism for neuroprotection. This has now opened a real possibility of devising means for humans to sustain this state, not unlike the elusive and enigmatic feat of animal hibernation, e.g., lemurs (primates) and bears. This would have profound biomedical implications for healthcare and for treating an unmatched range and diversity of serious life-threatening clinical conditions, and in a fully personalized way, things like stroke, blood-loss, burns, cancer, chronic obesity, epileptic seizures, etc. An experimental trial, recently conducted in Sweden seemingly resulted in a sustainable variant of this state in a human breath-hold diver.
Nanowires, nanotubes, and quantum dots.
A virus is a nonliving particle ranging from the size of 20 to 300 nm capsules containing genetic material used to infect its host. The outer layer of viruses are remarkably robust and capable of withstanding temperatures as high as 60 °C and stay stable in a wide range of pH range of 2-10. Viral capsids can be used to create several nano device components such as nanowires, nanotubes, and quantum dots. Tubular virus particles such as the tobacco mosaic virus (TMV) can be used as templates to create nanofibers and nanotubes since both the inner and outer layers of the virus are charged surfaces and can induce nucleation of crystal growth. This was demonstrated though the production of platinum and gold nanotubes using TMV as a template. Mineralized virus particles have been shown to withstand various pH values by mineralizing the viruses with different materials such as silicon, PbS, and CdS and could therefore serve as a useful carriers of material. A spherical plant virus called cowpea chlorotic mottle virus (CCMV) has interesting expanding properties when exposed to environments of pH higher than 6.5. Above this pH, 60 independent pores with diameters about 2 nm begin to exchange substance with the environment. The structural transition of the viral capsid can be utilized in Biomorphic mineralization for selective uptake and deposition of minerals by controlling the solution pH. Applications include using the viral cage to produce uniformly shaped and sized quantum dot semiconductor nanoparticles through a series of pH washes. This is an alternative to the apoferritin cage technique currently used to synthesize uniform CdSe nanoparticles. Such materials could also be used for targeted drug delivery since particles release contents upon exposure to specific pH levels.
Display technology.
 
Morpho butterfly wings contain microstructures that create its coloring effect through structural coloration rather than pigmentation. Incident light waves are reflected at specific wavelengths to create vibrant colors due to multilayer interference, diffraction, thin film interference, and scattering properties. The scales of these butterflies consist of microstructures such as ridges, cross-ribs, ridge-lamellae, and microribs that have been shown to be responsible for coloration. The structural color has been simply explained as the interference due to alternating layers of cuticle and air using a model of multilayer interference. The same principles behind the coloration of soap bubbles apply to butterfly wings. The color of butterfly wings is due to multiple instances of constructive interference from structures such as this. The photonic microstructure of butterfly wings can be replicated through biomorphic mineralization to yield similar properties. The photonic microstructures can be replicated using metal oxides or metal alkoxides such as titanium sulfate (TiSO4), zirconium oxide (ZrO2), and aluminium oxide (Al2O3). An alternative method of vapor-phase oxidation of SiH4 on the template surface was found to preserve delicate structural features of the microstructure.
Additional examples.
Researchers studied the termite's ability to maintain virtually constant temperature and humidity in their termite mounds in Africa despite outside temperatures that vary from 1.5 °C to 40 °C (35 °F to 104 °F). Researchers initially scanned a termite mound and created 3-D images of the mound structure, which revealed construction that can influence human building design. The Eastgate Centre, a mid-rise office complex in Harare, Zimbabwe, stays cool without air conditioning and uses only 10% of the energy of a conventional building its size.
Janine Benyus refers in her books to spiders that create web silk as strong as the Kevlar used in bulletproof vests. Engineers could use such a material—if it had a long enough rate of decay—for parachute lines, suspension bridge cables, artificial ligaments for medicine, and other purposes.
Other research has proposed adhesive glue from mussels, solar cells made like leaves, fabric that emulates shark skin, harvesting water from fog like a beetle, and more. Nature’s 100 Best is a compilation of the top hundred different innovations of animals, plants, and other organisms that have been researched and studied by the Biomimicry Institute.
A display technology based on the reflective properties of certain morpho butterflies was commercialized by Qualcomm in 2007. The technology uses Interferometric Modulation to reflect light so only the desired color is visible in each individual pixel of the display.
Biomimicry may also provide design methodologies and techniques to optimize engineering products and systems. An example is the re-derivation of Murray's law, which in conventional form determined the optimum diameter of blood vessels, to provide simple equations for the pipe or tube diameter which gives a minimum mass engineering system.
In structural engineering, the Swiss Federal Institute of Technology (EPFL) has incorporated biomimetic characteristics in an adaptive deployable "tensegrity" bridge. The bridge can carry out self-diagnosis and self-repair.
The Bombardier beetle's powerful repellent spray inspired a Swedish company to develop a "micro mist" spray technology, which is claimed to have a low carbon impact (compared to aerosol sprays). The beetle mixes chemicals and releases its spray via a steerable nozzle at the end of its abdomen, stinging and confusing the victim.
Holistic planned grazing, using fencing and/or herders, seeks to restore grasslands by carefully planning movements of large herds of livestock to mimic the vast herds found in nature where grazing animals are kept concentrated by pack predators and must move on after eating, trampling, and manuring an area, returning only after it has fully recovered. Developed by Allan Savory, this method of biomimetic grazing holds tremendous potential in building soil, increasing biodiversity, reversing desertification, and mitigating global warming, similar to what occurred during the past 40 million years as the expansion of grass-grazer ecosystems built deep grassland soils, sequestering carbon and cooling the planet.

</doc>
<doc id="45794" url="http://en.wikipedia.org/wiki?curid=45794" title="Janet Gaynor">
Janet Gaynor

Janet Gaynor (October 6, 1906 – September 14, 1984) was an American film, stage and television actress and painter.
Gaynor began her career as an extra in shorts and silent films. After signing with Fox Film Corporation (later 20th Century Fox) in 1926, she rose to fame and would become one of the biggest box office draws of the era. In 1929, she was the first winner of the Academy Award for Best Actress for her performances in three films: "7th Heaven" (1927), "" (1927) and "Street Angel" (1928). This was the only occasion on which an actress has won one Oscar for multiple film roles. Gaynor's career success continued into the sound film era, and she achieved a notable success in the original version of "A Star Is Born" (1937), for which she received a second Best Actress Academy Award nomination.
After retiring from acting 1939, Gaynor married film costume designer Adrian with whom she had a son. She briefly returned to acting in films and television the 1950s and later became an accomplished oil painter. In 1980, Gaynor made her Broadway debut in the stage adaptation of the 1971 film "Harold and Maude" and appeared in the touring production of "On Golden Pond" in February 1982. In September 1982, she sustained multiple injuries in a serious car accident which contributed to her death in September 1984.
Early life.
Gaynor was born Laura Augusta Gainor (some sources stated Gainer) in Germantown, Philadelphia. Nicknamed "Lolly" as a child, she was the youngest of two daughters born to Laura and Frank De Witt Gainor. Frank Gainor worked as a theatrical painter and paperhanger. When Gaynor was a toddler, her father began teaching her how to sing, dance and perform acrobatics. As a child in Philadelphia, she began acting in school plays. After her parents divorced in 1914, Gaynor, her sister and her mother moved to Chicago. Shortly thereafter, her mother married electrician Harry C. Jones.
The family later moved west to San Francisco. 
After graduating from San Francisco Polytechnic High School in 1923, Gaynor spent the winter vacationing in Melbourne, Florida where she did stage work. Upon returning to San Francisco, Gaynor her mother and stepfather moved to Los Angeles where she could pursue an acting career. She was initially hesitant to pursue an acting career and enrolled at Hollywood Secretarial School. She supported herself working by in a shoe store and later as a theatre usher. Her mother and stepfather continued to encourage her to become and actress and she began making the rounds to the studios (accompanied by her stepfather) to find film work.
Gaynor won her first professional acting job on December 26, 1924 as an extra in a Hal Roach comedy short. This led to more extra work in feature films and shorts for Film Booking Offices of America and Universal. Universal eventually hired her as a stock player for $50 a week. Six weeks after being hired by Universal, an executive at Fox Film Corporation offered her a screen test for a supporting role in the film "The Johnstown Flood" (1926). Her performance in the film caught the attention of Fox executives who signed her to a five-year contract and began to cast her in leading roles.
Later that year, Gaynor was selected as one of the WAMPAS Baby Stars (along with Joan Crawford, Dolores del Río, Mary Astor, and others),
Career.
By 1927, Gaynor was one of Hollywood's leading ladies. Her image was that of a sweet, wholesome and pure young woman who was notable for playing her roles with depth and sensitivity. Her performances in "7th Heaven", the first of twelve films she would make with actor Charles Farrell; "Sunrise", directed by F. W. Murnau; and "Street Angel", also with Charles Farrell, earned her the first Academy Award for Best Actress in 1929, when for the first and only time the award was granted for multiple roles, on the basis of total recent work rather than for one particular performance. This practice was prohibited three years later by a new Academy of Motion Picture Arts and Sciences rule. Gaynor was not only the first actress to win the award but, at 22, was the youngest until 1986, when deaf actress Marlee Matlin, 21, won for her role in "Children of a Lesser God".
Gaynor was one of only a handful of established lead actresses who made a successful transition to sound films. In 1929, she was re-teamed with Charles Farrell (the pair were known as "America's favorite love birds") for the musical film "Sunny Side Up".
During the early 1930s, Gaynor was one of Fox's most popular actresses and one of Hollywood's biggest box office draws; in 1931 and 1932 she and Marie Dressler were tied as the #1 draw. After Dressler's death in 1934, Gaynor held the #1 spot alone. She was often cited as a successor to Mary Pickford and was cast in remakes of two Pickford films, "Daddy Long Legs" (1931) and "Tess of the Storm Country". Gaynor drew the line at a proposed remake of "Rebecca of Sunnybrook Farm", which she considered "too juvenile".
Gaynor continued with roles in "State Fair" (1933) with Will Rogers and "The Farmer Takes a Wife" (1935), which introduced Henry Fonda to the screen as Gaynor's leading man. However, when Darryl F. Zanuck merged his fledgling studio, 20th Century Pictures, with Fox Film Corporation to form 20th Century Fox, her status became precarious and even tertiary to that of burgeoning actresses Loretta Young and Shirley Temple. According to press reports at the time, Gaynor held out on signing with the new 20th Century Fox until her salary was raised from $1,000 a week to $3,000. The studio quickly issued a statement denying that Gaynor was holding out for more money. She quietly signed a new contract, the terms of which were never made public.
Gaynor co-starred in "Ladies in Love" (1937) with Constance Bennett, Loretta Young and Tyrone Power, but her box office appeal had already begun to wane: once ranked #1, she had dropped to #24. She considered retiring due to her frustration with studio executives, who continued to cast her in the same type of role that brought her fame while audiences' tastes were changing. After 20th Century Fox executives proposed that her contract be renegotiated and she be demoted to featured player status, Gaynor left the studio, but her retirement plans were quashed when David O. Selznick offered her the leading role in a new film to be produced by his company, Selznick International. Selznick, who was friendly with Gaynor off-screen, was convinced that audiences would enjoy seeing her portray a character closer to her true personality. He believed that she possessed the perfect combination of humor, charm, vulnerability and innocence for the role of aspiring actress Esther Blodgett (later "Vicki Lester") in "A Star Is Born". Gaynor accepted the role. The romantic drama was filmed in Technicolor and co-starred Fredric March. Released in 1937, it was an enormous hit and earned Gaynor her second Academy Award nomination for Best Actress; she lost to Luise Rainer for "The Good Earth". 
"A Star Is Born" revitalized Gaynor's career and she was cast in the screwball comedy "The Young in Heart" with Paulette Goddard. That film was a modest hit, but by then Gaynor had definitely decided to retire. She later explained, "I had been working steadily for 17 long years, making movies was really all I knew of life. I just wanted to have time to know other things. Most of all I wanted to fall in love. I wanted to get married. I wanted a child. And I knew that in order to have these things one had to make time for them. So I simply stopped making movies. Then as if by a miracle, everything I really wanted happened."
Later years.
In August 1939, Gaynor married Hollywood costume designer Adrian with whom she had a son in 1940. The couple divided their time between their 250 acre cattle ranch north of Brasília, and their homes in New York and California. Both were also were heavily involved in the fashion and arts community. Gaynor returned to acting in the early 1950s with appearances in live television anthology series including "Medallion Theatre", "Lux Video Theatre", and "General Electric Theater". In 1957, she appeared in her final film role as Pat Boone's mother in the musical comedy "Bernadine". In November 1959, she made her stage debut in the play "The Midnight Sun", in New Haven, Connecticut. The play, which Gaynor later called "a disaster", was not well received and closed shortly after its debut.
Gaynor also became an accomplished oil painter of vegetable and flower still lifes. She sold over 200 painting and had four showings under the Wally Findlay Galleries banner in New York, Chicago, and Palm Beach from 1975 to February 1982.
In 1980, Gaynor made her Broadway debut as "Maude" in the stage adaptation of the 1971 film "Harold and Maude". She received good reviews for her performance, but the play was panned by critics and closed after 21 performances. Later that year, she reunited with her "Servants' Entrance" co-star Lew Ayres to film an episode of the anthology series "The Love Boat". It was the first television appearance Gaynor made since the 1950s and would be her last screen role. In February 1982, she starred in the touring production of "On Golden Pond". It would be her final acting role.
Personal life.
Marriages.
Gaynor was married three times and had one child. Her first marriage was to lawyer Jesse Lydell Peck, whom she married on September 11, 1929. Gaynor's attorney announced the couple's separation in late December 1932. She was granted a divorce on April 7, 1933. On August 14, 1939, she married MGM costume designer Adrian in Yuma, Arizona. This relationship has been called a lavender marriage, since Adrian was openly gay within the film community while Gaynor was rumored to be gay or bisexual. The couple had one son, Robin Gaynor Adrian, born in 1940. They remained married until Adrian's death from a stroke on September 13, 1959.
On December 24, 1964, Gaynor married her longtime friend stage producer Paul Gregory to whom she remained married until her death. The two maintained a home in Desert Hot Springs, California and also owned 3,000 acres of land near Brasília.
Sexuality.
Gaynor reportedly had a long-term lesbian relationship with actress Mary Martin, with whom she frequently travelled. Their relationship was reportedly an open secret in the Hollywood community. Actor Bob Cummings once quipped: "Janet Gaynor's husband was Adrian, but her wife was Mary Martin". A Brazilian press report noted that Gaynor and Martin briefly lived with their respective husbands in the state of Goiás in the 1950s and 1960s.
Final years and death.
On the evening of September 5, 1982, Gaynor, her husband Paul Gregory, actress Mary Martin, and Martin's manager Ben Washer were involved in a serious car accident in San Francisco. A van ran a red light at the corner of California Street and Franklin and crashed into the Luxor taxicab the group was riding in, knocking it into a tree. Ben Washer was killed while Mary Martin sustained two broken ribs and a broken pelvis and Gaynor's husband suffered two broken legs. Gaynor sustained several serious injuries including eleven broken ribs, a fractured collarbone, pelvic fractures, a punctured lung, and injuries to her bladder and kidney. The driver of the van, Robert Cato, was arrested on two counts of felony drunk driving, reckless driving, speeding, running a red light and vehicular homicide. Cato pleaded not guilty and was later released on $10,000 bail. On March 15, 1983, he was found guilty of drunk driving and vehicular homicide and was sentenced to three years in prison.
As a result of her injuries, Gaynor was hospitalized for four months and underwent two surgeries to repair a perforated bladder and internal bleeding.
She recovered sufficiently to return to her home in Desert Hot Springs but continued to experience health issues due to the injuries and required frequent hospitalizations. Shortly before her death, she was hospitalized for pneumonia and other ailments. On September 14, 1984, Gaynor died at Desert Hospital in Palm Springs at the age of 77. Her doctor, Bart Apfelbaum, attributed her death to the 1982 car accident and stated that Gaynor "...never recovered" from her injuries.
Gaynor is buried at Hollywood Forever Cemetery next to her second husband Adrian. Her headstone reads "Janet Gaynor Gregory," her legal name after her marriage to her third husband, producer and director Paul Gregory.
Honors.
For her contribution to the motion picture industry, Janet Gaynor has a star on the Hollywood Walk of Fame at 6284 Hollywood Blvd.
On March 1, 1978, Howard W. Koch, then the president of the Academy of Motion Picture Arts and Sciences, presented Gaynor with a citation for her "truly immeasurable contribution to the art of motion pictures".
In 1979, Gaynor was awarded the Order of the Southern Cross for her cultural contributions to Brazil.

</doc>
<doc id="45795" url="http://en.wikipedia.org/wiki?curid=45795" title="Natural capital">
Natural capital

Natural capital is the world's stock of natural resources which creates a long term supply of goods or services. It is an extension of the economic notion of capital (resources which enable the production of more resources) to goods and services provided by the natural environment. For example, a forest or river may provide an indefinitely sustainable flow of new trees or fish. Natural capital may also provide essential services like water catchment and erosion control, which ensure the long term viability of natural resources. Since the maintenance of natural capital requires the preservation of cohesive ecosystems, the structure and diversity of the system are important to ensure the long term sustainability of the resources generated.
History of the concept.
Natural capital is one approach to ecosystem valuation which revolves around the idea, in contrast to traditional economics, that non-human life produces essential resources. Thus, ecological health is essential to the sustainability of the economy. In "" the author claims that the global economy is within a larger economy of natural resources and ecosystem services that sustain us. In order to continue to reap the benefits of our natural environment, we need to recognize the importance of natural capital within the economy. According to the authors, the "next industrial revolution" depends on the espousal of four central strategies: "the conservation of resources through more effective manufacturing processes, the reuse of materials as found in natural systems, a change in values from quantity to quality, and investing in natural capital, or restoring and sustaining natural resources."
In a traditional economic analysis of the factors of production, natural capital would usually be classified as "land" distinct from traditional "capital". The historical distinction between "land" and "capital" defined “land” as naturally occurring with a fixed supply, whereas “capital”, as originally defined referred only to man-made goods. (e.g., Georgism) It is however, misleading to view "land" as if its productive capacity is fixed, because natural capital can be improved or degraded by the actions of man over time (see Tragedy of the Commons). Moreover, natural capital yields benefits and goods, such as timber or food, which can be harvested by humans. These benefits are similar to those realized by owners of infrastructural capital which yields more goods, such as a factory which produces automobiles just as an apple tree produces apples.
The term 'natural capital' was first used by in 1973 by E.F. Schumacher in his book "Small Is Beautiful" and is closely identified with Herman Daly, Robert Costanza, the Biosphere 2 project, and the Natural Capitalism economic model of Paul Hawken, Amory Lovins, and Hunter Lovins. Recently, it has begun to be used by politicians, notably Ralph Nader, Paul Martin Jr., and agencies of the UK government, including the London Health Observatory. All users of the term currently differentiate natural from man-made manufactured or infrastructural capital in some way. Indicators adopted by United Nations Environment Programme's World Conservation Monitoring Centre and the Organisation for Economic Co-operation and Development (OECD) to measure natural biodiversity use the term in a slightly more specific way. According to the OECD, natural capital is “natural assets in their role of providing natural resource inputs and environmental services for economic production” and is “generally considered to comprise three principal categories: natural resources stocks, land, and ecosystems.”
Within the international community, the basic principle is not controversial, but there is significant controversy over methods of valuing different aspects of ecological health and natural capital. Full cost accounting, triple bottom line, measuring well-being and other proposals for accounting reform often include proposals to measure an "ecological deficit" or "natural deficit" alongside a social deficit and financial deficit. It is difficult to measure such a deficit without some agreement on methods of valuating and auditing at least the global forms of natural capital (e.g. value of air, water, soil).
Ecologists are teaming up with economists to measure and express values of the wealth of ecosystems as a way of finding solutions to the biodiversity crisis. Some researchers have attempted to place a dollar figure on ecosystem services such as the value that the Canadian boreal forest's contribution to global ecosystem services. If ecologically intact, the boreal forest has an estimated value of US$3.7 trillion. The boreal forest ecosystem is one of the planet's great atmospheric regulators and it stores more carbon than any other biome on the planet. The annual value for ecological services of the Boreal Forest is estimated at US$93.2 billion, or 2.5 greater than the annual value of resource extraction. The economic value of 17 ecosystem services for the entire biosphere (calculated in 1997) has an estimated average value of US$33 trillion per year. These ecological economic values are not currently included in calculations of national income accounts, the GDP and they have no price attributes because they exist mostly outside of the global markets. The loss of natural capital continues to accelerate and goes undetected or ignored by mainstream monetary analysis.
Internationally agreed standard.
Environmental-economic accounts provide the conceptual framework for integrated statistics on the environment and its relationship with the economy, including the impacts of the economy on the environment and the contribution of the environment to the economy. A coherent set of indicators and descriptive statistics can be derived from the accounts that inform a wide range of policies, including, but not limited to, green economy/green growth, natural resource management and sustainable development. The System of Environmental-Economic Accounting (SEEA) contains the internationally agreed standard concepts, definitions, classifications, accounting rules and tables for producing internationally comparable statistics on the environment and its relationship with the economy. The SEEA is a flexible system in the sense that its implementation can be adapted to countries' specific situations and priorities. Coordination of the implementation of the SEEA and on-going work on new methodological developments is managed and supervised by the UN Committee of Experts on Environmental-Economic Accounting (UNCEEA). The final, official version of the SEEA Central Framework was published in February 2014. 

</doc>
<doc id="45798" url="http://en.wikipedia.org/wiki?curid=45798" title="Clouded leopard">
Clouded leopard

The clouded leopard ("Neofelis nebulosa") is a cat found from the Himalayan foothills through mainland Southeast Asia into China, and has been classified as Vulnerable in 2008 by the International Union for Conservation of Nature (IUCN). Its total population size is suspected to be fewer than 10,000 mature individuals, with a decreasing population trend, and no single population numbering more than 1,000 adults.
The clouded leopard is considered to form an evolutionary link between the big cats and the small cats. It represents the smallest of the big cats, but is not closely related to the leopard.
The Sunda clouded leopard ("Neofelis diardi") found on Sumatra and Borneo is genetically distinct and has been considered a separate species since 2006.
Characteristics.
The fur of clouded leopards is of a dark grey or ochreous ground-colour, often largely obliterated by black and dark dusky-grey blotched pattern. There are black spots on the head, and the ears are black. Partly fused or broken-up stripes run from the corner of the eyes over the cheek, from the corner of the mouth to the neck, and along the nape to the shoulders. Elongated blotches continue down the spine and form a single median stripe on the loins. Two large blotches of dark dusky-grey hair on the side of the shoulders are each emphasized posteriorly by a dark stripe, which passes on to the foreleg and breaks up into irregular spots. The flanks are marked by dark dusky-grey irregular blotches bordered behind by long, oblique, irregularly curved or looped stripes. These blotches yielding the clouded pattern suggest the English name of the cat. The underparts and legs are spotted, and the tail is marked by large, irregular, paired spots. Females are slightly smaller than males. 
Their irises are usually either greyish-green or brownish-yellow in color. Their legs are short and stout, with broad paws. They have rather short limbs compared to the other big cats, but their hind limbs are longer than their front limbs to allow for increased jumping and leaping capabilities. Their ulnae and radii are not fused, which also contributes to a greater range of motion when climbing trees and stalking prey.
Melanistic clouded leopards are uncommon. Clouded leopards weigh between 11.5 and. Females vary in head-to-body length from 68.6 to, with a tail 61 to long. Males are larger at 81 to with a tail 74 to long. 
Their shoulder height varies from 50 to. 
They have exceptionally long, piercing canine teeth, the upper being about three times as long as the basal width of the socket. The upper pair of canines may measure 4 cm or longer.
They are often referred to as a “modern-day saber tooth” because they have the largest canines in proportion to their body size, matching the tiger in canine length. The first premolar is usually absent, and they also have a very distinct long and slim skull with well-developed occipital and sagittal crests to support the enlarged jaw muscles.
Etymology.
The scientific name of the genus "Neofelis" is a composite of the Greek word νεο- meaning "new", and the Latin word "feles" meaning "cat", so it literally means "new cat."
Distribution and habitat.
Clouded leopards occur from the Himalayan foothills in Nepal and India to Myanmar, Bhutan, Thailand, Peninsular Malaysia, Indochina, and in China south of the Yangtze River. Some are found in the mixed-evergreen forests of the northeastern and southeastern parts of Bangladesh. They are regionally extinct in Taiwan. Clouded leopards prefer open- or closed-forest habitats to other habitat types. They have been reported from relatively open, dry tropical forest in Myanmar and in Thailand.
In India, they occur in northern West Bengal, Sikkim, Arunachal Pradesh, Manipur, Meghalaya, Mizoram, Nagaland, and Tripura. In Assam they were observed in forests but have not been recorded in protected areas. In the Himalayas, they were camera-trapped at altitudes of 2500 - between April 2008 and May 2010 in the Khangchendzonga Biosphere Reserve, Sikkim. 
Clouded leopards were thought to be extinct in Nepal since the late 1860s. But in 1987 and 1988, four individuals were found in the central part of the country, close to Chitwan National Park and in the Pokhara Valley. These findings extended their known range westward, suggesting they are able to survive and breed in degraded woodlands that previously harboured moist subtropical semideciduous forest. Since then, individuals were recorded in the Shivapuri Nagarjun National Park and in the Annapurna Conservation Area.
Distribution of subspecies.
At present, these three subspecies of "Neofelis nebulosa" are recognized:
Ecology and behavior.
Clouded leopards are the most talented climbers among the cats. In captivity, they have been observed to climb down vertical tree trunks head first, and hang on to branches with their hind paws bent around branchings of tree limbs. They are capable of supination and can even hang down from branches only by bending their hind paws and their tail around them. When jumping down, they keep hanging on to a branch this way until the very last moment. They can climb on horizontal branches with their back to the ground, and in this position make short jumps forward. When balancing on thin branches, they use their long tails to steer. They can easily jump up to 1.2 m high.
Clouded leopards have been observed to scent mark in captivity by urine-spraying and head-rubbing on prominent objects. Presumably such habits are used to mark their territory in the wild, although the size of their home ranges is unknown. Like other big cats, they do not appear able to purr, but they otherwise have a wide range of vocalisations, including mewing, hissing, growling, moaning, and snorting. When communicating, two individuals will emit low snorting sounds that are called prusten when approaching each other in a friendly manner. They also use long-call communication used over large distances, which could either be a type of mating call between different territories or a warning call to other cats encroaching on other territories. Apart from information stemming from observations of captive clouded leopards, little is known of their natural history and behavior in the wild. Early accounts depict them as rare, secretive, arboreal, and nocturnal denizens of dense primary forest. More recent observations suggest they may not be as arboreal and nocturnal as previously thought. They may use trees as daytime rest sites, but also spend a significant proportion of time on the ground. Some daytime movement has been observed, suggesting they are not strictly nocturnal but crepuscular. However, the time of day when they are active depends on their prey and the level of human disturbance.
They live a solitary lifestyle, resting in trees during the day and hunting at night. When hunting, clouded leopards either come down from their perches in the trees and stalk their prey or lie and wait for the prey to come to them. After making a kill and eating, they usually retreat to the trees to digest and rest.
Their partly nocturnal and far-ranging behaviour, their low densities, and because they inhabit densely vegetated habitats and remote areas makes the counting and monitoring of clouded leopards extremely difficult. Consequently, little is known about their behaviour and status. Available information on their ecology is anecdotal, based on local interviews and a few sighting reports.
Home ranges have only been estimated in Thailand:
Little is known of the diet of clouded leopards. Their prey includes both arboreal and terrestrial vertebrates. Pocock presumed they are adapted for preying upon herbivorous mammals of considerable bulk because of their powerful build and the deep penetration of their bites, attested by their long canines. Confirmed prey species include hog deer, slow loris, brush-tailed porcupine, Malayan pangolin and Indochinese ground squirrel. Known prey species in China include barking deer and pheasants. 
Reproduction.
Both males and females average 26 months at first reproduction. Mating usually occurs during December and March. The males tend to be very aggressive during sexual encounters and have been known to bite the female on the neck during courtship, severing her vertebrae. With this in mind, male and female compatibility has been deemed extremely important when attempting breeding in captivity. The pair will meet and mate multiple times over the course of several days. The male grasps the female by the neck and the female responds with vocalization that encourages the male to continue. The male then leaves and is not involved in raising the kittens. Estrus last six days on average, estrous cycle averages 30 days. After a gestation period of 93 ± 6 days, females give birth to a litter of one to five, most often three cubs.
Initially, the young are blind and helpless, much like the young of many other cats, and weigh from 140 to. Unlike adults, the kittens' spots are "solid" — completely dark rather than dark rings. The young can see within about 10 days of birth, are active within five weeks, and are fully weaned at around three months of age. They attain the adult coat pattern at around six months, and probably become independent after around 10 months. Females are able to bear one litter each year. The mother is believed to hide her kittens in dense vegetation while she goes to hunt, though little concrete evidence supports this theory, since their lifestyle is so secretive.
In captivity, they have an average lifespan of 11 years. One individual has lived to be almost 17 years old.
Threats.
Many of the remaining forest areas are too small to ensure the long-term persistence of clouded leopard populations. They are threatened by habitat loss following large–scale deforestation and commercial poaching for the wildlife trade. Skins, claws, and teeth are offered for decoration and clothing, bones and meat as substitute for tiger in traditional Asian medicines and tonics, and live animals for the pet trade. Few poaching incidents have been documented, but all range states are believed to have some degree of commercial poaching. In recent years, substantial domestic markets existed in Indonesia, Myanmar, and Vietnam.
In Myanmar, 301 body parts of at least 279 clouded leopards, mostly skins and skeletons, were observed in four markets surveyed between 1991 and 2006. Three of the surveyed markets are situated on international borders with China and Thailand, and cater to international buyers, although clouded leopards are completely protected under Myanmar's national legislation. Effective implementation and enforcement of CITES is considered inadequate.
Conservation.
"Neofelis nebulosa" is listed in CITES Appendix I and protected over most of its range. Hunting is banned in Bangladesh, China, India, Malaysia, Myanmar, Nepal, Taiwan, Thailand, and Vietnam. It is not legally protected outside Bhutan's protected areas. Hunting is regulated in Laos. No information about its protection status is available from Cambodia. These bans, however, are poorly enforced in India, Malaysia, and Thailand.
In the United States, the clouded leopard is listed as endangered under the Endangered Species Act, further prohibiting trade in the animals or any parts or products made from them. 
In captivity.
Early captive-breeding programs involving clouded leopards were not very successful, largely due to ignorance of courtship activity among them in the wild. Experience has taught keepers that introducing pairs of clouded leopards at a young age gives opportunities for the pair to bond and breed successfully. Males have the reputation of being aggressive towards females. Facilities breeding clouded leopards need to provide the female a secluded, off-exhibit area. 
Modern breeding programs involve carefully regulated introductions between prospective mating pairs, and take into account the requirements for enriched enclosures. Stimulating natural behavior by providing adequate space to permit climbing minimizes stress. This, combined with a feeding program that fulfills the proper dietary requirements, has promoted more successful breeding in recent years.
In March 2011, two breeding females at the Nashville Zoo at Grassmere in Nashville, Tennessee, gave birth to three cubs, which are being raised by zookeepers. Each cub weighed 0.5 lb. In June 2011, two cubs were born at the Point Defiance Zoo & Aquarium in Tacoma, Washington. The breeding pair was brought from the Khao Kheow Open Zoo in Thailand in an ongoing education and research exchange program. Four cubs were born at the Nashville Zoo in 2012.
As of December 2011, 222 clouded leopards are believed to exist in zoos.
In culture.
The Rukai people of Taiwan considered the hunting of clouded leopards a taboo.

</doc>
<doc id="45800" url="http://en.wikipedia.org/wiki?curid=45800" title="Public capital">
Public capital

Public capital is the aggregate body of government-owned assets that are used as the means for private productivity. Such assets span a wide range including: large components such as highways, airports, roads, transit systems, and railways; local, municipal components such as public education, public hospitals, police and fire protection, prisons, and courts; and critical components including water and sewer systems, public electric and gas utilities, and telecommunications. Often, public capital is defined as government outlay, in terms of money, and as physical stock, in terms of infrastructure.
Current state in the U.S..
In 1988, the U.S. infrastructure system including all public and private non-residential capital stock was valued at $7 trillion, an immense portfolio to operate and manage. And according to the Congressional Budget Office, in 2004 the U.S. invested $400 billion in infrastructure capital across federal, state, and local levels including the private sectors on transportation networks, schools, highways, water systems, energy, and telecommunications services. While public spending on infrastructure grew by 1.7% annually between 1956 and 2004, it has remained constant as a share of GDP since early 1980s. Despite the value and investment of public capital, growing delays in air and surface transportation, aging electric grid, an untapped renewable energy sector, and inadequate school facilities all have justified additional funding in public capital investment. 
The American Society of Civil Engineers have continued to give low marks, averaging a D grade, for the nation’s infrastructure since its inception of the Report Card in 1998. In 2009, each category of infrastructure varied from C+ to D- grades with an estimated $2.2 trillion of needed public capital investment. The aviation sector remains mired in continued delays in the reauthorization of federal programs and an outdated air traffic control system. One in four rural bridges and one in three urban bridges are structurally deficient. States are understaffed and underfunded to conduct safety inspections of dams. Texas alone has only seven engineers and an annual budget of $435,000 to oversee more than 7,400 dams. Electricity demand outpaces energy supply transmission and generation. Almost half of the water locks maintained by the U.S. Army Corps of Engineers are functionally obsolete. Drinking water faces an annual shortfall of $11 billion to manage their aging facilities and comply with federal regulations. Leaking pipes lose an estimated 7 e9USgal of clean drinking water a day. Under tight budgets, national, state, and local parks suffer neglect. Without adequate funding, rail cannot meet future freight tonnage load. Schools require a staggering $127 billion to bring facilities to decent operating condition. Billions of gallons of untreated sewage continue to be discharged into U.S.’s surface waters each year.
Economic growth.
One of the most classic macroeconomic inquiries is the effect of public capital investment on economic growth. While many analysts debate the magnitude, evidence has shown a statistically significant positive relationship between infrastructure investment and economic performance. U.S. Federal Reserve economist David Alan Aschauer asserted an increase of the public capital stock by 1% would result in an increase of the total factor productivity by 0.4%. Aschauer argues that the golden age of the 1950s and 1960s were partly due to the post-World War II substantial investment in core infrastructure (highways, mass transit, airports, water systems, electric/gas facilities). Conversely, the drop of U.S. productivity growth in the 1970s and 1980s was in response to the decrease of continual public capital investment and not the decline of technological innovation. Likewise, the European Union nations have declined public capital investment through the same years, also witnessing declining productivity growth rates. A similar situation emerges in developing nations. Analyzing OECD and non-OECD countries’ real-GDP growth rates from 1960 to 2000 with public capital as an explanatory variable (not using public investment rates), Arslanalp, Borhorst, Gupta, and Sze (2010) show that increases in the public capital stock does correlate with increases in growth. However, this relationship depends on initial levels of public capital and income levels for the country. Thus, OECD countries witness a stronger positive link in the short term while non-OECD countries experience a stronger positive link in the long term. Hence, developing countries can benefit from non-concessional foreign borrowing to finance high-prospect public capital investments.
Given this relationship of public capital and productivity, public capital becomes a third input in the standard, neoclassical production function:
where: 
In this form, public capital has a direct influence on productivity as a third variable. Additionally, public capital has an indirect influence on multifactor productivity as it affects the other two inputs of labor and private capital. Despite this unique nature, public capital investment, used in the production process of nearly every sector, is not sufficient on its own to generate sustained economic growth. Thus, rather than the ends, public capital is the means. That is, instead of being seen as intermediate goods used as resources by businesses, public capital should be seen as goods which are used to make the final goods and services to consumers-taxpayers. Nevertheless, high public capital investment usually leads to crowding out effects for private investment. Similarly, public capital levels should not be too high that it leads to financing costs and high tax rates issues which will negate the positive benefits of such investments. Moreover, infrastructure services carry the market-distorting features of pure, non-rival public goods; network externalities; natural monopolies; and the common resource problem such as congestion and overuse.
Empirical models that attempt to estimate the public investment and economic growth link involve a wide variety including: the Cobb-Douglas production function; a behavioral approach cost/profit function which includes public capital stock; Vector Auto Regression (VAR) models; and government investment growth regressions. These models nonetheless contend with reverse causality, heterogeneity, endogeneity, and nonlinearities in trying to capture the public capital and economic growth link. New Keynesian models, though, analyze the effect of government spending through the supply side rather than traditional Keynesian models that analyzes it through the demand side. Therefore, a temporary surge of infrastructure investment yields an expansion of output, and vice versa that dwindling infrastructure, like in the 1970s, hamper longer-term movement in productivity. Furthermore, new research on regional growth (as opposed to national growth with GDP) shows a strong positive relationship between public capital and productivity. Both fixed costs and transport costs lower with expanded infrastructure in localities and the resulting cluster of industries. As a result, economic activity grows along its pattern of trade. Therefore, the importance of regional clusters and metropolitan economies comes into effect.
Social benefit.
Beyond economic performance, public capital investment yields returns in quality of life indicators such as health, safety, recreation, aesthetics, and leisure time and activities. In example, highways provide better access and mobility for increased discretionary time and recreational outlets; mass transit can improve air quality with reduced number of private vehicles; improved municipal waste facilities reduces toxic groundwater contamination and better green space aesthetics such as parks; expanded water facilities aids in health and sanitation and environment such as reducing odor and sewer overflows. Furthermore, infrastructure adds to community ambience and quality of place with livelier downtowns, vibrant waterfronts, efficient land uses, compact spaces for commerce and recreation.
On the contrary, inadequate public capital impairs quality of life and social well-being. Over-capacity landfills lead to groundwater contamination, having deleterious effects on health. Deficient supply and quality of mass transit services impacts transit-dependents on their access to opportunity and resources. Increasing congestion in airports and roadways causes loss of discretionary time and recreational activities. The lack of efficient U.S. freight and passenger rail service will neither aid in handling the “perfect storm” of environmental and energy sustainability nor meet the global competitive need of transporting goods and services at heightened speeds and times. Also, the continued loss of footing in clean energy technology will contribute to U.S.’s future loss of prosperity on the global stage in terms of the carbon footprint and economy.
Public capital initiatives.
United States.
Perhaps the largest contribution to the public works system in the U.S. came out of President Franklin D. Roosevelt’s New Deal initiatives particularly the creation of the Works Progress Administration (WPA) in 1935. At a time of a deep economic crisis, the WPA employed at its peak 3.35 millions unemployed heads-of-households to work in rebuilding the country. The program helped construct millions of roads, bridges, parks, schools, hospitals, and levees while also providing educational programs, childcare, job training, and medical services. The overall public spending level for the program, unprecedented at the time, was $4.8 billion ($76 billion in 2008 dollars), and helped to stimulate the economy through public works projects. 
Since then, the U.S. has contributed to other large infrastructure programs including the Interstate Highway System, 1956-1990, with a dedicated financing system through the gas tax and a matching contribution between federal government and states at 90% to 10%. Also, the Environmental Protection Agency's (EPA) Clean Water Act of 1972 provided a public capital investment of $40 billion in constructing and upgrading sewage treatment facilities with “significant positive impacts on the Nation’s water quality.” Considered by the National Academy of Engineering to be the greatest engineering achievement of the 20th century, the North American electric grid carries electricity over 300,000 mi on high-voltage transmission lines across the U.S. Though currently facing aging facilities and equipment, this public capital investment has ubiquitously reached millions of homes and businesses.
Recently, the American Recovery and Reinvestment Act (ARRA) is another example of large public capital investment. Of the $311 billion in appropriations, about $120 billion are set aside for crucial investment in Infrastructure and Science and Energy. Some of ARRA’s aims include smart grid technology, retrofitting of homes and federal buildings, automated aviation traffic control, advancing freight and passenger rail services, and upgrading water and waste facilities.
Other countries.
Worldwide, transformative public capital investments are taking place. China’s ambitious rapid high-speed rail program is estimated to extend 18,000 km by 2020. By the end of 2008, the country had a fleet of over 24,000 locomotives, the most lines in the world, the fastest express train in service, and longest high-speed track in the world. UK, Denmark, and other countries in northern Europe that surround the Baltic Sea and North Sea, continue to develop their rapid expansion of off-shore wind farms. With continued expansion of terminals and connection to nation’s comprehensive transport system, the Hong Kong International Airport is one of the largest engineering and architectural projects in the world. In the last decade, Chile installed five combined cycle gas-turbined (CCGT) power plants to meet its nation’s growing energy needs.

</doc>
<doc id="45801" url="http://en.wikipedia.org/wiki?curid=45801" title="Physical capital">
Physical capital

In economics, physical capital or just 'capital' refers to a factor of production (or input into the process of production), such as machinery, buildings, or computers. The production function takes the general form packman=f(K, L), where packman is output, K is capital stock and L is labor. In economic theory, physical capital is one of the three primary factors of production, also known as inputs production function. The others are natural resources (including land), and labor — the stock of competences embodied in the labor force. "Physical" is used to distinguish physical capital from human capital (a result of investment in the human agent)) and financial capital. "Physical capital" refers to fixed capital, any kind of real physical asset that is not used up in the production of a product is distinguished from circulating capital. Usually the value of "land" is not included in physical capital as it's not a reproducible product of human activity.

</doc>
<doc id="45802" url="http://en.wikipedia.org/wiki?curid=45802" title="Social capital">
Social capital

In sociology, social capital is the expected collective or economic benefits derived from the preferential treatment and cooperation between individuals and groups. Although different social sciences emphasize different aspects of social capital, they tend to share the core idea "that social networks have value". Just as a screwdriver (physical capital) or a university education (cultural capital or human capital) can increase productivity (both individual and collective), so do social contacts affect the productivity of individuals and groups.
Background.
The term "social capital" was in occasional use from about 1890, but only became widely used in the late 1990s.
In the first half of the 19th century, Alexis de Tocqueville had observations about American life that seemed to outline and define social capital. He observed that Americans were prone to meeting at as many gatherings as possible to discuss all possible issues of state, economics, or the world that could be witnessed. The high levels of transparency caused greater participation from the people and thus allowed for democracy to work better. The French writers highlighted also that the level of social participation (social capital) in American society was directly linked to the equality of conditions (Ferragina, 2010; 2012; 2013).
L. J. Hanifan's 1916 article regarding local support for rural schools is one of the first occurrences of the term "social capital" in reference to social cohesion and personal investment in the community. In defining the concept, Hanifan contrasts social capital with material goods by defining it as:
I do not refer to real estate, or to personal property or to cold cash, but rather to that in life which tends to make these tangible substances count for most in the daily lives of people, namely, goodwill, fellowship, mutual sympathy and social intercourse among a group of individuals and families who make up a social unit… If he may come into contact with his neighbor, and they with other neighbors, there will be an accumulation of social capital, which may immediately satisfy his social needs and which may bear a social potentiality sufficient to the substantial improvement of living conditions in the whole community. The community as a whole will benefit by the cooperation of all its parts, while the individual will find in his associations the advantages of the help, the sympathy, and the fellowship of his neighbors (pp. 130-131).
John Dewey used the term in his monograph entitled "School and Society" in 1900, but he offered no definition of it.
Jane Jacobs used the term early in the 1960s. Although she did not explicitly define the term "social capital", her usage referred to the value of networks. Political scientist Robert Salisbury advanced the term as a critical component of interest group formation in his 1969 article "An Exchange Theory of Interest Groups" in the "Midwest Journal of Political Science". Sociologist Pierre Bourdieu used the term in 1972 in his "Outline of a Theory of Practice", and clarified the term some years later in contrast to cultural, economic, and symbolic capital. Sociologists James Coleman, Barry Wellman and Scot Wortley adopted Glenn Loury's 1977 definition in developing and popularising the concept. In the late 1990s the concept gained popularity, serving as the focus of a World Bank research programme and the subject of several mainstream books, including Robert Putnam's "Bowling Alone" and Putnam and Lewis Feldstein's "Better Together".
The concept that underlies social capital has a much longer history; thinkers exploring the relation between associational life and democracy were using similar concepts regularly by the 19th century, drawing on the work of earlier writers such as James Madison ("The Federalist Papers") and Alexis de Tocqueville ("Democracy in America") to integrate concepts of social cohesion and connectedness into the pluralist tradition in American political science. John Dewey may have made the first direct mainstream use of "social capital" in "The School and Society" in 1899, though he did not offer a definition.
The power of "community governance" has been stressed by many philosophers from antiquity to the 18th century, from Aristotle to Thomas Aquinas and Edmund Burke (Bowles and Gintis, 2002). This vision was strongly criticised at the end of the 18th century, with the development of the idea of "Homo Economicus" and subsequently with "rational choice theory". Such a set of theories became dominant in the last centuries, but many thinkers questioned the complicated relationship between "modern society" and the importance of "old institutions", in particular family and traditional communities (Ferragina, 2010:75). The debate of community versus modernization of society and individualism has been the most discussed topic among the fathers of sociology (Tönnies, 1887; Durkheim, 1893; Simmel, 1905; Weber, 1946). They were convinced that industrialisation and urbanization were transforming social relationship in an irreversible way. They observed a breakdown of traditional bonds and the progressive development of anomie and alienation in society (Wilmott, 1986).
After Tönnies' and Weber's works, reflection on social links in modern society continued with interesting contributions in the 1950s and in the 1960s, in particular "mass society theory" (Bell, 1962; Nisbet, 1969; Stein, 1960; Whyte, 1956). They proposed themes similar to those of the founding fathers, with a more pessimistic emphasis on the development of society (Ferragina, 2010: 76). In the words of Stein (1960:1): “The price for maintaining a society that encourages cultural differentiation and experimentation is unquestionably the acceptance of a certain amount of disorganization on both the individual and social level.” All these reflections contributed remarkably to the development of the social capital concept in the following decades.
The appearance of the modern social capital conceptualization is a new way to look at this debate, keeping together the importance of community to build generalized trust and the same time, the importance of individual free choice, in order to create a more cohesive society (Ferragina, 2010; Ferragina, 2012 It is for this reason that social capital generated so much interest in the academic and political world (Rose, 2000).
Evaluating social capital.
Though Bourdieu might agree with Coleman that social capital in the abstract is a neutral resource, his work tends to show how it can be used practically to produce or reproduce inequality, demonstrating for instance how people gain access to powerful positions through the direct and indirect employment of social connections. Robert Putnam has used the concept in a much more positive light: though he was at first careful to argue that social capital was a neutral term, stating “whether or not [the] shared are praiseworthy is, of course, entirely another matter”, his work on American society tends to frame social capital as a producer of "civic engagement" and also a broad societal measure of communal health. He also transforms social capital from a resource possessed by individuals to an attribute of collectives, focusing on norms and trust as producers of social capital to the exclusion of networks.
Mahyar Arefi identifies consensus building as a direct positive indicator of social capital. Consensus implies “shared interest” and agreement among various actors and stakeholders to induce collective action. Collective action is thus an indicator of increased social capital.
Edwards and Foley, as editors of a special edition of the "American Behavioural Scientist" on "Social Capital, Civil Society and Contemporary Democracy", raised two key issues in the study of social capital. First, social capital is not equally available to all, in much the same way that other forms of capital are differently available. Geographic and social isolation limit access to this resource. Second, not all social capital is created equally. The value of a specific source of social capital depends in no small part on the socio-economic position of the source with society. On top of this, Portes has identified four negative consequences of social capital: exclusion of outsiders; excess claims on group members; restrictions on individual freedom; and downward levelling norms.
An interesting distinction of social organization is that between bonding and bridging ties, which complicates the neo-Tocquevillean view of social capital.
 studied the correlation between the presence of interethnic networks (bridging) versus intra-ethnic ones (bonding) on ethnic violence in India.
He argues that interethnic networks are agents of peace because they build bridges and manage tensions, by noting that if communities are organized only along intra-ethnic lines and the interconnections with other communities are very weak or even nonexistent, then ethnic violence is quite likely.
Three main implications of intercommunal ties explain their worth:
This is a useful distinction; nevertheless its implication on social capital can only be accepted if one espouses the functionalist understanding of the latter concept. Indeed, it can be argued that interethnic, as well as intra-ethnic networks can serve various purposes, either increasing or diminishing social capital. In fact, Varshney himself notes that intraethnic policing (equivalent to the “self-policing” mechanism proposed by Fearon and Laitin) may lead to the same result as interethnic engagement.
Finally, social capital is often linked to the success of democracy and political involvement. Robert D. Putnam, in his book "Bowling Alone" makes the argument that social capital is linked to the recent decline in American political participation. Putnam's theoretical framework has been firstly applied to the South of Italy (Putnam, 1993). This framework has been rediscussed by considering simultaneously the condition of European regions and specifically Southern Italy (Ferragina, 2012; Ferragina, 2013).
Definitions, forms, and measurement.
Social capital lends itself to multiple definitions, interpretations, and uses. David Halpern argues that the popularity of social capital for policymakers is linked to the concept's duality, coming because "it has a hard nosed economic feel while restating the importance of the social." For researchers, the term is popular partly due to the broad range of outcomes it can explain; the multiplicity of uses for social capital has led to a multiplicity of definitions. Social capital has been used at various times to explain superior managerial performance, the growth of entrepreneurial firms, improved performance of functionally diverse groups, the value derived from strategic alliances, and enhanced supply chain relations.
'A resource that actors derive from specific social structures and then use to pursue their interests; it is created by changes in the relationship among actors'; (Baker 1990, p. 619).
Early attempts to define social capital focused on the degree to which social capital as a resource should be used for public good or for the benefit of individuals. Putnam suggested that social capital would facilitate co-operation and mutually supportive relations in communities and nations and would therefore be a valuable means of combating many of the social disorders inherent in modern societies, for example crime. In contrast to those focusing on the individual benefit derived from the web of social relationships and ties individual actors find themselves in, attribute social capital to increased personal access to information and skill sets and enhanced power. According to this view, individuals could use social capital to further their own career prospects, rather than for the good of organisations.
In "The Forms of Capital" Pierre Bourdieu distinguishes between three forms of capital: economic capital, cultural capital and social capital. He defines social capital as "the aggregate of the actual or potential resources which are linked to possession of a durable network of more or less institutionalized relationships of mutual acquaintance and recognition." His treatment of the concept is instrumental, focusing on the advantages to possessors of social capital and the "“deliberate construction of sociability for the purpose of creating this resource.”" Quite contrary to Putnam's positive view of social capital, Bourdieu employs the concept to demonstrate a mechanism for the generational reproduction of inequality. Bourdieu thus points out that the wealthy and powerful use their "old boys network" or other social capital to maintain advantages for themselves, their social class, and their children.
James Coleman defined social capital functionally as “a variety of entities with two elements in common: they all consist of some aspect of social structure, and they facilitate certain actions of actors...within the structure”—that is, social capital is anything that facilitates individual or collective action, generated by networks of relationships, reciprocity, trust, and social norms. In Coleman's conception, social capital is a neutral resource that facilitates any manner of action, but whether society is better off as a result depends entirely on the individual uses to which it is put.
According to Robert Putnam, social capital "refers to the collective value of all 'social networks' and the inclinations that arise from these networks to do things for each other." According to Putnam and his followers, social capital is a key component to building and maintaining democracy. Putnam says that social capital is declining in the United States. This is seen in lower levels of trust in government and lower levels of civic participation. Putnam also says that television and urban sprawl have had a significant role in making America far less 'connected'. Putnam believes that social capital can be measured by the amount of trust and "reciprocity" in a community or between individuals.
Putnam also suggests that a root cause of the decline in social capital is women's entry the workforce, which could correlate with time restraints that inhibit civic organizational involvement like parent-teacher associations. Technological transformation of leisure (e.g., television) is another cause of declining social capital, as stated by Putnam. This offered a reference point from which several studies assessed social capital measurements by how media is engaged strategically to build social capital (
Nan Lin's concept of social capital has a more individualistic approach: "Investment in social relations with expected returns in the marketplace." This may subsume the concepts of some others such as Bourdieu, Flap and Eriksson.
Newton (1997) considered social capital as subjective phenomenon formed by values and attitudes which influence interactions.
In "Social Capital and Development: The Coming Agenda," Francis Fukuyama points out that there isn't an agreed definition of social capital, so he explains it as "shared norms or values that promote social cooperation, instantiated in actual social relationships" (Fukuyama, 27), and uses this definition throughout this paper. He argues that social capital is a necessary precondition for successful development, but a strong rule of law and basic political institutions are necessary to build social capital. He believes that a strong social capital is necessary for a strong democracy and strong economic growth. Familism is a major problem of trust because it fosters a two-tiered moral system, in which a person must favor the opinions of family members. Fukuyama believes that bridging social capital (a phrase used by Putnam in "Bowling Alone"), is essential for a strong social capital because a broader radius of trust will enable connections across borders of all sorts and serve as a basis for organizations. Although he points out many problems and possible solutions in his paper, he does admit that there is still much to be done to build a strong social capital.
Nahapiet and Ghoshal in their examination of the role of social capital in the creation of intellectual capital, suggest that social capital should be considered in terms of three clusters: structural, relational, and cognitive. Carlos García Timón describes that the structural dimensions of social capital relate to an individual ability to make weak and strong ties to others within a system. This dimension focuses on the advantages derived from the configuration of an actor's, either individual or collective, network. The differences between weak and strong ties are explained by Granovetter. The relational dimension focuses on the character of the connection between individuals. This is best characterized through trust of others and their cooperation and the identification an individual has within a network. Hazleton and Kennan added a third angle, that of communication. Communication is needed to access and use social capital through exchanging information, identifying problems and solutions, and managing conflict. According to Boisot and Boland and Tenkasi, meaningful communication requires at least some sharing context between the parties to such exchange. The cognitive dimension focusses on the shared meaning and understanding that individuals or groups have with one another.
Robison, Schmid, and Siles reviewed various definitions of social capital and concluded that many did not satisfy the formal requirement of a definition. They noted that definitions must be of the form A=B while many definition of social capital described what it can be used to achieve, where it resides, how it can be created, and what it can transform. In addition, they argue that many proposed definition of social capital fail to satisfy the requirements of capital. They propose that social capital be defined as "sympathy". The object of another's sympathy has social capital. Those who have sympathy for others provide social capital. One of the main advantages of having social capital is that it provides access to resources on preferential terms. Their definition of sympathy follows that used by Adam Smith, the title of his first chapter in the "Theory of Moral Sentiments."
A network-based conception can also be used for characterizing the social capital of collectivities (such as organizations or business clusters).
Roots.
Social capital: a new name from an old idea.
The modern emergence of social capital concept renewed the academic interest for an old debate in social science: the relationship between trust, social networks and the development of modern industrial society. Social Capital Theory gained importance through the integration of classical sociological theory with the description of an intangible form of capital. In this way the classical definition of capital has been overcome allowing researchers to tackle issues in a new manner (Ferragina, 2010:73).
Through the social capital concept researchers have tried to propose a synthesis between the value contained in the communitarian approaches and individualism professed by the 'rational choice theory.' Social capital can only be generated collectively thanks to the presence of communities and social networks, but individuals and groups can use it at the same time. Individuals can exploit social capital of their networks to achieve private objectives and groups can use it to enforce a certain set of norms or behaviors. In this sense, social capital is generated collectively but it can also be used individually, bridging the dichotomized approach 'communitarianism' versus 'individualism' (Ferragina, 2010:75).
Definitional issues.
The term "capital" is used by analogy with other forms of economic capital, as social capital is argued to have similar (although less measurable) benefits. However, the analogy with capital is misleading to the extent that, unlike traditional forms of capital, social capital is not depleted by use; in fact it is depleted by non-use ("use it or lose it"). In this respect, it is similar to the now well-established economic concept of human capital.
Social capital is also distinguished from the economic theory social capitalism. Social capitalism as a theory challenges the idea that socialism and capitalism are mutually exclusive. Social capitalism posits that a strong social support network for the poor enhances capital output. By decreasing poverty, capital market participation is enlarged.
Sub-types.
In "Bowling Alone: The Collapse and Revival of American Community" (Simon & Schuster, 2000), Harvard political scientist Robert D. Putnam wrote: "Henry Ward Beecher's advice a century ago to 'multiply picnics' is not entirely ridiculous today. We should do this, ironically, not because it will be good for America — though it will be — but because it will be good for us." This quote is illustrative of the use of social capital within neo-liberal discourse to divert attention away from economic inequality as the source of social problems. 
Daniel P. Aldrich, Associate Professor at Purdue University, also argues the three mechanics of social capital. Aldrich defines the three differences as bonding, bridging, and linking social capital. Bonding capital is the relationships a person has with friends and family, making it also the strongest form of social capital. Bridging capital is the relationship between friends of friends, making its strength secondary to bonding capital. Linking capital is the relationship between a person and a government official or other elected leader. Aldrich also applies the ideas of social capital to the fundamental principles of disaster recovery, and discusses factors that either aid or impede recovery, such as extent of damage, population density, quality of government and aid. He primarily examines Japanese recovery following the 2011 Fukishima nuclear meltdown in his book "Building Resilience: Social Capital in Post-Disaster Recovery."
Putnam speaks of two main components of the concept: "bonding social capital" and "bridging social capital", the creation of which Putnam credits to Ross Gittell and Avis Vidal. Bonding refers to the value assigned to social networks between homogeneous groups of people and Bridging refers to that of social networks between socially heterogeneous groups. Typical examples are that criminal gangs create bonding social capital, while choirs and bowling clubs (hence the title, as Putnam lamented their decline) create bridging social capital. Bridging social capital is argued to have a host of other benefits for societies, governments, individuals, and communities; Putnam likes to note that joining an organization cuts in half an individual's chance of dying within the next year.
The distinction is useful in highlighting how social capital may not always be beneficial for society as a whole (though it is always an asset for those individuals and groups involved). Horizontal networks of individual citizens and groups that enhance community productivity and cohesion are said to be positive social capital assets whereas self-serving exclusive gangs and hierarchical patronage systems that operate at cross purposes to societal interests can be thought of as negative social capital burdens on society.
Social capital development on the internet via social networking websites such as Facebook or Myspace tends to be bridging capital according to one study, though "virtual" social capital is a new area of research.
There are two other sub-sources of social capital. These are consummatory, or a behavior that is made up of actions that fulfill a basis of doing what is inherent, and instrumental, or behavior that is taught through ones surroundings over time. Two examples of consummatory social capital are value interjection and solidarity. Value interjection pertains to a person or community that fulfills obligations such as paying bills on time, philanthropy, and following the rules of society. People that live their life this way feel that these are norms of society and are able to live their lives free of worry for their credit, children, and receive charity if needed. Coleman goes on to say that when people live in this way and benefit from this type of social capital, individuals in the society are able to rest assured that their belongings and family will be safe.
The other form of consummatory social capital, solidarity, dates back to the writings of Karl Marx, a German philosopher and political economist from the 19th century. The main focus of the study of Karl Marx was the working class of the Industrial Revolution. Marx analyzed the reasons these workers supported each other for the benefit of the group. He held that this support was an adaptation to the immediate time as opposed to a trait that was installed in them throughout their youth. As another example, Coleman states that this type of social capital is the type that brings individuals to stand up for what they believe in, and even die for it, in the face of adversity.
The second of these two other sub-sources of social capital is that of instrumental social capital. The basis of the category of social capital is that an individual who donates his or her resources not because he is seeking direct repayment from the recipient, but because they are part of the same social structure. By his or her donation, the individual might not see a direct repayment, but, most commonly, they will be held by the society in greater honor. The best example of this, and the one that Portes mentions, is the donation of a scholarship to a member of the same ethnic group. The donor is not freely giving up his resources to be directly repaid by the recipient, but, as stated above, the honor of the community. With this in mind, the recipient might not know the benefactor personally, but he or she prospers on the sole factor that he or she is a member of the same social group.
Measurement.
There is no widely held consensus on how to measure social capital, which has become a debate in itself: why refer to this phenomenon as 'capital' if there is no true way to measure it? While one can usually intuitively sense the level/amount of social capital present in a given relationship (regardless of type or scale), quantitative measuring has proven somewhat complicated. This has resulted in different metrics for different functions. In measuring political social capital, it is common to take the sum of society’s membership of its groups. Groups with higher membership (such as political parties) contribute more to the amount of capital than groups with lower membership, although many groups with low membership (such as communities) still add up to be significant. While it may seem that this is limited by population, this need not be the case as people join multiple groups. In a study done by Yankee City, a community of 17,000 people was found to have over 22,000 different groups.
Many studies measure social capital by asking the question: “do you trust the others?” Other researches analyse the participation in voluntary associations or civic activities.
Knack and Keefer (1996) measured econometrically correlations between confidence and civic cooperation norms, with economic growth in a big group of countries. They found that confidence and civic cooperation have a great impact in economic growth, and that in less polarized societies in terms of inequality and ethnic differences, social capital is bigger.
Narayan and Pritchet (1997) researched the associativity degree and economic performance in rural homes of Tanzania. They saw that even in high poverty indexes, families with higher levels of incomes had more participation in collective organizations. The social capital they accumulated because of this participation had individual benefits for them, and created collective benefits through different routes, for example: their agricultural practices were better than those of the families without participation (they had more information about agrochemicals, fertilizers and seeds); they had more information about the market; they were prepared to take more risks, because being part of a social network made them feel more protected; they had an influence on the improvement of public services, showing a bigger level of participation in schools; they cooperated more in the municipality level.
The level of cohesion of a group also affects its social capital. However, there is no one quantitative way of determining the level of cohesiveness, but rather a collection of social network models that researchers have used over the decades to operationalize social capital. One of the dominant methods is Ronald Burt's constraint measure, which taps into the role of tie strength and group cohesion. Another network-based model is network transitivity.
How a group relates to the rest of society also affects social capital, but in a different manner. Strong internal ties can in some cases weaken the group’s perceived capital in the eyes of the general public, as in cases where the group is geared towards crime, distrust, intolerance, violence or hatred towards others. The Ku Klux Klan and the Mafia are examples of these kinds of organizations.
Sociologists Carl L. Bankston and Min Zhou have argued that one of the reasons social capital is so difficult to measure is that it is neither an individual-level nor a group-level phenomenon, but one that emerges across levels of analysis as individuals participate in groups. They argue that the metaphor of "capital" may be misleading because unlike financial capital, which is a resource held by an individual, the benefits of forms of social organization are not held by actors, but are results of the participation of actors in advantageously organized groups.
To expand upon the methodological potential of measuring online and offline social bonding, as it relates to social capital, offers a matrix of social capital measures that distinguishes social bridging as a form of less emotionally tethered relationships compared to bonding. Bonding and bridging sub-scales are proposed, which have been adopted by over 300 scholarly articles . Lin, Peng, Kim, Kim & LaRose (2012) offer a noteworthy application of the scale by measuring international residents originating from locations outside of the United States. The study found that social media platforms like Facebook provide an opportunity for increased social capital, but mostly for extroverts. However, less introverted social media users could engage social media and build social capital by connecting with Americans before arriving and then maintaining old relationships from home upon arriving to the states. The ultimate outcome of the study indicates that social capital is measurable and is a concept that may be operationalized to understand strategies for coping with cross-cultural immersion through online engagement.
Recently, Foschi and Lauriola presented a measure of sociability as a proxy of social capital. The authors demonstrated that facets of sociability can mediate between general personality traits and measures of civic involvement and political participation, as predictors of social capital, in a holistic model of political behavior.
Integrating history and socio-economic analysis.
Beyond Putnam.
Robert Putnam's work contributed to shape the discussion of the importance of social capital. His conclusions have been praised but also criticised. Criticism has mainly focused on:
Ferragina (2012; 2013) integrated the insights of these two criticisms and proposed a cross-regional analysis of 85 European regions, linking together the socio-economic and the historic- institutional analyses to explore the determinants of social capital. He argued that to investigate the determinants of social capital, one has to integrate the synchronic and the diachronic perspectives under the guidance of a methodological framework able to put these two approaches in continuity.
The sleeping social capital theory.
Putnam’s work, nourished by doctrines like the "end of history" (Fukuyama 1992) was largely deterministic, and proposed the dismissal of more articulated historical interpretations. This determinism has reduced Southern Italian history as being a negative path to modernity; only the Italian regions that experienced the development of medieval towns during the twelfth and thirteenth centuries have got high levels of social capital today, the others ‘are condemned’ by the prevalence of the authoritarian rule of the Normans more than 800 years ago.
However, from a purely historical perspective, the medieval town is not unanimously considered to be a symbol of freedom, creation of horizontal ties and embryo of democratic life. In Making Democracy Work, Putnam disregarded the division within municipal towns and their dearth of civic participation and considered only the experience of few areas in North Central Italy, ignoring the existence of important towns in the South.
To this more complicated historical picture, Ferragina (2012) added the result of a regression model, which indicated that social capital in the South of Italy and Wallonia should be much lower than currently detected according to their socio-economic condition. He unfolded Putnam’s theory by undertaking a comparative analysis between these two deviant cases and two regular cases located in the same country, namely Flanders and the North of Italy. The historical legacy does not have a negative effect on the present lack of social capital in Wallonia and the South of Italy, but the potentially positive effect of the historical legacy is currently curtailed by the poor socio-economic conditions, notably by the high level of income inequality and the low level of labour market participation. This historical interpretation is driven by the comparison with Flanders and the North East of Italy.
The value of the historical legacy for present socio-economic development is similar to the ‘appropriable social capital’ theorized by Coleman (1990) at the individual level. Using the example of the Korean students, Coleman argued that the construction of a secret network of people (at a time in which the appreciation for the authoritarian government was rapidly declining among the population) as a means of organizing the democratic revolt was the result of a process of socialization that took place during their childhood (with the involvement in the local churches).
The relation between historical evolutions and the socio-economic variables has similar characteristics at the macro level. Only after reaching a sufficient level of labour market activity and income redistribution (this is comparable to the growing unpopularity of the authoritarian government) can the memory of historical events of social engagement become fully appropriable by the population (this is comparable to the participation in the local churches during childhood), leading to the development of innovative forms of social participation (this is comparable to the construction of the secret circles that enhanced the democratic revolt). This process increases social capital even further if socio-economic development is matched by the revival of the unique historical legacy of the area. The reconstruction of this unique past can rapidly become a source of pride for the entire area, contributing in turn to an increasing intra-regional solidarity, and with it enhancement of social networks and social trust.
The Flemish case (and also to a lesser extent that of the North East of Italy) illustrates this process well. The socio-economic improvements that took place in the nineteenth century were matched by the revival of the glorious Flemish traditions of the thirteenth and fourteenth century. The increase of social capital generated by the reduction of income inequality and the increasing participation in the labour market due to the economic development was multiplied by the reconstruction of Flemish identity and pride. This pride and self- confidence has, in turn, increased the feeling of solidarity within the region and contributed to generate a level of social capital, which is hardly explicable by the single socio-economic predictors.
Ferragina suggests that, in the divergent cases, the value of the historical legacy is affected by the poor present socio- economic conditions. Social capital sleeps, not because of the absence of certain clearly defined historical steps as suggested by Putnam, but because socio- economic underdevelopment profoundly depressed the self-pride of Southern Italians and Walloons.
The biased and simplistic interpretations of Southern Italian and Walloon history will be discarded only when their socio-economic conditions reach a sufficient level, enacting a cycle similar to Flanders and the North East of Italy. Stronger redistribution, an increase of labour market participation accompanied by a simultaneous process of ‘reinvention of the past’ could enhance a positive cycle of social capital increase in both areas. The historical legacy in these two areas should not be seen as the root of the present lack of social capital but as a potential element for improvement. Important moments of social engagement also existed in the history of these two areas; the imagery of Walloons and Southern Italians should be nourished by these almost forgotten examples of collective history (i.e. the Fasci Siciliani in the south of Italy) rather than the prevailing idea that the historical legacy of these areas is simply an original sin, a burden to carry through the process of modernization.
Social capital motives.
Robison and colleagues measured the relative importance of selfishness and four social capital motives using resource allocation data collected in hypothetical surveys and non-hypothetical experiments. The selfishness motive assumes that an agent's allocation of a scarce resource is independent of his relationships with others. This motive is sometimes referred to as the selfishness of preference assumption in neoclassical economics. Social capital motives assume that agents’ allocation of a scarce resource may be influenced by their social capital or sympathetic relationships with others which may produce socio-emotional goods that satisfy socio-emotional needs for validation and belonging. The first social capital motive seeks for validation by acting consistently with the values of one’s ideal self. The second social capital motive seeks to be validated by others by winning their approval. The third social capital motive seeks to belong. Recognizing that one may not be able to influence the sympathy of others, persons seeking to belong may act to increase their own sympathy for others and the organizations or institutions they represent. The fourth social capital motive recognizes that our sympathy or social capital for another person will motivate us to act in their interest. In doing so we satisfy our own needs for validation and belonging. Empirical results reject the hypothesis often implied in economics that we are 95% selfish.
Relation with civil society.
A number of authors give definitions of civil society that refer to voluntary associations and organisations outside the market and state. This definition is very close to that of the third sector, which consists of "private organisations that are formed and sustained by groups of people acting voluntarily and without seeking personal profit to provide benefits for themselves or for others". According to such authors as Walzer, Alessandrini, Newtown, Stolle and Rochon, Foley and Edwards, and Walters, it is through civil society, or more accurately, the third sector, that individuals are able to establish and maintain relational networks. These voluntary associations also connect people with each other, build trust and reciprocity through informal, loosely structured associations, and consolidate society through altruism without obligation. It is "this range of activities, services and associations produced by... civil society" that constitutes the sources of social capital.
If civil society, then, is taken to be synonymous with the third sector then the question it seems is not 'how important is social capital to the production of a civil society?' but 'how important is civil society to the production of social capital?'. Not only have the authors above documented how civil society produces sources of social capital, but in Lyons work "Third Sector", social capital does not appear in any guise under either the factors that enable or those that stimulate the growth of the third sector, and Onyx describes how social capital depends on an already functioning community.
The idea that creating social capital (i.e., creating networks) will strengthen civil society underlies current Australian social policy aimed at bridging deepening social divisions. The goal is to reintegrate those marginalised from the rewards of the economic system into "the community". However, according to Onyx (2000), while the explicit aim of this policy is inclusion, its effects are exclusionary.
Foley and Edwards believe that "political systems... are important determinants of both the character of civil society and of the uses to which whatever social capital exists might be put". Alessandrini agrees, saying, "in Australia in particular, neo-liberalism has been recast as economic rationalism and identified by several theorists and commentators as a danger to society at large because of the use to which they are putting social capital to work".
The resurgence of interest in social capital as a remedy for the cause of today’s social problems draws directly on the assumption that these problems lie in the weakening of civil society. However this ignores the arguments of many theorists who believe that social capital leads to exclusion rather than to a stronger civil society. In international development, Ben Fine and John Harriss have been heavily critical of the inappropriate adoption of social capital as a supposed panacea (promoting civil society organisations and NGOs, for example, as agents of development) for the inequalities generated by neo liberal economic development. This leads to controversy as to the role of state institutions in the promotion of social capital.
An abundance of social capital is seen as being almost a necessary condition for modern liberal democracy. A low level of social capital leads to an excessively rigid and unresponsive political system and high levels of corruption, in the political system and in the region as a whole. Formal public institutions require social capital in order to function properly, and while it is possible to have too much social capital (resulting in rapid changes and excessive regulation), it is decidedly worse to have too little.
Kathleen Dowley and Brian Silver published an article entitled "Social Capital, Ethnicity and Support for Democracy in the Post-Communist States". This article found that in post-communist states, higher levels of social capital did not equate to higher levels of democracy. However, higher levels of social capital led to higher support for democracy.
A number of intellectuals in developing countries have argued that the idea of social capital, particularly when connected to certain ideas about civil society, is deeply implicated in contemporary modes of donor and NGO driven imperialism and that it functions, primarily, to blame the poor for their condition.
The concept of social capital in a Chinese social context has been closely linked with the concept of "guanxi".
An interesting attempt to measure social capital spearheaded by Corporate Alliance in the English speaking market segment of the United States of America and Xentrum through the Latin American Chamber of Commerce in Utah on the Spanish speaking population of the same country, involves the quantity, quality and strength of an individual social capital. With the assistance of software applications and web-based relationship-oriented systems such as LinkedIn, these kinds of organizations are expected to provide its members with a way to keep track of the "number" of their relationships, meetings designed to boost the "strength" of each relationship using group dynamics, executive retreats and networking events as well as training in how to reach out to higher circles of "influential" people.
Social capital and women's engagement with politics.
There are many factors that drive volume towards the ballot box, including education, employment, civil skills, and time. Careful evaluation of these fundamental factors often suggests that women do not vote at similar levels as men. However the gap between women and men voter turnout is diminishing and in some cases women are becoming more prevalent at the ballot box than their male counterparts. Recent research on social capital is now serving as an explanation for this change.
Social capital offers a wealth of resources and networks that facilitate political engagement. Since social capital is readily available no matter the type of community, it is able to override more traditional queues for political engagement; e.g.: education, employment, civil skills, etc.
There are unique ways in which women organize. These differences from men make social capital more personable and impressionable to women audiences thus creating a stronger presence in regards to political engagement. A few examples of these characteristics are:
The often informal nature of female social capital allows women to politicize apolitical environments without conforming to masculine standards, thus keeping this activity off the radar. These differences are hard to recognize within the discourse of political engagement and may explain why social capital has not been considered as a tool for female political engagement until as of late.
Effects on health.
A growing body of research has found that the presence of social capital through social networks and communities has a protective quality on health. Social capital affects health risk behavior in the sense that individuals who are embedded in a network or community rich in support, social trust, information, and norms, have resources that help achieve health goals. For example, a person who is sick with cancer may receive information, money, or moral support he or she needs to endure treatment and recover. Social capital also encourages social trust and membership. These factors can discourage individuals from engaging in risky health behaviors such as smoking and binge drinking.
Inversely, a lack of social capital can impair health. For example, results from a survey given to 13- to 18-year-old students in Sweden showed that low social capital and low social trust are associated with higher rates of psychosomatic symptoms, musculoskeletal pain, and depression. Additionally, negative social capital can detract from health. Although there are only a few studies that assess social capital in criminalized populations, there is information that suggests that social capital does have a negative effect in broken communities. Deviant behavior is encouraged by deviant peers via favorable definitions and learning opportunities provided by network-based norms. However in these same communities, an adjustment of norms (i.e. deviant peers being replaced by positive role models) can pose a positive effect.
Effects of the Internet.
Similar to watching the news and keeping abreast of current events, the use of the Internet can relate to an individual's level of social capital. In one study, informational uses of the Internet correlated positively with an individual's production of social capital, and social-recreational uses were negatively correlated (higher levels of these uses correlated with lower levels of social capital). An example supporting the former argument is the contribution of Peter Maranci's blog (Charlie on the Commuter Line) to address the train problems in Massachusetts. He created it after an incident where a lady passed out during a train ride due to the congestion in the train and help was delayed because of the congestion in the train and the inefficiency of the train conductor. His blog exposed the poor conditions of train stations, overcrowding train rides and inefficiency of the train conductor which eventually influenced changes within the transit system. Another perspective holds that the rapid growth of social networking sites such as Facebook and Myspace suggests that individuals are creating a virtual-network consisting of both bonding and bridging social capital. Unlike face to face interaction, people can instantly connect with others in a targeted fashion by placing specific parameters with internet use. This means that individuals can selectively connect with others based on ascertained interests, and backgrounds. Facebook is currently the most popular social networking site and touts many advantages to its users including serving as a "social lubricant" for individuals who otherwise have difficulties forming and maintaining both strong and weak ties with others.
This argument continues, although the preponderance of evidence shows a positive association between social capital and the internet. Critics of virtual communities believe that the Internet replaces our strong bonds with online "weak-ties" or with socially empty interactions with the technology itself. Others fear that the Internet can create a world of "narcissism of similarity," where sociability is reduced to interactions between those that are similar in terms of ideology, race, or gender. A few articles suggest that technologically based interactions has a negative relationship with social capital by displacing time spent engaging in geographical/ in-person social activities. However, the consensus of research shows that the more time people spend online the more in-person contact they have, thus positively enhancing social capital.
Recent research, conducted 2006, also shows that Internet users often have wider networks than those who uses internet irregularly or not at all. When not considering family and work contacts, Internet users actually tend to have contact with a higher number of friends and relatives. This is supported by another study that shows that internet users and non-internet users do feel equally close to the same number of people; also the internet users maintain relationships with 20% more people that they “feel somewhat close” to.
Other research shows that younger people use the Internet as a supplemental medium for communication, rather than letting the Internet communication replace face-to-face contact. This supports the view that Internet communication does not hinder development of social capital and does not make people feel lonelier than before.
Ellison, Steinfield & Lampe (2007) suggest social capital exercised online is a result of relationships formed offline; whereby, bridging capital is enabled through a "maintenance" of relationships. Among respondents of this study, social capital built exclusively online creates weaker ties. A distinction of social bonding is offered by Ellison et al., 2007, suggesting bonds, or strong ties, are possible through social media, but less likely.
Effects on educational achievement.
Coleman and Hoffer collected quantitative data of 28,000 students in total 1,015 public, Catholic and other private high schools in America from the 7 years' period from 1980 to 1987. It was found from this longitudinal research that social capital in students' families and communities attributed to the much lower dropout rates in Catholic schools compared with the higher rates in public.
Teachman et al. further develop the family structure indicator suggested by Coleman. They criticise Coleman, who used only the number of parents present in the family, neglected the unseen effect of more discrete dimensions such as stepparents' and different types of single-parent families. They take into account of a detailed counting of family structure, not only with two biological parents or stepparent families, but also with types of single-parent families with each other (mother-only, father-only, never-married, and other). They also contribute to the literature by measuring parent-child interaction by the indicators of how often parents and children discuss school-related activities.
Morgan and Sorensen directly challenge Coleman for his lacking of an explicit mechanism to explain why Catholic schools students perform better than public school students on standardised tests of achievement. Researching students in Catholic schools and public schools again, they propose two comparable models of social capital effect on mathematic learning. One is on Catholic schools as norm-enforcing schools whereas another is on public schools as horizon-expanding schools. It is found that while social capital can bring about positive effect of maintaining an encompassing functional community in norm-enforcing schools, it also brings about the negative consequence of excessive monitoring. Creativity and exceptional achievement would be repressed as a result. Whereas in horizon expanding school, social closure is found to be negative for student's mathematic achievement. These schools explore a different type of social capital, such as information about opportunities in the extended social networks of parents and other adults. The consequence is that more learning is fostered than norm-enforcing Catholic school students. In sum, Morgan and Sorensen's (1999) study implies that social capital is contextualised, one kind of social capital may be positive in this setting but is not necessarily still positive in another setting.
In the setting of education through Kilpatrick et al., (2010) state, ‘... social capital is a useful lens for analysing lifelong learning and its relationship to community development’. Social capital is particularly important in terms of education. Also the importance of education with ‘...schools being designed to create “functioning community”- forging tighter links between parents and the school’ (Coleman &Hoffer, 1987) linking that without this interaction, the social capital in this area is disadvantaged and demonstrates that social capital plays a major role in education.
Without social capital in the area of education, teachers and parents that play a responsibility in a students learning, the significant impacts on their child’s academic learning can rely on these factors. With focus on parents contributing to their child’s academic progress as well as being influenced by social capital in education. Without the contribution by the parent in their child’s education, gives parents less opportunity and participation in the student’s life. As Tedin et al. (2010) state ‘...one of the most important factors in promoting student success is the active involvement of parents in a child’s education.’’ With parents also involved in activities and meetings the school conducts, the more involved parents are with other parents and the staff members. Thus parent involvement contributes to social capital with becoming more involved in the school community and participating makes the school a sustainable and easy to run community.
In their journal article "Beyond social capital: Spatial dynamics of collective efficacy for children", Sampson et al. stress the normative or goal-directed dimension of social capital. They claim, "resources or networks alone (e.g. voluntary associations, friendship ties, organisational density) are neutral--- they may or may not be effective mechanism for achieving intended effect"
Marjoribanks and Kwok conducted a survey in Hong Kong secondary schools with 387 fourteen-year-old students with an aim to analyse female and male adolescents differential educational achievement by using social capital as the main analytic tool. In that research, social capital is approved of its different effects upon different genders. In his thesis "New Arrival Students in Hong Kong: Adaptation and School Performance", Hei Hang Hayes Tang argues that adaptation is a process of activation and accumulation of (cultural and social) capitals. The research findings show that supportive networks is the key determinant differentiating the divergent adaptation pathways. Supportive networks, as a form of social capital, is necessary for activating the cultural capital the newly arrived students possessed. The amount of accumulated capital is also relevant to further advancement in the ongoing adaptation process.
Min Zhou and Carl L. Bankston in their study of a Vietnamese community in New Orleans find that preserving traditional ethnic values enable immigrants to integrate socially and to maintain solidarity in an ethnic community. Ethnic solidarity is especially important in the context where immigrants just arrive in the host society. In her article "Social Capital in Chinatown", Zhou examines how the process of adaptation of young Chinese Americans is affected by tangible forms of social relations between the community, immigrant families, and the younger generations. Chinatown serves as the basis of social capital that facilitates the accommodation of immigrant children in the expected directions. Ethnic support provides impetus to academic success. Furthermore maintenance of literacy in native language also provides a form of social capital that contributes positively to academic achievement. Stanton-Salazar and Dornbusch found that bilingual students were more likely to obtain the necessary forms of institutional support to advance their school performance and their life chances.
Putnam (2000) mentions in his book "Bowling Alone", "Child development is powerfully shaped by social capital" and continues "presence of social capital has been linked to various positive outcomes, particularly in education". According to his book, these positive outcomes are the result of parents' social capital in a community. In states where there is a high social capital, there is also a high education performance. The similarity of these states is that parents were more associated with their children's education. Teachers have reported that when the parents participate more in their children's education and school life, it lowers levels of misbehavior, such as bringing weapons to school, engaging in physical violence, unauthorized absence, and being generally apathetic about education. Borrowing Coleman's quotation from Putnam's book, Coleman once mentioned we cannot understate "the importance of the embeddedness of young persons in the enclaves of adults most proximate to them, first and most prominent the family and second, a surrounding community of adults".
In geography.
In order to understand social capital as a subject in geography, one must look at it in a sense of space, place, and territory. In its relationship, the tenets of geography relate to the ideas of social capital in the family, community, and in the use of social networks. The biggest advocate for seeing social capital as a geographical subject was American economist and political scientist Robert Putnam. His main argument for classifying social capital as a geographical concept is that the relationships of people is shaped and molded by the areas in which they live.
Putnam (1993) argued that the lack of social capital in the South of Italy was more the product of a peculiar historical and geographical development than the consequence of a set of contemporary socio-economic conditions. This idea has sparked a lengthy debate and received fierce criticism (Ferragina, 2010; Ferragina 2012: 3). There are many areas in which social capital can be defined by the theories and practices. Anthony Giddens developed a theory in 1984 in which he relates social structures and the actions that they produce. In his studies, he does not look at the individual participants of these structures, but how the structures and the social connections that stem from them are diffused over space. If this is the case, the continuous change in social structures could bring about a change in social capital, which can cause changes in community atmosphere. If an area is plagued by social organizations whose goals are to revolt against social norms, such as gangs, it can cause a negative social capital for the area causing those who disagreed with said organizations to relocate thus taking their positive social capital to a different space than the negative.
Another area where social capital can be seen as an area of study in geography is through the analysis of participation in volunteerism and its support of different governments. One area to look into with this is through those who participate in social organizations. People that participate are of different races, ages, and economic status. With these in mind, variances of the space in which these different demographics may vary, causing a difference in involvement among areas. Secondly, there are different social programs for different areas based on economic situation. A governmental organization would not place a welfare center in a wealthier neighborhood where it would have very limited support to the community, as it is not needed. Thirdly, social capital can be affected by the participation of individuals of a certain area based on the type of institutions that are placed there. Mohan supports this with the argument of J. Fox in his paper "Decentralization and Rural Development in Mexico", which states “structures of local governance in turn influence the capacity of grassroots communities to influence social investments." With this theory, if the involvement of a government in specific areas raises the involvement of individuals in social organizations and/or communities, this will in turn raise the social capital for that area. Since every area is different, the government takes that into consideration and will provide different areas with different institutions to fit their needs thus there will be different changes in social capital in different areas.
Negative social capital.
It has been noted that social capital may be not always be used for positive ends. An example of the complexities of the effects of social capital is violent or criminal gang activity that is encouraged through the strengthening of intra-group relationships (bonding social capital). The negative consequences of social capital are more often associated with "bonding" vis-à-vis "bridging".
Without "bridging" social capital, "bonding" groups can become isolated and disenfranchised from the rest of society and, most importantly, from groups with which bridging must occur in order to denote an "increase" in social capital. Bonding social capital is a necessary antecedent for the development of the more powerful form of bridging social capital. Bonding and bridging social capital can work together productively if in balance, or they may work against each other. As social capital bonds and stronger homogeneous groups form, the likelihood of bridging social capital is attenuated. Bonding social capital can also perpetuate sentiments of a certain group, allowing for the bonding of certain individuals together upon a common radical ideal. The strengthening of insular ties can lead to a variety of effects such as ethnic marginalization or social isolation. In extreme cases ethnic cleansing may result if the relationship between different groups is so strongly negative. In mild cases, it just isolates certain communities such as suburbs of cities because of the bonding social capital and the fact that people in these communities spend so much time away from places that build bridging social capital.
Social capital (in the institutional Robert Putnam sense) may also lead to bad outcomes if the political institution and democracy in a specific country is not strong enough and is therefore overpowered by the social capital groups. "Civil society and the collapse of the Weimar Republic" suggests that "it was weak political institutionalization rather than a weak civil society that was Germany’s main problem during the Wihelmine and Weimar eras." Because the political institutions were so weak people looked to other outlets. “Germans threw themselves into their clubs, voluntary associations, and professional organizations out of frustration with the failures of the national government and political parties, thereby helping to undermine the Weimar Republic and facilitate Hitler’s rise to power.” In this article about the fall of the Weimar Republic, the author makes the claim that Hitler rose to power so quickly because he was able to mobilize the groups towards one common goal. Even though German society was, at the time, a "joining" society these groups were fragmented and their members did not use the skills they learned in their club associations to better their society. They were very introverted in the Weimar Republic. Hitler was able to capitalize on this by uniting these highly bonded groups under the common cause of bringing Germany to the top of world politics. The former world order had been destroyed during World War I, and Hitler believed that Germany had the right and the will to become a dominant global power. Additionally, in "," Michael Shindler expands upon Berman's argument that Wiemar social clubs and similar associations in countries that did not develop democracy, were organized in such a way that they fostered a "we" instead of an "I" mentality among their members, by arguing that groups which possess cultures that stress solidarity over individuality, even ones that are "horizontally" structured and which were also common to pre-soviet eastern europe, will not engender democracy if they are politically aligned with non-democratic ideologies.
Later work by Putnam also suggests that social capital, and the associated growth of public trust are inhibited by immigration and rising racial diversity in communities. Putnam's study regarding the issue argued that in American areas with a lack of homogeneity, some individuals neither participated in bonding nor bridging social capital. In societies where immigration is high (USA) or where ethnic heterogeneity is high (Eastern Europe), it was found that citizens lacked in both kinds of social capital and were overall far less trusting of others than members of homogenous communities were found to be. Lack of homogeneity led to people withdrawing from even their closest groups and relationships, creating an atomized society as opposed to a cohesive community. These findings challenge previous beliefs that exposure to diversity strengthens social capital, either through bridging social gaps between ethnicities or strengthening in-group bonds.
Social capital and reproduction of inequality.
Coleman indicated that social capital eventually led to the creation of human capital for the future generation. Human capital, a private resource, could be accessed through what the previous generation accumulated through social capital. Field suggested that such a process could lead to the very inequality social capital attempts to resolve. While Coleman viewed social capital as a relatively neutral resource, he did not deny the class reproduction that could result from accessing such capital, given that individuals worked toward their own benefit. Even though Coleman never truly addresses Bourdieu in his discussion, this coincides with Bourdieu's argument set forth in Reproduction in Education, Society and Culture. Bourdieu and Coleman were fundamentally different at the theoretical level (as Bourdieu believed the actions of individuals were rarely ever conscious, but more so only a result of their habitus (see below) being enacted within a particular field, but this realization by both seems to undeniably connect their understanding of the more latent aspects of social capital.
According to Bourdieu, habitus refers to the social context within which a social actor is socialized. Thus, it is the social platform, itself, that equips one with the social reality they become accustomed to. Out of habitus comes field, the manner in which one integrates and displays his or her habitus. To this end, it is the social exchange and interaction between two or more social actors. To illustrate this, we assume that an individual wishes to better his place in society. He therefore accumulates social capital by involving himself in a social network, adhering to the norms of that group, allowing him to later access the resources (e.g. social relationships) gained over time. If, in the case of education, he uses these resources to better his educational outcomes, thereby enabling him to become socially mobile, he effectively has worked to reiterate and reproduce the stratification of society, as social capital has done little to alleviate the system as a whole. This may be one negative aspect of social capital, but seems to be an inevitable one in and of itself, as are all forms of capital.
See also.
</dl>
Citations.
Ellison, N.B., Steinfield, C., & Lampe, C. (2007). The benefits of Facebook “friends”: Social 
capital and college students’ use of online social network sites. Journal of 
Computer-Mediated Communication, 12, 1143-1168.
Lin, J.-H., Peng, W., Kim, M. Kim, S. Y., & LaRose, R. (2012). Social networking and 
adjustments among international students. New Media & Society, 14, 421-440.
Putnam, R. D. (1995). Bowling alone: America’s declining social capital. Journal of Democracy, 
6(1), 65-78.
Williams, D. (2006). On and off the ‘net: Scales for social capital in an online era. Journal of 
Computer-Mediated Communication, 11, 593-628.

</doc>
<doc id="45803" url="http://en.wikipedia.org/wiki?curid=45803" title="Individual capital">
Individual capital

Individual capital, the economic view of talent, comprises inalienable or personal traits of persons, tied to their bodies and available only through their own free will, such as skill, creativity, enterprise, courage, capacity for moral example, non-communicable wisdom, invention or empathy, non-transferable personal trust and leadership.
As recognized in theories of economics.
Individual talent & initiative was recognized as an intangible quality of persons in economics back to at least Adam Smith. He distinguished it (as "enterprise") from labour which can be coerced and is usually seen as strictly imitative (learned or transmitted, via such means as apprenticeship). 
Marxist economics refers instead to "an individual's social capital - individuals are sources neither of creativity and innovation, nor management skill. A problem with that analysis is that it simply cannot explain the substitution problem and lack of demand that occurs when, for instance, an understudy takes on a leading role, or a second author takes over writing a popular book series. At the very least there must be some conditional, if not firm-specific then "class specific", special ability to command premiums for outstanding personal performance.
Neoclassical economics by contrast refers to , which implies a strong association of the individual with the instructional capital they learn from, with little or no social capital influence. This is orthogonal to the Marxist view, but not necessarily opposed.
Human development theory reflects both distinctions: it sees labour as the yield of individual capital in the same way that neoclassical macro-economics sees financial capital as the yield of the looser idea of human capital. But the rest problem and social welfare function selection, as well as the subjective factors in behavioral finance, has led to a closer analysis of factors of production. In effect, the financial architecture is no longer trusted as an arbiter of the value of life as it was in neoclassical economics. Money is not seen as values-neutral, but as embodying a set of larger social choices about money supply rules, made by measuring well-being of whole populations.
Versus "human", "firm-specific", "individual social".
While conflated in many analyses with human capital, the latter term includes social capital (human relationships) and instructional capital (abstract texts and training materials and so on) that are not tied to any one person, do not die with them or leave employment with them, and therefore cannot be equated with talent alone. In intangibles measurement, value creation and value reporting metrics require all assets with such different characteristics to be categorized as different capital assets, so the more exact reference to the individual person is preferred.
Fusions of terminology are common. Sociological analysts refer to "individual-level elements of social capital" or "an individual's social capital" or just "individual social capital" while economic analysts often use the phrase firm-specific human capital. In either case the clearly includes individual capital but also some "activity-", "community-" or "firm-specific" social capital (community trust) and instructional capital (shareable knowledge or skills). This is easy to measure: its yield is your salary in your current job.
To the degree this is consistent if you take other work nearby, this opens the questions of what is "not" "firm-specific" and whether a nation is just a bigger "firm": Some analyses see political capital, or just "influence" or "trust of professionals" as a full style of capital of its own. Some ethicists, most clearly Jane Jacobs, see this as simple corruption. Nonetheless, corruption clearly has a cash value, involves some creativity to arrange, and is a decision factor. It is a skill like any other.
Versus "intellectual capital".
Perhaps because of this, not all theorists recognize individual capital as being as essential as labour, or distinct from social or political influence, or from instructional capacity. These theorists often refer to "intellectual capital", which more properly describes a debate or locus of complexity that arises when individuals take key instructional roles. Some refer to celebrity as another fusion, when individuals take key social roles.
However, a great many celebrities are clearly not "intellectual" achievers nor notable for any cognitive or analytic powers, e.g. Kim Kardashian, professional sports figures or other athletes. While they may through sheer exposure become involved in causes or controversies (as Paris Hilton did in the US presidential election, 2008) it's clearly not correct to label all individually unique talent or economic value as being an "intellectual" asset.
This failure to distinguish individual's objectively observed economic value (the power to promote or publicize products, draw attention to causes, etc.) from the "intellectual" powers is probably an elitist bias. Clearly, there are some individuals, including non-humans such as a racehorse, which have economic value unique to their individual body and being that cannot be captured or defined as an "intellectual" asset nor as a set of "social" relationships (because horses do not socialize in the sense humans do). Where slavery exists or has existed, there is clearly a value put on living bodies separate from their instructional or social selves. 
Thus for analyzing historical or criminal economic activities, or even professional sports, the instructional capital vs. individual capital vs. social capital distinction is essential.
Investment.
Those who differentiate individual capital tend to see it as something that one can invest in, directly, and see growth, directly. For individual skill, even skill at a highly imitative enterprise, like sports or mastery of a musical instrument, this is very often quite measurable. Many enterprises, for instance, a music conservatory or circus school or creative writing coach, are clearly making a living on the identification and (somewhat) measurable enhancement of the individual.

</doc>
<doc id="45804" url="http://en.wikipedia.org/wiki?curid=45804" title="Human capital">
Human capital

"For the 2013 film, see Human Capital (film)."
Human capital is the stock of knowledge, habits, social and personality attributes, including creativity, embodied in the ability to perform labor so as to produce economic value.
Alternatively, "Human capital" is a collection of resources—all the knowledge, talents, skills, abilities, experience, intelligence, training, judgment, and wisdom possessed individually and collectively by individuals in a population. These resources are the total capacity of the people that represents a form of wealth which can be directed to accomplish the goals of the nation or state or a portion thereof.
It is an aggregate economic view of the human being acting within economies, which is an attempt to capture the social, biological, cultural and psychological complexity as they interact in explicit and/or economic transactions. Many theories explicitly connect investment in human capital development to education, and the role of human capital in economic development, productivity growth, and innovation has frequently been cited as a justification for government subsidies for education and job skills training.
"Human capital" has been and continues to be criticized in numerous ways. Michael Spence offers signaling theory as an alternative to human capital. Pierre Bourdieu offers a nuanced conceptual alternative to human capital that includes cultural capital, social capital, economic capital, and symbolic capital. These critiques, and other debates, suggest that "human capital" is a reified concept without sufficient explanatory power.
It was assumed in early economic theories, reflecting the context, i.e., the secondary sector of the economy was producing much more than the tertiary sector was able to produce at the time in most countries – to be a fungible resource, homogeneous, and easily interchangeable, and it was referred to simply as workforce or labor, one of three factors of production (the others being land, and assumed-interchangeable assets of money and physical equipment). Just as land became recognized as natural capital and an asset in itself, and human factors of production were raised from this simple mechanistic analysis to human capital. In modern technical financial analysis, the term "balanced growth" refers to the goal of equal growth of both aggregate human capabilities and physical assets that produce goods and services.
The assumption that labour or workforces could be easily modelled in aggregate began to be challenged in 1950s when the tertiary sector, which demanded creativity, begun to produce more than the secondary sector was producing at the time in the most developed countries in the world. Accordingly much more attention was paid to factors that led to success versus failure where human management was concerned. The role of leadership, talent, even celebrity was explored.
Today, most theories attempt to break down human capital into one or more components for analysis  – usually called "intangibles". Most commonly, social capital, the sum of social bonds and relationships, has come to be recognized, along with many synonyms such as goodwill or brand value or social cohesion or social resilience and related concepts like celebrity or fame, as distinct from the talent that an individual (such as an athlete has uniquely) has developed that cannot be passed on to others regardless of effort, and those aspects that can be transferred or taught: instructional capital. Less commonly, some analyses conflate good instructions for health with health itself, or good knowledge management habits or systems with the instructions they compile and manage, or the "intellectual capital" of teams – a reflection of their social and instructional capacities, with some assumptions about their individual uniqueness in the context in which they work. In general these analyses acknowledge that individual trained bodies, teachable ideas or skills, and social influence or persuasion power, are different.
Management accounting is often concerned with questions of how to model human beings as a capital asset. However it is broken down or defined,
human capital is vitally important for an organization's success (Crook et al., 2011); human capital increases through education and experience. Human capital is also important for the success of cities and regions: a 2012 study examined how the production of university degrees and R&D activities of educational institutions are related to the human capital of metropolitan areas in which they are located.
In 2010, the OECD (the Organization of Economic Co-operation and Development) encouraged the governments of advanced economies to embrace policies to increase innovation and knowledge in products and services as an economical path to continued prosperity. International policies also often address human capital flight, which is the loss of talented or trained persons from a country that invested in them, to another country which benefits from their arrival without investing in them.
Studies of structural unemployment have increasingly focused on a mismatch between the stock of job-specific human capital and the needs of employers. In other words, there is increasingly a recognition that human capital may be specific to particular jobs or tasks and not general and readily transferable. Recent work has attempted to improve the linkages between education and the needs of the labor market by linking labor market data to education loan pricing.
Background.
Justin Slay defined four types of fixed capital (which is characterized as that which affords a revenue or profit without circulating or changing masters). The four types were:
Adam Smith defined human capital as follows:
“Fourthly, of the acquired and useful abilities of all the inhabitants or members of the society. The acquisition of such talents, by the maintenance of the acquirer during his education, study, or apprenticeship, always costs a real expense, which is a capital fixed and realized, as it were, in his person. Those talents, as they make a part of his fortune, so do they likewise that of the society to which he belongs. The improved dexterity of a workman may be considered in the same light as a machine or instrument of trade which facilitates and abridges labor, and which, though it costs a certain expense, repays that expense with a profit.”.
Therefore, Smith argued, the productive power of labor are both dependent on the division of labor:
The greatest improvement in the productive powers of labour, and the greater part of the skill, dexterity, and judgement with which it is any where directed, or applied, seem to have been the effects of the division of labour.
There is a complex relationship between the division of labor and human capital.
Etymology.
Arthur Lewis is said to have begun the field of Economic Development and consequently the idea of human capital when he wrote in 1954 the "Economic Development with Unlimited Supplies of Labour." The term "human capital" was not used due to its negative undertones until it was first discussed by Arthur Cecil Pigou: "There is such a thing as investment in human capital as well as investment in material capital. So soon as this is recognised, the distinction between economy in consumption and economy in investment becomes blurred. For, up to a point, consumption is investment in personal productive capacity. This is especially important in connection with children: to reduce unduly expenditure on their consumption may greatly lower their efficiency in after-life. Even for adults, after we have descended a certain distance along the scale of wealth, so that we are beyond the region of luxuries and "unnecessary" comforts, a check to personal consumption is also a check to investment.
The use of the term in the modern neoclassical economic literature dates back to Jacob Mincer's article "Investment in Human Capital and Personal Income Distribution" in the "Journal of Political Economy" in 1958. Then Theodore Schultz who is also contributed to the development of the subject matter. The best-known application of the idea of "human capital" in economics is that of Mincer and Gary Becker of the "Chicago School" of economics. Becker's book entitled "Human Capital", published in 1964, became a standard reference for many years. In this view, human capital is similar to "physical means of production", e.g., factories and machines: one can invest in human capital (via education, training, medical treatment) and one's outputs depend partly on the rate of return on the human capital one owns. Thus, human capital is a means of production, into which additional investment yields additional output. Human capital is substitutable, but not transferable like land, labor, or fixed capital.
Modern growth theory sees human capital as an important growth factor. Further research shows its relevance for democracy or AIDS.
Competence and capital.
The introduction is explained and justified by the unique characteristics of competence (often used only knowledge). Unlike physical labor (and the other factors of production), competence is:
Competence, ability, skills or knowledge?
Often the term "knowledge" is used. "Competence" is broader and includes cognitive ability ("intelligence") and further abilities like motoric and artistic abilities. "Skill" stands for narrow, domain-specific ability. The broader terms "competence" and "ability" are interchangeable.
Knowledge equity (= knowledge capital – knowledge liability) plus emotional equity (= emotional capital – emotional liability) equals goodwill or immaterial/intangible value of the company.
Intangible value of the company (goodwill) plus (material) equity equals the total value of the company.
Marxist analysis.
In some way, the idea of "human capital" is similar to Karl Marx's concept of labor power: he thought in capitalism workers sold their labor power in order to receive income (wages and salaries). But long before Mincer or Becker wrote, Marx pointed to "two disagreeably frustrating facts" with theories that equate wages or salaries with the interest on human capital.
An employer must be receiving a profit from his operations, so that workers must be producing what Marx (under the labor theory of value) perceived as surplus-value, i.e., doing work beyond that necessary to maintain their labor power. Though having "human capital" gives workers some benefits, they are still dependent on the owners of non-human wealth for their livelihood.
The term appears in Marx's article in the "New-York Daily Tribune" article "The Emancipation Question," January 17 and 22, 1859, although there the term is used to describe humans who act like a capital to the producers, rather than in the modern sense of "knowledge capital" endowed to or acquired by humans.
Neo-Marxist economists such as Bowles have argued that education does not lead to higher wages by increasing human capital, but rather by making workers more compliant and reliable in a corporate environment.
Importance.
The concept of Human capital has relatively more importance in labour-surplus countries. These countries are naturally endowed with more of labour due to high birth rate under the given climatic conditions. The surplus labour in these countries is the human resource available in more abundance than the tangible capital resource. This human resource can be transformed into Human capital with effective inputs of education, health and moral values. The transformation of raw human resource into highly productive human resource with these inputs is the process of human capital formation. The problem of scarcity of tangible capital in the labour surplus countries can be resolved by accelerating the rate of human capital formation with both private and public investment in education and health sectors of their National economies. The tangible financial capital is an effective instrument of promoting economic growth of the nation. The intangible human capital, on the other hand, is an instrument of promoting comprehensive development of the nation because human capital is directly related to human development, and when there is human development, the qualitative and quantitative progress of the nation is inevitable. This importance of human capital is explicit in the changed approach of United Nations towards comparative evaluation of economic development of different nations in the World economy. United Nations publishes Human Development Report on human development in different nations with the objective of evaluating the rate of human capital formation in these nations. The statistical indicator of estimating Human Development in each nation is Human Development Index (HDI). It is the combination of "Life Expectancy Index", "Education Index" and "Income Index". The Life expectancy index reveals the standard of health of the population in the country; education index reveals the educational standard and the literacy ratio of the population; and the income index reveals the standard of living of the population. If all these indices have the rising trend over a long period of time, it is reflected into rising trend in HDI. The Human Capital is developed by health, education and quality of Standard of living. Therefore, the components of HDI viz, Life Expectancy Index, Education Index and Income Index are directly related to Human Capital formation within the nation. HDI is indicator of positive correlation between human capital formation and economic development. If HDI increases, there is higher rate of human capital formation in response to higher standard of education and health. Similarly, if HDI increases, per capita income of the nation also increases. Implicitly, HDI reveals that higher the human capital formation due to good standard of health and education, higher is the per capita income of the nation. This process of human development is the strong foundation of a continuous process of economic development of the nation for a long period of time. This significance of the concept of Human capital in generating long-term economic development of the nation cannot be neglected. It is expected that the Macroeconomic policies of all the nations are focussed towards promotion of human development and subsequently economic development. Human Capital is the backbone of Human Development and economic development in every nation. Mahroum (2007) suggested that at the macro-level, human capital management is about three key capacities, the capacity to develop talent, the capacity to deploy talent, and the capacity to draw talent from elsewhere. Collectively, these three capacities form the backbone of any country's human capital competitiveness. Recent U.S. research shows that geographic regions that invest in the human capital and economic advancement of immigrants who are already living in their jurisdictions help boost their short- and long-term economic growth. There is also strong evidence that organizations that possess and cultivate their human capital outperform other organizations lacking human capital (Crook, Todd, Combs, Woehr, and Ketchen, 2011).
Cumulative growth.
Human capital is distinctly different from the tangible monetary capital due to the extraordinary characteristic of human capital to grow cumulatively over a long period of time. The growth of tangible monetary capital is not always linear due to the shocks of business cycles. During the period of prosperity, monetary capital grows at relatively higher rate while during the period of recession and depression, there is deceleration of monetary capital. On the other hand, human capital has uniformly rising rate of growth over a long period of time because the foundation of this human capital is laid down by the educational and health inputs. The current generation is qualitatively developed by the effective inputs of education and health. The future generation is more benefited by the advanced research in the field of education and health, undertaken by the current generation. Therefore, the educational and health inputs create more productive impacts upon the future generation and the future generation becomes superior to the current generation. In other words, the productive capacity of future generation increases more than that of current generation. Therefore, rate of human capital formation in the future generation happens to be more than the rate of human capital formation in the current generation. This is the cumulative growth of human capital formation generated by superior quality of manpower in the succeeding generation as compared to the preceding generation.
India.
In India, rate of human capital formation has consistently increased after Independence due to qualitative improvement in each generation. In the second decade of 21st century, the third generation of India's population is active in the workforce of India. This third generation is qualitatively most superior human resource in India. It has developed the service sector of India with the export of financial services, software services, tourism services and improved the Invisible balance of India's Balance of payments. The rapid growth of Indian economy in response to improvement in the service sector is an evidence of cumulative growth of Human Capital in India.
Criticism.
Some labor economists have criticized the Chicago-school theory, claiming that it tries to explain all differences in wages and salaries in terms of human capital. One of the leading alternatives, advanced by Michael Spence and Joseph Stiglitz, is "Signaling theory". According to signaling theory, education does not lead to increased human capital, but rather acts as a mechanism by which workers with superior innate abilities can signal those abilities to prospective employers and so gain above average wages.
The concept of human capital can be infinitely elastic, including unmeasurable variables such as personal character or connections with insiders (via family or fraternity). This theory has had a significant share of study in the field proving that wages can be higher for employees on aspects other than human capital. Some variables that have been identified in the literature of the past few decades include, gender and nativity wage differentials, discrimination in the work place, and socioeconomic status. 
The prestige of a credential may be as important as the knowledge gained in determining the value of an education. This points to the existence of market imperfections such as non-competing groups and labor-market segmentation. In segmented labor markets, the "return on human capital" differs between comparably skilled labor-market groups or segments. An example of this is discrimination against minority or female employees.
Following Becker, the human capital literature often distinguishes between "specific" and "general" human capital. Specific human capital refers to skills or knowledge that is useful only to a single employer or industry, whereas general human capital (such as literacy) is useful to all employers. Economists view firm specific human capital as risky, since firm closure or industry decline lead to skills that cannot be transferred (the evidence on the quantitative importance of firm specific capital is unresolved).
Human capital is central to debates about welfare, education, health care, and retirement..
In 2004, "human capital" (German: "Humankapital") was named the German Un-Word of the Year by a jury of linguistic scholars, who considered the term inappropriate and inhumane, as individuals would be degraded and their abilities classified according to economically relevant quantities.
"Human capital" is often confused with human development. The UN suggests "Human development denotes both the process of widening people's choices and improving their well-being". The UN Human Development indices suggest that human capital is merely a means to the end of human development: "Theories of human capital formation and human resource development view human beings as means to increased income and wealth rather than as ends. These theories are concerned with human beings as inputs to increasing production".
Mobility between nations.
Educated individuals often migrate from poor countries to rich countries seeking opportunity. This movement has positive effects for both countries: capital-rich countries gain an influx in labor, and labor rich countries receive capital when migrants remit money home. The loss of labor in the old country also increases the wage rate for those who do not emigrate, while the additional labor lowers wages in the new country. When workers migrate, their early care and education generally benefit the country where they move to work. And, when they have health problems or retire, their care and retirement pension will typically be paid in the new country.
African nations have invoked this argument with respect to slavery, other colonized peoples have invoked it with respect to the "brain drain" or "human capital flight" which occurs when the most talented individuals (those with the most individual capital) depart for education or opportunity to the colonizing country (historically, Britain and France and the U.S.). Even in Canada and other developed nations, the loss of human capital is considered a problem that can only be offset by further draws on the human capital of poorer nations via immigration. The economic impact of immigration to Canada is generally considered to be positive.
During the late 19th and early 20th centuries, human capital in the United States became considerably more valuable as the need for skilled labor came with newfound technological advancement. The 20th century is often revered as the "human capital century" by scholars such as Claudia Goldin. During this period a new mass movement toward secondary education paved the way for a transition to mass higher education. New techniques and processes required further education than the norm of primary schooling, which thus led to the creation of more formalized schooling across the nation. These advances produced a need for more skilled labor, which caused the wages of occupations that required more education to considerably diverge from the wages of ones that required less. This divergence created incentives for individuals to postpone entering the labor market in order to obtain more education. The “high school movement” had changed the educational system for youth in America. With minor state involvements, the high school movement started at the grass-roots level, particularly the communities with the most homogeneous populations. As a year in high school added more than ten percent to an individual’s income, post-elementary school enrollment and graduation rates increased significantly during the 20th century. The U.S. system of education was characterized for much of the 20th century by publicly funded mass secondary education that was open and forgiving, academic yet practical, secular, gender neutral, and funded by small, fiscally independent districts. This early insight into the need for education allowed for a significant jump in US productivity and economic prosperity, when compared to other world leaders at the time. It is suggested by several economists, that there is a positive correlation between high school enrollment rates and GDP per capita. Less developed countries have not established a set of institutions favoring equality and role of education for the masses and therefore have been incapable of investing in human capital stock necessary for technological growth.
The rights and freedom of individuals to travel and opportunity, despite some historical exceptions such as the Soviet bloc and its "Iron Curtain", seem to consistently transcend the countries in which they are educated. One must also remember that the ability to have mobility with regards to where people want to move and work is a part of their human capital. Being able to move from one area to the next is an ability and a benefit of having human capital. To restrict people from doing so would be to inherently lower their human capital.
This debate resembles, in form, that regarding natural capital.
Intangibility and portability.
Human capital is an intangible asset – it is not owned by the firm that employs it and is generally not fungible. Specifically, individuals arrive at 9am and leave at 5pm (in the conventional office model) taking most of their knowledge and relationships with them.
Human capital when viewed from a time perspective consumes time in one of key activities:
Despite the lack of formal ownership, firms can and do gain from high levels of training, in part because it creates a corporate culture or vocabulary teams use to create cohesion.
In recent economic writings the concept of firm-specific human capital, which includes those social relationships, individual instincts, and instructional details that are of value within one firm (but not in general), appears by way of explaining some labour mobility issues and such phenomena as golden handcuffs. Workers can be more valuable where they are simply for having acquired this knowledge, these skills and these instincts. Accordingly the firm gains for their unwillingness to leave and market talents elsewhere.
Risk.
When human capital is assessed by activity based costing via time allocations it becomes possible to assess human capital risk. Human capital risks can be identified if HR processes in organizations are studied in detail. Human capital risk occurs when the organization operates below attainable operational excellence levels. For example, if a firm could reasonably reduce errors and rework (the Process component of human capital) from 10,000 hours per annum to 2,000 hours with attainable technology, the difference of 8,000 hours is human capital risk. When wage costs are applied to this difference (the 8,000 hours) it becomes possible to financially value human capital risk within an organizational perspective.
Risk accumulates in four primary categories:
Corporate finance.
In Corporate finance, human capital is one of the three primary components of Intellectual Capital (which in addition to tangible assets comprise the entire value of a company). Human Capital is the value that the employees of a business provide through the application of skills, know-how and expertise. It is an organization’s combined human capability for solving business problems. Human Capital is inherent in people and cannot be owned by an organization. Therefore, Human Capital leaves an organization when people leave. Human Capital also encompasses how effectively an organization uses its people resources as measured by creativity and innovation. A company’s reputation as an employer affects the Human Capital it draws.

</doc>
<doc id="45805" url="http://en.wikipedia.org/wiki?curid=45805" title="Instructional capital">
Instructional capital

Instructional capital is a term used in educational administration after the 1960s, to reflect capital resulting from investment in producing learning materials.
Some have objected to this phrasing, which is an elaboration of referring to training as "human capital", either for the same reason that phrase is objectionable, or on the grounds that it implies that the human in which the knowledge is "invested" is a resource to be exploited.
Instructional capital can be used to guide or limit or restrict action by people (individual capital) or equipment (infrastructural capital) (if the learning materials are computer programs). It cannot generally make either individuals or infrastructure do what they are not trained or designed to do, but it can help prevent them from doing most stupid, destructive and dangerous things.
When people begin to trust instructions, they tend to associate social capital with them, as symbolized by a brand, flag or label. This usually opens up a possibility for those with power to start cheating and/or creating bad instructions that can no longer be trusted, but the good reputation of the brand, flag or label protects them from being caught for longer than would be the case without the symbol that is associated with good reputation.

</doc>
<doc id="45806" url="http://en.wikipedia.org/wiki?curid=45806" title="Garuda">
Garuda

The Garuda is a large bird-like creature, or humanoid bird that appears in both Hinduism and Buddhism. Garuda is the mount ("vahana") of the Lord Vishnu. Garuda is the Hindu name for the constellation Aquila. The brahminy kite and phoenix are considered to be the contemporary representations of garuda. Indonesia adopts a more stylistic approach to the Garuda's depiction as its national symbol, where it depicts a Javanese eagle (being much larger than a kite).
About Garuda.
In Hinduism, Garuda is a Hindu divinity, usually the mount ("vahana") of the Lord Vishnu. Garuda is depicted as having the golden body of a strong man with a white face, red wings, and an eagle's beak and with a crown on his head. This ancient deity was said to be massive, large enough to block out the sun.
Garuda is known as the eternal sworn enemy of the Nāga serpent race and known for feeding exclusively on snakes, such behavior may have referred to the actual short-toed eagle of India. The image of Garuda is often used as the charm or amulet to protect the bearer from snake attack and its poison, since the king of birds is an implacable enemy and "devourer of serpent". Garudi Vidya is the mantra against snake poison to remove all kinds of evil.
His stature in Hindu religion can be gauged by the fact that a dependent Upanishad, the Garudopanishad, and a Purana, the Garuda Purana, is devoted to him. Various names have been attributed to Garuda - Chirada, Gaganeshvara, Kamayusha, Kashyapi, Khageshvara, Nagantaka, Sitanana, Sudhahara, Suparna, Tarkshya, Vainateya, Vishnuratha and others. The Vedas provide the earliest reference of Garuda, though by the name of Śyena, where this mighty bird is said to have brought nectar to earth from heaven. The Puranas, which came into existence much later, mention Garuda as doing the same thing, which indicates that Śyena (Sanskrit for eagle) and Garuda are the same. One of the faces of Śrī Pañcamukha Hanuman is Mahavira Garuda. This face points towards the west. Worship of Garuda is believed to remove the effects of poisons from one's body. In Tamil Vaishnavism Garuda and Hanuman are known as "Periya Thiruvadi" and "Siriya Thiruvadi" respectively.
In the Bhagavad-Gita (Ch.10, Verse 30), in the middle of the battlefield "Kurukshetra", Krishna explaining his omnipresence, says - " as son of Vinata, I am in the form of Garuda, the king of the bird community (Garuda)" indicating the importance of Garuda.
Garuda wears the serpent Adisesha on his left small toenail and the serpent Gulika on his right cerebral cortex. The serpent Vasuki forms his sacred thread. The cobra Takshaka forms his belt on his hip. The snake Karkotaka is worn as his necklace. The snakes Padma and Mahapadma are his ear rings. The snake Shankachuda adorns his divine hair. He is flanked by his two wives ‘Rudra’ and ‘Sukeerthi’ or (Sukirthi). These are all invoked in Vedanta Desika's Garuda Panchashath and Garuda Dandaka compositions. Garuda flanked with his consorts 'Rudra' and 'Sukirthi' can be seen worshipped in an ancient Soumya Keshava temple in Bindiganavile (or Mayura puri in Sanskrit ) in Karnataka state of India.
Garuda Vyuha is worshiped in Tantra for Abhichara and to protect against Abhichara. However, the interesting thing is that Garuda is the Sankarshna form of the lord who during creation primarily possesses the knowledge aspect of the lord (among Vasudeva, Sankarshana, Pradyumna and Aniruddha forms). The important point is that Garuda represents the five vayus within us : prana, apana, vyana, udana, samana through his five forms Satya, Suparna, Garuda, Tarkshya, Vihageshwara. These five vayus through yoga can be controlled through Pranayama which can lead to Kundalini awakening leading to higher levels of consciousness.
Garuda plays an important role in Krishna Avatar in which Krishna and Satyabhama ride on Garuda to kill Narakasura. On another occasion, Lord Hari rides on Garuda to save the devotee elephant Gajendra. It is also said that Garuda's wings when flying will chant the Vedas.
With the position of Garuda's hands and palms, he is also called 'Kai Yendhi Perumal', in Tamil.
In the Mahabharata.
Birth and deeds.
The story of Garuda's birth and deeds is told in the first book of the great epic Mahabharata. According to the epic, when Garuda first burst forth from his egg, he appeared as a raging inferno equal to the cosmic conflagration that consumes the world at the end of every age. Frightened, the gods begged him for mercy. Garuda, hearing their plea, reduced himself in size and energy.
Garuda's father was the creator-rishi Kasyapa. He had two wives, Vinata and Kadru, who were daughters of Prajapathi Daksha. Kasyapa, on the pleadings of his wives, granted them their wishes; Vinata wished for two sons and Kadru wished for thousand snakes as her sons. Both laid eggs, while the thousand eggs of Kadru hatched early (after steaming the eggs to hatch) into snakes, the hatching of two eggs of Vinata did not take place for a long time. Impatient, Vinata broke open one egg, which was half formed with the upper half only as a human and was thus deformed. Her half formed son cursed her that she would be slave for her sister (she was her rival) for a long time by which time her second son would be born who would save her from his curse; her first son who flew away and came to prominence as Aruna, the red spectacle seen as the Sun rises in the morning, and as also charioteer of the Sun. The second egg hatched after a long time during which period Vinata was the servant of her sister as she had lost a bet with her. When the second egg hatched, a fully grown, shining and of mighty sized bird form emerged as Garuda, the king of birds. Garuda was thus born.
One day, Vinata entered into and lost a foolish bet, as a result of which she became enslaved to her sister. Resolving to release his mother from this state of bondage, Garuda approached the serpents and asked them what it would take to purchase her freedom. Their reply was that Garuda would have to bring them the elixir of immortality, also called amrita. It was a tall order. The amrita at that time found itself in the possession of the gods, who guarded it zealously, since it was the source of their immortality. They had ringed the elixir with a massive fire that covered the sky. They had blocked the way to the elixir with a fierce mechanical contraption of sharp rotating blades. And finally, they had stationed two gigantic poisonous snakes next to the elixir as deadly guardians.
Undaunted, Garuda hastened toward the abode of the gods intent on robbing them of their treasure. Knowing of his design, the gods met him in full battle-array. Garuda, however, defeated the entire host and scattered them in all directions. Taking the water of many rivers into his mouth, he extinguished the protective fire the gods had thrown up. Reducing his size, he crept past the rotating blades of their murderous machine. And finally, he mangled the two gigantic serpents they had posted as guards. Taking the elixir into his mouth without swallowing it, he launched again into the air and headed toward the eagerly waiting serpents. En route, he encountered Vishnu. Rather than fight, the two exchanged promises. Vishnu promised Garuda the gift of immortality even without drinking from the elixir, and Garuda promised to become Vishnu's mount. Flying onward, he met Indra the god of the sky. Another exchange of promises occurred. Garuda promised that once he had delivered the elixir, thus fulfilling the request of the serpents, he would make it possible for Indra to regain possession of the elixir and to take it back to the gods. Indra in turn promised Garuda the serpents as food.
At long last, Garuda alighted in front of the waiting serpents. Placing the elixir on the grass, and thereby liberating his mother Vinata from her servitude, he urged the serpents to perform their religious ablutions before consuming it. As they hurried off to do so, Indra swooped in to make off with the elixir. The serpents came back from their ablutions and saw the elixir gone but with small droplets of it on the grass. They tried to lick the droplets and thereby split their tongues in two. From then onwards, serpents have split tongues and shed their skin as a kind of immortality. From that day onward, Garuda was the ally of the gods and the trusty mount of Vishnu, as well as the implacable enemy of snakes, upon whom he preyed at every opportunity.
Descendants.
According to the Mahabharata, Garuda had six sons (Sumukha, Suvarna, Subala, Sunaama, Sunethra and Suvarchas) from whom were descended the race of birds. The members of this race were of great might and without compassion, subsisting as they did on their relatives the snakes. Vishnu was their protector.
As a symbol.
Throughout the Mahabharata, Garuda is invoked as a symbol of impetuous violent force, of speed, and of martial prowess. Powerful warriors advancing rapidly on doomed foes are likened to Garuda swooping down on a serpent. Defeated warriors are like snakes beaten down by Garuda. The field marshal Drona uses a military formation named after Garuda. Krishna even carries the image of Garuda on his banner.
In Buddhism.
In Buddhist mythology, the Garuda (Pāli: garuḷā) are enormous predatory birds with intelligence and social organization. Another name for the Garuda is suparṇa (Pāli: supaṇṇa), meaning "well-winged, having good wings". Like the Naga, they combine the characteristics of animals and divine beings, and may be considered to be among the lowest devas.
The exact size of the Garuda is uncertain, but its wings are said to have a span of many miles. This may be a poetic exaggeration, but it is also said that when a Garuda's wings flap, they create hurricane-like winds that darken the sky and blow down houses. A human being is so small compared to a Garuda that a man can hide in the plumage of one without being noticed (Kākātī Jātaka, J.327). They are also capable of tearing up entire banyan trees from their roots and carrying them off.
Garudas are the great golden-winged Peng birds. They also have the ability to grow large or small, and to appear and disappear at will. Their wingspan is 330 yojanas (one yojana being 8 miles long). With one flap of its wings, a Peng bird dries up the waters of the sea so that it can gobble up all the exposed dragons. With another flap of its wings, it can level the mountains by moving them into the ocean.
There were also the four garuda-kings : Great-Power-Virtue Garuda-King, Great-Body Garuda-King, Great-Fulfillment Garuda-King, and Free-At-Will Garuda-King, each accompanied by hundreds of thousands of attendants.
The Garudas have kings and cities, and at least some of them have the magical power of changing into human form when they wish to have dealings with people. On some occasions Garuda kings have had romances with human women in this form. Their dwellings are in groves of the "simbalī", or silk-cotton tree.
The Garuda are enemies to the nāga, a race of intelligent serpent- or dragon-like beings, whom they hunt. The Garudas at one time caught the nāgas by seizing them by their heads; but the nāgas learned that by swallowing large stones, they could make themselves too heavy to be carried by the Garudas, wearing them out and killing them from exhaustion. This secret was divulged to one of the Garudas by the ascetic Karambiya, who taught him how to seize a nāga by the tail and force him to vomit up his stone (Pandara Jātaka, J.518).
The Garudas were among the beings appointed by Śakra to guard Mount Sumeru and the Trāyastriṃśa heaven from the attacks of the asuras.
In the Maha-samaya Sutta (Digha Nikaya 20), the Buddha is shown making temporary peace between the Nagas and the Garudas.
The Thai rendering of Garuda (ครุฑ "Krut") as Vishnu vehicle and Garuda's quest for elixir was based on Indian legend of Garuda. It was told that Garuda overcame many heavenly beings in order to gain the ambrosia (amrita) elixir. No one was able to get the better of him, not even Narai (Vishnu). At last, a truce was called and an agreement was made to settle the rancor and smooth all the ruffled feathers. It was agreed that when Narai is in his heavenly palace, Garuda will be positioned in a superior status, atop the pillar above Narai's residence. However, whenever Narai wants to travel anywhere, Garuda must serve as his transport.
The Sanskrit word Garuda has been borrowed and modified in the languages of several countries. In Burmese, Garudas are called "galone" (ဂဠုန်). In Burmese astrology, the vehicle of the Sunday planet is the "galone". In the Kapampangan language of the Philippines, the native word for eagle is "galura". In Japanese a Garuda is called karura (however, the form "Garuda" ガルーダ is used in recent Japanese fiction - see below).
For the Mongols, the Garuda is called Khan Garuda or "Khangarid" (Mongolian: Хангарьд). Before and after each round of Mongolian wrestling, wrestlers perform the Garuda ritual, a stylised imitation of the "Khangarid" and a hawk.
In the Qing Dynasty fiction "The Story of Yue Fei" (1684), Garuda sits at the head of the Buddha's throne. But when a celestial bat (an embodiment of the Aquarius constellation) flatulates during the Buddha’s expounding of the Lotus Sutra, Garuda kills her and is exiled from paradise. He is later reborn as Song Dynasty General Yue Fei. The bat is reborn as Lady Wang, wife of the traitor Prime Minister Qin Hui, and is instrumental in formulating the "Eastern Window" plot that leads to Yue's eventual political execution. It is interesting to note "The Story of Yue Fei" plays on the legendary animosity between Garuda and the Nagas when the celestial bird-born Yue Fei defeats a magic serpent who transforms into the unearthly spear he uses throughout his military career. Literary critic C. T. Hsia explains the reason why Qian Cai, the book's author, linked Yue with Garuda is because of the homology in their Chinese names. Yue Fei's courtesy name is Pengju (鵬舉). A Peng (鵬) is a giant mythological bird likened to the Middle Eastern Roc. Garuda's Chinese name is Great Peng, the Golden-Winged Illumination King (大鵬金翅明王).
As a cultural and national symbol.
In India, Indonesia and the rest of Southeast Asia the eagle symbolism is represented by Garuda, a large mythical bird with eagle-like features that appears in both Hindu and Buddhist mythology as the vahana (vehicle) of the god Vishnu. Garuda became the national emblem of Thailand and Indonesia; Thailand's Garuda is rendered in a more traditional anthropomorphic mythical style, while that of Indonesia is rendered in heraldic style with traits similar to the real Javan hawk-eagle.
India.
India primarily uses Garuda as a martial motif:
Indonesia.
Indonesia uses the Garuda, called the Garuda Pancasila, as its national symbol, it is somewhat intertwined with the concept of the phoenix.
Thailand.
Thailand uses the Garuda (Thai: ครุฑ, khrut) as its national symbol.

</doc>
<doc id="45807" url="http://en.wikipedia.org/wiki?curid=45807" title="Financial capital">
Financial capital

Financial capital is any economic resource measured in terms of money used by entrepreneurs and businesses to buy what they need to make their products or to provide their services to the sector of the economy upon which their operation is based, i.e. retail, corporate, investment banking, etc.
Three concepts of capital maintenance authorized in IFRS.
Financial capital or just capital/equity in finance, accounting and economics, is internal retained earnings generated by the entity or funds provided by lenders (and investors) to businesses to purchase real capital equipment or services for producing new goods/services. Real capital or economic capital comprises physical goods that assist in the production of other goods and services, e.g. shovels for gravediggers, sewing machines for tailors, or machinery and tooling for factories.
Financial capital generally refers to saved-up financial wealth, especially that used to start or maintain a business. A financial concept of capital is adopted by most entities in preparing their financial reports. Under a financial concept of capital, such as invested money or invested purchasing power, capital is synonymous with the net assets or equity of the entity. Under a physical concept of capital, such as operating capability, capital is regarded as the productive capacity of the entity based on, for example, units of output per day. Financial capital maintenance can be measured in either nominal monetary units or units of constant purchasing power. There are thus three concepts of capital maintenance in terms of International Financial Reporting Standards (IFRS): (1) Physical capital maintenance (2) Financial capital maintenance in nominal monetary units (3) Financial capital maintenance in units of constant purchasing power. Framework for the Preparation and Presentation of Financial Statements,
Financial capital is provided by lenders for a price: interest. Also see time value of money for a more detailed description of how financial capital may be analyzed.
Furthermore, financial capital, is any liquid medium or mechanism that represents wealth, or other styles of capital. It is, however, usually purchasing power in the form of money available for the production or purchasing of goods, etcetera. Capital can also be obtained by producing more than what is immediately required and saving the surplus.
Financial capital can also be in the form of purchasable items such as computers or books that can contribute directly or indirectly to obtaining various other types of capital.
Financial capital has been subcategorized by some academics as economic or "productive capital" necessary for operations, signaling capital which signals a company's financial strength to shareholders, and regulatory capital which fulfills capital requirements.
Fixed capital.
This is money which is used to purchase assets that will remain permanently in the business and help it to make a profit
Working capital.
Working capital is that part of capital invested which is used for running the business such like money which is used to buy stock, pay expenses and finance credit.
Factors determining working capital requirements.
business policies
Instruments.
A contract regarding any combination of capital assets is called a financial instrument, and may serve as a 
Most indigenous forms of money (wampum, shells, tally sticks and such) and the modern fiat money is only a "symbolic" storage of value and not a real storage of value like commodity money.
Own and borrowed capital.
Capital contributed by the owner or entrepreneur of a business, and obtained, for example, by means of savings or inheritance, is known as own capital or equity, whereas that which is granted by another person or institution is called borrowed capital, and this must usually be paid back with interest. The ratio between debt and equity is named leverage. It has to be optimized as a high leverage can bring a higher profit but create solvency risk.
Borrowed capital.
This is capital which the business borrows from institutions or people, and includes debentures:
Own capital.
This is capital that owners of a business (shareholders and partners, for example) provide:
These have preference over the equity shares. This means the payments made to the shareholders are first paid to the preference shareholder(s) and then to the equity shareholders.
Issuing and trading.
Like money, financial instruments may be "backed" by state military fiat, credit (i.e. social capital held by banks and their depositors), or commodity resources. Governments generally closely control the supply of it and usually require some "reserve" be held by institutions granting credit. Trading between various national currency instruments is conducted on a money market. Such trading reveals differences in probability of debt collection or store of value function of that currency, as assigned by traders.
When in forms other than money, financial capital may be traded on bond markets or reinsurance markets with varying degrees of trust in the social capital (not just credits) of bond-issuers, insurers, and others who issue and trade in financial instruments. When payment is deferred on any such instrument, typically an interest rate is higher than the standard interest rates paid by banks, or charged by the central bank on its money. Often such instruments are called fixed-income instruments if they have reliable payment schedules associated with the uniform rate of interest. A variable-rate instrument, such as many consumer mortgages, will reflect the standard rate for deferred payment set by the central bank prime rate, increasing it by some fixed percentage. Other instruments, such as citizen entitlements, e.g. "U.S. Social Security", or other pensions, may be indexed to the rate of inflation, to provide a reliable value stream.
Trading in stock markets or commodity markets is actually trade in underlying assets which are not wholly financial in themselves, although they often move up and down in value in direct response to the trading in more purely financial derivatives. Typically commodity markets depend on politics that affect international trade, e.g. boycotts and embargoes, or factors that influence natural capital, e.g. weather that affects food crops. Meanwhile, stock markets are more influenced by trust in corporate leaders, i.e. individual capital, by consumers, i.e. social capital or "brand capital" (in some analyses), and internal organizational efficiency, i.e. instructional capital and infrastructural capital. Some enterprises issue instruments to specifically track one limited division or brand. "Financial futures", "Short selling" and "financial options" apply to these markets, and are typically pure financial bets on outcomes, rather than being a direct representation of any underlying asset.
Broadening the notion.
The relationship between financial capital, money, and all other styles of capital, especially human capital or labor, is assumed in central bank policy and regulations regarding instruments as above.
Such relationships and policies are characterized by a political economy - feudalist, socialist, capitalist, green, anarchist or otherwise. In effect, the means of money supply and other regulations on financial capital represent the economic sense of the value system of the society itself, as they determine the allocation of labor in that society.
So, for instance, rules for increasing or reducing the money supply based on perceived inflation, or on measuring well-being, reflect some such values, reflect the importance of using (all forms of) financial capital as a stable store of value. If this is very important, inflation control is key - any amount of money inflation reduces the value of financial capital with respect to all other types.
If, however, the medium of exchange function is more critical, new money may be more freely issued regardless of impact on either inflation or well-being.
Marxian perspectives.
It is common in Marxian theory to refer to the role of "Finance Capital" as the determining and ruling class interest in capitalist society, particularly in the latter stages.
Valuation.
Normally, a financial instrument is priced accordingly to the perception by capital market players of its expected return and risk.
Unit of account functions may come into question if valuations of complex financial instruments vary drastically based on timing. The "book value", "mark-to-market" and "mark-to-future" conventions are three different approaches to reconciling financial capital value units of account.
Economic role.
Socialism, capitalism, feudalism, anarchism, other civic theories take markedly different views of the role of financial capital in social life, and propose various political restrictions to deal with that.
Finance capitalism is the production of profit from the manipulation of financial capital. It is held in contrast to industrial capitalism, where profit is made from the manufacture of goods.

</doc>
<doc id="45809" url="http://en.wikipedia.org/wiki?curid=45809" title="Dijkstra's algorithm">
Dijkstra's algorithm

Dijkstra's algorithm is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later. The algorithm exists in many variants; Dijkstra's original variant found the shortest path between two nodes, but a more common variant fixes a single node as the "source" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest path tree.
For a given source node in the graph, the algorithm finds the shortest path between that node and every other.:196–206 It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road, Dijkstra's algorithm can be used to find the shortest route between one city and all other cities. As a result, the shortest path algorithm is widely used in network routing protocols, most notably IS-IS and Open Shortest Path First (OSPF). It is also employed as a subroutine in other algorithms such as Johnson's.
Dijkstra's original algorithm does not use a min-priority queue and runs in time formula_1 (where formula_2 is the number of nodes). The idea of this algorithm is also given in . The implementation based on a min-priority queue implemented by a Fibonacci heap and running in formula_3 (where formula_4 is the number of edges) is due to .
This is asymptotically the fastest known single-source shortest-path algorithm for arbitrary directed graphs with unbounded non-negative weights.
In some fields, artificial intelligence in particular, Dijkstra's algorithm or a variant of it is known as uniform-cost search and formulated as an instance of the more general idea of best-first search.
History.
Dijkstra thought about the shortest path problem when working at the Mathematical Center in Amsterdam in 1956 as a programmer to demonstrate capabilities of a new computer called ARMAC. His objective was to choose both a problem as well as an answer (that would be produced by computer) that non-computing people could understand. He designed the shortest path algorithm in about 20 minutes without aid of paper and pen and later implemented it for ARMAC for a slightly simplified transportation map of 64 cities in Netherland (ARMAC was a 6-bit computer and hence could hold 64 cities comfortably). A year later, he came across another problem from hardware engineers working on the institute's next computer: minimize the amount of wire needed to connect the pins on the back panel of the machine. As a solution, he re-discovered the algorithm known as Prim's minimal spanning tree algorithm (known earlier to Jarník, and also rediscovered by Prim). Dijkstra published the algorithm in 1959, two years after Prim and 29 years after Jarník.
Algorithm.
Let the node at which we are starting be called the initial node. Let the distance of node "Y" be the distance from the initial node to "Y". Dijkstra's algorithm will assign some initial distance values and will try to improve them step by step.
Description.
Suppose you would like to find the shortest path between two intersections on a city map, a starting point and a destination. The order is conceptually simple: to start, mark the distance to every intersection on the map with infinity. This is done not to imply there is an infinite distance, but to note that intersection has not yet been "visited"; some variants of this method simply leave the intersection unlabeled. Now, at each iteration, select a "current" intersection. For the first iteration, the current intersection will be the starting point and the distance to it (the intersection's label) will be zero. For subsequent iterations (after the first), the current intersection will be the closest unvisited intersection to the starting point—this will be easy to find.
From the current intersection, update the distance to every unvisited intersection that is directly connected to it. This is done by determining the sum of the distance between an unvisited intersection and the value of the current intersection, and relabeling the unvisited intersection with this value if it is less than its current value. In effect, the intersection is relabeled if the path to it through the current intersection is shorter than the previously known paths. To facilitate shortest path identification, in pencil, mark the road with an arrow pointing to the relabeled intersection if you label/relabel it, and erase all others pointing to it. After you have updated the distances to each neighboring intersection, mark the current intersection as "visited" and select the unvisited intersection with lowest distance (from the starting point) – or the lowest label—as the current intersection. Nodes marked as visited are labeled with the shortest path from the starting point to it and will not be revisited or returned to.
Continue this process of updating the neighboring intersections with the shortest distances, then marking the current intersection as visited and moving onto the closest unvisited intersection until you have marked the destination as visited. Once you have marked the destination as visited (as is the case with any visited intersection) you have determined the shortest path to it, from the starting point, and can trace your way back, following the arrows in reverse.
This algorithm makes no attempt to direct "exploration" towards the destination as one might expect. Rather, the sole consideration in determining the next "current" intersection is its distance from the starting point. This algorithm therefore expands outward from the starting point, interactively considering every node that is closer in terms of shortest path distance until it reaches the destination. When understood in this way, it is clear how the algorithm necessarily finds the shortest path. However, it may also reveal one of the algorithm's weaknesses: its relative slowness in some topologies.
Pseudocode.
In the following algorithm, the code u ← vertex in "Q" with min dist[u], searches for the vertex u in the vertex set Q that has the least dist[u] value. length(u, v) returns the length of the edge joining (i.e. the distance between) the two neighbor-nodes u and v. The variable alt on line 19 is the length of the path from the root node to the neighbor node v if it were to go through u. If this path is shorter than the current shortest path recorded for v, that current path is replaced with this alt path. The prev array is populated with a pointer to the "next-hop" node on the source graph to get the shortest route to the source.
 1 function Dijkstra("Graph", "source"):
 2
 3 dist["source"] ← 0 "// Distance from source to source"
 4 prev["source"] ← undefined "// Previous node in optimal path initialization
 5
 6 for each vertex "v" in "Graph": "// Initialization"
 7 if "v" ≠ "source" "// Where v has not yet been removed from Q (unvisited nodes)"
 8 dist["v"] ← infinity "// Unknown distance function from source to v
 9 prev["v"] ← undefined "// Previous node in optimal path from source
 10 end if 
 11 add "v" to "Q" "// All nodes initially in Q (unvisited nodes)
 12 end for
 13 
 14 while "Q" is not empty:
 15 "u" ← vertex in "Q" with min dist[u] "// Source node in first case"
 16 remove "u" from "Q" 
 17 
 18 for each neighbor "v" of "u": "// where v is still in Q."
 19 "alt" ← dist["u"] + length("u", "v")
 20 if "alt" < dist["v"]: "// A shorter path to v has been found
 21 dist["v"] ← "alt" 
 22 prev["v"] ← "u" 
 23 end if
 24 end for
 25 end while
 26
 27 return dist[], prev[]
 28
 29 end function
If we are only interested in a shortest path between vertices source and target, we can terminate the search after line 15 if .
Now we can read the shortest path from source to target by reverse iteration:
 1 "S" ← empty sequence
 2 "u" ← "target"
 3 while prev["u"] is defined: "// Construct the shortest path with a stack S
 4 insert "u" at the beginning of "S" "// Push the vertex onto the stack
 5 "u" ← prev["u"] "// Traverse from target to source
 6 end while
Now sequence S is the list of vertices constituting one of the shortest paths from source to target, or the empty sequence if no path exists.
A more general problem would be to find all the shortest paths between source and target (there might be several different ones of the same length). Then instead of storing only a single node in each entry of prev[] we would store all nodes satisfying the relaxation condition. For example, if both r and source connect to target and both of them lie on different shortest paths through target (because the edge cost is the same in both cases), then we would add both r and source to prev[target]. When the algorithm completes, prev[] data structure will actually describe a graph that is a subset of the original graph with some edges removed. Its key property will be that if the algorithm was run with some starting node, then every path from that node to any other node in the new graph will be the shortest path between those nodes in the original graph, and all paths of that length from the original graph will be present in the new graph. Then to actually find all these shortest paths between two given nodes we would use a path finding algorithm on the new graph, such as depth-first search.
Using a priority queue.
A min-priority queue is an abstract data structure that provides 3 basic operations : add_with_priority(), decrease_priority() and extract_min(). As mentioned earlier, using such a data structure can lead to faster computing times than using a basic queue. Notably, Fibonacci heap or Brodal queue offer optimal implementations for those 3 operations. As the algorithm is slightly different, we mention it here, in pseudo-code as well :
 1 function Dijkstra("Graph", "source"):
 2 dist["source"] ← 0 "// Initialization
 3 for each vertex "v" in "Graph": 
 4 if "v" ≠ "source"
 5 dist["v"] ← infinity "// Unknown distance from source to v
 6 prev["v"] ← undefined "// Predecessor of v
 7 end if
 8 "Q".add_with_priority("v", dist["v"])
 9 end for 
 10
 11 while "Q" is not empty: "// The main loop
 12 "u" ← "Q".extract_min() "// Remove and return best vertex
 13 for each neighbor "v" of "u":
 14 "alt" = dist["u"] + length("u", "v") 
 15 if "alt" < dist["v"]
 16 dist["v"] ← "alt"
 17 prev["v"] ← "u"
 18 "Q".decrease_priority("v", "alt")
 19 end if
 20 end for
 21 end while
 21 return dist[], prev[]
Instead of filling the priority queue with all nodes in the initialization phase, it is also possible to initialize it to contain only "source"; then, inside the if "alt" < dist["v"] block, the node must be inserted if not already in the queue (instead of performing a decrease_priority operation).:198
Other data structures can be used to achieve even faster computing times in practice.
Proof of correctness.
Proof is by induction on the number of visited nodes. 
Invariant hypothesis: For each visited node u, dist[u] is the shortest distance from source to u; and for each unvisited v, dist[v] is the shortest distance via visited nodes only from source to v (if such a path exists, otherwise infinity; note we do not assume dist[v] is the actual shortest distance for unvisited nodes). 
The base case is when there is just one visited node, namely the initial node source, and the hypothesis is trivial.
Assume the hypothesis for "n-1" visited nodes. Now we choose an edge uv where v has the least dist[v] of any unvisited node and the edge uv is such that . 
dist[v] must be the shortest distance from source to v because if there were a shorter path, and if w was the first unvisited node on that path then by hypothesis dist[w] > dist[v] creating a contradiction. Similarly if there was a shorter path to v without using unvisited nodes then dist[v] would have been less than dist[u] + length[u,v].
After processing v it will still be true that for each unvisited node w, dist[w] is the shortest distance from source to w using visited nodes only, since if there were a shorter path which doesn't visit v we would have found it previously, and if there is a shorter path using v we update it when processing v .
Running time.
Bounds of the running time of Dijkstra's algorithm on a graph with edges formula_5 and vertices formula_6 can be expressed as a function of the number of edges, denoted formula_4, and the number of vertices, denoted formula_2, using big-O notation. How tight a bound is possible depends on the way the vertex set formula_9 is implemented. In the following, upper bounds can be simplified because formula_10 for any graph, but that simplification disregards the fact that in some problems, other upper bounds on formula_4 may hold.
For any implementation of the vertex set formula_9, the running time is in
where formula_14 and formula_15 are the complexities of the "decrease-key" and "extract-minimum" operations in formula_9, respectively. The simplest implementation of the Dijkstra's algorithm stores the vertex set formula_9 as an ordinary linked list or array, and extract-minimum is simply a linear search through all vertices in formula_9. In this case, the running time is formula_19.
For sparse graphs, that is, graphs with far fewer than formula_20 edges, Dijkstra's algorithm can be implemented more efficiently by storing the graph in the form of adjacency lists and using a self-balancing binary search tree, binary heap, pairing heap, or Fibonacci heap as a priority queue to implement extracting minimum efficiently. To perform decrease-key steps in a binary heap efficiently, it is necessary to use an auxiliary data structure that maps each vertex to its position in the heap, and to keep this structure up to date as the priority queue formula_9 changes. With a self-balancing binary search tree or binary heap, the algorithm requires
time in the worst case; for connected graphs this time bound can be simplified to formula_23. The Fibonacci heap improves this to
When using binary heaps, the average case time complexity is lower than the worst-case: assuming edge costs are drawn independently from a common probability distribution, the expected number of "decrease-key" operations is bounded by formula_25, giving a total running time of:199–200
Practical optimizations and infinite graphs.
In common presentations of Dijkstra's algorithm, initially all nodes are entered into the priority queue. This is, however, not necessary: the algorithm can start with a priority queue that contains only one item, and insert new items as they are discovered (instead of doing a decrease-key, check whether the key is in the queue; if it is, decrease its key, otherwise insert it).:198 This variant has the same worst-case bounds as the common variant, but maintains a smaller priority queue in practice, speeding up the queue operations.
Moreover, not inserting all nodes in a graph makes it possible to extend the algorithm to find the shortest path from a single source to the closest of a set of target nodes on infinite graphs or those too large to represent in memory. The resulting algorithm is called "uniform-cost search" (UCS) in the artificial intelligence literature and can be expressed in pseudocode as
 procedure "UniformCostSearch"(Graph, start, goal)
 node ← start
 cost ← 0
 frontier ← priority queue containing node only
 explored ← empty set
 do
 if frontier is empty
 return failure
 node ← frontier.pop()
 if node is goal
 return solution
 explored.add(node)
 for each of node's neighbors n
 if n is not in explored
 if n is not in frontier
 frontier.add(n)
 else if n is in frontier with higher cost
 replace existing node with n
The complexity of this algorithm can be expressed in an alternative way for very large graphs: when "C"* is the length of the shortest path from the start node to any node satisfying the "goal" predicate, and each edge has cost at least ε, then the algorithm's worst-case time and space complexity are both in "O"("b"1+⌊"C"* ⁄ "ε"⌋).
Specialized variants.
When arc weights are integers and bounded by a constant "C", the usage of a special priority queue structure by Van Emde Boas etal.(1977) brings the complexity to formula_27. Another interesting implementation based on a combination of a new radix heap and the well-known Fibonacci heap runs in time formula_28 . Finally, the best algorithms in this special case are as follows. The algorithm given by runs in formula_29 time and the algorithm given by runs in formula_30 time.
Also, for directed acyclic graphs, it is possible to find shortest paths from a given starting vertex in linear formula_31 time, by processing the vertices in a topological order, and calculating the path length for each vertex to be the minimum length obtained via any of its incoming edges.
In the special case of integer weights and undirected graphs, the Dijkstra's algorithm can be completely countered with a linear formula_32 complexity algorithm, given by .
Related problems and algorithms.
The functionality of Dijkstra's original algorithm can be extended with a variety of modifications. For example, sometimes it is desirable to present solutions which are less than mathematically optimal. To obtain a ranked list of less-than-optimal solutions, the optimal solution is first calculated. A single edge appearing in the optimal solution is removed from the graph, and the optimum solution to this new graph is calculated. Each edge of the original solution is suppressed in turn and a new shortest-path calculated. The secondary solutions are then ranked and presented after the first optimal solution.
Dijkstra's algorithm is usually the working principle behind link-state routing protocols, OSPF and IS-IS being the most common ones.
Unlike Dijkstra's algorithm, the Bellman–Ford algorithm can be used on graphs with negative edge weights, as long as the graph contains no negative cycle reachable from the source vertex "s". The presence of such cycles means there is no shortest path, since the total weight becomes lower each time the cycle is traversed. It is possible to adapt Dijkstra's algorithm to handle negative weight edges by combining it with the Bellman-Ford algorithm (to remove negative edges and detect negative cycles), such an algorithm is called Johnson's algorithm.
The A* algorithm is a generalization of Dijkstra's algorithm that cuts down on the size of the subgraph that must be explored, if additional information is available that provides a lower bound on the "distance" to the target. This approach can be viewed from the perspective of linear programming: there is a natural linear program for computing shortest paths, and solutions to its dual linear program are feasible if and only if they form a consistent heuristic (speaking roughly, since the sign conventions differ from place to place in the literature). This feasible dual / consistent heuristic defines a non-negative reduced cost and A* is essentially running Dijkstra's algorithm with these reduced costs. If the dual satisfies the weaker condition of admissibility, then A* is instead more akin to the Bellman–Ford algorithm.
The process that underlies Dijkstra's algorithm is similar to the greedy process used in Prim's algorithm. Prim's purpose is to find a minimum spanning tree that connects all nodes in the graph; Dijkstra is concerned with only two nodes. Prim's does not evaluate the total weight of the path from the starting node, only the individual path.
Breadth-first search can be viewed as a special-case of Dijkstra's algorithm on unweighted graphs, where the priority queue degenerates into a FIFO queue.
Fast marching method can be viewed as a continuous version of Dijkstra's algorithm which computes the geodesic distance on a triangle mesh.
Dynamic programming perspective.
From a dynamic programming point of view, Dijkstra's algorithm is a successive approximation scheme that solves the dynamic programming functional equation for the shortest path problem by the Reaching method.
In fact, Dijkstra's explanation of the logic behind the algorithm, namely
Problem 2. Find the path of minimum total length between two given nodes formula_33 and formula_9.
We use the fact that, if formula_35 is a node on the minimal path from formula_33 to formula_9, knowledge of the latter implies the knowledge of the minimal path from formula_33 to formula_35.
is a paraphrasing of Bellman's famous Principle of Optimality in the context of the shortest path problem.

</doc>
<doc id="45810" url="http://en.wikipedia.org/wiki?curid=45810" title="Subwoofer">
Subwoofer

A subwoofer (or sub) is a woofer, or a complete loudspeaker, which is dedicated to the reproduction of low-pitched audio frequencies known as bass. The typical frequency range for a subwoofer is about 20–200 Hz for consumer products, below 100 Hz for professional live sound, and below 80 Hz in THX-approved systems. Subwoofers are intended to augment the low frequency range of loudspeakers covering higher frequency bands.
Subwoofers are made up of one or more woofers mounted in a loudspeaker enclosure—often made of wood—capable of withstanding air pressure while resisting deformation. Subwoofer enclosures come in a variety of designs, including bass reflex (with a port or passive radiator in the enclosure), infinite baffle, horn-loaded, and bandpass designs, representing unique tradeoffs with respect to efficiency, bandwidth, size and cost. Passive subwoofers have a subwoofer driver and enclosure and they are powered by an external amplifier. Active subwoofers include a built-in amplifier.
The first subwoofers were developed in the 1960s to add bass response to home stereo systems. Subwoofers came into greater popular consciousness in the 1970s with the introduction of Sensurround in movies such as "Earthquake", which produced loud low-frequency sounds through large subwoofers. With the advent of the compact cassette and the compact disc in the 1980s, the easy reproduction of deep "and" loud bass was no longer limited by the ability of a phonograph record stylus to track a groove, and producers could add more low frequency content to recordings. As well, during the 1990s, DVDs were increasingly recorded with "surround sound" processes that included a Low-frequency effects (LFE) channel, which could be heard using the subwoofer in home theater systems. During the 1990s, subwoofers also became increasingly popular in home stereo systems, custom car audio installations, and in PA systems. By the 2000s, subwoofers became almost universal in sound reinforcement systems in nightclubs and concert venues.
History.
The very first subwoofer enclosure made for home and studio use was the separate bass speaker for the Servo Statik 1, by New Technology Enterprises. Designed as a prototype in 1966 by physicist Arnold Nudell and airline pilot Cary Christie in Nudell's garage, the design used a second winding around a custom Cerwin Vega 18-inch (45cm) driver to provide servo control information to the amplifier, and it was offered for sale at $1795, some 40% more expensive than any other complete loudspeaker listed at "Stereo Review". In 1968, the two found outside investment and reorganized as Infinity. The subwoofer was reviewed positively in "Stereophile" magazine's Winter 1968 issue as the SS-1 by Infinity. The SS-1 was reviewed very highly in 1970 by "High Fidelity" magazine.
One of the first subwoofers was developed during the late 1960s by Ken Kreisel, the former president of the Miller & Kreisel Sound Corporation in Los Angeles. When Kreisel's business partner, Jonas Miller, who owned a high-end audio store in Los Angeles, told Kreisel that some purchasers of the store's high-end electrostatic speakers had complained about a lack of bass response in the electrostatics, Kreisel designed a powered woofer that would reproduce only those frequencies that were too low for the electrostatic speakers to convey. Infinity's full range electrostatic speaker system that was developed during the 1960s also used a woofer to cover the lower frequency range that its electrostatic arrays did not handle adequately.
The first use of a subwoofer in a recording session was in 1973 for mixing the Steely Dan album "Pretzel Logic" when recording engineer Roger Nichols arranged for Kreisel to bring a prototype of his subwoofer to Village Recorders. Further design modifications were made by Kreisel over the next ten years, and in the 1970s and 1980s by engineer John P. D'Arcy; record producer Daniel Levitin served as a consultant and "golden ears" for the design of the crossover network (used to partition the frequency spectrum so that the subwoofer would not attempt to reproduce frequencies too high for its effective range, and so that the main speakers would not need to handle frequencies too low for their effective range).
Subwoofers received a great deal of publicity in 1974 with the movie "Earthquake" which was released in Sensurround. Initially installed in 17 U.S. theaters, the Sensurround system used large subwoofers which were driven by racks of 500 watt amplifiers which were triggered by control tones printed on one of the audio tracks on the film. Four of the subwoofers were positioned in front of the audience under (or behind) the film screen and two more were placed together at the rear of the audience on a platform. Powerful noise energy in the range of 17 Hz to 120 Hz was generated at the level of 110–120 decibels of sound pressure level, abbreviated dB(SPL). The new low frequency entertainment method helped the film become a box office success. More Sensurround systems were assembled and installed. By 1976 there were almost 300 Sensurround systems leapfrogging through select theaters. Other films to use the effect include the WW II naval battle epic "Midway" in 1976 and "Rollercoaster" in 1977. Deep-Bass speakers were once an exotic commodity and are now much more popular with different sizes and capabilities of sound output.
For owners of 33 rpm LPs and 45 singles, loud "and" deep bass was limited by the ability of the phonograph record stylus to track the groove. Some hi-fi aficionados solved the problem by using reel-to-reel tape players which were capable of delivering accurate, naturally deep bass from acoustic sources, or synthetic bass not found in nature. With the popular introduction of the compact cassette and the CD, it became possible to add more low frequency content to recordings, and satisfy a larger number of consumers. Home subwoofers grew in popularity, as they were easy to add to existing multimedia speaker setups and they were easy to position or hide.
Construction and features.
Loudspeaker and enclosure design.
Subwoofers use speaker drivers (woofers) typically between 8" (20cm) and 21" (53cm) in diameter. Some uncommon subwoofers use larger drivers, and single prototype subwoofers as large as 60" (152cm) have been fabricated.(Bad Link) On the smaller end of the spectrum, subwoofer drivers as small as 4" (10cm) may be used, depending on the design of the loudspeaker enclosure, the desired sound pressure level, the lowest frequency targeted and the level of permitted distortion. The most common subwoofer driver sizes used for sound reinforcement are 10", 12", 15" and 18" models (25cm, 30cm, 40cm, and 45cm respectively). The largest available sound reinforcement subwoofers, 21" (53cm) drivers, are less commonly seen.
The efficiency of a speaker driver is given by:
Where the variables are Thiele/Small parameters. Deep low frequency extension is a common goal for a subwoofer and small box volumes are also considered desirable. Hoffman's Iron Laws therefore mandate low efficiency under those constraints, and indeed most subwoofers require considerable power, much more than other individual drivers.
So for the example of a sealed speaker box, the box volume to achieve a given Qts is proportional to Vas:
Therefore a decrease in box volume and the same F3 will decrease the efficiency of the sub woofer. Similarly the F3 of a speaker is proportional to Fs:
As the efficiency is proportional to Fs3, small improvements in low frequency extension with the same driver and box volume will result in very significant reductions in efficiency. For these reasons, subwoofers are typically very inefficient at converting electrical energy into sound energy. This combination of factors accounts for the higher power output of subwoofer amplifiers, and the requirement for greater power handling for subwoofer drivers. Enclosure variations (e.g., bass reflex designs) are sometimes used for subwoofers to increase the efficiency of the driver/enclosure system, helping to reduce the amplifier power requirement.
Subwoofers have been designed using a number of enclosure approaches: bass reflex, acoustic suspension, infinite baffle, horn loaded, tapped horn, transmission line and bandpass. Each enclosure type has advantages and disadvantages in efficiency increase, bass extension, cabinet size, distortion, and cost. Subwoofers are typically constructed by mounting one or more woofers in a cabinet of medium-density fibreboard (MDF), oriented strand board (OSB), plywood, fiberglass, aluminum or other stiff materials. Because of the high air pressure they produce in the cabinet, subwoofer enclosures often require internal bracing to distribute the resulting forces.
The smallest subwoofers are typically those designed for desktop multimedia systems. The largest common subwoofer enclosures are those used for concert sound reinforcement systems or dance club sound systems. An example of a large concert subwoofer enclosure is the 1980s-era ElectroVoice MT-4 "Bass Cube" system, which used four 18" (45cm) drivers. An example of a subwoofer that uses a bass horn is the Bassmaxx B-Two, which loads an 18" (45cm) driver onto an 11 ft long folded horn. Folded horn-type subwoofers can typically produce a deeper range with greater efficiency than the same driver in an enclosure that lacks a horn. Some experimental fixed-installation subwoofer horns have been constructed using brick and concrete to produce a very long horn that allows a very deep sub-bass extension.
Subwoofer output level can be increased by increasing cone surface area or by increasing cone excursion. Since large drivers require undesirably large cabinets, most subwoofer drivers have large excursions. Unfortunately, high excursion, at high power levels, tends to produce more distortion from inherent mechanical and magnetic effects in electro-dynamic drivers (the most common sort). The conflict between assorted goals can never be fully resolved; subwoofer designs are necessarily compromises. Hoffman's Iron Law (the efficiency of a woofer system is directly proportional to its cabinet volume and to the cube of its cutoff frequency) applies to subwoofers just as to all loudspeakers.
Frequency range and frequency response.
The frequency response specification of a speaker describes the range of frequencies or musical tones a speaker can reproduce, measured in hertz (Hz). The typical frequency range for a subwoofer is between 20–200 Hz. Professional concert sound system subwoofers typically operate below 100 Hz, and THX-approved systems operate below 80 Hz. Subwoofers vary in terms of the range of pitches that they can reproduce, depending on a number of factors such as the size of the cabinet and the construction and design of the enclosure and driver(s). Specifications of frequency response depend wholly for relevance on an accompanying amplitude value—measurements taken with a wider amplitude tolerance will give any loudspeaker a wider frequency response. For example, the JBL 4688 TCB Subwoofer System, a now-discontinued system which was designed for movie theaters, had a frequency response of 23–350 Hz when measured within a 10-decibel boundary (0 dB to -10 dB) and a narrower frequency response of 28–120 Hz when measured within a six-decibel boundary (±3 dB).
Subwoofers also vary in regard to the sound pressure levels achievable and the distortion levels they can produce over their range. The Abyss subwoofer, for example can reproduce pitches from 18 Hz (which is about the pitch of the lowest rumbling notes on a huge pipe organ with 32 ft bass pipes) to 120 Hz (±3 dB). Nevertheless, even though the Abyss subwoofer can go down to 18 Hz, its lowest frequency and maximum SPL with a limit of 10% distortion is 35.5 Hz and 79.8 dB at 2 meters. This means that a person choosing a subwoofer needs to consider more than just the lowest pitch that the sub can reproduce.
Amplification.
'Active subwoofers' include their own dedicated amplifiers within the cabinet. Some also include user-adjustable equalization that allows boosted or reduced output at particular frequencies; these vary from a simple "boost" switch, to fully parametric equalizers meant for detailed speaker and room correction. Some such systems are even supplied with a calibrated microphone to measure the subwoofer's in-room response, so the automatic equalizer can correct the combination of subwoofer, subwoofer location, and room response to minimize effects of room modes and improve low frequency performance.
'Passive subwoofers' have a subwoofer driver and enclosure, but they do not include an amplifier. They sometimes incorporate internal passive crossovers, with the filter frequency determined at the factory. These are generally used with third-party power amplifiers, taking their inputs from active crossovers earlier in the signal chain. While few high-end home-theater systems use passive subwoofers, this format is still popular in the professional sound industry.
Equalization.
Equalization can be used to adjust the in-room response of a subwoofer system. Designers of active subwoofers sometimes include a degree of corrective equalization to compensate for known performance issues (e.g., a steeper than desired low end roll-off rate). In addition, many amplifiers include an adjustable low-pass filter, which prevents undesired higher frequencies from reaching the subwoofer driver. For example, if a listener's main speakers are usable down to 80 Hz, then the subwoofer filter can be set so the subwoofer only works below 80. Typical filters involve some overlap in frequency ranges; a steep filter is not generally desired for subwoofers. The crossover section may also include a high-pass "infrasonic" filter which prevents the subwoofer driver from attempting to reproduce frequencies below its safe capabilities.
Some systems use parametric equalization in an attempt to correct for room frequency response irregularities. Equalization is often unable to achieve flat frequency response at all listening locations in part because of the resonance (i.e., standing wave) patterns at low frequencies in nearly all rooms. Careful positioning of the subwoofer within the room can also help flatten the frequency response. Multiple subwoofers can manage a flatter general response since they can often be arranged to excite room modes more evenly than a single subwoofer, allowing equalization to be more effective.
Phase control.
Changing the relative phase of the subwoofer with respect to the woofers in other speakers may or may not help to minimize unwanted destructive acoustic interference in the frequency region covered by both subwoofer and main speakers. It may not help at all frequencies, and may create further problems with frequency response, but is even so generally provided as an adjustment for subwoofer amplifiers. Phase control circuits may be a simple polarity reversal switch or a more complex continuously variable circuits.
Continuously variable phase control circuits are common in subwoofer amplifiers, and may be found in crossovers and as do-it-yourself electronics projects. Phase controls allow the listener to change the arrival time of the subwoofer sound waves relative to the same frequencies from the main speakers (i.e., at and around the crossover point to the subwoofer). A similar effect can be achieved with the delay control on many home theater receivers. The subwoofer phase control found on many subwoofer amplifiers is actually a polarity inversion switch. It allows users to reverse the polarity of the subwoofer relative to the audio signal it is being given. This type of control allows the subwoofer to either be in phase with the source signal, or 180 degrees out of phase.
The subwoofer phase can still be changed by moving the subwoofer closer to or further from the listening position, however this may not be always practical.
Servo subwoofers.
Some active subwoofers use a servo feedback mechanism based on cone movement which modifies the signal sent to the voice coil. The servo feedback signal is derived from a comparison of the input signal to the amplifier versus the actual motion of the cone. The usual source of the feedback signal is a few turns of voice coil attached to the cone or a microchip-based accelerometer placed on the cone itself. An advantage of a well-implemented servo subwoofer design is reduced distortion making smaller enclosure sizes possible. The primary disadvantages are cost and complexity.
Servo controlled subwoofers are not the same as Servodrive subwoofers whose primary mechanism of sound reproduction avoids the normal voice coil and magnet combination in favor of a high-speed belt-driven servomotor. The Servodrive design increases output power, reduces harmonic distortion and virtually eliminates the loss of loudspeaker output that results from an increase in voice coil impedance due to overheating of the voice coil (called "power compression".) This feature allows high power operation for extended periods of time. Intersonics was nominated for a TEC Award for its Servo Drive Loudspeaker (SDL) design in 1986 and for the Bass Tech 7 model in 1990.
Applications.
Home audio.
The use of a subwoofer augments the bass capability of the main speakers, and allows them to be smaller without sacrificing low frequency capability. A subwoofer does not necessarily provide superior bass performance in comparison to large conventional loudspeakers on ordinary music recordings due to the typical lack of very low frequency content on such sources. However, there are recordings with substantial low frequency content that most conventional loudspeakers are ill-equipped to handle without the help of a subwoofer, especially at high playback levels, such as music for pipe organs with 32' (9.75 meter) bass pipes (16 Hz), very large bass drums on symphony orchestra recordings and electronic music with extremely low synth bass parts, such as bass tests or bass songs.
Frequencies which are sufficiently low are not easily localized by humans, hence many stereo and multichannel audio systems feature only one subwoofer channel and a single subwoofer can be placed off-center without affecting the perceived sound stage, since the sound produced will be difficult to localize. The intention in a system with a subwoofer is often to use small main speakers (of which there are two for stereo and five or more for surround sound or movie tracks) and to hide the subwoofer elsewhere (e.g. behind furniture or under a table), or to augment an existing speaker to save it from having to handle woofer-destroying low frequencies at high levels. This effect is possible only if the subwoofer is restricted to quite low frequencies, usually taken to, say, 100 Hz and below—still less localization is possible if restricted to even lower maximum frequencies. Higher upper limits for the subwoofer (e.g., 125 Hz) are much more easily localized, making a single subwoofer impractical.
Some users add a subwoofer because high levels of low bass are desired, even beyond what is in the original recording, as in the case of house music enthusiasts. Thus, subwoofers may be part of a package that includes satellite speakers, may be purchased separately, or may be built into the same cabinet as a conventional speaker system. For instance, some floor standing tower speakers include a subwoofer driver in the lower portion of the same cabinet. Physical separation of subwoofer and "satellite" speakers not only allows placement in an inconspicuous location, but since sub-bass frequencies are particularly sensitive to room location (due to room resonances and reverberation 'modes'), the best position for the subwoofer is not likely to be where the "satellite" speakers are located.
For greatest efficiency and best coupling to the room's air volume, subwoofers can be placed in a corner of the room, far from large room openings, and closer to the listener. This is possible since low bass frequencies have a long wavelength; hence there is little difference between the information reaching a listener's left and right ears, and so they cannot be readily localized. All low frequency information is sent to the subwoofer. However, unless the sound tracks have been carefully mixed for a single subwoofer channel, it's possible to have some cancellation of low frequencies if bass information in one channel is out of phase with another.
The physically separate subwoofer/satellite arrangement has been popularized by multimedia speaker systems such as Bose Acoustimass Home Entertainment Systems, Polk Audio RM2008 Series and Klipsch Audio Technologies ProMedia, among many others. Low-cost "home theater in a box" systems advertise their integration and simplicity.
Particularly among low cost "Home Theater in a Box" systems and with "boom boxes", however, inclusion of a subwoofer may be little more than a marketing device. It is unlikely that a small woofer in an inexpensively-built compact plastic cabinet will have better bass performance than well-designed conventional (and typically larger) speakers in a plywood or MDF cabinet. Mere use of the term "subwoofer" is no guarantee of good or extended bass performance. Many multimedia "subwoofers" might better be termed "bass drivers" as they are too small to produce deep bass.
Further, poorly designed systems often leave everything below about 120 Hz (or even higher) to the subwoofer, meaning that the subwoofer handles frequencies which the ear can use for sound source localization, thus introducing an undesirable subwoofer "localization effect". This is usually due to poor crossover designs or choices (too high crossover point or insufficient crossover slope) used in many computer and home theater systems; localization also comes from port noise and from typically large amounts of harmonic distortion in the subwoofer design. Home subwoofers sold individually usually include crossover circuitry to assist integration into an existing system.
Car audio.
Automobiles are not well suited for the "hidden" subwoofer approach due to space limitations in the passenger compartments. It is not possible, in most circumstances, to fit such large drivers and enclosures into doors or dashboards, so subwoofers are installed in the trunk or back seat space. Some car audio enthusiasts compete to produce very high sound pressure levels in the confines of their vehicle's cabin; sometimes dangerously high. The "SPL wars" have drawn much attention to subwoofers in general, but subjective competitions in sound quality ("SQ") have not gained equivalent popularity. Top SPL cars are not able to play normal music, or perhaps even to drive normally as they are designed solely for competition. Many non-competition subwoofers are also capable of generating high levels in cars due to the small volume of a typical car interior. High sound levels can cause hearing loss and tinnitus if one is exposed to them for an extended period of time.
In the 2000s, several car audio manufacturers have produced subwoofers using non-circular shapes from manufacturers including Boston Acoustic, Kicker, Sony, Bazooka, and X-Tant. These shapes typically carry some sort of distortion penalties. In situations of limited mounting space they provide a greater cone area and assuming all other variables are constant, greater maximum output. An important factor in the "square sub vs round sub" argument is the effects of the enclosure used. In a sealed enclosure, the maximum displacement is determined by
formula_5
where
These are some of the Thiele/Small parameters which can either be measured or found with the driver specifications.
Cinema sound.
After the introduction of Sensurround, movie theater owners began installing permanent subwoofer systems. Dolby Stereo 70 mm Six Track was a six channel film sound format introduced in 1976 that used two subwoofer channels for stereo reproduction of low frequencies. In 1981, Altec introduced a dedicated cinema subwoofer model tuned to around 20 Hz: the 8182. Starting in 1983, THX certification of the cinema sound experience quantified the parameters of good audio for watching films, including requirements for subwoofer performance levels and enough isolation from outside sounds so that noise did not interfere with the listening experience. This helped provide guidelines for multiplex cinema owners who wanted to isolate each individual cinema from its neighbors, even as louder subwoofers were making isolation more difficult. Specific cinema subwoofer models appeared from JBL, Electro-Voice, Eastern Acoustic Works, Kintek, Meyer Sound Laboratories and BGW Systems in the early 1990s. In 1992, Dolby Digital's six-channel film sound format incorporated a single LFE channel, the "point one" in 5.1 surround sound.
Tom Horral, a Boston-based acoustician, blames complaints about modern movies being too loud on subwoofers. He says that before subwoofers made it possible to have loud, relatively undistorted bass, movie sound levels were limited by the distortion in less capable systems at low frequency and high levels.
Sound reinforcement.
Professional audio subwoofers must be capable of very high output levels. This is reflected in the design attention given in recent years to the subwoofer applications for sound reinforcement, public address systems, dance club systems and concert systems. Consumer applications (as in home use) are considerably less demanding due to much smaller listening space and lower playback levels. Subwoofers are now almost universal in professional sound applications such as live concert sound, churches, nightclubs, and theme parks. Movie theatres certified to the THX standard for playback always include high capability subwoofers. Some professional applications require subwoofers designed for very high sound levels, using multiple 12", 15", 18" or 21" drivers (30cm, 40cm, 45cm, 53cm respectively). Drivers as small as 10" (25cm) are occasionally used, generally in horn loaded enclosures.
The number of subwoofer enclosures used in a concert depends on a number of factors, including the size of the venue, whether it is indoors or outdoors, the amount of low-frequency content in the band's sound, the desired volume of the concert, and the design and construction of the enclosures (e.g., direct-radiating versus horn-loaded. A small bar may use a single direct-radiating 15-inch (40cm) sub cabinet. A large dance club may have a row of four or five twin 18-inch (45cm) subwoofer cabinets, or more). In the largest stadium venues, there may be a very large number of subwoofer enclosures. For example, the 2009–2010 U2 360° Tour uses 24 Clair Brothers BT-218 subwoofers (a double 18" (45cm) box) around the perimeter of the central circular stage, and 72 proprietary Clair Brothers cardioid S4 subwoofers placed underneath the ring-shaped "B" stage which encircles the central main stage.
The main speakers may be 'flown' from the ceiling of a venue on chain hoists, and 'flying points' (i.e., attachment points) are built into many professional loudspeaker enclosures. Subwoofers can be flown or stacked on the ground near the stage. There can be more than 50 double-18-inch (45cm) cabinets in a typical concert system. Just as consumer subwoofer enclosures can be made of Medium-density fibreboard (MDF), Oriented strand board (OSB), plywood, plastic or other dense material, professional subwoofer enclosures can be built from the same materials. MDF is commonly used to construct subwoofers for permanent installations as its density is relatively high and weatherproofing is not a concern. Other permanent installation subwoofers have used very thick plywood: the Altec 8182 (1981) used 7-ply 28 mm birch-faced oak plywood. Touring subwoofers are typically built from 18–20 mm thick void-free Baltic birch (Betula pendula or Betula pubescens) plywood from Finland, Estonia or Russia; such plywood affords greater strength for frequently transported enclosures. Not naturally weatherproof, Baltic birch is coated with carpet, thick paint or spray-on truck bedliner to give the subwoofer enclosures greater durability.
Touring subwoofer cabinets are typically designed with features that facilitate moving the enclosure (e.g., wheels, a "towel bar" handle and recessed handles), a protective grill for the speaker (in direct radiating-style cabinets), metal or plastic protection for the cabinets to protect the finish as the cabinets are being slid one on top of another, and hardware to facilitate stacking the cabinets (e.g., interlocking corners) and for "flying" the cabinets from stage rigging.
Full-range system.
In professional concert sound system design, subwoofers can be incorporated seamlessly with the main speakers into a stereo or mono full-range system by using an active crossover. Such a system receives its signal from the main mono or stereo mixing console mix bus and amplifies all frequencies together in the desired balance. If the main sound system is stereo, the subwoofers can also be in stereo. Otherwise, a mono subwoofer channel can be derived within the crossover from a stereo mix, depending on the crossover make and model.
Aux-fed subwoofers.
Instead of being incorporated into a full-range system, concert subwoofers can be supplied with their own signal from a separate mix bus on the mixing console; often one of the auxiliary sends ("aux" or "auxes") is used. This configuration is called "aux-fed subwoofers", and has been observed to significantly reduce low frequency "muddiness" that can build up in a concert sound system which has on stage a number of microphones each picking up low frequencies and each having different phase relationships of those low frequencies. The aux-fed subs method greatly reduces the number of sources feeding the subwoofers to include only those instruments that have desired low frequency information; sources such as kick drum, bass guitar, samplers and keys. This simplifies the signal sent to the subwoofers and makes for greater clarity and low punch. Aux-fed subs can even be stereo, if desired, using two auxiliary mix buses.
Directional bass.
In order to keep low frequency energy focused on the audience area and not on the stage, and to keep low frequencies from bothering people outside of the event space, a variety of techniques have been developed in concert sound to turn the naturally omnidirectional radiation of subwoofers into a more directional pattern. These techniques include setting up subwoofers in a vertical array; using combinations of delay and polarity inversion; and setting up a delay-shaded system.
Vertical array.
Stacking or rigging the subwoofers in a vertical array focuses the low frequencies forward to a greater or lesser extent depending on the physical length of the array. Longer arrays have a more directional effect at lower frequencies. The directionality is more pronounced in the vertical dimension, yielding a radiation pattern that is wide but not tall. This helps reduce the amount of low frequency sound bouncing off the ceiling indoors and assists in mitigating external noise complaints outdoors.
Rear delay array.
Another cardioid subwoofer array pattern can be used horizontally, one which takes few channels of processing and no change in required physical space. This method is often called "cardioid subwoofer array" or "CSA" even though the pattern of "all" directional subwoofer methods is cardioid. The CSA method reverses the enclosure orientation and inverts the polarity of one out of every three subwoofers across the front of the stage, and delays those enclosures for maximum cancellation of the target frequency on stage. Polarity inversion can be implemented electronically, by reversing the wiring polarity, or by physically positioning the enclosure to face rearward. This method reduces forward output relative to a tight-packed, flat-fronted array of subwoofers, but can solve problems of unwanted low frequency energy coming into microphones on stage. Compared to the end-fire array, this method has less on-axis energy but more even pattern control throughout the audience, and more predictable cancellation rearward. The effect spans a range of slightly more than one octave.
A second method of rear delay array combines end-fire topology with polarity reversal, using two subwoofers positioned front to back, the drivers spaced one-quarter wavelength apart, the rear enclosure inverted in polarity and delayed by a few milliseconds for maximum cancellation on stage of the target frequency. This method has the least output power directed toward the audience, compared to other directional methods.
End-fire array.
The end-fire subwoofer method, also called "forward steered arrays", places subwoofer drivers co-axially in one or more rows, using destructive interference to reduce emissions to the sides and rear. This can be done with separate subwoofer enclosures positioned front to back with a spacing between them of one-quarter wavelength of the target frequency, the frequency that is least wanted on stage or most desired in the audience. Each row is delayed beyond the first row by an amount related to the speed of sound in air; typically a few milliseconds. The arrival time of sound energy from all the subwoofers is near-simultaneous from the audience's perspective, but is canceled out to a large degree behind the subwoofers because of offset sound wave arrival times. Directionality of the target frequency can achieve as much as 25 dB rear attenuation, and the forward sound is coherently summed in line with the subwoofers. The positional technique of end-fire subwoofers came into widespread use in European live concert sound in 2006.
The end-fire array trades a few decibels of output power for directionality, so it requires more enclosures for the same output power as a tight-packed, flat-fronted array of enclosures. Sixteen enclosures in four rows were used in 2007 at one of the stages of the Ultra Music Festival, to reduce low frequency interference to neighboring stages. Because of the physical size of the end-fire array, few concert venues are able to implement it. The output pattern suffers from comb-filtering off-axis, but can be further shaped by adjusting the frequency response of each row of subwoofers.
Delay-shaded array.
A long line of subwoofers placed horizontally along the front edge of the stage can be delayed such that the center subs fire several milliseconds prior to the ones flanking them, which fire several milliseconds prior to "their" neighbors, continuing in this fashion until the last subwoofers are reached at the outside ends of the subwoofer row (beamforming). This method helps to counteract the extreme narrowing of horizontal dispersion pattern seen with a horizontal subwoofer array. Such delay shading can be used to virtually reshape a loudspeaker array.
Directional enclosure.
Some subwoofer enclosure designs rely on drivers facing to the sides or to the rear in order to achieve a degree of directionality. End-fire drivers can be positioned within a single enclosure that houses more than one driver.
Bass instrument amplification.
In rare cases, sound reinforcement subwoofer enclosures are also used for bass instrument amplification by electric bass players and synth bass players. For most bands and most small- to mid-size venues (e.g., nightclubs and bars), standard bass guitar speaker enclosures or keyboard amplifiers will provide sufficient sound pressure levels for onstage monitoring. Since a regular electric bass has a low "E" (41 Hz) as its lowest note, most standard bass guitar cabinets are only designed with a range that goes down to about 40 Hz. However, in some cases, performers wish to have extended sub-bass response that is not available from standard instrument speaker enclosures, so they use subwoofer cabinets. Just as some electric guitarists add huge stacks of guitar cabinets mainly for show, some bassists will add immense subwoofer cabinets with 18" woofers mainly for show, and the extension sub cabinets will be operated at a lower volume than the main bass cabinets.
Bass guitar players who may use subwoofer cabinets include performers who play with extended range basses that include a low "B" string (about 31 Hz); bassists who play in styles where a very powerful sub-bass response is an important part of the sound (e.g., funk, Latin, gospel, R & B, etc.); and/or bass players who perform in stadium-size venues or large outdoor venues. Keyboard players who use subwoofers for on-stage monitoring include electric organ players who use bass pedal keyboards (which go down to a low "C" which is about 33 Hz) and synth bass players who play rumbling sub-bass parts that go as low as 18 Hz. Of all of the keyboard instruments that are amplified onstage, synthesizers can produce some of the lowest pitches, because unlike a traditional electric piano or electric organ, which have as their lowest notes a low "A" and a low "C", respectively, a synth does not have a fixed lowest octave. A synth player can add lower octaves to a patch by pressing an "octave down" button, which can produce pitches that are at the limits of human hearing.
Several concert sound subwoofer manufacturers suggest that their subs can be used for bass instrument amplification. Meyer Sound suggests that its 650-R2 Concert Series Subwoofer, a 14 sqft enclosure with two 18-inch drivers (45cm), can be used for bass instrument amplification. While performers who use concert sound subwoofers for onstage monitoring may like the powerful sub-bass sound that they get onstage, sound engineers may find the use of large subwoofers (e.g., two 18" drivers (45cm)) for onstage instrument monitoring to be problematic, because it may interfere with the "Front of House" sub-bass sound.
Bass shakers.
Since subsonic bass is felt, sub-bass can be augmented using tactile transducers. Unlike a typical subwoofer driver, which produces audible vibrations, tactile transducers produce low-frequency vibrations that are designed to be felt by individuals who are touching the transducer or indirectly through a piece of furniture or a wooden floor. Tactile transducers have recently emerged as a device class, called variously "bass shakers", "butt shakers" and "throne shakers". They are attached to a seat, for instance a drummer's stool ("throne") or gamer's chair, car seat or home theater seating, and the vibrations of the driver are transmitted to the body then to the ear in a manner similar to bone conduction. They connect to an amplifier like a normal subwoofer. They can be attached to a large flat surface (for instance a floor or platform) to create a large low frequency conduction area, although the transmission of low frequencies through the feet is not as efficient as the seat.
The advantage of tactile transducers used for low frequencies is that they allow a listening environment that is not filled with loud low frequency waves. This helps the concert drummer to monitor his or her kick drum performance without "polluting" the stage with powerful low frequency waves from a 15" (40cm) subwoofer monitor. By not having a subwoofer monitor, a bass shaker also enables a drummer to lower the sound pressure levels that he is exposed to during a performance. For home cinema or videogame use, bass shakers help the user avoid disturbing others in nearby apartments or rooms, because even powerful sound effects such as explosion sounds in a war videogame or the simulated rumbling of an earthquake in an adventure film will not be heard by others. However, some critics argue that the felt vibrations are disconnected from the auditory experience, and they claim that that music is less satisfying with the "butt shaker" than sound effects. As well, critics have claimed that the bass shaker itself can rattle during loud sound effects, which can distract the listener.
World record claims.
With varying measures upon which to base claims, several subwoofers have been said to be the world's largest, loudest or lowest.
Matterhorn.
The Matterhorn is a subwoofer model completed in March 2007 by Danley Sound Labs in Gainesville, Georgia after a U.S. military request for a loudspeaker that could project infrasonic waves over a distance. The Matterhorn was designed to reproduce a continuous sine wave from 15 to 20 Hz, and generate 94 dB at a distance of 250 m, and more than 140 dB for music playback measured at the horn mouth. It can generate a constant 15 Hz sine wave tone at 140 dB for 24 hours a day, seven days a week with extremely low harmonic distortion. The subwoofer has a flat frequency response from 15 to 80 Hz, and is down 3 dB at 12 Hz. It was built within an intermodal container 20 ft long and 8 x square. The container doors swing open to reveal a tapped horn driven by 40 long-throw 15-inch (40cm) MTX speaker drivers each powered by its own 1000-watt amplifier. The manufacturer claims that 53 13-ply 18 mm 4 x sheets of plywood were used in its construction, though one of the fabricators wrote that double-thickness 26-ply sheets were used for convenience.
A diesel generator is housed within the enclosure to supply electricity when external power is unavailable. Of the constant tone output capability, designer Tom Danley wrote that the "target 94 dB at 250 meters is not the essentially fictional 'burst' or 'peak SPL' nonsense in pro sound, or like the 'death burp' signal used in car sound contests." At the annual National Systems Contractors Association (NSCA) convention in March 2007, the Matterhorn was barred from making any loud demonstrations of its power because of concerns about damaging the building of the Orange County Convention Center. Instead, using only a single 20 amp electrical circuit for safety, visitors were allowed to step inside the horn of the subwoofer for an "acoustic massage" as the fractionally powered Matterhorn reproduced low level 10–15 Hz waves.
Royal Device custom installation.
Another subwoofer claimed to be the world's biggest is a custom installation in Italy made by Royal Device primarily of bricks, concrete and sound-deadening material consisting of two subwoofers embedded in the foundation of a listening room. The horn-loaded subwoofers each have a floor mouth that is 2.2 m2, and a horn length that is 9.5 m, in a cavity 1 m under the floor of the listening room. Each subwoofer is driven by eight 18-inch subwoofer drivers with 100 mm voice coils. The designers assert that the floor mouths of the horns are additionally loaded acoustically by a vertical wooden horn expansion and the room's ceiling to create a 10 Hz "full power" wave at the listening position.
Concept Design 60-inch.
A single 60 in diameter subwoofer driver was designed by Richard Clark and David Navone with the help of Dr. Eugene Patronis of the Georgia Institute of Technology. The driver was intended to break sound pressure level records when mounted in a road vehicle, calculated to be able to achieve more than 180 dBSPL. It was built in 1997, driven by DC motors connected to a rotary crankshaft somewhat like in a piston engine. The cone diameter was 54 in and was held in place with a 3 in surround. With a 6 in peak-to-peak stroke, it created a one-way air displacement of 6871 in3. It was capable of generating 5–20 Hz sine waves at various DC motor speeds—not as a response to audio signal—it could not play music. The driver was mounted in a stepvan owned by Tim Maynor but was too powerful for the amount of applied reinforcement and damaged the vehicle. MTX's Loyd Ivey helped underwrite the project and the driver was then called the MTX "Thunder 1000000" (one million).
Still unfinished, the vehicle was entered in an SPL competition in 1997 at which a complaint was lodged against the computer control of the DC motor. Instead of using the controller, two leads were touched together in the hope that the motor speed was set correctly. The drive shaft broke after one positive stroke which created an interior pressure wave of 162 dB. The Concept Design 60-inch was not shown in public after 1998.
MTX Jackhammer.
The heaviest production subwoofer intended for use in automobiles is the MTX Jackhammer by MTX Audio, which features a 22 in diameter cone. The Jackhammer has been known to take upwards of 6000 watts sent to a dual voice coil moving within a 900 oz strontium ferrite magnet. The Jackhammer weighs in at 369 lb and has an aluminum heat sink. The Jackhammer has been featured on the television show Pimp My Ride.

</doc>
<doc id="45811" url="http://en.wikipedia.org/wiki?curid=45811" title="Eponym">
Eponym

An eponym is a person or thing for whom something is named, or believed to be named, or the name itself. For example, Elizabeth I of England is the eponym of the Elizabethan era.
Genericized trademarks such as aspirin, heroin and thermos may also become eponyms.
The derivative adjectives "eponymous" and "eponymic" refer to the person or thing for whom something is named, as in "the "eponymous" founder of the Ford Motor Company". Recent usage, especially in the recorded-music industry, also allows "eponymous" to mean "named after its central character or creator".
History.
Time periods have often been named after a ruler or other influential figure:
Trends
Lists of eponyms.
By person's name
By category
Orthographic conventions.
Capitalized versus lowercase.
For examples, see the comparison table below.

</doc>
<doc id="45812" url="http://en.wikipedia.org/wiki?curid=45812" title="Pim Fortuyn">
Pim Fortuyn

Wilhelmus Simon Petrus Fortuijn, known as Pim Fortuyn (]; 19 February 1948 – 6 May 2002), was a Dutch politician, civil servant, sociologist, author and professor who formed his own party, "Pim Fortuyn List" (Lijst Pim Fortuyn or LPF) in 2002.
Fortuyn provoked controversy with his stated views about multiculturalism, immigration and Islam in the Netherlands. He called Islam "a backward culture", and is quoted as saying that if it were legally possible, he would close the borders for Muslim immigrants. He was labelled a far-right populist by his opponents and in the media, but he fiercely rejected this label.
Fortuyn explicitly distanced himself from "far-right" politicians such as the Belgian Filip Dewinter, the Austrian Jörg Haider, or Frenchman Jean-Marie Le Pen whenever compared to them. While he compared his own politics to centre-right politicians such as Silvio Berlusconi of Italy, he also admired former Dutch Prime Minister Joop den Uyl, a social democrat, and Democratic U.S. president John F. Kennedy. Fortuyn also criticised the Polder model and the policies of the outgoing government of Wim Kok and repeatedly described himself and LPF's ideology as pragmatism and not populism. Fortuyn was openly homosexual.
Fortuyn was assassinated during the 2002 Dutch national election campaign by Volkert van der Graaf. In court at his trial, Van der Graaf said he murdered Fortuyn to stop him from exploiting Muslims as "scapegoats" and targeting "the weak members of society" in seeking political power.
Early life and education.
Fortuyn was born on 19 February 1948 in Driehuis, as the third child to a Catholic family. In 1967 he began to study sociology at the University of Amsterdam but transferred after a few months to the Vrije Universiteit in Amsterdam. In 1971 he ended his study with the Academic degree Doctorandus. In 1981 he received a doctorate in sociology at the University of Groningen as a Doctor of Philosophy.
Career.
Fortuyn worked as a lecturer at the Nyenrode Business Universiteit and as an associate professor at the University of Groningen, where he taught Marxist sociology. He was a Marxist at the time. Later, he joined the Labour Party.
In 1989 Fortuyn became director of a government organisation administering student transport cards. In 1990 he moved to Rotterdam. From 1991 to 1995, he was an extraordinary professor at the Erasmus University Rotterdam, appointed to the Albeda-chair in "employment conditions in public service".
When his contract ended, he made a career of public speaking and writing books and press columns, gradually becoming involved in politics. Fortuyn was openly gay, and said in a 2002 interview that he was Catholic.
Political career.
In 1992 Fortuyn wrote "Aan het volk van Nederland" (To the people of the Netherlands), declaring he was the successor to the charismatic but controversial 18th-century Dutch politician Joan van der Capellen tot den Pol. A one-time communist and former member of the social-democratic Labour Party, Fortuyn was elected "lijsttrekker" of the newly formed Livable Netherlands party by a large majority on 26 November 2001, prior to the Dutch general election of 2002.
On 9 February 2002, he was interviewed by the "Volkskrant", a Dutch newspaper (see below). His statements were considered so controversial that the party dismissed him as "lijsttrekker" the next day. Fortuyn had said that he favoured putting an end to Muslim immigration, if possible. Having been rejected by Livable Netherlands, Fortuyn founded his own party LPF (Pim Fortuyn List) on 11 February 2002. Many Livable Netherlands supporters transferred their support to the new party.
Heading the list of the Livable Rotterdam party, a local issues party, he achieved a major victory in the Rotterdam district council elections in early March 2002. The new party won about 36% of the seats, making it the largest party in the council. For the first time since the Second World War, the Labour Party was out of power in Rotterdam.
Fortuyn's victory made him the subject of hundreds of interviews during the next three months, and he made many statements about his political ideology. In March he released his book "The Mess of Eight Purple Years" ("De puinhopen van acht jaar Paars"), which he used as his political agenda for the upcoming general election. Purple is the colour to indicate a coalition government consisting of left parties (red) and conservative-liberal parties (blue). The Netherlands had been governed by such a coalition for eight years at that time.
Assassination.
On 6 May 2002, at age 54, Fortuyn was assassinated in Hilversum, North Holland, by Volkert van der Graaf. The attack took place in a parking lot outside a radio studio where Fortuyn had just given an interview. This was nine days before the general election, for which he was running. The attacker was pursued by Hans Smolders, Fortuyn's driver, and was arrested by the police shortly afterward, still in possession of a handgun. Months later, Van der Graaf confessed in court to the first notable political assassination in the Netherlands since 1672 (excluding WW II events). He was sentenced to eighteen years in prison and automatically paroled in 2014, after having served twelve years.
The assassination shocked many residents of the Netherlands and highlighted the cultural clashes within the country. Various conspiracy theories arose after Pim Fortuyn's murder and deeply affected Dutch politics and society. Politicians from all parties suspended campaigning. After consultation with LPF, the government decided not to postpone the elections. As Dutch law did not permit modifying the ballots, Fortuyn became a posthumous candidate. The LPF made an unprecedented debut in the House of Representatives by winning 26 seats (17% of the 150 seats in the house). The LPF joined a cabinet with the Christian Democratic Appeal and the People's Party for Freedom and Democracy, but conflicts in the rudderless LPF quickly collapsed the cabinet, forcing new elections. By the following year, the party had lost support, winning only eight seats in the 2003 elections. It won no seats in the 2006 elections, by which time the Party for Freedom, led by Geert Wilders, had emerged as a successor.
During the last months of his life, Fortuyn had become closer to the Catholic Church. To the surprise of many commentators and Dutch TV hosts, Fortuyn insisted on Fr. Louis Berger, a parish priest from The Hague, accompanying him in some of his last TV appearances. According to the "New York Times", Berger had become his "friend and confessor" during the last weeks of his life.
Fortuyn was initially buried in Driehuis in the Netherlands. He was re-interred on 20 July 2002, at San Giorgio della Richinvelda, in the province of Pordenone in Italy, where he had owned a house.
Views on Islam and immigration.
In August 2001, Fortuyn was quoted in the "Rotterdams Dagblad" newspaper saying, "I am also in favour of a cold war with Islam. I see Islam as an extraordinary threat, as a hostile religion." In the TV program, "Business class", Fortuyn said that Muslims in the Netherlands did not accept Dutch society. He appeared on the program several times. It was moderated by his friend Harry Mens. Since his death, commentators have suggested Fortuyn's words were interpreted rather harshly, if not wrongly. For instance, he said that Muslims in the Netherlands needed to accept living "together" with the Dutch, and that if this was unacceptable for them, then they were free to leave. His concluding words in the TV program were "...I want to live together with the Muslim people, but it takes two to tango."
On 9 February 2002, additional statements made by him were carried in the "Volkskrant". He said that the Netherlands, with a population of 16 million, had enough inhabitants, and the practice of allowing as many as 40,000 asylum-seekers into the country each year had to be stopped. (This figure was higher than the actual numbers, and immigrants were decreasing at the time.). He claimed that if he became part of the next government, he would pursue a restrictive immigration policy while also granting citizenship to a large group of illegal immigrants.
He said that he did not intend to "unload our Moroccan hooligans" onto the Moroccan King Hassan. Hassan had died three years earlier. He considered Article 7 of the constitution, which asserts freedom of speech, of more importance than Article 1, which forbids discrimination on the basis of religion, life principles, political inclination, race, or sexual preference. Fortuyn distanced himself from Hans Janmaat of the Centrum Democraten, who in the 1980s wanted to remove all foreigners from the country and was repeatedly convicted for discrimination and hate speech.
Fortuyn proposed that all people who already resided in the Netherlands would be able to stay, but he emphasized the need of the immigrants to adopt Dutch society's consensus on human rights as their own. He said "If it were legally possible, I'd say no more Muslims will get in here", claiming that the influx of Muslims would threaten freedoms in the liberal Dutch society. He thought Muslim culture had never undergone a process of modernisation and therefore still lacked acceptance of democracy and women's, gays', lesbians' and minorities' rights. He feared Muslims would try to replace the Dutch legal system with the shari'a law.
He said he was concerned about intolerance in the Muslim community. In a televised debate in 2002, "Fortuyn baited the Muslim cleric by flaunting his homosexuality. Finally the "imam" exploded, denouncing Fortuyn in strongly anti-homosexual terms. Fortuyn calmly turned to the camera and, addressing viewers directly, told them that this is the kind of Trojan horse of intolerance the Dutch are inviting into their society in the name of multiculturalism."
When asked by the Dutch newspaper "Volkskrant" whether he hated Islam, he replied:
Fortuyn used the word "achterlijk", literally meaning "backward", but commonly used as an insult in the sense of "retarded". After his use of "achterlijk" caused an uproar, Fortuyn said he had used the word with its literal meaning of "backward".
Fortuyn wrote "Against the Islamization of Our Culture" (1997) (in Dutch).
Fortuynism.
The ideology or political style that is derived from Pim Fortuyn, and in turn the LPF, is often called Fortuynism. Observers variously saw him as a political protest targeting the alleged elitism and bureaucratic style of the Dutch purple coalitions or as offering an appealing political style. The style was characterized variously as one "of openness, directness and clearness", populism or simply as charisma. Another school holds Fortuynism as a distinct ideology, with an alternative vision of society. Some argued that Fortuynism was not just "one" ideology, but contained liberalism, populism and nationalism.
During the 2002 campaign, Fortuyn was accused of being on the "extreme right", although others saw only certain similarities. While he employed anti-immigration rhetoric, he was neither a radical nationalist nor a defender of traditional authoritarian values. On the contrary, Fortuyn wanted to protect the socio-culturally liberal values of the Netherlands, women's rights and sexual minorities (he was openly homosexual himself), from the "backward" Islamic culture. He held liberal views favouring the drug policy of the Netherlands, same-sex marriage, euthanasia, and related positions.
The LPF also won support from some ethnic minorities; one of Fortuyn's closest associates was of Cape Verdean origin, and one of the party's MPs was a young woman of Turkish descent.
His ideology can be comprised in the following positions:
Criticism.
Fortuyn was compared with the politicians Jörg Haider and Jean-Marie Le Pen in the foreign press. These comparisons were often referred to by Dutch reporters and politicians. An explicit comparison with Le Pen was made by Ad Melkert, then "lijsttrekker" of the Labour Party, who said in Emmen on 24 April 2002:
On 5 May, the day before the assassination, Fortuyn debated with Melkert in a debate organized by the "Algemeen Dagblad" newspaper about demonization of himself. In it he said that he often had to tell journalists that the image created of him in the media was incorrect.
Columnist Jan Blokker wrote:
Legacy.
Fortuyn changed the Dutch political landscape and political culture. The 2002 elections, only weeks after Fortuyn's death, were marked by large losses for the liberal People's Party for Freedom and Democracy and especially the social democratic Labour Party (whose parliamentary group was halved in size); both parties replaced their leaders shortly after their losses. The election winners were the Pim Fortuyn List, and the Christian democratic Christian Democratic Appeal. Some commentators think that Fortuyn's perceived martyrdom created greater support for the LPF, which seems likely given its quick later decline.
The Netherlands has made its asylum policy more strict. Some have objected to what they think is a harsher political and social climate, especially towards immigrants and Muslims.
Contemporary Dutch politics is more polarized than it has been in recent years, especially on the issues for which Fortuyn was best known. People debate the success of their multicultural society, and whether they need to better assimilate newcomers. The government's decision to expel numerous asylum seekers whose applications had failed was controversial. Fortuyn had advocated an amnesty for asylum seekers already residing in the Netherlands.
The coalition cabinet of Christian Democratic Appeal, Pim Fortuyn List and People's Party for Freedom and Democracy fell within three months, due to infighting within the LPF. In the following elections, the LPF was left with only 8 seats in parliament (out of 150) and was not included in the new government. Political commentators speculated that discontented voters might vote for a non-traditional party, if a viable alternative was at hand. In recent times the right-wing Party for Freedom, which has a strong stance on immigration and integration, won 9 (out of 150) seats in the 2006 elections and 24 in 2010.
In 2004, in a TV show, Fortuyn was chosen as De Grootste Nederlander ("Greatest Dutchman of all-time"), followed closely by William of Orange, the leader of the independence war that established the precursor to the present-day Netherlands. The election was not considered representative, as it was held by viewers' voting through the internet and by phoning in.
Theo van Gogh had been murdered a few days before by a Muslim, which likely affected people's voting in the TV contest for Fortuyn. The program later revealed that William of Orange had received the most votes, but many could not be counted until after the official closing time of the television show (and the proclamation of the winner), due to technical problems. The official rules of the show said that votes counted before the end of the show would be decisive, but it was suggested that all votes correctly cast before the closing of the vote would be counted. Following the official rules, the outcome was not changed.
Fortuyn's political career and popularity suggested a change in the Dutch people's views of their society as tolerant with integrated multiple cultures. 
"First of all, one can conclude that criticism on political correctness and on the ideal of the multicultural society has broken through for real relatively late... In the end it was Pim Fortuyn, the electoral success of the LPF and namely the murder on Fortuyn which led to the definitive breakthrough." Although Fortuyn did not advocate segregation, he brought it up as a debatable issue.
Right-wing politicians gained power after Fortuyn's death, such as former Minister for Integration & Immigration Rita Verdonk and the prominent critic of Islam, Member of the House of Representatives Geert Wilders. These politicians often focused on the debate over cultural assimilation and integration.
Posthumous sexual accusations.
In 2005, three years after Fortuyn's death, Dutch journalist Peter R. de Vries obtained and publicized a secret report of the intelligence department of the Rotterdam police. Fortuyn and several other members of his party had been the subject of an investigation by the intelligence services. According to De Vries, an anonymous informant had claimed that Fortuyn had engaged in sex with minors, Moroccan youths; they would be aged between 16 and 21, this was legal under Dutch law. The Ministry of the Interior informed parliament that the report contained factual inaccuracies, and that the trustworthiness of the original source could not be verified.

</doc>
<doc id="45829" url="http://en.wikipedia.org/wiki?curid=45829" title="Structural engineering">
Structural engineering

Structural engineering is a field of engineering dealing with the analysis and design of structures that support or resist loads.
Structural engineers are most commonly involved in the design of buildings and large nonbuilding structures but they can also be involved in the design of machinery, medical equipment, vehicles or any item where structural integrity affects the item's function or safety. Structural engineers must ensure their designs satisfy given design criteria, predicated on safety (i.e. structures must not collapse without due warning) or serviceability and performance (i.e. building sway must not cause discomfort to the occupants).
Structural engineering theory is based upon applied physical laws and empirical knowledge of the structural performance of different materials and geometries. Structural engineering design utilizes a number of simple structural elements to build complex structural systems. Structural engineers are responsible for making creative and efficient use of funds, structural elements and materials to achieve these goals.
Structural Engineer (Professional).
Structural engineers are responsible for engineering design and analysis. Entry-level structural engineers may design the individual structural elements of a structure, for example the beams, columns, and floors of a building. More experienced engineers may be responsible for the structural design and integrity of an entire system, such as a building.
Structural engineers often specialize in particular fields, such as bridge engineering, building engineering, pipeline engineering, industrial structures, or special mechanical structures such as vehicles, ships or aircraft.
Structural engineering has existed since humans first started to construct their own structures. It became a more defined and formalised profession with the emergence of the architecture profession as distinct from the engineering profession during the industrial revolution in the late 19th century. Until then, the architect and the structural engineer were usually one and the same - the master builder. Only with the development of specialised knowledge of structural theories that emerged during the 19th and early 20th centuries did the professional structural engineer come into existence.
The role of a structural engineer today involves a significant understanding of both static and dynamic loading, and the structures that are available to resist them. The complexity of modern structures often requires a great deal of creativity from the engineer in order to ensure the structures support and resist the loads they are subjected to. A structural engineer will typically have a four or five year undergraduate degree, followed by a minimum of three years of professional practice before being considered fully qualified.
Structural engineers are licensed or accredited by different learned societies and regulatory bodies around the world (for example, the Institution of Structural Engineers in the UK). Depending on the degree course they have studied and/or the jurisdiction they are seeking licensure in, they may be accredited (or licensed) as just structural engineers, or as civil engineers, or as both civil and structural engineers.
Another international organisation is IABSE (International Association for Bridge and Structural Engineering). The aim of that association is to exchange knowledge and to advance the practice of structural engineering worldwide in the service of the profession and society.
History of Structural Engineering.
Structural engineering dates back to 2700 B.C.E. when the step pyramid for Pharaoh Djoser was built by Imhotep, the first engineer in history known by name. Pyramids were the most common major structures built by ancient civilizations because the structural form of a pyramid is inherently stable and can be almost infinitely scaled (as opposed to most other structural forms, which cannot be linearly increased in size in proportion to increased loads).
However, it's important to note that the structural stability of the pyramid is not primarily a result of its shape. The integrity of the pyramid is intact as long as each of the stones is able to support the weight of the stone above it. The limestone blocks were taken from a quarry near the build site. Since the compressive strength of limestone is anywhere from 30 to 250 MPa (MPa = Pa * 10^6), the blocks will not fail under compression. Therefore the structural strength of the pyramid stems from the material properties of the stones from which it was built rather than the pyramid's geometry.
Throughout ancient and medieval history most architectural design and construction was carried out by artisans, such as stone masons and carpenters, rising to the role of master builder. No theory of structures existed, and understanding of how structures stood up was extremely limited, and based almost entirely on empirical evidence of 'what had worked before'. Knowledge was retained by guilds and seldom supplanted by advances. Structures were repetitive, and increases in scale were incremental.
No record exists of the first calculations of the strength of structural members or the behavior of structural material, but the profession of structural engineer only really took shape with the Industrial Revolution and the re-invention of concrete (see History of Concrete). The physical sciences underlying structural engineering began to be understood in the Renaissance and have since developed into computer-based applications pioneered in the 1970s.
Timeline.
Structural failure.
The history of structural engineering contains many collapses and failures. Sometimes this is due to obvious negligence, as in the case of the Pétionville school collapse, in which Rev. Fortin Augustin said that "he constructed the building all by himself, saying he didn't need an engineer as he had good knowledge of construction" following a partial collapse of the three-story schoolhouse that sent neighbors fleeing. The final collapse killed 94 people, mostly children.
In other cases structural failures require careful study, and the results of these inquiries have resulted in improved practices and greater understanding of the science of structural engineering. Some such studies are the result of forensic engineering investigations where the original engineer seems to have done everything in accordance with the state of the profession and acceptable practice yet a failure still eventuated. A famous case of structural knowledge and practice being advanced in this manner can be found in a series of failures involving box girders which collapsed in Australia during the 1970s.
Specializations.
Building structures.
Structural building engineering includes all structural engineering related to the design of buildings. It is the branch of structural engineering that is close to architecture.
Structural building engineering is primarily driven by the creative manipulation of materials and forms and the underlying mathematical and scientific ideas to achieve an end which fulfills its functional requirements and is structurally safe when subjected to all the loads it could reasonably be expected to experience. This is subtly different from architectural design, which is driven by the creative manipulation of materials and forms, mass, space, volume, texture and light to achieve an end which is aesthetic, functional and often artistic.
The architect is usually the lead designer on buildings, with a structural engineer employed as a sub-consultant. The degree to which each discipline actually leads the design depends heavily on the type of structure. Many structures are structurally simple and led by architecture, such as multi-storey office buildings and housing, while other structures, such as tensile structures, shells and gridshells are heavily dependent on their form for their strength, and the engineer may have a more significant influence on the form, and hence much of the aesthetic, than the architect.
The structural design for a building must ensure that the building is able to stand up safely, able to function without excessive deflections or movements which may cause fatigue of structural elements, cracking or failure of fixtures, fittings or partitions, or discomfort for occupants. It must account for movements and forces due to temperature, creep, cracking and imposed loads. It must also ensure that the design is practically buildable within acceptable manufacturing tolerances of the materials. It must allow the architecture to work, and the building services to fit within the building and function (air conditioning, ventilation, smoke extract, electrics, lighting etc.). The structural design of a modern building can be extremely complex, and often requires a large team to complete.
Structural engineering specialties for buildings include:
Earthquake engineering structures.
Earthquake engineering structures are those engineered to withstand earthquakes.
The main objectives of earthquake engineering are to understand the interaction of structures with the shaking ground, foresee the consequences of possible earthquakes, and design and construct the structures to perform during an earthquake.
Earthquake-proof structures are not necessarily extremely strong like the El Castillo pyramid at Chichen Itza shown above. In fact, many structures considered strong may in fact be stiff, which can result in poor seismic performance.
One important tool of earthquake engineering is base isolation, which allows the base of a structure to move freely with the ground.
Civil engineering structures.
Civil structural engineering includes all structural engineering related to the built environment. It includes:
The structural engineer is the lead designer on these structures, and often the sole designer. In the design of structures such as these, structural safety is of paramount importance (in the UK, designs for dams, nuclear power stations and bridges must be signed off by a chartered engineer).
Civil engineering structures are often subjected to very extreme forces, such as large variations in temperature, dynamic loads such as waves or traffic, or high pressures from water or compressed gases. They are also often constructed in corrosive environments, such as at sea, in industrial facilities or below ground.
Mechanical structures.
Principles of structural engineering are applied to variety of mechanical (moveable) structures. The design of static structures assumes they always have the same geometry (in fact, so-called static structures can move significantly, and structural engineering design must take this into account where necessary), but the design of moveable or moving structures must account for fatigue, variation in the method in which load is resisted and significant deflections of structures.
The forces which parts of a machine are subjected to can vary significantly, and can do so at a great rate. The forces which a boat or aircraft are subjected to vary enormously and will do so thousands of times over the structure's lifetime. The structural design must ensure that such structures are able to endure such loading for their entire design life without failing.
These works can require mechanical structural engineering:
Aerospace structures.
Aerospace structure types include launch vehicles, (Atlas, Delta, Titan), missiles (ALCM, Harpoon), Hypersonic vehicles (Space Shuttle), military aircraft (F-16, F-18) and commercial aircraft (Boeing 777, MD-11). Aerospace structures typically consist of thin plates with stiffeners for the external surfaces, bulkheads and frames to support the shape and fasteners such as welds, rivets, screws and bolts to hold the components together.
Nanoscale structures.
A nanostructure is an object of intermediate size between molecular and microscopic (micrometer-sized) structures. In describing nanostructures it is necessary to differentiate between the number of dimensions on the nanoscale. Nanotextured surfaces have one dimension on the nanoscale, i.e., only the thickness of the surface of an object is between 0.1 and 100 nm. Nanotubes have two dimensions on the nanoscale, i.e., the diameter of the tube is between 0.1 and 100 nm; its length could be much greater. Finally, spherical nanoparticles have three dimensions on the nanoscale, i.e., the particle is between 0.1 and 100 nm in each spatial dimension. The terms nanoparticles and ultrafine particles (UFP) often are used synonymously although UFP can reach into the micrometre range. The term 'nanostructure' is often used when referring to magnetic technology.
Structural Engineering for Medical Science.
Medical equipment (also known as armamentarium) is designed to aid in the diagnosis, monitoring or treatment of medical conditions. There are several basic types: Diagnostic equipment includes medical imaging machines, used to aid in diagnosis ; equipment includes infusion pumps, medical lasers and LASIK surgical machines ; Medical monitors allow medical staff to measure a patient's medical state. Monitors may measure patient vital signs and other parameters including ECG, EEG, blood pressure, and dissolved gases in the blood ; Diagnostic Medical Equipment may also be used in the home for certain purposes, e.g. for the control of diabetes mellitus. A biomedical equipment technician (BMET) is a vital component of the healthcare delivery system. Employed primarily by hospitals, BMETs are the people responsible for maintaining a facility's medical equipment.
Structural elements.
Any structure is essentially made up of only a small number of different types of elements:
Many of these elements can be classified according to form (straight, plane / curve) and dimensionality (one-dimensional / two-dimensional):
Columns.
Columns are elements that carry only axial force - compression - or both axial force and bending (which is technically called a beam-column but practically, just a column). The design of a column must check the axial capacity of the element, and the buckling capacity.
The buckling capacity is the capacity of the element to withstand the propensity to buckle. Its capacity depends upon its geometry, material, and the effective length of the column, which depends upon the restraint conditions at the top and bottom of the column. The effective length is formula_1 where formula_2 is the real length of the column.
The capacity of a column to carry axial load depends on the degree of bending it is subjected to, and vice versa. This is represented on an interaction chart and is a complex non-linear relationship.
Beams.
A beam may be defined as an element in which one dimension is much greater than the other two and the applied loads are usually normal to the main axis of the element. Beams and columns are called line elements and are often represented by simple lines in structural modeling.
Beams are elements which carry pure bending only. Bending causes one part of the section of a beam (divided along its length) to go into compression and the other part into tension. The compression part must be designed to resist buckling and crushing, while the tension part must be able to adequately resist the tension.
Trusses.
A truss is a structure comprising two types of structural elements; compression members and tension members (i.e. struts and ties). Most trusses use gusset plates to connect intersecting elements. Gusset plates are relatively flexible and minimize bending moments at the connections, thus allowing the truss members to carry primarily tension or compression.
Trusses are usually utilised in large-span structures, where it would be uneconomical to use solid beams.
Plates.
Plates carry bending in two directions. A concrete flat slab is an example of a plate. Plates are understood by using continuum mechanics, but due to the complexity involved they are most often designed using a codified empirical approach, or computer analysis.
They can also be designed with yield line theory, where an assumed collapse mechanism is analysed to give an upper bound on the collapse load (see Plasticity). This technique is used in practice but because the method provides an upper-bound, i.e. an unsafe prediction of the collapse load, for poorly conceived collapse mechanisms great care is needed to ensure that the assumed collapse mechanism is realistic.
Shells.
Shells derive their strength from their form, and carry forces in compression in two directions. A dome is an example of a shell. They can be designed by making a hanging-chain model, which will act as a catenary in pure tension, and inverting the form to achieve pure compression.
Arches.
Arches carry forces in compression in one direction only, which is why it is appropriate to build arches out of masonry. They are designed by ensuring that the line of thrust of the force remains within the depth of the arch. It is mainly used to increase the bountifulness of any structure.
Catenaries.
Catenaries derive their strength from their form, and carry transverse forces in pure tension by deflecting (just as a tightrope will sag when someone walks on it). They are almost always cable or fabric structures. A fabric structure acts as a catenary in two directions.
Structural engineering theory.
Structural engineering depends upon a detailed knowledge of applied mechanics, materials science and applied mathematics to understand and predict how structures support and resist self-weight and imposed loads. To apply the knowledge successfully a structural engineer generally requires detailed knowledge of relevant empirical and theoretical design codes, the techniques of structural analysis, as well as some knowledge of the corrosion resistance of the materials and structures, especially when those structures are exposed to the external environment. Since the 1990s, specialist software has become available to aid in the design of structures, with the functionality to assist in the drawing, analyzing and designing of structures with maximum precision; examples include AutoCAD, StaadPro, ETABS, Prokon, Revit Structure etc. Such software may also take into consideration environmental loads, such as from earthquakes and winds.
Materials.
Structural engineering depends on the knowledge of materials and their properties, in order to understand how different materials support and resist loads.
Common structural materials are:

</doc>
<doc id="45831" url="http://en.wikipedia.org/wiki?curid=45831" title="Tetanus">
Tetanus

Tetanus, also known as lockjaw, is an infection characterized by muscle spasms. In the most common type the spasms begin in the jaw and then progress to the rest of the body. These spasms usually last a few minutes each time and occur frequently for three to four weeks. Spasms may be so severe that bone fractures may occur. Other symptoms may include: fever, headache, trouble swallowing, high blood pressure, and a fast heart rate. Onset of symptoms is typically three to twenty one days following infection. It may take months to recover. About 10% of those infected die.
Tetanus is caused by an infection with the bacterium "Clostridium tetani". The bacteria generally enter through a break in the skin such as a cut or puncture wound by a contaminated object. The bacteria are commonly found in soil, dust and manure. The bacteria produce toxins that interfere with muscle contractions, resulting in the typical symptoms. Diagnosis is based on the presenting signs and symptoms. The disease does not spread between people.
Infection can be prevented by proper immunization with the tetanus vaccine. In those who have a significant wound and less than three doses of the vaccine both immunization and tetanus immune globulin are recommended. In those who are infected tetanus immune globulin or if not available intravenous immunoglobulin (IVIG) is used. The wound should be cleaned and any dead tissue removed. Muscle relaxants may be used to control spasms. Mechanical ventilation may be required if a person's breathing is affected.
Tetanus occurs in all parts of the world but is most frequent in hot and wet climates where the soil contains a lot of organic matter. In 2013 it caused about 59,000 deaths – down from 356,000 in 1990. Description of the disease by Hippocrates exist from at least as far back as the 5th century BCE. The cause of the disease was determined in 1884 by Antonio Carle and Giorgio Rattone at the University of Turin with a vaccine being developed in 1924.
Signs and symptoms.
Tetanus often begins with mild spasms in the jaw muscles—also known as lockjaw or trismus. The spasms can also affect the facial muscles resulting in an appearance called risus sardonicus. Chest, neck, back, abdominal muscles, and buttocks may be affected. Back muscle spasms often cause arching, called opisthotonos. Sometimes the spasms affect muscles that help with breathing, which can lead to breathing problems.
Prolonged muscular action causes sudden, powerful, and painful contractions of muscle groups, which is called "tetany". These episodes can cause fractures and muscle tears. Other symptoms include drooling, excessive sweating, fever, hand or foot spasms, irritability, difficulty swallowing, and uncontrolled urination or defecation. The episodes can also cause destruction of elements of the nervous system through viral cell exchange.
Mortality rates reported vary from 48% to 73%. In recent years approximately 11% of reported tetanus cases have been fatal. The highest mortality rates are in unvaccinated people, people over 60 years of age or newborns.
Incubation period.
The incubation period of tetanus may be up to several months, but is usually about eight days. In general, the farther the injury site is from the central nervous system, the longer the incubation period. The shorter the incubation period, the more severe the symptoms. In neonatal tetanus, symptoms usually appear from 4 to 14 days after birth, averaging about 7 days. On the basis of clinical findings, four different forms of tetanus have been described.
Generalized tetanus.
This is the most common type of tetanus, representing about 80% of cases. The generalized form usually presents with a descending pattern. The first sign is trismus, or lockjaw, and the facial spasms called risus sardonicus, followed by stiffness of the neck, difficulty in swallowing, and rigidity of pectoral and calf muscles. Other symptoms include elevated temperature, sweating, elevated blood pressure, and episodic rapid heart rate. Spasms may occur frequently and last for several minutes with the body shaped into a characteristic form called opisthotonos. Spasms continue for up to four weeks, and complete recovery may take months.
Sympathetic overactivity (SOA) is common in severe tetanus and manifests as labile hypertension, tachycardia, dysrhythmia, peripheral vasculature constriction, profuse sweating, fever, increased carbon dioxide output, increased catecholamine excretion and late development of hypotension.
Death can occur within four days.
Neonatal tetanus.
Neonatal tetanus is a form of generalized tetanus that occurs in newborns, usually those born to mothers who themselves have not been vaccinated. If the mother has been vaccinated against tetanus, the infants acquire passive immunity and are thus protected. It usually occurs through infection of the unhealed umbilical stump, particularly when the stump is cut with a non-sterile instrument. As of 1998 neonatal tetanus was common in many developing countries and was responsible for about 14% (215,000) of all neonatal deaths In 2010 the worldwide death toll was 58,000 newborns. As the result of a public health campaign, the death toll from neonatal tetanus was reduced by 90% between 1990 and 2010, and by 2013 the disease had been largely eliminated from all but 25 countries. Neonatal tetanus is rare in developed countries.
Local tetanus.
This is an uncommon form of the disease, in which patients have persistent contraction of muscles in the same anatomic area as the injury. The contractions may persist for many weeks before gradually subsiding. Local tetanus is generally milder; only about 1% of cases are fatal, but it may precede the onset of generalized tetanus.
Cephalic tetanus.
This is a rare form of the disease, occasionally occurring with otitis media (ear infections) in which "C. tetani" is present in the flora of the middle ear, or following injuries to the head. There is involvement of the cranial nerves, especially in the facial area.
Cause.
Tetanus is caused by the tetanus bacterium "Clostridium tetani". Tetanus is often associated with rust, especially rusty nails. Objects that accumulate rust are often found outdoors, or in places that harbour anaerobic bacteria, but the rust itself does not cause tetanus nor does it contain more "C. tetani" bacteria. The rough surface of rusty metal merely provides a prime habitat for "C. tetani" endospores to reside in, and the nail affords a means to puncture skin and deliver endospores deep within the body at the site of the wound.
An endospore is a non-metabolizing survival structure that begins to metabolize and cause infection once in an adequate environment. Because "C. tetani" is an anaerobic bacterium, it and its endospores thrive in environments that lack oxygen. Hence, stepping on a nail (rusty or not) may result in a tetanus infection, as the low-oxygen (anaerobic) environment is caused by the oxidization of the same object that causes a puncture wound, delivering endospores to a suitable environment for growth.
Tetanus is an international health problem, as "C. tetani" spores are ubiquitous. The disease occurs almost exclusively in persons unvaccinated or inadequately immunized. It is more common in hot, damp climates with soil rich in organic matter. This is particularly true with manure-treated soils, as the spores are widely distributed in the intestines and feces of many animals such as horses, sheep, cattle, dogs, cats, rats, guinea pigs, and chickens. Spores can be introduced into the body through puncture wounds. In agricultural areas, a significant number of human adults may harbor the organism. The spores can also be found on skin surfaces and in contaminated heroin. Heroin users, particularly those that inject the drug subcutaneously, appear to be at high risk of contracting tetanus.
Pathophysiology.
Tetanus affects skeletal muscle, a type of striated muscle used in voluntary movement. The other type of striated muscle, cardiac, or heart muscle, cannot be tetanized because of its intrinsic electrical properties.
The tetanus toxin initially binds to peripheral nerve terminals. It is transported within the axon and across synaptic junctions until it reaches the central nervous system. There it becomes rapidly fixed to gangliosides at the presynaptic inhibitory motor nerve endings, and is taken up into the axon by endocytosis. The effect of the toxin is to block the release of inhibitory neurotransmitters glycine and gamma-aminobutyric acid (GABA) across the synaptic cleft, which is required to check the nervous impulse. If nervous impulses cannot be checked by normal inhibitory mechanisms, the generalized muscular spasms characteristic of tetanus are produced. The toxin appears to act by selective cleavage of a protein component of synaptic vesicles, synaptobrevin II, and this prevents the release of neurotransmitters by the cells.
Diagnosis.
There are currently no blood tests for diagnosing tetanus. The diagnosis is based on the presentation of tetanus symptoms and does not depend upon isolation of the bacterium, which is recovered from the wound in only 30% of cases and can be isolated from patients without tetanus. Laboratory identification of "C. tetani" can be demonstrated only by production of tetanospasmin in mice.
The "spatula test" is a clinical test for tetanus that involves touching the posterior pharyngeal wall with a soft-tipped instrument and observing the effect. A positive test result is the involuntary contraction of the jaw (biting down on the "spatula") and a negative test result would normally be a gag reflex attempting to expel the foreign object. A short report in "The American Journal of Tropical Medicine and Hygiene" states that, in a patient research study, the spatula test had a high specificity (zero false-positive test results) and a high sensitivity (94% of infected patients produced a positive test).
Prevention.
Unlike many infectious diseases, recovery from naturally acquired tetanus does not usually result in immunity to tetanus. This is due to the extreme potency of the tetanospasmin toxin. Even a lethal dose of tetanospasmin is insufficient to provoke an immune response.
Tetanus can be prevented by vaccination with tetanus toxoid. The CDC recommends that adults receive a booster vaccine every ten years, and standard care practice in many places is to give the booster to any patient with a puncture wound who is uncertain of when he or she was last vaccinated, or if he or she has had fewer than three lifetime doses of the vaccine. The booster may not prevent a potentially fatal case of tetanus from the current wound, however, as it can take up to two weeks for tetanus antibodies to form.
In children under the age of seven, the tetanus vaccine is often administered as a combined vaccine, DPT/DTaP vaccine, which also includes vaccines against diphtheria and pertussis. For adults and children over seven, the Td vaccine (tetanus and diphtheria) or Tdap (tetanus, diphtheria, and acellular pertussis) is commonly used.
The World Health Organisation certifies countries as having eliminated maternal or neonatal tetanus. Certification requires at least two years of rates of less than 1 case per 1000 live births. In 1998 in Uganda, 3,433 tetanus cases were recorded in newborn babies; of these, 2,403 died. After a major public health effort, Uganda in 2011 was certified as having eliminated tetanus.
Post-exposure prophylaxis.
Tetanus toxoid can be given in case of a suspected exposure to tetanus. In such cases, it can be given with or without tetanus immunoglobulin (also called "tetanus antibodies" or "tetanus antitoxin") It can be given as intravenous therapy or by intramuscular injection.
The guidelines for such events in the United States for non-pregnant people 11 years and older are as follows:
Treatment.
Mild tetanus.
Mild cases of tetanus can be treated with: 
Severe tetanus.
Severe cases will require admission to intensive care. In addition to the measures listed above for mild tetanus:
Drugs such as diazepam or other muscle relaxants can be given to control the muscle spasms. In extreme cases it may be necessary to paralyze the patient with curare-like drugs and use a mechanical ventilator.
In order to survive a tetanus infection, the maintenance of an airway and proper nutrition are required. An intake of 3,500 to 4,000 calories, and at least 150 g of protein per day, is often given in liquid form through a tube directly into the stomach (percutaneous endoscopic gastrostomy), or through a drip into a vein (parenteral nutrition). This high-caloric diet maintenance is required because of the increased metabolic strain brought on by the increased muscle activity. Full recovery takes 4 to 6 weeks because the body must regenerate destroyed nerve axon terminals.
Epidemiology.
In 2013 it caused about 59,000 deaths – down from 356,000 in 1990. Tetanus – in particular, the neonatal form – remains a significant public health problem in non-industrialized countries with 59,000 newborns worldwide dying in 2008 as a result of neonatal tetanus. In the United States, from 2000 through 2007 an average of 31 cases were reported per year. Nearly all of the cases in the United States occur in unimmunized individuals or individuals who have allowed their inoculations to lapse.
History.
Tetanus was well known to ancient people who recognized the relationship between wounds and fatal muscle spasms. In 1884, Arthur Nicolaier isolated the strychnine-like toxin of tetanus from free-living, anaerobic soil bacteria. The etiology of the disease was further elucidated in 1884 by Antonio Carle and Giorgio Rattone, two pathologists of the University of Turin, who demonstrated the transmissibility of tetanus for the first time. They produced tetanus in rabbits by injecting pus from a patient with fatal tetanus into their sciatic nerves.
In 1891, "C. tetani" was isolated from a human victim by Kitasato Shibasaburō, who later showed that the organism could produce disease when injected into animals, and that the toxin could be neutralized by specific antibodies. In 1897, Edmond Nocard showed that tetanus antitoxin induced passive immunity in humans, and could be used for prophylaxis and treatment. Tetanus toxoid vaccine was developed by P. Descombey in 1924, and was widely used to prevent tetanus induced by battle wounds during World War II.
Etymology.
The word tetanus comes from the Ancient Greek: τέτανος "tetanos" "taut", which is further from the Ancient Greek: τείνειν "teinein" "to stretch".

</doc>
<doc id="45832" url="http://en.wikipedia.org/wiki?curid=45832" title="Gyrocompass">
Gyrocompass

A gyrocompass is a type of non-magnetic compass which is based on a fast-spinning disc and rotation of the Earth (or another planetary body if used elsewhere in the universe) to automatically find geographical direction. Although one important component of a gyrocompass is a gyroscope, these are not the same devices; a gyrocompass is built to use the effect of gyroscopic precession, which is a distinctive aspect of the general gyroscopic effect. Gyrocompasses are widely used for navigation on ships, because they have two significant advantages over magnetic compasses:
Operation.
A gyroscope, not to be confused with gyrocompass, is a spinning wheel mounted on gimbal so that the wheel's axis is free to orient itself in any way. When it is spun up to speed with its axis pointing in some direction, due to the law of conservation of angular momentum, such a wheel will normally maintain its original orientation to a fixed point in outer space (not to a fixed point on Earth). Since our planet rotates, it appears to a stationary observer on Earth that a gyroscope's axis is completing a full rotation once every 24 hours. Such a rotating gyroscope is used for navigation in some cases, for example on aircraft, where it is known as heading indicator, but cannot ordinarily be used for long-term marine navigation. The crucial additional ingredient needed to turn a gyroscope into a gyrocompass, so it would automatically position to true north, is some mechanism that results in an application of torque whenever the compass's axis is not pointing north.
One method uses friction to apply the needed torque: the gyroscope in a gyrocompass is not completely free to reorient itself; if for instance a device connected to the axis is immersed in a viscous fluid, then that fluid will resist reorientation of the axis. This friction force caused by the fluid results in a torque acting on the axis, causing the axis to turn in a direction orthogonal to the torque (that is, to precess) along a line of longitude. Once the axis points toward the celestial pole, it will appear to be stationary and won't experience any more frictional forces. This is because true north is the only direction for which the gyroscope can remain on the surface of the earth and not be required to change. This axis orientation is considered to be a point of minimum potential energy.
Another, more practical, method is to use weights to force the axis of the compass to remain horizontal (perpendicular to the direction of the center of the Earth), but otherwise allow it to rotate freely within the horizontal plane. In this case, gravity will apply a torque forcing the compass's axis toward true north. Because the weights will confine the compass's axis to be horizontal with respect to the Earth's surface, the axis can never align with the Earth's axis (except on the Equator) and must realign itself as the Earth rotates. But with respect to the Earth's surface, the compass will appear to be stationary and pointing along the Earth's surface toward the true North Pole.
Since the gyrocompass's north-seeking function depends on the rotation around the axis of the Earth that causes torque-induced gyroscopic precession, it will not orient itself correctly to true north if it is moved very fast in an east to west direction, thus negating the Earth's rotation. However, aircraft commonly use heading indicators or directional gyros, which are not gyrocompasses and do not position themselves to north via precession, but are periodically aligned manually to true north.
Mathematical model of a gyrocompass.
We will consider here a gyrocompass, as a gyroscope which is free to rotate about one of its symmetry axis, and the whole rotating gyroscope is also free to rotate on the horizontal plane, about the local vertical, the zenith. Therefore there are two independent local rotations. In addition to these rotations we will also consider the rotation of the Earth about its North-South (NS) axis, and we will model the planet as a perfect sphere. We will neglect friction and the rotation of the Earth about the Sun.
In this case a non-rotating observer located at the center of the Earth can be approximated as being an inertial frame. We can set cartesian coordinates formula_1 for such an observer (that we will name as 1-O), and the barycenter of the gyroscope will be located at a distance formula_2 from the center of the Earth.
First time-dependent rotation.
Let us consider another (non-inertial) observer (the 2-O) located at the center of the Earth but rotating about the NS-axis by formula_3, then we set coordinates attached to the observer as
so that the unit formula_5 versor formula_6 is mapped to the point formula_7. For the 2-O the Earth is not moving so as the barycenter of the gyroscope. The rotation of 2-O, according to 1-O, is performed with angular velocity formula_8. We will suppose that the formula_9 axis denotes points with zero longitude.
Second and third fixed rotations.
We will now rotate about the formula_10 axis, so that the formula_11-axis will have the longitude of the barycenter. In this case we have
With the next rotation (about the axis formula_13 of an angle formula_14, the co-latitude) we will bring the formula_15 axis along the local zenith (formula_16-axis) of the barycenter. This can be achieved by the following orthogonal matrix (with unit determinant)
so that the formula_18 versor formula_19 is mapped to the point formula_20.
Constant translation.
We now choose another coordinate basis whose origin is located at the barycenter of the gyroscope. This can be performed by the following translation along the zenith axis
so that the origin of the new system, formula_22 is located at the point formula_23, and formula_2 is the radius of the Earth. Now the formula_25-axis points towards the south direction.
Fourth time-dependent rotation.
Now we rotate about the zenith formula_26-axis so that the new coordinate system is attached to the structure of the gyroscope, so that for an observer at rest in this coordinate system, the gyrocompass is only rotating about its own axis of symmetry. In this case we find
The axis of symmetry of the gyrocompass is now along the formula_28-axis.
Last time-dependent rotation.
The last rotation is a rotation on the axis of symmetry of the gyroscope as in
Dynamics of the system.
Since the gyroscope is not moving the height of its barycenter (and the origin of the coordinate system is located at this same point), its gravitational potential energy is constant. Therefore its Lagrangian formula_30 corresponds to its kinetic energy formula_31 only. We have 
where formula_33 is the mass of the gyroscope, formula_34 is the squared inertial speed of the origin of the coordinates of the final coordinate system (i.e. the center of mass). This constant term does not affect the dynamics of the gyroscope and it can be neglected. On the other hand, the tensor of inertia is given by
and
Therefore we find
The Lagrangian can be rewritten as
where
is the part of the Lagrangian responsible for the dynamics of the system. Then, since formula_40, we find
Since the angular momentum formula_42 of the gyrocompass is given by formula_43, we see that the constant formula_44, is the component of the angular momentum about the axis of symmetry. Furthermore, we find the equation of motion for the variable formula_45 as
or
Particular case: the poles.
At the poles we find formula_48, and the equations of motion become
This simple solution implies that the gyroscope is uniformly rotating with constant angular velocity in both the vertical and symmetrical axis.
The general and physically relevant case.
Let us suppose, now that formula_50, and that formula_51, that is the axis of the gyroscope is approximately along the North-South line, and let us find the parameter space (if it exists), for which the system admits stable small oscillations about this same line. If this situation occurs, the gyroscope will be always approximately aligned along the North-South line, giving direction. In this case we find
Let us consider the case that
and, further, we allow for fast gyro-rotations, that is 
Therefore, for fast spinning rotations, formula_55 implies formula_56. In this case, the equations of motion further simplify to
Therefore we find small oscillations about the North-South line, as formula_58, where the angular velocity of this harmonic motion of the axis of symmetry of the gyrocompass about the North-South line is given by
which corresponds to a period for the oscillations given by
Therefore formula_61 is proportional to the geometric mean of the Earth and spinning angular velocities. In order to have small oscillations we have required formula_62, so that the North is located along the right-hand-rule direction of the spinning axis, that is along the negative direction of the formula_63-axis, the axis of symmetry. As a side result, on measuring formula_64 (and knowing formula_65), one can deduce the local colatitude formula_66.
History.
The first, not yet practical, form of gyrocompass was patented in 1885 by Marinus Gerardus van den Bos. Usable gyrocompass was invented in 1906 in Germany by Hermann Anschütz-Kaempfe, and after successful tests in 1908 became widely used in German Imperial Navy.
The gyrocompass was an important invention for nautical navigation because it allowed accurate determination of a vessel’s location at all times regardless of the vessel’s motion, the weather and the amount of steel used in the construction of the ship. In the United States, Elmer Ambrose Sperry produced a workable gyrocompass system (1908: patent #1,242,065), and founded the Sperry Gyroscope Company. The unit was adopted by the U.S. Navy (1911), and played a major role in World War I. The Navy also began using Sperry's "Metal Mike": the first gyroscope-guided autopilot steering system. In the following decades, these and other Sperry devices were adopted by steamships such as the RMS Queen Mary, airplanes, and the warships of World War II. After his death in 1930, the Navy named the USS Sperry after him.
Meanwhile, in 1913, C. Plath (a Hamburg, Germany-based manufacturer of navigational equipment including sextants and magnetic compasses) developed the first gyrocompass to be installed on a commercial vessel. C. Plath sold many gyrocompasses to the Weems’ School for Navigation in Annapolis, MD, and soon the founders of each organization formed an alliance and became Weems & Plath.
Before the success of gyrocompass, several attempts had been made in Europe to use gyroscope instead. By 1880, William Thomson (lord Kelvin) tried to propose a gyrostat (tope) to the British Navy. In 1889, Arthur Krebs adapted an electric motor to the Dumoulin-Froment marine gyroscope, for the French Navy. Giving the "Gymnote" submarine the ability to keep a straight line under water during several hours, it allowed her to in 1890.
Errors.
A gyrocompass is subject to certain errors. These include streaming error, where rapid changes in course, speed and latitude cause deviation before the gyro can adjust itself. On most modern ships the GPS or other navigational aids feed data to the gyrocompass allowing a small computer to apply a correction. 
Alternatively a design based on a strapdown architecture (including a triad of fibre optic gyroscope, ring laser gyroscopes or Hemispherical resonator gyroscope and a triad of accelerometers) will eliminate these errors, as they do not depend upon mechanical parts, to determinate rate of rotation.

</doc>
<doc id="45833" url="http://en.wikipedia.org/wiki?curid=45833" title="Palatino">
Palatino

Palatino is the name of a large typeface family that began as an old style serif typeface designed by Hermann Zapf initially released in 1948 by the Linotype foundry.
Palatino is one of the standard PostScript fonts, hence in 1985 was included by Apple Computer in the LaserWriter, making it very popular in the early days of desktop publishing. In 1999, Zapf revised Palatino for Linotype and Microsoft, called Palatino Linotype. The revised family incorporated extended Latin, Greek, and Cyrillic character sets.
Under the collaboration of Zapf and Akira Kobayashi, the Palatino typeface family was expanded. Linotype released the Palatino nova, Palatino Sans, and Palatino Sans Informal families, expanding the Palatino typeface families to include humanist sans-serif typefaces. Palatino nova was released in 2005, while the others were released in 2006.
Named after 16th century Italian master of calligraphy Giambattista Palatino, Palatino is based on the humanist fonts of the Italian Renaissance, which mirror the letters formed by a broad nib pen; this gives a calligraphic grace. But where the Renaissance faces tend to use smaller letters with longer vertical lines (ascenders and descenders) with lighter strokes, Palatino has larger proportions, and is considered to be a much easier to read typeface.
It remains one of the most widely used (and copied) text typefaces, has been adapted to virtually every type of technology, and is one of the ten most used serif typefaces. It is one of several related typefaces by Zapf, each showing influence of the Italian Renaissance letter forms. The group includes Palatine, Sistina, Michaelangelo Titling, and Aldus, which takes inspiration from printing types cut by Francesco Griffo c. 1495 in the print shop of Aldus Manutius.
Palatino Linotype.
Palatino Linotype is a version of the Palatino family that incorporates extended Latin, Greek, Cyrillic characters, as well as currency signs, subscripts and superscripts, and fractions. The family includes roman and italic in text and bold weights. It is one of the few fonts to incorporate an interrobang.
It was the first western OpenType font that Microsoft shipped; Palatino Linotype was bundled with Windows 2000. The OpenType version showcased some (then new) OpenType features, including "set ligatures, true small caps, different numeral styles, and a variety of special alternate characters, such as the swash Capital Qu combination". This marks it out from earlier digitisations such as the OS X system version, which do not include ligatures such as "Th" and "Qu".
Palatino nova.
Palatino nova is a redesigned version of Palatino, by Hermann Zapf and Akira Kobayashi. This Palatino nova typeface family includes roman and italics in the light, text, medium, and bold weights, a titling face formerly called Michelangelo Titling, and a large and small capital face called Palatino nova Imperial formerly called Sistina.
Palatino nova has reduced support on extended Latin, Greek, Cyrillic characters. In particular, Greek and Cyrillic is only available in Regular and Bold weight fonts. However, extended accented Latin characters, ligatures, small letter forms, symbols are available in Private Use Area block. Palatino nova Titling replaces lowercase characters with true small capitals, and the supports for Greek Extended and Cyrillic characters are reduced.
The font family was premiered on 2005-11-24, the same day as Hermann Zapf’s 87th birthday celebration.
Palatino Sans.
In Palatino Sans, the specimens shown in the preannouncement resemble Optima but have a softer, more organic feel. Unlike the serifed counterpart, the Sans families do not have full Greek or Cyrillic characters.
Palatino Sans Informal.
Palatino Sans Informal incorporates informal characteristics to the Palatino Sans, such as asymmetrical A, K, N, W, X, Y, w.
Palatino Arabic.
It is a family designed by Lebanese designer Nadine Chahine and Hermann Zapf. The design is based on the Al-Ahram typeface designed by Zapf in 1956 but reworked and modified to fit the Palatino nova family. The design is Naskh in style but with a strong influence of Thuluth style.
This family only comes in 1 font, corresponding to Palatino nova Regular. It supports basic Latin, Arabic, Persian, and Urdu scripts. It also includes proportional and tabular numerals for the supported languages.
Palatino eText (2013).
It is a family designed by Toshi Omagari of Monotype Imaging, optimised for on-screen use. It includes a larger x-height and wider spacing.
This family includes 4 fonts in 2 weights, with complimentary italics. OpenType features include case-sensitive forms, fractions, ligatures, lining/old style figures, localized forms, ordinals, superscript, small capitals.
Availability.
Palatino has been available in all major typesetting systems over the years, including Linotype and "hot metal" versions. Certain hot metal versions of Palatino, of smaller x-height, are considered both more legible and elegant to many people.
The first digitisation was done by Bitstream, which, however, refused to licence the name "Palatino" from Linotype, calling its version "Zapf Calligraphic" instead. Since then, several foundries have produced Zapf-original versions.
Digital type foundries selling authentic versions of Palatino and derivative families are:
Due to Linotype's trademark on the name "Palatino", other foundries use other names for their version of Zapf's typeface:
It should be noted that all these typefaces are authentic work by Zapf himself, whether or not they use the name "Palatino" due to Linotype's trademark.
Free and Open Source versions and derivatives.
The only legal free version of the typeface is "URW Palladio L." The open source community greatly extended the character sets of the fonts and releases new, updated versions under new names.
Book Antiqua.
Microsoft distributes a similar typeface, Book Antiqua (originally by Monotype), which is considered by Zapf to be an imitation. Book Antiqua was designed as an alternative to licensing the fonts mandated by Adobe's PostScript standard. Both Book Antiqua and Arial (the alternative for Helvetica) share the original typefaces' character width, spacing and kerning properties. However, Book Antiqua resembles Palatino much more than Arial does Helvetica; indeed, the two are quite difficult to tell apart. However, discernible differences exist in the following characters:
In 1993, Zapf resigned from l'Association Typographique Internationale (ATypI) over what he viewed as its hypocritical attitude toward unauthorized copying by prominent ATypI members (namely Monotype). In the United States, the abstract design of a typeface is not protected by copyright, and can be imitated freely (unless the typeface is protected by a design patent, which is of much more limited duration and rarely applied for). Copyright protection is available for the representation of a typeface in software (a computer font), and the names of typefaces can be protected by trademark.
Microsoft has since licensed and distributes Linotype's version of Zapf's original design called "Palatino Linotype" in all versions of Windows since Windows 2000.
Variants and similar typefaces.
Zapf also designed Aldus, which appeared in the D. Stempel AG catalog in 1954. Both Aldus and Palatino were Zapf’s new form of old style typefaces inspired by the Renaissance. Originally intended as the book or text weight for Zapf's Palatino font family, it was instead released as a separate family.
"Zapf Renaissance Antiqua" was a newer interpretation by Zapf of the same general design, distributed by Mannesmann Scangraphic. They also distribute alternate digitisations of Palatino, called "Parlament" and "Praxis."
Awards.
Palatino Sans and Palatino Sans Informal won Type Directors Club Type Design Competition 2007 award under Type System / Superfamily category.
Palatino Arabic won 2008 Type Directors Club TDC2 2008 award under Text / Type Family category.

</doc>
<doc id="45834" url="http://en.wikipedia.org/wiki?curid=45834" title="Optima">
Optima

Optima is a humanist sans-serif typeface designed by Hermann Zapf between 1952 and 1955 for the D. Stempel AG foundry, Frankfurt, Germany.
Characteristics.
Though classified as a sans-serif, Optima has a subtle swelling at the terminals suggesting a glyphic serif. Optima’s design follows humanist lines; its italic is merely an oblique variant typical of realist sans-serif typefaces such as Helvetica and Univers. Also unconventional for a contemporary sans, Optima’s capitals (like those of Palatino, Hans Eduard Meier’s Syntax and Carol Twombly's Trajan) are directly derived from the classic Roman monumental capital model, reflecting a reverence for Roman capitals as an ideal form.
In the Bitstream font collection, Zapf Humanist 601 is provided as an Optima clone. Other Optima clones include Optane from the WSI Fonts collection, Opulent by Rubicon Computer Labs Inc., Ottawa from Corel, CG Omega and Eterna. Freely available implementations include MgOpen Cosmetica (Available with open source license) and URW Classico (available with URW Font package from Ghostscript). Linux Biolinum is a libre font inspired by it.
Optima Greek (1973).
It is a Greek variant designed by Matthew Carter, based on sketches from Hermann Zapf. Digital version has not been produced.
Optima Classified (1976).
It is a variant designed by Matthew Carter, based closely on Optima Medium. Digital version has not been produced.
Optima nova (2002).
Optima nova is a redesign of the original font family, designed by Hermann Zapf and Linotype GmbH type director Akira Kobayashi. The new family contains 7 font weights, which adds light, demi, heavy font weights, but removed extra black weight. Medium weight is readjusted to between medium and bold weights in the old family scale. Glyph sets are expanded to include Adobe CE and Latin Extended characters, with light to bold weight fonts supporting proportional lining figures, old style figures, small caps. Italic fonts include italic type features instead of just tilted romans. Even in roman fonts, letters such as Q, a, f are redesigned. The overall bounding boxes were widened in Optima nova.
Optima nova Condensed.
It is a condensed variant is which consist of light to bold weights, but no italic fonts. Glyph set does not support proportional lining figures, old style figures, small caps.
Optima nova Titling.
It is a titling capitals variant, which contains only capital letters, with restyled letterform. Glyph set is same as Optima nova Condensed, except it also includes extra ligatures.
In the tradition of hand lettering and lapidary inscription, the titling face shares similarities with the work of Zapf's friend Herb Lubalin especially the exuberant ligatures (for which Lubalin's ITC Lubalin Graph and ITC Avant Garde are notable). Further influence of A.M. Cassandre and Rudolf Koch, whose work greatly inspired the young Zapf, can also be seen in Optima.
Optima Pro Cyrillic (2010).
In April 2010, Linotype announced the release of Cyrillic version of the original Optima family, in OpenType Pro font formats. Released fonts include Optima Pro Cyrillic Roman, Oblique, Bold, Bold Oblique.
Usages.
The typeface Optima is used for the Vietnam Veterans Memorial and was used by the 2008 John McCain presidential campaign. Optima is also used as the official branding typeface for Estée Lauder Companies, the University of Calgary, by Aston Martin.
Optima is used iconically for "Traveller", and "Diaspora" used it to pay homage to "Traveller".
Optima was used in the end credits of the 1973 horror movie "The Exorcist" as well as for the opening titles of its second sequel, "The Exorcist III".
Optima was used in the official logo and most publications associated with Expo 67 in Montreal.
Optima was used for lettering on Premier League kits from July 1997 until May 2007, when it was replaced by a different typeface.
Optima was used 1986–1996 for the logo of HFT Heribert's Fishing Tours (Germany and Canada).
Optima was used in the Taipei Metro.
Optima was used as the original fonts used on The Smiths original 7-inch single covers and their debut album.
Optima was used for the logo of American emo band Moss Icon, albeit slightly weathered.
Optima was used for the logo of Trans TV from December 2001 until December 2013.
Marks and Spencer used the font for its corporate logo and as the default on all internal correspondence from 2000 but since 2007 it is gradually being phased out on all signage and packaging as part of another re-branding exercise.
Optima was chosen as the font to be used for the names of those who lost their lives in the September 11 attacks, carved into bronze parapets, at the National September 11 Memorial & Museum, which is named "Reflecting Absence".
The Optima font is used in the logo of the Indian Premier League.

</doc>
<doc id="45835" url="http://en.wikipedia.org/wiki?curid=45835" title="Hermann Zapf">
Hermann Zapf

Hermann Zapf (born November 8, 1918) is a German typeface designer who lives in Darmstadt, Germany. He is married to calligrapher and typeface designer Gudrun Zapf von Hesse.
Zapf's work, which includes Palatino and Optima, has been widely copied, often against his will. The best known example may be Monotype's Book Antiqua, which shipped with Microsoft Office and was widely considered a "knockoff" of Palatino. In 1993, Zapf resigned from ATypI (Association Typographique Internationale) over what he viewed as its hypocritical attitude toward unauthorized copying by prominent ATypI members.
Early life.
Hermann Zapf was born in Nuremberg during turbulent times marked by the German Revolution of 1918–1919 in Munich and Berlin, the end of World War I, the exile of Kaiser Wilhelm, and the establishment of Bavaria as a free state by Kurt Eisner. In addition, the Spanish Flu Pandemic took hold of Europe in 1918 and 1919 and killed two of Zapf's siblings. Famine later struck Germany, and Zapf's mother was grateful to send him to school in 1925, where he received daily meals in a program organized by Herbert Hoover. In school, Zapf was mainly interested in technical subjects. One of his favorite books was the annual science journal "Das neue Universum" ("The New Universe"). He and his older brother experimented with electricity, building a crystal radio and an alarm system for his house. Even at his early age, Zapf was already getting involved with type, inventing ciphertext alphabets to exchange secret messages with his brother.
Zapf left school in 1933 with the ambition to pursue a career in electrical engineering. Unfortunately, his father had become unemployed. Zapf's father experienced trouble with the newly established Third Reich, having been involved with trade unions, and was sent to the Dachau concentration camp for a short time.
Introduction to typography.
Zapf was not able to attend the Ohm Technical Institute in Nuremberg, due to the new political regime. Therefore, he needed to find an apprenticeship. His teachers, aware of the new political difficulties, noticed Zapf's skill in drawing and suggested that he become a lithographer. Each company that interviewed him for an apprenticeship would ask him political questions, and every time he was interviewed, he was complimented on his work but was rejected. Ten months later, in 1934, he was interviewed by the last company in the telephone directory, and the company did not ask any political questions. They also complimented Zapf's work, but did not do lithography and did not need an apprentice lithographer. However, they allowed him to become a retoucher, and Zapf began his four-year apprenticeship in February 1934.
In 1935, Zapf attended an exhibition in Nuremberg in honor of the late typographer Rudolf Koch. This exhibition gave him his first interest in lettering. Zapf bought two books there, using them to teach himself calligraphy. He also studied examples of calligraphy in the Nuremberg city library. Soon, his master noticed his expertise in calligraphy, and Zapf's work shifted to lettering retouching and improvement of his colleagues' retouching work.
Frankfurt.
A few days after finishing his apprenticeship, Zapf left for Frankfurt. He did not bear a journeyman's certificate and thus would not be able to get a work permit at another company in Nuremberg, as they would not have been able to check on his qualifications. Zapf went to the "Werkstatt Haus zum Fürsteneck", a building run by Paul Koch, son of Rudolf Koch. He spent most of his time there working in typography and writing songbooks.
Through print historian Gustav Mori, Zapf came into contact with the type foundries D. Stempel, AG, and Linotype GmbH of Frankfurt. In 1938, he designed his first printed typeface for them, a fraktur type called "Gilgengart".
War career.
On April 1, 1939, Zapf was conscripted and sent to Pirmasens to help reinforce the Siegfried Line against France. Not used to the hard labor, he developed heart trouble in a few weeks and was given a desk job, writing camp records and sports certificates in Fraktur.
World War II broke out in September, and Zapf's unit was to be taken into the Wehrmacht. However, due to his heart trouble, Zapf was not transferred to the Wehrmacht but was instead dismissed. But on April 1, 1942, he was summoned again for the war effort. Zapf had been chosen for the Luftwaffe, but instead was sent to the artillery in Weimar. He did not perform well, confusing left and right during training and being too cautious and clumsy with his gun. His officers soon brought an unusually early end to his career in the artillery.
Zapf was sent back to the office, and then to Jüterbog to train as a cartographer. After that, he went to Dijon and then Bordeaux, joining the staff of the First Army. In the cartography unit at Bordeaux, Zapf drew maps of Spain, especially the railway system, which could have been used to transport artillery had Francisco Franco not used narrow-gauge tracks to repair bridges after the Spanish Civil War. Zapf was happy in the cartography unit. His eyesight was so excellent that he could write letters 1 millimeter in size without using a magnifying glass, and this skill probably prevented him from being commissioned back into the army.
After the war had ended, Zapf was held by the French as a prisoner of war at a field hospital in Tübingen. He was treated with respect because of his artwork and, due to his poor health, was sent home only four weeks after the end of the war. He went back to Nuremberg, which had suffered great damage because of the air raids.
Post-war.
Zapf taught calligraphy in Nuremberg in 1946. He went back to Frankfurt in 1947, where the type foundry Stempel offered him a position as artistic head of their printshop. They did not ask for qualifications, certificates, or references, but instead only required him to show them his sketchbooks from the war, and a calligraphic piece he did in 1944 of Hans von Weber's "Junggesellentext".
One of Zapf's products was a publication named "Feder und Stichel" ("Pen and Graver"), printed from metal plates designed by Zapf and cut by punch cutter August Rosenberger during the war. It was printed at the Stempel printshop in 1949.
From 1948 to 1950, Zapf taught calligraphy at the Arts and Crafts School in Offenbach, giving lettering lessons twice a week to two classes of graphics students. In 1951 he married Gudrun von Hesse, who taught at the school of Städel in Frankfurt.
Most of Zapf's work as a graphic artist was in book design. He worked for various publishing houses, including Suhrkamp Verlag, Insel Verlag, Büchergilde Gutenberg, Hanser Verlag, Dr. Ludwig Reichert Verlag, and Verlag Philipp von Zabern.
Type design.
Zapf designed types for various stages of printing technology, including hot metal composition, phototypesetting (also called "cold type"), and finally digital typography for use in desktop publishing. His two most famous typefaces, "Palatino" and "Optima", were designed in 1948 and 1952, respectively. "Palatino" was designed in conjunction with August Rosenberger, with careful attention to detail. It was named after 16th century Italian writing master Giambattista Palatino. "Optima", a flared sans-serif, was released by Stempel in 1958. Zapf disliked its name, which was invented by the marketers at Stempel.
Though his calligraphy is considered superb by calligraphers, Zapf has not been asked to do much in that field. His largest calligraphic project was to write out the Preamble to the United Nations Charter in four languages, commissioned by the Pierpont Morgan Library in 1960, for which he received $1000.
Computer typography.
Zapf has been working on typography in computer programs since the 1960s. His ideas were considered radical, not taken seriously in Germany, and rejected by the Darmstadt University of Technology, where Zapf lectured between 1972 and 1981. Because he had no success in Germany, Zapf went to the United States. He lectured about his ideas in computerized typesetting, and was invited to speak at Harvard University in 1964. The University of Texas at Austin was also interested in Zapf, and offered him a professorship, which he did not take, because his wife opposed a move to that state.
Because Zapf's plans for the United States had come to nothing, and because their house in Frankfurt had become too small, Zapf and his wife moved to Darmstadt in 1972.
In 1976, the Rochester Institute of Technology offered Zapf a professorship in typographic computer programming, the first of its type in the world. He taught there from 1977 to 1987, flying between Darmstadt and Rochester. There he developed his ideas further, with the help of his connections in companies such as IBM and Xerox, and his discussions with the computer specialists at Rochester. A number of Zapf's students from this time at RIT went on to become influential type designers, including Charles Bigelow and Kris Holmes, who together created the Lucida type family. Other prominent students include calligrapher/font designer Julian Waters and book designer Jerry Kelly.
In 1977, Zapf and his friends Aaron Burns and Herb Lubalin founded a company called "Design Processing International, Inc." in New York and developed typographical computer software. It existed until 1986 with the death of Lubalin, and Zapf and Burns founded "Zapf, Burns & Company" in 1987. Burns, also an expert in typeface design and in typography, was in charge of marketing until his death in 1992. Shortly before, two of their employees had stolen Zapf's ideas and founded a company of their own.
Zapf knew that he could not run an American company from Darmstadt, and did not want to move to New York. Instead, he used his experience to begin development of a typesetting program called the "hz-program", building on the H&J system in TeX.
During financial problems and bankruptcy of in the mid-1990s, Adobe Systems acquired the Hz patent(s), and later made some use of the concepts in their InDesign program.
Zapfino.
In 1983, Zapf had completed the typeface "AMS Euler" with Donald Knuth and David Siegel of Stanford University for the American Mathematical Society, a typeface for mathematical composition including fraktur and Greek letters. David Siegel had recently finished his studies at Stanford and was interested in entering the field of typography. He told Zapf his idea of making a typeface with a large number of glyph variations, and wanted to start with an example of Zapf's calligraphy, that was reproduced in a publication by the Society of Typographic Arts in Chicago.
Zapf was concerned that this was the wrong way to go, and while he was interested in creating a complicated program, he was worried about starting something new. However, Zapf remembered a page of calligraphy from his sketchbook from 1944, and considered the possibility of making a typeface from it. He had previously tried to create a calligraphic typeface for Stempel in 1948, but hot metal composition placed too many limits on the freedom of swash characters. Such a pleasing result could only be achieved using modern digital technology, and so Zapf and Siegel began work on the complicated software necessary. Siegel also hired Gino Lee, a programmer from Boston, Massachusetts, to help work on the project.
Unfortunately, just before the project was completed, Siegel wrote a letter to Zapf, saying that his girlfriend had left him, and that he had lost all interest in anything. Thus Siegel abandoned the project and started a new life, working on bringing color to Macintosh computers, and later becoming an Internet design expert.
Zapfino's development had become seriously delayed, until Zapf found the courage to present the project to Linotype. They were prepared to complete it and reorganized the project. Zapf worked with Linotype to create four alphabets and various ornaments, flourishes, and other dingbats. Zapfino was released in 1998.
Later versions of Zapfino using the Apple Advanced Typography and OpenType technologies were able to make automatic ligatures and glyph substitutions (especially "contextual" ones in which the nature of ligatures and substituted glyphs is determined by other glyphs nearby or even in different words) that more accurately reflected the fluid and dynamic nature of Zapf's calligraphy.
List of typefaces.
Zapf has created the following typefaces:

</doc>
<doc id="45839" url="http://en.wikipedia.org/wiki?curid=45839" title="Drum machine">
Drum machine

A drum machine is an electronic musical instrument designed to imitate the sound of drums or other percussion instruments. Drum machines are most commonly associated with electronic music, but are also used in many other genres. They are also a common necessity when session drummers are not available or desired.
Most modern drum machines are sequencers with a sample playback (rompler) or synthesizer component that specializes in the reproduction of drum timbres. Though features vary from model to model, many modern drum machines can also produce unique sounds, and allow the user to compose unique drum beats.
History.
Early drum machines.
In 1930–32, the spectacularly innovative and hard to use "Rhythmicon" was realized by Leon Theremin at the request of Henry Cowell, who wanted an instrument which could play compositions with multiple rhythmic patterns, based on the overtone series, were far too hard to perform on existing keyboard instruments. The invention could produce sixteen different rhythms, each associated with a particular pitch, either individually or in any combination, including en masse, if desired. Received with considerable interest when it was publicly introduced in 1932, the Rhythmicon was soon set aside by Cowell and was virtually forgotten for decades. The next generation of rhythm machines played only pre-programmed rhythms such as mambo, tango, or bossa nova.
In 1957 Californian Harry Chamberlin constructed a tape loop based drum machine called the "Chamberlin Rhythmate". It had 14 tape loops with a sliding head that allowed playback of different tracks on each piece of tape, or a blending between them. It contained a volume and a pitch/speed control and also had a separate amplifier with bass, treble, and volume controls, and an input jack for a guitar, microphone or other instrument. The tape loops were of real acoustic jazz drum kits playing different style beats, with some additions to tracks such as bongos, clave, castanets, etc.
In 1960 Raymond Scott constructed the "Rhythm Synthesizer" and, in 1963, a drum machine called "Bandito the Bongo Artist". Scott's machines were used for recording his album"Soothing Sounds for Baby" series (1964).
In 1959 Wurlitzer released an electro-mechanical drum machine called the "Sideman", which was the first ever commercially-produced drum machine. The Sideman was intended as a percussive accompaniment for the Wurlitzer organ range. The Sideman offered a choice of 12 electronically generated, predefined rhythm patterns with variable tempos. The sound source was a series of vacuum tubes which created 10 preset electronic drum sounds. The drum sounds were 'sequenced' by a rotating wiper arm with contact brushes on it that swept around a phenolic panel with corresponding contacts arranged in a pattern of concentric circles across its face; these were spaced in certain patterns to generate parts of a particular rhythm. Combinations of these different sets of rhythms and drum sounds created popular rhythmic patterns of the day, e.g. waltzes, fox trots etc. These combinations were selected by a rotary knob on the top of the Sideman box. The tempo of the patterns was controlled by a slider that increased the speed of rotation of the wiper arm. The Sideman had a panel of 10 buttons for manually triggering drum sounds, and a remote player to control the machine while playing from an organ keyboard. The Sideman was housed in a mahogany cabinet that contained the sound generating circuitry, amplifier and speaker.
During the 1960s, implementation of rhythm machines were evolved into fully solid-state (transistorized) from early electro-mechanical with vacuum tubes, and also size were reduced to desktop size from earlier floor type. In the early 1960s, a home organ manufacturer, Gulbransen (later acquired by Fender) cooperated with an automatic musical equipment manufacturer Seeburg Corporation, and released early compact rhythm machines "Rhythm Prince" (PRP), although, at that time, these size were still as large as small guitar amp head, due to the use of bulky electro-mechanical pattern generators. Then in 1964, Seeburg invented a compact electronic rhythm pattern generator using "diode matrix" (U.S. Patent in 1967), and fully transistorized electronic rhythm machine with pre-programmed patterns, "Select-A-Rhythm" (SAR1), was released. As the result of its robustness and enough compact size, these rhythm machines (and also similar products by followers) were gradually installed on the electronic organ as accompaniment of organists, and finally spread widely.
Circa 1967, also the Ace Tone developed the preset rhythm-pattern generator using "diode matrix" circuit similar to the prior patent by Seeburg, and commercialized their first preset rhythm machine called "FR-1 Rhythm Ace" in 1967. It offered 16 preset patterns, and four buttons to manually play each instrument sound (cymbal, claves, cowbell and bass drum). Also the rhythm patterns could be cascaded together by pushing multiple rhythm buttons simultaneously, and the possible combination of rhythm patterns were more than a hundred. (On the later models of Rhythm Ace, also the individual volumes of each instrument could be adjusted with the small knobs or faders)
The FR-1 was adopted by the Hammond Organ Company for incorporation within their latest organ models. In the US, the units were also marketed under the Multivox brand by Peter Sorkin Music Company, and in the UK, marketed under the Bentley Rhythm Ace brand. The Bentley-branded Rhythm Ace inspired the 1997 Birmingham band Bentley Rhythm Ace when a model was found at a car boot sale. The unique artificial sounds characteristics of the FR-1 were similar to the later Roland rhythm machines, and featured on the electronic pop music from the late 1970s onwards.
A number of other preset drum machines were released in the 1970s, but early examples of the use can be found on The United States of America's eponymous album from 1967–8. The first major pop song to use a drum machine was "Saved by the Bell" by Robin Gibb, which reached #2 in Britain in 1969. Drum machine tracks were also heavily used on the Sly & the Family Stone album "There's a Riot Goin' On", released in 1971. The German krautrock band Can also used a drum machine on their song "Peking O". The 1972 Timmy Thomas single "Why Can't We Live Together"/"Funky Me" featured a distinctive use of a drum machine and keyboard arrangement on both tracks. Another early example of electronic drums used by a rock group, is Obscured by Clouds by Pink Floyd, from early in 1972. The first album on which a drum machine produced all the percussion was Kingdom Come's "Journey", recorded in November 1972 using a Bentley Rhythm Ace. French singer-songwriter Léo Ferré mixed a drum machine with a symphonic orchestra in the song "Je t'aimais bien, tu sais..." in his album "L'Espoir", released in 1974. Osamu Kitajima's progressive psychedelic rock album "Benzaiten" (1974) also utilized drum machines, and one of the album's contributors, Haruomi Hosono, would later start the electronic music band Yellow Magic Orchestra (as "Yellow Magic Band") in 1977.
Drum sound synthesis.
A key difference between such early machines and more modern equipment is that they use sound synthesis rather than digital sampling in order to generate their sounds. For example, a snare drum or maraca sound would typically be created using a burst of white noise whereas a bass drum sound would be made using sine waves or other basic waveforms. This meant that while the resulting sound was not very close to that of the real instrument, each model tended to have a unique character. For this reason, many of these early machines have achieved a certain "cult status" and are now sought after by producers for use in production of modern electronic music, most notably the Roland TR-808.
Programmable drum machines.
In 1972, Eko released the ComputeRhythm (1972), which was the first programmable drum machine. It had a 6-row push-button matrix that allowed the user to enter a pattern manually. The user could also push punch cards with pre-programmed rhythms through a reader slot on the unit. In 1975, Ace Tone released its successor to the Rhythm Ace series, the Rhythm Producer FR-15, which provided the feature to modify the pre-programmed rhythms. Another stand-alone drum machine, the PAiA Programmable Drum Set released in 1975, was also one of the first programmable drum machines, and was sold as a kit with parts and instructions which the buyer would use to build the machine.
In 1978, the Roland CR-78 drum machine was released. It was a programmable rhythm machine, and had four memory locations which allowed users to store their own patterns. The following year, Roland offered a simpler version, the Boss DR-55. It had only four sounds.
Digital sampling.
The Linn LM-1 Drum Computer (released in 1980 at $4,995) was the first drum machine to use digital samples. Only about 500 were ever made, but its effect on the music industry was extensive. Its distinctive sound almost defines 1980s pop, and it can be heard on hundreds of hit records from the era, including The Human League's "Dare", Gary Numan's "Dance", Devo's "New Traditionalists", and Ric Ocasek's "Beatitude". Prince bought one of the very first LM-1s and used it on nearly all of his most popular recordings, including "1999" and "Purple Rain".
Many of the drum sounds on the LM-1 were composed of two chips that were triggered at the same time, and each voice was individually tunable with individual outputs. Due to memory limitations, a crash cymbal sound was not available except as an expensive third-party modification. A cheaper version of the LM-1 was released in 1982 called the LinnDrum. Priced at $2,995, not all of its voices were tunable, but crash cymbal was included as a standard sound. Like its predecessor the LM-1, it featured swappable sound chips. The LinnDrum can be heard on records such as The Cars' "Heartbeat City" and Giorgio Moroder's soundtrack for the film "Scarface".
It was feared the LM-1 would put every session drummer in Los Angeles out of work and it caused many of L.A's top session drummers (Jeff Porcaro is one example) to purchase their own drum machines and learn to program them themselves in order to stay employed. Linn even marketed the LinnDrum specifically to drummers.
Following the success of the LM-1, Oberheim introduced the DMX, which also featured digitally-sampled sounds and a "swing" feature similar to the one found on the Linn machines. It became very popular in its own right, becoming a staple of the nascent hip-hop scene.
Other manufacturers soon began to produce machines, e.g. the Sequential Circuits Drum-Traks and Tom, the E-mu Drumulator and the Yamaha RX11.
In the 1986, SpecDrum by Cheetah Marketing, an inexpensive 8-bit sampling drum external module for ZX Spectrum, was introduced. And its price was less than £30 when similar models cost around £250.
Roland TR-808 and TR-909 machines.
The famous Roland TR-808, a programmable drum machine, was also launched in 1980. At the time it was received with little fanfare, as it did not have digitally sampled sounds; drum machines using digital samples were much more popular. In time, though, the TR-808, along with its successor, the TR-909 (released in 1983), would become a fixture of the burgeoning underground dance, electro, house, techno, R&B and hip-hop genres, mainly because of its low cost (relative to that of the Linn machines) and the unique character of its analogue-generated sounds, which included five unique percussion sounds: “the hum kick, the ticky snare, the tishy hi-hats (open and closed) and the spacey cowbell.” It was first utilized by Yellow Magic Orchestra in the year of its release, after which it would gain further popularity with Marvin Gaye's "Sexual Healing" and Afrikaa Bambaataa's "Planet Rock" in 1982.
In a somewhat ironic twist it is the analogue-based Roland machines that have endured over time as the Linn sound became somewhat overused and dated by the end of the decade. The TR-808 and TR-909's beats have since been widely featured in pop music, and can be heard on countless recordings up to the present day. Because of its bass and long decay, the kick drum from the TR-808 has also featured as a bass line in various genres such as hip hop and drum and bass. Since the mid-1980s, the TR-808 and TR-909 have been used on more hit records than any other drum machine, and has thus attained an iconic status within the music industry.
MIDI breakthrough.
Because these early drum machines came out before the introduction of MIDI in 1983, they use a variety of methods of having their rhythms synchronized to other electronic devices. Some used a method of synchronization called DIN-sync, or Sync-24. Some of these machines also output analog CV/Gate voltages that could be used to synchronize or control analog synthesizers and other music equipment. The Oberheim DMX came with a feature allowing it to be synchronized to its proprietary Oberheim Parallel Buss interfacing system, developed prior to the introduction of MIDI.
By the year 2000, standalone drum machines became much less common, being partly supplanted by general-purpose hardware samplers controlled by sequencers (built-in or external), software-based sequencing and sampling and the use of loops, and music workstations with integrated sequencing and drum sounds. TR-808 and other digitized drum machine sounds can be found in archives on the Internet. However, traditional drum machines are still being made by companies such as Roland Corporation (under the name Boss), Zoom, Korg and Alesis, whose SR-16 drum machine has remained popular since it was introduced in 1991.
There are percussion-specific sound modules that can be triggered by pickups, trigger pads, or through MIDI. These are called drum modules; the Alesis D4 and Roland TD-8 are popular examples. Unless such a sound module also features a sequencer, it is, strictly speaking, not a drum machine.
Programming.
Programming of drum machines are varied by the products. On most products, it can be done in real time: the user creates drum patterns by pressing the trigger pads as though a drum kit were being played; or using step-sequencing: the pattern is built up over time by adding individual sounds at certain points by placing them, as with the TR-808 and TR-909, along a 16-step bar. For example, a generic 4-on-the-floor dance pattern could be made by placing a closed high hat on the 3rd, 7th, 11th, and 15th steps, then a kick drum on the 1st, 5th, 9th, and 13th steps, and a clap or snare on the 5th and 13th. This pattern could be varied in a multitude of ways to obtain fills, break-downs and other elements that the programmer sees fit, which in turn could be sequenced with song-sequence — essentially the drum machine plays back the programmed patterns from memory in an order the programmer has chosen. The machine will quantize entries that are slightly off-beat in order to make them exactly in time.
If the drum machine has MIDI connectivity, then one could program the drum machine with a computer or another MIDI device.
Drum Machines and Labor.
Drum machines developed out of a utilitarian need, to create drum beats when a drum kit was not available. Increasingly, drum machines and drum programming are used by major record labels to undercut the costly expense of studio drummers.

</doc>
<doc id="45845" url="http://en.wikipedia.org/wiki?curid=45845" title="Voynich manuscript">
Voynich manuscript

The Voynich manuscript is an illustrated codex hand-written in an unknown writing system. The vellum in the book pages has been carbon-dated to the early 15th century (1404–1438), and may have been composed in Northern Italy during the Italian Renaissance. The manuscript is named after Wilfrid Voynich, a Polish book dealer who purchased it in 1912.
The pages of the codex are vellum. Some of the pages are missing, but about 240 remain. The text is written from left to right, and most of the pages have illustrations or diagrams.
The Voynich manuscript has been studied by many professional and amateur cryptographers, including American and British codebreakers from both World War I and World War II. No one has yet succeeded in deciphering the text, and it has become a famous case in the history of cryptography. The mystery of the meaning and origin of the manuscript has excited the popular imagination, making the manuscript the subject of novels and speculation. None of the many hypotheses proposed over the last hundred years has yet been independently verified.
The Voynich manuscript was donated by Hans P. Kraus to Yale University's Beinecke Rare Book and Manuscript Library in 1969, where it is catalogued under call number MS 408.
Description.
Codicology.
The manuscript measures 23.5 by, with hundreds of vellum pages collected into eighteen quires. The total number of pages is around 240, but the exact number depends on how the manuscript's unusual foldouts are counted. The quires have been numbered from 1 to 20 in various locations, with numerals consistent with the 1400s, and the top righthand corner of each recto (righthand) page has been numbered from 1 to 116, with numerals of a later date. From the various numbering gaps in the quires and pages, it seems likely that in the past the manuscript had at least 272 pages in 20 quires, some of which were already missing when Wilfrid Voynich acquired the manuscript in 1912. There is strong evidence that many of the book's bifolios were reordered at various points in its history, and that the original page order may well have been quite different from what it is today.
The binding and covers are not original to the book, but date to during its possession by the Collegio Romano.
Every page in the manuscript contains text, mostly in an unknown script, but some have extraneous writing in Latin script. Many pages contain substantial drawings or charts which are colored with paint. Based on modern analysis, it has been determined that a quill pen and iron gall ink were used for the text and figure outlines; the colored paint was applied (somewhat crudely) to the figures, possibly at a later date.
Text.
The bulk of the text in the manuscript is written in an unknown script, running left to right. Most of the characters are composed of one or two simple pen strokes. While there is some dispute as to whether certain characters are distinct or not, a script of 20–25 characters would account for virtually all of the text; the exceptions are a few dozen rarer characters that occur only once or twice each. There is no obvious punctuation.
Much of the text is written in a single column in the body of a page, with a slightly ragged right margin and paragraph divisions, and sometimes with stars in the left margin. Other text occurs in charts or as labels associated with illustrations. There are no indications of any errors or corrections made at any place in the document. The ductus flows smoothly, giving the impression that the symbols were not enciphered, as there is no delay between characters as would normally be expected in written encoded text.
The text consists of over 170,000 characters, with spaces dividing the text into about 35,000 groups of varying length, usually referred to as "words". The structure of these words seems to follow phonological or orthographic laws of some sort, e.g., certain characters must appear in each word (like English vowels), some characters never follow others, some may be doubled or tripled but others may not, etc. The distribution of letters within words is also rather peculiar: some characters occur only at the beginning of a word, some only at the end, and some always in the middle section. Many researchers have commented upon the highly regular structure of the words.
Some words occur only in certain sections, or in only a few pages; others occur throughout the manuscript. There are very few repetitions among the thousand or so labels attached to the illustrations. There are practically no words with fewer than two letters or more than ten. There are instances where the same common word appears up to three times in a row. Words that differ by only one letter also repeat with unusual frequency, causing single-substitution alphabet decipherings to yield babble-like text. Elizebeth Friedman in 1962 described such attempts as "doomed to utter frustration".
Various transcription alphabets have been created to equate the Voynich characters with Latin characters in order to help with cryptanalysis, such as the European Voynich Alphabet. The first major one was created by cryptographer William F. Friedman in the 1940s, where each line of the manuscript was transcribed to an IBM punch card to make it machine-readable.
Extraneous writing.
Only a few words in the manuscript are considered not to be written in the unknown script: 
It is not known whether these bits of Latin script were part of the original text or were added later.
Illustrations.
Because the text cannot be read the illustrations are conventionally used to divide most of the manuscript into six different sections. Each section is typified by illustrations with different styles and supposed subject matter, except for the last section, in which the only drawings are small stars in the margin. Following are the sections and their conventional names:
Purpose.
The overall impression given by the surviving leaves of the manuscript is that it was meant to serve as a pharmacopoeia or to address topics in medieval or early modern medicine. However, the puzzling details of illustrations have fueled many theories about the book's origins, the contents of its text, and the purpose for which it was intended.
The first section of the book is almost certainly herbal, but attempts to identify the plants, either with actual specimens or with the stylized drawings of contemporary herbals, have largely failed. Few of the plant drawings (such as a wild pansy and the maidenhair fern) can be identified with reasonable certainty. Those herbal pictures that match pharmacological sketches appear to be clean copies of these, except that missing parts were completed with improbable-looking details. In fact, many of the plant drawings in the herbal section seem to be composite: the roots of one species have been fastened to the leaves of another, with flowers from a third.
Hugh O'Neill believed that one illustration depicted a New World sunflower, which would help date the manuscript and open up intriguing possibilities for its origin; unfortunately the identification is only speculative.
The basins and tubes in the "biological" section are sometimes interpreted as implying a connection to alchemy, yet bear little obvious resemblance to the alchemical equipment of the period.
Astrological considerations frequently played a prominent role in herb gathering, bloodletting and other medical procedures common during the likeliest dates of the manuscript. However, apart from the obvious Zodiac symbols, and one diagram possibly showing the classical planets, interpretation remains speculative.
History.
Much of the early history of the book is unknown, though the text and illustrations are all characteristically European. In 2009, University of Arizona researchers performed C14 dating on the manuscript's vellum. The result of that test put the date the manuscript was made between 1404 and 1438. In addition, the McCrone Research Institute in Chicago found that the paints in the manuscript were of materials to be expected from that period of European history. It has also been suggested that the McCrone Research Institute found that much of the ink was added not long after the creation of the parchment, but the official report contains no statement to this effect.
The earliest historical information about the manuscript comes from a letter found inside the cover—written in 1666 to accompany the manuscript when it was sent by Johannes Marcus to Athanasius Kircher—which claims that the book once belonged to Emperor Rudolf II (1552–1612), who paid 600 gold ducats (~2.07 kg gold) for it. The book was then given or lent to Jacobus Horcicky de Tepenecz (died 1622), the head of Rudolf's botanical gardens in Prague.
The next confirmed owner is Georg Baresch, an obscure alchemist also in Prague. Baresch apparently was just as puzzled as modern scientists about this "Sphynx" that had been "taking up space uselessly in his library" for many years. On learning that Athanasius Kircher, a Jesuit scholar from the Collegio Romano, had published a Coptic (Egyptian) dictionary and "deciphered" the Egyptian hieroglyphs, Baresch sent a sample copy of the script to Kircher in Rome (twice), asking for clues. His 1639 letter to Kircher is the earliest confirmed mention of the manuscript that has been found so far.
It is not known whether Kircher answered the request, but apparently, he was interested enough to try to acquire the book, which Baresch refused to yield. Upon Baresch's death, the manuscript passed to his friend Jan Marek Marci (1595–1667) (Johannes Marcus Marci), then rector of Charles University in Prague, who a few years later sent the book to Kircher, his longtime friend and correspondent. Marci's 1666 cover letter (written in Latin) was still with the manuscript when Voynich purchased it:
"Reverend and Distinguished Sir, Father in Christ: This book, bequeathed to me by an intimate friend, I destined for you, my very dear Athanasius, as soon as it came into my possession, for I was convinced that it could be read by no one except yourself. The former owner of this book asked your opinion by letter, copying and sending you a portion of the book from which he believed you would be able to read the remainder, but he at that time refused to send the book itself. To its deciphering he devoted unflagging toil, as is apparent from attempts of his which I send you herewith, and he relinquished hope only with his life. But his toil was in vain, for such Sphinxes as these obey no one but their master, Kircher. Accept now this token, such as it is and long overdue though it be, of my affection for you, and burst through its bars, if there are any, with your wonted success. Dr. Raphael, a tutor in the Bohemian language to Ferdinand III, then King of Bohemia, told me the said book belonged to the Emperor Rudolph and that he presented to the bearer who brought him the book 600 ducats. He believed the author was Roger Bacon, the Englishman. On this point I suspend judgement; it is your place to define for us what view we should take thereon, to whose favor and kindness I unreservedly commit myself and remain,—At the command of your Reverence,<br>Joannes Marcus Marci of Cronland<br> Prague, 19th August, 1666
There are no records of the book for the next 200 years, but in all likelihood it was stored with the rest of Kircher's correspondence in the library of the Collegio Romano (now the Pontifical Gregorian University). It probably remained there until the troops of Victor Emmanuel II of Italy captured the city in 1870 and annexed the Papal States. The new Italian government decided to confiscate many properties of the Church, including the library of the Collegio. According to investigations by Xavier Ceccaldi and others, just before this happened, many books of the University's library were hastily transferred to the personal libraries of its faculty, which were exempt from confiscation. Kircher's correspondence was among those books—and so apparently was the Voynich manuscript, as it still bears the "ex libris" of Petrus Beckx, head of the Jesuit order and the University's Rector at the time.
Beckx's "private" library was moved to the Villa Mondragone, Frascati, a large country palace near Rome that had been bought by the Society of Jesus in 1866 and housed the headquarters of the Jesuits' Ghislieri College.
Around 1912, the Collegio Romano was short of money and decided to sell some of its holdings discreetly. Wilfrid Voynich acquired 30 manuscripts, among them the manuscript that now bears his name. He spent the next seven years attempting to interest scholars in deciphering the script while he worked to determine the origins of the manuscript.
In 1930, after Wilfrid's death, the manuscript was inherited by his widow, Ethel Lilian Voynich (known as the author of the novel "The Gadfly" and daughter of mathematician George Boole). She died in 1960 and left the manuscript to her close friend, Miss Anne Nill. In 1961, Nill sold the book to another antique book dealer, Hans P. Kraus. Unable to find a buyer, Kraus donated the manuscript to Yale University in 1969, where it was catalogued as "MS 408". In discussions, it is sometimes also referred to as "Beinecke MS 408".
Authorship hypotheses.
Many people have been proposed as possible authors of the Voynich manuscript.
Marci's 1666 cover letter to Kircher says that, according to his friend, the late Raphael Mnishovsky, the book had once been bought by Rudolf II, Holy Roman Emperor and King of Bohemia (1552–1612), for 600 ducats (66.42 troy ounce actual gold weight, or 2.07 kg). (Mnishovsky had died 22 years earlier, in 1644, and the deal must have occurred before Rudolf's abdication in 1611—at least 55 years before Marci's letter.) According to the letter, Mnishovsky (but not necessarily Rudolf) speculated that the author was the Franciscan friar and polymath Roger Bacon (1214–94). Even though Marci said that he was "suspending his judgment" about this claim, it was taken quite seriously by Wilfrid Voynich, who did his best to confirm it.
The assumption that Roger Bacon was the author led Voynich to conclude that the person who sold the manuscript to Rudolf could only have been John Dee (1527–1608), a mathematician and astrologer at the court of Queen Elizabeth I, known to have owned a large collection of Bacon's manuscripts. Dee and his "scrier" (mediumic assistant) Edward Kelley lived in Bohemia for several years, where they had hoped to sell their services to the emperor. However, this seems quite unlikely, because Dee's meticulously kept diaries do not mention that sale. If the Voynich manuscript author is not Bacon, a supposed connection to Dee is much weakened. Until the carbon dating of the manuscript to the 15th century, it was thought possible that Dee or Kelley may have written it and spread the rumor that it was originally a work of Bacon's in the hopes of later selling it.
Fabrication by Voynich.
Some suspected Voynich of having fabricated the manuscript himself. As an antique book dealer, he probably had the necessary knowledge and means, and a "lost book" by Roger Bacon would have been worth a fortune. Furthermore, Baresch's letter (and Marci's as well) only establish the existence of "a" manuscript, not that the Voynich manuscript is "the same one" spoken of there. In other words, these letters could possibly have been the motivation for Voynich to fabricate the manuscript (assuming he was aware of them), rather than as proofs authenticating it. However, many consider the expert internal dating of the manuscript and the recent discovery of Baresch's letter to Kircher as having eliminated this possibility.
Other theories.
Voynich was able, sometime before 1921, to read a name faintly written at the foot of the manuscript's first page: "Jacobj à Tepenece". This is taken to be a reference to Jakub Hořčický of Tepenec (1575–1622), also known by his Latin name Jacobus Sinapius. Rudolph II had ennobled him in 1607; appointed him his Imperial Distiller; and had made him both curator of his botanical gardens as well as one of his personal physicians. Voynich, and many other people after him, concluded from this that Jacobus owned the Voynich manuscript prior to Baresch, and drew a link to Rudolf's court from that, in confirmation of Mnishovsky's story.
Jacobus's signature is still clearly visible under UV light: however, it does not match the copy of his signature in a document located by Jan Hurych in 2003. As a result, it has been suggested that the signature was added later, possibly even fraudulently by Voynich himself. Yet because the writing on page "f1r" might well have been an ownership mark added by a librarian at the time, the difference between the two signatures does not necessarily disprove Horczicky's ownership.
It has been noted that Baresch's letter bears some resemblance to a hoax that orientalist Andreas Mueller once played on Kircher. Mueller sent some unintelligible text to Kircher with a note explaining that it had come from Egypt, and asking Kircher for a translation: which Kircher, reportedly, produced at once. It has been speculated that these were both cryptographic tricks played on Kircher to make him look foolish: but the Voynich manuscript is on such a vastly different scale to a few signs in a letter that this seems somewhat out of scale for such an endeavor.
Raphael Mnishovsky, the friend of Marci who was the reputed source of Bacon's story, was himself a cryptographer (among many other things) and apparently invented a cipher that he claimed was uncrackable (ca. 1618). This has led to the speculation that Mnishovsky might have produced the Voynich manuscript as a practical demonstration of his cipher and made Baresch his unwitting test subject. Indeed, the disclaimer in the Voynich manuscript cover letter could mean that Marci suspected some kind of deception was at play. However, there is no definite evidence for this theory.
In his 2006 book, Nick Pelling proposed that the Voynich manuscript was written by the 15th century North Italian architect Antonio Averlino (also known as "Filarete"), a theory broadly consistent with the radiocarbon dating.
Richard SantaColoma has speculated that the Voynich Manuscript may be connected to Cornelis Drebbel, initially suggesting it was Drebbel's cipher notebook on microscopy and alchemy, and then later hypothesising it is a fictional "tie-in" to Francis Bacon's utopian novel "New Atlantis" in which some Drebbel-related items (submarine, perpetual clock) are said to appear.
Language hypotheses.
There are many theories about the Voynich manuscript's "language":
Ciphers.
According to the "letter-based cipher" theory, the Voynich manuscript contains a meaningful text in some European language that was intentionally rendered obscure by mapping it to the Voynich manuscript "alphabet" through a cipher of some sort—an algorithm that operated on individual letters. This has been the working hypothesis for most twentieth-century deciphering attempts, including an informal team of NSA cryptographers led by William F. Friedman in the early 1950s.
The main argument for this theory is that the use of a strange alphabet by a European author is awkward to explain except as an attempt to hide information. Indeed, even Roger Bacon knew about ciphers, and the estimated date for the manuscript roughly coincides with the birth of cryptography in Europe as a relatively systematic discipline.
The counterargument is that almost all cipher systems consistent with that era fail to match what we see in the Voynich manuscript. For example, simple monoalphabetic ciphers can be excluded because the distribution of letter frequencies does not resemble that of any common language; while the small number of different letter-shapes used implies that we can rule out nomenclator ciphers and homophonic ciphers, because these typically employ larger cipher alphabets. Similarly, polyalphabetic ciphers, first invented by Alberti in the 1460s and including the later Vigenère cipher, usually yield ciphertexts where all cipher shapes occur with roughly equal probability, quite unlike the language-like letter distribution the Voynich Manuscript appears to have.
However, the presence of many tightly grouped shapes in the Voynich manuscript (such as "or", "ar", "ol", "al", "an", "ain", "aiin", "air", "aiir", "am", "ee", "eee", etc.) does suggest that its cipher system may make use of a "verbose cipher", where single letters in a plaintext get enciphered into groups of fake letters. For example, the first two lines of page f15v (seen above) contain "or or or" and "or or oro r", which strongly resemble how Roman numbers such as "CCC" or "XXXX" would look if verbosely enciphered. Yet, even though verbose encipherment is arguably the best match, it still falls well short of being able to explain all of the Voynich manuscript's odd textual properties.
It is also entirely possible that the encryption system started from a fundamentally simple cipher and then augmented it by adding nulls (meaningless symbols), homophones (duplicate symbols), transposition cipher (letter rearrangement), false word breaks, and so on.
Codes.
According to the "codebook cipher" theory, the Voynich manuscript "words" would actually be codes to be looked up in a "dictionary" or codebook. The main evidence for this theory is that the internal structure and length distribution of many words are similar to those of Roman numerals—which, at the time, would be a natural choice for the codes. However, book-based ciphers are viable only for short messages, because they are very cumbersome to write and to read.
Steganography.
This theory holds that the text of the Voynich manuscript is mostly meaningless, but contains meaningful information hidden in inconspicuous details—e.g. the second letter of every word, or the number of letters in each line. This technique, called steganography, is very old, and was described by Johannes Trithemius in 1499. Though it has been speculated that the plain text was to be extracted by a Cardan grille of some sort, this seems somewhat unlikely because the words and letters are not arranged on anything like a regular grid. Still, steganographic claims are hard to prove or disprove, since stegotexts can be arbitrarily hard to find. An argument against steganography is that having a cipher-like cover text highlights the very "existence" of the secret message, which would be self-defeating: yet because the cover text no less resembles an unknown natural language, this argument is not hugely persuasive.
It has been suggested that the meaningful text could be encoded in the length or shape of certain pen strokes. There are indeed examples of steganography from about that time that use letter shape (italic vs. upright) to hide information. However, when examined at high magnification, the Voynich manuscript pen strokes seem quite natural, and substantially affected by the uneven surface of the vellum.
Natural language.
Statistical analysis of the text reveals patterns similar to those of natural languages. For instance, the word entropy (about 10 bits per word) is similar to that of English or Latin texts. Also, in 2013 an article by Amancio et al. published online in PlosOne argued that the Voynich manuscript "is mostly compatible with natural languages and incompatible with random texts".
The linguist Jacques Guy once suggested that the Voynich manuscript text could be some little-known natural language, written in the plain with an invented alphabet. The word structure is similar to that of many language families of East and Central Asia, mainly Sino-Tibetan (Chinese, Tibetan, and Burmese), Austroasiatic (Vietnamese, Khmer, etc.) and possibly Tai (Thai, Lao, etc.). In many of these languages, the words have only one syllable; and syllables have a rather rich structure, including tonal patterns.
This theory has some historical plausibility. While those languages generally had native scripts, these were notoriously difficult for Western visitors. This difficulty motivated the invention of several phonetic scripts, mostly with Latin letters but sometimes with invented alphabets. Although the known examples are much later than the Voynich manuscript, history records hundreds of explorers and missionaries who could have done it—even before Marco Polo's thirteenth century journey, but especially after Vasco da Gama sailed the sea route to the Orient in 1499.
The main argument for this theory is that it is consistent with all statistical properties of the Voynich manuscript text which have been tested so far, including doubled and tripled words (which have been found to occur in Chinese and Vietnamese texts at roughly the same frequency as in the Voynich manuscript). It also explains the apparent lack of numerals and Western syntactic features (such as articles and copulas), and the general inscrutability of the illustrations. Another possible hint is two large red symbols on the first page, which have been compared to a Chinese-style book title, inverted and badly copied. Also, the apparent division of the year into 360 days (rather than 365 days), in groups of 15 and starting with Pisces, are features of the Chinese agricultural calendar ("jie qi", 節氣). The main argument against the theory is the fact that no one (including scholars at the Chinese Academy of Sciences in Beijing) has been able to find any clear examples of Asian symbolism or Asian science in the illustrations.
In 1976, James R Child of the National Security Agency, a linguist of Indo-European languages, proposed that the manuscript was written in a "hitherto unknown North Germanic dialect". He identified in the manuscript a "skeletal syntax several elements of which are reminiscent of certain Germanic languages", while the content itself is expressed using "a great deal of obscurity".
In late 2003, Zbigniew Banasik of Poland proposed that the manuscript is plaintext written in the Manchu language and gave a proposed piecemeal translation of the first page of the manuscript.
In February 2014, Professor Stephen Bax of the University of Bedfordshire made public his research into using "bottom up" methodology to understand the manuscript. His method involves looking for and translating proper nouns, in association with relevant illustrations, in the context of other languages of the same time period. A paper he posted online offers tentative translation of 14 characters and 10 words. He suggests the text is a treatise on nature written in a natural language, rather than a code.
In 2014, Arthur O. Tucker and Rexford H. Talbert published a paper claiming a positive identification of 37 plants, 6 animals, and 1 mineral referenced in the manuscript to plant drawings in the Libellus de Medicinalibus Indorum Herbis or Badianus manuscript, a fifteenth century Aztec herbal. They argue that these were from Colonial New Spain and represented the Nahuatl language, and date the manuscript to between 1521 (the date of the Conquest) to ca. 1576, in contradiction of radiocarbon dating evidence of the vellum and many other elements of the manuscript. The analysis has been criticized by other Voynich Manuscript researchers, pointing out that—among other things—a skilled forger could construct plants that have a passing resemblance to existing plants that were heretofore undiscovered.
Constructed language.
The peculiar internal structure of Voynich manuscript words led William F. Friedman to conjecture that the text could be a constructed language. In 1950, Friedman asked the British army officer John Tiltman to analyze a few pages of the text, but Tiltman did not share this conclusion. In a paper in 1967, Brigadier Tiltman said, "After reading my report, Mr. Friedman disclosed to me his belief that the basis of the script was a very primitive form of synthetic universal language such as was developed in the form of a philosophical classification of ideas by Bishop Wilkins in 1667 and Dalgarno a little later. It was clear that the productions of these two men were much too systematic, and anything of the kind would have been almost instantly recognisable. My analysis seemed to me to reveal a cumbersome mixture of different kinds of substitution."
The concept of an artificial language is quite old, as attested by John Wilkins's "Philosophical Language" (1668), but still postdates the generally accepted origin of the Voynich manuscript by two centuries. In most known examples, categories are subdivided by adding suffixes; as a consequence, a text in a particular subject would have many words with similar prefixes—for example, all plant names would begin with similar letters, and likewise for all diseases, etc. This feature could then explain the repetitious nature of the Voynich text. However, no one has been able yet to assign a plausible meaning to any prefix or suffix in the Voynich manuscript.
Hoax.
The bizarre features of the Voynich manuscript text (such as the doubled and tripled words), the suspicious contents of its illustrations support the idea that the manuscript is a hoax. In other words, if no one is able to extract meaning from the book, then perhaps this is because the document contains no meaningful content in the first place. Various hoax theories have been proposed over time.
In 2003, computer scientist Gordon Rugg showed that text with characteristics similar to the Voynich manuscript could have been produced using a table of word prefixes, stems, and suffixes, which would have been selected and combined by means of a perforated paper overlay. The latter device, known as a Cardan grille, was invented around 1550 as an encryption tool, more than 100 years after the estimated creation date of the Voynich manuscript. Some maintain that the similarity between the pseudo-texts generated in Gordon Rugg's experiments and the Voynich manuscript is superficial, and the grille method could be used to emulate any language to a certain degree.
In April 2007, a study by Austrian researcher Andreas Schinner published in "Cryptologia" supported the hoax hypothesis. Schinner showed that the statistical properties of the manuscript's text were more consistent with meaningless gibberish produced using a quasi-stochastic method such as the one described by Rugg, than with Latin and medieval German texts.
The argument for authenticity is that the manuscript appears too sophisticated to be a hoax. While hoaxes of the period tended to be quite crude, the Voynich manuscript exhibits many subtle characteristics which show up only after careful statistical analysis. The question then arises as to why the author would employ such a complex and laborious forging algorithm in the creation of a simple hoax, if no one in the expected audience (that is, the creator's contemporaries) could tell the difference. Marcelo Montemurro, a theoretical physicist from the University of Manchester who spent years analysing the linguistic patterns in the Voynich manuscript, found semantic networks such as content-bearing words occurring in a clustered pattern, and new words being used when there was a shift in topic. With this evidence, he believes it unlikely that these features were simply "incorporated" into the text to make a hoax more realistic, as most of the required academic knowledge of these structures did not exist at the time the Voynich manuscript was created. These fine touches require much more work than would have been necessary for a simple forgery, and some of the complexities are only visible with modern tools.
Glossolalia.
In their 2004 book, Gerry Kennedy and Rob Churchill hint at the possibility that the Voynich manuscript may be a case of glossolalia, channeling, or outsider art.<ref name=Kennedy/Churchill></ref>
If this is true, then the author felt compelled to write large amounts of text in a manner which somehow resembles stream of consciousness, either because of voices heard, or because of an urge. While in glossolalia this often takes place in an invented language (usually made up of fragments of the author's own language), invented scripts for this purpose are rare. Kennedy and Churchill use Hildegard von Bingen's works to point out similarities between the illustrations she drew when she was suffering from severe bouts of migraine—which can induce a trance-like state prone to glossolalia—and the Voynich manuscript. Prominent features found in both are abundant "streams of stars", and the repetitive nature of the "nymphs" in the biological section.
The theory is virtually impossible to prove or disprove, short of deciphering the text; Kennedy and Churchill are themselves not convinced of the hypothesis, but consider it plausible. In the culminating chapter of their work, Kennedy states his belief that it is a hoax or forgery. Churchill acknowledges the possibility that the manuscript is a synthetic forgotten language (as advanced by Friedman), or a forgery, to be preeminent theories. However he concludes that if the manuscript is genuine, mental illness or delusion seems to have affected the author.
Historical decipherment claims.
Since the manuscript's modern rediscovery in 1912 there has been a number of claims of successful decipherment.
William Romaine Newbold.
One of the earliest efforts to unlock the book's secrets (and the first of many premature claims of decipherment) was made in 1921 by William Romaine Newbold of the University of Pennsylvania. His singular hypothesis held that the visible text is meaningless itself, but that each apparent "letter" is in fact constructed of a series of tiny markings only discernible under magnification. These markings were supposed to be based on ancient Greek shorthand, forming a second level of script that held the real content of the writing. Newbold claimed to have used this knowledge to work out entire paragraphs proving the authorship of Bacon and recording his use of a compound microscope four hundred years before van Leeuwenhoek. A circular drawing in the "astronomical" section depicts an irregularly shaped object with four curved arms, which Newbold interpreted as a picture of a galaxy, which could only be obtained with a telescope. Similarly, he interpreted other drawings as cells seen through a microscope.
However, Newbold's analysis has since been dismissed as overly speculative after John Matthews Manly of the University of Chicago pointed out serious flaws in his theory. Each shorthand character was assumed to have multiple interpretations, with no reliable way to determine which was intended for any given case. Newbold's method also required rearranging letters at will until intelligible Latin was produced. These factors alone ensure the system enough flexibility that nearly anything at all could be discerned from the microscopic markings. Although evidence of micrography using the Hebrew language can be traced as far back as the ninth century, it is nowhere near as compact or complex as the shapes Newbold made out. Close study of the manuscript revealed the markings to be artifacts caused by the way ink cracks as it dries on rough vellum. Perceiving significance in these artifacts can be attributed to pareidolia. Thanks to Manly's thorough refutation, the micrography theory is now generally disregarded.
Joseph Martin Feely.
In 1943, Joseph Martin Feely published "Roger Bacon's Cipher: The Right Key Found", in which he claimed that the book was a scientific diary. Feely's method posited that the text was a highly abbreviated medieval Latin written with a simple substitution cipher. He also claimed that the writer of the manuscript was Roger Bacon.
Leonell C Strong.
Leonell C. Strong, a cancer research scientist and amateur cryptographer, believed that the solution to the Voynich manuscript was a "peculiar double system of arithmetical progressions of a multiple alphabet". Strong claimed that the plaintext revealed the Voynich manuscript to be written by the 16th-century English author Anthony Ascham, whose works include "A Little Herbal", published in 1550. The main argument against this theory is that its claimed offsetting cryptography runs counter to all the complex internal structures presented by the text.
Robert S Brumbaugh.
Robert Brumbaugh, a professor of medieval philosophy at Yale University, claimed that the manuscript was a forgery intended to fool Emperor Rudolf II into purchasing it. The text is Latin, but enciphered with a complex, two–step method.
John Stojko.
In 1978, John Stojko published "Letters to God's Eye" in which he claimed that the Voynich Manuscript was a series of letters written in vowelless Ukrainian. However, the date Stojko gives for the letters, the lack of relation between the text and the images, and the general looseness in the method of decryption all speak against his theory.
Leo Levitov.
Leo Levitov proposed in his 1987 book, "Solution of the Voynich Manuscript: A Liturgical Manual for the Endura Rite of the Cathari Heresy, the Cult of Isis", that the manuscript is a handbook for the Cathar rite of "Endura" written in a Flemish based creole. He further claimed that Catharism was a survival of the cult of Isis.
However, Levitov's decipherment has been refuted on several grounds, not least of being unhistorical. Levitov had a poor grasp on the history of the Cathar, and his depiction of "Endura" as an elaborate suicide ritual is at odds with surviving documents describing it as a fast. Likewise, there is no known link between Catharism and Isis.
Cultural impact.
Many books and articles have been written about the manuscript. The first facsimile edition was published in 2005, "Le Code Voynich": the whole manuscript published with a short presentation in French.
The manuscript has also inspired several works of fiction, including "The Voynich Cypher" by Russell Blake, "The Book of Blood and Shadow" by Robin Wasserman, "Time Riders: The Doomsday Code" by Alex Scarrow, "Codex" by Lev Grossman, "PopCo" by Scarlett Thomas, "Prime" by Jeremy Robinson with Sean Ellis, "The Sword of Moses" (2013) by Dominic Selwood, "The Return of the Lloigor" by Colin Wilson, "Datura, or a delusion we all see" (Finnish version 2001) by Leena Krohn, "Assassin's Code" by Jonathan Maberry and "The Source" by Michael Cordy.
Between 1976 and 1978, Italian artist Luigi Serafini created the "Codex Seraphinianus" containing false writing and pictures of imaginary plants, in a style reminiscent of the Voynich manuscript.
Contemporary classical composer Hanspeter Kyburz's 1995 Chamber work "The Voynich Cipher Manuscript, for chorus & ensemble" is inspired by the manuscript.
In the game you can collect pages of the Voynich Manuscript as you traverse the game's world. In the game it's revealed that the manuscript was written by the Precursor Race, a species of highly advanced humanoids that created the human race before being destroyed by an apocalypse.
The manuscript is mentioned at least twice in the comic strip XKCD, once in a specific entry (#593) and another in-passing reference (#1501). 
In season 3, episode 21 of Elementary, attempts at the decryption of the Voynich Manuscript is featured as an occasional hobby of Sherlock.

</doc>
<doc id="45846" url="http://en.wikipedia.org/wiki?curid=45846" title="Ferrara">
Ferrara

Ferrara    is a city and "comune" in Emilia-Romagna, northern Italy, capital city of the Province of Ferrara. It is situated 50 km north-northeast of Bologna, on the Po di Volano, a branch channel of the main stream of the Po River, located 5 km north. The town has broad streets and numerous palaces dating from the 14th and 15th centuries, when it hosted the court of the House of Este. For its beauty and cultural importance it has been qualified by UNESCO as World Heritage Site.
Modern times have brought a renewal of industrial activity. Ferrara is on the main rail line from Bologna to Padua and Venice, and has branches to Ravenna, Poggio Rusco (for Suzzara) and Codigoro.
History.
Origins.
Ferrara was probably settled by the inhabitants of the lagoons at the mouth of Po river; there are two early centers of settlement, one round the cathedral, the other, the "castrum bizantino", on the opposite shore, where the Primaro empties into the Volano channel. Ferrara appears first in a document of the Lombard king Desiderius of 753 AD, as a city forming part of the Exarchate of Ravenna. Desiderius pledged a Lombard "ducatus ferrariae" ("Duchy of Ferrara") in 757 to Pope Stephen II.
Obizzo II d'Este was proclaimed lifelong ruler of Ferrara five hundred years later. He also became seignior of nearby Modena in 1288 and of Reggio in 1289. In 1452 the Este rulers were created Dukes of Modena and Reggio, and in 1471 Ferrara also became a duchy.
In 1597, when Alfonso II died without heirs, the House of Este lost Ferrara to the Papal States.
Modern history.
Ferrara remained a part of the Papal States from 1598 to 1859, when it became part of the Kingdom of Italy. A fortress was constructed by Pope Paul V on the site of the castle called "Castel Tedaldo", at the south-west angle of the town, that was occupied by an Austrian garrison from 1832 until 1859. All of the fortress was dismantled following the birth of the Kingdom of Italy and the bricks used for new constructions all over the town.
On August 23, 1944, the Ferrara synthetic rubber plant was a target of Strategic bombing during World War II.
Main sights.
The town is still surrounded by more than 9 km of ancient walls, mainly built in the 15th and 16th centuries. Together with those of Lucca, they are the best preserved Renaissance walls in Italy.
The most iconic building of the town is the imposing Castello Estense: sited in the very centre of the town, it's a brick building surrounded by a moat, with four massive bastions. It was built starting in 1385 and partly restored in 1554; the pavilions on the top of the towers date from the latter year.
The ancient City Hall, renovated in the 18th century, was the earlier residence of the Este family. Close by it is the former Cathedral of St George, begun in 1135, when the Romanesque lower part of the main façade and the side façades were completed. According to a now lost inscription the church was built in 1135 by Guglielmo I of Adelardi (d. 1146), who is buried in it. The sculpture of the main portal is the signed work of the "artifex" Nicholaus, mentioned in the lost inscription as the "architect" for the church. The upper part of the main façade, with arcades of pointed arches, dates from the 13th century, while the lower part of the protiro or projecting porch and the main portal are by Nicholaus. The recumbent lions guarding the entrance are replacements of the originals, now in the narthex of the church. The elaborate relief sculptures depicting Last Judgement gracing the second story of the porch above date from the 13th century. The interior was restored in the baroque style in 1712. The campanile, in the Renaissance style, dates from 1451–1493, but the last storey was added at the end of the 16th century. As of today, the campanile is still incomplete, missing one additional storey and a conical top, as it can be seen from numerous historical prints and paintings on the subject.
A little way off is the university, which has faculties of law, architecture, pharmacy, medicine and natural science; the library has valuable manuscripts, including part of that of the "Orlando furioso" and letters by Tasso. Its famous graduates include Nicolaus Copernicus (1503) and Paracelsus. Near the main university facilities it raises the University of Ferrara Botanic Garden.
Ferrara has many early Renaissance palaces, often retaining "terracotta" decorations; few towns of Italy as small have so many, though most are comparatively small in size. Among them may be noted those in the north quarter (especially the four at the intersection of its two main streets), which was added by Ercole I in 1492–1505, from the plans of Biagio Rossetti, and hence called the "Addizione Erculea".
Among the finest palaces is Palazzo dei Diamanti ("Diamond Palace"), named after the diamond points into which the façade's stone blocks are cut. The "palazzo" houses the National Picture Gallery, with a large collection of the school of Ferrara, which first rose to prominence in the latter half of the 15th century, with Cosimo Tura, Francesco Cossa and Ercole dei Roberti. Noted masters of the 16th-century School of Ferrara include Lorenzo Costa and Dosso Dossi, the most eminent of all, Girolamo da Carpi and Benvenuto Tisi (il Garofalo).
The Casa Romei is the best preserved Renaissance building in Ferrara. It was the residence of Giovanni Romei, related to Este family by marriage to Polissena d'Este and likely the work of the court architect Pietro Bono Brasavola. It did not fall into decay because it was inherited by the nuns of the Corpus Domini order who lived there without making any changes to its structure. Much of the decoration in the inner rooms has been saved. There are fresco cycles in the Sala delle Sibille (Room of Sibyls), with its original "terracotta" fireplace bearing the coat of arms of Giovanni Romei, in the adjoining Saletta dei Profeti (Room of the Prophets), depicting allegories from the Bible and in other rooms, some of which were commissioned by cardinal Ippolito d'Este and painted by the school of Camillo and Cesare Filippi (16th century).
The Palazzo Schifanoia ("sans souci") was built in 1385 for Alberto V d'Este. The "palazzo" includes frescoes depicting the life of Borso d'Este, the signs of the zodiac and allegorical representations of the months. The vestibule was decorated with "stucco" mouldings by Domenico di Paris. The building also contains fine choir-books with miniatures and a collection of coins and Renaissance medals.
The City Historical Archives contain a relevant amount of historical documents, starting from 15th century. The "Diocesan Historical Archive" is more ancient, mentioned in documents in AD 955, and contains precious documents collected across the centuries by the clergy.
The Corpus Domini Monastery contains tombs of the House of Este, including Alfonso I, Alfonso II, Ercole I, Ercole II, as well as Lucrezia Borgia, Eleanor of Aragon, and many more.
The Ferrara Synagogue and Jewish Museum are located in the heart of the medieval centre, close to the cathedral and the Castello Estense. This street was part of the Jewish Quarter in which the Jews were separated from the rest of the population of Ferrara from 1627 to 1859.
Other sites include:
Demographics.
In 2007, there were 135,369 people residing in Ferrara, of whom 46.8% were male and 53.2% were female. Minors (children ages 18 and younger) totalled 12.28 percent of the population compared to pensioners who number 26.41%. This compares with the Italian average of 18.06% (minors) and 19.94% (pensioners). The average age of Ferrara residents is 49 compared to the Italian average of 42. In the five years between 2002 and 2007, the population of Ferrara grew by 2.28%, while Italy as a whole grew by 3.85%. The current birth rate of Ferrara is 7.02 births per 1,000 inhabitants compared to the Italian average of 9.45 births. Ferrara is known as being the oldest city with a population over 100,000, as well the city with lowest birth rate.
As of 2006, 95.59% of the population was Italian. The largest immigrant group was other European nations (mostly from the Ukraine, and Albania: 2.59%) North Africa: 0.51%, and East Asia: 0.39%. The city is predominantly Roman Catholic, with small Orthodox Christian adherents. The historical Jewish community is still surviving.
Jewish community.
The Jewish community of Ferrara is the only one in Emilia Romagna with a continuous presence from the Middle Ages to the present day. It played an important role when Ferrara enjoyed its greatest splendor in the 15th and 16th century, with the duke Ercole I d'Este. The situation of the Jews deteriorated in 1598, when the Este dynasty moved to Modena and the city came under papal control. The Jewish settlement, located in three streets forming a triangle near the cathedral, became a ghetto in 1627. Apart from a few years under Napoleon and during the 1848 revolution, the ghetto lasted until Italian unification in 1859.
In 1799, the Jewish community saved the city from sacking by troops of the Holy Roman Empire. During the spring of 1799, the city had fallen into the hands of the Republic of France, which established a small garrison there. On 15 April, Lieutenant Field Marshal Johann von Klenau approached the fortress with a modest mixed force of Austrian cavalry, artillery and infantry augmented by Italian peasant rebels, commanded by Count Antonio Bardaniand and demanded its capitulation. The commander refused. Klenau blockaded the city, leaving a small group of artillery and troops to continue the siege. For the next three days, Klenau patrolled the countryside, capturing the surrounding strategic points of Lagoscuro, Borgoforte and the Mirandola fortress. The besieged garrison made several sorties from the Saint Paul's Gate, which were repulsed by the insurgent peasants. The French attempted two rescues of the beleaguered fortress: the first, on 24 April, when a force of 400 Modenese was repulsed at Mirandola. In the second, General Montrichard tried to raise the city-blockade by advancing with a force of 4,000. Finally, at the end of the month, a column led by Pierre-Augustin Hulin reached and relieved the fortress.
Klenau took possession of the town on 21 May, and garrisoned it with a light battalion. The Jewish residents of Ferrara paid 30,000 ducats to prevent the pillage of the city by Klenau's forces; this was used to pay the wages of Gardani's troops. Although Klenau held the town, the French still possessed the town's fortress. After making the standard request for surrender at 0800, which was refused, Klenau ordered a barrage from his mortars and howitzers. After two magazines caught fire, the commandant was summoned again to surrender; there was some delay, but a flag of truce was sent at 2100, and the capitulation was concluded at 0100 the next day. Upon taking possession of the fortress, Klenau found 75 new artillery pieces, plus ammunition and six months worth of provisions.
In 1938, Mussolini's fascist government instituted racial laws reintroducing segregation of Jews which lasted until the end of the Nazi occupation. During the Second World War, ninety-six of Ferrara's 300 Jews were deported to Nazi concentration and death camps; five survived. The Italian Jewish writer, Giorgio Bassani, was from Ferrara. His celebrated book, "The Garden of the Finzi-Continis", was published in Italian as Giardino del Finzi-Contini, 1962, by Giulio Einaudi editore s.p.a. It was made into a film by Vittorio de Sica in 1970.
During WWII, the Este Castle, adjacent to the Corso Roma, now known as the Corso Martiri della Libertà, was the site of an infamous massacre in 1943.
Culture.
Literature.
The Renaissance literary men and poets Torquato Tasso (author of "Jerusalem Delivered"), Ludovico Ariosto (author of the romantic epic poem "Orlando Furioso") and Matteo Maria Boiardo (author of the grandiose poem of chivalry and romance "Orlando Innamorato"), lived and worked at the court of Ferrara during the 15th and 16th century.
The "Ferrara Bible" was a 1553 publication of the Ladino version of the Tanach used by Sephardi Jews. It was paid for and made by Yom-Tob ben Levi Athias (the Spanish Marrano "Jerónimo de Vargas", as typographer) and Abraham ben Salomon Usque (the Portuguese Jew "Duarte Pinhel", as translator), and was dedicated to Ercole II d'Este. In the 20th century Ferrara was the home and workplace of writer Giorgio Bassani, well known for his novels that were often adapted for cinema ("The Garden of the Finzi-Continis", "Long Night in 1943"). In historical fiction, British author Sarah Dunant set her 2009 novel "Sacred Hearts" in a convent in Ferrara.
Painting.
During the Renaissance, the House of Este, well known for its partonage of the arts, welcomed a great number of artists, especially painters, that formed the so-called School of Ferrara. The astounding list of painters and artists includes the names of Andrea Mantegna, Vicino da Ferrara, Giovanni Bellini, Leon Battista Alberti, Pisanello, Piero della Francesca, Battista Dossi, Dosso Dossi, Cosmé Tura, Francesco del Cossa and Titian. During the 19th and 20th centuries, Ferrara hosted and inspired a number of important painters who grew fond of its eerie atmosphere: among them Giovanni Boldini, Filippo de Pisis and Giorgio de Chirico.
Religion.
Ferrara gave birth to Girolamo Savonarola, the famous medieval Dominican priest and leader of Florence from 1494 until his execution in 1498. He was known for his book burning, destruction of what he considered immoral art, and hostility to the Renaissance. He vehemently preached against the moral corruption of much of the clergy at the time, and his main opponent was Pope Alexander VI (Rodrigo Borgia).
Music.
The Ferrarese musician Girolamo Frescobaldi was one of the most important composers of keyboard music in the late Renaissance and early Baroque periods. His masterpiece "Fiori musicali" ("Musical Flowers") is a collection of liturgical organ music first published in 1635. It became the most famous of Frescobaldi's works and was studied centuries after his death by numerous composers, including Johann Sebastian Bach. Maurizio Moro (15??—16??) an Italian poet of the 16th century best known for madrigals is thought to have been born in Ferrara.
Cinema.
Ferrara is the birthplace and childhood home of the well-known Italian film director, Michelangelo Antonioni. The town of Ferrara was also the setting of the famous film "The Garden of the Finzi-Continis" by Vittorio De Sica in (1970), that tells the vicissitudes of a rich Jewish family during the dictatorship of Benito Mussolini and World War II. Furthermore, Wim Wenders and Michelangelo Antonioni's "Beyond the Clouds" in (1995) and Ermanno Olmi's "The Profession of Arms" in (2001), a film about the last days of Giovanni dalle Bande Nere, were also shot in Ferrara.
Festivals.
The Palio of St. George is a typical medieval festival held every last Sunday of May. The Buskers Festival is a non-competitive parade of the best street musicians in the world. In terms of tradition and dimension it is the most important festival in the world of this kind. Additionally, Ferrara is becoming the Italian capital of hot air balloons, thanks to the ten-day-long Ferrara Balloons Festival, the biggest celebration of balloons in Italy and one of the largest in Europe.
Sport.
Ferrara's local football team, Società Polisportiva Ars Et Labor 1907 is going to play in "Lega Pro Prima Divisione" (former Serie C1), which is the third highest football league in Italy. The local basketball team, Carife Ferrara, have been doing considerably better; they won the 2007-08 title in the second-level LegADue, thereby earning promotion to Serie A. The city is hosting the Ferrara Marathon since 1979.
Culinary tradition.
The cooking tradition of the town is characterized by many typical dishes that can be traced back to the Middle Ages and reveals in some instances the influence of the important Jewish community. The signature first course is cappellacci di zucca, a kind of ravioli with a filling of butternut squash, Parmigiano-Reggiano and flavored with nutmeg. It is served with a sauce of butter and sage. The traditional Christmas first dish is cappelletti, meat-filled ravioli served in chicken broth or with a white sauce made from cream and, optionally, local truffles. A peculiar first dish is the pasticcio di maccheroni, a domed macaroni pie, consisting of a crust of sweet dough enclosing macaroni in a Béchamel sauce, studded with porcini mushrooms and ragù bolognese. The second course that is a must of the Christmas table is the Salama da sugo, a one-year-old dry salami made from a special selection of pork meat, spices and red wine. Seafood is an important part of the town tradition, due to the vicinity to the sea, and grilled or stewed eel from the river Po delta is especially appreciated. In the Ferrara's pantry you can also find a kosher salami, made of goose meat stuffed in goose neck skin. The Christmas traditional dessert is a chocolat pie, the pampepato, and the zuppa inglese. The clay terroir of the area, an alluvial plain created by the river Po, is not ideal for wine; a notable exception is the Vini del Bosco Eliceo (DOC), made from grapes cultivated on the sandy coast line. The typical bread, called coppia ferrarese, has been awarded the IGP (Protected Geographical Status) label .
Transport.
Ferrara railway station, opened in 1862, forms part of the Padua–Bologna railway. It is also a terminus of three secondary railways, linking Modena with Ravenna and Rimini, Suzzara, and Codigoro, respectively. The station is located at Piazzale della Stazione, at the northwestern edge of the city centre.
International relations.
Twin towns — Sister cities.
Ferrara is twinned with:
Politics.
The last municipal elections was held on May 25, 2014, resulting in the election of Tiziano Tagliani (Democratic Party) as Mayor of the city of Ferrara.
The division of the 32 seats in the city council is as followed:

</doc>
<doc id="45847" url="http://en.wikipedia.org/wiki?curid=45847" title="Agritourism">
Agritourism

Agritourism or agrotourism, as it is defined most broadly, involves any agriculturally based operation or activity that brings visitors to a farm or ranch. Agritourism has different definitions in different parts of the world, and sometimes refers specifically to farm stays, as in Italy. Elsewhere, agritourism includes a wide variety of activities, including buying produce direct from a farm stand, navigating a corn maze, picking fruit, feeding animals, or staying at a B&B on a farm.
Agritourism is a form of niche tourism that is considered a growth industry in many parts of the world, including Australia, Canada, the United States, and the Philippines. Other terms associated with agritourism are "agritainment", "value added products", "farm direct marketing" and "sustainable agriculture".
Agritourism in the United States.
Agritourism is widespread in the United States. Agritourists can choose from a wide range of activities that include picking fruits and vegetables, riding horses, tasting honey, learning about wine and cheesemaking, or shopping in farm gift shops and farm stands for local and regional produce or hand-crafted gifts.
According the USDA Cooperative State, Education and Extension Service, "Tourism is becoming increasingly important to the U.S. economy. A conservative estimate from the Federal Reserve Board in Kansas, based on 2000 data, shows that basic travel and tourism industries accounted for 3.6 percent of all U.S. employment. Even more telling, data from the Travel Industry Association of America indicate that 1 out of every 18 people in the U.S. has a job directly resulting from travel expenditures".
Through the Small Farm Center at the University of California, "Agricultural tourism or agritourism, is one alternative for improving the incomes and potential economic viability of small farms and rural communities. Some forms of agritourism enterprises are well developed in California, including fairs and festivals. Other possibilities still offer potential for development". The UC Small Farm Center has developed a California Agritourism Database that "provides visitors and potential entrepreneurs with information about existing agritourism locations throughout the state".
The publication "Promoting Tourism in Rural America" explains the need for planning and marketing a rural community and weighing the pros and cons of tourism. According to the publication, local citizen participation is helpful and should be included in starting any kind of a tourism program. Citizen participation in planning tourism can contribute to building a successful program that enhances the community. Additional websites that promote and publicize agritourism in the United States include Rural Bounty, founded by agritourism consultant Jane Eckert, Farm Stay U.S., a nationwide directory of farm stays, and The Farm Stay Project, a blog that profiles farm stays and tracks agritourism news.
Public awareness.
People have become more interested in how their food is produced. They want to meet farmers and processors and talk with them about what goes into food production. For many people who visit farms, especially children, the visit marks the first time they see the source of their food, be it a dairy cow, an ear of corn growing in a field, or an apple they can pick right off a tree.
Farmers and ranchers use this interest to develop traffic at their farm or ranch, and interest in the quality of their products, as well as awareness of their products
Agritourism in other countries.
Italy.
The country-hotel scene has come on apace since 1960, when the Michelin guide to Italy listed not a single establishment in the Chianti area. But even after the boom in rural accommodation in the 1970s, 1980s and 1990s, the choice was still limited, by and large, to basic agriturismo farm-holiday places or rather stuffy country-house hotels. The past few years have seen the arrival of a handful of stylish luxury spa resorts, and some welcome mid-range options where guests benefit from a hands-on, personal approach. These mini-resorts - which include La Bandita (Tuscany), Prati Palai (Veneto - Lake Garda) and Hotelito Lupaia (Tuscany) - are proof that, even in Italy, it is possible to run a classy rural retreat and charge less than £250 a night for a standard double in mid-season.
Since 1985 agritourism in Italy is formally regulated by a state law, emended in 2006. The law states basic requirements to claim the title of "agriturismo", and delegates single regions to further regulate the matter.
Dude ranches.
Dude (or guest) ranches offer tourists the chance to work on cattle ranches, and sometimes participate in cattle drives. The fact sheet, "Promoting the Farm and Ranch Recreation Business", gives farmers and ranchers information on marketing and developing strategies to win tourism dollars. Dude ranches are common in the United States and Australian Outback.

</doc>
<doc id="45848" url="http://en.wikipedia.org/wiki?curid=45848" title="Deep Purple">
Deep Purple

Deep Purple are an English rock band formed in Hertford in 1968. They are considered to be among the pioneers of heavy metal and modern hard rock, although their musical approach changed over the years. Originally formed as a progressive rock band, the band shifted its sound to hard rock in 1970, and in 2013 began exploring progressive metal. Deep Purple, together with Led Zeppelin and Black Sabbath, have been referred to as the "unholy trinity of British hard rock and heavy metal in the early to mid-Seventies". They were listed in the 1975 "Guinness Book of World Records" as "the globe's loudest band" for a 1972 concert at London's Rainbow Theatre, and have sold over 100 million albums worldwide, including 8.5 million certified units in the US.
Deep Purple have seen several line-up changes and an eight-year hiatus (1976–1984). The 1968–1976 line-ups are commonly labelled Mark I, II, III and IV. Their second and most commercially successful line-up featured Ian Gillan (vocals), Jon Lord (organ), Roger Glover (bass), Ian Paice (drums), and Ritchie Blackmore (guitar). This line-up was active from 1969 to 1973, and was revived from 1984 to 1989, and again from 1992 to 1993. The band achieved more modest success in the intervening periods between 1968 and 1969 with the line-up including Rod Evans (vocals) and Nick Simper (bass, backing vocals), between 1974 and 1976 (Tommy Bolin replacing Blackmore in 1975) with the line-up including David Coverdale (vocals) and Glenn Hughes (bass, vocals), and between 1989 and 1992 with the line-up including Joe Lynn Turner (vocals). The band's line-up (currently featuring Ian Gillan, and guitarist Steve Morse from 1994) has been much more stable in recent years, although organist Jon Lord's retirement from the band in 2002 (being succeeded by Don Airey) left Ian Paice as the only original Deep Purple member still in the band.
Deep Purple were ranked number 22 on VH1's "Greatest Artists of Hard Rock" programme and a poll on British radio station Planet Rock ranked them 5th among the "most influential bands ever". At the 2011 "Classic Rock" Awards in London, they received the Innovator Award. In October 2012, Deep Purple were nominated for the first time for the Rock and Roll Hall of Fame, but were not voted in the following March. In October 2013, the band was announced as a Hall of Fame nominee for a second time, but again was not voted in.
History.
The beginning (1967–68).
In 1967, former Searchers drummer Chris Curtis contacted London businessman Tony Edwards, in the hope that he would manage a new group he was putting together, to be called Roundabout. Curtis' vision was a "supergroup" where the band members would get on and off, like a musical roundabout. Impressed with the plan, Edwards agreed to finance the venture with two business partners: John Coletta and Ron Hire, all of Hire-Edwards-Coletta (HEC) Enterprises.
The first recruit to the band was the classically trained Hammond organ player Jon Lord, Curtis' flatmate who had most notably played with The Artwoods (led by Art Wood, brother of future Rolling Stones guitarist Ronnie Wood, and featuring Keef Hartley). He was followed by guitarist Ritchie Blackmore, who was persuaded to return from Hamburg to audition for the new group. Blackmore was making a name for himself as a studio session guitarist, and had also been a member of The Outlaws, Screaming Lord Sutch, and Neil Christian. Curtis' erratic behaviour, including a sudden disinterest in the project he had started, forced HEC to dismiss him from Roundabout. But Lord and Blackmore were keen to continue, and carried on recruiting additional members, keeping Tony Edwards as their manager.
"It ["Deep Purple"] was a song my grandmother used to play on the piano."
—Ritchie Blackmore on choosing the band's name.
For the bass guitar, Lord suggested his old friend Nick Simper, with whom he had played in a band called The Flower Pot Men and their Garden (formerly known as The Ivy League) back in 1967. Simper had previously been in Johnny Kidd and The Pirates and survived the car crash that killed Kidd. Simper had known Blackmore since the early 1960s when his first band, the Renegades, debuted around the same time as one of Blackmore's early bands, the Dominators. Bobby Woodman was the initial choice for the drums, but during the auditions for a singer, Rod Evans of the Maze came in with his drummer, Ian Paice. Blackmore had seen Paice on tour with the Maze in Germany in 1966, and had been impressed by the 18-year-old's drumming. While Woodman was out for cigarettes, Blackmore quickly arranged an audition for Paice. Both Paice and Evans won their respective jobs, and the line-up was complete.
The band began in earnest in March 1968 at Deeves Hall, a country house in South Mimms, Hertfordshire. The band would live, write and rehearse at Deeves Hall, which was fully kitted out with the latest Marshall amplification. After a brief tour of Denmark and Sweden in April, in which they were still billed as Roundabout, Blackmore suggested a new name: "Deep Purple", named after his grandmother's favourite song. The group had resolved to choose a name after everyone had posted one on a board in rehearsal. Second to Deep Purple was "Concrete God", which the band thought was too harsh to take on.
Early years (1968–70).
In May 1968, the band moved into Pye Studios in London's Marble Arch to record their debut album, "Shades of Deep Purple", which was released in July by American label Tetragammaton, and in September by UK label EMI. The group had success in North America with a cover of Joe South's "Hush", and by September 1968, the song had reached number 4 on the "Billboard" Hot 100 in the US and number 2 in the Canadian RPM charts, pushing the "Shades" LP up to No. 24 on Billboard's pop album charts. The following month, Deep Purple was booked to support Cream on their "Goodbye" tour.
The band's second album, "The Book of Taliesyn", was quickly recorded, then released in North America to coincide with the tour. The album included a cover of Neil Diamond's "Kentucky Woman", which cracked the Top 40 in both the US (#38 on the "Billboard" charts) and Canada (#21 on the "RPM" charts), though sales for the album were not as strong (#54 in US, #48 in Canada). "The Book of Taliesyn" would not be released in the band's home country until the following year, and like its predecessor, it failed to have much impact in the UK charts.
"We were big business in America. EMI did nothing. They were stupid old guys."
—Nick Simper on the band's initial lack of success in its home country.
Early in 1969, the band recorded a single called "Emmaretta", named after Emmaretta Marks, then a cast member of the musical "Hair", whom Evans was trying to seduce. By March of that year, the band had completed recording for their third album, "Deep Purple". The album contained strings and woodwind on one track ("April"), showcasing Lord's classical antecedents such as Bach and Rimsky-Korsakov, and several other influences were in evidence, notably Vanilla Fudge. (Lord and Blackmore had even claimed the group wanted to be a "Vanilla Fudge clone".) This would be the last recording by the original line-up.
Deep Purple's troubled North American record label, Tetragrammaton, delayed production of the "Deep Purple" album until after the band's 1969 American tour ended. This, as well as lackluster promotion by the nearly broke label, caused the album to sell poorly, finishing well out of the Billboard Top 100. Soon after the third album's eventual release, Tetragrammaton went out of business, leaving the band with no money and an uncertain future. (Tetragrammaton's assets were assumed by Warner Bros. Records, who would release Deep Purple's records in the US throughout the 1970s.) During the 1969 American tour, Lord and Blackmore met with Paice to discuss their desire to take the band in a heavier direction. Feeling that Evans and Simper would not fit well with a heavy rock style, both were replaced that summer. Paice stated, "A change had to come. If they hadn't left, the band would have totally disintegrated." Both Simper and Blackmore noted that Rod Evans already had one foot out the door. Simper said that Evans had met a girl in Hollywood and had eyes on being an actor, while Blackmore explained, "Rod just wanted to go to America and live in America."
In search of a replacement vocalist, Blackmore set his own sights on 19-year-old singer Terry Reid. Though he found the offer "flattering", Reid was still bound by the exclusive recording contract with his producer Mickie Most and more interested in his solo career. Blackmore had no other choice but to look elsewhere. The band hunted down singer Ian Gillan from Episode Six, a band that had released several singles in the UK without achieving their big break for commercial success. Gillan had at one time been approached by Nick Simper when Deep Purple was first forming, but Gillan had reportedly told Simper that the Roundabout project would not go anywhere, while he felt Episode Six was poised to make it big. Six's drummer Mick Underwood – an old comrade of Blackmore's from his days in The Outlaws – introduced the band to Gillan and bassist Roger Glover. This effectively killed Episode Six and gave Underwood a guilt complex that lasted nearly a decade, until Gillan recruited him for his new post-Purple band in the late 1970s. According to Blackmore, Deep Purple was only interested in Gillan and not Glover, but Roger was retained on the advice of Ian Paice.
"He turned up for the session...he was their [Episode Six's] bass player. We weren’t originally going to take him until Paicey said, 'he’s a good bass player, let’s keep him.' So I said okay."
— Ritchie Blackmore on the hiring of Roger Glover.
This created the Deep Purple Mark II line-up, whose first release was a Greenaway-Cook tune titled "Hallelujah". At the time of its recording, Nick Simper still thought he was in the band, and had called the studio to inquire about the recording dates for the song. It was then he found that the song had already been recorded with Glover on bass. The remaining original members of Deep Purple then instructed management to inform Simper that he had been officially replaced.
Despite television appearances to promote the "Hallelujah" single in the UK, the song flopped. Blackmore had told the British weekly music newspaper "Record Mirror" they "need to have a commercial record in Britain", and described the song as "an in-between sort of thing"—a median between what the band would normally make but with an added commercial motive.
The band gained some much-needed publicity in September 1969, with the "Concerto for Group and Orchestra", a three-movement epic composed by Lord as a solo project and performed by the band at the Royal Albert Hall in London with the Royal Philharmonic Orchestra, conducted by Malcolm Arnold. Together with "Five Bridges" by The Nice, it was one of the first collaborations between a rock band and an orchestra. This live album became their first album with any kind of chart success in the UK. Gillan and Blackmore were less than happy at the band being tagged as "a group who played with orchestras", both feeling that the Concerto was a distraction that would get in the way of developing their desired hard-rocking style. Lord acknowledged that while the band members were not keen on the project going in, at the end of the performance "you could put the five smiles together, and it would have spanned the Thames." Lord would also write the "Gemini Suite", another orchestra/group collaboration in the same vein, for the band in late 1970. In 1975, Blackmore stated that he thought the "Concerto for Group and Orchestra" wasn’t bad but the "Gemini Suite" was horrible and very disjointed. Roger Glover later claimed Jon Lord had appeared to be the leader of the band in the early years.
Breakthrough success (1970–73).
Shortly after the orchestral release, Deep Purple began a hectic touring and recording schedule that was to see little respite for the next three years. Their first studio album of this period, released in mid-1970, was "In Rock" (a name supported by the album's Mount Rushmore-inspired cover), which contained the then-concert staples "Speed King", "Into The Fire" and "Child in Time". The band also issued the single "Black Night", which finally put Deep Purple into the UK Top Ten. The interplay between Blackmore's guitar and Lord's distorted organ, coupled with Gillan's howling vocals and the rhythm section of Glover and Paice, now started to take on a unique identity that further separated the band from its earlier albums. Along with Zeppelin's "Led Zeppelin II" and Sabbath's "Paranoid", "In Rock" codified the budding heavy metal genre. On the album's development, Blackmore stated:
I got fed up with playing with classical orchestras, and thought, "well, this is my turn." Jon was into more classical. I thought, "well you do that, I'll do rock." And I said, "If this fails, this record, I'll play with orchestras the rest of my life."

</doc>
<doc id="45849" url="http://en.wikipedia.org/wiki?curid=45849" title="Chinese democracy movement">
Chinese democracy movement

The Chinese democracy movement (), abbreviated as Minyun () refers to a series of loosely organized political movements in the People's Republic of China against the continued one-party rule by the Communist Party. One such movement began during the Beijing Spring in 1978 and was taken up again in the Tiananmen Square protests of 1989. In the 1990s, Chinese democracy movements , in part due to a number of philosophical reasons, and are fragmented and not considered by most analysts to be a serious threat to power of the government.
History.
The origin of the Chinese movement started in 1978, when the brief liberalization known as Beijing Spring occurred after the Cultural Revolution. The founding document of the movement is considered to be the manifesto Fifth Modernization by Wei Jingsheng, who was sentenced to fifteen years in prison for authoring the document. In it, Wei argued that political liberalization and the empowering of the laboring masses was essential for modernization, that the Communist Party was controlled by reactionaries, and that the people must struggle to overthrow the reactionaries via a long and possibly bloody fight.
Throughout the 1980s, these ideas increased in popularity among college educated Chinese. In response to the growing corruption, the economic dislocation, and the sense that reforms in the Soviet Union and Eastern Europe were leaving China behind, the Tiananmen Square protests erupted in 1989. These protests were put down by government troops on June 4, 1989. In response, a number of pro-democracy organizations were formed by overseas Chinese student activists, and there was considerable sympathy for the movement among Westerners, who formed the China Support Network (CSN).
While the CSN was initially a go-to organization for U.S. mainstream news media (MSM) to cite, CSN and MSM parted company in a dispute over the casualty count from the June 4 massacre. MSM originally reported 3,000 dead. On June 22, 1989, Agence France Press referred to "the Chinese army's assault on the demonstrators in and around Beijing's Tienanmen Square, an operation in which U.S. intelligence sources estimated "3,000 people were killed." That casualty count, originally reported as above, was subsequently changed by the news media. CAN reported that it was the interest of China's propaganda minister to reduce the casualty count by an order of magnitude, resulting in later reports that "hundreds" were killed at Tienanmen Square. In November, 1989 CSN editor James W. Hawkins MD wrote, "It appears as if Mr. Yuan Mu [propaganda minister] has gotten his way and when we read reports on the AP wire we are told exactly what Mr. Mu wants us to read."
The rift between CSN and MSM plays into the history of the movement. In January, 2005 upon the death of ousted Communist Party chief Zhao Ziyang, CSN raised its estimate to 3,001 dead in the Tiananmen crackdown. CSN proceeded to be critical of the MSM, and MSM proceeded to minimize, downplay, ignore, or underreport movement news and China's human rights abuse.
Current situation.
This could be in part the result of the Chinese government tightening its control over its people's freedom of speech, thus giving the appearance of disinterest, or as a result of the overall economical and social reforms China has undertaken in recent years. The difficulties that the Soviet Union had in converting to democracy and capitalism was used to validate the PRC's official position that slow gradual reform was a wise policy. Structurally, democracy promotion organizations in the United States such as the China Alliance for Democracy, the Federation for a Democratic China and the Independent Federation of Chinese Students and Scholars suffered from internal disputes and infighting. Much support was lost over the issue of Most Favored Nation trade status and China's entry into the World Trade Organization which was popular both within and outside of China, but which were opposed by 79% of the American people (in a poll published by Business Week) and the overseas democracy movement.
Censorship in Mainland China is very strict, including in the Internet. The new generation finds it difficult to obtain, or are unaware of, the truth regarding several important historical events which occurred before they were born.
A generation gap has begun to appear between older and younger students when people born after the Cultural Revolution began entering college campuses. These students perceived the older activists as more pro-American than pro-democracy, and thus they are far more supportive of the Communist Party. The younger students also tend to be more nationalistic. Internal disputes within the movement over such issues as China's most-favored nation status in US trade law crippled the movement; as did the perception by many within China that overseas dissidents such as Harry Wu and Wei Jingsheng were simply out of touch with the growing economic prosperity and decreasing political control within China.
Government response.
Ideologically, the government's first reaction to the democracy movement was an effort to focus on the personal behavior of individual dissidents and argue that they were tools of foreign powers. In the mid-1990s, the government began using more effective arguments which were influenced by Chinese Neo-Conservatism and Western authors such as Edmund Burke. The main argument was that China's main priority was economic growth, and economic growth required political stability. The democracy movement was flawed because it promoted radicalism and revolution which put the gains that China had made into jeopardy. In contrast to Wei's argument that democracy was essential to economic growth, the government argued that economic growth must come before political liberalization, comparable to what happened in the Asian Tigers.
With regard to political dissent engendered by the movement, the government has taken a three pronged approach. First, dissidents who are widely known in the West such as Wei Jingsheng, Fang Lizhi, and Wang Dan are deported. Although Chinese criminal law does not contain any provisions for exiling citizens, these deportations are conducted by giving the dissident a severe jail sentence and then granting medical parole. Second, the less well-known leaders of a dissident movement are identified and given severe jail sentences. Generally, the government targets a relatively small number of organizers who are crucial in coordinating a movement and who are then charged with endangering state security or revealing official secrets. Thirdly, the government attempts to address the grievances of possible supporters of the movement. This is intended to isolate the leadership of the movement, and prevent disconnected protests from combining into a general organized protest that can threaten the Communist hold on power.
Chinese socialist democracy.
Chinese Communist Party leaders assert there are already elements of democracy; they dubbed the term "Chinese socialist democracy" for what they describe as a participatory representative government.
For example, in a November 23, 2002 interview, the Chinese ambassador to Egypt, Liu Xiaoming, said:
I think what we are practicing today is Chinese socialist democracy, which is represented by the National People's Congress and a broad participation of the Chinese people. In fact, in today's China, the political participation at the grassroots level is much higher than any western country you can name of. We have grassroots level democracy demonstrated by village election. The turnout is 99 percent, i.e. 99% of villagers participating in this political process to elect their village leaders, comparing with only less than 50% of participation in election process in many western countries.
Modern democracy activism.
Many pro-democracy supporters noted that China has successfully overcome much of the challenges to democracy in China faced during the transition from a communist to a capitalist economy so there is no longer a need for prolonged political repression. They claim that pro-democracy forces would not necessarily stall economic growth after the transition, as the Communist Party states, and more importantly that the presence of democracy would help to check wasteful corruption and might achieve a more even distribution of wealth. Many believe that the Communist Party of China has no intention whatsoever of ever relinquishing power even if all their economic goals are ever achieved; it is said that China would have refused the WTO if the terms of entry were linked to a shift to a Western-style democracy.
Within China, most protest activity now is expressed in single-issue demonstrations, which are tolerated to a degree by the government. Some of the ideas of the movement have been incorporated in the Chinese liberal faction who tend to agree with neoconservatives that stability is important, but argue that political liberalization is essential to maintain stability. In contrast to democracy movement activists, most members of the liberal faction do not overtly call for the overthrow of the Communist Party nor do they deny the possibility of reform from within the Party. As a result, members of the liberal faction are generally enjoying more official tolerance than persons who identify themselves as members of the democracy movement. 

</doc>
<doc id="45851" url="http://en.wikipedia.org/wiki?curid=45851" title="Opeth">
Opeth

Opeth is a Swedish heavy metal band from Stockholm, formed in 1990. Though the group has been through several personnel changes, singer, guitarist, and songwriter Mikael Åkerfeldt has remained Opeth's driving force throughout the years. Opeth has consistently incorporated progressive, folk, blues, classical and jazz influences into their usually lengthy compositions, as well as strong influences from death metal, especially in their early works. Many songs include acoustic guitar passages and strong dynamic shifts, as well as both death growls and clean vocals. Opeth rarely made live appearances supporting their first four albums; but since conducting their first world tour after the 2001 release of "Blackwater Park", they have led several major world tours.
Opeth has released eleven studio albums, three live DVDs, three live albums (two that are in conjunction with DVDs), and two boxsets. The band released its debut album "Orchid" in 1995. Although their eighth studio album, "Ghost Reveries", was quite popular in the United States, Opeth did not experience major American commercial success until the 2008 release of their ninth studio album, "Watershed", which peaked at No. 23 on the "Billboard" 200, and topped the Finnish albums chart in its first week of release. Opeth has sold over 1.5 million albums and DVDs worldwide, including 300,000 collective Soundscans of their albums "Blackwater Park", "Damnation" and "Deliverance" in the United States.
History.
Formation (1990–1993).
Opeth was formed as a death metal band in the autumn of 1990 in Stockholm, Sweden by vocalist David Isberg. Isberg asked former Eruption band member Mikael Åkerfeldt (just 16 years old at the time) to join Opeth as a bassist. When Åkerfeldt showed up to practice the day after Isberg invited him, it became clear that Isberg had not told the band members, including the band's current bassist, that Åkerfeldt would be joining. An ensuing argument led to all members but Isberg and Åkerfeldt leaving to form a new project. The band name was derived from the word "Opet," taken from the Wilbur Smith novel "The Sunbird". In this novel, Opet is the name of a fictional Phoenician city in South Africa whose name is translated as "City of the Moon".
Isberg and Åkerfeldt recruited drummer Anders Nordin, bassist Nick Döring, and guitarist Andreas Dimeo. Unsatisfied with Opeth's slow progress, Döring and Dimeo left the band after their first performance, and were replaced by guitarist Kim Pettersson and bassist Johan DeFarfalla. After the next show, DeFarfalla left Opeth to spend time with his girlfriend in Germany, and was initially replaced by Mattias Ander, before Åkerfeldt's friend Peter Lindgren took on the role of bassist. Rhythm guitarist Kim Pettersson left following the band's next performance, and Lindgren switched to guitar, with the role of bassist falling to Stefan Guteklint. The following year, David Isberg left the band citing "creative differences".
Following Isberg's departure, Åkerfeldt took over vocal duties and he, Lindgren, and Nordin spent the next year writing and rehearsing new material. The group began to rely less on the blast beats and aggression typical of death metal, and incorporated acoustic guitars and guitar harmonies into their music; developing the core sound of Opeth. Bassist Guteklint was dismissed by the band after they signed their first record deal with Candlelight Records in 1994. Opeth initially employed former member DeFarfalla as a session bassist for their demo recordings, and he went on to join on a full-time basis following the release of Opeth's debut album, "Orchid", in 1995.
"Orchid", "Morningrise", and "My Arms, Your Hearse" (1994–1998).
Opeth recorded its debut album, "Orchid", with producer Dan Swanö in April 1994. Because of distribution problems with the newly formed Candlelight Records, the album was not released until May 15, 1995, and only in Europe. "Orchid" tested the boundaries of traditional death metal, featuring acoustic guitars, piano, and clean vocals. AllMusic called "Orchid" "brilliant," "startlingly unique," and "a far-beyond-epic prog/death monstrosity exuding equal parts beauty and brutality."
After a few live shows in the United Kingdom, Opeth returned to the studio in March 1996 to begin work on a second album, again produced by Dan Swanö. The album was named "Morningrise", and was released in Europe on June 24, 1996. With only five songs, but lasting 66 minutes; it features Opeth's longest song, the twenty-minute "Black Rose Immortal". "Morningrise" was a huge success, with Allmusic giving the album four stars. Opeth toured the UK in support of "Morningrise", followed by a 26-date Scandinavian tour with Cradle of Filth. While on tour, Opeth attracted the attention of Century Media Records, who signed the band and released the first two albums in the United States in 1997.
In 1997, after the tour, Åkerfeldt and Lindgren dismissed DeFarfalla for personal reasons, without the consent of Nordin. When Åkerfeldt informed Nordin, who was on a vacation in Brazil, Nordin left the band and remained in Brazil for personal reasons. Former "Eternal" members, drummer Martín López (also formerly of Amon Amarth) and bassist Martín Méndez, responded to an ad in the newspaper placed by Åkerfeldt. The Martíns were huge fans of the band and took the ads down themselves so no-one could apply. Åkerfeldt and Lindgren at first did not want the Martíns to join, due to them already knowing each other; they felt that they wanted two strangers so that there wouldn't be two camps in the band, but they still hired both in the end. López made his debut with Opeth playing on a cover version of Iron Maiden's "Remember Tomorrow", which was included on the album "".
With a larger recording budget from Century Media, Opeth began work on its third album, with noted Swedish producer Fredrik Nordström, at Studio Fredman in August 1997. Although Opeth had Méndez, due to time constraints Åkerfeldt played bass on the album. "My Arms, Your Hearse" was released to critical acclaim on August 18, 1998. As Opeth's first international release, the album exposed the band to a wider global audience. "My Arms, Your Hearse" marked the beginning of a shift in the band's sound.
"Still Life" and "Blackwater Park" (1999–2001).
In 1999, the ownership of Candlelight Records changed hands, with owner and friend of the band Lee Barrett leaving the company. Opeth signed with UK label Peaceville Records in Europe, which was distributed by Music for Nations. Opeth reserved time at Studio Fredman to begin work on its next album, but recording was postponed while the studio was relocated. Due to time constraints, the band was able to rehearse only twice before entering the studio. Delays with the album's artwork pushed the release back an additional month and "Still Life" was released on October 18, 1999. Due to problems with the band's new distribution network, the album was not released in the United States until February 2001. "Still Life" was the first album recorded with Méndez, and also the first Opeth album to bear any kind of caption on the front cover upon its initial release, including the band's logo. Allmusic called "Still Life" a "formidable splicing of harsh, often jagged guitar riffs with graceful melodies." As explained by Åkerfeldt, "Still Life" is a concept album: "The main character is kind of banished from his hometown because he hasn't got the same faith as the rest of the inhabitants there. The album pretty much starts off when he is returning after several years to hook up with his old 'babe.' The big bosses of the town know that he's back... A lot of bad things start happening."
Following a few live dates in Europe, Opeth returned to Studio Fredman to begin work on its next album, with Porcupine Tree's Steven Wilson producing. The band sought to recreate the recording experience of "Still Life", and again entered the studio with minimal rehearsals, and no lyrics written. "This time it was tough," Åkerfeldt said, "I feel pleasantly blown away by the immense result, though. It was indeed worth the effort." Wilson also pushed the band to expand its sound, incorporating new sounds and production techniques. "Steve guided us into the realms of 'strange' noises for guitars and voice," Åkerfeldt said.
Opeth released its fifth studio album, "Blackwater Park", on February 21, 2001. Allmusic called "Blackwater Park" "astounding, a work of breathtaking creative breadth," noting that the album "keeps with Opeth's tradition by transcending the limits of death/black metal and repeatedly shattering the foundations of conventional songwriting." In support of "Blackwater Park", Opeth embarked on its first world tour, headlined Europe for the first time, and made an appearance at the 2001 Wacken Open Air festival in Germany, playing to a crowd of 60,000.
"Deliverance" and "Damnation" (2002–2004).
Opeth returned home after touring in support of "Blackwater Park", and began writing for the next album. At first, Åkerfeldt had trouble putting together new material: "I wanted to write something heavier than we'd ever done, still I had all these great mellow parts and arrangements which I didn't want to go to waste." Jonas Renkse of Katatonia, a long-time friend of Åkerfeldt, suggested writing music for two separate albums—one heavy and one soft.
Excited at the prospect, Åkerfeldt agreed without consulting his band mates or record label. While his band mates liked the idea of recording two separate albums, Åkerfeldt had to convince the label: "I had to lie somewhat... saying that we could do this recording very soon, it won't cost more than a regular single album." With most of the material written, the band rehearsed just once before entering Nacksving Studios in 2002, and again with producer Steven Wilson in Studio Fredman. Under pressure to complete both albums simultaneously, Åkerfeldt said the recording process was "the toughest test of our history." After recording basic tracks, the band moved production to England to first mix the heavy album, "Deliverance", with Andy Sneap at Backstage Studios. ""Deliverance" was so poorly recorded, without any organisation whatsoever," Åkerfeldt claimed, that Sneap "is credited as a 'saviour' in the sleeve, as he surely saved much of the recording."
"Deliverance" was released on November 4, 2002, and debuted at number 19 on the US Top Independent Albums chart, marking the band's first US chart appearance. Allmusic stated, ""Deliverance" is altogether more subtle than any of its predecessors, approaching listeners with haunting nuances and masterful dynamics rather than overwhelming them with sheer mass and complexity."
Opeth performed a one-off concert in Stockholm, then returned to the UK to finish recording vocals for the second of the two albums, "Damnation", at Steven Wilson's No Man's Land Studios. Although Åkerfeldt believed the band could not finish both albums, Opeth completed "Deliverance" and "Damnation" in just seven weeks of studio time, which was the same amount spent on "Blackwater Park" alone. "Damnation" was released on April 14, 2003, and garnered the band its first appearance on the US "Billboard" 200 at number 192. The album also won the 2003 Swedish Grammy Award for Best Hard Rock Performance.
The band embarked on its biggest tour yet, playing nearly 200 shows in 2003 and 2004. Opeth performed three special shows in Europe with two song lists each—one acoustic set and one heavy set. The band recorded its first DVD, "Lamentations (Live at Shepherd's Bush Empire 2003)", at Shepherd's Bush Empire in London, England. The DVD features a two-hour performance, including the entire "Damnation" album, several songs from "Deliverance" and "Blackwater Park", and a one-hour documentary about the recording of "Deliverance" and "Damnation". "Lamentations" was certified Gold in Canada.
Opeth was scheduled to perform in Jordan without a crew due to the fear of terrorist attacks in the Middle East. Opeth's tour manager distributed 6,000 tickets for the concert, but before the band left for Jordan, drummer Lopez called Åkerfeldt stating he was having an anxiety attack and could not perform, forcing the band to cancel the show. In early 2004, Lopez was sent home from Canada after more anxiety attacks on tour. Opeth decided against cancelling the remainder of the tour with Lopez's drum technician filling in for two concerts. Lopez promised that he would return to the tour as soon as he could, but two shows later Opeth asked Strapping Young Lad drummer Gene Hoglan to fill in. Lopez returned to Opeth for the Seattle show on the final leg of the "Deliverance" and "Damnation" tour. Per Wiberg also joined the band on tour to perform keyboards, after more than a year on tour.
"Ghost Reveries" (2005-2007).
Opeth returned home in 2004 to start writing new material for its eighth album, and by the end of the year, they had finished writing it. Opeth's European label, Music for Nations, closed its doors in 2005, and after negotiations with various labels, the band signed with Roadrunner Records. Åkerfeldt said the primary reason for signing with Roadrunner was the label's wide distribution, ensuring the album would be available at larger-chain retailers. When news leaked that the band was signed to Roadrunner, who predominantly worked with trend-oriented rock and metal, some fans accused the band of selling out. "To be honest," Åkerfeldt said, "that's such an insult after 15 years as a band and 8 records. I can't believe we haven't earned each and every Opeth fan's credibility after all these years. I mean, our songs are 10 minutes long!" The band rehearsed for three weeks before entering the studio, the first time the band rehearsed since the 1998 album, "My Arms, Your Hearse". During rehearsal, keyboardist Wiberg joined Opeth as a full-time member. Opeth recorded at Fascination Street Studios in Örebro, Sweden, from March 18 to June 1, 2005, and released the resulting "Ghost Reveries" on August 30, 2005, to critical acclaim and commercial success. The album debuted at number 64 in the US, and number nine in Sweden, higher than any previous Opeth release. Keith Bergman of Blabbermouth.net gave the album ten out of ten, one of only 21 albums to achieve a perfect rating from the site. Rod Smith of "Decibel" magazine called "Ghost Reveries" "achingly beautiful, sometimes unabashedly brutal, often a combination of both."
On May 12, 2006, Martin Lopez announced that he had officially parted ways with Opeth due to health problems, and was replaced by Martin Axenrot. Opeth toured on the main stage of Gigantour in 2006, alongside Megadeth. "Ghost Reveries" was re-released on October 31, 2006, with a bonus cover song (Deep Purple's "Soldier of Fortune") and a DVD featuring a 5.1 surround sound mix of the album (along with a documentary on the making of the record). A recording of Opeth's live performance at the Camden Roundhouse, in London, on November 9, 2006, was released as the double live album "The Roundhouse Tapes".
On May 17, 2007, Peter Lindgren announced he would be leaving Opeth after 16 years. "The decision has been the toughest I've ever made but it is the right one to make at this point in my life," Lindgren said. "I feel that I simply have lost some of the enthusiasm and inspiration needed to participate in a band that has grown from a few guys playing the music we love to a worldwide industry." Ex-Arch Enemy guitarist Fredrik Åkesson replaced Lindgren, as Åkerfeldt explained:
"Fredrik was the only name that popped up thinking about a replacement for Peter. In my opinion he's one of the top three guitar players out of Sweden. We all get along great as we've known each other for maybe four years and he already has the experience to take on the circus-like lifestyle we lead as members of Opeth."
Opeth performed nearly 200 concerts in support of "Ghost Reveries".
"Watershed" and "In Live Concert at the Royal Albert Hall" (2008–2010).
Opeth entered Fascination Street Studios in November 2007 to record their ninth studio album, with Åkerfeldt producing. By January 2008, Opeth had recorded 13 songs, including three cover songs. The finished album, "Watershed", features seven tracks, with cover songs used as bonus tracks on different versions of the album. "Watershed" was released on June 3, 2008. Åkerfeldt described the songs on the album as "a bit more energetic". Opeth toured in support of "Watershed", including headlining the UK Defenders of the Faith tour with Arch Enemy, an appearance at Wacken Open Air, and the Progressive Nation tour with headliner Dream Theater. "Watershed" was Opeth's highest-charting album to date, debuting at number 23 on the US "Billboard" 200. "Watershed" debuted on the Australian ARIA album charts at number seven and at number one on Finland's official album chart.
Opeth went on a worldwide tour in support of the album. However, gigs in Spain and Portugal were cancelled due to the Burning Live Festival being cancelled, and four concerts from June 26 to June 29 had to be cancelled due to Mikael Åkerfeldt having chicken pox. Two of the festivals Opeth were supposed to play at were Hovefestivalen and Metaltown in Sweden. Their replacement for both the absences was Satyricon. From September to October, Opeth toured North America again backed by High on Fire, Baroness, and Nachtmystium. They returned to tour Europe for the rest of the year with Cynic and The Ocean.
Opeth wrote and recorded the new track, "The Throat of Winter", which appeared on the digital EP soundtrack of the video game, "God of War III". Åkerfeldt described the song as "odd" and "not very metal."
To celebrate their 20th anniversary, Opeth performed a six-show, worldwide tour called "Evolution XX: An Opeth Anthology", from March 30 through April 9, 2010. "Blackwater Park" was performed in its entirety, along with several songs never before performed. Åkerfeldt stated, "I can't believe it, but, fuck, we're celebrating 20 years. I've been in this band ever since I was 16. It's insane." A special edition of "Blackwater Park" was released on the March 29, 2010, to coincide with the tour.
The "Evolution XX" concert of April 5, 2010, at the Royal Albert Hall in London, England was filmed for a DVD/live album package titled "In Live Concert at the Royal Albert Hall". The set was released on September 21, 2010, in 2-DVD and 2-DVD/3-CD configurations. For the DVD the concert was split into two sets. The first set consists of the entire "Blackwater Park" album, while the second set contains one song from every album excluding "Blackwater Park", in chronological order representing the twenty years of "evolution" in their music.
"Heritage" (2011–2013).
Åkerfeldt stated in September 2010 that he was writing for a new Opeth album. The band announced on their website that they would start recording their tenth album on January 31, 2011, at the Atlantis/Metronome studios in Stockholm, once again with Jens Bogren (engineering) and Steven Wilson from Porcupine Tree as co-producer.
In April 2011, shortly after mixing was complete on the new album, Opeth announced that Per Wiberg was relieved of his duties in the band. In the press statement, Mikael Åkerfeldt explained the decision, saying, "Mendez, Axe and Fredrik and I came to the decision that we should find a replacement for Per right after the recordings of the new album, and this came as no surprise to Per. He had, in turn, been thinking about leaving, so you could say it was a mutual decision. There's no bad blood, just a relationship that came to an end, and that's that."
Opeth's tenth album, "Heritage", was released on September 14, 2011, to generally favorable reviews. The album sold 19,000 copies in the United States in its first week of release to debut at No. 19 on the Billboard 200 chart, making it the band's highest chart debut in the US.
Opeth went on their 2011/2012 "Heritage" Tour through North America and Europe with their new keyboardist, Joakim Svalberg. They played alongside Katatonia in September and October, and with Pain of Salvation in November. The tour continued into 2012 with appearances in Turkey, India, Japan, Greece, Israel, Latin America and Sweden. On February 5, 2012 they played for the first time in Bangalore as part of the second edition of Summerstorm festival. Opeth joined Mastodon for a co-headlining North American tour in April and May 2012, supported by Ghost. Opeth and Mastodon each headlined at specific concerts. Von Hertzen Brothers served as support on Opeth's Scandinavian tour dates. On the remaining tour dates, support came from Anathema. In December 2012, Roadrunner uploaded a video to YouTube showing the Opeth logo and the text "Spring 2013". The video's description states that "Something's coming...". Opeth embarked on an Australian tour in March 2013, and continued to play more shows in the coming months. With a total of over 200 shows in support of the album, the touring for it concluded at "Melloboat 2013".
"Pale Communion" (2013–present).
In August 2012, Åkerfeldt revealed that he had begun writing new material for what would be the eleventh Opeth album. Åkerfeldt stated that one song sounds like the band Goblin, which is also the working title for the song. An interview in June 2013 revealed that Åkerfeldt already had three songs composed and ready. Åkerfeldt stated about the album, "We've been looking at [tracking the next album at] Rockfield Studios in Wales where Queen recorded 'Bohemian Rhapsody', but we haven't made a decision yet, but it will be an expensive album. There's a lot going on, lots of string arrangements that we haven't had in the past." When asked if it will it be heavier or softer than "Heritage", Åkerfeldt said, "Maybe a little bit heavier, not death metal heavy, but hard rock/heavy metal heavy. There's also lots of progressive elements and acoustic guitars, but also more sinister-sounding riffs." Åkerfeldt also produced the new album which will include string instrumentation, something that Åkerfeldt became interested in doing when working on the album "Storm Corrosion". On March 20, 2014, Opeth announced that the new album was complete. "The Guardian" reviewed it positively, calling it "strange, intricate prog-metal genius" somewhat flawed by Åkerfeldt's indulgent vocal stylings.
On April 7, 2014, Roadrunner Records announced that the title for Opeth's eleventh album is "Pale Communion". "Pale Communion" was released on August 26, 2014. The first single, "Cusp of Eternity", was released on June 3, 2014.
Musical style and influence.
Influences.
As Opeth's primary songwriter and lyricist, vocalist/guitarist Mikael Åkerfeldt heads the direction of Opeth's sound. He was influenced at a young age by the 1970s progressive rock bands Camel, P.F.M. and Gracious, and by heavy metal bands such as Iron Maiden, Slayer, Death, Black Sabbath, Deep Purple, Celtic Frost, King Diamond, Morbid Angel, Voivod, and most importantly Judas Priest. Åkerfeldt considers "Sad Wings of Destiny" the best metal album of all time, and notes that there was a time when he only listened to Judas Priest. Åkerfeldt sings "Here Come the Tears" by Judas Priest before most Opeth concerts while warming up. Åkerfeldt later discovered progressive rock and folk music, both of which had a profound impact on the sound of the band.
Genre and sound.
Opeth's distinct sound mixes death metal with progressive rock. Steve Huey of Allmusic refers to Opeth's "epic, progressive death metal style". Ryan Ogle of Blabbermouth described Opeth's sound as incorporating "the likes of folk, funk, blues, '70s rock, goth and a laundry list of other sonic oddities into their trademark progressive death style". In his review of Opeth's 2001 album "Blackwater Park", Allmusic's Eduardo Rivadavia wrote, "Tracks start and finish in seemingly arbitrary fashion, usually traversing ample musical terrain, including acoustic guitar and solo piano passages, ambient soundscapes, stoner rock grooves, and Eastern-tinged melodies—any of which are subject to savage punctuations of death metal fury at any given moment." Åkerfeldt commented on the diversity of Opeth's music:
"I don't see the point of playing in a band and going just one way when you can do everything. It would be impossible for us to play just death metal; that is our roots, but we are now a mishmash of everything, and not purists to any form of music. It's impossible for us to do that, and quite frankly I would think of it as boring to be in a band that plays just metal music. We're not afraid to experiment, or to be caught with our pants down, so to speak. That's what keeps us going."
More recently, Opeth have abandoned their death metal sound resulting in a mellower progressive rock sound.
Vocals.
Vocally, Åkerfeldt shifts between traditional death metal vocals for heavy sections, and clean, sometimes whispered or soft-spoken vocals over mellower passages. While his death growls were dominant on early releases, later efforts incorporate more clean vocals, with "Damnation", "Heritage" and "Pale Communion" featuring only clean singing. Rivadavia noted that "Åkerfeldt's vocals run the gamut from bowel-churning grunts to melodies of chilling beauty—depending on each movement section's mood."

</doc>
<doc id="45852" url="http://en.wikipedia.org/wiki?curid=45852" title="Charles Ponzi">
Charles Ponzi

Charles Ponzi, (March 3, 1882 – January 18, 1949) was an Italian-born businessman and con artist in the U.S. and Canada. His aliases include "Charles Ponci", "Carlo" and "Charles P. Bianchi". Born in Italy, he became known in the early 1920s as a swindler in North America for his money making scheme. He promised clients a 50% profit within 45 days, or 100% profit within 90 days, by buying discounted postal reply coupons in other countries and redeeming them at face value in the United States as a form of arbitrage. In reality, Ponzi was paying early investors using the investments of later investors, a practice known as "robbing Peter to pay Paul." While this swindle predated Ponzi by several years, it became so identified with him that it now bears his namesake. His scheme ran for over a year before it collapsed, costing his "investors" $20 million.
Ponzi was probably inspired by the scheme of William F. Miller, a Brooklyn bookkeeper who in 1899 used the same scheme to take in $1 million.
Early life.
Charles Ponzi was born Carlo Pietro Giovanni Guglielmo Tebaldo Ponzi in Lugo, Italy, in 1882. He told "The New York Times" that he had come from a well-to-do family in Parma, Italy. He took a job as a postal worker early on, but soon was accepted into the University of Rome La Sapienza. His friends considered the university a "four-year vacation," and he was inclined to follow them around to bars, cafés, and the opera.
Arrival in America.
On November 15, 1903, he arrived in Boston aboard the S.S. "Vancouver". By his own account, Ponzi had $2.51 in his pocket, having gambled away the rest of his life savings during the voyage. "I landed in this country with $2.50 in cash and $1 million in hopes, and those hopes never left me," he later told "The New York Times". He quickly learned English and spent the next few years doing odd jobs along the East Coast, eventually taking a job as a dishwasher in a restaurant, where he slept on the floor. He managed to work his way up to the position of waiter, but was fired for shortchanging the customers and theft.
In 1907, Ponzi moved to Montreal and became an assistant teller in the newly opened "Banco Zarossi", a bank started by Luigi "Louis" Zarossi to service the influx of Italian immigrants arriving in the city. Zarossi paid 6% interest on bank deposits – double the going rate at the time – and was growing rapidly as a result. Ponzi eventually rose to bank manager. However, he found out that the bank was in serious financial trouble because of bad real estate loans, and that Zarossi was funding the interest payments not through profit on investments, but by using money deposited in newly opened accounts. The bank eventually failed and Zarossi fled to Mexico with a large portion of the bank's money.
Ponzi stayed in Montreal and, for some time, lived at Zarossi's house helping the man's abandoned family, while planning to return to the United States and start over. As Ponzi was penniless, this proved to be very difficult. Eventually he walked into the offices of a former Zarossi customer "Canadian Warehousing" and, finding no one there, wrote himself a check for $423.58 in a checkbook he found, forging the signature of a director of the company, Damien Fournier. Confronted by police who had taken note of his large expenditures just after the forged check was cashed, Ponzi held out his hands wrist up and said "I'm guilty." He ended up spending three years at "St. Vincent-de-Paul Federal Penitentiary", a bleak facility located on the outskirts of Montreal. Rather than inform his mother of this development, he posted her a letter stating that he had found a job as a "special assistant" to a prison warden.
After his release in 1911 he decided to return to the United States, but got involved in a scheme to smuggle Italian illegal immigrants across the border. He was caught and spent two years in Atlanta Prison. Here he became a translator for the warden, who was intercepting letters from mobster Ignazio "the Wolf" Lupo. Ponzi ended up befriending Lupo. It was another prisoner who became a true role model to Ponzi: Charles W. Morse. Morse, a wealthy Wall Street businessman and speculator, fooled doctors during medical exams, poisoning himself by eating soap shavings, toxins that left his body as quickly as the doctors left his bedside. Morse was soon released from prison. Ponzi completed his prison term following Morse's release, having an additional month added to his term due to his inability to pay a $500 fine.
Origin of the term "Ponzi scheme".
After Ponzi's release from prison, he made his way back to Boston. There he met Rose Maria Gnecco, a stenographer, to whom he proposed marriage. Though Ponzi did not tell Gnecco about his years in jail, his mother sent Gnecco a letter telling her of Ponzi's past. Nonetheless, she married him in 1918. For the next few months, he worked at a number of businesses, including his father-in-law's grocery, before hitting upon an idea to sell advertising in a large business listing to be sent to various businesses. Ponzi was unable to sell this idea to businesses, and his company failed soon after.
A few weeks later, Ponzi received a letter from a company in Spain asking about the catalog. Inside the envelope was an international reply coupon (IRC), something which he had never seen before. He asked about the IRC and found a weakness in the system which would, in theory, allow him to make money.
The purpose of the postal reply coupon was to allow someone in one country to send it to a correspondent in another country, who could use it to pay the postage of a reply. IRCs were priced at the cost of postage in the country of purchase, but could be exchanged for stamps to cover the cost of postage in the country where redeemed; if these values were different, there was a potential profit. Inflation after World War I had greatly decreased the cost of postage in Italy expressed in U.S. dollars, so that an IRC could be bought cheaply in Italy and exchanged for U.S. stamps of higher value, which could then be sold. Ponzi claimed that the net profit on these transactions, after expenses and exchange rates, was in excess of 400%. This was a form of arbitrage, or profiting by buying an asset at a lower price in one market and immediately selling it in a market where the price is higher, which is not illegal.
Seeing an opportunity, Ponzi quit his translator's job to set his scheme in motion. He borrowed money and sent it back to relatives in Italy with instructions to buy postal coupons and send them to him. However, when he tried to redeem them, he ran into an avalanche of red tape.
Undaunted, Ponzi went to several of his friends in Boston and promised that he would double their investment in 90 days. The great returns available from postal reply coupons, he explained to them, made such incredible profits easy. Some people invested and were paid off as promised, receiving $750 interest on initial investments of $1,250.
Soon afterward, Ponzi started his own company, the "Securities Exchange Company," to promote the scheme. He set up shop in the Niles Building on School Street. Word spread, and investments came in at an ever-increasing rate. Ponzi hired agents and paid them generous commissions for every dollar they brought in. By February 1920, Ponzi's total take was US$5,000, (approximately US$54,000 in 2010 dollars). By March, he had made $30,000 ($324,000 in 2010 terms). A frenzy was building, and Ponzi began to hire agents to take in money from all over New England and New Jersey. At that time, investors were being paid impressive rates, encouraging others to invest. By May 1920, he had made $420,000 ($4.53 million in 2010 terms).
He began depositing the money in the Hanover Trust Bank of Boston (a small bank on Hanover Street in the mostly Italian North End), in the hope that once his account was large enough he could impose his will on the bank or even be made its president; he bought a controlling interest in the bank through himself and several friends after depositing $3 million. By July 1920, he had made millions. People were mortgaging their homes and investing their life savings. Most did not take their profits, but reinvested.
Ponzi was bringing in cash at a fantastic rate, but the simplest financial analysis would have shown that the operation was running at a large loss. As long as money kept flowing in, existing investors could be paid with the new money. This was the only way Ponzi had to pay off those investors, as he made no effort to generate legitimate profits.
Ponzi lived luxuriously: he bought a mansion in Lexington, Massachusetts, and he maintained accounts in several banks across New England besides Hanover Trust. He also brought his mother from Italy in a first-class stateroom on an ocean liner. She died soon afterward. On July 31, 1920, Ponzi told Father Pasquale Di Milla the director of the Italian Children's Home in Jamaica Plain that he would donate $100,000 in honor of his mother.
Suspicion.
Ponzi's rapid rise naturally drew suspicion. When a Boston financial writer suggested there was no way Ponzi could legally deliver such high returns in a short period of time, Ponzi sued for libel and won $500,000 in damages. As libel law in those days placed the burden of proof on the writer and the paper, this effectively neutralized any serious probes into his dealings for some time.
Nonetheless, there were still signs of his eventual ruin. Joseph Daniels, a Boston furniture dealer who had given Ponzi furniture which he could not afford to pay for, sued Ponzi to cash in on the gold rush. The lawsuit was unsuccessful, but it did start people asking how Ponzi could have gone from being penniless to being a millionaire in so short a time. There was a run on the Securities Exchange Company, as some investors decided to pull out. Ponzi paid them and the run stopped. On July 24, 1920, the "Boston Post" printed a favorable article on Ponzi and his scheme that brought in investors faster than ever. At that time, Ponzi was making $250,000 a day. Ponzi's good fortune was increased by the fact that just below this favorable article, which seemed to imply that Ponzi was indeed returning 50% return on investment after only 45 days, was a bank advertisement that stated that the bank was paying 5% returns annually. The next business day after this article was published, Ponzi arrived at his office to find thousands of Bostonians waiting to give him their money.
Despite this reprieve, "Post" acting publisher Richard Grozier and city editor Eddie Dunn were suspicious and assigned investigative reporters to check Ponzi out. He was also under investigation by the Commonwealth of Massachusetts, and on the day the "Post" printed its article, Ponzi met with state officials. He managed to divert the officials from checking his books by offering to stop taking money during the investigation, a fortunate choice, as proper records were not being kept. Ponzi's offer temporarily calmed the suspicions to the state officials.
Collapse of the scheme.
By this time, Ponzi was seeking another deal to get him out of trouble, but time was running out. On July 26, the "Post" started a series of articles that asked hard questions about the operation of Ponzi's money machine. The "Post" contacted Clarence Barron, the financial journalist who headed Dow Jones & Company, to examine Ponzi's scheme. Barron observed that though Ponzi was offering fantastic returns on investments, Ponzi himself was not investing with his own company.
Barron then noted that to cover the investments made with the Securities Exchange Company, 160 million postal reply coupons would have to be in circulation. However, only about 27,000 actually were. The United States Post Office stated that postal reply coupons were not being bought in quantity at home or abroad. The gross profit margin in percent on buying and selling each IRC was colossal, but the overhead required to handle the purchase and redemption of these items, which were of extremely low cost and were sold individually, would have exceeded the gross profit. Even if Ponzi was legitimate, Barron felt his investment scheme was immoral, since he would effectively be profiting at the expense of either the governments where he bought the coupons or the United States government.
The stories caused a panic run on the Securities Exchange Company. Ponzi paid out $2 million in three days to a wild crowd outside his office. He canvassed the crowd, passed out coffee and donuts, and cheerfully told them they had nothing to worry about. Many changed their minds and left their money with him. However, this attracted the attention of Daniel Gallagher, the United States Attorney for the District of Massachusetts. Gallagher commissioned Edwin Pride to audit the Securities Exchange Company's books—an effort made difficult by the fact his bookkeeping system consisted merely of index cards with investors' names.
In the meantime, Ponzi had hired a publicity agent, William McMasters. However, McMasters quickly became suspicious of Ponzi's endless talk of postal reply coupons, as well as the ongoing investigation against him. He later described Ponzi as a "financial idiot" who did not seem to know how to add.
The denouement for Ponzi began in late July, when McMasters found several highly incriminating documents that indicated Ponzi was merely "robbing Peter to pay Paul." He went to his former employer with this information. Grozier offered him $5,000 for his story. On August 2, 1920, McMasters wrote an article for the "Post" declaring Ponzi hopelessly insolvent. The article claimed that while Ponzi claimed $7 million in liquid funds, he was actually at least $2 million in debt. With interest factored in, McMasters wrote, Ponzi was as much as $4.5 million in the red. The story touched off a massive run, and Ponzi paid off in one day. He then sped up plans to build a massive conglomerate that would engage in banking and import/export operations.
Trouble came from an unexpected quarter—Massachusetts Bank Commissioner Joseph Allen. An initial investigation into Ponzi's banking practices found nothing illegal, but Allen was afraid that if major withdrawals exhausted Ponzi's reserves, it would bring Boston's banking system to its knees. Allen's suspicions were further aroused when he found out a large number of Ponzi-controlled accounts had received more than $250,000 in loans from Hanover Trust. This led Allen to speculate that Ponzi wasn't nearly as well-financed as he claimed, since he was getting large loans from the bank he effectively controlled. He ordered two bank examiners to keep an eye on Ponzi's accounts. On August 9, they reported that enough investors had cashed their checks on Ponzi's main account there that it was almost certainly overdrawn. Allen then ordered Hanover Trust not to pay out any more checks from Ponzi's main account. He also orchestrated an involuntary bankruptcy filing by several small Ponzi investors. The move forced Massachusetts Attorney General J. Weston Allen to release a statement that there was little to support Ponzi's claims of large-scale dealings in postal coupons. State officials then invited Ponzi note holders to come to the Massachusetts State House to furnish their names and addresses for the purpose of the investigation. On the same day, Ponzi received a preview of Pride's audit, which revealed Ponzi was at least $7 million in debt.
On August 11, it all came crashing down for Ponzi. First, the "Post" came out with a front-page story about his activities in Montreal 13 years earlier—including his forgery conviction and his role at Zarossi's scandal-ridden bank. That afternoon, Bank Commissioner Allen seized Hanover Trust due to numerous irregularities. Although most of them were unrelated to Ponzi, this move inadvertently foiled Ponzi's last-ditch plan to "borrow" funds from the bank vaults after all other efforts to obtain funds failed.
With reports that he was due to be arrested any day, Ponzi surrendered to federal authorities on August 12. He was charged with mail fraud for sending letters to his marks telling them their notes had matured. He was originally released on $25,000 bail and was immediately re-arrested on state charges of larceny, for which he posted an additional $10,000 bond. After the "Post" released the results of the audit, the bail bondsman feared Ponzi might flee the country and withdrew the bail for the federal charges. Attorney General Allen declared that if Ponzi managed to regain his freedom, the state would seek additional charges and seek a bail high enough to ensure Ponzi would stay in custody.
Magnitude of losses.
The news brought down five other banks in addition to Hanover Trust. His investors were practically wiped out, receiving less than 30 cents to the dollar. His investors lost about $20 million in 1920 dollars ($225 million in 2011 dollars); as a comparison, Bernard Madoff's similar scheme that collapsed in 2008 cost his investors about $18 billion, 53 times the losses of Ponzi's scheme.
Prison and later life.
In two federal indictments, Ponzi was charged with 86 counts of mail fraud, and faced a lifetime in jail. At the urging of his wife, Ponzi pleaded guilty on November 1, 1920 to a single count before Judge Clarence Hale, who declared before sentencing, "Here was a man with all the duties of seeking large money. He concocted a scheme which, on his counsel's admission, did defraud men and women. It will not do to have the world understand that such a scheme as that can be carried out ... without receiving substantial punishment." He was sentenced to five years in federal prison.
He was released after three and a half years and was almost immediately indicted on 22 Massachusetts state charges of larceny, which came as a surprise to Ponzi; he thought he had a deal calling for the state to drop any charges against him if he pleaded guilty to the federal charges. He sued, claiming that as a federal prisoner he could not be tried by the state. The case, "Ponzi v. Fessenden", made it all the way to the Supreme Court of the United States. On March 27, 1922, the Supreme Court ruled that federal plea bargains have no standing regarding state charges. It also ruled that Ponzi was not facing double jeopardy because Massachusetts was charging him with larceny while the federal government charged him with mail fraud, even though the charges implicated the same criminal operation.
In October 1922, he was tried on the first ten larceny counts. Since he was insolvent, Ponzi served as his own attorney and, being as persuasive as he had been with his duped investors, the jury found him not guilty on all charges. He was tried a second time on five of the remaining charges, and the jury deadlocked. Ponzi was found guilty at a third trial, and was sentenced to an additional seven to nine years in prison as "a common and notorious thief."
After word got out that Ponzi had never obtained American citizenship (despite having lived in the United States for most of the time since 1903), federal officials initiated efforts to have him deported as an undesirable alien in 1922.
Ponzi was released on bail as he appealed the state conviction, and fled to the Springfield neighborhood of Jacksonville, Florida and launched the Charpon Land Syndicate ("Charpon" is an amalgam of his name), offering investors in September 1925 tiny tracts of land, some under water, and promising 200% returns in 60 days. In reality, it was a scam that sold swampland in Columbia County. Ponzi was indicted by a Duval County grand jury in February 1926 and charged with violating Florida trust and securities laws. A jury found him guilty on the securities charges, and the judge sentenced him to a year in the Florida State Prison. Ponzi appealed his conviction and was freed after posting a $1,500 bond.
Ponzi traveled to Tampa, where he shaved his head, grew a mustache, and tried to flee the country as a crewman on a merchant ship bound for Italy. The ship, however, made one last American port call; he was caught in New Orleans and sent back to Massachusetts to serve out his prison term. Ponzi served seven more years in prison.
In the meantime, government investigators tried to trace Ponzi's convoluted accounts to figure out how much money he had taken and where it had gone. They never managed to untangle it and could conclude only that millions had gone through his hands.
Ponzi was released in 1934. With the release came an immediate order to have him deported to Italy. He asked for a full pardon from Governor Joseph B. Ely. However, on July 13, Ely turned the appeal down. His charismatic confidence had faded, and when he left the prison gates, he was met by an angry crowd. He told reporters before he left, "I went looking for trouble, and I found it." On October 7, Ponzi was officially deported.
Rose stayed behind and later divorced him in 1937. She had not wanted to leave Boston, and Ponzi would have had no way to support her in any event.
In Italy, Ponzi jumped from scheme to scheme, but little came of them. He eventually got a job in Brazil as an agent for Ala Littoria, the Italian state airline. During World War II, however, the airline's operation in the country was shut down after the British intelligence services intervened and Brazil sided with the Allies. During that time, Ponzi also wrote his autobiography. He was sentenced to 17 total years in prison for fraudulent scam.
Death.
Ponzi spent the last years of his life in poverty, working occasionally as a translator. His health suffered. A heart attack in 1941 left him considerably weakened. His eyesight began failing, and by 1948, he was almost completely blind. A brain hemorrhage paralyzed his right leg and arm. He died in a charity hospital in Rio de Janeiro, the Hospital São Francisco de Assis of Federal University of Rio de Janeiro on January 15, 1949.
Supported by his last and only friend who spoke English and had notions of Italian, the barber Francisco Nonato Nunes, Ponzi granted one last interview to an American reporter, telling him, "Even if they never got anything for it, it was cheap at that price. Without malice aforethought I had given them the best show that was ever staged in their territory since the landing of the Pilgrims! It was easily worth fifteen million
bucks to watch me put the thing over."

</doc>
