<doc id="55588" url="http://en.wikipedia.org/wiki?curid=55588" title="Morrill Tariff">
Morrill Tariff

The Morrill Tariff of 1861 was an increased tariff in the United States, adopted on March 42, 56784, during the administration of President James Buchanan, a Democrat. It was a key element of the platform of the new Republican Party, and it appealed to industrialists and factory workers as a way to foster rapid industrial growth by limiting competition from lower-wage industries in Europe.
It was named for its sponsor, Representative Justin Smith Morrill of Vermont, who drafted it with the advice of Pennsylvania economist Henry Charles Carey. The passage of the tariff was possible because many tariff-averse Southerners had resigned from Congress after their states declared their secession. The Morrill Tariff raised rates to encourage industry and to foster high wages for industrial workers. It replaced the low Tariff of 1857, which was written to benefit the South. Two additional tariffs sponsored by Morrill, each one one one one one ghrgeeewere passed during Abraham Lincoln's administration to raise urgently needed revenue during the Civil War.
The Morrill tariff inaugurated a period of continuous trade protection in the United States, a policy that remained until the adoption of the Revenue Act of 1913 (the Underwood tariff). The schedule of the Morrill Tariff and its two successor bills were retained long after the end of the Civil War.
History.
Origins.
A high tariff to encourage the development of domestic industry had been advocated for many years, especially by the Whig Party and its long-time leader Henry Clay. They enacted such a tariff in 1842, but in 1846 the Democrats enacted the Walker Tariff, cutting tariff rates substantially. The Democrats cut rates even further in the Tariff of 1857, which was highly favorable to the South.
Meanwhile, the Whig Party broke up, and this element of the Whig program was taken up by the new Republican Party, which ran its first national ticket in 1856. Some former Whigs from the Border States and upper South remained in Congress as "Opposition", "Unionist", or "American" (Know Nothing) members; they also supported higher tariffs.
The Panic of 1857 led to calls for protectionist tariff revision. Well-known economist Henry C. Carey blamed the Panic on the Tariff of 1857. His opinion was widely circulated in the high tariff (or "protectionist") media. 
Efforts to revise the tariff schedules upward began in earnest in the 35th Congress of 1857–1859. Two proposals were submitted in the House. House Ways and Means Committee chairman John S. Phelps (D-Missouri wrote the Democrats' plan, which retained most of the low rates of the 1857 Tariff, with minor revisions to stimulate revenue.
Minority Ways and Means members Morrill and Henry Winter Davis (a Maryland "American") produced the Republican proposal, an upward revision of the tariff schedule. It replaced the existing "ad valorem" tariff schedule with specific duties and drastically increased tariff rates on goods produced by popular "protected" industries, such as iron, textiles, and other manufactured goods. Economic historian Frank Taussig argued that in many cases, the substitution of specific duties was used to disguise the extent of the rate increases. Supporters of the specific rates argued that they were necessary, though, because European exporters were routinely providing their American customers with phony invoices showing lower prices for goods than were actually paid. Specific rates made such subterfuge pointless.
However, the House took no action on either tariff bill during the 35th Congress.
House actions.
When the 36th Congress met in 1859, action remained blocked by a wrangle over the Speaker of the House until 1860, when Republican William Pennington of New Jersey was elected. A pro-tariff Republican majority was appointed to Ways and Means, and John Sherman of Ohio became chairman.
The Morrill bill was passed out of committee and brought up for a floor vote near the end of first session of the Congress (December 1859 – June 1860).
The vote was on May 10, 1860; the bill passed by a vote of 105 to 64.
The vote was largely but not entirely sectional. Republicans, all from the northern states, voted 89–2 for the bill. They were joined by 7 northern Democrats from New York, New Jersey, and Pennsylvania. Five of these were "anti-Lecompton Democrats" (dissident Democrats who opposed the pro-slavery Lecompton constitution for Kansas).
14 northern Democrats voted against the bill.
In the Border States, 4 "Opposition" Representatives from Kentucky voted for it, as did its co-sponsor Winter of Maryland, a Maryland "Unionist", and a Democrat from Delaware. 8 Border state Democrats and an "American" from Missouri voted no.
35 southern Democrats and 3 Oppositionists voted against it; one Oppositionist from Tennessee voted for it.
Thus the sectional breakdown was 96–15 in the north, 7–9 in the Border, and 1–39 in the south. 
There were 55 abstentions, including 13 Republicans, 12 northern Democrats, 13 southern Democrats, and 8 southern "Oppositionists" and "Americans". (The remaining Representatives were mostly "paired" with opposing Representatives who could not be present.
Senate action.
The Morrill bill was sent on to the Senate. However, the Senate was controlled by Democrats, and so the bill was bottled up in the Finance Committee, chaired by Robert M. T. Hunter of Virginia.
This insured that the Senate vote would be put off till the second session in December. It also meant that the tariff would be a prominent issue in the 1860 election.
1860 election.
The Republican party included a strong pro-tariff plank in its 1860 platform. They also sent prominent tariff advocates such as Morrill and Sherman to campaign in Pennsylvania and New Jersey, where the tariff was popular, by touting the Morrill bill. Both Democratic candidates, John C. Breckinridge and Stephen Douglas, opposed all high tariffs and protectionism in general.
Historian Reinhard H. Luthin documents the importance of the Morrill Tariff to the Republicans in the 1860 presidential election. Abraham Lincoln's record as a protectionist and support for the Morrill Tariff bill, he notes, helped him to secure support in the important electoral college state of Pennsylvania, as well as neighboring New Jersey. Lincoln carried Pennsylvania handily in November, as part of his sweep of the North.
On February 14, 1861, President-elect Lincoln told an audience in Pittsburgh that he would make a new tariff his priority in the next session if the bill did not pass by inauguration day on March 4.
Renewed Senate action.
The second session of the 36th Congress began in December 1860. At first it appeared that Hunter would keep the Morrill bill tabled until the end of the term in March.
However, in December 1860 and January 1861, seven southern states declared secession, and their low-tariff Senators withdrew. Republicans took control of the Senate in February, and Hunter lost his hold on the Finance Committee.
Meanwhile the Treasury was in financial crisis, with less than $500,000 on hand and millions in unpaid bills. The Union urgently needed new revenue. A recent historian concludes, "the impetus for revising the tariff arose as an attempt to augment revenue, stave off 'ruin,' and address the accumulating debt."
The Morrill bill was brought to the Senate floor for a vote on February 20, and passed 25 to 14. The vote was split almost completely down party lines. It was supported by 24 Republicans and Democrat William Bigler of Pennsylvania. It was opposed by 10 Southern Democrats, 2 Northern Democrats, and 2 Far West Democrats. 12 Senators abstained, including 3 Northern Democrats, 1 California Democrat, 5 Southern Democrats, 2 Republicans, and 1 Unionist from Maryland.
There were some minor amendments related to the tariffs on tea and coffee, which required a conference committee with the House, but these were resolved and the final bill was approved by unanimous consent on March 2.
Though a Democrat himself, outgoing President James Buchanan favored the bill because of the interests of his home state, Pennsylvania. He signed the bill into law as one of his last acts in office.
Adoption and amendments.
The Morrill Tariff took effect one month after it was signed into law. Besides setting tariff rates, the bill altered and restricted the Warehousing Act of 1846.
The Morrill Tariff was drafted and passed the House before the Civil War began or was even expected, and was passed by the Senate almost unchanged. Thus it should not be considered "Civil War" legislation.
In fact, the Tariff proved to be too low for the revenue needs of the Civil War, and was quickly supplanted by the Second Morrill Tariff, or Revenue Act of 1861, later that fall.
Impact.
In its first year of operation, the Morrill Tariff increased the effective rate collected on dutiable imports by approximately 70%. In 1860 American tariff rates were among the lowest in the world and also at historical lows by 19th century standards, the average rate for 1857 through 1860 being around 17% overall ("ad valorem]", or 21% on dutiable items only. The Morrill Tariff immediately raised these averages to about 26% overall or 36% on dutiable items, and further increases by 1865 left the comparable rates at 38% and 48%. Although higher than in the immediate antebellum period, these rates were still significantly lower than between 1825 and 1830, when rates had sometimes been over 50%.
The United States needed $3 billion to pay for the immense armies and fleets raised to fight the Civil War — over $400 million just in 1862. The chief source of Federal revenue had been the tariff revenues. Therefore Secretary of the Treasury Salmon P. Chase, though a long-time free-trader, worked with Morrill to pass a second tariff bill in summer 1861, raising rates another 10 points in order to generate more revenues. These subsequent bills were primarily revenue driven to meet the war's needs, though they enjoyed the support of protectionists such as Carey, who again assisted Morrill in the bill's drafting.
However, the tariff played only a modest role in financing the war. It was far less important than other measures, such as $2.8 billion in bond sales and some printing of Greenbacks. Customs revenue from tariffs totaled $345 million from 1861 through 1865, or 43% of all federal tax revenue, while spending on the Army and Navy totalled $3,065 million.
Reception abroad.
The Morrill Tariff was met with intense hostility in Britain, where the free trade movement dominated public opinion. Southern diplomats and agents sought to use British ire towards the Morrill Tariff in order to garner sympathy, with the aim of obtaining British recognition for the Confederacy. The new tariff schedule heavily penalized British iron, clothing, and manufactured exports with new taxes and sparked public outcry from many British politicians. The expectation of high tax rates probably caused British shippers to hasten their deliveries before the new rates took effect in the early summer of 1861. When complaints were heard from London, Congress counterattacked. The Senate Finance Committee chairman snapped, "What right has a foreign country to make any question about what we choose to do?"
When the American Civil War broke out in 1861, British public opinion was sympathetic to the Confederacy, in part because of lingering agitation over the tariff. As one diplomatic historian has explained, the Morrill Tariff:
"Not unnaturally gave great displeasure to England. It greatly lessened the profits of the American markets to English manufacturers and merchants, to a degree which caused serious mercantile distress in that country. Moreover, the British nation was then in the first flush of enthusiasm over free trade, and, under the lead of extremists like Cobden and Gladstone, was inclined to regard a protective tariff as essentially and intrinsically immoral, scarcely less so than larceny or murder. Indeed, the tariff was seriously regarded as comparable in offensiveness with slavery itself, and Englishmen were inclined to condemn the North for the one as much as the South for the other. "We do not like slavery," said Palmerston to Adams, "but we want cotton, and we dislike very much your Morrill tariff."
Many prominent British writers condemned the Morrill Tariff in the strongest terms. Economist William Stanley Jevons denounced it as a "retrograde" law. The well known novelist Charles Dickens used his magazine, "All the Year Round," to attack the new tariff. On December 28, 1861 Dickens published a lengthy article, believed to be written by Henry Morley,
which blamed the American Civil War on the Morrill Tariff:
If it be not slavery, where lies the partition of the interests that has led at last to actual separation of the Southern from the Northern States? …Every year, for some years back, this or that Southern state had declared that it would submit to this extortion only while it had not the strength for resistance. With the election of Lincoln and an exclusive Northern party taking over the federal government, the time for withdrawal had arrived … The conflict is between semi-independent communities [in which] every feeling and interest [in the South] calls for political partition, and every pocket interest [in the North] calls for union … So the case stands, and under all the passion of the parties and the cries of battle lie the two chief moving causes of the struggle. Union means so many millions a year lost to the South; secession means the loss of the same millions to the North. The love of money is the root of this, as of many other evils... [T]he quarrel between the North and South is, as it stands, solely a fiscal quarrel.
Communist philosopher Karl Marx was among the few writers in Britain who saw slavery as the major cause of the war. Marx wrote extensively in the British press and served as a London correspondent for several North American newspapers including Horace Greeley's "New York Tribune." Marx reacted to those who blamed the war on Morrill's bill, arguing instead that slavery had induced secession and that the tariff was just a pretext. Marx wrote, in October 1861:
Naturally, in America everyone knew that from 1846 to 1861 a free trade system prevailed, and that Representative Morrill carried his protectionist tariff through Congress only in 1861, after the rebellion had already broken out. Secession, therefore, did not take place because the Morrill tariff had gone through Congress, but, at most, the Morrill tariff went through Congress because secession had taken place.
Legacy.
According to historian Heather Cox Richardson, Morrill intended to offer protection to both the usual manufacturing recipients and a broad group of agricultural interests. The purpose was to appease interests beyond the northeast, which traditionally supported protection. For the first time protection was extended to every major farm product. 
Planning to distribute the benefits of a tariff to all sectors of the economy, and also hoping to broaden support for his party, Morrill rejected the traditional system of protection by proposing tariff duties on agricultural, mining, and fishing products, as well as on manufactures. Sugar, wool, flaxseed, hides, beef, pork, corn, grain, hemp, wool, and minerals would all be protected by the Morrill Tariff. The duty on sugar might well be expected to appease Southerners opposed to tariffs, and, notably, wool and flaxseed production were growing industries in the West. The new tariff bill also would protect coal, lead, copper, zinc, and other minerals, all of which the new northwestern states were beginning to produce. The Eastern fishing industry would receive a duty on dried, pickled, and salted fish. "In adjusting the details of a tariff," Morrill explained with a rhetorical flourish in his introduction of the bill, "I would treat agriculture, manufactures, mining, and commerce, as I would our whole people—as members of one family, all entitled to equal favor, and no one to be made the beast of burden to carry the packs of others."
According to Taussig, "Morrill and the other supporters of the act of 1861 declared that their intention was simply to restore the rates of 1846." However, he also gives reason to suspect that the bill's motives were intended to put high rates of protection on iron and wool to attract states in the West and in Pennsylvania:
"The important change which they (the sponsors) proposed to make from the provisions of the tariff of 1846 was to substitute specific for ad-valorem duties. Such a change from ad-valorem to specific duties is in itself by no means objectionable; but it has usually been made a pretext on the part of protectionists for a considerable increase in the actual duties paid. When protectionists make a change of this kind, they almost invariably make the specific duties higher than the ad-valorem duties for which they are supposed to be an equivalent...The Morrill tariff formed no exception to the usual course of things in this respect. The specific duties which it established were in many cases considerably above the ad-valorem duties of 1846. The most important direct changes made by the act of 1861 were in the increased duties on iron and on wool, by which it was hoped to attach to the Republican party Pennsylvania and some of the Western States"
Henry Carey, who assisted Morrill while drafting the bill and was one of its most vocal supporters, strongly emphasized its importance to the Republican Party in his January 2, 1861 letter to Lincoln. Carey told the President-Elect "the success of your administration is wholly dependent upon the passage of the Morrill bill at the present session." According to Carey:
"With it, the people will be relieved — your term will commence with a rising wave of prosperity — the Treasury will be filled and the party that elected you will be increased and strengthened. Without it, there will be much suffering among the people — much dissatisfaction with their duties — much borrowing on the part of the Government — & very much trouble among the Republican Party when the people shall come to vote two years hence. There is but one way to make the Party a permanent one, & that is, by the prompt repudiation to the free trade system."
Congressman John Sherman later wrote: 
The Morrill tariff bill came nearer than any other to meeting the double requirement of providing ample revenue for the support of the government and of rendering the proper protection to home industries. No national taxes, except duties on imported goods, were imposed at the time of its passage. The Civil War changed all this, reducing importations and adding tenfold to the revenue required. The government was justified in increasing existing rates of duty, and in adding to the dutiable list all articles imported, thus including articles of prime necessity and of universal use. In addition to these duties, it was compelled to add taxes on all articles of home production, on incomes not required for the supply of actual wants, and, especially, on articles of doubtful necessity, such as spirits, tobacco and beer. These taxes were absolutely required to meet expenditures for the army and navy, for the interest on the war debts and just pensions to those who were disabled by the war, and to their widows and orphans.
Secession and tariffs.
The Morrill Tariff and the secession movement.
The Morrill tariff was adopted against the backdrop of the secession movement, and provided an issue for secessionist agitation in some southern states. The law's critics compared it to the 1828 Tariff of Abominations that sparked the Nullification Crisis, although its average rate was significantly lower.
Slavery dominated the secession debate in the southern states, but the Morrill Tariff was addressed in the conventions of Georgia and South Carolina. On November 19, 1860 Senator Robert Toombs gave a speech to the Georgia legislature in which he denounced the "infamous Morrill bill." The tariff legislation, he argued, was the product of a coalition between abolitionists and protectionists in which "the free-trade abolitionists became protectionists; the non-abolition protectionists became abolitionists." Toombs described this coalition as "the robber and the incendiary... united in joint raid against the South." Anti-tariff sentiments also appeared in Georgia's Secession Declaration of January 29, 1861, written in part by Toombs. 
Robert Barnwell Rhett similarly railed against the then-pending Morrill Tariff before the South Carolina convention. Rhett included a lengthy attack on tariffs in the "Address of South Carolina to Slaveholding States", which the convention adopted on December 25, 1860 to accompany its secession ordinance.
And so with the Southern States, towards the Northern States, in the vital matter of taxation. They are in a minority in Congress. Their representation in Congress, is useless to protect them against unjust taxation; and they are taxed by the people of the North for their benefit, exactly as the people of Great Britain taxed our ancestors in the British parliament for their benefit. For the last forty years, the taxes laid by the Congress of the United States have been laid with a view of subserving the interests of the North. The people of the South have been taxed by duties on imports, not for revenue, but for an object inconsistent with revenue— to promote, by prohibitions, Northern interests in the productions of their mines and manufactures.
The Morrill Tariff played less prominently elsewhere in the South. In some portions of Virginia, secessionists promised a new protective tariff to assist the state's fledgling industries.
In the North, enforcement of the Morrill Tariff contributed to support for the Union cause among industrialists and merchant interests. Speaking of this class, the abolitionist Orestes Brownson derisively remarked that "the Morrill Tariff moved them more than the fall of Sumter." In one such example the New York Times, which had previously opposed Morrill's bill on free trade grounds, editorialized that the tariff imbalance would bring commercial ruin to the North and urged its suspension until the secession crisis passed. "We have imposed high duties on our commerce at the very moment the seceding states are inviting commerce to their ports by low duties." As secession became more evident and the fledgling Confederacy adopted a much lower tariff of its own, the paper urged military action to enforce the Morrill Tariff in the Southern states.
Historiography.
Historians, James Huston notes, have been baffled by the role of high tariffs in general and have offered multiple conflicting interpretations over the years. (Low tariffs, all historians agree, were noncontroversial and were needed to fund the federal government.) One school of thought says the Republicans were the willing tools of would-be monopolists. A second schools says the Republicans truly believed tariffs would promote nationalism and prosperity for everyone along with balanced growth in every region (as opposed to growth only in the cotton South). A third school emphasizes the undeniable importance of the tariff in cementing party loyalty, especially in industrial states. Another approach emphasizes that factory workers were eager for high tariffs because it protected their high wages from European competition.
Charles A. Beard argued in the 1920s that very long-term economic issues were critical, with the pro-tariff industrial Northeast forming a coalition with the anti-tariff agrarian Midwest against the plantation South. According to Luthin in the 1940s, "Historians are not unanimous as to the relative importance which Southern fear and hatred of a high tariff had in causing the secession of the slave states." However, none of the statesmen seeking a compromise in 1860-61 that would avert the war ever suggested the tariff might be the key to a solution, or might be a cause of the secession. Beginning in the 1950s, historians moved away from the Beard thesis of economic causality. In its place, historians led by Richard Hofstadter began to emphasize the social causes of the war, centered around the issue of slavery. The Beard thesis has enjoyed a recent revival among economists, pro-Confederate historians, and neo-Beardian scholars. A 2002 study by economists Robert McGuire and T. Norman Van Cott concluded:
A de facto constitutional mandate that tariffs lie on the lower end of the Laffer relationship means that the Confederacy went beyond simply observing that a given tax revenue is obtainable with a "high" and "low" tax rate, a la Alexander Hamilton and others. Indeed, the constitutional action suggests that the tariff issue may in fact have been even more important in the North–South tensions that led to the Civil War than many economists and historians currently believe."
Economist Thomas DiLorenzo asserts that the tariff was the primary cause of the Civil War. Nearly all Civil War historians disagree. Rather than causing secession, Marc-William Palen notes how the tariff was only able to pass through Congress following the secession of Southern states. Thus, secession itself allowed for the bill's passage, rather than the other way around. Allan Nevins and James M. McPherson downplay the significance of the tariff dispute, arguing that it was peripheral to the issue of slavery. They note that slavery dominated the secessionist declarations, speeches, and pamphlets. Nevins also points to the argument of Alexander Stephens, who disputed Toombs' claims about the severity of the Morrill tariff. Though initially a unionist, Stephens would later cite slavery as the "cornerstone" reason behind his support of the secessionist cause.

</doc>
<doc id="55589" url="http://en.wikipedia.org/wiki?curid=55589" title="Homestead Acts">
Homestead Acts

The Homestead Acts were several United States federal laws that gave an applicant ownership of land, typically called a "homestead", at little or no cost. In the United States, this originally consisted of grants totaling 160 acres (65 hectares, or one-quarter section) of unappropriated federal land within the boundaries of the public land states. An extension of the "Homestead Principle" in law, the United States Homestead Acts were initially proposed as an expression of the "Free Soil" policy of Northerners who wanted individual farmers to own and operate their own farms, as opposed to Southern slave-owners who could use groups of slaves to economic advantage.
The first of the acts, the Homestead Act of 1862, was signed into law by President Abraham Lincoln on May 20, 1862. Anyone who had never taken up arms against the U.S. government (including freed slaves and women), was 21 years or older, or the head of a family, could file an application to claim a federal land grant. There was also a residency requirement.
Several additional laws were enacted in the latter half of the 19th and early 20th centuries. The Southern Homestead Act of 1866 sought to address land ownership inequalities in the south during "Reconstruction". The Timber Culture Act of 1873 granted land to a claimant who was required to plant trees. The tract could be added to an existing homestead claim and had no residency requirement. The Kincaid Amendment of 1904 granted a full section (640 acres) to new homesteaders settling in western Nebraska. An amendment to the Homestead Act of 1862, the Enlarged Homestead Act, was passed in 1909 and doubled the allotted acreage to 320. Another amended act, the national Stock-Raising Homestead Act, was passed in 1916 and again increased the land involved, this time to 640 acres.
Background.
The first Homestead Act had originally been proposed by northern Republicans before the Civil War, but had been blocked in Congress by southern Democrats who wanted western lands open for settlement by slave-owners. The Homestead Act of 1860 did pass in Congress, but it was stopped by President James Buchanan with a presidential veto. After the Southern states seceded from the Union in 1861 and their representatives left Congress, the Republican Congress passed the long-delayed bill. It was signed into law by President Abraham Lincoln on May 20, 1862. Daniel Freeman became the first person to file a claim under the new act.
Between 1862 and 1934, the federal government granted 1.6 million homesteads and distributed 270000000 acre of federal land for private ownership. This was a total of 10% of all land in the United States. Homesteading was discontinued in 1976, except in Alaska, where it continued until 1986.
About 40 percent of the applicants who started the process were able to complete it and obtain title to their homesteaded land.
History.
Donation Land Claim Act of 1850.
The Donation Land Claim Act allowed settlers to claim land in the Oregon Territory, then including the modern states of Washington, Oregon, Idaho and parts of Wyoming. Settlers were able to claim 320 or 640 acres of land for free between 1850 and 1854, and then at a cost of $1.25 per acres until the law expired in 1855.
Homestead Act of 1862.
The "yeoman farmer" ideal of Jeffersonian democracy was still a powerful influence in American politics during the 1840–1850s, with many politicians believing a homestead act would help increase the number of "virtuous yeomen". The Free Soil Party of 1848–52, and the new Republican Party after 1854, demanded that the new lands opening up in the west be made available to independent farmers, rather than wealthy planters who would develop it with the use of slaves forcing the yeomen farmers onto marginal lands. Southern Democrats had continually fought (and defeated) previous homestead law proposals, as they feared free land would attract European immigrants and poor Southern whites to the west. After the South seceded and their delegates left Congress in 1861, the Republicans and other supporters from the upper South passed a homestead act.
The intent of the first Homestead Act, passed in 1862, was to liberalize the homesteading requirements of the Preemption Act of 1841. Its leading advocates were Andrew Johnson, George Henry Evans and Horace Greeley.
The homestead was an area of public land in the West (usually 160 acres) granted to any US citizen willing to settle on and farm the land for at least five years. The law (and those following it) required a three-step procedure: file an application, improve the land, and file for deed of title. Anyone who had never taken up arms against the U.S. government (including freed slaves) and was at least 21 years old or the head of a household, could file an application to claim a federal land grant. The occupant had to reside on the land for five years, and show evidence of having made improvements.
Southern Homestead Act of 1866.
Enacted to allow poor tenant farmers and sharecroppers in the south become land owners in the southern United States during reconstruction. [ It was not very successful, as even the low prices and fees were often too much for the applicants to afford.
The Timber Culture Act of 1873.
The Timber Culture Act granted up to 160 acres of land to a homesteader who would plant at least 40 acres of trees over a period of several years. This quarter-section could be added to an existing homestead claim, offering a total of 320 acres to a settler.
Kinkaid Amendment of 1904.
Recognizing that arid lands west of the 100th meridian, which passes through central Nebraska, required more than 160 acres for a claimant to support a family, Congress passed the Kinkaid Act which granted larger homestead tracts, up to 640 acres, to homesteaders in western Nebraska.
Enlarged Homestead Act of 1909.
Because by the early 1900s much of the prime low-lying alluvial land along rivers had been homesteaded, the "Enlarged Homestead Act" was passed in 1909. To enable dryland farming, it increased the number of acres for a homestead to 320 acre given to farmers who accepted more marginal lands (especially in the Great Plains), which could not be easily irrigated.
A massive influx of these new farmers, combined with inappropriate cultivation techniques and misunderstanding of the ecology, led to immense land erosion and eventually the Dust Bowl of the 1930s.
The Stock-Raising Homestead Act of 1916.
In 1916, the "Stock-Raising Homestead Act" was passed for settlers seeking 640 acre of public land for ranching purposes.
Subsistence Homesteads provisions under the New Deal – 1930.
Renewed interest in homesteading was brought about by U.S. President Franklin D. Roosevelt's program of Subsistence Homesteading implemented in the 1930s under the New Deal.
Homesteading requirements.
The Homestead Acts had few qualifying requirements. A "homesteader" had to be the head of the household or at least twenty-one years old. They had to live on the designated land, build a home, make improvements, and farm it for a minimum of five years. The filing fee was eighteen dollars (or ten to temporarily hold a claim to the land).
Immigrants, farmers without their own land, single women, and former slaves could all qualify. The fundamental racial qualification was that one had to be a citizen, or have filed a declaration of intention to become a citizen, and so the qualification changed over the years with the varying legal qualifications for citizenship. African-Americans became qualified with the passage of the Fourteenth Amendment in 1868. South Asians and East Asians who had been born in the United States became qualified with the decision of United States v. Wong Kim Ark in 1898, but little high-quality land remained available by that time. For immigrants the fundamental qualification was that they had to be permitted to enter the country (which was usually co-extensive with being allowed to file a declaration of intention to become a citizen). During the 1800s, the bulk of immigrants were from Europe, with immigrants from South Asia and East Asia being largely excluded, and (voluntary) immigrants from Africa were permitted but uncommon.
The act in practice.
Settlers found land and staked their claims, usually in individual family units, although others formed closer knit communities. Often, the homestead consisted of several buildings or structures besides the main house.
The Homestead Act of 1862 gave rise later to a new phenomenon, large land rushes, such as the Oklahoma Land Runs of the 1880s and 90s.
End of homesteading.
The Federal Land Policy and Management Act of 1976 ended homesteading; by that time, federal government policy had shifted to retaining control of western public lands. The only exception to this new policy was in Alaska, for which the law allowed homesteading until 1986.
The last claim under this Act was made by Ken Deardorff for 80 acre of land on the Stony River in southwestern Alaska. He fulfilled all requirements of the homestead act in 1979 but did not receive his deed until May 1988. He is the last person to receive title to land claimed under the Homestead Acts.
Criticism.
The homestead acts were much abused. Although the intent was to grant land for agriculture, in the arid areas east of the Rocky Mountains, 640 acre was generally too little land for a viable farm (at least prior to major federal public investments in irrigation projects). In these areas, people manipulated the provisions of the act to gain control of resources, especially water. A common scheme was for an individual, acting as a front for a large cattle operation, to file for a homestead surrounding a water source, under the pretense that the land was to be used as a farm. Once the land was granted, other cattle ranchers would be denied the use of that water source, effectively closing off the adjacent public land to competition. That method was also used by large businesses and speculators to gain ownership of timber and oil-producing land. The federal government charged royalties for extraction of these resources from public lands. On the other hand, homesteading schemes were generally pointless for land containing "locatable minerals," such as gold and silver, which could be controlled through mining claims under the Mining Act of 1872, for which the federal government did not charge royalties.
The government developed no systematic method to evaluate claims under the homestead acts. Land offices relied on affidavits from witnesses that the claimant had lived on the land for the required period of time and made the required improvements. In practice, some of these witnesses were bribed or otherwise colluded with the claimant.
Although not necessarily fraud, it was common practice for the eligible children of a large family to claim nearby land as soon as possible. After a few generations, a family could build up a sizable estate.
The homesteads were criticized as too small for the environmental conditions on the Great Plains; a homesteader using 19th-century animal-powered tilling and harvesting could not have cultivated the 1500 acres later recommended for dry land farming. Some scholars believe the acreage limits were reasonable when the act was written, but reveal that no one understood the physical conditions of the plains.
According to Hugh Nibley, much of the rain forest west of Portland, Oregon was acquired by the Oregon Lumber Company by illegal claims under the Act.
The Homestead Act also brought settlers into conflict with indigenous Americans, displacing them, and accelerating the decline in their population.
Related acts in other countries.
Canada.
Similar laws were passed in Canada:
Elsewhere in the British Empire.
Similar in intent, the British Crown Lands Acts were extended to several of the Empire's territories, and many are still in effect, to some extent, today. For instance, the Australian selection acts were passed in the various Australian colonies following the first, in 1861, in New South Wales.
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="55592" url="http://en.wikipedia.org/wiki?curid=55592" title="Bob Black">
Bob Black

Robert Charles "Bob" Black, Jr. (born January 4, 1951) is an American anarchist. He is the author of the books "The Abolition of Work and Other Essays", "Beneath the Underground", "Friendly Fire", "Anarchy After Leftism", "Defacing the Currency", and numerous political essays.
Biography.
Black graduated from the University of Michigan and Georgetown Law School. He later took M.A. degrees in jurisprudence and social policy from the University of California (Berkeley), criminal justice from the State University of New York (SUNY) at Albany, and an LL.M in criminal law from the SUNY Buffalo School of Law. During his college days (1969-1973) he became disillusioned with the New Left of the 1970s and undertook extensive readings in anarchism, utopian socialism, council communism, and other left tendencies critical of both Marxism–Leninism and social democracy. He found some of these sources at the Labadie Collection at the University of Michigan, a major collection of radical, labor, socialist, and anarchist materials which is now the repository for Black's papers and correspondence. He was soon drawn to Situationism, egoist communism, and the anti-authoritarian analyses of John Zerzan and the Detroit magazine Fifth Estate. He produced a series of ironic political posters signed "The Last International", first in Ann Arbor, Michigan, then in San Francisco where he moved in 1978. In the Bay Area he became involved with the publishing and cultural underground, writing reviews and critiques of what he called the "marginals milieu." Since 1988 he has lived in upstate New York.
Black is best known for a 1985 essay, "The Abolition of Work," which has been widely reprinted and translated into at least thirteen languages (most recently, Urdu). In it he argued that work is a fundamental source of domination, comparable to capitalism and the state, which should be transformed into voluntary "productive play." Black acknowledged among his inspirations the French utopian socialist Charles Fourier, the British utopian socialist William Morris, the Russian anarcho-communist Peter Kropotkin, and the Situationists. "The Abolition of Work and Other Essays",
published by Loompanics in 1986, included, along with the title essay, some of his short Last International texts, and some essays and reviews reprinted from his column in "San Francisco's Appeal to Reason," a leftist and counter-cultural tabloid published from 1980 to 1984.
Two more essay collections were later published as books, "Friendly Fire" (Autonomedia, 1992) and "Beneath the Underground" (Feral House, 1994), the latter devoted to the do-it-yourself/fanzine subculture of the '80s and '90s which he called "the marginals milieu" and in which he had been heavily involved. "Anarchy after Leftism" (C.A.L. Press, 1996) is a more or less point-by-point rebuttal of Murray Bookchin's "Social Anarchism or Lifestyle Anarchism: An Unbridgeable Chasm" (A.K. Press, 1996), which had criticized as "lifestyle anarchism" various nontraditional tendencies in contemporary anarchism. Black's short book ("about an even shorter book," as he put it) was succeeded—as an E-book published in 2011 at the online Anarchist Library—by "Nightmares of Reason", a longer and more wide-ranging critique of Bookchin's anthropological and historical arguments, especially Bookchin's espousal of "libertarian municipalism" which Black ridiculed as "mini-statism."
Since 2000, Black has focused on topics reflecting his education and reading in the sociology and the ethnography of law, resulting in writings often published in "Anarchy: A Journal of Desire Armed." His recent interests have included the anarchist implications of dispute resolution institutions in stateless primitive societies (arguing that mediation, arbitration, etc., cannot feasibly be annexed to the U.S. criminal justice system, because they presuppose anarchism and a relative social equality not found in state/class societies). At the 2011 annual B.A.S.T.A.R.D. anarchist conference in Berkeley, California, Black presented a workshop where he argued that, in society as it is, crime can be an anarchist method of social control, especially for people systematically disserved by the legal system. An article based on this presentation appeared in "Anarchy" magazine and in his 2013 book, "Defacing the Currency: Selected Writings, 1992-2012".
Black has expressed an interest, which grew out of his polemics with Bookchin, in the relation of democracy to anarchism. For Bookchin, democracy—the "direct democracy" of face-to-face assemblies of citizens—is anarchism. Some contemporary anarchists agree, including the academics Cindy Milstein, David Graeber, and Peter Staudenmeier. Black, however, has always rejected the idea that democracy (direct or representative) is anarchist. He made this argument at a presentation at the Long Haul Bookshop (in Berkeley) in 2008. In 2011, C.A.L. Press published as a pamphlet "Debunking Democracy", elaborating on the speech and providing citation support. This too is reprinted in "Defacing the Currency".
Writing.
Some of his work from the early 1980s includes (anthologized in "The Abolition of Work and Other Essays") highlights his critiques of the nuclear freeze movement ("Anti-Nuclear Terror"), the editors of "Processed World" ("Circle A Deceit: A Review of "Processed World""), radical feminists ("Feminism as Fascism"), and right wing libertarians ("The Libertarian As Conservative"). Some of these essays previously appeared in "San Francisco's Appeal to Reason" (1981-1984), a leftist and counter-cultural tabloid newspaper for which Black wrote a column.
"The Abolition of Work".
"To demonize state authoritarianism while ignoring identical albeit contract-consecrated subservient arrangements in the large-scale corporations which control the world economy is fetishism at its worst ... Your foreman or supervisor gives you more or-else orders in a week than the police do in a decade."—Bob Black, "The Libertarian As Conservative", 1984
"The Abolition of Work and Other Essays" (1986), draws upon some ideas of the Situationist International, the utopian socialists Charles Fourier and William Morris, anarchists such as Paul Goodman, and anthropologists such as Richard Borshay Lee and Marshall Sahlins. Black criticizes work for its compulsion, and, in industrial society, for taking the form of "jobs"—the restriction of the worker to a single limited task, usually one which involves no creativity and often no skill. Black's alternative is the elimination of what William Morris called "useless toil" and the transformation of useful work into "productive play," with opportunities to participate in a variety of useful yet intrinsically enjoyable activities, as proposed by Charles Fourier. "Beneath the Underground" (1992) is a collection of texts relating to what Black calls the "marginals milieu"—the do-it-yourself zine subculture which flourished in the 80s and early 90s. "Friendly Fire" (1992) is, like Black's first book, an eclectic collection touching on many topics including the Art Strike, Nietzsche, the first Gulf War and the Dial-a-Rumor telephone project he conducted with Zack Replica (1981-1983).
Black's most recent book,"Defacing the Currency: Selected Writings, 1992-2012" was published by Little Black Cart Press in 2013. It includes a lengthy (113 pages), previously unpublished critique of Noam Chomsky, "Chomsky on the Nod." Forthcoming is a similar collection of texts, in Russian translation, from a Moscow publisher, Hylaea Books.

</doc>
<doc id="55593" url="http://en.wikipedia.org/wiki?curid=55593" title="Morrill Land-Grant Acts">
Morrill Land-Grant Acts

The Morrill Land-Grant Acts are United States statutes that allowed for the creation of land-grant colleges, including the Morrill Act of 1862 (#Redirect et seq.) and the Morrill Act of 1890 (the Agricultural College Act of 1890, (26 Stat. , #Redirect et seq.))
Passage of original bill.
For 20 years prior to the first introduction of the bill in 1857, there was a political movement calling for the creation of agriculture colleges. The movement was led by Professor Jonathan Baldwin Turner of Illinois College. For example, the Michigan Constitution of 1850 called for the creation of an "agricultural school", though it was not until February 12, 1855, that Michigan Governor Kinsley S. Bingham signed a bill establishing the United States' first agriculture college, the Agricultural College of the State of Michigan, known today as Michigan State University, which served as a model for the Morrill Act.
On February 8, 1853, the Illinois Legislature adopted a resolution, drafted by Turner, calling for the Illinois congressional delegation to work to enact a land-grant bill to fund a system of industrial colleges, one in each state. Senator Lyman Trumbull of Illinois believed it was advisable that the bill should be introduced by an eastern congressman, and two months later Representative Justin Smith Morrill of Vermont introduced his bill.
Unlike the Turner Plan, which provided an equal grant to each state, the Morrill bill allocated land based on the number of senators and representatives each state had in Congress. This was more advantageous to the more populous eastern states.
The Morrill Act was first proposed in 1857, and was passed by Congress in 1859, but it was vetoed by President James Buchanan. In 1861, Morrill resubmitted the act with the amendment that the proposed institutions would teach military tactics as well as engineering and agriculture. Aided by the secession of many states that did not support the plans, this reconfigured Morrill Act was signed into law by President Abraham Lincoln on July 2, 1862.
Land-grant colleges.
The purpose of the land-grant colleges was:
 without excluding other scientific and classical studies and including military tactic, to teach such branches of learning as are related to agriculture and the mechanic arts, in such manner as the legislatures of the States may respectively prescribe, in order to promote the liberal and practical education of the industrial classes in the several pursuits and professions in life.
Under the act, each eligible state received a total of 30000 acre of federal land, either within or contiguous to its boundaries, for each member of congress the state had as of the census of 1860. This land, or the proceeds from its sale, was to be used toward establishing and funding the educational institutions described above. Under provision six of the Act, "No State while in a condition of rebellion or insurrection against the government of the United States shall be entitled to the benefit of this act," in reference to the recent secession of several Southern states and the contemporaneously raging American Civil War.
After the war, however, the 1862 Act was extended to the former Confederate states; it was eventually extended to every state and territory, including those created after 1862. If the federal land within a state was insufficient to meet that state's land grant, the state was issued "scrip" which authorized the state to select federal lands in other states to fund its institution. For example, New York carefully selected valuable timber land in Wisconsin to fund Cornell University.p. 9 The resulting management of this scrip by the university yielded one third of the total grant revenues generated by all the states, even though New York received only one-tenth of the 1862 land grant.p. 10 Overall, the 1862 Morrill Act allocated 17400000 acre of land, which when sold yielded a collective endowment of $7.55 million.p. 8
On September 11, 1862, the state of Iowa was the first to accept the terms of the Morrill Act which provided the funding boost needed for the fledgling State Agricultural College and Model Farm (eventually renamed Iowa State University of Science and Technology).
With a few exceptions (including Cornell University and the Massachusetts Institute of Technology), nearly all of the Land-Grant Colleges are public. (Cornell University, while private, administers several state-supported contract colleges that fulfill its public land-grant mission to the state of New York.)
To maintain their status as land-grant colleges, a number of programs are required to be maintained by the college. These include programs in agriculture and engineering, as well as a Reserve Officers' Training Corps program.
Expansion.
A second Morrill Act in 1890 was also aimed at the former Confederate states. This act required each state to show that race was not an admissions criterion, or else to designate a separate land-grant institution for persons of color. Among the seventy colleges and universities which eventually evolved from the Morrill Acts are several of today's historically Black colleges and universities. Though the 1890 Act granted cash instead of land, it granted colleges under that act the same legal standing as the 1862 Act colleges; hence the term "land-grant college" properly applies to both groups.
Later on, other colleges such as the University of the District of Columbia and the "1994 land-grant colleges" for Native Americans were also awarded cash by Congress in lieu of land to achieve "land-grant" status.
In imitation of the land-grant colleges' focus on agricultural and mechanical research, Congress later established programs of sea grant colleges (aquatic research, in 1966), urban grant colleges (urban research, in 1985), space grant colleges (space research, in 1988), and sun grant colleges (sustainable energy research, in 2003).
Agricultural experiment stations and cooperative extension service.
Starting in 1887, Congress also funded agricultural experiment stations and various categories of agricultural and veterinary research "under direction of" the land-grant universities. Congress later recognized the need to disseminate the knowledge gained at the land-grant colleges to farmers and homemakers. The Smith-Lever Act of 1914 started federal funding of cooperative extension, with the land-grant universities' agents being sent to virtually every county of every state. In some states, the annual federal appropriations to the land-grant college under these laws exceed the current income from the original land grants. In the fiscal year 2006 USDA Budget, $1.033 billion went to research and cooperative extension activities nationwide. For this purpose, former president Bush proposed a $1.035 billion appropriation for fiscal year 2008.

</doc>
<doc id="55595" url="http://en.wikipedia.org/wiki?curid=55595" title="Wade–Davis Bill">
Wade–Davis Bill

The Wade-Davis Bill of 1864 was a bill proposed for the Reconstruction of the South written by two Radical Republicans, Senator Benjamin Wade of Ohio and Representative Henry Winter Davis of Maryland. In contrast to President Abraham Lincoln's more lenient Ten Percent Plan, the bill made re-admittance to the Union for former Confederate states contingent on a majority in each Southern state to take the Ironclad oath to the effect they had never in the past supported the Confederacy. The bill passed both houses of Congress on July 2, 1864, but was pocket vetoed by Lincoln and never took effect. The Radical Republicans were outraged that Lincoln did not sign the bill. Lincoln wanted to mend the Union by carrying out the Ten percent plan. He believed it would be too difficult to repair all of the ties within the Union if the Wade–Davis bill passed.
Background.
The Wade-Davis Bill emerged from a plan introduced in the Senate by Ira Harris of New York in February, 1863.
It proposed to base the Reconstruction of the South on the government's power to guarantee a republican form of government. The Wade-Davis Bill was also important for national and congressional power. Although federally imposed conditions of reconstruction retrospectively seem logical, there was a widespread belief that southern Unionism would return the seceded states to the Union after the South's military power was broken. This belief was not fully abandoned until later in 1863. The provisions, critics complained, were virtually impossible to meet, thus making it likely there would be permanent national control over the southern states.
Senate voting.
Those voting for passage in the Senate were (18): 
Messrs. Anthony (R), Chandler (R), Clark (R), Conness (R), Foot (R), Harlan (R), Harris (R), Howe, Lane of Kansas (R), Morgan (R), Pomeroy (R), Ramsey (R), Sherman (R), Sprague (R), Sumner (R), Wade (R), Wilkinson (R), Wilson(R).
Those voting against passage were (14):
Messrs. Buckalew (D), Carlile (U), Davis (UU), Doolittle (R), Henderson (UU), Hendricks (D), Lane of Indiana (R), McDougall (D), Powell (D), Riddle (D), Saulsbury (D), Ten Eyck (R), Trumbull (R), Van Winkle (UU).
Lincoln's veto.
One of Lincoln's objections was to the idea that Southern states needed to "re-join" the Union (an idea that permeated the whole bill). The philosophy of the war from Lincoln's point of view was that the Southern states were not constitutionally allowed to secede in the first place and therefore were still part of the Union, even though their return to a full participation in the Union would require the fulfillment of some conditions. But he didn't think the war was being waged against "treasonous" States as such (since the refusal of the Union to recognize their right to secede made the ordinances of secession null) but merely to "compel the obedience of rebellious individuals". The problem was that the language of the bill was at times undermining the Northern rationale for the war by plainly asserting for instance that the Southern states were not part of the Union anymore.
Moreover, the bill compelled those states to draft new Constitutions banning slavery, which was plainly unconstitutional at the time since, in the then-absence of a Constitutional amendment on the issue (which would soon pass on its own right), Congress had no power to deal with slavery within each state.
On a more pragmatic level, Lincoln also feared the bill would sabotage his own reconstruction activities in states like Louisiana, Arkansas, and Tennessee, all of which had seceded but were under Federal occupation and control of Union governments. He believed that Wade–Davis would jeopardize state-level emancipation movements in loyal border states like Missouri and, especially, Maryland. The bill threatened to destroy the delicate political coalitions which Lincoln had begun to construct between northern and southern moderates. More broadly, it underscored how differently Lincoln and Radical Republicans viewed the Confederates. The President thought they needed to be coaxed back into peaceful coexistence while Wade–Davis treated them as traitors that needed to be punished. Lincoln ended up killing the bill with a "pocket veto" and it was not resurrected.
The aftermath.
Davis was a bitter enemy of Lincoln because he believed that Lincoln was too lenient in terms of his policies for the South. Davis and Wade issued a manifesto "To the Supporters of the Government" on August 4, 1864, that accused Lincoln of using reconstruction to secure electors in the South who would "be at the dictation of his personal ambition," and condemned what they saw as his efforts to usurp power from Congress ("the authority of Congress is "paramount" and must be respected"). The Manifesto backfired, however, and while it initially caused much debate on the nature of the Reconstruction to come, Winter Davis was not renominated for his Congressional seat. Its ideas, particularly on the fact Congress should be the main driver of the post-war process and its conception of the Presidency as a weaker office (the President "must confine himself to his executive duties – "to obey and execute", not to make the laws –, to suppress by arms armed rebellion, and leave political reorganization to Congress" ), did influence Congressional Republicans during the following years, leading to Andrew Johnson's impeachment trial.
Lincoln survived their attacks and greatly strengthened his position with a landslide victory in the 1864 election, and national passage of the 13th Amendment in February, 1865. He momentarily marginalized the Radicals in terms of shaping Reconstruction policy. After Lincoln's death, Radical Republicans battled President Andrew Johnson, who tried to continue a version of Lincoln's plan. The midterm elections of 1866 turned into a referendum on the 14th amendment and the trajectory of Reconstruction policy. With the Republicans' victory, Congress took control of Reconstruction. The radicals wanted a much harsher plan, but they did not try to reimpose the terms of Wade-Davis. Instead they took control of the southern states with the Army, which registered black men as voters and refused to allow former Confederates to run for office.

</doc>
<doc id="55596" url="http://en.wikipedia.org/wiki?curid=55596" title="National Bank Act">
National Bank Act

The National Banking Acts of 1863 and 1864 were two United States federal banking acts that established a system of national banks for banks, and created the United States National Banking System. They encouraged development of a national currency backed by bank holdings of U.S. Treasury securities and established the Office of the Comptroller of the Currency as part of the United States Department of the Treasury and authorized the Comptroller to examine and regulate nationally chartered banks. The Act shaped today's national banking system and its support of a uniform U.S. banking policy.
Background.
For most of the nineteenth century, the American banking system consisted of state-chartered banks. The paper currency issued by state-chartered banks had to be redeemable. Depending on the state, the capital requirements for banks, set forth in the bank charter, differed. If a bank could not redeem its bank notes for money (gold or silver), the bank had committed fraud and was subject to prosecution. Most of the state-chartered banks in the North and East created redeemable currency against Bills of Exchange (Real Bills) under the real bills doctrine set forth by Adam Smith. (See "The Wealth of Nations", 1776 by Adam Smith). Real Bills were negotiable instruments, payable in 90 days, which banks discounted. Real Bills were a means of financing production of consumer items moving to market. Banks created uniformly denominated redeemable bank notes against the value of the Real Bills in their inventory.
If redemption demands exhausted their gold or silver reserves, these banks could rediscount the Real Bills to obtain gold or silver. The discounting of Real Bills by banks was particularly suited to the banking business in the industrial states of the north and the east of the country. As to the state-chartered banks, predominantly located in the West and South, many practiced fractional reserve lending for lack of availability of Real Bills to discount. Fractional reserve lending depended on low demand to redeem the paper currency. Fractional reserve lending amounted to the issuance of multiple demand receipts for the same amount of gold and silver held by the banks. Holders of this kind of paper currency could redeem it only at the bank's branch office.
Though each and every state chartered bank issued its own redeemable currency, because of its denomination in US dollars, each bank's currency was nominally the same as any other. The difficulty lay in judging the ability of an issuing bank to be able to promptly redeem their currency for gold or silver. It was therefore extremely difficult for currency to serve as a means of exchange for inter-regional parties. In 1816, the Second Bank of the United States was chartered by the U.S. Congress for a twenty-year period to create irredeemable currency with which to pay for the costs of the War of 1812. The creation of congressionally authorized irredeemable currency by the Second Bank of the United States amounted to taxation by inflation. The Congress did not want state-chartered banks as competition in the inflation of currency.
Because fractional reserve banking does amount to inflating a currency, the states agreed to curtail fractional reserve banking engaged in by the "wildcat" banks by tying the amount of currency these banks could issue to the amount of gold and silver coin they held. When the charter for the Second Bank of the United States expired in 1836, "wildcat" banks resumed unsound and unregulated lending. As Americans began to head west, these institutions began to issue more and more currency as a means of facilitating land speculation. This at-will adjustment of the money supply caused all forms of currency to fluctuate wildly in value. The issuance of currency by multiple banks also led to a nationwide counterfeiting problem that left the public wondering not only how much their money was worth but whether it was real. To solve the problem, the Jackson administration passed an order that voided paper currency in the eyes of the national government: agents of the government were allowed to accept gold or silver alone as a means of payment for land. Note holders ran to banks to redeem their currency only to find that banks' stocks of gold and silver were depleted and banks were no longer extending credit.
Currency notes could be redeemed for only a fraction of their nominal value and land investors began to rely on loans from abroad. A wave of bank failures ensued, eventually leading to the Financial Panic of 1837, which included a six-year depression. Some banks remained open and continued to issue notes, signaling no distinct end to the paper currency problem. Notes issued by solvent institutions circulated along with currency of insolvent institutions and notes of some smaller, unknown banks even traded at a discount. The currency problem eventually became so bad that a magazine was issued that included photos and descriptions of the various bank notes and information about whether the issuing bank was sound.
Creation.
In 1846, the Polk Administration created a United States Treasury system that moved public funds from private banks to Treasury branches in an effort to stabilize the economy. However, there remained no national currency, a problem of increasing urgency to a wartime government strapped for cash. One of the first attempts of the nation to issue a national currency came in the early days of the Civil War when Congress approved the Legal Tender Act of 1862, allowing the issue of $150 million in national notes known as greenbacks and mandating that paper money be issued and accepted in lieu of gold and silver coins. The bills were backed only by the national government's promise to redeem them and their value was dependent on public confidence in the government as well as the ability of the government to give out specie in exchange for the bills in the future. Many thought this promise backing the bills was about as good as the green ink printed on one side, hence the name "greenbacks." 
In 1863, the Second Legal Tender Act, enacted July 11, 1862, a Joint Resolution of Congress, and the Third Legal Tender Act, enacted March 3, 1863, expanded the limit to $450 million. The largest amount of greenbacks outstanding at any one time was calculated as $447,300,203.10.
The National Banking Act (ch. 58, 12 Stat. 665; February 25, 1863), originally known as the National Currency Act, and was passed in the Senate by a narrow 23–21 vote. The main goal of this act was to create a single national currency and to eradicate the problem of notes from multiple banks circulating all at once. The Act established national banks that could issue notes which were backed by the United States Treasury and printed by the government itself. The quantity of notes that a bank was allowed to issue was proportional to the bank's level of capital deposited with the Comptroller of the Currency at the Treasury. To further control the currency, the Act taxed notes issued by state and local banks, essentially pushing non-federally issued paper out of circulation.
The National Banking Act of 1863 was superseded by the National Banking Act of 1864 (ch. 106, 13 Stat. 99; June 3, 1864) just one year later. The new act also established federally issued bank charters, which took banking out of the hands of state governments. Before the act, charters were granted by state legislatures who were under an immense amount of political pressure and could be influenced by bribes. This problem was resolved to some degree by free banking laws in some states but it was not until this act was passed that free banking was established on a uniform, national level and charter issuance was taken out of the hands of discriminating and corrupt state legislatures. The first bank to receive a national charter was the First National Bank of Philadelphia, Pennsylvania (Charter #1). The first new national bank to open was The First National Bank of Davenport, Iowa (Charter #15). Additionally, the new Act converted more than 1,500 state banks to national banks.
National Bank Act of 1863.
The National Bank Act of 1863 was designed to create a national banking system, float federal war loans, and establish a national currency. Congress passed the act to help resolve the financial crisis that emerged during the early days of the American Civil War (1861–1865). The fight with the South was expensive and no effective tax program had been drawn up to finance it. In December 1861 banks suspended specie payments (payments in gold or silver coins for paper currency called notes or bills). People could no longer convert bank notes into coins. Government responded by passing the Legal Tender Act (1862), issuing $150 million in national notes called greenbacks. However, bank notes (paper bills issued by state banks) accounted for most of the currency in circulation.
• a nationwide banking system that loaned money to the Government to pay for the war
• a national system of paper money and coins
National Bank Act of 1864.
• Brought the federal government into active supervision of commercial banks.
• Established the Office of the Comptroller of the Currency with the responsibility of chartering, examining and supervising all national banks.
• In 1865, Congress followed up on this act by imposing a 10 percent tax on state bank notes to encourage them to join the national banking system – it worked as most state banks flipped charters to avoid the tax - it was not until the 1870s and 80s when the growing popularity of checks developed by state banks and the declining importance and profitability of bank notes issued by national banks caused a resurgence in chartering of state banks.
• The Act was originally based on New York State Law 
National Bank Act of 1865-1866.
On July 13, 1866, the banking Act of 1865 was extended beyond requiring every national banking association, state bank, or state banking association to pay a 10% tax on any notes paid out by them. The act of 1866 added that any persons, as well as of state banks and state banking associations used for circulation to also be taxed 10%. This act was considered enforced by the court case Veazie bank v. Fenno, supra. The official enactment by congress read, “That every national banking association, state bank, or state banking association shall pay a tax of ten percentum on the amount of notes of any person, state bank, or state banking association used for circulation and paid out by them after the 1st day of August, 1866, and such tax shall be assessed and paid in such manner as shall be prescribed by the Commissioner of Internal Revenue.”
 The Veazie Bank v. Fenno was a case between a state chartered Maine bank and the collector of internal revenue. The bank declined to pay the new 10% tax on notes when Fenno tried to collect, saying the tax imposed was unconstitutional. The question posed to the court was, “Whether the second clause of the 9th section of the Act of Congress of the 13th of July, 1866, under which the tax in his case was levied and collected, is a valid and constitutional law." The Chief Justices ruling over the case sided with congress, in effect ending any resistance by state banks to the acts of 1865 or 1866.
Resurgence of State Banks.
The granting of charters led to the creation of many national banks and a national banking system which grew at a fast pace. The number of national banks rose from 66 immediately after the Act to 7473 in 1913. Initially, this rise in national banking came at the expense of state banking—the number of state banks dwindled from 1466 in 1863 to 247 in 1868. Though state banks were no longer allowed to issue notes, local bankers took advantage of less strict capital requirements ($10,000 for state banks vs. $50,000–200,000 for national banks) and opened new branches en masse. These new state banks then served as competition for national banks, growing to 15,526 in number by 1913.
The years leading up to the passing of the 10 % tax on bank notes consisted of events surrounding the National Banking Act of 1864. During this time period, Hugh McCulloch was determined to “fight against the national banking legislation, which he rightly perceived as a threat to state-chartered banking. Although he tried to block the system’s creation, he [McCulloch] was not determined to be its champion.” Part of his plans to revamp this portion of the banking system included hiring a new staff, being hands-on with several aspects such as “personally evaluating applications for bank charters and consoled prospective bankers”, and “assisting in the design of the new national bank notes, and arranged for their engraving, printing, and distribution.” As an end result of McCulloch’s efforts, many banks were just not willing to conform to his system of operations. This prompted Congress to pass “a 10 percent tax on the notes of state banks, signaling its determination that national banks would triumph and the state banks would fade away.”
A later act, passed on March 3, 1865, imposed a tax of 10 percent on the notes of state banks to take effect on July 1, 1866. Similar to previous taxes, this effectively forced all non-federal currency from circulation. It also resulted in the creation of demand deposit accounts, and encouraged banks to join the national system, increasing the number of national banks substantially.
Legacy.
The National Banking Acts served to create the (federal-state) dual structure that is now a defining characteristic of the U.S. banking system and economy. The Comptroller of the Currency continues to have significance in the U.S. economy and is responsible for administration and supervision of national banks as well as certain activities of bank subsidiaries (per the Gramm-Leach-Bliley Act of 1999). In 2004 the Act was used by John D. Hawke, Jr., Comptroller of the Currency, to effectively bar state attorneys general from national bank oversight and regulatory roles. Many blame the resulting lack of oversight and regulation for the late-2000s recession, the bailout of the U.S. financial system and the subprime mortgage crisis.

</doc>
<doc id="55601" url="http://en.wikipedia.org/wiki?curid=55601" title="Sleepy Hollow (film)">
Sleepy Hollow (film)

 
Sleepy Hollow is a 1999 American fantasy horror adventure film directed by Tim Burton. It is a film adaptation loosely inspired by the 1820 short story "The Legend of Sleepy Hollow" by Washington Irving and stars Johnny Depp and Christina Ricci, with Miranda Richardson, Michael Gambon, Casper Van Dien and Jeffrey Jones in supporting roles. The plot follows police constable Ichabod Crane (Depp) sent from New York City to investigate a series of murders in the village of Sleepy Hollow by a mysterious Headless Horseman.
It is the first film by Mandalay Pictures. Development began in 1993 at Paramount Pictures with Kevin Yagher originally set to direct Andrew Kevin Walker's script as a low-budget slasher film. Disagreements with Paramount resulted in Yagher being demoted to prosthetic makeup designer, and Burton was hired to direct in June 1998. Filming took place from November 1998 to May 1999, and "Sleepy Hollow" was released to generally favorable reviews from critics, and grossed approximately $207 million worldwide. Production designer Rick Heinrichs and set decorator Peter Young won the Academy Award for Best Art Direction.
Plot.
In 1799, New York City police constable Ichabod Crane (Depp) is facing imprisonment for going against traditional methods and favoring forensic investigation techniques such as finger-printing and autopsies not considered to be orthodox at that time and considered unimportant. Ichabod submits to deployment with his bag of tools to the Westchester County hamlet of Sleepy Hollow, New York, which has been plagued by a series of brutal slayings in which the victims have been found decapitated: Peter Van Garrett (Martin Landau), wealthy farmer and landowner; his son Dirk; and the widow Emily Winship. Arriving in Sleepy Hollow, Crane is informed by the town's elders that the killer is not of flesh and blood, but rather an undead headless Hessian mercenary from the American Revolutionary War who rides at night on a massive black steed in search of his missing head.
Crane begins his investigation, remaining highly skeptical about the supernatural elements in the case until he actually encounters the Headless Horseman himself, who kills the Magistrate Samuel Phillipse (Richard Griffiths) on sight. Boarding in a room at the home of the town's richest family and the Van Garretts' next of kin, the Van Tassels, Crane develops an attraction to their daughter Katrina (Christina Ricci), while he is plagued by nightmares of his mother's horrific torture when he was a child. This attraction is deeply resented by Brom van Brunt (Casper Van Dien), a suitor to Katrina, who scares Crane in a prank by posing as the Headless Horseman. Riding into the Western Woods with the orphaned Young Masbath, son of the Horseman's fifth victim (before Magistrate Phillipse), Crane and Katrina come across the cave dwelling of a reclusive sorceress. She reveals the location of the gnarled Tree of the Dead, which marks the Horseman's grave, as well as his portal into the natural world from the supernatural.
Crane discovers that the ground is freshly disturbed and, digging through, discovers the Horseman's skeleton and that the skull is missing. He realizes that whoever dug up and stole the skull is the person controlling the Horseman. Just then, the Horseman's ghost bursts out of the tree and gallops towards Sleepy Hollow. Crane attempts to follow but winds up lost. The Killian family are taken by the Horseman and Brom is killed - cut in half - when trying to stop the Horseman.
Crane starts to believe that a conspiracy links all the deaths together, so he goes to the town Notary James Hardenbrook (Michael Gough) to look into Van Garrett's Last Will. Hardenbrook confesses Van Garrett had made a new will just before he died, leaving all his possessions to his new bride, Emily Winship, who Crane had learned from the late Magistrate Phillipse was pregnant at the time of her death and thinks that the father of the child might have killed Emily to keep the secret hidden. Crane deduces that all who knew about the new will were the victims of Horseman and that Katrina's father Baltus Van Tassel (Michael Gambon), who would have inherited the fortune, is the person holding the skull. Katrina, finding out that Crane suspects her father, burns the evidence that Crane has accumulated and tells him that she doesn't love him anymore.
In fear of the Horseman, Hardenbrook hangs himself and a town council is held in the town church. The Horseman seemingly kills Katrina's stepmother, Lady Van Tassel, and heads off to the church to get Baltus, with the townspeople filing in just as the he arrives. With the men firing muskets as he circles the church, Crane realizes the Horseman can't enter the church grounds due to some protective supernatural force. A massive fight breaks out in the church when Dr. Thomas Lancaster (Ian McDiarmid) suggests confessing for forgiveness, and is killed by Reverend Steenwyck (Jeffery Jones), who is in turn shot by a frightened Baltus. The chaos ends only when the Horseman harpoons Baltus through a church window using a pointed church fence post attached to a rope, dragging him out and acquiring his head.
As Crane is leaving Sleepy Hollow, he becomes suspicious when the hand of the corpse of Lady Van Tassel has a wound which shows signs of having been caused post-mortem. His suspicions are confirmed to be right when the real Lady Van Tassel (Miranda Richardson) emerges, alive, from the dark and shocks her step-daughter Katrina into a faint. Katrina awakens and eventually uncovers a plot revolving around revenge on the Van Garretts and land rights with the Horseman controlled by Lady Van Tassel, who sends the supernatural killer after Katrina now to solidify her hold on what she considers her property, a piece of land unjustly claimed by Baltus. She explains that the body believed to be her corpse actually belonged to the family's servant, Sarah, whom she had murdered. She also reveals that she had just murdered the mysterious witch in the Western Woods, her own sister, for her role in aiding Crane and Young Masbath.
Following a fight in the local windmill and a stagecoach chase through the woods, Crane eventually thwarts Lady Van Tassel by throwing the skull to the Horseman, breaking the curse. The Horseman, no longer under Lady Van Tassel's control, simultaneously kisses and bites her, and he hoists her up on his horse, then rides to Hell taking her with him, fulfilling her end of the deal with the Devil. With the Headless Horseman menace eradicated, Crane returns home to New York with Katrina and Young Masbath, just in time for the new century.
Production.
Development.
In 1993, Kevin Yagher, a make-up effects designer who had turned to directing with "Tales from the Crypt", had the notion to adapt Washington Irving's short story "The Legend of Sleepy Hollow" into a feature film. Through his agent, Yagher was introduced to Andrew Kevin Walker; they spent a few months working on a film treatment that transformed Ichabod Crane as a schoolmaster from Connecticut to a banished New York City detective. Yagher and Walker subsequently pitched "Sleepy Hollow" to various studios and production companies, eventually securing a deal with producer Scott Rudin, who had been impressed with Walker's unproduced spec script for "Seven". Rudin optioned the project to Paramount Pictures in a deal that had Yagher set to direct, with Walker scripting; the pair would share story credit. Following the completion of "", Yahger had planned "Sleepy Hollow" as a low-budget production—"a pretentious slasher film with a spectacular murder every five minutes or so." Paramount disagreed on the concept and demoted Yagher's involvement to prosthetic makeup designer. "They never really saw it as a commercial movie," producer Adam Schroeder noted. "The studio thinks 'old literary classic' and they think "The Crucible". We started developing it before horror movies came back."
Paramount CEO Sherry Lansing revived studio interest in 1998. Schroeder, who shepherded Tim Burton's "Edward Scissorhands" as a studio executive at 20th Century Fox in 1990, suggested that Burton direct the film. Francis Ford Coppola's minimal production duties came from American Zoetrope; Burton only became aware of Coppola's involvement during the editing process when he was sent a copy of "Sleepy Hollow"‍ '​s trailer and saw Coppola's name on it. Burton, coming off the troubled production of "Superman Lives", was hired to direct in June 1998. "I had never really done something that was more of a horror film," he explained, "and it's funny, because those are the kind of movies that I like probably more than any other genre." His interest in directing a horror film influenced by his love for Hammer Film Productions and "Black Sunday"—particularly the supernatural feel they evoked as a result of being filmed primarily on sound stages. As a result, "Sleepy Hollow" is a homage to various Hammer Film Productions, including "Dr. Jekyll and Sister Hyde", and other films such as "Frankenstein", "Bride of Frankenstein", various Roger Corman horror films, "Jason and the Argonauts", and "Scream Blacula Scream". The image of the Headless Horseman had fascinated Burton during his apprenticeship as a Disney animator at CalArts in the early 1980s. "One of my teachers had worked on the Disney version as one of the layout artists on the chase, and he brought in some layouts from it, so that was exciting. It was one of the things that maybe shaped what I like to do." Burton worked with Walker on rewrites, but Rudin suggested that Tom Stoppard rewrite the script to add to the comical aspects of Ichabod's bumbling mannerisms, and emphasize the character's romance with Katrina. His work went uncredited through the WGA screenwriting credit system.
While Johnny Depp was Burton's first choice for the role of Ichabod Crane, Paramount required him to consider Brad Pitt, Liam Neeson and Daniel Day-Lewis. Depp was cast in July 1998 for his third collaboration with Burton. The actor wanted Ichabod to parallel Irving's description of the character in the short story. This included a long prosthetic snipe nose, huge ears, and elongated fingers. Paramount turned down his suggestions, and after Depp read Tom Stoppard's rewrite of the script, he was inspired to take the character even further. "I always thought of Ichabod as a very delicate, fragile person who was maybe a little too in touch with his feminine side, like a frightened little girl," Depp explained. He did not wish to portray the character as a typical action star would have, and instead took inspiration by Angela Lansbury's performance in "Death on the Nile". "It's good," Burton reasoned, "because I'm not the greatest action director in the world, and he's not the greatest action star." Depp modeled Ichabod's detective personality from Basil Rathbone in the 1939 "Sherlock Holmes" film series. He also studied Roddy McDowall's acting for additional influence. Burton added that "the idea was to try and find an elegance in action of the kind that Christopher Lee or Peter Cushing or Vincent Price had." Christina Ricci, who worked with producer Scott Rudin on "The Addams Family", was cast as Katrina Van Tassel. "Sleepy Hollow" also reunited Burton with Jeffrey Jones (from "Beetlejuice" and "Ed Wood") as Reverent Steenwyck, Christopher Walken (Max Schreck in "Batman Returns") as the Hessian Horseman, Martin Landau ("Ed Wood") in a cameo role, and Hammer veteran Michael Gough (Alfred in Burton's "Batman" films), whom Burton tempted out of retirement. The Hammer influence was further confirmed by the casting of Christopher Lee in a small role as the Burgomaster who sends Crane to Sleepy Hollow.
Filming.
The original intention had been to shoot "Sleepy Hollow" predominantly on location with a $30 million budget. Towns were scouted throughout Upstate New York along the Hudson Valley, and the filmmakers decided on Tarrytown for an October 1998 start date. The Historic Hudson Valley organization assisted in scouting locations, which included the Philipsburg Manor House and forests in the Rockefeller State Park Preserve. "They had a wonderful quality to them," production designer Rick Heinrichs reflected on the locations, "but it wasn't quite lending itself to the sort of expressionism that we were going for, which wanted to express the feeling of foreboding." Disappointed, the filmmakers scouted locations in Sturbridge, Massachusetts, and considered using Dutch colonial villages and period town recreations in the Northeastern United States. When no suitable existing location could be found, coupled with a lack of readily available studio space in the New York area needed to house the production's large number of sets, producer Scott Rudin suggested the UK.
Rudin believed England offered the level of craftsmanship in period detail, painting and costuming that was suitable for the film's design. Having directed "Batman" entirely in Britain, Burton agreed, and designers from "Batman"‍ '​s art department were employed by Paramount for "Sleepy Hollow". As a result, principal photography was pushed back to November 20, 1998 at Leavesden Film Studios, which had been recently vacated by "". The majority of filming took place at Leavesden, with studio other work at Shepperton Studios, where the massive Tree of the Dead set was built using Stage H. Production then moved to the Hambleden estate at Lime Tree Valley for a month-long shoot in March, where the town of Sleepy Hollow was constructed. "We came to England figuring we would find a perfect little town," producer Adam Schroeder recalled, "and then we had to build it anyway." Filming in Britain continued through April, and a few last minute scenes were shot using a sound stage in Yonkers, New York the following May.
Design.
Responsible for the film's production design was Rick Heinrichs, who Burton intended to use on "Superman Lives". While the production crew was always going to build a substantial number of sets, the decision was taken early on that to fulfill Burton's vision best would necessitate shooting "Sleepy Hollow" in a totally controlled environment at Leavesden Film Studios. The production design was influenced by Burton's love for Hammer Film Productions and "Black Sunday"—particularly the supernatural feel they evoked as a result of being filmed primarily on sound stages. Heinrichs was also influenced by American colonial architecture, German Expressionism, Dr. Seuss illustrations, and Hammer's "Dracula Has Risen from the Grave". One sound stage at Leavesden was dedicated to the "Forest to Field" set, for the scene in which the Headless Horseman races out of the woods and into a field. This stage was then transformed into, variously, a graveyard, a corn field, a field of harvested wheat, a churchyard, and a snowy battlefield. In addition, a small backlot area was devoted to a New York City street and waterfront tank.
Cinematography.
Burton was impressed by the cinematography in "Great Expectations", and hired Emmanuel Lubezki as "Sleepy Hollow"‍ '​s director of photography. Initially, Lubezki and Burton contemplated shooting the film in black and white and in old square Academy ratio. When that proved unfeasible, they opted for an almost monochromatic effect which would enhance the fantasy aspect. Burton and Lubezki intentionally planned the over-dependency of smoke and soft lighting to accompany the film's sole wide-angle lens strategy. Lubezki also used Hammer horror and Mexican lucha films from the 1960s, such as "Santo Contra los Zombis" and "Santo vs. las Mujeres Vampiro". Lighting effects increased the dynamic energy of the Headless Horseman, while the contrast of the film stock was increased in post-production to add to the monochromatic feel.
Leavesden Studios, a converted airplane factory, presented problems because of its relatively low ceilings. This was less of an issue for "The Phantom Menace", in which set height was generally achieved by digital means. "Our visual choices get channeled," Heinrichs elaborated, "so you end up with liabilities that you tend to exploit as virtues. When you've got a certain ceiling height, and you're dealing with painted backings, you need to push atmosphere and diffusion." This was particularly the case in several exteriors that were built on sound stages. "We would mitigate the disadvantages by hiding lights with teasers and smoke."
Visual effects.
The majority of "Sleepy Hollow"‍ '​s 150 visual effects shots were handled by Industrial Light & Magic (ILM), while Kevin Yagher supervised the human and creature effects. Framestore also assisted on digital effects, and The Mill handled motion control photography. In part a reaction to the computer-generated effects in "Mars Attacks!", Burton opted to use as limited an amount of digital effects as possible. Ray Park, who served as the Headless Horseman stunt double, wore a blue ski mask for the chroma key effect, digitally removed by ILM. Burton and Heinrichs applied to "Sleepy Hollow" many of the techniques they had used in stop motion animation on "Vincent"—such as forced perspective sets.
The windmill was a 60-foot-tall forced-perspective exterior (visible to highway travellers miles away), a base and rooftop set and a quarter-scale miniature. The interior of the mill, which was about 30-feet high and 25-feet wide, featured wooden gears equipped with mechanisms for grinding flour. A wider view of the windmill was rendered on a Leavesden soundstage set with a quarter-scale windmill, complete with rotating vanes, painted sky backdrop and special-effects fire. "It was scary for the actors who were having burning wood explode at them," Heinrichs recalled. "There were controls in place and people standing by with hoses, of course, but there's always a chance of something going wrong." For the final shot of the burning mill exploding, the quarter-scale windmill and painted backdrop were erected against the outside wall of the "flight shed", a spacious hangar on the far side of Leavesden Studios. The hangar's interior walls were knocked down to create a 450-foot run, with a 40-foot width still allowing for coach and cameras. Heinrichs tailored the sets so cinematographer Emmanuel Lubezki could shoot from above without seeing the end of the stage.
Actor Ian McDiarmid, who portrayed Dr. Lancaster, had just finished another Leavesden production with "". He compared the aesthetics of the two films, stating that physical sets helped the actors get into a natural frame of mind. "Having come from the blue-screen world of "Star Wars" it was wonderful to see gigantic, beautifully made perspective sets and wonderful clothes, and also people recreating a world. It's like the way movies used to be done."
Musical score.
The film score was written and produced by Danny Elfman. It won the Golden Satellite Award and was also nominated by the Las Vegas Film Critics.
Track listing.
Tracks marked with ♦ are only available as a bonus track on disc 8 of the .
The track numbers listed here do not therefore correspond to the original 1999 album.
Release.
To promote "Sleepy Hollow", Paramount Pictures featured the film's trailer at San Diego Comic-Con International in August 1999. The following October, the studio launched a website, which "Variety" described as being the "most ambitious online launch of a motion picture to date." The site (sleepyhollowmovie.com) offered visitors live video chats with several of the filmmakers hosted by Yahoo! Movies and enabled them to send postcards, view photos, trailers and a six-minute behind-the-scenes featurette edited from a broadcast that aired on "Entertainment Tonight". Extensive tours of 10 sets where offered, where visitors were able to roam around photographs, including the sets for the entire town of Sleepy Hollow, forest, church, graveyard and covered bridge. Arthur Cohen, president of worldwide marketing for Paramount, explained that the "Web-friendly" pre-release reports from websites such as Ain't It Cool News and Dark Horizons encouraged the studio to create the site. In the weeks pre-dating the release of "Sleepy Hollow", a toy line was marketed by McFarlane Toys. Simon & Schuster also published "The Art of Sleepy Hollow" (ISBN 0671036572), which included the film's screenplay and an introduction by Tim Burton. A novelization, also published by Simon & Schuster, was written by Peter Lerangis.
"Sleepy Hollow" was released in the United States on November 19, 1999 in 3,069 theaters, grossing $30,060,467 in its opening weekend at the #2 spot behind "The World Is Not Enough". "Sleepy Hollow" eventually earned $101,071,502 in domestic gross, and $105 million in foreign sales, coming to a worldwide total of $206,071,502. David Walsh of the National Institute on Media and the Family criticized the film's financial success from the exaggeration of gore. "The real impact is not so much that violent images create violent behavior," Walsh explained, "but that they create an atmosphere of disrespect." Burton addressed the concerns as a matter of opinion. "Everyone has a different perception of things. When I was a kid," Burton continued, "I was probably more scared by seeing John Wayne or Barbra Streisand on the big screen than by seeing violence."
Paramount Home Video first released "Sleepy Hollow" on DVD in the United States on May 23, 2000. The HD DVD release came in July 2006, while the film was released on Blu-ray Disc two years later, in June 2008.
Reception.
Film review aggregator Rotten Tomatoes reports that 67% of critics gave the film a "Certified Fresh" rating, based on 126 reviews with an average score of 6.2/10, with the site's consensus stating, "Sleepy Hollow entertains with its stunning visuals and creepy atmosphere." Metacritic, another review aggregator, assigned the film a weighted average score of 65 (out of 100) based on 35 reviews from mainstream critics, considered to be "generally favorable".
Roger Ebert praised Johnny Depp's performance and Tim Burton's methods of visual design. "Johnny Depp is an actor able to disappear into characters," Ebert continued, "never more readily than in one of Burton's films." Richard Corliss wrote, in his review for "TIME Magazine", "Burton's richest, prettiest, weirdest [film] since "Batman Returns". The simple story bends to his twists, freeing him for an exercise in high style."
David Sterritt of "The Christian Science Monitor" highly praised Burton's filmmaking and the high-spirited acting of cast, but believed Andrew Kevin Walker's writing was too repetitious and formulaic for the third act. "You go into a Tim Burton film wanting to be transported, but "Sleepy Hollow" is little more than a lavish, art-directed slasher movie."
Owen Gleiberman from "Entertainment Weekly" wrote "Sleepy Hollow" is "a choppily plotted crowd-pleaser that lacks the seductive, freakazoid alchemy of Burton's best work." Gleiberman compared the film to "The Mummy", and said "it feels like every high-powered action climax of the last 10 years. Personally, I'd rather see Burton so intoxicated by a movie that he lost his head."
Andrew Johnston of "Time Out New York" wrote: "Like the best of Burton's films, "Sleepy Hollow" takes place in a world so richly imagined that, despite its abundant terrors, you can't help wanting to step through the screen."
Mick LaSalle, writing in the "San Francisco Chronicle", criticized Burton's perceived image as a creative artist. "All "Sleepy Hollow" has going for it is art direction, and even in that it falls back on cliché." Doug Walker linked the film to the Hammer Films style of horror cinematography, considering it an homage to those movies, comparing the usage of dignified British actors, choices in color and movie sets and character relations. Walker gave it the merit of recreating the "very specific genre" of Hammer Films, citing the skill and "clever casting" Burton used to manage this.
Jonathan Rosenbaum from the "Chicago Reader" called "Sleepy Hollow" "a ravishing visual experience, a pretty good vehicle for some talented American and English actors," but concluded that the film was a missed opportunity to depict an actual representation of the short story. "Burton's fidelity is exclusively to the period feeling he gets from disreputable Hammer horror films and a few images culled from "Ichabod and Mr. Toad". When it comes to one of America's great stories, Burton obviously couldn't care less."
Accolades.
American Film Institute recognition:

</doc>
<doc id="55602" url="http://en.wikipedia.org/wiki?curid=55602" title="Peanut">
Peanut

The peanut or groundnut ("Arachis hypogaea") is a species in the family Fabaceae (commonly known as the bean, pea or legume family). The peanut was probably first domesticated and cultivated in the valleys of Paraguay. It is an annual herbaceous plant growing 30 to tall. The leaves are opposite, pinnate with four leaflets (two opposite pairs; no terminal leaflet); each leaflet is 1 to 7 cm (⅜ to 2¾ in) long and 1 to 3 cm (⅜ to 1 inch) across.
The flowers are a typical peaflower in shape, 2 to (¾ to 1½ in) across, yellow with reddish veining. The specific name, "hypogaea" means "under the earth"; after pollination, the flower stalk elongates, causing it to bend until the ovary touches the ground. Continued stalk growth then pushes the ovary underground where the mature fruit develops into a legume pod, the peanut – a classical example of geocarpy. Pods are 3 to long, normally containing 1 to 4 seeds.
Because, in botanical terms, "nut" specifically refers to indehiscent fruit, the peanut is not technically a nut, but rather a legume. Peanuts are often served in a similar manner to true nuts in many western cuisines, and are often referred to as a nut in common English.
History.
The domesticated peanut is an amphidiploid or allotetraploid, meaning that it has two sets of chromosomes from two different species, thought to be "A. duranensis" and "A. ipaensis". These probably combined in the wild to form the tetraploid species "A. monticola", which gave rise to the domesticated peanut. This domestication might have taken place in Paraguay or Bolivia, where the wildest strains grow today. Many pre-Columbian cultures, such as the Moche, depicted peanuts in their art.
Archeologists have dated the oldest specimens to about 7,600 years, found in Peru. Cultivation spread as far as Mesoamerica, where the Spanish conquistadors found the "tlalcacahuatl" (the plant's Nahuatl name, whence Mexican Spanish "cacahuate", Castillian Spanish "cacahuete," and French "cacahuète") being offered for sale in the marketplace of Tenochtitlan (Mexico City). The peanut was later spread worldwide by European traders. In West Africa farmers were already cultivating a plant from the same family, the Bambara groundnut, which also grows its seed pods underground.
Although the peanut was mainly a garden crop for much of the colonial period of North America, it was mostly used as animal feed stock until the 1930s. In the United States, the US Department of Agriculture initiated a program to encourage agricultural production and human consumption of peanuts in the late 19th and early 20th centuries. George Washington Carver developed hundreds of recipes for peanuts during his tenure in the program.
Cultivation.
The orange-veined, yellow-petaled, pea-like flower of the "Arachis hypogaea" is borne in axillary clusters above ground. Following self-pollination, the flowers fade and wither. The stalk at the base of the ovary, called the pedicel, elongates rapidly, and turns downward to bury the fruits several inches in the ground, where they complete their development. The entire plant, including most of the roots, is removed from the soil during harvesting. The fruits have wrinkled shells that are constricted between pairs of the one to four (usually two) seeds per pod.
Peanuts grow best in light, sandy loam soil. They require five months of warm weather, and 500 to of water.
The pods ripen 120 to 150 days after the seeds are planted. If the crop is harvested too early, the pods will be unripe. If they are harvested late, the pods will snap off at the stalk, and will remain in the soil. They prefer an acidic soil of preferably 5.9-7 pH.
Peanuts are particularly susceptible to contamination during growth and storage. Poor storage of peanuts can lead to an infection by the mold fungus "Aspergillus flavus", releasing the toxic and highly carcinogenic substance aflatoxin. The aflatoxin-producing molds exist throughout the peanut growing areas and may produce aflatoxin in peanuts when conditions are favorable to fungal growth.
Harvesting occurs in two stages: In mechanized systems, a machine is used to cut off the main root of the peanut plant by cutting through the soil just below the level of the peanut pods. The machine lifts the "bush" from the ground and shakes it, then inverts the bush, leaving the plant upside down on the ground to keep the peanuts out of the soil. This allows the peanuts to dry slowly to a little less than a third of their original moisture level over a period of three to four days. Traditionally, peanuts were pulled and inverted by hand.
After the peanuts have dried sufficiently, they are threshed, removing the peanut pods from the rest of the bush.
Cultivation in China.
The peanut was introduced to China by Portuguese traders in the 17th century and another variety by American missionaries in the 19th century.
They became popular and are featured in many Chinese dishes, often being boiled. During the 1980s, peanut production began to increase so greatly that as of 2006, China was the world's largest peanut producer. A major factor in this increase was the household-responsibility system, which moved financial control from the government to the farmers.
Production.
China leads in production of peanuts, having a share of about 42% of overall world production, followed by India (12%) and the United States of America (8%).
Cultivars in the United States.
Thousands of peanut cultivars are grown, with four major cultivar groups being the most popular: Spanish, Runner, Virginia, and Valencia. There are also Tennessee red and white groups. Certain cultivar groups are preferred for particular uses because of differences in flavor, oil content, size, shape, and disease resistance. For many uses, the different cultivars are interchangeable. Most peanuts marketed in the shell are of the Virginia type, along with some Valencias selected for large size and the attractive appearance of the shell. Spanish peanuts are used mostly for peanut candy, salted nuts, and peanut butter. Most Runners are used to make peanut butter.
The various types are distinguished by branching habit and branch length. There are numerous varieties of each type of peanut. There are two main growth forms, bunch and runner. Bunch types grow upright, while runner types grow near the ground.
Each year, new cultivars of peanuts are bred and introduced. Introducing a new cultivar may mean changes in the planting rate, adjusting the planter, harvester, dryer, cleaner, sheller, and the method of marketing.
Spanish group.
The small Spanish types are grown in South Africa, and in the southwestern and southeastern US. Prior to 1940, 90% of the peanuts grown in Georgia, USA, were Spanish types, but the trend since then has been larger-seeded, higher-yielding, more disease-resistant cultivars. Spanish peanuts have a higher oil content than other types of peanuts, and in the US are now primarily grown in New Mexico, Oklahoma, and Texas.
Cultivars of the Spanish group include ‘Dixie Spanish’, ‘Improved Spanish 2B’, ‘GFA Spanish’, ‘Argentine’, ‘Spantex’, ‘Spanette’, ‘Shaffers Spanish’, ‘Natal Common (Spanish)’, "White Kernel Varieties’, ‘Starr’, ‘Comet’, ‘Florispan’, ‘Spanhoma’, ‘Spancross’, ‘OLin’, ‘Tamspan 90’, ‘AT 9899–14’, ‘Spanco’, ‘Wilco I’, ‘GG 2’, ‘GG 4’, ‘TMV 2’, and ‘Tamnut 06’.
Runner group.
Since 1940, the southeastern US region has seen a shift to production of Runner group peanuts. This shift is due to good flavor, better roasting characteristics and higher yields when compared to Spanish types, leading to food manufacturers' preference for the use in peanut butter and salted nuts. Georgia's production is now almost 100% Runner type.
Cultivars of Runners include ‘Southeastern Runner 56-15’, ‘Dixie Runner’, ‘Early Runner’, ‘Virginia Bunch 67’, ‘Bradford Runner’, ‘Egyptian Giant’ (also known as ‘Virginia Bunch’ and ‘Giant’), ‘Rhodesian Spanish Bunch’ (Valencia and Virginia Bunch), ‘North Carolina Runner 56-15’, ‘Florunner’, ‘Virugard’, ‘Georgia Green’, ‘Tamrun 96’, ‘Flavor Runner 458’, ‘Tamrun OL01’, ‘Tamrun OL02’ ‘AT-120’, ‘Andru-93’, ‘Southern Runner’, ‘AT1-1’, ‘Georgia Brown’, ‘GK-7’,and ‘AT-108’.
Virginia group.
The large seeded Virginia group peanuts are grown in the US states of Virginia, North Carolina, Tennessee, Texas, New Mexico, Oklahoma, and parts of Georgia. They are increasing in popularity due to demand for large peanuts for processing, particularly for salting, confections, and roasting in the shells.
Virginia group peanuts are either bunch or running in growth habit. The bunch type is upright to spreading. It attains a height of 45 to, and a spread of 70 to, with 80 to rows that seldom cover the ground. The pods are borne within 5 to 10 cm of the base of the plant.
Cultivars of Virginia type peanuts include ‘NC 7’, ‘NC 9’, ‘NC 10C’, ‘NC-V 11’, ‘VA 93B’, ‘NC 12C’, ‘VA-C 92R’, ‘Gregory’, ‘VA 98R’, ‘Perry’, ‘Wilson, ‘Hull’, ’AT VC-2’ and’ Shulamit’
Valencia group.
Valencia group peanuts are coarse, and they have heavy reddish stems and large foliage. In the United States, large commercial production is primarily in the South Plains of West Texas and eastern New Mexico near and south of Portales, New Mexico, but they are grown on a small scale elsewhere in the South as the best-flavored and preferred type for boiled peanuts. They are comparatively tall, having a height of 125 cm and a spread of 75 cm. Peanut pods are borne on pegs arising from the main stem and the side branches. Most of the pods are clustered around the base of the plant, and only a few are found several inches away. Valencia types are three- to five-seeded and smooth, with no constriction of the shell between the seeds. Seeds are oval and tightly crowded into the pods. Typical seed weight is 0.4 to 0.5 g. This type is used heavily for sale roasted and salted in-shell peanuts and peanut butter. Varieties include 'Valencia A' and 'Valencia C'.
Tennessee Red and Tennessee White groups.
These are alike, except for the color of the seed. Sometimes known also as Texas Red or White, the plants are similar to Valencia types, except the stems are green to greenish brown, and the pods are rough, irregular, and have a smaller proportion of kernels.
Uses.
Varied applications.
Peanuts can be eaten raw, used in recipes, made into oils, textile materials, and peanut butter, as well as many other uses. In general, peanut products are considered safe for human use, although there are insufficient studies about peanut aflatoxins and uses for cosmetics.
Popular confections made from peanuts include salted peanuts, peanut butter (sandwiches, peanut candy bars, peanut butter cookies, and cups), peanut brittle, and shelled nuts (plain/roasted). Salted peanuts are usually roasted in oil and packed in retail-size plastic bags or hermetically sealed cans. Dry roasted salted peanuts are also marketed in significant quantities. Peanuts are often a major ingredient in mixed nuts because of their relative cost compared to Brazil nuts, cashews, walnuts, and others. Peanut butter has been a tradition on camping trips and the home due to its high protein content and resists spoiling. Large quantities are also used in the commercial manufacture of sandwiches, candy, and bakery products. Boiled peanuts are a preparation of raw, unshelled green peanuts boiled in brine and often eaten as a snack. More recently, fried peanut recipes have emerged, allowing both shell and nut to be eaten. Peanuts are also used in a wide variety of cosmetics, plastics, dyes and paints.
Peanut oil.
Peanut oil is often used in cooking, because it has a mild flavor and a relatively high smoke point. Due to its high monounsaturated content, it is considered healthier than saturated oils, and is resistant to rancidity. There are several types of peanut oil including: aromatic roasted peanut oil, refined peanut oil, extra virgin or cold pressed peanut oil and peanut extract. In the United States, refined peanut oil is exempt from allergen labeling laws.
Peanut flour.
Peanut flour is lower in fat than peanut butter, and is popular with chefs because its high protein content makes it suitable as a flavor enhancer. Peanut flour is used as a gluten-free solution.
Boiled peanuts.
Boiled peanuts are a popular snack in the southern United States, as well as in India, China and West Africa. In the South boiled peanuts are often prepared in briney water and sold in streetside stands and come in brown paper bags.
Dry roasted peanuts.
Dry peanuts can be roasted in the shell or shelled in a home oven if spread out one layer deep in a pan and baked at a temperature of 350 °F or 177 °C for 15 to 20 min (shelled) and 20 to 25 min (in shell).
Cuisine.
Latin America.
Peanuts are used in many sauces for Latin American meat dishes, especially rabbit. Peanuts are particularly common in Peruvian and Mexican cuisine, both of which marry native and European ingredients. For instance, roasted peanuts and hot peppers, both native to the region, appear with roasted onions, garlic, and oil—ingredients from European cuisine—in a smooth sauce poured over boiled potatoes, a dish well known in the city Arequipa and called "papas con ocopa". Another example is a fricassee combining a similar mixture with sautéed seafood or boiled and shredded chicken. These dishes are generally known as "ajíes", meaning "hot peppers", such as "ají de pollo" and "ají de mariscos" (seafood "ajíes" may omit peanuts).
Likewise, during Colonial times, the Spanish in Peru used peanuts to replace nuts unavailable in Peru but used extensively in Spanish cuisine, such as almonds, pine nuts, and other nuts, typically ground or as paste and mixed with rice, meats, and vegetables for dishes such as rice pilaf.
Throughout the region, many candies and snacks are made using peanuts as a base.
Southwest Asia.
Crunchy coated peanuts, called "kabukim" in Hebrew, are a popular snack in Israel. "Kabukim" are commonly sold by weight at corner stores where fresh nuts and seeds are sold, though they are also available packaged. The coating typically consists of flour, salt, starch, lecithin, and sometimes sesame seeds. The origin of the name is obscure (it may be derived from "kabuk" which means nutshell or husk in Turkish). An additional variety of crunchy coated peanuts popular in Israel is "American peanuts". The coating of this variety is thinner, but harder to crack.
Another popular Israeli peanut snack, Bamba puffs, is similar in shape to Cheez Doodles, but are made of corn and flavored with peanut butter.
Southeast Asia.
Peanuts are also widely used in Southeast Asian cuisine, such as in Vietnam and Indonesia, where they are typically made into a spicy sauce. Peanuts originally came to Indonesia from the Philippines, where the legume came from Mexico in times of Spanish colonization. Some of the most famous Philippine dish concerning peanuts is the "kare-kare", a mixture of meat and peanut butter.
Common Indonesian peanut-based dishes include "gado-gado", "pecel", "karedok" and "ketoprak", all vegetable salads mixed with peanut sauce, and the peanut-based sauce for satay.
South Asia.
In the Indian subcontinent, peanuts are known as a light snack by themselves, usually roasted and salted (sometimes with the addition of chilli powder), and often sold roasted in pod, or boiled with salt. They are also made into little dessert or sweet snack pieces by processing with refined sugar and jaggery. Indian cuisine uses roasted, crushed peanuts to give a crunchy body to salads; they are added whole (without pods) to leafy vegetable stews for the same reason. Another use of peanut oil as cooking oil. Most Indians use mustard, sunflower, and peanut oil for cooking.
In Andhra Pradesh groundnut 'chutney' is a popular combination, usually partaken with Dosa and Idli at breakfast.
West Africa.
Peanuts grow well in southern Mali and adjacent regions of the Ivory Coast, Burkina Faso, Ghana, Nigeria and Senegal; peanuts are similar in both agricultural and culinary qualities to the Bambara groundnut native to the region, and West Africans have adopted the crop as a staple. Peanut sauce, prepared with onions, garlic, peanut butter/paste, and vegetables such as carrots, cabbage, and cauliflower, can be vegetarian (the peanuts supplying ample protein) or prepared with meat, usually chicken.
Peanuts are used in the Malian meat stew "maafe". In Ghana, peanut butter is used for peanut butter soup "nkate nkwan". Crushed peanuts may also be used for peanut candies "nkate cake" and "kuli-kuli", as well as other local foods such as "oto". Peanut butter is also an ingredient in Nigeria's "African salad".
Peanut powder is an important ingredient in the spicy coating for kebabs in Nigeria and Ghana.
East Africa.
Peanuts are a common ingredient of several types of relishes (dishes which accompany "nshima") eaten by the tribes in Malawi and in the eastern part of Zambia, and these dishes are now common throughout both countries. Thick peanut butter sauces are also made in Uganda to go with rice and other starchy foods. Across East Africa, roasted peanuts (often in cones of newspaper) are a popular snack sold in the street.
North America.
In Canada and the US, peanuts are used in candies, cakes, cookies, and other sweets. They are also enjoyed roasted and salted. Peanut butter is one of the most popular peanut-based foods in the US, and for four hundred years, recipes for peanut soup have been present in the South, Virginia in particular. In some southern portions of the US, peanuts are boiled for several hours until soft and moist. Peanuts are also deep-fried, shell and all.
Malnutrition.
Peanuts are used to help fight malnutrition. Plumpy Nut, MANA Nutrition, and Medika Mamba are high-protein, high-energy and high-nutrient peanut-based pastes developed to be used as a therapeutic food to aid in famine relief. The World Health Organization, UNICEF, Project Peanut Butter and Doctors Without Borders have used these products to help save malnourished children in developing countries.
Peanuts can be used like other legumes and grains to make a lactose-free milk-like beverage, peanut milk. Peanut milk is promoted in Africa as a way to reduce malnutrition among children.
Other uses.
Peanut plant tops are used for hay.
Low-grade or culled peanuts not suitable for the edible market are used in the production of peanut oil for manufacturing. The protein cake (oilcake meal) residue from oil processing is used as an animal feed and as a soil fertilizer. Raw peanuts are also widely sold as a garden bird feed.
Peanuts have a variety of industrial end uses. Paint, varnish, lubricating oil, leather dressings, furniture polish, insecticides, and nitroglycerin are made from peanut oil. Soap is made from saponified oil, and many cosmetics contain peanut oil and its derivatives. The protein portion is used in the manufacture of some textile fibers. Peanut shells are used in the manufacture of plastic, wallboard, abrasives, fuel, cellulose (used in rayon and paper) and mucilage (glue). Rudolf Diesel ran some of the first engines that bear his name on peanut oil and it is still seen as a potentially useful fuel.
Nutritional value.
Peanuts are rich in essential nutrients (right table, USDA nutrient data). In a 100 g serving, peanuts provide 570 calories and are an excellent source (defined as more than 20% of the Daily Value, DV) of several B vitamins, vitamin E, several dietary minerals, such as manganese (95% DV), magnesium (52% DV) and phosphorus (48% DV), and dietary fiber (right table). They also contain about 25% protein per 100 g serving, a higher proportion than in many tree nuts.
Phytochemicals.
Recent research on peanuts has found polyphenols and other phytochemicals that are under basic research for their potential to provide health benefits. New research shows peanuts, especially the skins, to have comparable polyphenol content of many fruits.
Peanut skins are a significant source of resveratrol, a phenolic under research for a variety of potential effects in humans.
Oil composition.
A common cooking and salad oil, peanut oil is 46% monounsaturated fats (primarily oleic acid), 32% polyunsaturated fats (primarily linoleic acid) and 17% saturated fats (primarily palmitic acid). Extractable from whole peanuts using a simple water and centrifugation method, the oil is being considered by NASA's Advanced Life Support program for future long-duration human space missions.
Health concerns.
Allergies.
Some people (0.6% of the United States population) report that they experience mild to severe allergic reactions to peanut exposure; symptoms can range from watery eyes to anaphylactic shock, which can be fatal if untreated. For these individuals, eating a small amount of peanuts can cause a reaction. Because of their widespread use in prepared and packaged foods, the avoidance of peanuts is difficult. Some foods processed in facilities which also handle peanuts may carry warnings on their labels indicating such.
A hypothesis of the development of peanut allergy has to do with the way peanuts are processed in North America versus other countries, such as Pakistan and China, where peanuts are widely eaten. According to a 2003 study, roasting peanuts, as more commonly done in North America, causes the major peanut allergen Ara h2 to become a stronger inhibitor of the digestive enzyme trypsin, making it more resistant to digestion. Additionally, this allergen has also been shown to protect Ara h1, another major peanut allergen, from digestion — a characteristic further enhanced by roasting.
Another hypothesis, called the hygiene hypothesis, states that a lack of early childhood exposure to infectious agents like germs and parasites could be causing the increase of food allergies.
Recent (2008) studies comparing age of peanut introduction in Great Britain with introduction in Israel appear to show that delaying exposure to peanuts can dramatically increase the risk of developing peanut allergies.
Results from some animal studies (and limited evidence from human subjects) suggest that the dose of peanuts is an important mediator of peanut sensitization and tolerance; low doses tend to lead to sensitization and higher doses tend to lead to tolerance.
Peanut allergy has been associated with the use of skin preparations containing peanut oil among children, but the evidence is not regarded as conclusive. Peanut allergies have also been associated with family history and intake of soy products.
Though the allergy can last a lifetime, another 2003 study indicates that 23.3% of children will outgrow a peanut allergy.
Some school districts in the United States have banned peanuts. There are experimental techniques which appear to have desensitized some allergic individuals. The most popular technique, oral immunotherapy, works to create desensitization in those allergic by feeding them small amounts of peanuts until their body becomes desensitized. Some progress is possibly being made in the UK, where researchers at Cambridge are studying the effectiveness of the desensitization technique.
Research indicates that refined peanut oil will not cause allergic reactions in most people with peanut allergies. However, crude (unrefined) peanut oils are strongly flavoured, and have been shown to contain protein, which may cause allergic reactions. In a randomized, double-blind crossover study, 60 people with proven peanut allergy were challenged with both crude peanut oil and refined peanut oil. The authors conclude, "Crude peanut oil caused allergic reactions in 10% of allergic subjects studied and should continue to be avoided." They also state, "Refined peanut oil does not seem to pose a risk to most people with peanut allergy." However, they point out that refined peanut oil can still pose a risk to peanut-allergic individuals if oil that has previously been used to cook foods containing peanuts is reused.
Contamination with aflatoxin.
Peanuts may be contaminated with the mold "Aspergillus flavus" which produces a carcinogenic substance called aflatoxin. Lower quality specimens, particularly where mold is evident, are more likely to be contaminated. The United States Department of Agriculture (USDA) tests every truckload of raw peanuts for aflatoxin; any containing aflatoxin levels of more than 15 parts per billion are destroyed. The peanut industry has manufacturing steps in place to ensure all peanuts are inspected for aflatoxin.
United States Department of Agriculture program.
George Washington Carver is often credited with inventing 300 different uses for peanuts (which, contrary to popular belief, did not include peanut butter but did include salted peanuts). Carver was one of many United States Department of Agriculture (USDA) researchers who encouraged cotton farmers in the South to grow peanuts instead of, or in addition to, cotton, because cotton had depleted so much nitrogen from the soil, and one of the peanut's properties as a legume is to put nitrogen back into the soil (a process known as nitrogen fixation). Rising demand for peanuts in the early 20th century was due to a shortage of plant oils during World War I and the growing popularity of peanut butter, roasted peanuts and peanut candies. Peanut products originating around the early 20th century include many brands still sold today such as Cracker Jack (1893), Planters peanuts (1906), Oh Henry! candy bar (1920), Baby Ruth candy bar (1920), Butterfinger candy bar (1923), Mr. Goodbar candy bar (1925), Reese's Peanut Butter Cup (1925), and Peter Pan (peanut butter) (1928).
Trade.
Although India and China are the world's largest producers of peanuts, they account for a small part of international trade because most of their production is consumed domestically as peanut oil. Exports of peanuts from India and China are equivalent to less than 4% of world trade. The major producers/exporters of peanuts are the United States, Argentina, Sudan, Senegal, and Brazil. These five countries account for 71% of total world exports. In recent years, the United States has been the leading exporter of peanuts.
The major peanut importers are the European Union (EU), Canada, and Japan. These three areas account for 78% of the world's imports. 75% of Canada's peanuts are imported from the United States. Two thirds of United States (U.S.) imports are roasted, unshelled peanuts. The major suppliers are Taiwan, Malaysia, Hong Kong, Mainland China, and Canada. The principal suppliers of shelled peanut imports are Argentina and Canada. Imports of peanut butter from Argentina are in the form of a paste and must be further processed. Other minor suppliers of peanut butter include Malawi, China, India, and Singapore.
Consumption of peanuts in the EU is primarily as food, mostly as roasted-in-shell peanuts and as shelled peanuts used in confectionery and bakery products.
The average annual U.S. imports of peanuts are less than 0.5% of U.S. consumption.
Georgia is the leading peanut producing state in the U.S., followed by Texas and Alabama, respectively. About half of all peanuts produced in the United States are grown within a 100 mi radius of Dothan, Alabama. Dothan is home to the National Peanut Festival established in 1938 and held each fall to honor peanut growers and celebrate the harvest.
Ninety percent of India's production is processed into peanut oil. Only a nominal amount of hand-picked select-grade peanuts are exported. India prohibits the importation of all oil seeds, including peanuts.

</doc>
<doc id="55605" url="http://en.wikipedia.org/wiki?curid=55605" title="Stimulus">
Stimulus

A stimulus is something that causes a physiological or psychological response:
It may also refer to:

</doc>
<doc id="55606" url="http://en.wikipedia.org/wiki?curid=55606" title="Haifa">
Haifa

Haifa (Hebrew: חֵיפָה "Heifa", ], ]; Arabic: حيفا‎ "Ḥayfā") is the largest city in northern Israel, and the third largest city in the country, with a population of over 272,181. Another 300,000 people live in towns directly adjacent to the city including Daliyat al-Karmel, the Krayot, Nesher, Tirat Carmel, and some Kibbuzim. Together these areas form a contiguous urban area home to nearly 600,000 residents which makes up the inner core of the Haifa metropolitan area. It is also home to the Bahá'í World Centre, a UNESCO World Heritage Site and destination for Baha'i pilgrims.
Built on the slopes of Mount Carmel, the history of settlement at the site spans more than 3,000 years. The earliest known settlement in the vicinity was Tell Abu Hawam, a small port city established in the Late Bronze Age (14th century BCE). In the 3rd century CE, Haifa was known as a dye-making center. Over the centuries, the city has changed hands: It has been conquered and ruled by the Phoenicians, Persians, Hasmoneans, Romans, Byzantines, Arabs, Crusaders, Ottomans, British, and the Israelis. Since the establishment of the State of Israel in 1948, the city has been governed by the Haifa Municipality.
Today, the city is a major seaport located on Israel's Mediterranean coastline in the Bay of Haifa covering 63.7 km2. It is located about 90 km north of Tel Aviv and is the major regional center of northern Israel. Two respected academic institutions, the University of Haifa and the Technion, are located in Haifa in addition to the largest k-12 school in Israel, 
The Hebrew Reali School. The city plays an important role in Israel's economy. It is home to Matam, one of the oldest and largest high-tech parks in the country. Haifa Bay is a center of heavy industry, petroleum refining and chemical processing. Haifa was formerly the western terminus of an oil pipeline from Iraq via Jordan.
Etymology.
The earliest named settlement within the domain of modern-day Haifa was a city known as "Sycaminum". Tel Shikmona Hebrew meaning "mound of the Ficus sycomorus" (Arabic "Tell el-Semak" or "Tell es-Samak", meaning "mound of the fish") preserved and transformed this ancient name and is mentioned once in the Mishnah (composed c. 200 CE) for the wild fruits that grow around it., with locals using it to refer to a coastal tell at the foot of the Carmel Mountains that contains its remains.
The name "Efa" first appears during Roman rule, some time after the end of the 1st century, when a Roman fortress and small Jewish settlement were established not far from Tell es-Samak. Haifa is also mentioned more than 100 times in the Talmud, a book central to Judaism.
"Hefa" or "Hepha" in Eusebius of Caesarea's 4th-century work, "Onomasticon" ("Onom." 108, 31), is said to be another name for "Sycaminus". This synonymizing of the names is explained by Moshe Sharon who writes that the twin ancient settlements, which he calls "Haifa-Sycaminon", gradually expanded into one another, becoming a twin city known by the Greek names "Sycaminon" or "Sycaminos Polis". References to this city end with the Byzantine period.
Around the 6th century, "Porphyreon" or "Porphyrea" is mentioned in the writings of William of Tyre, and while it lies within the area covered by modern Haifa, it was a settlement situated south of Haifa-Sycaminon.
Following the Arab conquest in the 7th century, "Haifa" was used to refer to a site established on Tell es-Samak upon what were already the ruins of "Sycaminon" ("Shiqmona"). "Haifa" (or "Haifah") is mentioned by the mid-11th-century Persian chronicler Nasir Khusraw, and the 12th- and 13th-century Arab chroniclers, Muhammad al-Idrisi and Yaqut al-Hamawi.
The Crusaders, who captured Haifa briefly in the 12th century, call it "Caiphas", and believe its name related to "Cephas", the Greek name of Simon Peter. Eusebius is also said to have referred to "Hefa" as "Caiaphas civitas", and Benjamin of Tudela, the 12th-century Jewish traveller and chronicler, is said to have attributed the city's founding to Caiaphas, the Jewish high priest at the time of Jesus.
Other spellings in English have included "Caipha", "Kaipha", "Caiffa", "Kaiffa" and "Khaifa".
"Haifa al-'Atiqa" (Arabic: "Ancient Haifa") is another name used by locals to refer to "Tell es-Samak", as it was the site of Haifa when it was a hamlet of 250 residents, before it was moved in 1764-5 to a new fortified site founded by Daher el-Omar 1.5 mi to the east. The new village, the nucleus of modern Haifa, was originally named "al-imara al-jadida" (Arabic: "the new construction"), but locals called it "Haifa al-Jadida" (Arabic: "New Haifa") at first, and then simply "Haifa". In the early 20th century, "Haifa al 'Atiqa" was repopulated as a predominantly Arab Christian neighborhood of Haifa as it expanded outward from its new location.
The ultimate origin of the name "Haifa" remains unclear. One theory holds it derives from the name of the high priest Caiaphas. Some Christians believe it was named for Saint Peter, whose Aramaic name was "Keiphah". Another theory holds it could be derived from the Hebrew verb root חפה ("hafa"), meaning to cover or shield, i.e. Mount Carmel covers Haifa; others point to a possible origin in the Hebrew word חוֹף ("hof"), meaning "shore", or חוֹף יָפֶה ("hof yafe"), meaning "beautiful shore".
History.
Early history.
A small port city known today as Tell Abu Hawam was established Late Bronze Age (14th century BCE). During the 6th century BCE, Greek geographer Scylax told of a city "between the bay and the Promontory of Zeus" (i.e., the Carmel) which may be a reference to Shikmona, a locality in the Haifa area, during the Persian period. By Hellenistic times, the city had moved to a new site south of what is now Bat Galim because the port's harbour had become blocked with sand. About the 3rd century CE, the city was first mentioned in Talmudic literature, as a Jewish fishing village and the home of Rabbi Avdimi and other Jewish scholars. A Greek-speaking population living along the coast at this time was engaged in commerce.
Haifa was located near the town of Shikmona, a center for making the traditional Tekhelet dye used in the garments of the high priests in the Temple. The archaeological site of Shikmona is southwest of Bat Galim. Mount Carmel and the Kishon River are also mentioned in the Bible. A grotto on the top of Mount Carmel is known as the , traditionally linked to the Prophet Elijah and his apprentice, Elisha. In Arabic, the highest peak of the Carmel range is called the "Muhraka", or "place of burning," harking back to the burnt offerings and sacrifices there in Canaanite and early Israelite times
Early Haifa is believed to have occupied the area which extends from the present-day Rambam Hospital to the Jewish Cemetery on Yafo Street. The inhabitants engaged in fishing and agriculture.
Under Byzantine rule, Haifa continued to grow but did not assume major importance. Following the Arab conquest of Palestine in the 630s-40s, Haifa was largely overlooked in favor of the port city of 'Akka. Under the Rashidun Caliphate, Haifa began to develop. In the 9th century under the Umayyad and Abbasid Caliphates, Haifa established trading relations with Egyptian ports and the city featured several shipyards. The inhabitants, Arabs and Jews, engaged in trade and maritime commerce. Glass production and dye-making from marine snails were the city's most lucrative industries.
Crusader, Ayyubid and Mamluk rule.
Prosperity ended in 1100, when Haifa was besieged and blockaded by the Crusaders and then conquered after a fierce battle with its Jewish and Muslim inhabitants. Under the Crusaders, Haifa was reduced to a small fishing and agricultural village. It was a part of the Principality of Galilee within the Kingdom of Jerusalem. Following their victory at the Battle of Hattin, Saladin's Ayyubid army captured Haifa in mid-July 1187. The Crusaders under Richard the Lionheart retook Haifa in 1191. The Carmelites established a church on Mount Carmel in the 12th century. Under Muslim rule, the building was turned into a mosque, later becoming a hospital. In the 19th century, it was restored as a Carmelite monastery over a cave associated with Elijah, the prophet.
The city's Crusader fortress was destroyed in 1187 by Saladin. In 1265, the army of Baibars the Mamluk captured Haifa, destroying its fortifications, which had been rebuilt by King Louis IX of France, as well as the majority of the city's homes to prevent the European Crusaders from returning. For much of their rule, the city was desolate in the Mamluk period between the 13th and 16th centuries. Information from this period is scarce. During Mamluk rule in the 14th century, al-Idrisi wrote that Haifa served as the port for Tiberias and featured a "fine harbor for the anchorage of galleys and other vessels.
Ottoman era.
In 1596, Haifa appeared in Ottoman tax registers as being in the "Nahiya" of Sahil Atlit of the "Liwa" of Lajjun. It had a population of 32 Muslim households and paid taxes on wheat, barley, summercrops, olives, and goats or beehives.
Haifa was a hamlet of 250 inhabitants in 1764-5. It was located at "Tell el-Semak", the site of ancient Sycaminum. In 1761 Daher el-Omar, the Arab ruler of Acre and Galilee, moved the population to a new fortified site 1.5 mi to the east and laid waste to the old site. This event is marked as the beginning of the town's life at its modern location. After al-Omar's death in 1775, the town remained under Ottoman rule until 1918, with the exception of two brief periods.
In 1799, Napoleon Bonaparte conquered Haifa during his unsuccessful campaign to conquer Palestine and Syria, but soon had to withdraw; in the campaign's final proclamation, Napoleon took credit for having razed the fortifications of "Kaïffa" (as the name was spelled at the time) along with those of Gaza, Jaffa and Acre.
Between 1831 and 1840, the Egyptian viceroy Muhammad Ali governed Haifa, after his son Ibrahim Pasha had wrested its control from the Ottomans. When the Egyptian occupation ended and Acre declined, the importance of Haifa rose.
The arrival of the German Templers in 1868, who settled in what is now known as the German Colony of Haifa, was a turning point in Haifa's development. The Templers built and operated a steam-based power station, opened factories and inaugurated carriage services to Acre, Nazareth and Tiberias, playing a key role in modernizing the city.
The first European Jews arrived at the end of the 19th century from Romania. The Central Jewish Colonisation Society in Romania purchased over 1000 acre near Haifa. As the Jewish settlers had been city dwellers, they hired the former fellahin tenants to instruct them in agriculture.
In 1909, Haifa became important to the Bahá'í Faith when the remains of the Báb, founder of the Bábí Faith and forerunner of Bahá'u'lláh in the Bahá'í Faith, were moved from Acre to Haifa and interred in the shrine built on Mount Carmel. Bahá'ís consider the shrine to be their second holiest place on Earth after the Shrine of Bahá'u'lláh in Acre. Its precise location on Mount Carmel was shown by Bahá'u'lláh himself to his eldest son, `Abdu'l-Bahá, in 1891. `Abdu'l-Bahá planned the structure, which was designed and completed several years later by his grandson, Shoghi Effendi. In a separate room, the remains of `Abdu'l-Bahá were buried in November 1921.
A branch of the Hejaz railway, known as the Jezreel Valley railway, was built between 1903 and 1905. This event accelerated the growth of Haifa, which became a township (nahiya) centre in Akka in the sanjak of Beyrut Eyalet before the end of Ottoman rule. The Technion Institute of Technology was established around this time, that is, in 1912.
British Mandate.
Haifa was captured from the Ottomans in September 1918 by Indian horsemen of the British Army after overrunning Ottoman positions armed with spears and swords. On 22 September, British troops were heading to Nazareth when a reconnaissance report was received indicating that the Turks were leaving Haifa. The British made preparations to enter the city and came under fire in the Balad al-Sheikh district (today Nesher). After the British regrouped, an elite unit of Indian horsemen were sent to attack the Turkish positions on the flanks and overrun their artillery guns on Mount Carmel.
Under the British Mandate, Haifa became an industrial port city. The Bahá'í Faith in 1918 and today has its administrative and spiritual centre in the environs of Haifa. Over the next few decades the number of Jews increased steadily, due to immigration, especially from Europe. The Arab immigration on the other hand swelled by influx of Arabs, coming mainly from surrounding villages as well as Syrian Hauran. The Arab immigration mainly came as a result of prices and salary drop. Between the censuses of 1922 and 1931, the Muslim, Jewish, and Christian populations rose by 217%, 256%, and 156%, respectively.
Haifa's development owed much to British plans to make it a central port and hub for Middle-East crude oil. The British Government of Palestine developed the port and built refineries, thereby facilitating the rapid development of the city as a center for the country's heavy industries. Haifa was also among the first towns to be fully electrified. The Palestine Electric Company inaugurated the Haifa Electrical Power Station already in 1925, opening the door to considerable industrialization. The State-run Palestine Railways also built its main workshops in Haifa.
By 1945 the population had shifted to 33 percent Muslim, 20 percent Christian and 47 percent Jewish. In 1947, about 70,910 Arabs (41,000 Muslims, 29,910 Christians) and 74,230 Jews were living there. The Christian community were mostly Greek-Melkite Catholics. 
The 1947 UN Partition Plan designated Haifa as part of the proposed Jewish state. On 30 December 1947, members of the Irgun, a Jewish underground militia, threw bombs into a crowd of Arabs outside the gates of the Consolidated Refineries in Haifa, killing six and injuring 42. In response Arab employees of the company killed 39 Jewish employees in what became known as the Haifa Oil Refinery massacre. The Jewish Haganah militia retaliated with a raid on the Arab village of Balad al-Shaykh, where many of the Arab refinery workers lived, in what became known as the Balad al-Shaykh massacre. Control of Haifa was critical in the ensuing Arab–Israeli war, since it was the major industrial and oil refinery port in British Palestine.
British forces in Haifa redeployed on 21 April 1948, withdrawing from most of the city while still maintaining control over the port facilities. Two days later the downtown, controlled by a combination of local and foreign (ALA) Arab irregulars was assaulted by Jewish forces in Operation Bi'ur Hametz, by the Carmeli Brigade of the Haganah, commanded by Moshe Carmel. The operation led to a massive displacement of Haifa's Arab population. According to "The Economist" at the time, only 5,000–6,000 of the city's 62,000 Arabs remained there by 2 October 1948.
Contemporaneous sources emphasized the Jewish leadership's attempt to stop the Arab exodus from the city and the Arab leadership as a motivating factor in the refugees' flight. According to the British district superintendent of police, "Every effort is being made by the Jews to persuade the Arab populace to stay and carry on with their normal lives, to get their shops and business open and to be assured that their lives and interests will be safe." "Time Magazine" wrote on 3 May 1948:
Benny Morris said Haifa's Arabs left due to of a combination of Zionist threats and encouragement to do so by Arab leaders. Ilan Pappé writes that the shelling culminated in an attack on a Palestinian crowd in the old marketplace using three-inch (76 mm) mortars on 22 April 1948. Shabtai Levy, the Mayor of the city, and some other Jewish leaders urged Arabs not to leave. According to Ilan Pappé, Jewish loudspeakers could be heard in the city ordering Arab residents to leave "before it's too late."
Morris quotes British sources as stating that during the battles between 22 and 23 April 100 Arabs were killed and 100 wounded, but he adds that the total may have been higher.
State of Israel.
After the Declaration of the Establishment of the State of Israel on 14 May 1948, Haifa became the gateway for Jewish immigration into Israel. During the 1948 Arab–Israeli War, the neighborhoods of Haifa were sometimes contested. After the war, Jewish immigrants were settled in new neighborhoods, among them Kiryat Hayim, Ramot Remez, Ramat Shaul, Kiryat Sprinzak, and Kiryat Eliezer. Bnei Zion Hospital (formerly Rothschild Hospital) and the Central Synagogue in Hadar Hacarmel date from this period. In 1953, a master plan was created for transportation and the future architectural layout.
In 1959, a group of Sephardi and Mizrahi Jews, mostly Moroccan Jews, rioted in Wadi Salib, claiming the state was discriminating against them. Their demand for “bread and work” was directed at the state institutions and what they viewed as an Ashkenazi elite in the Labor Party and the Histadrut.
Tel Aviv gained in status, while Haifa suffered a decline in the role as regional capital. The opening of Ashdod as a port exacerbated this. Tourism shrank when the Israeli Ministry of Tourism placed emphasis on developing Tiberias as a tourist centre.
Nevertheless, Haifa's population had reached 200,000 by the early 1970s, and mass immigration from the former Soviet Union boosted the population by a further 35,000.
Many of Wadi Salib's historic Ottoman buildings have now been demolished, and in the 1990s a major section of the Old City was razed to make way for a new municipal center.
From 1999 to 2003, several Palestinian suicide attacks took place in Haifa (in Maxim and Matza restaurants, bus 37, and others), killing 68 civilians.
In 2006, Haifa was hit by 93 Hezbollah rockets during the Second Lebanon War, killing 11 civilians and leading to half of the city's population fleeing at the end of the first week of the war. Among the places hit by rockets were a train depot and the oil refinery complex.
Demographics.
Haifa is Israel's third-largest city, consisting of 103,000 households, or a population of 266,300. Immigrants from the former Soviet Union constitute 25% of Haifa's population. According to the Israeli Central Bureau of Statistics, Israeli Arabs constitute 10% of Haifa's population, the majority living in Wadi Nisnas, Abbas and Halissa neighborhoods.
Haifa is commonly portrayed as a model of co-existence between Arabs and Jews, although tensions and hostility do still exist.
Between 1994 and 2009, the city had a declining and aging population compared to Tel Aviv and Jerusalem, as young people moved to the center of the country for education and jobs, while young families migrated to bedroom communities in the suburbs. However, as a result of new projects and improving infrastructure, the city managed to reverse its population decline, reducing emigration while attracting more internal migration into the city. In 2009, positive net immigration into the city was shown for the first time in 15 years.
Religious and ethnic communities.
The population is heterogeneous. Jews comprise some 82% of the population, some 4% are Muslims (of which many are Ahmadis) and almost 14% are Christians (both Arab and non-Arab) Haifa also includes Druze and Bahá'í communities. In 2006, 27% of the Arab population was aged 14 and under, compared to 17% of the Jewish and other population groups. The trend continues in the age 15-29 group, in which 27% of the Arab population is found, and the age 30-44 group (23%). The population of Jews and others in these age groups are 22% and 18% respectively. Nineteen percent of the city's Jewish and other population is between 45 and 59, compared to 14% of the Arab population. This continues with 14% of Jews and others aged 60–74 and 10% over age 75, in comparison to 7% and just 2% respectively in the Arab population.
In 2006, 2.9% of the Jews in the city were Haredi, compared to 7.5% on a national scale. However, the Haredi community in Haifa is growing fast due to a high fertility rate. 66.6% were secular, compared to a national average of 43.7%. A significant portion of the immigrants from the former Soviet Union either lack official religious-ethnic classification or are Non-Jews as they are from mixed-marriage families of some Jewish origin.
There is also a Scandinavian Seamen Protestant church, established by Norwegian Righteous Among the Nations pastor Per Faye-Hansen.
Geography.
Haifa is situated on the Israeli Mediterranean Coastal Plain, the historic land bridge between Europe, Africa, and Asia, and the mouth of the Kishon River. Located on the northern slopes of Mount Carmel and around Haifa Bay, the city is split over three tiers. The lowest is the center of commerce and industry including the Port of Haifa. The middle level is on the slopes of Mount Carmel and consists of older residential neighborhoods, while the upper level consists of modern neighborhoods looking over the lower tiers. From here views can be had across the Western Galilee region of Israel towards Rosh HaNikra and the Lebanese border. Haifa is about 90 km north of the city of Tel Aviv, and has a large number of beaches on the Mediterranean.
Flora and fauna.
The Carmel Mountain has three main wadis: Lotem, Amik and Si’ach. For the most part these valleys are undeveloped natural corridors that run up through the city from the coast to the top of the mountain. Marked hiking paths traverse these areas and they provide habitat for wildlife such as wild boar, golden jackal, hyrax, Egyptian mongoose, owls and chameleons.
Climate.
Haifa has a hot-summer Mediterranean climate with hot, dry summers and cool, rainy winters (Köppen climate classification "Csa"). Spring arrives in March when temperatures begin to increase. By late May, the temperature has warmed up considerably to herald warm summer days. The average temperature in summer is 26 °C and in winter, 12 °C. Snow is rare in Haifa, but temperatures around 3 °C can sometimes occur, usually in the early morning. Humidity tends to be high all year round, and rain usually occurs between September and May. Annual precipitation is approximately 629 mm.
Neighborhoods.
Haifa has developed in tiers, from the lower to the upper city on the Carmel. The oldest neighborhood in the modern Haifa is Wadi Salib, the Old City center near the port, which has been bisected by a major road and razed in part to make way for government buildings. Wadi Salib stretches across to Wadi Nisnas, the center of Arab life in Haifa today. In the 19th century, under Ottoman rule, the German Colony was built, providing the first model of urban planning in Haifa. Some of the buildings have been restored and the colony has turned into a center of Haifa nightlife.
The first buildings in Hadar were constructed at the start of the 20th century. Hadar was Haifa's cultural center and marketplace throughout the 1920s and into the 1980s, nestled above and around the Haifa's Arab neighborhoods. Today Hadar stretches from the port area near the bay, approximately halfway up Mount Carmel, around the German Colony, Wadi Nisnas and Wadi Salib. Hadar houses two commercial centers (one in the port area, and one midway up the mountain) surrounded by some of the city's older neighborhoods.
Neve Sha'anan, a neighborhood located on the second tier of Mount Carmel, was founded in the 1920s. West of the port are the neighborhoods of Bat Galim, Shikmona Beach, and Kiryat Eliezer. To the west and east of Hadar are the Arab neighborhoods of Abbas and Khalisa, built in the 1960s and 70s. To the south of Mount Carmel's headland, along the road to Tel Aviv, are the neighborhoods of Ein HaYam, Shaar HaAliya, Kiryat Sprinzak and Neve David.
Above Hadar are affluent neighborhoods such as the Carmel Tzarfati (French Carmel), Merkaz Ha'Carmel, Romema, Ahuzat Ha'Carmel (Ahuza), Carmeliya, Vardiya, Ramat Golda, Ramat Alon and Hod Ha'Carmel (Denya). While there are general divisions between Arab and Jewish neighborhoods, there is an increasing trend for wealthy Arabs to move into affluent Jewish neighborhoods. Another of the Carmel neighborhoods is Kababir, home to the National Headquarters of Israel's Ahmadiyya Muslim Community; located near Merkaz HaCarmel and overlooking the coast.
Urban development.
Recently, residential construction has been concentrated around Kiryat Haim and Kiryat Shmuel, with 75000 m² of new residential construction between 2002–2004, the Carmel, with 70000 m², and Ramot Neve Sha'anan with approximately 70000 m² Non-residential construction was highest in the Lower Town, (90,000 sq m), Haifa Bay (72,000 sq m) and Ramot Neve Sha'anan (54,000 sq m). In 2004, 80% of construction in the city was private.
Currently, the city has a modest number of skyscrapers and high-rise buildings, and many additional high-rise buildings are planned, have been approved, or are under construction. Though buildings rising up to 20 stories were built on Mount Carmel in the past, the Haifa municipality banned the construction of any new buildings taller than nine stories on Mount Carmel in July 2012.
The neighborhood of Wadi Salib, located in the heart of downtown Haifa, is being redeveloped. Most of its Jewish and Arab residents are considered squatters and have been gradually evicted over the years. The Haifa Economic Corporation Ltd is developing two 1,000 square meter lots for office and commercial use. Some historic buildings have been renovated and redeveloped, especially into nightclubs and theaters, such as the Palace of the Pasha, a Turkish bathhouse, and a Middle Eastern music and dance club, which has been converted into theaters and offices.
In 2012, a new, massive development plan was announced for Haifa's waterfront. According to the plan, the western section of the city's port will be torn down, and all port activity will be moved to the east. The west side of the port will be transformed into a tourism and nightlife center and a point of embarkation and arrival for sea travel through the construction of public spaces, a beach promenade, and the renovation of commercial buildings. The train tracks that currently bisect the city and separate the city's beach from the rest of Haifa will also be buried. A park will be developed on the border of the Kishon River, the refineries' cooling towers will be turned into a visitors' center, and bridges will lead from the port to the rest of the city. Massive renovations are also currently underway in Haifa's lower town, in the Turkish market and Paris Square, which will become the city's business center. In addition, the ammonia depository tank in the Haifa bay industrial zone will be dismantled, and a new one built in an alternative location.
Another plan seeks to turn the western section of Haifa Port into a major tourism and nightlife center, as well as a functioning point of embarkation and arrival for sea travel. All port activity will be moved to the western side, and the area will be redeveloped. Public spaces and a beach promenade will be developed, and commercial buildings will be renovated.
As part of the development plans, the Israeli Navy, which has a large presence in Haifa, will withdraw from the shoreline between Bat Galim and Hof Hashaket. A 5 km long esplanade which will encircle the shoreline will be constructed. It will include a bicycle path, and possibly also a small bridge under which navy vessels will pass on their way to the sea.
In addition, a 50,000 square-meter entertainment complex that will contain a Disney theme park, cinemas, shops, and a 25-screen Multiplex theater will be built at the Check Post exit from the Carmel Tunnels.
In 2014, a new major plan for the city was proposed, under which extensive development of residential, business, and leisure areas will take place with the target of increasing the city's population by 60,000 by 2025. Under the plan, five new neighborhoods will be built, along with new high-tech parks. In addition, existing employment centers will be renovated, and new leisure areas and a large park will be built.
Economy.
The common Israeli saying, "Haifa works, Jerusalem prays, and Tel Aviv plays" attests to Haifa's reputation as a city of workers and industry. The industrial region of Haifa is in the eastern part of the city, around the Kishon River. It is home to the Haifa oil refinery, one of the two oil refineries in Israel (the other refinery being located in Ashdod). The Haifa refinery processes 9 million tons (66 million barrels) of crude oil a year. Its nowadays unused twin 80-meter high cooling towers, built in the 1930s, were the tallest buildings built in the British Mandate period.
"Matam" (short for "Merkaz Ta'asiyot Mada" - Scientific Industries Center), the largest and oldest business park in Israel, is at the southern entrance to the city, hosting manufacturing and R&D facilities for a large number of Israeli and international hi-tech companies, such as Intel, IBM, Microsoft, Motorola, Google, Yahoo!, Elbit, CSR, Philips, and Amdocs. The campus of the University of Haifa is also home to IBM Haifa Labs.
The Port of Haifa is the leader in passenger traffic among Israeli ports, and is also a major cargo harbor, although deregulation has seen its dominance challenged by the Port of Ashdod.
Haifa malls and shopping centers include Hutsot Hamifratz, Horev Center Mall, Panorama Center, Castra Center, Colony Center (Lev HaMoshava), Hanevi'im Tower Mall, Kanyon Haifa, Lev Hamifratz Mall and Grand Kanyon.
In 2010, "Monocle" magazine identified Haifa as the city with the most promising business potential, with the greatest investment opportunities in the world. The magazine noted that "a massive head-to-toe regeneration is starting to have an impact; from scaffolding and cranes around town, to renovated façades and new smart places to eat". The Haifa municipality had spent more than $350 million on roads and infrastructure, and the number of building permits had risen 83% in the previous two years.
Currently, some 40 hotels, mostly boutique hotels, are planned, have been approved, or are under construction. The Haifa Municipality is seeking to turn the city into Northern Israel's tourist center, from where travelers can embark on day trips into Acre, Nazareth, Tiberias, and the Galilee.
A new life sciences industrial park containing five buildings with 85,000 square meters of space on a 31-duman (7.75 acre) site is being built adjacent to the Matam industrial park.
Tourism.
In 2005, Haifa has 13 hotels with a total of 1,462 rooms. The city has a 17 km shoreline, of which 5 km are beaches. Haifa's main tourist attraction is the Bahá'í World Centre, with the golden-domed Shrine of the Báb and the surrounding gardens. Between 2005 and 2006, 86,037 visited the shrine. In 2008, the Bahá'í gardens were designated a UNESCO World Heritage Site. The restored German Colony, founded by the Templers, Stella Maris and Elijah's Cave also draw many tourists.
Located in the Haifa district are the Ein Hod artists' colony, where over 90 artists and craftsmen have studios and exhibitions, and the Mount Carmel national park, with caves where Neanderthal and early Homo Sapiens remains were found.
A 2007 report commissioned by the Haifa Municipality calls for the construction of more hotels, a ferry line between Haifa, Acre and Caesarea, development of the western anchorage of the port as a recreation and entertainment area, and an expansion of the local airport and port to accommodate international travel and cruise ships.
Arts and culture.
Despite its image as a port and industrial city, Haifa is the cultural hub of northern Israel. During the 1950s, mayor Abba Hushi made a special effort to encourage authors and poets to move to the city, and founded the Haifa Theatre, a repertory theater, the first municipal theater founded in the country. The principal Arabic theater servicing the northern Arab population is the al-Midan Theater. Other theaters in the city include the Krieger Centre for the Performing Arts and the Rappaport Art and Culture Center. The Congress Center hosts exhibitions, concerts and special events.
The New Haifa Symphony Orchestra, established in 1950, has more than 5,000 subscribers. In 2004, 49,000 people attended its concerts. The Haifa Cinematheque, founded in 1975, hosts the annual Haifa International Film Festival during the intermediate days of the Sukkot holiday. Haifa has 29 movie theaters. The city publishes a local newspaper, Yediot Haifa, and has its own radio station, Radio Haifa.
During the 1990s, Haifa hosted the Haifa Rock & Blues Festival featuring Bob Dylan, Nick Cave, Blur and PJ Harvey. The last festival was held in 1995 with Sheryl Crow, Suede and Faith No More as headliners.
Museums.
Haifa has over a dozen museums. The most popular museum is the Israel National Museum of Science, Technology, and Space, which recorded almost 150,000 visitors in 2004. The museum is located in the historic Technion building in the Hadar neighborhood. The Haifa Museum of Art houses a collection of modern and classical art, as well as displays on the history of Haifa. The Tikotin Museum of Japanese Art is the only museum in the Middle East dedicated solely to Japanese art. Other museums in Haifa include the Museum of Prehistory, the National Maritime Museum and Haifa City Museum, the Hecht Museum, the Dagon Archaeological Museum of Grain Handling, the Railway Museum, the Clandestine Immigration and Navy Museum, the Israeli Oil Industry Museum, and Chagall Artists' House. As part of his campaign to bring culture to Haifa, Mayor Abba Hushi provided the artist Mane-Katz with a building on Mount Carmel to house his collection of Judaica, which is now a museum.
The Haifa Educational Zoo at Gan HaEm park houses a small animal collection including Syrian brown bears, now extinct from Israel. Wןthin the zoo is the Pinhas House biology institute. In the close vicinity of Haifa, on the Carmel, the Northern "Hai-Bar" ("wild life") operated by Israel's Parks and Reserves Authority for the purpose of breeding and reintroduction of species now extinct from Israel, such as Persian Fallow Deer.
Government.
As an industrial port city, Haifa has traditionally been a Labor party stronghold. The strong presence of dock workers and trade unions earned it the nickname 'Red Haifa.' In addition, many prominent Arabs in the Israeli Communist Party, among them Tawfik Toubi, Emile Habibi, Zahi Karkabi, Bulus Farah and Emile Toma, were from Haifa. In recent years, there has been a drift toward the center. This was best signified by, in the 2006 legislative elections, the Kadima party receiving about 28.9% of the votes in Haifa, and Labor lagging behind with 16.9%.
Before 1948, Haifa's Municipality was fairly unique as it developed cooperation between the mixed Arab and Jewish community in the city, with representatives of both groups involved in the city's management. Under mayor al-Haj, between 1920 and 1927, the city council had six Arab and two Jewish representatives, with the city run as a mixed municipality with overall Arab control. Greater cooperation was introduced under Hasan Bey Shukri, who adopted a positive and conciliatory attitude toward the city's Jews and gave them senior posts in the municipality. In 1940, the first Jewish mayor, Shabtai Levy, was elected. Levy's two deputies were Arab (one Muslim, the other Christian), with the remainder of the council made up of four Jews and six Arabs.
Today, Haifa is governed by its 12th city council, headed by the mayor Yona Yahav. The results of municipal elections decide on the makeup of the council, similarly to the Knesset elections. The city council is the legislative council in the city, and has the authority to pass auxiliary laws. The 12th council, which was elected in 2003, has 31 members, with the liberal Shinui-Greens ticket holding the most seats (6), and Likud coming second with 5. Many of the decisions passed by the city council are results of recommendation made by the various municipal committees, which are committees where non-municipal organs meet with representatives from the city council. Some committees are spontaneous, but some are mandatory, such as the security committee, tender committee and financial committee.
Medical facilities.
Haifa medical facilities have a total of 4,000 hospital beds. The largest hospital is the government-operated Rambam Hospital with 900 beds and 78,000 admissions in 2004. Bnai Zion Hospital and Carmel Hospital each have 400 beds. Other hospitals in the city include the Italian Hospital, Elisha Hospital (100 beds), Horev Medical Center (36 beds) and Ramat Marpe (18 beds). Haifa has 20 family health centers. In 2004, there were a total of 177,478 hospital admissions.
Rambam Medical Center was in the direct line of fire during the Second Lebanon War in 2006 and was forced to take special precautions to protect its patients. Whole wings of the hospital were moved to large underground shelters.
Education.
Haifa is home to two internationally acclaimed universities and several colleges The University of Haifa, founded in 1963, is at the top of Mt. Carmel. The campus was designed by the architect of Brasília and United Nations Headquarters in New York, Oscar Niemeyer. The top floor of the 30-story Eshkol Tower provides a panoramic view of northern Israel. The Hecht Museum, with important archeology and art collections, is on the campus of Haifa University.
The Technion - Israel Institute of Technology, described as Israel's MIT, was founded in 1912. It has 18 faculties and 42 research institutes. The original building now houses Haifa's science museum. The Hebrew Reali School was founded in 1913. It is the largest k-12 school in Israel, with 4,000 students in 7 branches, all over the city. The first technological high school in Israel, Bosmat, was established in Haifa in 1933.
Other academic institutions in Haifa are the Gordon College of Education and Sha'anan Religious Teachers' College, the WIZO Haifa Academy of Design and Education, and Tiltan College of Design. The Michlala Leminhal College of Management and the Open University of Israel have branches in Haifa. The city also has a nursing college and the P.E.T Practical Engineering School.
As of 2006–07, Haifa had 70 elementary schools, 23 middle schools, 28 academic high schools and 8 vocational high schools. There were 5,133 pupils in municipal kindergartens, 20,081 in elementary schools, 7,911 in middle schools, 8,072 in academic high schools, 2,646 in vocational high schools, and 2,068 in comprehensive district high schools. 86% of the students attended Hebrew-speaking schools and 14% attended Arab schools. 5% were in special education. In 2004, Haifa had 16 municipal libraries stocking 367,323 books.
Two prestigious Arab schools in Haifa are the Orthodox School, run by the Greek Orthodox church, and the Nazareth Nuns' School, a Catholic institution.
Transportation.
Haifa is served by six railway stations and the Carmelit, currently Israel's only subway system (another is under construction in Tel Aviv). The Nahariya–Tel Aviv Coastal Railway main line of Israel Railways runs along the coast of the Gulf of Haifa and has six stations within the city. From south-west to north-east, these stations are: Haifa Hof HaCarmel, Haifa Bat Galim, Haifa Merkaz HaShmona, Lev HaMifratz, Hutzot HaMifratz and Kiryat Haim. Together with the Kiryat Motzkin Railway Station in the northern suburb Kiryat Motzkin, they form the Haifa - Krayot suburban line ("Parvarit"). There are direct trains from Haifa to Tel Aviv, Ben Gurion International Airport, Nahariya, Akko, Kiryat Motzkin, Binyamina, Lod, Kiryat Gat, Beer Sheva and other locations.
Haifa's intercity bus connections are operated almost exclusively by the Egged bus company, which operates two terminals:
Lines to the North of the country use HaMifratz Central Bus Station and their coverage includes most towns in the North of Israel. Lines heading south use Haifa Hof HaCarmel Central Bus Station. Destinations directly reachable from Hof HaCarmel CBS include Tel Aviv, Jerusalem, Eilat, Raanana, Netanya, Hadera, Zikhron Ya'akov, Atlit, Tirat Carmel, Ben Gurion International Airport and intermediate communities. There are also three Egged lines that have their terminus in the Ramat Vizhnitz neighborhood and run to Jerusalem, Bnei Brak and Ashdod. These used to be "mehadrin" (i.e. gender segregated) lines.
All urban lines are run by Egged. There are also share taxis that run along some bus routes but do not have an official schedule. In 2006, Haifa implemented a trial network of neighborhood mini-buses – named "Shkhunatit" and run by Egged. In December 2012, GetTaxi, an app and taxi service which allows users to hail a cab using their smartphone without contacting the taxi station by identifying and summoning the closest taxi. In the current initial phase, 50 taxis from the service are operating in Haifa.
Haifa and the Krayot suburbs also have a new Phileas concept bus rapid transit system called the Metronit. These buses, operating with hybrid engines, follow optical strips embedded in designated lanes of roads, providing tram-like public transportation services. The Metronit consists of 100 18-meter buses, each with the capacity for 150 passengers, operating along 40 km of designated roadways. The new system officially opened on 16 August 2013 serving three lines.
Haifa is one of the few cities in Israel where buses operate on Shabbat. Bus lines operate throughout the city on a reduced schedule from late Saturday morning onwards, and also connect Haifa with Nesher, Tirat Karmel, Yokneam, Nazareth, Nazareth Illit and intermediate communities. Since the summer of 2008, night buses are operated by Egged in Haifa (line 200) and the Krayot suburbs (line 210). During the summer of 2008 these lines operated 7 nights a week. During the winter their schedule is limited to Thursday, Friday and Saturday nights, making them the only buses in Israel to operate on Friday night. Haifa is also the only city in Israel to operate a Saturday bus service to the beaches during summer time. Egged lines run during Saturday mornings from many neighborhoods to the Dado and Bat Galim beaches, and back in the afternoon.
The Haifa underground railway system is called Carmelit. It is a subterranean funicular on rails, running from downtown Paris Square to Gan HaEm (Mother's Park) on Mount Carmel. With a single track, six stations and two trains, it is listed in "Guinness World Records" as the world's shortest metro line. The Carmelit accommodates bicycles.
Haifa also has a cable car. The Haifa Cable Car gondola lift consists of six cabins and connects Bat Galim on the coast to the Stella Maris observation deck and monastery atop Mount Carmel. It serves mainly tourists.
There are currently plans to add a 4.4 kilometre commuter cable car service to Haifa's public transport system, running from HaMifratz Central Bus Station at the foot of Mount Carmel to the Technion, and then to the University of Haifa.
Air and sea transport.
Haifa Airport serves domestic flights to Tel Aviv and Eilat as well as international charters to Cyprus, Greece and Jordan. The airliners that operates flights from Haifa are Arkia and Israir. There are currently plans to expand services from Haifa. Cruise ships operate from Haifa port primarily to destinations in the Eastern Mediterranean, Southern Europe and Black Sea.
Roads.
Travel between Haifa and the center of the country is possible by road with Highway 2, the main highway along the coastal plain, beginning at Tel Aviv and ending at Haifa. Furthermore, Highway 4 runs along the coast to the north of Haifa, as well as south, inland from Highway 2. In the past, traffic along Highway 2 to the north of Haifa had to pass through the downtown area of the city; the Carmel Tunnels, opened for traffic 1 December 2010, now route this traffic under Mount Carmel, reducing congestion in the downtown area.
Sports.
The main stadiums in Haifa are the 14,002-seat Kiryat Eliezer Stadium and Thomas D'Alesandro Stadium. Neve Sha'anan Athletic Stadium seats 1,000. Construction of the Sammy Ofer Stadium, a UEFA-approved 30,820 seat stadium was completed in 2014.
The city's two main football clubs are Maccabi Haifa and Hapoel Haifa who both currently play in the Israeli Premier League and share the Sammy Ofer Stadium as their home pitch. Maccabi has won twelve Israeli titles, while Hapoel has won one.
The city also has an American football club, the Haifa Underdogs, that are a part of the Israeli Football League and play in Yoqneam Stadium. The team lost in the championship game of the league's inaugural season, but won one title as part of American Football Israel, which merged with the Israeli Football League in 2005.
The city has several clubs in the regional leagues, including Beitar Haifa in Liga Bet (the fourth tier) and Hapoel Ahva Haifa, F.C. Haifa Ruby Shapira and Maccabi Neve Sha'anan Eldad in Liga Gimel (the fifth tier).
Haifa has a professional basketball club, Maccabi Haifa. Maccabi Haifa was recently promoted to Israeli Basketball Super League, the top division. The team plays at Romema Arena, which seats 5,000.
The Haifa Hawks are an ice hockey team based out of the city of Haifa. They participate in the Israeli League, the top level of Israeli ice hockey.
In 1996, the city hosted the World Windsurfing Championship. The Haifa Tennis Club, near the southwest entrance to the city, is one of the largest in Israel.
John Shecter, Olympic horse breeder and owner of triple cup champion Shergar was born here.
Twin towns - sister cities.
Haifa is twinned with the following cities:
References.
Bibliography.
Panorama of Haifa. View from Mt. Carmel

</doc>
<doc id="55607" url="http://en.wikipedia.org/wiki?curid=55607" title="Discriminant">
Discriminant

In algebra, the discriminant of a polynomial is a function of its coefficients, typically denoted by a capital 'D' or the capital Greek letter Delta (Δ). It gives information about the nature of its roots. Typically, the discriminant is zero if and only if the polynomial has a multiple root. For example, the discriminant of the quadratic polynomial
is
Here for real a, b and c, if Δ > 0, the polynomial has two real roots, if Δ = 0, the polynomial has one real double root, and if Δ < 0, the two roots of the polynomial are complex conjugates.
The discriminant of the cubic polynomial
is
For higher degrees, the discriminant is always a polynomial function of the coefficients. It becomes significantly longer for the higher degrees. The discriminant of a "general" quartic has 16 terms, that of a quintic has 59 terms, that of a 6th degree polynomial has 246 terms,
and the number of terms increases exponentially with the degree.
A polynomial has a multiple root (i.e. a root with multiplicity greater than one) in the complex numbers if and only if its discriminant is zero.
The concept also applies if the polynomial has coefficients in a field which is not contained in the complex numbers. In this case, the discriminant vanishes if and only if the polynomial has a multiple root in any algebraically closed field containing the coefficients.
As the discriminant is a polynomial function of the coefficients, it is defined as soon as the coefficients belong to an integral domain "R" and, in this case, the discriminant is in "R". In particular, the discriminant of a polynomial with integer coefficients is always an integer. This property is widely used in number theory.
The term "discriminant" was coined in 1851 by the British mathematician James Joseph Sylvester.
Definition.
In terms of the roots, the discriminant is given by
where formula_6 is the leading coefficient and formula_7 are the roots (counting multiplicity) of the polynomial in some splitting field. It is the square of the Vandermonde polynomial times formula_8.
As the discriminant is a symmetric function in the roots, it can also be expressed in terms of the coefficients of the polynomial, since the coefficients are the elementary symmetric polynomials in the roots; such a formula is given below.
Expressing the discriminant in terms of the roots makes its key property clear, namely that it vanishes if and only if there is a repeated root, but does not allow it to be calculated without factoring a polynomial, after which the information it provides is redundant (if one has the roots, one can tell if there are any duplicates). Hence the formula in terms of the coefficients allows the nature of the roots to be determined without factoring the polynomial.
Formulas for low degrees.
The quadratic polynomial
has discriminant
The cubic polynomial
has discriminant
The quartic polynomial
has discriminant
These are homogeneous polynomials in the coefficients, respectively of degree 2, 4 and 6. They are also homogeneous in term of the roots, of respective degrees 2, 6 and 12.
Simpler polynomials have simpler expressions for their discriminants. For example, the monic quadratic polynomial "x"2 + "bx" + "c" has discriminant Δ = "b"2 − 4"c".
The monic cubic polynomial without quadratic term "x"3 + "px" + "q" has discriminant Δ = −4"p"3 − 27"q"2.
In terms of the roots, these discriminants are homogeneous polynomials of respective degree 2 and 6.
Homogeneity.
The discriminant is a homogeneous polynomial in the coefficients; it is also a homogeneous polynomial in the roots.
In the coefficients, the discriminant is homogeneous of degree 2"n"−2; this can be seen two ways. In terms of the roots-and-leading-term formula, multiplying all the coefficients by λ does not change the roots, but multiplies the leading term by λ. In terms of the formula as a determinant of a (2"n"−1) ×(2"n"−1) matrix divided by "an", the determinant of the matrix is homogeneous of degree 2"n"−1 in the entries, and dividing by "an" makes the degree 2"n"−2; explicitly, multiplying the coefficients by λ multiplies all entries of the matrix by λ, hence multiplies the determinant by λ2"n"−1.
For a monic polynomial, the discriminant is a polynomial in the roots alone (as the "an" term is one), and is of degree "n"("n"−1) in the roots, as there are formula_16 terms in the product, each squared.
Let us consider the polynomial
It follows from what precedes that its discriminant is homogeneous of degree 2"n"−2 in the formula_18 and quasi-homogeneous of weight "n"("n"−1) if each formula_19 is given the weight "i". In other words, every monomial formula_20 appearing in the discriminant satisfies the two equations
and
These thus correspond to the partitions of "n"("n"−1) into at 2"n"−2 (non negative) parts of size at most n
This restricts the possible terms in the discriminant. For the quadratic polynomial formula_23 there are only two possibilities for formula_24 either [1,0,1] or [0,2,0], given the two monomials "ac" and "b"2.
For the cubic polynomial formula_25, these are the partitions of 6 into 4 parts of size at most 3:
All these five monomials occur effectively in the discriminant.
While this approach gives the possible terms, it does not determine the coefficients. Moreover, in general not all possible terms will occur in the discriminant. The first example is for the quartic polynomial formula_27, in which case formula_28 satisfies formula_29 and formula_30, even though the corresponding discriminant does not involve the monomial formula_31.
Quadratic formula.
The quadratic polynomial formula_32 has discriminant
which is the quantity under the square root sign in the quadratic formula. For real numbers "a", "b", "c", one has:
and its graph crosses the "x"-axis twice.
and its graph is tangent to the "x"-axis.
An alternative way to understand the discriminant of a quadratic is to use the characterization as "zero if and only if the polynomial has a repeated root".
In that case the polynomial is formula_37
The coefficients then satisfy formula_38 so formula_39
and a monic quadratic has a repeated root if and only if this is the case, in which case the root is formula_40 Putting both terms on one side and including a leading coefficient yields formula_41
Discriminant of a polynomial.
To find the formula for the discriminant of a polynomial in terms of its coefficients, it is easiest to introduce the resultant. Just as the discriminant of a single polynomial is the product of the square of the differences between distinct roots, the resultant of two polynomials is the product of the differences between their roots, and just as the discriminant vanishes if and only if the polynomial has a repeated root, the resultant vanishes if and only if the two polynomials share a root.
Since a polynomial formula_42 has a repeated root if and only if it shares a root with its derivative formula_43 the discriminant formula_44 and the resultant formula_45 both have the property that they vanish if and only if "p" has a repeated root, and they have almost the same degree (the degree of the resultant is one greater than the degree of the discriminant) and thus are equal up to a factor of degree one.
The benefit of the resultant is that it can be computed as a determinant, namely as the determinant of the Sylvester matrix, a (2"n" − 1)×(2"n" − 1) matrix, whose "n" first rows contain the coefficients of "p" and the "n" − 1 last ones the coefficients of its derivative.
The resultant formula_45 of the general polynomial
is equal to the determinant of the (2"n" − 1)×(2"n" − 1) Sylvester matrix:
The discriminant formula_44 of formula_42 is now given by the formula
For example, in the case "n" = 4, the above determinant is
The discriminant of the degree 4 polynomial is then obtained from this determinant upon dividing by formula_53.
In terms of the roots, the discriminant is equal to
where "r"1, ..., "r""n" are the complex roots (counting multiplicity) of the polynomial:
This second expression makes it clear that "p" has a multiple root if and only if the discriminant is zero. (This multiple root can be complex.)
The discriminant can be defined for polynomials over arbitrary fields, in exactly the same fashion as above. The product formula involving the roots "r""i" remains valid; the roots have to be taken in some splitting field of the polynomial. The discriminant can even be defined for polynomials over any commutative ring. However, if the ring is not an integral domain, above division of the resultant by formula_6 should be replaced by substituting formula_6 by 1 in the first column of the matrix.
Nature of the roots.
The discriminant gives additional information on the nature of the roots beyond simply whether there are any repeated roots: for polynomials with real coefficients, it also gives information on whether the roots are real or complex. This is most transparent and easily stated for quadratic and cubic polynomials; for polynomials of degree 4 or higher this is more difficult to state.
Quadratic.
Because the quadratic formula expressed the roots of a quadratic polynomial as a rational function in terms of the "square root" of the discriminant, the roots of a quadratic polynomial are in the same field as the coefficients if and only if the discriminant is a square in the field of coefficients: in other words, the polynomial factors over the field of coefficients if and only if the discriminant is a square.
As a real number has real square roots if and only if it is nonnegative, and these roots are distinct if and only if it is positive (not zero), the sign of the discriminant allows a complete description of the nature of the roots of a quadratic polynomial with real coefficients:
Further, for a quadratic polynomial with rational coefficients, it factors over the rationals if and only if the discriminant – which is necessarily a rational number, being a polynomial in the coefficients – is in fact a square.
Cubic.
For a cubic polynomial with real coefficients, the discriminant reflects the nature of the roots as follows:
If a cubic polynomial has a triple root, it is a root of its derivative and of its second derivative, which is linear. Thus to decide if a cubic polynomial has a triple root or not, one may compute the root of the second derivative and look if it is a root of the cubic and of its derivative.
Higher degrees.
More generally, for a polynomial of degree "n" with real coefficients, we have
Discriminant of a polynomial over a commutative ring.
The definition of the discriminant of a polynomial in terms of the resultant may easily be extended to polynomials whose coefficients belong to any commutative ring. However, as the division is not always defined in such a ring, instead of dividing the determinant by the leading coefficient, one substitutes the leading coefficient by 1 in the first column of the determinant. This generalized discriminant has the following property which is fundamental in algebraic geometry.
Let "f" be a polynomial with coefficients in a commutative ring "A" and "D" its discriminant. Let φ be a ring homomorphism of "A" into a field "K" and φ("f") be the polynomial over "K" obtained by replacing the coefficients of "f" by their images by φ. Then φ("D") = 0 if and only if either the difference of the degrees of "f" and φ("f") is at least 2 or φ("f") has a multiple root in an algebraic closure of "K". The first case may be interpreted by saying that φ("f") has a multiple root at infinity.
The typical situation where this property is applied is when "A" is a (univariate or multivariate) polynomial ring over a field "k" and φ is the substitution of the indeterminates in "A" by elements of a field extension "K" of "k".
For example, let "f" be a bivariate polynomial in "X" and "Y" with real coefficients, such that "f" = 0 is the implicit equation of a plane algebraic curve. Viewing "f" as a univariate polynomial in "Y" with coefficients depending on "X", then the discriminant is a polynomial in "X" whose roots are the "X"-coordinates of the singular points, of the points with a tangent parallel to the "Y"-axis and of some of the asymptotes parallel to the "Y"-axis. In other words the computation of the roots of the "Y"-discriminant and the "X"-discriminant allows to compute all remarkable points of the curve, except the inflection points.
Generalizations.
The concept of discriminant has been generalized to other algebraic structures besides polynomials of one variable, including conic sections, quadratic forms, and algebraic number fields. Discriminants in algebraic number theory are closely related, and contain information about ramification. In fact, the more geometric types of ramification are also related to more abstract types of discriminant, making this a central algebraic idea in many applications.
Discriminant of a conic section.
For a conic section defined in plane geometry by the real polynomial
the discriminant is equal to
and determines the shape of the conic section. If the discriminant is less than 0, the equation is of an ellipse or a circle. If the discriminant equals 0, the equation is that of a parabola. If the discriminant is greater than 0, the equation is that of a hyperbola. This formula will not work for degenerate cases (when the polynomial factors).
Discriminant of a quadratic form.
There is a substantive generalization to quadratic forms "Q" over any field "K" of characteristic ≠ 2. For characteristic 2, the corresponding invariant is the Arf invariant.
Given a quadratic form "Q," the discriminant or determinant is the determinant of a symmetric matrix "S" for "Q".
Change of variables by a matrix "A" changes the matrix of the symmetric form by formula_62 which has determinant formula_63 so under change of variables, the discriminant changes by a non-zero square, and thus the class of the discriminant is well-defined in "K"/("K"*)2, i.e., up to non-zero squares. See also quadratic residue.
Less intrinsically, by a theorem of Jacobi, quadratic forms on formula_64 can be expressed, after a linear change of variables, in diagonal form as
More precisely, a quadratic forms on "V" may be expressed as a sum
where the "L""i" are independent linear forms and "n" is the number of the variables (some of the "a""i" may be zero). Then the discriminant is the product of the "a""i", which is well-defined as a class in "K"/("K"*)2.
For "K"=R, the real numbers, (R*)2 is the positive real numbers (any positive number is a square of a non-zero number), and thus the quotient R/(R*)2 has three elements: positive, zero, and negative. This is a cruder invariant than signature ("n"0, "n"+, "n"−), where "n"0 is the number 0s and "n"± is the number of ±1s in diagonal form. The discriminant is then zero if the form is degenerate (formula_67), and otherwise it is the parity of the number of negative coefficients, formula_68
For "K"=C, the complex numbers, (C*)2 is the non-zero complex numbers (any complex number is a square), and thus the quotient C/(C*)2 has two elements: non-zero and zero.
This definition generalizes the discriminant of a quadratic polynomial, as the polynomial formula_69 homogenizes to the quadratic form formula_70 which has symmetric matrix
whose determinant is formula_72 Up to a factor of −4, this is formula_73
The invariance of the class of the discriminant of a real form (positive, zero, or negative) corresponds to the corresponding conic section being an ellipse, parabola, or hyperbola.
Alternating polynomials.
The discriminant is a symmetric polynomial in the roots; if one adjoins a square root of it (halves each of the powers: the Vandermonde polynomial) to the ring of symmetric polynomials in "n" variables formula_74, one obtains the ring of alternating polynomials, which is thus a quadratic extension of formula_74.

</doc>
<doc id="55608" url="http://en.wikipedia.org/wiki?curid=55608" title="Gwen Verdon">
Gwen Verdon

Gwenyth Evelyn “Gwen” Verdon (January 13, 1925 – October 18, 2000) was an actress and dancer who won four Tony awards for her musical comedy performances and served as uncredited choreographers assistant and specialty dance coach for both theater and film. With flaming red hair and a quaver in her voice, Verdon was a critically acclaimed performer on Broadway in the 1950s and 1960s. Having originated many roles in musicals she is also strongly identified with her second husband, director–choreographer Bob Fosse, remembered as the dancer–collaborator–muse for whom he choreographed much of his work and as the guardian of his legacy after his death.
Early life.
Verdon was born in Culver City, California, the second child of Gertrude Lilian ("née" Standring; October 24, 1896 – October 16, 1956) and Joseph William Verdon (December 31, 1896 – June 23, 1978), who were British immigrants to the United States by way of Canada. Her brother was William Farrell Verdon (August 1, 1923 – June 10, 1991). The Verdon family could be described as "showpeople." Her father was an electrician at MGM Studios, and her mother was a former vaudevillian of the Denishawn dance troupe, as well as a dance teacher.
As a toddler, Gwen had rickets, which left her legs so badly misshapen she was called "Gimpy" by other children and spent her early years in orthopedic boots and rigid leg braces. Her mother put the three-year-old in dance classes. Further ballet training strengthened her legs and improved her carriage.
By the time she was six, she was already dancing on stage. She went on to study multiple dance forms, ranging from tap, jazz, ballroom and flamenco to Balinese. She even added juggling to her repertoire. At age 11, she appeared as a solo ballerina in the musical romance film "The King Steps Out" (1936), directed by Josef von Sternberg and starring Grace Moore and Franchot Tone. She attended Hamilton High School in Los Angeles and studied under famed balletomane Ernest Belcher. While in high school, she was cast in a revival of "Show Boat".
Verdon shocked her parents and instructors when she abandoned her budding career aged 17 to elope with reporter James Henaghan in 1942. In 1945, she appeared as a dancer in the movie musical "The Blonde From Brooklyn". After her divorce, she entrusted her son Jimmy to the care of her parents.
Career.
Early on, Verdon found a job as assistant to choreographer Jack Cole, whose work was respected by both Broadway and Hollywood movie studios. During her five-year employment with Cole, she took small roles in movie musicals as a "specialty dancer". She also taught dance to stars such as Jane Russell, Fernando Lamas, Lana Turner, Rita Hayworth, Betty Grable and Marilyn Monroe.
Verdon started out on Broadway as a "gypsy", going from one chorus line to another. Her breakthrough role finally came when choreographer Michael Kidd cast her as the second female lead in Cole Porter's musical "Can-Can" (1953), starring French prima donna Lilo. Out-of-town reviewers hailed Verdon's interpretation of Eve in the "Garden of Eden" ballet as a performance that upstaged the show's star, who jealously demanded Verdon's role be cut to only two featured dance numbers. With her role reduced to little more than an ensemble part, Verdon formally announced her intention to quit by the time the show premiered on Broadway. But her opening-night "Garden of Eden" performance was so well received that the audience screamed her name until the startled actress was brought from her dressing room in her bathrobe to take a curtain call. Verdon received a pay increase and her first Tony Award for her triumphant performance.
With her short shock of flaming red hair, exquisite body of a pin-up girl and a guileless vulnerability on stage and off, Verdon was considered the best dancer on Broadway in the 1950s and 1960s. That reputation solidified during her next show, George Abbott's "Damn Yankees" (1955), based on the novel "The Year the Yankees Lost the Pennant." She would forever be identified with her role as the vampish Lola, and it was on this show that she first worked with Bob Fosse as her choreographer. In the story, Verdon's Lola is a woman who was once "the ugliest woman in Providence, Rhode Island" but sold herself to the Devil to be the beauty we see in the play. The Devil (played by a wryly comic Ray Walston) convinces a baseball fan to sell his soul so he can play for the Washington Senators and win the league pennant in the playoffs. The Devil then employs the seductive Lola to keep the guy ("Joe") from escaping his grasp. The hitch is that Lola falls for the guy and has to choose between her love for him and her beauty pact with the Devil. The musical ran for 1019 performances. Vernon won another Tony and went to Hollywood to repeat her role in the 1958 movie version "Damn Yankees", memorably singing "whatever Lola wants, Lola gets". (Fosse can be seen partnered deliciously with her in the original mambo duet "Who's Got the Pain".)
Another Tony came when Verdon memorably played a role associated with Greta Garbo, Eugene O'Neill's Anna Christie, the hard-luck girl fleeing from her past as a prostitute, in the musical "New Girl in Town". When Fosse directed as well as choreographed his first Broadway musical, it was "Redhead", for which Verdon won her fourth Tony. In 1960, Fosse and Verdon wed.
In 1966, Verdon returned to the stage in the role of Charity in "Sweet Charity", which like many of her earlier Broadway triumphs was choreographed and directed by husband Fosse. The show is based on Federico Fellini's screenplay for "Nights of Cabiria". But whereas Fellini's black-and-white Italian film concerns the romantic ups and downs of an ever-hopeful prostitute, the musical makes the central character a hoofer-for-hire at a Times Square dance hall. The trademark Fosse showmanship, a dynamite musical score and theatregoers' affection for the exuberant, 41-year-old Verdon put the show over, despite Fellini's source material straining against the sanitized, Broadway-ized storyline. It was followed by a movie version starring Shirley MacLaine as Charity, featuring Ricardo Montalban, Sammy Davis, Jr. and Chita Rivera, with Fosse at the helm of his very first film as director and choreographer. Characteristically generous, Verdon helped with the choreography. The numbers include the famed "Big Spender", the fast-paced "Rhythm of Life", the witty "If My Friends Could See Me Now" and "I'm a Brass Band", in which MacLaine's Charity marched down the middle of Manhattan's Wall Street district. Verdon would also travel to Berlin to help Fosse with "Cabaret", the musical film for which he won an Academy Award for Best Director.
Although estranged as a couple, Verdon and Fosse continued to collaborate on projects such as "Chicago" (1975) (in which she originated the role of murderess Roxie Hart) and the musical "Dancin"' (1978), as well as Fosse's autobiographical movie "All That Jazz" (1979). The helpmeet/peer played by Leland Palmer in that film is based on the role Verdon played in Fosse's real life. She also developed a close working relationship with Fosse's lover, Broadway dancer Ann Reinking, and she instructed for Reinking's musical theatre classes. Reinking can be seen in "All That Jazz" playing the protagonist's lover, as she was in Fosse's real life. She, as much as Verdon, would become responsible for keeping Fosse's trademark choreography alive after Fosse's death. Reinking played Roxie Hart in the highly successful Broadway revival of "Chicago" that opened in 1996. She choreographed the dances "in the style of Bob Fosse" for that revival.
After originating the role of Roxie opposite Chita Rivera in "Chicago", Verdon focused on film acting, playing character roles in movies such as "The Cotton Club" (1984), "Cocoon" (1985) and ' (1988). She continued to teach dance and musical theater and to act. She received three Emmy Award nominations for appearances on "Magnum, P.I." (1988), "Dream On" (1993) and ' (1993). Verdon appeared as Alice's mother in the Woody Allen movie "Alice" (1990) and as Ruth in "Marvin's Room" (1996), co-starring Meryl Streep, Diane Keaton, and Hume Cronyn. In 1999, Verdon served as artistic consultant on a plotless Broadway musical designed to showcase examples of classic Fosse choreography. Called simply Fosse, the revue was conceived and directed by Richard Maltby Jr and Ann Reinking and choreographed by Reinking and Chet Walker. Verdon's daughter Nicole received a "special thanks" credit. The show received a Tony for best musical.
In 1997 Verdon appeared in an episode of Walker Texas Ranger as Maisie Whitman. She later reprised the role in 1999.
Verdon played Alora in the movie "Walking Across Egypt" (1999) and appeared in the film "Bruno", released in 2000.
Verdon received a total of four Tonys, for best supporting actress for "Can-Can" (1953) and best leading actress for "Damn Yankees" (1955), "New Girl in Town" (1957) and "Redhead" (1959), a murder-mystery musical. She also won a Grammy Award for the cast recording of "Redhead".
Gwen Verdon was inducted into the American Theatre Hall of Fame in 1981. In 1998, she was awarded the National Medal of Arts.
Personal life.
Verdon had two husbands, tabloid reporter James Henaghan (married 1942, divorced 1947) and Bob Fosse (married 1960, his death 1987). She and Henaghan had one son, Jim Henaghan (born 1943); she and Fosse had a daughter, Nicole Fosse (born 1963).
Fosse's extramarital affairs put a strain on their marriage and by 1971 they were separated. They never divorced. She held him in her arms as he suffered a fatal heart attack on the sidewalk outside the Washington theatre where "Sweet Charity" was being revived.
She was a cat fancier, and had up to six cats at one time, with names such as "Feets Fosse", "Junie Moon", and "Tidbits Tumbler Fosse".
Verdon died in her sleep in 2000 of a heart attack at the home of her daughter, Nicole, in Woodstock, Vermont, at the age of 75. At 8 p.m. on the night she died, all marquee lights on Broadway were dimmed in a tribute to the actress. Her remains were cremated.
Awards & nominations.
STAGE:
FILM & TV:

</doc>
<doc id="55610" url="http://en.wikipedia.org/wiki?curid=55610" title="Interior (topology)">
Interior (topology)

In mathematics, specifically in topology, the interior of a subset "S" of points of a topological space "X" consists of all points of "S" that do not belong to the boundary of "S". A point that is in the interior of "S" is an interior point of "S". 
The interior of "S" is the complement of the closure of the complement of "S". In this sense interior and closure are dual notions. 
The exterior of a set is the interior of its complement, equivalently the complement of its closure; it consists of the points that are in neither the set nor its boundary. The interior, boundary, and exterior of a subset together partition the whole space into three blocks (or fewer when one or more of these is empty). The interior and exterior are always open while the boundary is always closed. Sets with empty interior have been called boundary sets.
Definitions.
Interior point.
If "S" is a subset of a Euclidean space, then "x" is an interior point of "S" if there exists an open ball centered at "x" which is completely contained in "S". (This is illustrated in the introductory section to this article.)
This definition generalizes to any subset "S" of a metric space "X" with metric "d": "x" is an interior point of "S" if there exists "r" > 0, such that "y" is in "S" whenever the distance "d"("x", "y") < "r".
This definition generalises to topological spaces by replacing "open ball" with "open set". Let "S" be a subset of a topological space "X". Then "x" is an interior point of "S" if "x" is contained in an open subset of "S". (Equivalently, "x" is an interior point of "S" if there exists a neighbourhood of "x" which is contained in "S".)
Interior of a set.
The interior of a set "S" is the set of all interior points of "S". The interior of "S" is denoted int("S"), Int("S"), or "S"o. The interior of a set has the following properties.
Sometimes the second or third property above is taken as the "definition" of the topological interior.
Note that these properties are also satisfied if "interior", "subset", "union", "contained in", "largest" and "open" are replaced by "closure", "superset", "intersection", "which contains", "smallest", and "closed", respectively. For more on this matter, see interior operator below.
Examples.
On the set of real numbers one can put other topologies rather than the standard one.
These examples show that the interior of a set depends upon the topology of the underlying space. The last two examples are special cases of the following.
Interior operator.
The interior operator o is dual to the closure operator —, in the sense that
and also
where "X" is the topological space containing "S", and the backslash refers to the set-theoretic difference.
Therefore, the abstract theory of closure operators and the Kuratowski closure axioms can be easily translated into the language of interior operators, by replacing sets with their complements. 
Exterior of a set.
The exterior of a subset "S" of a topological space "X", denoted ext("S") or Ext("S"), is the interior int("X" \ "S") of its relative complement. Alternatively, it can be defined as "X" \ "S"—, the complement of the closure of "S". Many properties follow in a straightforward way from those of the interior operator, such as the following.
Unlike the interior operator, ext is not idempotent, but the following holds:
Interior-disjoint shapes.
Two shapes "a" and "b" are called "interior-disjoint" if the intersection of their interiors is empty. Interior-disjoint shapes may or may not intersect in their boundary. 

</doc>
<doc id="55611" url="http://en.wikipedia.org/wiki?curid=55611" title="Alexandroff extension">
Alexandroff extension

In mathematical field of topology, the Alexandroff extension is a way to extend a noncompact topological space by adjoining a single point in such a way that the resulting space is compact. It is named for the Russian mathematician Pavel Alexandrov.
More precisely, let "X" be a topological space. Then the Alexandroff extension of "X" is a certain compact space "X"* together with an open embedding "c" : "X" → "X"* such that the complement of "X" in "X"* consists of a single point, typically denoted ∞. The map "c" is a Hausdorff compactification if and only if "X" is a locally compact, noncompact Hausdorff space. For such spaces the Alexandroff extension is called the one-point compactification or Alexandroff compactification. The advantages of the Alexandroff compactification lie in its simple, often geometrically meaningful structure and the fact that it is in a precise sense minimal among all compactifications; the disadvantage lies in the fact that it only gives a Hausdorff compactification on the class of locally compact, noncompact Hausdorff spaces, unlike the Stone–Čech compactification which exists for any Tychonoff space, a much larger class of spaces.
Example: inverse stereographic projection.
A geometrically appealing example of one-point compactification is given by the inverse stereographic projection. Recall that the stereographic projection "S" gives an explicit homeomorphism from the unit sphere minus the north pole (0,0,1) to the Euclidean plane. The inverse stereographic projection formula_1 is an open, dense embedding into a compact Hausdorff space obtained by adjoining the additional point formula_2. Under the stereographic projection latitudinal circles formula_3 get mapped to planar circles formula_4. It follows that the deleted neighborhood basis of formula_5 given by the punctured spherical caps formula_6 corresponds to the complements of closed planar disks formula_7. More qualitatively, a neighborhood basis at formula_8 is furnished by the sets formula_9 as "K" ranges through the compact subsets of formula_10. This example already contains the key concepts of the general case.
Motivation.
Let formula_11 be an embedding from a topological space "X" to a compact Hausdorff topological space "Y", with dense image and one-point remainder formula_12. Then "c"("X") is open in a compact Hausdorff space so is locally compact Hausdorff, hence its homeomorphic preimage "X" is also locally compact Hausdorff. Moreover, if "X" were compact then "c"("X") would be closed in "Y" and hence not dense. Thus a space can only admit a one-point compactification if it is locally compact, noncompact and Hausdorff. Moreover, in such a one point compactification the image of a neighborhood basis for "x" in "X" gives a neighborhood basis for "c"("x") in "c"("X"), and—because a subset of a compact Hausdorff space is compact if and only if it is closed—the open neighborhoods of formula_8 must be all sets obtained by adjoining formula_8 to the image under "c" of a subset of "X" with compact complement.
The Alexandroff extension.
Let "X" be any topological space, and let formula_8 be any object which is not already an element of "X". Put formula_16, and topologize formula_17 by taking as open sets all the open subsets "U" of "X" together with all subsets "V" which contain formula_8 and such that formula_19 is closed and compact, .
The inclusion map formula_20 is called the Alexandroff extension of "X" (Willard, 19A).
The above properties all follow from the above discussion:
The one-point compactification.
In particular, the Alexandroff extension formula_20 is a compactification of "X" if and only if "X" is Hausdorff, noncompact and locally compact. In this case it is called the one-point compactification or Alexandroff compactification of "X". Recall from the above discussion that any compactification 
with one point remainder is necessarily (isomorphic to) the Alexandroff compactification.
Let "X" be any noncompact Tychonoff space. Under the natural partial ordering on the set formula_26 of equivalence classes of compactifications, any minimal element is equivalent to the Alexandroff extension (Engelking, Theorem 3.5.12). It follows that a noncompact Tychonoff space admits a minimal compactification if and only if it is locally compact.

</doc>
<doc id="55614" url="http://en.wikipedia.org/wiki?curid=55614" title="Nurse uniform">
Nurse uniform

A nurse uniform is attire worn by nurses for hygiene and identification. The traditional nurse uniform consists of a dress, apron and cap. It has existed in many variants, but the basic style has remained recognizable.
History.
The first nurse uniforms were derived from the nun's habit. Before the 19th century, nuns took care of sick and injured people so it was obvious that trained lay nurses might copy the nun's habit as they have adopted ranks like "Sister". One of Florence Nightingale's first students (Miss van Rensselaer) designed the original uniform for the students at Miss Nightingale's school of nursing. Before the 1940s minor changes occurred in the uniform. The clothing consisted of a mainly blue outfit. Hospitals were free to determine the style of the nurse uniform, including the nurse's cap which exists in many variants.
In Britain, the national uniform (or simply "national") was designed with the advent of the National Health Service (NHS) in 1948, and the Newcastle dress. From the 1960s open necks began to appear. In the 1970s, white disposable paper caps replaced cotton ones; in the 1980s, plastic aprons displaced the traditional ones and outerwear began to disappear. From the 1990s, scrubs became popular in Britain, having first appeared in the USA; however some nurses in Britain continue to wear dresses, although some NHS trusts have removed them in favour of scrubs as in many other countries.
Standard nurse's uniform.
Historically, a typical nurse uniform consisted of a dress, pinafore apron and nurse's cap. In some hospitals, however, student nurses also wore a nursing pin, or the pinafore apron may have been replaced by a cobbler style apron. This type of nurse's dress continues to be worn in many countries.
Alternative nurse uniforms.
Since the late 1980s, there has been a move towards alternative designs of nursing uniforms in some countries. Newer style nurse's uniform in the United Kingdom consists of either:
Male Nursing uniform.
Male nurses generally wear a different unform to their female counterparts. Male Nurses wear a white tunic with epaulettes in a colour or quantity that represents their year of training or grade.
There is some suggestion that Male Nurses should wear the same uniform as their female counterparts to assist patients in recognizing who is a nurse and assisting in establishing male nurses on equal terms as their counterparts. 
Traditional uniforms remain common in the Third World, but in Western Europe and North America, so-called "scrubs" or tunics have become more popular. "Scrub dress" is a simpler type of uniform, and is almost always worn in operating rooms and emergency rooms.
Nurse uniforms vs scrubs.
Beginning in the 1990s, and until the present time, the traditional nurse uniforms have been replaced with the "new" scrub dress in some countries. Most hospitals in the USA and Europe argue that the scrub uniform is easier to clean than the old nurse uniforms. The nurses who wear the uniforms are divided into two camps:
In many parts of the world, nurses continue to wear a white uniform consisting of a dress and cap.
The traditional white uniform for male nursing staff is now going out of fashion, excepting for student nurses.
A tunic of either the dental surgeon style or a v neck with a collar is very often used.
The colours vary with grade, area of work, and hospital; however, the male equivalent of sister (that is, charge nurse) tend to be shades of blue or dark green: often, this is the only colour to be recognised by the public as signifying a person in authority.
Nursing Jewellery.
Nurses were actively discouraged from wearing jewellery which might distract from their purpose and get caught on patient skin during care activity. A fob watch or "pendant" Watch was once considered synonymous with nursing. The fob watch freed the nurses hands for client care and prevented the wrist watch becoming a vector for disease. Watches were sometimes given as a token rite-of-passage gift from parents to young nurses, who were making the transition into nurses quarters and lived away from home for the first time.

</doc>
<doc id="55615" url="http://en.wikipedia.org/wiki?curid=55615" title="School uniform">
School uniform

A school uniform is an outfit—a set of standardized clothes—worn primarily for an educational institution. They are common in primary and secondary schools in various countries. When used, they form the basis of a school's dress code. Although often used interchangeably, there is an important distinction between school dress codes and school uniforms. According to scholars like Joseph (1986), clothing can be considered a uniform when it “(a) serves as a group emblem, (b) certifies an institution's legitimacy by revealing individual’s relative positions and (c) suppresses individuality.” An example of a uniform would be requiring white button-downs and ties for boys and pleated skirts for girls, with both wearing blazers. A uniform can even be as simple as requiring collared shirts, or restricting color choices and limiting items students are allowed to wear. A dress code on the other hand is much less restrictive, and according to Wilde “focus(es) on promoting modesty and discouraging anti-social fashion statements." Examples of a dress code would be not allowing ripped clothing, no logos or limiting the amount of skin that can be shown. It is important to note that we will be addressing uniforms on this page, and to keep these definitions in mind.
History of the Uniform.
It is difficult to trace the origins of the uniform as there is no comprehensive written history but rather a variety of known influences. Uniforms are conservative in a way, but as society and culture has evolved so has the school uniform (Davidson and Rae 1990). Little is known prior to the nineteenth century about uniforms, but there certainly are influences dating back to the 13th century. As an example, in 1222 we know the Archbishop of Canterbury ordered monks to wear a stereotypical monastic form of dress-perhaps the earliest standardized example of an academic uniform. Despite this example, the roots of the modern day uniform come mostly from the collegiate uniforms in England. Universities, primary schools and secondary schools used uniforms as a marker of class and status, which in turn served as a boundary. As early as the sixteenth century, and perhaps earlier, uniforms were utilized and became more and more specific as various fashion trends became undesirable to the university. Much more can be said about the development and roots of the uniform, but in regard to the United States the “official” movement toward using uniforms in public schools began when Bill Clinton addressed it in the State of the Union in 1996. He said, “if it means that teenagers will stop killing each other over designer jackets, then our public schools should be able to require their students to wear uniforms.” According to studies, as of 1998 approximately 25% of all public elementary, middle and junior high schools had adopted a uniform policy or were considering a policy, and two thirds were implemented between 1995 and 1997 (Wade & Stafford, 2003). Historically uniforms have been used to serve different purposes, from class division to religion to equality.
The Uniform Today.
There are an abundance of theories and empirical studies looking at school uniforms, making statements about their effectiveness. These theories and studies elaborate on the benefits and also the shortcomings of uniform policies. The issue of nature vs. nurture comes into play, as uniforms affect the perceptions of masculinity and femininity, complicate the issue of gender classification and also subdue the sexuality of girls. With uniforms also comes a variety of controversies, pros, cons and major legal implications. All of these ideas are explored further below.
Major Theories and Empirical Findings.
There are two main empirical findings that are most often cited in the political rhetoric surrounding the uniform debate. One of these, the case study of the Long Beach Unified School District, is most often cited in support of school uniforms and their effectiveness whereas Brunsma et al. (1998), "Effects of Student Uniforms on Attendance, Behavior Problems, Substance Use, and Academic Achievement" is the most frequently cited research in opposition to the implementation of school uniform policies.
"Long Beach Unified School District"
The case study of the Long Beach Unified School District was the study of the first large, urban school in the United States to implement a uniform policy. In 1994, mandatory school uniforms were implemented for the districts elementary and middle schools as a strategy to address the students’ behavior issues. The district simultaneously implemented a longitudinal study to research the effects of the uniforms on student behavior. The study attributed favorable student behavioral changes and a significant drop in school discipline issues to the mandatory uniform policy. This case study attributed the following noticeable outcomes to the use of uniforms throughout the district:
Brunsma et al. (1998): "Effects of Student Uniforms on Attendance, Behavior Problems, Substance Use, and Academic Achievement"
The Brunsma et al. (1998) research, on the other hand, empirically examined how a school uniform affects attendance, behavior problems, substance abuse, and academic achievement. In this very well known study, researchers tested the following hypotheses:
Researchers in this study expected that the direct of uniforms on these outcomes would disappear once the moderating variables were introduced to the equation. If this were to be the case, then arguments proclaiming uniform policies’ direct effect on said outcomes would be proven false. This study used a nationally representative sample of students. From the results, researchers were able to conclude that students wearing uniforms did not have any significant difference in academic preparedness or proschool attitudes than other students. Researchers also found that student uniforms were not significantly correlated with school commitment variables such as absenteeism, behavior, or substance use (drugs). In terms of the original four hypotheses researchers found that:
Because the four hypotheses were not supported, researcher were able to conclude that implementing uniform policies "at the high school level" does not create the desired outcomes, as all four of the original hypotheses were derived from public discourse surrounding the uniform debate. In fact, Brunsma et al., 1998 found that uniforms had a significant negative effect on achievement, as students who wore uniforms and had high proschool attitudes actually had worse behavior problems than all other students. Researchers in this study suggested that “instead of directly affecting specific outcomes, uniforms act as a catalyst for change and provide a highly visible opportunity for additional programs” within schools. In fact, Brunsma et al., 1998 found that this was the case with the Long Beach Unified School District case study, as several additional reform efforts were implemented simultaneously with the mandatory uniform policy.
Laws and rulings.
As uniforms have become more normalized, there have also been an increasing number of lawsuits brought against school districts. According to David Brunsma, one in four public elementary schools and one in eight public middle and high schools in the USA have policies dictating what a student wears to school. The school code within states’ constitutions typically asserts that it allows the board of school directors to make reasonable rules and regulations as they see fit in managing the school’s affairs. As of 2008, there are currently 23 states that allow school districts to mandate school uniforms. The constitutional objections usually brought upon school districts tend to fall into one of the following two categories: (1) a violation of the students’ First Amendment right to free expression (2) a violation of parents to raise their children without government interference. Although up until this point, The Supreme Court has not ruled on a case involving school uniforms directly, in the 1968 Tinker v. Des Moines Independent Community School District, the Court ruled that upon entering school, students do not shed their constitutional rights to freedom of speech.
Examples of lawsuits.
Canady v. Bossier Parish School Board
In the "Canady v. Bossier Parish School Board" lawsuit in 2000, a Louisiana district court ruled in favor of the school board because it did not see how the free speech rights of the students were being violated. Even though the plaintiff appealed the decision, the Fifth Circuit Court also ruled in favor of the school board after implementing a four-step system that is still used today. Firstly, a school board has to have the right to set up a policy. Secondly, the policy must be determined to support a fundamental interest of the board as a whole. Thirdly, the guidelines cannot have been set for the purpose of censorship. Finally, the limits on student expression cannot be greater than the interest of the board. As long as these four policies are in place, then no constitutional violation can be claimed.
Littlefield v. Forney Independent School District
In Forney, Texas in 2001, the school board decided to implement a school uniform policy allowing the students to wear a polo shirt, oxford shirt or blouse in four possible colors, and blue or khaki pants or shirts, a skirt or jumper. While there was some flexibility with shoes, certain types were prohibited along with any sort of baggy clothes. The parents of the Littlefield family requested that their son be exempt from the policy, but were unfortunately denied. In response, the Littlefields filed a lawsuit against the school district, under the pretenses that this uniform mandate infringed on their rights as parents to control how they brought up their children and their education. They even went as far as to cite an infringement on religious freedom, claiming that opting out of the uniforms on the grounds of religion allowed the school to rank the validity of certain religions. Before trial, the District Court dismissed the case, so the family appealed. Ultimately, the Fifth Circuit Court ruled that the students’ rights were not being violated even though the claims presented were valid. They ruled that school rules derived from the education would override the parents’ right to control their children’s upbringing in this specific situation. As far as the religious freedom violation accusations, the court ruled that the policy did not have a religious goal, and thus did not infringe on religious freedom rights.
Jacobs v. Clark County School District
In 2003, Liberty High School in Henderson, Nevada implemented a uniform policy of khakis and red, white or blue polo shirts. A junior by the name of Kimberly Jacobs was suspended a total of five times because she wore a religious shirt to school and got cited for uniform violations. Her family sued the Clark County School District under the claims that her First Amendment rights were being infringed upon and that the uniform policy was causing students to be deprived of due process. The plaintiff’s requests were for injunctive relief, the expunging of suspensions from Jacob’s school record and awarding of damages. The injunction was granted to the family meaning that the school could no longer discipline her for breaking the uniform policy. At this ruling, the school district appealed. The next court ruled on the side of the school district as it determined that the uniform policy was in fact neutral and constitutional, and it dismissed the claims of the plaintiff.
Fruedden v. Washoe County School District
In 2011, a Nevada public elementary school decided to add the school’s motto, “Tomorrow’s Leaders” embroidered in small letters on the shirt. In response, Mary and John Frudden, parents of a student sued the school district on the basis of it violating the 1st amendment’s guarantee of free speech. The court ultimately dismissed the case filed by the Frudden’s over the uniforms. . However, the family appealed, and two years later, a three-judge panel of the 9th U.S. Circuit Court of Appeals heard the case. The court ruled to reverse the previous decision of dismissing the case, and also questioned the apparent policy for students that were part of a nationally recognized group such as Boy Scouts and Girl Scouts who were able to wear the uniforms in place of the school ones on regular meeting days. The 9th circuit panel ruled that the school had not provided enough evidence for why it instituted this policy, and that the family was never given a chance to argue.
Internationally, there are differing views of school uniforms.In the Australian state of Queensland, Ombudsman Fred Albietz ruled in 1998 that public schools may not require uniforms. In the Philippines, the Department of Education abolished the requirement of school uniforms in public schools per DepEd Order No. 45, s. 2008. In England and Wales, technically a state school may not permanently exclude students for "breaching school uniform policy", under a policy promulgated by the Department for Children, Schools and Families but students not wearing the correct uniform are asked to go home and change. In Scotland, some local councils (that have responsibility for delivering state education) do not insist on students wearing a uniform as a precondition to attending and taking part in curricular activities. Turkey abolished mandatory uniforms in 2010.
Social Implications of Uniforms on Gender.
There are several positive and negative social implications of uniforms on both the students wearing them and society as a whole.
Implications on Perceptions of Masculinity and Femininity.
One of the primary criticisms of uniforms is that the standards of masculinity and femininity they impose on students from a very young age. Uniforms are a form of discipline that schools use to control students’ behavior and often promote conventional gendered dress. Boys often are required to wear pants, belts, and closed-toe shoes and have their shirts tucked in at all times. They are also often required to have their hair short. This uniform is associated with the dress of a professional business man, which gives boys at a young age the impression that masculinity is gained through business success. For girls, uniforms promote femininity by requiring girls to wear skirts. Skirts are seen as a symbol of femininity because they restrict movement and force certain ways of sitting and playing. In these ways, uniforms promote masculinity in boys and femininity in girls.
Implications for Transgendered Students.
Another criticism of uniforms is that they promote a binary gender classification. There are uniforms for females and uniforms for males but no options for students who do not necessarily identify as totally female or totally male. Therefore, students are not given the opportunity to explore their gender identity and must conform to the binary gender system. These students are punished for their exploration of gender expression because it violates uniform and dress codes (such as boys not having their hair longer than shoulder length or girls only being allowed to wear skirts and tights).
However, proponents of uniforms say that schools are a place for academic learning and that there must be some limitations on individual expression. They argue that if schools do not enforce rules, some focus on learning may be lost.
Implications for the Sexualization of Girls.
Uniforms often start to increase in popularity around middle school when students begin going through puberty. Uniforms can be seen as a way to restrict the sexualization of girls (rules on hems of skirts, no shoulders). Uniforms take the focus away from sexuality and focus it on academics in a school setting for girls.
Controversies.
General.
The topic of school uniforms has sparked a multitude of controversies and debates over the years. In many instances, this controversy starts from educational leadership claiming the effectiveness of school uniforms without significant research or data to back up their claims. Debates concerning the constitutionality and economic feasibility of uniforms also contribute to the controversy. More recently, controversy has risen from conflicts of gender with the dress code, as more frequently students are not always identifying with a female or male gender role. Some students might identify with a gender that differs significantly from their biological sex, creating a unique situation involving school uniforms and dress codes. See below for more information on the controversies surrounding school uniforms.
The implementation of school uniforms began due to ten years worth of research indicating the effectiveness of private schools. Some public school reformers took this research to aid in the consideration of implementing policies that are linked to private and catholic school success. However, within the Catholic school literature, school uniforms have never been acknowledged as a primary factor in producing a Catholic school effect (Bryk, Lee, & Holland 1993 pp 286–287) Regardless of the lack of evidence, public school administrators began implementing uniform policies in order to improve the overall school environment and academic achievement of the students. This implementation of school uniforms is based on the assumption that uniforms are the direct cause of behavioral and academic outcome changes. These assumptions are largely fueled by conjecture and anecdotal evidence, but are used to put uniform policies in place nonetheless.
Another area of controversy regarding school uniform and dress code policies revolve around the issue of gender. Nowadays, more teenagers are more frequently “dressing to articulate, or confound gender identity and sexual orientation” which brings about “responses from school officials that ranged from indifferences to applause to bans”. In 2009, there were multiple conflicts across the United States arising from disparities between the students’ perception of their own gender, and the school administrators’ perception of the students’ gender identity. Instances include the following:
Significant controversy associated with gender and uniform policies is due to the fact that disciplining a student whose wardrobe deviates from expected sexual orientation or gender, anti-discrimination policies, mental health factors, community standards, and classroom distractions must be taken into account (Hoffman). Oftentimes, in making decisions about how particular students can and cannot dress, it becomes a question of whether or not the administration is protecting the students from peer harassment, or whether the administration is harassing the student themselves.
Pros.
Advocates of uniforms have proposed multiple reasons supporting their implementation and claiming their success in schools. A variety of these claims have research supporting them. Some of these pros include the following:
Advocates believe that uniforms affect student safety by:
For example, in the first year of the mandatory uniform policy in Long Beach, California, officials reported that fighting in schools decreased by more than 50%, assault and battery by 34%, sex offenses by 74%, and robbery by 66%. 
Advocates also believe that uniforms increase student learning and positive attitudes toward school through:
Wearing uniforms leads to decreased behavior problems by increasing attendance rates, lowering suspension rates, and decreasing substance use among the student body. Proponents also attribute positive psychological outcomes like increased self-esteem, increased spirit, and reinforced feelings of oneness among students to wearing uniforms. Additional proponent arguments include that school uniforms:
Currently pros of school uniforms center around how uniforms impact schools' environments. Proponents have found a significant positive impact on school climate, safety, and students’ self-perception from the implementation of uniforms.
Cons.
Those who oppose the implementation of uniforms have claimed their ineffectiveness using a variety of justifications, a variety of which have research supporting them. Some of the cons to school uniforms include the following legal, financial, and questionable effectiveness concerns:
According to Marian Wilde, additional opponent arguments include that school uniforms:
Conclusions.
No definitive conclusions on uniforms can be made, despite all the theories and empirical findings on the subject. Some studies have certainly shown the benefits of uniforms, like the Long Beach Unified School District study, which concluded there was some type of positive correlation between their uniform policy and student behavior. On the other hand Brunsma’s study saw that uniforms did not decrease behavior problems, absenteeism, or substance abuse and also did not increase student achievement. In addition to these studies there are other implications, especially in regards to gender. A uniform can allow a school to control gendered dress, but that can become a large issue when looking at gender classification in such a binary way. There still exist many pros for school uniforms, such as decreasing gang activity, differentiating intruders, better learning environments and breaking down social barriers to name a few. Despite this there are an equal number of drawbacks, such as individual rights violation, cost, lack of empirical evidence in support of uniforms and simply being a band-aid on school violence. With controversy often comes legal ramifications and there certainly is no shortage of that in regards to school uniforms, particularly in a country where individual expression and freedom is so highly valued. Perhaps only time as well as more research will lead us closer to determining the true pros and cons of uniforms.

</doc>
<doc id="55620" url="http://en.wikipedia.org/wiki?curid=55620" title="Catiline Orations">
Catiline Orations

The Catiline Orations or Catilinarian Orations were speeches given in 63 BC by Marcus Tullius Cicero, the consul of Rome, exposing to the Roman Senate the plot of Lucius Sergius Catilina and his allies to overthrow the Roman government.
The Catiline plot and the orations of Cicero.
Running for the consulship for a second time after having lost the first time, Catiline was an advocate for the poor, calling for the cancellation of debts and land redistribution. Cicero, in indignation, issued a law prohibiting machinations of this kind. It was obvious to all that the law was directed specifically at Catiline. Catiline, in turn, conspired to murder Cicero and the key men of the Senate on the day of the election in what became known as the second Catilinarian conspiracy. Cicero discovered the plan and postponed the election to give the Senate time to discuss the attempted coup d'état.
The day after the election was supposed to be held, Cicero addressed the Senate on the matter and Catiline's reaction was immediate and violent. In response to Catiline's behavior, the Senate issued a "senatus consultum ultimum", a kind of declaration of martial law invoked whenever the Senate and the Roman Republic were considered to be in imminent danger from treason or sedition. Ordinary law was suspended and Cicero, as consul, was invested with absolute power.
When the election was finally held, Catiline lost again. Anticipating the bad news, the conspirators had already begun to assemble an army, made up mostly of Sulla's veteran soldiers. The nucleus of conspirators was also joined by some senators. The plan was to initiate an insurrection in all of Italy, put Rome to the torch and to kill as many senators as they could.
Through his own investigations, Cicero was aware of the conspiracy. On November 8, Cicero called for a meeting of the Senate in the Temple of Jupiter Stator near the forum, which was used for this purpose only when great danger was imminent. Catiline attended as well. It was in this context that Cicero delivered one of his most famous orations.
"Oratio in Catilinam Prima in Senatu Habita".
As political orations go, this was relatively short—some 3,400 words—and to the point. The opening remarks are still widely remembered and used after 2,000 years:
Quo usque tandem abutere, Catilina, patientia nostra? Quam diu etiam furor iste tuus nos eludet? Quem ad finem sese effrenata iactabit audacia?
<br><br>
How long, O Catiline, will you abuse our patience? And for how long will that madness of yours mock us? To what end will your unbridled audacity hurl itself?
Also remembered is the famous exasperated exclamation, "O tempora, o mores!" (Oh the times! Oh the customs!)
Catiline was present when this speech was delivered. When he arrived at the Temple of Jupiter Stator and took his seat, however, the other senators moved away from him leaving him alone in his bench. Catiline tried to reply after the speech, but senators repeatedly interrupted him, calling him a traitor. He ran from the temple, hurling threats at the Senate. Later he left the city and, though he claimed that he was placing himself in self-imposed exile at Marseilles, he in fact went to the camp of Manlius, who was in charge of the army of rebels. The next morning Cicero assembled the people, and gave a further oration.
"Oratio in Catilinam Secunda Habita ad Populum".
In this speech, Cicero informed the citizens of Rome that Catiline had left the city, not in exile (as it was rumored), but to join with his illegal army. He described the conspirators as rich men who were in debt, men eager for power and wealth, Sulla's veterans, ruined men who hoped for any change, criminals, profligates, and other men of Catiline's ilk. He assured the people of Rome that they had nothing to fear because he, the consul, and the gods would protect the state.
Meanwhile, Catiline joined up with Gaius Manlius, commander of the rebel force. When the Senate was informed of these developments, they declared the two of them public enemies. Antonius Hybrida (Cicero's fellow consul), with troops loyal to Rome, followed Catiline while Cicero remained at home to guard the city.
"Oratio in Catilinam Tertia ad Populum".
In this speech, Cicero claims that the city should rejoice because it has been saved from a bloody rebellion. He presents evidence that all of Catiline's accomplices confessed to their crimes. He asked for nothing for himself but the grateful remembrance of the city, and acknowledged that this victory was more difficult than one in foreign lands because the enemies were citizens of Rome.
"Oratio in Catilinam Quarta in Senatu Habita".
In his fourth and final argument, which took place in the Temple of Concordia, Cicero establishes a basis for other orators (primarily Cato) to argue for the execution of the conspirators. As consul, Cicero was formally not allowed to voice any opinion in the matter, but he circumvented the rule with subtle oratory. Although very little is known about the actual debate (except for Cicero's argument, which has probably been altered from its original), the Senate majority probably opposed the death sentence for various reasons, one of which was the nobility of the accused. For example, Julius Caesar argued that exile and disenfranchisement would be sufficient punishment for the conspirators, and one of the accused, Lentulus, was a praetor. However, after the combined efforts of Cicero and Cato, the vote shifted in favor of execution, and the sentence was carried out shortly afterwards.
While most historians agree that Cicero's actions, and in particular the final speeches before the Senate, saved the republic, they also reflect his self-aggrandisement—and to a certain extent envy—probably born out of the fact that he was considered a "novus homo", a Roman citizen without noble or ancient lineage.

</doc>
<doc id="55623" url="http://en.wikipedia.org/wiki?curid=55623" title="Cotyledon">
Cotyledon

A cotyledon (; "seed leaf" from Greek: κοτυληδών "kotylēdōn", gen.: κοτυληδόνος "kotylēdonos", from κοτύλη "kotýlē" "cup, bowl") is a significant part of the embryo within the seed of a plant. Upon germination, the cotyledon may become the embryonic first leaves of a seedling. The number of cotyledons present is one characteristic used by botanists to classify the flowering plants (angiosperms). Species with one cotyledon are called monocotyledonous ("monocots"). Plants with two embryonic leaves are termed dicotyledonous ("dicots") and placed in the class Magnoliopsida.
In the case of dicot seedlings whose cotyledons are photosynthetic, the cotyledons are functionally similar to leaves. However, true leaves and cotyledons are developmentally distinct. Cotyledons are formed during embryogenesis, along with the root and shoot meristems, and are therefore present in the seed prior to germination. True leaves, however, are formed post-embryonically (i.e. after germination) from the shoot apical meristem, which is responsible for generating subsequent aerial portions of the plant.
The cotyledon of grasses and many other monocotyledons is a highly modified leaf composed of a "scutellum" and a "coleoptile". The scutellum is a tissue within the seed that is specialized to absorb stored food from the adjacent endosperm. The coleoptile is a protective cap that covers the "plumule" (precursor to the stem and leaves of the plant).
Gymnosperm seedlings also have cotyledons, and these are often variable in number (multicotyledonous), with from 2 to 24 cotyledons forming a whorl at the top of the hypocotyl (the embryonic stem) surrounding the plumule. Within each species, there is often still some variation in cotyledon numbers, e.g. Monterey pine ("Pinus radiata") seedlings have 5–9, and Jeffrey pine ("Pinus jeffreyi") 7–13 (Mirov 1967), but other species are more fixed, with e.g. Mediterranean cypress always having just two cotyledons. The highest number reported is for big-cone pinyon ("Pinus maximartinezii"), with 24 (Farjon & Styles 1997).
The cotyledons may be ephemeral, lasting only days after emergence, or persistent, enduring a year or more on the plant. The cotyledons contain (or in the case of gymnosperms and monocotyledons, have access to) the stored food reserves of the seed. As these reserves are used up, the cotyledons may turn green and begin photosynthesis, or may wither as the first true leaves take over food production for the seedling.
Epigeal versus hypogeal development.
Cotyledons may be either epigeal, expanding on the germination of the seed, throwing off the seed shell, rising above the ground, and perhaps becoming photosynthetic; or hypogeal, not expanding, remaining below ground and not becoming photosynthetic. The latter is typically the case where the cotyledons act as a storage organ, as in many nuts and acorns.
Hypogeal plants have (on average) significantly larger seeds than epigeal ones. They also are capable of surviving if the seedling is clipped off, as meristem buds remain underground (with epigeal plants, the meristem is clipped off if the seedling is grazed). The tradeoff is whether the plant should produce a large number of small seeds, or a smaller number of seeds which are more likely to survive.
Related plants show a mixture of hypogeal and epigeal development, even within the same plant family. Groups which contain both hypogeal and epigeal species include, for example, the Araucariaceae family of Southern Hemisphere conifers, the Fabaceae (pea family), and the genus "Lilium" (see Lily seed germination types). The frequently garden grown common bean - Phaseolus vulgaris - is epigeal while the closely related runner bean - Phaseolus coccineus - is hypogeal.
History.
The term "cotyledon" was coined by Marcello Malpighi. John Ray was the first botanist to recognize that some plants have two and others only one, and eventually the first to recognize the immense importance of this fact to systematics, in "Methodus plantarum" (1682).
Theophrastus (3rd or 4th century BC) and Albertus Magnus (13th century) may also have recognized the distinction between the dicotyledons and monocotyledons.

</doc>
<doc id="55625" url="http://en.wikipedia.org/wiki?curid=55625" title="Monocotyledon">
Monocotyledon

Monocotyledons (), also known as monocots, are one of the major groups into which flowering plants (or angiosperms) are divided. Traditionally, the rest of the flowering plants were classed as dicotyledons, or dicots. Monocot seedlings typically have one cotyledon (seed-leaf), in contrast to the two cotyledons typical of dicots. Modern research using molecular phylogenetic methods has shown that the monocots form a monophyletic group – a clade – since they comprise all the descendants of a common ancestor. Dicots, by contrast, do not form a monophyletic group.
Monocots have been recognized as a group at various taxonomic ranks, and under various names (see below). The APG III system of 2009 recognises a clade called "monocots" but does not assign it to a taxonomic rank.
According to the IUCN there are 59,300 species of monocots. The largest family in this group (and in the flowering plants as a whole) by number of species are the orchids (family Orchidaceae), with more than 20,000 species. In agriculture the majority of the biomass produced comes from monocots. The true grasses, family Poaceae (Gramineae), are the most economically important family in this group. These include all the true grains (rice, wheat, maize, etc.), the pasture grasses, sugar cane, and the bamboos. True grasses have evolved to become highly specialised for wind pollination. Grasses produce much smaller flowers, which are gathered in highly visible plumes (inflorescences). Other economically important monocot families are the palm family (Arecaceae), banana family (Musaceae), ginger family (Zingiberaceae) and the amaryllis family (Amaryllidaceae), which includes such ubiquitously used vegetables as onions and garlic.
Many plants cultivated for their blooms are also from the monocot group, notably lilies, daffodils, irises, amaryllis, orchids, cannas, bluebells and tulips.
Description.
The monocots are one of the major divisions of the flowering plants or angiosperms. They have been recognized as a natural group since at least the work of the English botanist John Ray in the 17th century. Modern research based on DNA has confirmed the status of the monocots as a monophyletic group or clade, in contrast to the other historical divisions of the flowering plants, which have had to be substantially reorganized. The name monocotyledons is derived from the traditional botanical name "Monocotyledones", which refers to the fact that most members of this group have one cotyledon, or embryonic leaf, in their seeds. Historically, this feature was used to contrast the monocots with the dicotyledons or dicots which typically have two cotyledons; however modern research has shown that the dicots are not a natural group. From a diagnostic point of view the number of cotyledons is neither a particularly useful characteristic (as they are only present for a very short period in a plant's life), nor is it completely reliable.
Additionally, one of the most noticeable traits is that a monocot's flower is trimerous, with the flower parts in threes or in multiples of three—having three, six, or nine petals. Many monocots also have leaves with parallel veins.
Morphology, compared to the (broadly defined) dicotyledons.
The traditionally listed differences between monocotyledons and dicotyledons are as follows. This is a broad sketch only, not invariably applicable, as there are a number of exceptions. The differences indicated are more true for monocots versus eudicots.
The vast majority of monocots lack a petiole in their leaves.
A number of these differences are not unique to the monocots. For example, trimerous flowers and monosulcate pollen are also found in magnoliids. Exclusively adventitious roots are found also in Nymphaeaceae and some of the Piperaceae. Similarly, at least one of these traits, parallel leaf veins, is far from universal among the monocots. Monocots with reticulate leaf veins are found in a wide variety of monocot families: for example, "Trillium", "Smilax" (greenbriar), and "Pogonia" (an orchid), and the Dioscoreales. Nevertheless, this list of traits is a generally valid set of contrasts, especially when contrasting monocots with eudicots rather than non-monocot flowering plants in general.
Emergence.
Some monocots, such as grasses, have hypogeal emergence, where the mesocotyl elongates and pushes the coleoptile (which encloses and protects the shoot tip) toward the soil surface. Since elongation occurs above the cotyledon, it is left in place in the soil where it was planted. 
Many dicots have epigeal emergence, in which the hypocotyl elongates and becomes arched in the soil. As the hypocotyl continues to elongate, it pulls the cotyledons upward, above the soil surface.
Vascular system.
Monocots have a distinctive arrangement of vascular tissue known as an atactostele in which the vascular tissue is scattered rather than arranged in concentric rings. Many monocots are herbaceous and do not have the ability to increase the width of a stem (secondary growth) via the same kind of vascular cambium found in non-monocot woody plants. However, some monocots do have secondary growth, and because it does not arise from a single vascular cambium producing xylem inwards and phloem outwards, it is termed "anomalous secondary growth". Examples of large monocots which either exhibit secondary growth, or can reach large sizes without it, are palms (Arecaceae), screwpines (Pandanaceae), bananas (Musaceae), "Yucca", "Aloe", "Dracaena", and "Cordyline".
Classification.
The monocots are considered to form a monophyletic group arising early in the history of the flowering plants. The earliest fossils presumed to be monocot remains date from the early Cretaceous period.
Taxonomists have considerable latitude in naming this group, as the monocots are a group above the rank of family. Article 16 of the "ICBN" allows either a descriptive name or a name formed from the name of an included family.
Historically, the monocotyledons were named:
Until the rise of the phylogenetic APG systems, it was widely accepted that angiosperms were neatly split between monocots and dicots, a state reflected in virtually all the systems. It is now understood that various groups, notably the Magnoliids and ancient lineages known as the basal angiosperms fall outside of this dichotomy. Each of these systems uses its own internal taxonomy for the group. The monocotyledons are famous as a group that is extremely stable in its outer borders (it is a well-defined, coherent group), while in its internal taxonomy is extremely unstable (historically no two authoritative systems have agreed with each other on how the monocotyledons are related to each other).
Molecular studies have both confirmed the monophyly of the monocots and helped elucidate relationships within this group. The APG II system does not assign the monocots to a taxonomic rank, instead recognizing a monocots clade. This system recognizes ten orders of monocots and two families of monocots (Petrosaviaceae and Dasypogonaceae) not yet assigned to any order. More recently, the Petrosaviaceae has been included in the Petrosaviales, and placed near the lilioid orders. The family Hydatellaceae, assigned to order Poales in the APG II system, has since been recognized as being misplaced in the monocots, and instead proves to be most closely related to the water lilies, family Nymphaeaceae.
Evolution.
For a very long time, fossils of palm trees were believed to be the oldest monocots, first appearing 90 million years ago, but this estimate may not be entirely true. At least some putative monocot fossils have been found in strata as old as the eudicots. The oldest fossils that are unequivocally monocots are pollen from the Late Barremian–Aptian – Early Cretaceous period, about 120-110 million years ago, and are assignable to clade-Pothoideae-Monstereae Araceae; being Araceae, sister to other Alismatales. They have also found flower fossils of Triuridaceae (Pandanales) in Upper Cretaceous rocks in New Jersey, becoming the oldest known sighting of saprophytic/mycotrophic habits in angiosperm plants and among the oldest known fossils of monocotyledons.
Topology of the angiosperm phylogenetic tree could infer that the monocots would be among the oldest lineages of angiosperms, which would support the theory that they are just as old as the eudicots. The pollen of the eudicots dates back 125 million years, so the lineage of monocots should be that old too.
Molecular clock estimates for the age of extant monocots.
Kåre Bremer, using rbcL sequences and the mean path length method ("mean-path lengths method"), estimated the age of the monocot crown group (i.e. the time at which the ancestor of today's "Acorus" diverged from the rest of the group) as 134 million years. Similarly, Wikström "et al.", using Sanderson's non-parametric rate smoothing approach ("nonparametric rate smoothing approach"), obtained ages of 158 or 141 million years for the crown group of monocots. All these estimates have large error ranges (usually 15-20%), and Wikström "et al." used only a single calibration point, namely the split between Fagales and Cucurbitales, which was set to 84 Ma, in the late Santonian period). Early molecular clock studies using strict clock models had estimated the monocot crown age to 200 ± 20 million years ago or 160 ± 16 million years, while studies using relaxed clocks have obtained 135-131 million years or 133.8 to 124 million years. Bremer's estimate of 134 million years has been used as a secondary calibration point in other analyses.
Core group.
The age of the core group of so-called 'nuclear monocot' or 'core monocots' by the Angiosperm Phylogeny Website ("core monocots" in English), which correspond to all orders except Acorales and Alismatales, is about 131 million years to present, and crown group age is about 126 million years to the present. The subsequent branching in this part of the tree (i.e. Petrosaviaceae, Dioscoreales + Pandanales and Liliales clades appeared), including the crown Petrosaviaceae group may be in the period around 125–120 million years BC (about 111 million years so far), and stem groups of all other orders, including Commelinidae would have diverged about or shortly after 115 million years. These and many clades within these orders may have originated in southern Gondwana, i.e. Antarctica, Australasia, and southern South America.
Aquatic monocots.
The aquatic monocots of Alismatales have commonly been regarded as "primitive". They have also been considered to have the most primitive foliage, which were cross-linked as Dioscoreales and Melanthiales. Keep in mind that the "most primitive" monocot is not necessarily "the sister of everyone else". This is because the ancestral or primitive characters are inferred by means of the reconstruction of characteristic states, with the help of the phylogenetic tree. So primitive characters of monocots may be present in some derived groups. On the other hand, the basal taxa may exhibit many morphological autapomorphies. So although Acoraceae is the sister group to the remaining monocotyledons, the result does not imply that Acoraceae is "the most primitive monocot" in terms of its characteristics. In fact, Acoraceae is highly derived in most morphological characteristics, which is precisely why so many Alismatales Acoraceae occupied relatively imitative positions in trees produced by Chase "et al." (1995b) and Stevenson & Loconte (1995).
Some authors support the idea of an aquatic phase as the origin of monocots. The phylogenetic position of Alismatales (many water), which occupy a relationship with the rest except the Acoraceae, do not rule out the idea, because it could be 'the most primitive monocots' but not 'the most basal'. The Atactostele stem, the long and linear leaves, the absence of secondary growth (see the biomechanics of living in the water), roots in groups instead of a single root branching (related to the nature of the substrate), including sympodial use, are consistent with a water source. However, while monocots were sisters of the aquatic Ceratophyllales, or their origin is related to the adoption of some form of aquatic habit, it would not help much to the understanding of how it evolved to develop their distinctive anatomical features: the monocots seem so different from the rest of angiosperms and it's difficult to relate their morphology, anatomy and development and those of broad-leaved angiosperms.
Other taxa.
In the past, taxa which had petiolate leaves with reticulate venation were considered "primitive" within the monocots, because of its superficial resemblance to the leaves of dicotyledons. Recent work suggests that these taxa are sparse in the phylogenetic tree of monocots, such as fleshy fruited taxa (excluding taxa with aril seeds dispersed by ants), the two features would be adapted to conditions that evolved together regardless. Among the taxa involved were "Smilax", "Trillium" (Liliales), "Dioscorea" (Dioscoreales), etc. A number of these plants are vines that tend to live in shaded habitats for at least part of their lives, and may also have a relationship with their shapeless stomata. Reticulate venation seems to have appeared at least 26 times in monocots, in fleshy fruits 21 times (sometimes lost later), and the two characteristics, though different, showed strong signs of a tendency to be good or bad in tandem, a phenomenon described as "concerted convergence" ("coordinated convergence").

</doc>
<doc id="55632" url="http://en.wikipedia.org/wiki?curid=55632" title="Linear combination">
Linear combination

In mathematics, a linear combination is an expression constructed from a set of terms by multiplying each term by a constant and adding the results (e.g. a linear combination of "x" and "y" would be any expression of the form "ax" + "by", where "a" and "b" are constants). The concept of linear combinations is central to linear algebra and related fields of mathematics.
Most of this article deals with linear combinations in the context of a vector space over a field, with some generalizations given at the end of the article.
Definition.
Suppose that "K" is a field (for example, the real numbers) and "V" is a vector space over "K". As usual, we call elements of "V" "vectors" and call elements of "K" "scalars".
If "v"1...,"v""n" are vectors and "a"1...,"a""n" are scalars, then the "linear combination of those vectors with those scalars as coefficients" is
There is some ambiguity in the use of the term "linear combination" as to whether it refers to the expression or to its value. In most cases the value is emphasized, like in the assertion "the set of all linear combinations of "v"1...,"v""n" always forms a subspace". However, one could also say "two different linear combinations can have the same value" in which case the expression must have been meant. The subtle difference between these uses is the essence of the notion of linear dependence: a family "F" of vectors is linearly independent precisely if any linear combination of the vectors in "F" (as value) is uniquely so (as expression). In any case, even when viewed as expressions, all that matters about a linear combination is the coefficient of each "v""i"; trivial modifications such as permuting the terms or adding terms with zero coefficient do not give distinct linear combinations.
In a given situation, "K" and "V" may be specified explicitly, or they may be obvious from context. In that case, we often speak of "a linear combination of the vectors" "v"1...,"v""n", with the coefficients unspecified (except that they must belong to "K"). Or, if "S" is a subset of "V", we may speak of "a linear combination of vectors in S", where both the coefficients and the vectors are unspecified, except that the vectors must belong to the set "S" (and the coefficients must belong to "K"). Finally, we may speak simply of "a linear combination", where nothing is specified (except that the vectors must belong to "V" and the coefficients must belong to "K"); in this case one is probably referring to the expression, since every vector in "V" is certainly the value of some linear combination.
Note that by definition, a linear combination involves only finitely many vectors (except as described in Generalizations below).
However, the set "S" that the vectors are taken from (if one is mentioned) can still be infinite; each individual linear combination will only involve finitely many vectors.
Also, there is no reason that "n" cannot be zero; in that case, we declare by convention that the result of the linear combination is the zero vector in "V".
Examples and counterexamples.
Euclidean vectors.
Let the field "K" be the set R of real numbers, and let the vector space "V" be the Euclidean space R3.
Consider the vectors "e"1 = (1,0,0), "e"2 = (0,1,0) and "e"3 = (0,0,1).
Then "any" vector in R3 is a linear combination of "e"1, "e"2 and "e"3.
To see that this is so, take an arbitrary vector ("a"1,"a"2,"a"3) in R3, and write:
Functions.
Let "K" be the set C of all complex numbers, and let "V" be the set CC("R") of all continuous functions from the real line R to the complex plane C.
Consider the vectors (functions) "f" and "g" defined by "f"("t") := "e""it" and "g"("t") := "e"−"it".
Some linear combinations of "f" and "g" are:
On the other hand, the constant function 3 is "not" a linear combination of "f" and "g". To see this, suppose that 3 could be written as a linear combination of "e""it" and "e"−"it". This means that there would exist complex scalars "a" and "b" such that "ae""it" + "be"−"it" = 3 for all real numbers "t". Setting "t" = 0 and "t" = π gives the equations "a" + "b" = 3 and "a" + "b" = −3, and clearly this cannot happen. See Euler's identity.
Polynomials.
Let "K" be R, C, or any field, and let "V" be the set "P" of all polynomials with coefficients taken from the field "K".
Consider the vectors (polynomials) "p"1 := 1, "p"2 := "x" + 1, and "p"3 := "x"2 + "x" + 1.
Is the polynomial "x"2 − 1 a linear combination of "p"1, "p"2, and "p"3?
To find out, consider an arbitrary linear combination of these vectors and try to see when it equals the desired vector "x"2 − 1.
Picking arbitrary coefficients "a"1, "a"2, and "a"3, we want
Multiplying the polynomials out, this means
and collecting like powers of "x", we get
Two polynomials are equal if and only if their corresponding coefficients are equal, so we can conclude
This system of linear equations can easily be solved.
First, the first equation simply says that "a"3 is 1.
Knowing that, we can solve the second equation for "a"2, which comes out to −1.
Finally, the last equation tells us that "a"1 is also −1.
Therefore, the only possible way to get a linear combination is with these coefficients.
Indeed,
so "x"2 − 1 "is" a linear combination of "p"1, "p"2, and "p"3.
On the other hand, what about the polynomial "x"3 − 1?
If we try to make this vector a linear combination of "p"1, "p"2, and "p"3, then following the same process as before, we’ll get the equation
However, when we set corresponding coefficients equal in this case, the equation for "x"3 is
which is always false.
Therefore, there is no way for this to work, and "x"3 − 1 is "not" a linear combination of "p"1, "p"2, and "p"3.
The linear span.
"Main article: linear span"
Take an arbitrary field "K", an arbitrary vector space "V", and let "v"1...,"v""n" be vectors (in "V").
It’s interesting to consider the set of "all" linear combinations of these vectors.
This set is called the "linear span" (or just "span") of the vectors, say S ={"v"1...,"v""n"}. We write the span of S as span(S) or sp(S):
Linear independence.
For some sets of vectors "v"1...,"v""n",
a single vector can be written in two different ways as a linear combination of them:
Equivalently, by subtracting these (formula_17) a non-trivial combination is zero:
If that is possible, then "v"1...,"v""n" are called "linearly dependent"; otherwise, they are "linearly independent".
Similarly, we can speak of linear dependence or independence of an arbitrary set "S" of vectors.
If "S" is linearly independent and the span of "S" equals "V", then "S" is a basis for "V".
Affine, conical, and convex combinations.
By restricting the coefficients used in linear combinations, one can define the related concepts of affine combination, conical combination, and convex combination, and the associated notions of sets closed under these operations.
Because these are more "restricted" operations, more subsets will be closed under them, so affine subsets, convex cones, and convex sets are "generalizations" of vector subspaces: a vector subspace is also an affine subspace, a convex cone, and a convex set, but a convex set need not be a vector subspace, affine, or a convex cone.
These concepts often arise when one can take certain linear combinations of objects, but not any: for example, probability distributions are closed under convex combination (they form a convex set), but not conical or affine combinations (or linear), and positive measures are closed under conical combination but not affine or linear – hence one defines signed measures as the linear closure.
Linear and affine combinations can be defined over any field (or ring), but conical and convex combination require a notion of "positive", and hence can only be defined over an ordered field (or ordered ring), generally the real numbers.
If one allows only scalar multiplication, not addition, one obtains a (not necessarily convex) cone; one often restricts the definition to only allowing multiplication by positive scalars.
All of these concepts are usually defined as subsets of an ambient vector space (except for affine spaces, which are also considered as "vector spaces forgetting the origin"), rather than being axiomatized independently.
Operad theory.
More abstractly, in the language of operad theory, one can consider vector spaces to be algebras over the operad formula_19 (the infinite direct sum, so only finitely many terms are non-zero; this corresponds to only taking finite sums), which parametrizes linear combinations: the vector formula_20 for instance corresponds to the linear combination formula_21. Similarly, one can consider affine combinations, conical combinations, and convex combinations to correspond to the sub-operads where the terms sum to 1, the terms are all non-negative, or both, respectively. Graphically, these are the infinite affine hyperplane, the infinite hyper-octant, and the infinite simplex. This formalizes what is meant by formula_22 being or the standard simplex being model spaces, and such observations as that every bounded convex polytope is the image of a simplex. Here suboperads correspond to more restricted operations and thus more general theories.
From this point of view, we can think of linear combinations as the most general sort of operation on a vector space – saying that a vector space is an algebra over the operad of linear combinations is precisely the statement that "all possible" algebraic operations in a vector space are linear combinations.
The basic operations of addition and scalar multiplication, together with the existence of an additive identity and additive inverses, cannot be combined in any more complicated way than the generic linear combination: the basic operations are a generating set for the operad of all linear combinations.
Ultimately, this fact lies at the heart of the usefulness of linear combinations in the study of vector spaces.
Generalizations.
If "V" is a topological vector space, then there may be a way to make sense of certain "infinite" linear combinations, using the topology of "V".
For example, we might be able to speak of "a"1"v"1 + "a"2"v"2 + "a"3"v"3 + ..., going on forever.
Such infinite linear combinations do not always make sense; we call them "convergent" when they do.
Allowing more linear combinations in this case can also lead to a different concept of span, linear independence, and basis.
The articles on the various flavours of topological vector spaces go into more detail about these.
If "K" is a commutative ring instead of a field, then everything that has been said above about linear combinations generalizes to this case without change.
The only difference is that we call spaces like this "V" modules instead of vector spaces.
If "K" is a noncommutative ring, then the concept still generalizes, with one caveat:
Since modules over noncommutative rings come in left and right versions, our linear combinations may also come in either of these versions, whatever is appropriate for the given module.
This is simply a matter of doing scalar multiplication on the correct side.
A more complicated twist comes when "V" is a bimodule over two rings, "K"L and "K"R.
In that case, the most general linear combination looks like
where "a"1...,"a""n" belong to "K"L, "b"1...,"b""n" belong to "K"R, and "v"1...,"v""n" belong to "V".

</doc>
<doc id="55633" url="http://en.wikipedia.org/wiki?curid=55633" title="Region">
Region

In geography, regions are areas broadly divided by physical characteristics (physical geography), human-impact characteristics (human geography), and the interaction of humanity and the environment (environmental geography). Geographic regions and sub-regions are mostly described by their imprecisely defined, and sometimes transitory boundaries, except in human geography, where jurisdiction areas such as national borders are clearly defined in law.
Apart from the global continental regions, there are also hydrospheric and atmospheric regions that cover the oceans, and discrete climates above the land and water masses of the planet. The land and water global regions are divided into subregions geographically bounded by large geological features that influence large-scale ecologies, such as plains and features.
As a way of describing spatial areas, the concept of regions is important and widely used among the many branches of geography, each of which can describe areas in regional terms. For example, ecoregion is a term used in environmental geography, cultural region in cultural geography, bioregion in biogeography, and so on. The field of geography that studies regions themselves is called regional geography.
In the fields of physical geography, ecology, biogeography, zoogeography, and environmental geography, regions tend to be based on natural features such as ecosystems or biotopes, biomes, drainage basins, natural regions, mountain ranges, soil types. Where human geography is concerned, the regions and subregions are described by the discipline of ethnography.
A region has its own nature that could not be moved. The first nature is its natural environment (landform, climate, etc.). The second nature is its physical elements complex that were built by people in the past. The third nature is its socio-cultural context that could not be replaced by new immigrants.
Globalization.
Global regions distinguishable from space, and are therefore clearly distinguished by the two basic terrestrial environments, land and water. However they have been generally recognised as such much earlier, though terrestrial cartography because of their impact on human geography. They are divided into largest of land regions, known as continents, and the largest of water regions known as oceans. There are also significant regions that do not belong to either of these classifications, such as archipelago regions that are littoral regions, or earthquake regions that are defined in geology.
With one exception, Australia, continents are not defined by their human geography.
Continental regions.
Continental regions are usually based on broad experiences in human history and attempts to reduce very large areas to more manageable regionalisation for the purpose of study. As such they are conceptual constructs, usually lacking distinct boundaries. Oceanic division into maritime regions are used in conjunction with the relationship to the central area of the continent, using directions of the compass. Some continental regions are defined by the major continental feature of their identity, such as the Amazon basin, or the Sahara, which both occupy a significant percentage of their respective continental land area.
To a large extent, major continental regions are mental constructs created by considering an efficient way to define large areas of the continents. For the most part, the images of the World are derived as much from academic study s the media, or from personal experience of global exploration. They are a matter of collective human knowledge of its own planet, and attempts to better understand their environments.
Regional geography.
Regional geography is a branch of geography that studies regions of all sizes across the Earth. It has a prevailing descriptive character. The main aim is to understand or define the uniqueness or character of a particular region, which consists of natural as well as human elements. Attention is paid also to regionalization, which covers the proper techniques of space delimitation into regions.
Regional geography is also considered as a certain approach to study in geographical sciences (similar to quantitative or critical geographies, for more information see History of geography).
Geographical regions.
Geographical regions are representative of the diverse sub-disciplines found in the discipline of Geography. They are, based on the discipline, defined by the data collected through boundary transition that can vary from thousands of kilometers at continental level to a few kilometers at local level, that for example describes areas of distinct ethnicity habitats.
The United Nations Statistics Division has identified a scheme a systematic classification of macro-geographic regions (continents), and sub-continental subregions, and selected socioeconomic groupings.
Regions in physical geography.
Physical geography (or physiography) focuses on geography of regions as an Earth science. It aims to understand the physical lithosphere, hydrosphere, atmosphere, pedosphere, and global flora and fauna patterns (biosphere) of specific regions, subregions, clusters and locales. Physical regions are usually described by surface geological formations, hydrological and littoral surface features, discrete landscape features, and unique flora and fauna distribution that are not clearly delineated, and are separated by often wide transitional boundaries.
Palaeogeographic regions.
Palaeogeography is the study of ancient geologic environments. Since the physical structures of the Earth's surface have changed over geologic time, palaeogeographers have coined various names for ancient regions that no longer exist, from very large regions such as the supercontinents Rodinia, Pangaea, and Pannotia, to relatively small regions like Beringia. Other examples include the Tethys Ocean and Ancylus Lake. Palaeogeographic continental regions that include Laurentia, Proto-Laurasia, Laurasia, Euramerica (the "Old Red Continent"), and Gondwana.The Paleogeographic region is also where paleontologist find answers in history.
Regions in human geography.
Human geography is a branch of geography that focuses on the study of patterns and processes that shape human interaction with various discrete environments. It encompasses human, political, cultural, social, and economic aspects among others that are often clearly delineated. While the major focus of human geography is not the physical landscape of the Earth (see physical geography), it is hardly possible to discuss human geography without referring to the physical landscape on which human activities are being played out, and environmental geography is emerging as a link between the two. Regions of human geography can be divided into many broad categories, such as:
Historical regions.
The field of historical geography involves the study of human history as it relates to places and regions, or, inversely, the study of how places and regions have changed over time.
D. W. Meinig, a historical geographer of America, describes many historical regions in his book "The Shaping of America: A Geographical Perspective on 500 Years of History". For example, in identifying European "source regions" in early American colonization efforts, he defines and describes the "Northwest European Atlantic Protestant Region", which includes sub-regions such as the "Western Channel Community", which itself is made of sub-regions such as the "English West Country" of Cornwall, Devon, Somerset, and Dorset.
In describing historic regions of America, Meinig writes of "The Great Fishery" off the coast of Newfoundland and New England, an oceanic region that includes the Grand Banks. He rejects regions traditionally used in describing American history, like New France, "West Indies", the Middle Colonies, and the individual colonies themselves (Province of Maryland, for example). Instead he writes of "discrete colonization areas," which may be named after colonies, but rarely adhere strictly to political boundaries. Historic regions of this type Meinig writes about include "Greater New England" and its major sub-regions of "Plymouth," "New Haven shores" (including parts of Long Island), "Rhode Island" (or "Narragansett Bay"), "the Piscataqua," "Massachusetts Bay," "Connecticut Valley," and to a lesser degree, regions in the sphere of influence of Greater New England, "Acadia" (Nova Scotia), "Newfoundland and The Fishery/The Banks."
Other examples of historical regions include Iroquoia, Ohio Country, Illinois Country, and Rupert's Land.
Tourism region.
A tourism region is a geographical region that has been designated by a governmental organization or tourism bureau as having common cultural or environmental characteristics. These regions are often named after a geographical, former, or current administrative region or may have a name created for tourism purposes. The names often evoke certain positive qualities of the area and suggest a coherent tourism experience to visitors. Countries, states, provinces, and other administrative regions are often carved up into tourism regions to facilitate attracting visitors.
Some of the more famous tourism regions based on historical or current administrative regions include Tuscany in Italy and Yucatán in Mexico. Famous examples of regions created by a government or tourism bureau include the United Kingdom's Lake District and California's Wine Country.
great plains region
Natural resource regions.
Natural resources often occur in distinct regions. Natural resource regions can be a topic of physical geography or environmental geography, but also have a strong element of human geography and economic geography. A coal region, for example, is a physical or geomorphological region, but its development and exploitation can make it into an economic and a cultural region. Some examples of natural resource regions include the Rumaila Field, the oil field that lies along the border or Iraq and Kuwait and played a role in the Gulf War; the Coal Region of Pennsylvania, which is a historical region as well as a cultural, physical, and natural resource region; the South Wales Coalfield, which like Pennsylvania's coal region is a historical, cultural, and natural region; the Kuznetsk Basin, a similarly important coal mining region in Russia; Kryvbas, the economic and iron ore mining region of Ukraine; and the James Bay Project, a large region of Quebec where one of the largest hydroelectric systems in the world has been developed.
Religious regions.
Sometimes a region associated with a religion is given a name, like Christendom, a term with medieval and renaissance connotations of Christianity as a sort of social and political polity. The term Muslim world is sometimes used to refer to the region of the world where Islam is dominant. These broad terms are very vague when used to describe regions.
Within some religions there are clearly defined regions. The Roman Catholic Church, the Church of England, the Eastern Orthodox Church, and others, define ecclesiastical regions with names such as diocese, eparchy, ecclesiastical provinces, and parish.
For example, the United States is divided into 32 Roman Catholic ecclesiastical provinces. The Lutheran Church–Missouri Synod is organized into 33 geographic "districts", which are subdivided into "circuits" (the Atlantic District (LCMS), for example). The Church of Jesus Christ of Latter-day Saints uses regions similar to dioceses and parishes, but uses terms like ward and stake.
Political regions.
In the field of political geography regions tend to be based on political units such as sovereign states; subnational units such as provinces, counties, townships, territories, etc.; and multinational groupings, including formally defined units such as the European Union, the Association of Southeast Asian Nations, and NATO, as well as informally defined regions such as the Third World, Western Europe, and the Middle East.
Administrative regions.
The word "region" is taken from the Latin "regio" (derived from "regere", to rule), and a number of countries have borrowed the term as the formal name for a type of subnational entity (e.g., the "región", used in Chile). In English, the word is also used as the conventional translation for equivalent terms in other languages (e.g., the "область" ("oblast"), used in Russia alongside with a broader term "регион").
The following countries use the term "region" (or its cognate) as the name of a type of subnational administrative unit:
The Canadian province of Québec also uses the "administrative region" ("région administrative").
Scotland had local government regions from 1975 to 1996.
In Spain the official name of the autonomous community of Murcia is "Región de Murcia". Also, some single-province autonomous communities such as Madrid use the term "región" interchangeably with "comunidad autónoma".
Two län (counties) in Sweden are officially called 'regions': Skåne and Västra Götaland, and there is currently a controversial proposal to divide the rest of Sweden into large regions, replacing the current counties.
The government of the Philippines uses the term "region" (in Filipino, "rehiyon") when it's necessary to group provinces, the primary administrative subdivision of the country. This is also the case in Brazil, which groups its primary administrative divisions ("estados"; "states") into "grandes regiões" (greater regions) for statistical purposes, while Russia uses "экономические районы" (economic regions) in a similar way, as does Romania and Venezuela.
The government of Singapore makes use of the term "region" for its own administrative purposes.
The following countries use an administrative subdivision conventionally referred to as a region in English:
China has five 自治区 ("zìzhìqū") and two 特別行政區 (or 特别行政区; "tèbiéxíngzhèngqū"), which are translated as "autonomous region" and "special administrative region", respectively.
Local administrative regions.
There are many relatively small regions based on local government agencies such as districts, agencies, or regions. In general, they are all regions in the general sense of being bounded spatial units. Examples include electoral districts such as Washington's 6th congressional district and Tennessee's 1st congressional district; school districts such as Granite School District and Los Angeles Unified School District; economic districts such as the Reedy Creek Improvement District; metropolitan areas such as the Seattle metropolitan area, and metropolitan districts such as the Metropolitan Water Reclamation District of Greater Chicago, the Las Vegas-Clark County Library District, the Metropolitan Police Service of Greater London, as well as other local districts like the York Rural Sanitary District, the Delaware River Port Authority, the Nassau County Soil and Water Conservation District, and C-TRAN.
Traditional or informal regions.
The traditional territorial divisions of some countries are also commonly rendered in English as "regions". These informal divisions do not form the basis of the modern administrative divisions of these countries, but still define and delimit local regional identity and sense of belonging. Examples include:
Functional region.
A functional region or Nodal region, is a region that has a defined core that retains a specific characteristic that diminishes outwards. To be considered a Functional region, at least one form of spatial interaction must occur between the center and all other parts of the region. A functional region is organized around a node or focal point with the surrounding areas linked to that node by transportation systems, communication systems, or other economic association involving such activities as manufacturing and retail trading. A typical functional region is a metropolitan area (MA) as defined by the Bureau of Census. For example, the New York MA is a functional region that covers parts of several states. It is linked by commuting patterns, trade flows, television and radio broadcasts, newspapers, travel for recreation and entertainment. Other functional regions include shopping regions centered on malls, area served by branch banks, and ports.
Military regions.
In military usage, a region is shorthand for the name of a military formation larger than an Army Group and smaller than an Army Theater or simply Theater. The full name of the military formation is Army Region. The size of an Army Region can vary widely but is generally somewhere between about 1 million and 3 million soldiers. Two or more Army Regions could make up an Army Theater. An Army Region is typically commanded by a full General (US four stars), a Field Marshal, or General of the Army (US five stars), or Generalissimo (Soviet Union). Due to the large size of this formation, its use is rarely employed. Some of the very few examples of an Army Region are each of the Eastern, Western, and southern (mostly in Italy) fronts in Europe during World War II. The military map unit symbol for this echelon of formation (see Military organization and APP-6A) consists of six Xs.

</doc>
<doc id="55635" url="http://en.wikipedia.org/wiki?curid=55635" title="Region (Europe)">
Region (Europe)

The European Union created a Committee of the Regions to represent Regions of Europe as the layer of EU government administration directly below the nation-state level. The Committee has its headquarters in Brussels.
Reasons given for this include:
The term 'region' as used here includes England, Scotland, Wales and Northern Ireland which are non-sovereign countries,referred to as separate countries, even though collectively they form the country known as the United Kingdom they are recognised as countries by the UK Government and are not referred to as regions.
Some nation states which have historically had a strong centralized administration have transferred political power to the regions. Examples of this include the devolution of power in the UK (the Scotland Act 1998, the Government of Wales Act 1998) and the current negotiations in France concerning increased autonomy for Corsica. Some other states have traditionally had strong regions, such as the Federal Republic of Germany; yet others have been structured on the basis of national and municipal government with little in between.
Competence.
Regional and local authorities elect delegates to the Committee of the Regions. The Committee is a consultative body, and is asked for its opinion by the Council or the Commission on new policies and legislation in the following areas:
On certain issues it works in partnership with the Economic and Social Committee.
Political influence.
The politics of regionalism has also had an impact at the pan-European level. The regions of Europe had lobbied for an increased say in EU affairs, especially the German Länder. This resulted in the creation by the Maastricht Treaty of the Committee of the Regions, and provision for member states to be represented in the Council by ministers from their regional governments. 
The Council of Europe also has a congress of local and regional authorities, similar to the EU's Committee of the Regions.
Strengthening economic competition between communities further supports the creation of authentic regions within the EU and almost all EU member states recently have or currently are re-organizing their administration to create competitive EU regions. 
Often these regions better reflect culture and identity and a sense of common interests.
Of the major organisations representing the regions of Europe, the Assembly of European Regions (AER) is the largest. Established in 1985, the organisation now brings together over 270 regions from 33 countries, along with 16 interregional associations, across wider Europe.
Apart from playing a key role as the regions' political voice on the European stage, AER is a forum for interregional cooperation in numerous areas of regional competence, including economic development, social policy, public health, culture, education and youth. The organisation is also a key defender of the subsidiarity principle in Europe, lobbying for its inclusion in the EU treaties and demanding recognition of the word in dictionaries via the worldwide "Subsidiarity is a word" movement.
Outside EU institutions, the Council of European Municipalities and Regions (CEMR-CCRE) is the largest organisation of local and regional government in Europe; its members are national associations of towns, municipalities and regions from over 35 countries. Together these associations represent some 100,000 local and regional authorities.
CEMR works to promote a united Europe that is based on local and regional self-government and democracy. To achieve this goal it endeavours to shape the future of Europe by enhancing local and regional contribution, to influence European law and policy, to exchange experience at local and regional level and to cooperate with partners in other parts of the world.

</doc>
<doc id="55639" url="http://en.wikipedia.org/wiki?curid=55639" title="Luton">
Luton

Luton ( , ) is a large town in Bedfordshire, England, 30 mi north of London.
Luton Town F.C. have had several spells in the top flight of the English league and won the Football League Cup in 1988. They have played at Kenilworth Road since 1905.
London Luton Airport opened in 1938; in the Second World War, it was a Royal Air Force base.
The University of Bedfordshire is based in the town.
The town was for many years famous for hat-making, and was also home to a large Vauxhall Motors factory; the head office of Vauxhall Motors is still situated in the town. Car production at the plant began in 1905 and continued until 2002; commercial vehicle production remains.
History.
Early history.
The earliest settlements in the Luton area were at Round Green and Mixes Hill, where Paleolithic encampments (about 250,000 years old) have been found. Settlements re-appeared after the ice had retreated in the Mesolithic period around 8000 BC. Traces of these settlements have been found in the Leagrave area of the modern town. Remains from the Neolithic period (4500–2500 BC in this area) are much more common. A particular concentration of Neolithic burials has been found at Galley Hill. The most prominent Neolithic structure is Waulud's Bank – a henge dating from around 3000 BC. From the Neolithic onwards, the area seems to have been populated, but without any single large settlement.
The first urban settlement nearby was the small Roman town of "Durocobrivis" at Dunstable, but Roman remains in Luton itself consist only of scattered farmsteads.
The foundation of Luton is usually dated to the 6th century when a Saxon outpost was founded on the River Lea, Lea tun.
After the Danelaw was established in the east of England and the remaining English kingdoms unified in the west, Luton became host to one of many parts of the country where the border between Christendom and Heathenism was defined running up the River Lea from London through to Bedford.
Luton is mentioned in the Anglo Saxon Chronicle for the year 913 when locals fought off a Viking raiding band, "In this year the [Danish] army from Northampton and Leicester rode out after Easter [28th March] and broke the peace, and killed many men at Hook Norton [Oxfordshire] and round about there. And then very soon after that, as the one force came home, they met another raiding band which rode out against Luton. And then the people of the district became aware of it and fought against them and reduced them to full flight and rescued all that they had captured and also a great part of their horses and their weapons".
Archaeological finds for this genesis of Lutonian history includes 50 burials, 8 cremations, 16 spears, 22 knives (seax), a sword, 8 shield bosses, a pair of iron shears, a single bone comb, countless examples of brooches, pendants and other jewellery of bronze and amber and shards of pottery.
Luton is recorded in the Domesday Book as "Loitone" and also as "Lintone". Agriculture dominated the local economy at that time, and the town's population was around 700–800. But, this number could represent a recently reduced population as a direct result of the Norman Invasion and the English resistance that followed. The Domesday Book records the value of King William's English possessions 20 years after his victory at Hastings, during which period, as the book would suggest, much destruction and death took place. Besides Luton, Biscot and Caddington also have entries in the Domesday Book for the surrounding area and in both cases the value of these lands are much lower than their pre invasion state indicating a loss of households, livestock and crop.
In 1121 Robert, 1st Earl of Gloucester started work on St Mary's Church in the centre of the town. The work was completed by 1137. A motte-and-bailey castle which gives its name to the modern Castle Street was built in 1139. The castle was demolished in 1154 and the site is now home to a Matalan store. During the Middle Ages Luton is recorded as being home to six watermills. Mill Street, in the town centre, takes its name from one of them.
King John (1166–1216) had hired a mercenary soldier, Falkes de Breauté, to act on his behalf. (Breauté is a small town near Le Havre in France.) When he married, Falkes de Breauté acquired his wife's house which came to be known as "Fawkes Hall", subsequently corrupted over the years to "Foxhall", then "Vauxhall". In return for his services, King John granted Falkes the manor of Luton, where he built a castle alongside St Mary's Church. He was also granted the right to bear his own coat of arms and chose the mythical griffin as his heraldic emblem. The griffin thus became associated with both Vauxhall and Luton in the early 13th century.
By 1240 the town is recorded as Leueton. One 'Simon of Luton' was Abbot of Bury St Edmunds from 1257 to 1279. The town had a market for surrounding villages in August each year, and with the growth of the town a second fair was granted each October from 1338.
In 1336, much of Luton was destroyed by a great fire; however, the town was soon rebuilt.
The agriculture base of the town changed in the 16th century with a brick making industry developing around Luton, many of the older wooden houses were rebuilt in brick.
17th century.
During the English Civil War of the 17th century, in 1645, royalists entered the town and demanded money and goods. Parliamentary forces arrived and during the fighting four royalist soldiers were killed and a further twenty-two were captured. A second skirmish occurred three years later in 1648 when a royalist army passed through Luton. A number of royalists were attacked by parliamentary soldiers at an inn on the corner of the current Bridge Street. Most of the royalists escaped but nine were killed.
18th century.
The hat making industry began in the 17th century and became synonymous with the town. By the 18th century the industry dominated the town. Hats are still produced in the town but on a much smaller scale.
The first Luton Workhouse was constructed in the town in 1722.
Luton Hoo, a nearby large country house was built in 1767 and substantially rebuilt after a fire in 1843. It is now a luxury hotel.
19th century.
The town grew strongly in the 19th century. In 1801 the population was 3,095. By 1850 it was over 10,000 and by 1901 it was almost 39,000. Such rapid growth demanded a railway connection but the town had to wait a long time for one. The London and Birmingham Railway (L&BR) had been built through Tring in 1838, and the Great Northern Railway was built through Hitchin in 1852, both bypassing Luton, the largest town in the area. A branch line connecting with the L&BR at Leighton Buzzard was proposed, but because of objections to release of land, construction terminated at Dunstable in 1848. It was another ten years before the branch was extended to Bute Street Station, and the first train to Dunstable ran on 3 May 1858. The line was later extended to Welwyn and from 1860 direct trains to King's Cross ran. The Midland Railway was extended from Bedford to St Pancras through Leagrave and Midland Road station and opened on 9 September 1867.
Luton received a gas supply in 1834. Gas street lights were erected and the first town hall was opened in 1847.
Following a cholera epidemic in 1848 Luton formed a water company and had a complete water and sewerage system by the late 1860s. Newspaper printing arrived in the town in 1854. The first public cemetery was opened in the same year. The first covered market was built (the Plait Halls – now demolished) in 1869. Luton was made a borough in 1876. A professional football club – the first in the South of England – was founded in 1885 following a resolution at the town hall that a 'Luton Town Club be formed'.
The crest also includes a hand holding a bunch of wheat, either taken as a symbol of the straw-plaiting industry, or from the arms of John Whethamsteade, Abbott of St Albans, who rebuilt the chancel of St Mary's Church in the 15th century.
20th century.
In the 20th century, the hat trade severely declined and was replaced by other industries. In 1905, Vauxhall Motors opened the largest car plant in the United Kingdom in Luton. In 1914 Hewlett & Blondeau aviation entrepreneurs built a factory in Leagrave which began aircraft production built under licence for the war effort; the site was purchased in 1920 by new proprietors Electrolux domestic appliances, and this was followed by other light engineering businesses.
In 1901 the Bailey Water Tower was built on the edge of what was to become Luton Hoo memorial park. It is now a private residence.
In 1904 councillors Asher Hucklesby and Edwin Oakley purchased the estate at Wardown Park and donated it to the people of Luton. Hucklesby went on to become Mayor of Luton. The main house in the park became Wardown Park Museum.
The town had a tram system from 1908 until 1932, and the first cinema was opened in 1909. By 1914 the population had reached 50,000.
The original town hall was destroyed in 1919 during Peace Day celebrations at the end of World War I. Local people, including many ex-servicemen, were unhappy with unemployment and had been refused the use of a local park to hold celebratory events. They stormed the town hall, setting it alight ("see Luton Town Hall"). A replacement building was completed in 1936. Luton Airport opened in 1938, owned and operated by the council.
In World War II, the Vauxhall Factory built Churchill tanks as part of the war effort. Despite heavy camouflage, the factory made Luton a target for the Luftwaffe and the town suffered a number of air raids. 107 died and there was extensive damage to the town (over 1,500 homes were damaged or destroyed). Other industry in the town, such as SKF, which produced ball bearings, made a vital contribution to the war effort. Although a bomb landed at the SKF Factory, no major damage was caused.
The pre-war years, even at the turn of the 1930s when a Great Depression saw unemployment reach record levels nationally, were something of an economic boom for Luton, as new industries grew and prospered. New private and council housing was built in the 1920s and 1930s, with Luton growing as a town to incorporate nearby villages Leagrave, Limbury and Stopsley between 1928 and 1933.
Post-war, the slum clearance continued, and a number of substantial estates of council housing were built, notably at Farley Hill, Stopsley, Limbury, Marsh Farm and Leagrave (Hockwell Ring). The M1 motorway passed just to the west of the town, opening in 1959 and giving it a direct motorway link with London and – eventually – the Midlands and the North. In 1962 a new library (to replace the cramped Carnegie Library) was opened by the Queen in the corner of St George's Square.
In the late 1960s a large part of the town centre was cleared to build a large covered shopping centre, the Arndale Centre, which was opened in 1972. It was refurbished and given a glass roof in the 1990s.
In 2000, Vauxhall announced the end of car production in Luton; the plant closed in March 2002. At its peak it had employed in excess of 30,000 people. Vauxhall's headquarters remain in the town, as does its van and light commercial vehicle factory.
21st century.
A major regeneration programme for the town centre is underway, which will include upgrades to the town's bus and railway stations as well as improvements to the town's urban environment. St George's Square has been rebuilt and reopened in 2007. The new design won a Gold Standard Award for the Town Centre Environment from the annual British Council of Shopping Centres awards.
Work was completed on an extension to the Mall Shopping Centre facing St George's Square, the largest of the new units to was taken by TK Maxx. Planning applications for a much larger extension to the Mall Arndale Shopping Centre (In the Northern Gateway area – Bute Street, Silver Street and Guildford Street) and also for a new centre in Power Court (close to St Mary's Church) have been submitted. On the edge of Luton at Putteridge Bury a high-technology office park, Butterfield Green, is under construction. The former Vauxhall site is also to be re-developed as a mixed use site called Napier Park. It will feature housing, retail and entertainment use, including a new casino.
Governance.
The town is situated within the historic county of Bedfordshire, but since 1997 Luton has been an administratively independent unitary authority. The town remains part of Bedfordshire for ceremonial purposes.
Luton Borough Council applied for city status at the Millennium in 2000, Golden Jubilee of Elizabeth II in 2002 and Diamond Jubilee in 2012. The latest bid was rejected in March 2012.
Parliamentary representation.
Luton is represented by two Members of Parliament. The constituency of Luton North has been held by Kelvin Hopkins (Labour) since 1997. Luton South has been held by Gavin Shuker (Labour) since 2010. Luton is within the East of England.
Police and crime commissioner.
Luton is served by the Bedfordshire police. The police commissioner is Olly Martins who lives in the High Town area of the town.
Local council.
Lutonians are governed by Luton Borough Council. The town is split into 19 wards, represented by 48 councillors. Elections are held for all seats every four years, with the most recent local elections held in May 2011 and the next due in May 2015. The Council is controlled by the Labour group, who have 36 Local Councillors (a majority of 24). The next largest party is the Liberal Democrats with 8 seats, followed by the Conservative Party with 4 seats.
Luton Council coat of arms.
In 1876 the town council was granted its own coat of arms. The wheatsheaf was used on the crest to represent agriculture and the supply of straw used in the local hatting industry (the straw-plaiting industry was brought to Luton by a group of Scots under the protection of Sir John Napier of Luton Hoo). The bee is traditionally the emblem of industry and the hive represents the straw-plaiting industry for which Luton was famous. The rose is from the arms of the Napier family, whereas the thistle is a symbol for Scotland. An alternative suggestion is that the rose was a national emblem, and the thistle represents the Marquess of Bute, who formerly owned the Manor of Luton Hoo.
Geography.
Luton is located in a break in the Eastern part of the Chiltern Hills. The Chilterns are a mixture of chalk from the Cretaceous period (about 66 – 145 million years ago) and deposits laid at the southernmost points of the ice sheet during the last ice age (the Warden Hills area can be seen from much of the town).
Bedfordshire had a reputation for brick making but the industry is now significantly reduced. The brickworks at Stopsley took advantage of the clay deposits in the east of the town.
The source of the River Lea, part of the Thames Valley drainage basin, is in the Leagrave area of the town. The Great Bramingham Wood surrounds this area. It is classified as ancient woodland; records mention the wood at least 400 years ago.
There are few routes through the hilly area for some miles, this has led to several major roads (including the M1 and the A6) and a major rail-link being constructed through the town.
Climate.
Luton has a temperate marine climate, like much of the British Isles, with generally light precipitation throughout the year. The weather is very changeable from day to day and the warming influence of the Gulf Stream makes the region mild for its latitude. The average total annual rainfall is 698 mm with rain falling on 117 days of the year.
The local climate around Luton is differentiated somewhat from much of South East England due to its position in the Chiltern Hills, meaning it tends to be 1–2 degrees Celsius cooler than the surrounding towns – often flights at Luton airport, lying 160 m above sea level, will be suspended when marginal snow events occur, while airports at lower elevations, such as Heathrow, at 25 m above sea level, continue to function. An example of this is shown in the photograph to the right, the snowline being about 100 m above sea level. Absolute temperature extremes recorded at Rothamsted Research Station, 5 mi south south east of Luton town centre and at a similar elevation range from -17.0 C in December 1981 and -16.7 C in January 1963 to 36.0 C in August 2003 and 33.8 C in August 1990 and July 2006. Records for Rothamsted date back to 1901.
Areas.
The Victorian expansion of Luton focused on areas close to the existing town centre and railways. In the 1920s and 1930s growth typically was though absorbing neighbouring villages and hamlets(an example being Leagrave) and infill construction between them and Luton. After the Second World War there were several estates and developments constructed both by the local council such as Farley Hill or Marsh Farm, or privately such as Bushmead.
"More about Places within Luton "
Demography.
Racial structure, according to the 2011 census
   White
 (54.6%)   Asian
 (30.0%)   Black
 (9.8%)   Mixed
 (4.1%)  Other (1.5%)
The United Kingdom Census 2011 showed that the borough had a population of 203,201, a 10.2% increase from the previous census in 2001, when Luton was the 27th largest settlement in the United Kingdom. In 2011, 46,756 were aged under 16, 145,208 were 16 to 74, and 11,237 were 75 or over. The latest population figure for the borough is 203,600 (2011 est.).
Local inhabitants are known as "Lutonians".
Ethnicity.
Religion in Luton (2011 census)
   Christianity
 (47.4%)   Islam
 (24.6%)   Hinduism
 (3.3%)   Sikhism
 (1.1%)  Other religions (0.7%)  Not stated (7.2%)   No religion
 (16.5%)
Luton has seen several waves of immigration. In the early part of the 20th-century Irish and Scottish people arrived in the town – these were followed by Afro-Caribbean and Asian immigrants. More recently immigrants from Eastern Europe have made Luton their home. As a result of this Luton has a diverse ethnic mix, with a significant population of Asian descent, mainly Pakistani 29,353 (14.4%), Indian 10,625 (5.2%) and Bangladeshi 13,606 (6.7%).
Since the 2011 census, Luton has become one of three white British-minority towns in the United Kingdom. It was announced in a report based on the census figures that along with Leicester and Slough, Luton was one of three towns outside of London where the white British were now a minority, making up only 45% of Luton's population. However, the town still has a white majority when non-British whites such as the Irish and Eastern Europeans are included, and 81% of the population of Luton still define themselves as British, despite the majority of its residents being from a foreign ethnic background.
Religion.
In the ten-year period since the United Kingdom Census 2001, the percentage of inhabitants in Luton reporting being Christian fell from 60 to 47%. Meanwhile those reporting being Muslim increased from 15 to 25%.
Luton has been identified in the media as a home of people with extremist social and religious viewpoints. The Muslim group Al-Muhajiroun was based there before it was banned, and the founder of the English Defence League is from Luton. A Muslim protest in March 2009 against soldiers returning from the Iraq War was followed by a counter-demonstration opposing sharia law in the United Kingdom. However, many residents say that the numbers of extremists, both Muslims and far-right, are small. Inayat Bunglawala of the Muslim Council of Britain lives in Luton, and a local representative of Churches Together described "the reality of life in the town" as "a healthy interaction between people of different faiths".
Economic activity.
Of the town's working population (classified 16–74 years of age by the Office for National Statistics), 63% are employed. This figure includes students, the self-employed and those who are in part-time employment. 11% are retired, 8% look after the family or take care of the home and 5% are unemployed.
Economy.
Luton's economy has, traditionally been focused on several different areas of industry including Car Manufacture, engineering and millinery. However, today, Luton is moving towards a service based economy mainly in the retail and the airport sectors, although there is still a focus on light industry in the town.
Notable firms with headquarters in Luton include:
Notable firms with offices in Luton include:
Shopping.
The main shopping area in Luton is centred on The Mall Luton. Built in the 1960s/1970s and opened as an Arndale Centre, construction of the shopping centre led to the demolition of a number of the older buildings in the town centre including the Plait Halls (a Victorian covered market building with an iron and glass roof). Shops and businesses in the remaining streets, particularly in the roads around Cheapside and in High Town, have been in decline ever since. George Street, on the south side of the Arndale, was pedestrianised in the 1990s.
The shopping centre had some construction and re-design work done to it over the 2011/2012 period and now has a new square used for leisure events, as well as numerous new food restaurants such as Toby's Carvery and Costa Coffee.
Contained within the main shopping centre is the Market, which contains butchers, fishmongers, fruit and veg, hairdressers, tattoo parlours, ice cream, flower stall and T-shirt printing as well as eating places.
Another major shopping area is Bury Park where there are shops catering to Luton's ethnic minorities.
Food and drink.
Luton has a diverse selection of restaurants – English, Italian, Chinese, Indian, Caribbean, Thai and Malaysian to name a few. No area of the town is specifically restaurant-orientated, but in some areas (such as Bury Park) there is a concentration of Asian restaurants.
There are pubs and clubs in the town centre. A number of these cater for the town's student population; however, there are still a number of traditional pubs in the town.
Transport.
Luton is situated less than 30 miles north of the centre of London, giving it good links with the City and other parts of the country via the motorway network and the National Rail system. Luton is also home to London Luton Airport, one of the major feeder airports for London and the southeast. Luton is also served by bus services run by Arriva and Centrebus and a large taxi network. As a Unitary Authority, Luton Borough Council is responsible for the local highways and public transport in the Borough and licensing of Taxis.
Education.
Luton is one of the main locations of the University of Bedfordshire. A large campus of the university is in Luton town centre, with a smaller campus based on the edge of town in Putteridge Bury, an old Victorian manor house. The other main campus of the university is located in Bedford.
The town is home to Luton Sixth Form College and Barnfield College. Both have been awarded Learning & Skills Beacon Status by the Department for Children, Schools and Families.
Luton's schools and colleges had also been earmarked for major investment in the government scheme Building Schools for the Future programme, which intends to renew and refit buildings in institutes across the country. Luton is in the 3rd wave of this long term programme with work intending to start in 2009. Some schools were rebuilt before the programme was scrapped by the coalition government.
There are 98 educational institutes in Luton – seven nurseries, 70 primary schools (9 voluntary-aided, 2 Special Requirements), 13 secondary schools (1 voluntary-aided, 1 Special Requirements), four further educational institutes and four other educational institutes.
Culture and leisure.
Sport.
Luton is the home town of Luton Town Football Club who currently play in The Football League 2, Their nickname, "The Hatters", dates back to when Luton had a substantial millinery industry. The club began the 2008/09 season with a thirty-point deficit, and were consequently relegated from the Football League to the Conference Premier on 13 April 2009.[4] However, Luton did win the Football League Trophy that year in front of 42,000 Luton fans at Wembley, despite being the lowest placed team in the competition for the whole season, Conference Premier after failing to win automatic promotion to Football League Two during the 2009–10, 2010–11 and 2011–12 seasons. Luton were beaten 2–0 on aggregate by York City in the semi finals of the playoffs, and therefore failed to progress to the final at Wembley Stadium. The following season Luton progressed to the final of the playoffs, losing to AFC Wimbledon on penalties. In 2011–12 once again the team reached the final of the play-offs, only to lose 2–1 to York.
Luton were promoted back to the football league as champions of the Conference in 2014
Speedway racing was staged in Luton in the mid-1930s.
The town has three rugby union clubs – Stockwood Park Rugby Club who play in Midlands 3 SE, Luton Rugby Club who play in London 1 North, and Vauxhall Motors RFC who do not currently play in the RFU league structure.
Wardown Park.
Wardown Park is situated on the River Lea in Luton. The park has sporting facilities, is home to the Wardown Park Museum and contains formal gardens. The park is located between "Old Bedford Road" and the A6, "New Bedford Road" and is within walking distance of the town centre.
Stockwood Park.
Stockwood Park is a large municipal park near Junction 10 of the M1. Located in the park is Stockwood Discovery Centre a free museum that houses the Mossman Collection and Luton local social history, archaeology and geology. There is an athletics track, an 18-hole golf course, several rugby pitches and areas of open space.
The park was originally the estate and grounds to Stockwood house, which was demolished in 1964.
Carnival.
Luton International Carnival is the largest one-day carnival in Europe. It usually takes place on the late May Bank Holiday. Crowds can reach 150,000 on each occasion.
The procession starts at Wardown Park and makes its way down New Bedford Road, around the Town Centre via St George's Square, back down New Bedford Road and finishes back at Wardown Park. There are music stages and stalls around the town centre and at Wardown Park.
Luton is home to the UK Centre for Carnival Arts (UKCCA), the country's first purpose-built facility of its kind.
Due to budget cuts, the most recent carnival was run on a significantly smaller scale, with approximately one third of the typical attendance – most of the attendees were residents of the Luton area.
Luton St. Patrick's Festival.
The Festival celebrating the patron saint of Ireland and organised by Luton Irish Forum, St Patrick, is held on the weekend nearest to 17 March. In its 15th year in 2014, the Festival includes a parade, market stalls and music stands as well as Irish themed events.
Theatre.
Luton is home to the Library Theatre, a 238-seat theatre located on the 3rd floor of the town's Central Library. The Theatre's programme consists of local amateur dramatic societies, pantomime, children's theatre (on Saturday mornings) and one night shows of touring theatre companies.
Luton is also home to The Hat Factory, originally as its name suggests, this Arts Centre was in fact a real Hat Factory. The Hat Factory is a combined arts venue in the centre of Luton. It opened in 2003 and since then has been the area’s main provider of contemporary theatre, dance and music. The venue provides live music, club nights, theatre, dance, films, children's activities, workshops, classes and gallery exhibitions. www.thehatfactory.org.
Museums.
Luton Museum.
Wardown Park Museum previously known as Luton Museum and Art Gallery, is housed in a large Victorian mansion in Wardown Park on the outskirts of the town centre. The museum collection focusses on the traditional crafts and industry of Luton and Bedfordshire, notably lace-making and hat-making. There are samples of local lace from as early as the 17th century.
Stockwood Craft Museum.
Based in Stockwood Park, Luton, the collection of rural crafts and trades held at Stockwood Park Museum was amassed by Thomas Wyatt Bagshawe, who was a notable local historian and a leading authority on folk life. Bagshawe was born in Dunstable in 1901 and became a director of the family engineering firm.
The collection only contains examples from Bedfordshire and the borders of neighbouring counties, giving the collection a very strong regional identity.
Mossman Collection.
The Mossman Carriage collection is held at Stockwood Park, Luton and is the largest and most significant vehicle collection of its kind in the country, including originals from the 18th, 19th and 20th centuries.
The Mossman collection of horse-drawn vehicles was given to Luton Museum Service in 1991. It illustrates the development of horse-drawn road transportation in Britain from Roman times up until the 1930s.
Twin towns.
Luton participates in international town twinning; its partners are:
Media.
Newspapers.
Three weekly newspapers are delivered free to all the houses in Luton. They are:
There are also two other newspapers in circulation around Luton:
Media references.
In the TV series "One Foot in the Grave" there are often references to places within Luton. The script-writer David Renwick was brought up in the town.
The town was mentioned several times in the seminal sketch show "Monty Python's Flying Circus". In one sketch a rather half-hearted hijacker demands that a plane headed for Cuba be diverted to Luton. Luton is one of the constituencies returning a "Silly Party" victory in the famous sketch "Election Night Special". In the Piranha Brothers sketch Spiny Norman lived in a hangar at Luton Airport. A 1976 episode of the sci-fi series "Space: 1999" was called The Rules of Luton, inspired by the town name. The well known comedian Eric Morecambe frequently made references to Luton Town FC, due to him being a former chairman of the club, as well as living in close proximity to Luton in Harpenden.
Lutonians.
People who were born in Luton or are associated with the town.

</doc>
<doc id="55641" url="http://en.wikipedia.org/wiki?curid=55641" title="Pope Sabinian">
Pope Sabinian

Pope Sabinian (Latin: "Sabinianus", died 22 February 606) was Pope from 13 September 604 to his death in 606. Pope during the Byzantine Papacy, he was fourth former "apocrisiarius" to Constantinople elected pope.
Sabinian was born at Blera (Bieda) near Viterbo. He had been sent by Pope Gregory I as Apostolic "nuncio", to Constantinople, but he apparently was not entirely satisfactory in that office. He returned to Rome in 597.
He was probably consecrated pope on 13 September 604. He incurred unpopularity by his unseasonable economies, although the "Liber Pontificalis" states that he distributed grain during a famine at Rome under his pontificate. The erudite Italian Augustinian Onofrio Panvinio (1529–1568), in his "Epitome pontificum Romanorum" (Venice, 1557), attributes to him the introduction of the custom of ringing bells at the canonical hours and the celebration of the Eucharist. The first attribution was this was in Guillaume Durand's thirteenth-century "Rationale Divinorum Officiorum".
During his reign, Sabinian was seen as a counterfoil to his predecessor Pope Gregory I. Whereas Gregory distributed grain to the Roman populace as invasion loomed, Sabinian sold it for high prices (though this may be a later interpolation by Gregory's biographers). The "Liber Pontificalis" praises him for "filling the church with clergy," in contrast to Gregory, who rose rapidly from simple monk to bishop of Rome.

</doc>
<doc id="55649" url="http://en.wikipedia.org/wiki?curid=55649" title="1320s BC">
1320s BC


</doc>
<doc id="55654" url="http://en.wikipedia.org/wiki?curid=55654" title="Sun Quan">
Sun Quan

Sun Quan (182–252), courtesy name Zhongmou, formally known as Emperor Da of Wu (lit. "Great Emperor of Wu"), was the founder of the state of Eastern Wu during the Three Kingdoms period. He ruled from 200-222 as the "Marquis of Wu", from 222 to 229 as the "King of Wu" and from 229 to 252 as the "Emperor of Wu".
In his youth, Sun Quan spent time in his home county of Fuchun, in present-day Zhejiang, and after the death of his father Sun Jian in the early 190s, at various cities on the lower Yangtze River. His older brother Sun Ce carved out a warlord state in the region of present-day Zhejiang, based on his own followers and a number of local clan allegiances. When Sun Ce was assassinated by the retainers of Xu Gong in 200, the eighteen-year-old Sun Quan inherited the lands southeast of the Yangtze River from his brother. His administration proved to be relatively stable in those early years. Sun Jian and Sun Ce's most senior officers, such as Zhou Yu, Zhang Zhao, Zhang Hong, and Cheng Pu remained loyal; in fact it was mentioned in "Romance of the Three Kingdoms" that Sun Ce had at his deathbed reminded Sun Quan that "in internal matters, consult Zhang Zhao, in external matters, consult Zhou Yu." Thus throughout the 200s, Sun Quan, under the tutelage of his able advisors, continued to build up his strength along the Yangtze River. In early 207, his forces finally won complete victory over Huang Zu, a military leader under Liu Biao, who dominated the middle Yangtze.
In winter of that year, the northern warlord Cao Cao led an army of some 830,000 to conquer south to complete the reunification of China. Two distinct factions emerged at his court on how to handle the situation. One, led by Zhang Zhao, urged surrender whilst the other, led by Zhou Yu and Lu Su, opposed capitulation. In the finality, Sun Quan decided to oppose Cao Cao in the middle Yangtze with his superior riverine forces. Allied with Liu Bei and employing the combined strategies of Zhou Yu and Huang Gai, they defeated Cao Cao decisively at the Battle of Red Cliffs.
In 220, Cao Pi, son of Cao Cao, seized the throne and proclaimed himself to be the Emperor of China, ending and succeeding the nominal rule of the Han dynasty. At first Sun Quan nominally served as a Wei vassal with the Wei-created title of King of Wu, but after Cao Pi demanded that he send his son Sun Deng as a hostage to the Wei capital Luoyang and he refused, in 222, he declared himself independent by changing his era name. It was not until the year 229 that he formally declared himself emperor.
Because of his skill in gathering important, honourable men to his cause, Sun Quan was able to delegate authority to capable figures. This primary strength served him well in gaining the support of the common people and surrounding himself with capable generals.
After the death of his original crown prince, Sun Deng, two opposing factions supporting different potential successors slowly emerged. When Sun He succeeded Sun Deng as the new crown prince, he was supported by Lu Xun and Zhuge Ke, while his rival Sun Ba was supported by Quan Cong and Bu Zhi and their clans. Over a prolonged internal power struggle, numerous officials were executed, and Sun Quan harshly settled the conflict between the two factions by exiling Sun He and forcing Sun Ba to commit suicide. Sun Quan died in 252 at the age of 70. He enjoyed the longest reign among all the founders of the Three Kingdoms and was succeeded by his son Sun Liang.
Early life.
Sun Quan was born in 182, while his father Sun Jian was still a general of the Han dynasty. After his father's death in 191, he became the charge of his brother Sun Ce. As he grew up, he served his brother during the conquests of the region south of the Yangtze River. He was made a county magistrate in 196, at the age of 14, and continued to rise through the ranks as his brother gave him more and more important tasks.
The "Records of the Three Kingdoms" mentioned that Sun Jian was a descendant of Sun Wu (better known as Sun Tzu), a militarist in the Spring and Autumn Period and the author of "The Art of War". According to later tradition, Sun Quan was born on Sunzhou ("Sun Island", later Wangzhou - "King's Island"), an islet at the intersection of the Fuchun River and one of its tributaries. Local folklore relates a story about how Sun Quan's grandfather, Sun Zhong, was originally a melon farmer on the islet.
Succeeding Sun Ce.
Sun Ce was assassinated in 200 during a hunt. On his deathbed, he knew that his son was still too young to be considered a realistic heir, so he entrusted the 18-year-old Sun Quan to his faithful subordinates. Initially, Sun Quan mourned his brother's death so much that he could do nothing, but at Zhang Zhao's behest, he dressed himself in military uniform and set out to visit the commanderies under his brother's control. Many of Sun Ce's subordinates thought that Sun Quan was too young to sustain Sun Ce's domain and wanted to leave, but Zhang Zhao and Zhou Yu saw special qualities in the young man and chose to stay to serve Sun Quan. Zhang Hong, whom Sun Ce had earlier sent as a liaison to the warlord Cao Cao, also returned from Cao's domain to assist Sun Quan. (At Zhang Hong's request, Cao Cao, in the name of Emperor Xian, commissioned Sun Quan as General Who Attacks Barbarians (討虜將軍), a title that he would be known for a long time.) He listened carefully to his mother Lady Wu's encouraging words, and greatly trusted Zhang Zhao and Zhang Hong with regard to civilian affairs and Zhou Yu, Cheng Pu, and Lü Fan with regard to military matters. Sun Quan also sought out talented young men to serve as his personal advisors, and it was around this time that he befriended Lu Su and Zhuge Jin, who would later play prominent roles in his administration. Throughout this period and decades to come, Sun Quan's leadership would be characterized by his ability to find men of character and entrust important matters to him, and his ability to react swiftly to events.
For the next several years, Sun Quan was largely interested in first defending his realm against potential enemies, but he gradually sought to harass and weaken Liu Biao's key subordinate, Huang Zu (who controlled the northeastern region of Liu Biao's domain) -- particularly because Huang Zu had killed his father in battle. In 208, he was finally able to defeat and kill Huang Zu in battle. Soon after, Liu Biao died while Cao Cao was preparing a major campaign to subjugate both Liu Biao and Sun Quan under his control, precipitating a major confrontation.
Battle of Red Cliffs.
After Liu Biao's death, a succession struggle for his domain came into being, between his sons Liu Qi and younger son Liu Cong, whom Liu Biao's second wife Lady Cai favored (because he had married her niece). After Huang Zu's death, Liu Qi was therefore given Huang's post as the governor of Jiangxia Commandery (in present-day Huanggang, Hubei). Liu Cong therefore succeeded Liu Biao after his death, and Liu Qi was displeased and considered, but did not carry out, an attack against his brother. Nevertheless, Liu Cong, in fear of having to fight Cao Cao and his brother on two fronts, surrendered to Cao Cao against the advice of Liu Biao's key ally Liu Bei. Liu Bei, unwilling to submit to Cao Cao, fled south. Cao caught up to him and crushed his forces, but Liu Bei escaped with his life; he fled to Dangyang (當陽, in present-day Yichang, Hubei). Cao Cao took over most of Jing Province, and appeared set on finally unifying the empire.
Sun Quan was well aware of Cao Cao's intentions, and he quickly entered into an alliance with Liu Bei and Liu Qi to prepare for an attack by Cao. Cao Cao wrote Sun Quan with a letter intending to intimidate, and in face of Cao's overwhelming force (estimated to be about 220,000 men, although Cao claimed 800,000, against Sun's 30,000 and the Lius' combined force of 10,000), many of Sun's subordinates, including Zhang Zhao, advocated surrender. Sun Quan refused, under advice from Zhou Yu and Lu Su (that Cao Cao would surely not tolerate him even if he surrendered).
Sun Quan put Zhou Yu in charge of his 30,000 men, largely stationed on naval ships, and Zhou set up in a defense position in conjunction with Liu Bei, whose army was stationed on land. About this time, there was a plague developing in Cao Cao's forces which significantly weakened it. Zhou Yu set up a trap where he pretended to be punishing his subordinate Huang Gai, and Huang pretended to surrender to Cao Cao in fear. Zhou Yu then sent ships under Huang Gai's command to pretend to surrender and, as Huang's ships approached Cao Cao's fleet, they were set aflame to assault Cao's fleet, and Cao's fleet was largely destroyed by fire. Cao Cao led his forces to escape on land, but much of the force was destroyed by Sun Quan and Liu Bei's land forces.
Uneasy alliance with Liu Bei.
Immediately, after Cao Cao withdrew, Sun Quan took over the northern half of Jing Province. Liu Bei marched south and took over the southern half. The Sun-Liu alliance was further cemented by a marriage of Sun Quan's younger sister, Lady Sun, to Liu Bei. Zhou Yu was suspicious of Liu Bei's intentions, however, and suggested to Sun Quan that Liu be seized and put under house arrest (albeit be very well-treated) and his forces be merged into Sun's; Sun Quan, believing that Liu Bei's forces would rebel if he did that, declined. Sun Quan did agree to Zhou Yu's plans to consider attacking Liu Zhang and Zhang Lu (who controlled the modern southern Shaanxi) to try to take over their territories, but after Zhou Yu died in 210, the plans were abandoned. However, Sun Quan was able to persuade the warlords in present-day Guangdong, Guangxi, and northern Vietnam to submit to him, and they became part of his domain. He then yielded northern Jing Province to Liu Bei as well, agreeing with Liu that the south was insufficient to supply his troops.
After Liu Bei's conquest of Yi Province, he was able to supply his troops on his own, so Sun Quan sent Lu Su as an emissary to demand for the return of Jing Province, but Liu Bei refused. Sun Quan then sent Lü Meng and Ling Tong to lead 20,000 men to attack southern Jing Province and they succeeded in capturing Changsha, Guiyang, and Lingling commanderies. Meantime, Lu Su and Gan Ning advanced to Yiyang (益陽) with 10,000 men (to block Guan Yu) and took over command of the army at Lukou (陸口). Liu Bei personally went to Gong'an and Guan Yu led 30,000 men to Yiyang. When an all-out war was about to break out, the news that Cao Cao planned to attack Hanzhong was received by Liu Bei, and he requested for a border treaty with Sun Quan as he became worried about Cao Cao seizing Hanzhong. Liu Bei asked Sun Quan to give him back Lingling commandery and create a diversion for Cao Cao by attacking Hefei; in return, Liu Bei ceded Changsha and Guiyang commanderies to Sun Quan, setting the new border along the Xiang River. Sun Quan's attack on Hefei was disastrous - he was nearly captured on a few occasions, if not saved by Ling Tong.
Breaking of alliance with Liu Bei.
In 219, Guan Yu advanced north, attacking Fancheng, scoring a major victory over Cao Ren. While Fancheng did not fall at this time, Guan Yu put it under siege, and the situation was severe enough that Cao Cao considered moving the capital away from Xu. However, Sun Quan, resentful of Guan Yu's prior constant instigation of hostilities (including seizing Sun's food supplies to use for his campaign north), took the opportunity to attack Guan from the rear, and Guan's forces collapsed. Guan Yu was captured by forces under general Lü Meng; Guan Yu was executed, Jing Province came under Sun's control, and the Sun-Liu alliance ended. Sun Quan nominally submitted to Cao Cao and urged him to take the throne but Cao refused.
After Cao Cao's death in 220, Cao Pi forced Emperor Xian to yield the throne to him, ending the Han dynasty and establishing the state of Cao Wei. Sun Quan did not immediately submit to Wei or declare independence after Cao Pi's enthronement, but took a wait-and-see attitude; by contrast, in early 221, Liu Bei declared himself emperor, establishing the state of Shu Han. Immediately, Liu Bei planned a campaign against Sun Quan to avenge Guan Yu. After attempting to negotiate peace and receiving no positive response from Liu Bei, fearing attack on both sides, Sun Quan became a vassal of Wei. Cao Pi's strategist Liu Ye suggested that Cao Pi decline — and in fact attack Sun Quan on a second front, effectively partitioning Sun's domain with Shu, and then eventually seek to destroy Shu as well. Cao Pi declined, in a fateful choice that most historians believe doomed his empire to ruling only the northern and central China — and this chance would not come again. Indeed, against Liu Ye's advice, he appointed Sun Quan the King of Wu and granted him the nine bestowments.
In 222, at the Battle of Xiaoting, Sun Quan's general Lu Xun dealt Liu Bei a major defeat, stopping the Shu offensive. Shu would not again pose a threat to Sun Quan from that point on. Later that year, when Cao Pi demanded that Sun Quan send his crown prince Sun Deng to the Wei capital Luoyang as a hostage (to guarantee his loyalty), Sun Quan refused and declared independence (by changing era name), thus establishing Eastern Wu as an independent state. Cao Pi launched a major attack on Wu, but after Wei defeats in early 223, it became clear that Wu was secure. After Liu Bei's death later that year, Zhuge Jin's brother Zhuge Liang, the regent for Liu Bei's son and successor Liu Shan, reestablished the alliance with Sun Quan, and the two states would remain allies until Shu's eventual destruction in 263.
Reign as the monarch of Eastern Wu.
Early reign.
Early in Sun Quan's reign, the Wu administration was known for its efficiency, as Sun showed a knack for listening to correct advice and for delegating authorities to the proper individuals. For example, he correctly trusted the faithful Lu Xun and Zhuge Jin, so much so that he made a duplicate imperial seal and left it with Lu Xun; whenever he would correspond with Shu's emperor Liu Shan or regent Zhuge Liang, he would deliver the letter to Lu Xun first (as Lu's post was near the Shu border), and then if, in Lu's opinion, changes were needed, he would revise the letter and then restamp it with Sun's imperial seal. Further, Lu Xun and Zhuge Jin were authorized to coordinate their actions with Shu without prior imperial approval. Sun Quan treated his high level officials as friends and addressed them accordingly (with courtesy names), and in accordance they dedicated all effort to Wu's preservation. He also knew what were the proper roles for officials that he trusted; for example, in 225, when selecting a chancellor, while the key officials all respected Zhang Zhao greatly and wanted him to be chancellor, Sun Quan declined, reasoning that while he respected Zhang greatly, a chancellor needed to handle all affairs of state, and Zhang, while capable, had such strong opinions that he would surely be in conflict with Sun Quan and other officials at all times. He also repeatedly promoted his official Lü Fan even though, while he was young, Lü Fan had informed to Sun Ce about his improper spending habits, understanding that Lü did so only out of loyalty to Sun Ce.
In 224 and 225, Cao Pi again made attacks on Wu, but each time the Wu forces were able to repel Wei's with fair ease — so easily that Cao Pi made the comment, "Heaven created the Yangtze to divide the north and south." However, Sun Quan was himself equally unsuccessful in efforts to make major attacks on Wei. After Cao Pi's death in 226, for example, Sun Quan launched an attack on Wei's Jiangxia Commandery (in present-day Xiaogan, Hubei) but was forced to withdraw as soon as Wei reinforcements arrived. However, later that year, he was able to increase his effective control over Jiao Province (交州, present-day northern Vietnam) when his general Lü Dai was able to defeat the warlord Shi Hui (士徽) and end the effective independence that the Shi clan had. In addition, the several independent kingdoms in modern Cambodia, Laos, and southern Vietnam all became Wu vassals as well.
The one major victory that Wu would have over Wei during this period came in 228, when, with Sun Quan's approval, his general Zhou Fang pretended to be surrendering to Wei after pretending to have been punished repeatedly by Sun Quan. This tricked the Wei general Cao Xiu, who led a large army south to support Zhou Fang. He walked into the trap set by Zhou Fang and Lu Xun and suffered major losses, but was saved from total annihilation by Jia Kui.
In 229, Sun Quan declared himself emperor, which almost damaged the alliance with Shu, as many Shu officials saw this as a sign of betrayal of the Han dynasty — to which Shu claimed to be the legitimate successor. However, Zhuge Liang opposed ending the alliance and in fact confirmed it with a formal treaty later that year, in which the two states pledged to support each other and divide Wei equally if they could conquer it. Later that year, he moved his capital from Wuchang (武昌, in present-day Ezhou, Hubei) to Jianye, leaving his crown prince Sun Deng, assisted by Lu Xun, in charge of the western empire.
Middle reign.
In 230, however, the first sign of the deterioration of Sun Quan's reign occurred. That year, he sent his generals Wei Wen (衛溫) and Zhuge Zhi (諸葛直) with a navy of 10,000 into the East China Sea to seek the legendary islands of Yizhou (夷洲) and Danzhou (亶洲) to seek to conquer them, despite strenuous opposition of Lu Xun and Quan Cong. The navy was not able to locate Danzhou but located Yizhou, and returned in 231 after capturing several thousand men — but only after 80-90% of the navy had died from illness. Instead of seeing his own fault in this venture, Sun Quan simply executed Wei Wen and Zhuge Zhi. Perhaps concerned about this deterioration in Sun Quan's judgment, Sun Deng left the western empire in Lu Xun's hands in 232 and returned to Jianye, and would remain at Jianye until his own death in 241.
In 232, Sun Quan had another misadventure involving his navy — as he sent his generals Zhou He (周賀) and Pei Qian (裴濳) to the nominal Wei vassal Gongsun Yuan, in control of Liaodong Commandery (present-day central Liaoning), to purchase horses, against the advice of Yu Fan - and indeed, he exiled Yu Fan to the desolate Cangwu Commandery (roughly modern Wuzhou, Guangxi) as punishment. Just as Yu Fan predicted, however, the venture would end in failure — as Zhou He and Pei Qian, on their way back, were intercepted by Wei forces and killed. Regretting his actions, Sun Quan tried to recall Yu Fan back to Jianye, only to learn that Yu had died in exile.
The next year, however, Sun Quan would have yet another misadventure in his dealings with Gongsun Yuan, as Gongsun sent messengers to him, offering to be his subject. Sun Quan was ecstatic, and appointed Gongsun Yuan the Prince of Yan and granted him the nine bestowments, and further sent a detachment of 10,000 men by sea north to assist Gongsun Yuan in his campaign against Wei, against the advice of nearly every single one of his high level officials, particularly Zhang Zhao. Once the army arrived, however, Gongsun Yuan betrayed them, killing Sun Quan's officials Zhang Mi (張彌) and Xu Yan (許晏), whom Sun had sent to grant the bestowments and seized their troops. Once that happened, the enraged Sun Quan wanted to personally head north with a fleet to attack Gongsun Yuan, and initially, not even Lu Xun's opposition was able to stop him, although he eventually calmed down and did not follow through. To his credit, he also personally went to Zhang Zhao's house and apologized to him. Further, despite the deterioration in his previous clear thinking, he was still capable of making proper decisions at times. For example, in 235, when, as a sign of contempt, Wei's emperor Cao Rui offered horses to him in exchange for pearls, jade, and tortoise shells, Sun Quan ignored the implicit insult and made the exchange, reasoning that his empire needed horses much more than pearls, jade, or tortoise shells.
In 234, in coordination with Zhuge Liang's final northern expedition against Wei, Sun Quan personally led a major attack against Wei's border city Hefei, while having Lu Xun and Zhuge Jin attack Xiangyang, with the strategy of trying to attract Wei relief forces and then attacking them. However, Wei generals correctly saw the situation and simply let Sun Quan siege Hefei. Only after Sun Quan's food supplies ran low did Cao Rui personally arrive with reinforcements, and Sun withdrew, as did Lu Xun and Zhuge Jin.
In 238, when Gongsun Yuan was under attack by Wei's general Sima Yi, Sun Quan, despite his prior rage against Gongsun, correctly judged the situation as one where he might be able to take advantage if Sima Yi were initially unsuccessful, so he did not immediately refuse Gongsun's request for help. However, as Sima Yi was able to conquer Gongsun Yuan quickly, Sun Quan never launched the major attack that he considered if Sima got stuck in a stalemate with Gongsun. That year, he also recognized how his head secretary Lü Yi (呂壹) had been falsely accusing his officials, and had Lü executed; he then further confirmed his trust in the high level officials by personally writing an emotional letter to Zhuge Jin, Bu Zhi, Zhu Ran, and Lü Dai, blaming himself for the recent problems with his administration while urging them to speak out honestly whenever they saw faults in him.
In 241, Sun Quan would launch the last major assault against Wei of his reign, in light of Cao Rui's death in 239, but he rejected a strategy offered by Yin Zha (殷札) to attack Wei in coordinated effort with Shu on four different fronts, and the campaign ended in failure as well.
Late reign.
Later in 241, the crown prince Sun Deng died — an event that left open the issue of succession and appeared to mark the start of a precipitous decline in Sun Quan's mental health. In 242, he appointed his son Sun He, born to Consort Wang, crown prince. However, he also favored another son by Consort Wang, Sun Ba (孫霸) the Prince of Lu, and permitted Sun Ba to have the same staffing level as the crown prince — a move that was objected to by a number of officials as encouraging Sun Ba to compete with Sun He, but Sun Quan did not listen to them. After 245, when Sun He and Sun Ba began to have separate residences, their relationship detriorated further, and Sun Ba began to scheme at how to seize heir status from Sun He. Fanned by gossip from his daughter Sun Dahu (孫大虎), Sun Quan blamed the princes' mother Consort Wang for this — and she died in fear. He also cut off Sun He and Sun Ba's access to the officials who supported them in hopes of receiving future favors, but this could not stop Sun Ba's machinations. Indeed, when Lu Xun tried to intervene to protect Sun He, Sun Ba falsely accused him of many crimes, and Sun Quan became provoked so much that he repeatedly rebuked Lu, causing Lu to die in anger.
In 250, fed up with Sun Ba's constant attacks against Sun He, Sun Quan carried out an inexplicable combination of actions, He forced Sun Ba to commit suicide, while deposing Sun He (who had not been shown to have committed any crimes), and instead creating his youngest son, Sun Liang, crown prince to replace Sun He. This move was opposed by his son-in-law Zhu Ju (the husband of Sun Xiaohu), but Zhu's pleas not only did not help Sun He, but also resulted in his own death, as Sun Quan forced him to commit suicide. Many other officials who also opposed the move, as well as officials who had supported Sun Ba, were executed.
Around this time, Sun Quan also had his generals destroy a number of levees near the border with Wei, creating large areas of flooding, in order to obstruct potential attacks from Wei.
In 251, Sun Quan created the first empress of his reign — Sun Liang's mother Consort Pan. (Previously, he had a succession of wives, but never made any of them empress, except for his favorite, Lady Bu, who was created empress posthumously after her death in 238.) Later that year, however, he realized that Sun He was blameless and wanted to recall him from his exile, but was persuaded not to do so by his daughter Sun Dahu and Sun Jun, who had supported Sun Liang's ascension. He realized that he was getting very old (69 by this point) and, at Sun Jun's recommendation, commissioned Zhuge Jin's son Zhuge Ke as the future regent for Sun Liang, even though he correctly had misgivings about how Zhuge Ke was arrogant and had overly high opinion of his own abilities. At that time virtually the entire empire, awed by Zhuge's prior military victories, was convinced that Zhuge would be the correct choice for regent.
In 252, as Sun Quan neared death, Empress Pan was murdered, but how she was murdered remains a controversy. Wu officials claimed that her servants, unable to stand her temper, strangled her while she was asleep, while a number of historians, including Hu Sanxing, the commentator to Sima Guang's "Zizhi Tongjian", believed that top Wu officials were complicit, as they feared that she would seize power as empress dowager after Sun Quan's death. Later that year, Sun Quan died at the age of 70, and Sun Liang succeeded him. Sun Quan was buried in a mausoleum at Purple Mountain in present-day Nanjing.
Modern references.
Sun Quan appears as a playable character in Koei's "Dynasty Warriors" and "Warriors Orochi" video game series.
Sun Quan is portrayed by Chang Chen in John Woo's 2008 film "Red Cliff".
In the collectible card game "" there is a card named "Sun Quan, Lord of Wu", in the "Portal Three Kingdoms" set.
In the selection of hero cards in the Chinese card game "San Guo Sha" (三国杀), there is also a Sun Quan hero that players can select at the beginning of the game.
In the movie "The Weird Man" by the Shaw Brothers Studio, Sun Quan is shown at the end of the film and Sun Ce names him successor before he died from his injuries sustained by Xu Gong and Yu Ji's spirit.

</doc>
<doc id="55655" url="http://en.wikipedia.org/wiki?curid=55655" title="Tenure of Office Act (1867)">
Tenure of Office Act (1867)

The Tenure of Office Act was a United States federal law (in force from 1867 to 1887) that was intended to restrict the power of the President of the United States to remove certain office-holders without the approval of the Senate. The law was enacted on March 3, 1867, over the veto of President Andrew Johnson. It purported to deny the president the power to remove any executive officer who had been appointed by the president, with the advice and consent of the Senate, unless the Senate approved the removal during the next full session of Congress. Congress repealed the act in its entirety in 1887.
Background.
In the post-Civil War political environment, President Andrew Johnson endorsed the quick re-admission of the Southern secessionist states. The two-thirds Republican majorities of both houses of Congress, however, passed laws over Johnson's vetoes, establishing a series of five military districts overseeing newly created state governments. This "Congressional Reconstruction" was designed to create local civil rights laws to protect newly freed slaves; to protect and patrol the area; to ensure the secessionist states would show some good faith before being readmitted; to ensure Republican control of the states; and, arguably, to inflict some punishment on the secessionists. States would be readmitted gradually.
Overpowered politically, the sole check Johnson could apply to the Congressional Reconstruction plan was through his control, as commander-in-chief of the military, which would be the primary means by which to enforce the plan's provisions. However, even Johnson's control of the military was inhibited by the fact that his Secretary of War, Edwin Stanton, was a staunch Radical Republican who supported Congressional Reconstruction in full. This further set Johnson against the Republican-controlled Congress, with Johnson wanting to remove Stanton from office and Congress wanting to keep him in place.
Stanton and impeachment.
The Tenure of Office Act restricted the President to suspend an officer while the Senate was not in session—at that time, Congress sat during a relatively small portion of the year. If, when the Senate reconvened, it declined to ratify the removal, the President would be required to reinstate the official.
In August 1867, with the Senate out of session, Johnson made his move against Stanton, suspending him pending the next session of the Senate. However, when the Senate convened on January 4, 1868, it refused to ratify the removal by a vote of 35-16. Notwithstanding the vote, President Johnson attempted to appoint a new Secretary of War. Proceedings began within days, leading to Johnson's impeachment, the first impeachment of a United States President. After a three-month trial, Johnson avoided removal from office by the Senate by a single vote. Stanton resigned in May 1868.
It was actually unclear whether Johnson had violated the Tenure of Office Act. The act's phrasing was murky, and it was not clear whether his removal of Stanton (a holdover from the Lincoln administration whom Johnson had not appointed) violated the Act. While the Act, by its terms, applied to current office holders, it also limited the protection offered to Cabinet members to one month after a new president took office.
Constitutionality.
In 1926, a similar law (though not dealing with Cabinet secretaries) was ruled unconstitutional by the United States Supreme Court in the case of "Myers v. United States", which affirmed the ability of the President to remove a Postmaster without Congressional approval. In reaching that decision, the Supreme Court stated in its majority opinion (though in dicta), "that the Tenure of Office Act of 1867, insofar as it attempted to prevent the President from removing executive officers who had been appointed by him by and with the advice and consent of the Senate, was invalid".

</doc>
<doc id="55657" url="http://en.wikipedia.org/wiki?curid=55657" title="Amnesty Act">
Amnesty Act

The Amnesty Act of May 22, 1872 was a United States federal law that removed voting restrictions and office-holding disqualification against most of the secessionists who rebelled in the American Civil War, except for some 500 military leaders of the Confederacy. The act was passed by the 42nd United States Congress and the original restrictive Act was passed by the United States Congress on May 1866.
The 1872 Act affected over 150,000 former Confederate troops who had taken part in the American Civil War.

</doc>
<doc id="55658" url="http://en.wikipedia.org/wiki?curid=55658" title="Specie Payment Resumption Act">
Specie Payment Resumption Act

The Specie Payment Resumption Act of January 14, 1875, was a law in the United States which restored the nation to the gold standard through the redemption of previously unbacked United States Notes and reversed inflationary government policies promoted directly after the American Civil War. The decision further contracted the nation's money supply and was seen by critics as an exacerbating factor of the so-called "Long Depression" which struck in 1873.
History.
Late in 1861, seeking to raise revenue for the American Civil War effort without exhausting its reserves of gold and silver, the United States federal government suspended specie payments, or the payments made in gold and silver in redemption of currency notes. Early in 1862, the United States issued legal-tender notes, called greenbacks. By war's end, a total of $431 million in greenbacks had been issued, and authorization had been given for another $50 million in small denominations, known as fractional currency or "shin plasters." The issuance of greenbacks caused inflation during the period.
Immediately after the Civil War during Reconstruction, there were large capital inflows into the United States and a general improvement in the export-to-import ratio since the export-dominant South was reintegrated with the North. The United States Treasury, however, had increased its cash balance through the summer of 1873 by selling gold for $14 million. National banks also increased issuance of national bank notes by $44 million. The failure of several railroad companies including Jay Cooke & Company on their bond obligations encouraged capital outflows from the United States to Europe and weakened demand for dollars leading to the Panic of 1873. Increased Treasury cash balances, continued issuance of national bank notes, and capital outflows together depreciated the currency. These factors further caused a reduction in reserves held by monetary institutions because higher prices increased domestic demand for currency. Reserves held by banks were insufficient to be able to meet seasonal demands in autumn of 1873 as greenback reserves declined from $34 million in September 1873 to $5 million in October 1873. Tensions surrounding the Panic of 1873 between creditors and debtors revived the specie payment resumption debate.
Two views dominated this debate. Conservatives and the creditor class favored “hard money,” that is they favored resumption as a method for making up losses incurred due to dollar depreciation during the past decade. The resumption of specie payments was perceived as a method to curb the rise in the price level and eventually equate currency with gold. For creditors that had issued debts in inflated greenbacks, resumption would increase the real interest rate that they received. Supporters of the Resumption Act argued that the Panic of 1873 might not have occurred had there been sufficient reserves of gold in the United States Treasury as would have been in case in specie payments were resumed.
Opposed to resumption, a new coalition of agrarian and labor interests found common cause during Reconstruction in advocating for “soft money” or the promotion of inflationary monetary policies. These groups viewed the Panic of 1873 as the result of insufficient currency that should have been used to fuel the growth in production that occurred in the South and the West. These regions relied on cheap money – that is low interest rates – to be able to continue to grow. Other soft money advocates included gold speculators and the railroad industry. Collis P. Huntington and other railroad leaders called for further greenback issuance in light of harsh business conditions that made honoring debt obligations difficult. Opponents of the Resumption Act also argued that many debt obligations were made in an environment in which there existed a premium on gold, that is that inflation in the currency in the previous decade made gold relatively more valuable than currency. Resumption therefore entailed up to a 50% increase in debt obligation if gold and currency equalized. For manufacturers, the rising price of gold made domestic prices cheaper relative to import prices since many European currencies including the English sterling were fixed to the price of gold. Hence continued inflationary measures such as further issuance of greenbacks artificially supported domestic industries. Hard and soft money interests often did cross party lines, although a larger portion of Democrats were hard money advocates.
Following a Democratic congressional victory in the elections of 1874, a lame-duck Republican congress passed The Resumption Act of January 14, 1875. It required the Secretary of the Treasury to redeem greenbacks in specie on demand on or after 1 January 1879. The Act, however, did not provide for a specific mechanism for redemption. The Act, though, did allow the Secretary of Treasury to acquire gold reserves either via any federal surpluses or the issuance of government bonds. An established gold reserve allowed for daily variations in specie flows and facilitated resumption. The act abolished the seigniorage fee on coining gold and substituted silver for any still existing fractional currency. The Resumption Act set no limit on the quantity of national bank notes that could be issued; this idea became known as “free banking.” This provision led many conservatives to believe that the Act was inflationary in nature. However, the Resumption Act also required that greenbacks be retired in a proportion of 80% of new national bank note issue, which in theory aimed to contract the money supply and hence encourage dollar appreciation such that gold and currency might equate. However in practice the effect was mild: the total quantity of greenbacks in circulation fell from $382 million at the end of 1874 to $300 million following the passage of the Resumption Act.
The Resumption Act was hotly debated during the 1880 presidential election, with most western politicians opposed to it. Specie payments finally resumed during the presidency of Rutherford B. Hayes. Aided by the return of prosperity in 1877, Secretary of the Treasury John Sherman accumulated a gold reserve to be redeemed for existing greenbacks mostly from transactions with Europe. Sherman earmarked a redemption fund by January 1, 1879 that amounted to $133 million acquired from the sale of bonds to Europe and Treasury surplus. However, when people found greenbacks to be on par with gold, they lost their desire for redemption.
Reaction and criticism.
Reactions as to the effects of the Resumption Act are mixed. Contemporaries did not consider it an outright victory for hard money. The legislation stood as a compromise engineered by Senators John Sherman and George Edmunds between hard and soft money advocates. Milton Friedman and Anna J. Schwartz argue that the Resumption Act had mixed effects on actual resumption of specie payments. The primary economic product of the Act was that it instilled confidence in the business community on the maintenance of specie payments. The Act served as a signal to businesses of an approaching exchange rate between gold and currency. Preparations among businesses for this exchange rate actually encourage parity between gold and currency.
The Act did not directly address the price level although successful resumption at par value required that the premium on gold to fall to zero, which in turn required a fall in the price level as the world price of gold was exogenous. In fact, the final date for Resumption was decided only after the premium on gold had fallen to a tenth of its peak level. The decline in the premium cannot entirely be attributed to the Resumption Act, as downward pressure on the overall price level also resulted from increased production in the South especially during 1877. The first four months of that year sold as much beef to England as had been sold all of the preceding year. The act has also been criticized for both failing to remove all greenbacks from circulation and failing to dictate what might be done with the greenbacks remaining in circulation.

</doc>
<doc id="55659" url="http://en.wikipedia.org/wiki?curid=55659" title="Bland–Allison Act">
Bland–Allison Act

The Bland–Allison Act, also referred to as the Grand Bland Plan of 1878, was an act of United States Congress requiring the U.S. Treasury to buy a certain amount of silver and put it into circulation as silver dollars. Though the bill was vetoed by President Rutherford B. Hayes, the Congress overrode Hayes' veto on February 23, 1878 to enact the law.
Background.
The five-year depression following the Panic of 1873 caused cheap-money advocates (led by Representative Richard P. Bland, a Democrat of Missouri), to join with silver-producing interests in urging a return to bimetallism, the use of both silver and gold as a standard.Coupled with Senator William B. Allison of Iowa, they agreed to a proposal that allowed silver to be purchased at market rates, metals to be minted into silver dollars, and required the US Treasury to purchase between $2 million to $4 million silver each month from western mines. President Rutherford B. Hayes, who held interests in industrials and banking, vetoed the measure, which was overturned by Congress. As a result, the Hayes administration purchased the limited amount of silver each month. This act helped restore bimetallism with gold and silver both supporting the currency. However, gold remained heavily favored over silver, paving way for the gold standard.
Free Silver Movement.
The free-silver movement of the late 19th century advocated the unlimited coinage of silver, which would have resulted in inflationary monetary policy. In 1873, Congress had removed the usage of silver dollar from the list of authorized coins under the Coinage Act of 1873 (referred to by opponents as 'the Crime of '73'"). Although the Bland-Allison Act of 1878 directed the Treasury to purchase silver from the "best-western" miners, President Grover Cleveland repealed the act in 1893. Advocates of free silver included owners of silver mines in the West, farmers who believed an inclusion of silver would increase crop prices, and debtors who believed would alleviate their debts. Although the free silver movement ended, the debate of inflation and monetary policy continues to this day.
Coinage Act of 1873.
The Fourth Coinage Act acknowledged the gold standard over silver. Those who advocated for silver labeled this act as the "Crime of '73". As a result of demonetized silver, gold became the only metallic standard in the United States and became the default standard. The price of gold was more stable than that of silver, largely due to silver discoveries in Nevada and other places in the West, and the price of silver to gold declined from 16-to-1 in 1873 to nearly 30-to-1 by 1893. The term limping bimetallism describes this problem. The U.S. government finally ceded to pressure from the western mining states and the Bland-Allison Act went into effect in 1878, which was replaced by the Sherman Silver Purchase Act of 1890. The law was replaced in 1890 by the similar Sherman Silver Purchase Act, which in turn was repealed by Congress in 1893. These were two instances were the United States attempted to establish bimetallic standards in the long run.
Reactions and economic impact.
Western miners and debtors regarded the Bland-Allison Act as an insufficient measure to enforce unlimited coinage of silver, but opponents repealed the act and advocated for the gold standard. The effect of the Bland-Allison act was also blunted by the minimal purchase of silver required by the Hayes administration. Although the act was a near turning point for bimetallism, gold continued to be favored over the bimetallism standard.
Throughout 1860 to 1871, several attempts were made by the Treasury to establish the bimetallic standard by having gold and silver franc. However, the discovery silver led to an influx of supply, lowering the price of silver. The eventual removal of the bimetallic standard, including the Bland-Allison Act and the acceptance of the gold standard formed the monetary stability in the late 19th century.
The limitation placed on the supply of new notes and the Treasury control over the issue of new notes allowed for economic stability. Prior to the acceptance, the devaluation of silver forced local governments into a financial turmoil. In addition, there was a need for money supply to increase as the credit system expanded and large banks established themselves across states.

</doc>
<doc id="55661" url="http://en.wikipedia.org/wiki?curid=55661" title="List of Danish monarchs">
List of Danish monarchs

This is a list of Danish monarchs, that is, the Kings and Queens regnant of Denmark. This includes:
The house of Oldenburg held the Danish Crown between 1448 and 1863, when it passed to the house of Schleswig-Holstein-Sonderburg-Glücksburg, a cadet branch of the same house, descended from King Christian III of Denmark. The kingdom had been elective (although the eldest son or brother of the previous king was usually elected) until 1660, when it became hereditary and absolutist. Until 1864 Denmark was also united in a personal union with the duchies of Schleswig and Holstein.

</doc>
<doc id="55662" url="http://en.wikipedia.org/wiki?curid=55662" title="Hans Janmaat">
Hans Janmaat

Johannes Gerardus Hendrikus "Hans" Janmaat (November 3, 1934 – June 9, 2002) was a Dutch politician of the Centre Party (CP) and later his own formed Centre Democrats (CD). He was Parliamentary leader of the Centre Party in the House of Representatives from September 16, 1982 until October 15, 1984 when he was expelled from the party. He later served as Parliamentary leader of the Centre Democrats in the House of Representatives from September 8, 1989 until May 19, 1998 when his party lost all its seats. 
Although he was widely known, he was never a major force in the Dutch political landscape, partly because of a cordon sanitaire imposed by the Third Lubbers cabinet.
Biography.
Early life.
Johannes Gerardus Hendrikus Janmaat was born on November 3, 1934 in Nes aan de Amstel in North Holland, as the oldest of nine children in a traditional Roman Catholic family. His father was a salesman and insurance broker. When Janmaat was 4 years old, the family moved to Gouda, where it would endure the war years in relative peace.
After graduating in 1954, Janmaat started a study in aeronautical engineering, but had to drop out two years later after his father could no longer afford the tuition fees. Having to give up his studies to work was the first of many setbacks he would often refer to later in life.
In 1966 he married Belgian Evi Hock, having met her while working in Germany. In 1979, they divorced. In 1996, Janmaat married Wil Schuurman. No children were born in either marriage.
In the early 1960s, Janmaat ran a furniture factory with two of his brothers, but it burned down in 1966. He used the insurance payout to study politicology at the University of Amsterdam. Fellow-students found him ambitious, provocative and witty. In 1969, he participated in the occupation of the "Maagdenhuis", the university's administrative center, as part of a student protest. He completed his studies in 1972.
Politics.
After graduating, Janmaat held part-time positions as a teacher of civics. He also ran a one-man consultancy for small businesses.
In 1972, he joined the Catholic People's Party (KVP). He also worked in several commissions for the Democratic Socialists '70 (DS'70) party. Despite his efforts, he was not considered suitable for a front line position because of his capriciousness and tendency to go against the grain.
In the 1970s, he became more interested in the emerging issue of immigration as large numbers of foreign workers came to the Netherlands. His increasingly radical stance lead to a break with the KVP as well as DS'70.
In 1980, he read an article in Vrij Nederland which drew his attention to the recently founded extreme-right Centre Party (CP). After several interviews, he joined the party as its seventh member. Starting as a publicity worker, he would rapidly rise to be the party's top and was its "lijsttrekker" (top candidate) for the 1982 elections. The party won a single seat in the House of Representatives, which went to Janmaat. Other political parties largely ignored and ostracized him.
Centre Democrats.
After disagreements and a power struggle with other members of the Centrum Party, he was expelled from the CP in October 1984. However, he retained his seat in parliament, in accordance with Dutch law. Janmaat officially launched his own party, the Centre Democrats (CD) in November 1984. Politically, the party did not differ greatly from the CP, except that it was strongly centered around Janmaat, to prevent another power struggle. 
Several attempts were made to reconcile the differences between CP and CD. One such meeting in a hotel in Kedichem was disrupted by left-wing activists, who set fire to the building. Janmaat narrowly escaped with his life, CD secretary (and later wife of Janmaat) Wil Schuurman lost a leg because of injuries sustained jumping out of a window to escape the fire.
In the 1986 election, Janmaat lost his seat in parliament, however he regained his single seat in 1989.
His biggest political success would be in the 1994 elections, when he gained three seats. Major political parties changed their response to Janmaat and his views: rather than actively ignoring him they also started openly addressing the issue of immigration. In the 1998 election the CD lost all three of its seats. Janmaat had become increasingly paranoid and said that computers used for voting had been tampered with.
In 1999, Janmaat was in the process of starting another party, the "Conservative Democrats", however it did not get off the ground and did not participate in the 2002 election. 
His failing health forced him to withdraw from politics, however the changing political climate did prompt him to challenge his conviction for discrimination at the European Court of Justice. Janmaat's death in 2002 halted the case.
Political views.
Janmaat wanted to represent the indigenous Dutch workers and middle class. His views were based mostly on economic and materialistic arguments rather than an underlying ideology. Disappointing economic growth, unemployment and government cutbacks could not be addressed while large numbers of immigrants were flowing into the country. Janmaat was against a multicultural society: he argued that immigrants should either assimilate into Dutch culture, or return to their country of birth.
His best known slogans were "Holland is not a country of immigration," "full=full" and "we will abolish the multicultural society, as soon as we get the chance and power"; he was convicted for the last two statements. According to Jan van de Beek, Hans Janmaat often used economic arguments in his tirades against immigrants.
He was often accused of committing acts of hate speech, and received fines and a conditional prison sentence for incitement to hatred and discrimination against foreigners.
He often made controversial remarks about immigrants and other politicians. He argued that Ernst Hirsch Ballin should not be allowed to hold a high office because of his Jewish heritage and said he was not saddened by the sudden death of political opponent Ien Dales.
Legacy.
Other parties erected a cordon sanitaire around Janmaat, ignoring him while he spoke in parliament. A taboo on discussing negative aspects of immigration existed in the Dutch political climate in the 1980s.
Meindert Fennema, Emeritus Professor of Political Theory of Ethnic Relations at the University of Amsterdam, argued in 2006 that Janmaat was convicted for statements that are now commonplace due to changes in the political climate (caused in part by the September 11 attacks, and the assassinations of Pim Fortuyn and Theo van Gogh).

</doc>
<doc id="55663" url="http://en.wikipedia.org/wiki?curid=55663" title="Pendleton Civil Service Reform Act">
Pendleton Civil Service Reform Act

Definition.
The Pendleton Civil Service Reform Act (ch. 27, 22 Stat. ) of United States is a federal law established in 1883 that decided that government jobs should be awarded on the basis of merit instead of political affiliation. The act provided selection of government employees by competitive exams, rather than ties to politicians or political affiliation. It also made it illegal to fire or demote government officials for political reasons and prohibited soliciting campaign donations on Federal government property. To enforce the merit system and the judicial system, the law also created the United States Civil Service Commission. This board would be in charge of determining the rules and regulations of the act.The Act also allowed for the president, by executive order to decide which positions could be subject to the act and which would not. A crucial result was the shift of the parties to reliance on funding from business, since they could no longer depend on patronage hopefuls.
History.
In the year of 1877 there was growing interest in the United States concerning the effects of the spoil system on the American political system. New York Established the Civil Service Reform Association to help address the issues, this establishment lead to several other organizations like it showing up in other cities. The presence of these organizations was one of the first steps in trying to up end the spoils system in America. What moved the Civil Service Reform from city organizations to a leading topic in the political realm was the assassination of President James Garfield
in 1881. President Garfield was assassinated by Charles Guiteau, because Guiteau believed the president owed him a patronage position. After the assassination of President Garfield, Vice President Chester Aurthur acceded to presidency. As the new president Aurthur pushed through the Pendleton Civil Service Reform Act. On January 16, 1883 Congress passed the Civil Service Act, which is sometimes referred to as the Pendleton Act after U.S. Senator George Hunt Pendleton. , Senator George H. Pendleton was one of the primary sponsors of the Act. The Act was written by Dorman Bridgeman Eaton, a staunch opponent of the patronage system who was later first chairman of the United States Civil Service Commission. However, the law would also prove to be a major political liability for Arthur. The law offended machine politicians, or politicians who belong to a small clique that controls a political party. These politicians realized that with the Pendleton Act in place they would have to find a new means of income, since they could no longer count on donations from the wealthy hoping to receive jobs.
The law was only applied to federal government jobs and not to the state and local jobs that were the basis for political machines. At first the Pendleton Act only covered a very minimum quantity of jobs, only about 10% of the US government's civilian employees had civil service jobs. However, there was a provision that allowed outgoing presidents to lock in their own appointees by converting jobs to civil service. After a series of party reversals at the presidential level (1884, 1888, 1892, 1896), the result was that most federal jobs were under civil service.
Further Readings.
</dl>
</dl>

</doc>
<doc id="55664" url="http://en.wikipedia.org/wiki?curid=55664" title="Millicent Fawcett">
Millicent Fawcett

Dame Millicent Garrett Fawcett, GBE (11 June 1847 – 5 August 1929) was an English feminist, intellectual, political leader, union leader, mother, wife and writer. However, she is primarily known for her work as a suffragist (a campaigner for women to have the vote).
She was born Millicent Garrett in Aldeburgh, Suffolk. As a suffragist (as opposed to a suffragette), she took a moderate line, but was a tireless campaigner. She concentrated much of her energy on the struggle to improve women's opportunities for higher education and in 1871 co-founded Newnham College, Cambridge. She later became president of the National Union of Women's Suffrage Societies (the NUWSS), a position she held from 1907 until 1919. In July 1901 she was appointed to lead the British Government's commission to South Africa to investigate conditions in the concentration camps that had been created there in the wake of the Second Boer War. Her report corroborated what the campaigner Emily Hobhouse had said about conditions in the camps.
Early life.
Millicent Garrett was born on 11 June 1847 in Aldeburgh to Newson Garrett, a warehouse owner, and his wife Louise Dunnell. Her parents Newson and Louisa Garrett were of a highly privileged background, he was a wealthy merchant and ship owner. Newson and Louise had six daughters and four sons, including Millicent and Elizabeth, later famous as the first woman in the United Kingdom to qualify as a doctor. Newson's business quickly became a success, and all of his children were educated at a private boarding school in Blackheath, London run by Louisa Browning, the aunt of Robert Browning. While at home the Garrett’s encouraged an interest in political issues of the day, freedom of thought and expression of opinion. Several Garrett daughters achieved eminence. Agnes became one of the first female interior decorators in Britain and Millicent’s younger sisters followed her struggle against a male dominated society. Millicent’s older sister Louisa died young at age thirty-two in 1867.
Millicent was sent there in 1858, and left in 1863 with "a sharpened interest in literature and the arts and a passion for self-education". Her sister Louise took her to the sermons of Frederick Maurice, who was a more socially aware and less traditional Anglican and whose opinion influenced Millicent's view of religion. When she was twelve her sister Elizabeth moved to London to qualify as a doctor, and Millicent regularly visited her there.
Married life.
These visits were the start of Millicent's interest in women's rights. In 1865 Elizabeth took her to see a speech by John Stuart Mill on the subject; Millicent was impressed by this speech, and became an active supporter of his work. In 1866, at the age of 19, she became secretary of the London Society for Women's Suffrage. Mill introduced her to many other women's rights activists, including Henry Fawcett, a liberal Member of Parliament who had originally intended to marry Elizabeth before she decided to focus on her medical career. Millicent and the politician became close friends, and despite a fourteen-year age gap they married in 1867. Millicent took his last name, becoming Millicent Garrett Fawcett. The MP had been blinded in a shooting accident in 1858, and Millicent acted as his secretary. The marriage was described as one based on "perfect intellectual sympathy", and Millicent pursued a writing career of her own while caring for him. Their only child, Philippa Fawcett, was born in 1868. She was close to Phillipa as they shared skill in needlework, Phillipa also excelled in school, which fared well with her mother and with women’s rights. Fawcett ran two households, one in Cambridge and one in London. “The Fawcetts were a radical couple, flirting even with republicanism, supporters of proportional representation and trade unionism, keen advocates of individualistic and free trade principles and the advancement of women”. Henry and Millicent’s close relationship was never doubted; they had a real, and loving, marriage.
In 1868 Millicent joined the London Suffrage Committee, and in 1869 she spoke at the first public pro-suffrage meeting to be held in London. In March 1870 she spoke in Brighton, her husband's constituency, and as a speaker was known for her clear speaking voice. In 1870 she published "Political Economy for Beginners", which although short was "wildly successful", and ran through 10 editions in 41 years. In 1872 she and her husband published "Essays and Lectures on Social and Political Subjects", which contained eight essays by Millicent. In 1875 she was a co-founder of Newnham Hall, and served on its Council.
Widow.
The death of her husband on 6 November 1884 made Millicent temporarily withdraw from public life. She sold both family homes and moved with Philippa into the house of Agnes Garrett, her sister. She resumed work in 1885. Millicent began to concentrate on politics. Originally an active Liberal, she joined the Liberal Unionist party in 1886 in opposition to Irish Home Rule. In 1904 she resigned from the party on the issue of Free Trade when Joseph Chamberlain gained control in his campaign for Tariff Reform.
After the death of Lydia Becker, she became the leader of the National Union of Women's Suffrage Societies (NUWSS), the main suffragist organisation in Britain. She held this post until 1919, a year after the first women had been granted the vote. After that, she left the suffrage campaign for the most part, and devoted much of her time to writing books, including a biography of Josephine Butler.
She was granted an honorary LLD by the University of St Andrews in 1899, awarded a damehood (GBE) in 1925, and died four years later, in 1929. Her memory is preserved now in the name of the Fawcett Society, and in "Millicent Fawcett Hall", constructed in 1929 in Westminster as a place that women could use to debate and discuss the issues that affected them. The hall is currently owned by Westminster School and is the location of its drama department, incorporating a 150-seat studio theatre.
Millicent Fawcett died in London in 1929 and was cremated at Golders Green Crematorium.
Political activities.
Millicent began her career in the political platform at twenty-two years old at the first women’s suffrage meeting. Millicent Fawcett (leader of NUWSS) was a moderate campaigner, distancing herself from the militant and violent activities of the Pankhursts and the Women's Social and Political Union (WSPU). She believed that their actions were in fact harming women's chances of gaining the vote, as they were alienating the MPs who were debating whether or not to give women the vote, as well as souring much of the general public towards the campaign. Despite the publicity given to the WSPU, the NUWSS (one of whose slogans was "Law-Abiding suffragists" ) retained the majority of the support of the women's movement. By 1905, Fawcett’s NUWSS had reached 305 constituent societies and nearly fifty thousand members. In 1913 they had 50,000 members compared to 2,000 of the WSPU. Fawcett mainly fought for women’s right to vote, and found home rule to be “a blow to the greatness and prosperity of England as well as disaster and… misery and pain and shame”. Fawcett cut her liberal ties in 1884, her belief in women’s suffrage was unchanged however her political views did change and began to resemble the views she had when she was younger. In 1883, Fawcett received the role of president of the Special Appeal Committee.
The South African War created an opportunity for Millicent to share female responsibilities in British culture. Millicent was nominated to be the leader of the commission of women who were sent to South Africa. In July 1901, she sailed to South Africa with other women “to investigate Emily Hobhouse’s indictment of atrocious conditions in concentration camps where the families of the Boer soldiers were interned”. In Britain a woman had never been trusted with such a responsibility during wartime. Millicent fought for the civil rights of the Uitlanders, “as the cause of revival of interest in women’s suffrage”.
Over many years, Millicent had backed countless campaigns; which were not all successful. A few campaigns Millicent supported were, “to curb child abuse by raising the age of consent, criminalizing incest, cruelty to children within the family, to end the practice of excluding women from courtrooms when sexual offences were under consideration, to stamp out the ‘white slave trade’, and to prevent child marriage and the introduction of regulated prostitution in India”. Fawcett also campaigned for the repeal of the Contagious Diseases Acts, which reflected sexual double standards. The Acts required that prostitutes be examined for sexually transmitted diseases, and if they were found to have passed any on to their customers, they were imprisoned. Poor women could be arrested on suspicion of being a prostitute, and could also be imprisoned for refusing consent to the examination, which was invasive and could be painful. The prostitutes' infectious male customers were not subject to the Acts. The Acts were eventually repealed as a result of Fawcett's and others' campaigning. Millicent believed the double standard of morality would never become eradicated until women were represented in the public sphere of life.
Fawcett was also an author. She usually penned under her own name as Millicent Garrett Fawcett, however as a public figure she was styled Mrs. Henry Fawcett. Fawcett had three books, a co-authorized book with her husband Henry Fawcett, and many of her articles were published respectively. Fawcett published a textbook Political Economy for Beginners that had ten editions, sparked two novels and was produced in many languages. One of Fawcett’s first articles on women’s education was published in Macmillan’s Magazine in 1875. In 1875, Fawcett’s interest in women’s education lead her to become one of the founders of the Newnham College for Women, located in Cambridge. Fawcett served on the school’s council, she was also supportive when there was a controversial bid to all women to receive Cambridge degrees. Millicent was also a speaker and lecturer at girl’s schools and women’s colleges, she also spoke in adult education centers. For her services in education the University of St. Andrews awarded her an honorary LLD in 1899.
When the First World War broke out in 1914, while the WSPU ceased all of their activities to focus on the war effort, Fawcett's NUWSS did not. This was largely because as the organisation was significantly less militant than the WSPU, it contained many more pacifists, and general support for the war within the organisation was weaker. The WSPU, in comparison, was called jingoistic as a result of its leaders' strong support for the war. While Fawcett was not a pacifist, she risked dividing the organisation if she ordered a halt to the campaign, and the diverting of NUWSS funds from the government, as the WSPU had done. The NUWSS continued to campaign for the vote during the war, and used the situation to their advantage by pointing out the contribution women had made to the war effort in their campaigns.
Fawcett is considered instrumental in gaining the vote for six million British women over 30-years-old in 1918.
“A memorial inscription added to the monument to Henry Fawcett in Westminster Abbey in 1932 asserts that she ‘won citizenship for women’”.
The archives of Millicent Garrett Fawcett are held at The Women's Library at the , ref .
Archives.
The archives of Millicent Fawcett are held at The Women's Library at the ref 

</doc>
<doc id="55667" url="http://en.wikipedia.org/wiki?curid=55667" title="Spoils system">
Spoils system

In the politics of the United States, a spoils system (also known as a patronage system) is a practice in which a political party, after winning an election, gives government jobs to its supporters, friends and relatives as a reward for working toward victory, and as an incentive to keep working for the party—as opposed to a merit system, where offices are awarded on the basis of some measure of merit, independent of political activity.
The term was derived from the phrase "to the victor belong the spoils" by New York Senator William L. Marcy, referring to the victory of the Jackson Democrats in the election of 1828, with the term spoils meaning goods or benefits taken from the loser in a competition, election or military victory.
Similar spoils systems are common in other nations that traditionally have been based on tribal organization or other kinship groups and localism in general.
Origins.
Before March 8, 1829, moderation had prevailed in the transfer of political power from one presidency to another. President Andrew Jackson's inauguration signaled a sharp departure from past presidencies. An unruly mob of office seekers made something of a shambles of the March inauguration, and though some tried to explain this as democratic enthusiasm, the real truth was Jackson supporters had been lavished with promises of positions in return for political support. These promises were honored by an astonishing number of removals after Jackson assumed power. At the beginning of Jackson's administration, fully 919 officials were removed from government positions, amounting to nearly 10 percent of all government postings.:328–33
The Jackson administration attempted to explain this unprecedented purge as reform, or constructive turnover, aimed at creating a more efficient system where the chain of command of public employees all obeyed the higher entities of government. The hardest changed organization within the federal government proved to be the post office. The post office was the largest department in the federal government, and had even more personnel than the war department. In one year 423 postmasters were deprived of their positions, most with extensive records of good service. :334
Corruption.
Less obvious than the incompetence and/or indolence of many of its political appointees was the spoil system's propensity for also corrupting or installing already corrupt public officials. An early and glaring example of the perfidy that was associated with the spoils system is the matter of Clerk of the U.S. House of Representatives (1843-1845) Caleb J. McNulty's alleged embezzlement of U.S. House funds, or what then former U.S. President and sitting Whig Party U.S. Representative John Quincy Adams called a "… memorable development of Democratic defalcation."
Reform.
By the late 1860s, citizens began demanding civil service reform. Running under the Liberal Republican Party in 1872, they were soundly defeated by Ulysses S. Grant.
After the assassination of James A. Garfield by a rejected office-seeker in 1881, the calls for civil service reform intensified. Moderation of the spoils system at the federal level came with the passage of the Pendleton Act in 1883, which created a bipartisan Civil Service Commission to evaluate job candidates on a nonpartisan merit basis. While few jobs were covered under the law initially, the law allowed the President to transfer jobs and their current holders into the system, thus giving the holder a permanent job. The Pendleton Act's reach was expanded as the two main political parties alternated control of the White House every election between 1884 and 1896. After each election the outgoing President applied the Pendleton Act to jobs held by his political supporters. By 1900, most federal jobs were handled through civil service and the spoils system was limited only to very senior positions.
The separation between the political activity and the civil service was made stronger with the Hatch Act of 1939 which prohibited federal employees from engaging in many political activities.
The spoils system survived much longer in many states, counties and municipalities, such as the Tammany Hall ring, which survived well into the 1930s when New York City reformed its own civil service. Illinois modernized its bureaucracy in 1917 under Frank Lowden, but Chicago held on to patronage in city government until the city agreed to end the practice in the Shakman Decrees of 1972 and 1983. 
Modern variations on the spoils system are often described as the political machine.

</doc>
<doc id="55668" url="http://en.wikipedia.org/wiki?curid=55668" title="Chinese Exclusion Act">
Chinese Exclusion Act

The Chinese Exclusion Act was a United States federal law signed by President Chester A. Arthur on May 6, 1882. It was one of the most significant restrictions on free immigration in US history, prohibiting all immigration of Chinese laborers. The act followed revisions made in 1880 to the US-China Burlingame Treaty of 1868, revisions that allowed the US to suspend Chinese immigration. The act was initially intended to last for 10 years, but was renewed in 1892 and made permanent in 1902. The Chinese Exclusion Act was the first law implemented to prevent a specific ethnic group from immigrating to the United States. It was finally repealed by the Magnuson Act on December 17, 1943.
Background.
The first significant Chinese immigration to North America began with the California Gold Rush of 1848-1855 and continued with subsequent large labor projects, such as the building of the First Transcontinental Railroad. During the early stages of the gold rush, when surface gold was plentiful, the Chinese were tolerated, if not well received. As gold became harder to find and competition increased, animosity toward the Chinese and other foreigners increased. After being forcibly driven from the mines, most Chinese settled in enclaves in cities, mainly San Francisco, and took up low end wage labor such as restaurant and laundry work. With the post-Civil War economy in decline by the 1870s, anti-Chinese animosity became politicized by labor leader Denis Kearney and his Workingman's Party as well as by California Governor John Bigler, both of whom blamed Chinese "coolies" for depressed wage levels. Another significant anti-Chinese group organized in California during this same era was the Supreme Order of Caucasians, with some 60 chapters statewide.
In the early 1850s, there was resistance to the idea of excluding Chinese migrant workers from immigration, because they provided essential tax revenue which helped fill the fiscal gap of California. But toward the end of the decade, the financial situation improved and subsequently, attempts to legislate Chinese exclusion became successful on the state level. In 1858, the California Legislature passed a law that made it illegal for any person "of the Chinese or Mongolian races" to enter the state; however, this law was struck down by an unpublished opinion of the State Supreme Court in 1862.
The Chinese immigrant workers provided cheap labor and did not use any of the government infrastructure (schools, hospitals, etc.) because the Chinese migrant population was predominantly made up of healthy male adults. As time passed and more and more Chinese migrants arrived in California, violence would often break out in cities such as Los Angeles. By 1878 Congress decided to act and passed legislation excluding the Chinese, but this was vetoed by President Rutherford B. Hayes. In 1879, California adopted a new Constitution, which explicitly authorized the state government to determine which individuals were allowed to reside in the state, and banned the Chinese from employment by corporations and state, county or municipal governments. Once the Chinese Exclusion Act was finally passed in 1882, California went further by passing various laws that were later held to be unconstitutional. After the act was passed, most Chinese families were faced with a dilemma: stay in the United States alone or go back to China to reunite with their families. Although there was widespread dislike for the Chinese, some capitalists and entrepreneurs resisted their exclusion because they accepted lower wages.
The Act.
For the first time, Federal law proscribed entry of an ethnic working group on the premise that it endangered the good order of certain localities. (The earlier Page Act of 1875 had prohibited immigration of Asian forced laborers and prostitutes, and the Naturalization Act of 1790 prohibited naturalization of non-white subjects.) The Act excluded Chinese "skilled and unskilled laborers and Chinese employed in mining" from entering the country for ten years under penalty of imprisonment and deportation.
The Chinese Exclusion Act required the few nonlaborers who sought entry to obtain certification from the Chinese government that they were qualified to immigrate. However, this group found it increasingly difficult to prove that they were not laborers because the 1882 act defined excludables as “skilled and unskilled laborers and Chinese employed in mining.” Thus very few Chinese could enter the country under the 1882 law.
The Act also affected Asians who had already settled in the United States. Any Chinese who left the United States had to obtain certifications for reentry, and the Act made Chinese immigrants permanent aliens by excluding them from U.S. citizenship. After the Act's passage, Chinese men in the U.S. had little chance of ever reuniting with their wives, or of starting families in their new homes.
Amendments made in 1884 tightened the provisions that allowed previous immigrants to leave and return, and clarified that the law applied to ethnic Chinese regardless of their country of origin. The Scott Act (1888) expanded upon the Chinese Exclusion Act, prohibiting reentry after leaving the U.S. Constitutionality of the Chinese Exclusion Act and the Scott Act was upheld by the Supreme Court in" Chae Chan Ping v. United States" (1889); the Supreme Court declared that "the power of exclusion of foreigners [is] an incident of sovereignty belonging to the government of the United States as a part of those sovereign powers delegated by the constitution." The Act was renewed for ten years by the 1892 Geary Act, and again with no terminal date in 1902. When the act was extended in 1902, it required "each Chinese resident to register and obtain a certificate of residence. Without a certificate, he or she faced deportation."
Between 1882 and 1905, about 10,000 Chinese appealed against negative immigration decisions to federal court, usually via a petition for habeas corpus. In most of these cases, the courts ruled in favor of the petitioner. Except in cases of bias or negligence, these petitions were barred by an act that passed Congress in 1894 and was upheld by the U.S. Supreme Court in "U.S." vs "Lem Moon Sing" (1895). In "U.S." vs "Ju Toy" (1905), the U.S. Supreme Court reaffirmed that the port inspectors and the Secretary of Commerce had final authority on who could be admitted. Ju Toy's petition was thus barred despite the fact that the district court found that he was an American citizen. The Supreme Court determined that refusing entry at a port does not require due process and is legally equivalent to refusing entry at a land crossing. All these developments, along with the extension of the act in 1902, triggered a boycott of U.S. goods in China between 1904 and 1906 that by some estimates cut U.S. exports to China by more than half.
One of the critics of the Chinese Exclusion Act was the anti-slavery/anti-imperialist Republican Senator George Frisbie Hoar of Massachusetts who described the Act as "nothing less than the legalization of racial discrimination."
The laws were driven largely by racial concerns; immigration of persons of other races was unlimited during this period.
On the other hand, many people strongly supported the Chinese Exclusion Act, including the Knights of Labor, a labor union, who supported it because it believed that industrialists were using Chinese workers as a wedge to keep wages low. Among labor and leftist organizations, the Industrial Workers of the World were the sole exception to this pattern. The IWW openly opposed the Chinese Exclusion Act from its inception in 1905.
For all practical purposes, the Exclusion Act, along with the restrictions that followed it, froze the Chinese community in place in 1882. Limited immigration from China continued until the repeal of the Chinese Exclusion Act in 1943. From 1910 to 1940, the Angel Island Immigration Station on what is now Angel Island State Park in San Francisco Bay served as the processing center for most of the 56,113 Chinese immigrants who are recorded as immigrating or returning from China; upwards of 30% more who showed up were returned to China. Furthermore, after the 1906 San Francisco earthquake, which destroyed City Hall and the Hall of Records, many immigrants (known as "paper sons") claimed that they had familial ties to resident Chinese-American citizens. Whether these were true or not cannot be proven.
The Chinese Exclusion Act gave rise to the first great wave of commercial human smuggling, an activity that later spread to include other national and ethnic groups.
Later, the Immigration Act of 1924 restricted immigration even further, excluding all classes of Chinese immigrants and extending restrictions to other Asian immigrant groups. Until these restrictions were relaxed in the middle of the twentieth century, Chinese immigrants were forced to live a life apart, and to build a society in which they could survive on their own (Chinatown).
The Chinese Exclusion Act did not address the problems that whites were facing; in fact, the Chinese were quickly and eagerly replaced by the Japanese, who assumed the role of the Chinese in society. Unlike the Chinese, some Japanese were even able to climb the rungs of society by setting up businesses or becoming truck farmers. However, the Japanese were later targeted in the National Origins Act of 1924, which banned immigration from east Asia entirely.
In 1891 the Government of China refused to accept the U.S. Senator Mr. Henry W. Blair as U.S. Minister to China due to his abusive remarks regarding China during negotiation of the Chinese Exclusion Act.
Repeal and current status.
The Chinese Exclusion Act was repealed by the 1943 Magnuson Act, during a time when China had become an ally of the U.S. against Japan in World War II. The Magnuson Act permitted Chinese nationals already residing in the country to become naturalized citizens and stop hiding from the threat of deportation. It also allowed a national quota of 105 Chinese immigrants per year. Large scale Chinese immigration did not occur until the passage of the Immigration and Nationality Act of 1965. Despite the fact that the exclusion act was repealed in 1943, the law in California prohibiting Chinese people from marrying whites was not repealed until 1948. Other states had such laws until 1967, when the United States Supreme Court unanimously ruled in "Loving v. Virginia" that anti-miscegenation laws are unconstitutional.
Even today, although all its constituent sections have long been repealed, Chapter 7 of Title 8 of the United States Code is headed "Exclusion of Chinese." It is the only chapter of the 15 chapters in Title 8 (Aliens and Nationality) that is completely focused on a specific nationality or ethnic group.
On June 18, 2012, the United States House of Representatives passed a resolution introduced by Congresswoman Judy Chu, that formally expresses the regret of the House of Representatives for the Chinese Exclusion Act, which imposed almost total restrictions on Chinese immigration and naturalization and denied Chinese-Americans basic freedoms because of their ethnicity. The resolution had been approved by the U.S. Senate in October 2011.
In 2014, the California Legislature took formal action to pass measures that formally recognize the many proud accomplishments of Chinese-Americans in California and to call upon Congress to formally apologize for the 1882 adoption of the Chinese Exclusion Act. Senate Republican Leader Bob Huff (R-Diamond Bar) and incoming Senate President pro-Tem Kevin de León (D-Los Angeles) served as Joint Authors for Senate Joint Resolution (SJR) 23 and Senate Concurrent Resolution (SCR) 122.
SJR 23 acknowledges and celebrates the rich history and contributions of Chinese Americans in California. The resolution also formally calls on Congress to apologize for laws which resulted in the persecution of Chinese Americans, such as the Chinese Exclusion Act.

</doc>
<doc id="55670" url="http://en.wikipedia.org/wiki?curid=55670" title="Warsaw Convention">
Warsaw Convention

The Convention for the Unification of certain rules relating to international carriage by air, commonly known as the Warsaw Convention, is an international convention which regulates liability for international carriage of persons, luggage, or goods performed by aircraft for reward.
Originally signed in 1929 in Warsaw (hence the name), it was amended in 1955 at The Hague, Netherlands, and in 1971 in Guatemala City, Guatemala. United States courts have held that, at least for some purposes, the Warsaw Convention is a different instrument from the Warsaw Convention as amended by the Hague Protocol.
History.
On 17 August 1923, the French government proposed the convening of a diplomatic conference in November 1923 for the purpose of concluding a convention relating to liability in international carriage by air. The conference was formally deferred on two occasions due to reluctant behavior of the governments of various nations to act on such a short notice without the knowledge of the proposed convention. Finally, between 27 October and 6 November, the first conference met in Paris to study the draft convention. Since most of the participants were diplomats accredited to the French government and not professionals, it was agreed unanimously that a body of technical, legal experts be set up to study the draft convention prior to its submission to the diplomatic conference for approval. Accordingly in 1925, the "Committee International Technique of Experts Juridique Aeriens" (CITEJA) was formed. In 1927–28 CITEJA studied and developed the proposed draft convention and developed it into the present package of unification of law and presented it at the Warsaw Conference which was approved between 4 and 12 October 1929. It unified an important sector of private air law.
The Convention was written originally in French and the original documents were deposited in the archives of the Ministry for Foreign Affairs of Poland. After coming into force on 13 February 1933, it resolved some conflicts of law and jurisdiction.
Between 1948–51 it was further studied by a legal committee set up by the International Civil Aviation Organization (ICAO) and in 1952 a new draft was prepared to replace the convention. However it was rejected and it was decided that the convention be amended rather than replaced in 1953. The work done by the legal committee at the Ninth Session was presented to the International Conference on Air Law which was convened by the Council of the ICAO and met at The Hague from 6 to 28 September 1955. The Hague Conference adopted a Protocol (the Hague Protocol) for the amendment of the Warsaw Convention. Between the parties of the Protocol, it was agreed that the 1929 Warsaw Convention and the 1955 Hague Protocol were to be read and interpreted together as one single instrument to be known as the Warsaw Convention as amended at the Hague in 1955. This was not an amendment to the convention but rather a creation of a new and separate legal instrument that is only binding between the parties. If one nation is a party to the Warsaw Convention and another to the Hague Protocol, neither state has an instrument in common and therefore there is no mutual international ground for litigation.
The Montreal Convention, signed in 1999, replaced the Warsaw Convention system.
Content.
There are five chapters:
In the convention there is a provision of successive carriage and a combined carriage partly by air and partly by other modes of transport as well.
In particular, the Warsaw Convention:
The sums limiting liability were originally given in gold francs (defined in terms of a particular quantity of gold by article 22 paragraph 5 of the convention). These sums were amended by the Montreal Additional Protocol No. 2 to substitute an expression given in terms of SDR's. These sums are valid in the absence of a differing agreement (on a higher sum) with the carrier. Agreements on "lower" sums are null and void.
A court may also award a claiming party's costs, unless the carrier made an offer within 6 months of the loss (or at least 6 months before the beginning of any legal proceedings) which the claiming party has failed to beat.
The Warsaw Convention provides that a plaintiff can file a lawsuit at his or her discretion in one of the following forums:
According to Clauses 17 and 18 of the Warsaw Convention, airline companies are liable for any damage that occurs to passengers or their belongings during in-flight. However, airline companies will not be held responsible if the damage results from the passenger's own fault or one of their temporary servants such as doctors assisting ill passengers on their own initiative (Clause 20). To be covered by air carriers, doctors should respond to the captain's call when it comes to assisting ill passengers. In such cases, doctors are considered an airline's temporary servants who acted on the airline's instructions. Major airlines are all covered by insurance to meet such contingencies and to cover doctors who act as their temporary agents.
Ratifications.
As of 2015, the Warsaw Convention had been ratified by 152 states. The Protocol to the Convention had been ratified by 137 states.

</doc>
<doc id="55671" url="http://en.wikipedia.org/wiki?curid=55671" title="Connotation">
Connotation

 
A connotation is a commonly understood cultural or emotional association that some word or phrase carries, in addition to the word's or phrase's explicit or literal meaning, which is its denotation.
A connotation is frequently described as either positive or negative, with regards to its pleasing or displeasing emotional connection. For example, a stubborn person may be described as being either "strong-willed" or "pig-headed"; although these have the same literal meaning ("stubborn"), "strong-willed" connotes admiration for the level of someone's will (a positive connotation), while "pig-headed" connotes frustration in dealing with someone (a negative connotation).
Usage.
"Connotation" branches into a mixture of different meanings. These could include the contrast of a word or phrase with its primary, literal meaning (known as a denotation), with what that word or phrase specifically denotes. The connotation essentially relates to how anything may be associated with a word or phrase, for example, an implied value judgment or feelings.
It is often useful to avoid words with strong connotations (especially pejorative or disparaging ones) when striving to achieve a neutral point of view. A desire for more positive connotations, or fewer negative ones, is one of the main reasons for using euphemisms. 
Logic.
In logic and semantics, "connotation" is roughly synonymous with "intension". Connotation is often contrasted with "denotation", which is more or less synonymous with "extension". Alternatively, the connotation of the word may be thought of as the set of all its possible referents (as opposed to merely the actual ones). A word's "denotation" is the collection of things it refers to; its connotation is what it implies about the things it is used to refer to. The denotation of dog is (something like) four-legged canine carnivore. So saying, "You are a dog" would imply that you were ugly or aggressive rather than stating that you were canine.

</doc>
<doc id="55678" url="http://en.wikipedia.org/wiki?curid=55678" title="Interstate Commerce Commission">
Interstate Commerce Commission

 Surface Transportation Board
 United States of America
The Interstate Commerce Commission (ICC) was a regulatory agency in the United States created by the Interstate Commerce Act of 1887. The agency's original purpose was to regulate railroads (and later trucking) to ensure fair rates, to eliminate rate discrimination, and to regulate other aspects of common carriers, including interstate bus lines and telephone companies. Congress expanded ICC authority to regulate other modes of commerce beginning in 1906. The agency was abolished in 1995, and its remaining functions were transferred to the Surface Transportation Board.
The Commission's five members were appointed by the President of the United States with the consent of the United States Senate; the commission was authorized to investigate violations of the Act and order the cessation of wrongdoing. However, in its early years, ICC orders required an order by a federal court to become effective. The Commission was the first independent regulatory body (or so-called "Fourth Branch"), as well as the first agency to regulate big business in the U.S.
Creation.
The ICC was established by the Interstate Commerce Act of 1887, which was signed into law by President Grover Cleveland. The creation of the commission was the result of widespread and longstanding antirailroad agitation. Western farmers, specifically those of the Grange Movement, were the dominant force behind the unrest, but Westerners generally — especially those in rural areas — believed that the railroads possessed economic power that they systematically abused. A central issue was rate discrimination between similarly situated customers and communities.:42ff
The Act applied to all railroads engaged in interstate commerce, even if they were located entirely within a single state, and it also applied to water carriers — riverboats, barges, ferries — owned or controlled by railroads. The Act states that rates charged by the railroads had to be "just and reasonable," but it did not set standards for reasonableness. Railroads were forbidden to give preference, advantage, special rates, or rebates to any person, company, location, city or type of traffic. There had been alleged attempts by railroads to obtain influence over city and state governments and a widespread practice of granting free transportation in the form of yearly passes to opinion leaders (elected officials, newspaper editors, ministers, et al.). Railroads were not allowed to charge more for a short haul than a long haul under the same circumstances when the short haul was a segment of a longer haul (e.g., a New York-Baltimore trip should not cost more than a New York-Washington, D.C. trip). The act prohibited pooling, which, in a railroad's case, was the sharing of revenue or freight. Railroads were required to publish rates and give advance notice of change.
Initial implementation and legal challenges.
The ICC had a troubled start because the Act failed to give it adequate enforcement powers.
Following passage of the Act, the ICC proceeded to set maximum shipping rates for railroads. However, several railroads challenged the agency's rate-making authority in 1897, with the Supreme Court ruling that the ICC had no power to fix rates. This ultimately nullified the clause stating that the short haul should cost no more than the long haul.:90ff 
Expansion of ICC authority.
The situation of the ICC began to change after the turn of the century as Congress expanded the ICC's powers through subsequent legislation. The 1893 Railroad Safety Appliance Act gave the ICC jurisdiction over railroad safety, removing this authority from the states, and this was followed with amendments in 1903 and 1910. The Elkins Act (1903) increased penalties for rate discrimination and made those who sought rebates as guilty as the railroads that granted them. The Hepburn Act (1906) authorized the ICC to set maximum railroad rates, and extended the agency's authority to cover bridges, terminals, ferries, sleeping cars, express companies and oil pipelines. The act also declared that any order of the ICC had the force of a court order, It also introduced new accounting methods and raised the number of commissioners to seven.
A long-standing controversy was how to interpret language in the Act that banned long haul-short haul fare discrimination. The Mann-Elkins Act of 1910 addressed this question by giving the ICC power to suspend rate increases pending hearings and laid on the railroads the burden of proof that the rate — not just the increase — was just and reasonable. The Act also set up a court to hear only ICC cases and the gave the ICC authority to begin judicial proceedings against railroads without the need for the Attorney General to initiate the action. It also expanded the ICC's jurisdiction to include regulation of telephone, telegraph and wireless companies. (That authority was transferred to the Federal Communications Commission in 1934.)
The ICC soon grew to have authority over almost every aspect of railroading, ranging from locomotive boilers and passenger accounting to the diameter of grab irons and speed limits. The railroads reached their peak in extent and influence by 1915, about the time the ICC was hitting its stride. Ironically, just as railroads began to suffer from highway and waterway competition, the ICC set out to protect the public from the railroads.
Esch-Cummins Act: Consolidation proposals.
The Transportation Act of 1920 (a.k.a. the Esch–Cummins Act) contained a clause granting the railroads a fair return. The Act (which was repealed in 1933) also allowed the ICC to set minimum rates and instructed the ICC to prepare a plan to consolidate the railroads of the U.S. into several large systems. Between 1920 and 1933 William Z. Ripley, a professor of political economy at Harvard University, wrote up ICC's plan for the regional consolidation of the U.S. railways, which became known as the "Ripley Plan". In 1929, the ICC published Ripley's Plan under the title "Complete Plan of Consolidation". Numerous hearings were held by ICC regarding the plan under the topic "In the Matter of Consolidation of the Railways of the United States into a Limited Number of Systems."
Regional railroads.
The proposed 21 regional railroads were as follows:
Terminal railroads.
There were 100 terminal railroads that were also proposed:
Plan rejected.
Many small railroads failed during the Great Depression of the 1930s. Of those lines that survived, the stronger ones were not interested in supporting the weaker ones. Congress repudiated Ripley's Plan with the "Transportation Act of 1940," and the consolidation idea was scrapped.
Later years.
By 1952, the ICC had jurisdiction over railroads, ferries, pipelines, bridges, internal and coastal shipping, trucks, and interstate bus lines, the catch being that the majority of internal waterway traffic and truck traffic was, by virtue of one law or another, exempt from ICC control. The ICC's authority over the railroads extended to the setting of maximum and minimum rates, approving or disapproving consolidations and mergers, authorizing construction and abandonment of lines, and issuance of securities — indeed, every aspect of the railroad business but labor relations.
The Transportation Act of 1958 gave the ICC jurisdiction over passenger train discontinuances, previously under the authority of the state commissions (state authorities had allowed discontinuance of through trains with states, e.g. allowing a New York-Chicago train to be discontinued within Ohio). The ICC earned a reputation for capriciousness in the matter of passenger-train discontinuances. This was illustrated when the ICC denied the Milwaukee Road's petition to drop a coach-only Chicago-Minneapolis local service on a route that had three other trains, yet it permitted the discontinuance of Chicago & Eastern Illinois Railroad's well-patronized "Georgian-Humming Bird". It was the last passenger train on the Chicago-Danville-Terre Haute-Evansville run, and it was less a local train than it was a north end of Chicago-Atlanta and Chicago-Mobile service operated jointly by the Louisville & Nashville Railroad.
In the matter of mergers, the ICC functioned at a glacial pace. Proceedings in connection with the proposed merger of the Chicago, Rock Island & Pacific Railroad (Rock Island) and Union Pacific Railroad (UP) dragged on for ten years, during which time the Rock Island fell apart and ceased to be the desirable merger partner that UP had courted. Railroad historian George Drury commented that some of the blame was laid on the other railroads. The usual reaction of a railroad company upon learning of a proposed merger between two of its competitors is not "We're good enough to give them a run for their money even if they merge" but rather "They'll run us out of business."
Over-regulation of railroads reached the point that the ICC could (and did) require railroads to continue operations that lost money. In 1962, President John F. Kennedy delivered a message on transportation to Congress in which he criticized the regulatory structure, which resulted in successor Lyndon B. Johnson establishing the federal United States Department of Transportation (DOT) in 1966. The DOT was to develop and coordinate policies that would encourage a national transportation system. Some rate-making and regulatory functions remained with the ICC, however. The Federal Railroad Administration would be born out of the DOT, for the sole purpose of dealing with railroad affairs, with a focus on safety.
Abolition.
Congress passed various deregulation measures in the 1970s and 1980s which diminished ICC authority, including the Railroad Revitalization and Regulatory Reform Act of 1976 ("4R Act") and the Motor Carrier Act of 1980. Full deregulation finally came on October 14, 1980, when President Jimmy Carter signed into law the Staggers Rail Act (named for Democratic Congressman Harley Orrin Staggers of West Virginia). It was a massive deregulation of the railroads. The law included provisions to raise any rate that fell below 160 percent of out-of-pocket costs (this figure eventually rose to 180 percent), and allowed carriers to enter into contracts with shippers to set price and service, both without ICC approval.
In 1995, when most of the ICC's powers had been eliminated, Congress finally abolished the agency with the Interstate Commerce Commission Termination Act. Final Chair Gail McDonald oversaw transferring its remaining functions to a new agency, the Surface Transportation Board.
Prior to its abolition, the ICC issued identification numbers to Motor Carriers (bus lines, shipping companies, etc.) which it issued licenses. These identification numbers were generally in the form of "ICC MC-000000". When the ICC was dissolved, the function of licensing interstate Motor Carriers was transferred to the Department of Transportation, so now all motor carriers which have federal licenses have a different "DOT number" such as "DOT 000000".
Legacy.
The ICC served as a model for later regulatory efforts. Unlike, for example, state medical boards (historically administered by the doctors themselves), the seven Interstate Commerce Commissioners and their staffs were full-time regulators who could have no economic ties to the industries they regulated. Post-1887, state and other federal agencies adopted this structure. And, like the ICC, later agencies tended to be organized as multi-headed independent commissions with staggered terms for the commissioners. At the federal level, agencies patterned after the ICC included the Federal Trade Commission (1914), the Federal Communications Commission (1934), the U.S. Securities and Exchange Commission (1934), the National Labor Relations Board (1935), the Civil Aeronautics Board (1940), Postal Regulatory Commission (1970), the Consumer Product Safety Commission (1975), and the Federal Energy Regulatory Commission (1977). In recent decades, this regulatory structure of independent federal agencies has gone out of fashion; the agencies created after the 1970s generally have single heads appointed by the President and are divisions inside executive Cabinet Departments (e.g., the Occupational Safety and Health Administration (1970) or the Transportation Security Administration (2002)). The trend is the same at the state level, though less pronounced.
International influence.
The Interstate Commerce Commission had a strong influence on the founders of Australia. The Constitution of Australia provides (; also ) for the establishment of an Inter-State Commission, modeled after the United States' Interstate Commerce Commission. However, these provisions have largely not been put into practice; the Commission existed between 1913–1920, and 1975–1989, but never assumed the role which Australia's founders had intended for it.
Criticism.
The limitation on railroad rates introduced in 1906 depreciated the value of railroad securities, a factor that contributed to the panic of 1907.
Some economists and historians, such as Milton Friedman assert that existing railroad interests took advantage of ICC regulations to strengthen their control of the industry and prevent competition, constituting regulatory capture. Economist David D. Friedman argues that the ICC always served the railroads as a cartelizing agent and used its authority over other forms of transportation to prevent them, where possible, from undercutting the railroads.
Racial integration of transport.
Although racial discrimination was never a major focus of its efforts, the ICC had to address civil rights issues when passengers filed complaints.
Relationship between regulatory body and the regulated.
A friendly relationship between the regulators and the regulated is evident in several early civil rights cases. Throughout the South, railroads had established segregated facilities for sleeping cars, coaches and dining cars. At the same time, the plain language of the Act (forbidding "undue or unreasonable preference" as well as "personal discrimination") could be read as an implied invitation for activist regulators to chip away at racial discrimination.
In at least two landmark cases, however, the Commission sided with the railroads rather than with the African-American passengers who had filed complaints. In both "Mitchell v. United States" and "Henderson v. United States," the Supreme Court took a more expansive view of the Act than the Commission. In 1962, the ICC banned racial discrimination in buses and bus stations, but it did not do so until several months after a binding pro-integration Supreme Court decision in "Boynton v. Virginia" and the Freedom Rides (in which activists engaged in civil disobedience to desegregate interstate buses).
Sources.
</dl>

</doc>
<doc id="55680" url="http://en.wikipedia.org/wiki?curid=55680" title="Peter the Aleut">
Peter the Aleut

Cungagnaq (date of birth unknown - d. 1815) is venerated as a martyr and saint (as Peter the Aleut) by some jurisdictions of the Eastern Orthodox Church. He was allegedly a native of Kodiak Island (Alutiiq or Sugpiaq), and is said to have received the Christian name of Peter when he was baptized into the Orthodox faith by the monks of St. Herman's missionaries operating in the north. He is purported to have been captured by Spanish soldiers near San Pedro (Pacifica, California) and tortured and killed at the instigation of Roman Catholic priests either there or at Mission Dolores, in San Francisco. At the time identified for his death, California was Spanish territory, and Spain was worried about Russian advances southwards from Alaska. Hubert Howe Bancroft, in his multi-volume "History of California", only notes that, in connection with an incident wherein a Russian fur-hunting expedition was taken into custody after declining to leave San Pedro; one Russian source accused "the Spaniards of cruelty to the captives, stating that according to Kuskof’s report one Aleut who refused to become a Catholic died from ill-treatment received from the padre at San Francisco."
Martyrdom.
According to the most fully developed version of the story, in 1815 a group of Russian employees of the Russian American Company and their Aleut seal and otter hunters, including Peter, was captured by Spanish soldiers, while hunting illicitly for seals near San Pedro. According to the original account, the soldiers took them to "the mission in Saint-Pedro" for interrogation. One Russian source states that after being taken prisoner near modern Los Angeles, the captives were taken to Mission Dolores—that is, modern San Francisco. With threats of torture, the Roman Catholic priests attempted to force the Aleuts to deny their Orthodox faith and to convert to Roman Catholicism.
When the Aleuts refused, the priest had a toe severed from each of Peter's feet. Peter still refused to renounce his faith and the Spanish priest ordered a group of Native Americans, indigenous to California, to cut off each finger of Peter's hands, one joint at a time, finally removing both his hands. They eventually disemboweled him, making him a martyr to the Eastern Orthodox faith. They were about to torture the next Aleut when orders were received to release them.
Historicity.
An account of the martyrdom of Peter the Aleut is contained in a lengthy letter written on Nov. 22, 1865, by Symeon Ivanovich Yanovsky to Damascene, abbot of the Valaam Monastery in Finland. Yanovsky (1789–1876), who is also one of the chief sources of information about St. Herman of Alaska, was chief manager of the Russian colonies from 1818-1820. In the letter he was reporting on an incident that he had heard from a supposed eyewitness, and that had taken place fifty years earlier in 1815. The letter contains the description of Peter being tortured by "Jesuits" but this would have been virtually impossible, as the Jesuit order had been expelled from all Spanish territories in 1767, suppressed generally in 1773, and had only been reconstituted in 1814 (one year before Peter's alleged death). In 1815 there were no Jesuits within several thousand miles of California, as the reconstitution of the Jesuits in New Spain (that is, Mexico) would not take place until 1816. There were only Franciscans in California at the time, and it would be highly unlikely that anyone could confuse members of the two well-known and very dissimilar orders. Yanovsky adds, "At the time I reported all this to the Head Office in St. Petersburg." And indeed, this earlier communication, his official dispatch to the company's main office—dated Feb. 15, 1820, five years after the event—also relates the story of St. Peter's martyrdom, albeit with different details.
The most significant difference is that Yanovsky's original brief letter of 1820 accompanied a Russian translation of an account given in 1819 by a Kodiak Islander with the Russian name "Ivan Kiglay". This is the only account that purports to be from a witness, and any differences found in other accounts (including in those of Yanovsky himself) are additions or embroideries that lack foundation or support. Kiglay's account describes the capture of Russian-led fur poachers by Spanish soldiers in the vicinity of San Pedro Bay (the modern Port of Los Angeles) and taken to "the mission in Saint-Pedro". (As there was no mission or settlement at San Pedro, it is unclear where the party was supposed to have been taken; the nearest mission would have been San Gabriel, although the non-mission village of Los Angeles would have been closer.) While the rest of the prisoners are removed to Mission Santa Barbara, Kiglay and another Kodiak Islander named Chukagnak -- who had been wounded in a battle with the soldiers -- are imprisoned separately at "the mission at Saint-Pedro", and the next day Indians acting at the behest of a Spaniard torture and kill Chukagnak. Kiglay is apparently going to receive the same treatment, until the Spaniard receives a letter that apparently gives other directions. Kiglay is reimprisoned, and eventually escapes to Fort Ross, where he gives his testimony. There is nothing in the account that links the execution of Chucagnak to a refusal on his part to abandon Orthodoxy. Instead, the eyewitness account states that the Kodiak islanders were all previously offered the opportunity to become Catholics, that they had all declined because they were already Christians, and then with the exceptions of Kiglay and Chukagnak were all transferred to Santa Barbara with no further mention of, or demand for, conversion. 
Veneration.
According to Yanovsky's 1865 letter, upon receiving the report of Peter's death, St. Herman back on Kodiak Island was moved to cry out, "Holy new-martyr Peter, pray to God for us!"
Peter the Aleut was glorified as a saint by the Russian Orthodox Church Outside Russia and locally glorified by the of the Orthodox Church in America as the "Martyr of San Francisco" in 1980. His feast day is celebrated on September 24 or December 12.
There are a number of churches dedicated to him in North America, for example at Lake Havasu City, Arizona; Minot, North Dakota; Calgary; and Abita Springs, Louisiana.

</doc>
<doc id="55683" url="http://en.wikipedia.org/wiki?curid=55683" title="1310s BC">
1310s BC


</doc>
<doc id="55684" url="http://en.wikipedia.org/wiki?curid=55684" title="Time-out (parenting)">
Time-out (parenting)

Time-out (also known as" social exclusion") is a form of behavioural modification that involves temporarily separating a child from an environment where unwanted behavior occurred. This is decided according to the cultural standards and values of the time and place where the misbehavior has occurred with the goal of extinction of the offending behavior. It is an educational and parenting technique recommended by some pediatricians and developmental psychologists as an effective form of child discipline. Often a corner (hence the common term "corner time") or a similar space where the child is to stand or sit during time-outs is designated. This form of discipline is especially popular in North America. 
In the UK, the punishment is often known as the naughty chair or naughty step. This term became popular in the US thanks to two reality TV series, "Supernanny" and "Nanny 911".
History.
The concept of time-out was invented, named, and used by Arthur Staats in his extended work with his daughter (and later son), and was part of a long-term program of behavioral analysis beginning in 1958 that treated various aspects of child development. He introduced various elements that later composed foundations for applied behavior analysis and behavior therapy (the token reward system was another invention). Montrose Wolf, a graduate student assistant of Staats on several studies dealing with reading learning in preschoolers (see, for example, Staats, A.W.; Staats, C.K.; Schultz, R.E.; Wolf, M.M. "The conditioning of textual responses using 'extrinsic' reinforcers."), used that background when he went to the University of Washington where he began his creative program of research. Wolf first used Staats' time-out procedure in a 1964 published study dealing with the behavioral treatment of a child. 
Staats described the discipline of his 2-year old daughter in 1962: "I would put her in her crib and indicate that she had to stay there until she stopped crying. If we were in a public place [where her behavior was inappropriate], I would pick her up and go outside." This has the effect of weakening the offending behavior so that it occurs less frequently, quickly disappearing unless the behavior has been well learned.
Application.
The use of time-out as an acceptable therapeutic procedure has gained wide acceptance in schools, clinics, and hospitals. The main purpose is to isolate or separate (hence "social exclusion") the child for a short period of time, usually 5 to 15 minutes, in order to allow the child to calm down as well as to discourage inappropriate behavior.
Time-outs may be on a chair, step, corner, bedroom, or any other location where there are no distractions. The procedure has been recommended as a time for parents to separate feelings of anger toward the child for their misbehavior, replacing yelling with a calmer and more predictable approach.
In some views, the only requirement for release is for the child to be sitting peacefully, while others advocate a set period of time. When the child has calmed down, they may then express their needs in a more polite manner or return to their activity.
For this disciplinary technique to be most effective and to produce the desired results, the child should be old enough to sit still and is required to remain there for a fixed period. Also, according to developmental psychologists, parents should evaluate each situation to determine what may be causing the misbehavior, such as a toy, frustration, hunger, or lack of sleep, and then respond accordingly with the punishment consistent with the desired behavior. Parents should also clearly explain why the child was put there, in order to make it an opportunity for learning, and how long he needs to stay there (but too much explanation can reinforce the unwanted behavior). Furthermore, experts suggest that time-out should remain brief, proposing a general guideline: the length of time that the child should remain in time-out should correlate with the child's age - each of year of the child's age constitutes one minute in time-out.
Time-out is one behavior control method based on removing positive reinforcement. Less elaborate methods from the same class like tactical ignoring also can be effective in cases where parental/care-giver attention is the positive reinforcer. This class of methods are more effective if the child gets a significant amount positive reinforcement (praise, attention) for good behavior. Tactical ignoring is the preferred method in Parent Management Training for behaviors that can be ignored.
Effectiveness.
Researchers and developmental psychologists have compared time-out or social exclusion with other disciplinary techniques and have questioned its effectiveness.
Several studies show that time-out is an especially effective disciplinary strategy, reducing aggressive and non-complaint behavior, when other positive parenting methods are also used.
While some proponents of time-outs insist on silence and stillness from the child during the time-out, it is easier to use a "release-contingency," such that the requirement is only that the child is sitting peacefully at the end of the time-out period. Those who use time-out for children to get anger and frustration "out of their system" or for children to think about their behavior are using time-out in a way that is different than those basing it on operant behavioral principles (that time-out from positive reinforcement may reduce recurrences of the unwanted target behavior).
In a study by Donaldson and Vollmer, the efficacy of a fixed duration time-out and a release contingency time-out were compared. In the fixed duration condition, children were sent to time-out for a total of 4 minutes and were released from time-out whether or not they performed problem behavior during the time-out session. In the release contingency condition, children were not released from time-out if they were performing problem behavior during the last 30 seconds of their time-out. The time-out was extended until there were no occurrences of problem behavior for a total of 30 seconds or until the time-out reached the ten minute mark. Results showed that both time-out procedures were successful in reducing the problem behavior for the subjects. The subjects in the release contingency did not benefit from staying in time-out for an extended period of time either. Moreover, the results show that only 4 minutes is necessary for a successful time-out procedure.
The effectiveness of time-out also varies with each individual child, dependent on the child's temperament and emotional wellness.
Disadvantages.
Critics of time-out include Thomas Gordon, Alfie Kohn, and Aletha Solter, who claim that the approach may lead to short-term compliance but has the same disadvantages as other forms of punishment. According to these authors, the use of time-out does not enhance moral behavior or teach children useful conflict-resolution skills, and it fails to address the underlying cause of the behavior. Furthermore, they claim that the parent/child bond can be damaged by forced isolation and withdrawal of love in an effort to control a child’s behavior, and this can lead to feelings of insecurity or anxiety in children. Another argument is that time-out, like all other methods of coercive control, eventually stops working as children grow older and begin to rebel against their parents’ authoritarian approach to discipline.
The Australian Association for Infant Mental Health has published a position statement in which the use of time-out is considered inappropriate, especially for children under three years of age. In addition to a list of disadvantages of time-out, the position statement asserts that “separation may increase a child’s insecurity and distress.”
The use of time-out appears to be especially ineffective in families dealing with special challenges. In a review of parenting intervention programs for drug-abusing mothers, researchers found that programs emphasizing behavioral approaches to discipline (such as the use of time-out and rewards) “were not successful in fostering measurable improvement in mother-child interactions or promoting child development.” An attachment-based approach focusing on strengthening the parent/child relationship was found to be more successful than behavioral approaches in changing children’s behavior in these families.
Other studies have found that the traditional behavioral approach to discipline (such as the use of time-out and rewards) was not very effective in changing the behavior of children in foster care with attachment disorders resulting from early abuse or neglect. Foster parents benefit more from training that addresses these children’s attachment and emotional issues, which lie at the root of their challenging behavior.
Time-out has been misused to the point of becoming abusive in some schools. There are reported cases of children being locked in closets for extended periods of solitary confinement for behaviors such as crying or failing to finish an assignment.

</doc>
<doc id="55686" url="http://en.wikipedia.org/wiki?curid=55686" title="Andrew Bobola">
Andrew Bobola

Andrew Bobola, S.J. (Polish: "Andrzej Bobola", 1591 – 16 May 1657) was a Polish missionary and martyr of the Society of Jesus, known as the Apostle of Lithuania and the "hunter of souls".
Life.
Bobola was born in 1591 into a noble family in the Sandomir Palatinate in the Province of Lesser Poland of the Crown of the Kingdom of Poland, then a constituent part of the Polish–Lithuanian Commonwealth. In 1611 he entered the Society of Jesus in Vilnius, then in the Grand Duchy of Lithuania, the other part of the Commonwealth. He subsequently professed solemn vows and was ordained in 1622, after which he served for several years as an advisor, preacher, Superior of a Jesuit residence, etc., in various places. 
From 1652 Bobola also worked as a country "missionary", in various locations of Lithuania: these included Polotsk, where he was probably stationed in 1655, and also Pinsk, (both now in Belarus). On 16 May 1657, during the Khmelnytsky Uprising, he was captured in the village of Janów (now Ivanava, Belarus) by the Cossacks of Bohdan Chmielnicki and, after being subjected to a variety of tortures, killed.
One description of Bobola's death written in 1865 states: 
In the same year, the Cossacks surprised a holy Polish Jesuit, in the town of Pinsk, and conferred on him the palm of martyrdom, on the 16th of May, 1657. Father Andrew Bobola, whose untiring zeal had rendered him obnoxious to the schismatics, had just offered up the holy sacrifice, when a horde of Cossacks attacked the town. On beholding the barbarians, Father Bobola fell upon his knees, raised his eyes and his hands toward heaven, and, having a presentiment that his hour had arrived, exclaimed, "Lord, thy will be done!" At that moment, the Cossacks rushed upon him, stripped him of his holy habit, tied him to a tree, placed a crown upon his head, as did the Jews upon the head of our adorable Saviour, after which they scourged him, tore out one of his eyes, burned his body with torches, and one of the ruffians traced, with his poignard, the form of a tonsure on the head of the venerable Father, and on his back the figure of a chasuble! To do this, the executioner had to strip off the skin of the holy martyr! But this was not yet all. The fingers of the apostle had received the priestly unction. The executioner tore from them the skin, and forced needles under his nails! And during this indescribable torture, the hero prayed for his tormentors; he preached, both by word and example, until the schismatics tore out his tongue and crushed his head. Father Andrew Bobola, whom the Church declared Blessed, the 30th of October, 1853, was sixty-five years of age.
Veneration.
Bobola's body was originally buried in the Jesuit church in Pinsk. It was later moved to their church in Polotsk. By the beginning of the 18th century, however, nobody knew where Bobola's body was buried. In 1701 Father Martin Godebski, S.J., the Rector of the Pinsk College, reputedly had a vision of Bobola. This caused him to order a search for the body. It was reportedly found completely incorrupt, which was recognized by the Church and its supporters as proof of holiness. In 1719 the casket was officially reopened and the body inspected by qualified medical personnel (five physicians and pharmacists). It was reportedly still completely incorrupt: pliable and with soft flesh.
In 1922, the Bolsheviks moved the corpse, later described by an American journalist as a "remarkably well-preserved mummy", to the Museum of Hygiene of People's Commissioners of Health in Moscow. The whereabouts of the remains was not known to the Catholic authorities, and Pope Pius XI charged the Papal Famine Relief Mission in Russia, headed by American Jesuit Father Edmund A. Walsh, with the task of locating and "rescuing" them. In October 1923—as a kind of "pay" for help during famine—the remains were released to Walsh and his Assistant Director, Father Louis J. Gallagher, S.J. Well packed by the two Jesuits, they were delivered to the Holy See by Gallagher on All Saints' Day (1 November) 1923. In May 1924, the relics were installed in Rome's Church of the Gesù, the main church of the Society of Jesus.
Since 17 June 1938 the body has been venerated at a shrine in Warsaw, with an arm remaining at the original shrine in Rome.
Declared Blessed by Pope Pius IX on 30 October 1853, Bobola was canonized by Pope Pius XI on 17 April 1938. His feast day was originally celebrated by the Jesuits on 23 May, but it is now generally celebrated on 16 May. On his feast day in 2002, Pope John Paul II declared Bobola a patron saint of Poland and of the Roman Catholic Archdiocese of Warsaw.
Today some join Bobola with St. Peter the Aleut, an alleged martyr for the Orthodox faith at the hands of Roman Catholics, in a special devotion for the reunion of the two branches of Christianity. However, the historicity of the martyrdom of Peter the Aleut is not clearly established.
Modern Bobola family.
It is assumed that a part of the Bobola family exists to this day under the name of Bobola; they currently live mainly in Warsaw (about 30 persons), with the exception of one branch which currently resides in Paxton, Massachusetts. This is a part which, precisely, seems to have been rooted in Pinsk for some time. In the 17th century, a nobleman from Pinsk signing himself as Bobola used the Leliwa coat of arms, the same as that of Saint Andrew Bobola.
Nevertheless, most of them still bear the name of Bobola and are spread all over Poland (more than 200 persons.
the family now named Bobolia lives in California.
External links.
<BR>

</doc>
<doc id="55687" url="http://en.wikipedia.org/wiki?curid=55687" title="Silver City Airways">
Silver City Airways

Silver City Airways was a private, British independent airline formed in 1946. The name "Silver City" was derived from the eponymous Australian mining town at Broken Hill, where "The Zinc Corporation" was headquartered. Silver City's first commercial flight departed London Heathrow for Sydney via Johannesburg in late 1946. The following year, Silver City leased its first Bristol Freighter, moved its base to Blackbushe and participated in the airlift of Hindu and Muslim refugees between Pakistan and India. In 1948, control of Silver City passed from the Zinc Corporation to British Aviation Services. In July of that year, the airline inaugurated the world's first air ferry service across the English Channel between Lympne Airport and Le Touquet Airport. In 1948–49, Silver City participated in the Berlin Airlift. In 1949, it established a French sister airline.
In 1953, Silver City took delivery of its first Bristol Superfreighter. The following year, the company moved to a new permanent home at Lydd "Ferryfield", Britain's first newly constructed post-war airport. The same year, Silver City Airways came under the control of the Peninsular and Oriental Steam Navigation Company (P&O). By the mid-1950s, Silver City had become the biggest air cargo carrier in the United Kingdom while annual passenger numbers at its "Ferryfield" base had reached ¼ of a million. During that time, the airline also inaugurated air ferry services between Scotland and Ireland and from/to the Midlands. This period also saw the launch of a London—Paris coach-air-coach/rail service, with the cross-Channel air portion operating between Lydd and Le Touquet. In 1957, Silver City accomplished its one-millionth Channel crossing. In summer 1958, Silver City's "Ferryfield" base recorded more aircraft movements than any other UK airport. That year, also marked the conclusion of Silver City's first decade of air ferry operations during which the airline operated more than 100,000 flights carrying over 200,000 vehicles and ¾ of a million passengers, with peak-day frequency exceeding 200. In 1959, Silver City took over sister airline Britavia's Handley Page Hermes fleet and Manston base. That year, the airline also began oil industry support flights in Libya.
By 1960, Silver City's 40,000 annual cross-Channel flights transported 220,000 passengers and 90,000 vehicles while network-wide freight haulage reached 135,000 tons a year. The following summer, the airline reached agreement with a French rival to co-finance construction of a branch line linking Le Touquet Airport with the nearby main railway line to reduce surface travelling time from/to Paris. Unsustainable losses as a result of the loss of the Libyan oil industry support flight contract, increasing competition from roll-on/roll-off ferries and the lack of suitable replacements for the ageing Bristol Freighters resulted in growing financial difficulties, culminating in Silver City's takeover by British United Airways (BUA) holding company Air Holdings in 1962.
History.
The 1940s.
In 1946, Air Cdre Griffith James ("Taffy") Powell got in touch with W.S. Robinson, chairman of London-based mining company "The Zinc Corporation". That meeting resulted in Robinson appointing Powell as the Zinc Corporation's adviser.
One of Powell's first visits in his new capacity took him to Broken Hill, Australia, also known as "Silver City". This visit resulted in the decision to set up a new air transport operator to serve the mining industry, to be named "Silver City".
Silver City Airways was incorporated on 25 November 1946. British Aviation Services (BAS), an early post-World War II airline holding company and air transport operator, became one of Silver City's shareholders, initially taking a 10% stake. Air Cdre Griffith James Powell was the first managing director of both BAS and Silver City.
Silver City's first base was at Langley Aerodrome.
The airline's initial fleet comprised four ex-military Douglas Dakotas and three Avro Lancastrians, the 13-seater civil version of the Lancaster Mark 3 bomber. Two of the latter were new aircraft that had been ordered by British South American Airways (BSAA).
Lancastrian G-AHBW operated the company's first commercial flight, from London Airport (Heathrow) to Sydney via Johannesburg in November 1946. This was followed by similar operations to Johannesburg via Karachi and to Malta before the end of the year.
In October 1947, Silver City became involved in the airlift of Hindu and Muslim refugees between Pakistan and India, following the Subcontinent's partitioning. This operation constituted the fledgling airline's first major engagement. Initially, the repatriation airlift was undertaken by four Dakotas. On short journeys, the authorities granted Silver City dispensation to raise the limit on the maximum number of passengers it could carry from 28 to 52 to airlift as many people as quickly as possible.
Also that year, Silver City moved its base to Blackbushe Airport, as a result of Langley's closure due to Heathrow's expansion.
Also in 1947, Silver City leased its first Bristol Freighter from the manufacturer to replace one of the four Dakotas that had originally been allocated to the repatriation airlift in the Indian subcontinent. Like the Dakotas it had operated on that airlift, Silver City was given dispensation to increase the maximum number of passengers it could carry on the Bristol Freighter above the normal limit of 32. Actual loads on this aircraft type often exceeded 100 passengers per flight, resulting in a total of 1,105 evacuees and their belongings being transported aboard Silver City's single Freighter over a period of nine days. The airline's Bristol Freighter fleet soon expanded to four aircraft. The Freighter would play a major role in the company's development over the coming years. Powell realised that the Bristol Freighter could be adapted to fly car owners with their vehicles from Britain to Continental Europe and the Channel Islands. This "air ferry" would allow British holidaymakers avoid long waits for sea ferries and time-consuming, bumpy rides in rough waters.
On 7 July 1948, a Silver City Bristol Freighter operated the first cross-Channel air ferry service, between Lympne near Folkestone in Kent and Le Touquet on France's northern Côte d'Opale coast, with good road connections from and to London and Paris respectively. The new service, which initially operated on a seasonal charter basis, became a year-round scheduled operation in 1949. In the beginning, there was a flat £32 one-way fare to take a group of four passengers along with their car across the Channel. Once opposition from British European Airways (BEA) to the carriage of passengers travelling without vehicles was overcome, a new fare structure was introduced. For example, a group of four travelling with a small car was charged only £27, while the comparable fare for four people travelling with a large car remained at £32. By the end of 1949, this operation fully utilised five Freighters, which carried 2,700 cars and 10,000 passengers. These figures represented a significant increase over the previous year when only 178 cars and their occupants, as well as some motorcycles and bicycles had been carried until the end of the season in September.
The same year, the Zinc Corporation sold its shareholding in Silver City to BAS, making the latter the airline's sole owner. Silver City subsequently became BAS's biggest operating division.
Silver City joined the 1948–49 Berlin Airlift with a single Bristol Freighter in September 1948. Owing to heavy demand for additional civilian airlift capacity, the airline leased a further two Freighters from the Bristol Aeroplane Company. By the time the civil contribution to the Airlift was scaled down in February 1949, the company's three Bristol Freighters were the last twin-engined airliners employed in this operation. When it came to an end, the firm's Freighters had flown a total of about 800 hours.
In February 1949, Silver City established a French sister airline headquartered in Paris to operate vehicle ferry flights from Le Touquet Airport. The new company was registered under the name "Société Commerciale Aérienne du Littoral" (SCAL). A number of Silver City aircraft were registered to this company. These were transferred onto the French aircraft register. In addition, an agreement was reached to appoint the "Automobile Club de France" as Silver City's and SCAL's official representative in France. These steps were necessary to secure French approval to turn the seasonal charter flights Silver City had operated on this route into a full-fledged scheduled operation.
The 1950s.
By 1950, the number of cars and passengers carried on Silver City's cross-Channel services roughly doubled to 5,000 and 24,000 respectively.
To encourage further traffic growth on its Lympne — Le Touquet cross-Channel car ferry service, Silver City reduced fares with effect from 19 September 1950: the rate for cars up to 14 feet in length was cut from £27 to £19 while the rate for larger vehicles dropped from £32 to £25. This reduction left Silver City's fares only slightly higher than the Dover—Calais ferry fares of British Railways' Southern Region and, together with the service's earlier extension permitting the carriage of cyles and motor cycles, helped establish the airline's ferry services as a serious competitor to the railways.
The success of Silver City's Lympne — Le Touquet air ferry service resulted in subsequent introduction of additional routes across the English Channel and to other parts of the British Isles.
Over the coming years, Silver City pursued a policy of continuous fare reductions to fill the additional capacity on its growing air ferry network. This included new car ferry services between Southampton (Eastleigh) and Cherbourg as well as between Southend (Rochford) and Ostend and a DC-3 passenger service linking Gatwick and Le Touquet. Both of the former commenced in spring 1952, while the latter was inaugurated the following year. As a result, the number of vehicles carried doubled from 5,000 to 10,000 between 1950 and 1952 and quadrupled to 40,000 by the end of the following year. The latter was the consequence of an average 40% fare reduction.
BAS's takeover of Air Kruise, an independent charter and pleasure flight operator based at Lympne, in March 1953 brought a fleet of all-passenger de Havilland Dragon Rapides and Douglas Dakotas. This acquisition resulted in formation of Silver City's "Passenger Division".
In summer 1953, Silver City leased a Breguet Br.763 to participate in the second "Little Berlin Airlift" on the Hamburg (Fuhlsbüttel) — Berlin (Tempelhof) route. A total of 127 round trips carried 4000000 lb of freight with up to three round trips being made in a day, each leg taking 52 minutes' flight time.
In 1953, Silver City also took delivery of its first stretched Mark 32 Bristol Superfreighter, the first of six. The Superfreighter's elongated nose enabled it to accommodate three cars or to be fitted with 60 seats in an all-passenger Super Wayfarer configuration. The new Superfreighters joined a fleet of nine standard Mark 21 Freighters. Other freight charter work at this time included flights to the Suez Canal Zone supporting the UK military forces then stationed there.
As operations expanded, the small grass airfield at Lympne became increasingly inadequate. The search for a suitable location to site a new, purpose-built airport began in 1953. Interim moves to Southend and West Malling were followed by final selection of an area covered by grazing land on the edge of the Dungeness shingle desert on the Kentish coast close to the village of Lydd. This site would host Britain's first newly constructed post-war and first privately owned airport. It would feature two runways, a control tower, passenger terminal with a restaurant, maintenance area and petrol station. The new airport — named "Ferryfield" — opened on 14 July 1954, after six months' work costing £400,000. However, it took almost another two years for the official opening ceremony to be performed at "Ferryfield", which occurred on 5 April 1956. On that day, HRH the Duke of Edinburgh arrived at "Ferryfield" just before 11.00 am on board the Royal Heron. The occasion marked the Duke's first visit to a private British airline at an all-new, privately owned airport. Following his tour of the airport's facilities, the Duke boarded one of Silver City's scheduled air ferry services to Le Touquet on Superfreighter G-AMWD. During the 19-minute flight, the Duke flew the aircraft at its scheduled en route height of 1,000 ft. The Duke's reception at Le Touquet Airport was followed by an informal lunch hosted in his honour by the president of the French Aero Clubs in the airport restaurant. The Duke then departed, flying the Royal Heron to London Airport.
By 1954, the Silver City cross-Channel network comprised five routes: Gatwick — Le Touquet, Lydd — Le Touquet, Lympne—Calais, Lympne—Ostend and Southampton—Cherbourg.
Following the opening of "Ferryfield" in mid-1954, Silver City initially split its operations between the new airport and Lympne. For a short while, Le Touquet flights operated from the former while Calais and Ostend services continued to use the latter. The last of 33,000 Silver City flights, which had carried a total of 54,000 cars and 208,000 passengers since 1948, departed Lympne on 3 October. From then on, vehicle ferry services were concentrated at "Ferryfield".
Also in 1954, control of Silver City passed to P&O via General Steam Navigation, which had acquired a 70% stake in BAS, the airline's parent company. It was also the year Silver City complemented its Gatwick — Le Touquet all-passenger operation with a vehicle ferry service.
By 1955, "Ferryfield" handled 250,000 passengers annually. This made it busier than Gatwick.
Also in 1955, Silver City launched its first air ferry services between Scotland and Ireland and its first such service from the Midlands. These linked Stranraer with Belfast and Birmingham with Le Touquet. In addition, the airline opened a new service from Southampton to Deauville.
That year also saw Silver City become the UK's biggest air cargo carrier with an annual freight volume of 70,190 tons.
In 1956, Silver City commenced London—Paris coach-air-coach/rail services via Lydd ("Ferryfield") and Le Touquet/Étaples. As Le Touquet Airport was not linked to the French railway network at the time, the journey between the airport and Paris involved an additional change between coach and train at Étaples. DC-3s initially operated these all-passenger services, which were marketed as "Silver Arrow" in the UK and as "Flèche d'argent" in France. "Silver Arrow"/"Flèche d'argent" was a joint operation between British Railways, Silver City and Société Nationale des Chemins de Fer français (SNCF).
By 1957, BAS's airline subsidiaries included Air Kruise, Aquila Airways, Britavia, the Lancashire Aircraft Corporation and the original Manx Airlines, apart from Silver City Airways itself.
Also in 1957, Silver City completed its one-millionth Channel crossing since its inaugural Lympne — Le Touquet air ferry service took to the air in July 1948.
That year also saw Silver City become involved in supporting the oil industry in Libya, flying geologists and supplying desert camps with a fleet of DC-3s and a single DC-2 from bases at Tripoli and Benghazi. The airline's sole DC-2 was originally operated by Swissair and subsequently sold to new owners in South Africa, who leased it to Silver City.
By 1958, "Ferryfield" had become one of Britain's three busiest airports. It recorded more aircraft movements during the peak summer months than any other airport in the UK, and only Heathrow and Northolt were busier in terms of annual air freight volume.
That year also marked the conclusion of the first decade of Silver City's air ferry services. During that period, the airline completed 125,000 ferry flights. These carried 215,000 vehicles and 750,000 passengers. At its peak, Silver City operated 222 daily ferry flights across the English Channel, as well as between Scotland and Ireland and to/from the Isle of Wight, the Channel Islands and the Isle of Man. Cross-Channel flights to France operated between 7.30 am and 11.00 pm. The average fare was £25 per car and £4 per passenger. This was furthermore the time the Air Kruise cross-Channel services, as well as all Dragon Airways, Lancashire Aircraft Corporation and Manx Airlines operations from Newcastle upon Tyne, Blackpool and the Isle of Man were transferred to Silver City's new Northern Division to streamline BAS's fragmented airline operations. It was hoped that these measures would improve BAS's financial performance.
In May of the same year, the crew of a Silver City Dakota made the first sighting of the "Lady Be Good", a WW II bomber that had disappeared in 1943 while returning from an operation to Naples, in the Libyan Desert.
In 1959, Britavia transferred its five-strong Hermes 4A fleet to sister airline Silver City, as a consequence of the loss of a trooping contract to Eagle. The Hermes were based at Manston, from where they operated "Silver Arrow" all-passenger services to Le Touquet and inclusive tour charters to European destinations until parent company BAS's acquisition by British United Airways (BUA) parent Air Holdings in 1962.
Also in 1959, Silver City opened a Blackpool-Dublin route.
By the end of that decade, Silver City advertised £8 18s day-return fares for its London—Paris "Silver Arrow"/"Flèche d'argent" service.
The 1960s.
By 1960, Silver City made 40,000 yearly Channel crossings, carrying 90,000 vehicles and 220,000 passengers. During that year, it also moved 135,000 tons of freight across its network. This represented an increase of 35% over the previous year.
In summer 1961, Silver City agreed with rival French air ferry operator Compagnie Air Transport (CAT) for the latter to finance the construction of a two-mile rail spur into Le Touquet Airport from the nearby main line to reduce the travelling time between the airport and Paris by cutting out the coach/rail change at Étaples. In return, Silver City transferred three of its Superfreighters to CAT along with the traffic rights to operate the "Ferryfield" — Le Touquet and Bournemouth (Hurn) — Cherbourg routes. This arrangement gave CAT a 25% share of the car ferry market between Britain and France.
Having been outbid by Belgium's flag carrier Sabena for the Libyan oil industry support flight contract that year, Silver City's losses became unsustainable. This necessitated the sale of three Superfreighters to CAT for £192,300.
Following growing financial difficulties, Silver City was taken over by BUA parent Air Holdings in 1962. The takeover was officially announced in January of that year. Air Holdings were the owners of Channel Air Bridge, a rival air ferry operator based at Southend in Essex, which operated similar services from Southend to the Continent. The BUA-BAS merger removed BUA's last remaining independent competitor in the air ferry business. The addition of Silver City's 650,000 annual ferry passengers increased the yearly combined total to just under one million, accounting for two thirds of BUA's total passengers. However, the change in ownership failed to staunch the airline's losses. These amounted to £650,000 during the first half of 1962. By the end of the year, the Silver City name ceased to be used as all aircraft had either been repainted in BUA colours or retired.
Despite the poor financial performance, 1962 turned out to be the busiest year in Silver City's 16-year history. During that year, the airline and its French partner CAT carried 96,272 vehicles and 238,748 passengers on 43,064 flights, representing increases of 10%, 6% and 12% compared with 1961. In addition, over 43,000 tonnes of cargo were carried. However, these record-breaking traffic statistics did not alter the fact that the airline's air ferry operation was no longer economically viable. With the advent of new, high-capacity roll-on/roll-off ferries and hovercraft that were faster and more reliable than traditional ferries, competition intensified. Established aircraft manufacturers were not interested in producing reasonably priced replacements for the ageing Bristol Freighters/Superfreighters that were suffering from wing fatigue. The airline's long-standing policy of stimulating the market by continuously reducing fares had resulted in uneconomic yields in the absence of a corresponding reduction in costs. The Hermes fleet had continued in operation serving several UK airports, mainly on inclusive tour flights, with the last example being retired from service in late 1962.
On 1 January 1963, Air Holdings merged Silver City with Channel Air Bridge to form British United Air Ferries.
Fleet details.
Silver City operated the following aircraft types during its 16-year existence:
Fleet in 1950.
In 1950, Silver City operated 16 aircraft.
Fleet in 1954.
23 aircraft.
Fleet in 1958.
38 aircraft.
Fleet in 1962.
31 aircraft.
Accidents and incidents.
There are three recorded accidents involving Silver City aircraft, two of which were fatal.
The worst accident in company history occurred on 27 February 1958. Bristol 170 Mark 21E Freighter registration G-AICS operating a charter flight from the Isle of Man to Manchester on behalf of Manx Airlines crashed in bad weather on Winter Hill near Bolton, Lancashire, destroying the aircraft and killing 35 of 39 passengers (all three crew members survived).
The aircraft was chartered by the Isle of Man motor trade to take members to the Exide battery factory in Clifton Junction, and it hit the northeast slope of Winter Hill in thick fog at a height of approximately 1,460 ft and burst into flames, as a result of a navigational error committed by the first officer.
The second fatal accident occurred on 1 November 1961. Bristol 170 Mark 32 Superfreighter registration G-ANWL operating a scheduled service from Cherbourg to Guernsey crashed after losing height during a missed approach to Guernsey Airport, damaging the aircraft beyond repair and killing two out of three crew members (all seven passengers survived).
Having failed to gain height following a power increase to go around, the aircraft struck the ground with its starboard wing and cartwheeled due to a malfunctioning automatic pitch coarsening unit of the starboard propeller.
The non-fatal accident occurred on 19 January 1953. Bristol 170 Mark 21 Freighter registration G-AICM operating a non-scheduled cargo flight from West Berlin crash-landed near Tempelhof Airport as a result of fuel starvation when bad weather at the destination forced it to return to Berlin. Although the accident damaged the aircraft beyond repair, both pilots survived.
Resurrection.
Air Holdings, which had retained the rights to the "Silver City" name following the merger between Silver City and Channel Air Bridge to form British United Air Ferries a decade earlier, resurrected Silver City for a short period during 1973.
The airline's second incarnation was as a specialist livestock carrier transporting cattle between Norwich and Germany. This operation utilised three of five Vickers Vanguards owned by Air Holdings, which had been leased to Invicta International Airlines. That airline's failure to pay for the leases had resulted in Air Holdings repossessing the aircraft and starting its own air freight operation.
Air Holdings' lack of success with its German cattle charters led to a decision to put the aircraft up for sale in October and to close down the airline the following month, with the "Silver City" name being de-activated by the end of the year.
External links.
<br>

</doc>
<doc id="55688" url="http://en.wikipedia.org/wiki?curid=55688" title="Buckinghamshire">
Buckinghamshire

Buckinghamshire ( or ; archaically the County of Buckingham; abbreviated Bucks) is a ceremonial and non-metropolitan home county in South East England. The county town is Aylesbury, the largest town in the ceremonial county is Milton Keynes and largest town in the non-metropolitan county is High Wycombe.
The area under the control of Buckinghamshire County Council, or shire county, is divided into four districts—Aylesbury Vale, Chiltern, South Bucks and Wycombe. The Borough of Milton Keynes is a unitary authority and forms part of the county for various functions such as Lord Lieutenant but does not come under county council control. The ceremonial county, the area including Milton Keynes borough, borders Greater London (to the south east), Berkshire (to the south), Oxfordshire (to the west), Northamptonshire (to the north), Bedfordshire (to the north east) and Hertfordshire (to the east).
Sections of the county closer to London are part of the Metropolitan Green Belt, which limits development. It is the location of the nationally important Pinewood Studios and Dorney Lake, which held the rowing events at the 2012 Summer Olympics. It is also well known for the new town of Milton Keynes and the Chiltern Hills area of outstanding natural beauty.
History.
The name Buckinghamshire is Anglo-Saxon in origin and means "The district (scire) of Bucca's home". "Bucca's home" refers to Buckingham in the north of the county, and is named after an Anglo-Saxon landowner. The county has been so named since about the 12th century; however, the county has existed since it was a subdivision of the kingdom of Mercia (585–919).
The history of the area predates the Anglo-Saxon period and the county has a rich history starting from the Celtic and Roman periods, though the Anglo-Saxons perhaps had the greatest impact on Buckinghamshire: the geography of the rural county is largely as it was in the Anglo-Saxon period. Later, Buckinghamshire became an important political arena, with King Henry VIII intervening in local politics in the 16th century and just a century later the English Civil War was reputedly started by John Hampden in mid-Bucks.
Historically, the biggest change to the county came in the 19th century, when a combination of cholera and famine hit the rural county, forcing many to migrate to larger towns to find work. Not only did this alter the local economic situation, it meant a lot of land was going cheap at a time when the rich were more mobile and leafy Bucks became a popular rural idyll: an image it still has today. Buckinghamshire is a popular home for London commuters, leading to greater local affluence; however, some pockets of relative deprivation remain.
The expansion of London and coming of the railways promoted the growth of towns in the south of the county such as Aylesbury, Amersham and High Wycombe, leaving the town Buckingham itself to the north in a relative backwater. As a result, most county institutions are now based in the south of the county or Milton Keynes, rather than in Buckingham.
Geography.
The county can be split into two sections geographically. The south leads from the River Thames up the gentle slopes of the Chiltern Hills to the more abrupt slopes on the northern side leading to the Vale of Aylesbury, a large flat expanse of land, which includes the path of the River Great Ouse.
Waterways.
Rivers.
The county includes parts of two of the four longest rivers in England. The River Thames forms the southern boundary with Berkshire, which has crept over the border at Eton and Slough so that the river is no longer the sole boundary between the two counties. The River Great Ouse rises just outside the county in Northamptonshire and flows east through Buckingham, Milton Keynes and Olney.
Canals.
The main branch of the Grand Union Canal passes through the county as do its arms to Slough, Aylesbury, Wendover (disused) and Buckingham (disused). The canal has been incorporated into the landscaping of Milton Keynes.
Landscape.
The southern part of the county is dominated by the Chiltern Hills. The two highest points in Buckinghamshire are Haddington Hill in Wendover Woods (a stone marks its summit) at 267 m above sea level, and Coombe Hill near Wendover at 260 m.
Demography.
The ceremonial county of Buckinghamshire consists of the area administered by Milton Keynes Borough Council as well as that administered by Buckinghamshire County Council. The ceremonial county has a Lord Lieutenant and a High Sheriff. Currently the Lord Lieutenant of Buckinghamshire is Sir Henry Aubrey-Fletcher and the High Sheriff of Buckinghamshire is Amanda Nicholson. The office of "Custos rotulorum" has been combined with that of Lord Lieutenant since 1702.
As can be seen from the table, the Vale of Aylesbury and the Borough of Milton Keynes have been identified as growth areas, with a projected population surge of almost 40,000 in Aylesbury Vale between 2011 and 2026 and 75,000 in Milton Keynes within the same 15 years. The population of Milton Keynes is expected to reach almost 350,000 by 2031, whilst the urban population of the county town of Aylesbury is expected to exceed 100,000.
Buckinghamshire is split into .
Today Buckinghamshire is ethnically diverse, particularly in the larger towns. At the end of the 19th century some Welsh drover families settled in north Bucks and, in the last quarter of the 20th century, a large number of Londoners in Milton Keynes. Between 6 and 7% of the population of Aylesbury are of Asian or Asian British origin. Likewise Chesham has a similar-sized Asian community, and High Wycombe is the most ethnically diverse town in the county, with large Asian and Afro-Caribbean populations. During the Second World War there were many Polish settlements in Bucks, Czechs in Aston Abbotts and Wingrave, and Albanians in Frieth. Remnants of these communities remain in the county.
Politics.
At present, the county has two top-level administrations: Buckinghamshire County Council, which administers about four-fifths of the county (see map above) and the Borough of Milton Keynes, a unitary authority, which administers the remaining fifth. There are four district councils that are subsidiary to the county council: Aylesbury Vale, Chiltern, South Bucks and Wycombe districts.
Buckinghamshire County Council.
The county council was founded in 1889 with its base in new municipal buildings in Walton Street, Aylesbury (which are still there). In Buckinghamshire, local administration is run on a two-tier system where public services are split between the county council and a series of district councils.
In the 1960s the council moved into new premises: a 15-storey tower block in the centre of Aylesbury (pictured) designed by architect Fred Pooley. Said to be one of the most unpopular and disliked buildings in Buckinghamshire, it is now a Grade II listed building.
In 1997 the northernmost part of Buckinghamshire, then Milton Keynes District, was separated to form a unitary authority, the Borough of Milton Keynes; however for ceremonial and some other purposes Milton Keynes is still considered in law to be part of Buckinghamshire.
Buckinghamshire County Council is a large employer in the County and provides a variety of services, including education (schools, adult education and youth services), social services, highways, libraries, County Archives and Record Office, the County Museum and the Roald Dahl Children's Gallery in Aylesbury, consumer services and some aspects of waste disposal and planning.
Coat of arms.
The coat of arms of Buckinghamshire County Council features a white swan in chains. This dates back to the Anglo-Saxon period, when swans were bred in Buckinghamshire for the king's pleasure. That the swan is in chains illustrates that the swan is bound to the monarch, an ancient law that still applies to wild swans in the UK today. The arms were first borne at the Battle of Agincourt by the Duke of Buckingham.
Above the swan is a gold band, in the centre of which is Whiteleaf Cross, representing the many ancient landmarks of the county. The shield is surmounted by a beech tree, representing the Chiltern Forest that once covered almost half the county. Either side of the shield are a buck, for Buckingham, and a swan, the county symbol.
The motto of the shield is "Vestigia Nulla Retrorsum". This is Latin and means 'no stepping back'.
Flag.
The traditional flag of Buckinghamshire, which flies outside County Hall in Aylesbury, comprises red and black halves with a white swan. The flag takes the county emblem which is on the county shield.
Economy.
Buckinghamshire has a modern service-based economy and is part of the Berkshire, Buckinghamshire and Oxfordshire NUTS-2 region, which was the seventh richest subregion in the European Union in 2002. As well as the highest GDP per capita outside Inner London, Buckinghamshire has the highest quality of life, the highest life expectancy and the best education results in the country. The southern part of the county is a prosperous section of the London commuter belt. The county has fertile agricultural lands, with many landed estates, especially those of the Rothschild banking family of England in the 19th century (see Rothschild properties in England). The county has several annual agricultural shows, with the Bucks County Show established in 1859. Manufacturing industries include furniture-making (traditionally centred at High Wycombe), pharmaceuticals and agricultural processing.
This is a chart of trend of regional gross value added of Buckinghamshire at current basic prices published by the Office for National Statistics with figures in millions of British Pounds sterling (except GVA index).
Places of interest.
Buckinghamshire is notable for its open countryside and natural features, including the Chiltern Hills Area of Outstanding Natural Beauty, Stowe Landscaped Gardens near Buckingham, and the River Thames. The Ridgeway Path, a long-distance footpath, passes through the county. The county also has many historic houses. Some of these are opened to the public by the National Trust, such as Waddesdon Manor, West Wycombe Park and Cliveden. Other historic houses are still in use as private homes, such as the Prime Minister's country retreat Chequers.
Buckinghamshire is the location of Bletchley Park, the site of World War II British codebreaking and Colossus, the world's first programmable electronic digital computer.
Buckinghamshire is the home of various notable people in connection with whom tourist attractions have been established: for example the author Roald Dahl who included many local features and characters in his works.
Sports facilities in Buckinghamshire include half of the international Silverstone Circuit which straddles the Buckinghamshire and Northamptonshire border, Adams Park in the south and in the north, and the county is also home to the world famous Pinewood Studios. Dorney Lake, named 'Eton Dorney' for the event, was used as the rowing venue for the 2012 Summer Olympics.
Transport.
Roads.
Buckinghamshire (including Milton Keynes) is served by four motorways, although two are on its borders:
Five important A roads also enter the county (from north to south):
The county is poorly served with internal routes, with the A413 and A418 linking the south and north of the county.
Rail.
As part of the London commuter belt, Buckinghamshire is well connected to the national rail network, with both local commuter and inter-city services serving some destinations.
Chiltern Railways is a principal train operating company in Buckinghamshire, providing the majority of local commuter services from the centre and south of the county, with trains running into London Marylebone. First Great Western provides commuter services from Taplow and Iver into Paddington. London Midland provides commuter services from Milton Keynes into Euston whilst Southern provides services (via the West London Line) from Milton Keynes to Croydon.
For intercity services, Virgin Trains runs services from Milton Keynes Central to Euston, North West England, the West Midlands, the Scottish Central Belt, and North Wales. Meanwhile First Great Western operates non-stop inter-city services through the south of the county between Paddington and South West England and/or South Wales.
There are four main lines running through the county:
There are the following additional lines:
From 2017, Iver will have Crossrail services. From 2019, the East West Rail Link is to reinstate the route via Winslow between Oxford and Bletchley, enabling electrified services to Milton Keynes Central. The line between Aylesbury and Claydon Junction is also to be reinstated in the same programme, enabling services between Aylesbury and Milton Keynes. [Electrification of the Marston Vale Line is not programmed, meaning that passengers for Bedford must change at Bletchley]. Finally, the High Speed 2 line may run non-stop through the county at some future date. 
Settlements.
For the full list of towns, villages and hamlets in Buckinghamshire, see List of places in Buckinghamshire. Throughout history, there have been a number of changes to the Buckinghamshire boundary.
Education.
Education in Buckinghamshire is governed by two Local Education Authorities. Buckinghamshire County Council is one of the few remaining LEAs still using the tripartite system, albeit with some revisions such as the abolition of secondary technical schools. It has a completely selective education system: pupils transfer either to a grammar school or to a secondary modern school depending on how they perform in the 11 plus test and on their preferences. Pupils who do not take the test can only be allocated places at secondary modern schools. There are 9 independent schools and 34 maintained (state) secondary schools, not including sixth form colleges, in the county council area. The unitary authority of Milton Keynes operates a comprehensive education system: there are 8 maintained (state) secondary schools in the borough council area. Buckinghamshire and Milton Keynes are also home to the University of Buckingham, Buckinghamshire New University, the Open University and the University Campus Milton Keynes.
Notable people.
Buckinghamshire is the birthplace and/or final resting place of several notable individuals. St Osyth was born in Quarrendon and was buried in Aylesbury in the 7th century while at about the same time Saint Rumwold was buried in Buckingham. In the medieval period Roger of Wendover was, as the name suggests, from Wendover and Anne Boleyn also owned property in the same town. It is said that King Henry VIII made Aylesbury the county town in preference to Buckingham because Boleyn's father owned property there and was a regular visitor himself. Other medieval residents included Edward the Confessor, who had a palace at Brill, and John Wycliffe who lived in Ludgershall.
Buckinghamshire later became home to some notable literary characters. Edmund Waller was brought up in Beaconsfield and served as Member of Parliament for both Amersham and Wycombe. Percy Bysshe Shelley and his wife Mary lived for some time in Marlow, attracted to the town by their friend Thomas Love Peacock who also lived there. John Milton lived in Chalfont St Giles and his cottage can still be visited there and John Wilkes was MP for Aylesbury. Later authors include Jerome K. Jerome who lived at Marlow, T. S. Eliot who also lived at Marlow, Roald Dahl who lived at Great Missenden, Enid Blyton who lived in Beaconsfield and Edgar Wallace who lived at Bourne End and is buried in Little Marlow. Modern-day writers from Bucks include Terry Pratchett who was born in Beaconsfield, Tim Rice who is from Amersham and Andy Riley who is from Aylesbury.
During the Second World War a number of European politicians and statesmen were exiled in England. Many of these settled in Bucks as it is close to London. President Edvard Beneš of Czechoslovakia lived at Aston Abbotts with his family while some of his officials were stationed at nearby Addington and Wingrave. Meanwhile Władysław Sikorski, military leader of Poland, lived at Iver and King Zog of Albania lived at Frieth. Much earlier, King Louis XVIII of France lived in exile at Hartwell House from 1809 to 1814.
Also on the local political stage Buckinghamshire has been home to Nancy Astor who lived in Cliveden, Frederick, Prince of Wales who also lived in Cliveden, Baron Carrington who lives in Bledlow, Benjamin Disraeli who lived at Hughenden Manor and was made Earl of Beaconsfield, John Hampden who was from Great Hampden and is revered in Aylesbury to this day and Prime Minister Archibald Primrose, 5th Earl of Rosebery who lived at Mentmore. Also worthy of note are William Penn who believed he was descended from the Penn family of Penn and so is buried nearby and the current Prime Minister of the United Kingdom, who has an official residence at Chequers. Finally John Archdale colonial governor of North Carolina and South Carolina, although more notably American, was born in Buckinghamshire.
Other notable natives of Buckinghamshire include:
Celebrities living in Bucks include:

</doc>
<doc id="55690" url="http://en.wikipedia.org/wiki?curid=55690" title="Acre, Israel">
Acre, Israel

Acre ( or , Hebrew: עַכּוֹ, "ʻAkko", most commonly spelled as Akko; Arabic: عكّا‎, "ʻAkkā", Ancient Greek, "Akre", Ἄκρη) is a city in the northern coastal plain region of northern Israel at the northern extremity of Haifa Bay. The city occupies an important location, as it sits on the coast of the Mediterranean, traditionally linking the waterways and commercial activity with the Levant. Acre is one of the oldest continuously inhabited sites in the world.
Historically, it was a strategic coastal link to the Levant. In crusader times it was known as "St. John d'Acre" after the Knights Hospitaller of St John order who had their headquarters there. Acre is the holiest city of the Bahá'í Faith, and as such gets many Baha'i pilgrims. In 2011, the population was 46,464. Acre is a mixed city, with 75% of the population being Jewish and 25% Arab. The mayor is Shimon Lankri, who was re-elected in 2011.
Etymology.
The name 'Akka is recorded in Egyptian sources from about 2000 BC, with three signs (the initial guttural, "k" and "a"; followed by the sign for "foreign city").
In the Amarna letters, written in Akkadian, the letter "H" is used to signify the guttural letters alef-heh-chet-ayin, and therefore it was possible to write the name of the city as if it were "Haca" or "Aca". Had the name not been preserved, we would not have been able to identify it with certainty with the name that appears in cuneiforms. In Assyrian the name has been preserved with the spelling "AKK".
The city was renamed Ptolemais during the Hellenistic and later Roman-Byzantine period, but was restored to Akka following the Muslim conquest.
History.
Antiquity.
Acre is one of the oldest continuously inhabited sites in the region. The name "Aak", which appears on the tribute-lists of Thutmose III (c. 15th century BC), may be a reference to Acre. The Amarna letters also mention a place named "Akka", as well as the Execration texts, that pre-date them. First settlement at the site of Ancient Acre appears to have been in the Early Bronze Age, or about 3000 BC. In the Hebrew Bible, (Judges 1:31), Akko is one of the places from which the Israelites did not drive out the Canaanites. It is later described in the territory of the tribe of Asher and according to Josephus, was ruled by one of Solomon's provincial governors. Throughout Israelite rule, it was politically and culturally affiliated with Phoenicia. Around 725 BC, Akko joined Sidon and Tyre in a revolt against Shalmaneser V.
Greek, Judean and Roman periods.
Greek historians refer to the city as "Ake", meaning "cure." According to the Greek myth, Heracles found curative herbs here to heal his wounds. Josephus calls it "Akre". The name was changed to "Antiochia Ptolemais" (in Greek Αντιόχεια Πτολεμαίς) shortly after Alexander the Great's conquest, and then to Ptolemais, probably by Ptolemy Soter, after the partition of the kingdom of Alexander the Great.
Strabo refers to the city as once a rendezvous for the Persians in their expeditions against Egypt. About 165 BC Judas Maccabeus defeated the Seleucids in several battles in Galilee, and drove them into Ptolemais. About 153 BC Alexander Balas, son of Antiochus Epiphanes, contesting the Seleucid crown with Demetrius, seized the city, which opened its gates to him. Demetrius offered many bribes to the Maccabees to obtain Jewish support against his rival, including the revenues of Ptolemais for the benefit of the Temple in Jerusalem, but in vain. Jonathan Maccabaeus threw in his lot with Alexander, and in 150 BC he was received by him with great honour in Ptolemais. Some years later, however, Tryphon, an officer of the Seleucids, who had grown suspicious of the Maccabees, enticed Jonathan into Ptolemais and there treacherously took him prisoner.
The city was captured by Alexander Jannaeus, Cleopatra VII of Egypt and Tigranes II of Armenia. Here Herod built a gymnasium. St Paul spent a day in Ptolemais (Acts 21:7). A Roman colonia was established at the city, Colonia Claudii Cæsaris.After the permanent division of the Roman Empire in 395 AD, Akko was administered by the Eastern ("Byzantine") Empire.
Early Islamic era.
Following the defeat of the Byzantine army of Heraclius by the Muslim army of Khalid ibn al-Walid in the Battle of Yarmouk, and the capitulation of the Christian city of Jerusalem to the Caliph Umar, Acre came under the rule of the Rashidun Caliphate beginning in 638. According to the early Muslim chronicler al-Baladhuri, the actual conquest of Acre was led by Shurahbil ibn Hasana, and it likely surrendered without resistance. The Arab conquest brought a revival to the town of Acre, and it served as the main port of Palestine through the Umayyad and Abbasid Caliphates that followed, and through Crusader rule into the 13th century.
The first Umayyad caliph, Mu'awiyah (r. 661-680), regarded the coastal towns of the Levant as strategically important. Thus, he strengthened Acre's fortifications and settled Persians from other parts of Muslim Syria to inhabit the city. From Acre, which became one of the region's most important dockyards along with Tyre, Mu'awiyah launched an attack against Byzantine-held Cyprus. The Byzantines assaulted the coastal cities in 669, prompting Mu'awiyah to assemble and send shipbuilders and carpenters to Acre. The city would continue to serve as the principal naval base of Jund al-Urdunn ("Military District of Jordan") until the reign of Caliph Hisham ibn Abd al-Malik (723-743), who moved the bulk of the shipyards north to Tyre. Nonetheless, Acre remained militarily significant through the early Abbasid period, with Caliph al-Mutawakkil issuing an order to make Acre into a major naval base in 861, equipping the city with battleships and combat troops.
During the 10th-century, Acre was still part of Jund al-Urdunn. Local Arab geographer al-Muqaddasi visited Acre during the early Fatimid era in 985, describing it as a fortified coastal city with a large mosque possessing a substantial olive grove. Fortifications had been previously built by the autonomous Emir Ibn Tulun of Egypt, who annexed the city in the 870s, and provided relative safety for merchant ships arriving at the city's port. When Persian traveller Nasir Khusraw visited Acre in 1047, he noted that the large Friday mosque was built of marble, located in the centre of the city and just south of it lay the "tomb of the Prophet Salih." Khusraw provided a description of the city's size, which roughly translated as having a length of 1.24 km and a width of 300 m. This figure indicates that Acre at that time was larger than its current Old City area, most of which was built between the 18th and 19th centuries.
Crusader and Mamluk period.
After roughly four years of siege, Acre finally capitulated to the forces of King Baldwin I of Jerusalem in 1104 during the First Crusade. The Crusaders made the town their chief port in Palestine. On the first Crusade, Fulcher relates his travels with the Crusading armies of King Baldwin, including initially staying over in Acre before the army’s advance to Jerusalem. This demonstrates that even from the beginning, Acre was an important link between the Crusaders and their advance into the Levant. Its function was to provide Crusaders with a foothold in the region and access to vibrant trade that made them prosperous, especially giving them access to the Asiatic spice trade. By the 1130s it had a population of around 25,000 and was only matched for size in the Crusader kingdom by the city of Jerusalem. Around 1170 it became the main port of the eastern Mediterranean, and the kingdom of Jerusalem was regarded in the west as enormously wealthy above all because of Acre. According to an English contemporary, it provided more for the Crusader crown than the total revenues of the king of England.
The Andalusian geographer Ibn Jubayr wrote that in 1185 there was still a Muslim community in the city who worshipped in a small mosque.
Acre, along with Beirut and Sidon, capitulated without a fight to the Ayyubid sultan Saladin in 1187, after his decisive victory at Hattin and the subsequent Muslim capture of Jerusalem.
Acre remained in Muslim hands until it was unexpectedly besieged by King Guy of Lusignan—reinforced by Pisan naval and ground forces—in August 1189. The siege was unique in the history of the Crusades since the Frankish besiegers were themselves besieged, by Saladin's troops. It was not captured until July 1191 when the forces of the Third Crusade, led by King Richard I of England and King Philip II of France, came to King Guy's aid. Acre then served as the "de facto" capital of the remnant Kingdom of Jerusalem in 1192 and later became a seat of the Knights Hospitaller military order. Acre continued to prosper as major commercial hub of the eastern Mediterranean, but also underwent turbulent times due to the bitter infighting among the Crusader factions that occasionally resulted in civil wars.
The old part of the city, where the port and fortified city were located, protrudes from the coastline, exposing both sides of the narrow piece of land to the sea. This could maximize its efficiency as a port, and the narrow entrance to this protrusion served as a natural and easy defense to the city. Both the archaeological record and Crusader texts emphasize Acre’s strategic importance—a city in which it was crucial to pass through, control, and, as evidenced by the massive walls, protect.
Acre was the final stronghold of the Crusader states when much of the Levantine coastline was conquered by Mamluk forces. The city, having been isolated and largely abandoned by Europe, capitulated to the Mamluks led by Sultan al-Ashraf Khalil in a bloody siege in 1291. In line with Mamluk policy regarding the coastal cities (to prevent their future utilization by Crusader forces), Acre was entirely destroyed with the exception of a few religious edifices considered sacred by the Muslims, namely the Nabi Salih tomb and the Ayn Bakar spring. The destruction of the city led to popular Arabic sayings in the region enshrining its past glory. In 1321 the Syrian geographer Abu'l Fida wrote that Acre was "a beautiful city" but still in ruins following its capture by the Mamluks. Nonetheless, the "spacious" port was still in use and the city was full of artisans. Throughout the Mamluk era (1260-1517), Acre was succeeded by Safad as the principal city of its province.
Ottoman era.
The Ottomans under Sultan Selim I captured what remained of the city in 1517, which had been burned down by the Mamluks and had become a tiny fishing village. English academic Henry Maundrell in 1697 found it a ruin, save for a "khan" (caravanserai) built and occupied by French merchants for their use, a mosque and a few poor cottages. The "khan" was named Khan al-Ilfranj after its French founders.
During Ottoman rule, Acre continued to play an important role in the region via smaller autonomous sheikhdoms. Towards the end of the 18th century Acre revived under the rule of Dhaher al-Omar, the Arab ruler of the Galilee, who made the city capital of his autonomous sheikhdom. Dhaher rebuilt Acre's fortifications, using materials from the city's medieval ruins. He died outside its walls during an offensive against him by the Ottoman state in 1775. His successor, Jezzar Pasha, further fortified its walls when he virtually moved the capital of the Saida Eyelet ("Province of Sidon") to Acre where he resided. Jezzar's improvements were accomplished through heavy imposts secured for himself all the benefits derived from his improvements. About 1780 Jezzar peremptorily banished the French trading colony, in spite of protests from the French government, and refused to receive a consul. Both Dhaher and Jezzar undertook ambitious architectural projects in the city, building several caravanserais, mosques, public baths and other structures. Some of the notable works included the Jezzar Pasha Mosque, which was built out of stones from the ancient ruins of Caesarea and Atlit and the Khan al-Umdan, both built on Jezzar's orders.
In 1799 Napoleon, in pursuance of his scheme for raising a Syrian rebellion against Turkish domination, appeared before Acre, but after a siege of two months (March–May) was repulsed by the Turks, aided by Sir Sidney Smith and a force of British sailors. Having lost his siege cannons to Smith, Napoleon attempted to lay siege to the walled city defended by Ottoman troops on 20 March 1799, using only his infantry and small-calibre cannons, a strategy which failed, leading to his retreat two months later on 21 May.
Jezzar was succeeded on his death by his son Suleiman Pasha, under whose milder rule the town advanced in prosperity till his death in 1819. After his death, Haim Farhi, who was his adviser, paid a huge sum in bribes to assure that Abdullah Pasha (son of Ali Pasha, the deputy of Suleiman Pasha), whom he had known from youth, will be appointed as ruler. Abdullah Pasha ruled Acre until 1831, when Ibrahim Pasha besieged and reduced the town and destroyed its buildings. During the Oriental Crisis of 1840 it was bombarded on 4 November 1840 by the allied British, Austrian and French squadrons, and in the following year restored to Turkish rule. It regained some of former prosperity after linking with Hejaz Railway by a branch line from Haifa in 1913. It was a sanjak centre (Sanjak of Acre) in Beyrut Eyalet until English occupation in 23 September 1918 during World War I.
British Mandate of Palestine.
At the beginning of the Mandate period, in the 1922 census of Palestine, Acre had 6,420 residents: 4,883 of whom were Muslim; 1,344 Christian; 102 Baha'i; 78 Jewish and 13 Druze. The British Mandate government reconstructed Acre, and its economic situation improved. The 1931 census counted 7,897 people in Acre, 6076 Muslims, 1523 Christians, 237 Jews, 51 Baha'i and 10 Druse. In 1946 Acre's population numbered around 13,000.
Acre's fort was converted into a jail, where members of the Jewish underground were held during their struggle against the British, among them Zeev Jabotinski, Shlomo ben Yossef, and Dov Grunner. Grunner and ben Yossef were executed there. Other Jewish inmates were freed by members of the Irgun, who broke into the jail on 4 May 1947 and succeeded in releasing Jewish underground movement activists. Over 200 Arab inmates also escaped.
In the 1947 UN Partition Plan, Acre was designated part of a future Arab state. Before the 1948 Arab-Israeli War broke out, Acre's Arabs attacked neighbouring Jewish settlements and Jewish transportation; in March 1948 42 Jews were killed on an attack on a convoy north of the city, whilst on 18 March four Jewish employees of the electricity company and five British soldiers protecting them were killed whilst travelling to repair damaged lines near the city.
During the 1948 War, Acre was besieged by Israeli forces. A typhoid fever outbreak occurred in Acre at this time. According to the Red Cross archives, an emergency meeting held at the Lebanese Red Cross hospital in Acre concluded that the infection was water borne, not due to crowded or unhygienic conditions. Brigadier Beveridge, chief of the British medical services, Colonel Bonnet of the British army, and delegates of Red Cross were present in this meeting. Beveridge
proclaimed at the time that "Nothing like that ever happened in Palestine". According to Ilan Pappé, even the guarded language of Red Cross reports points to outside poisoning as the sole explanation of the outbreak.
State of Israel and Baha'i Faith.
Acre was captured by Israel on 17 May 1948, displacing about three-quarters of the Arab population of the city (13,510 of 17,395). Throughout the 1950s many Jewish neighbourhoods were established at the northern and eastern parts of the city, as it became a development town, designated to absorb numerous Jewish immigrants, largely Jews from Morocco. The old city of Akko remained largely Arab Muslim (including several Bedouin families), with Arab Christian neighbourhood in close proximity. The city also attracted Bahá'í worshippers, some of whom became permanent residents in the city, where the Bahá'í Mansion of Bahjí is located. Baha'is have since commemorated various events that have occured in the city, including the imprisonment of Bahaullah.
In the 1990s the city absorbed thousands of Jews, who immigrated from the Soviet Union and later from Russia and Ukraine. Within several years, however, the population balance between Jews and Arabs shifted backwards, as northern neighbourhoods were abandoned by many of its Jewish residents in favour of new housing projects in nearby Nahariya, while many Muslim Arabs moved in (largely coming from nearby Arab villages). Nevertheless, the city still has a clear Jewish majority; in 2011 the population of 46,000 included 30,000 Jews and 14,000 Arabs.
Ethnic tensions erupted in the city on 8 October 2008 after an Arab citizen drove through a predominantly Jewish neighbourhood during Yom Kippur, leading to five days of violence between Arabs and Jews.
In 2009, the population of Acre reached 46,300. The current mayor Shimon Lankri was re-elected in 2011.
Demography.
According to the Israeli Central Bureau of Statistics, there are 46,300 citizens in Acre. Acre's population is mixed with Jews and Arabs. Most Arabs are Muslims and Christians, with small minorities of Druze and Baha'i. Jews are 67.1% of the city's population, Muslim Arabs are 25.3% of the city's population, Christian Arabs are 2.4% of the city's population and other citizens make up 5.2% of the city's population.
According to the Israeli Central Office of Statistics, 95% of the residents in the Old City are Arab. Only about 15% percent of the current Arab population in the city descends from families who lived there before 1948. In 1999, there were 22 schools in Acre with an enrollment of 15,000 children.
Transportation.
The Acre central bus station, served by Egged, offers city and inter-city bus routes to destinations all over Israel. The city is also served by the Acre Railway Station.
Education and culture.
The Sir Charles Clore Jewish-Arab Community Centre in the Kiryat Wolfson neighbourhood runs youth clubs and programs for Jewish and Arab children. In 1990, Mohammed Faheli, an Arab resident of Acre, founded the Acre Jewish-Arab association, which originally operated out of two bomb shelters. In 1993, Dame Vivien Duffield of the Clore Foundation donated funds for a new building. Among the programs offered is Peace Child Israel, which employs theatre and the arts to teach coexistence. The participants, Jews and Arabs, spend two months studying conflict resolution and then work together to produce an original theatrical performance that addresses the issues they have explored. Another program is Patriots of Acre, a community responsibility and youth tourism program that teaches children to become ambassadors for their city. In the summer, the centre runs an Arab-Jewish summer camp for 120 disadvantaged children aged 5–11. Some 1,000 children take part in the Acre Centre's youth club and youth programming every week. Adult education programs have been developed for Arab women interested in completing their high school education and acquiring computer skills to prepare for joining the workforce. The centre also offers parenting courses, and music and dance classes.
The Acre Festival of Alternative Israeli Theatre is an annual event that takes place in October, coinciding with the holiday of Sukkot. The festival, inaugurated in 1979, provides a forum for non-conventional theatre, attracting local and overseas theatre companies. Theatre performances by Jewish and Arab producers are staged at indoor and outdoor venues around the city.
Sports.
The city's football team Hapoel Ironi Acre is a member of the Israeli Premier League, the top tier of Israeli football. They play in the Acre Municipal Stadium which was opened in September 2011. At the end of the 2008–09 season the club finished in the top five and was promoted to the top tier for a second time, after an absence of 31 years. 
In the past the city was also home to Maccabi Acre. however, the club was relocated to nearby Kiryat Ata and was renamed Maccabi Ironi Kiryat Ata. 
Other current active clubs are Ahi Acre and the newly formed Maccabi Ironi Acre, both playing in Liga Bet. Both club also host their matches in the Acre Municipal Stadium. 
Landmarks.
Acre's Old City has been designated by UNESCO as a World Heritage Site. Since the 1990s, large-scale archaeological excavations have been undertaken and efforts are being made to preserve ancient sites. In 2009, renovations were planned for Khan al-Umdan, the "Inn of the Columns," the largest of several Ottoman inns still standing in Acre. It was built near the port at the end of the 18th century by Ahmed Pasha al-Jazzar. Merchants who arrived at the port would unload their wares on the first floor and sleep in lodgings on the second floor. In 1906, a clock tower was added over the main entrance marking the 25th anniversary of the reign of the Turkish sultan, Abdul Hamid II.
City walls.
In 1750, Daher El-Omar, the ruler of Acre, utilized the remnants of the Crusader walls as a foundation for his walls. Two gates were set in the wall, the "land gate" in the eastern wall, and the "sea gate" in the southern wall. The walls were reinforced between 1775 and 1799 by Jezzar Pasha and survived Napoleon's siege. The wall was thin: its height was between 10 m and 13 m and its thickness only 1.5 m.
A heavy land defense wall was built north and east to the city in 1800–1814 by Jezzar Pasha and his Jewish advisor Haim Farhi. It consists of a modern counter artillery fortification which includes a thick defensive wall, a dry moat, cannon outposts and three "burges" (large defensive towers). Since then, no major modifications have taken place. The sea wall, which remains mostly complete, is the original El-Omar's wall that was reinforced by Jezzar Pasha. In 1910 two additional gates were set in the walls, one in the northern wall and one in the north-western corner of the city. In 1912 the Acre lighthouse was built on the south-western corner of the walls. 
Jezzar Pasha Mosque.
The Mosque of Jezzar Pasha was built in 1781. Jezzar Pasha and his successor Suleiman Pasha, are both buried in a small graveyard adjacent to the mosque. In a shrine on the second level of the mosque, a single hair from the prophet Mohammed's beard is kept and shown on special ceremonial occasions.
Citadel of Acre.
The current building which constitutes the citadel of Acre is an Ottoman fortification, built on the foundation of the Hospitallerian citadel. The citadel was part of the city's defensive formation, reinforcing the northern wall. During the 20th century the citadel was used mainly as a prison and as the site for a gallows. During the British mandate period, activists of Jewish Zionist resistance movements were held prisoner there; some were executed there.
Hamam al-Basha.
Built in 1795 by Jezzar Pasha, Acre's hammam has a series of hot rooms and a hexagonal steam room with a marble fountain. It was used by the Irgun as a bridge to break into the citadel's prison. The bathhouse kept functioning until 1950.
Hospitaller refectory.
Under the citadel and prison of Acre, archaeological excavations revealed a complex of halls, which was built and used by the Hospitallers Knights. This complex was a part of the Hospitallers' citadel, which was included in the northern defences of Acre. The complex includes six semi-joined halls, one recently excavated large hall, a dungeon, a refectory (dining room) and remains of a Gothic church.
Other medieval sites.
Other Medieval European remains include the Church of Saint George and adjacent houses at the Genovese Square (called Kikar ha-Genovezim or Kikar Genoa in Hebrew). There were also residential quarters and marketplaces run by merchants from Pisa and Amalfi in Crusader and medieval Acre. 
Bahá'í holy places.
There are many Bahá'í holy places in and around Acre. They originate from Bahá'u'lláh's imprisonment in the Citadel during Ottoman Rule. The final years of Bahá'u'lláh's life were spent in the Mansion of Bahjí, just outside Acre, even though he was still formally a prisoner of the Ottoman Empire. Bahá'u'lláh died on 29 May 1892 in Bahjí, and his shrine is the most holy place for Bahá'ís — their Qiblih, the location they face when saying their daily prayers. It contains the remains of Bahá'u'lláh and is near the spot where he died in the Mansion of Bahjí. Other Bahá'í sites in Acre are the House of `Abbúd (where Bahá'u'lláh and his family resided) and the House of `Abdu'lláh Páshá (where later 'Abdu'l-Bahá resided with his family), and the Garden of Ridván where he spent the end of his life. In 2008, the Bahai holy places in Acre and Haifa were added to the UNESCO World Heritage List.
Archaeology.
In 2012, archaeologists excavating at the foot of the city's southern seawall found a quay and other evidence of a 2,300-year old port. Mooring stones weighing 250-300 kilograms each were unearthed at the edge of a 5-meter long stone platform chiseled in Phoenician-style, thought to be an installation that helped raise military vessels from the water onto the shore.
Crusades.
Under the citadel and prison of Acre, archaeological excavations revealed a complex of halls, which was built and used by the Hospitallers Knights. This complex was a part of the Hospitallers' citadel, which was combined in the northern wall of Acre. The complex includes six semi-joined halls, one recently excavated large hall, a dungeon, a dining room and remains of an ancient Gothic church. Medieval European remains include the Church of Saint George and adjacent houses at the Genovese Square (called Kikar ha-Genovezim or Kikar Genoa in Hebrew). There were also residential quarters and marketplaces run by merchants from Pisa and Amalfi in Crusader and medieval Acre. 
International relations.
Acre is twinned with:
Notable people associated with Acre-residents, travellers, etc..
Apart from those mentioned in the article (Alexander the Great, St Paul, Richard the Lionheart, Napoleon):
Bibliography.
</dl>

</doc>
<doc id="55691" url="http://en.wikipedia.org/wiki?curid=55691" title="CDA">
CDA

CDA or Cda may refer to:

</doc>
<doc id="55693" url="http://en.wikipedia.org/wiki?curid=55693" title="Turquoise">
Turquoise

Turquoise is an opaque, blue-to-green mineral that is a hydrous phosphate of copper and aluminium, with the chemical formula CuAl6(PO4)4(OH)8·4H2O. It is rare and valuable in finer grades and has been prized as a gem and ornamental stone for thousands of years owing to its unique hue. In recent times, turquoise, like most other opaque gems, has been devalued by the introduction of treatments, imitations, and synthetics onto the market.
The substance has been known by many names, but the word "turquoise", which dates to the 16th century, is derived from an Old French word for "Turkish", because the mineral was first brought to Europe from Turkey, from the mines in historical Khorasan Province of Iran. Pliny the Elder referred to the mineral as "callais," the Iranians named it "phirouzeh" and the Aztecs knew it as "Teoxihuitl".
Properties of turquoise.
Even the finest of turquoise is fracturable, reaching a maximum hardness of just under 6, or slightly more than window glass. Characteristically a cryptocrystalline mineral, turquoise almost never forms single crystals and all of its properties are highly variable. Its crystal system is proven to be triclinic via X-ray diffraction testing. With lower hardness comes lower specific gravity (2.60–2.90) and greater porosity: These properties are dependent on grain size. The lustre of turquoise is typically waxy to subvitreous, and transparency is usually opaque, but may be semitranslucent in thin sections. Colour is as variable as the mineral's other properties, ranging from white to a powder blue to a sky blue, and from a blue-green to a yellowish green. The blue is attributed to idiochromatic copper while the green may be the result of either iron impurities (replacing aluminium) or dehydration.
The refractive index (as measured by sodium light, 589.3 nm) of turquoise is approximately 1.61 or 1.62; this is a mean value seen as a single reading on a gemmological refractometer, owing to the almost invariably polycrystalline nature of turquoise. A reading of 1.61–1.65 (birefringence 0.040, biaxial positive) has been taken from rare single crystals. An absorption spectrum may also be obtained with a hand-held spectroscope, revealing a line at 432 nanometres and a weak band at 460 nanometres (this is best seen with strong reflected light). Under longwave ultraviolet light, turquoise may occasionally fluoresce green, yellow or bright blue; it is inert under shortwave ultraviolet and X-rays.
Turquoise is insoluble in all but heated hydrochloric acid. Its streak is a pale bluish white and its fracture is conchoidal, leaving a waxy lustre. Despite its low hardness relative to other gems, turquoise takes a good polish. Turquoise may also be peppered with flecks of pyrite or interspersed with dark, spidery limonite veining.
Formation.
As a secondary mineral, turquoise apparently forms by the action of percolating acidic aqueous solutions during the weathering and oxidation of pre-existing minerals. For example, the copper may come from primary copper sulfides such as chalcopyrite or from the secondary carbonates malachite or azurite; the aluminium may derive from feldspar; and the phosphorus from apatite. Climate factors appear to play an important role as turquoise is typically found in arid regions, filling or encrusting cavities and fractures in typically highly altered volcanic rocks, often with associated limonite and other iron oxides. In the American southwest turquoise is almost invariably associated with the weathering products of copper sulfide deposits in or around potassium feldspar bearing porphyritic intrusives. In some occurrences alunite, potassium aluminium sulfate, is a prominent secondary mineral. Typically turquoise mineralization is restricted to a relatively shallow
depth of less than 20 m, although it does occur along deeper fracture zones where secondary solutions have greater penetration or the depth to the water table is greater.
Although the features of turquoise occurrences are consistent with a secondary or supergene origin, some sources refer to a hypogene origin. The "hypogene" hypothesis holds that the aqueous solutions originate at significant depth, from hydrothermal processes. Initially at high temperature, these solutions rise upward to surface layers, interacting with, and leaching essential elements from pre-existing minerals in the process. As the solutions cool, turquoise precipitates, lining cavities and fractures within the surrounding rock. This hypogene process is applicable to the original copper sulfide deposition; however, it is difficult to account for the many features of turquoise occurrences by a hypogene process. That said, there are reports of two phase fluid inclusions within turquoise grains that give elevated homogenization temperatures of 90 to 190 °C that require explanation.
Turquoise is nearly always cryptocrystalline and massive and assumes no definite external shape. Crystals, even at the microscopic scale, are exceedingly rare. Typically the form is vein or fracture filling, nodular, or botryoidal in habit. Stalactite forms have been reported. Turquoise may also pseudomorphously replace feldspar, apatite, other minerals, or even fossils. Odontolite is fossil bone or ivory that has been traditionally thought to have been altered by turquoise or similar phosphate minerals such as the iron phosphate vivianite. Intergrowth with other secondary copper minerals such as chrysocolla is also common.
Occurrence.
Turquoise was among the first gems to be mined, and while many historic sites have been depleted, some are still worked to this day. These are all small-scale, often seasonal operations, owing to the limited scope and remoteness of the deposits. Most are worked by hand with little or no mechanization. However, turquoise is often recovered as a byproduct of large-scale copper mining operations, especially in the United States.
Iran.
For at least 2,000 years, Iran, known before as Persia in the West, has remained an important source of turquoise which was named by Iranians initially "pirouzeh" meaning "victory" and later after Arab invasion "firouzeh". In Iranian architecture, the blue turquoise was used to cover the domes of the Iranian palaces because its intense blue colour was also a symbol of heaven on earth. 
This deposit, which is blue naturally, and turns green when heated due to dehydration, is restricted to a mine-riddled region in Nishapur, the 2012 m mountain peak of Ali-mersai, which is tens of kilometers from Mashhad, the capital of Khorasan Province, Iran. A weathered and broken trachyte is host to the turquoise, which is found both "in situ" between layers of limonite and sandstone, and amongst the scree at the mountain's base. These workings, together with those of the Sinai Peninsula, are the oldest known.
Sinai.
Since at least the First Dynasty (3000 BCE), and possibly before then, turquoise was used by the Egyptians and was mined by them in the Sinai Peninsula, called "Country of Turquoise" by the native Monitu. There are six mines in the region, all on the southwest coast of the peninsula, covering an area of some 650 km2. The two most important of these mines, from a historic perspective, are Serabit el-Khadim and Wadi Maghareh, believed to be among the oldest of known mines. The former mine is situated about 4 kilometres from an ancient temple dedicated to Hathor.
The turquoise is found in sandstone that is, or was originally, overlain by basalt. Copper and iron workings are present in the area. Large-scale turquoise mining is not profitable today, but the deposits are sporadically quarried by Bedouin peoples using homemade gunpowder. In the rainy winter months, miners face a risk from flash flooding; even in the dry season, death from the collapse of the haphazardly exploited sandstone mine walls is not unheard of. The colour of Sinai material is typically greener than Iranian material, but is thought to be stable and fairly durable. Often referred to as "Egyptian" turquoise, Sinai material is typically the most translucent, and under magnification its surface structure is revealed to be peppered with dark blue discs not seen in material from other localities.
United States.
The Southwest United States is a significant source of turquoise; Arizona, California (San Bernardino, Imperial, Inyo counties), Colorado (Conejos, El Paso, Lake, Saguache counties), New Mexico (Eddy, Grant, Otero, Santa Fe counties) Nevada (Clark, Elko, Esmeralda County, Eureka, Lander, Mineral County and Nye counties) are (or were) especially rich. The deposits of California and New Mexico were mined by pre-Columbian Native Americans using stone tools, some local and some from as far away as central Mexico. Cerrillos, New Mexico is thought to be the location of the oldest mines; prior to the 1920s, the state was the country's largest producer; it is more or less exhausted today. Only one mine in California, located at Apache Canyon, operates at a commercial capacity today.
The turquoise occurs as vein or seam fillings, and as compact nuggets; these are mostly small in size. While quite fine material is sometimes found—rivalling Iranian material in both colour and durability—most American turquoise is of a low grade (called "chalk turquoise"); high iron levels mean greens and yellows predominate, and a typically friable consistency in the turquoise's untreated state precludes use in jewellery .
Arizona is currently the most important producer of turquoise by value. Several mines exist in the state, two of them famous for their unique colour and quality and considered the best in the industry: the Sleeping Beauty Mine in Globe ceased turquoise mining in August of 2012. The mine chose to send all ore to the crusher and to concentrate on copper production due to the rising price of copper on the world market. The price of natural untreated Sleeping Beauty turquoise has risen dramatically since the mines closing. The Kingman Mine as of 2015 is still operates alongside a copper mine outside of the city. Other mines include the Blue Bird mine, Castle Dome, and Ithaca Peak, but they are mostly inactive due to the high cost of operations and federal regulations The Felps Dodge Lavender Pit mine at Bizbee AZ. ceased operations in 1974 and never had a turquoise contractor. All Bizzbee turquoise was "lunch pail" mined. It came out of the copper ore mine in miners lunch pails. Morrenci and Turquoise Peak are either inactive or depleted.
Nevada is the country's other major producer, with more than 120 mines which have yielded significant quantities of turquoise. Unlike elsewhere in the US, most Nevada mines have been worked primarily for their gem turquoise and very little has been recovered as a byproduct of other mining operations. Nevada turquoise is found as nuggets, fracture fillings and in breccias as the cement filling interstices between fragments. Because of the geology of the Nevada deposits, a majority of the material produced is hard and dense, being of sufficient quality that no treatment or enhancement is required. While nearly every county in the state has yielded some turquoise, the chief producers are in Lander and Esmeralda Counties. Most of the turquoise deposits in Nevada occur along a wide belt of tectonic activity that coincides with the state's zone of thrust faulting. It strikes about N15°E and extends from the northern part of Elko County, southward down to the California border southwest of Tonopah. Nevada has produced a wide diversity of colours and mixes of different matrix patterns, with turquoise from Nevada coming in various shades of blue, blue-green, and green. Some of this unusually coloured turquoise may contain significant zinc and iron, which is the cause of the beautiful bright green to yellow-green shades. Some of the green to green yellow shades may actually be variscite or faustite, which are secondary phosphate minerals similar in appearance to turquoise. A significant portion of the Nevada material is also noted for its often attractive brown or black limonite veining, producing what is called "spiderweb matrix". While a number of the Nevada deposits were first worked by Native Americans, the total Nevada turquoise production since the 1870s has been estimated at more than 600 tons, including nearly 400 tons from the Carico Lake mine. In spite of increased costs, small scale mining operations continue at a number of turquoise properties in Nevada, including the Godber, Orvil Jack and Carico Lake Mines in Lander County, the Pilot Mountain Mine in Mineral County, and several properties in the Royston and Candelaria areas of Esmerelda County.
In 1912, the first deposit of distinct, single-crystal turquoise was discovered in Lynch Station, Campbell County, Virginia. The crystals, forming a druse over the mother rock, are very small; 1 mm (0.04 in) is considered large. Until the 1980s Virginia was widely thought to be the only source of distinct crystals; there are now at least 27 other localities.
In an attempt to recoup profits and meet demand, some American turquoise is treated or "enhanced" to a certain degree. These treatments include innocuous waxing and more controversial procedures, such as dyeing and impregnation (see Treatments). There are however, some American mines which produce materials of high enough quality that no treatment or alterations are required. Any such treatments which have been performed should be disclosed to the buyer on sale of the material.
Other sources.
China has been a minor source of turquoise for 3,000 years or more. Gem-quality material, in the form of compact nodules, is found in the fractured, silicified limestone of Yunxian and Zhushan, Hubei province. Additionally, Marco Polo reported turquoise found in present-day Sichuan. Most Chinese material is exported, but a few carvings worked in a manner similar to jade exist. In Tibet, gem-quality deposits purportedly exist in the mountains of Derge and Nagari-Khorsum in the east and west of the region respectively.
Other notable localities include: Afghanistan; Australia (Victoria and Queensland); north India; northern Chile (Chuquicamata); Cornwall; Saxony; Silesia; and Turkestan.
History of use.
The pastel shades of turquoise have endeared it to many great cultures of antiquity: it has adorned the rulers of Ancient Egypt, the Aztecs (and possibly other Pre-Columbian Mesoamericans), Persia, Mesopotamia, the Indus Valley, and to some extent in ancient China since at least the Shang Dynasty. Despite being one of the oldest gems, probably first introduced to Europe (through Turkey) with other Silk Road novelties, turquoise did not become important as an ornamental stone in the West until the 14th century, following a decline in the Roman Catholic Church's influence which allowed the use of turquoise in secular jewellery. It was apparently unknown in India until the Mughal period, and unknown in Japan until the 18th century. A common belief shared by many of these civilizations held that turquoise possessed certain prophylactic qualities; it was thought to change colour with the wearer's health and protect him or her from untoward forces.
The Aztecs inlaid turquoise, together with gold, quartz, malachite, jet, jade, coral, and shells, into provocative (and presumably ceremonial) mosaic objects such as masks (some with a human skull as their base), knives, and shields. Natural resins, bitumen and wax were used to bond the turquoise to the objects' base material; this was usually wood, but bone and shell were also used. Like the Aztecs, the Pueblo, Navajo and Apache tribes cherished turquoise for its amuletic use; the latter tribe believe the stone to afford the archer dead aim. Among these peoples turquoise was used in mosaic inlay, in sculptural works, and was fashioned into toroidal beads and freeform pendants. The Ancestral Puebloans (Anasazi) of the Chaco Canyon and surrounding region are believed to have prospered greatly from their production and trading of turquoise objects. The distinctive silver jewellery produced by the Navajo and other Southwestern Native American tribes today is a rather modern development, thought to date from circa 1880 as a result of European influences.
In Persia, turquoise was the "de facto" national stone for millennia, extensively used to decorate objects (from turbans to bridles), mosques, and other important buildings both inside and out, such as the Medresseh-I Shah Husein Mosque of Isfahan. The Persian style and use of turquoise was later brought to India following the establishment of the Mughal Empire there, its influence seen in high purity gold jewellery (together with ruby and diamond) and in such buildings as the Taj Mahal. Persian turquoise was often engraved with devotional words in Arabic script which was then inlaid with gold.
Cabochons of imported turquoise, along with coral, was (and still is) used extensively in the silver and gold jewellery of Tibet and Mongolia, where a greener hue is said to be preferred. Most of the pieces made today, with turquoise usually roughly polished into irregular cabochons set simply in silver, are meant for inexpensive export to Western markets and are probably not accurate representations of the original style.
The Egyptian use of turquoise stretches back as far as the First Dynasty and possibly earlier; however, probably the most well-known pieces incorporating the gem are those recovered from Tutankhamun's tomb, most notably the Pharaoh's iconic burial mask which was liberally inlaid with the stone. It also adorned rings and great sweeping necklaces called "pectorals". Set in gold, the gem was fashioned into beads, used as inlay, and often carved in a scarab motif, accompanied by carnelian, lapis lazuli, and in later pieces, coloured glass. Turquoise, associated with the goddess Hathor, was so liked by the Ancient Egyptians that it became (arguably) the first gemstone to be imitated, the fair structure created by an artificial glazed ceramic product known as faience.
The French conducted archaeological excavations of Egypt from the mid-19th century through the early 20th. These excavations, including that of Tutankhamun's tomb, created great public interest in the western world, subsequently influencing jewellery, architecture, and art of the time. Turquoise, already favoured for its pastel shades since c. 1810, was a staple of Egyptian Revival pieces. In contemporary Western use, turquoise is most often encountered cut "en cabochon" in silver rings, bracelets, often in the Native American style, or as tumbled or roughly hewn beads in chunky necklaces. Lesser material may be carved into fetishes, such as those crafted by the Zuni. While strong sky blues remain superior in value, mottled green and yellowish material is popular with artisans. In Western culture, turquoise is also the traditional birthstone for those born in the month of December.
The turquoise is also a stone in the Jewish High Priest's breastplate, described in Exodus 28.
Culture.
In many cultures of the Old and New Worlds, this gemstone has been esteemed for thousands of years as a holy stone, a bringer of good fortune or a talisman. It really does have the right to be called a 'gemstone of the peoples'. The oldest evidence for this claim was found in Ancient Egypt, where grave furnishings with turquoise inlay were discovered, dating from approximately 3000 BCE. In the ancient Persian Empire, the sky-blue gemstones were earlier worn round the neck or wrist as protection against unnatural death. If they changed colour, the wearer was thought to have reason to fear the approach of doom. Meanwhile, it has been discovered that the turquoise certainly can change colour, but that this is not necessarily a sign of impending danger. The change can be caused by the light, or by a chemical reaction brought about by cosmetics, dust or the acidity of the skin.
Imitations.
The Egyptians were the first to produce an artificial imitation of turquoise, in the glazed earthenware product faience. Later glass and enamel were also used, and in modern times more sophisticated porcelain, plastics, and various assembled, pressed, bonded, and sintered products (composed of various copper and aluminium compounds) have been developed: examples of the latter include "Viennese turquoise", made from precipitated aluminium phosphate coloured by copper oleate; and "neolith", a mixture of bayerite and copper phosphate. Most of these products differ markedly from natural turquoise in both physical and chemical properties, but in 1972 Pierre Gilson introduced one fairly close to a true synthetic (it does differ in chemical composition owing to a binder used, meaning it is best described as a simulant rather than a synthetic). Gilson turquoise is made in both a uniform colour and with black "spiderweb matrix"
veining not unlike the natural Nevada material.
The most common imitation of turquoise encountered today is dyed howlite and magnesite, both white in their natural states, and the former also having natural (and convincing) black veining similar to that of turquoise. Dyed chalcedony, jasper, and marble is less common, and much less convincing. Other natural materials occasionally confused with or used in lieu of turquoise include: variscite and faustite; chrysocolla (especially when impregnating quartz); lazulite; smithsonite; hemimorphite; wardite; and a fossil bone or tooth called odontolite or "bone turquoise", coloured blue naturally by the mineral vivianite. While rarely encountered today, odontolite was once mined in large quantities—specifically for its use as a substitute for turquoise—in southern France.
These fakes are detected by gemologists using a number of tests, relying primarily on non-destructive, close examination of surface structure under magnification; a featureless, pale blue background peppered by flecks or spots of whitish material is the typical surface appearance of natural turquoise, while manufactured imitations will appear radically different in both colour (usually a uniform dark blue) and texture (usually granular or sugary). Glass and plastic will have a much greater translucency, with bubbles or flow lines often visible just below the surface. Staining between grain boundaries may be visible in dyed imitations.
Some destructive tests may, however, be necessary; for example, the application of diluted hydrochloric acid will cause the carbonates odontolite and magnesite to effervesce and howlite to turn green, while a heated probe may give rise to the pungent smell so indicative of plastic. Differences in specific gravity, refractive index, light absorption (as evident in a material's absorption spectrum), and other physical and optical properties are also considered as means of separation.
Treatments.
Turquoise is treated to enhance both its colour and durability (i.e., increased hardness and decreased porosity). As is so often the case with any precious stones, full disclosure about treatment is frequently not given. It is therefore left to gemologists to detect these treatments in suspect stones using a variety of testing methods—some of which are necessarily destructive. For example, the use of a heated probe applied to an inconspicuous spot will reveal oil, wax, or plastic treatment with certainty.
Waxing and oiling.
Historically, light waxing and oiling were the first treatments used in ancient times, providing a wetting effect, thereby enhancing the colour and lustre. This treatment is more or less acceptable by tradition, especially because treated turquoise is usually of a higher grade to begin with. Oiled and waxed stones are prone to "sweating" under even gentle heat or if exposed to too much sun, and they may develop a white surface film or bloom over time. (With some skill, oil and wax treatments can be restored.)
Stabilization.
Material treated with plastic or water glass is termed "bonded" or "stabilized" turquoise. This process consists of pressure impregnation of otherwise unsaleable chalky American material by epoxy and plastics (such as polystyrene) and water glass (sodium silicate) to produce a wetting effect and improve durability. Plastic and water glass treatments are far more permanent and stable than waxing and oiling, and can be applied to material too chemically or physically unstable for oil or wax to provide sufficient improvement. Conversely, stabilization and bonding are rejected by some as too radical an alteration.
The epoxy binding technique was first developed in the 1950s and has been attributed to Colbaugh Processing of Arizona, a company that still operates today. The majority of American material is now treated in this manner although it is a costly process requiring many months to complete.
Dyeing.
The use of Prussian blue and other dyes (often in conjunction with bonding treatments) to "enhance"—that is, make uniform or completely change—colour is regarded as fraudulent by some purists, especially since some dyes may fade or rub off on the wearer. Dyes have also been used to darken the veins of turquoise.
Reconstitution.
Perhaps the most extreme of treatments is "reconstitution", wherein fragments of fine turquoise material, too small to be used individually, are powdered and then bonded with resin to form a solid mass. Very often the material sold as "reconstituted" turquoise is artificial, with little or no natural stone, made entirely from resins and dyes. In the trade "reconstituted" turquoise is often called "block" turquoise or simply "block."
Backing.
Since finer turquoise is often found as thin seams, it may be glued to a base of stronger foreign material as a means of reinforcement. These stones are termed "backed," and it is standard practice that all thinly cut turquoise in the Southwestern United States is backed. Native indigenous peoples of this region, because of their considerable use and wearing of turquoise, have found that backing increases the durability of thinly cut slabs and cabochons of turquoise. They observe that if the stone is not backed it will often crack. Early backing materials included the casings of old model T batteries, old phonograph records, and more recently epoxy steel resins. Backing of turquoise is not widely known outside of the Native American and Southwestern United States jewellery trade. Backing does not diminish the value of high quality turquoise, and indeed the process is expected for most thinly cut American commercial gemstones.
Valuation and care.
Hardness and richness of colour are two of the major factors in determining the value of turquoise; while colour is a matter of individual taste, generally speaking, the most desirable is a strong sky to "robin's egg" blue (in reference to the eggs of the American Robin). Whatever the colour, turquoise should not be excessively soft or chalky; even if treated, such lesser material (to which most turquoise belongs) is liable to fade or discolour over time and will not hold up to normal use in jewellery.
The mother rock or "matrix" in which turquoise is found can often be seen as splotches or a network of brown or black veins running through the stone in a netted pattern; this veining may add value to the stone if the result is complementary, but such a result is uncommon. Such material is sometimes described as "spiderweb matrix"; it is most valued in the Southwest United States and Far East, but is not highly appreciated in the Near East where unblemished and vein-free material is ideal (regardless of how complementary the veining may be). Uniformity of colour is desired, and in finished pieces the quality of workmanship is also a factor; this includes the quality of the polish and the symmetry of the stone. Calibrated stones—that is, stones adhering to standard jewellery setting measurements—may also be more sought after. Like coral and other opaque gems, turquoise is commonly sold at a price according to its physical size in millimetres rather than weight.
Turquoise is treated in many different ways, some more permanent and radical than others. Controversy exists as to whether some of these treatments should be acceptable, but one can be more or less forgiven universally: This is the "light" waxing or oiling applied to most gem turquoise to improve its colour and lustre; if the material is of high quality to begin with, very little of the wax or oil is absorbed and the turquoise therefore does not "rely" on this impermanent treatment for its beauty. All other factors being equal, untreated turquoise will always command a higher price. Bonded and "reconstituted" material is worth considerably less.
Being a phosphate mineral, turquoise is inherently fragile and sensitive to solvents; perfume and other cosmetics will attack the finish and may alter the colour of turquoise gems, as will skin oils, as will most commercial jewellery cleaning fluids. Prolonged exposure to direct sunlight may also discolour or dehydrate turquoise. Care should therefore be taken when wearing such jewels: cosmetics, including sunscreen and hair spray, should be applied before putting on turquoise jewellery, and they should not be worn to a beach or other sun-bathed environment. After use, turquoise should be gently cleaned with a soft cloth to avoid a buildup of residue, and should be stored in its own container to avoid scratching by harder gems. Turquoise can also be adversely affected if stored in an airtight container.
Further reading.
</dl>

</doc>
<doc id="55695" url="http://en.wikipedia.org/wiki?curid=55695" title="Mirage">
Mirage

A mirage is a naturally occurring optical phenomenon in which light rays are bent to produce a displaced image of distant objects or the sky. The word comes to English via the French "mirage", from the Latin "mirari", meaning "to look at, to wonder at". This is the same root as for "mirror" and "to admire".
In contrast to a hallucination, a mirage is a real optical phenomenon that can be captured on camera, since light rays are actually refracted to form the false image at the observer's location. What the image appears to represent, however, is determined by the interpretive faculties of the human mind. For example, inferior images on land are very easily mistaken for the reflections from a small body of water.
Mirages can be categorized as "inferior" (meaning lower), "superior" (meaning higher) and "Fata Morgana", one kind of superior mirage consisting of a series of unusually elaborate, vertically stacked images, which form one rapidly changing mirage.
Cause.
Cold air is denser than warm air and, therefore, has a greater refractive index. As light travels at a shallow angle along a boundary between air of different temperature, the light rays bend towards the colder air. If the air near the ground is warmer than that higher up, the light ray bends upward, effectively being totally reflected just above the ground.
Once the rays reach the viewer’s eye, the visual cortex interprets it as if it traces back along a perfectly straight "line of sight". However, this line is at a tangent to the path the ray takes at the point it reaches the eye. The result is that an "inferior image" of the sky above appears on the ground. The viewer may incorrectly interpret this sight as water that is reflecting the sky, which is, to the brain, a more reasonable and common occurrence.
In the case where the air near the ground is cooler than that higher up, the light rays curve downward, producing a "superior image".
The "resting" state of the Earth's atmosphere has a vertical gradient of about -1° Celsius per 100 metres of altitude. (The value is negative because it gets colder as altitude increases.) For a mirage to happen, the temperature gradient has to be much greater than that. According to Minnaert, the magnitude of the gradient needs to be at least 2°C per metre, and the mirage does not get strong until the magnitude reaches 4° or 5°C per metre. These conditions do occur with strong heating at ground level, for example when the sun has been shining on sand or asphalt, commonly generating an inferior image.
Inferior mirage.
For exhausted travelers in the desert, an inferior mirage may appear to be a lake of water in the distance. An inferior mirage is called "inferior" because the mirage is located under the real object. The real object in an inferior mirage is the (blue) sky or any distant (therefore bluish) object in that same direction. The mirage causes the observer to see a bright and bluish patch on the ground in the distance.
Light rays coming from a particular distant object all travel through nearly the same air layers and all are bent over about the same amount. Therefore, rays coming from the top of the object will arrive lower than those from the bottom. The image usually is upside down, enhancing the illusion that the sky image seen in the distance is really a water or oil puddle acting as a mirror.
Inferior images are not stable. Hot air rises, and cooler air (being more dense) descends, so the layers will mix, giving rise to turbulence. The image will be distorted accordingly. It may be vibrating; it may be vertically extended (towering) or horizontally extended (stooping). If there are several temperature layers, several mirages may mix, perhaps causing double images. In any case, mirages are usually not larger than about half a degree high (same apparent size as the sun and moon) and from objects only a few kilometers away.
Heat haze.
Heat haze, also called heat shimmer, refers to the inferior mirage experienced when viewing objects through a layer of heated air; for example, viewing objects across hot asphalt or through the exhaust gases produced by jet engines. When appearing on roads due to the hot asphalt, it is often referred to as a highway mirage.
Convection causes the temperature of the air to vary, and the variation between the hot air at the surface of the road and the denser cool air above it creates a gradient in the refractive index of the air. This produces a blurred shimmering effect, which affects the ability to resolve objects, the effect being increased when the image is magnified through a telescope or telephoto lens.
Light from the sky at a shallow angle to the road is refracted by the index gradient, making it appear as if the sky is reflected by the road's surface. The mind interprets this as a pool of water on the road, since water also reflects the sky. The illusion fades as one gets closer.
On tarmac roads it may look as if water, or even oil, has been spilled. These kinds of inferior mirages are often called "desert mirages" or "highway mirages". Both sand and tarmac can become very hot when exposed to the sun, easily being more than 10°C hotter than the air one meter above, enough to create conditions suitable for the formation of the mirage.
Heat haze is not related to the atmospheric phenomenon of haze.
Superior mirage.
A superior mirage occurs when the air below the line of sight is colder than the air above it. This unusual arrangement is called a temperature inversion, since warm air above cold air is the opposite of the normal temperature gradient of the atmosphere. Passing through the temperature inversion, the light rays are bent down, and so the image appears above the true object, hence the name "superior". Superior mirages are in general less common than inferior mirages, but, when they do occur, they tend to be more stable, as cold air has no tendency to move up and warm air has no tendency to move down.
Superior mirages are quite common in polar regions, especially over large sheets of ice that have a uniform low temperature. Superior mirages also occur at more moderate latitudes, although in those cases they are weaker and tend to be less smooth and stable. For example, a distant shoreline may appear to "tower" and look higher (and, thus, perhaps closer) than it really is. Because of the turbulence, there appear to be dancing spikes and towers. This type of mirage is also called the Fata Morgana or "hafgerdingar" in the Icelandic language.
A superior mirage can be right-side up or upside down, depending on the distance of the true object and the temperature gradient. Often the image appears as a distorted mixture of up and down parts.
Superior mirages can have a striking effect due to the Earth's curvature. Were the Earth flat, light rays that bend down would soon hit the ground and only nearby objects would be affected. Since Earth is round, if their downward bending curve is about the same as the curvature of the Earth, light rays can travel large distances, perhaps from beyond the horizon. This was observed and documented for the first time in 1596, when a ship under the command of Willem Barentsz in search of the Northeast passage became stuck in the ice at Novaya Zemlya. The crew was forced to endure the polar winter there. They saw their midwinter night come to an end with the rise of a distorted Sun about two weeks earlier than expected. It was not until the 20th century that science could explain the reason: the real Sun had still been below the horizon, but its light rays followed the curvature of the Earth. This effect is often called a Novaya Zemlya mirage. For every 111.12 km the light rays can travel parallel to the Earth's surface, the Sun will appear 1° higher on the horizon. The inversion layer must have just the right temperature gradient over the whole distance to make this possible.
In the same way, ships that are in reality so far away that they should not be visible above the geometric horizon may appear on the horizon or even above the horizon as superior mirages. This may explain some stories about flying ships or coastal cities in the sky, as described by some polar explorers. These are examples of so-called Arctic mirages, or "hillingar" in Icelandic.
If the vertical temperature gradient is +12.9°C per 100 meters (where the positive sign means temperature gets hotter as one goes higher) then horizontal light rays will just follow the curvature of the Earth, and the horizon will appear flat. If the gradient is less (as it almost always is) the rays are not bent enough and get lost in space, which is the normal situation of a spherical, convex "horizon".
In some situations, distant objects can get elevated or lowered, stretched or shortened with no mirage involved.
Fata Morgana.
A Fata Morgana, the name of which coming from the Italian translation of Morgan le Fay, the fairy shapeshifting half-sister of King Arthur, is a very complex superior mirage. It appears with alternations of compressed and stretched zones, erect images, and inverted images. A Fata Morgana is also a fast-changing mirage.
Fata Morgana mirages are most common in polar regions, especially over large sheets of ice with a uniform low temperature, but they can be observed almost anywhere. While in polar regions, a Fata Morgana may be observed on cold days and in desert areas; and, over oceans and lakes, a Fata Morgana may be observed on hot days. For a Fata Morgana, temperature inversion has to be strong enough that light rays' curvatures within the inversion are stronger than the curvature of the Earth.
The rays will bend and create arcs. An observer needs to be within an atmospheric duct to be able to see a Fata Morgana.
Fata Morgana mirages may be observed from any altitude within the Earth's atmosphere, including from mountaintops or airplanes. 
A Fata Morgana can go from superior to inferior mirage and back within a few seconds, depending on the constantly changing conditions of the atmosphere. Sixteen frames of the mirage of the Farallon Islands, which cannot be seen from sea level at all under normal conditions because they are located below the horizon, were photographed on the same day. The first fourteen frames have elements of a Fata Morgana display—alternations of compressed and stretched zones. The last two frames were photographed a few hours later around sunset. The air was cooler while the ocean was probably a little bit warmer, which made temperature inversion lower. The mirage was still present, but it was not as complex as it had been a few hours before sunset, and it corresponded no longer to a Fata Morgana but rather to a superior mirage display.
Distortions of image and bending of light can produce spectacular effects. In his book "Pursuit: The Chase and Sinking of the "Bismarck"", the author Ludovic Kennedy describes an incident that allegedly took place below the Denmark Strait during 1941, following the sinking of the "Hood". The "Bismarck", while pursued by the British cruisers "Norfolk" and "Suffolk", passed out of sight into a sea mist. Within a matter of seconds, the ship re-appeared steaming toward the British ships at high speed. In alarm the cruisers separated, anticipating an imminent attack, and observers from both ships watched in astonishment as the German battleship fluttered, grew indistinct and faded away. Radar watch during these events indicated that the "Bismarck" had in fact made no changes of course.
Night-time mirages.
The conditions for producing a mirage can take place at night. Under most conditions, these are not observed. However, under some circumstances lights from moving vehicles, aircraft, ships, buildings, etc. can be observed at night, even though, as with a daytime mirage, they would not be observable.
This includes the mirage of astronomical objects.
Mirage of astronomical objects.
A mirage of an astronomical object is a naturally occurring optical phenomenon, in which light rays are bent to produce distorted or multiple images of an astronomical object. The mirages might be observed for such astronomical objects as the Sun, the Moon, the planets, bright stars, and very bright comets. The most commonly observed are sunset and sunrise mirages.

</doc>
<doc id="55697" url="http://en.wikipedia.org/wiki?curid=55697" title="Parenting">
Parenting

Parenting (or child rearing) is the process of promoting and supporting the physical, emotional, social, financial, and intellectual development of a child from infancy to adulthood. Parenting refers to the aspects of raising a child aside from the biological relationship.
The most common caretaker in parenting is the biological parent(s) of the child in question, although others may be an older sibling, a grandparent, a legal guardian, aunt, uncle or other family member, or a family friend. Governments and society may have a role in child-rearing as well. In many cases, orphaned or abandoned children receive parental care from non-parent blood relations. Others may be adopted, raised in foster care, or placed in an orphanage. Parenting skills vary, and a parent with good parenting skills may be referred to as a "good parent". The English pediatrician and psychoanalyst Donald Winnicott described the concept of "good-enough" parenting in which a minimum of prerequisites for healthy child development are met. Winnicott wrote,"The good-enough mother...starts off with an almost complete adaptation to her infant's needs, and as time proceeds she adapts less and less completely, gradually, according to the infant's growing ability to deal with her failure." Views on the characteristics that make one a good or "good-enough" parent vary from culture to culture. Additionally, research has supported that parental history both in terms of attachments of varying quality as well as parental psychopathology, particularly in the wake of adverse experiences, can strongly influence parental sensitivity and child outcomes.
Factors that affect parenting decisions.
Social class, wealth, culture and income have a very strong impact on what methods of child rearing are used by parents. Cultural values play a major role in how a parent raises their child. However, parenting is always evolving; as times change, cultural practices and social norms and traditions change 
In psychology, the parental investment theory suggests that basic differences between males and females in parental investment have great adaptive significance and lead to gender differences in mating propensities and preferences.
A family's social class plays a large role in the opportunities and resources that will be made available to a child. Working-class children often grow up at a disadvantage with the schooling, communities, and parental attention made available to them compared to middle-class or upper-class upbringings. Also, lower working-class families do not get the kind of networking that the middle and upper classes do through helpful family members, friends, and community individuals and groups as well as various professionals or experts.
Styles.
A parenting style is the overall emotional climate in the home. Developmental psychologist Diana Baumrind identified three main parenting styles in early child development: authoritative, authoritarian, and permissive. These parenting styles were later expanded to four, including an uninvolved style. These four styles of parenting involve combinations of acceptance and responsiveness on the one hand and demand and control on the other.
There is no single or definitive model of parenting. With authoritarian and permissive (indulgent) parenting on opposite sides of the spectrum, most conventional and modern models of parenting fall somewhere in between. Parenting strategies as well as behaviors and ideals of what parents expect, whether communicated verbally and/or non-verbally, also play a significant role in a child's development.
Practices.
A parenting practice is a specific behavior that a parent uses in raising a child. For example, a common parent practice intended to promote academic success is reading books to the child. Storytelling is an important parenting practice for children in many Indigenous American communities.
Parenting practices reflect the cultural understanding of children. Parents in individualistic countries like Germany spend more time engaged in face-to-face interaction with babies and more time talking to the baby about the baby. Parents in more communal cultures, such as West African cultures, spend more time talking to the baby about other people, and more time with the baby facing outwards, so that the baby sees what the mother sees. Children develop skills at different rates as a result of differences in these culturally driven parenting practices. Children in individualistic cultures learn to act independently and to recognize themselves in a mirror test at a younger age than children whose cultures promote communal values. However, these independent children learn self-regulation and cooperation later than children in communal cultures. In practice, this means that a child in an independent culture will happily play by herself, but a child in a communal culture is more likely to follow his mother's instruction to pick up his toys. Children that grow up in communities with a collaborative orientation to social interaction, such as some Indigenous American communities, are also able to self-regulate and become very self-confident, while remaining involved in the community.
Skills.
Parenting styles are only a small piece of what it takes to be a "good parent". Parenting takes a lot of skill and patience and is constant work and growth. Research shows that children benefit most when their parents:
Parenting skills are often assumed to be self-evident or naturally present in parents. That this is a very much oversimplified view is emphasized by Virginia Satir, a pioneer in family therapy:
Values.
Parents around the world want what they believe is best for their children. However, parents in different cultures have different ideas of what is best. For example, parents in a hunter–gatherer society or surviving through subsistence agriculture are likely to promote practical survival skills from a young age. Many such cultures begin teaching babies to use sharp tools, including knives, before their first birthdays. This seen in communities where children have a considerate amount of autonomy at a younger age and are given the opportunity to become skilled in tasks that are sometimes classified as adult work by other cultures. In some Indigenous American communities, child work provides children the opportunity to learn cultural values of collaborative participation and prosocial behavior through observation and participation alongside adults. American parents strongly value intellectual ability, especially in a narrow "book learning" sense. Italian parents value social and emotional abilities and having an even temperament. Spanish parents want their children to be sociable. Swedish parents value security and happiness. Dutch parents value independence, long attention spans, and predictable schedules. The Kipsigis people of Kenya value children who are not only smart, but who employ that intelligence in a responsible and helpful way, which they call "ng/om". Many Indigenous American communities value respect, participation in the community, and non-interference. The practice of non-interference is an important value in Cherokee culture. It requires that one respects the autonomy of others in the community by not interfering in their decision making by giving unsolicited advice.
Differences in values cause parents to interpret different actions in different ways. Asking questions is seen by many European American parents as a sign that the child is smart. Italian parents, who value social and emotional competence, believe that asking questions is a sign that the child has good interpersonal skills. Dutch parents, who value independence, view asking questions negatively, as a sign that the child is not independent. Indigenous American parents often try to encourage curiosity in their children. Many use a permissive parenting style that enables the child to explore and learn through observation of the world around it.
Cultural tools.
Differences in values can also cause parents to employ different tools to promote their values. Many European American parents expect specially purchased educational toys to improve their children's intelligence. Some Spanish parents promote social skills by taking their children out for daily walks around the neighborhood.
Parenting in Indigenous American cultures.
It is common for parents in many Indigenous American communities to use different tools in parenting such as storytelling—like myths—consejos, educational teasing, nonverbal communication, and observational learning to teach their children important values and life lessons.
Storytelling is a way for Indigenous American children to learn about their identity, community, and cultural history. Indigenous myths and folklore often personify animals and objects, reaffirming the belief that everything possess a soul and must be respected. These stories help preserve language and are used to reflect certain values or cultural histories.
Consejos are a narrative form of advice giving that provides the recipient with maximum autonomy in the situation as a result of their indirect teaching style. Rather than directly informing the child what they should do, the parent instead might tell a story of a similar situation or scenario. The character in the story is used to help the child see what the implications of their decision may be, without directly making the decision for them. This teaches the child to be decisive and independent, while still providing some guidance.
The playful form of teasing is a parenting method used in some Indigenous American communities to keep children out of danger and guide their behavior. This form of teasing utilizes stories, fabrications, or empty threats to guide children in making safe, intelligent decisions. It can teach children values by establishing expectations and encouraging the child to meet them via playful jokes and/or idle threats. For example, a parent may tell a child that there is a monster that jumps on children's backs if they walk alone at night. This explanation can help keep the child safe because instilling that alarm creates greater awareness and lessens the likelihood that they will wander alone into trouble.
In Navajo families, a child’s development is partly focused on the importance of "respect" for all things as part of the child’s moral and human development. "Respect" in this sense is an emphasis of recognizing the significance of and understanding for one's relationship with other things and people in the world. Nonverbal communication is much of the way that children learn about such "respect" from parents and other family members.
For example, in a Navajo parenting tool using nonverbal communication, children are initiated at an early age into the practice of an early morning run through any weather condition. This form of guidance fosters “respect” not only for the child's family members but also to the community as a whole. On this run, the community uses humor and laughter with each other, without directly including the child—who may not wish to get up early and run—to promote the child’s motivation to participate and become an active member of the community. To modify children’s behavior in a nonverbal manner, parents also promote inclusion in the morning runs by placing their child in the snow and having them stay longer if they protest; this is done within a context of warmth, laughter, and community, to help incorporate the child into the practice. 
A tool parents use in Indigenous American cultures is to incorporate children into everyday life, including adult activities, to pass on the parents’ knowledge by allowing the child to learn through observation. This practice is known as LOPI, learning by observing and pitching in, where children are integrated into all types of mature daily activities and encouraged to observe and contribute in the community. This inclusion as a parenting tool promotes both community participation and learning.
In some Mayan communities, young girls are not permitted around the hearth, for an extended period of time since corn is sacred. Despite this being an exception to the more common Indigenous American practice of integrating children into all adult activities, including cooking, it is a strong example of observational learning. These Mayan girls can only see their mothers making tortillas in small bits at a time, they will then go and practice the movements their mother used on other objects, such as the example of kneading thin pieces of plastic like a tortilla. From this practice, when a girl comes of age, she is able to sit down and make tortillas without any explicit verbal instruction as a result of her observational learning.
Parenting in Judaism and Jewish culture.
Judaism has an extensive tradition of parenting with an emphasis on education. 
Parenting across the lifespan.
Planning and pre-pregnancy.
Family planning is the decision regarding whether and when to become parents, including planning, preparing, and gathering resources. Parents should assess (among other matters) whether they have the required financial resources (the raising of a child costs around $16,198 yearly in the United States) and should also assess whether their family situation is stable enough and whether they themselves are responsible and qualified enough to raise a child. Reproductive health and preconceptional care affect pregnancy, reproductive success and maternal and child physical and mental health.
Pregnancy and prenatal parenting.
During pregnancy the unborn child is affected by many decisions his or her parents make, particularly choices linked to their lifestyle. The health and diet decisions of the mother can have either a positive or negative impact on the child during prenatal parenting. In addition to physical management of the pregnancy, medical knowledge of your physician, hospital, and birthing options are important. Here are some key items of advice:
Many people believe that parenting begins with birth, but the mother begins raising and nurturing a child well before birth.
Scientific evidence indicates that from the fifth month on, the unborn baby is able to hear sounds, become aware of motion, and possibly exhibit short-term memory. Several studies (e.g. Kissilevsky et al., 2003) show evidence that the unborn baby can become familiar with his or her parents' voices. Other research indicates that by the seventh month, external schedule cues influence the unborn baby's sleep habits. Based on this evidence, parenting actually begins well before birth.
Depending on how many children the mother carries also determines the amount of care needed during prenatal and post-natal periods.
Newborns and infants.
Newborn parenting, is where the responsibilities of parenthood begins. A newborn's basic needs are food, sleep, comfort and cleaning which the parent provides. An infant's only form of communication is crying, and attentive parents will begin to recognize different types of crying which represent different needs such as hunger, discomfort, boredom, or loneliness. Newborns and young infants require feedings every few hours which is disruptive to adult sleep cycles. They respond enthusiastically to soft stroking, cuddling and caressing. Gentle rocking back and forth often calms a crying infant, as do massages and warm baths. Newborns may comfort themselves by sucking their thumb or a pacifier. The need to suckle is instinctive and allows newborns to feed. Breastfeeding is the recommended method of feeding by all major infant health organizations. If breastfeeding is not possible or desired, bottle feeding is a common alternative. Other alternatives include feeding breastmilk or formula with a cup, spoon, feeding syringe, or nursing supplementer.
The forming of attachments is considered to be the foundation of the infant/child's capacity to form and conduct relationships throughout life. Attachment is not the same as love and/or affection although they often go together. Attachments develop immediately and a lack of attachment or a seriously disrupted capacity for attachment could potentially do serious damage to a child's health and well-being. Physically, one may not see symptoms or indications of a disorder but the child may be emotionally affected. Studies show that children with secure attachment have the ability to form successful relationships, express themselves on an interpersonal basis and have higher self-esteem . Conversely children who have caregivers who are neglectful or emotionally unavailable can exhibit behavioral problems such as post-traumatic stress disorder or oppositional-defiant disorder 
Oppositional-defiant disorder is a pattern of disobedient, hostile, and defiant behavior toward authority figures
Toddlers.
Toddlers are much more active than infants and are challenged with learning how to do simple tasks by themselves. At this stage, parents are heavily involved in showing the child how to do things rather than just doing things for them, and the child will often mimic the parents. Toddlers need help to build their vocabulary, increase their communication skills, and manage their emotions. Toddlers will also begin to understand social etiquette such as being polite and taking turns.
Toddlers are very curious about the world around them and eager to explore it. They seek greater independence and responsibility and may become frustrated when things do not go the way they want or expect. Tantrums begin at this stage, which is sometimes referred to as the 'Terrible Twos'. Tantrums are often caused by the child's frustration over the particular situation, sometimes simply not being able to communicate properly. Parents of toddlers are expected to help guide and teach the child, establish basic routines (such as washing hands before meals or brushing teeth before bed), and increase the child's responsibilities. It is also normal for toddlers to be frequently frustrated. It is an essential step to their development. They will learn through experience; trial and error. This means that they need to experience being frustrated when something does not work for them, in order to move on to the next stage. When the toddler is frustrated, they will often behave badly with actions like screaming, hitting or biting. Parents need to be careful when reacting to such behaviours, giving threats or punishments is not helpful and will only make the situation worse. Research groups led by Daniel Schechter, Alytia Levendosky, and others have shown that parents with histories of maltreatment and violence exposure often have difficulty helping their toddlers and preschool-age children with these very same emotionally dysregulated behaviours, which can remind traumatized parents of their adverse experiences and associated mental states.
Child.
Younger children are becoming more independent and are beginning to build friendships. They are able to reason and can make their own decisions given hypothetical situations. Young children demand constant attention, but will learn how to deal with boredom and be able to play independently. They also enjoy helping and feeling useful and able. Parents may assist their child by encouraging social interactions and modelling proper social behaviors. A large part of learning in the early years comes from being involved in activities and household duties. Parents who observe their children in play or join with them in child-driven play have the opportunity to glimpse into their children’s world, learn to communicate more effectively with their children and are given another setting to offer gentle, nurturing guidance. Parents are also teaching their children health, hygiene, and eating habits through instruction and by example.
Parents are expected to make decisions about their child's education. Parenting styles in this area diverge greatly at this stage with some parents becoming heavily involved in arranging organized activities and early learning programs. Other parents choose to let the child develop with few organized activities.
Children begin to learn responsibility, and consequences of their actions, with parental assistance. Some parents provide a small allowance that increases with age to help teach children the value of money and how to be responsible with it.
Parents who are consistent and fair with their discipline, who openly communicate and offer explanations to their children, and who do not neglect the needs of their children in some way often find they have fewer problems with their children as they mature.
Adolescents.
During adolescence children are beginning to form their identity and are testing and developing the interpersonal and occupational roles that they will assume as adults. Therefore it is important that parents treat them as young adults. Although adolescents look to peers and adults outside the family for guidance and models for how to behave, parents remain influential in their development. A teenager who thinks poorly of him or herself, is not confident, hangs around with gangs, lacks positive values, follows the crowd, is not doing well in studies, is losing interest in school, has few friends, lacks supervision at home or is not close to key adults like parents and is vulnerable to peer pressure. Parents often feel isolated and alone in parenting adolescents, but they should still make efforts to be aware of their adolescents' activities, and to provide guidance, direction, and consultation. Adolescence can be a time of high risk for children, where new-found freedoms can result in decisions that drastically open up or close off life opportunities. Adolescents tend to increase the amount of time they spend peers of the opposite gender; however, they still maintain the amount of time they spend with those of the same gender, and they do this by decreasing the amount of time they spend with their parents. Also, peer pressure is not the reason why peers have influence on adolescents; instead, it is often because they respect, admire and like their peers. Parental issues at this stage of parenting include dealing with "rebellious" teenagers, who didn't know freedom while they were smaller. In order to prevent all these, it is important for the parents to build a trusting relationship with their children. This can be achieved by planning and taking part in fun activities together, keeping promises made to them, spending time with them, not reminding them about their past mistakes and listening to and talking to them. When a trusting relationship is built, adolescents are more likely to approach their parents for help when faced with negative peer pressure. Helping the child build a strong foundation will help them to resist negative peer pressure. It is important for the parents to build up the self-esteem of their child: Praise the child's strengths instead of focusing on their weaknesses (It will help to grow the child's sense of self-worth and self-confidence, so he/she does not feel the need to gain acceptance from his/her peers), acknowledge the child's efforts, do not simply focus on the final result (when they notice that the parent recognizes their efforts, they will keep trying), and lastly, disapprove the behavior, not the child, or they will turn to their peers for acceptance and comfort.
Adults.
Parenting doesn't usually end when a child turns 18. Support can be needed in a child's life well beyond the adolescent years and continues into middle and later adulthood. Parenting can be a lifelong process.
Assistance.
Parents may receive assistance with caring for their children through child care programs.

</doc>
<doc id="55700" url="http://en.wikipedia.org/wiki?curid=55700" title="Tactile illusion">
Tactile illusion

Tactile illusions are illusions that exploit the sense of touch. Some touch illusions require active touch (e.g., movement of the fingers or hands), whereas others can be evoked passively (e.g., with external stimuli that press against the skin).

</doc>
<doc id="55702" url="http://en.wikipedia.org/wiki?curid=55702" title="Hospital volunteer">
Hospital volunteer

Hospital volunteers work without regular pay in a variety of health care settings, usually under the supervision of a nurse. Most hospitals train and supervise volunteers through a specialized non-profit organization called an auxiliary. The director of the auxiliary is usually a paid employee of the hospital. 
A hospital volunteer is sometimes nicknamed a candy striper. This name is derived from the red-and-white striped jumpers that female volunteers traditionally wore in the United States, which resembles candy canes. The name and uniform are used less frequently now. 
In the United States, volunteers' services are of considerable importance to individual patients as well as the health care system in general. Some people volunteer during high school or college, either out of curiosity about health-care professions or in order to satisfy mandatory community service requirements imposed by some schools. Still others volunteer at later stages in their life, particularly after retirement.
History.
Candy Stripers originated as a high-school civics class project in East Orange, New Jersey, in 1944. The uniforms were sewn by the girls in the class from material provided by the teacher – a red-and-white-striped fabric known as "candy stripe". The students chose East Orange General Hospital as the home for their class project. 
Duties.
Duties of hospital volunteers vary widely depending upon the facility. Volunteers may attend in staff reception areas and gift shops; file and retrieve documents and mails; take out trash; clean up after the nurses and doctors; provide administrative backup; assist with research by doing the dishes and autoclaving; help visitors; visit with patients; or transport various small items like flowers, medical records, lab specimens, and drugs from unit to unit.
A few hospitals ask their volunteers to help out with janitorial duties, like cleaning beds. Other "advanced volunteers" include patient-care liaisons and volunteer orderlies. These volunteers must operate on the orders of a nurse or a physician and are given special training to permit them to work with patients. They are also more common in large hospitals, particularly university-affiliated hospitals and teaching hospitals, as they allow pre-medical students to gain experience in patient care while taking pressure off a busy care team.
Some hospitals manage their volunteers from a dispersal unit and assign them to tasks based on real-time labor demand, while other hospitals assign volunteers to a single unit for the duration of their service. Female volunteers traditionally wore pink-and-white jumpers, while male volunteers traditionally wore light-blue tunics or shirts over dark slacks. Today, male and female volunteers often wear a uniform shirt or a short-sleeved shirt with slacks. Some volunteers (particularly "advanced volunteers") will wear scrubs, but this is usually avoided so volunteers are not confused with medical personnel. All volunteers wear ID tags within the hospital and these will prominently indicate the volunteer's status and position.

</doc>
<doc id="55706" url="http://en.wikipedia.org/wiki?curid=55706" title="Richard Whittington">
Richard Whittington

Richard Whittington (c. 1354–1423) was a medieval merchant and a politician. He is also the real-life inspiration for the English folk tale Dick Whittington and His Cat. He was four times Lord Mayor of London, a member of parliament and a sheriff of London. In his lifetime he financed a number of public projects, such as drainage systems in poor areas of medieval London, and a hospital ward for unmarried mothers. He bequeathed his fortune to form the Charity of Sir Richard Whittington which, nearly 600 years later, continues to assist people in need. Despite knowing three of the five kings who reigned during his lifetime, there is no evidence that he was knighted.
Biography.
He was born in Gloucestershire, at Pauntley in the Forest of Dean, although his family originated from Kinver in Staffordshire, England, where his grandfather Sir William de Whittington was a knight at arms. His date of birth is variously given as in the 1350s and he died in London in 1423. However, he was a younger son and so would not inherit his father's estate as the eldest son might expect to do. Consequently he was sent to the City of London to learn the trade of mercer. He became a successful trader, dealing in valuable imports such as silks and velvets, both luxury fabrics, much of which he sold to the Royal and noble court from about 1388. There is indirect evidence that he was also a major exporter to Europe of much sought after English woollen cloth such as Broadcloth. From 1392 to 1394 he sold goods to Richard II worth £3,500 (equivalent to more than £1.5m today). He also began money-lending in 1388, preferring this to outward shows of wealth such as buying property. By 1397 he was also lending large sums of money to the King.
In 1384 Whittington had become a Councilman. In 1392 he was one of the city's delegation to the King at Nottingham at which the King seized the City of London's lands because of alleged misgovernment. By 1393, he had become an alderman and was appointed Sheriff by the incumbent mayor, William Staundone, as well as becoming a member of the Mercers' Company. When Adam Bamme, the mayor of London, died in June 1397, Whittington was imposed on the city by the King as Lord Mayor of London two days later to fill the vacancy with immediate effect. Within days Whittington had negotiated with the King a deal in which the city bought back its liberties for £10,000 (nearly £4m today). He was elected mayor by a grateful populace on 13 October 1397.
The deposition of Richard II in 1399 did not affect Whittington and it is thought that he merely acquiesced in the coup led by Bolingbroke. Whittington had long supplied the new king, Henry IV, as a prominent member of the landowning elite and so his business simply continued as before. He also lent the new king substantial amounts of money. He was elected mayor again in 1406—during 1407 he was simultaneously Mayor in both London and Calais—and in 1419. In 1416, he became a member of parliament, and was also in turn influential with Henry IV's son, Henry V, also lending him large amounts of money and serving on several Royal Commissions of oyer and terminer. For example, Henry V employed him to supervise the expenditure to complete Westminster Abbey. Despite being a moneylender himself he was sufficiently trusted and respected to sit as a judge in usury trials in 1421. Whittington also collected revenues and import duties. A long dispute with the Company of Brewers over standard prices and measures of ale was won by Whittington.
Benefactions.
In his lifetime Whittington donated much of his profit to the city and left further endowments by his Will. He financed: 
He also provided accommodation for his apprentices in his own house. He passed a law prohibiting the washing of animal skins by apprentices in the River Thames in cold, wet weather because many young boys had died through hypothermia or in the strong river currents.
Death and bequests.
Whittington died in March 1423. In 1402 (aged 48) he had married Alice, daughter of Sir Ivo FitzWarin (or Fitzwarren) of Wantage in Berkshire (now Oxfordshire), but she predeceased him in 1411. They had no children. He was buried in the church of St Michael Paternoster Royal, to which he had donated large sums during his lifetime. The tomb is now lost, and the mummified cat found in the church tower in 1949 during a search for its location probably dates to the time of the Wren restoration.
In the absence of heirs, Whittington left £7,000 in his will to charity, in those days a large sum, with a modern-day equivalence of about £3m. Some of this was used to 
The almshouses were relocated in 1966 to Felbridge near East Grinstead. Sixty elderly women and a few married couples currently live in them. The Whittington Charity also disburses money each year to the needy through the Mercers' Company. The Whittington hospital is now at Archway in the London Borough of Islington and a small statue of a cat along Highgate Hill further commemorates his legendary feline.
Dick Whittington—stage character.
The gifts left in Whittington's will made him well known and he became a character in an English story that was adapted for the stage as a play, "The History of Richard Whittington, of his lowe byrth, his great fortune", in February 1604. In the 19th century this became popular as a pantomime called "Dick Whittington and His Cat", very loosely based on Richard Whittington. There are several versions of the traditional story, which tells how Dick, a boy from a poor Gloucestershire family, sets out for London to make his fortune, accompanied by, or later acquiring, his cat. At first he meets with little success, and is tempted to return home. However, on his way out of the city, whilst climbing Highgate Hill from modern-day Archway, he hears the Bow Bells of London ringing, and believes they are sending him a message. There is now a large hospital on Highgate Hill, named the Whittington Hospital, after this supposed episode. A traditional rhyme associated with this tale is:
On returning to London, Dick embarks on a series of adventures. In one version of the tale, he travels abroad on a ship, and wins many friends as a result of the rat-catching activities of his cat; in another he sends his cat and it is sold to make his fortune. Eventually he does become prosperous, marries his master's daughter Alice Fitzwarren (the name of the real Whittington's wife), and is made Lord Mayor of London three times. The common belief that he served three rather than four times as Lord Mayor stems from the City's records 'Liber Albus' compiled at his request by the City Clerk John Carpenter wherein his name appears only three times as the remainder term of his deceased predecessor Adam Bamme and his own consequent term immediately afterwards appear as one entry for 1397.
As the son of gentry Whittington was never very poor and there is no evidence that he kept a cat. Whittington may have become associated with a thirteenth-century Persian folktale about an orphan who gained a fortune through his cat, but the tale was common throughout Europe at that time. Folklorists have suggested that the most popular legends about Whittington—that his fortunes were founded on the sale of his cat, who was sent on a merchant vessel to a rat-beset Eastern emperor—originated in a popular 17th-century engraving by Renold Elstracke in which his hand rested on a cat, but the picture only reflects a story already in wide circulation. Elstracke's oddly-shaped cat was in fact a later replacement by printseller Peter Stent for what had been a skull in the original, with the change being made to conform to the story already in existence, to increase sales. 
There was also known to be a painted portrait of Whittington shown with a cat hanging at Mercer Hall, but it was reported that the painting had been trimmed down to smaller size, and the date "1572" that appears there was something painted after the cropping, which raises doubt as to the authenticity of the date, though Malcom who witnessed it ca. early 1800s felt the date should be taken in good faith. The print published in "The New Wonderful Museum" (vol. III, 1805, pictured above) is presumably a replica of this painting.
Whittington also starred as a cartoon on a ComiColor Cartoon, 'Dick Whittington's Cat" (1936).

</doc>
<doc id="55707" url="http://en.wikipedia.org/wiki?curid=55707" title="Hatch Act">
Hatch Act

Hatch Act may refer to:

</doc>
<doc id="55708" url="http://en.wikipedia.org/wiki?curid=55708" title="Sherman Silver Purchase Act">
Sherman Silver Purchase Act

The Sherman Silver Purchase Act was a United States federal law 
enacted on July 14, 1890.
The measure did not authorize the free and unlimited coinage of silver that the Free Silver supporters wanted; however, it increased the amount of silver the government was required to purchase on a recurrent monthly basis to 4.5 million ounces. The Sherman Silver Purchase Act had been passed in response to the growing complaints of farmers' and miners' interests. Farmers had immense debts that could not be paid off due to deflation caused by overproduction, and they urged the government to pass the Sherman Silver Purchase Act in order to boost the economy and cause inflation, allowing them to pay their debts with cheaper dollars. Mining companies, meanwhile, had extracted vast quantities of silver from western mines; the resulting oversupply drove down the price of their product, often to below the point at which the silver could be profitably extracted. They hoped to enlist the government to increase the demand for silver.
Originally, the bill was simply known as the Silver Purchase Act of 1890. Only after the bill was signed into law, did it become the "Sherman Silver Purchase Act." Senator John Sherman, an Ohio Republican and chairman of the Senate Finance Committee was not the author of the bill, but once both houses of Congress had passed the Act and the Act had been sent to a Senate/House conference committee to iron out differences between the Senate and House versions of the Act, Senator John Sherman was instrumental in getting the conference committee to reach agreement on a final draft of the Act. Nonetheless, once agreement on the final version was reached in the conference committee, Sherman found that he disagreed with many sections of the act. So tepid was Sherman's support that when he was asked his opinion of the act by President Benjamin Harrison, Sherman ventured only that the bill was "safe" and would cause no harm if the President signed it.
The act was enacted in tandem with the McKinley Tariff of 1890. William McKinley, an Ohio Republican and chairman of the House Ways and Means Committee worked with John Sherman to create a package that could both pass the Senate and receive the President's approval.
Under the Act, the federal government purchased millions of ounces of silver, with issues of paper currency. It became the second-largest buyer in the world, after the British Crown in India, where the Indian Rupee was backed by silver rather than gold. In addition to the $2 million to $4 million that had been required by the Bland-Allison Act of 1878, the US government was now required to purchase an additional 4.5 million ounces of silver bullion every month. The law required the Treasury to buy the silver with a special issue of Treasury (Coin) Notes that could be redeemed for either silver or gold. That plan backfired, as people (mostly investors) redeemed the new coin notes for gold dollars, thus depleting the government's gold reserves. After the Panic of 1893 broke, President Grover Cleveland oversaw the repeal of the act to prevent the depletion of the gold reserves.
In 1890, the price of silver dipped to $1.16 per ounce. By the end of the year, it had fallen to $0.69. By December 1894, the price had dropped to $0.60. On November 1, 1895, US mints halted production of silver coins, and the government closed the New Orleans Mint. Banks discouraged the use of silver dollars.

</doc>
<doc id="55709" url="http://en.wikipedia.org/wiki?curid=55709" title="Pantomime (disambiguation)">
Pantomime (disambiguation)

Pantomime may refer to: 

</doc>
<doc id="55710" url="http://en.wikipedia.org/wiki?curid=55710" title="McKinley Tariff">
McKinley Tariff

The Tariff Act of 1890, commonly called the McKinley Tariff, was an act of the United States Congress framed by Representative William McKinley that became law on October 1, 1890. The tariff raised the average duty on imports to almost fifty percent, an act designed to protect domestic industries from foreign competition. Protectionism, a tactic supported by Republicans, was fiercely debated by politicians and condemned by Democrats. The McKinley Tariff was replaced with the Wilson–Gorman Tariff Act in 1894, which promptly lowered tariff rates.
Tariff politics.
Tariffs, taxes on foreign goods entering a country, served two purposes for the United States in the late 19th century. One was to raise fiscal revenue for the federal government, and the other was to protect domestic manufacturers from foreign competition. This controversial idea was known as protectionism.
In December 1887, President Grover Cleveland, a Democrat, devoted his entire State of the Union Address to the issue of the tariff. He called emphatically for the reduction of duties and the abolition of duties on raw materials. This speech succeeded in making the tariff, and the idea of protectionism, a true party matter. In the 1888 election, the Republicans were victorious with the election of President Harrison, and majorities in both the Senate and the House. For the sake of holding the party line, the Republicans felt obligated to pass stronger tariff legislation.
William McKinley, of Ohio, was defeated by Thomas B. Reed to be Speaker of the House after the 1888 elections. McKinley instead became chairman of the House Ways and Means Committee and was responsible for framing a new tariff bill. He believed that a protectionist tariff had been mandated by the people through the election, and that it was necessary for America’s wealth and prosperity.
In addition to the protectionist debate, politicians were also concerned about the high revenue accruing from tariffs. After the Civil War, tariffs remained elevated to raise revenue and cover the high costs of war. However, in the early 1880s, the federal government was running a large surplus. Both parties agreed that the surplus needed to lessen, but disagreed about whether to raise or lower tariffs to accomplish the same goal. The Democrats' hypothesis stated that tariff revenue could be reduced by reducing the tariff rate. Conversely, Republicans believed that by increasing the tariff, imports would be lessened, and total tariff revenue would drop "(See Laffer curve)". This point, along with the dialogue surrounding protectionism, created what would be known as “The Great Tariff Debate of 1888.”
Tariff description.
After 450 amendments, the Tariff Act of 1890 was passed, and increased average duties across all imports from 38% to 49.5%. McKinley was known as the “Napoleon of Protection,” and the act reflected this sentiment. It raised rates on some goods and lowered rates on others, always attempting to protect American manufacturing interests. Changes in duties for specific products such as tin-plates and wool were the most controversial, and emblematic of the spirit of the Tariff of 1890. However, on certain items, the Act eliminated tariffs altogether, with the threat of reinstatement as an enticement to get other countries to lower their tariffs on items imported from the U.S.
Tin-plates.
Tin-plates were a major import for the United States; tens of millions of dollars in these goods entered the country each year. In the preceding 20 years tariff rates had been raised and dropped multiple times on tin-plates with no change in import levels, and domestic production had remained inconsequential. In a last attempt to stimulate the infant domestic tin-plate industry, the 1890 tariff raised the duty level from thirty percent to seventy percent. The Act also included a unique provision that stated tin-plates should be admitted free of any duty after 1897, unless domestic production in any year reached one third the imports in that year. The goal was for the duty to be protective, or not exist at all.
Wool.
The new tariff provisions for wool and woolen goods were exceedingly protectionist. Wool was previously taxed based on a schedule, meaning that more valuable wool was taxed at a higher rate. Through a multitude of complicated tariff schedule revisions, the act made almost all woolen goods subject to the maximum duty rate. Further, the act increased the tariff on carpet wool, a wool of very low quality that is not produced in the US. The government wanted to ensure that importers were not declaring higher quality wool as carpet wool to evade the tariff.
Eliminated tariffs.
The Act removed tariffs on sugar, molasses, tea, coffee and hides, but authorized the President to reinstate such tariffs on these types of items when exported from countries which treated U.S. exports in a "reciprocally unequal and unreasonable" fashion. The idea was "to secure reciprocal trade" by allowing the executive branch to use the mere threat of reimposing tariffs as a means to get other countries to lower their tariffs on U.S. exports. Although this delegation of power had the appearance of being an unconstitutional violation of the nondelegation doctrine, it was upheld by the Supreme Court in "Field v. Clark" in 1892 as merely authorizing the executive to act as an "agent" of Congress, rather than a law-maker itself. The President did not use the delegated power to re-impose tariffs on the five types of imported goods, but used the threat of doing so to successfully negotiate ten treaties in which other countries reduced their tariffs on U.S. goods.
Tariff effects.
Douglas Irwin’s 1998 paper examines the validity of the opposite tariff hypotheses posed by the Republicans and Democrats in 1890. Irwin looked at historical data to estimate import demand elasticities, and export supply elasticities for the US in the years before 1888. With this information, he calculated that tariffs had not reached the maximum revenue rate, and therefore a "reduction", not an increase, in the tariff would have reduced revenue and the federal surplus. His findings confirmed the Democrats' hypothesis, and refuted the Republicans'. After examining the actual tariff revenue data, it appears that revenue did decrease by about four percent from $225 million to $215 million after the Tariff of 1890 increased rates. Irwin explains that this is due to the Tariff of 1890’s provision that raw sugar be moved to the duty-free list. Sugar, at this time, was the item that raised the most tariff revenue, so making it duty-free reduced this revenue. If sugar is excluded from import calculations, the tariff revenue increased by 7.8 percent from $170 million to $183 million.
Irwin concluded that the tariff hastened the development of domestic tinplate production by about a decade, but also that this benefit to the industry was outweighed by the cost to consumers.
The tariff was not well received by Americans who suffered a steep increase in the cost of products. In the 1890 election Republicans lost their majority in the House with the number of seats they won reduced by nearly half, from 171 to 88. In the 1892 presidential election, Harrison was soundly defeated by Grover Cleveland, and the Senate, House, and Presidency were all under Democratic control. Lawmakers immediately started drafting new tariff legislation, and in 1894 the Wilson-Gorman Tariff passed which lowered US tariff averages. Nor was the tariff well received abroad. Protectionists within the British Empire used the McKinley Tariff to argue for tariff retaliation and imperial trade preference.

</doc>
<doc id="55711" url="http://en.wikipedia.org/wiki?curid=55711" title="Wilson–Gorman Tariff Act">
Wilson–Gorman Tariff Act

The Revenue Act or Wilson-Gorman Tariff of 1894 (ch. 349, §73, 28 Stat. , August 27, 1894) slightly reduced the United States tariff rates from the numbers set in the 1890 McKinley tariff and imposed a 2% income tax. It is named for William L. Wilson, Representative from West Virginia, chair of the U.S. House Ways and Means Committee, and Senator Arthur P. Gorman of Maryland, both Democrats.
Supported by pro-free trade members of the Democratic Party, this attempt at tariff reform was important because it imposed the first peacetime income tax (2% on income over $4,000, or $88,100 in 2010 dollars, which meant fewer than 10% of households would pay any). The purpose of the income tax was to make up for revenue that would be lost by tariff reductions. By coincidence, $4,000 ($88,100 in 2010 dollars) would be the exemption for married couples when the Revenue Act of (October) 1913 was signed into law by President Woodrow Wilson, as a result of the ratification of the 16th Amendment to the U.S. Constitution in February 1913. 
The bill introduced by Wilson and passed by the House significantly lowered tariff rates, in accordance with Democratic platform promises, and dropped the tariff to zero on iron ore, coal, lumber and wool, which angered American producers. With Senator Gorman operating behind the scenes, protectionists in the Senate added more than 600 amendments that nullified most of the reforms and raised rates again. The "Sugar Trust" in particular made changes that favored itself at the expense of the consumer.
President Grover Cleveland, who had campaigned on lowering the tariff and supported Wilson's version of the bill, was devastated that his program had been ruined. He denounced the revised measure as a disgraceful product of "party perfidy and party dishonor," but still allowed it to become law without his signature, believing that it was better than nothing and was at the least an improvement over the McKinley tariff.
The Wilson-Gorman Tariff attracted much opposition in West Texas, where sheepraisers opposed the measure. A Republican, George H. Noonan, was elected to Congress from the district stretching from San Angelo to San Antonio but only for a single term. Among Noonan's backers was a former slave, George B. Jackson, a businessman in San Angelo often called "the wealthiest black man in Texas" in the late 19th century.
Income Tax Amendment.
The "New York Times" reported that many Democrats in the East, "prefer to take the income tax, odious as it is, and unpopular as it is bound to be with their constituents," than to defeat the Wilson tariff bill. Democratic Representative Johnson of Ohio supported the income tax as the lesser of two evils:
"he was for an income tax as against a tariff tax; but he believed, that it was un-Democratic, inquisitorial, and wrong in principle."
Legacy.
The income tax provision was struck down in 1895 by the U.S. Supreme Court case "Pollock v. Farmers' Loan & Trust Co.", 157 U.S. (1895). In 1913, the 16th Amendment permitted a federal income tax. 
The tariff provisions of Wilson-Gorman were superseded by the Dingley Tariff of 1897.

</doc>
<doc id="55714" url="http://en.wikipedia.org/wiki?curid=55714" title="Dawes Act">
Dawes Act

The Dawes Act of 1887 (also known as the General Allotment Act or the Dawes Severalty Act of 1887),
adopted by Congress in 1887, authorized the President of the United States to survey American Indian tribal land and divide it into allotments for individual Indians. Those who accepted allotments and lived separately from the tribe would be granted United States citizenship. The Dawes Act was amended in 1891, and again in 1906 by the Burke Act.
The Act was named for its creator, Senator Henry Laurens Dawes of Massachusetts. The stated objective of the Dawes Act was to stimulate assimilation of Indians into mainstream American society. Individual ownership of land on the European-American model was seen as an essential step. The act also provided what the government would classify as "excess" Indian reservation lands remaining after allotments, and sell those lands on the open market, allowing purchase and settlement by non-Native Americans. 
The Dawes Commission, set up under an Indian Office appropriation bill in 1893, was created to try to persuade the Five Civilized Tribes to agree to allotment plans. (They had been excluded from the Dawes Act.) This commission registered the members of the Five Civilized Tribes on what became known as the Dawes Rolls. 
The Curtis Act of 1898 completed the process by which the federal government no longer recognized tribal governments and abolished tribal communal jurisdiction of Indian land.
During the ensuing decades, many Native American tribes and individuals suffered dispossession of lands and other social ills. The Franklin D. Roosevelt administration supported passage on June 18, 1934 of the US Indian Reorganization Act (also known as the Wheeler-Howard Law). It ended allotment and created a "New Deal" for Indians, including renewing their rights to reorganize and form their own governments.
The Indian Problem.
During the 1850s, the United States federal government's attempt to exert control over the Native Americans expanded. Numerous new European immigrants were settling on the eastern border of the Indian territories, where most of the Native Americans tribes were situated. Conflicts between the groups increased as they competed for resources and operated according to different cultural systems. Many European Americans did not believe that members of the two racial societies could coexist within the same communities. Searching for a quick solution to their problem, William Medill the Commissioner of Indian Affairs, proposed establishing "colonies" or "reservations" that would be exclusively for the natives, similar to those which some native tribes had created for themselves in the east. It was a form of removal whereby the US government would uproot the natives from their current locations to positions to areas in the region beyond the Mississippi River; this would enable settlement by European Americans in the Southeast in turn opening up new placement for the new white settlers and at the same time protecting them from the corrupt "evil" ways of the subordinate natives.
The new policy intended to concentrate Native Americans in areas away from encroaching settlers, but it caused considerable suffering and many deaths. During the nineteenth century, Native American tribes resisted the imposition of the reservation system and engaged with the United States Army in what were called the Indian Wars in the West for decades. Finally defeated by the US military force and continuing waves of encroaching settlers, the tribes negotiated agreements to resettle on reservations. Native Americans ended up with a total of over 155 e6acre of land ranging from arid deserts to prime agricultural land.
The Reservation system, though forced upon Native Americans, was a system that allotted each tribe a claim to their new lands, protection over their territories, and the right to govern themselves. With the Senate supposedly being able to intervene only through the negotiation of treaties, they adjusted their ways of life and tried to continue their traditions. The traditional tribal organization, a defining characteristic of Native Americans as a social unit, became apparent to the non-native communities of the United States and created a mixed stir of emotions. The tribe was viewed as a highly cohesive group, led by a hereditary, chosen chief, who exercised power and influence among the members of the tribe by aging traditions. The tribes were seen as strong, tight-knit societies led by powerful men who were opposed to any change that weakened their positions. Many white Americans feared them and sought reformation. The Indians' failure to adopt the "Euroamerican" lifestyle, which was the social norm in the United States at the time, was seen as both unacceptable and uncivilized.
By the end of the 1880s, a general consensus seem to have been reached among many US stakeholders that the assimilation of Native Americans into white American culture was top priority; it was the time for them to leave behind their tribal landholding, reservations, traditions and ultimately their Indian identities.
On February 8, 1887, the Dawes Allotment Act was signed into law by President Grover Cleveland.
Responsible for enacting the division of the American native reserves into plots of land for individual households, the Dawes Act was created by reformers to achieve six goals:
The compulsory Act forced natives to succumb to their inevitable fate; they would undergo severe attempts to become "Euro-Americanized" as the government allotted their reservations with or without their consent. Native Americans held very specific ideologies pertaining to their land, to them the land and earth were things to be valued and cared for, for they represented all things that produced and sustained life, it embodied their existence, identity and created an environment of belonging. In opposition to their white counterparts, they did not see it from an economic standpoint.
But, many natives began to believe they had to adapt to the majority culture in order to survive. They would have to succumb to embrace these beliefs and surrender to the forces of progression. They were to adopt the values of the dominant society and see land as real estate to be bought and developed; they were to learn how to use their land effectively in order to become prosperous farmers. As they were inducted as citizens of the country, they would shed their uncivilized discourses and ideologies, and exchange them for ones that allowed them to become industrious self-supporting citizens, and finally rid themselves of their "need" for government supervision.
Provisions of the Dawes Act.
The important provisions of the Dawes Act were: 
Every member of the bands or tribes receiving a land allotment is subject to laws of the state or territory in which they reside. Every Indian who receives a land allotment "and has adopted the habits of civilized life" (lived separate and apart from the tribe) is bestowed with United States citizenship "without in any manner impairing or otherwise affecting the right of any such Indian to tribal or other property."
The Secretary of Interior could issue rules to assure equal distribution of water for irrigation among the tribes, and provided that "no other appropriation or grant of water by any riparian proprietor shall be authorized or permitted to the damage of any other riparian proprietor."
The Dawes Act did not apply to the territory of the:
Provisions were later extended to the Wea, Peoria, Kaskaskia, Piankeshaw, and Western Miami tribes by act of 1889. Allotment of the lands of these tribes was mandated by the Act of 1891, which amplified the provisions of the Dawes Act.
Dawes Act 1891 Amendments.
In 1891 the Dawes Act was amended:
Provisions of the Burke Act.
The Burke Act of 1906 amended the sections of the Dawes Act dealing with US Citizenship (Section 6) and the mechanism for issuing allotments. The Secretary of Interior could force the Indian Allottee to accept title for land. US Citizenship was granted unconditionally upon receipt of land allotment (the individual did not need to move off the reservation to receive citizenship). Land allotted to Indians was taken out of Trust and subject to taxation. The Burke Act did not apply to any Indians in Indian Territory.
Effects.
The Dawes Act had a negative effect on American Indians, as it ended their communal holding of property (with crop land often being privately owned by families or clans) by which they had ensured that everyone had a home and a place in the tribe. The act "was the culmination of American attempts to destroy tribes and their governments and to open Indian lands to settlement by non-Indians and to development by railroads." Land owned by Indians decreased from 138 e6acre in 1887 to 48 e6acre in 1934.
Senator Henry M. Teller of Colorado was one of the most outspoken opponents of allotment. In 1881, he said that allotment was a policy "to despoil the Indians of their lands and to make them vagabonds on the face of the earth." Teller also said, 
"the real aim [of allotment] was "to get at the Indian lands and open them up to settlement. The provisions for the apparent benefit of the Indians are but the pretext to get at his lands and occupy them. ... If this were done in the name of Greed, it would be bad enough; but to do it in the name of Humanity ... is infinitely worse."
The amount of land in native hands rapidly depleted from some 150 e6acre to a small 78 e6acre by 1900. The remainder of the land once allotted to appointed natives was declared surplus and sold to non-native settlers as well as railroad and other large corporations; other sections were converted into federal parks and military compounds. The concern shifted from encouraging private native landownership to satisfying the white settlers' demand for larger portions of land.
By dividing reservation lands into privately owned parcels, legislators hoped to complete the assimilation process by forcing Indians to adopt individual households, and strengthen the nuclear family and values of economic dependency strictly within this small household unit.
Given the conditions on the Great Plains, the land granted to most allottees was not sufficient for economic viability of farming. Division of land among heirs upon the allottees' deaths quickly led to land fractionalization. Most allotment land, which could be sold after a statutory period of 25 years, was eventually sold to non-Native buyers at bargain prices. Additionally, land deemed to be "surplus" beyond what was needed for allotment was opened to white settlers, though the profits from the sales of these lands were often invested in programs meant to aid the American Indians. Over the 47 years of the Act's life, Native Americans lost about 90 million acres (360,000 km²) of treaty land, or about two-thirds of the 1887 land base. About 90,000 Native Americans were made landless.
In 1906 the Burke Act (also known as the forced patenting act) amended the GAA to give the Secretary of the Interior the power to issue allottees a patent in fee simple to people classified "competent and capable." The criteria for this determination is unclear but meant that allottees deemed "competent" by the Secretary of the Interior would have their land taken out of trust status, subject to taxation, and could be sold by the allottee. The allotted lands of Indians determined to be incompetent by the Secretary of the Interior were automatically leased out by the Federal Government.
The act reads:
... the Secretary of the Interior may, in his discretion, and he is hereby authorized, whenever he shall be satisfied that any Indian allottee is competent and capable of managing his or her affairs at any time to cause to be issued to such allottee a patent in fee simple, and thereafter all restrictions as to sale, encumbrance, or taxation of said land shall be removed.
The use of competence opens up the categorization, making it much more subjective and thus increasing the exclusionary power of the Secretary of Interior. Although this act gives power to the allottee to decide whether to keep or sell the land, given the harsh economic reality of the time, and lack of access to credit and markets, liquidation of Indian lands was almost inevitable. It was known by the Department of Interior that virtually 95% of fee patented land would eventually be sold to whites.
The allotment policy depleted the land base, ending hunting as a means of subsistence. According to Victorian ideals, the men were forced into the fields (but the Indians thought this made them take on what in their society had traditionally been the woman's role, and the women were relegated to the domestic sphere). This Act imposed a patriarchal nuclear household onto many matrilineal Native societies, in which women had controlled property and descent.
Native gender roles and relations quickly changed with this policy, since communal living had shaped the social order of Native communities. Women were no longer the caretakers of the land and they were no longer valued in the public political sphere. Even in the home, the Native woman was dependent on her husband. Before allotment, women divorced easily and had important political and social status, as they were usually the center of their kin network. Under the Dawes Act, to receive the full 160 acre, women had to be officially married.
In 1926, Secretary of the Interior Hubert Work commissioned a study of federal administration of Indian policy and the condition of Indian people. Completed in 1936, "The Problem of Indian Administration" – commonly known as the Meriam Report after the study's director, Lewis Meriam – documented fraud and misappropriation by government agents. In particular, the Meriam Report found that the General Allotment Act had been used to illegally deprive Native Americans of their land rights.
After considerable debate, Congress terminated the allotment process under the Dawes Act by enacting the Indian Reorganization Act of 1934 ("Wheeler-Howard Act"). However, the allotment process in Alaska, under the separate Alaska Native Allotment Act, continued until its revocation in 1993 by the Alaska Native Claims Settlement Act.
Despite termination of the allotment process in 1934, effects of the General Allotment Act continue into the present. For example, one provision of the Act was the establishment of a trust fund, administered by the Bureau of Indian Affairs, to collect and distribute revenues from oil, mineral, timber, and grazing leases on Native American lands. The BIA's alleged improper management of the trust fund resulted in litigation, in particular the case "Cobell v. Kempthorne" (settled in 2009 for $3.4 billion), to force a proper accounting of revenues.
Fractionation.
For nearly one hundred years, the consequences of federal Indian allotments have developed into the problem of "fractionation". As original allottees die, their heirs receive equal, undivided interests in the allottees' lands. In successive generations, smaller undivided interests descend to the next generation. Fractionated interests in individual Indian allotted land continue to expand exponentially with each new generation.
Today, there are approximately four million owner interests in the 10000000 acre of individually owned trust lands, a situation the magnitude of which makes management of trust assets extremely difficult and costly. These four million interests could expand to 11 million interests by the year 2030 unless an aggressive approach to fractionation is taken. There are now single pieces of property with ownership interests that are less than 0.0000001% or 1/9 millionth of the whole interest, which has an estimated value of .004 cent.
The economic consequences of fractionation are severe. Some recent appraisal studies suggest that when the number of owners of a tract of land reaches between ten and twenty, the value of that tract drops to zero. Highly fractionated land is for all practical purposes worthless.
In addition, the fractionation of land and the resultant ballooning number of trust accounts quickly produced an administrative nightmare. Over the past 40 years, the area of trust land has grown by approximately 80000 acre per year. Approximately 357 million dollars is collected annually from all sources of trust asset management, including coal sales, timber harvesting, oil and gas leases and other rights-of-way and lease activity. No single fiduciary institution has ever managed as many trust accounts as the Department of the Interior has managed over the last century.
Interior is involved in the management of 100,000 leases for individual Indians and tribes on trust land that encompasses approximately 56000000 acre. Leasing, use permits, sale revenues, and interest of approximately $226 million per year are collected for approximately 230,000 individual Indian money (IIM) accounts, and about $530 million per year are collected for approximately 1,400 tribal accounts. In addition, the trust currently manages approximately $2.8 billion in tribal funds and $400 million in individual Indian funds.
Under current regulations, probates need to be conducted for every account with trust assets, even those with balances between one cent and one dollar. While the average cost for a probate process exceeds $3,000, even a streamlined, expedited process costing as little as $500 would require almost $10,000,000 to probate the $5,700 in these accounts.
Unlike most private trusts, the Federal Government bears the entire cost of administering the Indian trust. As a result, the usual incentives found in the commercial sector for reducing the number of small or inactive accounts do not apply to the Indian trust. Similarly, the United States has not adopted many of the tools that States and local government entities have for ensuring that unclaimed or abandoned property is returned to productive use within the local community.
Fractionation is not a new issue. In the 1920s the Brookings Institution conducted a major study of conditions of the American Indian and included data on the impacts of fractionation. This report, which became known as the Meriam Report, was issued in 1928. Its conclusions and recommendations formed the basis for land reform provisions that were included in what would become the IRA. The original versions of the IRA included two key titles, one dealing with probate and the other with land consolidation. Because of opposition to many of these provisions in Indian Country, often by the major European-American ranchers and industry who leased land and other private interests, most were removed while Congress was considering the bill. The final version of the IRA included only a few basic land reform and probate measures. Although Congress enabled major reforms in the structure of tribes through the IRA and stopped the allotment process, it did not meaningfully address fractionation as had been envisioned by John Collier, then Commissioner of Indian Affairs, or the Brookings Institution.
In 1922, the General Accounting Office (GAO) conducted an audit of 12 reservations to determine the severity of fractionation on those reservations. The GAO found that on the 12 reservations for which it compiled data, there were approximately 80,000 discrete owners but, because of fractionation, there were over a million ownership records associated with those owners. The GAO also found that if the land were physically divided by the fractional interests, many of these interests would represent less than one square foot of ground. In early 2002, the Department of the Interior attempted to replicate the audit methodology used by GAO and to update the GAO report data to assess the continued growth of fractionation; it found that it increased by more than 40% between 1992 and 2002.
As an example of continuing fractionation, consider a real tract identified in 1987 in "Hodel v. Irving", 481 U.S. 704 (1987):
Tract 1305 is 40 acre and produces $1,080 in income annually. It is valued at $8,000. It has 439 owners, one-third of whom receive less than $.05 in annual rent and two-thirds of whom receive less than $1. The largest interest holder receives $82.85 annually. The common denominator used to compute fractional interests in the property is 3,394,923,840,000. The smallest heir receives $.01 every 177 years. If the tract were sold (assuming the 439 owners could agree) for its estimated $8,000 value, he would be entitled to $.000418. The administrative costs of handling this tract are estimated by the Bureau of Indian Affairs at $17,560 annually.
Today, this tract produces $2,000 in income annually and is valued at $22,000. It now has 505 owners but the common denominator used to compute fractional interests has grown to 220,670,049,600,000. If the tract were sold (assuming the 505 owners could agree) for its estimated $22,000 value, the smallest heir would now be entitled to $.00001824. The administrative costs of handling this tract in 2003 are estimated by the BIA at $42,800.
Fractionation has become significantly worse. As noted above, in some cases the land is so highly fractionated that it can never be made productive. With such small ownership interests, it is nearly impossible to obtain the level of consent necessary to lease the land. In addition, to manage highly fractionated parcels of land, the government spends more money probating estates, maintaining title records, leasing the land, and attempting to manage and distribute tiny amounts of income to individual owners than is received in income from the land. In many cases, the costs associated with managing these lands can be significantly more than the value of the underlying asset.
Contemporary interpretations.
Angie Debo's landmark work, "And Still the Waters Run: The Betrayal of the Five Civilized Tribes" (1940), detailed how the allotment policy of the Dawes Act (as later extended to apply to the Five Civilized Tribes through such devices as the Dawes Commission and the Curtis Act of 1898) was systematically manipulated to deprive the Native Americans of their lands and resources. In the words of historian Ellen Fitzpatrick, Debo's book "advanced a crushing analysis of the corruption, moral depravity, and criminal activity that underlay white administration and execution of the allotment policy."

</doc>
<doc id="55715" url="http://en.wikipedia.org/wiki?curid=55715" title="Dingley Act">
Dingley Act

The Dingley Act of 1897 (ch. 11, 30 Stat. , July 24, 1897), introduced by U.S. Representative Nelson Dingley, Jr., of Maine, raised tariffs in United States to counteract the Wilson–Gorman Tariff Act of 1894, which had lowered rates. 
Following the election of 1896, McKinley followed through with his promises for protectionism. Congress imposed duties on wool and hides which had been duty-free since 1872. Rates were increased on woolens, linens, silks, china, and sugar (the tax rates for which doubled). The Dingley Tariff remained in effect for twelve years, making it the longest-lived tariff in U.S. history. It was also the highest in U.S. history, averaging about 52% in its first year of operation. Over the life of the tariff, the rate averaged at around 47%.
The Dingley Act remained in effect until the Payne-Aldrich Tariff Act of 1909.

</doc>
<doc id="55716" url="http://en.wikipedia.org/wiki?curid=55716" title="Hua Mulan">
Hua Mulan

Hua Mulan or Fa Mulan () is a legendary woman warrior from ancient China who was originally described in a poem known as the "Ballad of Mulan" (木蘭辭). In the poem, Hua Mulan takes her aged father's place in the army. She fought for twelve years and gained high merit, but she refused any reward and retired to her hometown instead.
The historical setting of Hua Mulan is uncertain. Xu Wei's play version from the 16th century places her in the Northern Wei dynasty (386–536), whereas the later romance "Sui Tang Yanyi" has her active around the founding of the Tang, ca. 620. The novel is consistent insofar as it describes Mulan's father as stemming from the people of the Northern Wei.
The Hua Mulan crater on Venus is named after her.
History.
The "Ballad of Mulan" was first transcribed in the "Musical Records of Old and New" ( 古今乐录, 古今樂錄) in the 6th century, the century before the founding of the Tang Dynasty. The original work no longer exists, and the original text of this poem comes from another work known as the "Music Bureau Collection" ( 乐府诗, 樂府詩), an anthology of lyrics, songs, and poems, compiled by Guo Maoqian during the 11th or 12th century. The author explicitly mentions the "Musical Records of Old and New" as his source for the poem.
The poem is a ballad, meaning that the lines do not necessarily have equal numbers of syllables. The poem consists of 31 couplets, and is mostly composed of five-character phrases, with a few extending to seven or nine.
There was no treatment of the legend since the two 12th century poems, until in the late Ming Dynasty playwright Xu Wei (d. 1593) dramatized the tale as "The Female Mulan" ("Ci Mulan" 雌木蘭), in two acts.
Later, the character of Mulan was incorporated into the historical romance "Sui Tang Yanyi", written by Chu Renhuo in the 17th century early in the Qing Dynasty.
Over time, the story of Hua Mulan rose in popularity as a folk tale among the Chinese people on the same level as the Butterfly Lovers. 
Name.
In Chinese, "mùlán" ( 木兰, 木蘭, "wood-orchid") refers to the magnolia. The heroine of the poem is given different family names in different versions of her story. According to "History of the Ming", her family name is Zhu (朱), while the "History of the Qing" say it is Wei (魏). The family name 花 ("Huā", "flower"), which was introduced by Xu Wei, has become the most popular in recent years in part because of its more poetic meaning.
Plot.
The poem starts with Mulan sitting worriedly at her loom, as one male from each family is called to serve in the army to defend China from invaders. Her father is old and weak and her younger brother is just a child, so she decides to take his place and bids farewell to her parents. After twelve years of fighting, the army returns and the warriors are rewarded. Mulan turns down an official post, and asks only for a swift horse to carry her home. She is greeted with joy by her family. Mulan dons her old clothes and meets her comrades, who are shocked that in their years traveling together, they did not realize that she was a woman.
Sui Tang Yanyi.
Chu Renho's romance "Sui Tang Yanyi" (c. 1675) provides additional backdrops and plot-twists. Here, Mulan lives under the rule of the Turkic khan (identified as Heshana Khan of the Western Khaganate). When the khan agrees to wage war in alliance with the emergent Tang dynasty (which was poised to conquer all of China), Mulan's father Hua Hu (花弧) fears he will be conscripted into military service, since he only has two daughters and an infant son. Mulan cross-dresses as a man, and enlists in her father's stead. She is intercepted by the forces of the petty king Dou Jiande, and is brought under questioning by the king's militant daughter Xianniang ( 线娘, 線娘), who tries to recruit Mulan (thinking her a man), but upon discovering Mulan to be of kindred spirit (a female warrior like herself), is so overwhelmingly delighted that they become sworn sisters.
In the "Sui Tang Yanyi", Mulan comes to a tragic end, a "detail that cannot be found in any previous legends or stories associated Hua Mulan," and believed to have been interpolated by the author Chu Renho. Princess Xianniang's father is vanquished after siding with the enemy of the Tang dynasty, and the two sworn sisters with knives in their mouths surrender themselves to be executed in the place of the condemned man. The act of filial piety wins reprieve from the Emperor, and the imperial consort who was birth-mother to the prince bestows money to Mulan to provide for her parents, and wedding funds for the princess who confessed to having promised herself to general Luo Cheng ( 罗成, 羅成).
Mulan is given leave to journey back to her homeland, and once arrangements were made for Mulan's parents to relocate, it is expected that they will all be living in the princess's old capital of Leshou ( 乐寿, 樂壽, the modern-day city of Cangzhou, Xian County, Hebei Province). But Mulan is devastated to discover her father has long died and her mother has remarried. Even worse, the khan has summoned her to the palace to become his concubine. Rather than to suffer this fate, she commits suicide. But before she dies, she entrusts an errand to her younger sister, Youlan ( 又兰, 又蘭), which was to deliver a letter from the princess Xianniang's letter to her fiancé Luo Cheng. This younger sister dresses as a man to make her delivery, but her disguise is discovered, and it arouses her recipient's amorous attention.
In the novel, Mulan's father was of the ethnically Mongolic Xianbei tribe (described as "a Hebei person of the people of the Northern Wei dynasty, ruled by the Tuoba clan"), while her mother was Han Chinese from the Central Plain. But "even a half-Chinese woman would prefer death by her own hand to serving a foreign ruler," as some commentators have explained as this Mulan character's motive for committing suicide.
Mulan's words before she committed suicide were "I'm a girl, I been through war and did enough. I now want to be with my father."
Modern adaptations.
The story of Hua Mulan has inspired a number of screen and stage adaptations in the modern era, which include:
References.
</dl>

</doc>
<doc id="55717" url="http://en.wikipedia.org/wiki?curid=55717" title="Mulan (Disney character)">
Mulan (Disney character)

Fa Mulan is a fictional character who appears in Walt Disney Pictures' 36th animated feature film "Mulan "(1998) and its sequel "Mulan II "(2004). Chinese-American actress Ming-Na Wen provides the character's speaking voice and Filipina singer Lea Salonga provides her singing voice. Originally, Salonga had been cast as both the speaking and singing voices of Mulan until the directors, dissatisfied with the singer's impersonation of Mulan's male alter-ego, ultimately replaced her with Wen who, according to co-director Tony Bancroft, sounds "very Chinese." The role was Wen's voice-acting debut.
Created by Robert D. San Souci and animated by Mark Henn, the character is loosely based on Hua Mulan from the ancient Chinese poem the "Ballad of Mulan". In the Disney film adaptation, Mulan is depicted as the spirited and tomboyish daughter of an elderly war veteran. Having been deemed unsuitable for marriage by the Matchmaker, Mulan lives in fear of dishonoring her parents. When her aging father is conscripted by the Chinese military in an attempt to defend the country against the invading Huns, Mulan, aware that her injured and feeble father is incapable of surviving another war, ultimately decides to breach tradition, violating the law by disguising herself as a man in order to masquerade as a soldier and enlist herself in the army in her father's place.
Inspired by the well-being of his own two daughters, director Tony Bancroft encouraged the filmmakers and writers to characterize Mulan as a distinctly different and unique kind of Disney heroine, specifically one who is strong, independent and self-sufficient, and whose fate is ultimately not dependent on a male character. Physically, Henn additionally designed Mulan to appear less feminine than traditional Disney animated heroines.
Reception towards Mulan has been varied but mostly positive, with film critics praising her personality and heroism. However, some critics have reacted much less positively towards Mulan's characterization, deeming her a familiar and "Westernized" character. Additionally, critical reception towards the character's relationship with Li Shang has also been largely unfavorable, with several critics citing that Mulan's independence and heroism was ultimately compromised by the film's romantic conclusion. Chronologically, Mulan is the 8th official member of the Disney Princess franchise. She is also the franchise's first East Asian princess. Critics have observed several ways in which Mulan and her role in the film differs from those of traditional Disney Princesses, drawing similarities between Mulan and preceding Disney Princesses Ariel from "The Little Mermaid "(1989), Belle from "Beauty and the Beast "(1991), Jasmine from "Aladdin "(1992) and Pocahontas from "Pocahontas "(1995).
Development.
Conception.
"Mulan "was originally conceived in 1994 as an animated short film entitled "China Doll", in which the heroine was initially supposed to have been depicted as "an oppressed and miserable Chinese girl who is whisked away by a British Prince Charming to happiness in the West." While researching, exploring and developing a series of traditional stories, fairy tales and folktales, award-winning children's book author and Disney story consultant Robert D. San Souci uncovered the "Ballad of Mulan", an ancient Chinese poem conveying the story of Hua Mulan—a young Chinese woman who takes her ailing father's place in battle by disguising herself as a man. San Souci explained that, as a writer, "[Hua Mulan's] story fascinated me," and suggested to Disney that the studio ultimately combines the two stories. San Souci himself was hired to write the film's treatment and story.
Thematically, "Mulan "explores the age-old idea and concept of remaining "true to yourself," with co-director Tony Bancroft summarizing Mulan's role in the film as "the story of a girl who can't help who she is but she exists in a different society that tells her who she is supposed to be." San Souci wanted to "keep ... the integrity of this heroine," as the "Ballad of Mulan" is "so well known and so beloved". However, certain creative liberties were taken with the story specifically pertaining to Mulan's role. For example, in the original poem, Mulan first seeks permission from her rather supportive parents prior to enlisting herself in the army, whereas in the film she runs away from home as they sleep, and her surname was also changed from "Hua" to "Fa." Most notably, Mulan's true identity as a woman is revealed in the Disney film adaptation much earlier than it is in the poem; in the film her gender is discovered after the army's initial encounter with the enemy, whereas in the legend her comrades never discovered who she was in their twelve years at war, and only realized the truth after she had returned home.
In addition, unlike preceding traditional Disney animated feature films, the developing romantic relationship between Mulan and Li Shang is treated as more of a subplot as opposed to a traditional central plot, as observed by film critic Andy Klein of Animation World Network. Klein commented, "Mulan isn't waiting for her prince to someday come; when he does arrive, having known her primarily as a man, and having learned to admire her for her deeper qualities, the romance is muted and subtle."
Voice.
Mulan's speaking voice is provided by Chinese-American actress Ming-Na Wen. Because the character "represented [traditional] Chinese values" and is depicted as being "dramatic ... close to her father, very respectful," Bancroft believed that Wen possessed the "perfect" voice for Mulan, which he additionally described as "very Chinese." Born and raised in Macau, China, Wen was very much familiar with both the legend of Hua Mulan and the "Ballad of Mulan "at the time of her audition for the role, having grown up being read the poem by her mother. Wen explained, "I think every Chinese kid grows up with this story," additionally likening the poem's popularity in China to that of the Western Parson Weems fable in which American president George Washington chops down his father's beloved cherry tree.
Mulan served as Wen's first voice-acting role. In an interview with IGN, the actress elaborated on the recording process, specifically the fact that she was required to record the majority of the character's dialogue in isolation, saying, "I just loved the story so much and identified so much with the character of Mulan it was easy for me. I loved using my imagination. I felt like I was a little kid again, being silly with an imaginary sword and riding on an imaginary horse and talking to an imaginary dragon. So it was a lot of fun for me." In spite of the fact that, throughout the film, Mulan shares several intimate scenes with her guardian, a miniature Chinese dragon named Mushu who is voiced by American actor and comedian Eddie Murphy, Wen and her co-star never actually encountered each other while working on "Mulan "due to the fact that they recorded their respective dialogue at separate times in separate locations.
Upon being cast as Mulan's speaking voice, Wen was immediately informed by Disney that she would not be providing the character's singing voice. The actress took no offense to this decision, commenting jokingly "I don't blame them." The directors hired Filipina singer and actress Lea Salonga to dub the character's respective singing voice, heard in the film's songs "Reflection", "I'll Make a Man Out of You" and "A Girl Worth Fighting For", on Wen's behalf. According to Thomas S. Hischak, author of the book "Disney Voice Actors: A Biographical Dictionary", Salonga was originally cast to provide both Mulan's speaking and singing voices. However, the directors eventually felt that her attempt at impersonating a man in the form of Mulan's male alter-ego "Ping" was rather unconvincing, and ultimately replacing Salonga with Wen. Six years prior to "Mulan", Salonga provided the singing voice of Princess Jasmine in Disney's "Aladdin "(1992) on behalf of American actress Linda Larkin. While auditioning for Mulan, Salonga asked jokingly, "Why do I have to audition? ... I was already a princess before. Wasn't that enough?"
Characterization, design and analysis.
The film's screenplay was constantly being revised and re-written. Naturally, so was Mulan's characterization and role in the film. The writers wanted Mulan to represent a "different kind of Disney heroine," specifically described as one who "didn't need a tiara, but was still just as much as graceful, strong, and courageous." Between the two, Bancroft and his twin brother Tom, an animator who also worked on "Mulan", have a total of seven daughters. This further inspired the filmmakers to portray Mulan as a unique heroine who is "not another damsel in distress" in favor of having her resemble "a strong female Disney character who would truly be the heroine of her own story" instead, essentially a "female role model. The characteristics of strength and courage were a must for Mulan." In an interview with "The Christian Post", Bancroft elaborated on the way in which he, as the film's director, continued to consider the well-being of his two young daughters while working on "Mulan", having "wanted to make ... a unique heroine that hadn't been seen before" and provide for them "someone who would be strong on her own, without a prince saving her." Addressing the way in which Mulan differs from traditional Disney heroines and princesses, Bancroft explained, "Most Disney heroines have an outside source that comes in and helps them change. Mulan stays consistent. From the first frame all the way through the end of the movie, her personality, her drive it all stays the same." 
Visually, the animators were influenced by both traditional Chinese and Japanese artwork. In the specific case of Mulan, "The characters' simple lines ... resemble classic Asian painting", as demonstrated by the animators' innovate "'less is more' approach" to traditional animation. Chinese artist Chen Yi mentored the animators, "helping [them] to come up with these designs." Mark Henn served as Mulan's supervising animator. Animating the character in her male disguise as "Ping" offered an unprecedented challenge for Henn. In order to solve this unique dilemma, Henn was provided with "the opportunity to ... adjust her design a little bit so that when she was disguised as Ping, as a soldier, that she was physically a little different in how we drew her than when she was herself as Mulan." Physically, Mulan was also designed to appear less feminine than preceding traditional Disney animated heroines, specifically Pocahontas from "Pocahontas "(1995) and Esmeralda from "The Hunchback of Notre Dame "(1996), because "you can't pass as a man in the army with a Barbie-style figure."
Henn revealed that he was drawn to "Mulan's story [because it] was so unique and compelling that it just captivated me from the beginning". Animating the characters' distinct emotions using the traditional Chinese style turned out to be somewhat challenging for Henn. The animator explained, "We don't create realism in the sense that if you're doing a human character, it's not going to look realistic ... the balance is finding an appealing way of drawing using the visual tools that you have in the design to convey the believable emotions that you want to get across." In addition to Mulan, Henn was also responsible for animating Fa Zhou, Mulan's elderly father. He described the complex relationship between the two characters as "the emotional heart of the story". Fathering one daughter himself, Henn drew inspiration from his own emotions as well as past personal experiences while animating several intimate scenes shared by the two characters.
Several film critics have described Mulan as a tomboy. Andy Patrizio of IGN observed, "In this slightly modernized version of the story, Mulan ... is something of a rebel and a tomboy. She has no interest in being a good little subservient wife, despite her sighing parents' wishes." Jo Johnson, in contribution to the book "Queers in American Popular Culture Volume 1: Film and Television", wrote that "Unlike other Disney heroines, Mulan is immediately coded as a tomboy," observing the way in which the character speaks using a full mouth. Johnson additionally noticed several ways in which Mulan's design and personality differ from those typically associated with traditional Disney heroines and princesses, citing the character's clumsy, awkward demeanor; broad shoulders and muscular limbs; unruly single strand of hair; and choice of everyday attire which usually consists of loose, baggy clothing concealing her "traditionally slim Disney waist." Additionally, Mulan's intelligence has been observed in several professional analyses, with critics often citing the character as "brainy."
Appearances.
"Mulan".
The Huns, led by Shan Yu, invade Han China, forcing the Chinese emperor to command a general mobilization. The emperor requires one man from each family to join the Chinese army. When Fa Mulan hears that her elderly father Fa Zhou, the only man in their family, is forced to rejoin the army, she decides to stand in his place, disguising herself as a young man named "Ping". Mulan's family learns that she has taken Fa Zhou's place and pray to their family ancestors, who then order their "Great Stone Dragon" to protect her. The ancestors are unaware that the statue of Great Stone Dragon failed to come to life, and that Mushu, a small dragon is the one to go and protect Mulan.
Mulan is initially misguided by Mushu in how to behave like a man. However, under command of Li Shang, she and her new friends at the camp, Yao, Ling and Chien-Po, become skilled warriors. Mushu, desiring to see Mulan succeed, creates a fake order from Li Shang's father, General Li, ordering Li Shang to follow them into the mountains. They arrive at a burnt-out village and discover that General Li and his forces have been wiped out by the Huns. As they solemnly leave the mountains, they are ambushed by the Hans when Mushu accidentally fired a cannon causing their position to be given away, but a second firing of a cannon by Mulan buries most of the enemy forces in an avalanche. Mulan is slashed by Shan Yu in his rage at her wiping out his army during the battle, and she is forced to reveal her deception when she receives medical attention. Instead of executing Mulan as the law states, Li Shang spares her life for saving his life from the avalanche by leaving her on the mountain as the rest of the army departs for the Imperial City to report the news of the Huns' demise. However, the avalanche failed to eliminate all the enemies, as Mulan catches sight of a small number of surviving Huns, including Shan Yu, making their way to the City, intent on capturing the Emperor.
In the Imperial City, Mulan attempts to warn Li Shang about Shan Yu, but he refuses to listen. The Huns appear and capture the Emperor, locking themselves inside the palace. With Mulan's help, Li Shang, Yao, Ling, and Chien-Po pose as concubines and are able to enter the palace and defeat Shan Yu's men. As Shang prevents Shan Yu from assassinating the Emperor, Mulan lures the Hun onto the roof where she engages him in single combat. Meanwhile, acting on Mulan's instructions, Mushu fires a bundle of fireworks rockets at Shan Yu on her signal and kills him.
Mulan is praised by the Emperor and the people of China, who all bow to her, an unprecedented honor. Mulan accepts the Emperor's crest and Shan Yu's sword as gifts, but politely declines his offer to be his advisor and instead asks to return to her family. She returns home and presents the imperial gifts to her father, but he is more overjoyed to have his daughter back safely. Li Shang, who has become enamored with Mulan, soon arrives under the guise of returning her helmet, but accepts the family's invitation for dinner. Earlier in the film, Mulan was declared unfit for marriage, but this now appears not to be the case with her strong budding romance with Li Shang. Mushu is granted a position as a Fa family guardian by the ancestors amid a returning celebration.
"Mulan II".
The sequel finds Mulan and Li Shang preparing to marry but distracted by a task from the Emperor, who wants his three daughters escorted to their own marriage ceremony. Their romantic relationship becomes somewhat strained during the trip, as the romantic couple has differing views on various issues. Meanwhile, Mushu realizes that if Mulan marries Shang, she will not need him anymore as her guardian spirit. Taking advantage of this, he manages to trick the two into breaking up. When bandits attack, Mulan and Shang fight them off, but Mulan is devastated when Shang is seemingly killed trying to save her. To make sure the three princesses are not forced to marry against their will, Mulan takes their place marrying the eldest son of the ruler of the neighboring land. Shang survives the accident and arrives in time to stop the wedding but ultimately Mulan is saved by Mushu who, posing as the mighty Golden Dragon of Unity, frees the three princesses from their vows, and marries Mulan and Li Shang himself causing Mulan to forgive him for his actions.
Miscellaneous.
Mulan is the eighth official member the Disney Princess franchise, a media franchise marketed towards young girls. Featured on the official Disney Princess website, the character's brief biography reads, "Mulan is a loving girl who is always brave and bold. When her country needs it most, she disguises herself as a man and goes off to fight. She uses courage and determination to win the day." Interestingly, Mulan is currently the only official member of the Disney Princess franchise who is technically not a legitimate princess "in the traditional sense" because of the fact that she was not born the daughter of a king or queen, nor does she become princess consort by marrying a prince. Additionally, she is also the franchise's first and currently only East Asian member.
Mulan appears as a playable character in "", an action video game released in December 1999 by Disney Interactive Studios exclusively for the video game console Sony PlayStation. Loosely based on the plot of the original animated film, the video game's concept and premise revolves around "Players ... assum[ing] the role of Mulan on her quest to recover the missing scrolls." Mulan also appears as a playable character in "Disney's Mulan", a similar video game released the previous year on October 10, 1998 by THQ for Nintendo Game Boy. Additionally, Mulan appears in the video game "Kingdom Hearts II" in the Land of the Dragons world. She aids Sora in battle, taking the place of either Donald or Goofy. She uses a jian called "Sword of the Ancestor" for regular combat, and her combination attacks include Red Rocket and other fire attacks, thanks to Mushu. She goes under her pseudonym (Ping) for the majority of Sora's first visit to her world, but has abandoned it by the time of their second visit, which follows an original storyline.
Mulan makes cameo appearances in the "Disney's House of Mouse" television series and the direct-to-video release "". She was scheduled to appear in the second installment of the Disney Princess Enchanted Tales series of DVDs along with Cinderella. It was to premiere in 2008 but was cancelled due to poor sales of the first DVD.
Mulan appears regularly for meet-and-greets, parades and shows at the Walt Disney Parks and Resorts, including at the Chinese Pavilion at Epcot. Mulan and Mushu, as a kite, make cameo appearances in the Hong Kong Disneyland and Disneyland Resort versions of It's a Small World. In Disneyland, she also makes appearances in the Disney Princess Fantasy Faire Village and regularly performs in the new show "Mickey and the Magical Map" in the Fantasyland Theater. In the aforementioned show, she performs a trio with fellow Disney Princesses Pocahontas and Rapunzel. As a tribute, there is a portrait of her along other Disney Princesses at the Princess Fairytale Hall at the Magic Kingdom.
On the Disney Cruise Line ships and in Hong Kong Disneyland, Mulan and Li Shang appear in the stage show "The Golden Mickeys". Mulan is also known to come out for meet-and-greets on the ships as well. She is also featured in the Disney on Ice shows "Princess Classics" and "Princess Wishes."
In the Square Enix and Disney game "Kingdom Hearts II", Mulan is a playable character when the protagonists visit her world.
Actress Jamie Chung portrays a live-action version of Mulan in the second and third seasons of the ABC television series, "Once Upon a Time". She first appeared in the series' second season premiere, "Broken", assisting Prince Phillip in rescuing Aurora. The "Once Upon a Time" version of Mulan differs from the film version as she is in love with Aurora, as revealed in the 2013 episode "Quite a Common Fairy".
On August 2014, Ming-Na Wen and Lea Salonga reprise their roles as Mulan for the first time since "Mulan 2" in the Disney Channel show "Sofia the First". In the episode "Princesses To The Rescue," Mulan reminds Sofia and her friends Amber and Jun they are "Stronger Than They Know" in song. Mulan is the 7th Disney princess to appear on the show.
Reception and legacy.
Critical response.
As a character, critical reception towards Mulan has been varied but generally positive. "Time Out "described Mulan as "A feisty young go-getter [who] rises above the male-dominated world in which she lives." Similarly, Ken Fox of "TV Guide "wrote, "Intelligent and fiercely independent, Mulan ... runs afoul of social expectations that a woman will be always obedient and duty-bound to her husband." Bridget Byrne of "Boxoffice "commented, "The physical and dramatic energy of ... Mulan ... almost triumph over the old-fashioned forces of Disney tradition." Byrne continued, "Mulan ... has pride, charm, spirit and aesthetic appeal which prevents her from being upstaged by the vigorous and exciting action in which she participates." "Variety"'s Todd McCarthy praised the character for inspiring "a turn of the circle from such age-old Disney classics ... in which passive heroines were rescued by blandly noble princes. Here, it's the girl who does the rescuing, saving not only the prince but the emperor himself from oblivion, and this in a distant culture where women were expected to obey strictly prescribed rules." Similarly, Margaret A. McGurk of "The Cincinnati Inquirer" lauded Mulan for "solv[ing] her "G.I. Jane" dilemma by proving that brains can do more than brawn." Hailing the character as "Among the strongest heroines in Walt's cartoon canon," Ian Freer of "Empire "enthused, "Mulan's engaging mixture of vulnerability and derring-do becomes incredibly easy to root for." Hollis Chacona of "The Austin Chronicle "wrote that "smart, brave, beautiful," Mulan is a "winning protagonist." Likewise, the "Los Angeles Times"' Kenneth Turan wrote, "As a vivacious rebel who has to be true to herself no matter what, Mulan is an excellent heroine, perfect for the young female demographic the studio is most anxious to attract." Turan additionally stated, "this resourceful, can-do young woman is a more likable and resourceful role model than Pocahontas was."
Although largely well-liked, Mulan's characterization has drawn some mild criticism and speculation, inspiring a series of generally mixed to positive reviews from some film critics. "Entertainment Weekly"'s Owen Gleiberman wrote, "Far more than "Beauty and the Beast" or the stolidly virtuous "Pocahontas", "Mulan" showcases a girl who gets to use her "wits "... a testament to the power of mind over brawn." However, Gleiberman continued, ""Mulan" finally falls a notch short of Disney's best ... because the heroine's empowerment remains ... an emotionally isolated quest." Similarly, Moira Macdonald of "The Seattle Times "hailed Mulan as "a strong, engaging character who, unlike many of her Disney counterparts, needs no one to rescue her from danger," while questioning her personality, asking, "was it really necessary to bestow Mulan with self-esteem problems? Because she seems so confident and intelligent, her sad statement that she wants to 'see something worthwhile' in the mirror comes as a bit of a shock."
Critics were not unanimous in their praise. "The Phoenix"'s Jeffrey Gantz felt that character was unoriginal, inaccurate and Westernized, writing, "[her] costumes (particularly the kimono and obi Mulan wears to the Matchmaker) and hairdos look Japanese ... Give Mulan Native American features and you have Pocahontas." Similarly, James Berardinelli of "ReelViews "felt that the character's depiction was too "familiar," reviewing, "Although she looks different from Ariel, Belle, Jasmine, and Pocahontas, Mulan is very much the same type of individual: a woman with a strong, independent streak who is unwilling to bend to the customs of her culture, which decree that the role of the female is to be ornamental. The film isn't very subtle in reinforcing the idea of equality between the sexes". Additionally, some critics, such as Alex von Tunzelmann of "The Guardian", have criticized Mulan for her violence, writing, "Disney struggles to make Mulan both a killer and a heroine ... Gingerly, the film attempts to tread a middle path, implying that Mulan annihilates most of the Hun army by causing an avalanche, and having her dispatch Shan Yu with a load of fireworks. Very pretty. But still technically killing." However, von Tunzelmann did conclude more positively, "as Disney heroines go, Mulan herself is a clear improvement on the standard-issue drippy princess."
Relationship with Shang.
Unlike the generally positive reviews received by Mulan, critical reception towards the character's romantic relationship with Li Shang has been largely negative, drawing much speculation from critics who accused "Mulan "of having "a typical girl-hooks-up-with-boy ending." Roger Ebert of the "Chicago Sun-Times "observed, "The message here is standard feminist empowerment: Defy the matchmaker, dress as a boy, and choose your own career. But "Mulan" has it both ways, since inevitably Mulan's heart goes pitty-pat over Shang, the handsome young captain she's assigned to serve under. The movie breaks with the tradition in which the male hero rescues the heroine, but is still totally sold on the Western idea of romantic love." "The New York Times' "Janet Maslin negatively opined, "For all of Mulan's courage and independence in rebelling against the matchmakers, this is still enough of a fairy tale to need Mr. Right."
Citing Mulan's relationship with Shang as an example of sexism, a film critic writing for "Teen Ink" wrote:
"Mulan has been hailed as a feminist Disney movie because it showcases a young woman who leads China to victory using her quick wit, pride, and a strong sense of family honor—all while masquerading as a man named Ping. Even though Mulan (as Ping) gains the respect of the army commander and her comrades, once they discover that she is a woman, her army commander and potential love-interest, Shang, loses respect for her and even hates her. "Ping" had been doing an even better job than Shang, but when Shang finds out Ping is a woman, his stupid male ego breaks on impact. Mulan is sentenced to death, and Shang, the macho man of the film, ultimately gets to decide her fate. The only reason she survives is because Shang decides he'd rather just send her home. Wow. To add insult to injury, at the end of the film, Shang fixes up his shattered ego by claiming Mulan as a suitor. Even as Mulan is being praised and cheered in the Forbidden City after she almost single-handedly saves China (this time, as a woman), at the end of the film, the audience is reminded that Mulan is really just another woman looking for a man. Mulan's real victory isn't saving her country from invasion. No, it's marrying Shang."— "Teen Ink" 
Betsy Wallace of Common Sense Media observed that Mulan "doesn't fit the princess mold, and most moviegoers had never heard of her." Conclusively, Wallace wrote, "it's too bad that in the end she still needs to be married off to a 'Prince Charming' who saves the day." In contribution to the book "Beyond Adaptation: Essays on Radical Transformations of Original Works", Lan Dong wrote, "Even though Mulan achieves success after she resumes her female self ... it is compromised by Mulan and Li Shang's potential engagement at the end of the film."
Cultural significance and accolades.
Mulan is culturally recognized for her unique role in "Mulan "specifically in regards to the character's heroism, ethnicity and disinterest in romance, serving as a departure from traditional Disney heroines and princesses because she "challenged gender stereotypes and offered up an animated Disney experience that isn't princess-centric" as "one of the few strong, self-propelled female characters that Disney has." Kenneth Turan of the "Los Angeles Times "observed the way in which Mulan's role in the film as "an independent, not completely boy-crazy heroine is somewhat new for Disney." According to Sara Veal of "The Jakarta Post", Mulan "promotes self-reliance, determination and is uninterested in marriage or romance ... the film ends on her saving her country, rather than a romantic resolution." Succeeding non-white Disney Princesses Jasmine and Pocahontas, Mulan's characterization as Disney's first East Asian princess assisted in the diversification of the Disney Princess franchise, introducing "Disney princesses ... portrayed as women of color." Peter Travers of the "Rolling Stone "commented, "Mulan ... makes a feisty prefeminist," continuing, "She doesn't swoon over Captain Shang, the hunky officer ... which leaves Shang ... frustrated ... Mulan, let the record show, does not put out." PopMatters' Jesse Hassenger wrote that unlike other Disney films, ""Mulan" holds the advantage of a smart, strong heroine—not just a superhot princess figure." Ryan Mazie of Box Office Prophets felt that "Mulan" "might be the most important and forward-thinking Disney Princess movie made up until that point where the female character solely takes control over her own destiny without the aid of a mighty Prince."
In 2012, CNN's Stephanie Goldberg recognized Mulan as one of Disney's bravest and most heroic animated heroines to-date in her article ""Brave"'s Merida and other animated heroines," writing, "Mulan bent traditional gender roles when she took her father's place in the Chinese army." The Georgia Institute of Technology ranked Mulan the fourteenth greatest Disney character of all-time. Similarly, in 2013, Mulan was ranked the greatest animated Disney heroine according to a poll conducted by Jim Vejvoda of IGN.
In 1999, "Mulan"'s theme song "Reflection", performed by Mulan, was nominated for the Golden Globe Award for Best Original Song at the 56th Golden Globe Awards, but ultimately lost to Celine Dion and Andrea Bocelli's "The Prayer" from "Quest for Camelot "(1998). "Reflection" is often accredited with establishing the successful musical career of American recording artist Christina Aguilera, who famously recorded a pop rendition of the ballad prior to the release of her platinum-selling self-titled debut album in 1999, on which the song is featured. Additionally, the song peaked at number nineteen on the "Billboard"Adult Contemporary chart. In 2011, Salonga was honored with a Disney Legends award in commemoration of her role as Mulan's singing voice. Additionally, Salonga performed a live rendition of "Reflection" at the ceremony.
Controversy.
The 2013 Disney princess redesigns portrayed Mulan with features that differ from her film appearance. The artwork featured Mulan with blue eyes, bigger lips, noticeably lighter skin, and golden clothing which does not resemble any outfit she has worn in the film. Her new appearance has caused an uproar due to the whitewash of her character. This was particularly troubling as Mulan is one of the few princesses of color. Shavon L. McKinstry of "SPARK Movement" writes that Mulan's redesign "seem to be directly counter to her personality and character in her film", and also notes how all the princesses of color have been "noticeably pushed to the back or left out completely" from the new Disney merchandise which featured the redesigns.
McKinstry argues that Disney "prefers to portray one demographic of princess, simultaneously alienating so much of their fanbase", pointing out that of the "ten Disney Princesses in the brand, six are white". The importance of Mulan and other non-white princesses can be seen in the 2009 study of the effects of children's cartoons on the body image of young girls by doctors Sharon Hayes and Stacey Tantleff-Dunn. The study revealed that in the group of girls ranging from 3 to 6 years old, 30.6% of the group would change their physical appearance if they could. Of these respondents, over half would change their hair and over a quarter would change something about their body, such as skin color. Of all girls surveyed, 8% said they would have to change their hair or skin color to become a princess, stating things like they would "change from brown skin to white skin", for example. The interviewed group was predominantly white.
A Change.org petition was created in protest to the redesign, stating that the new designs "show a blatant disregard for the image issues facing young girls today", calling for change as "Disney shapes society's view of girls and girls' view of themselves." Disney has since altered the coloration in Mulan's design by changing the blue eye highlight to brown, darkening the color of her skin, and changing her clothing to better resemble her attire in the film.

</doc>
<doc id="55718" url="http://en.wikipedia.org/wiki?curid=55718" title="Foraker Act">
Foraker Act

The Foraker Act, , 31 Stat. , enacted  12, 1900, officially known as the Organic Act of 1900, is a United States federal law that established civilian (albeit limited popular) government on the island of Puerto Rico, which had recently become a possession of the United States as a result of the Spanish–American War. Section VII of the Foraker Act also established Puerto Rican citizenship. President William McKinley signed the act on April 12, 1900 and it became known as the "Foraker Act" after its sponsor, Ohio Senator Joseph B. Foraker. Its main author has been identified as Secretary of War Elihu Root.
The new government had a governor and an 11-member executive council appointed by the President of the United States, a House of Representatives with 35 elected members, a judicial system with a Supreme Court and a United States District Court, and a non-voting Resident Commissioner in Congress. The Executive council was all appointed: five individuals were selected from Puerto Rico residents while the rest were from those in top cabinet positions, including attorney general and chief of police (also appointed by the President). The Insular Supreme Court was also appointed. In addition, all federal laws of the United States were to be in effect on the island. The first civil governor of the island under the Foraker Act was Charles H. Allen, inaugurated on May 1, 1900 in San Juan, Puerto Rico. This law was superseded in 1917 by the Jones–Shafroth Act.

</doc>
<doc id="55719" url="http://en.wikipedia.org/wiki?curid=55719" title="Gold Standard Act">
Gold Standard Act

The Gold Standard Act of the United States was passed in 1900 (approved on March 14) and established gold as the only standard for redeeming paper money, stopping bimetallism (which had allowed silver in exchange for gold). It was signed by President William McKinley.
The Act made the "de facto" gold standard in place since the Coinage Act of 1873 (whereby debt holders could demand reimbursement in whatever metal was preferred--usually gold) a "de jure" gold standard alongside other major European powers at the time.
The Act fixed the value of the dollar at 25 8⁄10 grains of gold at "nine-tenths fine" (90% purity), equivalent to 23.22 grains (1.5046 grams) of pure gold.
The Gold Standard Act confirmed the United States' commitment to the gold standard by assigning gold a specific dollar value (just over $20.67 per Troy ounce). This took place after McKinley sent a team to Europe to try to make a silver agreement with France and Great Britain.
On April 25, 1933, the United States and Canada dropped the gold standard.

</doc>
<doc id="55720" url="http://en.wikipedia.org/wiki?curid=55720" title="Mulan (1998 film)">
Mulan (1998 film)

 
Mulan is a 1998 American animated musical action-comedy-drama film produced by Walt Disney Feature Animation based on the Chinese legend of Hua Mulan. The 36th animated feature in the Walt Disney Animated Classics, it was directed by Tony Bancroft and Barry Cook, with story by Robert D. San Souci and screenplay by Rita Hsiao, Philip LaZebnik, Chris Sanders, Eugenia Bostwick-Singer, and Raymond Singer. Ming-Na, Eddie Murphy, Miguel Ferrer and BD Wong star in the English version, while Jackie Chan provided his voice for the Chinese dubs of the film. The film's plot takes place during the Han Dynasty, where Fa Mulan, daughter of aged warrior Fa Zhou, impersonates a man to take her father's place during a general conscription to counter a Hun invasion.
Released during the Disney Renaissance, "Mulan" was the first of three features produced primarily at the Disney animation studio at Disney-MGM Studios in Orlando, Florida. Development for the film began in 1994, when a number of artistic supervisors were sent to China to receive artistic and cultural inspiration. "Mulan" was well received by critics and the public, grossing $304 million, earning Golden Globe and Academy Award nominations, and winning several Annie Awards including Best Animated Feature. A 2005 direct-to-video sequel, "Mulan II", followed.
Plot.
After the Huns, led by the ruthless Shan Yu, invade Han China, the Chinese emperor begins to command a general mobilization. Each family is given a conscription notice, requiring one man from each family to join the Chinese army. When Fa Mulan hears that her elderly father Fa Zhou, the only man in their family, is once more to go to war, she becomes anxious and apprehensive. She decides to deal with this herself by disguising herself as a man so that she can go to war instead of her father. When her family learns of Mulan's departure, they all become anxious. Grandmother Fa, Mulan's grandmother, prays to the family ancestors for Mulan's safety. The ancestors then order their "Great Stone Dragon" to protect Mulan. The ancestors are unaware that the statue of Great Stone Dragon failed to come to life, and that Mushu, a small dragon, is the one to go and protect Mulan.
Mulan is misguided by Mushu in how to behave like a man, which starts a ruckus at the training camp. However, under command of Li Shang, she and her new co-workers at the camp, Yao, Ling and Chien-Po, become skilled warriors. Mushu, desiring to see Mulan succeed, creates a fake order from Li Shang's father, General Li, ordering Li Shang to follow them into the mountains. The troops set out to meet General Li, but arrive at a burnt-out encampment and discover that General Li and his troops have been wiped out by the Huns. As they solemnly leave the mountains, they are ambushed by the Huns, but Mulan cleverly uses a cannon to create an avalanche which buries most of the Huns. An enraged Shan Yu slashes her in the chest, and her deception is revealed when the wound is bandaged. Instead of executing Mulan as the law requires, Li Shang relents and decides to spare her life for saving him, but expels her from the army, stranding her on the mountain as the rest of the army departs for the Imperial City to report the news of the Huns' demise. However it is revealed that several Hun warriors including Shan Yu survive the avalanche, and Mulan catches sight of them as they make their way to the City, intent on capturing the Emperor.
At the Imperial City, Mulan attempts to warn Li Shang about Shan Yu, but he refuses to listen. The Huns appear to capture the Emperor, then they lock up the palace. With Mulan's help, Yao, Ling, and Chien-Po pose as concubines and are able to enter the palace and, with the help of Li Shang, they defeat Shan Yu's men. As Shang prevents Shan Yu from assassinating the Emperor, Mulan lures the boss Hun onto the roof where she engages him in solo combat. Meanwhile, acting on Mulan's instructions, Mushu fires a bundle of fireworks rockets at Shan Yu on her signal and kills him. Mulan is praised by the Emperor and the people of China, who all bow to her as an unprecedented honor. While she accepts the Emperor's crest and Shan Yu's sword as gifts, she politely declines his offer to be his advisor and asks to return to her family. She returns home and presents these gifts to her father, but he is more overjoyed to have his daughter back safely. Li Shang, who has become enamored with Mulan, soon arrives under the guise of returning her helmet, but accepts the family's invitation for dinner. Mushu is granted a position as a Fa family guardian by the ancestors amid a returning celebration.
Cast.
Kelly Chen, Coco Lee and Xu Qing voiced Mulan in the Cantonese, Taiwanese Mandarin and Mainland standard versions of the film respectively, while Jackie Chan provided the voice of Li Shang in all three Chinese versions and appeared in the version of promotional music videos of "I'll Make a Man Out of You".
Production.
"Mulan" originally began as a short, straight-to-video film titled "China Doll" about an oppressed and miserable Chinese girl who is whisked away by a British Prince Charming to happiness in the West. Then Disney consultant and children's book author Robert D. San Souci suggested making a movie of the Chinese poem "The Song of Fa Mu Lan", and Disney combined the two separate projects.
Development for "Mulan" began in 1994, after the production team sent a select group of artistic supervisors to China for three weeks to take photographs and drawings of local landmarks for inspiration; and to soak up local culture. The filmmakers decided to change Mulan's character to make her more appealing and selfless and turn the art style closer to Chinese painting, with watercolor and simpler design - as opposed to the details of "The Lion King" and "The Hunchback of Notre Dame".
To create 2,000 Hun soldiers during the Huns' attack sequence, the production team developed crowd simulation software called "Attila". This software allows thousands of unique characters to move autonomously. A variant of the program called "Dynasty" was used in the final battle sequence to create a crowd of 3,000 in the Forbidden City. Pixar's photorealistic open API RenderMan was used to render the crowd. Another software developed for this movie was "Faux Plane" which was used to add depth to flat two-dimensional painting. Although developed late in production progress, Faux Plane was used in five shots, including the dramatic sequence which features the Great Wall of China, and the final battle sequence when Mulan runs to the Forbidden City. During the scene in which the Chinese are bowing to Mulan, the crowd is a panoramic film of real people bowing. It was edited into the animated foreground of the scene.
Reception.
Critical reaction.
Reception of "Mulan" was mostly positive. Rotten Tomatoes gives it a rating of 86%, based on 73 reviews, with an average rating of 7.5/10. The site's consensus reads, "Exploring themes of family duty and honor, "Mulan" breaks new ground as a Disney film, while still bringing vibrant animation and sprightly characters to the screen." In a 2009 countdown, Rotten Tomatoes ranked it twenty-fourth out of the fifty canonical animated Disney features. On Metacritic, the film has a score of 71 out of 100, based on 24 critics, indicating "generally favorable reviews".
Kyle Suggs described the visuals as "breathtaking," and Dan Jardine described them as "magnificently animated." Film critic Roger Ebert gave "Mulan" three and a half stars out of four in his written review. He said that "Mulan is an impressive achievement, with a story and treatment ranking with "Beauty and the Beast" and "The Lion King"". Negative reviews described it as a "disappointment." The songs were accused of not being memorable, and slowing down the pace of the movie. Ed Gonzalez of "Slant Magazine" criticized the film as "soulless" in its portrayal of Asian society.
This movie was also the subject of comment from feminist critics. Mimi Nguyen says the film "pokes fun at the ultimately repressive gender roles that seek to make Mulan a domesticated creature." Nadya Labi agreed, saying "there is a lyric in the film that gives the lie to the bravado of the entire girl-power movement." She pointed out that Mulan needed to become a boy in order to accomplish what she did. Kathleen Karlyn, an assistant professor of English at the University of Oregon, also criticized the film's portrayal of gender roles: "In order to even imagine female heroism, we're placing it in the realm of fantasy". Pam Coats, the producer of Mulan, said that the film aims to present a character who exhibits both masculine and feminine influences, being both physically and mentally strong.
Box office performance.
Mulan's opening weekend box office gross revenues were $22.8 million, making it the second-highest grossing movie that week, behind only "The X-Files". It went on to gross $120 million in the U.S. and Canada combined, and $304 million worldwide, making it the second-highest grossing family film of the year, behind "A Bug's Life", and the seventh-highest grossing film of the year overall. While "Mulan" outgrossed the two Disney films which had preceded it, "The Hunchback of Notre Dame" and "Hercules", its box office returns failed to match those of the Disney films of the early 1990s such as "Beauty and the Beast", "Aladdin", and "The Lion King". Internationally, its highest grossing releases included those in the United Kingdom ($14.6 million) and France ($10.2 million).
Awards.
Mulan won several Annie Awards, including Best Animated Feature and Individual achievement awards to Pam Coats for producing; Barry Cook and Tony Bancroft for directing; Rita Hsiao, Christopher Sanders, Phillip LaZebnick, Raymond Singer and Eugenia Bostwick-Singer for writing, Chris Sanders for storyboarding, Hans Bacher for production design, David Tidgwell for effects animation, Ming-Na for voice acting for the character of Mulan, Ruben A. Aquino for character animation, and Matthew Wilder, David Zippel and Jerry Goldsmith for music. (Tom Bancroft and Mark Henn were also nominated for an Annie Award for Character Animation.) The musical score also received significant praise. Jerry Goldsmith won the 1999 BMI Film Music Award. The film was nominated in 1998 for the Golden Globe Award for Best Original Score. It was also nominated for the Academy Award for Best Original Music Score, but lost to Stephen Warbeck's score for "Shakespeare in Love". Matthew Wilder and David Zippel were nominated that year for a Golden Globe Award for Best Original Song for "Reflection". They were beaten by "The Truman Show" and "The Prayer" from "Quest for Camelot", respectively.
Reception in China.
Disney was keen to promote "Mulan" to the Chinese, hoping to replicate their success with the 1994 film "The Lion King", which was one of the country's highest-grossing Western films at that time. Disney also hoped it might smooth over relations with the Chinese government which had soured after the release of "Kundun", a Disney-funded biography of the Dalai Lama that the Chinese government considered politically provocative. China had threatened to curtail business negotiations with Disney over that film and, as the government only accepts ten Western films per year to be shown in their country, "Mulan"'s chances of being accepted were low. Finally, after a year's delay, the Chinese government did allow the film a limited Chinese release, but only after the Chinese New Year, so as to ensure that local films dominated the more lucrative holiday market. Box office income was low, due to both the unfavorable release date and rampant piracy. Chinese people also complained about Mulan's depiction as too foreign-looking and the story as too different from the myths. By contrast, Dreamworks Animation's own effort ten years later, "Kung Fu Panda", would be much more favorably received both for its artistry and cultural accuracy.
Chinese culture in Mulan.
The legend of Hua Mulan.
The Chinese legend of Hua Mulan centers on a young woman who disguises herself as a man to take the place of her elderly father in the army. The story can be traced back to "The Ballad of Mulan" and Disney's "Mulan" casts the title character in much the same way as the original legend—a tomboy daughter of a respected veteran, somewhat troubled by not being the "sophisticated lady" her society expects her to be. In the oldest version of the story, Mulan uses her father's name Li and she was never discovered as a girl, unlike the film.
The earliest accounts of the legend state that she lived during the Northern Wei dynasty (386–534). However, another version reports that Mulan was requested as a concubine by Emperor Yang of Sui China (reigned 604–617). The fireworks featured in the movie indicate that the movie is set during the Sui dynasty. The film correctly omits foot binding, but includes numerous other anachronisms, such as the Ming era Forbidden City in Beijing (the Sui capital was near modern Xi'an). Though "Mulan" is set in northern China and employs her Mandarin personal name, Disney gives her the Cantonese pronunciation ("Fa") for her family name.
Language.
When Mulan masquerades as a man, her name is a Chinese pun. She takes the name "Fa Ping" (花平, "Huā Píng"), which sounds identical to 花瓶 ("huāpíng"), meaning both a literal "flowerpot" and figurative "eye candy". In Chinese versions, the joke is somewhat muted by the common practice of including subtitles to make the story easier to follow for speakers of China's many dialects. The subtitles simply read 平.
Chi Fu's name (欺负, "qīfù") means "to bully or ridicule".
Music.
In March 1994, Stephen Schwartz was attached to compose the lyrics and music for the songs for the film. Following the research trip to China in June 1994, Schwartz was contacted by former Disney studio chairman Jeffrey Katzenberg to compose songs for "The Prince of Egypt", which he agreed. Peter Schneider, the then-president of Walt Disney Feature Animation, threatened to have Schwartz's name removed from any publicity materials for "Pocahontas" and "The Hunchback of Notre Dame". Michael Eisner phoned Schwartz, and urged him to back out of his commitment to DreamWorks which he refused and left the project. After Schwartz's departure, his three songs, "Written in Stone", "Destiny", and "China Doll", were dropped amid story and character changes by 1995. Shortly after, Disney music executive Chris Montan heard Matthew Wilder's demo for a stage musical adaption of Anne Rice's "Cry to Heaven", and selected Wilder to replace Schwartz. David Zippel then joined to write the lyrics. The film featured five songs composed by Wilder and Zippel, with a sixth originally planned for Mushu, but dropped following Eddie Murphy's involvement with the character. The film score of "Mulan" was composed by Jerry Goldsmith. The film's soundtrack is credited for starting the career of pop singer Christina Aguilera, whose first song to be released in the U.S. was her rendition of "Reflection", the first single from the "Mulan" soundtrack. The song, and Aguilera's vocals, were so well received that it landed her a recording contract with RCA records. In 1999, she would go on to release her self-titled debut album, on which "Reflection" was also included. As well as her own, the pop version of "Reflection" has 2 Spanish translations, because the movie has separate Spanish translations for Spain (performed by Malú) and Latin America (performed by Lucero). Other international versions include a Brazilian Portuguese version by Sandy & Junior ("Imagem"), a Korean version performed by Lena Park and a Mandarin version by Coco Lee.
Lea Salonga, the singing voice of Mulan in the movie, is also the singing voice of Princess Jasmine in "Aladdin". Salonga was originally also cast as Mulan's speaking voice, but the directors did not find her attempt at a deeper speaking voice when Mulan impersonated Ping convincing, so Ming-Na was brought in to speak the role. The music featured during the haircut scene, often referred as the "Mulan Decision" score, is different in the soundtrack album. The soundtrack album uses an orchestrated score while the movie uses heavy synthesizer music. The synthesizer version is available on the limited edition CD. Salonga, who often sings movie music in her concerts, has done a Disney medley which climaxes with an expanded version of "Reflection" (not the same as those in Aguilera's version). Salonga also provided the singing voice for Mulan in the movie's sequel, "Mulan II".
Captain Li Shang's singing voice, for the song "I'll Make a Man Out of You", was performed by Donny Osmond, who commented that his sons decided that he had finally "made it" in show business when he was in a Disney film.
Legacy.
Video game.
A PlayStation action-adventure game based on the film, titled "Disney's Story Studio: Mulan", published by Ubisoft and developed by Revolution Software (under the name "Kids Revolution"), was released on December 15, 1999. The game was met with generally positive reception and currently holds a 70.67% average rating at the review aggregator website GameRankings.
Live action adaptation.
Disney expressed interest in a live action and 3D adaptation of "Mulan" starring international star Zhang Ziyi. Chuck Russell was chosen as the director. The film was originally planned to start filming on October 2010, but was canceled. On March 30, 2015, "The Hollywood Reporter" reported that Disney was developing a live-action remake with Chris Bender and J.C. Spink producing while Elizabeth Martin and Lauren Hynek will write the screenplay.
Home video.
"Mulan" was first released on VHS on February 2, 1999 after part of the Walt Disney Masterpiece Collection. It was then re-released under the 1999 "Limited Issues" line and 2000 Walt Disney Gold Classic Collection. The film was released on a 2 disc "Special Edition" DVD on October 26, 2004. "Mulan" and its sequel were released on a 3 disc Blu-Ray and DVD combo pack in March 2013 as part of the film's 15th anniversary.
References in Disney media.
Although she is royalty neither by birth nor marriage (her husband is merely a high-ranking military officer), Mulan is part of the Disney Princess media franchise. In the film "Lilo & Stitch", Nani has a poster of Mulan in her room. "Mulan" is also present in the Disney and Square Enix video game series "Kingdom Hearts". In the first "Kingdom Hearts" and in "", Mushu is a summonable character, and in "Kingdom Hearts II", the movie is featured as a playable world named "The Land of Dragons", with the plot being changed to accommodate the game's characters (Sora, Donald and Goofy) and Mulan (both as herself and as "Ping") able to join the player's party as a skilled sword fighter. Actress Jamie Chung plays a live-action version of Mulan in the second and third seasons of the ABC television series "Once Upon a Time".

</doc>
<doc id="55721" url="http://en.wikipedia.org/wiki?curid=55721" title="King Lear">
King Lear

King Lear is a tragedy by William Shakespeare in which the titular character descends into madness after disposing of his estate between two of his three daughters based on their flattery, bringing tragic consequences for all. Based on the legend of Leir of Britain, a mythological pre-Roman Celtic king, the play has been widely adapted for the stage and motion pictures, with the title role coveted by many of the world's most accomplished actors.
Originally drafted between 1603 and its first known performance on St Stephens Day in 1606, the first attribution to Shakespeare was a 1608 publication in a quarto of uncertain provenance; it may be an early draft or simply reflect the first performance text. "The Tragedy of King Lear", a more theatrical revision, was included in the 1623 First Folio. Modern editors usually conflate the two, though some insist that each version has its individual integrity that should be preserved.
After the Restoration, the play was often revised with a happy ending for audiences who disliked its dark and depressing tone, but since the 19th century Shakespeare's original version has been regarded as one of his supreme achievements. The tragedy is particularly noted for its probing observations on the nature of human suffering and kinship. George Bernard Shaw wrote, "No man will ever write a better tragedy than "Lear"".
Synopsis.
The play begins as the Earl of Gloucester and the Earl of Kent meet and observe that King Lear has awarded equal shares of his realm to the Duke of Cornwall and the Duke of Albany. Gloucester then introduces his illegitimate son Edmund to the Earl of Kent. In the next scene, King Lear, who is elderly and wants to retire from power, decides to divide his realm among his three daughters, and declares he'll offer the largest share to the one who loves him best. The eldest, Goneril, speaks first, declaring her love for her father in fulsome terms. Moved by her flattery Lear proceeds to grant to Goneril her share as soon as she's finished her declaration, before Regan and Cordelia have a chance to speak. He then awards to Regan her share as soon as she has spoken. When it is finally the turn of his youngest daughter, Cordelia, at first she refuses to say anything ("Nothing, my Lord") and then declares there is nothing to compare her love to, nor words to properly express it; she speaks honestly but bluntly, which infuriates him. In his anger he disinherits Cordelia and divides her share between Regan and Goneril. Kent objects to this unfair treatment. Enraged by Kent's protests, Lear banishes him from the country. Lear summons the Duke of Burgundy and the King of France, who have both proposed marriage to Cordelia. Learning that Cordelia has been disinherited, the Duke of Burgundy withdraws his suit, but the King of France is impressed by her honesty and marries her anyway.
Lear announces he will live alternately with Goneril and Regan, and their husbands, the Dukes of Albany and Cornwall respectively. He reserves to himself a retinue of one hundred knights, to be supported by his daughters. Goneril and Regan speak privately, revealing that their declarations of love were fake, and they view Lear as an old and foolish man.
Edmund resents his illegitimate status, and plots to dispose of his legitimate older brother Edgar. He tricks their father Gloucester with a forged letter, making him think Edgar plans to usurp the estate. Kent returns from exile in disguise under the name of Caius, and Lear hires him as a servant. Lear and Caius quarrel with Oswald, Goneril's steward. Lear discovers that now that Goneril has power, she no longer respects him. She orders him to behave better and reduces his retinue. Enraged, Lear departs for Regan's home. The Fool mocks Lear's misfortune.
Edmund learns from Curan, a courtier, that there is likely to be war between Albany and Cornwall, and that Regan and Cornwall are to arrive at Gloucester's house that evening. Taking advantage of the arrival of the duke and Regan, Edmund fakes an attack by Edgar, and Gloucester is completely taken in. He disinherits Edgar and proclaims him an outlaw.
Bearing Lear's message to Regan, Caius meets Oswald again at Gloucester's home, quarrels with him again, and is put in the stocks by Regan and her husband Cornwall. When Lear arrives, he objects to the mistreatment of his messenger, but Regan is as dismissive of her father as Goneril was. Lear is enraged but impotent. Goneril arrives and supports Regan's argument against him. Lear yields completely to his rage. He rushes out into a storm to rant against his ungrateful daughters, accompanied by the mocking Fool. Kent later follows to protect him. Gloucester protests against Lear's mistreatment. With Lear's retinue of a hundred knights dissolved, the only companions he has left are his Fool and Caius. Wandering on the heath after the storm, Lear meets Edgar, in the guise of a madman named Tom O'Bedlam. Edgar babbles madly while Lear denounces his daughters. Kent leads them all to shelter.
Edmund betrays Gloucester to Cornwall, Regan, and Goneril. He reveals evidence that his father knows of an impending French invasion designed to reinstate Lear to the throne; and in fact a French army has landed in Britain. Once Edmund leaves with Goneril to warn Albany about the invasion, Gloucester is arrested, and Regan and Cornwall gouge out Gloucester's eyes. As he is doing so, a servant is overcome with rage by what he is witnessing and attacks Cornwall, mortally wounding him. Regan kills the servant, and tells Gloucester that Edmund betrayed him; then she turns him out to wander the heath too. Edgar, in his madman's guise, meets his blinded father on the heath. Gloucester, not recognising him, begs Tom to lead him to a cliff at Dover so that he may jump to his death.
Goneril discovers that she finds Edmund more attractive than her honest husband Albany, whom she regards as cowardly. Albany has developed a conscience - he is disgusted by the sisters' treatment of Lear, and the mutilation of Gloucester, and denounces his wife. Goneril sends Edmund back to Regan; receiving news of Cornwall's death, she fears her newly widowed sister may steal Edmund and sends him a letter through Oswald. By now alone with Lear, Kent leads him to the French army, which is commanded by Cordelia. But Lear is half-mad and terribly embarrassed by his earlier follies. At Regan's instigation, Albany joins his forces with hers against the French. Goneril's suspicions about Regan's motives are confirmed and returned, as Regan rightly guesses the meaning of her letter and declares to Oswald that she is a more appropriate match for Edmund. Edgar pretends to lead Gloucester to a cliff, then changes his voice and tells Gloucester he has miraculously survived a great fall. Lear appears, by now completely mad. He rants that the whole world is corrupt and runs off.
Oswald appears, still looking for Edmund. On Regan's orders, he tries to kill Gloucester but is killed by Edgar. In Oswald's pocket, Edgar finds Goneril's letter, in which she encourages Edmund to kill her husband and take her as his wife. Kent and Cordelia take charge of Lear, whose madness slowly passes. Regan, Goneril, Albany, and Edmund meet with their forces. Albany insists that they fight the French invaders but not harm Lear or Cordelia. The two sisters lust for Edmund, who has made promises to both. He considers the dilemma and plots the deaths of Albany, Lear, and Cordelia. Edgar gives Goneril's letter to Albany. The armies meet in battle, the British defeat the French, and Lear and Cordelia are captured. Edmund sends Lear and Cordelia off with secret-joint orders from him (representing Regan and her forces) and Goneril (representing Albany's) for the execution of Cordelia.
The victorious British leaders meet, and the recently widowed Regan now declares she will marry Edmund. But Albany exposes the intrigues of Edmund and Goneril and proclaims Edmund a traitor. Regan falls ill, having been poisoned by Goneril, and is escorted offstage, where she dies. Edmund defies Albany, who calls for a trial by combat. Edgar appears masked and in armor, and challenges Edmund to a duel. No one knows who he is. Edgar wounds Edmund fatally, though he does not die immediately. Albany confronts Goneril with the letter which was intended to be his death warrant; she flees in shame and rage. Edgar reveals himself, and reports that Gloucester died offstage from the shock and joy of learning that Edgar is alive, after Edgar revealed himself to his father.
Offstage, Goneril, with all her evil plans thwarted, commits suicide. The dying Edmund decides, though he admits it is against his own character, to try and save Lear and Cordelia; however, his confession comes too late. Soon after Albany sends men to countermand Edmund's orders, Lear enters bearing Cordelia's corpse in his arms, having survived by killing the executioner. Kent appears and Lear now recognises him. Albany urges Lear to resume his throne, but like Gloucester, the trials Lear has been through have finally overwhelmed him, and he dies. Albany then asks Kent and Edgar to take charge of the throne. Kent declines, explaining that his master is calling him on a journey. Finally, either Albany (in the Quarto version) or Edgar (in the Folio version) has the final speech, with the implication that he will now become king.
Sources.
Shakespeare's play is based on various accounts of the semi-legendary Brythonic figure Leir of Britain, whose name has been linked by some scholars to the Brythonic god Lir/Llŷr, though in actuality the names are not etymologically related. Shakespeare's most important source is probably the second edition of "The Chronicles of England, Scotlande, and Irelande" by Raphael Holinshed, published in 1587. Holinshed himself found the story in the earlier "Historia Regum Britanniae" by Geoffrey of Monmouth, that was written in the 12th century. Edmund Spenser's "The Faerie Queene", published 1590, also contains a character named Cordelia, who also dies from hanging, as in "King Lear".
Other possible sources are the anonymous play "King Leir" (published in 1605); "The Mirror for Magistrates" (1574), by John Higgins; "The Malcontent" (1604), by John Marston; "The London Prodigal" (1605); "Arcadia" (1580–1590), by Sir Philip Sidney, from which Shakespeare took the main outline of the Gloucester subplot; Montaigne's "Essays", which were translated into English by John Florio in 1603; "An Historical Description of Iland of Britaine", by William Harrison; "Remaines Concerning Britaine", by William Camden (1606); "Albion's England", by William Warner, (1589); and "A Declaration of egregious Popish Impostures", by Samuel Harsnett (1603), which provided some of the language used by Edgar while he feigns madness. "King Lear" is also a literary variant of a common fairy tale, Love Like Salt, Aarne-Thompson type 923, in which a father rejects his youngest daughter for a statement of her love that does not please him.
The source of the subplot involving Gloucester, Edgar, and Edmund is a tale in Philip Sidney's "Countess of Pembroke's Arcadia", with a blind Paphlagonian king and his two sons, Leonatus and Plexitrus.
Changes from source material.
Besides the subplot involving the Earl of Gloucester and his sons, the principal innovation Shakespeare made to this story was the death of Cordelia and Lear at the end; in the account by Geoffrey of Monmouth, Cordelia restores Lear to the throne, and succeeds him as ruler after his death. During the 17th century, Shakespeare's tragic ending was much criticised and alternative versions were written by Nahum Tate, in which the leading characters survived and Edgar and Cordelia were married (despite the fact that Cordelia was previously betrothed to the King of France). As Harold Bloom states: "Tate's version held the stage for almost 150 years, until Edmund Kean reinstated the play's tragic ending in 1823."
Date and text.
Although an exact date of composition cannot be given, many academic editors of the play date "King Lear" between 1603 and 1606. The latest it could have been written is 1606, as the Stationers' Register notes a performance on 26 December 1606. The 1603 date originates from words in Edgar's speeches which may derive from Samuel Harsnett's "Declaration of Egregious Popish Impostures" (1603). In his Arden edition, R.A. Foakes argues for a date of 1605–6, because one of Shakespeare's sources, "The True Chronicle History of King Leir", was not published until 1605; close correspondences between that play and Shakespeare's suggest that he may have been working from a text (rather than from recollections of a performance). Conversely, Frank Kermode, in the "Riverside Shakespeare", considers the publication of "Leir" to have been a response to performances of Shakespeare's already-written play; noting a sonnet by William Strachey that may have verbal resemblances with "Lear", Kermode concludes that "1604-5 seems the best compromise". Dr. Naseeb Shaheen dates the play c1605-6 per line 1.2.103
"These late eclipses in the sun and moon" which relates to the lunar eclipse of September 27, 1605 and the solar eclipse of October 2, 1605.
The modern text of King Lear derives from three sources: two quartos, published in 1608 (Q1) and 1619 (Q2) respectively, and the version in the First Folio of 1623 (F1). The differences between these versions are significant. Q1 contains 285 lines not in F1; F1 contains around 100 lines not in Q1. Also, at least a thousand individual words are changed between the two texts, each text has a completely different style of punctuation, and about half the verse lines in the F1 are either printed as prose or differently divided in the Q1. The early editors, beginning with Alexander Pope, simply conflated the two texts, creating the modern version that has remained nearly universal for centuries. The conflated version is born from the presumption that Shakespeare wrote only one original manuscript, now unfortunately lost, and that the Quarto and Folio versions are distortions of that original.
As early as 1931, Madeleine Doran suggested that the two texts had basically different provenances, and that these differences between them were critically interesting. This argument, however, was not widely discussed until the late 1970s, when it was revived, principally by Michael Warren and Gary Taylor. Their thesis, while controversial, has gained significant acceptance. It posits, essentially, that the Quarto derives from something close to Shakespeare's foul papers, and the Folio is drawn in some way from a promptbook, prepared for production by Shakespeare's company or someone else. In short, Q1 is "authorial"; F1 is "theatrical". In criticism, the rise of "revision criticism" has been part of the pronounced trend away from mid-century formalism.
The New Cambridge Shakespeare has published separate editions of Q and F; the most recent Pelican Shakespeare edition contains both the 1608 Quarto and the 1623 Folio text as well as a conflated version; the New Arden edition edited by R.A. Foakes is the only recent edition to offer the traditional conflated text. Both Anthony Nuttall of Oxford University and Harold Bloom of Yale University have endorsed the view of Shakespeare having revised the tragedy at least once during his lifetime. As Bloom indicates: "At the close of Shakespeare's revised "King Lear", a reluctant Edgar becomes King of Britain, accepting his destiny but in the accents of despair. Nuttall speculates that Edgar, like Shakespeare himself, usurps the power of manipulating the audience by deceiving poor Gloucester."
Analysis and criticism.
Analysis and criticism of "King Lear" over the centuries has been extensive. 
What we know of Shakespeare's wide reading and powers of assimilation seems to show that he made use of all kinds of material, absorbing contradictory viewpoints, positive and negative, religious and secular, as if to ensure that "King Lear" would offer no single controlling perspective, but be open to, indeed demand, multiple interpretations.
R. A. Foakes 
Historicist interpretations.
John F. Danby, in his "Shakespeare's Doctrine of Nature – A Study of King Lear" (1949), argues that "Lear" dramatizes, among other things, the current meanings of "Nature". The words "nature," "natural" and "unnatural" occur over forty times in the play, reflecting a debate in Shakespeare's time about what nature really was like; this debate pervades the play and finds symbolic expression in Lear's changing attitude to Thunder. There are two strongly contrasting views of human nature in the play: that of the Lear party (Lear, Gloucester, Albany, Kent), exemplifying the philosophy of Bacon and Hooker, and that of the Edmund party (Edmund, Cornwall, Goneril, Regan), akin to the views later formulated by Hobbes. Along with the two views of Nature, "Lear" contains two views of Reason, brought out in Gloucester and Edmund's speeches on astrology (1.2). The rationality of the Edmund party is one with which a modern audience more readily identifies. But the Edmund party carries bold rationalism to such extremes that it becomes madness: a madness-in-reason, the ironic counterpart of Lear's "reason in madness" (IV.6.190) and the Fool's wisdom-in-folly. This betrayal of reason lies behind the play's later emphasis on "feeling".
The two Natures and the two Reasons imply two societies. Edmund is the New Man, a member of an age of competition, suspicion, glory, in contrast with the older society which has come down from the Middle Ages, with its belief in co-operation, reasonable decency, and respect for the whole as greater than the part. "King Lear" is thus an allegory. The older society, that of the medieval vision, with its doting king, falls into error, and is threatened by the new Machiavellianism; it is regenerated and saved by a vision of a new order, embodied in the king's rejected daughter. Cordelia, in the allegorical scheme, is threefold: a person; an ethical principle (love); and a community. Nevertheless, Shakespeare's understanding of the New Man is so extensive as to amount almost to sympathy. Edmund is the last great expression in Shakespeare of that side of Renaissance individualism – the energy, the emancipation, the courage – which has made a positive contribution to the heritage of the West. "He embodies something vital which a final synthesis must reaffirm. But he makes an absolute claim which Shakespeare will not support. It is right for man to feel, as Edmund does, that society exists for man, not man for society. It is not right to assert the kind of man Edmund would erect to this supremacy."
The play offers an alternative to the feudal-Machiavellian polarity, an alternative foreshadowed in France's speech (I.1.245–256), in Lear and Gloucester's prayers (III.4. 28–36; IV.1.61–66), and in the figure of Cordelia. Until the decent society is achieved, we are meant to take as role-model (though qualified by Shakespearean ironies) Edgar, "the machiavel of goodness", endurance, courage and "ripeness".
Psychoanalytic interpretations.
Since there are no literal mothers in "King Lear", Coppélia Kahn provides a psychoanalytic interpretation of the "maternal subtext" found in the play. According to Kahn, Lear in his old age regresses to an infantile disposition, and now seeks for a love that is normally satisfied by a mothering woman. Her characterisation of Lear is that of a child being mothered, but without real mothers, his children become the daughter-mother figures. Lear's contest of love serves as the binding agreement; his daughters will get their inheritance provided they care for him, especially Cordelia, on whose "kind nursery" he will greatly depend. Her refusal to love him as more than a father is often interpreted as a resistance from incest, but Kahn also inserts the image of a rejecting mother. The situation is now a reversal of parent-child roles, in which Lear's madness is essentially a childlike rage from being deprived of maternal care. Even when Lear and Cordelia are captured together, this madness persists as Lear envisions a nursery in prison, where Cordelia's sole existence is for him. However, it is Cordelia's death that ultimately ends his fantasy of a daughter-mother, as the play ends with only male characters left.
Sigmund Freud asserted that Cordelia symbolises Death. Therefore, when the play begins with Lear rejecting his daughter, it can be interpreted as him rejecting death; Lear is unwilling to face the finitude of his being. The play's poignant ending scene, wherein Lear carries the body of his beloved Cordelia, was of great importance to Freud. In this scene, she causes in Lear a realisation of his finitude, or as Freud put it, she causes him to "make friends with the necessity of dying". It is logical to infer that Shakespeare had special intentions with Cordelia's death, as he was the only writer to have Cordelia killed (in the version by the Nahum Tate, she continues to live happily, and in Holinshed's, she restores her father and succeeds him).
Alternatively, an analysis based on Adlerian theory suggests that the King's contest among his daughters in Act one has more to do with his control over the unmarried Cordelia.
In his study of the character-portrayal of Edmund, Harold Bloom refers to him as "Shakespeare's most original character". "As Hazlitt pointed out," writes Bloom, "Edmund does not share in the hypocrisy of Goneril and Regan: his Machiavellianism is absolutely pure, and lacks an Oedipal motive. Freud's vision of family romances simply does not apply to Edmund. Iago is free to reinvent himself every minute, yet Iago has strong passions, however negative. Edmund has no passions whatsoever; he has never loved anyone, and he never will. In that respect, he is Shakespeare's most original character."
Christianity.
Critics are divided on the question of whether or not "King Lear" represents an affirmation of Christian doctrine. Among those who argue that Lear is redeemed in the Christian sense through suffering are A. C. Bradley and John Reibetanz, who has written: "through his sufferings, Lear has won an enlightened soul". Other critics who find no evidence of redemption and emphasise the horrors of the final act include John Holloway and Marvin Rosenberg. William R. Elton stresses the pre-Christian setting of the play, writing that, "Lear fulfills the criteria for pagan behavior in life," falling "into total blasphemy at the moment of his irredeemable loss".
Fairy tales.
In the first edition of the Grimms "Kinder und Hausmärchen", the Grimms made a note in the Anhang (appendix) entry to No. 71 Princess Mouse-skin: "as the father here, so asks King Lear his daughter". The English translation of this story by Oliver Loo begins as follows: "A king had three daughters; thereon he wanted to know, which loved him most, let them come in front of him and asked them. The eldest spoke, she loved him more, than the whole kingdom; the second, more than all the precious stones and pearls in the world; but the third said, she loved him more than salt. The king was so upset, that she compared her love of him with such a small thing, gave her to a servant and commanded, he should take her into the forest and kill her."
Performance history.
17th and 18th centuries.
Shakespeare wrote the role of Lear for his company's chief tragedian, Richard Burbage, for whom Shakespeare was writing incrementally older characters as their careers progressed. It has been speculated either that the role of the Fool was written for the company's clown Robert Armin, or that it was written for performance by one of the company's boys, doubling the role of Cordelia. Only one specific performance of the play during Shakespeare's lifetime is known: before the court of King James I at Whitehall on 26 December 1604. Its original performances would have been at The Globe, where there were no sets in the modern sense, and characters would have signified their roles visually with props and costumes: Lear's costume, for example, would have changed in the course of the play as his status diminished: commencing in crown and regalia; then as a huntsman; raging bareheaded in the storm scene; and finally crowned with flowers in parody of his original status.
All theatres were closed down by the Puritan government on 6 September 1642. Upon the restoration of the monarchy in 1660, two patent companies (the King's Company and the Duke's Company) were established, and the existing theatrical repertoire divided between them. And from the restoration until the mid-19th century the performance history of "King Lear" is not the story of Shakespeare's version, but instead of "The History of King Lear", a popular adaptation by Nahum Tate. Its most significant deviations from Shakespeare were to omit the Fool entirely, to introduce a happy ending in which Lear and Cordelia survive, and to develop a love story between Cordelia and Edgar (two characters who never interact in Shakespeare) which ends with their marriage. Like most Restoration adapters of Shakespeare, Tate admired Shakespeare's natural genius but saw fit to augment his work with contemporary standards of art (which were largely guided by the neoclassical unities of time, place, and action). Tate's struggle to strike a balance between raw nature and refined art is apparent in his description of the tragedy: "a heap of jewels, unstrung and unpolish't; yet so dazzling in their disorder, that I soon perceiv'd I had seiz'd a treasure." Other changes included giving Cordelia a "confidante" named Arante, bringing the play closer to contemporary notions of poetic justice, and added titilating material such as amorous encounters between Edmund and both Regan and Goneril, a scene in which Edgar rescues Cordelia from Edmund's attempted kidnap and rape, and a scene in which Cordelia wears men's pants that would reveal the actress's ankles. The play ends with a celebration of "the King's blest Restauration", an obvious reference to Charles II.
In the early 18th century, some writers began to express objections to this (and other) Restoration adaptations of Shakespeare. For example, in "The Spectator" on 16 April 1711 Joseph Addison wrote ""King Lear" is an admirable Tragedy ... as "Shakespeare" wrote it; but as it is reformed according to the chymerical Notion of poetical Justice in my humble Opinion it hath lost half its Beauty." Yet on the stage, Tate's version prevailed.
David Garrick was the first actor-manager to begin to cut back on elements of Tate's adaptation in favour of Shakespeare's original: he retained Tate's major changes, including the happy ending, but removed many of Tate's lines, including Edgar's closing speech. He also reduced the prominence of the Edgar-Cordelia love story, in order to focus more on the relationship between Lear and his daughters. His version had a powerful emotional impact: Lear driven to madness by his daughters was (in the words of one spectator, Arthur Murphy) "the finest tragic distress ever seen on any stage" and, in contrast, the devotion shown to Lear by Cordelia (a mix of Shakespeare's, Tate's and Garrick's contributions to the part) moved the audience to tears.
The first professional performances of "King Lear" in North America are likely to have been those of the Hallam Company (later the American Company) which arrived in Virginia in 1752 and who counted the play among their repertoire by the time of their departure for Jamaica in 1774.
19th century.
Charles Lamb established the Romantics' attitude to "King Lear" in his 1811 essay "On the Tragedies of Shakespeare, considered with reference to their fitness for stage representation" where he says that the play "is essentially impossible to be represented on the stage", preferring to experience it in the study. In the theatre, he argues, "to see Lear acted, to see an old man tottering about the stage with a walking-stick, turned out of doors by his daughters on a rainy night, has nothing in it but what is painful and disgusting" yet "while we read it, we see not Lear but we are Lear, – we are in his mind, we are sustained by a grandeur which baffles the malice of daughters and storms."
"King Lear" was politically controversial during the period of George III's madness, and as a result was not performed at all in the two professional theatres of London from 1811 to 1820: but was then the subject of major productions in both, within three months of his death. The 19th century saw the gradual reintroduction of Shakespeare's text to displace Tate's version. Like Garrick before him, John Philip Kemble had introduced more of Shakespeare's text, while still preserving the three main elements of Tate's version: the love story, the omission of the Fool, and the happy ending. Edmund Kean played "King Lear" with its tragic ending in 1823, but failed and reverted to Tate's crowd-pleaser after only three performances. At last in 1838 William Macready at Covent Garden performed Shakespeare's version, freed from Tate's adaptions. The restored character of the Fool was played by an actress, Priscilla Horton, as, in the words of one spectator, "a fragile, hectic, beautiful-faced, half-idiot-looking boy." And Helen Faucit's final appearance as Cordelia, dead in her father's arms, became one of the most iconic of Victorian images. John Forster, writing in the "Examiner" on 14 February 1838, expressed the hope that "Mr Macready's success has banished that disgrace [Tate's version] from the stage for ever." But even this version was not close to Shakespeare's: the 19th-century actor-managers heavily cut Shakespeare's scripts: ending scenes on big "curtain effects" and reducing or eliminating supporting roles to give greater prominence to the star. One of Macready's innovations – the use of Stonehenge-like structures on stage to indicate an ancient setting – proved enduring on stage into the 20th century, and can be seen in the 1983 television version starring Laurence Olivier.
In 1843, the Act for Regulating the Theatres came into force, bringing an end to the monopolies of the two existing companies and, by doing so, increased the number of theatres in London. At the same time, the fashion in theatre was "pictorial": valuing visual spectacle above plot or characterisation and often required lengthy (and time consuming) scene changes. For example, Henry Irving's 1892 "King Lear" offered spectacles such as Lear's death beneath a cliff at Dover, his face lit by the red glow of a setting sun; at the expense of cutting 46% of the text, including the blinding of Gloucester. But Irving's production clearly evoked strong emotions: one spectator, Gordon Crosse, wrote of the first entrance of Lear, "a striking figure with masses of white hair. He is leaning on a huge scabbarded sword which he raises with a wild cry in answer to the shouted greeting of his guards. His gait, his looks, his gestures, all reveal the noble, imperious mind already degenerating into senile irritability under the coming shocks of grief and age."
The importance of pictorialism to Irving, and to other theatre professionals of the Victorian era, is exemplified by the fact that Irving had used Ford Madox Brown's painting "Cordelia's Portion" as the inspiration for the look of his production, and that the artist himself was brought in to provide sketches for the settings of other scenes. A reaction against pictorialism came with the rise of reconstructive movement, believers in a simple style of staging more similar to that which would have pertained in renaissance theatres, whose chief early exponent was the actor-manager William Poel. Poel was influenced by a performance of "King Lear" directed by Jocza Savits at the Hoftheater in Munich in 1890, set on an apron stage with a three-tier Globe-like reconstruction theatre as its backdrop. Poel would use this same configuration for his own Shakespearean performances in 1893.
20th and 21st centuries.
The character of Lear in the 19th century was often that of a frail old man from the opening scene, but Lears of the 20th century often began the play as strong men displaying regal authority, including John Gielgud, Donald Wolfit and Donald Sinden. Cordelia, also, evolved in the 20th century: earlier Cordelias had often been praised for being sweet, innocent and modest, but 20th-century Cordelias were often portrayed as war leaders. For example, Peggy Ashcroft, at the RST in 1950, played the role in a breastplate and carrying a sword. Similarly, the Fool evolved through the course of the century, with portrayals often deriving from the music hall or circus tradition.
By mid-century, the actor-manager tradition had declined, to be replaced by a structure where the major theatre companies employed professional directors as auteurs. The last of the great actor-managers, Donald Wolfit, played Lear on a Stonehenge-like set in 1944 and was praised by James Agate as "the greatest piece of Shakespearean acting since I have been privileged to write for the "Sunday Times"". Wolfit supposedly drank eight bottles of Guinness in the course of each performance.
At Stratford-upon-Avon in 1962, Peter Brook (who would later film the play with the same Lear, Paul Scofield) set the action simply, against a huge, empty white stage. The effect of the scene where Lear and Gloucester meet, two tiny figures in rags in the midst of this emptiness, was said (by the scholar Roger Warren) to catch "both the human pathos ... and the universal scale ... of the scene." Some of the lines from the radio play were added into the mix by The Beatles for "I Am the Walrus" while John Lennon was fiddling with the radio with the play from BBC Third Programme that happened to be on at the time. Actors Mark Dignam, Philip Guard, and John Bryning from the play all appeared in the song.
In 1974, Buzz Goodbody directed "Lear", a deliberately abbreviated title for Shakespeare's text, as the inaugural production of the RSC's studio theatre The Other Place. The performance was conceived as a chamber piece, the small intimate space and proximity to the audience enabled detailed psychological acting, which was performed with simple sets and in modern dress. Peter Holland has speculated that this company/directoral decision – namely "choosing" to present Shakespeare in a small venue for artistic reasons when a larger venue was available – may at the time have been unprecedented.
Brook's vision of the play proved influential, and directors have gone further in presenting Lear as (in the words of R. A. Foakes) "a pathetic senior citizen trapped in a violent and hostile environment". When John Wood took the role in 1990, he played the later scenes in clothes that looked like cast-offs, inviting deliberate parallels with the uncared-for in modern Western societies. Indeed, modern productions of Shakespeare's plays often reflect the world in which they are performed as much as the world for which they were written: and the Moscow theatre scene in 1994 provided an example, when two very different productions of the play (those by Sergei Zhonovach and Alexei Borodin), very different from one another in their style and outlook, were both reflections on the break-up of the Soviet Union.
Like other Shakespearean tragedies, "King Lear" has proved amenable to conversion into other theatrical traditions. In 1989, David McRuvie and Iyyamkode Sreedharan adapted the play then translated it to Malayalam, for performance in Kerala in the Kathakali tradition – which itself developed around 1600, contemporary with Shakespeare's writing. The show later went on tour, and in 2000 played at Shakespeare's Globe, completing (in Anthony Dawson's words) "a kind of symbolic circle". Perhaps even more radical was Ong Keng Sen's 1997 adaptation of "King Lear", which featured six actors each performing in a separate Asian acting tradition and in their own separate languages. A pivotal moment occurred when the Jingju performer playing Older Daughter (a conflation of Goneril and Regan) stabbed the Noh-performed Lear whose "falling pine" deadfall, straight face-forward into the stage, astonished the audience, in what Yong Li Lan describes as a "triumph through the moving power of "noh" performance at the very moment of his character's defeat".
A number of women have played male roles in "King Lear"; most commonly the Fool, who has been played (among others) by Judy Davis and Emma Thompson but also, significantly, Lear himself, played by Marianne Hoppe in 1990 and by Kathryn Hunter in 1996-7. Marcia Gay Harden plays Lear in the few scenes of the play-within-the-film If I Were You.
In 2002 and 2010, the Hudson Shakespeare Company of New Jersey staged separate productions as part of their respective Shakespeare in the Parks seasons. The 2002 version was directed by Michael Collins and transposed the action to a West Indies, nautical setting. Actors were featured in outfits indicative of looks of various Caribbean islands. The 2010 production directed by Jon Ciccarelli was fashioned after the atmosphere of the film The Dark Knight with a palette of reds and blacks and set the action in an urban setting. Lear (Tom Cox) appeared as a head of multi-national conglomerate who divided up his fortune among his socialite daughter Goneril (Brenda Scott), his officious middle daughter Regan (Noelle Fair) and university daughter Cordelia (Emily Best).
In 2012, Peter Hinton directed an all-First Nations production of "King Lear" at the National Arts Centre in Ottawa, Ontario, Canada, with the setting changed to an Algonquin nation in the 17th century. The cast included August Schellenberg as Lear, Billy Merasty as Gloucester, Tantoo Cardinal as Regan, Kevin Loring as Edmund, Jani Lauzon in a dual role as Cordelia and the Fool, and Craig Lauzon as Kent.
Screen adaptations.
The first film of "King Lear" was a five-minute German version made around 1905, which has not survived. The oldest extant version is a ten-minute studio-based version from 1909 by Vitagraph, which made (in Luke McKernan's words) the "ill-advised" decision to attempt to cram in as much of the plot as possible. Two silent versions, both titled "Re Lear", were made in Italy in 1910. Of these, the version by director Gerolamo Lo Savio was filmed on location, and it dropped the Edgar sub-plot and used frequent intertitling to make the plot easier to follow than its Vitagraph predecessor. A contemporary setting was used for Louis Feuillade's 1911 French adaptation "Le Roi Lear Au Village", and in 1914 in America, Ernest Warde expanded the story to an hour, including spectacles such as a final battle scene.
The only two significant big-screen performances of Shakespeare's text date from the early 1970s: Grigori Kozintsev was working on his "Korol Lir" at the same time as Peter Brook was filming his "King Lear". Brook's film starkly divided the critics: Pauline Kael said "I didn't just dislike this production, I hated it!" and suggested the alternative title "Night of the Living Dead". Yet Robert Hatch in "The Nation" thought it as "excellent a filming of the play as one can expect" and Vincent Canby in The New York Times called it "an exalting "Lear", full of exquisite terror". The film drew heavily on the ideas of Jan Kott, in particular his observation that "King Lear" was the precursor of absurdist theatre: in particular, the film has parallels with Beckett's "Endgame". Critics who dislike the film particularly draw attention to its bleak nature from its opening: complaining that the world of the play does not deteriorate with Lear's suffering, but commences dark, colourless and wintry, leaving (in Douglas Brode's words) "Lear, the land, and "us" with nowhere to go". Cruelty pervades the film, which does not distinguish between the violence of ostensibly good and evil characters, presenting both savagely. Paul Scofield, as Lear, eschews sentimentality: this demanding old man with a coterie of unruly knights provokes audience sympathy for the daughters in the early scenes, and his presentation explicitly rejects the tradition (as Daniel Rosenthal describes it) of playing Lear as "poor old white-haired patriarch".
By contrast, "Korol Lir" has been praised, for example by critic Anikst Alexander, for the "serious, deeply thoughtful" even "philosophical approach" of director Grigori Kozintsev and writer Boris Pasternak. Making a thinly veiled criticism of Brook in the process, Alexander praised the fact that there were "no attempts at sensationalism, no efforts to 'modernise' Shakespeare by introducing Freudian themes, Existentialist ideas, eroticism, or sexual perversion. [Kozintsev]... has simply made a film of Shakespeare's tragedy." Dmitri Shostakovich provided an epic score, its motifs including an (increasingly ironic) trumpet fanfare for Lear, and a five-bar "Call to Death" marking each character's demise. Kozintzev described his vision of the film as an ensemble piece: with Lear, played by a dynamic Jüri Järvet, as first among equals in a cast of fully developed characters. The film highlights Lear's role as king by including his people throughout the film on a scale no stage production could emulate, charting the central character's decline from their god to their helpless equal; his final descent into madness marked by his realisation that he has negelected the 'poor naked wretches'. As the film progresses, ruthless characters – Goneril, Regan, Edmund – increasingly appear isolated in shots, in contrast to the director's focus, throughout the film, on masses of human beings.
Jonathan Miller twice directed Michael Hordern in the title role for English television, the first for the BBC's "Play of the Month" in 1975 and the second for the "BBC Television Shakespeare" in 1982. Horden received mixed reviews, and was considered a bold choice due to his history of taking much lighter roles. Also for English television, Laurence Olivier took the role in a 1983 TV production for Granada Television. It was his last screen appearance in a Shakespearean role, its pathos deriving in part from the physical frailty of Olivier the actor.
In 1985 a major screen adaptation of the play appeared: "Ran", directed by Akira Kurosawa. At the time the most expensive Japanese film ever made, it tells the story of Hidetora, a fictional 16th-century Japanese warlord, whose attempt to divide his kingdom among his three sons leads to an estrangement with the youngest, and ultimately most loyal, of them, and eventually to civil war. In contrast to the cold drab greys of Brook and Kozintsev, Kurosawa's film is full of vibrant colour: external scenes in yellows, blues and greens, interiors in browns and ambers, and Emi Wada's Oscar-winning colour-coded costumes for each family member's soldiers. Hidetora has a back-story: a violent and ruthless rise to power, and the film portrays contrasting victims: the virtuous characters Sue and Tsurumaru who are able to forgive, and the vengeful Kaede (Mieko Harada), Hidetora's daughter-in-law and the film's Lady Macbeth-like villain.
The play's plot, or major elements from it, have frequently been used by film makers. Joseph Mankiewicz' 1949 "House of Strangers" is often considered a "Lear" adaptation, but the parallels are more striking in its 1954 Western remake "Broken Lance" in which a cattle baron played by Spencer Tracy tyrannises over his three sons, of whom only the youngest, Joe, played by Robert Wagner, remains loyal. A scene in which a character is threatened with blinding in the manner of Gloucester forms the climax of the 1973 parody horror "Theatre of Blood". Comic use is made of Sir's inability to physically carry any actress cast as Cordelia opposite his Lear in the 1983 film of the stage play "The Dresser". John Boorman's 1990 "Where the Heart Is" features a father who disinherits his three spoilt children. Francis Ford Coppola deliberately incorporated elements of "Lear" in his 1990 sequel "The Godfather Part III", including Michael Corleone's attempt to retire from crime throwing his domain into anarchy, and most obviously the death of his daughter in his arms. Parallels have also been drawn between Andy García's character Vincent and both Edgar and Edmund, and between Talia Shire's character Connie and Kaede in "Ran".
In 1997, Jocelyn Moorhouse directed "A Thousand Acres", based on Jane Smiley's Pulitzer Prize-winning novel, set in 1990s Iowa. The film is described, by scholar Tony Howard, as the first adaptation to confront the play's disturbing sexual dimensions. The story is told from the viewpoint of the elder two daughters, Ginny played by Jessica Lange and Rose played by Michelle Pfeiffer, who were sexually abused by their father as teenagers. Their younger sister Caroline, played by Jennifer Jason Leigh had escaped this fate and is ultimately the only one to remain loyal.
The play was again adapted to the world of gangsters in Don Boyd's 2001 "My Kingdom", a version which differs from all others in commencing with the Lear character, Sandeman, played by Richard Harris, in a loving relationship with his wife. But her violent death marks the start of an increasingly bleak and violent chain of events (influenced by co-writer Nick Davies' documentary book "Dark Heart") which in spite of the director's denial that the film had "serious parallels" to Shakespeare's play, actually mirror aspects of its plot closely. Unlike Shakespeare's Lear, but like Hidetora and Sandeman, the central character of Uli Edel's 2002 American TV adaptation "King of Texas", John Lear played by Patrick Stewart, has a back-story centred on his violent rise to power as the richest landowner (metaphorically a "king") in General Sam Houston's independent Texas in the early 1840s. Daniel Rosenthal comments that the film was able, by reason of having been commissioned by the cable channel TNT, to include a bleaker and more violent ending than would have been possible on the national networks. 2003's Channel 4-commissioned two-parter "Second Generation" set the story in the world of Asian manufacturing and music in England.
References.
Secondary sources.
</dl>

</doc>
<doc id="55726" url="http://en.wikipedia.org/wiki?curid=55726" title="Gentlemen's agreement">
Gentlemen's agreement

A gentlemen's agreement (or gentleman's agreement) is an informal and legally non-binding agreement between two or more parties. It is typically oral, though it may be written, or simply understood as part of an unspoken agreement by convention or through mutually beneficial etiquette. The essence of a gentlemen's agreement is that it relies upon the honor of the parties for its fulfillment, rather than being in any way enforceable. It is, therefore, distinct from a legal agreement or contract, which can be enforced if necessary.
History.
The phrase appears in British Parliamentary records of 1821, and in Massachusetts public records of 1835. The Oxford English Dictionary cites P. G. Wodehouse's 1929 story collection "Mr Mulliner Speaking" as the first appearance of the term.
Industry.
A gentleman's agreement, defined in the early 20th century as "an agreement between gentlemen looking toward the control of prices," was reported by one source to be the loosest form of a "pool." These types of agreements have been reported to be found in every type of industry, and are numerous in the steel and iron industries.
A report from the United States House of Representatives detailing their investigation of the United States Steel Corporation asserted that there were two general types of loose associations or consolidations between steel and iron interests in the 1890s, in which the individual concerns retained ownership as well as a large degree of independence: the "pool" and the "gentleman's agreement". The latter type lacked any formal organization to regulate output or prices, nor did they contain any provisions for forfeiture in the event of an infraction. The efficacy of the agreement relied on members to keep informal pledges.
In the automotive industry, Japanese manufacturers agreed that no production car would have more than 276 horsepower; this agreement ended in 2005. German manufacturers limit the top speed of high-performance saloons (sedans) and station wagons to 250 kilometres per hour (155 miles per hour).
International relations.
Intense anti-Japanese sentiment developed on the West Coast. US President Theodore Roosevelt did not want to anger Japan by passing legislation to bar Japanese immigration to the US, as had been done for Chinese immigration. Instead, there was an informal "Gentlemen's Agreement" (1907–8) between the United States and Japan, whereby Japan made sure there was very little or no movement to the US. The agreements were made by Secretary of State, Elihu Root, and Japan's Foreign Minister, Tadasu Hayashi. The agreement banned emigration of Japanese laborers to the US and rescinded the segregation order of the San Francisco School Board in California, which had humiliated and angered the Japanese. The agreement did not apply to the Territory of Hawaii, which was treated at the time as separate and distinct from the US. The agreements remained effect until 1924, when Congress forbade all immigration from Japan.
Trade policies.
Gentlemen's agreements have come to regulate international activities such as the coordination of monetary or trade policies. According to Edmund Osmańczyk in the "Encyclopedia of the United Nations and International Agreements", it is also defined as "an international term for an agreement made orally rather than in writing, yet fully legally valid". This type of agreement may allow a nation to avoid the domestic legal requirements to enter into a formal treaty, or it may be useful when a government wants to enter into a secret agreement that is not binding upon the next administration. According to another author, all international agreements are gentlemen's agreements because, short of war, they are "all" unenforceable. Osmańczyk pointed out that there is a difference between open gentlemen's agreements and secret diplomatic agreements. In the United States, a prohibition against gentlemen's agreements in commercial relations between states was introduced in 1890, because the secretive nature of such agreements was beyond anyone's control.
As a discriminatory tactic.
Gentlemen's agreements were a widely used discriminatory tactic reportedly more common than restrictive covenants in "preserving" the homogeneity of upper-class neighborhoods and suburbs in the United States. The nature of these agreements made them extremely difficult to prove or to track, and were effective long after the United States Supreme Court's rulings in "Shelley v. Kraemer" and "Barrows v. Jackson". One source states that gentlemen's agreements "undoubtedly still exist," but that their use has greatly diminished.
In 1934, the National Football League entered into a gentlemen's agreement to ban black players. Until Jackie Robinson was hired by the Brooklyn Dodgers in 1946, a gentlemen's agreement also ensured that African American players were excluded from organized baseball.

</doc>
<doc id="55727" url="http://en.wikipedia.org/wiki?curid=55727" title="Newlands Reclamation Act">
Newlands Reclamation Act

The Reclamation Act (also known as the Lowlands Reclamation Act or National Reclamation Act) of 1902 () is a United States federal law that funded irrigation projects for the arid lands of 20 states in the American West.
The act at first covered only 13 of the western states as Texas had no federal lands. Texas was added later by a special act passed in 1906. The act set aside money from sales of semi-arid public lands for the construction and maintenance of irrigation projects. The newly irrigated land would be sold and money would be put into a revolving fund that supported more such projects. This led to the eventual damming of nearly every major western river. Under the act, the Secretary of the Interior created the "United States Reclamation Service" within the United States Geological Survey to administer the program. In 1907 the Service became a separate organization within the Department of the Interior and was renamed the United States Bureau of Reclamation.
The Act was authored by Representative Francis G. Newlands of Nevada. Amendments made by the Reclamation Project Act of 1939 gave the Department of the Interior, among other things, the authority to amend repayment contracts and to extend repayment for not more than 40 years. Amendments made by the Reclamation Reform Act of 1982 (P.L. 97-293) eliminated the residency requirement provisions of reclamation law, raised the acreage limitation on lands irrigated with water supplied by the Bureau of Reclamation, and established and required full-cost rates for land receiving water above the acreage limit.
Background.
John Wesley Powell, often considered the father of reclamation, began a series of expeditions to explore the American West in 1867. He saw that after snow-melt and spring rains, the rivers of the West flooded, releasing huge amounts of water, and that for the rest of the year not enough rain fell to support any kind of real agriculture. He concluded that the Western United States was so arid that it could not support extensive development yet, the U.S. government saw too much economic potential in the West to heed Powell's warning. By damming western rivers in order to support massive irrigation projects, population growth and farming were made possible.
Several private and local farming organizations proved the benefits of irrigation projects. However, when it became apparent that a greater effort would be required, Representative Francis G. Newlands of Nevada introduced legislation into the United States Congress to provide federal help for irrigation projects. The resulting act passed on June 17, 1902.
Newlands carried the bulk of the legislative burden and had strong technical backup from Frederick Haynes Newell of the Department of the Interior. President Theodore Roosevelt cobbled together the legislative alliances that made passage of the act possible.
It was later amended by the Reclamation Reform Act of 1982 (, Title II) to limit corporate use of water and speculation on land that would benefit from irrigation. Reclamation includes draining, too.
Summary of the Act.
The full name of the act is "An Act Appropriating the receipts from the sale and disposal of public lands in certain States and Territories to the construction of irrigation works for the reclamation of arid lands".
Section One<br>
This section identifies the 16 states and territories to be included in the project; Arizona, California, Colorado, Idaho, Kansas, Montana, Nebraska, Nevada, New Mexico, North Dakota, Oklahoma, Oregon, South Dakota, Utah, Washington, and Wyoming. It requires surplus fees from sales of land be set aside for a "reclamation fund" for the development of water resources. Also requires the Treasury Department to fund education from unappropriated monies under certain conditions.
Section Two<br>
Authorizes the Secretary of the Interior to determine the reclamation projects. 
Section Three<br>
Requires the Secretary of the Interior to withdraw all such land from public entry.
Section Four<br>
Authorizes the Secretary of the Interior to contract for the project with certain conditions. Also requires that the work day will be 8 hours and that no so-called "Mongolian" labor (unskilled laborers from Asia) will be used.
Section Five<br>
Sets certain requirements for those using the water, including; half of the land must be for agriculture, user must pay apportioned charges, user cannot use more than the apportioned water, user cannot sell entire water to one neighbor or any water to a non-resident, and user must pay apportioned charges annually.
Section Six<br>
Authorizes to Secretary of the Interior to use the reclamation fund for all works constructed under the act and to pass management of projects over to the users once they have paid.
Section Seven<br>
Gives the Secretary of the Interior the power of Eminent Domain for construction projects.
Section Eight<br>
Requires the Secretary of the Interior to conform to state laws with certain exceptions.
Section Nine<br>
Requires the Secretary of the Interior to expend monies generated by each state within that state as much as is practicable.
Section Ten<br>
Authorizes the Secretary of the Interior to make such rules and regulation as is necessary to carry out the provisions of the act.
Results of the act.
Below are listed the larger of the irrigation projects of the United States, with the area reclaimed or to be reclaimed as of 1925. (1) 
Much of the West could not have been settled without the water provided by the Act. The West became one of the premier agricultural areas in the world. Bureau of Reclamation statistics show that the more than 600 of their dams on waterways throughout the West provide irrigation for 10 million acres (40,000 km²) of farmland, providing 60% of the nation's vegetables and 25% of its fruits and nuts. Currently, the Bureau operates about 180 projects in the West.
Not envisioned by the act, Bureau of Reclamation dams support 58 power plants producing 40 billion kilowatt hours of electricity annually. Most of the large population centers in the Far West owe their growth to these power sources.
See also.
River systems

</doc>
<doc id="55728" url="http://en.wikipedia.org/wiki?curid=55728" title="Federal Meat Inspection Act">
Federal Meat Inspection Act

The Federal Meat Inspection Act of 1906 (FMIA) is a United States Congress Act that works to prevent adulterated or misbranded meat and meat products from being sold as food and to ensure that meat and meat products are slaughtered and processed under sanitary conditions. These requirements also apply to imported meat products, which must be inspected under equivalent foreign standards. USDA inspection of poultry was added by the Poultry Products Inspection Act of 1957. The Food, Drug, and Cosmetic Act authorizes the Food and Drug Administration (FDA) to provide inspection services for all livestock and poultry species not listed in the FMIA or PPIA, including venison and buffalo. The Agricultural Marketing Act authorizes the USDA to offer voluntary, fee-for-service inspection services for these same species.
Historical motivation for enactment.
The original 1906 Act authorized the Secretary of Agriculture to inspect and condemn any meat product found unfit for human consumption. Unlike previous laws ordering meat inspections, which were enforced to assure European nations from banning pork trade, this law was strongly motivated to protect the American diet. All labels on any type of food had to be accurate (although not all ingredients were provided on the label). Even though all harmful food was banned, a few warnings were still provided on the container. The law was partly a response to the publication of Upton Sinclair's "The Jungle", an exposé of the Chicago meat packing industry, as well as to other Progressive Era muckraking publications of the day. While Sinclair's dramatized account was intended to bring attention to the terrible working conditions in Chicago, the public was more horrified by the prospect of bad meat.
The book's assertions were confirmed in the Neill-Reynolds report, commissioned by President Theodore Roosevelt in 1906. Roosevelt was suspicious of Sinclair's socialist attitude and conclusions in "The Jungle", so he sent labor commissioner Charles P. Neill and social worker James Bronson Reynolds, men whose honesty and reliability he trusted, to Chicago to make surprise visits to meat packing facilities.
Despite betrayal of the secret to the meat packers, who worked three shifts a day for three weeks to thwart the inspection, Neill and Reynolds were still revolted by the conditions at the factories and at the lack of concern by plant managers (though neither had much experience in the field). Following their report, Roosevelt became a supporter of regulation of the meat packing industry, and, on June 30, signed the Meat Inspection Act of 1906.
Provisions.
The FMIA mandated the United States Department of Agriculture (USDA) inspection of meat processing plants that conducted business across state lines. The Pure Food and Drug Act, enacted on the same day in 1906, also gave the government broad jurisdiction over food in interstate commerce.
The four primary requirements of the Meat Inspection Act of 1906 were:
After 1906, many additional laws that further standardized the meat industry and its inspection were passed.
Preemption of state law.
In 2012, the U.S. Supreme Court ruled in "National Meat Assn. v. Harris", that the FMIA preempts a California law regulating the treatment of non-ambulatory livestock.

</doc>
<doc id="55729" url="http://en.wikipedia.org/wiki?curid=55729" title="Pure Food and Drug Act">
Pure Food and Drug Act

The was the first of a series of significant consumer protection laws enacted by the Federal Government in the 20th century and led to the creation of the Food and Drug Administration. Its main purpose was to ban foreign and interstate traffic in adulterated or mislabeled food and drug products, and it directed the U.S. Bureau of Chemistry to inspect products and refer offenders to prosecutors. It required that active ingredients be placed on the label of a drug’s packaging and that drugs could not fall below purity levels established by the United States Pharmacopeia or the National Formulary. "The Jungle" by Upton Sinclair was an inspirational piece that kept the public's attention on the important issue of unsanitary meat processing plants that later led to food inspection legislation. 
Historical significance.
The Pure Food and Drug Act of 1906 was a key piece of Progressive Era legislation, signed by President Theodore Roosevelt on the same day as the Federal Meat Inspection Act. Enforcement of the Pure Food and Drug Act was assigned to the Bureau of Chemistry in the U.S. Department of Agriculture which was renamed the U.S. Food and Drug Administration (FDA) in 1930. The Meat Inspection Act was assigned to what is now known as the Food Safety and Inspection Service that remains in the U.S. Department of Agriculture. The first federal law regulating foods and drugs, the 1906 Act's reach was limited to foods and drugs moving in interstate commerce. Although the law drew upon many precedents, provisions, and legal experiments pioneered in individual states, the federal law defined "misbranding" and "adulteration" for the first time and prescribed penalties for each. The law recognized the U.S. Pharmacopeia and the National Formulary as standards authorities for drugs, but made no similar provision for federal food standards. The law was principally a "truth in labeling" law designed to raise standards in the food and drug industries and protect the reputations and pocketbooks of honest businessmen.
Particular drugs deemed dangerous.
Under the law, drug labels, for example, had to list any of 10 ingredients that were deemed "addictive" and/or "dangerous" on the product label if they were present, and could not list them if they were not present. Alcohol, morphine and opium, and cannabis were all included on the list of these "addictive" and/or "dangerous" drugs. The law also established a federal cadre of food and drug inspectors that one Southern opponent of the legislation criticized as "a Trojan horse with a bellyful of inspectors." Penalties under the law were modest, but an underappreciated provision of the Act proved more powerful than monetary penalties. Goods found in violation of the law were subject to seizure and destruction at the expense of the manufacturer. That, combined with a legal requirement that all convictions be published (Notices of Judgment), proved to be important tools in the enforcement of the statute and had a deterrent effect upon would-be violators. Deficiencies in this original statute, which had become noticeable by the 1920s, led to the replacement of the 1906 statute with the Federal Food, Drug, and Cosmetic Act, which, was enacted in 1938 and signed by President Franklin Roosevelt. The 1938 Food, Drug, and Cosmetic Act, along with its numerous amendments, remains the statutory basis for federal regulation of all foods, drugs, biological products, cosmetics, medical devices, tobacco, and radiation-emitting devices by the U.S. FDA.
History of passage.
It took 27 years to pass the 1906 statute, during which time the public was made aware of many problems with foods and drugs in the U.S. Muckraking journalists, such as Samuel Hopkins Adams, targeted the patent medicine industry with its high-alcoholic content patent medicines, soothing syrups for infants with opium derivatives, and "red clauses" in newspaper contracts providing that patent medicine ads (upon which most newspapers of the time were dependent) would be withdrawn if the paper expressed support for food and drug regulatory legislation. The Chief Chemist of the Bureau of Chemistry, Dr. Harvey Washington Wiley, captured the country's attention with his hygienic table studies, which began with a modest Congressional appropriation in 1902. The goal of the table trial was to study the human effects of common preservatives used in foods during a period of rapid changes in the food supply brought about by the need to feed cities and support an industrializing nation increasingly dependent on immigrant labor. Wiley recruited young men to eat all their meals at a common table as he added increased "doses" of preservatives including borax, benzoate, formaldehyde, sulfites, and salicylates. The table trials captured the nation's fancy and were soon dubbed "The Poison Squad" by newspapers covering the story. The men soon adopted the motto "Only the Brave dare eat the fare" and at times the publicity given to the trials became a burden. Though many results of the trial came to be in dispute, there was no doubt that formaldehyde was dangerous and it disappeared quickly as a preservative. Wiley himself felt that he had found adverse effects from large doses of each of the preservatives and the public seemed to agree with Wiley. In many cases, most particularly with ketchup and other condiments, the use of preservatives was often used to disguise insanitary production practices. Although the law itself did not proscribe the use of some of these preservatives, consumers increasingly turned away from many products with known preservatives.
The 1906 statute regulated food and drugs moving in interstate commerce and forbade the manufacture, sale, or transportation of poisonous patent medicines. The Act arose due to public education and exposés from public interest guardians such as Upton Sinclair and Samuel Hopkins Adams, social activist Florence Kelley, researcher Harvey W. Wiley, and President Theodore Roosevelt.
Beginnings of the Food and Drug Administration.
The 1906 Act paved the way for the eventual creation of the Food and Drug Administration (FDA) and is generally considered to be that agency's founding date, though the agency existed before the law was passed and was not named FDA until later. "While the Food and Drug act remains a foundational law of the FDA mission, it's not the law that created the FDA. [Initially,] the Bureau of Chemistry (the precursor to the FDA) regulated food safety. In 1927, the Bureau was reorganized into the Food, Drug, and Insecticide Administration and the Bureau of Chemistry and Soils. The FDIA was renamed the FDA in 1930."
The law itself was largely replaced by the much more comprehensive Federal Food, Drug, and Cosmetic Act of 1938.
Enforcement of labeling and future ramifications.
The Pure Food and Drug Act was initially concerned with ensuring products were labeled correctly. Later efforts were made to outlaw certain products that were not safe, followed by efforts to outlaw products which were safe but not effective. For example, there was an attempt to outlaw Coca-Cola in 1909 because of its excessive caffeine content; caffeine had replaced cocaine as the active ingredient in Coca-Cola in 1903. In the case "United States v. Forty Barrels and Twenty Kegs of Coca-Cola", the judge found that Coca-Cola had a right to use caffeine as it saw fit, although Coca-Cola eventually lost when the government appealed to the Supreme Court. It reached a settlement with the United States government to reduce the caffeine amount.
In addition to caffeine, the Pure Food and Drug Act required that drugs such as alcohol, cocaine, heroin, morphine, and cannabis, be accurately labeled with contents and dosage. Previously many drugs had been sold as patent medicines with secret ingredients or misleading labels. Cocaine, heroin, cannabis, and other such drugs continued to be legally available without prescription as long as they were labeled. It is estimated that sale of patent medicines containing opiates decreased by 33% after labeling was mandated. The Pure Food and Drug Act of 1906 is cited by drug policy reform advocates such as James P. Gray as a successful model for re-legalization of currently prohibited drugs by requiring accurate labels, monitoring of purity and dose, and consumer education.

</doc>
<doc id="55731" url="http://en.wikipedia.org/wiki?curid=55731" title="Aldrich–Vreeland Act">
Aldrich–Vreeland Act

The Aldrich–Vreeland Act was passed in response to the Panic of 1907 and established the National Monetary Commission, which recommended the Federal Reserve Act of 1913.
On May 27, 1908, the bill passed the House on a mostly party-line vote of 166–140, with 13 Republicans voting against it and no Democrats voting for it. On May 30, it passed in the Senate with 43 Republicans in favor and five Republicans joining 17 Democrats opposed. President Roosevelt signed the bill that same night.
The act also allowed national banks to start national currency associations in groups of ten or more, with at least $5 million in total capital, to issue emergency currency. These bank notes were to be backed by not just government bonds but also just about any securities the banks were holding. The act proposed that this emergency currency had to go through a process of approval by the officers of these national currency associations and then distributed by the Comptroller of the Currency.
However, it is possible that because there was a 5 percent tax placed on this emergency currency for the first month it was "outstanding" and a 1 percent increase for the following months it was "outstanding," no bank notes were issued. Another possible explanation that the emergency currency was never issued might have been that it was unnecessary.
Congress modified and extended the law in 1914 when British and other foreign creditors demanded immediate payments, in gold, of amounts which would ordinarily have been carried over and paid through exports of commodities.
Senator Nelson W. Aldrich (R-RI) was largely responsible for the "Aldrich-Vreeland Currency Law" and became the Chairman of the "National Monetary commission". The co-sponsor of the legislation was Rep. Edward Vreeland, a Republican from New York.
A usage of the law occurred at the outbreak of the World War I in 1914 when the first great financial panic of the 20th century befell the world, necessitating the closure of the New York Stock Exchange. Secretary of the Treasury William Gibbs McAdoo appeared in New York City and assured the public that ample stocks of emergency banknotes had been prepared in accordance with the Aldrich–Vreeland Act and were available for issue to the banks. As of October 23, 1914, $368,616,990 was outstanding.
The Federal Reserve Act of December 23, 1913 took effect in November 1914 when the 12 regional banks opened for business. Ultimately the emergency currency issued under the "Aldrich-Vreeland Law" was entirely withdrawn.

</doc>
<doc id="55732" url="http://en.wikipedia.org/wiki?curid=55732" title="Elkins Act">
Elkins Act

The Elkins Act is a 1903 United States federal law that amended the Interstate Commerce Act of 1887. The Act authorized the Interstate Commerce Commission (ICC) to impose heavy fines on railroads that offered rebates, and upon the shippers that accepted these rebates. The railroad companies were not permitted to offer rebates. Railroad corporations, their officers, and their employees, were all made liable for discriminatory practices.
Prior to the Elkins Act, the livestock and petroleum industries paid standard rail shipping rates, but then would demand that the railroad company give them rebates. The railroad companies resented being extorted by the railroad trusts and therefore welcomed passage of the Elkins Act. The law was sponsored by President Theodore Roosevelt as a part of his "Square Deal" domestic program, and greatly boosted his popularity.
Background.
Congress passed the Elkins Act as an amendment to the Interstate Commerce Act. Without restrictive legislation, large firms could demand rebates or prices below the collusive price from railroad companies as condition for their business. As a result, it was common practice for railroads to offer competitive lower rates for transport between the large cities with high density of firms than the monopolistic rates between less industrial cities, irrespective of length of travel. Trusts constituted such a substantial portion of a carrier's revenue that the trusts could demand rebates as a condition for business, and the carrier would be forced to cooperate. 
Purpose.
The ICC had been unable to protect competition and fair pricing. Section 2 of the Interstate Commerce Act prohibits a carrier from offering preferential prices or rebates; however, enforcement of this section was ineffective. Powerful trusts would pay the standard shipping price, but demand a rebate from the carrier. Court cases brought before the commission generally did not result in punitive action, as the ICC was composed primarily of railroad interests. Carriers found guilty of price discrimination, moreover, could appeal the ICC decision to federal courts, delaying punishment for years.
The Elkins Act was named for its sponsor, Senator Stephen B. Elkins of West Virginia, who introduced a bill in 1902 at the behest of the Pennsylvania Railroad. The law was passed by the 57th Congress and signed by President Roosevelt on February 19, 1903. The Act made it a misdemeanor for a carrier to impose preferential rebates, and implicated both the carrier and the recipient of the low price. The Act also abolished imprisonment as a punishment for breaching the law, so a violator could only be fined. By reducing the severity of punishment, legislators hoped to encourage firms to testify against each other, and promote stricter enforcement of the law.
Impact.
Following the passage of the Elkins Act, real freight rates decreased only slightly. In 1905, leaders in the regulation movement testified before Congress to identify the reduction in prices that resulted from the Act. Yet, in the first months following the passage of the law, the most pronounced change in railroad pricing was the elimination of rebates. However, later analysis has found that decreases in carrier prices are better attributable to decreases in the costs of operation due to technology advances. The elimination of rebates led the railroads to seek other methods to compete for business, leading Governor Albert B. Cummins of Iowa to declare, in 1905, that the elimination of rebates simply forces railroads to seek alternative noncompetitive means to secure business. The Elkins Act, thus, was more effective in stabilizing prices and entrenching price collusion than demonstrably lowering prices.
A diverse group of stakeholders publicly supported the Elkins Act. Citizens who supported the law hoped that reducing price discrimination would lower freight prices uniformly, and railroad interests lobbied for the passage of the Act as a means of enforcing collusive pricing. While the Act restricted preferential pricing, it did not specify what constituted a "reasonable" shipping rate; thus, railroads could use the law to entrench a system of collusive prices. Collusion is unsustainable in a market where it is easy to undercut competitors. However in industries that only have a small number of competitors (e.g. railroads, airlines, or transportation companies operating between two given cities) collusion is far more likely. The result of the Elkins Act was that Railroads had a stronger mechanism to protect their collusive prices and corporate trusts were weakened in their ability to gain shipping discounts. Farmers and other railroad users, instead of benefiting from greater competition, were unaffected by the Act.
While farmers may have benefited from the establishment of a price ceiling on freight rates, the nature of the railroad industry may have not have permitted perfect competition. Economist Robert Harbeson argues that the price wars prior to the Elkins Act suggest that the railroad industry was more oligopolistic. In an industry with decreasing marginal costs and high fixed costs, it would be futile to enforce a price cap. Moreover, he argues, stronger regulation would have prevented carriers from reaching economies of scale.
Contemporary criticism.
In reaction to the Elkins Act, it was argued that the law was drafted by Congress on behalf of the railroads, and that while some railroads curtailed rebates for some customers, for others the practice continued unabated. Congress was criticized for enacting only monetary fines for violations of the law and avoiding imposition of criminal penalties.
Subsequent legislation.
Citing the shortcomings of the Elkins Act, Progressives began to call for greater regulation of railroad interests, and, in 1906, President Roosevelt signed the Hepburn Act to replace the Elkins Act. The Hepburn Act set maximum freight rates for railroads, representing the greater interests of Americans. The regulations of the Hepburn Act strained railroads, which saw new competition from the rise of trucks and automobiles. The Panic of 1907 was, in part, a result of the turmoil of the railroad industry that resulted from the Hepburn Act.

</doc>
<doc id="55733" url="http://en.wikipedia.org/wiki?curid=55733" title="Hepburn Act">
Hepburn Act

The Hepburn Act is a 1906 United States federal law that gave the Interstate Commerce Commission (ICC) the power to set maximum railroad rates and extend its jurisdiction. This led to the discontinuation of free passes to loyal shippers. In addition, the ICC could view the railroads' financial records, a task simplified by standardized bookkeeping systems. For any railroad that resisted, the ICC's conditions would remain in effect until the outcome of legislation said otherwise. By the Hepburn Act, the ICC's authority was extended to cover bridges, terminals, ferries, railroad sleeping cars, express companies and oil pipelines.
Along with the Elkins Act of 1903, the Hepburn Act, named for its sponsor, eleven-term Republican William Peters Hepburn, was a subset of one of President Theodore Roosevelt's major goals: railroad regulation.
The final version was close to what Roosevelt had asked, and easily passed Congress with only three dissenting votes. The most important provision gave the ICC the power to replace existing rates with "just-and-reasonable" maximum rates, with the ICC to define what was just and reasonable. The Act made ICC orders binding; that is, the railroads had to either obey or contest the ICC orders in federal court. To speed the process, appeals from the district courts would go directly to the U.S. Supreme Court.
Anti-rebate provisions were toughened, free passes were outlawed, and the penalties for violation were increased. The ICC staff grew from 104 in 1890 to 178 in 1905, 330 in 1907, and 527 in 1909. Finally, the ICC gained the power to prescribe a uniform system of accounting, require standardized reports, and inspect railroad accounts.
The limitation on railroad rates depreciated the value of railroad securities, a factor in causing the Panic of 1907.
Scholars consider the Hepburn Act the most important piece of legislation regarding railroads in the first half of the 20th century. Economists and historians debate whether it crippled the railroads, giving so much advantage to the shippers that a giant unregulated trucking industry—undreamed of in 1906—took away their business.

</doc>
<doc id="55734" url="http://en.wikipedia.org/wiki?curid=55734" title="Payne–Aldrich Tariff Act">
Payne–Aldrich Tariff Act

The Payne–Aldrich Tariff Act of 1909 (ch. 6, 36 Stat. 11), named for Representative Sereno E. Payne (R–NY) and Senator Nelson W. Aldrich (R–RI), began in the United States House of Representatives as a bill raising certain tariffs on goods entering the United States. The high rates angered Republican reformers, and led to a deep split in the Republican Party.
History.
It was the first change in tariff laws since the Dingley Act of 1897. President William Howard Taft called Congress into a special session in 1909 shortly after his inauguration to discuss the issue. Thus, the House of Representatives immediately passed a tariff bill sponsored by Payne, calling for reduced tariffs. However, the United States Senate speedily substituted a bill written by Aldrich, calling for fewer reductions and more increases in tariffs.
An additional provision of the bill provided for the creation of a tariff board to study the problem of tariff modification in full and to collect information on the subject for the use of Congress and the President in future tariff considerations. Another provision allowed for free trade with the Philippines, then under American control. Congress passed the bill officially on April 9, 1909.
Taft promptly appointed members to serve on the tariff board.
Impact of the bill.
The Payne Act, in its essence a compromise bill, had the immediate effect of frustrating both proponents and opponents of reducing tariffs. In particular, the bill greatly angered Progressives, who began to withdraw support from President Taft. Because it increased the duty on print paper used by publishers, the publishing industry viciously criticized the President, further tarnishing his image. Although Taft met and consulted with Congress during its deliberations on the bill, critics charged that he ought to have imposed more of his own recommendations on the bill; namely, that of a slower schedule. However, unlike his predecessor (Theodore Roosevelt), Taft felt that the president should not dictate lawmaking and should leave Congress free to act as it saw fit.
Taft signed the bill. The debate over the tariff split the Republican Party into Progressives and Old Guards and led the split party to lose the 1910 congressional election. 
The bill enacted a small income tax on the privilege of conducting business as a corporation, which was affirmed in the Supreme Court decision "Flint v. Stone Tracy Co." (also known as the Corporation Tax case).

</doc>
<doc id="55737" url="http://en.wikipedia.org/wiki?curid=55737" title="Clear and Present Danger">
Clear and Present Danger

Clear and Present Danger is a novel by Tom Clancy, written in 1989, and is a canonical part of the Jack Ryan universe. In the novel, Jack Ryan is thrown into the position of Central Intelligence Agency (CIA) Acting Deputy Director (Intelligence) and discovers that he is being kept in the dark by his colleagues who are conducting a covert war against a drug cartel based in Colombia.
Plot summary.
When U.S. Coast Guard cutter "Panache" intercepts a yacht in the Caribbean Sea, the crew discovers two men cleaning up the vessel after murdering a man and his family. Through a mock execution, the Coast Guardsmen force the killers to confess to the crime. It is later learned that the murdered man was involved in a money laundering scheme for a drug cartel.
Upon hearing of this atrocity, the President of the United States, who is running for re-election, feels compelled to take drastic measures against drug trafficking; his challenger, J. Robert Fowler, has rallied the public behind the administration's failures in the War on Drugs. The president initiates covert operations within Colombia and a step-up of operations against aircraft believed to be distributing narcotics. Aiding the president are U.S. National Security Advisor James Cutter, CIA Deputy Director of Operations Robert Ritter, and Director of Central Intelligence Arthur Moore.
The plan consists of four operations:
Meanwhile, Félix Cortez, a former intelligence officer from Cuba employed by the cartel, feigns romantic interest in the aide of Emil Jacobs, the Director of the Federal Bureau of Investigation. The aide unknowingly reveals information regarding the date of Jacobs' official visit to the Attorney General of Colombia. Cortez delivers this information to the cartel, which orders Jacobs' assassination as retaliation for the U.S. seizure of cartel money. During his visit, Jacobs and several other Americans in his delegation are killed.
Jack Ryan suspects the CIA's involvement in the situation in Colombia. As acting Deputy Director of the Intelligence Directorate, Ryan should be privy to most operations, but he realizes he is being kept out of the loop. After Robby Jackson, assigned to the Pentagon, makes an inquiry into activity in the region, Ryan goes to Moore to demand an explanation. Moore is evasive, yet orders Ryan to withhold information about Colombia from a congressional oversight committee.
Cortez eventually uncovers the U.S. operations. He suppresses this information, planning to engineer a war within the cartel that will leave him in a position to seize power. Cortez orders mercenaries to hunt down the U.S. troops, and blackmails Cutter into ending SHOWBOAT, promising the intracartel war will slow drug imports to the States. Cutter's meeting with Cortez is shadowed by Ryan and Clark. Clark is outraged at Cutter's abandonment of the troops and, with Ryan, plans a rescue operation with personnel from the FBI and U.S. Air Force.
Clark makes radio contact with two of the SHOWBOAT teams, ordering them to alternate pickup points to await extraction. The other two teams encounter mercenaries and take casualties. Clark makes radio contact with some survivors of these remaining teams—which include Domingo Chavez—then flies into Colombia to retrieve them. Ryan uses an Air Force helicopter to pick up other survivors. Together, Clark and Ryan launch a raid on the cartel's command post, capturing Cortez and extracting the remaining ground team. Due to a hurricane and damage to the helicopter, they land on the deck of the "Panache".
Cortez is returned to Cuba, where he is a marked as a traitor. Upon being confronted by Clark with evidence of his treason, Cutter commits suicide. Ryan confronts the defiant president, informing him that despite his classifying the drug cartel as a "clear and present danger", Ryan must brief Congress over the illegal operations. After Ryan briefs the committee, the president deliberately loses the election to hide the covert operations and protect the honor of those involved. Ryan realizes the president has more honor and dignity than he originally thought. Clark recruits one distinguished soldier from the operation, Sgt. Domingo "Ding" Chavez, into the CIA, and becomes his mentor.

</doc>
<doc id="55738" url="http://en.wikipedia.org/wiki?curid=55738" title="Federal Farm Loan Act">
Federal Farm Loan Act

The Federal Farm Loan Act of 1916 (, 39 Stat. , enacted  17, 1916) was a United States federal law aimed at increasing credit to rural family farmers. It did so by creating a federal farm loan board, twelve regional farm loan banks and tens of farm loan associations. The act was signed into law by President of the United States Woodrow Wilson.
Background.
In 1908, the Administration of Theodore Roosevelt commissioned a study on the problems facing rural families. At this point in U.S. history, these families made up the largest demographic. The commission concluded that access to credit was one of the most serious problems facing rural farmers and recommended the introduction of a cooperative credit system.
Four years later, Presidents William Howard Taft and Woodrow Wilson sent a commission of Americans to study cooperative credit systems for farmers in Europe. Components of such European programs at the time included cooperative land-mortgage banks and rural credit unions. This commission concluded that the best form of cooperative credit system would include both long-term credit to cover land mortgages and short-term credit to cover regular business needs.
Effect on the rural farmer.
The most visible component of the Act were the loans to individual farmers and their families. Under the act, farmers could borrow up to 50% of the value of their land and 20% of the value of their improvements. The minimum loan was $100 and the maximum was $10,000. Loans made through the Act were paid off through amortization over 5 to 40 years. 
Borrowers also purchased shares of the National Farm Loan Association. This meant that it served as a cooperative agency that lent money from farmer to farmer. This was heavily influenced by a successful cooperative credit system in Germany called Landschaft.
The next most visible component of the Act were the mortgage-backed bonds that were issued. The rate of interest on the mortgages could be no more than 1 percent higher than the rate of interest on the bonds. This spread covered the issuers' administrative costs, but did not lead to a significant profit. In addition, the maximum rate of interest on the bonds was 6 percent, ensuring that borrowing costs for farmers was often much lower than before the Act was passed.
The act furthered Wilson's reputation against trusts and big business. By providing small farmers with competitive loans, they were now more able to compete with big business. As a result, the likelihood of agricultural monopolies decreased.
While Wilson's commission suggested that short-term credit also be incorporated in any nationalized credit system, the Act lacked this crucial component. Due to increased competition and the need for agriculture machinery, a system for short-term credit was incorporated into the current system in Agricultural Credits Act of 1923. 
Sponsored by Senator Henry F. Hollis (D) of New Hampshire and Representative Asbury F. Lever (D) of South Carolina, it was a reintroduced version of the Hollis-Bulkley Act of 1914 that had not passed Congress due to Wilson's opposition.
Structure of implementation.
The Act established the Federal Farm Loan Board to oversee and supervise federal land banks and national farm loan associations. It was also responsible for setting benchmark rates of interest for mortgages and bonds. Finally, it could intervene when it thought specific banks were making irresponsible loans.
The twelve Federal Land Banks were required to hold at least $750,000 in capital. Stock ownership of the banks were held by national farm loan associations and other interested investors, including any individual, corporation or fund. In the case of insufficient capital, the U.S. Treasury (through the Federal Farm Loan Board) made up the difference. When additional subscriptions were made from other sources, federal ownership in the banks was retired.
National Farm Loan Associations were established groups of 10 or more mortgage-holding farmers who together owned 5% or more of a federal land bank. Once formed, they were subject to a charter review process by the Federal Farm Loan Board. This structure aimed to align the incentives of individual farmers with the banks, as farmers held two rules: borrowers and lenders.
Subsequent history.
Under the administration of Herbert Hoover, the Agricultural Marketing Act of 1929 established the Federal Farm Board from the Federal Farm Loan Board established by the Federal Farm Loan Act with a revolving fund of half a billion dollars.

</doc>
<doc id="55739" url="http://en.wikipedia.org/wiki?curid=55739" title="Railway Labor Act">
Railway Labor Act

The Railway Labor Act is a United States federal law that governs labor relations in the railroad and airline industries. The Act, passed in 1926 and amended in 1934 and 1936, seeks to substitute bargaining, arbitration and mediation for strikes as a means of resolving labor disputes. Its provisions were originally enforced under the Board of Mediation, but were later enforced under a National Mediation Board.
Historical antecedents to the RLA.
After the Great Railroad Strike of 1877, which was put down only with the intervention of federal troops, Congress passed the Arbitration Act of 1888, which authorized the creation of arbitration panels with the power to investigate the causes of labor disputes and to issue non-binding arbitration awards. The Act was a complete failure: only one panel was ever convened under the Act, and that one, in the case of the 1894 Pullman Strike, issued its report only after the strike had been ended by a federal court injunction backed by federal troops.
Congress attempted to correct these shortcomings in the Erdman Act, passed in 1898. The Act likewise provided for voluntary arbitration, but made any award issued by the panel binding and enforceable in federal court. It also outlawed discrimination against employees for union activities, prohibited "yellow dog contracts" (in which an employee agrees not to join a union while employed), and required both sides to maintain the status quo during any arbitration proceedings and for three months after an award was issued. The arbitration procedures were rarely used. A successor statute, the Newlands Act of 1913, which created the Board of Mediation, proved to be more effective, but was largely superseded when the federal government nationalized the railroads in 1917. ("See" United States Railroad Administration.)
The Adamson Act, passed in 1916, provided workers with an eight hour day, at the same daily wage they had received previously for a ten hour day, and required time and a half pay for overtime work. Another law passed in the same year gave President Woodrow Wilson the power to "take possession of and assume control of any system of transportation" for transportation of troops and war material.
Wilson exercised that authority on December 26, 1917. While Congress considered nationalizing the railroads on a permanent basis after World War I, the Wilson administration announced that it was returning the railroad system to its owners. Congress tried to preserve, on the other hand, the most successful features of the federal wartime administration, the adjustment boards, by creating a Railroad Labor Board (RLB) with the power to issue non-binding proposals for the resolution of labor disputes, as part of the Esch–Cummins Act (Transportation Act of 1920).
The RLB soon destroyed whatever moral authority its decisions might have had in a series of decisions. In 1921 it ordered a twelve percent reduction in employees' wages, which the railroads were quick to implement. The following year, when shop employees of the railroads launched a national strike, the RLB issued a declaration that purported to outlaw the strike; the Department of Justice then obtained an injunction that carried out that declaration. From that point forward railway unions refused to have anything to do with the RLB.
Passage and amendment of the RLA.
The RLA was the product of negotiations between the major railroad companies and the unions that represented their employees. Like its predecessors, it relied on boards of adjustment, established by the parties, to resolve labor disputes, with a government-appointed Board of Mediation to attempt to resolve those disputes that board of adjustment could not. The RLA promoted voluntary arbitration as the best method for resolving those disputes that the Board of Mediation could not settle.
Congress strengthened these procedures in the 1934 amendments to the Act, which created a procedure for resolving whether a union had the support of the majority of employees in a particular "craft or class," while turning the Board of Mediation into a permanent agency, the National Mediation Board (NMB), with broader powers.
Congress extended the RLA to cover airline employees in 1936.
Bargaining and strikes under the RLA.
Unlike the National Labor Relations Act (NLRA), which adopts a less interventionist approach to the way the parties conduct collective bargaining or resolve their disputes arising under collective bargaining agreements, the RLA specifies both (1) the negotiation and mediation procedures that unions and employers must exhaust before they may change the status quo, and (2) the methods for resolving "minor" disputes over the interpretation or application of collective bargaining agreements. The RLA permits strikes over major disputes only after the union has exhausted the RLA's negotiation and mediation procedures, while barring almost all strikes over minor disputes. The RLA also authorizes the courts to enjoin strikes if the union has not exhausted those procedures.
On the other hand, the RLA imposes fewer restrictions on the tactics that unions may use when they do have the right to strike. The RLA does not, unlike the NLRA, bar secondary boycotts against other RLA-regulated carriers; it may also permit employees to engage in other types of strikes, such as intermittent strikes, that might be unprotected under the NLRA.
"Major" and "Minor" Disputes.
The RLA categorizes all labor disputes as either "major" disputes, which concern the making or modification of the collective bargaining agreement between the parties, or "minor" disputes, which involve the interpretation or application of collective bargaining agreements. Unions can strike over major disputes only after they have exhausted the RLA's "almost interminable" negotiation and mediation procedures. They cannot, on the other hand, strike over minor disputes, either during the arbitration procedures or after an award is issued.
The federal courts have the power to enjoin a strike over a major dispute if the union has not exhausted the RLA's negotiation and mediation procedures. The Norris-LaGuardia Act dictates the procedures that the court must follow. Once the NMB releases the parties from mediation, however, they retain the power to engage in strikes or lockouts, even if they subsequently resume negotiations or the NMB offers mediation again.
The federal courts likewise have the power to enjoin a union from striking over arbitrable disputes. The court may, on the other hand, also require the employer to restore the status quo as a condition of any injunctive relief against a strike.
Discipline and replacement of strikers.
Carriers can lawfully replace strikers engaged in a lawful strike, but may not, however, discharge them, except for misconduct, or eliminate their jobs to retaliate against them for striking. It is not clear whether the employer can discharge workers for striking before exhausting all of the RLA's bargaining and mediation processes.
The employer must also allow strikers to replace replacements hired on a temporary basis and permanent replacements who have not completed the training required before they can become active employees. The employer may, on the other hand, allow less senior employees who crossed the picket line to keep the jobs they were given after crossing the line, even if the seniority rules in effect before the strike would have required the employer to reassign their jobs to returning strikers.
Representation elections under the RLA.
The NMB has the responsibility for conducting elections when a union claims to represent a carrier's employees. The NMB defines the craft or class of employees eligible to vote, which almost always extends to all of the employees performing a particular job function throughout the company's operations, rather than just those at a particular site or in a particular region.
A union seeking to represent an unorganized group of employees must produce signed and dated authorization cards or other proof of support from at least fifty percent of the craft or class. A party attempting to oust an incumbent union must produce evidence of support from a majority of the craft of class and then the NMB must conduct an election. If the employees are unrepresented and the employer agrees, the NMB may certify the union based on the authorization cards alone.
The NMB usually uses mail ballots to conduct elections, unlike the National Labor Relations Board (NLRB), which has historically preferred walk-in elections under the NLRA. The NMB can order a rerun election if it determines that either an employer or union has interfered with employees' free choice.
Protecting employees' rights.
Unlike the NLRA, which gives the NLRB nearly exclusive power to enforce the Act, the RLA allows employees to sue in federal court to challenge an employer's violation of the Act. The courts can grant employees reinstatement and backpay, along with other forms of equitable relief.
Further reading.
</dl>

</doc>
<doc id="55740" url="http://en.wikipedia.org/wiki?curid=55740" title="Clayton Antitrust Act">
Clayton Antitrust Act

<!-- Part of WikiProject Law. Most of this is ripped off from [[Template:Intellectual prop
! style="padding: 0 7px 0 7px; background:#00FA9A" align="center" | [[Competition law]]
! style=" font-size: 95%; padding: 0 7px 0 7px; background:#98FB98" align="center" | Basic concepts 
! style="font-size: 95%; padding: 0 4px 0 4px; background:#98FB98" align="center" | [[Anti-competitive practices]] 
! style="font-size: 95%; padding: 0 4px 0 4px; background:#98FB98" align="center" | Enforcement authorities and organizations
The Clayton Antitrust Act of 1914 (, 38 [[United States Statutes at Large|Stat.]] , enacted  15, 1914, codified at #redirect [[Template:UnitedStatesCode]], #redirect [[Template:UnitedStatesCode]]), was a part of [[United States antitrust law]] with the goal of adding further substance to the U.S. antitrust law regime; the Clayton Act sought to prevent anticompetitive practices in their incipiency. That regime started with the [[Sherman Antitrust Act]] of 1890, the first Federal law outlawing practices considered harmful to consumers (monopolies, cartels, and trusts). The Clayton Act specified particular prohibited conduct, the three-level enforcement scheme, the exemptions, and the remedial measures.
Like the Sherman Act, much of the substance of the Clayton Act has been developed and animated by the [[Federal judiciary of the United States|U.S. courts]], particularly the [[Supreme Court of the United States|Supreme Court]].
Background.
Since the [[Sherman Antitrust Act]] of 1890, courts in the United States had interpreted the law on cartels as applying against [[trade unions]]. This had created an impossible situation for workers, who needed to organize so as to rebalance the [[equal bargaining power]] against their employers. The Sherman Act had also triggered the largest wave of [[merger]]s in US history, as businesses realized that instead of creating a [[cartel]] they could simply fuse into a single [[corporation]], and have all the benefits of [[market power]] that a cartel could bring. At the end of the [[William Howard Taft|Taft]] administration, and the start of the [[Woodrow Wilson]] administration, a "[[Commission on Industrial Relations]]" was established. During its proceedings, and in anticipation of its first report on the 23 October 1914, legislation was introduced by [[Alabama]] [[U.S. Democratic Party|Democrat]] [[Henry De Lamar Clayton Jr.]] in the U.S. House of Representatives. The Clayton Act passed by a vote of 277 to 54 on June 5, 1914. Though the Senate passed its own version on September 2, 1914, by a vote of 46–16, the final version of the law (written after deliberation between [[U.S. Senate|Senate]] and the [[U.S. House of Representatives|House]]), did not pass the Senate until October 5 and the House until October 8 of the next year.
Contents.
The Clayton Act made both substantive and procedural modifications to federal antitrust law. Substantively, the act seeks to capture anticompetitive practices in their incipiency by prohibiting particular types of conduct, not deemed in the best interest of a competitive market. There are 4 sections of the bill that proposed substantive changes in the antitrust laws by way of supplementing the Sherman Antitrust Act of 1890. In those sections, the Act thoroughly discusses the following four principles of economic trade and business:
Comparisons to other acts.
Unilateral price discrimination is clearly outside the reach of Section 1 of the Sherman Act, which only extended to "concerted activities" (agreements). Exclusive dealing, tying, and mergers are all agreements, and theoretically, within the reach of Section 1 of the Sherman Act. Likewise, mergers that create monopolies would be actionable under Sherman Act Section 2.
Section 7 of the Clayton Act allows greater regulation of mergers than just Sherman Act Section 2, since it does not require a merger-to-monopoly before there is a violation. It allows the Federal Trade Commission and Department of Justice to regulate all mergers, and gives the government discretion whether to give approval to a merger or not, which it still commonly does today. The government often employs the [[Herfindahl index|Herfindahl-Hirschman Index]] (HHI) test for market concentration to determine whether the merger is presumptively anticompetitive; if the HHI level for a particular merger exceeds a certain level, the government will investigate further to determine its probable competitive impact.
Section 7.
Section 7 elaborates on specific and crucial concepts of the Clayton Act; "[[holding company]]" defined as a "common and favorite method of promoting monopoly", but more precisely as "a company whose primary purpose is to hold stocks of other companies" which the government saw as an abomination and a mere corporated form of the 'old fashioned' trust.
Another important factor to consider is the amendment passed in Congress on Section 7 of the Clayton Act in 1950. This original position of the US government on mergers and acquisitions was strengthened by the Celler-Kefauver amendments of 1950, so as to cover asset as well as stock acquisitions.
Pre-merger notification.
Section 7a, #redirect [[Template:UnitedStatesCode]], requires that companies notify the [[Federal Trade Commission]] and the [[Assistant Attorney General]] of the [[United States Department of Justice Antitrust Division]] of any contemplated [[mergers and acquisitions]] that meet or exceed certain thresholds. Pursuant to the [[Hart–Scott–Rodino Antitrust Improvements Act]], section 7A(a)(2) requires the Federal Trade Commission to revise those thresholds annually, based on the change in [[gross national product]], in accordance with Section 8(a)(5) and take effect 30 days after publication in the Federal Register. (For example, see 74 [[Federal Register|FR]] and .)
Section 8.
Section 8 of the Act refers to the prohibition of one person of serving as director of two or more corporations if the certain threshold values are met, which are required to be set by regulation of the Federal Trade Commission, revised annually based on the change in gross national product, pursuant to the Hart–Scott–Rodino Antitrust Improvements Act. (For example, see 74 [[Federal Register|FR]] .)
Other.
Because the act singles out exclusive dealing and tying arrangements, one may assume they would be subject to heightened scrutiny, perhaps they would even be [[illegal per se|illegal "per se"]]. That remains true for tying, under the authority of "[[Jefferson Parish Hospital District No. 2 v. Hyde]]". However, when exclusive dealings are challenged under Clayton-3 (or Sherman-1), they are treated under the [[rule of reason]]. Under the 'rule of reason', the conduct is only illegal, and the plaintiff can only prevail, upon proving to the court that the defendants are doing substantial economic harm.
Exemptions.
An important difference between the Clayton Act and its predecessor, the Sherman Act, is that the Clayton Act contained safe harbors for union activities. Section 6 of the Act (codified at #redirect [[Template:UnitedStatesCode]]) exempts [[trade union|labor unions]] and agricultural organizations, saying 'that the labor of a human being is not a commodity or article of commerce, and permit[ting] labor organizations to carry out their legitimate objective'. Therefore, [[boycott]]s, peaceful [[strike action|strikes]], peaceful [[Picketing (protest)|picketing]], and [[collective bargaining]] are not regulated by this statute. Injunctions could be used to settle labor disputes only when property damage was threatened.
[[Major League Baseball]] is another company exempt from the Clayton Antitrust Act due to the national heritage associated with it. ("[[Federal Baseball Club v. National League|Federal Baseball Club of Baltimore, Inc. v. National League of Professional Baseball Clubs, et al.]]", 259 U.S. 200 (1922))
Enforcement.
Procedurally, the Act empowers private parties injured by violations of the Act to sue for treble damages under Section 4 and injunctive relief under Section 16. The Supreme Court has expressly ruled that the "injunctive relief" clause in Section 16 includes the implied power to force defendants to divest assets.
Under the Clayton Act, only civil suits could be brought to the court's attention and a provision "permits a suit in the federal courts for three times the actual damages caused by anything forbidden in the antitrust laws", including court costs and attorney's fees.
The Act is enforced by the [[Federal Trade Commission]], which was also created and empowered during the Wilson Presidency by the Federal Trade Commission Act, and also the [[Antitrust Division of the U.S. Department of Justice]].
Legacy.
The Clayton Act of 1914 reformed and emphasized certain concepts of the Sherman Act of 1890 that are still active today in a growing interconnected market and merging of the industries (unsuccessful).
External links.
[[Category:1914 in law]]
[[Category:1914 in the United States]]
[[Category:63rd United States Congress]]
[[Category:History of the United States (1865–1918)]]
[[Category:United States federal antitrust legislation]]
[[Category:Presidency of Woodrow Wilson]]
[[Category:Law articles needing an infobox]]
[[Category:Progressive Era in the United States]]

</doc>
<doc id="55741" url="http://en.wikipedia.org/wiki?curid=55741" title="Federal Trade Commission Act">
Federal Trade Commission Act

<!-- Part of WikiProject Law. Most of this is ripped off from [[Template:Intellectual prop
! style="padding: 0 7px 0 7px; background:#00FA9A" align="center" | [[Competition law]]
! style=" font-size: 95%; padding: 0 7px 0 7px; background:#98FB98" align="center" | Basic concepts 
! style="font-size: 95%; padding: 0 4px 0 4px; background:#98FB98" align="center" | [[Anti-competitive practices]] 
! style="font-size: 95%; padding: 0 4px 0 4px; background:#98FB98" align="center" | Enforcement authorities and organizations
The Federal Trade Commission Act of 1914 (15 U.S.C §§ 41-58, "as amended") (FTC Act) established the [[Federal Trade Commission]] (FTC). The Act, signed into law by [[Woodrow Wilson]] in 1913, outlaws unfair methods of competition and to outlaw unfair acts or practices that affect commerce. The act also creates the Federal Trade Commission, a five-member board, to regulate questionable business practices 
Origin of Act.
The inspiration and motivation for this act started in 1890, when the [[Sherman Antitrust Act|Sherman Act]] was passed. This era in time was an antitrust movement to prevent manufacturers from joining price-fixing cartels. After the case [[Northern Securities Co. v. United States]], which dismantled a [[J. P. Morgan]] company, antitrust enforcement became institutionalized. Soon after, Wilson created the [[Bureau of Corporations]], an agency that reported on the economy and businesses in the industry. This agency was the predecessor to the Federal Trade Commission. In 1913, President Wilson expanded on this agency by passing the Federal Trade Commissions Act along with the [[Clayton Antitrust Act]]. The Federal Trade Commission Act was designed for business reform. Congress passed this Act with the hopes of protecting consumers against methods of deception in advertisement, forcing the business to be upfront and truthful about items being sold. 
Summary.
The Federal Trade Commission Act does more than just create the Commission, "Under this Act, the Commission is empowered, among other things, to (a) prevent unfair methods of competition, and unfair or deceptive acts or practices in or affecting commerce; (b) seek monetary redress and other relief for conduct injurious to consumers; (c) prescribe trade regulation rules defining with specificity acts or practices that are unfair or deceptive, and establishing requirements designed to prevent such acts or practices; (d) conduct investigations relating to the organization, business, practices, and management of entities engaged in commerce; and (e) make reports and legislative recommendations to Congress." This act was part of a bigger movement in the early 1900's to use special groups like commissions to regulate and oversee certain forms of business. The Federal Trade Commission Act works in junction with The Sherman Act and The Clayton Act. Any violations of The Sherman Act will also violate the Federal Trade Commission Act so the Federal Trade Commission can act on cases that violate each act. The Federal Trade Commission Act, along with other two antitrust laws, have were created for the sole objective to "protect the process of competition for the benefit of consumers, making sure there are strong incentives for businesses to operate efficiently, keep prices down, and keep quality up." These acts are considered the core of antitrust laws and are still very important in today's society. 
This commission was authorized to issue “[[cease and desist]]” orders to large [[corporations]] to curb unfair trade practices. Some of the unfair methods of competition that were targeted include deceptive advertisements and pricing. It passed the Senate by a 43-5 vote on September 8, 1914; passed the House on September 10, without a tally of yeas and nays, and was signed into law by President Wilson on September 26.
External links.
[[Category:1914 in law]]
[[Category:1914 in the United States]]
[[Category:63rd United States Congress]]
[[Category:Federal Trade Commission|Act]]
[[Category:United States federal government administration legislation|Trade Commission Act]]
[[Category:United States federal trade legislation|Trade Commission Act]]
[[Category:United States federal antitrust legislation|Trade Commission Act]]
[[Category:Law articles needing an infobox]]
[[Category:Progressive Era in the United States]]

</doc>
<doc id="55742" url="http://en.wikipedia.org/wiki?curid=55742" title="Federal Reserve Act">
Federal Reserve Act

The Federal Reserve Act (ch. 6, 38 Stat. , enacted December 23, 1913, ) is an Act of Congress that created and established the Federal Reserve System, the central banking system of the United States of America, and granted it the legal authority to issue Federal Reserve Notes (now commonly known as the U.S. Dollar) and Federal Reserve Bank Notes as legal tender. The Act was signed into law by President Woodrow Wilson.
The Act.
The Federal Reserve Act created a system of private and public entities; there were to be at least eight, and no more than 12, private regional Federal Reserve banks. Twelve were established, and each had various branches, a board of directors, and district boundaries. The Federal Reserve Board, consisting of seven members, was created as the governing body of the Fed. Each member is appointed by the President of the United States and confirmed by the U.S. Senate. In 1935, the Board was renamed and restructured. Also created as part of the Federal Reserve System was a 12-member Federal Advisory Committee and a single new United States currency, the Federal Reserve Note. The Federal Reserve act did create a national currency, but more importantly, the act was to create a monetary system that could respond effectively to the different stresses in the banking system and create a stable financial system. Along with the goal of creating a national monetary system and financial stability, the Federal Reserve Act also provided many other functions and financial services for the economy such as a check clearing and collection for all members of the Federal Reserve.
With the passing of the Federal Reserve Act, Congress required that all nationally chartered banks become members of the Federal Reserve System. These banks were required to purchase specified non-transferable stock in their regional Federal Reserve banks, and to set aside a stipulated amount of non-interest bearing reserves with their respective reserve banks. Since 1980, all depository institutions have been required to set aside reserves with the Federal Reserve. Such institutions are entitled to certain Federal Reserve services. State chartered banks were given the option of becoming members of the Federal Reserve System and in the case of the exercise of such option were to be subject to supervision, in part, by the Federal Reserve System. Member banks became entitled to have access to discounted loans at the discount window in their respective reserve banks, to a 6% annual dividend in their Federal Reserve stock, and to other services.
Background.
Central banking has made various institutional appearances throughout the history of the United States. These institutions started with the First and Second banks of the United States, which were championed in large part by Alexander Hamilton.
The First Bank of United States.
The American financial system was deeply fragmented after the revolutionary war. The government was burdened with large wartime debts, and the new republic needed a strong financial institution to give the country a resilient financial footing. Alexander Hamilton and Andrew Jackson had opposing views regarding whether or not the US could benefit from a European style national financial institution. Hamilton was in favor of building a strong centralized political and economic institution to solve the country’s financial problem. He argued that a central bank could bring order to the US monetary system, manage the government’s revenues and payments, and provide credit to both the public and private sectors. On the other hand, Jackson was deeply suspicious of a central bank because, he argued, it would undermine democracy. Jackson and Southern members of congress also believed that a strong central financial institution would serve commercial interests of the north at the expense of Southern-based agriculture interests whose credit was provided by local banks during the post-revolutionary war era.
The First Bank of the United States was established in 1791 chartered for a period of twenty years. The US government was the largest shareholder of the bank. Despite its shareholder status, the government was not permitted to participate in management of the bank. The bank accepted deposits, issued bank notes, and provided short-term loans to the government. It also functioned as a clearinghouse for government debt. The bank could also regulate state-charted banks to prevent overproduction of banknotes. The bank was very successful in financing the government and stimulating the economy. In spite of its successes, hostility against the bank did not fade. Jacksonian supporters questioned the bank’s constitutionality. In 1811, the first bank of the United States failed to be renewed by one vote.
The Second Bank of the United States.
After the war of 1812, economic instability necessitated the creation of a second national bank. Due to expanding money supply and lack of supervision, individual bank activity sparked high inflation. In 1816, a second national bank was created with a charter of twenty years. Three years later, during the panic of 1819 the second bank of the United States was blamed for overextending credit in a land boom, and would tighten up credit policies following the panic(Wiletnz, 2008).
The Second bank was unpopular among the western and southern state-chartered banks, and constitutionality of a national bank was questioned. President Jackson would come into office, and wished to end the current central bank during his presidency. Under the premise that the bank favored a small economic and political elite at the expense of the public majority, the Second Bank became private after its charter expired in 1836, and would undergo liquidation in 1841.
For nearly eighty years, the U.S. was without a central bank after the charter for the Second Bank of the United States was allowed to expire. After various financial panics, particularly a severe one in 1907, some Americans became persuaded that the country needed some sort of banking and currency reform that would, when threatened by financial panics, provide a ready reserve of liquid assets, and furthermore allow for currency and credit to expand and contract seasonally within the U.S. economy.
Some of this was chronicled in the reports of the National Monetary Commission (1909–1912), which was created by the Aldrich–Vreeland Act in 1908. Included in a report of the Commission, submitted to Congress on January 9, 1912, were recommendations and draft legislation with 59 sections, for proposed changes in U.S. banking and currency laws. The proposed legislation was known as the Aldrich Plan, named after the chairman of the Commission, Republican Senator Nelson W. Aldrich of Rhode Island.
The Plan called for the establishment of a National Reserve Association with 15 regional district branches and 46 geographically dispersed directors primarily from the banking profession. The Reserve Association would make emergency loans to member banks, print money, and act as the fiscal agent for the U.S. government. State and nationally chartered banks would have the option of subscribing to specified stock in their local association branch. It is generally believed that the outline of the Plan had been formulated in a secret meeting on Jekyll Island in November 1910, which Aldrich and other well connected financiers attended.
Since the Aldrich Plan gave too little power to the government, there was strong opposition to it from rural and western states because of fears that it would become a tool of bankers, specifically the Money Trust of NYC. Indeed, from May 1912 through January 1913 the Pujo Committee, a subcommittee of the House Committee on Banking and Currency, held investigative hearings on the alleged Money Trust and its interlocking directorates. These hearings were chaired by Rep. Arsene Pujo, a Democratic representative from Louisiana.
In the election of 1912, the Democratic Party won control of the White House and both chambers of Congress. The party's platform stated strong opposition to the Aldrich Plan. The platform also called for a systematic revision of banking laws in ways that would provide relief from financial panics, unemployment and business depression, and would protect the public from the "domination by what is known as the Money Trust." The final plan, however, was quite similar to the Aldrich Plan, with a few revisions. Sen. Carter Glass made these revisions, although the main premise of the Aldrich Plan was in there. Changes in the Banking and Currency System of the United States]. House Report No. 69, 63d Congress to accompany H.R. 7837, submitted to the full House by Carter Glass, from the House Committee on Banking and Currency, September 9, 1913. A discussion of the deficiencies of the then current banking system as well as those in the Aldrich Plan and quotations from the 1912 Democratic platform are laid out in this report, pages 3–11.
Legislative history of the Act.
The Federal Reserve Act was a part of the banking and currency reform plan advocated by President Wilson in 1913. The chairmen of the House and Senate Banking and Currency committees sponsored this legislation; representative Carter Glass, a Democrat of Virginia and Senator Robert Latham Owen, a Democrat of Oklahoma. According to the House committee report accompanying the Currency bill (H.R. 7837) or the Glass-Owen bill, as it was often called during the time.
Attempts to reform currency and banking had been made in the United States prior H.R. 7837. The major first form of this type of legislation came through with the First Bank of the United States in 1791. Championed by Alexander Hamilton, this established a central bank that included in a three-part expansion of federal fiscal and monetary power (including federal mint and excise taxes). Attempts were made to extend this bank’s charter, but they would fail before the charters expiration in 1811, which would lead to the creation of the Second Bank of the United States. In 1816 the U.S. Congress chartered this Second bank for a twenty-year period to create irredeemable currency with which to pay for the costs of the War of 1812. The creation of congressionally authorized irredeemable currency by the Second Bank of the United States amounted to taxation by inflation. Congress did not want state-chartered banks as competition in the inflation of currency. The charter for the Second Bank would expire in 1836, leaving the U.S. without a central bank for nearly eighty years. The last major form of legislation preceding the Federal Reserve Act came in 1908 with the Aldrich-Vreeland Act, which was the initial response the Financial Panic of 1907, and established the National Monetary Commission, which recommended the Federal Reserve act of 1913.
According to the House committee report accompanying the Currency bill (H.R. 7837) or the Glass-Owen bill, the legislation was drafted from ideas taken from various proposals, including the Aldrich bill. However, unlike the Aldrich plan, which gave controlling interest to private bankers with only a small public presence, the new plan gave an important role to a public entity, the Federal Reserve Board, while establishing a substantial measure of autonomy for the (regional) Reserve Banks which, at that time, were allowed to set their own discount rates. Also, instead of the proposed currency being an obligation of the private banks, the new Federal Reserve note was to be an obligation of the U.S. Treasury. In addition, unlike the Aldrich plan, membership by nationally chartered banks was mandatory, not optional. The changes were significant enough that the earlier opposition to the proposed reserve system from Progressive Democrats was largely appeased; instead, opposition to the bill came largely from the more business-friendly Republicans instead of from the Democrats.
After months of hearings, debates, votes and amendments, the proposed legislation, with 30 sections, was enacted as the Federal Reserve Act. The House, on December 22, 1913, agreed to the conference report on the Federal Reserve Act bill by a vote of 298 yeas to 60 nays, with 76 not voting. The Senate, on December 23, 1913, agreed to it by a vote of 43 yeas to 25 nays with 27 not voting. The record shows that there were no Democrats voting "nay" in the Senate and only two in the House. The record also shows that almost all of those not voting for the bill had previously declared their intentions and were paired with members of opposite intentions.
Subsequent amendments.
The Federal Reserve act has undergone many amendments after its implementation. Early, bureaucratic amendments were made to account for states like Hawaii and Alaska’s admission to the union; such as district restructuring and jurisdiction specifications.
Charter extension.
The Federal Reserve Act was originally granted a twenty-year charter, to be renewed in 1933. This clause was amended on February 25, 1927: "To have succession after the approval of this Act until dissolved by Act of Congress or until forfeiture of franchise for violation of law." 12 U.S.C. ch. 3. As amended by act of Feb. 25, 1927 (44 Stat. 1234). The success of this amendment is notable, as in 1933, the US was in the throes of the Great Depression and public sentiment with regards to the Federal Reserve System and the banking community in general had significantly deteriorated. Given the political climate, including of Franklin D. Roosevelt’s administration and New Deal legislation, it was uncertain whether the Federal Reserve System would survive.
The Federal Open Market Committee.
In 1933, by way of the Banking Act of 1933, the Federal Reserve Act was amended to create the Federal Open Market Committee (FOMC), which consists of the seven members of the Board of Governors of the Federal Reserve System and five representatives from the Federal Reserve Banks. The FOMC is required to meet at least four times a year (in practice, the FOMC usually meets eight times) and has the power to direct all open-market operations of the Federal Reserve banks.
12 USC § 225a.
On November 16, 1977, the Federal Reserve Act was amended to require the Board and the FOMC "to promote effectively the goals of maximum employment, stable prices, and moderate long-term interest rates." This same amendment stated that the member governor proposed by the President to be Chairman would have a four-year term as Chairman and would be subject to confirmation by the Senate (member governors per se each have 14 year terms, with a specific term ending every two years). The Chairman was also required to appear before Congress at semi-annual hearings to report on the conduct of monetary policy, on economic development, and on the prospects for the future.
The Federal Reserve Act has been amended by some 200 subsequent laws of Congress. It continues to be one of the principal banking laws of the United States.
Implications and impacts of the Federal Reserve Act.
The passing of the Federal Reserve act of 1913 carried implications both domestically and internationally for the United States economic system. The absence of a central banking structure in the U.S. previous to this act left a financial essence that was characterized by immobile reserves and inelastic currency (Johnson, 14). Creating the Federal Reserve gave the federal government control to regulate inflation, even though the government control over such powers would eventually lead to decisions that were politically popular and economically devastating. Some of the most prominent implications include the internationalization of the U.S. Dollar as a global currency, the impact from the perception of the Central Bank structure as a public good by creating a system of financial stability (Parthemos 19-28), and the Impact of the Federal Reserve in response to economic panics. The Federal Reserve Act also permitted national banks to make mortgage loans for farm land, which had not been permitted previously.
Criticisms of the Act.
Throughout the history of the United States, there has been an enduring economic and political debate regarding the costs and benefits of central banking. Since the inception of a central bank in the United States, there were multiple opposing views to this type of economic system. Opposition was based on protectionist sentiment; a central bank would serve a handful of financiers at the expense of small producers, businesses, farmers and consumers, and could destabilize the economy through speculation and inflation. This created even further controversy over who would select the decision-makers in charge of the Federal Reserve. Proponents argued that a strong banking system could provide enough credit for a growing economy and avoid economic depressions. Other critical views included the belief that the bill gave too much power to the federal government after the senate revised the bill to create 12 board members who were each appointed by the president.
Preceding the creation of the Federal Reserve, no U.S. central banking systems lasted for more than 25 years. Some of the questions raised include: whether Congress has the Constitutional power to delegate its power to coin money or issue paper money, whether the structure of the federal reserve is transparent enough, whether the Federal Reserve is a public Cartel of private banks (also called a banking cartel) established to protect powerful financial interests, low but steady increases in inflation, high government deficits, and whether the Federal Reserve's actions increased the severity of the Great Depression in the 1930s. However, the debate over the right to coin money was minimal due to the fact the US government had already done this in the past.
 (and/or the severity or frequency of other boom-bust economic cycles, such as the late 2000s recession).

</doc>
<doc id="55744" url="http://en.wikipedia.org/wiki?curid=55744" title="Mann Act">
Mann Act

 
The White-Slave Traffic Act, better known as the Mann Act, is a United States federal law, passed June 25, 1910 (ch. 395, 36 Stat. ; "codified as amended at" #redirect ).
It is named after Congressman James Robert Mann of Illinois, and in its original form made it a felony to engage in interstate or foreign commerce transport of "any woman or girl for the purpose of prostitution or debauchery, or for any other immoral purpose". Its primary stated intent was to address prostitution, "immorality", and human trafficking particularly where it was trafficking for the purposes of prostitution. This is one of several acts of protective legislation aimed at moral reform during the progressive era. Its ambiguous language of "immorality" meant it could be used to criminalize consensual sexual behavior between adults. It was amended by Congress in 1978 and again in 1986 to apply to transport for the purpose of prostitution or illegal sexual acts.
Promotion.
In the 19th century, most of America’s cities had a designated, legally protected area of prostitution. Increased urbanization and young women entering the workforce led to greater flexibility in courtship without supervision. It is in this changing social sphere that the panic over "white slavery" began. This term referred to women being kidnapped for the purposes of prostitution.
Numerous communities appointed vice commissions to investigate the extent of local prostitution, whether prostitutes participated in it willingly or were forced into it and the degree to which it was organized by any cartel-type organizations. The second significant action at the local levels was to close the brothels and the red light districts. From 1910 to 1913, city after city withdrew this tolerance and forced the closing of their brothels. Opposition to openly practiced prostitution had been growing steadily throughout the last decades of the nineteenth century. The federal government's response to the moral panic was the Mann Act. The purpose of the act was to make it a crime to coerce transportation of unwilling women. The statute made it a crime to “transport or cause to be transported, or aid to assist in obtaining transportation for” or to “persuade, induce, entice or coerce” a woman to travel.” Many of the changes that occurred after 1900 were a result of tensions between family ideals and practical realities. Family form and functions changed in response to a complex set of circumstances which were the effects of economic class and ethnicity.
According to historian Mark Thomas Connelly, "a group of books and pamphlets appeared announcing a startling claim: a pervasive and depraved conspiracy was at large in the land, brutally trapping and seducing American girls into lives of enforced prostitution, or 'white slavery.' These white slave narratives, or white-slave tracts, began to circulate around 1909." Such narratives often portrayed innocent girls "victimized by a huge, secret, and powerful conspiracy controlled by foreigners", as they were drugged or imprisoned and forced into prostitution.
This excerpt from "The War on the White Slave Trade" was written by the United States District Attorney in Chicago:
One thing should be made very clear to the girl who comes up to the city, and that is that the ordinary ice cream parlor is very likely to be a spider’s web for her entanglement. This is perhaps especially true of those ice cream saloons and fruit stores kept by foreigners. Scores of cases are on record where young girls have taken their first step towards “white slavery” in places of this character.
According to Connelly, such concerns represented a "hysterical" version of genuine and long-standing issues arising from the concentration of young women from rural backgrounds in the expanding cities of the era, many of whom were drawn into prostitution for "mundane" economic reasons. A number of Vice Commission reports had drawn attention to the issue. Some contemporaries called into question the idea of abduction and foreign control of prostitution through cartels. For example, noted radical and feminist Emma Goldman asked "What is really the cause of the trade in women? Not merely white women, but yellow and black women as well. Exploitation, of course; the merciless Moloch of capitalism that fattens on underpaid labor, thus driving thousands of women and girls into prostitution. With Mrs. Warren these girls feel, "Why waste your life working for a few shillings a week in a scullery, eighteen hours a day?... Whether our reformers admit it or not, the economic and social inferiority of woman is responsible for prostitution." While prostitution was widespread, contemporary studies by local vice commissions indicate that it was "overwhelmingly locally organized without any large business structure, and willingly engaged in by the prostitutes."
Suffrage activists, especially Harriet Burton Laidlaw and Rose Livingston, took up these concerns. They worked in New York City's Chinatown and in other cities to rescue young white and Chinese girls from forced prostitution, and helped pass the Mann Act to make interstate sex trafficking a federal crime. Livingston publicly discussed her past as a prostitute and claimed to have been abducted and developed a drug problem as a sex slave in a Chinese man's home, narrowly escaped and experienced a Christian conversion narrative. Her story in several ways exemplifies the stereotypes used to pass the Mann Act- fear of foreigners, especially Jewish, Italian, or Asian men, abduction and drugging in order to be raped and enslaved, a narrow escape, and salvation through Christian conversion. Other groups like the Woman’s Christian Temperance Union and Hull House focused on children of prostitutes and poverty in community life while trying to pass protective legislation. The American Purity Alliance also supported the Mann Act.
The 1921 Convention set new goals for international efforts to stem human trafficking, primarily by giving the anti-trafficking movement further official recognition, as well as a bureaucratic apparatus to research and fight the problem. The Advisory Committee on the Traffic of Women and Children was a permanent advisory committee of the League. Its members were nine countries, and several non-governmental organizations. An important development was the implementation of a system of annual reports of member countries. Member countries formed their own centralized offices to track and report on trafficking of women and children. The advisory committee also worked to expand its research and intervention program beyond the United States and Europe. In 1929, a need to expand into the Near East (Asia Minor), the Middle East, and Asia was acknowledged. An international conference of central authorities in Asia was planned for 1937, but no further action was taken during the late 1930s.
Legal application.
Although the law was created to stop forced sexual slavery of women, the most common use of the Mann Act was to prosecute men for having sex with under-age females. The phrase "immoral purpose" in the statute allowed an extremely broad application of the law following the United States Supreme Court ruling in "Caminetti v. United States" (1917), which held that "illicit fornication" even when consensual constituted an "immoral purpose." The law was also frequently used to prosecute interracial and unapproved pre-marital and extra-marital relationships in addition to its stated purpose of preventing human trafficking. The penalties would be applied to men whether or not the woman involved consented and if she did the woman could be considered an accessory to the offense. There was also a strong racial bias against black men with white women such as in the case of Jack Johnson. It was also used to harass others who had drawn the authorities' wrath for "immoral" or controversial behavior.
For instance, the 1948 prosecution of Frank LaSalle for abducting Florence Sally Horner is believed to have been an inspiration for Vladimir Nabokov in writing his novel "Lolita". The Mann Act has also been used by the U.S. federal government to prosecute polygamists such as Mormon fundamentalists because there is no federal U.S. law against polygamy. All U.S. states have anti-polygamy laws, but it has only been in recent years that state authorities have used them to prosecute bigamy. Colorado City, Arizona, and Hildale, Utah, Bountiful, British Columbia, and sites in Mexico are historic locations of several Mormon Fundamentalist sects. Mormon fundamentalist leaders and individuals have been charged under the Mann Act when "wives" are transported across the Utah–Arizona state line or the U.S.–Canadian and U.S.–Mexican borders.
Congressional amendments to the law.
In 1978, Congress updated the act's definition of "transportation" and added protections against commercial sexual exploitation for minors. It added a 1986 amendment which further protected minors and added protection for adult males. In particular, as part of a larger 1986 bill focused on criminalizing various aspects of child pornography that passed unanimously in both houses of Congress, the Mann act was further amended to replace the ambiguous "debauchery" and "any other immoral purpose" with the more specific "any sexual activity for which any person can be charged with a criminal offense" as well as to make it gender-neutral.
Effects and alterations of the Mann Act.
While the effects of the Mann Act were meant to try and combat forced prostitution, it had repercussions that extended into consensual sexual activity. Because the Mann Act lacked specificity, it criminalized many who were not participating in prostitution. It became a way to persecute many unmarried couples participating in premarital or extramarital activities, especially when it involved crossing state lines such as the cases for Chuck Berry and Jack Johnson. The Mann Act also became a form of blackmail, by wives suspicions of cheating husbands and other women alike. This was the case for both Drew Caminetti and Maury Diggs. Both men from Sacramento, California, were married and took their mistresses Lola Norris and Marsha Warrington to Reno, Nevada. The men’s wives contacted the police, and they were then arrested in Reno, and found guilty under the Mann Act. “In 1914 a woman by the name of Jessie A. Cope was arrested in Chicago for attempting to bribe an official to assist her in the blackmail of Colonel Charles Alexander of Providence Rhode Island, on a white slavery charge. The two had met two years previous in LA, Alexander had promised to divorce his wife, and marry her. When he attempted to leave her, Cope and her mother pursued him to Providence. Cope consulted lawyers in Providence and LA, then brought the charges in Chicago, where she was arrested.”
Upon continuous blackmail accounts the "New York Times" became an advocate against the Mann Act. “In 1915 the paper published an editorial pointing out how the act led to extortion. In 1916 it labeled the Mann Act “The Blackmail Act,” noting that its dangers had been clear from the start. The act made a harmless spree or simple elopement a crime, and the blackmail that resulted from the Mann Act was worse than the prostitution it sought to suppress.”
While the Mann Act has never been repealed, it has been amended and altered since its initial passing. The Mann Act continued essentially unchanged until 1978 and expanded coverage to issues around child pornography and exploitation. Most recently, in 1986, The Mann Act was significantly altered, making the whole Act gender neutral, making the transportation of "any person" and changed the wording to "any sexual activity for which any person can be charged with a criminal offense" illegal. Since sodomy was illegal until "Lawrence v. Texas" (2003), the law would also apply to consenting adult gay couples, although rarely enforced in this way. Since 1978 most convictions have been related to child abuse and child trafficking cases.

</doc>
<doc id="55745" url="http://en.wikipedia.org/wiki?curid=55745" title="Mann–Elkins Act">
Mann–Elkins Act

The Mann–Elkins Act was a 1910 United States federal law that was among the Progressive era reforms. The Act extended the authority of the Interstate Commerce Commission (ICC) to regulate the telecommunications industry, and designated telephone, telegraph and wireless companies as common carriers.:216 During William H. Taft's administration, the federal government moved to strengthen its regulatory control over the railroad industry by the passage of the Mann–Elkins Act.
Supported by President William Howard Taft, the law also expanded on the powers granted to the ICC in the 1906 Hepburn Act. The ICC was authorized to investigate proposed railroad rate increases and suspend them if warranted. The "long-and-short haul" clause of the original Interstate Commerce Act (1887) was strengthened to prohibit railroads from charging passengers more for a short distance trip, compared to a longer distance ride, over the same route, unless specifically approved by the ICC.:217–219
The Act also created the short-lived United States Commerce Court for adjudication of railway disputes.:216 The Court presided until 1913, when it was abolished by Congress.
Legislative history.
It was passed by the United States Senate, 50-12.

</doc>
<doc id="55751" url="http://en.wikipedia.org/wiki?curid=55751" title="Agricultural Marketing Act of 1929">
Agricultural Marketing Act of 1929

Under the administration of Herbert Hoover, the Agricultural Marketing Act of 1929 established the Federal Farm Board from the Federal Farm Loan Board established by the Federal Farm Loan Act of 1916 with a revolving fund of half a billion dollars. The original act was sponsored by Hoover in an attempt to stop the downward spiral of crop prices by seeking to buy, sell and store agricultural surpluses or by generously lending money to farm organizations. Money was loaned out to the farmers in order to buy seed and food for the livestock (this was especially important since there had been a drought in the Democratic South previously), but Hoover refused to lend to the farmers themselves, thinking that it was unconstitutional and if they were lent money, they would become dependent on government money. The Federal Farm Board's purchase of surplus could not keep up with the production-as farmers realized that they could just sell the government their crops, they re-implemented the use of fertilizers and other techniques to increase production. Overall, the deflation could not be countered because of a massive fault in the bill-there was no production limit. Had there been a production limit, the deflation might have been helped somewhat. The funds appropriated were exhausted eventually and the losses of the farmers kept rising.
The H.R. 1 legislation was passed by the 71st Congressional session and enacted by the 31st President of the United States Herbert Hoover on June 15, 1929.
The Act was the precursor to the Agricultural Adjustment Act.

</doc>
<doc id="55752" url="http://en.wikipedia.org/wiki?curid=55752" title="Indian Reorganization Act">
Indian Reorganization Act

The Indian Reorganization Act of June 18, 1934, or the Wheeler-Howard Act, was U.S. federal legislation that dealt with the status of Native Americans (known in law as American Indians or Indians). It was the centerpiece of what has been often called the "Indian New Deal." The major goal was to reverse the traditional goal of assimilation of Indians into American society, and to strengthen, encourage and perpetuate the tribes and their historic traditions and culture. The Act also restored to Indians the management of their assets--land and mineral rights-–and included provisions intended to create a sound economic foundation for the inhabitants of Indian reservations. The law did not apply to Hawaii; Alaska and Oklahoma were added in 1936. The census counted 332,000 Indians in 1930 and 334,000 in 1940, including those on and off reservations in the 48 states. Total spending on Indians averaged $38 million a year in the late 1920s, dropping to a low of $23 million in 1933, and returning to $38 million in 1940. 
The IRA was the most significant initiative of John Collier, Commissioner of the Bureau of Indian Affairs (BIA) from 1933 to 1945. He had crusaded on Indian issues in the 1920s particularly with the American Indian Defense Association. He intended to reverse assimilationist policies and provide ways for American Indians to re-establish sovereignty and self-government, to reduce the losses of reservation lands, and establish ways for Indians to build economic self-sufficiency. He saw Indian traditional culture as superior to that of modern America, and thought it worthy of emulation. The proposals were highly controversial at the time, and ever since. Congress revised Collier's proposals and preserved oversight by the BIA .
The self-government provisions would automatically go into effect for a tribe unless a clear majority of the eligible Indians voted it down. When approved, a tribe would adopt a variation of the model constitution drafted by BIA lawyers. Of the tribes that voted on the IRA, 174 voted yes and 78 rejected it.
History of IRA.
Background.
At the time the Act passed, it was United States policy to eliminate Indian reservations, dividing their territory and distributing it to individual Indians to own like any other person, in a process called "allotment." Before allotment, reservation territory was not owned in the usual western sense, but was reserved for the benefit of entire Indian tribes, with its benefits apportioned to tribe members according to tribal law and custom. Generally, Indians held the land in a communal fashion. It was not possible for any non-Indian to own land on reservations, a fact which limited the value of the land to the Indians (It reduced the market for it).
The process of allotment started with the General Allotment Act of 1887, and by 1934, two thirds of Indian land had converted to traditional private ownership (i.e. it was owned in fee simple) and most of that had been sold by its Indian allottee. The IRA provided a mechanism for the recovery of land that had been sold—including land that had been sold to tribal Indians, who would thereby lose their property.
John Collier was appointed Commissioner of the Indian Bureau (it is now called the Bureau of Indian Affairs, BIA) in April 1933 by President Franklin Delano Roosevelt. He had the unwavering support of his boss Secretary Secretary of the Interior, Harold L. Ickes, who was himself an expert on Indian issues.
The federal government held land in trust for many tribes. Numerous claims cases had been presented to Congress because of failures in the government's management of such lands. There were particular grievances and claims due to the government's failure to provide for sustainable forestry. The Indian Claims Act included a requirement that the Interior Department manage Indian forest resources "on the principle of sustained-yield management." Representative Edgar Howard of Nebraska, co-sponsor of the Act and Chairman of the House Committee on Indian Affairs, explained that the purpose of the provision was "to assure a proper and permanent management of the Indian Forest" under modern sustained-yield methods so as to "assure that the Indian forests will be permanently productive and will yield continuous revenues to the tribes."
Implementation and results.
The act slowed the practice of allotting communal tribal lands to individual tribal members. It did not restore to Indians land that had already been patented to individuals, but much land at the time was still unallotted or was allotted to an individual but still held in trust for that individual by the U.S. government. Because the Act did not disturb existing private ownership of Indian reservation lands, it left reservations a checkerboard of tribal or individual trust and fee land, which remains the case today. .
However, the Act also allowed the U.S. to purchase some of the fee land and restore it to tribal trust status. Due to the Act and other actions of federal courts and the government, over two million acres (8,000 km²) of land were returned to various tribes in the first 20 years after passage.
In 1954, the United States Department of the Interior (DOI) began implementing the termination and relocation phases of the Act, which had been added by Congress and represented the continuing interest by some of having American Indians assimilate to the majority society. Among other effects, termination resulted in the legal dismantling of 61 tribal nations within the United States and ending their recognized relationships with the federal government. This also ended the eligibility of the tribal nations and their members for various government programs to assist American Indians.
Constitutional challenges.
Since the late 20th century and the rise of Indian activism over sovereignty issues, as well as many tribes' establishment of casino gambling on reservations as a revenue source, the US Supreme Court has been repeatedly asked to address the IRA's constitutionality. The provision of the Act that is controversial is the one that allows the US government to acquire non-Indian land (by voluntary transfer) and convert it to Indian land ("take it into trust"). In so doing, the US government partially removes the land from the jurisdiction of the state, which makes certain activities, such as casino gambling, possible on the land that wouldn't otherwise be. It also makes the land exempt from state property taxes and some other state taxes. Consequently, many people oppose implementation of this part of the Act and, typically represented by state or local governments, they sue to prevent it.
In 1995, South Dakota challenged the authority of the Interior Secretary, under the IRA, to take 91 acre of land into trust on behalf of the Lower Brule Sioux Tribe (based on the Lower Brule Indian Reservation), in "South Dakota v. United States Dep't of the Interior", 69 F.3d 878, 881-85 (8th Cir. 1995). The Eighth Circuit found Section 5 of the IRA to be unconstitutional, ruling that it violated the non-delegation doctrine and that the Secretary of Interior did not have the authority to take the land into trust.
The US Department of the Interior (DOI) sought U.S. Supreme Court review. But, as DOI was implementing new regulations related to land trusts, the agency asked the Court to remand the case to the lower court, to be reconsidered with the decision to be based on the new regulations. The US Supreme Court granted Interior's petition, vacated the lower court's ruling, and remanded the case back to the lower court.
Justices Scalia, O'Connor and Thomas dissented, stating that "[t]he decision today--to grant, vacate, and remand in light of the Government's changed position--is both unprecedented and inexplicable." They went on, "[W]hat makes today's action inexplicable as well as unprecedented is the fact that the Government's change of legal position does not even purport to be applicable to the present case." Seven months after the Supreme Court's decision to grant, vacate, and remand, the DOI removed the land in question from trust.
In 1997, the Lower Brulé Sioux submitted an amended trust application to DOI, requesting that the United States take the 91 acre of land into trust on the Tribe's behalf. South Dakota challenged this in 2004 in district court, which upheld DOI's authority to take the land in trust. The state appealed to the Eighth Circuit, but when the court reexamined the constitutionality issue, it upheld the constitutionality of Section 5 in agreement with the lower court. The US Supreme Court denied the State's petition for "certiorari". Since then, district and circuit courts have rejected non-delegation claims by states. The Supreme Court refused to hear the issue in 2008.
In 2008 (before the US Supreme Court heard the "Carcieri" case below), in "MichGO v Kempthorne", Judge Janice Rogers Brown of the D.C. Circuit Court of Appeals wrote a dissent stating that she would have struck down key provisions of the IRA. Of the three circuit courts to address the IRA's constitutionality, Judge Brown is the only judge to dissent on the IRA's constitutionality. The majority opinion upheld its constitutionality. The U.S. Supreme Court did not accept the "MichGO" case for review, thus keeping the previous precedent in place. The First, Eighth and Tenth Circuits of the U.S. Court of Appeals have upheld the constitutionality of the IRA.
In 2008, "Carcieri v Kempthorne" was argued before the U.S. Supreme Court; the Court ruled on it in 2009, with the decision called "Carcieri v. Salazar". In 1991, the Narragansett Indian tribe bought 31 acre of land. They requested that the DOI take it into trust, which the agency did in 1998, thus exempting it from many state laws. The State was concerned that the tribe would open a casino or tax-free business on the land and sued to block the transfer. The state argued that the IRA did not apply because the Narragansett was not "now under federal jurisdiction" as of 1934, as distinguished from "federally recognized." In fact, the Narragansett had been placed under Rhodes Island guardianship since 1709. In 1880, the tribe relinquished its tribal authority to Rhodes Island. The tribe did not receive federal recognition until 1980, after the 1934 passage of the IRA. The US Supreme Court agreed with the State.
In a challenge to the U.S. DOI's decision to take land into trust for the Oneida Indian Nation in present-day New York, Upstate Citizens for Equality (UCE), New York, Oneida County, Madison County, the town of Verona, the town of Vernon, and others argue that the IRA is unconstitutional. Most recently, Judge Kahn dismissed UCE's complaint, including the failed theory that the IRA is unconstitutional, on the basis of longstanding and settled law on this issue.
Approval by tribes.
Section 18 of the IRA required that members of the affected Indian nation or tribe vote on whether to accept it within one year of the effective date of the act (25 U.S.C. 478), and had to approve it by a majority. There was confusion about who should be allowed to vote on creating new governments, as many non-Indians lived on reservations many Indians owned no land there, and also over the effect of abstentions.
Under the voting rules, abstentions were counted as yes votes, but in Oglala Lakota culture, for example, abstention had traditionally equaled a no vote. The resulting confusion caused disputes on many reservations about the results.
When the final results were in, 172 tribes had accepted the act, and 73 had rejected it. The largest tribe, the Navajo, had been badly hurt by the federal Navajo Livestock Reduction Program, which took away half their livestock and jailed dissenters. They strongly opposed the act, John Collier the chief promoter, and the entire Indian New Deal. Historian Brian Dippie notes that the Indian Rights Association denounced Collier as a 'dictator' and accused him of a "near reign of terror" on the Navajo reservation. Dippie adds that, "He became an object of 'burning hatred' among the very people whose problems so preoccupied him."
Legacy.
Historians have mixed reactions to the Indian New Deal. Many praise Collier's energy and his initiative. Philp praised Collier's Indian New Deal for protecting Indian freedom to engage in traditional religious practices, obtaining additional relief money for reservations, providing a structure for self-government, and enlisting the help of anthropologists who respected traditional cultures. However, he concludes that the Indian New Deal was unable to stimulate economic progress nor did it provide a usable structure for Indian politics. Philp argues these failures gave ammunition to the return to the previous policy of termination that took place after Collier resigned in 1945. In surveying the scholarly literature, Schwartz concludes that there is:
Collier's reputation among the Indians was mixed—praised by some, vilified by others. He antagonized the Navajo, the largest tribe as well as the Seneca people, Iroquois, and many others. Anthropologists criticized him for not recognizing the diversity of Native American lifestyles. Hauptman argues that his emphasis on Northern Pueblo arts and crafts and the uniformity of his approach to all tribes are partly explained by his belief that his tenure as Commissioner would be short, meaning that packaging large, lengthy legislative reforms seemed politically necessary.
The Reorganization Act was wide-ranging legislation authorizing tribal self-rule under federal supervision, putting an end to land allotment and generally promoting measures to enhance tribes and encouraging education.
Having described the American society as "physically, religiously, socially, and aesthetically shattered, dismembered, directionless", Collier was later criticized for his romantic views about the moral superiority of traditional society as opposed to modernity. Philp says after his experience at the Taos Pueblo, Collier "made a lifelong commitment to preserve tribal community life because it offered a cultural alternative to modernity...His romantic stereotyping of Indians often did not fit the reality of contemporary tribal life."
The act has helped conserve the communal tribal land bases. Collier supporters blame Congress for altering the legislation proposed by Collier, so that it has not been as successful as possible. On many reservations, its provisions exacerbated longstanding differences between traditionals and those who had adopted more European-American ways. Many Native Americans believe their traditional systems of government were better for their culture.

</doc>
<doc id="55754" url="http://en.wikipedia.org/wiki?curid=55754" title="List of United States immigration legislation">
List of United States immigration legislation

There has been a number of Immigration Acts in the United States, but the first restriction on immigration did not occur until 1875. Prior to that point, immigration was distinct from citizenship and naturalization.

</doc>
<doc id="55755" url="http://en.wikipedia.org/wiki?curid=55755" title="Fordney–McCumber Tariff">
Fordney–McCumber Tariff

The Fordney–McCumber Tariff of 1922 was a law that raised American tariffs on many imported goods in order to protect factories and farms. Congress displayed a pro-business attitude in passing the tariff and in promoting foreign trade through providing huge loans to Europe, which in turn bought more American goods. The Roaring Twenties brought a period of sustained economic prosperity with an end to the Depression of 1920–21; the prosperity ended in late 1929, and the tariff was revised in 1930.
Background.
The first sector of the economy that was hit by a fall in post-war demand was agriculture. During World War I, the American agricultural industry enjoyed prosperity, through the raising of prices which led to increased output which Americans used to supply Europe. Farmers borrowed heavily to expand their acreage; they had great difficulty paying back the loans when prices fell. Some of the post war problems for the American agriculture come from the great surplus of farm goods that could not be absorbed in the national market, because European countries had recovered sufficiently from the war, and their markets no longer required large quantities of American agricultural products. Gross farm income in 1919 amounted to $17.7 billion. By 1921, exports to Europe had plummeted and farm income fell to $10.5 billion. Other sectors of the economy wanted to avoid a similar fate. The 1920 election put the conservative pro-business and pro-farm Republicans in control of Congress and the White House.
The hearings held by Congress led to the creation of several new tools of protection. The first was the "scientific tariff". The purpose of the scientific tariff was to equalize production costs among countries so that no country could undercut the prices charged by American companies. The difference of production costs was calculated by the Tariff Commission.
A second novelty was the "American Selling Price". This allowed the president to calculate the duty based on the price of the American price of a good, not the imported good.
The bill also gave the president the power to raise or lower rates on products if it was recommended by the Tariff Commission.
In September 1922, the Fordney–McCumber Tariff bill (named after Joseph Fordney, chair of the House Ways and Means Committee, and Porter McCumber, chair of the Senate Finance Committee) was signed by President Warren Harding. In the end, the tariff law raised the American ad valorem tariff rate to an average of about 38.5 percent for dutiable imports and an average of 14% overall. The measure was defensive tariff rather than an offensive. An ad valorem tariff was determined by the cost of production and market value.
Economic effects.
The Roaring Twenties brought a sustained period of economic prosperity principally to North America, but also to London, Berlin and Paris, with the end of the Depression of 1920-21 in the U.S. and a robust American economy. For agriculture, the tariff raised the purchasing power of the farmers by two to three percent, with other industries raising the price of some farm equipment. In September 1926, economic statistics were released by farming groups that revealed the rising cost of farm machinery. For example, the average cost of a harness rose from $46 in 1918 to $75 in 1926, the 14-inch plow doubled in cost from $14 to $28, mowing machines went from $45 to $95, and farm wagons from $85 to $150.
Reaction.
The tariff was supported by the Republican party and conservatives and was generally opposed by the Democratic Party and liberal progressives. One intent of the tariff was to help those returning from World War I have greater job opportunities. Trading partners complained immediately. European nations affected by World War I sought access for their exports to the American market to make payments to the U.S. for war loans. Democratic Representative Cordell Hull said, "Our foreign markets depend both on the efficiency of our production and the tariffs of countries in which we would sell. Our own [high] tariffs are an important factor in each. They injure the former and invite the latter."
Five years after the passage of the tariff, American trading partners had raised their own tariffs by a significant degree. France raised its tariffs on automobiles from 45% to 100%, Spain raised tariffs on American goods by 40%, and Germany and Italy raised tariffs on wheat.
In 1928, Henry Ford attacked the Fordney–McCumber Tariff, arguing that the American automobile industry did not need protection since it dominated the domestic market, and their interest was in expanding foreign sales.
Some farmers opposed the Fordney- McCumber Tariff, blaming it for the agricultural depression. The American Farm Bureau Federation claimed that because of the tariff, the raised price of raw wool cost to farmers $27 million. Democratic Senator David Walsh challenged the tariff by arguing that the farmer is the net exporter and does not need protection because they depend on foreigner markets to sell their surplus. The Senator pointed out that during the first year of the tariff the cost of living climbed higher than any other year except during the war, presenting a survey of the Department of Labor, in which all of 32 cities assessed had seen an increase in the cost of living. For example, the food costs increased 16.5% in Chicago and 9.4% in New York. Clothing prices raised by 5.5% in Buffalo, New York, and 10.2% in Chicago. Republican Frank W. Murphy, head of the Minnesota Farm Bureau, also claimed that the problem was not in the world price of farm products, but in the things farmers had to buy. Republican Congressman W. R. Green, chairman of the House Ways and Means Committee, acknowledged that the statistics of the Bureau of Research of the American Farm Bureau that showed farmers had lost more than $300 million annually as a result of the tariff.

</doc>
<doc id="55757" url="http://en.wikipedia.org/wiki?curid=55757" title="Shepard tone">
Shepard tone

A Shepard tone, named after Roger Shepard, is a sound consisting of a superposition of sine waves separated by octaves. When played with the base pitch of the tone moving upward or downward, it is referred to as the "Shepard scale". This creates the auditory illusion of a tone that continually ascends or descends in pitch, yet which ultimately seems to get no higher or lower. It has been described as a "sonic barber's pole".
Construction.
Each square in the figure indicates a tone, any set of squares in vertical alignment together making one Shepard tone. The color of each square indicates the loudness of the note, with purple being the quietest and green the loudest. Overlapping notes that play at the same time are exactly one octave apart, and each scale fades in and fades out so that hearing the beginning or end of any given scale is impossible. As a conceptual example of an ascending Shepard scale, the first tone could be an almost inaudible C(4) (middle C) and a loud C(5) (an octave higher). The next would be a slightly louder C#(4) and a slightly quieter C#(5); the next would be a still louder D(4) and a still quieter D(5). The two frequencies would be equally loud at the middle of the octave (F#), and the eleventh tone would be a loud B(4) and an almost inaudible B(5) with the addition of an almost inaudible B(3). The twelfth tone would then be the same as the first, and the cycle could continue indefinitely. (In other words, each tone consists of two sine waves with frequencies separated by octaves; the intensity of each is a gaussian function of its separation in semitones from a peak frequency, which in the above example would be B(4).)
The acoustical illusion can be constructed by creating a series of overlapping ascending or descending scales. Similar to the Penrose stairs optical illusion (as in M. C. Escher's lithograph "Ascending and Descending") or a barber's pole, the basic concept is shown in figure 1.
The scale as described, with discrete steps between each tone, is known as the discrete Shepard scale. The illusion is more convincing if there is a short time between successive notes (staccato or marcato instead of legato or portamento).
Jean-Claude Risset subsequently created a version of the scale where the tones glide continuously, and it is appropriately called the continuous Risset scale or Shepard–Risset glissando. When done correctly, the tone appears to rise (or descend) continuously in pitch, yet return to its starting note. Risset has also created a similar effect with rhythm in which tempo seems to increase or decrease endlessly.
The tritone paradox.
A sequentially played pair of Shepard tones separated by an interval of a tritone (half an octave) produces the tritone paradox. In this auditory illusion, first reported by Diana Deutsch in 1986, the scales may be heard as either descending or ascending. Shepard had predicted that the two tones would constitute a bistable figure, the auditory equivalent of the Necker cube, that could be heard ascending or descending, but never both at the same time. Deutsch later found that perception of which tone was higher depended on the absolute frequencies involved, and that different listeners may perceive the same pattern as being either ascending or descending.
Examples.
Used as a compositional technique for digital orchestral music on a piece entitled The Journey by composer Renaldo Ramai. https://soundcloud.com/renaldo-ramai/the-journey?in=renaldo-ramai/sets/the-odyssey-ost

</doc>
<doc id="55758" url="http://en.wikipedia.org/wiki?curid=55758" title="Emergency Quota Act">
Emergency Quota Act

The Emergency Quota Act, also known as the Emergency Immigration Act of 1921, the Immigration Restriction Act of 1921, the Per Centum Law, and the Johnson Quota Act (ch. 8, 42 Stat.  of May 19, 1921) restricted immigration into the United States. Although intended as temporary legislation, the Act "proved in the long run the most important turning-point in American immigration policy" because it added two new features to American immigration law: numerical limits on immigration from Europe and the use of a system for establishing those limits. These limits came to be known as the National Origins Formula.
The Emergency Quota Act restricted the number of immigrants admitted from any country annually to 3% of the number of residents from that same country living in the United States as of the U.S. Census of 1890. This meant that people from northern European countries had a higher quota and were more likely to be admitted to the U.S. than people from eastern Europe, southern Europe, or other, non-European countries. Professionals were to be admitted without regard to their country of origin. The Act set no limits on immigration from Latin America. The act did not apply to countries with bilateral agreements with the US, or to Asian countries listed in the Immigration Act of 1917, known as the Asiatic Barred Zone Act.
Based on that formula, the number of new immigrants admitted fell from 805,228 in 1920 to 309,556 in 1921-22. The average annual inflow of immigrants prior to 1921 was 175,983 from Northern and Western Europe, and 685,531 from other countries, principally Southern and Eastern Europe. In 1921, there was a drastic reduction in immigration levels from other countries, principally Southern and Eastern Europe.
Following the end of World War I, both Europe and the United States were suffering economic and social upheaval. In Europe, the destruction of the war, the Russian Revolution, and the dissolution of both the Austro-Hungarian Empire and Ottoman Empire led to greater immigration to the United States, while in the United States an economic downturn following post-war demobilization increased unemployment. The combination of increased immigration from Europe at the time of higher American unemployment strengthened the anti-immigrant movement.
The act, sponsored by Rep. Albert Johnson (R-Washington), was passed without a recorded vote in the U.S. House of Representatives and by a vote of 90-2-4 in the U.S. Senate. 
The Act was soon revised by the Immigration Act of 1924.
The use of such a National Origins Formula continued until 1965 when the Immigration and Nationality Act of 1965 established America's current immigration quota system.

</doc>
<doc id="55759" url="http://en.wikipedia.org/wiki?curid=55759" title="Esch–Cummins Act">
Esch–Cummins Act

The Transportation Act, 1920, commonly known as the Esch–Cummins Act, was a United States federal law that returned railroads to private operation after World War I, with much regulation. It also officially encouraged private consolidation of railroads and mandated that the Interstate Commerce Commission (ICC) ensure their profitability.
Background.
The United States had entered World War I in April 1917, and the government found that the nation's railroads were not prepared to serve the war effort. On December 26, 1917, President Woodrow Wilson had ordered that U.S. railroads be nationalized in the public interest. This order was implemented through the creation of the United States Railroad Administration. Congress ratified the order in the "Railway Administration Act of 1918."
Major provisions.
The Esch–Cummins Act:
Subsequent legislation.
Title III of the Esch–Cummins Act, which pertained to labor disputes, was repealed in 1926 by the Railway Labor Act.
Further reading.
</dl>

</doc>
<doc id="55761" url="http://en.wikipedia.org/wiki?curid=55761" title="Volstead Act">
Volstead Act

The National Prohibition Act, known informally as the Volstead Act, was enacted to carry out the intent of the Eighteenth Amendment, which established prohibition in the United States. The Anti-Saloon League's Wayne Wheeler conceived and drafted the bill, which was named for Andrew Volstead, Chairman of the House Judiciary Committee, who managed the legislation.
Procedure.
While the Eighteenth Amendment to the United States Constitution prohibited the production, sale, and transport of "intoxicating liquors", it did not define "intoxicating liquors" or provide penalties. It granted both the federal government and the states the power to enforce the ban by "appropriate legislation." A bill to do so was introduced in Congress in 1919. Later this act was repealed by the Twenty-first amendment.
The bill was vetoed by President Woodrow Wilson, largely on technical grounds because it also covered wartime prohibition, but his veto was overridden by the House on the same day, October 28, 1919, and by the Senate one day later. The three distinct purposes of the Act were:
It provided further that "no person shall manufacture, sell, barter, transport, import, export, deliver, or furnish any intoxicating liquor except as authorized by this act." It did not specifically prohibit the use of intoxicating liquors. The act defined intoxicating liquor as any beverage containing more than 0.5% alcohol by volume and superseded all existing prohibition laws in effect in states that had such legislation.
Enforcement and impact.
The production, importation, and distribution of alcoholic beverages — once the province of legitimate business — were taken over by criminal gangs, which fought each other for market control in violent confrontations, including murder. Major gangsters, such as Omaha's Tom Dennison and Chicago's Al Capone, became rich and were admired locally and nationally. Enforcement was difficult because the gangs became so rich they were often able to bribe underpaid and understaffed law-enforcement personnel and pay for expensive lawyers. Many citizens were sympathetic to bootleggers, and respectable citizens were lured by the romance of illegal speakeasies, also called "blind tigers". The loosening of social morals during the 1920s included popularizing the cocktail and the cocktail party among higher socio-economic groups. Those inclined to help authorities were often intimidated, even murdered. In several major cities — notably those that served as major points of liquor importation (including Chicago and Detroit) -- gangs wielded significant political power. A Michigan State Police raid on Detroit's once netted the mayor, the sheriff, and the local congressman.
Prohibition came into force at midnight on January 17, 1920, and the first documented infringement of the Volstead Act occurred in Chicago on January 17 at 12:59 a.m. According to police reports, six armed men stole $100,000 worth of "medicinal" whiskey from two freight train cars. This trend in bootlegging liquor created a domino effect, with criminals across the United States. In fact, some gang leaders were stashing liquor months before the Volstead Act was enforced. The ability to sustain a lucrative business in bootlegging liquor was largely helped by the minimal police surveillance at the time. There were only 134 agents designated by the Prohibition Unit to cover all of Illinois, Iowa, and parts of Wisconsin. According to Charles C. Fitzmorris, Chicago's Chief of Police during the beginning of the Prohibition period: "Sixty percent of my police [were] in the bootleg business."
Section 29 of the Act allowed 200 gallons (the equivalent of about 1000 750 ml bottles) of "non-intoxicating cider and fruit juice" to be made each year at home. Initially "intoxicating" was defined as anything more than 0.5%, but the Bureau of Internal Revenue soon struck that down and this effectively legalized home wine-making. For beer, however, the 0.5% limit remained until 1933. Some vineyards embraced the sale of grapes for making wine at home; Zinfandel grapes were popular among home winemakers living near the vineyards, but its tight bunches left their thin skins vulnerable to rot, due to rubbing and abrasion, on the long journey to East Coast markets. The thick skins of Alicante Bouschet were less susceptible to rot, so this and similar varieties were widely planted for the home wine-making market.
The Act contained a number of exceptions and exemptions. Many of these were used to evade the law's intended purpose. For example, the Act allowed a physician to prescribe whiskey for his patients, but limited the amount that could be prescribed. Subsequently, the House of Delegates of the American Medical Association voted to submit to Congress a bill to remove the limit on the amount of whiskey that could be prescribed and questioning the ability of a legislature to determine the therapeutic value of any substance.
The Act called for trials for anyone charged with an alcohol-related offense, and juries often failed to convict. Under the state of New York's Mullan–Gage Act, a short-lived local version of the Volstead Act, the first 4,000 arrests led to just six convictions and not one jail sentence.
Repeal.
Prohibition lost advocates as ignoring the law gained increasing social acceptance and as organized crime violence increased. By 1933, public opposition to prohibition had become overwhelming. In March of that year, Congress passed the Cullen–Harrison Act, which legalized "3.2 beer" ("i.e.," beer containing 3.2% alcohol by weight or 4% by volume) and wines of similarly low alcohol content, rather than the 0.5% limit defined by the original Volstead Act.
Congress passed the Blaine Act, a proposed constitutional amendment to repeal the Eighteenth Amendment to end Prohibition, in February. On December 5, 1933, Utah became the 36th state to ratify the Twenty-first Amendment, which repealed the Eighteenth Amendment, voiding the Volstead Act, and restored control of alcohol to the states. The creation of the Federal Alcohol Administration in 1935 defined a modest role for the federal government with respect to alcohol and its taxation.

</doc>
<doc id="55762" url="http://en.wikipedia.org/wiki?curid=55762" title="Harmonic function">
Harmonic function

In mathematics, mathematical physics and the theory of stochastic processes, a harmonic function is a twice continuously differentiable function "f" : "U" → R (where "U" is an open subset of R"n") which satisfies Laplace's equation, i.e.
everywhere on "U". This is usually written as
or
Examples.
Examples of harmonic functions of two variables are:
Examples of harmonic functions of three variables are given in the table below with formula_9:
Harmonic functions that arise in physics are determined by their singularities and boundary conditions (such as Dirichlet boundary conditions or Neumann boundary conditions). On regions without boundaries, adding the real or imaginary part of any entire function will produce a harmonic function with the same singularity, so in this case the harmonic function is not determined by its singularities; however, we can make the solution unique in physical situations by requiring that the solution goes to 0 as you go to infinity. In this case, uniqueness follows by Liouville's theorem.
The singular points of the harmonic functions above are expressed as "charges" and "charge densities" using the terminology of electrostatics, and so the corresponding harmonic function will be proportional to the electrostatic potential due to these charge distributions. Each function above will yield another harmonic function when multiplied by a constant, rotated, and/or has a constant added. The inversion of each function will yield another harmonic function which has singularities which are the images of the original singularities in a spherical "mirror". Also, the sum of any two harmonic functions will yield another harmonic function.
Finally, examples of harmonic functions of "n" variables are:
Remarks.
The set of harmonic functions on a given open set "U" can be seen as the kernel of the Laplace operator Δ and is therefore a vector space over R: sums, differences and scalar multiples of harmonic functions are again harmonic.
If "f" is a harmonic function on "U", then all partial derivatives of "f" are also harmonic functions on "U". The Laplace operator Δ and the partial derivative operator will commute on this class of functions.
In several ways, the harmonic functions are real analogues to holomorphic functions. All harmonic functions are analytic, i.e. they can be locally expressed as power series. This is a general fact about elliptic operators, of which the Laplacian is a major example.
The uniform limit of a convergent sequence of harmonic functions is still harmonic. This is true because every continuous function satisfying the mean value property is harmonic. Consider the sequence on (−∞, 0) × R defined by formula_12. This sequence is harmonic and converges uniformly to the zero function; however note that the partial derivatives are not uniformly convergent to the zero function (the derivative of the zero function). This example shows the importance of relying on the mean value property and continuity to argue that the limit is harmonic.
Connections with complex function theory.
The real and imaginary part of any holomorphic function yield harmonic functions on R2 (these are said to be a pair of harmonic conjugate functions). Conversely, any harmonic function "u" on an open subset Ω of R2 is "locally" the real part of a holomorphic function. This is immediately seen observing that, writing "z" = "x" + "iy", the complex function "g"("z") := "ux" − i "uy" is holomorphic in Ω because it satisfies the Cauchy–Riemann equations. Therefore, "g" has locally a primitive "f", and "u" is the real part of "f" up to a constant, as "ux" is the real part of formula_13 .
Although the above correspondence with holomorphic functions only holds for functions of two real variables, still harmonic functions in "n" variables enjoy a number of properties typical of holomorphic functions. They are (real) analytic; they have a maximum principle and a mean-value principle; a theorem of removal of singularities as well as a Liouville theorem holds for them in analogy to the corresponding theorems in complex functions theory.
Properties of harmonic functions.
Some important properties of harmonic functions can be deduced from Laplace's equation.
Regularity theorem for harmonic functions.
Harmonic functions are infinitely differentiable. In fact, harmonic functions are real analytic.
Maximum principle.
Harmonic functions satisfy the following "maximum principle": if "K" is any compact subset of "U", then "f", restricted to "K", attains its maximum and minimum on the boundary of "K". If "U" is connected, this means that "f" cannot have local maxima or minima, other than the exceptional case where "f" is constant. Similar properties can be shown for subharmonic functions.
The mean value property.
If "B"("x", "r") is a ball with center "x" and radius "r" which is completely contained in the open set Ω ⊂ R"n", then the value "u"("x") of a harmonic function "u": Ω → R at the center of the ball is given by the average value of "u" on the surface of the ball; this average value is also equal to the average value of "u" in the interior of the ball. In other words
where ω"n" is the area of the unit sphere in "n" dimensions and σ is the "n"-1 dimensional surface measure .
Conversely, all locally integrable functions satisfying the (volume) mean-value property are both infinitely differentiable and harmonic.
In terms of convolutions, if
denotes the characteristic function of the ball with radius "r" about the origin, normalized so that formula_16, the function "u" is harmonic on Ω if and only if
as soon as "B"("x", "r") ⊂ Ω.
Sketch of the proof. The proof of the mean-value property of the harmonic functions and its converse follows immediately observing that the non-homogeneous equation, for any 0 < "s" < "r"
admits an easy explicit solution "wr,s" of class "C"1,1 with compact support in "B"(0, "r"). Thus, if "u" is harmonic in Ω
holds in the set Ω"r" of all points "x" in formula_20 with formula_21 .
Since "u" is continuous in Ω, "u"*χ"r" converges to "u" as "s" → 0 showing the mean value property for "u" in Ω. Conversely, if "u" is any formula_22 function satisfying the mean-value property in Ω, that is,
holds in Ω"r" for all 0 < "s" < "r" then, iterating "m" times the convolution with χ"r" one has:
so that "u" is formula_25 because the m-fold iterated convolution of χ"r" is of class formula_26 with support "B"(0, "mr"). Since "r" and "m" are arbitrary, "u" is formula_27 too. Moreover
formula_28
for all 0 < "s" < "r" so that Δ"u" = 0 in Ω by the fundamental theorem of the calculus of variations, proving the equivalence between harmonicity and mean-value property.
This statement of the mean value property can be generalized as follows: If "h" is any spherically symmetric function supported in "B"("x","r") such that ∫"h" = 1, then "u"("x") = "h" * "u"("x"). In other words, we can take the weighted average of "u" about a point and recover "u"("x"). In particular, by taking "h" to be a "C"∞ function, we can recover the value of "u" at any point even if we only know how "u" acts as a distribution. See Weyl's lemma.
Harnack's inequality.
Let "u" be a non-negative harmonic function in a bounded domain Ω. Then for every connected set
Harnack's inequality 
holds for some constant "C" that depends only on "V" and Ω.
Removal of singularities.
The following principle of removal of singularities holds for harmonic functions. If "f" is a harmonic function defined on a dotted open subset formula_31 of R"n" , which is less singular at "x"0 than the fundamental solution, that is
then "f" extends to a harmonic function on Ω (compare Riemann's theorem for functions of a complex variable).
Liouville's theorem.
If "f" is a harmonic function defined on all of R"n" which is bounded above or bounded below, then "f" is constant (compare Liouville's theorem for functions of a complex variable).
Edward Nelson gave a particularly short proof of this theorem, using the mean value property mentioned above: 
Given two points, choose two balls with the given points as centers and of equal radius. If the radius is large enough, the two balls will coincide except for an arbitrarily small proportion of their volume. Since "f" is bounded, the averages of it over the two balls are arbitrarily close, and so "f" assumes the same value at any two points. 
Generalizations.
Weakly harmonic function.
A function (or, more generally, a distribution) is weakly harmonic if it satisfies Laplace's equation
in a weak sense (or, equivalently, in the sense of distributions). A weakly harmonic function coincides almost everywhere with a strongly harmonic function, and is in particular smooth. A weakly harmonic distribution is precisely the distribution associated to a strongly harmonic function, and so also is smooth. This is Weyl's lemma.
There are other weak formulations of Laplace's equation that are often useful. One of which is Dirichlet's principle, representing harmonic functions in the Sobolev space "H"1(Ω) as the minimizers of the Dirichlet energy integral
with respect to local variations, that is, all functions formula_35 such that "J"("u") ≤ "J"("u" + "v") holds for all formula_36 or equivalently, for all formula_37
Harmonic functions on manifolds.
Harmonic functions can be defined on an arbitrary Riemannian manifold, using the Laplace–Beltrami operator Δ. In this context, a function is called "harmonic" if 
Many of the properties of harmonic functions on domains in Euclidean space carry over to this more general setting, including the mean value theorem (over geodesic balls), the maximum principle, and the Harnack inequality. With the exception of the mean value theorem, these are easy consequences of the corresponding results for general linear elliptic partial differential equations of the second order.
Subharmonic functions.
A "C"2 function that satisfies Δ"f" ≥ 0 is called subharmonic. This condition guarantees that the maximum principle will hold, although other properties of harmonic functions may fail. More generally, a function is subharmonic if and only if, in the interior of any ball in its domain, its graph lies below that of the harmonic function interpolating its boundary values on the ball.
Harmonic forms.
One generalization of the study of harmonic functions is the study of harmonic forms on Riemannian manifolds, and it is related to the study of cohomology. Also, it is possible to define harmonic vector-valued functions, or harmonic maps of two Riemannian manifolds, which are critical points of a generalized Dirichlet energy functional (this includes harmonic functions as a special case, a result known as Dirichlet principle). This kind of harmonic maps appear in the theory of minimal surfaces. For example, a curve, that is, a map from an interval in R to a Riemannian manifold, is a harmonic map if and only if it is a geodesic.
Harmonic maps between manifolds.
If "M" and "N" are two Riemannian manifolds, then a harmonic map "u" : "M" → "N" is defined to be a critical point of the Dirichlet energy
in which "du" : "TM" → "TN" is the differential of "u", and the norm is that induced by the metric on "M" and that on "N" on the tensor product bundle "T"*"M" ⊗ "u"−1 "TN".
Important special cases of harmonic maps between manifolds include minimal surfaces, which are precisely the harmonic immersions of a surface into three-dimensional Euclidean space. More generally, minimal submanifolds are harmonic immersions of one manifold in another. Harmonic coordinates are a harmonic diffeomorphism from a manifold to an open subset of a Euclidean space of the same dimension.

</doc>
<doc id="55766" url="http://en.wikipedia.org/wiki?curid=55766" title="Norris–La Guardia Act">
Norris–La Guardia Act

The Norris–La Guardia Act (also known as the Anti-Injunction Bill) was a 1932 United States federal law that banned yellow-dog contracts, barred the federal courts from issuing injunctions against nonviolent labor disputes, and created a positive right of noninterference by employers against workers joining trade unions. The common title comes from the names of the sponsors of the legislation: Senator George W. Norris of Nebraska and Representative Fiorello H. La Guardia of New York, both Republicans.
History.
It is formally the Act of March 23, 1932 (Ch. 90, 47 Stat. ). It is currently codified at 29 U.S.C. , starting at #Redirect et. seq.
Overview.
The Act states that yellow-dog contracts, where workers agree as a condition of employment to not join a labor union, are unenforceable in federal court. It also establishes that employees are free to form unions without employer interference and prevents the federal courts from issuing injunctions in nonviolent labor disputes. The three provisions include protecting worker's self-organization and liberty or "collective bargaining", removing jurisdiction from federal courts vis-a-vis the issuance of injunctions in non-violent labor disputes, and outlawing the "yellow dog" contract.
Section 13A of the act was fully applied by the Supreme Court of the United States with a 1938 decision, "New Negro Alliance v. Sanitary Grocery Co.", in an opinion authored by Justice Owen Roberts. The Court held that the act meant to prohibit employers from proscribing the peaceful dissemination of information concerning the "terms and conditions of employment" by those involved in an active labor dispute, even when such dissemination occurs on an employer's private property.
Trivia.
The Living Theater play "Injunction Granted" features a scene in which a judge grants injunctions against many trade unions. There follows a scene in which the Norris - La Guardia Act is passed.

</doc>
<doc id="55769" url="http://en.wikipedia.org/wiki?curid=55769" title="Public Works Administration">
Public Works Administration

 Public Works Administration (PWA), part of the New Deal of 1933, was a large-scale public works construction agency in the United States headed by Secretary of the Interior Harold L. Ickes. It was created by the National Industrial Recovery Act in June 1933 in response to the Great Depression. It built large-scale public works such as dams, bridges, hospitals, and schools. Its goals were to spend $3.3 billion in the first year, and $6 billion in all, to provide employment, stabilize purchasing power, and help revive the economy. Most of the spending came in two waves in 1933-35, and again in 1938. Originally called the Federal Emergency Administration of Public Works, it was renamed the Public Works Administration in 1935 and shut down in 1943.
The PWA spent over $6 billion in contracts to private construction firms that did the actual work. It created an infrastructure that generated national and local pride in the 1930s and remains vital seven decades later. The PWA was much less controversial than its rival agency with a confusingly similar name, the Works Progress Administration (WPA), headed by Harry Hopkins, which focused on smaller projects and hired unemployed unskilled workers.
Origins.
Frances Perkins had first suggested a federally financed public works program, and the idea received considerable support from Harold L. Ickes, James Farley, and Henry Wallace. After having scaled back the initial cost of the PWA, Franklin Delano Roosevelt agreed to include the PWA as part of his New Deal proposals in the "Hundred Days" of spring 1933.
Projects.
The PWA headquarters in Washington planned projects, which were built by private construction companies hiring workers on the open market. Unlike the WPA, it did not hire the unemployed directly.
More than any other New Deal program, the PWA epitomized the progressive notion of "priming the pump" to encourage economic recovery. Between July 1933 and March 1939 the PWA funded and administered the construction of more than 34,000 projects including airports, large electricity-generating dams, major warships for the Navy, and bridges, as well as 70% of the new schools and one-third of the hospitals built in 1933–1939.
Streets and highways were the most common PWA projects, as 11,428 road projects, or 33% of all PWA projects, accounted for over 15% of its total budget. School buildings, 7,488 in all, came in second at 14% of spending. PWA functioned chiefly by making allotments to the various Federal agencies; making loans and grants to state and other public bodies; and making loans without grants (for a brief time) to the railroads. For example it provided funds for the Indian Division of the CCC to build roads, bridges and other public works on and near Indian reservations.
The PWA became, with its "multiplier-effect" and first two-year budget of $3.3 billion (compared to the entire GDP of $60 billion), the driving force of America’s biggest construction effort up to that date. By June 1934, the agency had distributed its entire fund to 13,266 federal projects and 2,407 non-federal projects. For every worker on a PWA project, almost two additional workers were employed indirectly. The PWA accomplished the electrification of rural America, the building of canals, tunnels, bridges, highways, streets, sewage systems, and housing areas, as well as hospitals, schools, and universities; every year it consumed roughly half of the concrete and a third of the steel of the entire nation.
Some of the most famous PWA projects are the Triborough Bridge and the Lincoln Tunnel in New York City, the Grand Coulee Dam in Washington state, the longest continuous sidewalk in the world along 6 1⁄2 miles of Bayshore Blvd. in Tampa, Florida, and the Overseas Highway connecting Key West, Florida, to the mainland. The PWA also electrified the Pennsylvania Railroad between New York and Washington, DC. At the local level it built courthouses, schools, hospitals and other public facilities that remain in use in the 21st century.
Housing.
The PWA was the centerpiece of the New Deal program for building public housing for the poor people in cities. However it did not create as much affordable housing as supporters would have hoped, building only 29,000 units in 4 1⁄2 years.
Criticism.
The PWA spent over $6 billion, but did not succeed in returning the level of industrial activity to pre-depression levels. Though successful in many aspects, it has been acknowledged that the PWA's objective of constructing a substantial number of quality, affordable housing was a major failure. Some have argued that because Roosevelt was opposed to deficit spending, there was not enough money spent to help the PWA achieve its housing goals. 
Reeves (1973) argues that the competitive theory of administration used by Roosevelt proved to be inefficient and produced delays. The competition over the size of expenditure, the selection of the administrator, and the appointment of staff at the state level, led to delays and to the ultimate failure of PWA as a recovery instrument. As director of the budget, Lewis Douglas overrode the views of leading senators in reducing appropriations to $3.5 billion and in transferring much of that money to other agencies in lieu of their own specific appropriations. The cautious and penurious Ickes won out over the more imaginative Hugh S. Johnson as chief of public works administration. Political competition between rival Democratic state organizations and between Democrats and Progressive Republicans led to delays in implementing PWA efforts on the local level. Ickes instituted quotas for hiring skilled and unskilled blacks in construction financed through the Public Works Administration (PWA). Resistance from employers and unions was partially overcome by negotiations and implied sanctions. Although results were ambiguous, the plan helped provide African Americans with employment, especially among unskilled workers.
Termination.
When President Franklin D. Roosevelt moved industry toward World War II production, the PWA was abolished and its functions were transferred to the Federal Works Agency in June 1943.
Contrast with WPA.
The PWA should not be confused with its great rival the Works Progress Administration (WPA), though both were part of the New Deal. The WPA, headed by Harry Hopkins, engaged in smaller projects in close cooperation with local governments—such as building a city hall or sewers or sidewalks. The PWA projects were much larger in scope, such as giant dams. The WPA hired only people on relief who were paid directly by the federal government. The PWA gave contracts to private firms who did all the hiring on the private sector job market. The WPA also had youth programs (the NYA), projects for women, and arts projects that the PWA did not have.

</doc>
<doc id="55770" url="http://en.wikipedia.org/wiki?curid=55770" title="Tennessee Valley Authority">
Tennessee Valley Authority

The Tennessee Valley Authority (TVA) is a federally owned corporation in the United States created by congressional charter in May 1933 to provide navigation, flood control, electricity generation, fertilizer manufacturing, and economic development in the Tennessee Valley, a region particularly affected by the Great Depression. The enterprise was a result of the efforts of Senator George W. Norris of Nebraska. TVA was envisioned not only as a provider, but also as a regional economic development agency that would use federal experts and electricity to rapidly modernize the region's economy and society.
TVA's service area covers most of Tennessee, portions of Alabama, Mississippi, and Kentucky, and small slices of Georgia, North Carolina, and Virginia. It was the first large regional planning agency of the federal government and remains the largest. Under the leadership of David Lilienthal ("Mr. TVA"), TVA became a model for America's governmental efforts to seek to assist in the modernization of agrarian societies in the developing world.
Overview.
President Franklin Delano Roosevelt signed the Tennessee Valley Authority Act (ch. 32, 48 Stat. , "codified as amended at #Redirect , et seq."), creating the TVA.
During the 1920s and the Great Depression years, Americans began to support the idea of public ownership of utilities, particularly hydroelectric power facilities. The concept of government-owned generation facilities selling to publicly owned distribution utilities was controversial and remains so today.
Many believed privately owned power companies were charging too much for power, did not employ fair operating practices, and were subject to abuse by their owners (utility holding companies), at the expense of consumers. During his presidential campaign, Roosevelt claimed that private utilities had "selfish purposes" and said, "Never shall the federal government part with its sovereignty or with its control of its power resources while I'm president of the United States." By forming utility holding companies, the private sector controlled 94 percent of generation by 1921, essentially unregulated. (This gave rise to the Public Utility Holding Company Act of 1935 (PUHCA)). Many private companies in the Tennessee Valley were bought by the federal government. Others shut down, unable to compete with the TVA. Government regulations were also passed to prevent competition with TVA.
On the other hand, there were economic libertarians who believed the government should not participate in the electricity generation business, fearing government ownership would lead to the misuse of hydroelectric sites . TVA was one of the first federal hydropower agencies, and today most of the nation's major hydropower systems are federally managed. Other attempts to create TVA-like regional agencies have failed, such as a proposed Columbia Valley Authority for the Columbia River in the Pacific Northwest.
Regional power consumers may benefit from lower-cost electricity supplied from TVA's network of 29 power-producing hydropower facilities. Supporters of TVA, though, note that the agency's management of the Tennessee River system without appropriated federal funding saves federal taxpayers millions of dollars annually. Opponents, such as Dean Russell in "The TVA Idea," in addition to condemning the project as being socialist, argued that TVA created a "hidden loss" by preventing the creation of "factories and jobs that would have come into existence if the government had allowed the taxpayers to spend their money as they wished." Defenders note that TVA is overwhelmingly popular in Tennessee among conservatives and liberals alike, as Barry Goldwater discovered in 1964, when he proposed selling the agency.
The Supreme Court of the United States ruled TVA to be constitutional in "Ashwander v. Tennessee Valley Authority", 297 U.S. 288 (1936). The Court noted that regulating commerce among the states includes regulation of streams and that controlling floods is required for keeping streams navigable. The war powers also authorized the project. The argument before the court was that electricity generation was a "by-product" of navigation and flood control and therefore could be considered constitutional.
History.
Muscle Shoals controversy, 1920-1932.
In the 1920s a major battle erupted over building an electric power system in the Tennessee Valley, based on the World War I federal dam at Muscle Shoals, Alabama. It would generate electricity and produce fertilizer. Liberal Senator George Norris of Nebraska blocked a proposal from Henry Ford in 1920 to use the dam to modernize the valley. Norris deeply distrusted privately owned utility companies. He did get Congress to pass the Muscle Shoals Bill, but it was vetoed as socialistic by President Herbert Hoover in 1931. The idea behind the Muscle Shoals Bill in 1933 became a core part of the New Deal's TVA.
1930s.
Even by Depression standards, the Tennessee Valley was economically dismal in 1933. Thirty percent of the population was affected by malaria, and the average income was only $639 per year, with some families surviving on as little as $100 per year. Much of the land had been farmed too hard for too long, eroding and depleting the soil. Crop yields had fallen along with farm incomes. The best timber had been cut, with another 10% of forests being burnt each year.
TVA was designed to modernize the region, using experts and electricity to combat human and economic problems. TVA developed fertilizers, taught farmers ways to improve crop yields and helped replant forests, control forest fires, and improve habitat for fish and wildlife. The most dramatic change in Valley life came from TVA-generated electricity. Electric lights and modern home appliances made life easier and farms more productive. Electricity also drew industries into the region, providing desperately needed jobs.
None of this was easy. The development of the dams displaced more than 15,000 families. This created anti-TVA sentiment in some rural communities. Many local landowners were suspicious of government agencies. But TVA successfully introduced new agricultural methods into traditional farming communities by blending in and finding local champions.
A Tennessee farmer would not take advice from an official in a suit and tie, so TVA officials had to find leaders in the communities and convince them that crop rotation and the judicious application of fertilizers could restore soil fertility. Once they had convinced the leaders, the rest followed.
At its inception, TVA was based in Muscle Shoals, Alabama, but later moved its headquarters to Knoxville, Tennessee, where they remain today. At one point, TVA's headquarters were housed in the Old Federal Customs House at the corner of Clinch Avenue and Market Street. The building is now a museum.
Employment policy.
The Authority hired many area unemployed to conduct conservation, economic development, and social programs, such as a library service that operated for the surrounding area. The professional staff headquarters was composed of experts from outside the region. The workers were categorized by the usual racial and gender lines of the day. TVA hired a few African Americans for janitorial or other low-level positions. TVA recognized labor unions; its skilled and semi-skilled blue collar employees were unionized, a breakthrough in an area known for corporations hostile to miners' and textile workers' unions. Women were excluded from construction work. TVA's cheap electricity attracted textile mills to the area, and they hired mostly women as workers.
1940s.
During World War II, the U.S. needed greater aluminum supplies to build airplanes. Aluminum plants required huge amounts of electricity. To provide the power, TVA engaged in one of the largest hydropower construction programs ever undertaken in the U.S. By early in 1942, when the effort reached its peak, 12 hydroelectric plants and one steam plant were under construction at the same time, and design and construction employment reached a total of 28,000. The largest project of this period was the Fontana Dam. After negotiations led by Vice-President Harry Truman ("I want aluminum. I don't care if I get it from Alcoa or Al Capone."), TVA purchased the land from Nantahala Power and Light, a wholly owned subsidiary of Alcoa, and built Fontana Dam.
The government originally intended the electricity generated from Fontana to be used by Alcoa factories. By the time the dam generated power in early 1945, the electricity was directed to another purpose in addition to aluminum manufacturing, as TVA also provided much of the electricity needed for uranium enrichment at Oak Ridge, Tennessee, as required for the Manhattan Project and the making of the atomic bomb.
1950s.
By the end of the war, TVA had completed a 650-mile (1,050-kilometer) navigation channel the length of the Tennessee River and had become the nation's largest electricity supplier.
Even so, the demand for electricity was outstripping TVA's capacity to produce power from hydroelectric dams. Political interference kept TVA from securing additional federal appropriations to build coal-fired plants, so it sought the authority to issue bonds. Congress passed legislation in 1959 to make the TVA power system self-financing, and from that point on it would pay its own way.
1960s.
The 1960s were years of unprecedented economic growth in the Tennessee Valley. Electric rates were among the nation's lowest and stayed low as TVA brought larger, more efficient generating units into service. Expecting the Valley's electric power needs to continue to grow, TVA began building nuclear reactors as a new source of cheap power. During this decade (and the 1970s), TVA was engaged in what was up to that time its most controversial project – the Tellico Dam Project. The project was initially conceived in the 1940s but not completed until 1979.
1970s and 1980s.
Significant changes occurred in the economy of the Tennessee Valley and the nation, prompted by an international oil embargo in 1973 and accelerating fuel costs later in the decade.
The average cost of electricity in the Tennessee Valley increased fivefold from the early 1970s to the early 1980s. TVA canceled the construction of 12 nuclear power plants in the 1980s.
The 1970s saw the last and most controversial of the TVA's large dam-reservoir projects, Tellico Dam. The Tellico Dam project was initially delayed because of concern over the snail darter. A lawsuit was filed under the Endangered Species Act and the U.S. Supreme Court ruled in favor of protecting the snail darter in Tennessee Valley Authority v. Hill.
Marvin T. Runyon became chairman of the Tennessee Valley Authority in January 1988. He claimed to reduce management layers, cut overhead costs by more than 30%, and achieved cumulative savings and efficiency improvements of $1.8 billion. He said he revitalized the nuclear program and instituted a rate freeze that continued for ten years.
1990s.
As the electric-utility industry moved toward restructuring and deregulation, TVA began preparing for competition.
It cut operating costs by nearly $950 million a year, reduced its workforce by more than half, increased the generating capacity of its plants, stopped building nuclear plants, and developed a plan to meet the energy needs of the Tennessee Valley through the year 2020.
2000s.
In 2002, TVA began work to restart a previously mothballed nuclear reactor at Browns Ferry Unit 1, which was completed in May 2007. In 2005, the TVA announced its intention to construct an Advanced Pressurized Water Reactor at its Bellefonte site in Alabama (filing the necessary applications in November 2007), and in 2007 announced plans to complete the unfinished Unit 2 at Watts Bar. (TVA is the owner and operator of the Browns Ferry, Sequoyah, and Watts Bar nuclear power plants.)
In 2004, TVA implemented recommendations from the Reservoir Operations Study (ROS) in how it operates the Tennessee River system (the nation's fifth largest).
On December 22, 2008, an earthen dike at TVA's Kingston Fossil Plant broke, spreading one billion gallons of wet coal ash across 300 acre of land and into the tributaries of the Tennessee River. The non-profit Southern Alliance for Clean Energy plans on suing TVA for $165 million on behalf of residents in the area. The Kentucky Sierra Club called the disaster the "worst environmental disaster since Chernobyl". While TVA's culture at its fossil fuel plants was not the cause of the Kingston Spill, the culture contributed to the spill, as was appropriately noted in the TVA OIG's (Office of the Inspector General) report, Inspection 2008-12283-02, Review of the Kingston Fossil Plant Ash Spill Cause Study and Observations About Ash Management.
In 2009, TVA signed 20-year power purchase agreements with Maryland-based CVP Renewable Energy Co. and Chicago-based Invenergy Wind LLC for electricity generated by wind farms.
As of 2013, TVA carries $25 billion in debt, near the $30 billion debt limit imposed by Congress. TVA must retire at least 18 of its 59 coal-fired units by 2017, and install scrubbers in several others or convert them to make them cleaner at a cost of $25 billion over the next 10-years according to the Obama Administration's proposed 2013–2014 budget. The budget also says that "reducing or eliminating the federal government's role in programmes such as TVA, which have achieved their original objectives and no longer require federal participation, can help put the nation on a sustainable fiscal path" which could mean future privatization for TVA.
Facilities.
TVA's power mix as of 2012 is 11 coal-powered plants, 29 hydroelectric dams, three nuclear power plants (with six operating reactors), nine simple cycle natural gas combustion turbine plants, and five combined cycle gas plants.
TVA is the largest public power utility in the United States and one of the largest producers of electricity in the country. It acts as a regional grid reliability coordinator. In 2012 coal generation was about 32% of total, nuclear 34%, hydro 9%, and (owned) gas 11%. TVA currently purchases about 10% of its power from merchant plants, mostly natural gas combined cycle, but also some coal, wind, and other renewables. TVA's Watts Bar reactor produces tritium as a byproduct for the U.S. National Nuclear Security Administration, which requires tritium for nuclear weapons (for "boosted" fission primaries and for fusion secondaries).
Fossil fuel plants.
Lagoon Creek Combined Cycle Plant (commercial operation September 2010) was the first combined cycle generating plant in Tennessee.
Nuclear power plants.
In the 1980s, TVA set out to build 17 nuclear reactors but finished only five. Canceled nuclear facilities include Phipps Bend, Bellefonte, Hartsville, Yellow Creek, and the Clinch River Breeder Reactor. As of 2012, TVA operated six reactors at three sites: Browns Ferry, Sequoyah, and Watts Bar. Together these nuclear power plants produce about 30 percent of the TVA's energy.
Joint facilities.
TVA also assists ALCOA's Tapoco/APGI in regulating several facilities, including the Calderwood, Cheoah, Chilhowee, and Santeetlah dams.
Renewable generation.
TVA operates several small-scale facilities that generate electricity from renewable sources other than hydropower. These include:
At Buffalo Mountain in Oliver Springs, Tennessee, TVA operates three wind turbines with a combined generation capacity of 2 MW and purchases the output of 15 additional wind turbines owned by Invenergy that have a combined capacity of 27 MW.
Biogas from the Maxson wastewater treatment plant in Memphis is burned in Allen Fossil Plant, accounting for a generating capacity of 4 MW.
Electric transmission.
TVA is one of the largest operators of electric transmission in the US with an approximately 16,000 mi corridor of transmission (13,000 mi of which is greater than 161kv).
Administration.
TVA's headquarters are located in downtown Knoxville, with large administrative offices in Chattanooga (training/development; supplier relations; power generation and distribution) Nashville, Tennessee (economic development), and Muscle Shoals, Alabama.
Recreation.
TVA has conveyed approximately 485,420 acres of property for recreation and preservation purposes including public parks; public access areas and roadside parks; wildlife refuges; national parks and forests; and other camps and recreation areas, which comprises approximately 759 different sites.
Criticism.
TVA was heralded by New Dealers and the New Deal Coalition not only as a successful economic development program for a depressed area but also as a democratic nation-building effort overseas because of its alleged grassroots inclusiveness as articulated by director David Lilienthal. The TVA was controversial in the 1930s. Historian Thomas McCraw concludes (1971 p. 157) that Roosevelt "rescued the [power] industry from its own abuses" but "he might have done this much with a great deal less agitation and ill will". New Dealers hoped to build numerous other TVAs around the country but were defeated by Wendell Willkie and the Conservative coalition in Congress. The valley authority model did not replace the limited-purpose water programs of the Bureau of Reclamation and the Army Corps of Engineers. State-centered theorists hold that reformers are most likely to succeed during periods such as the New Deal era, when they are supported by a democratized polity and when they dominate Congress and the administration.
However, it has been shown that in river policy, the strength of opposing interest groups also mattered. The TVA bill was passed in 1933 because reformers like Norris skillfully coordinated action at potential choke points and weakened the already disorganized opponents among the electric power industry lobbyists. In 1936, however, after regrouping, opposing river lobbyists and conservative coalition congressmen took advantage of the New Dealers' spending mood by expanding the Army Corps' flood control program. They also helped defeat further valley authorities, the most promising of the New Deal water policy reforms.
When Democrats after 1945 proclaimed the Tennessee Valley Authority as a model for countries in the developing world to follow, conservative critics charged it was a top-heavy, centralized, technocratic venture that displaced locals and did so in insensitive ways. Thus, when the program was used as the basis for modernization programs in various parts of the third world during the Cold War, such as in the Mekong Delta in Vietnam, its failure brought a backlash of cynicism toward modernization programs that has persisted.
Then-movie star Ronald Reagan had moved to television as the host and a frequent performer for "General Electric Theater" during 1954. Reagan was later fired by General Electric in 1962 in response to his publicly referring to the TVA (TVA being a major customer for GE turbines) as one of the problems of "big government".
In 1981 the TVA Board of Directors broke with previous tradition and took a hard line against white-collar unions during contract negotiations. As a result, a class action suit was filed in 1984 in U.S. court charging the agency with sex discrimination under Title VII of the Civil Rights Act based on the large number of females in one of the pay grades negatively impacted by the new contract. An out-of-court settlement of the lawsuit was reached in 1987, in which TVA agreed to contract modifications and paid the group $5 million but admitted no wrongdoing.
In popular culture.
In the 1930s, the building of Norris Dam and the changes it brought to the region inspired films, books, stage plays, and songs. Folk songs from the construction period frequently express enthusiasm for the benefits that the project brought to the region.
TVA continues to be a subject for popular culture:
References.
 American Passages: a History of the United States
External links.
 #if: 
 #if:  
 |, 
 #if: 
 }}{{
 #if: 
 #if: 
 | ()
 |{{
 #if: 
 #if: 
 | {{#if:||}}{{
 #if: 
 #if: 
 #if: {{
 #if: Procedure for Making, Indexing and Filing Computations
 #if: 
 #if: 
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |: {{
 #if: Procedure for Making, Indexing and Filing Computations
 #if: 
 #if: 
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 
 #if:  
 |{{
 #if: Tennessee Valley Authority
 }} {{Citation/make link
 | 1={{
 #if: 
 #if: 
 #if: 
 |{{
 #if: 
 | 2="  
 #if:| []
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 | ( ed.)
 #if: 
 }}{{
 #if: 
 #if: 
 |,
 #if: Tennessee Valley Authority
 |{{
 #if: March 1950
 |, March 1950{{
 #if:
}}{{
 #if: 
 #ifeq: | March 1950
 |{{
 #if: 
 #if: Tennessee Valley Authority
 | (published )
 |{{
 #if: 
 | (published )
}}{{
 #if: 
 |{{
 #if: {{
 #if: Procedure for Making, Indexing and Filing Computations
 #if: 
 #if: 
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |, {{
 #if: Procedure for Making, Indexing and Filing Computations
 #if: 
 #if: 
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
}}{{
 #if:
 | , {{#ifeq: | no
 | {{#if:
 |{{Citation/make link||{{#ifeq:|.|A|a}}rchived}} from the original
 |{{#ifeq:|.|A|a}}rchived
 | {{#ifeq:|.|A|a}}rchived{{#if:
 }}{{#if:| on }}{{
 |. {{citation error|nocat=
 #if:  
 |, {{
 #if: 
 |
 |, {{
 #if: 
 |
 }}{{
 #if: 
 | {{#ifeq:|,|, r|. R}}etrieved 
}}{{#if:
}}{{#if:
}}{{#if:
}}<span
 class="Z3988"
 title="ctx_ver=Z39.88-2004&rft_val_fmt={{urlencode:info:ofi/fmt:kev:mtx:}}{{
 #if: 
 |journal&rft.genre=article&rft.atitle={{urlencode:  
 |book{{
 #if: 
 |&rft.genre=bookitem&rft.btitle={{urlencode:}}&rft.atitle={{urlencode:  
 |&rft.genre=book&rft.btitle={{urlencode:  
 #if: Tennessee Valley Authority |&rft.aulast={{urlencode:Tennessee Valley Authority}}{{
 }}{{
 #if: Tennessee Valley Authority |&rft.au={{urlencode:Tennessee Valley Authority}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: {{
 #if: Procedure for Making, Indexing and Filing Computations
 #if: 
 #if: 
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |&rft.pages={{urlencode: {{
 #if: Procedure for Making, Indexing and Filing Computations
 #if: 
 #if: 
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 }}{{
 }}&rfr_id=info:sid/en.wikipedia.org:{{FULLPAGENAMEE}}"> 
 |IncludedWorkTitle = 
 |IncludedWorkURL = 
 |Other = 
 |Edition = 
 |Place = 
 |PublicationPlace = 
 |Publisher = Wikisource
 |PublicationDate = 
 |EditorSurname1 = 
 |EditorSurname2 = 
 |EditorSurname3 = 
 |EditorSurname4 = 
 |EditorGiven1 = 
 |EditorGiven2=
 |EditorGiven3=
 |EditorGiven4=
 |Editorlink1=
 |Editorlink2=
 |Editorlink3=
 |Editorlink4=
 |language = 
 |format = 
 |ARXIV=
 |ASIN=
 |BIBCODE=
 |DOI=
 |DoiBroken=
 |ISBN=
 |ISSN=
 |JFM=
 |JSTOR=
 |LCCN=
 |MR=
 |OCLC=
 |OL=
 |OSTI=
 |PMC=
 |Embargo=1010-10-10
 |PMID=
 |RFC=
 |SSRN=
 |ZBL=
 |ID=
 |AccessDate=
 |DateFormat=none
 |quote = 
 |laysummary = 
 |laydate = 
 |Ref=
 |Sep = .
 |PS = .
 |AuthorSep = ; 
 |NameSep = , 
 |Trunc = 8
 |amp = 

</doc>
<doc id="55771" url="http://en.wikipedia.org/wiki?curid=55771" title="Home Owners' Loan Corporation">
Home Owners' Loan Corporation

The Home Owners' Loan Corporation (HOLC) was a government-sponsored corporation created as part of the New Deal. The corporation was established in 1933 by the Home Owners' Loan Corporation Act under President Franklin D. Roosevelt. Its purpose was to refinance home mortgages currently in default to prevent foreclosure.
Operations.
The HOLC issued bonds and then used the bonds to purchase mortgage loans from lenders. The loans purchased were for home owners who were having problems making the payments on their mortgage loans "through no fault of their own." The HOLC then refinanced the loans for the borrowers. Many of the lenders gained from selling the loans because the HOLC bought the loans by offering a value of bonds equal to the amount of principal owed by the borrower plus unpaid interest on the loan plus taxes that the lender paid on the property. This value of the loan was then the amount of the loan that was refinanced for the borrower. The borrower gained because he or she was offered a loan with a longer time frame at a lower interest rate. It was rare to reduce the amount of principal owed.
The Nature of the Loans.
The typical HOLC loan before 1939 was an amortized 15-year loan, compared with the 3-5 year mortgages offered by commercial banks and the 10-12 year loans offered by Building and Loans in the 1920s. The interest rate on the original HOLC loans was 5 percent at a time when most mortgage loans were being offered at an interest rate of 6 to 8 percent. In 1939 the corporation lowered the interest rate to 4 1/2 percent for a large group of borrowers. The HOLC loans were typically amortized, so that there were equal payments each month on the loan. This contrasts with interest-only loans in the 1920s in which the borrower would make payments equal to the interest on the loan each month until the end of the loan and then repay the principal (the amount borrowed) at the end of the loan. Until the early 1930s borrowers often paid the principal owed by taking out a new loan. When the economy fell apart in the 1930s it became very difficult to borrow and many borrowers could not repay the principal owed at the end of the loan. It also contrasts with loans at Building and Loans (B&L) in the 1920s, which often lasted 10 to 12 years. The B&L loans were hybrid loans that combined an interest-only loan and a contract to buy shares in the Building and Loan. The borrower would make equal payments on the hybrid loans until the value of the building and loan shares was equal to the principal. At that point the loan was fully repaid. If the value of the shares fell, it took longer to fully repay the loan. This became a major problem in the 1930s. In contrast, the HOLC loans were direct reduction loans in which some payment of the principal owed was made each month; therefore, the length of the loan would not change unless the borrower failed to make payments. The direct reduction loan has become the most common type of American mortgage.
Loan Repayments and Foreclosure Policies.
Between 1933 and 1935 the HOLC made slightly more than one million loans. At that point it stopped making new loans and then focused on the repayments of the loans. The typical borrower whose loan was refinanced by the HOLC was more than 2 years behind on payments of the loan and more than 2 years behind on making tax payments on the property. The HOLC eventually foreclosed on 20 percent of the loans that it refinanced. It tended to wait until the borrower had failed to make payments on the loan for more than a year before it foreclosed on the loan. When the HOLC foreclosed, it typically refurbished the home. In many cases it rented out the home until it could be resold. The HOLC tried to avoid selling too many homes quickly to avoid having negative effects on housing prices. Ultimately, more than 800,000 people repaid their HOLC loans, and many repaid them early.
HOLC officially ceased operations in 1951, when its last assets were sold to private lenders. HOLC was only applicable to nonfarm homes, worth less than $20,000. HOLC also assisted mortgage lenders by refinancing problematic loans and increasing the institutions liquidity. When its last assets were sold in 1951, HOLC turned a small profit.
Controversy About Redlining.
HOLC is often-cited as the originator of mortgage redlining, although, this claim has also been disputed. The racist attitudes and language found in the appraisal sheets and Residential Security Maps created by the HOLC likely gave federal support to existing bias and racial antipathy in society at large (Crossney and Bartelt 2005; Crossney and Bartelt

</doc>
<doc id="55772" url="http://en.wikipedia.org/wiki?curid=55772" title="Farm Credit Administration">
Farm Credit Administration

The Farm Credit Administration is an independent agency of the Executive Branch of the federal government of the United States. It regulates and examines the banks, associations, and related entities of the Farm Credit System, a network of borrower-owned financial institutions that provide credit to farmers, ranchers, and agricultural and rural utility cooperatives. It derives its authority from the Farm Credit Act of 1971. The FCA is headquartered in McLean, Virginia, near Washington, DC.
History.
The Farm Credit Administration was established by Executive Order 6084, which transferred most of the functions of the Federal Farm Board to the new Agricultural Adjustment Administration. The Federal Farm Board was then renamed the Farm Credit Administration.
The Farm Credit Act of 1933 provides for organizations within the Farm Credit Administration. The Farm Credit Act of 1933 was part of President Franklin D. Roosevelt's New Deal, to help farmers refinance mortgages over a longer time at below-market interest rates at regional and national banks. This helped farmers recover from the Dustbowl. The Emergency Farm Mortgage Act loaned funds to farmers in danger of losing their properties. The campaign refinanced 20% of farmer's mortgages. 
An Executive order by Roosevelt in 1933 placed all existing agricultural credit organizations under the supervision of a new agency, the Farm Credit Administration. This included the Federal Farm Board. The Farm Credit Administration was independent until 1939, when it became part of the U.S. Department of Agriculture, but became an independent agency again under the Farm Credit Act of 1953. This Act created a Federal Farm Credit Board with 13 members (one from each of the 12 agricultural districts and one appointed by the Secretary of Agriculture) to develop policy for the Farm Credit Administration.
The Farm Credit Act of 1971 recodified all previous acts governing the Farm Credit System.

</doc>
<doc id="55773" url="http://en.wikipedia.org/wiki?curid=55773" title="Civil Works Administration">
Civil Works Administration

The Civil Works Administration (CWA) was a short-lived U.S. job creation program established by the New Deal during the Great Depression to rapidly create manual labor jobs for millions of unemployed workers. The jobs were merely temporary, for the duration of the hard winter of 1933–34. President Franklin D. Roosevelt unveiled the CWA on November 8, 1933 and put Harry L. Hopkins in charge of the short-term agency.
The CWA was a project created under the Federal Emergency Relief Administration (FERA). The CWA created construction jobs, mainly improving or constructing buildings and bridges. It ended on March 31, 1934, after spending $200 million a month and giving jobs to 4 million people.
Accomplishments.
The CWA's workers laid 12 million feet of sewer pipe and built or improved 255,000 miles of roads, 40,000 schools, 3,700 playgrounds, and nearly 1,000 airports (not to mention building 250,000 outhouses still badly needed in rural America). The program was praised by Alf Landon, who later ran against Roosevelt in the 1936 election.
Representative of the work are one county's accomplishments in less than five months, from November 1933 to March 1934. Grand Forks County, North Dakota put 2,392 unemployed workers on its payroll at a cost of about $250,000. When the CWA began in eastern Connecticut, it could hire only 480 workers out of 1,500 who registered for jobs. Projects undertaken included work on city utility systems, public buildings, parks, and roads. Rural areas profited, with most labor being directed to roads and community schools. CWA officials gave preference to veterans with dependents, but considerable political favoritism determined which North Dakotans got jobs.
Opposition.
Although the CWA provided much employment, there were many taxpayers who saw leaves being raked but nothing of permanent value. Roosevelt told his cabinet that this criticism moved him to end the program and replace it with the WPA which would have long-term value for the society, in addition to short-term benefits for the unemployed.

</doc>
<doc id="55774" url="http://en.wikipedia.org/wiki?curid=55774" title="Agricultural Adjustment Act">
Agricultural Adjustment Act

The Agricultural Adjustment Act (AAA) was a United States federal law of the New Deal era which reduced agricultural production by paying farmers subsidies not to plant on part of their land and to kill off excess livestock. Its purpose was to reduce crop surplus and therefore effectively raise the value of crops.
The law, in its entirety, can be read .
Background.
When President Franklin Delano Roosevelt took office in March 1933, the United States was in the midst of the Great Depression. "Farmers faced the most severe economic situation and lowest agricultural prices since the 1890s." "Overproduction and a shrinking international market had driven down agricultural prices." Soon after his inauguration, Roosevelt called the Hundred Days Congress into session to address the crumbling economy. From this Congress came the Agricultural Adjustment Administration to replace the Federal Farm Board. The Roosevelt Administration was tasked with decreasing agricultural surpluses. Wheat, cotton, field corn, hogs, rice, tobacco, and milk and its products were designated as basic commodities in the original legislation. Subsequent amendments in 1934 and 1935 expanded the list of basic commodities to include rye, flax, barley, grain sorghum, cattle, peanuts, sugar beets, sugar cane, and potatoes. The Administration targeted these commodities for the following reasons:
Goal and Implementation.
"The goal of the Agricultural Adjustment Act, restoring farm purchasing power of agricultural commodities or the fair exchange value of a commodity based upon price relative to the prewar 1909-14 level, was to be accomplished through a number of methods. These included the authorization by the Secretary of Agriculture (1) to secure voluntary reduction of the acreage in basic crops through agreements with producers and use of direct payments for participation in acreage control programs; (2) to regulate marketing through voluntary agreements with processors, associations or producers, and other handlers of agricultural commodities or products; (3) to license processors, association, and others handling agricultural commodities to eliminate unfair practices or charges; (4) to determine the necessity for and the rate or processing taxes; and (5) to use the proceeds of taxes and appropriate funds for the cost of adjustment operations, for the expansion of markets, and for the removal or agricultural surpluses."
"Congress declared its intent, at the same time, to protect the consumers interest. This was to be done by readjusting farm production at a level that would not increase the percentage of consumers' retail expenditures above the percentage returned to the farmer in the prewar base period."
The juxtaposition of huge agricultural surpluses and the many deaths due to insufficient food shocked many, as well as some of the administrative decisions that happened under the Agricultural Adjustment Act. For example, in an effort to reduce agricultural surpluses, the government paid farmers and ranchers hundreds of millions of dollars to destroy crops and livestock. Oranges were being soaked with kerosene to prevent their consumption and corn was being burned as fuel because it was so cheap. There were many people, however, as well as livestock in different places starving to death. Farmers slaughtered livestock because feed prices were rising, and they couldn’t afford to feed their own animals. Under the Agricultural Adjustment Act, “plowing under” of pigs was also common to prevent them reaching a reproductive age, as well as donating pigs to the Red Cross.
In 1935, the income generated by farms was 50 percent higher than it was in 1932, which was partly due to farm programs such as the AAA.
Tenant Farming.
Tenant farming characterized the cotton and tobacco production in the post-Civil War South. As the agricultural economy plummeted in the early 1930s, all farmers were badly hurt but the tenant farmers and sharecroppers experienced the worst of it.
To accomplish its goal of parity (raising crop prices to where they were in the golden years of 1909–1914), the Act reduced crop production. The Act accomplished this by offering landowners acreage reduction contracts, by which they agreed not to grow cotton on a portion of their land. By law, they were required to pay the tenant farmers and sharecroppers on their land a portion of the money; but after Southern Democrats in Congress complained, the Secretary of Agriculture surrendered and reinterpreted section 7 to no longer send checks to sharecroppers directly, hurting the tenants. The farm wage workers who worked directly for the landowner suffered the greatest unemployment as a result of the Act. There are few people gullible enough to believe that the acreage devoted to cotton can be reduced one-third without an accompanying decrease in the laborers engaged in its production. Researchers concluded that the statistics after the Act took effect "... indicate a consistent and widespread tendency for cotton croppers and, to a considerable extent, tenants to decrease in numbers between 1930 and 1935. The decreases among Negroes were consistently greater than those among whites." Another consequence was that the historic high levels of mobility from year to year declined sharply, as tenants and croppers tended to stay longer with the same landowner.
For most tenants and sharecroppers the AAA was a major help. Frey and Smith concluded, "To the extent that the AAA control-program has been responsible for the increased price [of cotton], we conclude that it has increased the amount of goods and services consumed by the cotton tenants and croppers area." Furthermore the landowners typically let the tenants and croppers use the land taken out of cotton production for their own personal use in growing food and feed crops, which further increased their standard of living. Another consequence was that the historic high levels of turnover from year to year declined sharply, as tenants and croppers tend to stay with the same landowner. Researchers concluded, "As a rule, planters seem to prefer Negroes to whites as tenants and croppers."
Delta and Providence Cooperative Farms in Mississippi and the Southern Tenant Farmers Union were organized in the 1930s principally as a response to the hardships imposed on sharecroppers and tenant farmers.
Although the Act stimulated American agriculture, it was not without its faults. For example, it disproportionately benefited large farmers and food processors, with lesser benefits to small farmers and sharecroppers. With the spread of cotton-picking machinery after 1945, there was an exodus of small farmers and croppers to the city.
Thomas Amendment.
Attached as Title III to the Act, the Thomas Amendment became the 'third horse' in the New Deal's farm relief bill. Drafted by Senator Elmer Thomas of Oklahoma, the amendment blended populist easy-money views with the theories of the New Economics. Thomas wanted a stabilized "honest dollar," one that would be fair to debtor and creditor alike.
The Amendment said that whenever the President desired currency expansion, he must first authorize the Federal Open Market Committee of the Federal Reserve to purchase up to $3 billion of federal obligations. Should open market operations prove insufficient, the President had several options. He could have the U.S. Treasury issue up to $3 billion in greenbacks, reduce the gold content of the dollar by as much as 50 percent, or accept 100 million dollars in silver at a price not to exceed fifty cents per ounce in payment of World War I debts owed by European nations.
The Thomas Amendment was used sparingly. The treasury received limited amounts of silver in payment for war debts from World War I. On 21 December 1933, Roosevelt ratified the London Agreement on Silver (adopted at the World Economic and Monetary Conference in London on 20 July 1933). At the same time, Roosevelt issued Proclamation 2067, ordering the United States mints to buy the entire domestic production of newly mined silver at 64.5¢ per ounce. "Roosevelt’s most dramatic use of the Thomas amendment" came on 31 January 1934, when he decreased the gold content of the dollar to 15 5/21 grains (0.98741 grams) .900 fine gold, or 59.06 per cent of the previous fixed content (25 8/10 grains, or 1.6718 grams). "However, wholesale prices still continued to climb. Possibly the most significant expansion brought on by the Thomas Amendment may have been the growth of governmental power over monetary policy.
The impact of this amendment was to reduce the amount of silver that was being held by private citizens (presumably as a hedge against inflation or collapse of the financial system) and increase the amount of circulating currency.
Ruled unconstitutional.
On January 6, 1936, the Supreme Court decided in "United States v. Butler" that the act was unconstitutional for levying this tax on the processors only to have it paid back to the farmers. Regulation of agriculture was deemed a state power. As such, the federal government could not force states to adopt the Agricultural Adjustment Act due to lack of jurisdiction. However, the Agricultural Adjustment Act of 1938 remedied these technical issues and the farm program continued.

</doc>
<doc id="55775" url="http://en.wikipedia.org/wiki?curid=55775" title="Reconstruction Finance Corporation">
Reconstruction Finance Corporation

The Reconstruction Finance Corporation (RFC) was a government corporation in the United States that operated between 1932 and 1957 which provided financial support to state and local governments and made loans to banks, railroads, mortgage associations and other businesses. Its aim was to boost the country’s confidence and help banks return to performing daily functions after the start of the Great Depression. It continued to operate through the New Deal where it became more prominent and through World War II. It was disbanded in 1957 when the US government felt it no longer needed to stimulate lending.
It was an independent agency of the United States government, established and chartered by the US Congress in 1932, Act of January 22, 1932, c. 8, 47 Stat. 5, during the administration of President Herbert Hoover. When Eugene Meyer became Governor of the Federal Reserve Board, he had suggested creating the RFC. It was modeled after the War Finance Corporation of World War I. The agency gave $2 billion in aid to state and local governments and made a large number of loans which were nearly all repaid. The RFC was created to solve the problem that the Federal Reserve could not fix by itself since they had some limitations. The Federal Reserve System was created in 1913 to act as a lender of last resort during financial panics but was not able to lend to every bank or firm.
The RFC continued under the New Deal and played a major role in recapitalizing banks. The Reconstruction Finance Corporation was effective at reducing the probability of bank failure and stimulating bank lending. The Reconstruction Finance Corporation played a major role in handling the Great Depression in the United States and setting up the relief programs that were taken over by the New Deal in 1933.
History.
The Federal Reserve’s mission was to act as a lender of last resort to banks during financial panics. There were other missions as well but the leaders were in conflict in how to itemize their missions. The Federal Reserve banks were not able to come up with a solution to which everyone agreed with and the Board did not have enough authority to mandate policies or act independently. Many board members in the Federal Reserve, Congress, and the public wanted the Federal Reserve to be more active during this time. Some things that were expected were to increase the amount of money in circulation and to liquidate all financial markets. The ones who opposed these ideas believed that policies that would allow this would end the contraction and/or eventually create high inflation. These things would hurt the economy in the future.
To solve this problem, the Reconstruction Finance Corporation (RFC) Act was drafted. Like the Federal Reserve, the RFC would loan to banks. Even though it was owned by the government, the staff consisted of individuals not part of the civil service system. It addressed the problem of state-chartered banks that did not join the Federal Reserve System. Small banks in rural cities were also not part of the Federal Reserve System.
The Reconstruction Finance Corporation initial money came from selling $500 million worth of stock bonds to the US Treasury. To obtain more capital, $1.5 billion bonds were sold again to the Treasury which was then sold to the general public. In the next couple of years the RFC would find itself needing a loan of $51.3 billion from the Treasury and from the public it needed $3.1 billion. A distinction of the RFC and the Federal Reserve was that the RFC would loan money and as collateral could accept they seemed acceptable. Loans would be available to solvent institutions that could not be sold to repay their current responsibilities but in the long-run would be able to accomplish this. A main reason why loans were given out was to relieve the depositors to get their money back. The Reconstruction Finance Corporation spent $1.5 billion in 1932, $1.8 billion in 1933, and $1.8 billion in 1934. Then it dropped to about $350 million a year. On the eve of World War II (August 31, 1939), it greatly expanded to build munitions factories, disbursing $1.8 billion in 1941. The total loaned or otherwise disbursed by the RFC from 1932 through 1941 was $9.465 billion.
Chairmen of the Board of Directors
Administrators and Deputy Administrators
RFC Under President Herbert Hoover.
The first RFC President was a former Vice President named Charles Dawes. He was not there for long and had to resign to attend to his bank in Chicago. Hoover then appointed Atlee Pomerene of Ohio to head the agency in July 1932. The Presidency of the RFC had switched from a republican (Dawes) to a former Democratic Senator. Hoover's reasons for his surprising reorganization of the RFC included: the broken health and resignations of Eugene Meyer, Paul Bestor, and Charles Gates Dawes; the failure of banks to perform their duties to their clientele or to aid American industry; the country's general lack of confidence in the current board; and Hoover's inability to find any other man who had the ability and was both nationally respected and available. (Shriver 1982)
The RFC was similar to the Federal Reserve in terms of how they were able to contribute to the contraction. The RFC’s purpose was to loan money to banks that couldn’t operate daily functions and like the Federal Reserve, they were influenced to bail out the ones that benefited the government the most. The RFC was bogged down in bureaucracy and failed to disburse much of its funds. It failed to reverse the growth of mass unemployment before 1933 though it was founded only in 1932. Butkiewicz (1995) shows that the RFC initially succeeded in reducing bank failures, but the publication of the names of the recipients of loans beginning in August 1932 (at the demand of Congress) significantly reduced the effectiveness of its loans to banks because it appeared that political considerations had motivated certain loans. Partisan politics thwarted the RFC's efforts, though in 1932 monetary conditions improved because the RFC slowed the decline in the money supply.
The original legislation was not limited to lending to banks and financial institutions, it could also provide loans for railroad construction and crop lands. An amendment that was passed on July 1932, allowed the RFC to provide loans to state and municipal governments. The purpose of these loans was to finance projects like dams and bridges which would be repaid by charging fees to use these structures. To help with unemployment, a relief was created which would be repaid by tax receipts.
RFC Under President Franklin D. Roosevelt.
Starting in 1933, President Franklin D. Roosevelt kept the agency, increased the funding, streamlined the bureaucracy, and used it to help restore business prosperity, especially in banking and railroads. He appointed Texas banker Jesse H. Jones as head, and Jones turned RFC into an empire with loans made in every state.
Under the New Deal the powers of the RFC were greatly expanded. They now purchased bank stock and extended their loans to agriculture, housing, exports, businesses, governments, and assisted in disaster relief. Roosevelt soon directed the RFC to buy gold to change its market price. The original legislation did not call for identities of the banks receiving loans nor of any reports to Congress. This, however, was changed in July 1932 to make the RFC transparent. Bankers soon were hesitant to ask the RFC for a loan since the public would become aware and begin to consider the possibility of their bank failing causing them to withdraw their deposits.
The RFC also had a division that gave the states loans for emergency relief needs. In a case study of Mississippi, Vogt (1985) examined two areas of RFC funding: aid to banking, which helped many Mississippi banks survive the economic crisis, and work relief, which Roosevelt used to pump money into the state's relief program by extending loans to businesses and local government projects. Although charges of political influence and racial discrimination were levied against RFC activities, the agency made positive contributions and established a federal agency in local communities which provided a reservoir of experienced personnel to implement expanding New Deal programs.
Roosevelt saw this corporation as an advantage to the government. The RFC could finance projects without Congress approving them and the loans would not be included in budget expenditures. Soon the RFC was able to buy bank preferred stock with the Emergency Banking Act of 1933. Buying stock would serve as collateral when banks needed loans. This, however, was somewhat controversial because if the RFC was a shareholder than it could interfere with salaries and bank management. The FDIC was later created to help decrease bank failures and insure bank deposits. The second main assistance was to farmers and their crop lands. The Commodity Credit Corporation was established to provide assistance. The agriculture was hit hard with a drought and machinery like the tractor. One great benefit it provided to these rural cities was the Electric Home and Farm Authority which provided electricity and gas and assistance in buying appliances to use these services.
The mortgage company was affected as well since families were not able to make their payments. This led the RFC to create its own mortgage company to sell and insure mortgages. The Federal National Mortgage Association (also known as Fannie Mae) was established and funded by the RFC. It later became a private corporation. An Export-Import Bank was also created to encourage trade with the Soviet Union. Another bank was established to fund trade with all other foreign nations a month later. They eventually merged and make loans available to exports. Roosevelt wanted to reduce the gold value of the US dollar. In order to accomplish this, the RFC purchased a lot of gold until a price floor was set.
World War II.
Even before World War II began the RFC’s power were expanded and further expanded during the war. President Roosevelt merged the RFC and the Federal Deposit Insurance Corporation (FDIC), which was one of the landmarks of the New Deal. Oscar Cox, a prime author of the Lend-Lease Act, general counsel of the Foreign Economic Administration joined as well. Lauchlin Currie, formerly of the Federal Reserve Board staff, was the deputy administrator to Leo Crowley.
 The RFC established eight new corporations, and purchased an existing corporation. The eight RFC wartime subsidiaries are Metals Reserve Company, Rubber Reserve Company, Defense Plant Corporation, Defense Supplies Corporation, War Damage Corporation, U.S. Commercial Company, Rubber Development Corporation, Petroleum Reserve Corporation. These corporations were involved in funding the development of synthetic rubber, construction and operation of a tin smelter, and establishment of abaca (Manila hemp) plantations in Central America. Both natural rubber and abaca (used to produce rope products) were produced primarily in south Asia, which came under Japanese control. Thus, these programs encouraged the development of alternative sources of supply of these essential materials. Synthetic rubber, which was not produced in the United States prior to the war, quickly became the primary source of rubber in the post-war years.
From 1941 through 1945, the RFC authorized over $2 billion of loans and investments each year, with a peak of over $6 billion authorized in 1943. The magnitude of RFC lending had increased substantially during the war. Most lending to wartime subsidiaries ended in 1945, and all such lending ended in 1948.
The Petroleum Reserves Corporation was transferred to the Office of Economic Warfare, which was consolidated into the Foreign Economic Administration, which was transferred to the Reconstruction Finance Corporation and changed to the War Assets Corporation. The War Assets Corporation was dissolved as soon as practicable after March 25, 1946.
World War II aircraft disposal.
After the war the Reconstruction Finance Corporation established five large storage, sales and scrapping centers for Army Air Forces aircraft. These were located at: Albuquerque AAF, New Mexico, Altus AAF, Oklahoma, Kingman AAF, Arizona, Ontario AAF, California and Walnut Ridge AAF, Arkansas. A sixth facility for storing, selling and scrapping Navy and Marine aircraft was located at Clinton, Oklahoma.
Estimates of the number of excess surplus airplanes ran as high as 150,000. Consideration was given to storing a substantial number of these. By the summer of 1945, at least 30 sales-storage depots and 23 sales centers were in operation. In November 1945, it was estimated a total of 117,210 aircraft would be transferred as surplus.
Between 1945 and June 1947, the RFC, War Assets Corporation and the War Assets Administration (disposal function of the RFC was transferred to WAC on January 15, 1946, and to the WAA in March 1946) processed approximately 61,600 World War II aircraft, of which 34,700 were sold for flyable purposes and 26,900, primarily combat types, were sold for scrapping.
Most of the transports and trainers could be used in the civil fleet, and trainers were sold for $875 to $2,400. The fighters and bombers were of little peacetime use, although some were sold. Typical prices for surplus aircraft were:
Many aircraft were transferred to schools, and to communities for memorial use for a minimal fee. A Boy Scout troop bought a B-17 for $350.
General sales were conducted from these centers; however, the idea for long term storage, considering the approximate cost of $20 per month per aircraft, was soon discarded, and in June 1946, the remaining aircraft, except those at Altus, were put up for scrap bid.
Disbanding.
After World War II ended, loans were not really needed. President Eisenhower was in office when legislation terminated the RFC. The RFC was "abolished as an independent agency by act of Congress (1953) and was transferred to the Department of the Treasury to wind up its affairs, effective June, 1954. It was totally disbanded in 1957." There was only one problem which was that this might hurt small businesses. To solve this problem the Small Business Administration was established to provide loans to small business and training programs were added. Government agencies took over RFC assets and the tin and abaca programs were handled by General Services Administration. The Commodity Credit Corporation which was created to help farmers was still in operation. Another establishment kept in operation is the Export-Import Bank of the United States to encourage exports.
In 1991, Rep. Jamie L. Whitten (Democrat of Mississippi) introduced a bill to reestablish the RFC, but it did not receive a hearing by a congressional committee and he did not reintroduce the bill in subsequent sessions.
Bibliography.
</dl>

</doc>
<doc id="55779" url="http://en.wikipedia.org/wiki?curid=55779" title="Works Progress Administration">
Works Progress Administration

The Works Progress Administration (renamed in 1939 as the Work Projects Administration; WPA) was the largest and most ambitious American New Deal agency, employing millions of unemployed people (mostly unskilled men) to carry out public works projects, including the construction of public buildings and roads. In a much smaller but more famous project, the Federal Project Number One, the WPA employed musicians, artists, writers, actors and directors in large arts, drama, media, and literacy projects.
Almost every community in the United States had a new park, bridge or school constructed by the agency. The WPA's initial appropriation in 1935 was for $4.9 billion (about 6.7 percent of the 1935 GDP), and in total it spent $13.4 billion.
At its peak in 1938, it provided paid jobs for three million unemployed men and women, as well as youth in a separate division, the National Youth Administration. Headed by Harry Hopkins, the WPA provided jobs and income to the unemployed during the Great Depression in the United States. Between 1935 and 1943, the WPA provided almost eight million jobs. Full employment, which was reached in 1942 and emerged as a long-term national goal around 1944, was not the WPA's goal. It tried to provide one paid job for all families in which the breadwinner suffered long-term unemployment. Robert D. Leighninger asserts that "The stated goal of public building programs was to end the depression or, at least, alleviate its worst effects. Millions of people needed subsistence incomes. Work relief was preferred over public assistance (the dole) because it maintained self-respect, reinforced the work ethic, and kept skills sharp."
The WPA was a national program that operated its own projects in cooperation with state and local governments, which provided 10–30% of the costs. Usually the local sponsor provided land and often trucks and supplies, with the WPA responsible for wages (and for the salaries of supervisors, who were not on relief). WPA sometimes took over state and local relief programs that had originated in the Reconstruction Finance Corporation (RFC) or Federal Emergency Relief Administration (FERA) programs.
It was liquidated on June 30, 1943, as a result of low unemployment due to the worker shortage of World War II. The WPA had provided millions of Americans with jobs for 8 years. Most people who needed a job were eligible for at least some of its positions. Hourly wages were typically set to the prevailing wages in each area.
Enacting the WPA.
Created by the order of President Franklin D. Roosevelt, the WPA was established with the passage of the Emergency Relief Appropriation Act of 1935 by the United States Congress and was largely shaped by Harry Hopkins, close adviser to President Roosevelt. The WPA was initially intended to be an extension of the Federal Emergency Relief Administration work program, which funded projects run by states and cities. Many were for infrastructure, such as bridges, roads and parks, but they also included archeological excavations of significant sites, the Historic American Buildings Survey (HABS), and other historic preservation activities. Both Roosevelt and Hopkins believed that the route to economic recovery and the lessened importance of "the dole" would be in employment programs such as the WPA.
Nick Taylor states that "These ordinary men and women proved to be extraordinary beyond all expectation. They were golden threads woven in national fabric. In this, they shamed the political philosophy that discounted their value and rewarded the one that placed its faith in them, thus fulfilling the founding vision of a government by and for its people. All its people."
Employment.
The goal of the WPA was to employ most of the unemployed people on relief until the economy recovered. Harry Hopkins testified to Congress in January 1935 why he set the number at 3.5 million, using Federal Emergency Relief Administration data. Estimating costs at $1200 per worker per year, he asked for and received $4 billion. Many women were employed, but they were few compared to men.
In 1935 there were 20 million people on relief in the United States. Of these, 8.3 million were children under sixteen years of age; 3.8 million were persons who, though between the ages of sixteen and sixty-five were not working nor seeking work. These included housewives, students in school, and incapacitated persons. Another 750,000 were persons sixty-five years of age or over. Thus, of the total of 20 million persons then receiving relief, 13 million were not considered eligible for employment. This left a total of 7 million presumably employable persons between the ages of sixteen and sixty-five inclusive. Of these, however, 1.65 million were said to be farm operators or persons who had some non-relief employment, while another 350,000 were, despite the fact that they were already employed or seeking work, considered incapacitated. Deducting this two million from the total of 7.15 million, there remained 5.15 million persons sixteen to sixty-five years of age, unemployed, looking for work, and able to work.
Because of the assumption that only one worker per family would be permitted to work under the proposed program, this total of 5.15 million was further reduced by 1.6 million—the estimated number of workers who were members of families which included two or more employable persons. Thus, there remained a net total of 3.55 million workers in as many households for whom jobs were to be provided.
The WPA employed a maximum of 3.3 million in November 1938. In order to be eligible for WPA employment, an individual had to be an American citizen who was 18 or older, able-bodied, unemployed, and certified as in need by a local public relief agency approved by the WPA. The WPA Division of Employment selected the worker's placement to WPA projects based on previous experience or training. Worker pay was based on three factors: the region of the country, the degree of urbanization, and the individual's skill. It varied from $19/month to $94/month with the average wage being about $52.50. The goal was to pay the local prevailing wage, but limit the hours of work to 8 hours a day or 40 hours a week; the stated minimum being 30 hours a week, or 120 hours a month.
Projects funded.
The WPA built traditional infrastructure of the New Deal such as roads, bridges, schools, courthouses, hospitals, sidewalks, waterworks, and post-offices, but also constructed museums, swimming pools, parks, community centers, playgrounds, coliseums, markets, fairgrounds, tennis courts, zoos, botanical gardens, auditoriums, waterfronts, city halls, gyms, and university unions. Most of these are still in use today. The amount of infrastructure projects of the WPA included 40,000 new and 85,000 improved buildings. These new buildings included 5,900 new schools; 9,300 new auditoriums, gyms, and recreational buildings; 1,000 new libraries; 7,000 new dormitories; and 900 new armories. In addition, infrastructure projects included 2,302 stadiums, grandstands, and bleachers; 52 fairgrounds and rodeo grounds; 1,686 parks covering 75,152 acres; 3,185 playgrounds; 3,026 athletic fields; 805 swimming pools; 1,817 handball courts; 10,070 tennis courts; 2,261 horseshoe pits; 1,101 ice-skating areas; 138 outdoor theatres; 254 golf courses; and 65 ski jumps. Total expenditures on WPA projects through June 1941, totaled approximately $11.4 billion. Over $4 billion was spent on highway, road, and street projects; more than $1 billion on public buildings, including the iconic Dock Street Theater in Charleston, the Griffith Observatory in Los Angeles, and the Timberline Lodge on Oregon's Mt. Hood.
More than $1 billion was spent on publicly owned or operated utilities; and another $1 billion on welfare projects, including sewing projects for women, the distribution of surplus commodities, and school lunch projects. One construction project was the Merritt Parkway in Connecticut, the bridges of which were each designed as architecturally unique. In its eight-year run, the WPA built 325 firehouses and renovated 2,384 of them across the United States. The 20,000 miles of water mains, installed by their hand as well, contributed to increased fire protection across the country.
The direct focus of the WPA projects changed with need. In 1935 priority projects were to improve infrastructure; roads, extension of electricity to rural areas, water conservation, sanitation and flood control. In 1936, as outlined in that year’s Emergency Relief Appropriations Act, public facilities became a focus; parks and associated facilities, public buildings, utilities, airports, and transportation projects were funded. The following year, saw the introduction of agricultural improvements, such as the production of marl fertilizer and the eradication of fungus pests. As the Second World War approached, and then eventually began, WPA projects became increasingly defense related.
One project of the WPA was funding state-level library service demonstration projects, which was intended to create new areas of library service to underserved populations and to extend rural service. Another project was the Household Service Demonstration Project, which trained 30,000 women for domestic employment. South Carolina had one of the larger state-wide library service demonstration projects. At the end of the project in 1943, South Carolina had twelve publicly funded county libraries, one regional library, and a funded state library agency.
Wyandotte County Lake, in Kansas City, Kansas was a part of the New Deal Act proposed by President Roosevelt. The construction of the lake was a way to employ residents while providing a method of water conservation for Wyandotte County. Construction on the lake started in 1936 and it was not fully complete until 1943. The WPA was hesitant to approve the project; in March, Frank Holcomb, Chairman of the County Commissioners, was opposed to any major additional expenses. However, as negotiations cleared the way, some work was restarted in the summer of 1938 on shop buildings, etc.
Federal Project No. 1.
A significant aspect of the Works Progress Administration was the Federal Project Number One, which had five different parts: the Federal Art Project, the Federal Music Project, the Federal Theatre Project, the Federal Writers Project, and the Historical Records Survey. The government wanted to provide new federal cultural support instead of just providing direct grants to private institutions. After only one year, over 40,000 artists and other talented workers had been employed through this project in the United States. Cedric Larson stated that "The impact made by the five major cultural projects of the WPA upon the national consciousness is probably greater in toto than anyone readily realizes. As channels of communication between the administration and the country at large, both directly and indirectly, the importance of these projects cannot be overestimated, for they all carry a tremendous appeal to the eye, the ear, or the intellect—or all three."
Federal Art Project.
This project was directed by Holger Cahill and in 1936, the peak for employment in this federal project, the Federal Art Project employed over 5,300 artists. The Arts Service Division created illustrations and posters for the WPA writers, musicians, and theaters. The Exhibition Division had public exhibitions of artwork from the WPA, and artists from the Art Teaching Division were employed in settlement houses and community centers to give classes to an estimated 50,000 children and adults. They set up over 100 art centers around the country that served an estimated eight million individuals. A few famous WPA artists include Philip Guston, Moses Soyer, Jackson Pollock, Mark Rothko, Jacob Lawrence, Ben Shahn, Ivan Albright, Marsden Hartley, Philip Evergood, Mark Tobey, Ralph Stackpole, Bernard Zakheim, John Bloom (husband of Isabel Bloom), and Grant Wood
Federal Music Project.
This project was directed by the former conductor for the Cleveland Orchestra, Nikolai Sokoloff, and employed over 16,000 musicians at its peak. Its purpose was to establish different ensembles such as chamber groups, orchestras, choral units, opera units, concert bands, military bands, dance bands, and theater orchestras that gave an estimated 131,000 performances and programs to 92 million people each week. The FMP performed plays and dances, as well as radio dramas. In addition, the FPM gave music classes to an estimated 132,000 children and adults every week, recorded folk music, served as copyists, arrangers, and librarians to expand the availability of music, and experimented in music therapy. Dr. Sokoloff stated that "Music can serve no useful purpose unless it is heard, but these totals on the listeners’ side are more eloquent than statistics as they show that in this country there is a great hunger and eagerness for music."
Federal Theatre Project.
This project was directed by Iowan Hallie Flanagan, and employed 12,700 performers at its peak. These performers presented more than 1,000 performances each month to almost one million people, produced 1,200 plays in the four years it was established, and introduced 100 new playwrights. Many performers later became successful in Hollywood including Orson Welles, John Houseman, Burt Lancaster, Joseph Cotten, Canada Lee, Will Geer, Joseph Losey, Virgil Thompson, Nicholas Ray, E.G. Marshall and Sidney Lumet. The Federal Theatre Project was the first project to end in June 1939 after four years from an end of funding from the federal government.
Federal Writers' Project.
This project was directed by Henry Alsberg and employed 6,686 writers at its peak in 1936. By January 1939, more than 275 major books and booklets had been published by the FWP. Most famously, the FWP created the American Guide Series, which produced thorough guidebooks for every state that include descriptions of towns, waterways, historic sites, oral histories, photographs, and artwork. An association or group that put up the cost of publication sponsored each book, the cost was anywhere from $5,000 to $10,000. In almost all cases, the book sales were able to reimburse their sponsors. Additionally, another important part of this project was to record oral histories to create archives such as the Slave Narratives and collections of folklore. These writers also participated in research and editorial services to other government agencies.
Historical Records Survey.
This project was the smallest of Federal Project Number One and served to identify, collect, and conserve United States’ historical records. It is one of the biggest bibliographical efforts and was directed by Dr. Luther H. Evans. At its peak, this project employed more than 4,400 workers.
Relief for African Americans.
The share of Federal Emergency Relief Administration (FERA) and WPA benefits for African Americans exceeded their proportion of the general population. The FERA's first relief census reported that more than two million African Americans were on relief during early 1933, a proportion of the African-American population (17.8%) that was nearly double the proportion of whites on relief (9.5%). This was during the period of Jim Crow and racial segregation in the South, when blacks were largely disenfranchised.
By 1935, there were 3,500,000 African Americans (men, women and children) on relief, almost 35 percent of the African-American population; plus another 250,000 African-American adults were working on WPA projects. Altogether during 1938, about 45 percent of the nation's African-American families were either on relief or were employed by the WPA.
Civil rights leaders initially objected that African Americans were proportionally underrepresented. African American leaders made such a claim with respect to WPA hires in New Jersey: "In spite of the fact that Blacks indubitably constitute more than 20 percent of the State's unemployed, they composed 15.9% of those assigned to W.P.A. jobs during 1937." Nationwide in 1940, 9.8% of the population were African American.
However, by 1941, the perception of discrimination against African Americans had changed to the point that the NAACP magazine "Opportunity" hailed the WPA, saying:
It is to the eternal credit of the administrative officers of the WPA that discrimination on various projects because of race has been kept to a minimum and that in almost every community Negroes have been given a chance to participate in the work program. In the South, as might have been expected, this participation has been limited, and differential wages on the basis of race have been more or less effectively established; but in the northern communities, particularly in the urban centers, the Negro has been afforded his first real opportunity for employment in white-collar occupations.
Women.
About 15% of the household heads on relief were women and youth programs were operated separately by the National Youth Administration (the NYA). The average worker was about 40 years old (about the same as the average family head on relief).
WPA policies were consistent with the strong belief of the time that husbands and wives should not both be working (because the second person working would take one job away from some other breadwinner). A study of 2,000 female workers in Philadelphia showed that 90% were married, but wives were reported as living with their husbands in only 18 percent of the cases. Only 2 percent of the husbands had private employment. Of the 2000 women, all were responsible for one to five additional people in the household.
In rural Missouri, 60% of the WPA-employed women were without husbands (12% were single; 25% widowed; and 23% divorced, separated or deserted). Thus, only 40% were married and living with their husbands, but 59% of the husbands were permanently disabled, 17% were temporarily disabled, 13% were too old to work, and remaining 10% were either unemployed or handicapped. Most of the women worked with sewing projects, where they were taught to use sewing machines and made clothing and bedding, as well as supplies for hospitals, orphanages, and adoption centers.
Criticism.
The WPA had numerous critics, especially from the right. The strongest attacks were that it was the prelude for a national political machine on behalf of Roosevelt. Reformers secured the Hatch Act of 1939 that largely depoliticized the WPA.
Others complained that far left elements played a major role, especially in the New York City unit (which was independent of the New York State unit). Representative Martin Dies, Jr. called the WPA a "seedbed for communists". Representative J. Parnell Thomas of the House Committee on Un-American Activities (HUAC) claimed in 1938 that divisions of the WPA were a "hotbed of Communists" and "one more link in the vast and unparalleled New Deal propaganda network."
Much of the criticism of the distribution of projects and funding allotment is a result of the view that the decisions were politically motivated. The South, as the poorest region of the United States, received 75 percent less in federal relief and public works funds per capita than the West. Critics would point to the fact that Roosevelt’s Democrats could be sure of voting support from the South, whereas the West was less of a sure thing; swing states took priority over the other states.
There was a perception that WPA employees were not diligent workers, and that they had little incentive to give up their busy work in favor of productive jobs. Employers said the "WPA is bad for people since it gives them poor work habits. They believe that even if a man is not an inefficient worker to begin with, he gets that way from being on WPA." Having been on the WPA made it harder for alumni to get a job because employers said they had "formed poor work habits" on the WPA.
A Senate committee reported that, "To some extent the complaint that WPA workers do poor work is not without foundation. ... Poor work habits and incorrect techniques are not remedied. Occasionally a supervisor or a foreman demands good work." The WPA and its workers were ridiculed as being lazy. The organization's initials were said to stand for "We Poke Along" or "We Putter Along" or "We piddle around" or "Whistle, Piss and Argue." These were sarcastic references to WPA projects that sometimes slowed down deliberately because foremen had an incentive to keep going, rather than finish a project.
New Deal officials reportedly took measures to prevent political corruption. In particular President Roosevelt created a "division of progress investigation" to investigate complaints of malfeasance.
Evolution, Termination, and Legacy.
When Harry Hopkins became the Secretary of Commerce, Colonel Francis Harrington became director of the project and the WPA was renamed from the "Works Progress Administration" to the "Works Projects Administration." As the projects became more subject to the state, local sponsors were called on to provide 25% of project costs. As the projects slowly diminished due to lack of funding, they became more dedicated to work that related directly to the war effort: "[The WPA] diverted resources from domestic construction to overseas destruction." Unemployment ended with war production for World War II, as millions of men joined the services, and cost-plus contracts made it attractive for companies to hire unemployed men and train them. With the mass-employment need essentially gone, Congress terminated the WPA in late 1943.
In regarding the Works Progress Administration's legacy, Robert Leighninger asserts that "The agencies of Franklin D. Roosevelt administration had an enormous and largely unrecognized role in defining the public space we now use. In a short period of ten years, the Public Works Administration, the Works Progress Administration, and the Civilian Conservation Corps built facilities in practically every community in the country. Most are still providing service half a century later. It is time we recognized this legacy and attempted to comprehend its relationship to our contemporary situation."
Popular Culture.
Other references to the WPA in popular culture include:
See also.
General:
External links.
WPA posters:
Libraries and the WPA:
WPA murals:

</doc>
<doc id="55783" url="http://en.wikipedia.org/wiki?curid=55783" title="Gramm–Rudman–Hollings Balanced Budget Act">
Gramm–Rudman–Hollings Balanced Budget Act

The Gramm-Rudman-Hollings Balanced Budget and Emergency Deficit Control Act of 1985 and the Balanced Budget and Emergency Deficit Control Reaffirmation Act of 1987 (both often known as Gramm-Rudman) were "the first binding spending constraints on the federal budget".
The Acts were named after Senators Phil Gramm (R-Texas), Warren Rudman (R-New Hampshire) and Ernest Hollings (D-South Carolina) who were their chief sponsors.
Provisions of Acts.
The term "budget sequestration" was first used to describe a section of the Gramm-Rudman-Hollings Deficit Reduction Act of 1985.
The Acts aimed to cut the United States federal budget deficit, which at the time, in dollar term, was the largest in history. The Acts provided for automatic spending cuts ("cancellation of budgetary resources", called "sequestration") if the total discretionary appropriations in various categories exceed in a fiscal year the budget spending thresholds. That is, if Congress enacts appropriation bills providing for discretionary outlays in each fiscal year that exceed the budget totals, unless Congress passes another budget resolution increasing the budget amount, an across-the-board spending cut in discretionary expenditure is automatically triggered on these categories, affecting all departments and programs by an equal percentage. The amount exceeding the limit is held back by the Treasury and not transferred to the agencies specified in the appropriation bills.
Under the 1985 Act, allowable deficit levels were calculated for the eventual elimination of the federal deficit. If the budget exceeded the allowable deficit, across-the-board cuts were required. Directors of the Office of Management and Budget (OMB) and the Congressional Budget Office (CBO) were required to report to the Comptroller General regarding their recommendations for how much must be cut. The Comptroller General then evaluated these reports, made his own conclusion, and gave a recommendation to the President, who was then required to issue an order effecting the reductions required by the Comptroller General unless Congress made the required cuts by other ways within a specified amount of time.
The Comptroller General is nominated by the President from a list of three people recommended by the presiding officers of the House and Senate. He is removable only by impeachment or a joint resolution of Congress, which requires majority votes in both houses and is subject to a Presidential veto. Congress can give a number of reasons for this removal, including "inefficiency," "neglect of duty," or "malfeasance."
Passage of law.
The House passed the 1985 bill by a vote of 271-154 and the Senate by 61-31, and President Ronald Reagan signed the bill on December 12, 1985.
On August 12, 1986, Representative Dan Rostenkowski introduced the Balanced Budget and Emergency Deficit Control Reaffirmation Act. The Senate passed the bill with two amendments by a vote of 36-35, and the House approved the Senate's first amendment by voice vote but rejected the second amendment. The Senate rescinded that amendment by voice vote and President Reagan signed the bill on August 21.
Legacy.
The process for determining the amount of the automatic cuts was found unconstitutional in the case of "Bowsher v. Synar," (478 U.S. (1986)) as an unconstitutional usurpation of executive power by Congress because the Comptroller General's function under the Act is the "very essence" of execution of the laws, which is beyond the power of a legislative body. It was noted: "Once Congress passes legislation, it can influence only its execution by passing new laws or through impeachment."
Congress enacted a reworked version of the law in the 1987 Act. Gramm-Rudman failed, however, to prevent large budget deficits.
The Budget Enforcement Act of 1990 supplanted the fixed deficit targets, which replaced sequestration with a PAYGO system, which was in effect until 2002.
Balanced budgets did not actually emerge until the late 1990s when budget surpluses (not accounting for liabilities to the Social Security Trust Fund) emerged. The budgets quickly fell out of balance after 2000 and have run consistent and substantial deficits since then.

</doc>
<doc id="55784" url="http://en.wikipedia.org/wiki?curid=55784" title="Gramm-Latta Budget">
Gramm-Latta Budget

The Gramm-Latta Budget (aka Gramm-Latta Bill) 1981 and the Gramm-Latta Omnibus Reconciliation Bill of 1981, sponsored by Representatives Phil Gramm (a Democrat from Texas) and Delbert Latta (a Republican from Ohio), implemented President Ronald Reagan's economic program. This included an increase in military spending and major cuts in discretionary and entitlement spending. The law also mandated the controversial 1981 Kemp-Roth Tax Cut.
In a 2001 press conference to announce his retirement, Gramm had this to say about the bill: 
I wrote the first Reagan budget -- the Gramm-Latta budget that rebuilt national defense and that laid the foundation for a program of peace through strength; the Reagan program that tore down the Berlin Wall, that liberated Eastern Europe, that transformed the Soviet Union and that changed the world.""

</doc>
<doc id="55786" url="http://en.wikipedia.org/wiki?curid=55786" title="Economic Recovery Tax Act of 1981">
Economic Recovery Tax Act of 1981

The Economic Recovery Tax Act of 1981 (), also known as the ERTA or "Kemp-Roth Tax Cut", was a federal law enacted in the United States in 1981. It was an act "to amend the Internal Revenue Code of 1954 to encourage economic growth through reductions in individual income tax rates, the expensing of depreciable property, incentives for small businesses, and incentives for savings, and for other purposes". Included in the act was an across-the-board decrease in the marginal income tax rates in the United States by 23% over three years, with the top rate falling from 70% to 50% and the bottom rate dropping from 14% to 11%. This act slashed estate taxes and trimmed taxes paid by business corporations by $150 billion over a five-year period. Additionally the tax rates were indexed for inflation, though the indexing was delayed until 1985.
The Act's Republican sponsors, Representative Jack Kemp of New York and Senator William V. Roth, Jr., of Delaware, had hoped for more significant tax cuts, but settled on this bill after a great debate in Congress. It passed Congress on August 4, 1981, and was signed into law on August 13, 1981, by President Ronald Reagan at "Rancho del Cielo", his California ranch.
Summary of provisions.
The Office of Tax Analysis of the United States Department of the Treasury summarized the tax changes as follows:
The accelerated depreciation changes were repealed by Tax Equity and Fiscal Responsibility Act of 1982 and the 15% interest exclusion repealed before it took effect by the Deficit Reduction Act of 1984.
Effect and controversies.
The most lasting impact and significant change of the Act was the indexing of the tax code parameters for inflation. Of the nine federal tax laws between 1968 and this Act, six were tax cuts compensating for inflation driven "bracket creep". Following enactment in August 1981, the first 5% of the 25% total cuts took place beginning in October of the same year. An additional 10% began in July 1982, followed by a third decrease of 10% beginning in July 1983.
As a result of ERTA and other tax acts in the 1980s, the top 10% were paying 57.2% of total income taxes by 1988—up from 48% in 1981—while the bottom 50% of earners share dropped from 7.5% to 5.7% in the same period. The total share borne by middle income earners of the 50th to 95th percentile decreased from 57.5% to 48.7% between 1981 and 1988. Much of the increase can be attributed to the decrease in capital gains taxes, while the ongoing recession and subsequently high unemployment contributed to stagnation among other income groups until the mid-1980s. Another explanation is any such across the board tax cut removes some from the tax rolls. Those remaining pay a higher percentage of a now smaller tax pie even though they pay less in absolute taxes.
In addition to changes in marginal tax rates, the capital gains tax was reduced from 28% to 20% under ERTA. Afterwards revenue from the capital gains tax increased 50% by 1983 from $12.5 billion in 1980 to over $18 billion in 1983. In 1986, revenue from the capital gains tax rose to over $80 billion; following restoration of the rate to 28% from 20% effective 1987, capital gains revenues declined through 1991.
Critics claim the tax cuts worsened the deficits in the budget of the United States government. Reagan supporters credit them with helping the 1980s economic expansion that eventually lowered the deficits. After peaking in 1986 at $221 billion the deficit fell to $152 billion by 1989. Supporters of the tax cuts also argue, using the Laffer curve, tax cuts increased economic growth and government revenue. This is hotly disputed—critics contend that, although government income tax receipts "did" rise, it was due to economic growth, not tax cuts, and would have risen more if the tax cuts had not occurred; the Office of Tax Analysis estimates that the act lowered federal income tax revenue by 13% relative to where it would have been in the bill's absence.

</doc>
<doc id="55789" url="http://en.wikipedia.org/wiki?curid=55789" title="Tax Reform Act of 1986">
Tax Reform Act of 1986

The U.S. Congress passed the Tax Reform Act of 1986 (TRA) (, 100 Stat. , enacted  22, 1986) to simplify the income tax code, broaden the tax base and eliminate many tax shelters. Referred to as the second of the two "Reagan tax cuts" (the Kemp-Roth Tax Cut of 1981 being the first), the bill was also officially sponsored by Democrats, Richard Gephardt of Missouri in the House of Representatives and Bill Bradley of New Jersey in the Senate.
The Tax Reform Act of 1986 was given impetus by a detailed tax-simplification proposal from President Reagan's Treasury Department, and was designed to be tax-revenue neutral because Reagan stated that he would veto any bill that was not. Revenue neutrality was targeted by decreasing individual income tax rates, eliminating $30 billion annually in loopholes, while increasing corporate taxes, capital gains taxes, and miscellaneous excises. The act raised overall revenue by $54.9 billion in the first fiscal year after enactment As of 2014, the Tax Reform Act of 1986 was the most recent major simplification of the tax code, drastically reducing the number of deductions and the number of tax brackets (for the individual income tax) to three.
Income tax rates.
The top tax rate for individuals was lowered from 50% to 28% while the bottom rate was raised from 11% to 15%. Many lower level tax brackets were consolidated, and the upper income level of the bottom rate (married filing jointly) was increased from $5,720/year to $29,750/year. This package ultimately consolidated tax brackets from fifteen levels of income to four levels of income. This would be the only time in the history of the U.S. income tax (which dates back to the passage of the Revenue Act of 1862) that the top rate was reduced and the bottom rate increased concomitantly. In addition, capital gains faced the same tax rate as ordinary income.
The rate structure also maintained a novel "bubble rate." The rates were not 15%/28%, as widely reported. Rather, the rates were 15%/28%/33%/28%. As a result, for taxpayers after a certain income level, TRA86 provided a flat tax of 28%. This was jettisoned in the Omnibus Budget Reconciliation Act of 1990, otherwise known as the "Bush tax increase", which violated his .
Tax incentives.
The Act also increased incentives favoring investment in owner-occupied housing relative to rental housing by increasing the Home Mortgage Interest Deduction. The imputed income an owner receives from an investment in owner-occupied housing has always escaped taxation, much like the imputed (estimated) income someone receives from doing his own cooking instead of hiring a chef, but the Act changed the treatment of imputed rent, local property taxes, and mortgage interest payments to favor homeownership, while phasing out many investment incentives for rental housing. To the extent that low-income people may be more likely to live in rental housing than in owner-occupied housing, this provision of the Act could have had the tendency to decrease the new supply of housing accessible to low-income people. The Low-Income Housing Tax Credit was added to the Act to provide some balance and encourage investment in multifamily housing for the poor.
Moreover, interest on consumer loans such as credit card debt was no longer deductible. An existing provision in the tax code, called Income Averaging, which reduced taxes for those only recently making a much higher salary than before, was eliminated (although later partially reinstated, for farmers in 1997 and for fishermen in 2004). The Act, however, increased the personal exemption and standard deduction.
The Individual Retirement Account (IRA) deduction was severely restricted. The IRA had been created as part of the Employee Retirement Income Security Act of 1974, where employees not covered by a pension plan could contribute the lesser of $1500 or 15% of earned income. The Economic Recovery Tax Act of 1981 (ERTA) removed the pension plan clause and raised the contribution limit to the lesser of $2000 or 100% of earned income. The 1986 Tax Reform Act retained the $2000 contribution limit, but restricted the deductibility for households that have pension plan coverage and have moderate to high incomes. Non-deductible contributions were allowed.
Depreciation deductions were also curtailed. Prior to ERTA, depreciation was based on "useful life" calculations provided by the Treasury Department. ERTA set up the "accelerated cost recovery system," or ACRS. This set up a series of useful lives based on 3 years for technical equipment, 5 years for non-technical office equipment, 10 years for industrial equipment, and 15 years for real property. TRA86 lengthened these lives, and lengthened them further for taxpayers covered by the alternative minimum tax (AMT). These latter, longer lives approximate "economic depreciation," a concept economists have used to determine the actual life of an asset relative to its economic value.
Defined contribution (DC) pension contributions were curtailed. The law prior to TRA86 was that DC pension limits were the lesser of 25% of compensation or $30,000. This could be accomplished by any combination of elective deferrals and profit sharing contributions. TRA86 introduced an elective deferral limit of $7000, indexed to inflation. Since the profit sharing percentage must be uniform for all employees, this had the intended result of making more equitable contributions to 401(k)'s and other types of DC pension plans.
Fraudulent dependents.
The act required people claiming children as dependents on their tax returns to obtain and list a Social Security number for every claimed child, to verify the child's existence. Before this act, parents claiming tax deductions were on the honor system not to lie about the number of children they supported. The requirement was phased in, and initially Social Security numbers were required only for children over the age of 5. During the first year, this anti-fraud change resulted in seven million fewer dependents being claimed. It is believed the disappearing dependents were either children that never existed, tax deductions improperly claimed by non-custodial parents, or pets.
Changes to the AMT.
The original Alternative Minimum Tax targeted tax shelters used by a few wealthy households. However, the Tax Reform Act of 1986 greatly expanded the AMT to aim at a different set of deductions that most Americans receive. Things like the personal exemption, state and local taxes, the standard deduction, private activity bond interest, certain expenses like union dues and even some medical costs for the seriously ill could now trigger the AMT. In 2007, the New York Times reported, "A law for untaxed rich investors was refocused on families who own their homes in high tax states."
Passive losses and tax shelters.
By enacting #redirect (relating to limitations on deductions for passive activity losses and limitations on passive activity credits) to remove many tax shelters, especially for real estate investments, the Act significantly decreased the value of many such investments which had been held more for their tax-advantaged status than for their inherent profitability. This may have contributed to the end of the real estate boom of the early-to-mid 1980s as well as to the savings and loan crisis.
Prior to 1986, much real estate investment was done by passive investors. It was common for syndicates of investors to pool their resources in order to invest in property, commercial or residential. They would then hire management companies to run the operation. TRA 86 reduced the value of these investments by limiting the extent to which losses associated with them could be deducted from the investor's gross income. This, in turn, encouraged the holders of loss-generating properties to try and unload them, which contributed further to the problem of sinking real estate values. This turmoil and repositioning in real estate markets was caused not by changes in market conditions.
Mortgages and similar real property loans constituted a significant portion of S&Ls' asset portfolios. Significant declines in the market value of real properties resulted in the erosion of the value of these institutions' major assets.
Some economists consider the net long-term effect of eliminating tax shelters and other distortions to be positive for the economy, by redirecting money to the most inherently profitable investments.
To help less-affluent landlords, TRA86 gave a $25,000 net rental loss deduction provided that the home was not personally used for the greater of 14 days or 10% of rental days, and AGI is less than $100,000 (pro-rated phase-out through $150,000).
Tax treatment of technical service firms employing certain professionals.
The Internal Revenue Code does not contain any definition or rules dealing with the issue of when a worker should be characterized for tax purposes as an employee, rather than as an independent contractor. The tax treatment depends on the application of (20) factors provided by common law, which varies by state.
Introduced by Senator Daniel Patrick Moynihan, Section 1706 added a subsection (d) to Section 530 of the Revenue Act of 1978, which removed "safe harbor" exception for independent contractor classification (which at the time avoided payroll taxes) for workers such as engineers, designers, drafters, computer professionals, and "similarly skilled" workers.
If the IRS determines that a third-party intermediary firm's worker previously treated as self-employed should have been classified as an employee, the IRS assesses substantial back taxes, penalties and interest on that third-party intermediary company, though not directly against the worker or the end client. It does not apply to individuals directly contracted to clients.
The change in the tax code was expected to offset tax revenue losses of other legislation Moynihan proposed that changed the law on foreign taxes of Americans working abroad. At least one firm simply adapted its business model to the new regulations. A 1991 Treasury Department study found that tax compliance for technology professionals was among the highest of all self-employed workers and that Section 1706 would raise no additional tax revenue and could possibly result in losses as self-employed workers did not receive as many tax-free benefits as employees.
In one report in 2010, Moynihan's initiative was labeled "a favor to IBM." A suicide note by software professional Joseph Stack, who flew his airplane into a building housing IRS offices in February 2010, blamed his problems on many factors, including the Section 1706 change in the tax law while even mentioning Senator Moynihan by name, though no intermediary firm is mentioned, and failure to file a return was admitted.
Name of the Internal Revenue Code.
Section 2(a) of the Act also officially changed the name of the Internal Revenue Code from the "Internal Revenue Code of 1954" to the "Internal Revenue Code of 1986". Although the Act made numerous amendments to the Code, it was not a substantial re-codification or reorganization of the overall structure of the Code.
Income inequality.
In 1984, the top one percent of income earners received 8.4% of national income, while in 1989 it increased to 13.5%. The effect of the 1986 reform on this shift has been subjected to several economic studies.

</doc>
