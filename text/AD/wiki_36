<doc id="48981" url="http://en.wikipedia.org/wiki?curid=48981" title="Ascomycota">
Ascomycota

Ascomycota is a division or phylum of the kingdom Fungi that, together with the Basidiomycota, form the subkingdom Dikarya. Its members are commonly known as the sac fungi or ascomycetes. They are the largest phylum of Fungi, with over 64,000 species. The defining feature of this fungal group is the "ascus" (from Greek: ἀσκός ("askos"), meaning "sac" or "wineskin"), a microscopic sexual structure in which nonmotile spores, called ascospores, are formed. However, some species of the Ascomycota are asexual, meaning that they do not have a sexual cycle and thus do not form asci or ascospores. Previously placed in the Deuteromycota along with asexual species from other fungal taxa, asexual (or anamorphic) ascomycetes are now identified and classified based on morphological or physiological similarities to ascus-bearing taxa, and by phylogenetic analyses of DNA sequences.
The ascomycetes are a monophyletic group, i.e. it contains all descendants of one common ancestor. This group is of particular relevance to humans as sources for medicinally important compounds, such as antibiotics and for making bread, alcoholic beverages, and cheese, but also as pathogens of humans and plants. Familiar examples of sac fungi include morels, truffles, brewer's yeast and baker's yeast, dead man's fingers, and cup fungi. The fungal symbionts in the majority of lichens (loosely termed "ascolichens") such as "Cladonia" belong to the Ascomycota. There are many plant-pathogenic ascomycetes, including apple scab, rice blast, the ergot fungi, black knot, and the powdery mildews. Several species of ascomycetes are biological model organisms in laboratory research. Most famously, "Neurospora crassa", several species of yeasts, and "Aspergillus" species are used in many genetics and cell biology studies. "Penicillium" species on cheeses and those producing antibiotics for treating bacterial infectious diseases are examples of taxa that belong to the Ascomycota.
Ascomycetes versus Ascomycota.
Before the recognition of the fungal kingdom, the sac fungi were considered to be a "class", not a "phylum". The original collective term for these taxa was "Ascomycetes", which was first coined in the 1800s for a rankless nonlichenized taxon that possessed asci. The names Ascomycota, Ascomycetes, and others with the same root are based upon the term "ascus". "Ascomycetes" was soon used to include lichenized taxa, and became the standard term, at the class level, for all ascus-bearing species, just as the term "Basidiomycetes" became used for their basidium-bearing counterparts. Elevation of the taxonomic rank of the Ascomycetes resulted in the names Ascomycetae, Ascomycotina, and finally Ascomycota. Together, the Ascomycota and the Basidiomycota form the subkingdom Dikarya. The more familiar term, Ascomycetes, is still loosely used, e.g. at fungal forays it is often said of a fungus, such as "Peziza", "It is an ascomycete, not a basidiomycete" in reference to their sexual reproductive mode. The terms are further abbreviated to "ascos" and "basidos", which are not officially sanctioned technical names.
Modern classification of Ascomycota.
There are three subphyla that are described and accepted:
Outdated taxon names.
Several outdated taxon names—based on morphological features—are still occasionally used for species of the Ascomycota. These include the following sexual (teleomorphic) groups, defined by the structures of their sexual fruiting bodies: the Discomycetes, which included all species forming apothecia; the Pyrenomycetes, which included all sac fungi that formed perithecia or pseudothecia, or any structure resembling these morphological structures; and the Plectomycetes, which included those species that form cleistothecia. Hemiascomycetes included the yeasts and yeast-like fungi that have now been placed into the Saccharomycotina or Taphrinomycotina, while the Euascomycetes included the remaining species of the Ascomycota, which are now in the Pezizomycotina, and the Neolecta, which are in the Taphrinomycotina.
Some ascomycetes do not reproduce sexually or are not known to produce asci and are therefore anamorphic species. Those anamorphs that produce conidia (mitospores) were previously described as Mitosporic Ascomycota. Some taxonomists placed this group into a separate artificial phylum, the Deuteromycota (or "Fungi Imperfecti"). Where recent molecular analyses have identified close relationships with ascus-bearing taxa, anamorphic species have been grouped into the Ascomycota, despite the absence of the defining ascus. Sexual and asexual isolates of the same species commonly carry different binomial species names, as, for example, "Aspergillus nidulans" and "Emericella nidulans", for asexual and sexual isolates, respectively, of the same species.
Species of the Deuteromycota were classified as Coelomycetes if they produced their conidia in minute flask- or saucer-shaped conidiomata, known technically as "pycnidia" and "acervuli". The Hyphomycetes were those species where the conidiophores ("i.e.", the hyphal structures that carry conidia-forming cells at the end) are free or loosely organized. They are mostly isolated but sometimes also appear as bundles of cells aligned in parallel (described as "synnematal") or as cushion-shaped masses (described as "sporodochial").
Morphology.
Most species grow as filamentous, microscopic structures called hyphae. Many interconnected hyphae form a mycelium, which—when visible to the naked eye (macroscopic)—is commonly called mold (or, in botanical terminology, thallus). During sexual reproduction, many Ascomycota typically produce large numbers of asci. The asci is often contained in a multicellular, occasionally readily visible fruiting structure, the ascocarp (also called an "ascoma"). Ascocarps come in a very large variety of shapes: cup-shaped, club-shaped, potato-like, spongy, seed-like, oozing and pimple-like, coral-like, nit-like, golf-ball-shaped, perforated tennis ball-like, cushion-shaped, plated and feathered in miniature (Laboulbeniales), microscopic classic Greek shield-shaped, stalked or sessile. They can appear solitary or clustered. Their texture can likewise be very variable, including fleshy, like charcoal (carbonaceous), leathery, rubbery, gelatinous, slimy, powdery, or cob-web-like. Ascocarps come in multiple colors such as red, orange, yellow, brown, black, or, more rarely, green or blue. Some ascomyceous fungi, such as "Saccharomyces cerevisiae", grow as single-celled yeasts, which—during sexual reproduction—develop into an ascus, and do not form fruiting bodies.
In lichenized species, the thallus of the fungus defines the shape of the symbiotic colony. Some dimorphic species, such as "Candida albicans", can switch between growth as single cells and as filamentous, multicellular hyphae. Other species are pleomorphic, exhibiting asexual (anamorphic) as well as a sexual (teleomorphic) growth forms.
Except for lichens, the non-reproductive (vegetative) mycelium of most ascomycetes is usually inconspicuous because it is commonly embedded in the substrate, such as soil, or grows on or inside a living host, and only the ascoma may be seen when fruiting. Pigmentation, such as melanin in hyphal walls, along with prolific growth on surfaces can result in visible mold colonies; examples include "Cladosporium" species, which form black spots on bathroom caulking and other moist areas. Many ascomycetes cause food spoilage, and, therefore, the pellicles or moldy layers that develop on jams, juices, and other foods are the mycelia of these species or occasionally Mucoromycotina and almost never Basidiomycota. Sooty molds that develop on plants, especially in the tropics are the thalli of many species.
Large masses of yeast cells, asci or ascus-like cells, or conidia can also form macroscopic structures. For example. "Pneumocystis" species can colonize lung cavities (visible in x-rays), causing a form of pneumonia. Asci of "Ascosphaera" fill honey bee larvae and pupae causing mummification with a chalk-like appearance, hence the name "chalkbrood". Yeasts for small colonies in vitro and in vivo, and excessive growth of "Candida" species in the mouth or vagina causes "thrush", a form of candidiasis.
The cell walls of the ascomycetes almost always contain chitin and β-glucans, and divisions within the hyphae, called "septa", are the internal boundaries of individual cells (or compartments). The cell wall and septa give stability and rigidity to the hyphae and may prevent loss of cytoplasm in case of local damage to cell wall and cell membrane. The septa commonly have a small opening in the center, which functions as a cytoplasmic connection between adjacent cells, also sometimes allowing cell-to-cell movement of nuclei within a hypha. Vegetative hyphae of most ascomycetes contain only one nucleus per cell ("uninucleate" hyphae), but multinucleate cells—especially in the apical regions of growing hyphae—can also be present.
Metabolism.
In common with other fungal phyla, the Ascomycota are heterotrophic organisms that require organic compounds as energy sources. These are obtained by feeding on a variety of organic substrates including dead matter, foodstuffs, or as symbionts in or on other living organisms. To obtain these nutrients from their surroundings, ascomycetous fungi secrete powerful digestive enzymes that break down organic substances into smaller molecules, which are then taken up into the cell. Many species live on dead plant material such as leaves, twigs, or logs. Several species colonize plants, animals, or other fungi as parasites or mutualistic symbionts and derive all their metabolic energy in form of nutrients from the tissues of their hosts.
Owing to their long evolutionary history, the Ascomycota have evolved the capacity to break down almost every organic substance. Unlike most organisms, they are able to use their own enzymes to digest plant biopolymers such as cellulose or lignin. Collagen, an abundant structural protein in animals, and keratin—a protein that forms hair and nails—, can also serve as food sources. Unusual examples include "Aureobasidium pullulans", which feeds on wall paint, and the kerosene fungus "Amorphotheca resinae", which feeds on aircraft fuel (causing occasional problems for the airline industry), and may sometimes block fuel pipes. Other species can resist high osmotic stress and grow, for example, on salted fish, and a few ascomycetes are aquatic.
The Ascomycota is characterized by a high degree of specialization; for instance, certain species of Laboulbeniales attack only one particular leg of one particular insect species. Many Ascomycota engage in symbiotic relationships such as in lichens—symbiotic associations with green algae or cyanobacteria—in which the fungal symbiont directly obtains products of photosynthesis. In common with many basidiomycetes and Glomeromycota, some ascomycetes form symbioses with plants by colonizing the roots to form mycorrhizal associations. The Ascomycota also represents several carnivorous fungi, which have developed hyphal traps to capture small protists such as amoebae, as well as roundworms ("Nematoda"), rotifers, tardigrades, and small arthropods such as springtails ("Collembola").
Distribution and living environment.
The Ascomycota are represented in all land ecosystems worldwide, occurring on all continents including Antarctica. Spores and hyphal fragments are dispersed through the atmosphere and freshwater environments, as well as ocean beaches and tidal zones. The distribution of species is variable; while some are found on all continents, others, as for example the white truffle "Tuber magnatum", only occur in isolated locations in Italy and Eastern Europe. The distribution of plant-parasitic species is often restricted by host distributions; for example, "Cyttaria" is only found on "Nothofagus" (Southern Beech) in the Southern Hemisphere.
Reproduction.
Asexual reproduction.
Asexual reproduction is the dominant form of propagation in the Ascomycota, and is responsible for the rapid spread of these fungi into new areas. It occurs through vegetative reproductive spores, the conidia. The conidiospores commonly contain one nucleus and are products of mitotic cell divisions and thus are sometimes call mitospores, which are genetically identical to the mycelium from which they originate. They are typically formed at the ends of specialized hyphae, the "conidiophores". Depending on the species they may be dispersed by wind or water, or by animals.
Asexual spores.
Different types of asexual spores can be identified by colour, shape, and how they are released as individual spores. Spore types can be used as taxonomic characters in the classification within the Ascomycota. The most frequent types are the single-celled spores, which are designated "amerospores". If the spore is divided into two by a cross-wall (septum), it is called a "didymospore".
When there are two or more cross-walls, the classification depends on spore shape. If the septae are "transversal", like the rungs of a ladder, it is a "phragmospore", and if they possess a net-like structure it is a "dictyospore". In "staurospores" ray-like arms radiate from a central body; in others ("helicospores") the entire spore is wound up in a spiral like a spring. Very long worm-like spores with a length-to-diameter ratio of more than 15:1, are called "scolecospores".
Conidiogenesis and dehiscence.
Important characteristics of the anamorphs of the Ascomycota are "conidiogenesis", which includes spore formation and dehiscence (separation from the parent structure). Conidiogenesis corresponds to Embryology in animals and plants and can be divided into two fundamental forms of development: "blastic" conidiogenesis, where the spore is already evident before it separates from the conidiogenic hypha, and "thallic" conidiogenesis, during which a cross-wall forms and the newly created cell develops into a spore. The spores may or may not be generated in a large-scale specialized structure that helps to spread them.
These two basic types can be further classified as follows:
Sometimes the conidia are produced in structures visible to the naked eye, which help to distribute the spores. These structures are called "conidiomata" (singular: conidioma), and may take the form of "pycnidia" (which are flask-shaped and arise in the fungal tissue) or "acervuli" (which are cushion-shaped and arise in host tissue).
Dehiscence happens in two ways. In "schizolytic" dehiscence, a double-dividing wall with a central lamella (layer) forms between the cells; the central layer then breaks down thereby releasing the spores. In "rhexolytic" dehiscence, the cell wall that joins the spores on the outside degenerates and releases the conidia.
Heterokaryosis and parasexuality.
Several Ascomycota species are not known to have a sexual cycle. Such asexual species may be able to undergo genetic recombination between individuals by processes involving "heterokaryosis" and "parasexual" events.
 refers to the process of heterokaryosis, caused by merging of two hyphae belonging to different individuals, by a process called "anastomosis", followed by a series of events resulting in genetically different cell nuclei in the mycelium.
The merging of nuclei is not followed by meiotic events, such as gamete formation and results in an increased number of chromosomes per nuclei. "Mitotic crossover" may enable recombination, i.e., an exchange of genetic material between homologous chromosomes. The chromosome number may then be restored to its haploid state by nuclear division, with each daughter nuclei being genetically different from the original parent nuclei. Alternatively, nuclei may lose some chromosomes, resulting in aneuploid cells. "Candida albicans" (class Saccharomycetes) is an example of a fungus that has a parasexual cycle (see Candida albicans and Parasexual cycle).
Sexual reproduction.
Sexual reproduction in the Ascomycota leads to the formation of the "ascus", the structure that defines this fungal group and distinguishes it from other fungal phyla. The ascus is a tube-shaped vessel, a "meiosporangium", which contains the sexual spores produced by meiosis and which are called "ascospores".
Apart from a few exceptions, such as "Candida albicans", most ascomycetes are haploid, i.e., they contain one set of chromosomes per nuclei. During sexual reproduction there is a diploid phase, which commonly is very short, and meiosis restores the haploid state. The sexual cycle of one well-studied representative species of Ascomycota is described in greater detail in Neurospora crassa.
Formation of sexual spores.
The sexual part of the life cycle commences when two hyphal structures mate. In the case of "homothallic" species, mating is enabled between hyphae of the same fungal clone, whereas in "heterothallic" species, the two hyphae must originate from fungal clones that differ genetically, i.e., those that are of a different mating type. Mating types are typical of the fungi and correspond roughly to the sexes in plants and animals; however one species may have more than two mating types, resulting in sometimes complex vegetative incompatibility systems. The adaptive function of mating type is discussed in Neurospora crassa.
Gametangia are sexual structures formed from hyphae, and are the generative cells. A very fine hypha, called trichogyne emerges from one gametangium, the "ascogonium", and merges with a gametangium (the "antheridium") of the other fungal isolate. The nuclei in the antheridium then migrate into the ascogonium, and plasmogamy—the mixing of the cytoplasm—occurs. Unlike in animals and plants, plasmogamy is not immediately followed by the merging of the nuclei (called "karyogamy"). Instead, the nuclei from the two hyphae form pairs, initiating the "dikaryophase" of the sexual cycle, during which time the pairs of nuclei synchronously divide. Fusion of the paired nuclei leads to mixing of the genetic material and recombination and is followed by meiosis. A similar sexual cycle is present in the red algae (Rhodophyta).
From the fertilized ascogonium, "dinucleate" hyphae emerge in which each cell contains two nuclei. These hyphae are called "ascogenous" or fertile hyphae. They are supported by the vegetative mycelium containing uni– (or mono–) nucleate hyphae, which are sterile. The mycelium containing both sterile and fertile hyphae may grow into fruiting body, the "ascocarp", which may contain millions of fertile hyphae.
The sexual structures are formed in the fruiting layer of the ascocarp, the hymenium. At one end of ascogenous hyphae, characteristic U-shaped hooks develop, which curve back opposite to the growth direction of the hyphae. The two nuclei contained in the apical part of each hypha divide in such a way that the threads of their mitotic spindles run parallel, creating two pairs of genetically different nuclei. One daughter nucleus migrates close to the hook, while the other daughter nucleus locates to the basal part of the hypha. The formation of two parallel cross-walls then divides the hypha into three sections: one at the hook with one nucleus, one at the basal of the original hypha that contains one nucleus, and one that separates the U-shaped part, which contains the other two nuclei.
Fusion of the nuclei (karyogamy) takes place in the U-shaped cells in the hymenium, and results in the formation of a diploid zygote. The zygote grows into the ascus, an elongated tube-shaped or cylinder-shaped capsule. Meiosis then gives rise to four haploid nuclei, usually followed by a further mitotic division that results in eight nuclei in each ascus. The nuclei along with some cytoplasma become enclosed within membranes and a cell wall to give rise to ascospores that are aligned inside the ascus like peas in a pod. (For a general description of meiosis and its adaptive function see Meiosis and Bernstein and Bernstein).
Upon opening of the ascus, ascospores may be dispersed by the wind, while in some cases the spores are forcibly ejected form the ascus; certain species have evolved spore cannons, which can eject ascospores up to 30 cm. away. When the spores reach a suitable substrate, they germinate, form new hyphae, which restarts the fungal life cycle.
The form of the ascus is important for classification and is divided into four basic types: unitunicate-operculate, unitunicate-inoperculate, bitunicate, or prototunicate. See the article on asci for further details.
Ecology.
The Ascomycota fulfil a central role in most land-based ecosystems. They are important decomposers, breaking down organic materials, such as dead leaves and animals, and helping the detritivores (animals that feed on decomposing material) to obtain their nutrients. Ascomycetes along with other fungi can break down large molecules such as cellulose or lignin, and thus have important roles in nutrient cycling such as the carbon cycle.
The fruiting bodies of the Ascomycota provide food for many animals ranging from insects and slugs and snails ("Gastropoda") to rodents and larger mammals such as deer and wild boars.
Many ascomycetes also form symbiotic relationships with other organisms, including plants and animals.
Lichens.
Probably since early in their evolutionary history, the Ascomycota have formed symbiotic associations with green algae ("Chlorophyta"), and other types of algae and cyanobacteria. These mutualistic associations are commonly known as lichens, and can grow and persist in terrestrial regions of the earth that are inhospitable to other organisms and characterized by extremes in temperature and humidity, including the Arctic, the Antarctic, deserts, and mountaintops. While the photoautotrophic algal partner generates metabolic energy through photosynthesis, the fungus offers a stable, supportive matrix and protects cells from radiation and dehydration. Around 42% of the Ascomycota (about 18,000 species) form lichens, and almost all the fungal partners of lichens belong to the Ascomycota.
Mycorrhizal fungi and endophytes.
Members of the Ascomycota form two important types of relationship with plants: as mycorrhizal fungi and as endophytes. Mycorrhiza are symbiotic associations of fungi with the root systems of the plants, which can be of vital importance for growth and persistence for the plant. The fine mycelial network of the fungus enables the increased uptake of mineral salts that occur at low levels in the soil. In return, the plant provides the fungus with metabolic energy in the form of photosynthetic products. 
Endophytic fungi live inside plants, and those that form mutualistic or commensal associations with their host, do not damage their hosts. The exact nature of the relationship between endophytic fungus and host depends on the species involved, and in some cases fungal colonization of plants can bestow a higher resistance against insects, roundworms (nematodes), and bacteria; in the case of grass endophytes the fungal symbiont produces poisonous alkaloids, which can affect the health of plant-eating (herbivorous) mammals and deter or kill insect herbivores.
Symbiotic relationships with animals.
Several ascomycetes of the genus "Xylaria" colonize the nests of leafcutter ants and other fungus-growing ants of the tribe Attini, and the fungal gardens of termites (Isoptera). Since they do not generate fruiting bodies until the insects have left the nests, it is suspected that, as confirmed in several cases of Basidiomycota species, they may be cultivated.
Bark beetles (family Scolytidae) are important symbiotic partners of ascomycetes. The female beetles transport fungal spores to new hosts in characteristic tucks in their skin, the "mycetangia". The beetle tunnels into the wood and into large chambers in which they lay their eggs. Spores released from the mycetangia germinate into hyphae, which can break down the wood. The beetle larvae then feed on the fungal mycelium, and, on reaching maturity, carry new spores with them to renew the cycle of infection. A well-known example of this is Dutch elm disease, caused by "Ophiostoma ulmi", which is carried by the European elm bark beetle, "Scolytus multistriatus".
Importance for humans.
Ascomycetes make many contributions to the good of humanity, and also have many ill effects.
Harmful interactions.
One of their most harmful roles is as the agent of many plant diseases. For instance:
Positive effects.
On the other hand, ascus fungi have brought some important benefits to humanity.

</doc>
<doc id="48985" url="http://en.wikipedia.org/wiki?curid=48985" title="History of anatomy">
History of anatomy

The history of anatomy extends from the earliest examinations of sacrificial victims to the sophisticated analyses of the body performed by modern scientists. It has been characterized, over time, by a continually developing understanding of the functions of organs and structures in the body. Human anatomy was the most prominent of the biological sciences of the 19th and early 20th centuries. Methods have also improved dramatically.
Ancient anatomy.
Egypt.
The study of anatomy begins at least as early as 1600 BC, the date of the Edwin Smith Surgical Papyrus. This treatise shows that the heart, its vessels, liver, spleen, kidneys, hypothalamus, uterus and bladder were recognized, and that the blood vessels were known to emanate from the heart. Other vessels are described, some carrying air, some mucus, and two to the right ear are said to carry the "breath of life", while two to the left ear the "breath of death".The Ebers Papyrus (c. 1550 BC) features a treatise on the heart. It notes that the heart is the center of blood supply, and attached to it are vessels for every member of the body. The Egyptians seem to have known little about the function of the kidneys and made the heart the meeting point of a number of vessels which carried all the fluids of the body – blood, tears, urine and semen. However, they did not have a theory as to where saliva and sweat came from.
Greek Advances in Anatomy.
Nomenclature, methods and applications for the study of anatomy all date back to the Greeks. The early scientist Alcmaeon began to construct a background for medical and anatomical science with the dissection of animals. He identified the optic nerves and the tubes later termed the Eustachius. Others such as Acron (480 BC), Pausanias (480 BC), and Philistion of Locri made investigations into anatomy. One important figure during this time was Empedocles (480B.C.) who viewed the blood as the "innate heat" which he acquired from previous folklore. He also argued that the heart was the chief organ of both the vascular system and the pneuma (this could refer to either breath or soul; it was considered to be distributed by the blood vessels).
Many medical texts by various authors are collected in the "Hippocratic Corpus", none of which can definitely be ascribed to Hippocrates himself. The texts show an understanding of musculoskeletal structure, and the beginnings of understanding of the function of certain organs, such as the kidneys. The tricuspid valve of the heart and its function is documented in the treatise "On the Heart".
In the 4th century BCE, Aristotle and several contemporaries produced a more empirically founded system, based on animal dissection. Through his work with animal dissections and evolutionary biology, Aristotle founded comparative anatomy. Around this time, Praxagoras is credited as the first to identify the difference between arteries and veins, and the relations between organs are described more accurately than in previous works.
The first recorded school of anatomy was in Alexandria from about 300 to 2nd century BC. Ptolemy I Soter was the first to allow for medical officials to cut open and examine dead bodies for the purposes of learning how human bodies operated. On some occasions King Ptolemy even took part in these dissections. Most of the early dissections were done on executed criminals. The first use of human cadavers for anatomical research occurred later in the 4th century BCE when Herophilos and Erasistratus gained permission to perform live dissections, or vivisection, on criminals in Alexandria under the auspices of the Ptolemaic dynasty. Herophilos in particular developed a body of anatomical knowledge much more informed by the actual structure of the human body than previous works had been. Herophilos was the first physician to dissect human bodies and is considered to be the founder of Anatomy. He reversed the longstanding notion made by Aristotle that the heart was the "seat of intelligence." He argued instead that this seat was the brain. However, Herophilos was eventually accused by his contemporaries of dissecting live criminals. The number of victims is said to be around 600 prisoners.
From ancient to medieval.
Galen.
The final major anatomist of ancient times was Galen, active in the 2nd century. He compiled much of the knowledge obtained by previous writers, and furthered the inquiry into the function of organs by performing vivisection on animals. Due to a lack of readily available human specimens, discoveries through animal dissection were broadly applied to human anatomy as well. Galen served as chief physician to the gladiators in Pergamum (AD 158). Through his position with the gladiators, Galen was able to study all kinds of wounds without performing any actual human dissection. By default, Galen was able to view much of the abdominal cavity. His study on pigs and apes, however, gave him more detailed information about the organs and provided the basis for his medical tracts. Around 100 of these tracts survive and fill 22 volumes of modern text. His two great anatomical works are "On anatomical procedure" and "On the uses of the parts of the body of man". The information in these tracts became the foundation of authority for all medical writers and physicians for the next 1300 years until they were challenged by Vesalius and Harvey in the 16th century.
It was through his experiments that Galen was able to overturn many long-held beliefs, such as the theory that the arteries contained air which carried it to all parts of the body from the heart and the lungs. This belief was based originally on the arteries of dead animals, which appeared to be empty. Galen was able to demonstrate that living arteries contain blood, but in his error, which became the established medical orthodoxy for centuries, was to assume that the blood goes back and forth from the heart in an ebb-and-flow motion.
Early modern anatomy.
In 1275-1326 Mondino de Luzzi "Mundinus" carried out the first systematic human dissections since Herophilus of Chalcedon and Erasistratus of Ceos 1500 years earlier. The first major development in anatomy in Christian Europe since the fall of Rome occurred at Bologna, where anatomists dissected cadavers and contributed to the accurate description of organs and the identification of their functions. Following de Liuzzi's early studies, fifteenth century anatomists included Alessandro Achillini and Antonio Benivieni Pathological anatomy
Leonardo da Vinci.
Leonardo da Vinci (1452–1519) was trained in anatomy by Andrea del Verrocchio. In 1489 Leonardo began a series of anatomical drawings depicting the ideal human form. This work was carried out intermittently for over 2 decades. During this time he made use of his anatomical knowledge in his artwork, making many sketches of skeletal structures, muscles and organs of humans and other vertebrates that he dissected. Initially adopting an Aristotlean understanding of anatomy, he later studied Galen and adopted a more empirical approach, eventually abandoning Galen altogether and relying entirely on his own direct observation. His surviving 750 drawings represent groundbreaking studies in anatomy. Leonardo dissected around thirty human specimens until he was forced to stop under order of Pope Leo X.
As an artist-anatomist, Leonardo made many important discoveries, and had intended to publish a comprehensive treatise on human anatomy. For instance, he produced the first accurate depiction of the human spine, while his notes documenting his dissection of the Florentine centenarian contain the earliest known description of cirrhosis of the liver and arteriosclerosis. He was the first to develop drawing techniques in anatomy to convey information using cross-sections and multiple angles, although centuries would pass before anatomical drawings became accepted as crucial for learning anatomy. None of Leonardo's Notebooks were published during his lifetime, many being lost after his death, with the result that his anatomical discoveries remained unknown until they were later found and published centuries after his death.
Vesalius.
The Galenic doctrine in Europe was first seriously challenged in the 16th century. Thanks to the printing press, all over Europe a collective effort proceeded to circulate the works of Galen and later publish criticisms on their works. Andreas Vesalius, born and educated in Belgium, contributed the most to human anatomy. Vesalius was the first to publish a treatise, "De humani corporis fabrica", that challenged Galen "drawing for drawing." These drawings were a detailed series of explanations and vivid drawings of the anatomical parts of human bodies. Vesalius traveled all the way from Leuven to Padua for permission to dissect victims from the gallows without fear of persecution. His superbly executed drawings are triumphant descriptions of the differences between dogs and humans, but it took a century for Galen's influence to fade. His work led to anatomy marked a new era in the study of anatomy and its relation to medicine. Under Vesalius, anatomy became an actual discipline. “His skill in and attention to dissection featured prominently in his publications as well as his demonstrations, in his research as well as his teaching." 
In 1540, Vesalius gave a public demonstration of the inaccuracies of Galen's anatomical theories, which are still the orthodoxy of the medical profession. Vesalius now has on display, for comparison purposes, the skeletons of a human being alongside that of an ape of which he was able to show, that in many cases, Galen's observations were indeed correct for the ape, but bear little relation to man. Clearly what was needed was a new account of human anatomy. While the lecturer explained human anatomy, as revealed by Galen more than 1000 years earlier, an assistant pointed to the equivalent details on a dissected corpse. At times, the assistant was unable to find the organ as described, but invariably the corpse rather than Galen was held to be in error. Vesalius then decided that he will dissect corpses himself and trust to the evidence of what he found. His approach was highly controversial, but his evident skill led to his appointment as professor of surgery and anatomy at the University of Padua.
A succession of researchers proceeded to refine the body of anatomical knowledge, giving their names to a number of anatomical structures along the way. The 16th and 17th centuries also witnessed significant advances in the understanding of the circulatory system, as the purpose of valves in veins was identified, the left-to-right ventricle flow of blood through the circulatory system was described, and the hepatic veins were identified as a separate portion of the circulatory system. The lymphatic system was also identified as a separate system at this time.
17th and 18th centuries.
The study of anatomy flourished in the 17th and 18th centuries. The advent of the printing press facilitated the exchange of ideas. Because the study of anatomy concerned observation and drawings, the popularity of the anatomist was equal to the quality of his drawing talents, and one need not be an expert in Latin to take part. Many famous artists studied anatomy, attended dissections, and published drawings for money, from Michelangelo to Rembrandt. For the first time, prominent universities could teach something about anatomy through drawings, rather than relying on knowledge of Latin. Contrary to popular belief, the Church neither objected to nor obstructed anatomical research.
Only certified anatomists were allowed to perform dissections, and sometimes then only yearly. These dissections were sponsored by the city councilors and often charged an admission fee, rather like a circus act for scholars. Many European cities, such as Amsterdam, London, Copenhagen, Padua, and Paris, all had Royal anatomists (or some such office) tied to local government. Indeed, Nicolaes Tulp was Mayor of Amsterdam for three terms. Though it was a risky business to perform dissections, and unpredictable depending on the availability of fresh bodies, "attending" dissections was legal.
To cope with shortages of cadavers and the rise in medical students during the 17th and 18th centuries, body-snatching and even anatomy murder were practiced to obtain cadavers. 'Body snatching' was the act of sneaking into a graveyard, digging up a corpse and using it for study. Men known as 'resurrectionists' emerged as outside parties, who would steal corpses for a living and sell the bodies to anatomy schools. The leading London anatomist John Hunter paid for a regular supply of corpses for his anatomy school. The British Parliament passed the Anatomy Act 1832, which finally provided for an adequate and legitimate supply of corpses by allowing legal dissection of executed murderers. The view of anatomist at the time, however, became similar to that of an executioner. Having one's body dissected was seen as a punishment worse than death, "if you stole a pig, you were hung. If you killed a man, you were hung and then dissected." Demand grew so great that some anatomist resorted to dissecting their own family members (William Harvey dissected his own father and sister) as well as robbing bodies from their graves.
Many Europeans interested in the study of anatomy traveled to Italy, then the centre of anatomy. Only in Italy could certain important research methods be used, such as dissections on women. Realdo Colombo (also known as Realdus Columbus) and Gabriele Falloppio were pupils of Vesalius. Columbus, as Vesalius's immediate successor in Padua, and afterwards professor at Rome, distinguished himself by describing the shape and cavities of the heart, the structure of the pulmonary artery and aorta and their valves, and tracing the course of the blood from the right to the left side of the heart.
Anatomical Theatres.
Anatomical theatres became a popular form for anatomical teaching in the early 16th century. The University of Padua was the first and most widely known theatre, founded in 1594. As a result, Italy became the center for human dissection. People came from all over to watch as professors taught lectures on the human physiology and anatomy, as anyone was welcome to witness the spectacle. Participants “were fascinated by corporeal display, by the body undergoing dissection.”. Most professors did not do the dissections themselves. Instead they sat in seats above the bodies while hired hands did the cutting. Students and observers would be placed around the table in a circular, stadium like arena and listen as professors explained the various anatomical parts. The 19th century eventually saw a move from anatomical theatres to classrooms, reducing "the number of people who could benefit from each cadaver." 
19th century anatomy.
During the 19th century, anatomical research was extended with histology and developmental biology of both humans and animals. Women, who were not allowed to attend medical school, could attend the anatomy theatres. From 1822 the Royal College of Surgeons forced unregulated schools to close. Medical museums provided examples in comparative anatomy, and were often used in teaching.
Modern anatomy.
Anatomical research in the past hundred years has taken advantage of technological developments and growing understanding of sciences such as evolutionary and molecular biology to create a thorough understanding of the body's organs and structures. Disciplines such as endocrinology have explained the purpose of glands that anatomists previously could not explain; medical devices such as MRI machines and CAT scanners have enabled researchers to study organs, living or dead, in unprecedented detail. Progress today in anatomy is centered in the development, evolution, and function of anatomical features, as the macroscopic aspects of human anatomy have largely been catalogued. Non-human anatomy is particularly active as researchers use techniques ranging from finite element analysis to molecular biology.
To save time, some medical schools such as Birmingham, England have adopted prosection, where a demonstrator dissects and explains to an audience, in place of dissection by students. This enables students to observe more than one body. Improvements in colour images and photography means that an anatomy text is no longer an aid to dissection but rather a central material to learn from. Plastic models are regularly used in anatomy teaching, offering a good substitute to the real thing. Use of living models for anatomy demonstration is once again becoming popular within teaching of anatomy. Surface landmarks that can be palpated on another individual provide practice for future clinical situations. It is possible to do this on oneself; in the Integrated Biology course at the University of Berkeley, students are encouraged to "introspect" on themselves and link what they are being taught to their own body.
Donations of bodies have declined with public confidence in the medical profession. In Britain, the Human Tissue Act 2004 has tightened up the availability of resources to anatomy departments. The outbreaks of Bovine Spongiform Encephalitis (BSE) in the late 80s and early 90s further restricted the handling of brain tissue.
The controversy with Gunther von Hagens and public displays of dissections, preserved by plastination, may divide opinions on what is ethical or legal.

</doc>
<doc id="48987" url="http://en.wikipedia.org/wiki?curid=48987" title="Central City, Colorado">
Central City, Colorado

The City of Central, commonly known as Central City, is the Home Rule Municipality in Gilpin and Clear Creek counties that is the county seat and the most populous municipality of Gilpin County, Colorado, United States. The city population was 663 at the 2010 United States Census. The city is a historic mining settlement founded in 1859 during the Pike's Peak Gold Rush and came to be known as the "Richest Square Mile on Earth". Central City and the adjacent city of Black Hawk form the federally designated Central City/Black Hawk Historic District. The city is now a part of the Denver-Aurora-Lakewood, CO Metropolitan Statistical Area.
History.
On May 6, 1859, during the Pike's Peak Gold Rush, John H. Gregory found a gold-bearing vein (the Gregory Lode) in Gregory Gulch between Black Hawk and Central City. Within two months many other veins were discovered, including the Bates, Gunnell, Kansas, and Burroughs. By 1860, as many as 10,000 prospectors had flocked to the town, then known as Mountain City, and surrounding prospects, but most soon left, many returning east. The 1900 census showed 3,114 people.
The year 1863 brought the first attempt by hard rock miners to form a hard rock miners' union. Of 125 miners signing a union resolution in Central City, about fifty broke windows and doors at the Bob Tail mine, forcing other workers out. After a night of shooting and fighting, the union effort among Central City miners failed.
Many Chinese lived in Central City during the early days working the placer deposits of Gregory Gulch. They were forbidden work in the underground mines. Most of them are believed to have returned to China after making their stake.
The frontier gambler Poker Alice lived for a time in Central City and several other Colorado mining communities.
Gold mining in the Central City district decreased rapidly between 1900 to 1920, as the veins were exhausted. Mining revived in the early 1930s in response to the increase in the price of gold from $20 to $35 per ounce, but then virtually shut down during World War II when gold mining was declared nonessential to the war effort. The district was enlivened in the 1950s by efforts to locate uranium deposits, but these proved unsuccessful.
The population of Central City and its sister city Black Hawk fell to a few hundred by the 1950s. Casino gambling was introduced in both towns in the early 1990s, but had more success in Black Hawk (which has 18 casinos) than in Central City (which has 6 casinos), partly because the main road to Central City passed through Black Hawk, tempting gamblers to stop in Black Hawk instead. In an effort to compete, Central City completed a four-lane, 8.4 mi parkway from Interstate 70 to Central City, without going through Black Hawk. The highway was completed in 2004, but Black Hawk, which prior to the introduction of gambling was much smaller than Central City, continues to generate more than seven times the gambling revenue that Central City does. To compete, Central City has recently eliminated height restrictions for building on undeveloped land. Buildings were previously limited to heights of 53 ft, so as not to overshadow the town's historic buildings.
Tax from the gambling revenue provides funding for the State Historical Fund, administered by the Colorado Office of Archaeology and Historic Preservation.
Geography.
Central City is located in southern Gilpin County at (39.802631, -105.516782). The city limits extend south along the Central City Parkway into Clear Creek County, as far as Interstate 70. The city is bordered by Black Hawk to the east and Idaho Springs to the south.
According to the United States Census Bureau, the city has a total area of 6.3 km2. None of the area is covered with water.
Demographics.
As of the census of 2000, there were 515 people, 261 households, and 101 families residing in the city. The population density was 273.0 PD/sqmi. There were 394 housing units at an average density of 208.8 /sqmi. The racial makeup of the city was 91.84% White, 0.19% Black or African American, 1.55% Native American, 1.17% Asian, 1.17% Pacific Islander, 2.52% from other races, and 1.55% from two or more races. 9.32% of the population were Hispanic or Latino of any race.
There were 261 households out of which 17.6% had children under the age of 18 living with them, 26.8% were married couples living together, 8.8% had a female householder with no husband present, and 61.3% were non-families. 43.3% of all households were made up of individuals and 6.1% had someone living alone who was 65 years of age or older. The average household size was 1.97 and the average family size was 2.76.
In the city the population was spread out with 16.5% under the age of 18, 10.1% from 18 to 24, 34.0% from 25 to 44, 30.9% from 45 to 64, and 8.5% who were 65 years of age or older. The median age was 39 years. For every 100 females there were 115.5 males. For every 100 females age 18 and over, there were 115.0 males.
The median income for a household in the city was $30,921, and the median income for a family was $31,667. Males had a median income of $32,917 versus $25,446 for females. The per capita income for the city was $26,465. About 7.4% of families and 12.3% of the population were below the poverty line, including 8.3% of those under age 18 and 5.2% of those age 65 or over.
Education.
Central City Public Schools are part of the Gilpin County School District RE-1. The district has one elementary school and one high school, Gilpin County Elementary School and Gilpin County Undivided High School.
Tina Goar is the Superintendent of Schools.
There are approximately 380 students enrolled in the district.

</doc>
<doc id="48988" url="http://en.wikipedia.org/wiki?curid=48988" title="Idaho Springs, Colorado">
Idaho Springs, Colorado

The City of Idaho Springs is a Statutory City which is the most populous municipality in Clear Creek County, Colorado, United States. As of the 2010 census it had a population of 1,717. Idaho Springs is located in Clear Creek Canyon, in the mountains upstream from Golden, some 30 mi west of Denver. Local legend is that the name of the city derived from annual visits to the radium hot springs made by a Native American chief and his tribe who journeyed there each year from Idaho to bathe in the magic healing waters.
Founded in 1859 by prospectors during the early days of the Pike's Peak Gold Rush, the town was at the center of the region's mining district throughout the late nineteenth century. The Argo Tunnel drained and provided access to many lodes of ore between Idaho Springs and Central City. During the late twentieth century, the town evolved into a tourist center along U.S. Highway 6 and U.S. Highway 40, which ascend Clear Creek Canyon through the historic mining district.
The town today is squeezed along the north side of Interstate 70, with a historical downtown in the central portion, a strip of tourist-related businesses on its eastern end, and mostly residences on its western end. It also serves as a bedroom community for workers at the Loveland Ski Area farther up the canyon. The town today is the largest community in Clear Creek County, but, for historical reasons, the county seat has remained at Georgetown.
History.
On January 5, 1859, during the Colorado gold rush, prospector George A. Jackson discovered placer gold at the present site of Idaho Springs, where Chicago Creek empties into Clear Creek. It was the first substantial gold discovery in Colorado. Jackson, a Missouri native with experience in the California gold fields, was drawn to the area by clouds of steam rising from some nearby hot springs. Jackson kept his find secret for several months, but after he paid for some supplies with gold dust, others rushed to Jackson's diggings. The location was originally known as "Jackson's Diggings". Once the location became a permanent settlement, it was variously called "Sacramento City", "Idahoe", "Idaho City", and finally "Idaho Springs".
The first placer discoveries were soon followed by discoveries of gold veins in the rocks of the canyon walls on both sides of Clear Creek. Hard rock mining became the mainstay of the town long after the gold-bearing gravels were exhausted.
A strike by Idaho Springs miners demanding the eight-hour day in May 1903 erupted into violence. This was a local struggle in a much broader fight called the Colorado Labor Wars.
The 1969 film "Downhill Racer" portrayed an alpine ski racer from Idaho Springs, played by Robert Redford; a brief scene was shot on location in Idaho Springs. Several scenes from the comedy film "The Overbrook Brothers" were filmed here in the spring of 2008.
Geography.
Idaho Springs is located in northeastern Clear Creek County at (39.742456, -105.514391), along Clear Creek near the confluence of its tributary, Chicago Creek.
According to the United States Census Bureau, the city has a total area of 5.7 km2, of which 0.09 sqkm, or 1.53%, is water.
Demographics.
As of the census of 2000, there were 1,889 people, 841 households, and 485 families residing in the city. The population density was 1820.1 PD/sqmi. There were 904 housing units at an average density of 871.0 /sqmi. The racial makeup of the city was 94.71% White, 0.74% Black or African American, 1.06% Native American, 0.48% Asian, 1.54% from other races, and 1.48% from two or more races. 5.03% of the population were Hispanic or Latino of any race.
There were 841 households out of which 27.1% had children under the age of 18 living with them, 42.8% were married couples living together, 10.9% had a female householder with no husband present, and 42.3% were non-families. 33.2% of all households were made up of individuals and 8.4% had someone living alone who was 65 years of age or older. The average household size was 2.25 and the average family size was 2.87.
In the city the population was spread out with 23.1% under the age of 18, 9.1% from 18 to 24, 30.3% from 25 to 44, 28.0% from 45 to 64, and 9.5% who were 65 years of age or older. The median age was 39 years. For every 100 females there were 105.5 males. For every 100 females age 18 and over, there were 103.9 males.
The median income for a household in the city was $39,643, and the median income for a family was $48,790. Males had a median income of $35,446 versus $22,688 for females. The per capita income for the city was $20,789. About 2.2% of families and 6.7% of the population were below the poverty line, including 5.4% of those under age 18 and 13.4% of those age 65 or over.
Education.
Idaho Springs Public Schools are part of the Clear Creek School District RE-1. There are two elementary schools, one middle school, one high school, and one charter school. Students attend Clear Creek High School.
Carlson Elementary School is located in Idaho Springs.
Roslin Marshall is the Interim Superintendent of Schools.

</doc>
<doc id="48990" url="http://en.wikipedia.org/wiki?curid=48990" title="Social justice">
Social justice

Social justice is "justice in terms of the distribution of wealth, opportunities, and privileges within a society". Classically, "justice" (especially corrective justice or distributive justice) ensured that individuals both fulfilled their societal roles, and received what was due from society. Social justice assigns rights and duties in the institutions of society, which enables people to receive the basic benefits and burdens of cooperation. The relevant institutions can include education, health care, social security, labour rights, as well as a broader system of public services, progressive taxation and regulation of markets, to ensure fair distribution of wealth, equal opportunity, equality of outcome, and no gross social injustice.
While the concept of social justice can be traced through Ancient and Renaissance philosophy, such as Socrates, Thomas Aquinas, Spinoza and Thomas Paine, the term "social justice" only became used explicitly from the 1840s. A Jesuit priest named Luigi Taparelli is typically credited with coining the term, and it spread during the revolutions of 1848 with the work of Antonio Rosmini-Serbati. In the late industrial revolution, progressive American legal scholars began to use the term more, particularly Louis Brandeis and Roscoe Pound. From the early 20th century it was also embedded in international law and institutions, starting with the Treaty of Versailles 1919. The preamble to establish the International Labour Organization recalled that "universal and lasting peace can be established only if it is based upon social justice." In the later 20th century, social justice was made central to the philosophy of the social contract, primarily by John Rawls in "A Theory of Justice" (1971). In 1993, the Vienna Declaration and Programme of Action treats social justice as a purpose of the human rights education.
History.
The different concepts of justice, as discussed in ancient Western philosophy, were typically centered upon the community. Plato wrote in "The Republic" that it would be an ideal state that "every member of the community must be assigned to the class for which he finds himself best fitted." Aristotle believed rights existed only between free people, and the law should take "account in the first instance of relations of inequality in which individuals are treated in proportion to their worth and only secondarily of relations of equality." The Letter to the Ephesians attributed to Paul states that everyone should be bound to do his duty in the class where they were born. Reflecting this time when slavery and subjugation of women was typical, ancient views of justice tended to reflect the rigid class systems that still prevailed. On the other hand, for the privileged groups, strong concepts of fairness and the community existed. Distributive justice was said by Aristotle to require that people were distributed goods and assets according to their merit. Socrates (through Plato's dialogue "Crito") is attributed developing the idea of a social contract, whereby people ought to follow the rules of a society, and accept its burdens, because they have lived to accept its benefits. During the Middle Ages, religious scholars particularly, such as Thomas Aquinas continued discussion of justice in various ways, but ultimately connected to being a good citizen for the purpose of serving God.
After the Renaissance and Reformation, the modern concept of social justice, as developing human potential, began to emerge through the work of a series of authors. Baruch Spinoza in "On the Improvement of the Understanding" (1677) contended that the one true aim of life should be to acquire "a human character much more stable than [one's] own", and to achieve this "pitch of perfection... The chief good is that he should arrive, together with other individuals if possible, at the possession of the aforesaid character." During the enlightenment and responding to the French and American Revolutions, Thomas Paine similarly wrote in "The Rights of Man" (1792) society should give "genius a fair and universal chance" and so "the construction of government ought to be such as to bring forward... all that extent of capacity which never fails to appear in revolutions."
The first modern usage of the specific term "social justice" is typically attributed to Catholic thinkers from the 1840s, including to the Jesuit Luigi Taparelli in "Civiltà Cattolica", based on the work of St. Thomas Aquinas. He argued that rival capitalist and socialist theories, based on subjective Cartesian thinking, undermined the unity of society present in Thomistic metaphysics as neither were sufficiently concerned with moral philosophy. Writing in 1861, the influential British philosopher, politician and economist, John Stuart Mill stated in "Utilitarianism" his view that "Society should treat all equally well who have deserved equally well of it, that is, who have deserved equally well absolutely. This is the highest abstract standard of social and distributive justice; towards which all institutions, and the efforts of all virtuous citizens, should be made in the utmost degree to converge."
In the later 19th and early 20th century, social justice became an important theme in American political and legal philosophy, particularly with John Dewey, Roscoe Pound and Louis Brandeis. One of the prime concerns was the "Lochner era" decisions of the US Supreme Court to strike down legislation passed by state governments and the Federal government for social and economic improvement, such as the eight hour day or the right to join a trade union. After the First World War, the founding document of the International Labour Organization took up the same terminology in its preamble, stating that "peace can be established only if it is based on social justice". From this point, discussion of social justice entered into mainstream legal and academic discourse. In the late 20th century, a number of liberal and conservative thinkers, notably Friedrich von Hayek rejected the concept by stating that it did not mean anything, or meant too many things. However the concept remained highly influential, particularly with its promotion by philosophers such as John Rawls.
Contemporary theory.
Philosophical perspectives.
Cosmic values.
Hunter Lewis' work promoting natural healthcare and sustainable economies advocates for conservation as a key premise in social justice. His manifesto on sustainability ties the continued thriving of human life to real conditions, the environment supporting that life, and associates injustice with the detrimental effects of unintended consequences of human actions. Quoting classical Greek thinkers like Epicurus on the good of pursuing happiness, Hunter also cites ornithologist, naturalist, and philosopher Alexander Skutch in his book Moral Foundations:
The common feature which unites the activities most consistently forbidden by the moral codes of civilized peoples is that by their very nature they cannot be both habitual and enduring, because they tend to destroy the conditions which make them possible.
Pope Benedict XVI cites Teilhard de Chardin in a vision of the cosmos as a 'living host' embracing an understanding of ecology that includes mankinds's relationship to fellow men, that pollution affects not just the natural world but interpersonal relations also. Cosmic harmony, justice and peace are closely interrelated:
If you want to cultivate peace, protect creation.
John Rawls.
Political philosopher John Rawls draws on the utilitarian insights of Bentham and Mill, the social contract ideas of John Locke, and the categorical imperative ideas of Kant. His first statement of principle was made in "A Theory of Justice" where he proposed that, "Each person possesses an inviolability founded on justice that even the welfare of society as a whole cannot override. For this reason justice denies that the loss of freedom for some is made right by a greater good shared by others." A deontological proposition that echoes Kant in framing the moral good of justice in absolutist terms. His views are definitively restated in "Political Liberalism" where society is seen "as a fair system of co-operation over time, from one generation to the next.".
All societies have a basic structure of social, economic, and political institutions, both formal and informal. In testing how well these elements fit and work together, Rawls based a key test of legitimacy on the theories of social contract. To determine whether any particular system of collectively enforced social arrangements is legitimate, he argued that one must look for agreement by the people who are subject to it, but not necessarily to an objective notion of justice based on coherent ideological grounding. Obviously, not every citizen can be asked to participate in a poll to determine his or her consent to every proposal in which some degree of coercion is involved, so one has to assume that all citizens are reasonable. Rawls constructed an argument for a two-stage process to determine a citizen's hypothetical agreement:
This applies to one person who represents a small group (e.g., the organiser of a social event setting a dress code) as equally as it does to national governments, which are ultimate trustees, holding representative powers for the benefit of all citizens within their territorial boundaries. Governments that fail to provide for welfare of their citizens according to the principles of justice are not legitimate. To emphasise the general principle that justice should rise from the people and not be dictated by the law-making powers of governments, Rawls asserted that, "There is ... a general presumption against imposing legal and other restrictions on conduct without sufficient reason. But this presumption creates no special priority for any particular liberty." This is support for an unranked set of liberties that reasonable citizens in all states should respect and uphold — to some extent, the list proposed by Rawls matches the normative human rights that have international recognition and direct enforcement in some nation states where the citizens need encouragement to act in a way that fixes a greater degree of equality of outcome. According to Rawls, the basic liberties that every good society should guarantee are,
Thomas Pogge.
Thomas Pogge's arguments pertain to a standard of social justice that creates human rights deficits. He assigns responsibility to those who actively cooperate in designing or imposing the social institution, that the order is foreseeable as harming the global poor and is reasonably avoidable. Thomas argues that social institutions have a negative duty, that means that their duty is to not harm the poor.
Pogge speaks of Institutional Cosmopolitanism and assigns responsibility to institutional schemes for deficits of human rights. An example given is slavery and third parties. A third party should not recognize or enforce slavery. The institutional order should be held responsible only for deprivations of human rights that it establishes or authorizes. The current institutional design systematically harm developing economies by enabling corporate tax evasion, illicit financial flows, corruption, trafficking of people and weapons as a few examples. Joshua Cohen disputes his claims based on the fact that some poor countries have done well in spite of the current institutional design. Elizabeth Kahn argues that some of these responsibilities should apply globally.
United Nations.
The United Nations’ 2006 document "Social Justice in an Open World: The Role of the United Nations", states that “Social justice may be broadly understood as the fair and compassionate distribution of the fruits of economic growth...”:16
The term "social justice" was seen by the U.N. "as a substitute for the protection of human rights [and] first appeared in United Nations texts during the second half of the 1960s. At the initiative of the Soviet Union, and with the support of developing countries, the term was used in the Declaration on Social Progress and Development, adopted in 1969.":52
The same document reports, “From the comprehensive global perspective shaped by the United Nations Charter and the Universal Declaration of Human Rights, neglect of the pursuit of social justice in all its dimensions translates into de facto acceptance of a future marred by violence, repression and chaos.”:6 The report concludes, “Social justice is not possible without strong and coherent redistributive policies conceived and implemented by public agencies.”:16
The same UN document offers a concise history: “[T]he notion of social justice is relatively new. None of history’s great philosophers—not Plato or Aristotle, or Confucius or Averroes, or even Rousseau or Kant—saw the need to consider justice or the redress of injustices from a social perspective. The concept first surfaced in Western thought and political language in the wake of the industrial revolution and the parallel development of the socialist doctrine. It emerged as an expression of protest against what was perceived as the capitalist exploitation of labour and as a focal point for the development of measures to improve the human condition. It was born as a revolutionary slogan embodying the ideals of progress and fraternity. Following the revolutions that shook Europe in the mid-1800s, social justice became a rallying cry for progressive thinkers and political activists... By the mid-twentieth century, the concept of social justice had become central to the ideologies and programmes of virtually all the leftist and centrist political parties around the world...”:11–12
Religious perspectives.
Catholicism.
Catholic social teaching consists of those aspects of Roman Catholic doctrine which relate to matters dealing with the respect of the individual human life. A distinctive feature of the Catholic social doctrine is their concern for the poorest and most vulnerable members of society. Two of the seven key areas of "Catholic social teaching" are pertinent to social justice:
Even before it was propounded in the Catholic social doctrine, social justice appeared regularly in the history of the Catholic Church:
The Catechism (§1928–1948) contain more detail of the Church's view of social justice.
Hinduism.
Present day jati hierarchy is undergoing changes for variety of reasons including 'social justice',which is a politically popular stance in democratic India. Institutionalized affirmative action has swung the pendulum.
The disparity and wide inequalities in social behaviour to some of the jatis led to various reform movements in hinduism for centuries.
While legally outlawed, the caste system remains strong in practice.
Islam.
The Quran contains numerous references to elements of social justice. For example, one of Islam's Five Pillars is Zakāt, or alms-giving. Charity and assistance to the poor – concepts central to social justice – are and have historically been important parts of the Islamic faith.
In Muslim history, Islamic governance has often been associated with social justice. Establishment of social justice was one of the motivating factors of the Abbasid revolt against the Umayyads. The Shi'a believe that the return of the "Mahdi" will herald in "the messianic age of justice" and the Mahdi along with the Messiah (Jesus) will end plunder, torture, oppression and discrimination.
For the Muslim Brotherhood the implementation of social justice would require the rejection of consumerism and communism. The Brotherhood strongly affirmed the right to private property as well as differences in personal wealth due to factors such as hard work. However, the Brotherhood held Muslims had an obligation to assist those Muslims in need. It held that "zakat" (alms-giving) was not voluntary charity, but rather the poor had the right to assistance from the more fortunate. Most Islamic governments therefore enforce the "zakat" through taxes.
Though monetary donations are the most practiced way of zakat, Islam is deeply rooted in the tenets of volunteerism and social activism. Areas of one's communities which require assistance and beneficiaries must be a Muslim's foci if need be, rather than strictly her or his personal or superficial wants. For example, the ecological well-being of the planet (i.e.: animal rights, global warming, natural resources degradation); locally, nationally, globally, is a campaign to which every Muslim must adhere. Many Muslims practice this today by ensuring that they produce minimal waste, give to charity what they no longer need, and spend time in prayer and meditation upon the bounties of nature so as to more mindfully approach all that is provided by nature,and ultimately, Allah. Other areas of society in need may be the safety and security of minority populations, i.e.: women or persons of color, children, the elderly, the developmentally or physically disabled, animals, et al.
Social justice in Islam is a tenet to which every Muslim must corroborate in his or her daily life, and without which would create a void in all their efforts towards attaining true spirituality and a connection with God.islam lays great stress on doctorine"justice for all"
Judaism.
In "To Heal a Fractured World: The Ethics of Responsibility", Rabbi Jonathan Sacks states that social justice has a central place in Judaism. One of Judaism’s most distinctive and challenging ideas is its ethics of responsibility reflected in the concepts of simcha ("gladness" or "joy"), tzedakah ("the religious obligation to perform charity and philanthropic acts"), chesed ("deeds of kindness"), and tikkun olam ("repairing the world").
Methodism.
From its founding, Methodism was a Christian social justice movement. Under John Wesley's direction, Methodists became leaders in many social justice issues of the day, including the prison reform and abolitionism movements. Wesley himself was among the first to preach for slaves rights attracting significant opposition.
Today, social justice plays a major role in the United Methodist Church. The "Book of Discipline of the United Methodist Church" says, "We hold governments responsible for the protection of the rights of the people to free and fair elections and to the freedoms of speech, religion, assembly, communications media, and petition for redress of grievances without fear of reprisal; to the right to privacy; and to the guarantee of the rights to adequate food, clothing, shelter, education, and health care." The United Methodist Church also teaches Population control as part of its doctrine.
Criticism.
The concept of social justice has come under criticism from a variety of perspectives.
Many authors criticize the idea that there exists an objective standard of social justice. Moral relativists deny that there is any kind of objective standard for justice in general. Non-cognitivists, moral skeptics, moral nihilists, and most logical positivists deny the epistemic possibility of objective notions of justice. Political realists (such as Niccolò Machiavelli believe that any ideal of social justice is ultimately a mere justification for the status quo.
Many other people accept some of the basic principles of social justice, such as the idea that all human beings have a basic level of value, but disagree with the elaborate conclusions that may or may not follow from this. One example is the statement by H. G. Wells that all people are "equally entitled to the respect of their fellowmen."
On the other hand, some scholars reject the very idea of social justice as meaningless, religious, self-contradictory, and ideological, believing that to realize any degree of social justice is unfeasible, and that the attempt to do so must destroy all liberty. Perhaps the most complete rejection of the concept of social justice comes from Friedrich Hayek of the Austrian School of economics:
There can be no test by which we can discover what is 'socially unjust' because there is no subject by which such an injustice can be committed, and there are no rules of individual conduct the observance of which in the market order would secure to the individuals and groups the position which as such (as distinguished from the procedure by which it is determined) would appear just to us. [Social justice] does not belong to the category of error but to that of nonsense, like the term `a moral stone'.
Ben O'Neill of the University of New South Wales argues that, for proponents of "social justice":
the notion of "rights" is a mere term of entitlement, indicative of a claim for any possible desirable good, no matter how important or trivial, abstract or tangible, recent or ancient. It is merely an assertion of desire, and a declaration of intention to use the language of rights to acquire said desire.
In fact, since the program of social justice inevitably involves claims for government provision of goods, paid for through the efforts of others, the term actually refers to an intention to use "force" to acquire one's desires. Not to earn desirable goods by rational thought and action, production and voluntary exchange, but to go in there and forcibly take goods from those who can supply them!
Janusz Korwin-Mikke argues simply: "Either 'social justice' has the same meaning as 'justice' – or not. If so – why use the additional word 'social?' We lose time, we destroy trees to obtain paper necessary to print this word. If not, if 'social justice' means something different from 'justice' – then 'something different from justice' is by definition 'injustice.'"
Sociologist Carl L. Bankston has argued that a secular, leftist view of social justice entails viewing the redistribution of goods and resources as based on the rights of disadvantaged categories of people, rather than on compassion or national interest. Bankston maintains that this secular version of social justice became widely accepted due to the rise of demand-side economics and to the moral influence of the civil rights movement.
Social justice movements.
Social justice is also a concept that is used to describe the movement towards a socially just world, e.g., the Global Justice Movement. In this context, social justice is based on the concepts of human rights and equality, and can be defined as "the way in which human rights are manifested in the everyday lives of people at every level of society".
A number of movements are working to achieve social justice in society. These movements are working towards the realization of a world where all members of a society, regardless of background or procedural justice, have basic human rights and equal access to the benefits of their society.
Liberation theology.
Liberation theology is a movement in Christian theology which conveys the teachings of Jesus Christ in terms of a liberation from unjust economic, political, or social conditions. It has been described by proponents as "an interpretation of Christian faith through the poor's suffering, their struggle and hope, and a critique of society and the Catholic faith and Christianity through the eyes of the poor", and by detractors as Christianity perverted by Marxism and Communism.
Although liberation theology has grown into an international and inter-denominational movement, it began as a movement within the Catholic Church in Latin America in the 1950s–1960s. It arose principally as a moral reaction to the poverty caused by social injustice in that region. It achieved prominence in the 1970s and 1980s. The term was coined by the Peruvian priest, Gustavo Gutiérrez, who wrote one of the movement's most famous books, "A Theology of Liberation" (1971). According to Sarah Kleeb, "Marx would surely take issue," she writes, "with the appropriation of his works in a religious context...there is no way to reconcile Marx's views of religion with those of Gutierrez, they are simply incompatible. Despite this, in terms of their understanding of the necessity of a just and righteous world, and the nearly inevitable obstructions along such a path, the two have much in common; and, particularly in the first edition of [A Theology of Liberation], the use of Marxian theory is quite evident."
Other noted exponents are Leonardo Boff of Brazil, Jon Sobrino of El Salvador, and Juan Luis Segundo of Uruguay.
Health care.
Social justice has more recently made its way into the field of bioethics. Discussion involves topics such as affordable access to health care, especially for low income households and families. The discussion also raises questions such as whether society should bear healthcare costs for low income families, and whether the global marketplace is a good thing to deal with healthcare. Ruth Faden of the Johns Hopkins Berman Institute of Bioethics and Madison Powers of Georgetown University focus their analysis of social justice on which inequalities matter the most. They develop a social justice theory that answers some of these questions in concrete settings.
Social injustices occur when there is a preventable difference in health states among a population of people. These social injustices take on the form of health inequities when negative health states such as malnourishment, and infectious diseases are more prevalent among an impoverished nation. These negative health states can often be prevented by providing social and economic structures such as primary healthcare which ensure the general population has equal access to health care services regardless of income level, gender, education or any other stratifying factor. Integrating social justice to health inherently reflects the social determinants of health model without discounting the role of the bio-medical model.
Human rights education.
The Vienna Declaration and Programme of Action affirm that "Human rights education should include peace, democracy, development and social justice, as set forth in international and regional human rights instruments, in order to achieve common understanding and awareness with a view to strengthening universal commitment to human rights."
"Social Justice Warriors".
The term "social justice warrior" has been used to describe people who work for social justice issues, often "claiming a moral authority" and "questioning the motives and moral integrity of those they oppose".
In Internet culture, the term has been used as a pejorative for someone campaigning against things they perceive to be instances of racism, sexism, homophobia, or other social injustice. Frequently initialized as "SJW", it is used to accuse opponents of sanctimony, to insinuate pretense, as a pejorative, and as a general shorthand for a person believed to be overreacting to social issues.

</doc>
<doc id="48991" url="http://en.wikipedia.org/wiki?curid=48991" title="Arts and Crafts movement">
Arts and Crafts movement

The Arts and Crafts movement was an international movement in the decorative and fine arts that flourished in Europe and North America between 1880 and 1910, emerging later in Japan in the 1920s. It stood for traditional craftsmanship using simple forms and it often used medieval, romantic or folk styles of decoration. It advocated economic and social reform and has been said to be essentially anti-industrial. Its influence was felt in Europe until it was displaced by Modernism in the 1930s and continued among craft makers, designers and town planners long afterwards.
The term was first used by T. J. Cobden-Sanderson at a meeting of the Arts and Crafts Exhibition Society in 1887. although the principles and style on which it was based had been developing in England for at least twenty years. It was inspired by the writings of the architect Augustus Pugin (1812–1852), the writer John Ruskin (1819–1900) and the artist William Morris (1834–1896).
The movement developed earliest and most fully in the British Isles and spread across the British Empire and to the rest of Europe and North America. It was largely a reaction against the perceived impoverished state of the decorative arts at the time and the conditions in which they were produced.
Origins in England.
The influence of John Ruskin.
The Arts and Crafts philosophy derived partly from Ruskin's social criticism, which related the moral and social health of a nation to the qualities of its architecture and to the nature of work. Ruskin considered the sort of mechanized production and division of labour that had been created in the industrial revolution to be "servile labour" and he thought that a healthy and moral society required independent workers who designed the things they made. His followers in the Arts and Crafts movement favoured craft production over industrial manufacture and were concerned about the loss of traditional skills, but they were arguably more troubled by effects of the factory system than by machinery itself and William Morris's idea of "handicraft" was essentially work without any division of labour rather than work without any sort of machinery.
The Pre-Raphaelites.
The aesthetic and social vision of the Arts and Crafts movement also derived from ideas developed in the 1850s by the Pre-Raphaelite Brotherhood. The Brotherhood was formed by a group of friends at the University of Oxford, including William Morris, Edward Burne-Jones and some of Burne-Jones' associates from Birmingham at Pembroke College, who became known as the Birmingham Set. The Birmingham Set had first-hand experience of modern industrial society and combined their love of the Romantic literature of Tennyson, Keats and Shelley with a commitment to social reform. By 1855 they had discovered the writings of John Ruskin and, conscious of the contrast between the barbarity of contemporary culture and the art of the Middle Ages, in particular the art preceding Raphael (1483-1530), they formed themselves into the Pre-Raphaelite Brotherhood to pursue their literary and artistic aims. In Burne-Jones' words, they intended to "wage Holy warfare against the age".
Morris and Burne-Jones had originally intended to join the priesthood, but in 1855, returning to Burne-Jones' house in Birmingham from touring the cathedrals of Northern France, they decided instead to pursue careers in the visual arts, Burne-Jones resolving to become a painter and Morris an architect. The following day they discovered a copy of Mallory's "Morte d'Arthur" in a Birmingham bookshop; this work, more than any other, was to define the medievalism of their early style. In early 1856 Morris joined the Oxford office of the Gothic Revival architect G. E. Street, where he met fellow-architect Philip Webb and began experimenting with stone carving, wood carving, embroidery, metalwork and the making of illuminated manuscripts. Burne-Jones had become a pupil of the Pre-Raphaelite painter Dante Gabriel Rossetti in London, and in the summer of 1856 both Morris and Burne Jones moved into premises in Red Lion Square in Bloomsbury.
There they wrote articles on the politics of art for "The Oxford and Cambridge Magazine" with other members of the Birmingham Set, and Morris began to design furniture and interiors. Morris's radical departure was his personal involvement in the manufacture as well as the design of his products. Ruskin had argued that the separation of the intellectual act of design from the manual act of physical creation was both socially and aesthetically damaging. Morris further developed this idea, insisting that no work was carried out in his workshops before he had mastered the techniques and materials himself, and arguing that "without dignified, creative human occupation people became disconnected from life".
Red House, in Bexleyheath, London, designed for Morris in 1859 by architect Philip Webb, exemplifies the early Arts and Crafts style, with its well-proportioned solid forms, wide porches, steep roof, pointed window arches, brick fireplaces and wooden fittings. Webb rejected the grand classical style and based the design on British vernacular architecture expressing the texture of ordinary materials, such as stone and tiles, with an asymmetrical and quaint building composition.
Morris, Marshall, Faulkner & Co..
In 1861 Morris and some friends founded a company, Morris, Marshall, Faulkner & Co., which, as supervised by the partners, designed and made decorative objects for homes, including wallpaper, textiles, furniture and stained glass. Later the company (which Morris and his friends called "The Firm") was re-formed as Morris & Co. In 1890 Morris established the Kelmscott Press, for which he designed a typeface based on Nicolas Jenson's 15th-century letter forms. The press printed fine and de-luxe editions of contemporary and historical English literature.
Morris was influenced by the Gothic Revival (1830–1880) and was interested in medieval styles, using bold forms and strong colors based on medieval designs. His products were simple in form, without superfluous or excessive decoration, and in order to express the beauty of craft some products were deliberately left slightly unfinished, expressing the qualities of the materials used and producing a certain rustic and robust effect. He tended to use patterns inspired by British flora and fauna and forms inspired by the vernacular or domestic traditions of the British countryside. He carried out detailed research into old techniques, such as wood-block printing of fabrics. Truth to material, structure and function had also been advocated by A.W.N. Pugin (1812–1852), an exponent of the Gothic Revival in architecture. This approach to designing and making became characteristic of the Arts and Crafts movement.
Morris & Co. traded until 1940. Its designs were sold by Sanderson and Sons and some are still in production.
Social and design principles.
The Arts and Crafts style emerged from the attempt to reform design and decoration and the reaction against contemporary styles that the reformers associated with machine-production, partly a reaction against the style of many of the items shown in the Great Exhibition of 1851, which were ornate, artificial and ignored the qualities of the materials used. The art historian Nikolaus Pevsner has said that exhibits in the Great Exhibition showed "ignorance of that basic need in creating patterns, the integrity of the surface" and "vulgarity in detail". Design reform began with the organisers of the Exhibition itself, Henry Cole (1808–1882), Owen Jones (1809–1874), Matthew Digby Wyatt (1820–1877) and Richard Redgrave (1804–1888), and Morris's dislike of excessive ornament and badly made things was not exclusive to the Arts and Crafts movement. Owen Jones, for example, declared that "Ornament ... must be secondary to the thing decorated", that there must be "fitness in the ornament to the thing ornamented", and that wallpapers and carpets must not have any patterns "suggestive of anything but a level or plain". Where a fabric or wallpaper in the Great Exhibition might be decorated with a natural motif made to look as real as possible, a Morris & Co. wallpaper, like the Artichoke design illustrated above, would use a flat and simplified natural motif.
Morris mixed design criticism with social criticism, insisting that the artist should be a craftsman-designer working by hand and advocating a society of free craftspeople, such as he believed had existed during the Middle Ages. "Because craftsmen took pleasure in their work", he wrote, "the Middle Ages was a period of greatness in the art of the common people. ... The treasures in our museums now are only the common utensils used in households of that age, when hundreds of medieval churches - each one a masterpiece - were built by unsophisticated peasants."
There was inconsistency and disagreement about whether machinery should be rejected altogether. At one point Morris said that production by machinery was "altogether an evil", but when he could find manufacturers willing to work to his own exacting standards, he employed them to make his designs and he said that, in a "true society", where neither luxuries nor cheap trash were made, machinery could be improved and used to reduce the hours of labour. C.R.Ashbee shared his ambivalence. At the time of his Guild of Handicraft, initiated in 1888, he said, "We do not reject the machine, we welcome it. But we would desire to see it mastered." After unsuccessfully pitting his Guild and School of Handicraft guild against modern methods of manufacture, he acknowledged that "Modern civilization rests on machinery", but he continued to criticize the deleterious effects of what he called "mechanism", saying that "the production of certain mechanical commodities is as bad for the national health as is the production of slave-grown cane or child-sweated wares."
Ruskin's and Morris's idea that the division of labour was undesirable did not result in Arts and Crafts artists carrying out every stage in the making of goods and it was only in the twentieth century that that became an essential part of the definition of craftsmanship. The founders of the Arts and Crafts Exhibition Society did not insist that the designer should also be the maker or the 'executant'. Peter Floud, writing in the 1950s, said that "The founders of the Society ... never executed their own designs, but invariably turned them over to commercial firms." Floud argues that the idea that the designer should be the maker and the maker the designer derived "not from Morris or early Arts and Crafts teaching, but rather from the second-generation elaboration doctrine worked out in the first decade of [the twentieth] century by men such as W. R. Lethaby.
The movement was associated with socialist ideas in the persons of Morris, T. J. Cobden Sanderson, Crane, Ashbee and others. In the early 1880s Morris was spending more of his time on socialist propaganda than on designing and making. Ashbee established a community of craftsmen, the Guild of Handicraft, in east London, later moving to Chipping Campden. Those adherents who were not socialists, for example, Alfred Hoare Powell, advocated a more humane and personal relationship between employer and employee. In Britain the movement was associated with dress reform, ruralism, the garden city movement and the folk-song revival, and in continental Europe with the preservation of national traditions in building, the applied arts, domestic design and costume.
Development.
Morris's designs quickly became popular, attracting interest when The Firm exhibited at the 1862 International Exhibition. Much of The Firm's early work was for churches and Morris won important interior design commissions at St James's Palace and the South Kensington Museum (now the Victoria and Albert Museum). Later his work became popular with the middle and upper classes, despite his wish to create a democratic art, and by the end of the 19th century, Arts and Crafts design in houses and domestic interiors was the dominant style in Britain, copied in products made by conventional industrial methods.
The spread of Arts and Crafts ideas during the late 19th and early 20th centuries resulted in the establishment of many associations and craft communities, although Morris was not involved with them because of his preoccupation with socialism in the 1880s. A hundred and thirty Arts and Crafts organisations were formed in Britain, most between 1895 and 1905.
In 1881, Eglantyne Louisa Jebb, Mary Fraser Tytler and others initiated the Home Arts and Industries Association to encourage the working classes, especially those in rural areas, to take up handicrafts under supervision, not for profit, but in order to provide them with useful occupations and to improve their taste.
In 1882, architect A.H.Mackmurdo formed the Century Guild, a partnership of designers including Selwyn Image, Herbert Horne, Clement Heaton and Benjamin Creswick.
In 1884, the Art Workers Guild was initiated by five young architects, William Lethaby, Edward Prior, Ernest Newton, Mervyn Macartney and Gerald C. Horsley, with the goal of bringing together fine and applied arts and raising the status of the latter. It was directed originally by George Blackall Simonds. By 1890 the Guild had 150 members, representing the increasing number of practitioners of the Arts and Crafts style. It still exists.
The London department store Liberty & Co., founded in 1875, was a prominent retailer of goods in the style and of the "artistic dress" favoured by followers of the Arts and Crafts movement.
In 1887 the Arts and Crafts Exhibition Society, which gave its name to the movement, was formed with Walter Crane as president, holding its first exhibition in the New Gallery, London, in November 1888. It was the first show of contemporary decorative arts in London since the Grosvenor Gallery's Winter Exhibition of 1881. Morris & Co. was well represented in the exhibition with furniture, fabrics, carpets and embroideries. Edward Burne-Jones observed, "here for the first time one can measure a bit the change that has happened in the last twenty years". The society still exists as the Society of Designer Craftsmen.
In 1888, C.R.Ashbee, a major late practitioner of the style in England, founded the Guild and School of Handicraft in the East End of London. The guild was a craft co-operative modelled on the medieval guilds and intended to give working men satisfaction in their craftsmanship. Skilled craftsmen, working on the principles of Ruskin and Morris, were to produce hand-crafted goods and manage a school for apprentices. The idea was greeted with enthusiasm by almost everyone except Morris, who was by now involved with promoting socialism and thought Ashbee's scheme trivial. From 1888 to 1902 the guild prospered, employing about 50 men. In 1902 Ashbee relocated the guild out of London to begin an experimental community in Chipping Campden in the Cotswolds. The guild's work is characterized by plain surfaces of hammered silver, flowing wirework and colored stones in simple settings. Ashbee designed jewellery and silver tableware. The guild flourished at Chipping Camden but did not prosper and was liquidated in 1908. Some craftsmen stayed, contributing to the tradition of modern craftsmanship in the area.
Charles Francis Annesley Voysey (1857–1941) was an Arts and Crafts architect who also designed fabrics, tiles, ceramics, furniture and metalwork. His style combined simplicity with sophistication. His wallpapers and textiles, featuring stylised bird and plant forms in bold outlines with flat colors, were used widely.
Morris's ideas were adopted by the New Education philosophy in the late 1880s, which incorporated handicraft teaching in schools at Abbotsholme (1889) and Bedales (1892), and his influence has been noted in the social experiments of Dartington Hall during the mid-20th century.
Morris's thought influenced the distributism of G. K. Chesterton and Hilaire Belloc.
By the end of the nineteenth century, Arts and Crafts ideals had influenced architecture, painting, sculpture, graphics, illustration, book making and photography, domestic design and the decorative arts, including furniture and woodwork, stained glass, leatherwork, lacemaking, embroidery, rug making and weaving, jewelry and metalwork, enameling and ceramics. By 1910, there was a fashion for "Arts and Crafts" and all things hand-made and a proliferation of amateur handicrafts of variable quality.
The London suburb of Bedford Park, built mainly in the 1880s and 1890s, has about 360 Arts and Crafts style houses and was once famous for its Aesthetic residents. Several Almshouses were built in the Arts and Crafts style, for example, Whiteley Village, Surrey, built between 1914 and 1917, with over 280 buildings, and the Dyers Almshouses, Sussex, built between 1939 and 1971. Letchworth Garden City, the first garden city, was inspired by Arts and Crafts ideals. The first houses were build in the vernacular style popularized by the movement and the town became associated with high-mindedness and simple living. The sandal-making workshop set up by Edward Carpenter moved from Yorkshire to Letchworth Garden City and George Orwell's jibe about "every fruit-juice drinker, nudist, sandal-wearer, sex-maniac, Quaker, ‘Nature Cure’ quack, pacifist, and feminist in England" going to a socialist conference in Letchworth has become famous.
Art education.
Arts and Crafts practitioners were critical of the national system of art education, which was based on design in the abstract with little teaching of craft. The lack of practical craft training also caused concern in industrial and official circles, and in 1884, a Royal Commission - accepting the advice of William Morris - recommended that art education should pay more attention to the suitability of design to the material in which it was to be executed.
The first school to make this change was the Birmingham School of Arts and Crafts, which "led the way in introducing executed design to the teaching of art and design nationally (working in the material for which the design was intended rather than designing on paper). In his external examiner's report of 1889, Walter Crane praised Birmingham School of Art in that it 'considered design in relationship to materials and usage.'" Under the direction of Edward Taylor, its headmaster from 1877 to 1903, and with the help of Henry Payne and Joseph Southall, the Birmingham School became a leading Arts-and-Crafts centre.
Other local authority schools also began to introduce more practical teaching of crafts. In London, the Central School of Arts and Crafts was founded in 1896 by the London County Council, with Lethaby and George Frampton as its first principals, and the Camberwell School of Arts and Crafts was founded shortly after by the local borough. Crane became head of the Royal College of Art in 1898 and tried to reform it along more practical lines, but resigned after a year, defeated by the bureaucracy of the Board of Education, who then appointed Augustus Spencer to implement his plan. Spencer brought in Lethaby to head its school of design.
Influences.
Ireland and Scotland.
The movement spread to Ireland, representing an important time for the nation's cultural development, a visual counterpart to the literary revival of the same time and was a publication of Irish nationalism. The Arts and Crafts use of stained glass was popular in Ireland, with Harry Clarke the best-known artist and also with Evie Hone. The architecture of the style is represented by the Honan Chapel (1916) in Cork in the grounds of University College Cork. Other architects practicing in Ireland included Sir Edwin Lutyens (Heywood House in Co. Laois, Lambay Island and the Irish National War Memorial Gardens in Dublin) and Frederick 'Pa' Hicks (Malahide Castle estate buildings and round tower). Irish Celtic motifs were popular with the movement in silvercraft, carpet design, book illustrations and hand-carved furniture.
The beginnings of the Arts and Crafts movement in Scotland were in the stained glass revival of the 1850s, pioneered by James Ballantine (1808–77). His major works included the great west window of Dunfermline Abbey and the scheme for St. Giles Cathedral, Edinburgh. In Glasgow it was pioneered by Daniel Cottier (1838–91), who had probably studied with Ballantine, and was directly influenced by William Morris, Ford Madox Brown and John Ruskin. His key works included the "Baptism of Christ" in Paisley Abbey, (c. 1880). His followers included Stephen Adam and his son of the same name. The Glasgow-born designer and theorist Christopher Dresser (1834–1904) was one of the first, and most important, independent designers, a pivotal figure in the Aesthetic Movement and a major contributor to the allied Anglo-Japanese movement. The movement had an "extraordinary flowering" in Scotland where it was represented by the development of the 'Glasgow Style' which was based on the talent of the Glasgow School of Art. Celtic revival took hold here, and motifs such as the Glasgow rose became popularised. Charles Rennie Mackintosh and the Glasgow School of Art were to influence others worldwide.
Continental Europe.
Widely exhibited in Europe, the Arts and Crafts style's simplicity inspired designers like Henry van de Velde and styles such as Art Nouveau, the Dutch De Stijl group, Vienna Secession, and eventually the Bauhaus style. Pevsner regarded the style as a prelude to Modernism, which used simple forms without ornamentation.
The earliest Arts and Crafts activity in continental Europe was in Belgium in about 1890, where the English style inspired artists and architects including can de Velde, Gabriel Van Dievoet, Gustave Serrurier-Bovy and a group known as "La Libre Esthétique" (Free Aesthetic).
In Germany, after unification in 1871, the Arts and Crafts movement developed nationalist associations under the encouragement of the "Bund für Heimatschutz" (1897) and the "Vereinigte Werkstätten für Kunst im Handwerk" founded in 1898 by Karl Schmidt. The Deutscher Werkbund, formed in 1899, was an arts and crafts association influenced by William Morris. Its leading members, van de Velde and Hermann Muthesius, had opposing opinions about the place of machinery in production: van de Velde thought mass production threatened creativity and individuality; Muthesius championed mass production, standardisation and an affordable, democratic art.
In Austria, the style became popular in Vienna, inspired by an exhibition of the works of Charles Rennie Mackintosh and Charles Robert Ashbee. The Wiener Werkstätte, founded in 1903 by Josef Hoffmann and Koloman Moser, had an independent role in the development of Modernism, with its Wiener Werkstätte Style.
In Finland, an idealistic artists' colony in Helsinki was designed by Herman Gesellius, Armas Lindgren and Eliel Saarinen, who worked in the National Romantic style, akin to the British Gothic Revival.
In Hungary, under the influence of Ruskin and Morris, a group of artists and architects, including Károly Kós, Aladár Körösfői-Kriesch and Ede Toroczkai Wigand, discovered the folk art and vernacular architecture of Transylvania. Many of Kós's buildings, including those of the Budapest zoo, show this influence.
In Russia, Viktor Hartmann, Viktor Vasnetsov and other artists associated with Abramtsevo Colony sought to revive the quality of medieval Russian decorative arts quite independently from the movement in Great Britain.
In Iceland, Sölvi Helgason's work shows Arts and Crafts influence.
North America.
In the United States, the terms "American Craftsman" or "Craftsman style" are often used to denote the style of architecture, interior design, and decorative arts that prevailed between the dominant eras of Art Nouveau and Art Deco, or approximately the period from 1910 to 1925. The movement was particularly notable for the professional opportunities it opened up for women as artisans, designers and entrepreneurs who founded and ran, or were employed by, such successful enterprises as the Kalo Shops, Rookwood Pottery, and Tiffany Studios.
In Canada, the term "Arts and Crafts" predominates, but "Craftsman" is also recognized.
While the Europeans tried to recreate the virtuous crafts being replaced by industrialisation, Americans tried to establish a new type of virtue to replace heroic craft production: well-decorated middle-class homes. They claimed that the simple but refined aesthetics of Arts and Crafts decorative arts would ennoble the new experience of industrial consumerism, making individuals more rational and society more harmonious. The American Arts and Crafts movement was the aesthetic counterpart of its contemporary political philosophy, progressivism. Characteristically, when the Arts and Crafts Society began in October 1897 in Chicago, it was at Hull House, one of the first American settlement houses for social reform.
In the United States, the Arts and Crafts style initiated a variety of attempts to reinterpret European Arts and Crafts ideals for Americans. These included the "Craftsman"-style architecture, furniture, and other decorative arts such as designs promoted by Gustav Stickley in his magazine, "The Craftsman" and designs produced on the Roycroft campus as publicized in Elbert Hubbard's "The Fra". Both men used their magazines as a vehicle to promote the goods produced with the Craftsman workshop in Eastwood, NY and Elbert Hubbard's Roycroft campus in East Aurora, NY. A host of imitators of Stickley's furniture (the designs of which are often mislabelled the "Mission Style") included three companies established by his brothers.
Arts and Crafts ideals disseminated in America through journal and newspaper writing were supplemented by societies that sponsored lectures and programs. The first was organized in Boston in the late 1890s, when a group of influential architects, designers, and educators determined to bring to America the design reforms begun in Britain by William Morris; they met to organize an exhibition of contemporary craft objects. The first meeting was held on January 4, 1897, at the Museum of Fine Arts (MFA) in Boston to organize an exhibition of contemporary crafts. When craftsmen, consumers, and manufacturers realised the aesthetic and technical potential of the applied arts, the process of design reform in Boston started. Present at this meeting were General Charles Loring, Chairman of the Trustees of the MFA; William Sturgis Bigelow and Denman Ross, collectors, writers and MFA trustees; Ross Turner, painter; Sylvester Baxter, art critic for the "Boston Transcript"; Howard Baker, A.W. Longfellow Jr.; and Ralph Clipson Sturgis, architect.
The first American Arts and Crafts Exhibition began on April 5, 1897, at Copley Hall, Boston featuring more than 1000 objects made by 160 craftsmen, half of whom were women. Some of the advocates of the exhibit were Langford Warren, founder of Harvard's School of Architecture; Mrs. Richard Morris Hunt; Arthur Astor Carey and Edwin Mead, social reformers; and Will H. Bradley, graphic designer. The success of this exhibition resulted in the incorporation of The Society of Arts and Crafts (SAC), on June 28, 1897, with a mandate to "develop and encourage higher standards in the handicrafts." The 21 founders claimed to be interested in more than sales, and emphasized encouragement of artists to produce work with the best quality of workmanship and design. This mandate was soon expanded into a credo, possibly written by the SAC's first president, Charles Eliot Norton, which read:
This Society was incorporated for the purpose of promoting artistic work in all branches of handicraft. It hopes to bring Designers and Workmen into mutually helpful relations, and to encourage workmen to execute designs of their own. It endeavors to stimulate in workmen an appreciation of the dignity and value of good design; to counteract the popular impatience of Law and Form, and the desire for over-ornamentation and specious originality. It will insist upon the necessity of sobriety and restraint, or ordered arrangement, of due regard for the relation between the form of an object and its use, and of harmony and fitness in the decoration put upon it.
Also influential were the Roycroft community initiated by Elbert Hubbard in Buffalo and East Aurora, New York, Joseph Marbella, utopian communities like Byrdcliffe Colony in Woodstock, New York, and Rose Valley, Pennsylvania, developments such as Mountain Lakes, New Jersey, featuring clusters of bungalow and chateau homes built by Herbert J. Hapgood, and the contemporary studio craft style. Studio pottery—exemplified by the Grueby Faience Company, Newcomb Pottery in New Orleans, Marblehead Pottery, Teco pottery, Overbeck and Rookwood pottery and Mary Chase Perry Stratton's Pewabic Pottery in Detroit, as well as the art tiles made by Ernest A. Batchelder in Pasadena, California, and idiosyncratic furniture of Charles Rohlfs all demonstrate the influence of Arts and Crafts.
Architecture.
The "Prairie School" of Frank Lloyd Wright, George Washington Maher and other architects in Chicago, the Country Day School movement, the bungalow and ultimate bungalow style of houses popularized by Greene and Greene, Julia Morgan, and Bernard Maybeck are some examples of the American Arts and Crafts and American Craftsman style of architecture. Restored and landmark-protected examples are still present in America, especially in California in Berkeley and Pasadena, and the sections of other towns originally developed during the era and not experiencing post-war urban renewal. Mission Revival, Prairie School, and the 'California bungalow' styles of residential building remain popular in the United States today.
Asia.
In Japan, Yanagi Sōetsu, creator of the Mingei movement which promoted folk art from the 1920s onwards, was influenced by the writings of Morris and Ruskin. Like the Arts and Crafts movement in Europe, Mingei sought to preserve traditional crafts in the face of modernising industry.
Later influences.
One of Yanagi Soetsu's collaborators in Japan was the British artist potter Bernard Leach, who brought to England many of Yanagi's ideas about the moral and social value of simple crafts when he set up the St. Ives Pottery in the early 1920s. Leach was an active propagandist for these ideas, which struck a chord with practitioners of the crafts in the inter-war years, and he expounded them in his book "The Art of the Potter", published in 1940, which denounced industrial society in terms as vehement as those of Ruskin and Morris. Thus the Arts and Crafts philosophy was perpetuated in the small world of the crafts in the 1950s and 1960s, long after the demise of the Arts and Crafts movement and at the high tide of Modernism.
The British Utility furniture of the 1940s was simple in design and also derived from Arts and Crafts principles. Gordon Russell, chairman of the Utility Furniture Design Panel, manufactured in the Cotswold Hills, which had become a region of Arts and Crafts furniture making when Ashbee relocated there.
William Morris's biographer, Fiona MacCarthy, detected the Arts and Crafts philosophy behind the Festival of Britain (1951), the work of the designer Terence Conran and the founding of the British Crafts Council in the 1970s.
Museums.
The Museum of the American Arts and Crafts Movement is under construction in St. Petersburg, Florida, scheduled to open in 2017.

</doc>
<doc id="48993" url="http://en.wikipedia.org/wiki?curid=48993" title="Quiz bowl">
Quiz bowl

Quiz bowl (quizbowl, scholastic bowl, academic bowl etc.) is a quiz game that tests players on a wide variety of academic subjects. Standardized quiz bowl formats are played by lower school, middle school, high school, and university students throughout the United States, Canada, and the United Kingdom.
The game is typically played with a lockout buzzer system between at least two teams, usually consisting of four or five players each. Players are read questions and try to score points for their team by buzzing first and responding with the correct answer.
Quiz bowl is most commonly played in a tossup/bonus format, which consists of a series of two different types of questions. Other formats, particularly in local competitions, may deviate from the above rules.
History.
Most forms of modern quiz bowl are modeled after game shows. "College Bowl", which was created by Don Reid as a USO activity for US service men during World War II was an early influential quiz bowl program. "College Bowl", also known as "The College Quiz Bowl", started on radio in 1953 and then aired on national television from 1959 to 1970. 
In the first half of the 20th century, many other quiz bowl-like competitions were also created. Delco Hi-Q began in 1948 as a radio quiz competition sponsored by the Scott Paper Company for high school students in Delaware County, Pennsylvania. It claims to be the oldest continuously running student quiz contest in the United States. The "It's Academic" televised student quiz show program has been run for high school teams in the Washington, D.C. metropolitan area since 1961 and is recognized by the Guinness Book of World Records as the longest-running quiz program in television history. "It's Academic" has been spun off in many other US media markets and has inspired many other televised high school competitions.
In 1977, College Bowl was revived as an activity on college campuses by College Bowl Company Inc. (CBCI). In September 1990, the Academic Competition Federation (ACF) was founded as the first major alternative to The College Bowl Company. National Academic Quiz Tournaments (NAQT) was founded in 1996 and currently organizes national competitions at all levels in the United States and supplies tournament questions for grade school and college teams across North America and other parts of the world. In 2008 the College Bowl program abruptly ended, although the company itself continues to operate the Honda Campus All-Star Challenge (HCASC) for historically black colleges and universities.
Gameplay.
During a quiz bowl game, two or more teams of usually up to four or five players are read questions by a moderator. In most forms of quiz bowl, there are two basic types of questions: tossups and bonuses. Tossups are questions that any individual player can attempt to answer, and players are generally not allowed to confer with each other. Each player usually has a signaling device, also called a buzzer, to signal in at any time during the question to give an answer. If the answer given is incorrect, then no other member of that team may give an answer. If a tossup is successfully answered, the correctly answering team is given an opportunity to answer a bonus question. Bonuses are usually worth a total of 30 points, and consist of three individual questions worth ten points each. Team members are generally permitted to confer with each other on these questions.
Regional or local tournaments may dispose of any number of standard rules entirely. Some may only have tossups and not use bonuses at all. Some formats include a lightning round during which a team attempts to answer multiple questions as fast as possible under a given time limit, usually sixty seconds.
Match length is determined by either a game clock or the number of questions in a packet. In most formats, a game ends once the moderator has finished reading every question in a packet. Tie-breaking procedures may include reading extra tossups until the tie is broken or sudden-death tossups.
Quiz bowl tests players in a variety of academic subjects including literature, science, history, and fine arts. Additionally, some quiz bowl events may feature small amounts of popular culture content like sports, popular music, and other non-academic general knowledge subjects, although their inclusion is generally kept to a minimum.
In most quiz bowl competitions, players and coaches may protest the moderator's decision if they believe their answer was incorrectly rejected, or an opponent's answer was incorrectly accepted.
Tossups.
A pyramidal tossup on trumpets
 This instrument plays the opening Promenade in Mussorgsky's "Pictures at an Exhibition", as well as the rising theme C-G-C in the opening of Richard Strauss' "Thus Spake Zarathustra". This instrument's tone can be lowered by one-and-a-half steps by pressing either its third or both its first and second (*) valves. For 10 points, name this brass instrument whose timbre is slightly brighter than that of a cornet.<br>
ANSWER: Trumpets
2013 CMST
Two common types of tossups include buzzer-beaters and pyramidal tossups. Buzzer-beaters (also known as speed check or quick-recall questions) are relatively short, rarely more than two sentences long, and contain few clues. This type of question is written specifically to test quick recall skills of players, and does not discriminate the different levels of knowledge that the players possess.
Pyramidal or pyramid-style tossups include multiple clues and are written so that each question starts with more difficult clues and moves toward easier clues. This way the player with the most knowledge of the subject being asked about has the most opportunity to answer first. Pyramidal toss-ups are a common standard for college quiz bowl.
In most formats, correctly answering a tossup earns a team 10 points. Extra points, usually for a total of 15 or 20 points, may be awarded if a question is answered prior to a certain clue-providing keyword in the question, an action known as "powering". Answering a tossup incorrectly is called "negging", and may incur a 5-point penalty for a team. After a neg occurs the moderator continues reading the rest of the question for the other team. There are usually no further penalties after one team has already negged.
Bonuses.
A bonus question on amino acids
 
These biological monomers are usually in a zwitterionic form. For 10 points each: <br>
[10] Name this class of molecules that make up proteins, examples of which include tryptophan, alanine, and lysine.<br> ANSWER: amino acids<br>
[10] During translation, amino acids are polymerized by these complexes, which are formed in the nucleolus.<br> ANSWER: ribosomes <br>
[10] Some proteins can become infectious agents. This is the name of those misfolded proteins that are responsible for mad cow disease and Kuru.<br> ANSWER: prions
2011 Collegiate Novice
Bonuses usually have multiple parts that are related by some common thread and may or may not be related to corresponding tossup. A team is usually rewarded with 10 points upon correctly answering each bonus part. Usually, only the team that answered the tossup correctly can answer the bonus questions, though some formats allow the opposing team to answer certain parts of the bonus not correctly answered by the team in control of the bonus, a gameplay element known as a "bounceback". Less-used types of bonus questions include list bonuses, which require players to give their answers from a requested list, and "30-20-10" bonuses, which give a number of discrete clues for a single answer in order of decreasing difficulty, with more points being awarded for giving the correct answer on an earlier clue. The 30-20-10 bonus was officially banned from ACF in 2008 and NAQT in 2009.
Variations.
Several variations on the game of quiz bowl exist that affect question structure and content, rules of play, and round format. One standardized format is the pyramidal tossup/bonus format, which is used in ACF (or mACF) and NAQT competitions. ACF/mACF tossups are written in pyramidal style and are generally much longer than College Bowl and NAQT questions. It has a rigorous emphasis on academics, with very little popular culture. Games are usually untimed and last until a total of 20 tossups are read. NAQT is another common variation on the tossup/bonus format that balances academic rigor with a wider variety of subjects, including popular culture and an increased amount of current events and geography content. Unlike many mACF events, most questions used in this format are written and sold by NAQT themselves. NAQT also uses powers their in tossups, which reward players with 15 points instead of 10 for a tossup answered before a predetermined point. Games played on NAQT rules consist of two nine-minute halves and a total of 24 tossups. NAQT tossups are typically shorter than most other pyramidal tossups because of a character limit enforced on the questions. The format used for the now-defunct College Bowl tournament uses comparatively shorter questions. Gameplay is relatively quick as it is played in eight-minute halves, to a usual total of 22–24 tossups read. The Honda Campus All Star Challenge and "University Challenge" use similar formats.
Matches played at the National Academic Championship and its affiliated tournaments are split into four quarters, with differing styles of gameplay in each phase. Individual tournaments may use worksheet quizzes, lightning rounds, or tossups without accompanying bonuses.
Preparation.
Since questions are generally derived from an unofficial canon of topics, players commonly review question content from older competitions to prepare for upcoming tournaments. In this vein, NAQT also sells lists of topics that are frequently asked about in their questions. Players also research and write questions to prepare for quiz bowl. Active participation in academic coursework can also serve as means of preparing for quiz bowl. Blind memorization of out-of-context facts is often discouraged. Team members often specialize in a few subjects. Players benefit from exposure to a broad range of school and cultural subjects, memorization and study skills, and an improved ability to cooperate and work in teams.
Competitions.
Quiz bowl is primarily played at single-day tournaments. Some events have eligibility rules that dictate who may participate, such as allowing only freshman and sophomore players or excluding graduate students from play. Additionally, most tournaments allow multiple teams from a single school to compete.
Some schools hold intramural tournaments where any team formed from students can play. High school-level quiz bowl is occasionally played over an extended period of time schools within a league or preexisting athletic conference or even in single matches against other schools.
Some regional variants organized for grade school students include Knowledge Bowl, Ohio Academic Competition (OAC), Florida's Commissioner's Academic Challenge (CAC), and various television quiz competitions such as "It's Academic". Athletic and activities associations in some US states also organize quiz bowl competitions, including Missouri's MSHSAA, Illinois's IHSA, and Virginia's VHSL.
Additionally, various formats have been developed to test knowledge in specific areas like the Bible, classics, science, and agricultural science. DECA runs quiz bowl events at their competitions that tests knowledge on business and market topics. Gallaudet University sponsors a National Academic Bowl for deaf university students. Tournaments designated as "trash" focus on pop culture and sports trivia questions.
National tournaments.
There are several collegiate-level national championship tournaments for which teams usually qualify through regional competitions. These tournaments include:
Several national competitions are conducted in the United States every year for high school students. Compared to at the college level, there are usually many more tournaments at which teams can qualify. These include:
The following high school tournaments are for single all-star teams from each US state or other political subdivision:
Educational value.
Some proponents of reform seek to increase the educational value and fairness of quiz bowl, primarily by using pyramidal questions. Many competitions at grade school levels are criticized for their use of speed-check questions, which encourage participants to rely more on their ability to buzz in quickly than on knowledge of the subjects tested. Some tournaments such as College Bowl are criticized for being insufficiently academic, including superfluous clues in their questions, and for recycling questions from previous years. The use of "hoses", misleading clues that discourage players from buzzing in too early, is also considered a mark of "bad" quiz bowl. The use of mathematical computation problems in tossups is criticized by some for rewarding fast problem solving skills over conceptual knowledge and being non-pyramidal. Pyramidal questions are sometimes criticized for containing obscure information and being unsuitable for television.
Broadcasting.
Quiz bowl shows have been on television for many years in some areas and usually feature competitors from local high schools. Many of these competitions may have rules and formats that differ slightly from standardized quiz bowl.
"College Bowl" was broadcast on NBC radio from 1953 to 1955. The program moved to television as "General Electric College Bowl" and was broadcast from 1959 to 1970, first on CBS and later on NBC. "College Bowl" would return to CBS radio from 1979 to 82, and HCASC was broadcast on BET from 1990 to 1995. The "Texaco Star National Academic Championship" ran from 1989 to 1991 on Discovery Channel and was hosted by Chip Beall and Mark L. Walberg. In 1994, it was syndicated as the "Star Challenge" and hosted by Mark Wahlberg. "University Challenge" is licensed from CBCI by Granada TV Ltd. and broadcast in the United Kingdom. Reach for the Top, a Canadian competition with a quiz bowl-like format, has been broadcast on the CBC in the past.
Game show contestants.
Quiz bowl has received media coverage due to the number of highly successful game show contestants with backgrounds in the activity. Despite this, most game shows have little resemblance to quiz bowl in both question content and gameplay. NAQT maintains a list of current and former quiz bowl players at any level who have appeared on TV game shows. Several of the top dollar winners in the history of "Jeopardy!" include former players such as Ken Jennings, David Madden, and Brad Rutter.

</doc>
<doc id="48994" url="http://en.wikipedia.org/wiki?curid=48994" title="Limerick (poetry)">
Limerick (poetry)

A limerick is a form of poetry, especially one in five-line anapestic meter with a strict rhyme scheme (AABBA), which is sometimes obscene with humorous intent. The first, second and fifth lines are usually longer than the third and fourth. The oldest attested text in this form is a Latin prayer by Thomas Aquinas of the 13th century. 
<poem>
Sit vitiorum meorum evacuatio
Concupiscentae et libidinis exterminatio,
Caritatis et patientiae,
Humilitatis et obedientiae,
Omniumque virtutum augmentatio.
</poem>
The form appeared in England in the early years of the 18th century. It was popularized by Edward Lear in the 19th century, although he did not use the term.
The following limerick is of unknown origin:
<poem>
The limerick packs laughs anatomical
Into space that is quite economical.
 But the good ones I've seen
 So seldom are clean 
And the clean ones so seldom are comical.
</poem>
Gershon Legman, who compiled the largest and most scholarly anthology, held that the true limerick as a folk form is always obscene, and cites similar opinions by Arnold Bennett and George Bernard Shaw, describing the clean limerick as a "periodic fad and object of magazine contests, rarely rising above mediocrity". From a folkloric point of view, the form is essentially transgressive; violation of taboo is part of its function.
Form.
The standard form of a limerick is a stanza of five lines, with the first, second and fifth rhyming with one another and having three feet of three syllables each; and the shorter third and fourth lines also rhyming with each other, but having only two feet of three syllables. The defining "foot" of a limerick's meter is usually the anapaest, ("ta-ta-TUM"), but catalexis (missing a weak syllable at the beginning of a line) and extra-syllable rhyme (which adds an extra unstressed syllable) can make limericks appear amphibrachic ("ta-TUM-ta").
The first line traditionally introduces a person and a place, with the place appearing at the end of the first line and establishing the rhyme scheme for the second and fifth lines. In early limericks, the last line was often essentially a repeat of the first line, although this is no longer customary.
Within the genre, ordinary speech stress is often distorted in the first line, and may be regarded as a feature of the form: "There "was" a young "man" from the "coast";" "There "once" was a "girl" from De"troit"…" Legman takes this as a convention whereby prosody is violated simultaneously with propriety. Exploitation of geographical names, especially exotic ones, is also common, and has been seen as invoking memories of geography lessons in order to subvert the decorum taught in the schoolroom; Legman finds that the exchange of limericks is almost exclusive to comparatively well-educated males, women figuring in limericks almost exclusively as "villains or victims". The most prized limericks incorporate a kind of twist, which may be revealed in the final line or lie in the way the rhymes are often intentionally tortured, or both. Many limericks show some form of internal rhyme, alliteration or assonance, or some element of word play.
Verses in limerick form are sometimes combined with a refrain to form a limerick song, a traditional humorous drinking song often with obscene verses.
<poem>The limerick’s an art form complex
Whose contents run chiefly to sex;
 It’s famous for virgins 
 And masculine urgin’s
And vulgar erotic effects.</poem>
David Abercrombie, a phonetician, takes a different view of the limerick, and one which seems to accord better with the form. It is this: Lines one, two, and five have three feet, that is to say three stressed syllables, while lines three and four have two stressed syllables. The number and placement of the unstressed syllables is rather flexible. There is at least one unstressed syllable between the stresses but there may be more – as long as there are not so many as to make it impossible to keep the equal spacing of the stresses. (See “the young man of Japan”, below.) As an example, in first line of the limerick given above, there are three unstressed syllables between the first and second stresses, two between the second and third, but only one unstressed syllable before the first stress. There may or may not be an unstressed syllable (or, rarely, two) after the final stress of the line. In the example above there are unstressed syllables at the end of lines three and four but not at the end of the remaining lines.
Moreover, it is intrinsic to the limerick that there be a silent stress at the end of lines one, three, and five. A silent stress occurs when the reader undergoes the physiological changes associated with a stress but without any sound. To understand this, imagine that a drum is struck each time there is a stress. Then, in English verse, the drum beats would be equally spaced, regardless of the number of unstressed syllables that separate them. However, in reading a limerick, after the third beat of the first line, the next beat falls at the end of the line, not on the first stress of the second line. Thus, it is perhaps better to think of the limerick as having four stresses (the final one silent) in lines one, two, and five – and two stresses, of course, in lines three and four.
Origin of the name.
The origin of the name "limerick" for this type of poem is debated. As of several years ago, its usage was first documented in England in 1898 ("New English Dictionary") and in the United States in 1902, but in recent years several earlier uses have been documented. The name is generally taken to be a reference to the City or County of Limerick in Ireland sometimes particularly to the Maigue Poets, and may derive from an earlier form of nonsense verse parlour game that traditionally included a refrain that included "Will [or won't] you come (up) to Limerick?"
The earliest known use of the term "limerick" for this type of poem is an 1880 reference, in a Saint John, New Brunswick newspaper, to an apparently well-known tune,
<poem>
There was a young rustic named Mallory,
who drew but a very small salary.
 When he went to the show,
 his purse made him go
to a seat in the uppermost gallery.
</poem>
"Tune: Won't you come to Limerick."
Edward Lear.
The limerick form was popularized by Edward Lear in his first "Book of Nonsense" (1846) and a later work, "More Nonsense, Pictures, Rhymes, Botany, etc". (1872) . Lear wrote 212 limericks, mostly considered nonsense literature. It was customary at the time for limericks to accompany an absurd illustration of the same subject, and for the final line of the limerick to be a variant of the first line ending in the same word, but with slight differences that create a nonsensical, circular effect. The humor is not in the "punch line" ending but rather in the tension between meaning and its lack.
The following is an example of one of Edward Lear's limericks.
<poem>
There was a Young Person of Smyrna
Whose grandmother threatened to burn her.
 But she seized on the cat,
 and said 'Granny, burn that!
You incongruous old woman of Smyrna!'
</poem>
Lear's limericks were often typeset in three or four lines, according to the space available under the accompanying picture.
Variations.
The idiosyncratic link between spelling and pronunciation in the English language is explored in this Scottish example (where the name "Menzies" is pronounced ).
<poem>
A lively young damsel named Menzies
Inquired: "Do you know what this thenzies?"
 Her aunt, with a gasp,
 Replied: "It's a wasp,
And you're holding the end where the stenzies."
</poem>
The limerick form is so well known that it can be parodied in fairly subtle ways. These parodies are sometimes called anti-limericks. The following example, of unknown origin, subverts the structure of the true limerick by changing the number of syllables in the lines.
<poem>
There was a young man of Japan
Whose limericks never would scan.
 When asked why this was,
 He replied "It's because
I always try to fit as many syllables into the last line as ever I possibly can."
</poem>
Other anti-limericks follow the meter of a limerick but deliberately break the rhyme scheme, like the following example, attributed to W.S. Gilbert, in a parody of a limerick by Lear:
<poem>
There was an old man of St. Bees,
Who was stung in the arm by a wasp,
 When asked, "Does it hurt?"
 He replied, "No, it doesn't,
I'm so glad that it wasn't a hornet."
</poem>
Comedian John Clarke has also parodied Lear's style:
<poem>
There was an old man with a beard,
A funny old man with a beard
 He had a big beard
 A great big old beard
That amusing old man with a beard.
</poem>
Web Cartoonist Zach Weiner, author of SMBC-Comics, wrote a reversed limerick that makes sense read top-to-bottom, and vice versa.
<poem>
This limerick goes in reverse
 Unless I'm remiss
 The neat thing is this:
If you start from the bottom-most verse
This limerick's not any worse
</poem>
External links.
Limerick bibliographies:

</doc>
<doc id="48997" url="http://en.wikipedia.org/wiki?curid=48997" title="Overseas Chinese">
Overseas Chinese

 
Overseas Chinese () are people of Chinese birth or descent who live outside the People's Republic of China and Republic of China (Taiwan). People of partial Chinese ancestry living outside the Greater China Area may also consider themselves overseas Chinese. Overseas Chinese can be of the Han Chinese ethnic majority, or from any of the other ethnic groups in China.
Terminology.
The Chinese language has various terms equivalent to the English "overseas Chinese" which refers to Chinese citizens residing in countries other than China: Huáqiáo () or "Hoan-kheh" in Hokkien (). The term "haigui" (海归) refers to returned overseas Chinese and "guīqiáo qiáojuàn" (归侨侨眷) to their returning relatives.
Huáyì () refers to ethnic Chinese residing outside of China. Another often-used term is 海外华人 (Hǎiwài Huárén), a more literal translation of "overseas Chinese"; it is often used by the PRC government to refer to people of Chinese ethnicities who live outside the PRC, regardless of citizenship.
Overseas Chinese who are ethnically Han Chinese, such as Cantonese, Hoochew, Hokkien, or Hakka refer to overseas Chinese as 唐人 (Tángrén), pronounced "tòhng yàn" in Cantonese, "toung ning" in Hoochew, "Tn̂g-lâng" in Hokkien, and "tong nyin" in Hakka. Literally, it means "Tang people", a reference to Tang dynasty China when it was ruling China proper. It should be noted that this term is commonly used by the Cantonese, Hoochew, Hakka and Hokkien as a colloquial reference to the Chinese people, and has little relevance to the ancient dynasty.
The term "shǎoshù mínzú" (少数民族) is added to the various terms for overseas Chinese to indicate those in the diaspora who would be considered ethnic minorities in China. The terms "shǎoshù mínzú huáqiáo huárén"; "shǎoshù mínzú huáqiáo huárén"; and "shǎoshù mínzú hǎiwài qiáobāo" (少数民族海外侨胞) are all in usage. The Overseas Chinese Affairs Office of the PRC does not distinguish between Han and ethnic minority populations for official policy purposes. For example, members of the Tibetan diaspora may travel to China on passes granted to certain overseas Chinese. Various estimates of the overseas Chinese minority population include 3.1 million (1993), 3.4 million (2004), 5.7 million (2001, 2010), or approximately one tenth of all overseas Chinese (2006, 2011). Cross-border ethnic groups (跨境民族, "kuàjìng mínzú") are not considered overseas Chinese minorities unless they left China "after" the establishment of an independent state on China's border.
History.
The Chinese people have a long history of migrating overseas. One of the migrations dates back to the Ming dynasty when Zheng He (1371–1435) became the envoy of Ming. He sent people - many of them Cantonese and Hokkien - to explore and trade in the South China Sea and in the Indian Ocean.
Chinese Civil War.
When China was under the imperial rule of the Qing Dynasty, subjects who left the Qing Empire without the Administrator's consent were considered to be traitors and were executed. Their family members faced consequences as well. However, the establishment of the Lanfang Republic () in West Kalimantan, Indonesia, as a tributary state of Qing China, attests that it was possible to attain permission. The republic lasted until 1884, when it fell under Dutch occupation as Qing influence waned.
Under the administration of the Republic of China from 1911-1949, these rules were abolished and many migrated outside of the Republic of China, mostly through the coastal regions via the ports of Fujian, Guangdong, Hainan and Shanghai. These migrations are considered to be among the largest in China's history. Many nationals of the Republic of China fled and settled down in South East Asia mainly between the years 1911-1949, after the Nationalist government led by Kuomintang lost to the Communist Party of China in the Chinese Civil War in 1949. Most of the nationalist and neutral refugees fled Mainland China to South East Asia(Singapore, Malaysia, Philippines, Brunei and Indonesia) as well as Taiwan, Republic of China. Many nationalists who stayed behind were persecuted or even executed.
Most of the Chinese who fled during 1911 – 1949 under the Republic of China settled down in Singapore, Malaysia and automatically gain citizenship in 1957 and 1963 as these countries gained independence.Kuomintang members who settled down in Malaysia and Singapore played a major role in the establishment of Malaysian Chinese Association. There is some evidence that they intend to reclaim mainland China from the Communists by funding the Kuomintang in China.
Waves of immigration.
Different waves of immigration led to subgroups among overseas Chinese such as the new and old immigrants in Southeast Asia, North America, Oceania, the Caribbean, Latin America, South Africa, and Russia.
In the 19th century, the age of colonialism was at its height and the great Chinese diaspora began. Many colonies lacked a large pool of laborers. Meanwhile, in the provinces of Fujian and Guangdong in China, there was a surge in emigration as a result of the poverty and ruin caused by the Taiping rebellion. The Qing Empire was forced to allow its subjects to work overseas under colonial powers. Many Hokkien chose to work in Southeast Asia (where they had earlier links starting from the Ming era), as did the Cantonese. The city of Taishan in Guangdong province was the source for many of the economic migrants. For the countries in North America and Australasia, great numbers of laborers were needed in the dangerous tasks of gold mining and railway construction. Widespread famine in Guangdong impelled many Cantonese to work in these countries to improve the living conditions of their relatives. Some overseas Chinese were sold to South America during the Punti-Hakka Clan Wars (1855–1867) in the Pearl River Delta in Guangdong. After World War II many people from the New Territories in Hong Kong emigrated to the UK (mainly England) and to the Netherlands to earn a better living.
From the mid-19th century onward, emigration has been directed primarily to Western countries such as the United States, Canada, Australia, New Zealand, Brazil, and the nations of Western Europe; as well as to Peru where they are called "tusán", Panama, and to a lesser extent to Mexico. Many of these emigrants who entered Western countries were themselves overseas Chinese, particularly from the 1950s to the 1980s, a period during which the PRC placed severe restrictions on the movement of its citizens. In 1984, Britain agreed to transfer the sovereignty of Hong Kong to the PRC; this triggered another wave of migration to the United Kingdom (mainly England), Australia, Canada, USA, Latin America and other parts of the world. The Tiananmen Square protests of 1989 further accelerated the migration. The wave calmed after Hong Kong's transfer of sovereignty in 1997. In addition, many citizens of Hong Kong hold citizenships or have current visas in other countries so if the need arises, they can leave Hong Kong at short notice. In fact, after the Tiananmen Square incident, the lines for immigration visas increased at every consulate in Hong Kong. More recent Chinese presences have developed in Europe, where they number nearly a million, and in Russia, they number over 600,000, concentrated in Russian Far East. Chinese who emigrated to Vietnam beginning in the 18th century are referred to as "Hoa".
As of 2006, only 76,700 of the old Chinese community remained in South Korea. However, as of 2013, there were 420,000 of ethnic Korean descent.
In recent years, the People's Republic of China has built increasingly stronger ties with African nations. As of August 2007, there were an estimated 750,000 Chinese nationals working or living for extended periods in different African countries. An estimated 200,000 ethnic Chinese live in South Africa. In a 2007 "New York Times" article, Chad Chamber of Commerce Director estimated an "influx of at least 40,000 Chinese in coming years" to Chad. s of 2006[ [update]] as many as 40,000 Chinese lived in Namibia, an estimated 80,000 Chinese in Zambia and 50,000 Chinese in Nigeria. As many as 100,000 Chinese live and work across Angola. s of 2009[ [update]] 35,000 Chinese migrant workers lived in Algeria.
Russia’s main Pacific port and naval base of Vladivostok, once closed to foreigners and belonged to China until the late 19th century, as of 2010[ [update]] bristles with Chinese markets, restaurants and trade houses. Experts predict that the Chinese diaspora in Russia will increase to at least 10 million by 2010 and Chinese may become the dominant ethnic group in the Russian Far East region 20 to 30 years from now.
Other experts discount such stories estimating the numbers of Chinese in Russia at less than half a million, most of whom are temporary traders.
A growing Chinese community in Germany consists of around 76,000 people as of 2010[ [update]]. An estimated 15,000 to 30,000 Chinese live in Austria.
Occupations.
The Chinese in Southeast Asian countries have established themselves in commerce and finance. In North America, Europe and Oceania, occupations are diverse and impossible to generalize; ranging from catering to significant ranks in medicine, the arts, and academia.
Overseas Chinese experience.
The Chinese usually identify a person by ethnic origin instead of nationality. As long as the person is of Chinese descent, that person is considered Chinese, and if that person lives outside of China, that person is overseas Chinese. The majority of PRC Chinese do not understand the overseas Chinese experience of being a minority, as ethnic Han Chinese comprise approximately 91% of the population.
Discrimination.
Overseas Chinese have often experienced hostility and discrimination.
In countries with small Chinese minorities, the economic disparity can be remarkable. For example, in 1998, ethnic Chinese made up just 1% of the population of the Philippines and 4% of the population in Indonesia, but have wide influence in Philippines and Indonesian private economy. The book "", describing the Chinese as a "market-dominant minority", notes that "Chinese market dominance and intense resentment amongst the indigenous majority is characteristic of virtually every country in Southeast Asia except Thailand and Singapore". (Chinese market dominance is present in Thailand, which is noted for its lack of resentment, and Singapore is majority ethnic Chinese.)
This asymmetrical economic position has incited anti-Chinese sentiment among the poorer majorities. Sometimes the anti-Chinese attitudes turn violent, such as the May 13 Incident in Malaysia in 1969 and the Jakarta riots of May 1998 in Indonesia, in which more than 2,000 people died, mostly rioters burned to death in a shopping mall. During the colonial era, some genocides killed tens of thousands of Chinese.
During the Indonesian killings of 1965–66, in which more than 500,000 people died, ethnic Chinese were killed and their properties looted and burned as a result of anti-Chinese racism on the excuse that Dipa "Amat" Aidit had brought the PKI closer to China. The anti-Chinese legislation was in the Indonesian constitution until 1998.
It is commonly held that a major point of friction is the apparent tendency of overseas Chinese to segregate themselves into a subculture. For example, the anti-Chinese Kuala Lumpur Racial Riots of 13 May 1969 and Jakarta Riots of May 1998 were believed to have been motivated by these racially-biased perceptions. This analysis has been questioned by some historians, most notably Dr. Kua Kia Soong, the principal of New Era College, who has put forward the controversial argument that the May 13 Incident was a pre-meditated attempt by sections of the ruling Malay elite to incite racial hostility in preparation for a coup. In 2006, rioters damaged shops owned by Chinese-Tongans in Nukuʻalofa. Chinese migrants were evacuated from the riot-torn Solomon Islands.
Ethnic politics can be found to motivate both sides of the debate. In Malaysia, overseas Chinese tend to support equal and meritocratic treatment on the expectation that they would not be discriminated against in the resulting competition for government contracts, university places, etc., whereas many "Bumiputra" ("native sons") Malays oppose this on the grounds that their group needs such protections in order to retain their patrimony. The question of to what extent ethnic Malays, Chinese, or others are "native" to Malaysia is a sensitive political one. It is currently a taboo for Chinese politicians to raise the issue of Bumiputra protections in parliament, as this would be deemed ethnic incitement.
Many of the overseas Chinese who worked on railways in North America in the 19th century suffered from racial discrimination in Canada and the United States. Although discriminatory laws have been repealed or are no longer enforced today, both countries had at one time introduced statutes that barred Chinese from entering the country, for example the United States Chinese Exclusion Act of 1882 (repealed 1943) or the Canadian Chinese Immigration Act, 1923 (repealed 1947).
In Australia, Chinese were targeted by a system of discriminatory laws known as the 'White Australia Policy' which was enshrined in the Immigration Restriction Act of 1901. The policy was formally abolished in 1973, and in recent years Australians of Chinese background have publicly called for an apology from the Australian Federal Government similar to that given to the 'stolen generations' of indigenous people in 2007 by the then Prime Minister Kevin Rudd.
Assimilation.
Overseas Chinese vary widely as to their degree of assimilation, their interactions with the surrounding communities (see Chinatown), and their relationship with China.
Thailand has the largest overseas Chinese community and is also the most successful case of full assimilation, and they claim Thai identity. For over 400 years, Thai-Chinese have largely intermarried and assimilated with their compatriots. The present Thai monarch, Chakri Dynasty, is founded by King Rama I who himself is partly Chinese. His predecessor, King Taksin of the Thonburi Kingdom, is the son of a Chinese immigrant from Guangdong Province and was born with Chinese name. His mother, Lady Nok-iang (Thai: นกเอี้ยง), was Thai (and was later awarded the feudal title of Somdet Krom Phra Phithak Thephamat).
In the Philippines, Chinese from Guangdong were already migrating to the islands from the 9th century, and have largely intermarried with either indigenous Filipinos or Spanish colonisers. Their descendants would eventually form the bulk of the elite and ruling classes in a sovereign Philippines. Since the 1860s, most Chinese immigrants have come from Fujian; unlike earlier migrants, Fujianese settlers rarely intermarried, and thus form the bulk of the "unmixed" Chinese Filipinos. Older generations have retained Chinese traditions and the use of Minnan (Hokkien), while the vast majority of younger generations largely communicate in English, Filipino, and other Philippine languages, and have largely layered facets of both Western and Filipino culture onto their Chinese cultural background.
In Myanmar, the Chinese rarely intermarry (even amongst different Chinese linguistic groups), but have largely adopted the Burmese culture whilst maintaining Chinese cultural affinities.
In Cambodia, between 1965 to 1993, people with Chinese names were prevented from finding governmental employment, leading to a large number of people changing their names to a local, Cambodian name. Indonesia, and Myanmar were among the countries that do not allow birth names to be registered in foreign languages, including Chinese. But since 2003, the Indonesian government has allowed overseas Chinese to use their Chinese name or using their Chinese family name on their birth certificate.
In Vietnam, Chinese names are pronounced with Sino-Vietnamese readings. For example, the name of the previous Chinese president, 胡锦涛 (pinyin: Hú Jǐntāo), would be transcribed as "Hồ Cẩm Đào". In Western countries, the overseas Chinese generally use romanised versions of their Chinese names, and the use of local first names is also common. Vietnamese people have adopted the Chinese traditions, ancient Chinese characters, philosophy such as Confucianism, Taoism for thousands of years during the rule of China until the establishment of Ngo dynasty (Han-Nom: 吳朝), it is easier for the Hoa people to adopt the Vietnamese culture due to their similarities, however some Hoa still prefer maintaining Chinese cultural background (See Sinic world or Adoption of Chinese literary culture). The official census from 2009 accounted the Hoa population at some 823,000 individuals and ranked 6th in terms of its population size. 70% of the Hoa live in cities and towns, mostly in Ho Chi Minh city while the remainder live in the countryside in the southern provinces.
On the other hand, in Malaysia, Singapore, and Brunei, overseas Chinese have maintained a distinct communal identity, though the rate and state of being assimilated to the local (in this case multicultural) society, is currently on par with that of other Chinese communities (see Peranakan).
In East Timor, a large fraction of Chinese are of Hakka descent.
Language.
The usage of Chinese languages by overseas Chinese has been determined by a large number of factors, including their ancestry, their migrant ancestors' "regime of origin", assimilation through generational changes, and official policies of their country of residence. The general trend is that more established Chinese populations in the Western world and in many regions of Asia have Cantonese as either the dominant language or common community vernacular, while Mandarin is much more prevalent among new arrivals, making it increasingly common in many Chinatowns.
Southeast Asia.
Within Southeast Asia, Cantonese has traditionally served as the "lingua franca" among overseas Chinese across most of the region and within many of its countries. However, the language situation of overseas Chinese can vary greatly amongst neighboring nations or even within.
Singapore.
Singapore has an ethnic Chinese majority population, with Mandarin recognized as one of its official languages. Furthermore, simplified Chinese characters are used in contrast to other overseas Chinese communities, which almost exclusively use traditional Chinese characters. Although the majority of ethnic Chinese in Singapore are predominantly of Hokkien descent and Hokkien has historically been the most spoken Chinese variety, the government of Singapore discourages the usage of non-Mandarin Chinese languages through the Speak Mandarin Campaign (SMC). The Singaporean government also actively promotes English as the common language of the multiracial society of Singapore, with younger Chinese Singaporeans being mostly bilingual in Mandarin and English, while the older generations speak other Chinese dialects.
Under the SMC policy, all nationally produced non-Mandarin Chinese TV and radio programs were stopped after 1979. Additionally, Hong Kong (Cantonese) and Taiwanese dramas are unavailable in their original languages on non-cable TV. Nevertheless, since the government restriction on dialect media was relaxed in the mid-1990s, these media have become available once on again on cable TV and sold in stores. However, only Cantonese seems to have benefited from this uplift, thanks to a large following of Hong Kong popular culture, such as television dramas, cinema and Cantopop. Consequently, there has been a substantial of number of non-Cantonese Chinese Singaporeans being able to understand or speak the language, with a number of educational institutes offering Cantonese as an elective language course. Meanwhile, the number of speakers for other non-Mandarin Chinese varieties continues to decline.
Malaysia.
Malaysia is the only country besides Mainland China and Taiwan that has a complete Chinese education system, from primary school to university. Malaysian Chinese speak a wide variety of dialects, which are concentrated around particular population centers. Hokkien, the largest Chinese group, is concentrated in Penang, Klang, Kelantan and Malacca, with Penang having its own Hokkien variety. Cantonese is centered on Kuala Lumpur, Seremban, Kuantan and Ipoh, with Hakka minorities also found. Meanwhile, in East Malaysia (Malaysian Borneo), Hokkien, Teochew, Hakka and Mandarin are found except in Sibu, where the Fuzhou dialect is predominant, and in Sandakan, where Cantonese and Hakka are widely spoken.
Regardless of location, however, younger generations are educated in the Malaysian standard of Mandarin at Chinese-language schools. Also, most Chinese Malaysians can speak both Malay, the national language, and English, which is widely used in business and at tertiary level. Furthermore, Cantonese is understood by most Malaysian Chinese as it is the prevalent language used in Chinese-language media, although many are unable to speak it.
Indonesia.
Ethnic Chinese in Indonesia had been subjected for decades to official, and at times discriminatory, assimilation policies. As a result, a large number are no longer proficient in a Chinese language. Originally, the majority of the population emigrated from Fujian and Guangdong provinces in South China, with the first wave of arrivals preceding the Dutch colonial period in the 1700s. The four recognized varieties of Chinese spoken by the Chinese Indonesian community are, ordered by number of speakers: Hokkien, Hakka, Mandarin and Cantonese. Additionally, Teochew and Puxian Min are also found.
The distribution of Chinese variants are scattered throughout the archipelago. On Sumatra, two varieties of Hokkien exist, Medan Hokkien and Riau Hokkien, which incorporate local and Indonesian vocabulary. Hakka Chinese is concentrated in Bangka Belitung province, South Sumatra and West Kalimantan where they form a significant part of the local population. Meanwhile, Pontianak to Kendawangan on the southern tip of West Kalimantan is populated by Teochew speakers. Mandarin and historically Cantonese have been used in Chinese-language schools and both variants are found major cities such as Jakarta and Medan, with Mandarin usage increasing with recent Chinese arrivals. Younger generations of Indonesian Chinese are generally fluent in Indonesian.
Thailand.
Although Thailand is home to the largest Overseas Chinese community, the level of integration is also the highest and most Thai Chinese today speak Thai as their native or main language. Most ethnic Chinese live in major cities such as Bangkok, Chiang Mai, Phuket, Hat Yai and Nakhon Sawan, and Chinatowns in these cities still feature signage in both Chinese and Thai. As of the 2000s, only a little over 200,000 Thai Chinese still speak a variant of Chinese at home. A little over half speak Teochew, the largest dialect group, followed by Hakka, Hainanese, Cantonese, and Hokkien. In commerce, Teochew, Cantonese and Thai are used as common languages and Chinese-language schools often use Cantonese as the medium of instruction due to its "lingua franca" status among most Chinese in Southeast Asia.
Vietnam.
Ethnic Chinese in Vietnam are categorized into three groups that are based on migrant history, location and level of integration. The largest group is the Hoa, numbering almost a million individuals and have historically been influential in Vietnamese society and economy. They are largely concentrated in major cities of the former South Vietnam (especially in Ho Chi Minh City) and largely speak Cantonese, with Teochew found among a significant minority.
The smaller two Chinese groups consist of the San Diu and Ngái. The San Diu number over 100,000 and are concentrated in the mountains of northern Vietnam. They actually trace their origins to Yao people rather than Han Chinese, but nevertheless have been heavily influenced by Chinese culture and speak a variant of Cantonese. Meanwhile, the Ngái are concentrated in rural areas of Central Vietnam and number around 1,000. They speak Hakka natively and use Cantonese to communicate with Hoa communities.
Cambodia.
A 2013 census estimated there to be 15,000 ethnic Chinese in Cambodia. However, Chinese community organizations have estimated that up to around 7% of the population may have Chinese ancestry. Chinese Cambodians have historically played important economic and political roles in the country and are still often overrepresented in Cambodian commerce. 
As a vast majority of the group emigrated from Cambodia following the Khmer Rouge, the community has assimilated greatly into Cambodian society and many now speak Khmer as their main language. Over three-fourths of Chinese Cambodians belong to the Teochew group, which is also the most spoken Chinese variant. The other two largest groups include Hokkien and Hainanese. Cantonese formed the largest group from the 17th to the mid-20th century, but form only a minority today and are concentrated in major urban centers. Nevertheless, Cantonese continues to serve as the common community language among most Chinese Cambodians. In Chinese-language schools, Mandarin is taught.
Laos.
Among the small ethnic Chinese community of Laos, Teochew and Cantonese are the two most spoken Chinese languages. Ethnic Chinese living on the border with China speak Southwest varieties of Mandarin.
Myanmar.
Although the Burmese Chinese (or Chinese Burmese) officially make up three percent of the population, the actual figure is believed to be much higher. Among the under-counted Chinese populations are: those of mixed background; those that have registered themselves as ethnic Bamar to escape discrimination; illegal Chinese immigrants that have flooded Upper Burma since the 1990s (up to 2 million by some estimates) but are not counted due to the lack of reliable census taking. The Burmese Chinese dominate the Burmese economy today. They also have a very large presence in Burmese higher education, and make up a high percentage of the educated class in Burma. Most Burmese Chinese speak Burmese as their mother tongue. Those with higher education also speak Mandarin and/or English. The use of Chinese dialects still prevails. Hokkien (a dialect of Min Nan) is mostly used in Yangon as well as in Lower Burma, while Taishanese (a Yue dialect akin to Cantonese) and Yunnanese Mandarin are well preserved in Upper Burma.
Brunei.
A variety of Chinese dialects are spoken in Brunei. Mandarin and Hokkien are the most commonly spoken dialects in the country.
Philippines.
Chinese Filipinos officially comprise 1.5% of the country's population, although demographic surveys from third parties find that 18-27% of the Philippine population have at least some Chinese ancestry, totalling up to 27 million people.
Most Chinese Filipinos are trilingual, speaking a Chinese, English, and a Philippine language (most often Tagalog or Cebuano). Older Chinese Filipinos generally prefer to use Chinese, whereas younger generations prefer to use either English or a Philippine language, a result of the prohibition of Chinese language education enacted during the dictatorship of President Marcos (1972–1986).
The most widely spoken Chinese language is Hokkien, specifically a Filipino variant of it called Lan-nang. Other Hokkien dialects and Chinese variants such as Cantonese, Shanghainese, and Teochew are also spoken, albeit by a very tiny population. In contrast to much of Southeast Asia, the Chinese community in the Philippines does not use Cantonese as its preferred community language, but rather Philippine Hokkien, which is also spoken informally at schools and in business among Chinese Filipinos.
In Chinese-language schools, Mandarin is taught as "Standard Chinese", although most Chinese Filipinos do not speak it at home and do not attain the same level of fluency as those of Chinese descent in China, Taiwan, and Singapore.
Due to extensive albeit informal contacts with the Ministry of Education of the Taiwan (ROC) during 1950-1990, the traditional Chinese script as well as the bopomofo are still used, although these are gradually being eased out in favor of simplified Chinese characters and pinyin starting 2005, with Chinese Language textbooks increasingly imported from both China and Singapore.
As part of a recent trend, partly due to increased contacts with other overseas Chinese in Hong Kong and Singapore, more Chinese Filipino families are now opting to use English as their first language at home. There is also a trend among some young Chinese Filipinos to relearn Hokkien, a result of increasing pride in being "ethnic Chinese" and the popularity of Taiwanese films and shows, which is associated with the rise of China in the 21st century.
Despite the perceived widespread assimilation of the Chinese Filipinos into the general Philippine population, most still form part of a "Tsinoy" community where Chinese culture is celebrated and practiced. Despite the fact that not all Chinese Filipinos can fluently speak Hokkien or any other Chinese variant, most can still understand at least some Hokkien.
On the other hand, most Chinese Mestizos (called "chhut-si-ia" in Hokkien), or those who are of mixed Chinese and Filipino, Spanish, and/or American ancestry, tend to downplay their Chinese roots and invariably consider themselves Filipino. Most Chinese Mestizos speak Tagalog or English.
North America.
Many overseas Chinese populations in North America speak some variety of Chinese. In the United States and Canada, a Chinese language is the third most spoken language. Yue dialects have historically been the most prevalent variety due to immigrants being mostly from southern China from the 19th century up through the 1980s. However, Mandarin is becoming increasingly more prevalent due to the opening up of the PRC.
In New York City at least, although Mandarin is spoken as a native language among only 10% of Chinese speakers, it is used as a secondary dialect among the greatest number of them and is on its way to replace Cantonese as their lingua franca. Although Min Chinese or Hoochew, the majority of Min Chinese, is spoken natively by a third of the Chinese population there, it is not used as a lingua franca because speakers of other dialect groups do not learn Min.
In Richmond (part of the Greater Vancouver metropolitan area in Canada), 44% of the population is Chinese. Chinese words can be seen everywhere from local banks to grocery stores. In the broader Vancouver Census Metropolitan Area, 18% of the population is Chinese. Similarly in Toronto, which is the largest city in Canada, Chinese people make up 11.4% of the local population with the higher percentages of between 20-50% in the suburbs of Markham, Richmond Hill and within the city's east end, Scarborough. Cantonese and Mandarin are the most popular Chinese languages.
Economic growth in the People's Republic of China has given mainland Chinese more opportunities to emigrate. A 2011 survey showed that 60% of Chinese millionaires plan to emigrate, mostly to the USA or Canada. The EB-5 Investment Visa allows many powerful Chinese to seek U.S. citizenship, and recent reports show that 75% of applicants for this visa in 2011 were Chinese. Chinese multimillionaires benefited most from the EB-5 Immigrant Investor Program in the U.S. Now, as long as one has at least US$500,000 to invest in projects listed by United States Citizenship and Immigration Services (USCIS), where it is possible to get an EB-5 green card that comes with permanent U.S. residency rights, but only in states specified by the pilot project.
Relationship with China.
Both the People's Republic of China and Taiwan maintain highly complex relationships with overseas Chinese populations. Both maintain cabinet level ministries to deal with overseas Chinese affairs, and many local governments within the PRC have overseas Chinese bureaus. Both the PRC and ROC have some legislative representation for overseas Chinese. In the case of the PRC, some seats in the National People's Congress are allocated for returned overseas Chinese. In the ROC's Legislative Yuan, there used to be eight seats allocated for overseas Chinese. These seats were apportioned to the political parties based on their vote totals in the ROC, and then the parties assigned the seats to overseas Chinese party loyalists. Now, political parties in the ROC are still allowed to assign overseas Chinese into the Legislative Yuan, but they are not required to. Most of these members elected to the Legislative Yuan hold dual citizenship, but must renounce their foreign citizenship before being sworn in.
Overseas Chinese have sometimes played an important role in Chinese politics. Most of the funding for the Chinese revolution of 1911 came from overseas Chinese.
During the 1950s and 1960s, the ROC tended to seek the support of overseas Chinese communities through branches of the Kuomintang based on Sun Yat-sen's use of expatriate Chinese communities to raise money for his revolution. During this period, the People's Republic of China tended to view overseas Chinese with suspicion as possible capitalist infiltrators and tended to value relationships with southeast Asian nations as more important than gaining support of overseas Chinese, and in the Bandung declaration explicitly stated that overseas Chinese owed primary loyalty to their home nation. On the other hand, overseas Chinese in their home nations were often persecuted for suspected or fabricated ties to "Communist China". This was used as a pretext for the massacres of ethnic Chinese in Indonesia and other Southeast Asian countries.
After the Deng Xiaoping reforms, the attitude of the PRC toward overseas Chinese changed dramatically. Rather than being seen with suspicion, they were seen as people who could aid PRC development via their skills and capital. During the 1980s, the PRC actively attempted to court the support of overseas Chinese by among other things, returning properties that had been confiscated after the 1949 revolution. More recently PRC policy has attempted to maintain the support of recently emigrated Chinese, who consist largely of Chinese seeking graduate education in the West. Many overseas Chinese are now investing in People's Republic of China providing financial resources, social and cultural networks, contacts and opportunities.
The Nationality Law of the People's Republic of China, which does not recognise dual citizenship, provides for automatic loss of PRC citizenship when a former PRC citizen both settles in another country "and" acquires foreign citizenship. For children born overseas of a PRC citizen, whether the child receives PRC citizenship at birth depends on whether the PRC parent has settled overseas: "Any person born abroad whose parents are both Chinese nationals or one of whose parents is a Chinese national shall have Chinese nationality. But a person whose parents are both Chinese nationals and have both settled abroad, or one of whose parents is a Chinese national and has settled abroad, and who has acquired foreign nationality at birth shall not have Chinese nationality" (Art 5).
By contrast, the Nationality Law of the Republic of China, which both permits and recognises dual citizenship, considers these persons to be citizens of the ROC.
Returning and re-emigration.
With People's Republic of China's growing economic strength and the influence on the world, many overseas Chinese have begun to migrate back to China even though many mainland Chinese millionaires are considering emigrating out of the nation for better opportunities.
With China being the second largest economy in the world, this trend is expected to rise even more in the future as China's vigorous economy is poised to surpass the United States in the upcoming decade. For instance, in the case of Indonesia and Burma, political and ethnic strife has cause a significant number of people of Chinese origins to re-emigrate. Other Southeast Asian countries with large Chinese communities such as Malaysia, the economic rise of People's Republic of China has made it an attractive destination for many Malaysian Chinese to re-emigrate. As the Chinese economy opens up, Malaysian Chinese act as a bridge because many Malaysian Chinese are educated in the United States or Britain but can also understand the Chinese language and culture making it easier for potential entrepreneurial and business to be done between the people among the two countries.
Economic impact.
Overseas Chinese are estimated to control 1.5 to 2 trillion USD in liquid assets and have considerable amounts of wealth to stimulate economic power in China. Overseas Chinese often send remittances back home to family members to help better them financially and socioeconomically. China ranks second after India of top remittance receiving countries in 2010 with over 51 billion USD sent. The overseas Chinese business community of Southeast Asia, known as the bamboo network, has a prominent role in the region's private sectors.
Current numbers.
There are over 50 million overseas Chinese. Most overseas Chinese are living in Southeast Asia where they make up a majority of the population of Singapore and significant minority populations in Thailand, Malaysia, Indonesia, Brunei, the Philippines, and Vietnam. The overseas populations in those areas arrived between the 16th and 19th centuries mostly from the maritime provinces of Guangdong and Fujian, followed by Hainan. There were incidences of earlier emigration from the 15th centuries in particular to Malacca.
Urban areas with large Chinese populations within Asia include Bangkok with 2,900,000 (2009 census, registered resident only), Singapore with 2,800,000 (2010 census), Kuala Lumpur with 612,277 (2000 census, city only), Penang with 650,000 (2005), and Jakarta with 528,300 (2010 census). In the United States, according to the 2012 Census estimates, the three metropolitan areas with the largest Chinese American populations were the Greater New York Combined Statistical Area at 735,019 people, the San Jose-San Francisco-Oakland Combined Statistical Area at 629,243 people, and the Greater Los Angeles Combined Statistical Area at about 566,968 people. In Canada, the Greater Toronto Area had 486,300 Chinese (2006 Census, metropolitan area), while the Greater Vancouver Area had 402,000 (2006 Census, metropolitan area).

</doc>
<doc id="48998" url="http://en.wikipedia.org/wiki?curid=48998" title="591">
591

Year 591 (DXCI) was a common year starting on Monday (link will display the full calendar) of the Julian calendar. The denomination 591 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="48999" url="http://en.wikipedia.org/wiki?curid=48999" title="Chinese American">
Chinese American

Chinese Americans, also known as American Chinese or Sino-Americans, are Americans of full or partial Chinese – particularly Han Chinese – descent. Chinese Americans constitute one group of overseas Chinese and also a subgroup of East Asian Americans, which is further a subgroup of Asian Americans. Many Chinese Americans are immigrants along with their descendants from Mainland China, Hong Kong, Macau, Taiwan, as well as from other countries that include large populations of the Chinese diaspora.
Demographic research tends to categorize overseas Chinese who have immigrated from South East Asia and South America and immigrants from mainland China, Hong Kong, Macau, and Taiwan as Chinese Americans; however, both the governments of the Republic of China and the United States refer to Taiwanese Americans as a separate subgroup of Chinese Americans.
The Chinese American community is the largest overseas Chinese community outside of Asia. It is also the fourth largest in the Chinese diaspora, behind the Chinese communities in Thailand, Malaysia, and Indonesia. The Chinese American community comprises the largest and oldest ethnic group of Asian Americans, comprising 25.9% of the Asian American population as of 2010. Americans of Chinese descent, including those with partial Chinese ancestry constitute 1.2% of the total U.S. population as of 2010. According to the 2010 census, the Chinese American population numbered approximately 3.8 million. In 2010, half of Chinese-born people living in the United States lived either in California or New York State.
History.
The first Chinese immigrants arrived in 1820, according to U.S. government records. 325 men are known to have arrived before the 1849 California Gold Rush, which drew the first significant number of laborers from China who mined for gold and performed menial labor. There were 25,000 immigrants by 1852, and 105,465 by 1880, most of whom lived on the West Coast. They formed over a tenth of California's population. Nearly all of the early immigrants were young males with low educational levels from six districts in Guangdong Province.
The Chinese came to California in large numbers during the California Gold Rush, with 40,400 being recorded as arriving from 1851–1860, and again in the 1860s, when the Central Pacific Railroad recruited large labor gangs, many on five-year contracts, to build its portion of the Transcontinental Railroad. The Chinese laborers worked out well and thousands more were recruited until the railroad's completion in 1869. Chinese labor provided the massive workforce needed to build the majority of the Central Pacific's difficult route through the Sierra Nevada mountains and across Nevada.
The Chinese population rose from 2,716 in 1851 to 63,000 by 1871. In the decade 1861-70, 64,301 were recorded as arriving, followed by 123,201 in 1871-80 and 61,711 in 1881-1890. 77% were located in California, with the rest scattered across the West, the South, and New England. Most came from Southern China looking for a better life, escaping a high rate of poverty left after the Taiping Rebellion.
The initial immigration group may have been as high as 90% male, because most immigrants came with the thought of earning money, and then returning to China to start a family. Those that stayed in America faced the lack of suitable Chinese brides, because Chinese women were not allowed to immigrate to the US in significant numbers after 1872. As a result, many isolated mostly-bachelor communities slowly aged in place with very low Chinese birth rates. Later, as a result of the Fourteenth Amendment and the 1898 "United States v. Wong Kim Ark" Supreme Court decision, ethnic Chinese born in the United States became American citizens.
During and after World War II, severe immigration restrictions were eased as the United States allied with China against Japanese expansionism. Later reforms in the 1960s placed increasing value on family unification, allowing relatives of U.S. citizens to receive preference in immigration.
The Chinese American experience has been documented at the Museum of Chinese in America in Manhattan's Chinatown since 1980.
Demographics.
Statistics of the Chinese population in the United States (1840–present).
The chart on the right shows the total number of ethnic Chinese in the United States since 1850.
According to the 2012 Census estimates, the three metropolitan areas with the largest Chinese American populations were the Greater New York Combined Statistical Area at 735,019 people, the San Jose-San Francisco-Oakland Combined Statistical Area at 629,243 people, and the Greater Los Angeles Combined Statistical Area at about 566,968 people. New York City is home to the highest Chinese American population of any city proper (522,619), while the Los Angeles County city of Monterey Park has the highest percentage of Chinese Americans of any municipality, at 43.7% of its population, or 24,758 people.
The states with the largest estimated Chinese American populations, according to both the 2010 Census, were California (1,253,100; 3.4%), New York (577,000; 3.0%), Texas (157,000; 0.6%), New Jersey (134,500; 1.5%), Massachusetts (123,000; 1.9%), Illinois (104,200; 0.8%), Washington (94,200; 1.4%), Pennsylvania (85,000; 0.7%), Maryland (69,400; 1.2%), Virginia (59,800; 0.7%), and Ohio (51,033; 0.5%). The state of Hawaii has the highest concentration of Chinese Americans at 4.0%, or 55,000 people.
The New York City Metropolitan Area, consisting of New York City, Long Island, and nearby areas within the states of New York, New Jersey, Connecticut, and Pennsylvania, is home to the largest Chinese American population of any metropolitan area within the United States, enumerating 682,265 individuals as of the 2010 United States Census, and including at least nine Chinatowns. Continuing significant immigration from Mainland China, both legal and illegal in origin, has spurred the ongoing rise of the Chinese American population in the New York metropolitan area; this immigration continues to be fueled by New York's status as an alpha global city, its high population density, its extensive mass transit system, and the New York metropolitan area's enormous economic marketplace.
Also on the East Coast, the Washington, Boston, and Philadelphia metropolitan areas have significant Chinese American communities. The Washington, D.C. suburbs of Montgomery County, Maryland, and Fairfax County, Virginia, are 3.9% and 2.4% Chinese American, respectively. Boston's Chinatown is the only historical Chinese neighborhood within New England. The Boston suburb of Quincy also has a prominent Chinese American population, especially within the North Quincy area.
San Francisco, California has the highest per capita concentration of Chinese Americans of any major city in the United States, at an estimated 21.4%, or 172,181 people, and contains the second-largest total number of Chinese Americans of any U.S. city. San Francisco's Chinatown was established in the 1840s, making it the oldest Chinatown in North America and one of the largest neighborhoods of Chinese people outside of Asia, composed in large part by immigrants hailing from Guangdong province and also many from Hong Kong. The San Francisco neighborhoods of Sunset District and Richmond District also contain significant Chinese populations.
In addition to the big cities, smaller pockets of Chinese Americans are also dispersed in rural towns, often university-college towns, throughout the United States. For example, the number of Chinese Americans, including college professors, doctors, professionals, and students, has increased over 200% from 2005 to 2010 in Providence, Rhode Island, a small city with a large number of colleges.
Income and social status of these Chinese-American locations vary widely. Although many Chinese Americans in Chinatowns of large cities are often members of an impoverished working class, others are well-educated upper-class people living in affluent suburbs. The upper and lower-class Chinese are also widely separated by social status and class discrimination. In California's San Gabriel Valley, for example, the cities of Monterey Park and San Marino are both Chinese American communities lying geographically close to each other but they are separated by a large socio-economic and income gap.
A third of a million Chinese Americans are not United States citizens.
Significant Chinese population centers.
A list of large cities (250,000+ residents) with a Chinese-American population in excess of 1% of the general population in 2010.
Influence on American culture.
Some noteworthy historical Chinese contributions include building the western half of the Transcontinental Railroad, and levees in the Sacramento River Delta; the popularization of Chinese American food; science and technological innovation; and the introduction of Chinese and East Asian culture to America, such as Buddhism, Taoism, and Kung fu.
Chinese immigrants to the United States brought many of their ideas and values with them. Some of these have continued to influence later generations. Among them are Confucian respect for elders. Similarly, education and the civil service were the most important path for upward social mobility in China. The first Broadway show about Asian Americans was "Flower Drum Song" which premiered on Broadway in 1958; the hit "Chinglish" premiered on Broadway in 2011.
In most American cities with significant Chinese populations, the new year is celebrated with cultural festivals and parties. In Seattle, the is held every year. Other important festivals include the Dragon Boat Festival and the Mid-Autumn Festival.
Analysis indicated that most non-Asian Americans do not differentiate between Chinese Americans and East Asian Americans generally, and perceptions of both groups are nearly identical. A 2001 survey of Americans' attitudes toward Asian Americans and Chinese Americans indicated that one fourth of the respondents had somewhat or very negative attitude toward Chinese Americans in general. The study did find several positive perceptions of Chinese Americans: strong family values (91%); honesty as entrepreneurs (77%); high value on education (67%).
Language.
Chinese is the third most-spoken language in the United States, almost completely spoken within Chinese American populations and by immigrants or the descendants of immigrants, especially in California. Over 2 million Americans speak some variety of Chinese, with Mandarin Chinese becoming increasingly common due to immigration from mainland China and Taiwan.
In New York City at least, although Mandarin is spoken as a native language among only 10% of Chinese speakers, it is used as a secondary dialect among the greatest number of them and is on the way to replacing Cantonese as their lingua franca. In addition, immigration from Fujian is bringing an increasingly large number of Min speakers. Wu dialects, previously unheard of in the United States, are now spoken by a minority of recent Chinese immigrants hailing from Jiangsu, Zhejiang, and Shanghai.
Although Chinese Americans grow up learning English, some teach their children Chinese for a variety of reasons: preservation of an ancient civilization, preservation of a unique identity, pride in their cultural ancestry, desire for easy communication with them and other relatives, and the perception that Chinese will be a very useful language as China's economic strength increases. Cantonese, historically the language of most Chinese immigrants, is the third most widely spoken non-English language in the United States.
Religion.
Religions of Chinese Americans (2012)
   Protestantism
 (22%)   Buddhism
 (15%)   Catholic Church
 (8%)  Other (3%)  Not declared (52%)
The Chinese American community differs from the rest of the population in that the majority of Chinese Americans do not report a religious affiliation. 43% of Chinese Americans switched to a different religion and 54% stayed within their childhood religion within their lifetime. According to the Pew Research Center's 2012 Asian-American Survey, 52% of Chinese Americans aged 15 and over said that they didn't have any religious affiliation. This is also compared with the religious affiliation of Asian American average of 26% and a national average of 19%.
Of Chinese Americans who were religious, 15% were Buddhist, 8% were Catholic, and 22% belonged to a Protestant denomination. Fully half of Chinese Americans (52%)—including 55% of those born in the U.S. and 51% of those born overseas—describe themselves as religiously unaffiliated. Because Chinese Americans are the largest subgroup of Asian Americans, nearly half of all religiously unaffiliated Asians in the U.S. are of Chinese descent (49%).
Politics.
Chinese Americans are divided among many subgroups based on factors such as age, nativity, and socioeconomic status and do not have uniform attitudes about the People's Republic of China (Communist China) or the Republic of China (Taiwan Kuomintang), about the United States, or about Chinese nationalism. Different subgroups of Chinese Americans also have radically different and sometimes very conflicting political priorities and goals.
In 2013, Chinese Americans were the least likely Asian American ethnicity to be affiliated with a political party.
Nonetheless, Chinese Americans are clustered in majority-Democratic states and have increasingly voted Democratic in recent presidential elections, following the trend for Asian Americans in general. Polling just before the 2004 U.S. Presidential Election found John Kerry was favored by 58% of Chinese Americans and George W. Bush by only 23%, as compared with a 54/44 split in California, a 58/40 split in New York, and a 48/51 split in America as a whole on Election Day itself. In the 2012 presidential election, 81% of Chinese American voters selected Barack Obama over Mitt Romney.
Chinese Americans were an important source of funds for Han revolutionaries during the later Qing dynasty, and Sun Yat-sen was raising money in America at the time of the Xinhai Revolution, which established the Republic of China. During the Cultural Revolution, Chinese Americans, as overseas Chinese in general, were viewed as capitalist traitors by the PRC government. This attitude changed dramatically in the late 1970s with the reforms of Deng Xiaoping. Increasingly, Chinese Americans were seen as sources of business and technical expertise and capital who could aid in China's economic and other development.
Immigration.
Economic growth in the People's Republic of China has given mainland Chinese more opportunities to emigrate. A 2011 survey showed that 60% of Chinese millionaires plan to emigrate and 40% of Chinese millionaires selecting the United States as the top destination for immigration. The EB-5 Investment Visa allows many powerful Chinese to seek U.S. citizenship, and recent reports show that 75% of applicants for this visa in 2011 were Chinese. Chinese multimillionaires benefited most from the EB-5 Immigrant Investor Program in the U.S. Now, as long as one has at least US$500,000 to invest in projects listed by United States Citizenship and Immigration Services (USCIS), where it is possible to get an EB-5 green card that comes with permanent U.S. residency rights, but only in states specified by the pilot project.
Socioeconomics.
Education.
Overall, as a demographic group, Chinese Americans are highly educated and earn higher incomes when compared to other demographic groups in the United States. Educational achievements of Chinese in the United States are one of the highest among Asian Americans and also among all ethnic groups in the United States. Chinese Americans often have some of the highest averages in tests such as SAT, GRE, etc. in the United States. Although verbal scores lag somewhat due to the influx of new immigrants, combined SAT scores have also been higher than for most Americans. Chinese Americans are the largest racial group on all but one of the nine fully established University of California campuses.
They are the largest group among US National Merit Scholarship awardees in California, They are more likely to apply to competitively elite higher education institutions. They also constitute 24% of all Olympic Seattle Scholarship winners, 33% of USA Math Olympiad winners, 15.5% of Putnam Math Competition winners, and 36% of Duke Talent Identification Grand Recognition Ceremony attendees from the Dallas Metropolitan area.
International students studying at various higher education institutions around the United States account for a significant percentage of the international student body. International undergraduates, who make up 8% of Duke's undergraduate body, come from China more than any other country. International Chinese students also comprise 11% of the nearly 5,800 freshmen at the University of Washington. Mainland China is the top sending country of international students to the United States. As a result of its growing economy and large population, more middle-class families from China are able to afford American college tuition, bringing an influx of Chinese students to study abroad in the United States. With a more diverse educational background and higher level of English proficiency, international Chinese students also value American degrees, as it gives them a notable advantage over their college-educated counterparts in China by the time they return to their native country to seek employment.
Due to cultural factors, many Chinese international students are brand name conscious, choosing nationally ranked elite higher education institutes throughout the United States as their target schools. International Chinese students are also widely found at many elite liberal arts colleges such as Barnard College and Mount Holyoke. Students from China gravitate towards Americans colleges and universities for their high quality and the style of education which stresses interdisciplinary approaches, creativity, student participation and critical thinking.
Chinese students comprise 18% of the international student population in America, and make up 32.2% of the undergraduate students and 48.8% of the graduate students. Chinese international students tend to gravitate towards technical majors that involve heavy use of mathematics and the natural sciences. 27.5% of international Chinese students study business management, finance, or economics, 19.2% study engineering, 11.5% study the life sciences and 10.6% study math or computer science.
Largely driven by educational immigration, among American PhD recipients in fields related to science and engineering, 25% of the recipients are ethnic Chinese. According to the 2010 U.S. Census Bureau of Labor Statistics, 51.8% of all Chinese Americans have attained at least a bachelor's degree, compared with 28.2% nationally and 49.9% for all Asian American groups. The Census reports that 54.7% of Chinese American men attained a bachelor's degree and 49.3% of Chinese American women attained a bachelor's degree. In addition, 26.6% of all Chinese Americans in the United States possess a master's, doctorate or other professional degree, compared to 20.3% for all Asian Americans, and is roughly two and a half times above the national average, with high educational attainment largely driven by educational immigration.
Employment.
There has been a significant change in the perceptions about Chinese Americans. In as little as 100 years of American history, stereotypes of Chinese Americans have changed to portraying a hard working and educated minority. Thus, most Chinese Americans work as white collar professionals, many of whom are highly educated, salaried professionals whose work is largely self-directed in management, professional, and related occupations such as engineering, medicine, investment banking, law, and academia. 53.1% of Chinese Americans work in many white collar professions compared with 48.1% for all Asian Americans and a national average of 35.1%. They make up 2% of working physicians in the United States. Chinese Americans also make up a third of the Asian American high tech professional workforce and a tenth of the entire Silicon Valley workforce. Chinese Americans also hold lower unemployment rates than the population average with a figure of 4.7% compared to a national rate of 5.9% in 2010.
Many Chinese Americans have turned to the high tech center to jump-start potential computer and internet startups to capitalize on the regions wealth of venture capital, business expertise, and cultural and financial incentives for innovation. Ethnic Chinese have been successful in starting new firms in technology centers across the United States, including California's Silicon Valley. Chinese Americans have been disproportionately successful in high technology sectors, as evidenced by the Goldsea 100 Compilation of America's Most Successful Asian Entrepreneurs. Chinese Americans accounted for 4% of people listed in the 1998 Forbes Hi Tech 100 List.
Annalee Saxenian, a UC Berkeley professor, whose research interests include the contribution of Chinese immigrants on America's technology concludes that in Silicon Valley, carried out a study that showed that since 1998, one out of five high tech start-ups in Silicon Valley were led by Chinese Americans. During the same year, 5 of the 8 fastest growing companies had Chinese American CEO's except for Yahoo, whose Jerry Yang was a founder but not a CEO. In Silicon Valley there are at least 2 to 3 dozen Chinese American organizations according to professional interests each with at least 100 members. One prominent organization of which is the Committee of 100. Immigrants from China and Taiwan were key founders in 12.8% of all Silicon Valley start-ups between 1995 to 2005. Almost 6% of the immigrants who founded companies in the innovation/manufacturing-related services field are from Mainland China and Taiwan.
Research funded by the Public Policy Institute of California indicates that in 1996, 1,786 Silicon Valley technology companies with $12.5 billion in sales and 46,000 employees were run by Indian or Chinese executives. Moreover, the pace of entrepreneurship among local immigrants is increasing rapidly. While Chinese or Indian executives are at the helm of 13% of the Silicon Valley technology businesses started between 1980 and 1985, they are running 27% of the more than 4,000 businesses started between 1991 and 1996. Start-up firms remain a primary source for new ideas and innovation for Chinese American internet entrepreneurs. Many of them are employed or directly engaged in new start-up activities. The proportional share of start-up firms by ethnic Chinese in Silicon Valley skyrocketed from 9% in 1980-1984 to about 20% between 1995-1998. By 2006, Chinese American internet entrepreneurs continued to start 20% of all Silicon Valley start-up firms, leading 2000 Silicon Valley companies, and employing 58,000 workers. They still continue to own about 20% of all information technology companies that were founded in Silicon Valley since 1980.
Numerous professional organizations in perspective in the 1990s as a support network for fellow Chinese American high tech start-ups in the valley. Between 1980 and 1999, 17% of the 11,443 high-tech firms in Silicon Valley - including some 40 publicly traded firms were controlled by ethnic Chinese. In 1990, Chinese Americans made up a third of the Asian American high tech professional workforce or 11% of the entire Silicon Valley professional workforce. In 1998, Chinese Americans managed 2001 firms, employing 41,684 workers, and ran up 13.2 billion in sales. They also account for 17% of all Silicon Valley firm owners, 10% of the professional workforce in the Valley, and 13.5% of the total sales accounting for less than 1% of the U.S. population at the time.
Though Chinese Americans are also noted for their high rates of self-employment, as they have an extensive history of self-employment dating back to the California Gold Rush in the 1880s, However, as more Chinese Americans seek higher education to elevate themselves socioeconomically, rates of self-employment are generally lower than population average. In 2007, there were over 109,614 Chinese-owned employer firms, employing more than 780,000 workers, and generating more than $128 billion in revenue.
Among Chinese-owned U.S. firms, 40% were in the professional, scientific, and technical services sector; the accommodation and food services sector; and the repair, maintenance, personal, and laundry services sector. Chinese-owned U.S. firms comprised 2% of all U.S. businesses in these sectors. Wholesale trade and accommodation and food services accounted for 50.4% of Chinese-owned business revenue. 66,505 or 15.7% of Chinese-owned firms had receipts of $250,000 or more compared with 2% for all U.S. businesses.
Economics.
With their above average educational attainment rates, Chinese Americans from all socioeconomic backgrounds have achieved significant advances in their educational levels, income, life expectancy, and other social indicators as the financial and socioeconomic opportunities offered by the United States have lifted many Chinese Americans out of poverty, bringing them into the ranks of America's middle class, upper middle class, as well as the enjoyment of substantial well being.
Chinese Americans are more likely to own homes than the general American population. According to the 2000 U.S. Census, 65% of Chinese Americans owned a home, higher than the total population's rate of 54%. In 2003, real estate economist Gary Painter of the University of Southern California Lusk Center for Real Estate Research found out that when comparing homeowners with similar income levels Los Angeles, the Chinese-American home-ownership rate is 20% higher than Whites; in San Francisco, 23% higher; and in the New York metropolitan area, 18% higher. A 2008 Asian Real Estate Association of America report released on behalf of the American community survey, Chinese Americans living in the states of Texas, New York, and California all had high home ownership rates that were significantly near or above the general population average.
According to the 2010 U.S. Census, Chinese American men had a full-time median income of $57,061 and Chinese American women had a median income of $47,224. Chinese Americans also have one of the highest median household incomes among most demographic groups in the United States, which is 30% higher than the national average but is slightly lower compared with the Asian American population.
Despite positive economic indicators, a number of economic deterrents have been noted to afflict the Chinese American community. While median income remains above some ethnic groups in the United States, studies in the wake of the 2008 financial crisis revealed that Asian men have the highest rate of persistent long-term unemployment.

</doc>
<doc id="49001" url="http://en.wikipedia.org/wiki?curid=49001" title="African-American Civil Rights Movement (1954–68)">
African-American Civil Rights Movement (1954–68)

The African-American Civil Rights Movement or 1960s Civil Rights Movement encompasses social movements in the United States whose goals were to end racial segregation and discrimination against black Americans and to secure legal recognition and federal protection of the citizenship rights enumerated in the Constitution and federal law. This article covers the phase of the movement between 1954 and 1968, particularly in the South. The leadership was African-American, much of the political and financial support came from labor unions (led by Walter Reuther), major religious denominations, and prominent white politicians such as Hubert Humphrey and Lyndon B. Johnson.
The movement was characterized by major campaigns of civil resistance. Between 1955 and 1968, acts of nonviolent protest and civil disobedience produced crisis situations and productive dialogues between activists and government authorities. Federal, state, and local governments, businesses, and communities often had to respond immediately to these situations that highlighted the inequities faced by African Americans. Forms of protest and/or civil disobedience included boycotts such as the successful Montgomery Bus Boycott (1955–56) in Alabama; "sit-ins" such as the influential Greensboro sit-ins (1960) in North Carolina; marches, such as the Selma to Montgomery marches (1965) in Alabama; and a wide range of other nonviolent activities.
Noted legislative achievements during this phase of the civil rights movement were passage of the Civil Rights Act of 1964, that banned discrimination based on "race, color, religion, or national origin" in employment practices and public accommodations; the Voting Rights Act of 1965, that restored and protected voting rights; the Immigration and Nationality Services Act of 1965, that dramatically opened entry to the U.S. to immigrants other than traditional Northern European and Germanic groups; and the Fair Housing Act of 1968, that banned discrimination in the sale or rental of housing. African Americans re-entered politics in the South, and across the country young people were inspired to take action.
A wave of inner city riots in black communities from 1964 through 1970 undercut support from the white community. The emergence of the Black Power movement, which lasted from about 1966 to 1975, challenged the established black leadership for its cooperative attitude and its nonviolence, and instead demanded political and economic self-sufficiency.
Background.
Following the American Civil War, three constitutional amendments were passed, including the 13th Amendment (1865) that ended slavery; the 14th Amendment (1868) that gave African Americans citizenship, adding their total population of four million to the official population of southern states for Congressional apportionment; and the 15th Amendment (1870) that gave African-American males the right to vote (only males could vote in the U.S. at the time). From 1865 to 1877, the United States underwent a turbulent Reconstruction Era trying to establish free labor and civil rights of freedmen in the South after the end of slavery. Many whites resisted the social changes, leading to insurgent movements such as the Ku Klux Klan, whose members attacked black and white Republicans to maintain white supremacy. In 1871, President Ulysses S. Grant, the U.S. Army, and U.S. Attorney General Amos T. Akerman, initiated a campaign to repress the KKK under the Enforcement Acts.
Some states were reluctant to enforce the federal measures of the act; by the early 1870s, other white supremacist groups arose that violently opposed African-American legal equality and suffrage.
After the disputed election of 1876 resulted in the end of Reconstruction and federal troops were withdrawn, whites in the South regained political control of the region's state legislatures by the end of the century, after having intimidated and violently attacked blacks during elections, and lost power during a biracial fusionist coalition of Populists and Republicans in the late century.
From 1890 to 1908, southern states passed new constitutions and laws to disfranchise African Americans by creating barriers to voter registration; voting rolls were dramatically reduced as blacks were forced out of electoral politics. While progress was made in some areas, this status lasted in most southern states until national civil rights legislation was passed in the mid-1960s to provide federal enforcement of constitutional voting rights. For more than 60 years, blacks in the South were not able to elect anyone to represent their interests in Congress or local government. Since they could not vote, they could not serve on local juries.
During this period, the white-dominated Democratic Party maintained political control of the South. Because whites controlled all the seats representing the total population of the South, they had a powerful voting block in Congress. The Republican Party—the "party of Lincoln"—which had been the party that most blacks belonged to, shrank to insignificance as black voter registration was suppressed. Until 1965, the "solid South" was a one-party system under the Democrats. Outside a few areas (usually in remote Appalachia), the Democratic Party nomination was tantamount to election for state and local office. In 1901, President Theodore Roosevelt invited Booker T. Washington to dine at the White House, making him the first African American to attend an official dinner there. "The invitation was roundly criticized by southern politicians and newspapers." Washington persuaded the president to appoint more blacks to federal posts in the South and to try to boost African American leadership in state Republican organizations. However, this was resisted by both white Democrats and white Republicans as an unwanted federal intrusion into state politics.
During the same time as African Americans were being disenfranchised, white Democrats imposed racial segregation by law. Violence against blacks increased, with numerous lynchings through the turn of the century. The system of "de jure" state-sanctioned racial discrimination and oppression that emerged from the post-Reconstruction South became known as the "Jim Crow" system. The United States Supreme Court upheld the constitutionality of those state laws that required racial segregation in public facilities in its 1896 decision Plessy v. Ferguson, legitimizing them through the "separate but equal" doctrine.
Segregation remained intact into the mid-1950s, when many states began to gradually integrate their schools following the Supreme Court decision in Brown v. Board of Education that overturned Plessy v. Ferguson. The early 20th century is a period often referred to as the "nadir of American race relations". While problems and civil rights violations were most intense in the South, social discrimination and tensions affected African Americans in other regions as well. At the national level, the Southern bloc controlled important committees in Congress, defeated passage of laws against lynching, and exercised considerable power beyond the number of whites in the South.
Characteristics of the post-Reconstruction period:
African Americans and other ethnic minorities rejected this regime. They resisted it in numerous ways and sought better opportunities through lawsuits, new organizations, political redress, and labor organizing (see the African-American Civil Rights Movement (1896–1954)). The National Association for the Advancement of Colored People (NAACP) was founded in 1909. It fought to end race discrimination through litigation, education, and lobbying efforts. Its crowning achievement was its legal victory in the Supreme Court decision "Brown v. Board of Education" in 1954 when the Court rejected separate white and colored school systems and, by implication, overturned the "separate but equal" doctrine established in "Plessy v. Ferguson" of 1896.
The integration of Southern public libraries involved many of the same characteristics seen in the larger Civil Rights Movement. This includes sit-ins, beatings, and white resistance. For example, in 1963 in the city of Anniston, Alabama, two black ministers were brutally beaten for attempting to integrate the public library. Though there was resistance and violence, the integration of libraries were generally quicker than integration of other public institutions.
Black veterans of the military after both World Wars pressed for full civil rights and often led activist movements. In 1948 they gained integration in the military under President Harry Truman, who issued Executive Order 9981 to accomplish it. The situation for blacks outside the South was somewhat better (in most states they could vote and have their children educated, though they still faced discrimination in housing and jobs). From 1910 to 1970, African Americans sought better lives by migrating north and west out of the South. Nearly seven million blacks left the South in what was known as the Great Migration. So many people migrated that the demographics of some previously black-majority states changed to white majority (in combination with other developments).
Housing segregation was a nationwide problem, persistent well outside the South. Although the federal government had become increasingly involved in mortgage lending and development in the 1930s and 40s, it did not reject the use of race-restrictive covenants until 1950. Suburbanization was already connected with white flight by this time, a situation perpetuated by real estate agents' continuing discrimination. In particular, from the 1930s to the 1960s the National Association of Real Estate Boards (NAREB) issued guidelines that specified that a realtor "should never be instrumental in introducing to a neighborhood a character or property or occupancy, members of any race or nationality, or any individual whose presence will be clearly detrimental to property values in a neighborhood."
Invigorated by the victory of "Brown" and frustrated by the lack of immediate practical effect, private citizens increasingly rejected gradualist, legalistic approaches as the primary tool to bring about desegregation. They were faced with "massive resistance" in the South by proponents of racial segregation and voter suppression. In defiance, African American activists adopted a combined strategy of direct action, nonviolence, nonviolent resistance, and many events described as civil disobedience, giving rise to the African-American Civil Rights Movement of 1954–1968.
Mass action replacing litigation.
The strategy of public education, legislative lobbying, and litigation that had typified the civil rights movement during the first half of the 20th century broadened after "Brown" to a strategy that emphasized "direct action:" primarily boycotts, sit-ins, Freedom Rides, marches and similar tactics that relied on mass mobilization, nonviolent resistance and civil disobedience. This mass action approach typified the movement from 1960 to 1968.
Churches, local grassroots organizations, fraternal societies, and black-owned businesses mobilized volunteers to participate in broad-based actions. This was a more direct and potentially more rapid means of creating change than the traditional approach of mounting court challenges used by the NAACP and others.
In 1952, the Regional Council of Negro Leadership (RCNL), led by T. R. M. Howard, a black surgeon, entrepreneur, and planter, organized a successful boycott of gas stations in Mississippi that refused to provide restrooms for blacks. Through the RCNL, Howard led campaigns to expose brutality by the Mississippi state highway patrol and to encourage blacks to make deposits in the black-owned Tri-State Bank of Nashville which, in turn, gave loans to civil rights activists who were victims of a "credit squeeze" by the White Citizens' Councils.
After Rosa Parks' arrest in December 1955, Jo Ann Gibson-Robinson of the Montgomery Women's Political Council seized the opportunity to put a long-considered Bus Boycott protest in motion. Late that night, she, two students, and John Cannon, chairman of the Business Department at Alabama State, mimeographed and distributed approximately 52,500 leaflets calling for a boycott of the buses.
The first day of the boycott having been successful, King and other civic and religious leaders created the Montgomery Improvement Association—so as to continue the Montgomery Bus Boycott The MIA managed to keep the boycott going for over a year until a federal court order required Montgomery to desegregate its buses. The success in Montgomery made its leader Dr. Martin Luther King, Jr. a nationally known figure. It also inspired other bus boycotts, such as the highly successful Tallahassee, Florida, boycott of 1956–57.
In 1957 Dr. King and Rev. Ralph Abernathy, the leaders of the Montgomery Improvement Association, joined with other church leaders who had led similar boycott efforts, such as Rev. C. K. Steele of Tallahassee and Rev. T. J. Jemison of Baton Rouge; and other activists such as Rev. Fred Shuttlesworth, Ella Baker, A. Philip Randolph, Bayard Rustin and Stanley Levison, to form the Southern Christian Leadership Conference. The SCLC, with its headquarters in Atlanta, Georgia, did not attempt to create a network of chapters as the NAACP did. It offered training and leadership assistance for local efforts to fight segregation. The headquarters organization raised funds, mostly from Northern sources, to support such campaigns. It made non-violence both its central tenet and its primary method of confronting racism.
In 1959, Septima Clarke, Bernice Robinson, and Esau Jenkins, with the help of Myles Horton's Highlander Folk School in Tennessee, began the first Citizenship Schools in South Carolina's Sea Islands. They taught literacy to enable blacks to pass voting tests. The program was an enormous success and tripled the number of black voters on Johns Island. SCLC took over the program and duplicated its results elsewhere.
Key events.
"Brown v. Board of Education", 1954.
In the Spring of 1951, black students in Virginia protested their unequal status in the state's segregated educational system. Students at Moton High School protested the overcrowded conditions and failing facility. Some local leaders of the NAACP had tried to persuade the students to back down from their protest against the Jim Crow laws of school segregation. When the students did not budge, the NAACP joined their battle against school segregation. The NAACP proceeded with five cases challenging the school systems; these were later combined under what is known today as "Brown v. Board of Education".
On May 17, 1954, the U.S. Supreme Court handed down its decision regarding the case called "Brown v. Board of Education of Topeka, Kansas", in which the plaintiffs charged that the education of black children in separate public schools from their white counterparts was unconstitutional. The Court stated that the 
segregation of white and colored children in public schools has a detrimental effect upon the colored children. The impact is greater when it has the sanction of the law; for the policy of separating the races is usually interpreted as denoting the inferiority of the Negro group.
The lawyers from the NAACP had to gather some plausible evidence in order to win the case of Brown vs. Education. Their way of addressing the issue of school segregation was to enumerate several arguments. One of them pertained to having an exposure to interracial contact in a school environment. It was said that it would, in turn, help to prevent children to live with the pressures that society exerts in regards to race. Therefore, having a better chance of living in democracy. In addition, another was in reference to the emphasis of how "'education' comprehends the entire process of developing and training the mental, physical and moral powers and capabilities of human beings".
Risa Goluboff wrote that the NAACP's intention was to show the Court's that African American children were the victims of school segregation and their futures were at risk. The Court ruled that both "Plessy v. Ferguson" (1896), which had established the "separate but equal" standard in general, and "Cumming v. Richmond County Board of Education" (1899), which had applied that standard to schools, were unconstitutional.
The federal government filed a friend of the court brief in the case urging the judges to consider the effect that segregation had on America's image in the Cold War. Secretary of State Dean Acheson was quoted in the brief stating that ""The United States is under constant attack in the foreign press, over the foreign radio, and in such international bodies as the United Nations because of various practices of discrimination in this country."
"
The following year, in the case known as "Brown II", the Court ordered segregation to be phased out over time, "with all deliberate speed". "Brown v. Board of Education of Topeka, Kansas" (1954) did not overturn "Plessy v. Ferguson" (1896). "Plessy v. Ferguson" was segregation in transportation modes. "Brown v. Board of Education" dealt with segregation in education. "Brown v. Board of Education" did set in motion the future overturning of 'separate but equal'.
On May 18, 1954 Greensboro, North Carolina became the first city in the South to publicly announce that it would abide by the Supreme Court's "Brown v. Board of Education" ruling. "It is unthinkable,' remarked School Board Superintendent Benjamin Smith, 'that we will try to [override] the laws of the United States." This positive reception for Brown, together with the appointment of African American Dr. David Jones to the school board in 1953, convinced numerous white and black citizens that Greensboro was heading in a progressive direction. Integration in Greensboro occurred rather peacefully compared to the process in Southern states such as Alabama, Arkansas, and Virginia where "massive resistance" was practiced by top officials and throughout the states. In Virginia, some counties closed their public schools rather than integrate, and many white Christian private schools were founded to accommodate students who used to go to public schools. Even in Greensboro, much local resistance to desegregation continued, and in 1969, the federal government found the city was not in compliance with the 1964 Civil Rights Act. Transition to a fully integrated school system did not begin until 1971.
Many Northern cities also had de facto segregation policies, which resulted in a vast gulf in educational resources between black and white communities. In Harlem, New York for example, neither a single new school was built since the turn of the century, nor did a single nursery school exist – even as the Second Great Migration was causing overcrowding. Existing schools tended to be dilapidated and staffed with inexperienced teachers. "Brown" helped stimulate activism among New York City parents like Mae Mallory who, with support of the NAACP, initiated a successful lawsuit against the city and state on "Brown"'s principles. Mallory and thousands of other parents bolstered the pressure of the lawsuit with a school boycott in 1959. During the boycott, some of the first freedom schools of the period were established. The city responded to the campaign by permitting more open transfers to high-quality, historically-white schools. (New York's African-American community, and Northern desegregation activists generally, now found themselves contending with the problem of white flight, however.)
Rosa Parks and the Montgomery Bus Boycott, 1955–1956.
Civil rights leaders focused on Montgomery Alabama, highlight extreme forms of segregation there. Local black leader Rosa Parks on December 1, 1955, refused to give up her seat on a public bus to make room for a white passenger; she was arrested And received national publicity, hailed as the "mother of the civil rights movement." Parks was secretary of the Montgomery NAACP chapter and had recently returned from a meeting at the Highlander Center in Tennessee where nonviolent civil disobedience as a strategy was taught. African-Americans gathered and organized the Montgomery Bus Boycott to demand a bus system in which passengers would be treated equally. After the city rejected many of their suggested reforms, the NAACP, led by E.D. Nixon, pushed for full desegregation of public buses. With the support of most of Montgomery's 50,000 African Americans, the boycott lasted for 381 days, until the local ordinance segregating African Americans and whites on public buses was repealed. Ninety percent of African Americans in Montgomery partook in the boycotts, which reduced bus revenue significantly, as they comprised the majority of the riders. In November 1956, a federal court ordered Montgomery's buses desegregated and the boycott ended.
Local leaders established the Montgomery Improvement Association to focus their efforts. Martin Luther King, Jr., was elected President of this organization. The lengthy protest attracted national attention for him and the city. His eloquent appeals to Christian brotherhood and American idealism created a positive impression on people both inside and outside the South.
Desegregating Little Rock Central High School, 1957.
A crisis erupted in Little Rock, Arkansas when Governor of Arkansas Orval Faubus called out the National Guard on September 4 to prevent entry to the nine African-American students who had sued for the right to attend an integrated school, Little Rock Central High School. The nine students had been chosen to attend Central High because of their excellent grades.
On the first day of school, only one of the nine students showed up because she did not receive the phone call about the danger of going to school. She was harassed by white protesters outside the school, and the police had to take her away in a patrol car to protect her. Afterward, the nine students had to carpool to school and be escorted by military personnel in jeeps.
Faubus was not a proclaimed segregationist. The Arkansas Democratic Party, which then controlled politics in the state, put significant pressure on Faubus after he had indicated he would investigate bringing Arkansas into compliance with the "Brown" decision. Faubus then took his stand against integration and against the Federal court ruling.
Faubus' resistance received the attention of President Dwight D. Eisenhower, who was determined to enforce the orders of the Federal courts. Critics had charged he was lukewarm, at best, on the goal of desegregation of public schools. But, Eisenhower federalized the National Guard in Arkansas and ordered them to return to their barracks. Eisenhower deployed elements of the 101st Airborne Division to Little Rock to protect the students.
The students attended high school under harsh conditions. They had to pass through a gauntlet of spitting, jeering whites to arrive at school on their first day, and to put up with harassment from other students for the rest of the year. Although federal troops escorted the students between classes, the students were teased and even attacked by white students when the soldiers were not around. One of the Little Rock Nine, Minnijean Brown, was suspended for spilling a bowl of chili on the head of a white student who was harassing her in the school lunch line. Later, she was expelled for verbally abusing a white female student.
Only Ernest Green of the Little Rock Nine graduated from Central High School. After the 1957–58 school year was over, Little Rock closed its public school system completely rather than continue to integrate. Other school systems across the South followed suit.
Robert F. Williams and the debate on nonviolence, 1959–1964.
The Jim Crow system employed "terror as a means of social control," with the most organized manifestations being the Ku Klux Klan and their collaborators in local police departments. This violence played a key role in blocking the progress of the civil rights movement in the late 1950s. Some black organizations in the South began practicing armed self-defense. The first to do so openly was the Monroe, North Carolina chapter of the NAACP led by Robert F. Williams. Williams had rebuilt the chapter after its membership was terrorized out of public life by the Klan. He did so by encouraging a new, more working-class membership to arm itself thoroughly and defend against attack. When Klan nightriders attacked the home of NAACP member Dr. Albert Perry in October 1957, Williams' militia exchanged gunfire with the stunned Klansmen, who quickly retreated. The following day, the city council held an emergency session and passed an ordinance banning KKK motorcades. One year later, Lumbee Indians in North Carolina would have a similarly successful armed stand-off with the Klan (known as the Battle of Hayes Pond) which resulted in KKK leader James W. "Catfish" Cole being convicted of incitement to riot.
After the acquittal of several white men charged with sexually assaulting black women in Monroe, Williams announced to United Press International reporters that he would "meet violence with violence" as a policy. Williams' declaration was quoted on the front page of "The New York Times", and "The Carolina Times" considered it "the biggest civil rights story of 1959." NAACP National chairman Roy Wilkins immediately suspended Williams from his position, but the Monroe organizer won support from numerous NAACP chapters across the country. Ultimately, Wilkins resorted to bribing influential organizer Daisy Bates to campaign against Williams at the NAACP national convention and the suspension was upheld. The convention nonetheless passed a resolution which stated: "We do not deny, but reaffirm the right of individual and collective self-defense against unlawful assaults." Martin Luther King Jr. argued for Williams' removal, but Ella Baker and WEB Dubois both publicly praised the Monroe leader's position.
Robert F. Williams – along with his wife, Mabel Williams – continued to play a leadership role in the Monroe movement, and to some degree, in the national movement. The Williamses published "The Crusader", a nationally circulated newsletter, beginning in 1960, and the influential book in 1962. Williams did not call for full militarization in this period, but "flexibility in the freedom struggle." Williams was well-versed in legal tactics and publicity, which he had used successfully in the internationally known "Kissing Case" of 1958, as well as nonviolent methods, which he used at lunch counter sit-ins in Monroe – all with armed self-defense as a complementary tactic.
Williams led the Monroe movement in another armed stand-off with white supremacists during an August 1961 Freedom Ride; he had been invited to participate in the campaign by Ella Baker and James Forman of the Student Nonviolent Coordinating Committee (SNCC). The incident (along with his campaigns for peace with Cuba) resulted in him being targeted by the FBI and prosecuted for kidnapping; he was cleared of all charges in 1976. Meanwhile, armed self-defense continued discreetly in the Southern movement with such figures as SNCC's Amzie Moore, Hartman Turnbow, and Fannie Lou Hamer all willing to use arms to defend their lives from nightrides. Taking refuge from the FBI in Cuba, the Willamses broadcast the radio show Radio Free Dixie throughout the eastern United States via Radio Progresso beginning in 1962. In this period, Williams advocated guerilla warfare against racist institutions, and saw the large ghetto riots of the era as a manifestation of his strategy.
University of North Carolina historian Walter Rucker has written that "the emergence of Robert F Williams contributed to the marked decline in anti-black racial violence in the US…After centuries of anti-black violence, African-Americans across the country began to defend their communities aggressively – employing overt force when necessary. This in turn evoked in whites real fear of black vengeance…" This opened up space for African-Americans to use nonviolent demonstration with less fear of deadly reprisal. Of the many civil rights activists who share this view, the most prominent was Rosa Parks. Parks gave the eulogy at Williams' funeral in 1996, praising him for "his courage and for his commitment to freedom," and concluding that "The sacrifices he made, and what he did, should go down in history and never be forgotten."
Sit-ins, 1958–1960.
In July 1958, the NAACP Youth Council sponsored sit-ins at the lunch counter of a Dockum Drug Store in downtown Wichita, Kansas. After three weeks, the movement successfully got the store to change its policy of segregated seating, and soon afterward all Dockum stores in Kansas were desegregated. This movement was quickly followed in the same year by a student sit-in at a Katz Drug Store in Oklahoma City led by Clara Luper, which also was successful.
Mostly black students from area colleges led a sit-in at a Woolworth's store in Greensboro, North Carolina. On February 1, 1960, four students, Ezell A. Blair, Jr., David Richmond, Joseph McNeil, and Franklin McCain from North Carolina Agricultural & Technical College, an all-black college, sat down at the segregated lunch counter to protest Woolworth's policy of excluding African Americans from being served there. The four students purchased small items in other parts of the store and kept their receipts, then sat down at the lunch counter and asked to be served. After being denied service, they produced their receipts and asked why their money was good everywhere else at the store, but not at the lunch counter.
The protesters had been encouraged to dress professionally, to sit quietly, and to occupy every other stool so that potential white sympathizers could join in. The Greensboro sit-in was quickly followed by other sit-ins in Richmond, Virginia; Nashville, Tennessee; and Atlanta, Georgia. The most immediately effective of these was in Nashville, where hundreds of well organized and highly disciplined college students conducted sit-ins in coordination with a boycott campaign.
As students across the south began to "sit-in" at the lunch counters of local stores, police and other officials sometimes used brute force to physically escort the demonstrators from the lunch facilities.
The "sit-in" technique was not new—as far back as 1939, African-American attorney Samuel Wilbert Tucker organized a sit-in at the then-segregated Alexandria, Virginia library. In 1960 the technique succeeded in bringing national attention to the movement. 
On March 9, 1960 an Atlanta University Center group of students released An Appeal for Human Rights as a full page advertisement in newspapers, including the "Atlanta Constitution", "Atlanta Journal", and "Atlanta Daily World". Known as the Committee on the Appeal for Human Rights (COAHR), the group initiated the Atlanta Student Movement and began to lead sit-ins starting on March 15, 1960. By the end of 1960, the proces of sit-ins had spread to every southern and border state, and even to facilities in Nevada, Illinois, and Ohio that discriminated against blacks.
Demonstrators focused not only on lunch counters but also on parks, beaches, libraries, theaters, museums, and other public facilities. In April 1960 activists who had led these sit-ins were invited by SCLC activist Ella Baker to hold a conference at Shaw University, a historically black university in Raleigh, North Carolina. This conference led to the formation of the Student Nonviolent Coordinating Committee (SNCC). SNCC took these tactics of nonviolent confrontation further, and organized the freedom rides. As the constitution protected interstate commerce, they decided to challenge segregation on interstate buses and in public bus facilities by putting interracial teams on them, to travel from the North through the segregated South.
Freedom Rides, 1961.
Freedom Rides were journeys by Civil Rights activists on interstate buses into the segregated southern United States to test the United States Supreme Court decision "Boynton v. Virginia," (1960) 364 U.S., which ruled that segregation was unconstitutional for passengers engaged in interstate travel. Organized by CORE, the first Freedom Ride of the 1960s left Washington D.C. on May 4, 1961, and was scheduled to arrive in New Orleans on May 17.
During the first and subsequent Freedom Rides, activists traveled through the Deep South to integrate seating patterns on buses and desegregate bus terminals, including restrooms and water fountains. That proved to be a dangerous mission. In Anniston, Alabama, one bus was firebombed, forcing its passengers to flee for their lives.
In Birmingham, Alabama, an FBI informant reported that Public Safety Commissioner Eugene "Bull" Connor gave Ku Klux Klan members fifteen minutes to attack an incoming group of freedom riders before having police "protect" them. The riders were severely beaten "until it looked like a bulldog had got a hold of them." James Peck, a white activist, was beaten so badly that he required fifty stitches to his head.
In a similar occurrence in Montgomery, Alabama, the Freedom Riders followed in the footsteps of Rosa Parks and rode an integrated Greyhound bus from Birmingham. Although they were protesting interstate bus segregation in peace, they were met with violence in Montgomery as a large, white mob attacked them for their activism. They caused an enormous, 2-hour long riot which resulted in 22 injuries, five of whom were hospitalized.
Mob violence in Anniston and Birmingham temporarily halted the rides. SNCC activists from Nashville brought in new riders to continue the journey from Birmingham to New Orleans. In Montgomery, Alabama, at the Greyhound Bus Station, a mob charged another bus load of riders, knocking John Lewis unconscious with a crate and smashing "Life" photographer Don Urbrock in the face with his own camera. A dozen men surrounded James Zwerg, a white student from Fisk University, and beat him in the face with a suitcase, knocking out his teeth.
On May 24, 1961, the freedom riders continued their rides into Jackson, Mississippi, where they were arrested for "breaching the peace" by using "white only" facilities. New freedom rides were organized by many different organizations and continued to flow into the South. As riders arrived in Jackson, they were arrested. By the end of summer, more than 300 had been jailed in Mississippi.
The jailed freedom riders were treated harshly, crammed into tiny, filthy cells and sporadically beaten. In Jackson, some male prisoners were forced to do hard labor in 100-degree heat. Others were transferred to the Mississippi State Penitentiary at Parchman, where they were treated to harsh conditions. Sometimes the men were suspended by "wrist breakers" from the walls. Typically, the windows of their cells were shut tight on hot days, making it hard for them to breathe.
Public sympathy and support for the freedom riders led John F. Kennedy's administration to order the Interstate Commerce Commission (ICC) to issue a new desegregation order. When the new ICC rule took effect on November 1, 1961, passengers were permitted to sit wherever they chose on the bus; "white" and "colored" signs came down in the terminals; separate drinking fountains, toilets, and waiting rooms were consolidated; and lunch counters began serving people regardless of skin color.
The student movement involved such celebrated figures as John Lewis, a single-minded activist; James Lawson, the revered "guru" of nonviolent theory and tactics; Diane Nash, an articulate and intrepid public champion of justice; Bob Moses, pioneer of voting registration in Mississippi; and James Bevel, a fiery preacher and charismatic organizer and facilitator. Other prominent student activists included Charles McDew, Bernard Lafayette, Charles Jones, Lonnie King, Julian Bond, Hosea Williams, and Stokely Carmichael.
Voter registration organizing.
After the Freedom Rides, local black leaders in Mississippi such as Amzie Moore, Aaron Henry, Medgar Evers, and others asked SNCC to help register black voters and to build community organizations that could win a share of political power in the state. Since Mississippi ratified its new constitution in 1890 with provisions such as poll taxes, residency requirements, and literacy tests, it made registration more complicated and stripped blacks from voter rolls and voting. In addition, violence at the time of elections had earlier suppressed black voting.
By the mid-20th century, preventing blacks from voting had become an essential part of the culture of white supremacy. In the fall of 1961, SNCC organizer Robert Moses began the first voter registration project in McComb and the surrounding counties in the Southwest corner of the state. Their efforts were met with violent repression from state and local lawmen, the White Citizens' Council, and the Ku Klux Klan. Activists were beaten, there were hundreds of arrests of local citizens, and the voting activist Herbert Lee was murdered.
White opposition to black voter registration was so intense in Mississippi that Freedom Movement activists concluded that all of the state's civil rights organizations had to unite in a coordinated effort to have any chance of success. In February 1962, representatives of SNCC, CORE, and the NAACP formed the Council of Federated Organizations (COFO). At a subsequent meeting in August, SCLC became part of COFO.
In the Spring of 1962, with funds from the Voter Education Project, SNCC/COFO began voter registration organizing in the Mississippi Delta area around Greenwood, and the areas surrounding Hattiesburg, Laurel, and Holly Springs. As in McComb, their efforts were met with fierce opposition—arrests, beatings, shootings, arson, and murder. Registrars used the literacy test to keep blacks off the voting roles by creating standards that even highly educated people could not meet. In addition, employers fired blacks who tried to register, and landlords evicted them from their rental homes. Despite these actions, over the following years, the black voter registration campaign spread across the state.
Similar voter registration campaigns—with similar responses—were begun by SNCC, CORE, and SCLC in Louisiana, Alabama, southwest Georgia, and South Carolina. By 1963, voter registration campaigns in the South were as integral to the Freedom Movement as desegregation efforts. After passage of the Civil Rights Act of 1964, protecting and facilitating voter registration despite state barriers became the main effort of the movement. It resulted in passage of the Voting Rights Act of 1965, which had provisions to enforce the constitutional right to vote for all citizens.
Integration of Mississippi universities, 1956–65.
Beginning in 1956, Clyde Kennard, a black Korean War-veteran, wanted to enroll at Mississippi Southern College (now the University of Southern Mississippi) under the GI Bill at Hattiesburg. Dr. William David McCain, the college president, used the Mississippi State Sovereignty Commission, in order to prevent his enrollment by appealing to local black leaders and the segregationist state political establishment.
The state-funded organization tried to counter the civil rights movement by positively portraying segregationist policies. More significantly, it collected data on activists, harassed them legally, and used economic boycotts against them by threatening their jobs (or causing them to lose their jobs) to try to suppress their work.
Kennard was twice arrested on trumped-up charges, and eventually convicted and sentenced to seven years in the state prison. After three years at hard labor, Kennard was paroled by Mississippi Governor Ross Barnett. Journalists had investigated his case and publicized the state's mistreatment of his colon cancer.
McCain's role in Kennard's arrests and convictions is unknown. While trying to prevent Kennard's enrollment, McCain made a speech in Chicago, with his travel sponsored by the Mississippi State Sovereignty Commission. He described the blacks' seeking to desegregate Southern schools as "imports" from the North. (Kennard was a native and resident of Hattiesburg.) McCain said:
We insist that educationally and socially, we maintain a segregated society. ... In all fairness, I admit that we are not encouraging Negro voting ... The Negroes prefer that control of the government remain in the white man's hands.
Note: Mississippi had passed a new constitution in 1890 that effectively disfranchised most blacks by changing electoral and voter registration requirements; although it deprived them of constitutional rights authorized under post-Civil War amendments, it survived US Supreme Court challenges at the time. It was not until after passage of the 1965 Voting Rights Act that most blacks in Mississippi and other southern states gained federal protection to enforce the constitutional right of citizens to vote.
In September 1962, James Meredith won a lawsuit to secure admission to the previously segregated University of Mississippi. He attempted to enter campus on September 20, on September 25, and again on September 26. He was blocked by Mississippi Governor Ross Barnett, who said, "[N]o school will be integrated in Mississippi while I am your Governor." The Fifth U.S. Circuit Court of Appeals held Barnett and Lieutenant Governor Paul B. Johnson, Jr. in contempt, ordering them arrested and fined more than $10,000 for each day they refused to allow Meredith to enroll.
Attorney General Robert Kennedy sent in a force of U.S. Marshals. On September 30, 1962, Meredith entered the campus under their escort. Students and other whites began rioting that evening, throwing rocks and firing on the U.S. Marshals guarding Meredith at Lyceum Hall. Two people, including a French journalist, were killed; 28 marshals suffered gunshot wounds; and 160 others were injured. President John F. Kennedy sent regular US Army forces to the campus to quell the riot. Meredith began classes the day after the troops arrived.
Kennard and other activists continued to work on public university desegregation. In 1965 Raylawni Branch and Gwendolyn Elaine Armstrong became the first African-American students to attend the University of Southern Mississippi. By that time, McCain helped ensure they had a peaceful entry. In 2006, Judge Robert Helfrich ruled that Kennard was factually innocent of all charges for which he had been convicted in the 1950s.
Albany Movement, 1961–62.
The SCLC, which had been criticized by some student activists for its failure to participate more fully in the freedom rides, committed much of its prestige and resources to a desegregation campaign in Albany, Georgia, in November 1961. King, who had been criticized personally by some SNCC activists for his distance from the dangers that local organizers faced—and given the derisive nickname "De Lawd" as a result—intervened personally to assist the campaign led by both SNCC organizers and local leaders.
The campaign was a failure because of the canny tactics of Laurie Pritchett, the local police chief, and divisions within the black community. The goals may not have been specific enough. Pritchett contained the marchers without violent attacks on demonstrators that inflamed national opinion. He also arranged for arrested demonstrators to be taken to jails in surrounding communities, allowing plenty of room to remain in his jail. Prichett also foresaw King's presence as a danger and forced his release to avoid King's rallying the black community. King left in 1962 without having achieved any dramatic victories. The local movement, however, continued the struggle, and it obtained significant gains in the next few years.
Birmingham Campaign, 1963.
The Albany movement was shown to be an important education for the SCLC, however, when it undertook the Birmingham campaign in 1963. Executive Director Wyatt Tee Walker carefully planned the early strategy and tactics for the campaign. It focused on one goal—the desegregation of Birmingham's downtown merchants, rather than total desegregation, as in Albany.
The movement's efforts were helped by the brutal response of local authorities, in particular Eugene "Bull" Connor, the Commissioner of Public Safety. He had long held much political power, but had lost a recent election for mayor to a less rabidly segregationist candidate. Refusing to accept the new mayor's authority, Connor intended to stay in office.
The campaign used a variety of nonviolent methods of confrontation, including sit-ins, kneel-ins at local churches, and a march to the county building to mark the beginning of a drive to register voters. The city, however, obtained an injunction barring all such protests. Convinced that the order was unconstitutional, the campaign defied it and prepared for mass arrests of its supporters. King elected to be among those arrested on April 12, 1963.
While in jail, King wrote his famous "Letter from Birmingham Jail" on the margins of a newspaper, since he had not been allowed any writing paper while held in solitary confinement. Supporters appealed to the Kennedy administration, which intervened to obtain King's release. King was allowed to call his wife, who was recuperating at home after the birth of their fourth child, and was released early on April 19.
The campaign, however, faltered as it ran out of demonstrators willing to risk arrest. James Bevel, SCLC's Director of Direct Action and Director of Nonviolent Education, then came up with a bold and controversial alternative: to train high school students to take part in the demonstrations. As a result, in what would be called the Children's Crusade, more than one thousand students skipped school on May 2 to meet at the 16th Street Baptist Church to join the demonstrations. More than six hundred marched out of the church fifty at a time in an attempt to walk to City Hall to speak to Birmingham's mayor about segregation. They were arrested and put into jail.
In this first encounter the police acted with restraint. On the next day, however, another one thousand students gathered at the church. When Bevel started them marching fifty at a time, Bull Connor finally unleashed police dogs on them and then turned the city's fire hoses water streams on the children. National television networks broadcast the scenes of the dogs attacking demonstrators and the water from the fire hoses knocking down the schoolchildren.
Widespread public outrage led the Kennedy administration to intervene more forcefully in negotiations between the white business community and the SCLC. On May 10, the parties announced an agreement to desegregate the lunch counters and other public accommodations downtown, to create a committee to eliminate discriminatory hiring practices, to arrange for the release of jailed protesters, and to establish regular means of communication between black and white leaders.
Not everyone in the black community approved of the agreement— the Rev. Fred Shuttlesworth was particularly critical, since he was skeptical about the good faith of Birmingham's power structure from his experience in dealing with them. Parts of the white community reacted violently. They bombed the Gaston Motel, which housed the SCLC's unofficial headquarters, and the home of King's brother, the Reverend A. D. King. In response, thousands of blacks rioted, burning numerous buildings and stabbing a police officer.
Kennedy prepared to federalize the Alabama National Guard if the need arose. Four months later, on September 15, a conspiracy of Ku Klux Klan members bombed the Sixteenth Street Baptist Church in Birmingham, killing four young girls.
"Rising tide of discontent" and Kennedy's Response, 1963.
Birmingham was only one of over a hundred cities rocked by chaotic protest that spring and summer, some of them in the North. During the March on Washington, Martin Luther King would refer to such protests as "the whirlwinds of revolt." In Chicago, blacks rioted through the South Side in late May after a white police officer shot a fourteen-year-old black boy who was fleeing the scene of a robbery. Violent clashes between black activists and white workers took place in both Philadelphia and Harlem in successful efforts to integrate state construction projects. On June 6, over a thousand whites attacked a sit-in in Lexington, North Carolina; blacks fought back and one white man was killed. Edwin C. Berry of the National Urban League warned of a complete breakdown in race relations: "My message from the beer gardens and the barbershops all indicate the fact that the Negro is ready for war."
In Cambridge, Maryland, a working‐class city on the Eastern Shore, Gloria Richardson of SNCC led a movement that pressed for desegregation but also demanded low‐rent public housing, job‐training, public and private jobs, and an end to police brutality. On June 14, struggles between blacks and whites escalated to the point where local authorities declared martial law, and Attorney General Robert F. Kennedy directly intervened to negotiate a desegregation agreement. Richardson felt that the increasing participation of poor and working-class blacks was expanding both the power and parameters of the movement, asserting that "The people as a whole really do have more intelligence than a few of their leaders.ʺ
In their deliberations during this wave of protests, the Kennedy administration privately felt that militant demonstrations were ʺbad for the countryʺ and that "Negroes are going to push this thing too far." On May 24, Robert Kennedy had a meeting with prominent black intellectuals to discuss the racial situation. The blacks criticized Kennedy harshly for vacillating on civil rights, and said that the African-American community's thoughts were increasingly turning to violence. The meeting ended with ill will on all sides. Nonetheless, the Kennedys ultimately decided that new legislation for equal public accommodations was essential to drive activists "into the courts and out of the streets."
On June 11, 1963, George Wallace, Governor of Alabama, tried to block the integration of the University of Alabama. President John F. Kennedy sent a military force to make Governor Wallace step aside, allowing the enrollment of Vivian Malone Jones and James Hood. That evening, President Kennedy addressed the nation on TV and radio with his historic civil rights speech, where he lamented "a rising tide of discontent that threatens the public safety." He called on Congress to pass new civil rights legislation, and urged the country to embrace civil rights as "a moral issue...in our daily lives." In the early hours of June 12, Medgar Evers, field secretary of the Mississippi NAACP, was assassinated by a member of the Klan. The next week, as promised, on June 19, 1963, President Kennedy submitted his Civil Rights bill to Congress.
March on Washington, 1963.
A. Philip Randolph had planned a march on Washington, D.C. in 1941 to support demands for elimination of employment discrimination in defense industries; he called off the march when the Roosevelt administration met the demand by issuing Executive Order 8802 barring racial discrimination and creating an agency to oversee compliance with the order.
Randolph and Bayard Rustin were the chief planners of the second march, which they proposed in 1962. In 1963, the Kennedy administration initially opposed the march out of concern it would negatively impact the drive for passage of civil rights legislation. However, Randolph and King were firm that the march would proceed. With the march going forward, the Kennedys decided it was important to work to ensure its success. Concerned about the turnout, President Kennedy enlisted the aid of additional church leaders and the UAW union to help mobilize demonstrators for the cause.
The march was held on August 28, 1963. Unlike the planned 1941 march, for which Randolph included only black-led organizations in the planning, the 1963 march was a collaborative effort of all of the major civil rights organizations, the more progressive wing of the labor movement, and other liberal organizations. The march had six official goals:
Of these, the march's major focus was on passage of the civil rights law that the Kennedy administration had proposed after the upheavals in Birmingham.
National media attention also greatly contributed to the march's national exposure and probable impact. In his section "The March on Washington and Television News," notes: "Over five hundred cameramen, technicians, and correspondents from the major networks were set to cover the event. More cameras would be set up than had filmed the last presidential inauguration. One camera was positioned high in the Washington Monument, to give dramatic vistas of the marchers". By carrying the organizers' speeches and offering their own commentary, television stations framed the way their local audiences saw and understood the event.
The march was a success, although not without controversy. An estimated 200,000 to 300,000 demonstrators gathered in front of the Lincoln Memorial, where King delivered his famous "I Have a Dream" speech. While many speakers applauded the Kennedy administration for the efforts it had made toward obtaining new, more effective civil rights legislation protecting the right to vote and outlawing segregation, John Lewis of SNCC took the administration to task for not doing more to protect southern blacks and civil rights workers under attack in the Deep South.
After the march, King and other civil rights leaders met with President Kennedy at the White House. While the Kennedy administration appeared sincerely committed to passing the bill, it was not clear that it had the votes in Congress to do it. However when President Kennedy was assassinated on November 22, 1963, the new President Lyndon Johnson decided to use his influence in Congress to bring about much of Kennedy's legislative agenda.
Malcolm X joins the movement, 1964–1965.
In March 1964, Malcolm X (Malik El-Shabazz), national representative of the Nation of Islam, formally broke with that organization, and made a public offer to collaborate with any civil rights organization that accepted the right to self-defense and the philosophy of Black nationalism (which Malcolm said no longer required Black separatism). Gloria Richardson – head of the Cambridge, Maryland chapter of SNCC, leader of the Cambridge rebellion and an honored guest at The March on Washington – immediately embraced Malcolm's offer. Mrs. Richardson, "the nation's most prominent woman [civil rights] leader," told "The Baltimore Afro-American" that "Malcolm is being very practical…The federal government has moved into conflict situations only when matters approach the level of insurrection. Self-defense may force Washington to intervene sooner." Earlier, in May 1963, James Baldwin had stated publicly that "the Black Muslim movement is the only one in the country we can call grassroots, I hate to say it…Malcolm articulates for Negroes, their suffering…he corroborates their reality..." On the local level, Malcolm and the NOI had been allied with the Harlem chapter of the Congress of Racial Equality (CORE) since at least 1962.
On March 26, 1964, as the Civil Rights Act was facing stiff opposition in Congress, Malcolm had a public meeting with Martin Luther King Jr. at the Capitol building. Malcolm had attempted to begin a dialog with Dr. King as early as 1957, but King had rebuffed him. Malcolm had responded by calling King an "Uncle Tom" who turned his back on black militancy in order to appease the white power structure. However, the two men were on good terms at their face-to-face meeting. There is evidence that King was preparing to support Malcolm's plan to formally bring the US government before the United Nations on charges of human rights violations against African-Americans. Malcolm now encouraged Black nationalists to get involved in voter registration drives and other forms of community organizing to redefine and expand the movement.
Civil rights activists became increasingly combative in the 1963 to 1964 period, owing to events such as the thwarting of the Albany campaign, police repression and Ku Klux Klan terrorism in Birmingham, and the assassination of Medgar Evers. Mississippi NAACP Field Director Charles Evers–Medgar Evers' brother–told a public NAACP conference on February 15, 1964 that "non-violence won't work in Mississippi…we made up our minds…that if a white man shoots at a Negro in Mississippi, we will shoot back." The repression of sit-ins in Jacksonville, Florida provoked a riot that saw black youth throwing Molotov cocktails at police on March 24, 1964. Malcolm X gave extensive speeches in this period warning that such militant activity would escalate further if African-Americans' rights were not fully recognized. In his landmark April 1964 speech "The Ballot or the Bullet", Malcolm presented an ultimatum to white America: "There's new strategy coming in. It'll be Molotov cocktails this month, hand grenades next month, and something else next month. It'll be ballots, or it'll be bullets."
As noted in "Eyes on the Prize", "Malcolm X had a far reaching effect on the civil rights movement. In the South, there had been a long tradition of self reliance. Malcolm X's ideas now touched that tradition". Self-reliance was becoming paramount in light of the 1964 Democratic National Convention's decision to refuse seating to the Mississippi Freedom Democratic Party (MFDP) and to seat the state delegation elected in violation of the party's rules through Jim Crow law instead. SNCC moved in an increasingly militant direction and worked with Malcolm X on two Harlem MFDP fundraisers in December 1964. When Fannie Lou Hamer spoke to Harlemites about the Jim Crow violence that she'd suffered in Mississippi, she linked it directly to the Northern police brutality against blacks that Malcolm protested against; When Malcolm asserted that African-Americans should emulate the Mau Mau army of Kenya in efforts to gain their independence, many in SNCC applauded. During the Selma campaign for voting rights in 1965, Malcolm made it known that he'd heard reports of increased threats of lynching around Selma, and responded in late January with an open telegram to George Lincoln Rockwell, the head of the American Nazi Party, stating: "if your present racist agitation against our people there in Alabama causes physical harm to Reverend King or any other black Americans…you and your KKK friends will be met with maximum physical retaliation from those of us who are not handcuffed by the disarming philosophy of nonviolence." The following month, the Selma chapter of SNCC invited Malcolm to speak to a mass meeting there. On the day of Malcolm's appearance, President Johnson made his first public statement in support of the Selma campaign. Paul Ryan Haygood, a co-director of the NAACP Legal Defense Fund, credits Malcolm with a role in stimulating the responsiveness of the federal government. Haygood noted that "shortly after Malcolm's visit to Selma, a federal judge, responding to a suit brought by the Department of Justice, required Dallas County registrars to process at least 100 Black applications each day their offices were open."
St. Augustine, Florida, 1963–64.
St. Augustine, on the northeast coast of Florida was famous as the "Nation's Oldest City," founded by the Spanish in 1565. It became the stage for a great drama leading up to the passage of the landmark Civil Rights Act of 1964. A local movement, led by Dr. Robert B. Hayling, a black dentist and Air Force veteran, and affiliated with the NAACP, had been picketing segregated local institutions since 1963, as a result of which Dr. Hayling and three companions, James Jackson, Clyde Jenkins, and James Hauser, were brutally beaten at a Ku Klux Klan rally in the fall of that year.
Nightriders shot into black homes, and teenagers Audrey Nell Edwards, JoeAnn Anderson, Samuel White, and Willie Carl Singleton (who came to be known as "The St. Augustine Four") spent six months in jail and reform school after sitting in at the local Woolworth's lunch counter. It took a special action of the governor and cabinet of Florida to release them after national protests by the "Pittsburgh Courier", Jackie Robinson, and others.
In response to the repression, the St. Augustine movement practiced armed self-defense in addition to nonviolent direct action. In June 1963, Dr. Hayling publicly stated that "I and the others have armed. We will shoot first and answer questions later. We are not going to die like Medgar Evers." The comment made national headlines. When Klan nightriders terrorized black neighborhoods in St. Augustine, Hayling's NAACP members often drove them off with gunfire, and in October, a Klansman was killed.
In 1964, Dr. Hayling and other activists urged the Southern Christian Leadership Conference to come to St. Augustine. The first action came during spring break, when Hayling appealed to northern college students to come to the Ancient City, not to go to the beach, but to take part in demonstrations. Four prominent Massachusetts women—Mrs. Mary Parkman Peabody, Mrs. Esther Burgess, Mrs. Hester Campbell (all of whose husbands were Episcopal bishops), and Mrs. Florence Rowe (whose husband was vice president of John Hancock Insurance Company) came to lend their support. The arrest of Mrs. Peabody, the 72-year-old mother of the governor of Massachusetts, for attempting to eat at the segregated Ponce de Leon Motor Lodge in an integrated group, made front page news across the country, and brought the civil rights movement in St. Augustine to the attention of the world.
Widely publicized activities continued in the ensuing months, as Congress saw the longest filibuster against a civil rights bill in its history. Dr. Martin Luther King, Jr. was arrested at the Monson Motel in St. Augustine on June 11, 1964, the only place in Florida he was arrested. He sent a "Letter from the St. Augustine Jail" to a northern supporter, Rabbi Israel Dresner of New Jersey, urging him to recruit others to participate in the movement. This resulted, a week later, in the largest mass arrest of rabbis in American history—while conducting a pray-in at the Monson.
A famous photograph taken in St. Augustine shows the manager of the Monson Motel pouring acid in the swimming pool while blacks and whites are swimming in it. The horrifying photograph was run on the front page of the Washington newspaper the day the senate went to vote on passing the Civil Rights Act of 1964.
Mississippi Freedom Summer, 1964.
In the summer of 1964, COFO brought nearly 1,000 activists to Mississippi—most of them white college students—to join with local black activists to register voters, teach in "Freedom Schools," and organize the Mississippi Freedom Democratic Party (MFDP).
Many of Mississippi's white residents deeply resented the outsiders and attempts to change their society. State and local governments, police, the White Citizens' Council and the Ku Klux Klan used arrests, beatings, arson, murder, spying, firing, evictions, and other forms of intimidation and harassment to oppose the project and prevent blacks from registering to vote or achieving social equality.
On June 21, 1964, three civil rights workers disappeared. James Chaney, a young black Mississippian and plasterer's apprentice; and two Jewish activists, Andrew Goodman, a Queens College anthropology student; and Michael Schwerner, a CORE organizer from Manhattan's Lower East Side, were found weeks later, murdered by conspirators who turned out to be local members of the Klan, some of them members of the Neshoba County sheriff's department. This outraged the public, leading the U.S. Justice Department along with the FBI (the latter which had previously avoided dealing with the issue of segregation and persecution of blacks) to take action. The outrage over these murders helped lead to the passage of the Civil Rights Act. (See Mississippi civil rights workers murders for details).
From June to August, Freedom Summer activists worked in 38 local projects scattered across the state, with the largest number concentrated in the Mississippi Delta region. At least 30 Freedom Schools, with close to 3,500 students were established, and 28 community centers set up.
Over the course of the Summer Project, some 17,000 Mississippi blacks attempted to become registered voters in defiance of the red tape and forces of white supremacy arrayed against them—only 1,600 (less than 10%) succeeded. But more than 80,000 joined the Mississippi Freedom Democratic Party (MFDP), founded as an alternative political organization, showing their desire to vote and participate in politics.
Though Freedom Summer failed to register many voters, it had a significant effect on the course of the Civil Rights Movement. It helped break down the decades of people's isolation and repression that were the foundation of the Jim Crow system. Before Freedom Summer, the national news media had paid little attention to the persecution of black voters in the Deep South and the dangers endured by black civil rights workers. The progression of events throughout the South increased media attention to Mississippi.
The deaths of affluent northern white students and threats to other northerners attracted the full attention of the media spotlight to the state. Many black activists became embittered, believing the media valued lives of whites and blacks differently. Perhaps the most significant effect of Freedom Summer was on the volunteers, almost all of whom—black and white—still consider it to have been one of the defining periods of their lives.
Civil Rights Act of 1964.
Although President Kennedy had proposed civil rights legislation and it had support from Northern Congressmen and Senators of both parties, Southern Senators blocked the bill by threatening filibusters. After considerable parliamentary maneuvering and 54 days of filibuster on the floor of the United States Senate, President Johnson got a bill through the Congress.
On July 2, 1964, Johnson signed the Civil Rights Act of 1964, that banned discrimination based on "race, color, religion, sex or national origin" in employment practices and public accommodations. The bill authorized the Attorney General to file lawsuits to enforce the new law. The law also nullified state and local laws that required such discrimination.
Mississippi Freedom Democratic Party, 1964.
Blacks in Mississippi had been disfranchised by statutory and constitutional changes since the late 19th century. In 1963 COFO held a Freedom Vote in Mississippi to demonstrate the desire of black Mississippians to vote. More than 80,000 people registered and voted in the mock election, which pitted an integrated slate of candidates from the "Freedom Party" against the official state Democratic Party candidates.
In 1964, organizers launched the Mississippi Freedom Democratic Party (MFDP) to challenge the all-white official party. When Mississippi voting registrars refused to recognize their candidates, they held their own primary. They selected Fannie Lou Hamer, Annie Devine, and Victoria Gray to run for Congress, and a slate of delegates to represent Mississippi at the 1964 Democratic National Convention.
The presence of the Mississippi Freedom Democratic Party in Atlantic City, New Jersey, was inconvenient, however, for the convention organizers. They had planned a triumphant celebration of the Johnson administration's achievements in civil rights, rather than a fight over racism within the Democratic Party. All-white delegations from other Southern states threatened to walk out if the official slate from Mississippi was not seated. Johnson was worried about the inroads that Republican Barry Goldwater's campaign was making in what previously had been the white Democratic stronghold of the "Solid South", as well as support that George Wallace had received in the North during the Democratic primaries.
Johnson could not, however, prevent the MFDP from taking its case to the Credentials Committee. There Fannie Lou Hamer testified eloquently about the beatings that she and others endured and the threats they faced for trying to register to vote. Turning to the television cameras, Hamer asked, "Is this America?"
Johnson offered the MFDP a "compromise" under which it would receive two non-voting, at-large seats, while the white delegation sent by the official Democratic Party would retain its seats. The MFDP angrily rejected the "compromise."
The MFDP kept up its agitation at the convention, after it was denied official recognition. When all but three of the "regular" Mississippi delegates left because they refused to pledge allegiance to the party, the MFDP delegates borrowed passes from sympathetic delegates and took the seats vacated by the official Mississippi delegates. National party organizers removed them. When they returned the next day, they found convention organizers had removed the empty seats that had been there the day before. They stayed and sang "freedom songs".
The 1964 Democratic Party convention disillusioned many within the MFDP and the Civil Rights Movement, but it did not destroy the MFDP. The MFDP became more radical after Atlantic City. It invited Malcolm X to speak at one of its conventions and opposed the war in Vietnam.
King awarded Nobel Peace Prize.
On December 10, 1964, Dr. Martin Luther King, Jr. was awarded the Nobel Peace Prize, the youngest man to receive the award; he was 35 years of age.
Boycott of New Orleans by American Football League players, January 1965.
After the 1964 professional American Football League season, the AFL All-Star Game had been scheduled for early 1965 in New Orleans' Tulane Stadium. After numerous black players were refused service by a number of New Orleans hotels and businesses, and white cabdrivers refused to carry black passengers, black and white players alike lobbied for a boycott of New Orleans. Under the leadership of Buffalo Bills' players, including Cookie Gilchrist, the players put up a unified front. The game was moved to Jeppesen Stadium in Houston.
The discriminatory practices that prompted the boycott were illegal under the Civil Rights Act of 1964, which had been signed in July 1964. This new law likely encouraged the AFL players in their cause. It was the first boycott by a professional sports event of an entire city.
Selma Voting Rights Movement and the Voting Rights Act, 1965.
SNCC had undertaken an ambitious voter registration program in Selma, Alabama, in 1963, but by 1965 had made little headway in the face of opposition from Selma's sheriff, Jim Clark. After local residents asked the SCLC for assistance, King came to Selma to lead several marches, at which he was arrested along with 250 other demonstrators. The marchers continued to meet violent resistance from police. Jimmie Lee Jackson, a resident of nearby Marion, was killed by police at a later march in February 17, 1965. Jackson's death prompted James Bevel, director of the Selma Movement, to initiate a plan to march from Selma to Montgomery, the state capital.
On March 7, 1965, acting on Bevel's plan, Hosea Williams of the SCLC and John Lewis of SNCC led a march of 600 people to walk the 54 miles (87 km) from Selma to the state capital in Montgomery. Only six blocks into the march, at the Edmund Pettus Bridge, state troopers and local law enforcement, some mounted on horseback, attacked the peaceful demonstrators with billy clubs, tear gas, rubber tubes wrapped in barbed wire, and bull whips. They drove the marchers back into Selma. John Lewis was knocked unconscious and dragged to safety. At least 16 other marchers were hospitalized. Among those gassed and beaten was Amelia Boynton Robinson, who was at the center of civil rights activity at the time.
The national broadcast of the news footage of lawmen attacking unresisting marchers' seeking to exercise their constitutional right to vote provoked a national response, as had scenes from Birmingham two years earlier. The marchers were able to obtain a court order permitting them to make the march without incident two weeks later.
After a second march on March 9 to the site of Bloody Sunday, local whites attacked Rev. James Reeb, another voting rights supporter. He died of his injuries in a Birmingham hospital March 11. On March 25, four Klansmen shot and killed Detroit homemaker Viola Liuzzo as she drove marchers back to Selma at night after the successfully completed march to Montgomery.
Eight days after the first march, President Johnson delivered a televised address to support the voting rights bill he had sent to Congress. In it he stated:
But even if we pass this bill, the battle will not be over. What happened in Selma is part of a far larger movement which reaches into every section and state of America. It is the effort of American Negroes to secure for themselves the full blessings of American life.
Their cause must be our cause too. Because it is not just Negroes, but really it is all of us, who must overcome the crippling legacy of bigotry and injustice. And we shall overcome.
Johnson signed the Voting Rights Act of 1965 on August 6. The 1965 act suspended poll taxes, literacy tests, and other subjective voter registration tests. It authorized Federal supervision of voter registration in states and individual voting districts where such tests were being used. African Americans who had been barred from registering to vote finally had an alternative to taking suits to local or state courts, which had seldom prosecuted their cases to success. If discrimination in voter registration occurred, the 1965 act authorized the Attorney General of the United States to send Federal examiners to replace local registrars. Johnson reportedly told associates of his concern that signing the bill had lost the white South as voters for the Democratic Party for the foreseeable future.
The act had an immediate and positive effect for African Americans. Within months of its passage, 250,000 new black voters had been registered, one third of them by federal examiners. Within four years, voter registration in the South had more than doubled. In 1965, Mississippi had the highest black voter turnout at 74% and led the nation in the number of black public officials elected. In 1969, Tennessee had a 92.1% turnout among black voters; Arkansas, 77.9%; and Texas, 73.1%.
Several whites who had opposed the Voting Rights Act paid a quick price. In 1966 Sheriff Jim Clark of Alabama, infamous for using cattle prods against civil rights marchers, was up for reelection. Although he took off the notorious "Never" pin on his uniform, he was defeated. At the election, Clark lost as blacks voted to get him out of office. Clark later served a prison term for drug dealing.
Blacks' regaining the power to vote changed the political landscape of the South. When Congress passed the Voting Rights Act, only about 100 African Americans held elective office, all in northern states. By 1989, there were more than 7,200 African Americans in office, including more than 4,800 in the South. Nearly every Black Belt county (where populations were majority black) in Alabama had a black sheriff. Southern blacks held top positions in city, county, and state governments.
Atlanta elected a black mayor, Andrew Young, as did Jackson, Mississippi, with Harvey Johnson, Jr., and New Orleans, with Ernest Morial. Black politicians on the national level included Barbara Jordan, elected as a Representative from Texas in Congress, and President Jimmy Carter appointed Andrew Young as United States Ambassador to the United Nations. Julian Bond was elected to the Georgia State Legislature in 1965, although political reaction to his public Opposition to the U.S. involvement in the Vietnam War prevented him from taking his seat until 1967. John Lewis represents Georgia's 5th congressional district in the United States House of Representatives, where he has served since 1987.
Fair housing movements, 1966–1968.
The first major blow against housing segregation in the era, the Rumford Fair Housing Act, was passed in California in 1963. It was overturned by white California voters and real estate lobbyists the following year with Proposition 14, a move which helped precipitate the Watts Riots. In 1966, the California Supreme Court invalidated Proposition 14 and reinstated the Fair Housing Act.
Struggles for fair housing laws became a major project of the movement over the next two years, with Martin Luther King Jr. leading the Chicago Freedom Movement around the issue in 1966. In the following year, Father James Groppi and the NAACP Youth Council also attracted national attention with a fair housing campaign in Milwaukee. Both movements faced violent mob resistance from white homeowners and legal opposition from conservative politicians.
The fair housing bill was the most contentious civil rights legislation of the era. Senator Walter Mondale, who advocated for the bill, noted that over successive years, it was the most filibustered legislation in US history. It was opposed by most Northern and Southern senators, as well as the National Association of Real Estate Boards. A proposed "Civil Rights Act of 1966" had collapsed completely because of its fair housing provision. Mondale commented that:
Memphis, King assassination and the Poor People's March 1968.
Rev. James Lawson invited King to Memphis, Tennessee, in March 1968 to support a sanitation workers' strike. These workers launched a campaign for union representation after two workers were accidentally killed on the job, and King considered their struggle to be a vital part of the Poor People's Campaign he was planning.
A day after delivering his stirring "I've Been to the Mountaintop" sermon, which has become famous for his vision of American society, King was assassinated on April 4, 1968. Riots broke out in black neighborhoods in more than 110 cities across the United States in the days that followed, notably in Chicago, Baltimore, and in Washington, D.C. The damage done in many cities destroyed black businesses and homes, and slowed economic development for a generation.
The day before King's funeral, April 8, Coretta Scott King and three of the King children led 20,000 marchers through the streets of Memphis, holding signs that read, "Honor King: End Racism" and "Union Justice Now". Armed National Guardsmen lined the streets, sitting on M-48 tanks, to protect the marchers, and helicopters circled overhead. On April 9 Mrs. King led another 150,000 people in a funeral procession through the streets of Atlanta. Her dignity revived courage and hope in many of the Movement's members, cementing her place as the new leader in the struggle for racial equality.
Coretta Scott King said,
[Martin Luther King, Jr.] gave his life for the poor of the world, the garbage workers of Memphis and the peasants of Vietnam. The day that Negro people and others in bondage are truly free, on the day want is abolished, on the day wars are no more, on that day I know my husband will rest in a long-deserved peace.
Rev. Ralph Abernathy succeeded King as the head of the SCLC and attempted to carry forth King's plan for a Poor People's March. It was to unite blacks and whites to campaign for fundamental changes in American society and economic structure. The march went forward under Abernathy's plainspoken leadership but did not achieve its goals.
Civil Rights Act of 1968.
As 1968 began, the fair housing bill was being filibustered once again, but two developments revived it. The Kerner Commission report on the 1967 ghetto riots was delivered to Congress on March 1, and it strongly recommended "a comprehensive and enforceable federal open housing law" as a remedy to the civil disturbances. The Senate was moved to end their filibuster that week.
As the House of Representatives deliberated the bill in April, Dr. King was assassinated, and the largest wave of unrest since the Civil War swept the country. Senator Charles Mathias wrote that
The House passed the legislation on April 10, and President Johnson signed it the next day. The Civil Rights Act of 1968 prohibited discrimination concerning the sale, rental, and financing of housing based on race, religion, national origin. It also made it a federal crime to "by force or by threat of force, injure, intimidate, or interfere with anyone … by reason of their race, color, religion, or national origin."
Other issues.
Competing ideas.
Despite the common notion that the ideas of Martin Luther King, Jr., Malcolm X and Black Power only conflicted with each other and were the only ideologies of the Civil Rights Movement, there were other sentiments felt by many blacks. Fearing the events during the movement were occurring too quickly, there were some blacks who felt that leaders should take their activism at a slower pace. Others had reservations on how focused blacks were on the movement and felt that such attention was better spent on reforming issues within the black community.
While most popular representations of the movement are centered on the leadership and philosophy of Martin Luther King, Jr., some scholars note that the movement was too diverse to be credited to one person, organization, or strategy. Sociologist Doug McAdam has stated that, "in King's case, it would be inaccurate to say that he was the leader of the modern civil rights movement...but more importantly, there was no singular civil rights movement. The movement was, in fact, a coalition of thousands of local efforts nationwide, spanning several decades, hundreds of discrete groups, and all manner of strategies and tactics—legal, illegal, institutional, non-institutional, violent, non-violent. Without discounting King's importance, it would be sheer fiction to call him the leader of what was fundamentally an amorphous, fluid, dispersed movement."
Those who blatantly rejected integration usually had a legitimate rationale for doing so, such as fearing a change in the status quo they had been used to for so long, or fearing for their safety if they found themselves in environments where whites were much more present. However, there were also those who defended segregation for the sake of keeping ties with the white power structure from which many relied on for social and economic mobility above other blacks. Based on her interpretation of a 1966 study made by Donald Matthews and James Prothro detailing the relative percentage of blacks for integration, against it or feeling something else, Lauren Winner asserts that:
Black defenders of segregation look, at first blush, very much like black nationalists, especially in their preference for all-black institutions; but black defenders of segregation differ from nationalists in two key ways. First, while both groups criticize NAACP-style integration, nationalists articulate a third alternative to integration and Jim Crow, while segregationists preferred to stick with the status quo. Second, absent from black defenders of segregation's political vocabulary was the demand for self-determination. They called for all-black institutions, but not autonomous all-black institutions; indeed, some defenders of segregation asserted that black people needed white paternalism and oversight in order to thrive.
Oftentimes, African-American community leaders would be staunch defenders of segregation. Church ministers, businessmen and educators were among those who wished to keep segregation and segregationist ideals in order to retain the privileges they gained from patronage from whites, such as monetary gains. In addition, they relied on segregation to keep their jobs and economies in their communities thriving. It was feared that if integration became widespread in the South, black-owned businesses and other establishments would lose a large chunk of their customer base to white-owned businesses, and many blacks would lose opportunities for jobs that were presently exclusive to their interests. On the other hand, there were the everyday, average black people who criticized integration as well. For them, they took issue with different parts of the Civil Rights Movement and the potential for blacks to exercise consumerism and economic liberty without hindrance from whites.
For Martin Luther King, Jr., Malcolm X and other leading activists and groups during the movement, these opposing viewpoints acted as an obstacle against their ideas. These different views made such leaders' work much harder to accomplish, but they were nonetheless important in the overall scope of the movement. For the most part, the black individuals who had reservations on various aspects of the movement and ideologies of the activists were not able to make a game-changing dent in their efforts, but the existence of these alternate ideas gave some blacks an outlet to express their concerns about the changing social structure.
Avoiding the "Communist" label.
On December 17, 1951, the Communist Party–affiliated Civil Rights Congress delivered the petition "We Charge Genocide: "The Crime of Government Against the Negro People"", often shortened to "We Charge Genocide", to the United Nations in 1951, arguing that the U.S. federal government, by its failure to act against lynching in the United States, was guilty of genocide under Article II of the UN Genocide Convention. The petition was presented to the United Nations at two separate venues: Paul Robeson, concert singer and activist, to a UN official in New York City, while William L. Patterson, executive director of the CRC, delivered copies of the drafted petition to a UN delegation in Paris.
Patterson, the editor of the petition, was a leader in the Communist Party USA and head of the International Labor Defense, a group that offered legal representation to communists, trade unionists, and African-Americans in cases involving issues of political or racial persecution. The ILD was known for leading the defense of the Scottsboro boys in Alabama in 1931, where the Communist Party had considerable influence among African Americans in the 1930s. This had largely declined by the late 1950s, although they could command international attention. As earlier Civil Rights figures such as Robeson, Du Bois and Patterson became more politically radical (and therefore targets of Cold War anti-Communism by the US. Government), they lost favor with both mainstream Black America and the NAACP.
In order to secure a place in the mainstream and gain the broadest base, the new generation of civil rights activists believed they had to openly distance themselves from anything and anyone associated with the Communist party. According to Ella Baker, the Southern Christian Leadership Conference adopted "Christian" into its name to deter charges of Communism. The FBI under J Edgar Hoover had been concerned about communism since the early 20th century, and continued to label as "Communist" or "subversive" some of the civil rights activists, whom it kept under close surveillance. In the early 1960s, the practice of distancing the Civil Rights Movement from "Reds" was challenged by the Student Nonviolent Coordinating Committee who adopted a policy of accepting assistance and participation by anyone, regardless of political affiliation, who supported the SNCC program and was willing to "put their body on the line." At times this political openness put SNCC at odds with the NAACP.
Kennedy administration, 1961–63.
During the years preceding his election to the presidency, John F. Kennedy's record of voting on issues of racial discrimination had been minimal. Kennedy openly confessed to his closest advisors that during the first months of his presidency, his knowledge of the civil rights movement was "lacking".
For the first two years of the Kennedy administration, civil rights activists had mixed opinions of both the president and attorney general, Robert F. Kennedy. Many viewed the administration with suspicion. A well of historical cynicism toward white liberal politics had left African Americans with a sense of uneasy disdain for any white politician who claimed to share their concerns for freedom. Still, many had a strong sense that the Kennedys represented a new age of political dialogue.
Although observers frequently assert the phrases "The Kennedy administration" or "President Kennedy" when discussing the executive and legislative support of the Civil Rights movement between 1960 and 1963, many of the initiatives resulted from Robert Kennedy's passion. Through his rapid education in the realities of racism, Robert Kennedy underwent a thorough conversion of purpose as Attorney-General. The President came to share his brother's sense of urgency on the matters; the Attorney-General succeeded in urging the president to address the issue in a speech to the nation.
Robert Kennedy first became seriously concerned with civil rights in mid-May 1961 during the Freedom Rides, when photographs of the burning bus and savage beatings in Aniston and Birmingham were broadcast around the world. They came at an especially embarrassing time, as President Kennedy was about to have a summit with the Soviet premier in Vienna. The White House was concerned with its image among the populations of newly independent nations in Africa and Asia, and Robert Kennedy responded with an address for Voice of America stating that great progress had been made on the issue of race relations. Meanwhile, behind the scenes, the administration worked to resolve the crisis with a minimum of violence and prevent the Freedom Riders from generating a fresh crop of headlines that might divert attention from the President's international agenda. The "Freedom Riders" documentary notes that, "The back burner issue of civil rights had collided with the urgent demands of Cold War realpolitik."
On May 21, when a white mob attacked and burned the First Baptist Church in Montgomery, Alabama, where King was holding out with protesters, Robert Kennedy telephoned King to ask him to stay in the building until the U.S. Marshals and National Guard could secure the area. King proceeded to berate Kennedy for "allowing the situation to continue". King later publicly thanked Robert Kennedy's commanding the force to break up an attack, which might otherwise have ended King's life.
With a very small majority in Congress, the president's ability to press ahead with legislation relied considerably on a balancing game with the Senators and Congressmen of the South. Without the support of Vice-President Lyndon Johnson, a former Senator who had years of experience in Congress and longstanding relations there, many of the Attorney-General's programs would not have progressed.
By late 1962, frustration at the slow pace of political change was balanced by the movement's strong support for legislative initiatives: housing rights, administrative representation across all US Government departments, safe conditions at the ballot box, pressure on the courts to prosecute racist criminals. King remarked by the end of the year, 
This administration has reached out more creatively than its predecessors to blaze new trails, [notably in voting rights and government appointments]. Its vigorous young men [had launched] imaginative and bold forays [and displayed] a certain "élan" in the attention they give to civil-rights issues.
From squaring off against Governor George Wallace, to "tearing into" Vice-President Johnson (for failing to desegregate areas of the administration), to threatening corrupt white Southern judges with disbarment, to desegregating interstate transport, Robert Kennedy came to be consumed by the Civil Rights movement. He continued to work on these social justice issues in his bid for the presidency in 1968.
On the night of Governor Wallace's capitulation to African-American enrollment at the University of Alabama, President Kennedy gave an address to the nation, which marked the changing tide, an address that was to become a landmark for the ensuing change in political policy as to civil rights. In it President Kennedy spoke of the need to act decisively and to act now:
"We preach freedom around the world, and we mean it, and we cherish our freedom here at home, but are we to say to the world, and much more importantly, to each other that this is the land of the free except for the Negroes; that we have no second-class citizens except Negroes; that we have no class or caste system, no ghettoes, no master race except with respect to Negroes? Now the time has come for this Nation to fulfill its promise. The events in Birmingham and elsewhere have so increased the cries for equality that no city or State or legislative body can prudently choose to ignore them."—President Kennedy, 
Assassination cut short the life and careers of both the Kennedy brothers and Dr. Martin Luther King, Jr. The essential groundwork of the Civil Rights Act 1964 had been initiated before John F. Kennedy was assassinated. The dire need for political and administrative reform was driven home on Capitol Hill by the combined efforts of the Kennedy brothers, Dr. King (and other leaders) and President Lyndon Johnson.
In 1966, Robert Kennedy undertook a tour of South Africa in which he championed the cause of the anti-apartheid movement. His tour gained international praise at a time when few politicians dared to entangle themselves in the politics of South Africa. Kennedy spoke out against the oppression of the black population. He was welcomed by the black population as though a visiting head of state. In an interview with "LOOK" Magazine he said:
At the University of Natal in Durban, I was told the church to which most of the white population belongs teaches apartheid as a moral necessity. A questioner declared that few churches allow black Africans to pray with the white because the Bible says that is the way it should be, because God created Negroes to serve. "But suppose God is black", I replied. "What if we go to Heaven and we, all our lives, have treated the Negro as an inferior, and God is there, and we look up and He is not white? What then is our response?" There was no answer. Only silence.—Robert Kennedy, "LOOK" Magazine
American Jewish community and the Civil Rights movement.
Many in the Jewish community supported the Civil Rights Movement. In fact, statistically Jews were one of the most actively involved non-black groups in the Movement. Many Jewish students worked in concert with African Americans for CORE, SCLC, and SNCC as full-time organizers and summer volunteers during the Civil Rights era. Jews made up roughly half of the white northern volunteers involved in the 1964 Mississippi Freedom Summer project and approximately half of the civil rights attorneys active in the South during the 1960s.
Jewish leaders were arrested while heeding a call from Rev. Dr. Martin Luther King, Jr. in St. Augustine, Florida, in June 1964, where the largest mass arrest of rabbis in American history took place at the Monson Motor Lodge—a nationally important civil rights landmark that was demolished in 2003 so that a Hilton Hotel could be built on the site. Abraham Joshua Heschel, a writer, rabbi, and professor of theology at the Jewish Theological Seminary of America in New York, was outspoken on the subject of civil rights. He marched arm-in-arm with Dr. King in the 1965 Selma to Montgomery march. In the Mississippi civil rights workers' murders of 1964, the two white activists killed, Andrew Goodman and Michael Schwerner, were both Jewish.
Brandeis University, the only nonsectarian Jewish-sponsored college university in the world, created the Transitional Year Program (TYP) in 1968, in part response to Rev. Dr. Martin Luther King's assassination. The faculty created it to renew the University's commitment to social justice. Recognizing Brandeis as a university with a commitment to academic excellence, these faculty members created a chance to disadvantaged students to participate in an empowering educational experience.
The program began by admitting 20 black males. As it developed, two groups have been given chances. The first group consists of students whose secondary schooling experiences and/or home communities may have lacked the resources to foster adequate preparation for success at elite colleges like Brandeis. For example, their high schools do not offer AP or honors courses nor high quality laboratory experiences.
Students selected had to have excelled in the curricula offered by their schools. The second group of students includes those whose life circumstances have created formidable challenges that required focus, energy, and skills that otherwise would have been devoted to academic pursuits. Some have served as heads of their households, others have worked full-time while attending high school full-time, and others have shown leadership in other ways.
The American Jewish Committee, American Jewish Congress, and Anti-Defamation League (ADL) actively promoted civil rights.
While Jews were very active in the civil rights movement in the South, in the North, many had experienced a more strained relationship with African Americans. In communities experiencing white flight, racial rioting, and urban decay, Jewish Americans were more often the last remaining whites in the communities most affected. With Black militancy and the Black Power movements on the rise, Black Anti-Semitism increased leading to strained relations between Blacks and Jews in Northern communities. In New York City, most notably, there was a major socio-economic class difference in the perception of African Americans by Jews. Jews from better educated Upper Middle Class backgrounds were often very supportive of African American civil rights activities while the Jews in poorer urban communities that became increasingly minority were often less supportive largely in part due to more negative and violent interactions between the two groups.
Profile.
Despite large Jewish organisations such as the American Jewish Committee, American Jewish Congress and the ADL being actively involved in the Movement, many Jewish individuals in the Southern states who supported civil rights for African-Americans tended to keep a low profile on "the race issue", in order to avoid attracting the attention of the anti-Black and antisemitic Ku Klux Klan. However, Klan groups exploited the issue of African-American integration and Jewish involvement in the struggle to launch acts of violent antisemitism. As an example of this hatred, in one year alone, from November 1957 to October 1958, temples and other Jewish communal gatherings were bombed and desecrated in Atlanta, Nashville, Jacksonville, and Miami, and dynamite was found under synagogues in Birmingham, Charlotte, and Gastonia, North Carolina. Some rabbis received death threats, but there were no injuries following these outbursts of violence.
Fraying of alliances.
King reached the height of popular acclaim during his life in 1964, when he was awarded the Nobel Peace Prize. His career after that point was filled with frustrating challenges. The liberal coalition that had gained passage of the Civil Rights Act of 1964 and the Voting Rights Act of 1965 began to fray.
King was becoming more estranged from the Johnson administration. In 1965 he broke with it by calling for peace negotiations and a halt to the bombing of Vietnam. He moved further left in the following years, speaking of the need for economic justice and thoroughgoing changes in American society. He believed change was needed beyond the civil rights gained by the movement.
King's attempts to broaden the scope of the Civil Rights Movement were halting and largely unsuccessful, however. King made several efforts in 1965 to take the Movement north to address issues of employment and housing discrimination. SCLC's campaign in Chicago publicly failed, as Chicago Mayor Richard J. Daley marginalized SCLC's campaign by promising to "study" the city's problems. In 1966, white demonstrators holding "white power" signs in notoriously racist Cicero, a suburb of Chicago, threw stones at marchers demonstrating against housing segregation.
Race riots, 1963–70.
By the end of World War II, more than half of the country's black population lived in Northern and Western industrial cities rather than Southern rural areas. Migrating to those cities for better job opportunities, education and to escape legal segregation, African Americans often found segregation that existed in fact rather than in law.
While after the 1920s, the Ku Klux Klan was not prevalent, by the 1960s other problems prevailed in northern cities. Beginning in the 1950s, deindustrialization and restructuring of major industries: railroads and meatpacking, steel industry and car industry, markedly reduced working-class jobs, which had earlier provided middle-class incomes. As the last population to enter the industrial job market, blacks were disadvantaged by its collapse. At the same time, investment in highways and private development of suburbs in the postwar years had drawn many ethnic whites out of the cities to newer housing in expanding suburbs. Urban blacks who did not follow the middle class out of the cities became concentrated in the older housing of inner city neighborhoods, among the poorest in most major cities.
Because jobs in new service areas and parts of the economy were being created in suburbs, unemployment was much higher in many black than in white neighborhoods, and crime was frequent. African Americans rarely owned the stores or businesses where they lived. Many were limited to menial or blue-collar jobs, although union organizing in the 1930s and 1940s had opened up good working environments for some. African Americans often made only enough money to live in dilapidated tenements that were privately owned, or poorly maintained public housing. They also attended schools that were often the worst academically in the city and that had fewer white students than in the decades before WWII.
The racial makeup of most major city police departments, largely ethnic white (especially Irish), was a major factor in adding to racial tensions. Even a black neighborhood such as Harlem had a ratio of one black officer for every six white officers. The majority-black city of Newark, New Jersey had only 145 blacks among its 1322 police officers. Police forces in Northern cities were largely composed of white ethnics, descendants of 19th-century immigrants: mainly Irish, Italian, and Eastern European officers. They had established their own power bases in the police departments and in territories in cities. Some would routinely harass blacks with or without provocation.
Harlem riot of 1964.
One of the first major race riots took place in Harlem, New York, in the summer of 1964. A white Irish-American police officer, Thomas Gilligan, shot 15-year-old James Powell, who was black, for allegedly charging him armed with a knife. It was found that Powell was unarmed. A group of black citizens demanded Gilligan's suspension. Hundreds of young demonstrators marched peacefully to the 67th Street police station on July 17, 1964, the day after Powell's death.
The police department did not suspend Gilligan. Although the precinct had promoted the NYPD's first black station commander, neighborhood residents were frustrated with racial inequalities. Rioting broke out, and Bedford-Stuyvesant, a major black neighborhood in Brooklyn erupted next. That summer, rioting also broke out in Philadelphia, for similar reasons.
In the aftermath of the riots of July 1964, the federal government funded a pilot program called Project Uplift. Thousands of young people in Harlem were given jobs during the summer of 1965. The project was inspired by a report generated by HARYOU called "Youth in the Ghetto." HARYOU was given a major role in organizing the project, together with the National Urban League and nearly 100 smaller community organizations. Permanent jobs at living wages were still out of reach of many young black men.
Watts riot (1965).
In 1965, President Lyndon B. Johnson signed the Voting Rights Act, but the new law had no immediate effect on living conditions for blacks. A few days after the act became law, a riot broke out in the South Central Los Angeles neighborhood of Watts. Like Harlem, Watts was an impoverished neighborhood with very high unemployment. Its residents were supervised by a largely white police department that had a history of abuse against blacks.
While arresting a young man for drunk driving, police officers argued with the suspect's mother before onlookers. The conflict triggered a massive destruction of property through six days of rioting. Thirty-four people were killed and property valued at about $30 million was destroyed, making the Watts Riots among the most expensive in American history.
With black militancy on the rise, ghetto residents directed acts of anger at the police. Black residents growing tired of police brutality continued to riot. Some young people joined groups such as the Black Panthers, whose popularity was based in part on their reputation for confronting police officers. Riots among blacks occurred in 1966 and 1967 in cities such as Atlanta, San Francisco, Oakland, Baltimore, Seattle, Tacoma, Cleveland, Cincinnati, Columbus, Newark, Chicago, New York City (specifically in Brooklyn, Harlem and the Bronx), and worst of all in Detroit.
Detroit riot of 1967.
In Detroit, a small black middle class had begun to develop among those African-Americans who worked at unionized jobs in the automotive industry; these workers still contended with unsafe working conditions and racist practices, concerns which the United Auto Workers channeled into bureaucratic and ineffective grievance procedures. White mobs enforced the segregation of housing up through the 1960s; upon learning that a new homebuyer was black, whites would congregate outside the home picketing, often breaking windows, committing arson, and attacking their new neighbors. Blacks who were not upwardly mobile were living in substandard conditions, subject to the same problems as African-Americans in Watts and Harlem.
When white police officers shut down an illegal bar and arrested a large group of patrons during the hot summer, furious residents rioted. Blacks looted and destroyed property for five days, and National Guardsmen and federal troops patrolled in tanks through the streets. Residents reported that police officers shot at black people before even determining if the suspects were armed or dangerous. After five days, 41 people had been killed, hundreds injured and thousands left homeless. $40 to $45 million worth of damage was caused.
State and local governments responded to the riot with a dramatic increase in minority hiring. Mayor Cavanaugh in May 1968 appointed a Special Task Force on Police Recruitment and Hiring, and by July 1972, blacks made up 14 percent of the Detroit police, more than double their percentage in 1967. The Michigan government used its reviews of contracts issued by the state to secure a 21 percent increase in nonwhite employment. In the aftermath of the turmoil, the Greater Detroit Board of Commerce launched a campaign to find jobs for ten thousand "previously unemployable" persons, a preponderant number of whom were black.
Prior to the disorder, Detroit enacted no ordinances to end housing segregation, and few had been enacted in the state of Michigan at all. Governor George Romney immediately responded to the riot of 1967 with a special session of the Michigan legislature where he forwarded sweeping housing proposals that included not only fair housing, but "important relocation, tenants' rights and code enforcement legislation." Romney had supported such proposals in 1965, but abandoned them in the face of organized opposition. White conservative resistance was powerful in 1967 as well, but this time Romney did not relent and once again proposed the housing laws at the regular 1968 session of the legislature.
The governor publicly warned that if the housing measures were not passed, "it will accelerate the recruitment of revolutionary insurrectionists." The laws passed both houses of the legislature. The "Michigan Historical Review" wrote that: "The Michigan Fair Housing Act, which took effect on November 15, 1968, was stronger than the federal fair housing law…and than just about all the existing state fair housing acts. It is probably more than a coincidence that the state that had experienced the most severe racial disorder of the 1960s also adopted one of the strongest state fair housing acts."
Detroit's decline had begun in the 1950s, during which the city lost almost a tenth of its population. It has been argued – including by Mayor Coleman Young – that the riot was the primary accelerator of "white flight", an ethnic succession by which white residents moved out of inner-city neighborhoods into the suburbs. In contrast, urban affairs experts largely blame a Supreme Court decision against NAACP lawsuits on school desegregation – 1974's "Milliken v. Bradley" case – which maintained the suburban schools as a lily-white refuge. In his dissenting opinion, Supreme Court Justice William O. Douglas wrote that the "Milliken" decision perpetuated "restrictive covenants" that "maintained...black ghettos." (Detroit lost 12.8% of its white population in the 1950s, 15.2% of its white population in the 1960s, and 21.2% of its white population in the 1970s.)
Nationwide riots of 1967.
In addition to Detroit, over 100 US cities experienced riots in 1967, including Newark, Cincinnati, Cleveland, and Washington D.C. President Johnson created the National Advisory Commission on Civil Disorders in 1967. The commission's final report called for major reforms in employment and public assistance for black communities. It warned that the United States was moving toward separate white and black societies.
King riots (1968).
In April 1968 after the assassination of Dr. Martin Luther King, Jr. in Memphis, Tennessee, rioting broke out in cities across the country from frustration and despair. These included Cleveland, Baltimore, Washington, D.C., Chicago, New York City and Louisville, Kentucky. As in previous riots, most of the damage was done in black neighborhoods. In some cities, it has taken more than a quarter of a century for these areas to recover from the damage of the riots; in others, little recovery has been achieved.
Programs in affirmative action resulted in the hiring of more black police officers in every major city. Today blacks make up a proportional majority of the police departments in cities such as Baltimore, Washington, New Orleans, Atlanta, Newark, and Detroit. Civil rights laws have reduced employment discrimination.
The conditions that led to frequent rioting in the late 1960s have receded, but not all the problems have been solved. With industrial and economic restructuring, hundreds of thousands of industrial jobs disappeared since the later 1950s from the old industrial cities. Some moved South, as has much population following new jobs, and others out of the U.S. altogether. Civil unrest broke out in Miami in 1980, in Los Angeles in 1992, and in Cincinnati in 2001.
Black power, 1966.
During the Freedom Summer campaign of 1964, numerous tensions within the civil rights movement came to the forefront. Many blacks in SNCC developed concerns that white activists from the North were taking over the movement. The massive presence of white students was also not reducing the amount of violence that SNCC suffered, but seemed to be increasing it. Additionally, there was profound disillusionment at Lyndon Johnson's denial of voting status for the Mississippi Freedom Democratic Party. Meanwhile, during CORE's work in Louisiana that summer, that group found the federal government would not respond to requests to enforce the provisions of the Civil Rights Act of 1964, or to protect the lives of activists who challenged segregation. For the Louisiana campaign to survive it had to rely on a local African-American militia called the Deacons for Defense and Justice, who used arms to repel white supremacist violence and police repression. CORE's collaboration with the Deacons was effective against breaking Jim Crow in numerous Louisiana areas.
In 1965, SNCC helped organize an independent political party, the Lowndes County Freedom Organization (LCFO), in the heart of Alabama Klan territory, and permitted its black leaders to openly promote the use of armed self-defense. Meanwhile, the Deacons for Defense and Justice expanded into Mississippi and assisted Charles Evers' NAACP chapter with a successful campaign in Natchez. The same year, the Watts Rebellion took place in Los Angeles, and seemed to show that most black youth were now committed to the use of violence to protest inequality and oppression.
During the March Against Fear in 1966, SNCC and CORE fully embraced the slogan of "black power" to describe these trends towards militancy and self-reliance. In Mississippi, Stokely Carmichael declared, "I'm not going to beg the white man for anything that I deserve, I'm going to take it. We need power."
Several people engaging in the Black Power movement started to gain more of a sense in black pride and identity as well. In gaining more of a sense of a cultural identity, several blacks demanded that whites no longer refer to them as "Negroes" but as "Afro-Americans." Up until the mid-1960s, blacks had dressed similarly to whites and straightened their hair. As a part of gaining a unique identity, blacks started to wear loosely fit dashikis and had started to grow their hair out as a natural afro. The afro, sometimes nicknamed the "'fro," remained a popular black hairstyle until the late 1970s.
Black Power was made most public, however, by the Black Panther Party, which was founded by Huey Newton and Bobby Seale in Oakland, California, in 1966. This group followed the ideology of Malcolm X, a former member of the Nation of Islam, using a "by-any-means necessary" approach to stopping inequality. They sought to rid African American neighborhoods of police brutality and created a ten-point plan amongst other things.
Their dress code consisted of black leather jackets, berets, slacks, and light blue shirts. They wore an afro hairstyle. They are best remembered for setting up free breakfast programs, referring to police officers as "pigs", displaying shotguns and a raised fist, and often using the statement of "Power to the people".
Black Power was taken to another level inside prison walls. In 1966, George Jackson formed the Black Guerrilla Family in the California San Quentin State Prison. The goal of this group was to overthrow the white-run government in America and the prison system. In 1970, this group displayed their dedication after a white prison guard was found not guilty of shooting and killing three black prisoners from the prison tower. They retaliated by killing a white prison guard.
Numerous popular cultural expressions associated with black power appeared at this time. Released in August 1968, the number one Rhythm & Blues single for the Billboard Year-End list was James Brown's "Say It Loud – I'm Black and I'm Proud". In October 1968, Tommie Smith and John Carlos, while being awarded the gold and bronze medals, respectively, at the 1968 Summer Olympics, donned human rights badges and each raised a black-gloved Black Power salute during their podium ceremony.
King was not comfortable with the "Black Power" slogan, which sounded too much like black nationalism to him. When King was murdered in 1968, Stokely Carmichael stated that whites murdered the one person who would prevent rampant rioting and that blacks would burn every major city to the ground.
Prison reform.
"Gates v. Collier".
Conditions at the Mississippi State Penitentiary at Parchman, then known as Parchman Farm, became part of the public discussion of civil rights after activists were imprisoned there. In the spring of 1961, Freedom Riders came to the South to test the desegregation of public facilities. By the end of June 1963, Freedom Riders had been convicted in Jackson, Mississippi. Many were jailed in Mississippi State Penitentiary at Parchman. Mississippi employed the trusty system, a hierarchical order of inmates that used some inmates to control and enforce punishment of other inmates.
In 1970 the civil rights lawyer Roy Haber began taking statements from inmates. He collected 50 pages of details of murders, rapes, beatings and other abuses suffered by the inmates from 1969 to 1971 at Mississippi State Penitentiary. In a landmark case known as "Gates v. Collier" (1972), four inmates represented by Haber sued the superintendent of Parchman Farm for violating their rights under the United States Constitution.
Federal Judge William C. Keady found in favor of the inmates, writing that Parchman Farm violated the civil rights of the inmates by inflicting cruel and unusual punishment. He ordered an immediate end to all unconstitutional conditions and practices. Racial segregation of inmates was abolished. And the trustee system, which allow certain inmates to have power and control over others, was also abolished.
The prison was renovated in 1972 after the scathing ruling by Judge Keady; he wrote that the prison was an affront to "modern standards of decency." Among other reforms, the accommodations were made fit for human habitation. The system of "trusties" was abolished. (The prison had armed lifers with rifles and given them authority to oversee and guard other inmates, which led to many abuses and murders.)
In integrated correctional facilities in northern and western states, blacks represented a disproportionate number of the prisoners, in excess of their proportion of the general population. They were often treated as second-class citizens by white correctional officers. Blacks also represented a disproportionately high number of death row inmates. Eldridge Cleaver's book "Soul on Ice" was written from his experiences in the California correctional system; it contributed to black militancy.
Cold War.
There was an international context for the actions of the U.S. Federal government during these years. It had stature to maintain in Europe and a need to appeal to the people in the Third World.
In "Cold War Civil Rights: Race and the Image of American Democracy", the historian Mary L. Dudziak wrote that Communists critical of the United States accused the nation for its hypocrisy in portraying itself as the "leader of the free world," when so many of its citizens were subjected to severe racial discrimination and violence. She argued that this was a major factor in the government moving to support civil rights legislation.
See also.
History preservation:
Post-Civil Rights Movement:

</doc>
<doc id="49005" url="http://en.wikipedia.org/wiki?curid=49005" title="Atlantic slave trade">
Atlantic slave trade

The Atlantic slave trade or transatlantic slave trade took place across the Atlantic Ocean from the 16th through to the 19th centuries. The vast majority of those enslaved that were transported to the New World, many on the triangular trade route and its Middle Passage, were West Africans from the central and western parts of the continent sold by western Africans to western European slave traders, or by direct European capture to the Americas. The numbers were so great that Africans who came by way of the slave trade became the most numerous Old World immigrants in both North and South America before the late 18th century. Far more slaves were taken to South America than to the north. The South Atlantic economic system centered on producing commodity crops, and making goods and clothing to sell in Europe, and increasing the numbers of African slaves brought to the New World. This was crucial to those western European countries which, in the late 17th and 18th centuries, were vying with each other to create overseas empires.
The Portuguese were the first to engage in the New World slave trade in the 16th century, and others soon followed. Ship owners considered the slaves as cargo to be transported to the Americas as quickly and cheaply as possible, there to be sold to labour in coffee, tobacco, cocoa, sugar and cotton plantations, gold and silver mines, rice fields, construction industry, cutting timber for ships, in skilled labour, and as domestic servants. The first Africans imported to the English colonies were classified as "indentured servants", like workers coming from England, and also as "apprentices for life". By the middle of the 17th century, slavery had hardened as a racial caste; they and their offspring were legally the property of their owners, and children born to slave mothers were slaves. As property, the people were considered merchandise or units of labour, and were sold at markets with other goods and services.
The Atlantic slave traders, ordered by trade volume, were: the Portuguese, the British, the French, the Spanish, and the Dutch Empire. Several had established outposts on the African coast where they purchased slaves from local African leaders. These slaves were managed by a factor who was established on or near the coast to expedite the shipping of slaves to the New World. These slaves were kept in a factory while awaiting shipment. Current estimates are that about 12 million Africans were shipped across the Atlantic, although the number purchased by the traders is considerably higher.
The slave trade is sometimes called the "Maafa" by African and African-American scholars, meaning "great disaster" in Swahili. Some scholars, such as Marimba Ani and Maulana Karenga, use the terms "African Holocaust" or "Holocaust of Enslavement".
Background.
Atlantic travel.
The Atlantic slave trade arose after trade contacts were first made between the continents of the "Old World" (Europe, Africa, and Asia) and those of the "New World" (North America and South America). For centuries, tidal currents had made ocean travel particularly difficult and risky for the ships that were then available, and as such there had been very little, if any, naval contact between the peoples living in these continents. In the 15th century, however, new European developments in seafaring technologies meant that ships were better equipped to deal with the problem of tidal currents, and could begin traversing the Atlantic Ocean. Between 1600 and 1800, approximately 300,000 sailors engaged in the slave trade visited West Africa. In doing so, they came into contact with societies living along the west African coast and in the Americas which they had never previously encountered. Historian Pierre Chaunu termed the consequences of European navigation "disenclavement", with it marking an end of isolation for some societies and an increase in inter-societal contact for most others.
Historian John Thornton noted, "A number of technical and geographical factors combined to make Europeans the most likely people to explore the Atlantic and develop its commerce". He identified these as being the drive to find new and profitable commercial opportunities outside Europe as well as the desire to create an alternative trade network to that controlled by the Muslim Empire of the Middle East, which was viewed as a commercial, political and religious threat to European Christendom. In particular, European traders wanted to trade for gold, which could be found in western Africa, and also to find a naval route to "the Indies" (India), where they could trade for luxury goods such as spices without having to obtain these items from Middle Eastern Islamic traders.
Although the initial Atlantic naval explorations were performed purely by Europeans, members of many European nationalities were involved, including sailors from Portugal, Spain, the Italian kingdoms, England, France and the Netherlands. This diversity led Thornton to describe the initial "exploration of the Atlantic" as "a truly international exercise, even if many of the dramatic discoveries [such as those by Christopher Columbus and Ferdinand Magellan] were made under the sponsorship of the Iberian monarchs." That leadership later gave rise to the myth that "the Iberians were the sole leaders of the exploration".
African slavery.
Slavery was practiced in some parts of Africa, Europe, Asia and the Americas for many centuries before the beginning of the Atlantic slave trade. There is evidence that enslaved people from some African states were exported to other states in Africa, Europe and Asia prior to the European colonization of the Americas. The African slave trade provided a large number of slaves to Europeans and many more to people in Muslim countries.
The Atlantic slave trade was not the only slave trade from Africa, although it was the largest in volume and intensity. As Elikia M’bokolo wrote in "Le Monde diplomatique":
The African continent was bled of its human resources via all possible routes. Across the Sahara, through the Red Sea, from the Indian Ocean ports and across the Atlantic. At least ten centuries of slavery for the benefit of the Muslim countries (from the ninth to the nineteenth)... Four million enslaved people exported via the Red Sea, another four million through the Swahili ports of the Indian Ocean, perhaps as many as nine million along the trans-Saharan caravan route, and eleven to twenty million (depending on the author) across the Atlantic Ocean.
According to John K. Thornton, Europeans usually bought enslaved people who were captured in endemic warfare between African states. Some Africans had made a business out of capturing Africans from neighboring ethnic groups or war captives and selling them. A reminder of this practice is documented in the Slave Trade Debates of England in the early 19th century: "All the old writers... concur in stating not only that wars are entered into for the sole purpose of making slaves, but that they are fomented by Europeans, with a view to that object." People living around the Niger River were transported from these markets to the coast and sold at European trading ports in exchange for muskets and manufactured goods such as cloth or alcohol. However, the European demand for slaves provided a large new market for the already existing trade. While those held in slavery in their own region of Africa might hope to escape, those shipped away had little chance of returning to Africa.
European colonization and slavery in West Africa.
Upon discovering new lands through their naval explorations, European colonisers soon began to migrate to and settle in lands outside their native continent. Off the coast of Africa, European migrants, under the directions of the Kingdom of Castile, invaded and colonised the Canary Islands during the 15th century, where they converted much of the land to the production of wine and sugar. Along with this, they also captured native Canary Islanders, the Guanches, to use as slaves both on the Islands and across the Christian Mediterranean.
As historian John Thornton remarked, "the actual motivation for European expansion and for navigational breakthroughs was little more than to exploit the opportunity for immediate profits made by raiding and the seizure or purchase of trade commodities". Using the Canary Islands as a naval base, European, at the time primarily Portuguese traders, began to move their activities down the western coast of Africa, performing raids in which slaves would be captured to be later sold in the Mediterranean. Although initially successful in this venture, "it was not long before African naval forces were alerted to the new dangers, and the Portuguese [raiding] ships began to meet strong and effective resistance", with the crews of several of them being killed by African sailors, whose boats were better equipped at traversing the west African coasts and river systems.
By 1494, the Portuguese king had entered agreements with the rulers of several West African states that would allow trade between their respective peoples, enabling the Portuguese to "tap into" the "well-developed commercial economy in Africa... without engaging in hostilities". "Peaceful trade became the rule all along the African coast", although there were some rare exceptions when acts of aggression led to violence. For instance Portuguese traders attempted to conquer the Bissagos Islands in 1535. In 1571 Portugal, supported by the Kingdom of Kongo, took control of the south-western region of Angola in order to secure its threatened economic interest in the area. Although Kongo later joined a coalition in 1591 to force the Portuguese out, Portugal had secured a foothold on the continent that it continued to occupy until the 20th century. Despite these incidences of occasional violence between African and European forces, many African states ensured that any trade went on in their own terms, for instance, imposing custom duties on foreign ships. In 1525, the Kongolese king, Afonso I, seized a French vessel and its crew for illegally trading on his coast.
Historians have widely debated the nature of the relationship between these African kingdoms and the European traders. The Guyanese historian Walter Rodney (1972) has argued that it was an unequal relationship, with Africans being forced into a "colonial" trade with the more economically developed Europeans, exchanging raw materials and human resources (i.e. slaves) for manufactured goods. He argued that it was this economic trade agreement dating back to the 16th century that led to Africa being underdeveloped in his own time. These ideas were supported by other historians, including Ralph Austen (1987). This idea of an unequal relationship was contested by John Thornton (1998), who argued that "the Atlantic slave trade was not nearly as critical to the African economy as these scholars believed" and that "African manufacturing [at this period] was more than capable of handling competition from preindustrial Europe". However, Anne Bailey, commenting on Thornton's suggestion that Africans and Europeans were equal partners in the Atlantic slave trade, wrote:
"To see Africans as partners implies equal terms and equal influence on the global and intercontinental processes of the trade. Africans had great influence on the continent itself, but they had no direct influence on the engines behind the trade in the capital firms, the shipping and insurance companies of Europe and America, or the plantation systems in Americas. They did not wield any influence on the building manufacturing centers of the West."
16th, 17th and 18th centuries.
The Atlantic slave trade is customarily divided into two eras, known as the First and Second Atlantic Systems.
The First Atlantic system was the trade of enslaved Africans to, primarily, South American colonies of the Portuguese and Spanish empires; it accounted for slightly more than 3% of all Atlantic slave trade. It started (on a significant scale) in about 1502 and lasted until 1580 when Portugal was temporarily united with Spain. While the Portuguese were directly involved in trading enslaved peoples, the Spanish empire relied on the asiento system, awarding merchants (mostly from other countries) the license to trade enslaved people to their colonies. During the first Atlantic system most of these traders were Portuguese, giving them a near-monopoly during the era. Some Dutch, English, and French traders also participated in the slave trade. After the union, Portugal came under Spanish legislation that prohibited it from directly engaging in the slave trade as a carrier. It became a target for the traditional enemies of Spain, losing a large share of the trade to the Dutch, English and French.
The Second Atlantic system was the trade of enslaved Africans by mostly English, Portuguese, French and Dutch traders. The main destinations of this phase were the Caribbean colonies and Brazil, as European nations built up economically slave-dependent colonies in the New World. Slightly more than 3% of the enslaved people exported from Africa were traded between 1450 and 1600, and 16% in the 17th century.
It is estimated that more than half of the entire slave trade took place during the 18th century, with the British, Portuguese and French being the main carriers of nine out of ten slaves abducted from Africa. By the 1690s, the English were shipping the most slaves from West Africa. They maintained this position during the 18th century, becoming the biggest shippers of slaves across the Atlantic.
Following the British and United States' bans on the African slave trade in 1808, it declined, but the period still accounted for 28.5% of the total volume of the Atlantic slave trade.
European colonists initially practiced systems of both bonded labour and "Indian" slavery, enslaving many of the natives of the New World. For a variety of reasons, Africans replaced Native Americans as the main population of enslaved people in the Americas. In some cases, such as on some of the Caribbean Islands, diseases such as smallpox and warfare eliminated the natives completely. In other cases, such as in South Carolina, Virginia, and New England, colonists found they needed alliances with native tribes; together with the availability of enslaved Africans at affordable prices (beginning in the early 18th century for these colonies), they banned Native American slavery.
A burial ground in Campeche, Mexico, suggests slaves had been brought there not long after Hernán Cortés completed the subjugation of Aztec and Mayan Mexico in the 16th century. The graveyard had been in use from approximately 1550 to the late 17th century.
Triangular trade.
The first side of the triangle was the export of goods from Europe to Africa. A number of African kings and merchants took part in the trading of enslaved people from 1440 to about 1833. For each captive, the African rulers would receive a variety of goods from Europe. These included guns, ammunition and other factory made goods. The second leg of the triangle exported enslaved Africans across the Atlantic Ocean to the Americas and the Caribbean Islands. The third and final part of the triangle was the return of goods to Europe from the Americas. The goods were the products of slave-labour plantations and included cotton, sugar, tobacco, molasses and rum. Sir John Hawkins, considered the pioneer of the British slave trade, was the first to run the Triangular trade, making a profit at every stop.
Brazil (the main importer of slaves) manufactured these goods in South America and directly traded with African ports, thus not taking part in a triangular trade.
Labour and slavery.
The Atlantic Slave Trade was the result of, among other things, labour shortage, itself in turn created by the desire of European colonists to exploit New World land and resources for capital profits. Native peoples were at first utilized as slave labour by Europeans, until a large number died from overwork and Old World diseases. Alternative sources of labour, such as indentured servitude, failed to provide a sufficient workforce.
Many crops could not be sold for profit, or even grown, in Europe. Exporting crops and goods from the New World to Europe often proved to be more profitable than producing them on the European mainland. A vast amount of labour was needed to create and sustain plantations that required intensive labour to grow, harvest, and process prized tropical crops. Western Africa (part of which became known as "the Slave Coast"), and later Central Africa, became the source for enslaved people to meet the demand for labour.
The basic reason for the constant shortage of labour was that, with large amounts of cheap land available and lots of landowners searching for workers, free European immigrants were able to become landowners themselves after a relatively short time, thus increasing the need for workers.
Thomas Jefferson attributed the use of slave labour in part to the climate, and the consequent idle leisure afforded by slave labour: "For in a warm climate, no man will labour for himself who can make another labour for him. This is so true, that of the proprietors of slaves a very small proportion indeed are ever seen to labour."
African participation in the slave trade.
Africans played a direct role in the slave trade, selling their captives or prisoners of war to European buyers. The prisoners and captives who were sold were usually from neighbouring or enemy ethnic groups. These captive slaves were considered "other", not part of the people of the ethnic group or "tribe" ; African kings held no particular loyalty to them. Sometimes criminals would be sold so that they could no longer commit crimes in that area. Most other slaves were obtained from kidnappings, or through raids that occurred at gunpoint through joint ventures with the Europeans. But some African kings refused to sell any of their captives or criminals. King Jaja of Opobo, a former slave, refused to do business with the slavers completely. However, Shahadah notes that with the rise of a large commercial slave trade driven by European needs, enslaving enemies became less a consequence of war, and more and more a reason to go to war.
European participation in the slave trade.
Although Europeans were the market for slaves, Europeans rarely entered the interior of Africa, due to fear of disease and fierce African resistance. The enslaved people would be brought to coastal outposts where they would be traded for goods. Enslavement became a major by-product of internal wars in Africa as nation states expanded through military conflicts, in many cases through deliberate sponsorship of benefiting Western European nations. During such periods of rapid state formation or expansion (Asante and Dahomey being good examples), slavery formed an important element of political life which the Europeans exploited: as Queen Sara's plea to the Portuguese courts revealed, the system became "sell to the Europeans or be sold to the Europeans". In Africa, convicted criminals could be punished by enslavement, a punishment which became more prevalent as slavery became more lucrative. Since most of these nations did not have a prison system, convicts were often sold or used in the scattered local domestic slave market.
As of 1778, Thomas Kitchin estimated that Europeans were bringing an estimated 52,000 slaves to the Caribbean yearly, with the French bringing the most Africans to the French West Indies (13,000 out of the yearly estimate). The Atlantic slave trade peaked in the last two decades of the 18th century, during and following the Kongo Civil War. Wars among tiny states along the Niger River's Igbo-inhabited region and the accompanying banditry also spiked in this period. Another reason for surplus supply of enslaved people was major warfare conducted by expanding states, such as the kingdom of Dahomey, the Oyo Empire, and the Asante Empire.
Slavery in Africa and the New World contrasted.
Forms of slavery varied both in Africa and in the New World. In general, slavery in Africa was not heritable – that is, the children of slaves were free – while in the Americas, children of slave mothers were considered born into slavery. This was connected to another distinction: slavery in West Africa was not reserved for racial or religious minorities, as it was in European colonies, although the case was otherwise in places such as Somalia, where Bantus were taken as slaves for the ethnic Somalis.
The treatment of slaves in Africa was more variable than in the Americas. At one extreme, the kings of Dahomey routinely slaughtered slaves in hundreds or thousands in sacrificial rituals, and slaves as human sacrifices was also known in Cameroon. On the other hand, slaves in other places were often treated as part of the family, "adopted children," with significant rights including the right to marry without their masters' permission. Scottish explorer Mungo Park wrote:
"The slaves in Africa, I suppose, are nearly in the proportion of three to one to the freemen. They claim no reward for their services except food and clothing, and are treated with kindness or severity, according to the good or bad disposition of their masters... The slaves which are thus brought from the interior may be divided into two distinct classes – first, such as were slaves from their birth, having been born of enslaved mothers; secondly, such as were born free, but who afterwards, by whatever means, became slaves. Those of the first description are by far the most numerous..." In the Americas, slaves were denied the right to marry freely and masters did not generally accept them as equal members of the family. While slaves convicted of revolt or murder were executed, New World colonists did not submit slaves to arbitrary ritual sacrifice. New World slaves were useful and expensive enough to maintain and care for, but still the property of their owners.
Slave market regions and participation.
There were eight principal areas used by Europeans to buy and ship slaves to the Western Hemisphere. The number of enslaved people sold to the New World varied throughout the slave trade. As for the distribution of slaves from regions of activity, certain areas produced far more enslaved people than others. Between 1650 and 1900, 10.24 million enslaved Africans arrived in the Americas from the following regions in the following proportions:
African kingdoms of the era.
There were over 173 city-states and kingdoms in the African regions affected by the slave trade between 1502 and 1853, when Brazil became the last Atlantic import nation to outlaw the slave trade. Of those 173, no fewer than 68 could be deemed nation states with political and military infrastructures that enabled them to dominate their neighbours. Nearly every present-day nation had a pre-colonial predecessor, sometimes an African Empire with which European traders had to barter.
Ethnic groups.
The different ethnic groups brought to the Americas closely corresponds to the regions of heaviest activity in the slave trade. Over 45 distinct ethnic groups were taken to the Americas during the trade. Of the 45, the ten most prominent, according to slave documentation of the era are listed below.
Human toll.
The transatlantic slave trade resulted in a vast and as yet still unknown loss of life for African captives both in and outside America. Approximately 1.2 – 2.4 million Africans died during their transport to the New World. More died soon upon their arrival. The number of lives lost in the procurement of slaves remains a mystery but may equal or exceed the number who survived to be enslaved.
The savage nature of the trade led to the destruction of individuals and cultures. The following figures do not include deaths of enslaved Africans as a result of their labour, slave revolts, or diseases suffered while living among New World populations.
Historian Ana Lucia Araujo has noted that the process of enslavement did not end with arrival on the American shores; the different paths taken by the individuals and groups who were victims of the Atlantic slave trade were influenced by different factors—including the disembarking region, the kind of work performed, gender, age, religion, and language.
A database compiled in the late 1990s put the figure for the transatlantic slave trade at more than 11 million people. For a long time, an accepted figure was 15 million, although this has in recent years been revised down. Estimates by Patrick Manning are that about 12 million slaves entered the Atlantic trade between the 16th and 19th century, but about 1.5 million died on board ship. About 10.5 million slaves arrived in the Americas. Besides the slaves who died on the Middle Passage, more Africans likely died during the slave raids in Africa and forced marches to ports. Manning estimates that 4 million died inside Africa after capture, and many more died young. Manning's estimate covers the 12 million who were originally destined for the Atlantic, as well as the 6 million destined for Asian slave markets and the 8 million destined for African markets.
African conflicts.
According to Dr. Kimani Nehusi, the presence of European slavers affected the way in which the legal code in African societies responded to offenders. Crimes traditionally punishable by some other form of punishment became punishable by enslavement and sale to slave traders. According to David Stannard's "American Holocaust", 50% of African deaths occurred in Africa as a result of wars between native kingdoms, which produced the majority of slaves. This includes not only those who died in battles, but also those who died as a result of forced marches from inland areas to slave ports on the various coasts. The practice of enslaving enemy combatants and their villages was widespread throughout Western and West Central Africa, although wars were rarely started to procure slaves. The slave trade was largely a by-product of tribal and state warfare as a way of removing potential dissidents after victory, or financing future wars. However, some African groups proved particularly adept and brutal at the practice of enslaving, such as Oyo, Benin, Igala, Kaabu, Asanteman, Dahomey, the Aro Confederacy and the Imbangala war bands.
In letters written by the Manikongo, Nzinga Mbemba Afonso, to the King João III of Portugal, he writes that Portuguese merchandise flowing in is what is fueling the trade in Africans. He requests the King of Portugal to stop sending merchandise but should only send missionaries. In one of his letters he writes:
Before the arrival of the Portuguese, slavery had already existed in Kongo. Afonso believed that the slave trade should be subject to Kongo law. When he suspected the Portuguese of receiving illegally enslaved persons to sell, he wrote to King João III in 1526 imploring him to put a stop to the practice.
The kings of Dahomey sold war captives into transatlantic slavery; they would otherwise have been killed in a ceremony known as the Annual Customs. As one of West Africa's principal slave states, Dahomey became extremely unpopular with neighbouring peoples. Like the Bambara Empire to the east, the Khasso kingdoms depended heavily on the slave trade for their economy. A family's status was indicated by the number of slaves it owned, leading to wars for the sole purpose of taking more captives. This trade led the Khasso into increasing contact with the European settlements of Africa's west coast, particularly the French. Benin grew increasingly rich during the 16th and 17th centuries on the slave trade with Europe; slaves from enemy states of the interior were sold, and carried to the Americas in Dutch and Portuguese ships. The Bight of Benin's shore soon came to be known as the "Slave Coast".
King Gezo of Dahomey said in the 1840s:
In 1807, the UK Parliament passed the Bill that abolished the trading of slaves. The King of Bonny (now in Nigeria) was horrified at the conclusion of the practice:
Port factories.
After being marched to the coast for sale, enslaved people waited in large forts called factories. The amount of time in factories varied, but Milton Meltzer's "Slavery: A World History" states this period resulted in or around 4.5% of deaths during the transatlantic slave trade. In other words, over 820,000 people would have died in African ports such as Benguela, Elmina and Bonny, reducing the number of those shipped to 17.5 million.
Atlantic shipment.
After being captured and held in the factories, slaves entered the infamous Middle Passage. Meltzer's research puts this phase of the slave trade's overall mortality at 12.5%. Around 2.2 million Africans died during these voyages where they were packed into tight, unsanitary spaces on ships for months at a time. Measures were taken to stem the onboard mortality rate, such as enforced "dancing" (as exercise) above deck and the practice of force-feeding enslaved persons who tried to starve themselves. The conditions on board also resulted in the spread of fatal diseases. Other fatalities were suicides, slaves who escaped by jumping overboard. The slave traders would try to fit anywhere from 350 to 600 slaves on one ship. Before the African slave trade was completely banned by participating nations in 1853, 15.3 million enslaved people had arrived in the Americas.
Raymond L. Cohn, an economics professor whose research has focused on economic history and international migration, has researched the mortality rates among Africans during the voyages of the Atlantic slave trade. He found that mortality rates decreased over the history of the slave trade, primarily because the length of time necessary for the voyage was declining. "In the eighteenth century many slave voyages took at least 2½ months. In the nineteenth century, 2 months appears to have been the maximum length of the voyage, and many voyages were far shorter. Fewer slaves died in the Middle Passage over time mainly because the passage was shorter."
Seasoning camps.
Meltzer also states that 33% of Africans would have died in the first year at the seasoning camps found throughout the Caribbean. Many slaves shipped directly to North America bypassed this process; however, most slaves (destined for island or South American plantations) were likely to be put through this ordeal. The enslaved people were tortured for the purpose of "breaking" them and conditioning them to their new lot in life. Jamaica held one of the most notorious of these camps. Dysentery was the leading cause of death. All in all, 5 million Africans died in these camps, reducing the number of survivors to about 10 million.
European competition.
The trade of enslaved Africans in the Atlantic has its origins in the explorations of Portuguese mariners down the coast of West Africa in the 15th century. Before that, contact with African slave markets was made to ransom Portuguese who had been captured by the intense North African Barbary pirate attacks on Portuguese ships and coastal villages, frequently leaving them depopulated. The first Europeans to use enslaved Africans in the New World were the Spaniards, who sought auxiliaries for their conquest expeditions and labourers on islands such as Cuba and Hispaniola. The alarming decline in the native population had spurred the first royal laws protecting them (Laws of Burgos, 1512–13). The first enslaved Africans arrived in Hispaniola in 1501. After Portugal had succeeded in establishing sugar plantations ("engenhos") in northern Brazil ca. 1545, Portuguese merchants on the West African coast began to supply enslaved Africans to the sugar planters. While at first these planters had relied almost exclusively on the native Tupani for slave labour, after 1570 they began importing Africans, as a series of epidemics had decimated the already destabilized Tupani communities. By 1630, Africans had replaced the Tupani as the largest contingent of labour on Brazilian sugar plantations. This ended the European medieval household tradition of slavery, resulted in Brazil's receiving the most enslaved Africans, and revealed sugar cultivation and processing as the reason that roughly 84% of these Africans were shipped to the New World.
As Britain rose in naval power and settled continental North America and some islands of the West Indies, they became the leading slave traders. At one stage the trade was the monopoly of the Royal Africa Company, operating out of London. But, following the loss of the company's monopoly in 1689, Bristol and Liverpool merchants became increasingly involved in the trade. By the late 17th century, one out of every four ships that left Liverpool harbour was a slave trading ship. Much of the wealth on which the city of Manchester, and surrounding towns, was built in the late 18th century, and for much of the 19th century, was based on the processing of slave-picked cotton and manufacture of cloth. Other British cities also profited from the slave trade. Birmingham, the largest gun-producing town in Britain at the time, supplied guns to be traded for slaves. 75% of all sugar produced in the plantations was sent to London, and much of it was consumed in the highly lucrative coffee houses there.
New World destinations.
The first slaves to arrive as part of a labour force in the New World reached the island of Hispaniola (now Haiti and the Dominican Republic) in 1502. Cuba received its first four slaves in 1513. Jamaica received its first shipment of 4000 slaves in 1518. Slave exports to Honduras and Guatemala started in 1526.
The first enslaved Africans to reach what would become the United States arrived in January 1526 as part of a Spanish attempt to colonize South Carolina near Jamestown. By November the 300 Spanish colonists were reduced to 100, and their slaves from 100 to 70. The enslaved people revolted and joined a nearby Native American tribe, while the Spanish abandoned the colony altogether. Colombia received its first enslaved people in 1533. El Salvador, Costa Rica and Florida began their stints in the slave trade in 1541, 1563 and 1581, respectively.
The 17th century saw an increase in shipments, with Africans arriving in the English colony of Jamestown, Virginia in 1619. These first kidnapped Africans were classed as indentured servants and freed after seven years. Chattel slavery was codified in Virginia law in 1656, and in 1662, the colony adopted the principle of "partus sequitur ventrem", by which children of slave mothers were slaves, regardless of paternity. Irish immigrants took slaves to Montserrat in 1651, and in 1655, slaves were shipped to Belize.
By 1802 Russian colonists noted that "Boston" (U.S.-based) skippers were trading African slaves for otter pelts with the Tlingit people in Southeast Alaska.
The number of the Africans arrived in each area can be calculated taking into consideration that the total number of slaves was close to 10,000,000.
Economics of slavery.
The plantation economies of the New World were built on slave labour. Seventy percent of the enslaved people brought to the new world were forced to produce sugar, the most labour-intensive crop. The rest were employed harvesting coffee, cotton, and tobacco, and in some cases in mining. The West Indian colonies of the European powers were some of their most important possessions, so they went to extremes to protect and retain them. For example, at the end of the Seven Years' War in 1763, France agreed to cede the vast territory of New France (now Eastern Canada) to the victors in exchange for keeping the minute Antillean island of Guadeloupe.
In France in the 18th century, returns for investors in plantations averaged around 6%; as compared to 5% for most domestic alternatives, this represented a 20% profit advantage. Risks—maritime and commercial—were important for individual voyages. Investors mitigated it by buying small shares of many ships at the same time. In that way, they were able to diversify a large part of the risk away. Between voyages, ship shares could be freely sold and bought.
By far the most financially profitable West Indian colonies in 1800 belonged to the United Kingdom. After entering the sugar colony business late, British naval supremacy and control over key islands such as Jamaica, Trinidad, the Leeward Islands and Barbados and the territory of British Guiana gave it an important edge over all competitors; while many British did not make gains, a handful of individuals made small fortunes. This advantage was reinforced when France lost its most important colony, St. Domingue (western Hispaniola, now Haiti), to a slave revolt in 1791 and supported revolts against its rival Britain, after the 1793 French revolution in the name of liberty. Before 1791, British sugar had to be protected to compete against cheaper French sugar.
After 1791, the British islands produced the most sugar, and the British people quickly became the largest consumers. West Indian sugar became ubiquitous as an additive to Indian tea. It has been estimated that the profits of the slave trade and of West Indian plantations created up to one-in-twenty of every pound circulating in the British economy at the time of the Industrial Revolution in the latter half of the 18th century.
Effects.
Historian Walter Rodney has argued that at the start of the slave trade in the 16th century, although there was a technological gap between Europe and Africa, it was not very substantial. Both continents were using Iron Age technology. The major advantage that Europe had was in ship building. During the period of slavery, the populations of Europe and the Americas grew exponentially, while the population of Africa remained stagnant. Rodney contended that the profits from slavery were used to fund economic growth and technological advancement in Europe and the Americas. Based on earlier theories by Eric Williams, he asserted that the industrial revolution was at least in part funded by agricultural profits from the Americas. He cited examples such as the invention of the steam engine by James Watt, which was funded by plantation owners from the Caribbean.
Other historians have attacked both Rodney's methodology and accuracy. Joseph C. Miller has argued that the social change and demographic stagnation (which he researched on the example of West Central Africa) was caused primarily by domestic factors. Joseph Inikori provided a new line of argument, estimating counterfactual demographic developments in case the Atlantic slave trade had not existed. Patrick Manning has shown that the slave trade did have profound impact on African demographics and social institutions, but criticized Inikori's approach for not taking other factors (such as famine and drought) into account, and thus being highly speculative.
Effect on the economy of West Africa.
No scholars dispute the harm done to the enslaved people but the effect of the trade on African societies is much debated, due to the apparent influx of goods to Africans. Proponents of the slave trade, such as Archibald Dalzel, argued that African societies were robust and not much affected by the trade. In the 19th century, European abolitionists, most prominently Dr. David Livingstone, took the opposite view, arguing that the fragile local economy and societies were being severely harmed by the trade.
Because the negative effects of slavery on the economies of Africa have been well documented, namely the significant decline in population, some African rulers likely saw an economic benefit from trading their subjects with European slave traders. With the exception of Portuguese controlled Angola, coastal African leaders "generally controlled access to their coasts, and were able to prevent direct enslavement of their subjects and citizens." Thus, as African scholar John Thornton argues, African leaders who allowed the continuation of the slave trade likely derived an economic benefit from selling their subjects to Europeans. The Kingdom of Benin, for instance, participated in the African slave trade, at will, from 1715 to 1735, surprising Dutch traders, who had not expected to buy slaves in Benin. The benefit derived from trading slaves for European goods was enough to make the Kingdom of Benin rejoin the trans-Atlantic slave trade after centuries of non-participation. Such benefits included military technology (specifically guns and gunpowder), gold, or simply maintaining amicable trade relationships with European nations. The slave trade was therefore a means for some African elite to gain economic advantages. Historian Walter Rodney estimates that by c.1770, the King of Dahomey was earning an estimated £250,000 per year by selling captive African soldiers and enslaved people to the European slave-traders.
Both Thornton and Fage contend that while African political elite may have ultimately benefited from the slave trade, their decision to participate may have been influenced more by what they could lose by not participating. In Fage's article "Slavery and the Slave Trade in the Context of West African History," he notes that for West Africans "... there were really few effective means of mobilizing labour for the economic and political needs of the state" without the slave trade.
Effects on the British economy.
Historian Eric Williams in 1944 argued that the profits that Britain received from its sugar colonies, or from the slave trade between Africa and the Caribbean, was a major factor in financing Britain's industrial revolution. However, he says that by the time of its abolition in 1833 it had lost its profitability and it was in Britain's economic interest to ban it.
Other researchers and historians have strongly contested what has come to be referred to as the “Williams thesis” in academia. David Richardson has concluded that the profits from the slave trade amounted to less than 1% of domestic investment in Britain. Economic historian Stanley Engerman finds that even without subtracting the associated costs of the slave trade (e.g., shipping costs, slave mortality, mortality of British people in Africa, defense costs) or reinvestment of profits back into the slave trade, the total profits from the slave trade and of West Indian plantations amounted to less than 5% of the British economy during any year of the Industrial Revolution. Engerman’s 5% figure gives as much as possible in terms of benefit of the doubt to the Williams argument, not solely because it does not take into account the associated costs of the slave trade to Britain, but also because it carries the full-employment assumption from economics and holds the gross value of slave trade profits as a direct contribution to Britain’s national income. Historian Richard Pares, in an article written before Williams’ book, dismisses the influence of wealth generated from the West Indian plantations upon the financing of the Industrial Revolution, stating that whatever substantial flow of investment from West Indian profits into industry there was occurred after emancipation, not before.
Seymour Drescher and Robert Anstey argue the slave trade remained profitable until the end, and that moralistic reform, not economic incentive, was primarily responsible for abolition. They say slavery remained profitable in the 1830s because of innovations in agriculture.
Karl Marx in his influential economic history of capitalism "Das Kapital" wrote that "...the turning of Africa into a warren for the commercial hunting of black-skins, signaled the rosy dawn of the era of capitalist production." He argued that the slave trade was part of what he termed the "primitive accumulation" of capital, the 'non-capitalist' accumulation of wealth that preceded and created the financial conditions for Britain's industrialisation.
Demographics.
The demographic effects of the slave trade is a controversial and highly debated issue.
Walter Rodney argued that the export of so many people had been a demographic disaster and had left Africa permanently disadvantaged when compared to other parts of the world, and largely explains the continent's continued poverty. He presented numbers showing that Africa's population stagnated during this period, while that of Europe and Asia grew dramatically. According to Rodney, all other areas of the economy were disrupted by the slave trade as the top merchants abandoned traditional industries to pursue slaving, and the lower levels of the population were disrupted by the slaving itself.
Others have challenged this view. J. D. Fage compared the number effect on the continent as a whole. David Eltis has compared the numbers to the rate of emigration from Europe during this period. In the nineteenth century alone over 50 million people left Europe for the Americas, a far higher rate than were ever taken from Africa.
Other scholars accused Rodney of mischaracterizing the trade between Africans and Europeans. They argue that Africans, or more accurately African elites, deliberately let European traders join in an already large trade in enslaved people and were not patronized.
As Joseph E. Inikori argues, the history of the region shows that the effects were still quite deleterious. He argues that the African economic model of the period was very different from the European, and could not sustain such population losses. Population reductions in certain areas also led to widespread problems. Inikori also notes that after the suppression of the slave trade Africa's population almost immediately began to rapidly increase, even prior to the introduction of modern medicines. Owen Alik Shahadah also states that the trade was not only of demographic significance in aggregate population losses but also in the profound changes to settlement patterns, exposure to epidemics, and reproductive and social development potential.
Legacy of racism.
Professor Maulana Karenga states that the effects of slavery were that "the morally monstrous destruction of human possibility involved redefining African humanity to the world, poisoning past, present and future relations with others who only know us through this stereotyping and thus damaging the truly human relations among peoples." He states that it constituted the destruction of culture, language, religion and human possibility.
Walter Rodney states: "Above all, it was the institution of slavery in the Americas which ultimately conditioned racial attitudes, even when their more immediate derivation was the literature on Africa or contacts within Europe itself. It has been well attested that New World slave-plantation society was the laboratory of modern racism. The owners contempt for and fear of the black slaves was expressed in religious, scientific and philosophical terms, which became the stock attitudes of European and even Africans in subsequent generations. Although there have been contributions to racist philosophy both before and after the slave trade epoch, the historical experience of whites enslaving blacks for four centuries forged the tie between racist and colour prejudice, and produced not merely individual racists but a society where racism was so all-pervasive that it was not even perceived as what it actually was. The very concept of human racial variants was never satisfactorily established in biological terms,and the assumptions of scientists and laymen alike were rooted in the perception of a reality in which Europeans had succeeded in reducing Africans to the level of chattel."
Walter Rodney states, "The role of slavery in promoting racist prejudice and ideology has been carefully studied in certain situations, especially in the U.S.A. The simple fact is that no people can enslave another for four centuries without coming out with a notion of superiority, and when the colour and other physical traits of those peoples were quite different it was inevitable that the prejudice should take a racist form."
Eric Williams argued that, "A racial twist [was] given to what is basically an economic phenomenon. Slavery was not born of racism: rather, racism was the consequence of slavery."
End of the Atlantic slave trade.
In Britain, America, Portugal and in parts of Europe, opposition developed against the slave trade. Davis says that abolitionists assumed "that an end to slave imports would lead automatically to the amelioration and gradual abolition of slavery". Opposition to the trade was led by the Religious Society of Friends (Quakers) and establishment Evangelicals such as William Wilberforce. The movement was joined by many and began to protest against the trade, but they were opposed by the owners of the colonial holdings. Following Lord Mansfield's decision in 1772, slaves became free upon entering the British isles. Under the leadership of Thomas Jefferson, the new state of Virginia in 1778 became the first state and one of the first jurisdictions anywhere to stop the importation of slaves for sale; it made it a crime for traders to bring in slaves from out of state or from overseas for sale; migrants from other states were allowed to bring their own slaves. The new law freed all slaves brought in illegally after its passage and imposed heavy fines on violators. Denmark, which had been active in the slave trade, was the first country to ban the trade through legislation in 1792, which took effect in 1803. Britain banned the slave trade in 1807, imposing stiff fines for any slave found aboard a British ship ("see Slave Trade Act 1807"). The Royal Navy, which then controlled the world's seas, moved to stop other nations from continuing the slave trade and declared that slaving was equal to piracy and was punishable by death. The United States Congress passed the Slave Trade Act of 1794, which prohibited the building or outfitting of ships in the U.S. for use in the slave trade. In 1807 Congress outlawed the importation of slaves beginning on 1 January 1808, the earliest date permitted by the United States Constitution for such a ban.
On Sunday, 28 October 1787, William Wilberforce wrote in his diary: "God Almighty has set before me two great objects, the suppression of the slave trade and the Reformation of society." For the rest of his life, William Wilberforce dedicated his life as a Member of the British Parliament to opposing the slave trade and working for the abolition of slavery throughout the British Empire. On 22 February 1807, twenty years after he first began his crusade, and in the middle of Britain's war with France, Wilberforce and his team's labours were rewarded with victory. By an overwhelming 283 votes for to 16 against, the motion to abolish the Atlantic slave trade was carried in the House of Commons. The United States acted to abolish the slave trade the same year, but not its internal slave trade which became the dominant character in American slavery until the 1860s. In 1805 the British Order-in-Council had restricted the importation of slaves into colonies that had been captured from France and the Netherlands. Britain continued to press other nations to end its trade; in 1810 an Anglo-Portuguese treaty was signed whereby Portugal agreed to restrict its trade into its colonies; an 1813 Anglo-Swedish treaty whereby Sweden outlawed its slave trade; the Treaty of Paris 1814 where France agreed with Britain that the trade is "repugnant to the principles of natural justice" and agreed to abolish the slave trade in five years; the 1814 Anglo-Netherlands treaty where the Dutch outlawed its slave trade.
With peace in Europe from 1815, and British supremacy at sea secured, the Royal Navy turned its attention back to the challenge and established the West Africa Squadron in 1808, known as the "preventative squadron", which for the next 50 years operated against the slavers. By the 1850s, around 25 vessels and 2,000 officers and men were on the station, supported by some ships from the small United States Navy, and nearly 1,000 "Kroomen"—experienced fishermen recruited as sailors from what is now the coast of modern Liberia. Service on the West Africa Squadron was a thankless and overwhelming task, full of risk and posing a constant threat to the health of the crews involved. Contending with pestilential swamps and violent encounters, the mortality rate was 55 per 1,000 men, compared with 10 for fleets in the Mediterranean or in home waters. Between 1807 and 1860, the Royal Navy's Squadron seized approximately 1,600 ships involved in the slave trade and freed 150,000 Africans who were aboard these vessels. Several hundred slaves a year were transported by the navy to the British colony of Sierra Leone, where they were made to serve as "apprentices" in the colonial economy until the Slavery Abolition Act 1833. Action was taken against African leaders who refused to agree to British treaties to outlaw the trade, for example against "the usurping King of Lagos", deposed in 1851. Anti-slavery treaties were signed with over 50 African rulers.
The last recorded slave ship to land on American soil was the Clotilde, which in 1859 illegally smuggled a number of Africans into the town of Mobile, Alabama. The Africans on board were sold as slaves; however, slavery in the U.S. was abolished 5 years later following the end of the American Civil War in 1865. The last survivor of the voyage was Cudjoe Lewis, who died in 1935.
The last country to ban the Atlantic slave trade was Brazil in 1831. However, a vibrant illegal trade continued to ship large numbers of enslaved people to Brazil and also to Cuba until the 1860s, when British enforcement and further diplomacy finally ended the Atlantic trade. In 1870 Portugal ended the last trade route with the Americas where the last country to import slaves was Brazil. In Brazil slavery itself however did not end until 1888, which was the last country in the Americas to end involuntary servitude. 
The historian Walter Rodney contends that it was a decline in the profitability of the triangular trades that made it possible for certain basic human sentiments to be asserted at the decision-making level in a number of European countries- Britain being the most crucial because it was the greatest carrier of African captives across the Atlantic. Rodney states that changes in productivity, technology and patterns of exchange in Europe and the Americas informed the decision by the British to end their participation in the trade in 1807. In 1809 President James Madison outlawed the slave trade with the United States. 
Nevertheless, Michael Hardt and Antonio Negri argue that it was neither a matter of strictly economics nor of morals. Firstly because slavery was (in practice) still beneficial to capitalism, providing not only influx of capital, but also disciplining hardship into workers (a form of ‘apprenticeship’ to the capitalist industrial plant). The more 'recent' argument of a ‘moral shift’ (the basis of the previous lines of this article) is described by Hardt and Negri as an ‘ideological’ apparatus in order to eliminate the sentiment of guilt in western society. Although moral arguments did play a secondary role, it usually had major resonance when used as a strategy to undercut competitors' profits. Eurocentric history has been blind to the most important element in this fight for freedom, precisely, the constant revolt and antagonism of slaves' revolts. The most important of those being the Haitian Revolution. The shock of this revolution in 1804, certainly introduces an essential political argument into the end of slavery trade, which happen only three years later. The spectre of black revolution was haunting Europe's supremacy, and certainly still challenge the ethnocentrism of European ‘universal history’.
It is important to remember that slavery exploitation took many forms (from coolieism in the pacific, the peonage in Latin America, the apartheid in South Africa to the Wage labour in sweet shops). In this sense, there are still many forms of ‘not free’ work in the world.
Legacy.
African diaspora.
The African diaspora which was created via slavery has been a complex interwoven part of American history and culture. In the United States, the success of Alex Haley's book "", published in 1976, and the subsequent television miniseries based upon it "Roots", broadcast on the ABC network in January 1977, led to an increased interest and appreciation of African heritage amongst the African-American community. The influence of these led many African Americans to begin researching their family histories and making visits to West Africa. In turn, a tourist industry grew up to supply them. One notable example of this is through the Roots Homecoming Festival held annually in the Gambia, in which rituals are held through which African Americans can symbolically "come home" to Africa. Issues of dispute have however developed between African Americans and African authorities over how to display historic sites that were involved in the Atlantic slave trade, with prominent voices in the former criticising the latter for not displaying such sites sensitively, but instead treating them as a commercial enterprise.
"Back to Africa".
In 1816, a group of wealthy European-Americans, some of whom were abolitionists and others who were racial segregationists, founded the American Colonization Society with the express desire of returning African Americans who were in the United States to West Africa. In 1820, they sent their first ship to Liberia, and within a decade around two thousand African Americans had been settled in the west African country. Such re-settlement continued throughout the 19th century, increasing following the deterioration of race relations in the southern states of the US following Reconstruction in 1877.
Rastafari movement.
The Rastafari movement, which originated in Jamaica, where 98% of the population are descended from victims of the Atlantic slave trade, has made great efforts to publicize the slavery, and to ensure it is not forgotten, especially through reggae music.
Apologies.
Civil societies.
In 1998, UNESCO designated 23 August as International Day for the Remembrance of the Slave Trade and its Abolition. Since then there have been a number of events recognizing the effects of slavery.
On 9 December 1999 Liverpool City Council passed a formal motion apologizing for the City's part in the slave trade. It was unanimously agreed that Liverpool acknowledges its responsibility for its involvement in three centuries of the slave trade. The City Council has made an unreserved apology for Liverpool's involvement and the continual effect of slavery on Liverpool's Black communities.
Benin.
In 1999, President Mathieu Kerekou of Benin (formerly the Kingdom of Dahomey) issued a national apology for the role Africans played in the Atlantic slave trade. Luc Gnacadja, minister of environment and housing for Benin, later said: "The slave trade is a shame, and we do repent for it." Researchers estimate that 3 million slaves were exported out of the Slave Coast bordering the Bight of Benin.
Ghana.
President Jerry Rawlings of Ghana also apologized for his country's involvement in the slave trade.
World conference against racism.
At the 2001 World Conference Against Racism in Durban, South Africa, African nations demanded a clear apology for slavery from the former slave-trading countries. Some nations were ready to express an apology, but the opposition, mainly from the United Kingdom, Portugal, Spain, the Netherlands, and the United States blocked attempts to do so. A fear of monetary compensation might have been one of the reasons for the opposition. As of 2009, efforts are underway to create a UN Slavery Memorial as a permanent remembrance of the victims of the Atlantic slave trade.
France.
On 30 January 2006, Jacques Chirac (the then French President) said that 10 May would henceforth be a national day of remembrance for the victims of slavery in France, marking the day in 2001 when France passed a law recognising slavery as a crime against humanity.
United Kingdom.
On 27 November 2006, British Prime Minister Tony Blair made a partial apology for Britain's role in the African slavery trade. However African rights activists denounced it as "empty rhetoric" that failed to address the issue properly. They feel his apology stopped shy to prevent any legal retort. Mr Blair again apologized on March 14, 2007.
On 24 August 2007, Ken Livingstone (Mayor of London) apologized publicly for London's role in the slave trade. "You can look across there to see the institutions that still have the benefit of the wealth they created from slavery", he said pointing towards the financial district, before breaking down in tears. He claimed that London was still tainted by the horrors of slavery. Jesse Jackson praised Mayor Livingstone, and added that reparations should be made.
United States of America.
On 24 February 2007 the Virginia General Assembly passed House Joint Resolution Number 728 acknowledging "with profound regret the involuntary servitude of Africans and the exploitation of Native Americans, and call for reconciliation among all Virginians." With the passing of that resolution, Virginia became the first of the 50 United States to acknowledge through the state's governing body their state's involvement in slavery. The passing of this resolution came on the heels of the 400th anniversary celebration of the city of Jamestown, Virginia, which was the first permanent English colony to survive in what would become the United States. Jamestown is also recognized as one of the first slave ports of the American colonies. On 31 May 2007, the Governor of Alabama, Bob Riley, signed a resolution expressing "profound regret" for Alabama's role in slavery and apologizing for slavery's wrongs and lingering effects. Alabama is the fourth Southern state to pass a slavery apology, following votes by the legislatures in Maryland, Virginia, and North Carolina.
On 30 July 2008, the United States House of Representatives passed a resolution apologizing for American slavery and subsequent discriminatory laws. The language included a reference to the "fundamental injustice, cruelty, brutality and inhumanity of slavery and Jim Crow" segregation. On 18 June 2009, the United States Senate issued an apologetic statement decrying the "fundamental injustice, cruelty, brutality, and inhumanity of slavery". The news was welcomed by President Barack Obama.
Uganda.
In 1998, President Yoweri Museveni of Uganda, called tribal chieftains to apologize for their involvement in the slave trade: "African chiefs were the ones waging war on each other and capturing their own people and selling them. If anyone should apologise it should be the African chiefs. We still have those traitors here even today."
Nigeria.
In 2009, the Civil Rights Congress of Nigeria has written an open letter to all African chieftains who participated in trade calling for an apology for their role in the Atlantic slave trade: "We cannot continue to blame the white men, as Africans, particularly the traditional rulers, are not blameless. In view of the fact that the Americans and Europe have accepted the cruelty of their roles and have forcefully apologized, it would be logical, reasonable and humbling if African traditional rulers ... [can] accept blame and formally apologize to the descendants of the victims of their collaborative and exploitative slave trade."
References.
Bibliography.
</dl>
</dl>

</doc>
<doc id="49006" url="http://en.wikipedia.org/wiki?curid=49006" title="Diosdado Cabello">
Diosdado Cabello

Diosdado Cabello Rondón (born April 15, 1963) is a Venezuelan politician, President (Speaker) of the National Assembly of Venezuela and active member of the Venezuelan armed forces. He was involved in Hugo Chávez’s return to power after the 2002 coup d'état. He became a leading member of Chavez’s Movimiento V República (MVR), and remains a leading member of the United Socialist Party of Venezuela into which MVR was merged in 2007. Governor of Miranda from 2004 to 2008, he lost the 2008 election to Henrique Capriles Radonski, and was subsequently appointed Public Works & Housing Minister. In November 2009 he was additionally appointed head of the National Commission of Telecom, a position traditionally independent from Ministry of Public Works and Housing. In 2010, he was elected a member of parliament by his home state of Monagas. In 2011, President Hugo Chávez named him Vice-President of Venezuela’s ruling party, the PSUV. In 2012, he was elected and sworn in as President of the National Assembly of Venezuela, the country’s parliament.
Background.
Diosdado Cabello was born in El Furrial, Monagas State. His background is in engineering. He has an undergraduate degree in systems engineering from the Instituto Universitario Politécnico de las Fuerzas Armadas Nacionales and a graduate degree in engineering project management from the Andrés Bello Catholic University. A former member of the armed forces, he was involved in Chávez’s abortive coup d'état of February 1992, for which he was jailed. President Rafael Caldera pardoned him, like the rest of the coup participants and he was released after only two years without any charges.
Political career.
Following Chávez’s 1998 electoral victory, he helped set up the pro-Chávez grassroots civil society organizations known as "Bolivarian Circles". He was head of telecoms regulator Conatel during the time the market was opened to competition. In May 2001 he became Chavez' chief of staff, and was appointed Vice President by President Hugo Chávez on January 13, 2002, replacing Adina Bastidas. As such, he was responsible to both the president and the National Assembly, and for the relations between the executive and legislative branches of the government.
On April 13, 2002, he took on the duties of the presidency on a temporary basis, replacing Pedro Carmona, head of the Venezuelan Chamber of Commerce, as interim president during the coup d'état attempt when Chávez was kept prisoner and was consequently absent from office. Upon taking office, Cabello said that "I, Diosdado Cabello, am assuming the presidency until such time as the president of the republic, Hugo Chávez Frías, appears." A few hours later, Chávez was back in office. This made Cabello’s presidency the world’s second briefest, after that of Mexican President Pedro Lascuráin.
On April 28, 2002, Cabello was replaced as Vice President by José Vicente Rangel. Cabello was named interior minister in May 2002, and then infrastructure minister in January 2003.
In October 2004, Cabello was elected to a four-year term as Governor of Miranda State. He lost the 2008 election to Henrique Capriles Radonski, and was subsequently appointed Public Works & Housing Minister. In November 2009 he was additionally appointed head of Conatel.
In 2011 Cabello was installed as the Vice-President of the United Socialist Party (PSUV), thus becoming the second most powerful figure in the party after Hugo Chávez.
Cabello was appointed president of the National Assembly in early 2012 and was re-elected to that post in January 2013.
Cabello’s current status after the death of Hugo Chávez is disputed. Some argue that Cabello is constitutionally the acting President while the power remains in the hands of Nicolás Maduro.
His wife, Marlenys Contreras, also serves as a member of the National Assembly. Cabello’s sister, Glenna, is a political scientist and current Counsellor of the Venezuelan Permanent Mission to the United Nations. His brother, José David, previously minister of infrastructure, is in charge of the nation’s taxes as head of SENIAT, Venezuela’s revenue service.
Corruption.
Information presented to the United States State Department by Stratfor claimed that Cabello was "head of one of the major centers of corruption in Venezuela." A Wikileaked U.S. Embassy cable from 2009 characterized Cabello as a “major pole” of corruption within the regime, describing him as “amassing great power and control over the regime’s apparatus as well as a private fortune, often through intimidation behind the scenes.” The communiqué likewise created speculation that “Chavez himself might be concerned about Cabello's growing influence but unable to diminish it.”
He is described by a contributor to "The Atlantic" as the "Frank Underwood" of Venezuela under whose watch the National Assembly of Venezuela has made a habit of ignoring constitutional hurdles entirely—at various times preventing opposition members from speaking in session, suspending their salaries, stripping particularly problematic legislators of parliamentary immunity, and, on one occasion, even presiding over the physical beating of unfriendly lawmakers while the assembly was meeting. 
Allegations of corruption involving Cabello includes being head of an international drug trafficking organization, accepting bribes from Derwick Associates for public works projects in Venezuela, using nepotism to reward friends and family members and directing colectivos while paying them with funds from Petróleos de Venezuela. In 2013, there were at least 17 formal corruption allegations lodged against Cabello in Venezuela's prosecutors office.

</doc>
<doc id="49007" url="http://en.wikipedia.org/wiki?curid=49007" title="Stream cipher">
Stream cipher

A stream cipher is a symmetric key cipher where plaintext digits are combined with a pseudorandom cipher digit stream (keystream). In a stream cipher each plaintext digit is encrypted one at a time with the corresponding digit of the keystream, to give a digit of the ciphertext stream. An alternative name is a state cipher, as the encryption of each digit is dependent on the current state. In practice, a digit is typically a bit and the combining operation an exclusive-or (XOR).
The pseudorandom keystream is typically generated serially from a random seed value using digital shift registers. The seed value serves as the cryptographic key for decrypting the ciphertext stream.
Stream ciphers represent a different approach to symmetric encryption from block ciphers. Block ciphers operate on large blocks of digits with a fixed, unvarying transformation. This distinction is not always clear-cut: in some modes of operation, a block cipher primitive is used in such a way that it acts effectively as a stream cipher. Stream ciphers typically execute at a higher speed than block ciphers and have lower hardware complexity. However, stream ciphers can be susceptible to serious security problems if used incorrectly (see stream cipher attacks); in particular, the same starting state (seed) must never be used twice.
Loose inspiration from the one-time pad.
Stream ciphers can be viewed as approximating the action of a proven unbreakable cipher, the one-time pad (OTP), sometimes known as the Vernam cipher. A one-time pad uses a keystream of completely random digits. The keystream is combined with the plaintext digits one at a time to form the ciphertext. This system was proved to be secure by Claude E. Shannon in 1949. However, the keystream must be generated completely at random with at least the same length as the plaintext and cannot be used more than once. This makes the system very cumbersome to implement in practice, and as a result the one-time pad has not been widely used, except for the most critical applications.
A stream cipher makes use of a much smaller and more convenient key such as 128 bits. Based on this key, it generates a pseudorandom keystream which can be combined with the plaintext digits in a similar fashion to the one-time pad. However, this comes at a cost. The keystream is now pseudorandom and so is not truly random. The proof of security associated with the one-time pad no longer holds. It is quite possible for a stream cipher to be completely unsecure.
Types of stream ciphers.
A stream cipher generates successive elements of the keystream based on an internal state. This state is updated in essentially two ways: if the state changes independently of the plaintext or ciphertext messages, the cipher is classified as a "synchronous" stream cipher. By contrast, "self-synchronising" stream ciphers update their state based on previous ciphertext digits.
Synchronous stream ciphers.
In a synchronous stream cipher a stream of pseudo-random digits is generated independently of the plaintext and ciphertext messages, and then combined with the plaintext (to encrypt) or the ciphertext (to decrypt). In the most common form, binary digits are used (bits), and the keystream is combined with the plaintext using the exclusive or operation (XOR). This is termed a binary additive stream cipher.
In a synchronous stream cipher, the sender and receiver must be exactly in step for decryption to be successful. If digits are added or removed from the message during transmission, synchronisation is lost. To restore synchronisation, various offsets can be tried systematically to obtain the correct decryption. Another approach is to tag the ciphertext with markers at regular points in the output.
If, however, a digit is corrupted in transmission, rather than added or lost, only a single digit in the plaintext is affected and the error does not propagate to other parts of the message. This property is useful when the transmission error rate is high; however, it makes it less likely the error would be detected without further mechanisms. Moreover, because of this property, synchronous stream ciphers are very susceptible to active attacks: if an attacker can change a digit in the ciphertext, he might be able to make predictable changes to the corresponding plaintext bit; for example, flipping a bit in the ciphertext causes the same bit to be flipped in the plaintext.
Self-synchronizing stream ciphers.
Another approach uses several of the previous "N" ciphertext digits to compute the keystream. Such schemes are known as self-synchronizing stream ciphers, asynchronous stream ciphers or ciphertext autokey (CTAK). The idea of self-synchronization was patented in 1946, and has the advantage that the receiver will automatically synchronise with the keystream generator after receiving "N" ciphertext digits, making it easier to recover if digits are dropped or added to the message stream. Single-digit errors are limited in their effect, affecting only up to "N" plaintext digits.
An example of a self-synchronising stream cipher is a block cipher in cipher feedback (CFB) mode.
Linear feedback shift register-based stream ciphers.
Binary stream ciphers are often constructed using linear feedback shift registers (LFSRs) because they can be easily implemented in hardware and can be readily analysed mathematically. The use of LFSRs on their own, however, is insufficient to provide good security. Various schemes have been proposed to increase the security of LFSRs.
Non-linear combining functions.
Because LFSRs are inherently linear, one technique for removing the linearity is to feed the outputs of several parallel LFSRs into a non-linear Boolean function to form a "combination generator". Various properties of such a "combining function" are critical for ensuring the security of the resultant scheme, for example, in order to avoid correlation attacks. 
Clock-controlled generators.
Normally LFSRs are stepped regularly. One approach to introducing non-linearity is to have the LFSR clocked irregularly, controlled by the output of a second LFSR. Such generators include the stop-and-go generator, the alternating step generator and the shrinking generator.
An alternating step generator comprises three linear feedback shift registers, which we will call LFSR0, LFSR1 and LFSR2 for convenience. The output of one of the registers decides which of the other two is to be used; for instance if LFSR2 outputs a 0, LFSR0 is clocked, and if it outputs a 1, LFSR1 is clocked instead. The output is the exclusive OR of the last bit produced by LFSR0 and LFSR1. The initial state of the three LFSRs is the key.
The stop-and-go generator (Beth and Piper, 1984) consists of two LFSRs. One LFSR is clocked if the output of a second is a "1", otherwise it repeats its previous output. This output is then (in some versions) combined with the output of a third LFSR clocked at a regular rate.
The shrinking generator takes a different approach. Two LFSRs are used, both clocked regularly. If the output of the first LFSR is "1", the output of the second LFSR becomes the output of the generator. If the first LFSR outputs "0", however, the output of the second is discarded, and no bit is output by the generator. This mechanism suffers from timing attacks on the second generator, since the speed of the output is variable in a manner that depends on the second generator's state. This can be alleviated by buffering the output.
Filter generator.
Another approach to improving the security of an LFSR is to pass the entire state of a single LFSR into a non-linear "filtering function".
Other designs.
Instead of a linear driving device, one may use a nonlinear update function. For example, Klimov and Shamir proposed triangular functions (T-Functions) with a single cycle on n bit words.
Security.
For a stream cipher to be secure, its keystream must have a large period and it must be impossible to "recover the cipher's key" or internal state from the keystream. Cryptographers also demand that the keystream be free of even subtle biases that would let attackers "distinguish" a stream from random noise, and free of detectable relationships between keystreams that correspond to "related keys" or related cryptographic nonces. This should be true for all keys (there should be no "weak keys"), and true even if the attacker can "know" or "choose" some "plaintext" or "ciphertext".
As with other attacks in cryptography, stream cipher attacks can be "certificational", meaning they are not necessarily practical ways to break the cipher but indicate that the cipher might have other weaknesses.
Securely using a secure synchronous stream cipher requires that one never reuse the same keystream twice; that generally means a different nonce or key must be supplied to each invocation of the cipher. Application designers must also recognize that most stream ciphers don't provide "authenticity", only "privacy": encrypted messages may still have been modified in transit.
Short periods for stream ciphers have been a practical concern. For example, 64-bit block ciphers like DES can be used to generate a keystream in output feedback (OFB) mode. However, when not using full feedback, the resulting stream has a period of around 232 blocks on average; for many applications, this period is far too low. For example, if encryption is being performed at a rate of 8 megabytes per second, a stream of period 232 blocks will repeat after about a half an hour.
Some applications using the stream cipher RC4 are attackable because of weaknesses in RC4's key setup routine; new applications should either avoid RC4 or make sure all keys are unique and ideally unrelated (such as generated by a well-seeded CSPRNG or a cryptographic hash function) and that the first bytes of the keystream are discarded.
Usage.
Stream ciphers are often used for their speed and simplicity of implementation in hardware, and in applications where plaintext comes in quantities of unknowable length like a secure wireless connection. If a block cipher (not operating in a stream cipher mode) were to be used in this type of application, the designer would need to choose either transmission efficiency or implementation complexity, since block ciphers cannot directly work on blocks shorter than their block size. For example, if a 128-bit block cipher received separate 32-bit bursts of plaintext, three quarters of the data transmitted would be padding. Block ciphers must be used in ciphertext stealing or residual block termination mode to avoid padding, while stream ciphers eliminate this issue by naturally operating on the smallest unit that can be transmitted (usually bytes).
Another advantage of stream ciphers in military cryptography is that the cipher stream can be generated in a separate box that is subject to strict security measures and fed to other devices such as a radio set, which will perform the xor operation as part of their function. The latter device can then be designed and used in less stringent environments.
RC4 is the most widely used stream cipher in software; others include: 
A5/1,
A5/2,
Chameleon, 
FISH, 
Helix,
ISAAC, 
MUGI,
Panama,
Phelix, 
Pike,
SEAL, 
SOBER,
SOBER-128 and
WAKE.
External links.
1

</doc>
<doc id="49008" url="http://en.wikipedia.org/wiki?curid=49008" title="Robert Andrews Millikan">
Robert Andrews Millikan

Robert A. Millikan (March 22, 1868 – December 19, 1953) was an American experimental physicist honored with the Nobel Prize for Physics in 1923 for his measurement of the elementary electronic charge and for his work on the photoelectric effect.
Millikan graduated from Oberlin College in 1891 and obtained his doctorate at Columbia University in 1895. In 1896 he became an assistant at the University of Chicago, where he became a full professor in 1910. In 1909 Millikan began a series of experiments to determine the electric charge carried by a single electron. He began by measuring the course of charged water droplets in an electric field. The results suggested that the charge on the droplets is a multiple of the elementary electric charge, but the experiment was not accurate enough to be convincing. He obtained more precise results in 1910 with his famous oil-drop experiment in which he replaced water (which tended to evaporate too quickly) with oil.
In 1916 Millikan took up with similar skill the experimental verification of the equation introduced by Albert Einstein in 1905 to describe the photoelectric effect. He used this same research to obtain an accurate value of Planck’s constant. In 1921 Millikan left the University of Chicago to become director of the Norman Bridge Laboratory of Physics at the California Institute of Technology (Caltech) in Pasadena, California. There he undertook a major study of the radiation that the physicist Victor Hess had detected coming from outer space. Millikan proved that this radiation is indeed of extraterrestrial origin, and he named it “cosmic rays.” As chairman of the Executive Council of Caltech (the school's governing body at the time) from 1921 until his retirement in 1945, Millikan helped to turn the school into one of the leading research institutions in the United States. He also served on the board of trustees for Science Service, now known as Society for Science & the Public, from 1921 to 1953.
Biography.
Education.
Robert Andrews Millikan was born on March 22, 1868, in Morrison, Illinois. Millikan went to high school in Maquoketa, Iowa. Millikan received a bachelor's degree in the classics from Oberlin College in 1891 and his doctorate in physics from Columbia University in 1895 – he was the first to earn a Ph.D. from that department.
At the close of my sophomore year [...] my Greek professor [...] asked me to teach the course in elementary physics in the preparatory department during the next year. To my reply that I did not know any physics at all, his answer was, "Anyone who can do well in my Greek can teach physics." "All right," said I, "you will have to take the consequences, but I will try and see what I can do with it." I at once purchased an Avery's "Elements of Physics", and spent the greater part of my summer vacation of 1889 at home – trying to master the subject. [...] I doubt if I have ever taught better in my life than in my first course in physics in 1889. I was so intensely interested in keeping my knowledge ahead of that of the class that they may have caught some of my own interest and enthusiasm.
Millikan's enthusiasm for education continued throughout his career, and he was the coauthor of a popular and influential series of introductory textbooks, which were ahead of their time in many ways. Compared to other books of the time, they treated the subject more in the way in which it was thought about by physicists. They also included many homework problems that asked conceptual questions, rather than simply requiring the student to plug numbers into a formula.
In 1902 he married Greta Ervin Blanchard. They had three sons: Clark Blanchard, Glenn Allan, and Max Franklin.
Charge of the electron.
Starting in 1908, while a professor at the University of Chicago, Millikan worked on an oil-drop experiment in which he measured the charge on a single electron. J.J. Thomson had already discovered the charge-to-mass ratio of the electron. However, the actual charge and mass values were unknown. Therefore, if one of these two values were to be discovered, the other could easily be calculated. Millikan and his then graduate student Harvey Fletcher used the oil-drop experiment to measure the charge of the electron (as well as the electron mass, and Avogadro's number, since their relation to the electron charge was known).
Professor Millikan took sole credit, in return for Harvey Fletcher claiming full authorship on a related result for his dissertation. Millikan went on to win the 1923 Nobel Prize for Physics, in part for this work, and Fletcher kept the agreement a secret until his death. After a publication on his first results in 1910, contradictory observations by Felix Ehrenhaft started a controversy between the two physicists. After improving his setup, Millikan published his seminal study in 1913.
The elementary charge is one of the fundamental physical constants and accurate knowledge of its value is of great importance. His experiment measured the force on tiny charged droplets of oil suspended against gravity between two metal electrodes. Knowing the electric field, the charge on the droplet could be determined. Repeating the experiment for many droplets, Millikan showed that the results could be explained as integer multiples of a common value (1.592 × 10−19 coulomb), the charge on a single electron. That this is somewhat lower than the modern value of 1.602 176 53(14) x 10−19 coulomb is probably due to Millikan's use of an inaccurate value for the viscosity of air.
Although at the time of Millikan's oil-drop experiments it was becoming clear that there exist such things as subatomic particles, not everyone was convinced. Experimenting with cathode rays in 1897, J.J. Thomson had discovered negatively charged 'corpuscles', as he called them, with a charge to mass ratio 1840 times that of a hydrogen ion. Similar results had been found by George FitzGerald and Walter Kaufmann. Most of what was then known about electricity and magnetism, however, could be explained on the basis that charge is a continuous variable; in much the same way that many of the properties of light can be explained by treating it as a continuous wave rather than as a stream of photons.
The beauty of the oil-drop experiment is that as well as allowing quite accurate determination of the fundamental unit of charge, Millikan's apparatus also provided a 'hands on' demonstration that charge is actually quantized. The General Electric Company's Charles Steinmetz, who had previously thought that charge is a continuous variable, became convinced otherwise after working with Millikan's apparatus.
Data selection controversy.
There is some controversy over selectivity in Millikan's use of results from his second experiment measuring the electron charge. This has been discussed by Allan Franklin, a former high-energy experimentalist and current philosopher of science at the University of Colorado. Franklin contends that Millikan's exclusions of data do not affect the final value of the charge obtained, but that Millikan's substantial "cosmetic surgery" reduced the statistical error. This enabled Millikan to give the charge of the electron to better than one half of one percent; in fact, if Millikan had included all of the data he discarded, the error would have been within 2%. While this would still have resulted in Millikan's having measured the charge of "e−" better than anyone else at the time, the slightly larger uncertainty might have allowed more disagreement with his results within the physics community, which Millikan likely tried to avoid. David Goodstein argues that Millikan's statement, that all drops observed over a sixty-day period were used in the paper, was clarified in a subsequent sentence which specified all "drops upon which complete series of observations were made". Goodstein attests that this is indeed the case and notes that five pages of tables separate the two sentences.
Photoelectric effect.
When Einstein published his seminal 1905 paper on the particle theory of light, Millikan was convinced that it had to be wrong, because of the vast body of evidence that had already shown that light was a wave. He undertook a decade-long experimental program to test Einstein's theory, which required building what he described as "a machine shop "in vacuo"" in order to prepare the very clean metal surface of the photo electrode. His results published in 1914 confirmed Einstein's predictions in every detail, but Millikan was not convinced of Einstein's interpretation, and as late as 1916 he wrote, "Einstein's photoelectric equation... cannot in my judgment be looked upon at present as resting upon any sort of a satisfactory theoretical foundation," even though "it actually represents very accurately the behavior" of the photoelectric effect. In his 1950 autobiography, however, he simply declared that his work "scarcely permits of any other interpretation than that which Einstein had originally suggested, namely that of the semi-corpuscular or photon theory of light itself".
Since Millikan's work formed some of the basis for modern particle physics, it is ironic that he was rather conservative in his opinions about 20th century developments in physics, as in the case of the photon theory. Another example is that his textbook, as late as the 1927 version, unambiguously states the existence of the ether, and mentions Einstein's theory of relativity only in a noncommittal note at the end of the caption under Einstein's portrait, stating as the last in a list of accomplishments that he was "author of the special theory of relativity in 1905 and of the general theory of relativity in 1914, both of which have had great success in explaining otherwise unexplained phenomena and in predicting new ones."
Millikan is also credited with measuring the value of Planck's constant by using photoelectric emission graphs of various metals.
Later life.
In 1917, solar astronomer George Ellery Hale convinced Millikan to begin spending several months each year at the Throop College of Technology, a small academic institution in Pasadena, California that Hale wished to transform into a major center for scientific research and education. A few years later Throop College became the California Institute of Technology (Caltech), and Millikan left the University of Chicago in order to become Caltech's "chairman of the executive council" (effectively its president). Millikan would serve in that position from 1921 to 1945. At Caltech most of his scientific research focused on the study of "cosmic rays" (a term which he coined). In the 1930s he entered into a debate with Arthur Compton over whether cosmic rays were composed of high-energy photons (Millikan's view) or charged particles (Compton's view). Millikan thought his cosmic ray photons were the "birth cries" of new atoms continually being created to counteract entropy and prevent the heat death of the universe. Compton would eventually be proven right by the observation that cosmic rays are deflected by the Earth's magnetic field (and so must be charged particles).
Robert Millikan was Vice Chairman of the National Research Council during World War I. During that time, he helped to develop anti-submarine and meteorological devices. He received the Chinese Order of Jade. In his private life, Millikan was an enthusiastic tennis player. He was married and had three sons, the eldest of whom, Clark B. Millikan, became a prominent aerodynamic engineer. Another son, Glenn, also a physicist, married the daughter (Clare) of George Leigh Mallory of "Because it's there" Mount Everest fame. Glenn was killed in a climbing accident in Cumberland Mountains in 1947.
A religious man and the son of a minister, in his later life Millikan argued strongly for a complementary relationship between Christian faith and science. He dealt with this in his Terry Lectures at Yale in 1926–7, published as "Evolution in Science and Religion". A more controversial belief of his was eugenics. This led to his association with the Human Betterment Foundation and his praising of San Marino, California for being "the westernmost outpost of Nordic civilization . . . [with] a population which is twice as Anglo-Saxon as that existing in New York, Chicago or any of the great cities of this country." 
Westinghouse time capsule.
In 1938, he wrote a short passage to be placed in the Westinghouse Time Capsules.
At this moment, August 22, 1938, the principles of representative ballot government, such as are represented by the governments of the Anglo-Saxon, French, and Scandinavian countries, are in deadly conflict with the principles of despotism, which up to two centuries ago had controlled the destiny of man throughout practically the whole of recorded history. If the rational, scientific,
progressive principles win out in this struggle there is a possibility of a warless, golden age ahead for mankind. If the reactionary principles of despotism triumph now and in the future, the future history of mankind will repeat the sad story of war and oppression as in the past.
Death and legacy.
Millikan died of a heart attack at his home in San Marino, California in 1953 at age 85, and was interred in the "Court of Honor" at Forest Lawn Memorial Park Cemetery in Glendale, California.
Millikan Middle School (formerly Millikan Junior High School) in the suburban Los Angeles neighborhood of Sherman Oaks is named in his honor, as is Robert A. Millikan High School in Long Beach, California. The Millikan Library, the tallest building on the Caltech campus is also named after him. Additionally, a major street through the Tektronix campus in Portland, Oregon, is named after him, with the Millikan Way (MAX station), a station on Portland, Oregon's MAX Blue Line named after the street. One of four suites at the Athenaeum Hotel on the Caltech campus is named after him; Room #50, The Millikan Suite.
On January 26, 1982, he was honored by the United States Postal Service with a 37¢ Great Americans series (1980–2000) postage stamp.
Famous statements.
"If Einstein's equation and Aston's curve are even roughly correct, as I'm sure they are, for Dr. Cameron and I have computed with their aid the maximum energy evolved in radioactive change and found it to check well with observation, then this supposition of an energy evolution through the disintegration of the common elements is from the one point of view a childish Utopian dream, and from the other a foolish bugaboo." 

</doc>
<doc id="49013" url="http://en.wikipedia.org/wiki?curid=49013" title="Grimm's law">
Grimm's law

Grimm's Law (also known as the First Germanic Sound Shift or Rask's rule), named after Jakob Grimm, is a set of statements describing the inherited Proto-Indo-European (PIE) stop consonants as they developed in Proto-Germanic (the common ancestor of the Germanic branch of the Indo-European family) in the 1st millennium BC. It establishes a set of regular correspondences between early Germanic stops and fricatives and the stop consonants of certain other centum Indo-European languages (Grimm used mostly Latin and Greek for illustration).
History.
Grimm's law was the first non-trivial systematic sound change to be discovered in linguistics; its formulation was a turning point in the development of linguistics, enabling the introduction of a rigorous methodology to historical linguistic research. The correspondence between Latin p and Germanic f was first noted by Friedrich von Schlegel in 1806. In 1818 Rasmus Christian Rask elaborated the set of correspondences to include other Indo-European languages such as Sanskrit and Greek, and the full range of consonants involved. In 1822 Jakob Grimm, the elder of the Brothers Grimm, in his book "Deutsche Grammatik", formulated the law as a general rule (and extended to include standard German).
Grimm himself already noticed that there were many words that had different consonants from what his law predicted. These exceptions defied linguists for a few decades, but eventually received explanation from the Danish linguist Karl Verner, in the form of Verner's law.
Overview.
Grimm's law consists of three parts which form consecutive phases in the sense of a chain shift. The phases are usually constructed as follows:
This chain shift can be abstractly represented as:
Here each sound moves one position to the right to take on its new sound value. Note that within Proto-Germanic, the sounds denoted by ⟨b⟩, ⟨d⟩, ⟨g⟩ and ⟨gw⟩ were stops in some environments and fricatives in others, so bʰ > b should be understood here as bʰ > b/β, and likewise for the others. The voiceless fricatives are customarily spelled ⟨f⟩, ⟨þ⟩, ⟨h⟩ and ⟨hw⟩ in the context of Germanic.
The exact details of the shift are unknown, and it may have progressed in a variety of ways before arriving at the final situation. The three stages listed above show the progression of a "pull chain", in which each change leaves a "gap" in the phonological system that "pulls" other phonemes into it to fill the gap. But it is also conceivable that the shift happened as a push chain, where the changes happened in reverse order, with each change "pushing" the next forward to avoid merging the phonemes.
The steps could also have occurred somewhat differently. Another possible sequence of events could have been:
This sequence would lead to the same end result. This variety of Grimm's law is often suggested in the context of the glottalic theory of Proto-Indo-European, which is followed by a minority of linguists. This theoretical framework assumes that "voiced stops" in PIE were actually voiceless to begin with, so that the second phase did not actually exist as such, or was not actually devoicing but a loss of some other articulatory feature such as glottalization. This alternative sequence also accounts for the phonetics of Verner's law (see below), which are easier to explain within the glottalic theory framework when Grimm's law is formulated in this manner.
Further changes.
Once Grimm's law had taken place, there was only one type of voiced consonant, with no distinction between voiced stops and voiced fricatives. They eventually became stops at the beginning of a word (for the most part), as well as after a nasal consonants, but fricatives elsewhere. Whether they were plosives or fricatives at first is therefore not clear. The voiced aspirated stops may have first become voiced fricatives, before hardening to stops under certain conditions. But they may also have become stops at first, softening to fricatives in most positions later.
Around the same time as Grimm's law took place, another change occurred known as Verner's law. Verner's law caused the voicing of the voiceless fricatives that resulted from Grimm's law under some conditions, creating apparent exceptions to the rule. For example:
Here, the same sound "*t" appears as "*þ" in one word (following Grimm's law), but as "*d" in another (apparently violating Grimm's law). See the Verner's law article for a more detailed explanation of this discrepancy.
The early Germanic "*gw" that had arisen from Proto-Indo-European "*gʷʰ" (and from "*kʷ" through Verner's law) underwent further changes of various sorts:
Perhaps the usual reflex was "*b" (as suggested by the connection of "bid" < "*bidjaną" and Old Irish "guidid"), but "*w" appears in certain cases (possibly through dissimilation when another labial consonant followed?), such as in "warm" and "wife" (provided that the proposed explanations are correct). Proto-Germanic "*hw" voiced by Verner's law fell together with this sound and developed identically, compare the words for 'she-wolf': from Middle High German "wülbe" and Old Norse "ylgr", one can reconstruct Proto-Germanic nominative singular "*wulbī", genitive singular "*wulgijōz", from earlier "*wulgwī", "*wulgwijōz".
Examples.
Further changes following Grimm's law, as well as sound changes in other Indo-European languages, can sometimes obscure its effects. The most illustrative examples are used here.
This is strikingly regular. Each phase involves one single change which applies equally to the labials (p, b, bʰ, f) and their equivalent dentals (t, d, dʰ, þ), velars (k, g, gʰ, h) and rounded velars (kʷ, gʷ, gʷʰ, hʷ). The first phase left the phoneme repertoire of the language without voiceless stops, the second phase filled this gap, but created a new one, and so on until the chain had run its course.
Behaviour in consonant clusters.
When two obstruents occurred in a pair, the first underwent Grimm's law if possible, while the second did not. If either of the two was voiceless, the whole cluster was devoiced, and the first obstruent also lost its labialisation, if it was present.
Most examples of this occurred with obstruents preceded by *s (resulting in *sp, *st, *sk, *skʷ), or obstruents followed by *t (giving *ft, *ss, *ht, *ht) or *s (giving *fs, *ss, *hs, *hs). The latter change was frequent in suffixes, and became a phonotactic restriction known as the Germanic spirant law. This rule remained productive throughout the Proto-Germanic period. The cluster *tt became *ss, but this was often restored to *st later on.
Examples with preceding *s:
Examples with following *t:
Correspondences to PIE.
The Germanic "sound laws", combined with regular changes reconstructed for other Indo-European languages, allow one to define the expected sound correspondences between different branches of the family. For example, Germanic (word-initial) *b- corresponds regularly to Latin "*f-", Greek "pʰ-", Sanskrit "bʰ-", Slavic, Baltic or Celtic "b-", etc., while Germanic "*f-" corresponds to Latin, Greek, Sanskrit, Slavic and Baltic "p-" and to zero (no initial consonant) in Celtic. The former set goes back to PIE *bʰ- (faithfully reflected in Sanskrit and modified in various ways elsewhere), and the latter set to PIE *p- (shifted in Germanic, lost in Celtic, but preserved in the other groups mentioned here).
One of the more conspicuous present surface correspondences is the English digraph "wh" and the corresponding Latin and Romance digraph "qu," notably found in interrogative words ("wh"-words) such as the five Ws. These both come from kʷ, which is echoed in the phonemic spelling "kw" (as in "kwik") for "qu." The present pronunciations have undergone further sound changes, such as "wh"-cluster reductions in English, though the spellings reflect the history more; see Interrogative word: Etymology for details.

</doc>
<doc id="49016" url="http://en.wikipedia.org/wiki?curid=49016" title="Verner's law">
Verner's law

Verner's law, stated by Karl Verner in 1875, describes a historical sound change in the Proto-Germanic language whereby voiceless fricatives *"f", *"þ", *"s", *"h", *"hʷ", when immediately following an unstressed syllable in the same word, underwent voicing and became respectively the fricatives *"b", *"d", *"z", *"g", *"gʷ".
The problem.
When Grimm's law was discovered, a strange irregularity was spotted in its operation. The Proto-Indo-European (PIE) voiceless stops *"p", *"t" and *"k" should have changed into Proto-Germanic (PGmc) *"f" (bilabial fricative [ɸ]), *"þ" (dental fricative [θ]) and *"h" (velar fricative [x]), according to Grimm's Law. Indeed, that was known to be the usual development. However, there appeared to be a large set of words in which the agreement of Latin, Greek, Sanskrit, Baltic, Slavic etc. guaranteed PIE *"p", *"t" or *"k", and yet the Germanic reflex was voiced (*"b", *"d" or *"g").
At first, irregularities did not cause concern for scholars since there were many examples of the regular outcome. Increasingly, however, it became the ambition of linguists to formulate general and "exceptionless" rules of sound change that would account for all the data (or as close to all the data as possible), not merely for a well-behaved subset of it (see Neogrammarians).
One classic example of PIE *"t" → PGmc *"d" is the word for 'father'. PIE *"ph2tḗr" (here, the macron marks vowel length) → PGmc *"fadēr" (instead of expected *"faþēr"). The structurally similar family term *"bʰréh₂tēr" 'brother' did indeed develop as predicted by Grimm's Law (Gmc. *"brōþēr"). Even more curiously, they often found "both" *"þ" and *"d" as reflexes of PIE *"t" in different forms of one and the same root, e.g. *"werþaną" 'to turn', preterite third-person singular *"warþ" 'he turned', but preterite third-person plural *"wurdun" and past participle *"wurdanaz".
The solution.
Karl Verner was the first scholar to note the factor governing the distribution of the two outcomes. He observed that the apparently unexpected voicing of voiceless stops occurred if they were non-word-initial and if the vowel preceding them carried no stress in PIE. The original location of stress was often retained in Greek and early Sanskrit, though in Germanic stress eventually became fixed on the initial (root) syllable of all words. The crucial difference between "*patḗr" and "*bʰrā́tēr" was therefore one of second-syllable versus first-syllable stress (cf. Sanskrit "pitā́" versus "bhrā́tā").
The *"werþaną" : *"wurdun" contrast is likewise explained as due to stress on the root versus stress on the inflectional suffix (leaving the first syllable unstressed). There are also other Vernerian alternations, as illustrated by modern German "ziehen" 'to draw, pull' : Old High "zogōn" 'to tug, drag' ← PGmc. *"teuhaną" : *"tugōną" ← Pre-Germanic *"déwk-o-nom" : *"duk-éh₂-yo-nom" 'lead'.
There is a spinoff from Verner's Law: the rule accounts also for PGmc *"z" as the development of PIE *"s" in some words. Since this *"z" changed to *"r" in the Scandinavian languages and in West Germanic (German, Dutch, English, Frisian), Verner's Law resulted in alternation of *"s" and *"r" in some inflectional paradigms, known as grammatischer Wechsel. For example, the Old English verb "ceosan" 'choose' had the past plural form "curon" and the past participle "(ge)coren" ← *"keusaną" : *"kuzun" ~ *"kuzanaz" ← Pre-Germanic *"géws-o-nom" : *"gus-únt" ~ *"gus-o-nós" 'taste, try'. We would have "chorn" for "chosen" in Modern English if the consonantal shell of "choose" and "chose" had not been morphologically levelled (cf. obs. German †"kiesen" 'to choose' : "gekoren" 'chosen'). On the other hand, Vernerian *"r" has not been levelled out in En "were" ← PGmc *"wēzun", related to En "was". Similarly, En "lose", though it has the weak form "lost", also has the archaic form "lorn" (now seen in the compound "forlorn") (cf. Dutch "verliezen" : "verloren"); in German, on the other hand, the *"s" has been levelled out both in "war" 'was' (pl. "waren" 'were') and "verlieren" 'lose' (part. "verloren" 'lost').
The following table illustrates the sound changes according to Verner. In the bottom row, for each pair, the sound on the right represents the sound changed according to Verner's Law.
Significance.
Karl Verner published his discovery in the article "Eine Ausnahme der ersten Lautverschiebung" (an exception to the first sound shift) in Kuhns Zeitschrift in 1876, but he had presented his theory already on 1 May, 1875 in a comprehensive personal letter to his friend and mentor, Vilhelm Thomsen.
It was received with great enthusiasm by the young generation of comparative philologists, the so-called "Junggrammatiker", because it was an important argument in favour of the Neogrammarian dogma that the sound laws were without exceptions ("die Ausnahmslosigkeit der Lautgesetze").
Dating the change described by Verner's law.
It is worth noting that the change in the pronunciation of the consonant, described by Verner's Law, must have occurred before the shift of stress to the first syllable. The voicing of the new consonant in Proto-Germanic is conditioned by which syllable is stressed in Proto-Indo-European, yet this syllabic stress has disappeared in Proto-Germanic, so the change in the consonant must have occurred at a time when the syllabic stress in earlier Proto-Germanic still conformed to the Indo-European pattern. However, the syllabic stress shift erased the conditioning environment, and made the variation between voiceless fricatives and their voiced alternants look mysteriously haphazard. 
Until recently it was assumed that Verner's law was productive "after" Grimm's Law. Now it has been pointed out (Vennemann 1984:21, Kortlandt 1988:5-6) that, even if the sequence is reversed, the result can be just the same given certain conditions. Noske (2012) argues that Grimm's Law and Verner's Law must have been part of a single bifurcating chain shift.
Newer considerations regarding the dating.
Some scholars today—e.g. Wolfram Euler and Konrad Badenheuer (2009), pp. 54 f. and 61–64, see below—are inclined towards preferring a new theory in which the sequence of the two changes is the opposite of what was previously assumed. This chronological reordering, however, has far-reaching implications on the shape and development of the Proto-Germanic language. The traditionally assumed order has been gradually put into question since ca. 1998 based on the following two main arguments:
Moreover, the combination of the above-mentioned traditional order (Grimm's before Verner's) and the dating of Grimm's law to the 1st century BC requires an unusually fast change of the late Common Germanic at the turn of the millennium: within only a few decades, the three dramatic changes mentioned below would have had to happen in quick succession. This would be the only way to explain that all Germanic languages show these changes. Such a rapid language change seems implausible. Strictly speaking, it would have caused a child to be unable to understand his own grandparents.
Against this background, the thesis that Verner's Law might have been valid before Grimm's Law—maybe long before it—has been finding more and more acceptance. Accordingly this order now would have to be assumed:
If Kluge's law is valid, it also requires Verner's law to precede Grimm's.
Here is a table with an alternative view of Verner's law, occurring before the shift of Grimm's law.
It is required to postulate aspiration in the voiceless stops, because the results of Verner's law merge with the descendants of the voiced aspirate stops, not of the plain voiced stops. (This can however be bypassed in the glottalic theory framework, where the voiced aspirate stops are replaced with plain voiced stops, and plain voiced stops with glottalized stops.)
There is, however, a phonologic argument against this dating: The traditional order makes it possible to narrow down the effect of Verner's law to the voiceless fricatives. If on the other hand one wants to apply the First Sound Shift after Verner's law, one has to suppose that Verner's law applies both to voiceless plosives *"p", *"t", *"k" and *"kʷ" and to the voiceless fricative *"s". In other words, in this scenario, Verner's law affected all obstruents, not just fricatives. As for the names "Cimbri" and "Vacalus", it could simply be that the presence of /k/ in these two words was due to Roman scribes hearing the early Germanic *"h" (/x/) sound as a /k/ rather than an /h/, since their own /h/ did not often occur between vowels and was at any rate already in the process of going silent.

</doc>
<doc id="49020" url="http://en.wikipedia.org/wiki?curid=49020" title="Road transport">
Road transport

Road transport (British English) or road transportation (American English) is the transport of passengers or goods on roads.
History.
Early roads.
The first methods of road transport were horses, oxen or even humans carrying goods over dirt tracks that often followed game trail. The Persians later built a network of Royal Roads across their empire.
With the advent of the Roman Empire, there was a need for armies to be able to travel quickly from one area to another, and the roads that existed were often muddy, which greatly delayed the movement of large masses of troops. To resolve this issue, the Romans built great roads. The Roman roads used deep roadbeds of crushed stone as an underlying layer to ensure that they kept dry, as the water would flow out from the crushed stone, instead of becoming mud in clay soils. The Islamic Caliphate later built tar-paved roads in Baghdad.
New road networks.
As states developed and became richer, especially with the Renaissance, new roads and bridges began to be built, often based on Roman designs. Although there were attempts to rediscover Roman methods, there was little useful innovation in road building before the 18th century.
Starting in the early 18th century, the British Parliament began to pass a series of acts that gave the local justices powers to erect toll-gates on the roads, in exchange for professional upkeep. The toll-gate erected at Wadesmill became the first effective toll-gate in England. The first scheme that had trustees who were not justices was established through a Turnpike Act in 1707, for a section of the London-Chester road between Fornhill and Stony Stratford. The basic principle was that the trustees would manage resources from the several parishes through which the highway passed, augment this with tolls from users from outside the parishes and apply the whole to the maintenance of the main highway. This became the pattern for the turnpiking of a growing number of highways, sought by those who wished to improve flow of commerce through their part of a county.
The quality of early turnpike roads was varied. Although turnpiking did result in some improvement to each highway, the technologies used to deal with geological features, drainage, and the effects of weather, were all in their infancy. Road construction improved slowly, initially through the efforts of individual surveyors such as John Metcalf in Yorkshire in the 1760s. British turnpike builders began to realise the importance of selecting clean stones for surfacing, and excluding vegetable material and clay to make better lasting roads.
Industrial civil engineering.
By the late 18th and early 19th centuries, new methods of highway construction had been pioneered by the work of three British engineers, John Metcalf, Thomas Telford and John Loudon McAdam, and by the French road engineer Pierre-Marie-Jérôme Trésaguet.
The first professional road builder to emerge during the Industrial Revolution was John Metcalf, who constructed about 180 mi of turnpike road, mainly in the north of England, from 1765. He believed a good road should have good foundations, be well drained and have a smooth convex surface to allow rainwater to drain quickly into ditches at the side. He understood the importance of good drainage, knowing it was rain that caused most problems on the roads.
Pierre-Marie-Jérôme Trésaguet established the first scientific approach to road building in France at the same time. He wrote a memorandum on his method in 1775, which became general practice in France. It involved a layer of large rocks, covered by a layer of smaller gravel. The lower layer improved on Roman practice in that it was based on the understanding that the purpose of this layer (the sub-base or base course) is to transfer the weight of the road and its traffic to the ground, while protecting the ground from deformation by spreading the weight evenly. Therefore, the sub-base did not have to be a self-supporting structure. The upper running surface provided a smooth surface for vehicles, while protecting the large stones of the sub-base.
The surveyor and engineer Thomas Telford also made substantial advances in the engineering of new roads and the construction of bridges. His method of road building involved the digging of a large trench in which a foundation of heavy rock was set. He also designed his roads so that they sloped downwards from the centre, allowing drainage to take place, a major improvement on the work of Trésaguet. The surface of his roads consisted of broken stone. He also improved on methods for the building of roads by improving the selection of stone based on thickness, taking into account traffic, alignment and slopes. During his later years, Telford was responsible for rebuilding sections of the London to Holyhead road, a task completed by his assistant of ten years, John MacNeill.
It was another Scottish engineer, John Loudon McAdam, who designed the first modern roads. He developed an inexpensive paving material of soil and stone aggregate (known as macadam). His road building method was simpler than Telford's, yet more effective at protecting roadways: he discovered that massive foundations of rock upon rock were unnecessary, and asserted that native soil alone would support the road and traffic upon it, as long as it was covered by a road crust that would protect the soil underneath from water and wear.
Also unlike Telford and other road builders, McAdam laid his roads as level as possible. His 30 ft road required only a rise of three inches from the edges to the center. Cambering and elevation of the road above the water table enabled rain water to run off into ditches on either side. Size of stones was central to the McAdam's road building theory. The lower 200 mm road thickness was restricted to stones no larger than 75 mm. The upper 50 mm layer of stones was limited to 20 mm size and stones were checked by supervisors who carried scales. A workman could check the stone size himself by seeing if the stone would fit into his mouth. The importance of the 20 mm stone size was that the stones needed to be much smaller than the 100 mm width of the iron carriage tyres that traveled on the road. Macadam roads were being built widely in the United States and Australia in the 1820s and in Europe in the 1830s and 1840s.
Modern roads.
Macadam roads were adequate for use by horses and carriages or coaches, but they were very dusty and subject to erosion with heavy rain. Later on, they did not hold up to higher-speed motor vehicle use. Methods to stabilise macadam roads with tar date back to at least 1834 when John Henry Cassell, operating from "Cassell's Patent Lava Stone Works" in Millwall, patented "Pitch Macadam".
This method involved spreading tar on the subgrade, placing a typical macadam layer, and finally sealing the macadam with a mixture of tar and sand. Tar-grouted macadam was in use well before 1900, and involved scarifying the surface of an existing macadam pavement, spreading tar, and re-compacting. Although the use of tar in road construction was known in the 19th century, it was little used and was not introduced on a large scale until the motorcar arrived on the scene in the early 20th century.
Modern tarmac was patented by British civil engineer Edgar Purnell Hooley, who noticed that spilled tar on the roadway kept the tar down and created a smooth surface. He took out a patent in 1901 for tarmac.
Transportation.
Transport on roads can be roughly grouped into the transportation of goods and transportation of people. In many countries licencing requirements and safety regulations ensure a separation of the two industries.
The nature of road transportation of goods depends, apart from the degree of development of the local infrastructure, on the distance the goods are transported by road, the weight and volume of the individual shipment, and the type of goods transported. For short distances and light, small shipments a van or pickup truck may be used. For large shipments even if less than a full truckload a truck is more appropriate. (Also see Trucking and Hauling below). In some countries cargo is transported by road in horse-drawn carriages, donkey carts or other non-motorized mode. Delivery services are sometimes considered a separate category from cargo transport. In many places fast food is transported on roads by various types of vehicles. For inner city delivery of small packages and documents bike couriers are quite common.
People are transported on roads either in individual cars or automobiles, or in mass transit by bus or coach. Special modes of individual transport by road like rickshaws or velotaxis may also be locally available.
Trucking and haulage.
Trucking companies (AE) or haulage companies / hauliers (BE) accept cargo for road transport. Truck drivers operate either independently – working directly for the client – or through freight carriers or shipping agents. Some big companies (e.g. grocery store chains) operate their own internal trucking operations. The market size for general freight trucking was nearly $125 billion in 2010. Since 2005, the trucking industry has decreased by 1%.
In the U.S. many truckers own their truck (rig), and are known as owner-operators. Some road transportation is done on regular routes or for only one consignee per run, while others transport goods from many different loading stations/shippers to various consignees. On some long runs only cargo for one leg of the route (to) is known when the cargo is loaded. Truckers may have to wait at the destination for the return cargo (from).
A bill of lading issued by the shipper provides the basic document for road freight. On cross-border transportation the trucker will present the cargo and documentation provided by the shipper to customs for inspection (for EC see also Schengen Agreement). This also applies to shipments that are transported out of a free port.
To avoid accidents caused by fatigue, truckers have to keep to strict rules for drivetime and required rest periods. In the United States and Canada, these regulations are known as hours of service, and in the European Union as drivers working hours. One such regulation is the Hours of Work and Rest Periods (Road Transport) Convention, 1979. Tachographs record the times the vehicle is in motion and stopped. Some companies use two drivers per truck to ensure uninterrupted transportation; with one driver resting or sleeping in a bunk in the back of the cab while the other is driving.
Truck drivers often need special licences to drive, known in the U.S. as a commercial driver's license. In the U.K. a Large Goods Vehicle licence is required.
For transport of hazardous materials (see dangerous goods) truckers need a licence, which usually requires them to pass an exam (e.g. in the EU). They have to make sure they affix proper labels for the respective hazard(s) to their vehicle. Liquid goods are transported by road in tank trucks (AE) or tanker lorries (BE) (also road-tankers) or special tankcontainers for intermodal transport. For unpackaged goods and liquids weigh stations confirm weight after loading and before delivery. For transportation of live animals special requirements have to be met in many countries to prevent cruelty to animals (see animal rights). For fresh and frozen goods refrigerator trucks or reefer (container)s are used.
In Australia road trains replace rail transport for goods on routes throughout the center of the country. B-doubles and semi-trailers are used in urban areas because of their smaller size. Low-loader or flat-bed trailers are used to haul containers, see containerization, in intermodal transport.
Modern roads.
Today, roadways are primarily asphalt or concrete. Both are based on McAdam's concept of stone aggregate in a binder, asphalt cement or Portland cement respectively. Asphalt is known as a flexible pavement, one which slowly will "flow" under the pounding of traffic. Concrete is a rigid pavement, which can take heavier loads but is more expensive and requires more carefully prepared subbase. So, generally, major roads are concrete and local roads are asphalt. Concrete roads are often covered with a thin layer of asphalt to create a wearing surface.
Modern pavements are designed for heavier vehicle loads and faster speeds, requiring thicker slabs and deeper subbase. Subbase is the layer or successive layers of stone, gravel and sand supporting the pavement. It is needed to spread out the slab load bearing on the underlying soil and to conduct away any water getting under the slabs. Water will undermine a pavement over time, so much of pavement and pavement joint design are meant to minimize the amount of water getting and staying under the slabs.
Shoulders are also an integral part of highway design. They are multipurpose; they can provide a margin of side clearance, a refuge for incapacitated vehicles, an emergency lane, and parking space. They also serve a design purpose, and that is to prevent water from percolating into the soil near the main pavement's edge. Shoulder pavement is designed to a lower standard than the pavement in the traveled way and won't hold up as well to traffic, so driving on the shoulder is generally prohibited.
Pavement technology is still evolving, albeit in not easily noticed increments. For instance, chemical additives in the pavement mix make the pavement more weather resistant, grooving and other surface treatments improve resistance to skidding and hydroplaning, and joint seals which were once tar are now made of low maintenance neoprene.
Traffic control.
Nearly all roadways are built with devices meant to control traffic. Most notable to the motorist are those meant to communicate directly with the driver. Broadly, these fall into three categories: signs, signals or pavement markings. They help the driver navigate; they assign the right-of-way at intersections; they indicate laws such as speed limits and parking regulations; they advise of potential hazards; they indicate passing and no passing zones; and otherwise deliver information and to assure traffic is orderly and safe.
Two hundred years ago these devices were signs, nearly all informal. In the late 19th century signals began to appear in the biggest cities at a few highly congested intersections. They were manually operated, and consisted of semaphores, flags or paddles, or in some cases colored electric lights, all modeled on railroad signals. In the 20th century signals were automated, at first with electromechanical devices and later with computers. Signals can be quite sophisticated: with vehicle sensors embedded in the pavement, the signal can control and choreograph the turning movements of heavy traffic in the most complex of intersections. In the 1920s traffic engineers learned how to coordinate signals along a thoroughfare to increase its speeds and volumes. In the 1980s, with computers, similar coordination of whole networks became possible.
In the 1920s pavement markings were introduced. Initially they were used to indicate the road's centerline. Soon after they were coded with information to aid motorists in passing safely. Later, with multi-lane roads they were used to define lanes. Other uses, such as indicating permitted turning movements and pedestrian crossings soon followed.
In the 20th century traffic control devices were standardized. Before then every locality decided on what its devices would look like and where they would be applied. This could be confusing, especially to traffic from outside the locality. In the United States standardization was first taken at the state level, and late in the century at the federal level. Each country has a Manual of Uniform Traffic Control Devices (MUTCD) and there are efforts to blend them into a worldwide standard.
Besides signals, signs, and markings, other forms of traffic control are designed and built into the roadway. For instance, curbs and rumble strips can be used to keep traffic in a given lane and median barriers can prevent left turns and even U-turns.
Toll roads.
Early toll roads were usually built by private companies under a government franchise. They typically paralleled or replaced routes already with some volume of commerce, hoping the improved road would divert enough traffic to make the enterprise profitable. Plank roads were particularly attractive as they greatly reduced rolling resistance and mitigated the problem of getting mired in mud. Another improvement, better grading to lessen the steepness of the worst stretches, allowed draft animals to haul heavier loads.
A "toll road" in the United States is often called a "turnpike". The term "turnpike" probably originated from the gate, often a simple pike, which blocked passage until the fare was paid at a "toll house" (or "toll booth" in current terminology). When the toll was paid the pike, which was mounted on a swivel, was turned to allow the vehicle to pass. Tolls were usually based on the type of cargo being transported, not the type of vehicle. The practice of selecting routes so as to avoid tolls is called shunpiking. This may be simply to avoid the expense, as a form of economic protest (or boycott), or simply to seek a road less traveled as a bucolic interlude.
Companies were formed to build, improve, and maintain a particular section of roadway, and tolls were collected from users to finance the enterprise. The enterprise was usually named to indicate the locale of its roadway, often including the name of one of both of the termini. The word "turnpike" came into common use in the names of these roadways and companies, and is essentially used interchangeably with "toll road" in current terminology.
In the United States, toll roads began with the Lancaster Turnpike in the 1790s, within Pennsylvania, connecting Philadelphia and Lancaster. In the state of New York, the Great Western Turnpike was started in Albany in 1799 and eventually extended, by several alternate routes, to near what is now Syracuse, New York.
Toll roads peaked in the mid 19th century, and by the turn of the twentieth century most toll roads were taken over by state highway departments. The demise of this early toll road era was due to the rise of canals and railroads, which were more efficient (and thus cheaper) in moving freight over long distances. Roads wouldn't again be competitive with rails and barges until the first half of the 20th century when the internal combustion engine replaces draft animals as the source of motive power.
With the development, mass production, and popular embrace of the automobile, faster and higher capacity roads were needed. In the 1920s limited access highways appeared. Their main characteristics were dual roadways with access points limited to (but not always) grade-separated interchanges. Their dual roadways allowed high volumes of traffic, the need for no or few traffic lights along with relatively gentle grades and curves allowed higher speeds.
The first limited access highways were "Parkways", so called because of their often park-like landscaping and, in the metropolitan New York City area, they connected the region's system of parks. When the German autobahns built in the 1930s introduced higher design standards and speeds, road planners and road-builders in the United States started developing and building toll roads to similar high standards. The Pennsylvania Turnpike, which largely followed the path of a partially built railroad, was the first, opening in 1940.
After 1940 with the Pennsylvania Turnpike, toll roads saw a resurgence, this time to fund limited access highways. In the late 1940s and early 1950s, after World War II interrupted the evolution of the highway, the US resumed building toll roads. They were to still higher standards and one road, the New York State Thruway, had standards that became the prototype for the U.S. Interstate Highway System. Several other major toll-roads which connected with the Pennsylvania Turnpike were established before the creation of the Interstate Highway System. These were the Indiana Toll Road, Ohio Turnpike, and New Jersey Turnpike.
Interstate Highway System.
In the United States, beginning in 1956, Dwight D. Eisenhower National System of Interstate and Defense Highways, commonly called the Interstate Highway System was built. It uses 12 foot (3.65m) lanes, wide medians, a maximum of 4% grade, and full access control, though many sections don't meet these standards due to older construction or constraints. This system created a continental-sized network meant to connect every population center of 50,000 people or more.
By 1956, most limited access highways in the eastern United States were toll roads. In that year, the federal Interstate highway program was established, funding non-toll roads with 90% federal dollars and 10% state match, giving little incentive for states to expand their turnpike system. Funding rules initially restricted collections of tolls on newly funded roadways, bridges, and tunnels. In some situations, expansion or rebuilding of a toll facility using Interstate Highway Program funding resulted in the removal of existing tolls. This occurred in Virginia on Interstate 64 at the Hampton Roads Bridge-Tunnel when a second parallel roadway to the regional 1958 bridge-tunnel was completed in 1976.
Since the completion of the initial portion of the interstate highway system, regulations were changed, and portions of toll facilities have been added to the system. Some states are again looking at toll financing for new roads and maintenance, to supplement limited federal funding. In some areas, new road projects have been completed with public-private partnerships funded by tolls, such as the Pocahontas Parkway (I-895) near Richmond, Virginia.
The newest policy passed by Congress and the Obama Administration regarding highways is the Surface and Air Transportation Programs Extension Act of 2011.
Pneumatic tires.
As the horse-drawn carriage was replaced by the car and lorry or truck, and speeds increased, the need for smoother roads and less vertical displacement became more apparent, and pneumatic tires were developed to decrease the apparent roughness. Wagon and carriage wheels, made of wood, had a tire in the form of an iron strip that kept the wheel from wearing out quickly. Pneumatic tires, which had a larger footprint than iron tires, also were less likely to get bogged down in the mud on unpaved roads.
See also.
Other topics:

</doc>
<doc id="49021" url="http://en.wikipedia.org/wiki?curid=49021" title="Coen de Koning">
Coen de Koning

Coen de Koning (March 30, 1879 – July 29, 1954) was twice winner of the Elfstedentocht and the second Dutch speed skating World Champion.
Born in Edam, North Holland, Netherlands, he won the speed skating title in his only World Championship competition in 1905 in the city of Groningen, Groningen province, Netherlands. He faced only three other skaters, one of whom was from Norway, who won the 500-meter race. After that, De Koning won all three remaining distances, thus claiming the title. De Koning participated in the 1904 and 1905 European Championships and the 1903, 1905 and 1912 Dutch Championships, winning them all. He set National Records in 1905 in the 500-meter and 10,000-meter races. These records were broken in 1926 and 1929. Coen de Koning is also famous for winning the Elfstedentocht twice, in 1912 and 1917.

</doc>
<doc id="49022" url="http://en.wikipedia.org/wiki?curid=49022" title="International E-road network">
International E-road network

The international E-road network is a numbering system for roads in Europe developed by the United Nations Economic Commission for Europe (UNECE). The network is numbered from E 1 up and its roads cross national borders. It also reaches Central Asian countries like Kyrgyzstan, since they are members of the UNECE.
In most countries, roads carry the European route designation beside national road numbers. Other countries like Belgium, Denmark, Norway and Sweden have roads with exclusive European route signage (Examples: E 18 and E 6) while at the other end of the scale, British road signs do not show the routes at all.
Other continents have similar international road networks, e.g., the Pan-American Highway in the Americas, the Trans-African Highway network, and the Asian Highway Network.
History.
UNECE was formed in 1947, and their first major act to improve transportation was a joint UN declaration no. 1264, the Declaration on the Construction of Main International Traffic Arteries, signed in Geneva on September 16, 1950, which defined the first E-road network. Originally it was envisaged that the E-road network would be a motorway system comparable to the US Interstate Highway System. The declaration was amended several times before November 15, 1975, when it was replaced by the European Agreement on Main International Traffic Arteries or "AGR", which set up a route numbering system and improved standards for roads in the list. The AGR last went through a major change in 1992 and in 2001 was extended into Central Asia to include the Caucausus nations. There were several minor revisions since, last in 2008 (as of 2009).
Numbering system.
The route numbering system is as follows:
Exceptions.
In the first established and approved version, the road numbers were well ordered. Since then a number of exceptions to this principle have been allowed.
Two Class-A roads, namely E 47 and E 55, have been allowed to retain their pre-1992 numbers, E 6 and E 4 respectively, within Sweden and Norway. These exceptions were granted because of the excessive expense connected with re-signing not only the long routes themselves, but also the associated road network in the area, since Sweden and Norway have integrated the E-roads into their national networks and they are signposted as any other national route. These roads maintain their new numbers from Denmark and southward, though, as do other European routes within Scandinavia.
Further exceptions are E 67, going from Estonia to Poland (wrong side of E 75 and E 77), assigned around year 2000, simply because it was best available number for this new route, most of E 63 in Finland (wrong side of E 75) E 8 in Finland (partly on the wrong side of E 12 after a lengthening around 2002) and E 82 (Spain and Portugal, wrong side of E 80). These irregularities exist just because it is hard to maintain good order when extending the network, and the UNECE does not want to change road numbers unnecessarily.
Because the Socialist People's Republic of Albania refused to participate in international treaties such as the AGR, it was conspicuously excluded from the route scheme, with E 65 and E 90 making noticeable detours to go around it. In the 1990s, Albania opened up to the rest of Europe, but only ratified the AGR in August 2006, so its integration into the E-road network remains weak.
Signage.
Where the European routes are signed, green signs with white numbers are used.
There are different strategies for determining how frequently to signpost the roads.
Road design standards.
The following design standards should be applied to Euroroutes unless there are exceptional circumstances (such as mountain passes etc.):
These requirements are meant to be followed for road construction. When new E-roads have been added these requirements have not been followed stringently. For example the E 45 in Sweden, added in 2006, has long parts with 6 m width or the E 22 in eastern Europe forcing drivers to slow down to 30 km/h by taking the route through villages. In Norway, parts of the E 10 are 5 m wide and in Central Asia some gravel roads have even been included.
Notes to the listings.
In the road listings below, a dash ('–') indicates a land road connection between two towns/cities—the normal case—while an ellipsis ('…') denotes a stretch across water. There aren't ferry connections at all of these places and operating ferry connections are usually run by private companies without support from the respective governments, i.e. may cease operating at any time.

</doc>
<doc id="49023" url="http://en.wikipedia.org/wiki?curid=49023" title="Propulsion">
Propulsion

Propulsion is a means of creating force leading to movement.
A propulsion system has a source of mechanical power (some type of engine or motor, muscles), and some means of using this power to generate force, such as wheel and axles, propellers, a propulsive nozzle, wings, fins or legs.
Other components such as clutches, gearboxes and so forth may be needed to connect the power source to the force generating component.
The term "propulsion" is derived from two Latin words: "pro" meaning before or forwards and "pellere" meaning to drive.
Vehicular propulsion.
Air propulsion.
An aircraft propulsion system generally consists of an aircraft engine and some means to generate thrust, such as a propeller or a propulsive nozzle.
An aircraft propulsion system must achieve two things. First, the thrust from the propulsion system must balance the drag of the airplane when the airplane is cruising. And second, the thrust from the propulsion system must exceed the drag of the airplane for the airplane to accelerate. In fact, the greater the difference between the thrust and the drag, called the excess thrust, the faster the airplane will accelerate.
Some aircraft, like airliners and cargo planes, spend most of their life in a cruise condition. For these airplanes, excess thrust is not as important as high engine efficiency and low fuel usage. Since thrust depends on both the amount of gas moved and the velocity, we can generate high thrust by accelerating a large mass of gas by a small amount, or by accelerating a small mass of gas by a large amount. Because of the aerodynamic efficiency of propellers and fans, it is more fuel efficient to accelerate a large mass by a small amount. That is why we find high bypass fans and turboprops on cargo planes and airliners.
Some aircraft, like fighter planes or experimental high speed aircraft, require very high excess thrust to accelerate quickly and to overcome the high drag associated with high speeds. For these airplanes, engine efficiency is not as important as very high thrust. Modern military aircraft typically employ afterburners on a low bypass turbofan core. Future hypersonic aircraft will employ some type of ramjet or rocket propulsion.
Ground.
Ground propulsion is any mechanism for propelling solid bodies along the ground, usually for the purposes of transportation. The propulsion system often consists of a combination of an engine or motor, a gearbox and wheel and axles in standard applications.
Maglev.
Maglev (derived from magnetic levitation) is a system of transportation that uses magnetic levitation to suspend, guide and propel vehicles with magnets rather than using mechanical methods, such as wheels, axles and bearings. With maglev a vehicle is levitated a short distance away from a guideway using magnets to create both lift and thrust. Maglev vehicles are claimed to move more smoothly and quietly and to require less maintenance than wheeled mass transit systems. It is claimed that non-reliance on friction also means that acceleration and deceleration can far surpass that of existing forms of transport. The power needed for levitation is not a particularly large percentage of the overall energy consumption; most of the power used is needed to overcome air resistance (drag), as with any other high-speed form of transport.
Marine.
Marine propulsion is the mechanism or system used to generate thrust to move a ship or boat across water. While paddles and sails are still used on some smaller boats, most modern ships are propelled by mechanical systems consisting a motor or engine turning a propeller, or less frequently, in jet drives, an impeller. Marine engineering is the discipline concerned with the design of marine propulsion systems.
Steam engines were the first mechanical engines used in marine propulsion, but have mostly been replaced by two-stroke or four-stroke diesel engines, outboard motors, and gas turbine engines on faster ships. Nuclear reactors producing steam are used to propel warships and icebreakers, and there have been attempts to utilize them to power commercial vessels. Electric motors have been used on submarines and electric boats and have been proposed for energy-efficient propulsion. Recent development in liquified natural gas (LNG) fueled engines are gaining recognition for their low emissions and cost advantages.
Space.
Spacecraft propulsion is any method used to accelerate spacecraft and artificial satellites. There are many different methods. Each method has drawbacks and advantages, and spacecraft propulsion is an active area of research. However, most spacecraft today are propelled by forcing a gas from the back/rear of the vehicle at very high speed through a supersonic de Laval nozzle. This sort of engine is called a rocket engine.
All current spacecraft use chemical rockets (bipropellant or solid-fuel) for launch, though some (such as the Pegasus rocket and SpaceShipOne) have used air-breathing engines on their first stage. Most satellites have simple reliable chemical thrusters (often monopropellant rockets) or resistojet rockets for orbital station-keeping and some use momentum wheels for attitude control. Soviet bloc satellites have used electric propulsion for decades, and newer Western geo-orbiting spacecraft are starting to use them for north-south stationkeeping and orbit raising. Interplanetary vehicles mostly use chemical rockets as well, although a few have used ion thrusters and Hall effect thrusters (two different types of electric propulsion) to great success.
Animal.
Animal locomotion, which is the act of self-propulsion by an animal, has many manifestations, including running, swimming, jumping and flying. Animals move for a variety of reasons, such as to find food, a mate, or a suitable microhabitat, and to escape predators. For many animals the ability to move is essential to survival and, as a result, selective pressures have shaped the locomotion methods and mechanisms employed by moving organisms. For example, migratory animals that travel vast distances (such as the Arctic tern) typically have a locomotion mechanism that costs very little energy per unit distance, whereas non-migratory animals that must frequently move quickly to escape predators (such as frogs) are likely to have costly but very fast locomotion. The study of animal locomotion is typically considered to be a sub-field of biomechanics.
Locomotion requires energy to overcome friction, drag, inertia, and gravity, though in many circumstances some of these factors are negligible. In terrestrial environments gravity must be overcome, though the drag of air is much less of an issue. In aqueous environments however, friction (or drag) becomes the major challenge, with gravity being less of a concern. Although animals with natural buoyancy need not expend much energy maintaining vertical position, some will naturally sink and must expend energy to remain afloat. Drag may also present a problem in flight, and the aerodynamically efficient body shapes of birds highlight this point. Flight presents a different problem from movement in water however, as there is no way for a living organism to have lower density than air. Limbless organisms moving on land must often contend with surface friction, but do not usually need to expend significant energy to counteract gravity.
Newton's third law of motion is widely used in the study of animal locomotion: if at rest, to move forwards an animal must push something backwards. Terrestrial animals must push the solid ground, swimming and flying animals must push against a fluid or gas (either water or air). The effect of forces during locomotion on the design of the skeletal system is also important, as is the interaction between locomotion and muscle physiology, in determining how the structures and effectors of locomotion enable or limit animal movement.

</doc>
<doc id="49024" url="http://en.wikipedia.org/wiki?curid=49024" title="Mathematica">
Mathematica

Mathematica is a computational software program used in many scientific, engineering, mathematical and computing fields, based on symbolic mathematics. It was conceived by Stephen Wolfram and is developed by Wolfram Research of Champaign, Illinois. The Wolfram Language is the programming language used in Mathematica.
Features.
Features of Mathematica include:
Interface.
Mathematica is split into two parts, the kernel and the front end. The kernel interprets expressions (Mathematica code) and returns result expressions.
The front end, designed by Theodore Gray, provides a GUI, which allows the creation and editing of Notebook documents containing program code with prettyprinting, formatted text together with results including typeset mathematics, graphics, GUI components, tables, and sounds. All contents and formatting can be generated algorithmically or interactively edited. Most standard word processing capabilities are supported. It includes a spell-checker but does not spell check automatically as you type.
Documents can be structured using a hierarchy of cells, which allow for outlining and sectioning of a document and support automatic numbering index creation. Documents can be presented in a slideshow environment for presentations. Notebooks and their contents are represented as Mathematica expressions that can be created, modified or analysed by Mathematica programs. This allows conversion to other formats such as TeX or XML.
The front end includes development tools such as a debugger, input completion and automatic syntax coloring. 
Among the alternative front ends is the Wolfram Workbench, an Eclipse based IDE, introduced in 2006. It provides project-based code development tools for Mathematica, including revision management, debugging, profiling, and testing. The Mathematica Kernel also includes a command line front end. Other interfaces include JMath, based on GNU readline and MASH which runs self-contained Mathematica programs (with arguments) from the UNIX command line.
Wolfram Research has published a series of hands-on starter webcasts that introduce the user interface and the engine.
High-performance computing.
In recent years, the capabilities for high-performance computing have been extended with the introduction of packed arrays (version 4, 1999) and sparse matrices (version 5, 2003), and by adopting the GNU Multi-Precision Library to evaluate high-precision arithmetic.
Version 5.2 (2005) added automatic multi-threading when computations are performed on multi-core computers. This release included CPU specific optimized libraries. In addition Mathematica is supported by third party specialist acceleration hardware such as ClearSpeed.
In 2002, gridMathematica was introduced to allow user level parallel programming on heterogeneous clusters and multiprocessor systems and in 2008 parallel computing technology was included in all Mathematica licenses including support for grid technology such as Windows HPC Server 2008, Microsoft Compute Cluster Server and Sun Grid.
Support for CUDA and OpenCL GPU hardware was added in 2010. Also, since version 8 it can generate C code, which is automatically compiled by a system C compiler, such as the Intel C++ Compiler or Visual Studio 2010.
Deployment.
There are several ways to deploy applications written in Mathematica:
Connections with other applications.
Communication with other applications occurs through a protocol called . It allows communication between the Mathematica kernel and front-end, and also provides a general interface between the kernel and other applications. Wolfram Research freely distributes a developer kit for linking applications written in the C programming language to the Mathematica kernel through "MathLink". Using "J/Link"., a Java program can ask Mathematica to perform computations; likewise, a Mathematica program can load Java classes, manipulate Java objects and perform method calls. Similar functionality is achieved with ".NET /Link", but with .NET programs instead of Java programs. Other languages that connect to Mathematica include Haskell, AppleScript, Racket, Visual Basic, Python and Clojure.
Links are available to many specialized mathematical software packages including OpenOffice.org Calc, Microsoft Excel, MATLAB, R, Sage, SINGULAR, Wolfram SystemModeler, and Origin. Mathematical equations can be exchanged with other computational or typesetting software via MathML. 
Communication with SQL databases is achieved through built-in support for JDBC. Mathematica can also install web services from a WSDL description. It can access HDFS data via Hadoop.
Mathematica can capture real-time data via a link to LabVIEW, from financial data feeds and directly from hardware devices via GPIB (IEEE 488), USB and serial interfaces. It automatically detects and reads from HID devices.
Computable data.
Mathematica includes collections of curated data provided for use in computations. Mathematica is also integrated with Wolfram Alpha, an online service which provides additional data, some of which is kept updated in real time. Some of the data sets include astronomical, chemical, geopolitical, language, biomedical and weather data, in addition to mathematical data (such as knots and polyhedra).
Design.
Wolfram Research provides documents listing the algorithms used to implement the functions in Mathematica.
Related products.
Products from Wolfram Research associated with Mathematica include the following:
Licensing and platform availability.
Mathematica is proprietary software licensed at a range of prices for commercial, educational, and other uses.
Mathematica 10 is supported on various versions of Microsoft Windows (Vista, 7 and 8), Apple's OS X, Linux, Raspbian and as an online service. All platforms are supported with 64-bit implementations. Mathematica prior to version 10 for OS X required Java SE 6 which is a deprecated component of Mavericks. Earlier versions of Mathematica up to 6.0.3 supported other operating systems, including Solaris, AIX, Convex, HP-UX, IRIX, MS-DOS, NeXTSTEP, OS/2, Ultrix and Windows Me.
Version history.
Mathematica built on the ideas in Cole and Wolfram's earlier Symbolic Manipulation Program (SMP). The name of the program “Mathematica” was suggested to Stephen Wolfram by Apple co-founder Steve Jobs although Stephen Wolfram had thought about it earlier and rejected it.
Wolfram Research has released the following versions of Mathematica:

</doc>
<doc id="49026" url="http://en.wikipedia.org/wiki?curid=49026" title="Expendable launch system">
Expendable launch system

An expendable launch system is a launch system that uses an expendable launch vehicle (ELV) to carry a payload into space. The vehicles used in expendable launch systems are designed to be used only once (i.e. they are "expended" during a single flight), and their components are not recovered for re-use after launch. The vehicle typically consists of several rocket stages, discarded one by one as the vehicle gains altitude and speed.
Design rationale.
The ELV design differs from that of reusable launch systems, where the vehicle is launched and recovered more than once. Reuse might seem to make systems like the Space Shuttle more cost effective than ELVs, but in practice launches using ELVs have been less expensive than Shuttle launches. (See Space Shuttle Program and Criticism of the Space Shuttle program for discussion of Space Shuttle economics.) Most satellites are currently launched using expendable launchers; they are perceived as having a low risk of mission failure, a short time to launch and a relatively low cost.#redirect 
History.
Many orbital expendable launchers are derivatives of 1950s-era ballistic missiles. As such, cost was not a major consideration in their design. A prime example of this is the Titan IV, the costliest per-unit launch vehicle in history (following the Space Shuttle).
On the other hand, a reusable launcher such as the Shuttle requires a heavier structure and a recovery system (wings, thermal protection system, wheels, etc.) that reduce payload capacity. The Shuttle additionally carried a crew (though not inherent to a reusable system) whose weight, supplies and life support systems further decrease payload capacity.
The Space Shuttle was a major national asset, and its high cost (far more than a single expendable launch vehicle) and presence of a crew require stringent "man rated" flight safety precautions that increased launch and payload costs. Only five orbiters were built, and the loss of two (Challenger and Columbia) significantly impacted the capacity and viability of the Shuttle program. Each loss also resulted in an extended hiatus in Shuttle flights compared to that following most expendable launch failures, each of which impacted only that model of launcher.
For these reasons it is generally#redirect agreed that the Space Shuttle did not deliver on its original promise to reduce the costs of constructing and launching payloads into orbit. The Shuttle was originally intended to replace expendable launchers in the launching of satellites, but after the loss of "Challenger" the Shuttle was reserved for previously planned missions and those requiring a crew.
Spacecraft launched by the Shuttle included several TDRSS communications relays heavily used by the Shuttle program itself, a series of commercial communication satellites, and the interplanetary probes Magellan, Galileo and Ulysses. Several classified military payloads were also carried.
Development.
European sponsorship.
On March 26, 1980, the European Space Agency and the Centre National d'Etudes Spatiales (CNES) created Arianespace, the world's first commercial space transportation company. Arianespace produces, operates and markets the Ariane launcher family. By 1995 Arianespace lofted its 100th satellite and by 1997 the Ariane rocket had its 100th launch. Arianespace's 23 shareholders represent scientific, technical, financial and political entities from 10 different European countries. The major shareholder is the CNES, with 34.68% of capital.
American deregulation.
From the beginning of the Shuttle program until the "Challenger" disaster in 1986, it was the policy of the United States that NASA be the public-sector provider of U.S. launch capacity to the world market. Initially NASA subsidized satellite launches with the intention of eventually pricing Shuttle service for the commercial market at long-run marginal cost.
On October 30, 1984, United States President Ronald Reagan signed into law the Commercial Space Launch Act. This enabled an American industry of private operators of expendable launch systems. Prior to the signing of this law, all commercial satellite launches in the United States were limited to NASA's Space Shuttle.
On November 5, 1990, United States President George H. W. Bush signed into law the Launch Services Purchase Act. The Act, in a complete reversal of the earlier Space Shuttle monopoly, requires NASA to purchase launch services for its primary payloads from commercial providers whenever such services are required in the course of its activities.
Russian privatization.
The Russian government sold part of its stake in RSC Energia to private investors in 1994. Energia together with Khrunichev constituted most of the Russian manned space program. In 1997, the Russian government sold off enough of its share to lose the majority position.
American subsidization.
In 1996 the United States government selected Lockheed Martin and Boeing to each develop Evolved Expendable Launch Vehicles (EELV) to compete for launch contracts and provide assured access to space. The government's acquisition strategy relied on the strong commercial viability of both vehicles to lower unit costs. This anticipated market demand did not materialize, but both the Delta IV and Atlas V EELVs remain in active service.
Russian renationalization.
Beginning in 2013, the Russian government began a renationalization of the Russian space sector. Among the actions taken was the formation of the United Rocket and Space Corporation (Russian: Объединенная ракетно-космическая корпорация) to consolidate a large number of disparate companies and bureaus. 
The efforts have continued into 2014.
Launch alliances.
Since 1995 Khrunichev's Proton rocket is marketed through International Launch Services while the Soyuz rocket is marketed via Starsem. Energia builds the Soyuz rocket and owns part of the Sea Launch project which flies the Ukrainian Zenit rocket.
In 2003 Arianespace joined with Boeing Launch Services and Mitsubishi Heavy Industries to create the Launch Services Alliance. In 2005, continued weak commercial demand for EELV launches drove Lockheed Martin and Boeing to propose a joint venture called the United Launch Alliance to monopolize the United States government launch market.
A number of commercial space transportation companies offer launch services to commercial satellite operators and government space organizations around the world. For example, in 2005 there were 18 total commercial launches and 37 non-commercial launches. Russia flew 44% of the commercial orbital launches, while Europe had 28% and the United States had 6%.

</doc>
<doc id="49030" url="http://en.wikipedia.org/wiki?curid=49030" title="José Ramos-Horta">
José Ramos-Horta

José Manuel Ramos-Horta GCL AC (]; born 26 December 1949) is the United Nations' special Representative and Head of the United Nations Integrated Peacebuilding Office in Guinea-Bissau (UNIOGBIS). He was appointed to this position on 2 January 2013. Previously, he was special envoy to fellow Lusophone country, Guinea-Bissau, and was the President of East Timor from 20 May 2007 to 20 May 2012, the second since independence from Indonesia. He is a co-recipient of the 1996 Nobel Peace Prize and a former prime minister, having served from 2006 until his inauguration as president after winning the 2007 East Timorese presidential election.
As a founder and former member of the Revolutionary Front for an Independent East Timor (FRETILIN), Ramos-Horta served as the exiled spokesman for the East Timorese resistance during the years of the Indonesian occupation of East Timor (1975 to 1999). While he has continued to work with FRETILIN, Ramos-Horta resigned from the party in 1988, becoming an independent politician.
After East Timor achieved independence in 2002, Ramos-Horta was appointed as the country's first foreign minister. He served in this position until his resignation on 25 June 2006, amidst political turmoil. On 26 June, following the resignation of prime minister Mari Alkatiri, Ramos-Horta was appointed acting prime minister by then president, Xanana Gusmão. Two weeks later, on 10 July 2006, he was sworn in as the second prime minister of East Timor. On 11 February 2008, Ramos-Horta was injured when he was shot during an assassination attempt.
Early history and family.
Of "mestiço" ethnicity, Ramos-Horta was born in 1949 in Dili, capital of East Timor, to a Timorese mother and a Portuguese father who had been exiled to what was then Portuguese Timor by the Salazar dictatorship. He was educated in a Catholic mission in the small village of Soibada, later chosen by FRETILIN as headquarters after the Indonesian invasion. Of his eleven brothers and sisters, four were killed by the Indonesian military.
Ramos-Horta studied Public International Law at the Hague Academy of International Law (1983) and at Antioch College in Yellow Springs, Ohio where he completed a Master of Arts degree in Peace Studies (1984). He was trained in Human Rights Law at the International Institute of Human Rights in Strasbourg (1983). He completed Post-Graduate courses in American Foreign Policy at Columbia University in New York (1983). He is a Senior Associate Member of the University of Oxford's St Antony's College (1987) and speaks fluently in five languages: Portuguese, plus English, French, Spanish and the most commonly spoken East Timorese language, Tetum.
He is divorced from Ana Pessoa Pinto, East Timor's Minister for State and Internal Administration, with whom he has a son, Loro Horta, born in exile in Mozambique.
Political career.
He was actively involved in the development of political awareness in Portuguese Timor, which caused him to be exiled for two years in 1970–71 to Portuguese East Africa. His grandfather, before him, had also been exiled, from Portugal to the Azores Islands, then Cape Verde, Portuguese Guinea and finally to Portuguese Timor.
A moderate in the emerging Timorese nationalist leadership, he was appointed Foreign Minister in the "Democratic Republic of East Timor" government proclaimed by the pro-independence parties in November 1975. When appointed minister, Ramos-Horta was only 25 years old. Ramos-Horta left East Timor three days before the Indonesian troops invaded to plead the Timorese case before the UN.
Ramos-Horta arrived in New York to address the UN Security Council and urge them to take action in the face of the Indonesian occupation during which an estimated 102,000 East Timorese would die. Ramos-Horta was the Permanent Representative of FRETILIN to the UN for the next ten years. His friends at that time mentioned that he arrived in the United States with a total of $25 in his pocket. His pecuniary situation was often straitened in that period; he survived partly by grace of Americans who admired his politics and his determination. Further, he was obliged to travel worldwide to explain his party's position.
In 1993, the Rafto Prize was awarded to the people of East Timor. Foreign-minister-in-exile Ramos-Horta represented his nation at the prize ceremony. In May 1994, Philippine President Fidel Ramos (no relation), bowing to pressure from Jakarta, tried to ban an international conference on East Timor in Manila and blacklisted Ramos-Horta, with the Thai government following suit later that year by declaring him persona non grata.
In December 1996, Ramos-Horta shared the Nobel Peace Prize with fellow Timorese, Bishop Ximenes Belo. The Nobel Committee chose to honour the two laureates for their "sustained efforts to hinder the oppression of a small people", hoping that "this award will spur efforts to find a diplomatic solution to the conflict of East Timor based on the people's right to self-determination". The Committee considered Ramos-Horta "the leading international spokesman for East Timor's cause since 1975".
Ramos-Horta played a leading role in negotiating the institutional foundations for independence. He led the Timorese delegation at an important joint workshop with UNTAET on 1 March 2000 to tease out a new strategy, and identify institutional needs. The outcome was an agreed blueprint for a joint administration with executive powers, including leaders of the National Congress for Timorese Reconstruction (CNRT). Further details were worked out in a conference in May 2000. The Special Representative of the UN Secretary-General in East Timor, Sérgio Vieira de Mello, presented the new blueprint to a donor conference in Lisbon, on 22 June 2000, and to the UN Security Council on 27 June 2000. On 12 July 2000, the NCC adopted a regulation establishing a Transitional Cabinet composed of four East Timorese and four UNTAET representatives. The revamped joint administration successfully laid the institutional foundations for independence, and on 27 September 2002, East Timor joined the United Nations. Ramos-Horta was its first Foreign Minister.
On 3 June 2006, Ramos-Horta added the post of Interim Minister of Defense to his portfolio as Foreign Minister, in the wake of the resignations of the previous minister. He resigned as both Foreign and Defence Minister on 25 June 2006, announcing, "I do not wish to be associated with the present government or with any government involving Alkatiri." Prime Minister Alkatiri had been under pressure to resign his position in place of President Xanana Gusmão, but in a 25 June meeting, leaders of the FRETILIN party agreed to keep Alkatiri as Prime Minister; Ramos-Horta resigned immediately following this decision. Foreign Minister of Australia Alexander Downer expressed his personal disappointment at Ramos-Horta's resignation. Following Alkatiri's resignation on 26 June, Ramos-Horta withdrew his resignation to contest the prime ministership and served in the position on a temporary basis until a successor to Alkatiri was named. On 8 July 2006, Ramos-Horta himself was appointed Prime Minister by President Gusmão. He was sworn in on 10 July.
Before his appointment as Prime Minister, Ramos-Horta was considered a possible candidate to succeed Kofi Annan as United Nations Secretary-General. He dropped out of the race in order to serve as East Timor's Prime Minister, but he has indicated that he might run for the UN position at some time in the future: "I can wait five years if I am really interested in the job in 2012. I would be interested in that."
In an interview with Al Jazeera broadcast on 22 February 2007, Ramos-Horta said that he would run for president in the April 2007 election. On 25 February 2007, Ramos-Horta formally announced his candidacy. He received the support of Gusmão, who was not running for re-election. In an interview with Global South Development Magazine, Ramos-Horta revealed that Mahatma Gandhi was his greatest hero.
In the first round of the election, held on 9 April, Ramos-Horta took second place with 21.81% of the vote; he and FRETILIN candidate Francisco Guterres, who took first place, then participated in the second round of the election in May. The full results of the runoff elections were made public by East Timor's National Electoral Committee spokeswoman, Maria Angelina Sarmento, on 11 May, and Ramos-Horta won with 69% of the vote.
He was inaugurated as President of East Timor in a ceremony at the parliament house in Dili on 20 May 2007. He had resigned as Prime Minister the day before and was succeeded by Estanislau da Silva.
During the first round of the presidential elections of 2012, held on 17 March, Ramos-Horta, who was eligible for a second and final term as president, took third place with 19.43% of the vote behind the presidential candidates Francisco Guterres with 27.28% and Taur Matan Ruak 24.17% of the vote. He admitted defeat, and his term as president ended on 19 May, with the inauguration of Taur Matan Ruak as his successor.
Assassination attempt.
On 11 February 2008, José Ramos-Horta was shot in an assassination attempt. In the gun skirmish, one of Ramos-Horta's guards was wounded, and two rebel soldiers, including rebel leader Alfredo Reinado, were killed. Ramos-Horta was treated at a New Zealand Military base in Dili before being transferred to the Royal Darwin Hospital in Australia for further treatment. Doctors thought that he had been shot two or three times with the most serious injury being to his right lung. His condition was listed as critical but stable. He was placed in an induced coma on full life support, and regained consciousness on 21 February. A message from Ramos-Horta, still recovering in Darwin, was broadcast on 12 March. In this message, he thanked his supporters and Australia and said that he had "been very well looked after". A spokesman said that his condition was improving and that he had started taking short daily walks for exercise.
Ramos-Horta was released from the Royal Darwin Hospital on 19 March, although he said that he would stay in Australia for physical therapy for "a few more weeks". He also said on this occasion that he had remained conscious following the shooting and "remember[ed] every detail", describing how he was taken for treatment. On 17 April, Ramos-Horta returned to Dili from Darwin. He gave a press conference at the airport in which he urged the remaining rebels in the mountains to surrender.
Post-presidency.
Following the 2012 Guinea-Bissau coup d'état, he offered to mediate the conflict. As of 31 January 2013 he is the UN special envoy to the country.
Other activities.
Ramos-Horta is a frequent speaker, along with other Nobel Peace Prize Laureates, at Peacejam conferences. He has served as Chairman of the Advisory Board for TheCommunity.com, a web site for peace and human rights, since 2000. In 2001 he gathered the post 9/11 statements of 28 Nobel Peace Prize Laureates on the web site, and has spearheaded other peace initiatives with his fellow Nobel Laureates.
Ramos-Horta supported the U.S. invasion and occupation of Iraq and condemned the anti-American tone of its detractors as "hypocritical". In the 1990s he had supported the cause of Kurdish people in Iraq.
In May 2009 Ramos-Horta stated that he would ask the International Criminal Court to investigate the ruling junta of Myanmar if they continue to detain fellow Nobel Laureate Aung San Suu Kyi. However by August 2010, he had softened his views on Myanmar, warmly receiving the Myanmar Foreign Minister U Nyan Win, and said that he wants to improve relations and seek strong commercial ties with Myanmar.
Ramos-Horta is an early signatory of the International Simultaneous Policy (SIMPOL) which seeks to end the usual deadlock in tackling global issues. Lucas became a signatory in 2006.
On 5 August 2009, he attended the funeral of the late former president of the Philippines Corazon Aquino. He was the only foreign head of state to attend. On 30 June 2010, he attended the inauguration of Benigno S. Aquino III, the 15th President of the Philippines. He was, again, the only head of state who attended the inauguration and the first dignitary to arrive in the Philippines for the inauguration.
Ramos-Horta is a Member of the Global Leadership Foundation, an organization which works to support democratic leadership, prevent and resolve conflict through mediation and promote good governance in the form of democratic institutions, open markets, human rights and the rule of law. It does so by making available, discreetly and in confidence, the experience of former leaders to today’s national leaders. It is a not-for-profit organization composed of former heads of government, senior governmental and international organization officials who work closely with Heads of Government on governance-related issues of concern to them.
Awards.
The Roman Catholic priest Carlos Belo of East Timor and Ramos-Horta were jointly awarded the 1996 Nobel Peace Prize for their efforts. Both men also received the Order of Liberty, the Grand Cross, from Portugal in 1996.
On 25 November 2013, the Governor-General of Australia appointed Ramos-Horta an Honorary Companion of the Order of Australia (AC) "for eminent service to strengthening Australia and Timor-Leste bilateral relations and for his outstanding leadership to bring independence to Timor-Leste."
Depictions.
The 2000 documentary "The Diplomat", directed by Tom Zubrycki, follows Ramos-Horta in the period from 1998 to his return to East Timor in 2000. Ramos-Horta is played by Oscar Isaac in the 2009 film "Balibo". The film tells the story of the Balibo Five and the events preceding the Indonesian occupation of East Timor.

</doc>
<doc id="49031" url="http://en.wikipedia.org/wiki?curid=49031" title="River Thames">
River Thames

The River Thames ( ) is a river that flows through southern England. It is the longest river entirely in England and the second longest in the United Kingdom, after the River Severn. While it is best known for flowing through :London, the river also flows alongside other towns and cities, including Oxford, Reading, Henley-on-Thames, and Windsor.
The river gives its name to three informal areas: the Thames Valley, a region of England around the river between Oxford and west London; the Thames Gateway; and the greatly overlapping Thames Estuary around the tidal Thames to the east of London and including the waterway itself. Thames Valley Police is a formal body that takes its name from the river, covering three counties.
In an alternative name, derived from its long tidal reach up to Teddington Lock in south west London, the lower reaches of the river are called the Tideway.
The administrative powers of the Thames Conservancy have been taken on with modifications by the Environment Agency and, in respect of the Tideway part of the river, such powers are split between the agency and the Port of London Authority.
In non-administrative use, stemming directly from the river and its name are Thames Valley University, Thames Water, Thames Television productions, Thames & Hudson publishing, Thameslink (north-south railways passing through central London), and South Thames College. Historic entities include the Thames Ironworks and Shipbuilding Company.
Two broad canals link the river to other river basins: the Kennet and Avon Canal (Reading to Bath) and the Grand Union Canal (London to The Midlands). The Grand Union effectively bypassed the earlier, narrow and winding Oxford canal which also remains open as a popular scenic recreational route. Three further cross-basin canals are disused but are in various stages of reconstruction: the Thames and Severn Canal (via Stroud), which operated until 1927 (to the west coast of England), the Wey and Arun Canal to Littlehampton, which operated until 1871 (to the south coast), and the Wilts and Berks Canal.
Rowing and sailing clubs are common along the Thames, which is navigable to such vessels. Kayaking and canoeing also take place. Major annual events include the Henley Royal Regatta and The Boat Race, while the Thames has been used during two Summer Olympic Games: 1908 (rowing);1948 (rowing and canoeing). Safe headwaters and reaches are a summer venue for organised swimming, which is prohibited on safety grounds in a stretch centred on Central London. Non-Olympic watersports with a lesser presence include skiffing and punting.
Summary.
With a total length of 236 mi, the Thames is the longest river entirely in England and the second longest in the United Kingdom. It rises at Thames Head in Gloucestershire, and flows into the North Sea via the Thames Estuary. On its way, it passes through London, the country's capital, where the river is deep and navigable to ships; the Thames drains the whole of Greater London. Its tidal section, reaching up to Teddington Lock, includes most of its London stretch and has a rise and fall of 7 m. 
Along its course are 45 navigation locks with accompanying weirs. Its catchment area covers a large part of South Eastern and a small part of Western England and the river is fed by 38 named tributaries. The river contains over 80 islands. With its waters varying from freshwater to almost seawater, the Thames supports a variety of wildlife and has a number of adjoining Sites of Special Scientific Interest, with the largest being in the remaining parts of the North Kent Marshes and covering 5449 ha.
The marks of human activity, in some cases dating back to Pre-Roman Britain, are visible at various points along the river. These include a variety of structures connected with use of the river, such as navigations, bridges, and watermills, as well as prehistoric burial mounds. A major maritime route is formed for much of its length for shipping and supplies: through the Port of London for international trade, internally along its length and by its connection to the British canal system. The river's position has put it at the centre of many events in British history, leading to it being described by John Burns as "liquid history".
Running through some of the driest parts of mainland Britain, the Thames's discharge is low considering its length and breadth: the Severn has a discharge almost twice as large on average despite having a smaller basin area. In Scotland, the Tay achieves more than double the average discharge from a basin area that is 60% smaller.
Etymology.
The Thames, from Middle English "Temese", is derived from the Celtic name for the river, "Tamesas" (from *"tamēssa"), recorded in Latin as "Tamesis" and yielding modern Welsh "Tafwys" "Thames". The name probably meant "dark" and can be compared to other cognates such as Russian "темно" (Proto-Slavic " *tьmьnъ"), Sanskrit "tamas", Irish "teimheal" and Welsh "tywyll" "darkness" (Proto-Celtic *"temeslos") and Middle Irish "teimen" "dark grey", though Richard Coates mentions other theories: Kenneth Jackson's that it is non Indo-European (and of unknown meaning), and Peter Kitson's that it is Indo-European but pre-Celtic and has a name indicating "muddiness" from a root "*tā-", 'melt'.
Note also other river names such as Teme, Tavy, Teviot, Teifi (cf Tafwys).
The river's name has always been pronounced with a simple "t" /t/; the Middle English spelling was typically "Temese" and Celtic "Tamesis". A similar spelling from this era (1210 AD), "Tamisiam", is found in the Magna Carta. The "th" spelling lends an air of Greek to the name and was added during the Renaissance, possibly to reflect or support a claim that the name was derived from River Thyamis in the Epirus region of Greece, whence early Celtic tribes were wrongly thought to have migrated to Britain.
Indirect evidence for the antiquity of the name 'Thames' is provided by a Roman potsherd found at Oxford, bearing the inscription "Tamesubugus fecit" (Tamesubugus made this). It is believed that Tamesubugus' name was derived from that of the river.
The Thames through Oxford is sometimes given the name the River Isis. Historically, and especially in Victorian times, gazetteers and cartographers insisted that the entire river was correctly named the River Isis from its source down to Dorchester-on-Thames, and that only from this point, where the river meets the River Thame and becomes the "Thame-isis" (supposedly subsequently abbreviated to Thames) should it be so called. Ordnance Survey maps still label the Thames as "River Thames or Isis" down to Dorchester. However, since the early 20th century this distinction has been lost in common usage outside of Oxford, and some historians suggest the name "Isis" is nothing more than a truncation of "Tamesis", the Latin name for the Thames.
Richard Coates suggests that while the river was as a whole called the Thames, part of it, where it was too wide to ford, was called *"(p)lowonida". This gave the name to a settlement on its banks, which became known as Londinium, from the Indo-European roots *"pleu-" "flow" and *"-nedi" "river" meaning something like the flowing river or the wide flowing unfordable river.
Tamese was referred to as a place, not a river in the Ravenna Cosmography.
For merchant seamen, the Thames has long been just 'the London River'. Londoners often refer to it simply as 'the river', in expressions such as 'south of the river'.
Physical and natural aspects.
Course of the river.
The usually quoted source of the Thames is at Thames Head (at grid reference ). This is about 1200 m (three-quarters of a mile) north of Kemble parish church in southern Gloucestershire, near the town of Cirencester, in the Cotswolds. Seven Springs near Cheltenham, where the river Churn rises, is also sometimes quoted as the Thames' source, as this location is furthest from the mouth, and adds some 14 mi to the length.
The springs at Seven Springs flow throughout the year, while those at Thames Head are only seasonal (a winterbourne). The Thames is the longest river entirely in England, but the River Severn, which is partly in Wales, is the longest river in the United Kingdom. As the river Churn, sourced at Seven Springs is 14 mi longer than the Thames (from its traditional source at Thames head), its length 229 mi is greater than the Severn's length 220 mi. Thus, the "Churn/Thames" river may be regarded as the longest natural river flow in the United Kingdom. The stream from Seven Springs is joined at Coberley by a longer tributary which could further increase the length of the Thames, with its source in the grounds of the National Star College at Ullenwood.
The Thames flows through or alongside Ashton Keynes, Cricklade, Lechlade, Oxford, Abingdon, Wallingford, Goring-on-Thames and Streatley , Pangbourne and Whitchurch-on-Thames, Reading, Wargrave, Henley-on-Thames, Marlow, Maidenhead, Windsor and Eton, Staines-upon-Thames and Egham, Chertsey, Shepperton, Weybridge, Sunbury-on-Thames, Walton-on-Thames, Molesey and Thames Ditton. Minor redefining and widening of the main channel around Oxford, Abingdon and Marlow took place before 1850 since which specific cuts to ease navigation have assisted in cutting journey distances.
Molesey faces Hampton, London, and in Greater London the Thames passes Hampton Court, Surbiton, Kingston upon Thames, Teddington, Twickenham, Richmond (with a famous view of the Thames from Richmond Hill), Syon House, Kew, Brentford, Chiswick, Barnes, Hammersmith, Fulham, Putney, Wandsworth, Battersea and Chelsea. In central London, the river passes Pimlico and Vauxhall, and then forms one of the principal axes of the city, from the Palace of Westminster to the Tower of London. At this point, it historically formed the southern boundary of the medieval city, with Southwark, on the opposite bank, then being part of Surrey.
Beyond central London, the river passes Bermondsey, Wapping, Shadwell, Limehouse, Rotherhithe, Millwall, Deptford, Greenwich, Cubitt Town, Blackwall, New Charlton and Silvertown, before flowing through the Thames Barrier, which protects central London from flooding by storm surges. Below the barrier, the river passes Woolwich, Thamesmead, Dagenham, Erith, Purfleet, Dartford, West Thurrock, Northfleet, Tilbury and Gravesend before entering the Thames Estuary near Southend-on-Sea.
Catchment area and discharge.
The Thames River Basin District, including the Medway catchment, covers an area of 16133 km2. The river basin includes both rural and heavily urbanised areas in the east and northern parts while the western parts of the catchment are predominantly rural. The area is among the driest in the United Kingdom. Water resources consist of groundwater from aquifers and water taken from the Thames and its tributaries, much of it stored in large bank-side reservoirs.
The Thames itself provides two-thirds of London's drinking water while groundwater supplies about 40 per cent of public water supplies in the total catchment area. Groundwater is an important water source, especially in the drier months, so maintaining its quality and quantity is extremely important. Groundwater is vulnerable to surface pollution, especially in highly urbanised areas.
The non-tidal section.
Brooks, canals and rivers, within an area of 9950 km2, combine to form 38 main tributaries feeding the Thames between its source and Teddington Lock. This is the usual tidal limit; however, high spring tides can raise the head water level in the reach above Teddington and can occasionally reverse the river flow for a short time. In these circumstances, tidal effects can be observed upstream to the next lock beside Molesey weir, which is visible from the towpath and bridge beside Hampton Court Palace. Before Teddington Lock was built in 1810–12, the river was tidal at peak spring tides as far as Staines upon Thames.
In descending order, non-related tributaries of the non-tidal Thames, with river status, are the Churn, Leach, Cole, Ray, Coln, Windrush, Evenlode, Cherwell, Ock, Thame, Pang, Kennet, Loddon, Colne, Wey and Mole. In addition, there are occasional backwaters and artificial cuts that form islands, distributaries (most numerous in the case of the Colne), and man-made distributaries such as the Longford River. Three canals intersect this stretch: the Oxford Canal, Kennet and Avon Canal, and Wey Navigation.
Its longest artificial secondary channel (cut), the Jubilee River, was built between Maidenhead and Windsor for flood relief and completed in 2002.
The non-tidal section of the river is owned and managed by the Environment Agency, which is responsible for managing the flow of water to help prevent and mitigate flooding, and providing for navigation: the volume and speed of water downstream is managed by adjusting the sluices at each of the weirs and, at peak high water, levels are generally dissipated over preferred flood plains adjacent to the river. Occasionally, flooding of inhabited areas is unavoidable and the agency issues flood warnings. Due to stiff penalties applicable on the non-tidal river, which is a drinking water source before treatment, sanitary sewer overflow from the many sewage works covering the upper Thames basin is rare in the non-tidal Thames, which ensures clearer water compared to the river's tideway.
The tidal section.
Below Teddington Lock (about 55 mi upstream of the Thames Estuary), the river is subject to tidal activity from the North Sea. Before the lock was installed, the river was tidal as far as Staines, about 16 mi upstream. London, capital of Roman Britain, was established on two hills, now known as Cornhill and Ludgate Hill. These provided a firm base for a trading centre at the lowest possible point on the Thames.
A river crossing was built at the site of London Bridge. London Bridge is now used as the basis for published tide tables giving the times of high tide. High tide reaches Putney about 30 minutes later than London Bridge, and Teddington about an hour later. The tidal stretch of the river is known as "the Tideway". Tide tables are published by the Port of London Authority and are . Times of high and low tides are also .
The principal tributaries of the River Thames on the Tideway include the rivers Brent, Wandle, Effra, Westbourne, Fleet, Ravensbourne (the final part of which is called Deptford Creek), Lea, Roding, Darent and Ingrebourne. At London, the water is slightly brackish with sea salt, being a mix of sea and fresh water.
This part of the river is managed by the Port of London Authority. The flood threat here comes from high tides and strong winds from the North Sea, and the Thames Barrier was built in the 1980s to protect London from this risk.
Islands.
The River Thames contains over 80 islands ranging from the large estuarial marshlands of the Isle of Sheppey and Canvey Island to small tree-covered islets like Rose Isle in Oxfordshire and Headpile Eyot in Berkshire. They are found all the way from the Isle of Sheppey in Kent to Fiddler's Island in Oxfordshire. Some of the largest inland islands, for example Formosa Island near Cookham and Andersey Island at Abingdon, were created naturally when the course of the river divided into separate streams.
In the Oxford area the river splits into several streams across the floodplain (Seacourt Stream, Castle Mill Stream, Bulstake Stream and others), creating several islands (Fiddler's Island, Osney and others). Desborough Island, Ham Island at Old Windsor, and Penton Hook Island were artificially created by lock cuts and navigation channels. Chiswick Eyot is a familiar landmark on the Boat Race course, while Glover's Island forms the centrepiece of the spectacular view from Richmond Hill.
Islands of historical interest include Magna Carta Island at Runnymede, Fry's Island at Reading, and Pharaoh's Island near Shepperton. In more recent times Platts Eyot at Hampton was the place where MTBs were built, Tagg's Island near Molesey was associated with the impresario Fred Karno, and Eel Pie Island at Twickenham was the birthplace of the South East's R&B music scene.
Westminster Abbey and the Palace of Westminster (commonly known today as the Houses of Parliament) were built on Thorney Island, which used to be an eyot.
Geological and topographic history.
The River Thames can first be identified as a discrete drainage line as early as 58 million years ago, in the Thanetian stage of the late Palaeocene epoch. Until around 500,000 years ago, the Thames flowed on its existing course through what is now Oxfordshire, before turning to the north east through Hertfordshire and East Anglia and reaching the North Sea near Ipswich.
At this time the river system headwaters lay in the English West Midlands and may, at times, have received drainage from the Berwyn Mountains in North Wales. Streams and rivers like the River Brent, Colne Brook, and Bollo Brook either flowed into the then river Thames or went out to sea on the course of the present-day river Thames.
About 450,000 years ago, in the most extreme Ice Age of the Pleistocene, the Anglian, the furthest southern extent of the ice sheet was at Hornchurch in east London. It dammed the river in Hertfordshire, resulting in the formation of large ice lakes, which eventually burst their banks and caused the river to be diverted onto its present course through what is now London. Progressively, the channel was pushed south to form the St Albans depression by the repeated advances of the ice sheet.
This created a new river course through Berkshire and on into London, after which the river rejoined its original course in southern Essex, near the present River Blackwater estuary. Here it entered a substantial freshwater lake in the southern North Sea basin. The overspill of this lake caused the formation of the Dover Straits or Pas-de-Calais gap between Britain and France. Subsequent development led to the continuation of the course that the river follows at the present day.
Most of the bedrock of the Vale of Aylesbury is made up of clay and chalk that was formed at the end of the ice age and at one time was under the Proto-Thames. Also created at this time were the vast underground reserves of water that make the water table higher than average in the Vale of Aylesbury.
Ice age.
The last advance from that Scandinavian ice flow to have reached this far south covered much of NW Middlesex and finally forced the Proto-Thames to take roughly its present course. At the height of the last ice age, around 20,000 BC, Britain was connected to mainland Europe by a large expanse of land known as Doggerland in the southern North Sea basin. At this time, the Thames' course did not continue to Doggerland but flowed southwards from the eastern Essex coast where it met the Rhine, the Meuse and the Scheldt flowing from what are now the Netherlands and Belgium. These rivers formed a single river—the Channel River ("Fleuve Manche")—that passed through the Dover Strait and drained into the Atlantic Ocean in the western English Channel.
The ice sheet, which stopped around present day Finchley, deposited Boulder clay to form Dollis Hill and Hanger Hill. Its torrent of meltwater gushed through the Finchley Gap and south towards the new course of the Thames, and proceeded to carve out the Brent Valley in the process. Upon the valley sides there can be seen other terraces of brickearth, laid over and sometimes interlayered with the clays.
These deposits were brought in by the winds during the periglacial periods, suggesting that wide, flat marshes were then part of the landscape, which the new river Brent proceeded to cut down. The steepness of the valley sides is an indicator of the very much lower mean sea levels caused by the glaciation locking up so much water upon the land masses, thus causing the river water to flow rapidly seaward and so erode its bed quickly downwards.
The original land surface was around 110 to 130 metres (350 to 400 ft) above the current sea level. The surface had sandy deposits from an ancient sea, laid over sedimentary clay (this is the blue London Clay). All the erosion down from this higher land surface, and the sorting action by these changes of water flow and direction, formed what is known as the Thames River Gravel Terraces.
Since Roman times and perhaps earlier, the isostatic rebound from the weight of previous ice sheets, and its interplay with the eustatic change in sea level, have resulted in the old valley of the river Brent, together with that of the Thames, silting up again. Thus, along much of the Brent's present-day course, one can make out the water meadows of rich alluvium, which is augmented by frequent floods.
Conversion of marshland.
After the river took its present-day course, much of the banks of the Thames Estuary and the Thames Valley in London was partly covered in marshland, as was the adjoining Lower Lea Valley. Streams and rivers like the River Lea, Tyburn Brook, and Bollo Brook drained into the river, while some islands, e.g. Thorney Island, formed over the ages. The northern tip of the ancient parish of Lambeth, for example, was marshland known as "Lambeth Marshe", but it was drained in the 18th century; it is remembered in the street name Lower Marsh.
The East End of London, also known simply as the East End, was the area of London east of the medieval walled City of London and north of the River Thames, although it is not defined by universally accepted formal boundaries; the River Lea can be considered another boundary. Most of the local riverside was also marshland. The land was drained and became farmland; it was built on after the Industrial Revolution. Use of the term "East End" in a pejorative sense began in the late 19th century,
Canvey Island in southern Essex (area 18.45 km2; pop. 37,479) was once marshy, but is now a fully reclaimed island in the Thames estuary. It is separated from the mainland of south Essex by a network of creeks. Lying below sea level it is prone to flooding at exceptional tides, but has nevertheless been inhabited since Roman times.
Wildlife.
Various species of birds feed off the river or nest on it, some being found both at sea and inland. These include cormorant, black-headed gull, and herring gull. The mute swan is a familiar sight on the river but the escaped black swan is more rare. The annual ceremony of Swan Upping is an old tradition of counting stocks.
Non-native geese that can be seen include Canada geese, Egyptian geese, and bar-headed geese, and ducks include the familiar native mallard, plus introduced Mandarin duck and wood duck. Other water birds to be found on the Thames include the great crested grebe, coot, moorhen, heron, and kingfisher. Many types of British birds also live alongside the river, although they are not specific to the river habitat.
The Thames contains both sea water and fresh water, thus providing support for seawater and freshwater fish. However, many populations of fish are at risk and are being killed in tens of thousands because of pollutants leaking into the river from human activities. Salmon, which inhabit both environments, have been reintroduced and a succession of fish ladders have been built into weirs to enable them to travel upstream.
On 5 August 1993, the largest non-tidal salmon in recorded history was caught close to Boulters Lock in Maidenhead. The specimen weighed 6.5 kg or 14.5 pounds and measured 88 cm or 22 inches in length. The eel is particularly associated with the Thames and there were formerly many eel traps. Freshwater fish of the Thames and its tributaries include brown trout, chub, dace, roach, barbel, perch, pike, bleak, and flounder. Colonies of short-snouted seahorses have also recently been discovered in the river. The Thames is also host to some invasive crustaceans, including the signal crayfish and the Chinese mitten crab.
Aquatic mammals are also known to inhabit the Thames. The population of grey and harbour seals numbers up to 700 in the Thames Estuary. These animals have been sighted as far upriver as Richmond. Bottlenose dolphins and harbour porpoises are also sighted in the Thames.
On 20 January 2006, a 16–18 ft (5 m) northern bottle-nosed whale was seen in the Thames as far upstream as Chelsea. This was extremely unusual: this whale is generally found in deep sea waters. Crowds gathered along the riverbanks to witness the extraordinary spectacle but there was soon concern, as the animal came within yards of the banks, almost beaching, and crashed into an empty boat causing slight bleeding. About 12 hours later, the whale is believed to have been seen again near Greenwich, possibly heading back to sea. A rescue attempt lasted several hours, but the whale died on a barge. "See River Thames whale".
Human history.
The River Thames has served several roles in human history, being an economic resource, a maritime route, a boundary, a fresh water source, a source of food, and more recently a leisure facility. In 1929, John Burns, one-time MP for Battersea, responded to an American's unfavourable comparison of the Thames with the Mississippi by coining the expression "The Thames is liquid history".
There is evidence of human habitation living off the river along its length dating back to Neolithic times. The British Museum has a decorated bowl (3300–2700 BC), found in the River at Hedsor, Buckinghamshire, and a considerable amount of material was discovered during the excavations of Dorney Lake. A number of Bronze Age sites and artefacts have been discovered along the banks of the river including settlements at Lechlade, Cookham, and Sunbury-on-Thames.
So extensive have the changes to this landscape been that what little evidence there is of man's presence before the ice came has inevitably shown signs of transportation here by water and reveals nothing specifically local. Likewise, later evidence of occupation, even since the arrival of the Romans, may lie next to the original banks of the Brent but have been buried under centuries of silt.
Roman Britain.
Some of the earliest written references to the Thames (Latin: "Tamesis") occur in Julius Caesar's account of his second expedition to Britain in 54 BC, when the Thames presented a major obstacle and he encountered the Iron Age Belgic tribes the Catuvellauni and the Atrebates along the river. The confluence of the Thames and Cherwell was the site of early settlements and the river Cherwell marked the boundary between the Dobunni tribe to the west and the Catuvellauni tribe to the east (these were pre-Roman Celtic tribes). In the late 1980s a large Romano-British settlement was excavated on the edge of the village of Ashton Keynes in Wiltshire.
In AD 43, under the Emperor Claudius, the Romans occupied England and, recognising the river's strategic and economic importance, built fortifications along the Thames valley including a major camp at Dorchester. Cornhill and Ludgate Hill provided a defensible site near a point on the river both deep enough for the era's ships and narrow enough to be bridged; Londinium (London) grew up around the Walbrook on the north bank around the year 47. Boudica's Iceni razed the settlement in AD 60 or 61 but it was soon rebuilt and, following the completion of its bridge, it grew to become the provincial capital of the island.
The next Roman bridges upstream were at Pontes (Staines) on the Devil's Highway between Londinium and Calleva (Silchester). Boats could be swept up to it on the rising tide with no need for wind or muscle power.
Middle Ages.
A Romano-British settlement grew up north of the confluence, partly because the site was naturally protected from attack on the east side by the River Cherwell and on the west by the River Thames. This settlement dominated the pottery trade in what is now central southern England, and pottery was distributed by boats on the Thames and its tributaries.
Many of the Thames' riverside settlements trace their roots back to early times; for example, the suffix—"ing" in the names of towns such as Goring and Reading is of Saxon origin. Recent research suggests that these peoples preceded the Romans rather than replaced them. The river's long tradition of farming, fishing, milling and trade with other nations started with these peoples and has continued to the present day.
Competition for the use of the river created the centuries-old conflict between those who wanted to dam the river to build millraces and fish traps and those who wanted to travel and carry goods on it. Economic prosperity and the foundation of wealthy monasteries by the Anglo-Saxons attracted unwelcome visitors and by around AD 870 the Vikings were sweeping up the Thames on the tide and creating havoc as in their destruction of Chertsey Abbey.
Once King William had won total control of the strategically important Thames Valley, he went on to invade the rest of England. He had many castles built, including those at Wallingford, Rochester, Windsor, and most importantly the Tower of London. Many details of Thames activity are recorded in the Domesday book. The following centuries saw the conflict between king and barons coming to a head in AD 1215 when King John was forced to sign the Magna Carta on an island in the Thames at Runnymede. Among a host of other things, this granted the barons the right of Navigation under Clause 23.
Another major consequence of John's reign was the completion of the multi-piered London Bridge, which acted as a barricade and barrage on the river, affecting the tidal flow upstream and increasing the likelihood of the river freezing over. In Tudor and Stuart times, various kings and queens built magnificent riverside palaces at Hampton Court, Kew, Richmond on Thames, Whitehall, and Greenwich.
As early as the 1300s, the Thames was used as a means of disposing of waste produced from the city of London, effectively turning the river into an open sewer. In 1357, Edward III described the state of the river in a proclamation: "...dung and other filth had accumulated in divers places upon the banks of the river with... fumes and other abominable stenches arising therefrom."
The growth of the population of London greatly increased the amount of waste that entered the river, including human excrement, animal waste from slaughter houses, and waste from manufacturing processes. According to historian Peter Ackroyd, "a public lavatory on London Bridge showered its contents directly onto the river below, and latrines were built over all the tributaries that issued into the Thames."
Early modern period.
The 16th and 17th centuries saw the City of London grow with the expansion of world trade. The wharves of the Pool of London were thick with seagoing vessels while naval dockyards were built at Deptford. The Dutch navy even entered the Thames in 1667 in the raid on the Medway.
During a series of cold winters the Thames froze over above London Bridge: in the first Frost Fair in 1607, a tent city was set up on the river, along with a number of amusements, including ice bowling.
In good conditions, barges travelled daily from Oxford to London carrying timber, wool, foodstuffs, and livestock. The stone from the Cotswolds used to rebuild St Paul's Cathedral after the Great Fire in 1666 was brought all the way down from Radcot. The Thames provided the major route between the City of London and Westminster in the 16th and 17th centuries; the clannish guild of watermen ferried Londoners from landing to landing and tolerated no outside interference. In 1715, Thomas Doggett was so grateful to a local waterman for his efforts in ferrying him home, pulling against the tide, that he set up a rowing race for professional watermen known as "Doggett's Coat and Badge".
By the 18th century, the Thames was one of the world's busiest waterways, as London became the centre of the vast, mercantile British Empire, and progressively over the next century the docks expanded in the Isle of Dogs and beyond. Efforts were made to resolve the navigation conflicts upstream by building locks along the Thames. After temperatures began to rise again, starting in 1814, the river stopped freezing over. The building of a new London Bridge in 1825, with fewer piers (pillars) than the old, allowed the river to flow more freely and prevented it from freezing over in cold winters.
Throughout early modern history the population of London and its industries discarded their rubbish in the river. In the late 18th and 19th centuries people known as Mudlarks scavenged in the river mud for a meagre living.
Victorian era.
In the nineteenth century the quality of water in Thames deteriorated further. The dumping of raw sewage into the Thames was formerly only common in the City of London, making its tideway a harbour for many harmful bacteria. Four serious cholera outbreaks killed tens of thousands of people between the years of 1832 and 1865. Historians have attributed Prince Albert's death in 1861 to typhoid that had spread in the river's dirty waters beside Windsor Castle. Wells with water tables that mixed with tributaries (or the non-tidal Thames) faced such pollution with the widespread installation of the flush toilet in the 1850s. In the 'Great Stink' of 1858, pollution in the river reached such an extreme that sittings of the House of Commons at Westminster had to be abandoned.
A concerted effort to contain the city's sewage by constructing massive sanitary sewers on the north and south river embankments followed, under the supervision of engineer Joseph Bazalgette. Meanwhile, similar huge undertakings took place to ensure the water supply, with the building of reservoirs and pumping stations on the river to the west of London, slowly helping the quality of water to improve.
The Victorian era was one of imaginative engineering. The coming of the railways added railway bridges to the earlier road bridges and also reduced commercial activity on the river. However, sporting and leisure use increased with the establishment of regattas such as Henley and The Boat Race. On 3 September 1878, one of the worst river disasters in England took place, when the crowded pleasure boat "Princess Alice" collided with the "Bywell Castle", killing over 640 people.
20th century.
The growth of road transport, and the decline of the Empire in the years following 1914, reduced the economic prominence of the river. During World War II, the protection of certain Thames-side facilities, particularly docks and water treatment plants, was crucial to the munitions and water supply of the country. The river's defences included the Maunsell forts in the estuary, and the use of barrage balloons to counter German bombers using the reflectivity and shapes of the river to navigate during The Blitz.
In the post-war era, although the Port of London remains one of the UK's three main ports, most trade has moved downstream from central London.
The decline of heavy industry and tanneries, reduced use of oil-pollutants, and improved sewage treatment have led to much better water quality as compared with the late 19th and early- to mid-20th centuries, and aquatic life has returned to its formerly 'dead' stretches.
Alongside the entire river runs the Thames Path, a National Route for walkers and cyclists.
In the early 1980s a pioneering flood control device, the Thames Barrier, was opened – closed to tides several times a year to prevent water damage to London's low-lying areas upstream (the 1928 Thames flood demonstrated the severity of this type of event). In the late 1990s, the 7 mi long Jubilee River was built as a wide flood channel through partly already watercourse-covered land in Taplow and Eton which face Maidenhead and Windsor. Other shorter cuts already existed above and below this point, however only on the non-tidal Thames hence the need for the Barrier.
The active river.
One of the major resources provided by the Thames is the water distributed as drinking water by Thames Water, whose area of responsibility covers the length of the River Thames. The Thames Water Ring Main is the main distribution mechanism for water in London, with one major loop linking the Hampton, Walton, Ashford and Kempton Park Water Treatment Works with central London.
In the past, commercial activities on the Thames included fishing (particularly eel trapping), coppicing willows and osiers which provided wood, and the operation of watermills for flour and paper production and metal beating. These activities have disappeared. A hydro-electric plant at Romney Lock to power Windsor Castle using two Archimedes screws was opened in 2013 by the Queen.
The Thames is popular for a wide variety of riverside housing, including high-rise flats in central London and chalets on the banks and islands upstream. Some people live in houseboats, typically around Brentford and Tagg's Island.
Transport and tourism.
The tidal river.
In London there are many sightseeing tours in tourist boats, past the more famous riverside attractions such as the Houses of Parliament and the Tower of London as well as regular riverboat services co-ordinated by London River Services.
The upper river.
In summer, passenger services operate along the entire non-tidal river from Oxford to Teddington. The two largest operators are Salters Steamers and French Brothers. Salters operate services between Folly Bridge, Oxford and Staines. The whole journey takes 4 days and requires several changes of boat. French Brothers operate passenger services between Maidenhead and Hampton Court.
Along the course of the river a number of smaller private companies also offer river trips at Oxford, Wallingford, Reading and Hampton Court. Many companies also provide boat hire on the river.
The leisure navigation and sporting activities on the river have given rise to a number of businesses including boatbuilding, marinas, ships chandlers and salvage services.
Aerial lift.
The Air Line aerial cable system over the Thames from the Greenwich Peninsula to the Royal Docks has been in operation since the 2012 Summer Olympics.
Police and lifeboats.
The river is policed by five police forces. The Thames Division is the River Police arm of London's Metropolitan Police, while Surrey Police, Thames Valley Police, Essex Police and Kent Police have responsibilities on their parts of the river outside the metropolitan area. There is also a London Fire Brigade fire boat on the river. The river claims a number of lives each year.
As a result of the Marchioness disaster in 1989 when 51 people died, the Government asked the Maritime and Coastguard Agency, the Port of London Authority and the Royal National Lifeboat Institution (RNLI) to work together to set up a dedicated Search and Rescue service for the tidal River Thames. As a result, there are four lifeboat stations on the river Thames at Teddington (Teddington lifeboat station), Chiswick (Chiswick lifeboat station), Victoria Embankment/Waterloo Bridge (Tower Lifeboat Station) and Gravesend (Gravesend lifeboat station).
Navigation.
The Thames is maintained for navigation by powered craft from the estuary as far as Lechlade in Gloucestershire and for very small craft to Cricklade. From Teddington Lock to the head of navigation, the navigation authority is the Environment Agency. Between the sea and Teddington Lock, the river forms part of the Port of London and navigation is administered by the Port of London Authority. Both the tidal river through London and the non-tidal river upstream are intensively used for leisure navigation.
The non-tidal River Thames is divided into reaches by the 44 locks. The locks are staffed for the greater part of the day, but can be operated by experienced users out of hours. This part of the Thames links to existing navigations at the River Wey Navigation, the River Kennet and the Oxford Canal. All craft using it must be licensed. The Environment Agency has patrol boats (named after tributaries of the Thames) and can enforce the limit strictly since river traffic usually has to pass through a lock at some stage. A speed limit of 8 km/h applies. There are pairs of transit markers at various points along the non-tidal river that can be used to check speed – a boat travelling legally taking a minute or more to pass between the two markers.
The tidal river is navigable to large ocean-going ships as far upstream as the Pool of London and London Bridge. Although London's upstream enclosed docks have closed and central London sees only the occasional visiting cruise ship or warship, the tidal river remains one of Britain's main ports. Around 60 active terminals cater for shipping of all types including ro-ro ferries, cruise liners and vessels carrying containers, vehicles, timber, grain, paper, crude oil, petroleum products, liquified petroleum gas, etc. There is a regular traffic of aggregate or refuse vessels, operating from wharves in the west of London. The tidal Thames links to the canal network at the River Lea Navigation, the Regent's Canal at Limehouse Basin, and the Grand Union Canal at Brentford.
Upstream of Wandsworth Bridge a speed limit of 8 kn is in force for powered craft to protect the riverbank environment and to provide safe conditions for rowers and other river users. There is no absolute speed limit on most of the Tideway downstream of Wandsworth Bridge, although boats are not allowed to create undue wash. Powered boats are limited to 12 knots between Lambeth Bridge and downstream of Tower Bridge, with some exceptions. Boats can be approved by the harbour master to travel at speeds of up to 30 knots from below Tower Bridge to past the Thames Barrier.
History of the management of the river.
In the Middle Ages the Crown exercised general jurisdiction over the Thames, one of the four royal rivers, and appointed water bailiffs to oversee the river upstream of Staines. The City of London exercised jurisdiction over the tidal Thames. However, navigation was increasingly impeded by weirs and mills, and in the 14th century the river probably ceased to be navigable for heavy traffic between Henley and Oxford. In the late 16th century the river seems to have been reopened for navigation from Henley to Burcot.
The first commission concerned with the management of the river was the Oxford-Burcot Commission, formed in 1605 to make the river navigable between Burcot and Oxford.
In 1751 the Thames Navigation Commission was formed to manage the whole non-tidal river above Staines. The City of London long claimed responsibility for the tidal river. A long running dispute between the City and the Crown over ownership of the river was not settled until 1857, when the Thames Conservancy was formed to manage the river from Staines downstream. In 1866 the functions of the Thames Navigation Commission were transferred to the Thames Conservancy, which thus had responsibility for the whole river.
In 1909 the powers of the Thames Conservancy over the tidal river, below Teddington, were transferred to the Port of London Authority.
In 1974 the Thames Conservancy became part of the new Thames Water Authority. When Thames Water was privatised in 1990, its river management functions were transferred to the National Rivers Authority, in 1996 subsumed into the Environment Agency.
The river as a boundary.
Until enough crossings were established, the river presented a formidable barrier, with Belgic tribes and Anglo-Saxon kingdoms being defined by which side of the river they were on. When English counties were established their boundaries were partly determined by the Thames. On the northern bank were the ancient counties of Gloucestershire, Oxfordshire, Buckinghamshire, Middlesex and Essex. On the southern bank were the counties of Wiltshire, Berkshire, Surrey, and Kent.
The 214 bridges and 17 tunnels that have been built to date have changed the dynamics and made cross-river development and shared responsibilities more practicable. In 1965, upon the creation of Greater London, the London Borough of Richmond upon Thames incorporated the former 'Middlesex and Surrey' banks, Spelthorne moved from Middlesex to Surrey; and further changes in 1974 moved some of the boundaries away from the river. For example, some areas were transferred from Berkshire to Oxfordshire, and from Buckinghamshire to Berkshire. On occasion – for example in rowing – the banks are still referred to by their traditional county names.
Crossings.
Many of the present road bridges are on the site of earlier fords, ferries and wooden bridges. At Swinford Bridge, a toll bridge, there was first a ford and then a ferry prior to the bridge being built. The earliest known major crossings of the Thames by the Romans were at London Bridge and Staines Bridge. At Folly Bridge in Oxford the remains of an original Saxon structure can be seen, and medieval stone bridges such as Newbridge and Abingdon Bridge are still in use.
Kingston's growth is believed to stem from its having the only crossing between London Bridge and Staines until the beginning of the 18th century. During the 18th century, many stone and brick road bridges were built from new or to replace existing bridges both in London and along the length of the river. These included Putney Bridge, Westminster Bridge, Datchet Bridge, Windsor Bridge and Sonning Bridge.
Several central London road bridges were built in the 19th century, most conspicuously Tower Bridge, the only Bascule bridge on the river, designed to allow ocean-going ships to pass beneath it. The most recent road bridges are the bypasses at Isis Bridge and Marlow By-pass Bridge and the Motorway bridges, most notably the two on the M25 route Queen Elizabeth II Bridge and M25 Runnymede Bridge.
Railway development in the 19th century resulted in a spate of bridge building including Blackfriars Railway Bridge and Charing Cross (Hungerford) Railway Bridge in central London, and the spectacular railway bridges by Isambard Kingdom Brunel at Maidenhead Railway Bridge, Gatehampton Railway Bridge and Moulsford Railway Bridge.
The world's first underwater tunnel was Marc Brunel's Thames Tunnel built in 1843 and now used to carry the East London Line. The Tower Subway was the first railway under the Thames, which was followed by all the deep-level tube lines. Road tunnels were built in East London at the end of the 19th century, being the Blackwall Tunnel and the Rotherhithe Tunnel. The latest tunnels are the Dartford Crossings.
Many foot crossings were established across the weirs that were built on the non-tidal river, and some of these remained when the locks were built – for example at Benson Lock. Others were replaced by a footbridge when the weir was removed as at Hart's Weir Footbridge. Around 2000, several footbridges were added along the Thames, either as part of the Thames Path or in commemoration of the millennium. These include Temple Footbridge, Bloomers Hole Footbridge, the Hungerford Footbridges and the Millennium Bridge, all of which have distinctive design characteristics.
Before bridges were built, the main means of crossing the river was by ferry. A significant number of ferries were provided specifically for navigation purposes. When the towpath changed sides, it was necessary to take the towing horse and its driver across the river. This was no longer necessary when barges were powered by steam. Some ferries still operate on the river. The Woolwich Ferry carries cars and passengers across the river in the Thames Gateway and links the North Circular and South Circular roads. Upstream are smaller pedestrian ferries, for example Hampton Ferry and Shepperton to Weybridge Ferry the last being the only non-permanent crossing that remains on the Thames Path.
Sport.
There are several watersports prevalent on the Thames, with many clubs encouraging participation and organising racing and inter-club competitions.
Rowing.
The Thames is the historic heartland of rowing in the United Kingdom. There are over 200 clubs on the river, and over 8,000 members of British Rowing (over 40% of its membership). Most towns and districts of any size on the river have at least one club. Internationally attended centres are Oxford, Henley-on-Thames and events and clubs on the stretch of river from Chiswick to Putney.
Two rowing events on the River Thames are traditionally part of the wider English sporting calendar:
The University Boat Race is rowed between Oxford University Boat Club and the Cambridge University Boat Club in late March or early April, on the Championship Course from Putney to Mortlake in the west of London.
Henley Royal Regatta takes place over five days at the start of July in the upstream town of Henley-on-Thames. Besides its sporting significance the regatta is an important date on the English social calendar alongside events like Royal Ascot and Wimbledon.
Other significant or historic rowing events on the Thames include:
Other regattas, head races and University bumping races are held along the Thames which are described under Rowing on the River Thames.
Sailing.
Sailing is practised on both the tidal and non-tidal reaches of the river. The highest club upstream is at Oxford. The most popular sailing craft used on the Thames are lasers, GP14s, and Wayfarers. One sailing boat unique to the Thames is the Thames Rater, which is sailed around Raven's Ait.
Skiffing.
Skiffing has dwindled in favour of private motor boat ownership but is competed on the river in the summer months. Six clubs and a similar number of skiff regattas exist from The Skiff Club, Teddington upstream.
Punting.
Unlike the "pleasure punting" common on the Cherwell in Oxford and the Cam in Cambridge, punting on the Thames is competitive as well as recreational and uses narrower craft, typically based at the few skiff clubs.
Kayaking and canoeing.
Kayaking and canoeing are common, with sea kayakers using the tidal stretch for touring. Sheltered water kayakers and canoeists use the non-tidal section for training, racing and trips. Whitewater playboaters and slalom paddlers are catered for at weirs like those at Hurley Lock, Sunbury Lock and Boulter's Lock. At Teddington just before the tidal section of the river starts is Royal Canoe Club, said to be the oldest in the world and founded in 1866. Since 1950, almost every year at Easter, long distance canoeists have been competing in what is now known as the Devizes to Westminster International Canoe Race, which follows the course of the Kennet and Avon Canal, joins the River Thames at Reading and runs right up to a grand finish at Westminster Bridge.
Swimming.
In 2006 British swimmer and environmental campaigner Lewis Pugh became the first person to swim the full length of the Thames from outside Kemble to Southend-on-Sea to draw attention to the severe drought in England which saw record temperatures indicative of a degree of global warming. The 325 km (202 mi) swim took him 21 days to complete. The official headwater of the river had stopped flowing due to the drought forcing Pugh to run the first 26 mi.
Since June 2012 the Port of London Authority has made and enforces a by-law that bans swimming between Putney Bridge and Crossness, Thamesmead (thus including all of Central London) without obtaining prior permission, on the grounds that swimmers in that area of the river endanger not only themselves, due to the strong current of the river, but also other river users.
Organised swimming events take place at various points generally upstream of Hampton Court, including Windsor, Marlow and Henley. In 2011 comedian David Walliams swam the 140 mi from Lechlade to Westminster Bridge and raised over £1 million for charity.
In non-tidal stretches swimming was, and still is a leisure and fitness activity among experienced swimmers where safe, deeper outer channels are used in times of low stream.
Meanders.
A Thames meander is a long-distance journey over all or part of the Thames by running, swimming or using any of the above means. It is often carried out as an athletic challenge in a competition or for a record attempt.
The Thames in the arts.
Visual arts.
The River Thames has been a subject for artists, great and minor, over the centuries. Four major artists with works based on the Thames are Canaletto, J. M. W. Turner, Claude Monet, and James Abbott McNeill Whistler. The 20th-century British artist Stanley Spencer produced many works at Cookham.
The river is lined with various pieces of sculpture, but John Kaufman's sculpture The Diver: Regeneration is sited in the Thames near Rainham.
Literature.
The Thames is mentioned in many works of literature including novels, diaries and poetry. It is the central theme in three in particular:
"Three Men in a Boat" by Jerome K. Jerome, first published in 1889, is a humorous account of a boating holiday on the Thames between Kingston and Oxford. The book was intended initially to be a serious travel guide, with accounts of local history of places along the route, but the humorous elements eventually took over. The landscape and features of the Thames as described by Jerome are virtually unchanged, and the book's enduring popularity has meant that it has never been out of print since it was first published.
Charles Dickens "Our Mutual Friend" (written in the years 1864–65) describes the river in a grimmer light. It begins with a scavenger and his daughter pulling a dead man from the river near London Bridge, to salvage what the body might have in its pockets, and heads to its conclusion with the deaths of the villains drowned in Plashwater Lock upstream. The workings of the river and the influence of the tides are described with great accuracy. Dickens opens the novel with this sketch of the river, and the people who work on it:
"In these times of ours, though concerning the exact year there is no need to be precise, a boat of dirty and disreputable appearance, with two figures in it, floated on the Thames, between Southwark Bridge which is of iron, and London Bridge which is of stone, as an autumn evening was closing in."
"The figures in this boat were those of a strong man with ragged grizzled hair and a sun-browned face, and a girl of nineteen or twenty. The girl rowed, pulling a pair of sculls very easily; the man with the rudder-lines slack in his hands, and his hands loose in his waisteband, kept an eager look-out."
Kenneth Grahame's "The Wind in the Willows", written in 1908, is set in the middle to upper reaches of the river. It starts as a tale of anthropomorphic characters "simply messing about in boats" but develops into a more complex story combining elements of mysticism with adventure and reflection on Edwardian Society. It is generally considered one of the most beloved works of children's literature and the illustrations by E.H.Shepard and Arthur Rackham feature the Thames and its surroundings.
The river almost inevitably features in many books set in London. Most of Dickens' other novels include some aspect of the Thames. "Oliver Twist" finishes in the slums and rookeries along its south bank. The Sherlock Holmes stories by Arthur Conan Doyle often visit riverside parts as in "The Sign of Four". In "Heart of Darkness" by Joseph Conrad, the serenity of the contemporary Thames is contrasted with the savagery of the Congo River, and with the wilderness of the Thames as it would have appeared to a Roman soldier posted to Britannia two thousand years before. Conrad also gives a description of the approach to London from the Thames Estuary in his essays "" (1906). Upriver, Henry James' "Portrait of a Lady" uses a large riverside mansion on the Thames as one of its key settings.
Literary non-fiction works include Samuel Pepys' diary, in which he recorded many events relating to the Thames including the Fire of London. He was disturbed while writing it in June 1667 by the sound of gunfire as Dutch warships broke through the Royal Navy on the Thames.
In poetry, William Wordsworth's sonnet On Westminster Bridge closes with the lines:
T. S. Eliot makes several references to the Thames in The Fire Sermon, Section III of "The Waste Land".
and
The "Sweet Thames" line is taken from Edmund Spenser's Prothalamion which presents a more idyllic image:
Also writing of the upper reaches is Matthew Arnold in "The Scholar Gypsy":
Wendy Cope's poem 'After the Lunch' is set on Waterloo Bridge, beginning:
Dylan Thomas mentions the Thames in his poem "A Refusal To Mourn The Death, By Fire, Of A Child In London". "London's Daughter", the subject of the poem, lays "Deep with the first dead...secret by the unmourning water of the riding Thames".
Science-fiction novels make liberal use of a futuristic Thames. The utopian "News from Nowhere" by William Morris is mainly the account of a journey through the Thames valley in a socialist future. The Thames also features prominently in Philip Pullman's "His Dark Materials" trilogy, as a communications artery for the waterborne Gyptian people of Oxford and the Fens.
In "The Deptford Mice" trilogy by Robin Jarvis, the Thames appears several times. In one book, rat characters swim through it to Deptford. Winner of the Nestlé Children's Book Prize Gold Award "I, Coriander", by Sally Gardner is a fantasy novel in which the heroine lives on the banks of the Thames.
Mark Wallington describes a journey up the Thames in a camping skiff, in his 1989 book "Boogie up the River" (ISBN 978-0-09-965910-5).
Music.
The Water Music composed by George Frideric Handel premiered on 17 July 1717, when King George I requested a concert on the River Thames. The concert was performed for King George I on his barge and he is said to have enjoyed it so much that he ordered the 50 exhausted musicians to play the suites three times on the trip.
The song 'Old Father Thames' was recorded by Peter Dawson at Abbey Road Studios in 1933 and by Gracie Fields five years later.
Jessie Matthews sings "My river" in the 1938 film "Sailing Along", and the tune is the centrepiece of a major dance number near the end of the film.
The Sex Pistols played a concert on the "Queen Elizabeth Riverboat" on 7 June 1977, the Queen's Silver Jubilee year, while sailing down the river.
The choral line "(I) "(liaised)" live by the river" in the song "London Calling" by The Clash refers to the River Thames.
Two songs by The Kinks feature the Thames as the setting of the first song's title and, for the second song, arguably in its mention of 'the river': "Waterloo Sunset" is about a couple's meetings on Waterloo Bridge, London and starts: "Dirty old river, must you keep rolling, flowing into the night?" and continues "Terry meets Julie, Waterloo station" and "...but Terry and Julie cross over the river where they feel safe and sound...". "See My Friends" continually refers to the singer's friends "playing 'cross the river" instead of the girl who "just left". Furthermore, Ray Davies as a solo artist refers to the river Thames in his "London Song".
Ewan MacColl's "Sweet Thames, Flow Softly", written in the early 1960s, is a tragic love ballad set on trip up the river (see Edmund Spencer's love poem's refrain above)
English musician Imogen Heap wrote a song from the point of view of the River Thames entitled "You Know Where To Find Me". The song was released in 2012 on 18 October as the sixth single from her currently untitled album.
Major flood events.
London flood of 1928.
The 1928 Thames flood was a disastrous flood of the River Thames that affected much of riverside London on 7 January 1928, as well as places further downriver. Fourteen people were drowned in London and thousands were made homeless when flood waters poured over the top of the Thames Embankment and part of the Chelsea Embankment collapsed. It was the last major flood to affect central London, and, particularly following the disastrous North Sea flood of 1953, helped lead to the implementation of new flood-control measures that culminated in the construction of the Thames Barrier in the 1970s.
Thames Valley flood of 1947.
The 1947 Thames flood was worst overall 20th century flood of the River Thames, affecting much of the Thames Valley as well as elsewhere in England during the middle of March 1947 after a severe winter.
The floods were caused by 117 mm (4.6 inches) of rainfall (including snow); the peak flow was 61.7 billion litres of water per day and the damage cost a total of £12 million to repair.
War damage to some of the locks made matters worse.
Other significant Thames floods since 1947 have occurred in 1968, 1993, 1998, 2000, 2003, 2006 and 2014.
Canvey Island flood of 1953.
On the night of 31 January, the North Sea flood of 1953 devastated the island taking the lives of 58 islanders, and led to the temporary evacuation of the 13,000 residents. Canvey is consequently protected by modern sea defences comprising 15 mi of concrete seawall. Many of the victims were in the holiday bungalows of the eastern Newlands estate and perished as the water reached ceiling level. The small village area of the island is approximately two feet above sea level and consequently escaped the effects of the flood.

</doc>
<doc id="49033" url="http://en.wikipedia.org/wiki?curid=49033" title="Epigenetics">
Epigenetics

In genetics, epigenetics is the study of cellular and physiological trait variations that are "not" caused by changes in the DNA sequence; epigenetics describes the study of dynamic alterations in the transcriptional potential of a cell. These alterations may or may not be heritable, although the use of the term "epigenetic" to describe processes that are not heritable is controversial. Unlike genetics based on changes to the DNA sequence (the genotype), the changes in gene expression or cellular phenotype of epigenetics have other causes, thus use of the prefix "epi-" (Greek: "επί"- over, outside of, around).
The term also refers to the changes themselves: functionally relevant changes to the genome that do not involve a change in the nucleotide sequence. Examples of mechanisms that produce such changes are DNA methylation and histone modification, each of which alters how genes are expressed without altering the underlying DNA sequence. Gene expression can be controlled through the action of repressor proteins that attach to silencer regions of the DNA. These epigenetic changes may last through cell divisions for the duration of the cell's life, and may also last for multiple generations even though they do not involve changes in the underlying DNA sequence of the organism; instead, non-genetic factors cause the organism's genes to behave (or "express themselves") differently.
One example of an epigenetic change in eukaryotic biology is the process of cellular differentiation. During morphogenesis, totipotent stem cells become the various pluripotent cell lines of the embryo, which in turn become fully differentiated cells. In other words, as a single fertilized egg cell – the zygote – continues to divide, the resulting daughter cells change into all the different cell types in an organism, including neurons, muscle cells, epithelium, endothelium of blood vessels, etc., by activating some genes while inhibiting the expression of others.
Definitions of term.
Historical usage.
"Epigenetics" (as in "epigenetic landscape") was coined by C. H. Waddington in 1942 as a portmanteau of the words "epigenesis" and "genetics". "Epigenesis" is an old word that has more recently been used (see "preformationism" for historical background) to describe the differentiation of cells from their initial totipotent state in embryonic development.
When Waddington coined the term the physical nature of genes and their role in heredity was not known; he used it as a conceptual model of how genes might interact with their surroundings to produce a phenotype; he used the phrase "epigenetic landscape" as a metaphor for biological development. Waddington held that cell fates were established in development much like a marble rolls down to the point of lowest local elevation.
Waddington suggested visualising increasing irreversibility of cell type differentiation as ridges rising between the valleys where the marbles (cells) are travelling. In recent times Waddington's notion of the epigenetic landscape has been rigorously formalized in the context of the systems dynamics state approach to the study of cell-fate. Cell-fate determination is predicted to exhibit certain dynamics, such as attractor-convergence (the attractor can be an equilibrium point, limit cycle or strange attractor) or oscillatory.
The term "epigenetics" has also been used in developmental psychology to describe psychological development as the result of an ongoing, bi-directional interchange between heredity and the environment. Interactivist ideas of development have been discussed in various forms and under various names throughout the 19th and 20th centuries. An early version was proposed, among the founding statements in embryology, by Karl Ernst von Baer and popularized by Ernst Haeckel. A radical epigenetic view (physiological epigenesis) was developed by Paul Wintrebert. Another variation, probabilistic epigenesis, was presented by Gilbert Gottlieb in 2003. This view encompasses all of the possible developing factors on an organism and how they not only influence the organism and each other, but how the organism also influences its own development.
The developmental psychologist Erik Erikson used the term epigenetic principle in his book "Identity: Youth and Crisis" (1968), and used it to encompass the notion that we develop through an unfolding of our personality in predetermined stages, and that our environment and surrounding culture influence how we progress through these stages. This biological unfolding in relation to our socio-cultural settings is done in stages of psychosocial development, where "progress through each stage is in part determined by our success, or lack of success, in all the previous stages."
Contemporary usage.
Robin Holliday defined epigenetics as "the study of the mechanisms of temporal and spatial control of gene activity during the development of complex organisms." Thus "epigenetic" can be used to describe anything other than DNA sequence that influences the development of an organism.
The more recent usage of the word in science has a stricter definition. It is, as defined by Arthur Riggs and colleagues, "the study of mitotically and/or meiotically heritable changes in gene function that cannot be explained by changes in DNA sequence." The Greek prefix "epi-" in "epigenetics" implies features that are "on top of" or "in addition to" genetics; thus "epigenetic" traits exist on top of or in addition to the traditional molecular basis for inheritance.
The term "epigenetics", however, has been used to describe processes which have not been demonstrated to be heritable such as histone modification; there are therefore attempts to redefine it in broader terms that would avoid the constraints of requiring heritability. For example, Sir Adrian Bird defined epigenetics as "the structural adaptation of chromosomal regions so as to register, signal or perpetuate altered activity states." This definition would be inclusive of transient modifications associated with DNA repair or cell-cycle phases as well as stable changes maintained across multiple cell generations, but exclude others such as templating of membrane architecture and prions unless they impinge on chromosome function. Such redefinitions however are not universally accepted and are still subject to dispute. The NIH "Roadmap Epigenomics Project," ongoing as of 2013, uses the following definition: "...For purposes of this program, epigenetics refers to both heritable changes in gene activity and expression (in the progeny of cells or of individuals) and also stable, long-term alterations in the transcriptional potential of a cell that are not necessarily heritable."
In 2008, a consensus definition of the epigenetic trait, "stably heritable phenotype resulting from changes in a chromosome without alterations in the DNA sequence", was made at a Cold Spring Harbor meeting.
The similarity of the word to "genetics" has generated many parallel usages. The "epigenome" is a parallel to the word "genome", referring to the overall epigenetic state of a cell, and epigenomics refers to more global analyses of epigenetic changes across the entire genome. The phrase "genetic code" has also been adapted—the "epigenetic code" has been used to describe the set of epigenetic features that create different phenotypes in different cells. Taken to its extreme, the "epigenetic code" could represent the total state of the cell, with the position of each molecule accounted for in an "epigenomic map", a diagrammatic representation of the gene expression, DNA methylation and histone modification status of a particular genomic region. More typically, the term is used in reference to systematic efforts to measure specific, relevant forms of epigenetic information such as the histone code or DNA methylation patterns.
Molecular basis.
Epigenetic changes can modify the activation of certain genes, but not the sequence of DNA. Additionally, the chromatin proteins associated with DNA may be activated or silenced. This is why the differentiated cells in a multicellular organism express only the genes that are necessary for their own activity. Epigenetic changes are preserved when cells divide. Most epigenetic changes only occur within the course of one individual organism's lifetime, but, if gene inactivation occurs in a sperm or egg cell that results in fertilization, then some epigenetic changes can be transferred to the next generation. This raises the question of whether or not epigenetic changes in an organism can alter the basic structure of its DNA (see Evolution, below), a form of Lamarckism.
Specific epigenetic processes include paramutation, bookmarking, imprinting, gene silencing, X chromosome inactivation, position effect, reprogramming, transvection, maternal effects, the progress of carcinogenesis, many effects of teratogens, regulation of histone modifications and heterochromatin, and technical limitations affecting parthenogenesis and cloning.
DNA damage can also cause epigenetic changes. DNA damages are very frequent, occurring on average about 10,000 times a day per cell of the human body (see DNA damage (naturally occurring)). These damages are largely repaired, but at the site of a DNA repair, epigenetic changes can remain. In particular, a double strand break in DNA can initiate unprogrammed epigenetic gene silencing both by causing DNA methylation as well as by promoting silencing types of histone modifications (chromatin remodeling) (see next section). In addition, the enzyme Parp1 (poly(ADP)-ribose polymerase) and its product poly(ADP)-ribose (PAR) accumulate at sites of DNA damage as part of a repair process. This accumulation, in turn, directs recruitment and activation of the chromatin remodeling protein ALC1 that can cause nucleosome remodeling. Nucleosome remodeling has been found to cause, for instance, epigenetic silencing of DNA repair gene MLH1. DNA damaging chemicals, such as benzene, hydroquinone, styrene, carbon tetrachloride and trichloroethylene, cause considerable hypomethylation of DNA, some through the activation of oxidative stress pathways.
Foods are known to alter the epigenetics of rats on different diets. Some food components epigenetically increase the levels of DNA repair enzymes such as MGMT and MLH1 and p53. Other food components can reduce DNA damage, such as soy isoflavones and bilberry anthocyanins.
Epigenetic research uses a wide range of molecular biologic techniques to further our understanding of epigenetic phenomena, including chromatin immunoprecipitation (together with its large-scale variants ChIP-on-chip and ChIP-Seq), fluorescent in situ hybridization, methylation-sensitive restriction enzymes, DNA adenine methyltransferase identification (DamID) and bisulfite sequencing. Furthermore, the use of bioinformatic methods is playing an increasing role (computational epigenetics).
Computer simulations and molecular dynamics approaches revealed the atomistic motions associated with the molecular recognition of the histone tail through an allosteric mechanism.
Mechanisms.
Several types of epigenetic inheritance systems may play a role in what has become known as cell memory, note however that not all of these are universally accepted to be examples of epigenetics.
Covalent modifications of DNA and histones.
Covalent modifications of DNA (e.g. cytosine methylation and hydroxymethylation) and histone proteins (e.g. lysine acetylation, lysine and arginine methylation, serine and threonine phosphorylation, and lysine ubiquitination and sumoylation) play central roles in many types of epigenetic inheritance. Therefore, the word "epigenetics" is sometimes used as a synonym for these processes. However, this can be misleading. Chromatin remodeling is not always inherited, and not all epigenetic inheritance involves chromatin remodeling.
Because the phenotype of a cell or individual is affected by which of its genes are transcribed, heritable transcription states can give rise to epigenetic effects. There are several layers of regulation of gene expression. One way that genes are regulated is through the remodeling of chromatin. Chromatin is the complex of DNA and the histone proteins with which it associates. If the way that DNA is wrapped around the histones changes, gene expression can change as well. Chromatin remodeling is accomplished through two main mechanisms:
Mechanisms of heritability of histone state are not well understood; however, much is known about the mechanism of heritability of DNA methylation state during cell division and differentiation. Heritability of methylation state depends on certain enzymes (such as DNMT1) that have a higher affinity for 5-methylcytosine than for cytosine. If this enzyme reaches a "hemimethylated" portion of DNA (where 5-methylcytosine is in only one of the two DNA strands) the enzyme will methylate the other half.
Although histone modifications occur throughout the entire sequence, the unstructured N-termini of histones (called histone tails) are particularly highly modified. These modifications include acetylation, methylation, ubiquitylation, phosphorylation, sumoylation, ribosylation and citrullination. Acetylation is the most highly studied of these modifications. For example, acetylation of the K14 and K9 lysines of the tail of histone H3 by histone acetyltransferase enzymes (HATs) is generally related to transcriptional competence.
One mode of thinking is that this tendency of acetylation to be associated with "active" transcription is biophysical in nature. Because it normally has a positively charged nitrogen at its end, lysine can bind the negatively charged phosphates of the DNA backbone. The acetylation event converts the positively charged amine group on the side chain into a neutral amide linkage. This removes the positive charge, thus loosening the DNA from the histone. When this occurs, complexes like SWI/SNF and other transcriptional factors can bind to the DNA and allow transcription to occur. This is the "cis" model of epigenetic function. In other words, changes to the histone tails have a direct effect on the DNA itself.
Another model of epigenetic function is the "trans" model. In this model, changes to the histone tails act indirectly on the DNA. For example, lysine acetylation may create a binding site for chromatin-modifying enzymes (or transcription machinery as well). This chromatin remodeler can then cause changes to the state of the chromatin. Indeed, a bromodomain — a protein domain that specifically binds acetyl-lysine — is found in many enzymes that help activate transcription, including the SWI/SNF complex. It may be that acetylation acts in this and the previous way to aid in transcriptional activation.
The idea that modifications act as docking modules for related factors is borne out by histone methylation as well. Methylation of lysine 9 of histone H3 has long been associated with constitutively transcriptionally silent chromatin (constitutive heterochromatin). It has been determined that a chromodomain (a domain that specifically binds methyl-lysine) in the transcriptionally repressive protein HP1 recruits HP1 to K9 methylated regions. One example that seems to refute this biophysical model for methylation is that tri-methylation of histone H3 at lysine 4 is strongly associated with (and required for full) transcriptional activation. Tri-methylation in this case would introduce a fixed positive charge on the tail.
It has been shown that the histone lysine methyltransferase (KMT) is responsible for this methylation activity in the pattern of histones H3 & H4. This enzyme utilizes a catalytically active site called the SET domain (Suppressor of variegation, Enhancer of zeste, Trithorax). The SET domain is a 130-amino acid sequence involved in modulating gene activities. This domain has been demonstrated to bind to the histone tail and causes the methylation of the histone.
Differing histone modifications are likely to function in differing ways; acetylation at one position is likely to function differently from acetylation at another position. Also, multiple modifications may occur at the same time, and these modifications may work together to change the behavior of the nucleosome. The idea that multiple dynamic modifications regulate gene transcription in a systematic and reproducible way is called the histone code, although the idea that histone state can be read linearly as a digital information carrier has been largely debunked. One of the best-understood systems that orchestrates chromatin-based silencing is the SIR protein based silencing of the yeast hidden mating type loci HML and HMR.
DNA methylation frequently occurs in repeated sequences, and helps to suppress the expression and mobility of 'transposable elements': Because 5-methylcytosine can be spontaneously deaminated (replacing nitrogen by oxygen) to thymidine, CpG sites are frequently mutated and become rare in the genome, except at CpG islands where they remain unmethylated. Epigenetic changes of this type thus have the potential to direct increased frequencies of permanent genetic mutation. DNA methylation patterns are known to be established and modified in response to environmental factors by a complex interplay of at least three independent DNA methyltransferases, DNMT1, DNMT3A, and DNMT3B, the loss of any of which is lethal in mice. DNMT1 is the most abundant methyltransferase in somatic cells, localizes to replication foci, has a 10–40-fold preference for hemimethylated DNA and interacts with the proliferating cell nuclear antigen (PCNA).
By preferentially modifying hemimethylated DNA, DNMT1 transfers patterns of methylation to a newly synthesized strand after DNA replication, and therefore is often referred to as the ‘maintenance' methyltransferase. DNMT1 is essential for proper embryonic development, imprinting and X-inactivation. To emphasize the difference of this molecular mechanism of inheritance from the canonical Watson-Crick base-pairing mechanism of transmission of genetic information, the term 'Epigenetic templating' was introduced. Furthermore, in addition to the maintenance and transmission of methylated DNA states, the same principle could work in the maintenance and transmission of histone modifications and even cytoplasmic (structural) heritable states.
Histones H3 and H4 can also be manipulated through demethylation using histone lysine demethylase (KDM). This recently identified enzyme has a catalytically active site called the Jumonji domain (JmjC). The demethylation occurs when JmjC utilizes multiple cofactors to hydroxylate the methyl group, thereby removing it. JmjC is capable of demethylating mono-, di-, and tri-methylated substrates.
Chromosomal regions can adopt stable and heritable alternative states resulting in bistable gene expression without changes to the DNA sequence. Epigenetic control is often associated with alternative covalent modifications of histones. The stability and heritability of states of larger chromosomal regions are suggested to involve positive feedback where modified nucleosomes recruit enzymes that similarly modify nearby nucleosomes. A simplified stochastic model for this type of epigenetics is found here.
It has been suggested that chromatin-based transcriptional regulation could be mediated by the effect of small RNAs. Small interfering RNAs can modulate transcriptional gene expression via epigenetic modulation of targeted promoters.
RNA transcripts and their encoded proteins.
Sometimes a gene, after being turned on, transcribes a product that (directly or indirectly) maintains the activity of that gene. For example, Hnf4 and MyoD enhance the transcription of many liver- and muscle-specific genes, respectively, including their own, through the transcription factor activity of the proteins they encode. RNA signalling includes differential recruitment of a hierarchy of generic chromatin modifying complexes and DNA methyltransferases to specific loci by RNAs during differentiation and development. Other epigenetic changes are mediated by the production of different splice forms of RNA, or by formation of double-stranded RNA (RNAi). Descendants of the cell in which the gene was turned on will inherit this activity, even if the original stimulus for gene-activation is no longer present. These genes are often turned on or off by signal transduction, although in some systems where syncytia or gap junctions are important, RNA may spread directly to other cells or nuclei by diffusion. A large amount of RNA and protein is contributed to the zygote by the mother during oogenesis or via nurse cells, resulting in maternal effect phenotypes. A smaller quantity of sperm RNA is transmitted from the father, but there is recent evidence that this epigenetic information can lead to visible changes in several generations of offspring.
MicroRNAs.
MicroRNAs (miRNAs) are members of non-coding RNAs that range in size from 17 to 25 nucleotides. miRNAs regulate a large variety of biological functions in plants and animals. So far, in 2013, about 2000 miRNAs have been discovered in humans and these can be found online in an miRNA database. Each miRNA expressed in a cell may target about 100 to 200 messenger RNAs that it downregulates. Most of the downregulation of mRNAs occurs by causing the decay of the targeted mRNA, while some downregulation occurs at the level of translation into protein.
It appears that about 60% of human protein coding genes are regulated by miRNAs. Many miRNAs are epigenetically regulated. About 50% of miRNA genes are associated with CpG islands, that may be repressed by epigenetic methylation. Transcription from methylated CpG islands is strongly and heritably repressed. Other miRNAs are epigenetically regulated by either histone modifications or by combined DNA methylation and histone modification.
mRNA.
In 2011, it was demonstrated that the methylation of mRNA plays a critical role in human energy homeostasis. The obesity-associated FTO gene is shown to be able to demethylate N6-methyladenosine in RNA.
sRNAs.
sRNAs are small (50–250 nucleotides), highly structured, non-coding RNA fragments found in bacteria. They control gene expression including virulence genes in pathogens and are viewed as new targets in the fight against drug-resistant bacteria. They play an important role in many biological processes, binding to mRNA and protein targets in prokaryotes. Their phylogenetic analyses, for example through sRNA–mRNA target interactions or protein binding properties, are used to build comprehensive databases. sRNA-gene maps based on their targets in microbial genomes are also constructed.
Prions.
Prions are infectious forms of proteins. In general, proteins fold into discrete units that perform distinct cellular functions, but some proteins are also capable of forming an infectious conformational state known as a prion. Although often viewed in the context of infectious disease, prions are more loosely defined by their ability to catalytically convert other native state versions of the same protein to an infectious conformational state. It is in this latter sense that they can be viewed as epigenetic agents capable of inducing a phenotypic change without a modification of the genome.
Fungal prions are considered by some to be epigenetic because the infectious phenotype caused by the prion can be inherited without modification of the genome. PSI+ and URE3, discovered in yeast in 1965 and 1971, are the two best studied of this type of prion. Prions can have a phenotypic effect through the sequestration of protein in aggregates, thereby reducing that protein's activity. In PSI+ cells, the loss of the Sup35 protein (which is involved in termination of translation) causes ribosomes to have a higher rate of read-through of stop codons, an effect that results in suppression of nonsense mutations in other genes. The ability of Sup35 to form prions may be a conserved trait. It could confer an adaptive advantage by giving cells the ability to switch into a PSI+ state and express dormant genetic features normally terminated by stop codon mutations.
Structural inheritance systems.
In ciliates such as "Tetrahymena" and "Paramecium", genetically identical cells show heritable differences in the patterns of ciliary rows on their cell surface. Experimentally altered patterns can be transmitted to daughter cells. It seems existing structures act as templates for new structures. The mechanisms of such inheritance are unclear, but reasons exist to assume that multicellular organisms also use existing cell structures to assemble new ones.
Nucleosome positioning.
Eukaryotic genomes are packed with the help of nucleosomes. Nucleosome positions are not random, and determine the accessibility of DNA to regulatory proteins. This determines differences in gene expression and cell differentiation. It has been shown that at least some nucleosomes are retained in sperm cells (where most but not all histones are replaced by protamines). Thus nucleosome positioning is to some degree inheritable. Recent studies have uncovered connections between nucleosome positioning and other epigenetic factors, such as DNA methylation and hydroxymethylation 
Functions and consequences.
Development.
Somatic epigenetic inheritance, particularly through DNA and histone covalent modifications and nucleosome repositioning, is very important in the development of multicellular eukaryotic organisms. The genome sequence is static (with some notable exceptions), but cells differentiate into many different types, which perform different functions, and respond differently to the environment and intercellular signalling. Thus, as individuals develop, morphogens activate or silence genes in an epigenetically heritable fashion, giving cells a memory. In mammals, most cells terminally differentiate, with only stem cells retaining the ability to differentiate into several cell types ("totipotency" and "multipotency"). In mammals, some stem cells continue producing new differentiated cells throughout life, such as in neurogenesis, but mammals are not able to respond to loss of some tissues, for example, the inability to regenerate limbs, which some other animals are capable of. Epigenetic modifications regulate the transition from neural neural stem cells to glial progenitor cells (for example, differentiation into oligodendrocytes is regulated by the deacetylation and methylation of histones. Unlike animals, plant cells do not terminally differentiate, remaining totipotent with the ability to give rise to a new individual plant. While plants do utilise many of the same epigenetic mechanisms as animals, such as chromatin remodeling, it has been hypothesised that some kinds of plant cells do not use or require "cellular memories", resetting their gene expression patterns using positional information from the environment and surrounding cells to determine their fate.
Epigenetics can be divided into predetermined and probabilistic epigenesis. Predetermined epigenesis is a unidirectional movement from structural development in DNA to the functional maturation of the protein. "Predetermined" here means that development is scripted and predictable. Probabilistic epigenesis on the other hand is a bidirectional structure-function development with experiences and external molding development.
Medicine.
Epigenetics has many and varied potential medical applications as it tends to be multidimensional in nature.
Congenital genetic disease is well understood and it is clear that epigenetics can play a role, for example, in the case of Angelman syndrome and Prader-Willi syndrome. These are normal genetic diseases caused by gene deletions or inactivation of the genes, but are unusually common because individuals are essentially hemizygous because of genomic imprinting, and therefore a single gene knock out is sufficient to cause the disease, where most cases would require both copies to be knocked out.
Evolution.
Epigenetics can impact evolution when epigenetic changes are heritable. A sequestered germ line or Weismann barrier is specific to animals, and epigenetic inheritance is more common in plants and microbes. Eva Jablonka and Marion Lamb have argued that these effects may require enhancements to the standard conceptual framework of the modern evolutionary synthesis. Other evolutionary biologists have incorporated epigenetic inheritance into population genetics models or are openly skeptical.
Two important ways in which epigenetic inheritance can be different from traditional genetic inheritance, with important consequences for evolution, are that rates of epimutation can be much faster than rates of mutation and the epimutations are more easily reversible. In plants heritable DNA methylation mutations are 100.000 times more likely to occur compared to DNA mutations. 
An epigenetically inherited element such as the PSI+ system can act as a "stop-gap", good enough for short-term adaptation that allows the lineage to survive for long enough for mutation and/or recombination to genetically assimilate the adaptive phenotypic change. The existence of this possibility increases the evolvability of a species.
Current research findings and examples of effects.
Epigenetic changes have been observed to occur in response to environmental exposure—for example, mice given some dietary supplements have epigenetic changes affecting expression of the agouti gene, which affects their fur color, weight, and propensity to develop cancer.
One study indicates that traumatic experiences can produce fearful memories which are passed to future generations via epigenetics. A study carried out on mice in 2013 found that mice could produce offspring which had an aversion to certain items which had been the source of negative experiences for their ancestors. Reports stated that:
 For the study, author Brian Dias and co-author Kerry Ressler trained mice, using foot shocks, to fear an odour that resembles cherry blossoms. Later, they tested the extent to which the animals' offspring startled when exposed to the same smell. The younger generation had not even been conceived when their fathers underwent the training, and had never smelt the odour before the experiment.
The offspring of trained mice were "able to detect and respond to far less amounts of odour... suggesting they are more sensitive" to it, Ressler told AFP of the findings published in the journal Nature Neuroscience. They did not react the same way to other odours, and compared to the offspring of non-trained mice, their reaction to the cherry blossom whiff was about 200 percent stronger, he said.
The scientists then looked at a gene, M71, that governs the functioning of an odour receptor in the nose that responds specifically to the cherry blossom smell. The gene, inherited through the sperm of trained mice, had undergone no change to its DNA encoding, the team found. But the gene did carry epigenetic marks that could alter its behaviour and cause it to be "expressed more" in descendants, said Dias. This in turn caused a physical change in the brains of the trained mice, their sons and grandsons, who all had a larger glomerulus—a section in the olfactory (smell) unit of the brain. 
This study received several criticisms. In a paper in GENETICS, author Gregory Francis cited the study's low statistical power as evidence of some irregularity such as bias in reporting results. Due to limits of sample size, there is a probability that an effect will not be demonstrated to within statistical significance even if it exists. Francis calculated the probability that all the experiments reported would show positive results if an identical protocol was followed, assuming the claimed effects exist, to be 0.4%. Another criticism was that the authors did not indicate which mice were siblings, and treated all of the mice as statistically independent. Dias and Ressler responded by pointing out negative results in the paper's appendix that Francis did not use in his calculations, and by saying they would track which mice were siblings in the future.
In the case of humans with different environmental exposures, monozygotic (identical) twins were epigenetically indistinguishable during their early years, while older twins had remarkable differences in the overall content and genomic distribution of 5-methylcytosine DNA and histone acetylation. The twin pairs who had spent less of their lifetime together and/or had greater differences in their medical histories were those who showed the largest differences in their levels of 5-methylcytosine DNA and acetylation of histones H3 and H4.
More than 100 cases of transgenerational epigenetic inheritance phenomena have been reported in a wide range of organisms, including prokaryotes, plants, and animals. For instance, Mourning Cloak butterflies will change color through hormone changes in response to experimentation of varying temperatures.
Recent analyses have suggested that members of the APOBEC/AID family of cytosine deaminases are capable of simultaneously mediating genetic and epigenetic inheritance using similar molecular mechanisms.
Epigenetic effects in humans.
Genomic imprinting and related disorders.
Some human disorders are associated with genomic imprinting, a phenomenon in mammals where the father and mother contribute different epigenetic patterns for specific genomic loci in their germ cells. The best-known case of imprinting in human disorders is that of Angelman syndrome and Prader-Willi syndrome—both can be produced by the same genetic mutation, chromosome 15q partial deletion, and the particular syndrome that will develop depends on whether the mutation is inherited from the child's mother or from their father. This is due to the presence of genomic imprinting in the region. Beckwith-Wiedemann syndrome is also associated with genomic imprinting, often caused by abnormalities in maternal genomic imprinting of a region on chromosome 11.
Rett syndrome is underlied by mutations in the MECP2 gene despite no large-scale changes in expression of MeCP2 being found in microarray analyses. BDNF is downregulated in the MECP2 mutant resulting in Rett syndrome.
Transgenerational epigenetic observations.
In the Överkalix study, Marcus Pembrey and colleagues observed that the paternal (but not maternal) grandsons of Swedish men who were exposed during preadolescence to famine in the 19th century were less likely to die of cardiovascular disease. If food was plentiful, then diabetes mortality in the grandchildren increased, suggesting that this was a transgenerational epigenetic inheritance. The opposite effect was observed for females—the paternal (but not maternal) granddaughters of women who experienced famine while in the womb (and therefore while their eggs were being formed) lived shorter lives on average.
Cancer and developmental abnormalities.
A variety of compounds are considered as epigenetic carcinogens—they result in an increased incidence of tumors, but they do not show mutagen activity (toxic compounds or pathogens that cause tumors incident to increased regeneration should also be excluded). Examples include diethylstilbestrol, arsenite, hexachlorobenzene, and nickel compounds.
Many teratogens exert specific effects on the fetus by epigenetic mechanisms. While epigenetic effects may preserve the effect of a teratogen such as diethylstilbestrol throughout the life of an affected child, the possibility of birth defects resulting from exposure of fathers or in second and succeeding generations of offspring has generally been rejected on theoretical grounds and for lack of evidence. However, a range of male-mediated abnormalities have been demonstrated, and more are likely to exist. FDA label information for Vidaza, a formulation of 5-azacitidine (an unmethylatable analog of cytidine that causes hypomethylation when incorporated into DNA) states that "men should be advised not to father a child" while using the drug, citing evidence in treated male mice of reduced fertility, increased embryo loss, and abnormal embryo development. In rats, endocrine differences were observed in offspring of males exposed to morphine. In mice, second generation effects of diethylstilbesterol have been described occurring by epigenetic mechanisms.
Recent studies have shown that the mixed-lineage leukemia (MLL) gene causes leukemia by rearranging and fusing with other genes in different chromosomes, which is a process under epigenetic control.
Other investigations have concluded that alterations in histone acetylation and DNA methylation occur in various genes influencing prostate cancer. Gene expression in the prostate can be modulated by nutrition and lifestyle changes.
In 2008, the National Institutes of Health announced that $190 million had been earmarked for epigenetics research over the next five years. In announcing the funding, government officials noted that epigenetics has the potential to explain mechanisms of aging, human development, and the origins of cancer, heart disease, mental illness, as well as several other conditions. Some investigators, like Randy Jirtle, PhD, of Duke University Medical Center, think epigenetics may ultimately turn out to have a greater role in disease than genetics.
DNA methylation in cancer.
DNA methylation is an important regulator of gene transcription and a large body of evidence has demonstrated that aberrant DNA methylation is associated with unscheduled gene silencing, and the genes with high levels of 5-methylcytosine in their promoter region are transcriptionally silent. DNA methylation is essential during embryonic development, and in somatic cells, patterns of DNA methylation are in general transmitted to daughter cells with a high fidelity. Aberrant DNA methylation patterns have been associated with a large number of human malignancies and found in two distinct forms: hypermethylation and hypomethylation compared to normal tissue. Hypermethylation is one of the major epigenetic modifications that repress transcription via promoter region of tumour suppressor genes. Hypermethylation typically occurs at CpG islands in the promoter region and is associated with gene inactivation. Global hypomethylation has also been implicated in the development and progression of cancer through different mechanisms.
DNA repair epigenetics in cancer.
Germ line (familial) mutations have been identified in 34 different DNA repair genes that cause a high risk of cancer, including, for example BRCA1 and ATM. These are listed in the article DNA repair-deficiency disorder. However, cancers caused by such germ line mutations make up only a very small proportion of cancers. For instance, germ line mutations cause only 2% to 5% of colon cancer cases.
Epigenetic reductions in expression of DNA repair genes, however, are very frequent in sporadic (non-germ line) cancers, as shown among some representative cancers in the table in this section, while mutations in DNA repair genes in sporadic cancer are very rare.
Deficiencies in expression of DNA repair genes cause increased mutation rates. Mutations rates increase in mice defective for mismatch DNA repair genes PMS2, MLH1, MSH2, MSH3 or MSH6 or for DNA repair gene BRCA2, while chromosomal rearrangements and aneuploidy are noted to increase in humans defective in DNA repair gene BLM. Thus, deficiency in DNA repair causes genome instability and this genome instability is likely the main underlying cause of the genetic alterations leading to cancer. In fact, the first event in many sporadic neoplasias is a heritable alteration that affects genetic instability and epigenetic defects in DNA repair are somatically heritable.
Variant histones H2A in cancer.
The histone variants of the H2A family are highly conserved in mammals, playing critical roles in regulating many nuclear processes by altering chromatin structure. One of the key H2A variants, H2A.X, marks DNA damage, facilitating the recruitment of DNA repair proteins to restore genomic integrity. Another variant, H2A.Z, plays an important role in both gene activation and repression. A high level of H2A.Z expression is ubiquitously detected in many cancers and is significantly associated with cellular proliferation and genomic instability. Histone variant macroH2A1 is important in the pathogenesis of many types of cancers, for instance in hepatocellular carcinoma.
Cancer treatment.
Current research has shown that epigenetic pharmaceuticals could be a replacement or adjuvant therapy for currently accepted treatment methods such as radiation and chemotherapy, or could enhance the effects of these current treatments. It has been shown that the epigenetic control of the proto-onco regions and the tumor suppressor sequences by conformational changes in histones directly affects the formation and progression of cancer. Epigenetics also has the factor of reversibility, a characteristic that other cancer treatments do not offer.
Drug development has focused mainly on histone acetyltransferase (HAT) and histone deacetylase (HDAC), and has included the introduction to the market of the new pharmaceutical vorinostat, an HDAC inhibitor. HDAC has been shown to play an integral role in the progression of oral squamous cancer.
Current front-runner candidates for new drug targets are histone lysine methyltransferases (KMT) and protein arginine methyltransferases (PRMT).
Twin studies.
Recent studies involving both dizygotic (fraternal) and monozygotic (identical) twins have produced some evidence of epigenetic influence in humans.
Direct comparisons between identical twins constitute the ideal experimental model for testing environmental epigenetics, because DNA sequence differences that would be abundant in a singleton-based study do not interfere with the analysis. Research has shown that a difference in the environment can produce long-term epigenetic effects, and different developmental monozygotic twin subtypes may be different with respect to their susceptibility to be discordant from an epigenetic point of view.
One of the first high-throughput studies of epigenetic differences between monozygotic twins focused in comparing global and locus-specific changes in DNA methylation and histone modifications in a sample of 40 monozygotic twin pairs. In this case, only healthy twin pairs were studied, but a wide range of ages was represented, between 3 and 74 years. One of the major conclusions from this study was that there is an age-dependent accumulation of epigenetic differences between the two siblings of twin pairs. This accumulation suggests the existence of epigenetic “drift”.
A more recent study, where 114 monozygotic twins and 80 dizygotic twins were analyzed for the DNA methylation status of around 6000 unique genomic regions, concluded that epigenetic similarity at the time of blastocyst splitting may also contribute to phenotypic similarities in monozygotic co-twins. This supports the notion that microenvironment at early stages of embryonic development can be quite important for the establishment of epigenetic marks.
Epigenetics in microorganisms.
Bacteria make widespread use of postreplicative DNA methylation for the epigenetic control of DNA-protein interactions. Bacteria make use of DNA adenine methylation (rather than DNA cytosine methylation) as an epigenetic signal. DNA adenine methylation is important in bacteria virulence in organisms such as "Escherichia coli", "Salmonella, Vibrio, Yersinia, Haemophilus", and "Brucella". In "Alphaproteobacteria", methylation of adenine regulates the cell cycle and couples gene transcription to DNA replication. In "Gammaproteobacteria", adenine methylation provides signals for DNA replication, chromosome segregation, mismatch repair, packaging of bacteriophage, transposase activity and regulation of gene expression.
The filamentous fungus "Neurospora crassa" is a prominent model system for understanding the control and function of cytosine methylation. In this organisms, DNA methylation is associated with relics of a genome defense system called RIP (repeat-induced point mutation) and silences gene expression by inhibiting transcription elongation.
The yeast prion PSI is generated by a conformational change of a translation termination factor, which is then inherited by daughter cells. This can provide a survival advantage under adverse conditions. This is an example of epigenetic regulation enabling unicellular organisms to respond rapidly to environmental stress. Prions can be viewed as epigenetic agents capable of inducing a phenotypic change without modification of the genome.
Direct detection of epigenetic marks in microorganisms is possible with single molecule real time sequencing, in which polymerase sensitivity allows for measuring methylation and other modifications as a DNA molecule is being sequenced. Several projects have demonstrated the ability to collect genome-wide epigenetic data in bacteria.

</doc>
<doc id="49035" url="http://en.wikipedia.org/wiki?curid=49035" title="Hereford">
Hereford

Hereford () is a cathedral city, civil parish and county town of Herefordshire, England. It lies on the River Wye, approximately 16 mi east of the border with Wales, 24 mi southwest of Worcester, and 23 mi northwest of Gloucester. With a population of 53,516 people, it is the largest settlement in the county.
The name "Hereford" is said to come from the Anglo-Saxon "here", an army or formation of soldiers, and the "ford", a place for crossing a river. If this is the origin it suggests that Hereford was a place where a body of armed men forded or crossed the Wye. The Welsh name for Hereford is "Henffordd", meaning "old road", and probably refers to the Roman road and Roman settlement at nearby Stretton Sugwas.
An early town charter from 1189 granted by Richard I of England describes it as "Hereford in Wales". Hereford has been recognised as a city since time immemorial, with the status being reconfirmed as recently as October 2000.
It is now known chiefly as a trading centre for a wider agricultural and rural area. Products from Hereford include: cider, beer, leather goods, nickel alloys, poultry, chemicals, and cattle, including the famous Hereford breed.
History.
Hereford became the seat of Putta, Bishop of Hereford, some time between AD 676 and 688, after which the settlement continued to grow due to its proximity to the border between Mercia and Wales, becoming the Saxon capital of West Mercia by the beginning of the 8th century.
Hostilities between the Anglo-Saxons and the Welsh came to a head with the Battle of Hereford in 760, in which the Britons freed themselves from the influence of the English. Hereford was again targeted by the Welsh during their conflict with the Anglo-Saxon King Edward the Confessor in AD 1056 when, supported by Viking allies, Gruffydd ap Llywelyn, King of Gwynedd and Powys, marched on the town and put it to the torch before returning home in triumph. Hereford had the only mint west of the Severn in the reign of Athelstan (924–39), and it was to Hereford, then a border town, that Athelstan summoned the leading Welsh princes.
The present Hereford Cathedral dates from the early 12th century, as does the first bridge across the Wye. Former Bishops of Hereford include Saint Thomas de Cantilupe and Lord High Treasurer of England Thomas Charlton.
The city gave its name to two suburbs of Paris, France: Maisons-Alfort (population 54,600) and Alfortville (population 36,232), due to a manor built there by Peter of Aigueblanche, Bishop of Hereford, in the middle of the 13th century.
Hereford, a base for successive holders of the title Earl of Hereford, was once the site of a castle, Hereford Castle, which rivalled that of Windsor in size and scale. This was a base for repelling Welsh attacks and a secure stronghold for English kings such as King Henry IV when on campaign in the Welsh Marches against Owain Glyndŵr. The castle was dismantled in the 18th century and landscaped into Castle Green.
After the Battle of Mortimer's Cross in 1461, during the Wars of the Roses, the defeated Lancastrian leader Owen Tudor (grandfather of the future Henry VII of England) was taken to Hereford by Sir Roger Vaughan and executed in High Town. A plaque now marks the spot of the execution. Vaughan was later himself executed, under a flag of truce, by Owen's son Jasper.
During the civil war the city changed hands several times. On 30 September 1642 Parliamentarians led by Sir Robert Harley and Henry Grey, 1st Earl of Stamford occupied the city without opposition. In December they withdrew to Gloucester because of the presence in the area of a Royalist army under Lord Herbert. The city was again occupied briefly from 23 April to 18 May 1643 by Parliamentarians commanded by Sir William Waller but it was in 1645 that the city saw most action. On 31 July 1645 a Scottish army of 14,000 under Alexander Leslie, 1st Earl of Leven besieged the city but met stiff resistance from its garrison and inhabitants. They withdrew on 1 September when they received news that a force led by King Charles was approaching. The city was finally taken for Parliament on 18 December 1645 by Colonel Birch and Colonel Morgan. King Charles showed his gratitude to the city of Hereford on 16 September 1645 by augmenting the city's coat of arms with the three lions of Richard I of England, ten Scottish Saltires signifying the ten defeated Scottish regiments, a very rare lion crest on top of the coat of arms signifying "defender of the faith" and the even rarer gold-barred peer's helm, found only on the arms of one other municipal authority: those of the City of London.
Nell Gwynne, actress and mistress of King Charles II, is said to have been born in Hereford in 1650 (although other towns and cities, notably Oxford, also claim her as their own); Gwynn Street is named after her. Another famous actor born in Hereford is David Garrick (1717–1779).
The Bishop's Palace next to the Cathedral was built in 1204 and continually used to the present day. Hereford Cathedral School is also one of the oldest schools in England.
During World War I, in 1916, a fire at the Garrick Theatre killed eight young girls who had been performing at a charity concert.
Governance.
The main local government body covering Hereford is Herefordshire Council. Hereford has a "City Council" but this is actually a parish council with city status, and has only limited powers.
Historically Hereford has been the county town of Herefordshire. In 1974 Herefordshire was merged with Worcestershire to become part of the county of Hereford and Worcester, and Hereford became a district of the new county. Hereford had formed a historic borough and was reformed by the Municipal Corporations Act 1835. On 1 April 1998 the County of Hereford and Worcester was abolished, and Herefordshire and Worcestershire were re-established as separate counties, although with slightly altered borders.
However the new Herefordshire was a unitary authority without any districts, and so Hereford lost its district status (although, confusingly, the authority's full legal name is the County of Herefordshire District Council). Charter Trustees were appointed to preserve mayoral traditions until a civil parish council could be set up, which happened in 2000. Hereford is one of only seven civil parishes in England which have city status.
Hereford was the name of a parliamentary constituency centring on the city, from 1295 to 2010, when it was renamed as Hereford and South Herefordshire. The current member of the House of Commons for Hereford and South Herefordshire is Jesse Norman of the Conservative Party.
Climate.
As with all of the UK, Hereford experiences a maritime climate, with limited seasonal temperature ranges, and generally moderate rainfall throughout the year. The nearest Met Office weather station for which 30-year averages are available is Preston Wynne weather station, about 5 mi north east of the city centre. Since 2001 a weather station at Credenhill, under 4 mi to the west, has provided data.
Since 2001, extremes at Hereford Credenhill have ranged from 33.6 °C during July 2006, to as low as -15.8 °C during December 2010
Transport.
Roads.
There have been plans for many years for a north–south bypass and currently the plan is for a nine-mile (14 km) dual carriageway; however, HM Government as yet has refused to grant permission or supply funds. Until then the A49 Trunk Road, A465, and A438 continue to run through the city centre.
Rail.
Hereford is served by a 4-platform railway station on the Welsh Marches Line which opened in 1854. Services regularly connect to Worcester, Birmingham, London, Manchester, Cardiff and other settlements in south Wales. The station is currently operated by Arriva Trains Wales. A second station, Hereford Barton, was closed and later redeveloped. A new station is proposed for construction in the government-designated Enterprise zone in Rotherwas, south of the River Wye.
Air.
There is no airport in or near Hereford. The nearest are at Gloucester, Bristol, Birmingham, and Cardiff.
Military associations.
RAF Hereford was a non-flying station of the Royal Air Force located nearby to Hereford. It was the home of a wide variety of training schools from 1940 until it closed for RAF training in 1999. Subsequently the Special Air Service (SAS) moved their base to there from its previous location in the city. There is a clocktower in Hereford where the names of dead SAS men are inscribed. 
Economy.
The main public service employers in Hereford include:
In 2005 Hereford was granted Fairtrade City status.
Major employers in the city include:
Other major companies based in Herefordshire include:
Herefordshire is a global centre for cider production as it supports many acres of orchards, so many breweries and associated organisations exist here, along with other heavy and light industries. Within the city, many are based at the Rotherwas Industrial Estate.
Regeneration.
For much of the 20th century, Hereford City remained structurally stagnant with no major investment. Since the council became independent from Worcester in 1998, much has changed in that regard.
Many of the schools in Hereford have been rebuilt and improved. The Herefordshire College of Technology has also been rebuilt to a 21st-century standard. A new NMITE (New Model in Technology and Engineering) university is also planned, which will teach STEM subjects and is slated to open in 2016. There has also been a number of improvements at Hereford Sixth Form College, where a new business block extension was completed in 2013 and a new reception area was completed in 2015.
Hereford benefitted from the PFI reconstruction schemes for NHS hospitals, with the former County Hospital site having £60 million spent on a brand new, one-site hospital to replace the former 3 hospitals: the General, the Eye Hospital, and the County Hospital. The new Hereford County Hospital was the single largest investment in Herefordshire at that point. In 2015, further funds for more improvements at the hospital were granted.
Current and future projects.
A major regeneration project is taking place in Hereford city centre, formerly known as the Edgar Street Grid. This covers an area of around 100 acre just north of the old city walls. Work started on 8 October 2012, and should take around 15 years to complete the whole project. The regeneration includes the rebuilding of the canal basin at the end of the currently disused Herefordshire and Gloucestershire Canal.
The £80 million phase 1 includes a supermarket, department store, multiplex cinema, shops, restaurants, and other facilities and opened in late Spring 2014.
The Butter Market is also due for refurbishment and proposals are being examined.
A proposed bypass has been drawn up to circulate the city, which suffers from rush hour traffic, with potential routes either to the east or west of the city. Both routes would connect with the Rotherwas Access Road which was recently completed, connecting the Rotherwas Industrial Estate to the A49. Rotherwas itself has recently been awarded an Enterprise Zone status by the government which is expected to boost the economy and bring in thousands of new jobs.
A second railway station for Hereford is planned, it would be situated in Rotherwas as part of the Enterprise Zone.
Hereford is due to receive half of the 20,600 new homes expected to be built in the county by 2026 as part of the Regional Spatial Strategy.
Sport.
Hereford was the home of Hereford United Football Club, best known for beating Newcastle in the FA Cup in January 1972, when they were still a non-league side and Newcastle were in the top division of English football. They had a spell in the Football League from 1972 to 1997 reaching the second tier of English football in 1976, and were relegated to non-League status in 1997 before returning to beat Halifax Town A.F.C. 3–2 in the Nationwide Conference play-off final in 2005–06 to book a return to the Football League. They were again promoted, this time automatically, during the 2007–08 season, projecting them to this level of football for the first time since the late 1970s.
As part of the regeneration of Hereford City Centre, the football club were renovating their ground. However, the club was wound up in 2014. A phoenix club is being set up called Hereford F.C. 
Football within the county is administered by the Herefordshire Football Association
Hereford Rugby Club is also a popular local team. In 2012, the club announced plans for a major £6 million move to a new home.
Hereford Hockey Club is based at the Hereford City Sports Club, with teams entered into leagues in the West Hockey Association.
The city was also home to Hereford Racecourse, a traditional National Hunt course to the north of the city centre which hosted around twenty meetings a year. The company who leased the site decided in 2012 that the site was not viable. The last meeting was held on 16 December 2012.
Many golf courses surround the city at Wormsley (Herefordshire GC), Burghill and Brockington. The racecourse surrounds a golf course in Holmer
Public leisure.
Hereford's public leisure facilities are managed by a not-for-profit trust called HALO Leisure, which runs the Hereford Leisure Centre (that includes huge sports halls, gymnasium, squash courts, and an outdoor athletics facility), and the Hereford Leisure Pool (which includes a gymnasium, full size swimming pool, leisure pool, diving pool, and learners pool).
Clubs and societies.
The Hereford Rowing Club (along with the Kayak Club) uses the River Wye; it is a popular club with a strong junior group. The stretch of river is also used for other water sports.
Hereford has a thriving nine pin skittle league, formed on 24 October 1902, and today consisting of five divisions.
Hereford has other well attended set of clubs and societies including the Railway Club, Welsh Club, Military Club, Richmond Place Club and the Whitecross Squash & Lawn Tennis Club.
Hereford also has several music clubs/societies such as Hereford Youth Orchestra, a group for those up to the age of 18 which anyone in and around the Herefordshire area can audition for. The orchestra is conducted by both Sir Richard Mynors and Hazel Davies.
Education.
University.
Herefordshire is one of only three English counties not to have a university. However, until 1977 Hereford was home to Hereford College of Education, a higher education institution which offered teacher training qualifications.
Plans are now in progress to create, from scratch, a New University in Hereford, to teach STEM subjects from 2017.
Colleges.
Hereford is home to five colleges, including:
The National School of Blacksmithing is the oldest established Blacksmithing college in the UK, also the largest facility for training smiths in Europe. This is also part of HCT.
Schools.
Hereford's many secondary schools include:
Primary schools in the city include Hereford Cathedral Junior School, a co-educational independent school. Hereford Cathedral Junior School is, with Hereford Cathedral School, part of the ancient Hereford Cathedral Foundation dating back to 676. The Junior School was founded as an independent school in 1898.
The City's other primary schools are: Lord Scudamore Academy, St James C of E, St Francis Xavier R.C, Trinity, Holmer C of E, Marlbrook, Riverside, St Martin's, Broadlands, Riverside, Hampton Dene and St Paul's C of E.
Health and social care.
In early 2008, Herefordshire Council and NHS Herefordshire became the first local authority and Primary Care Trust to form a new kind of partnership. A single chief executive leads both organisations and there is also a joint management team and several joined up teams and services, which work as one organisation to plan, purchase, design and deliver care, which reduces duplication and expenditure.
The major hospital in Hereford is the Hereford County Hospital. Ambulance services are provided by the West Midlands Ambulance Service NHS Trust. The Midlands Air Ambulance charity provides air ambulance services across Herefordshire.
A private national firm operates a hospital in Hereford, and the city is well populated with council-funded, private and charity based nursing, residential and other elderly care homes & facilities.
Society and culture.
Agriculture.
Farming has played a major part in the history of the county of Herefordshire, and for many years the City of Hereford was the epicentre, playing host to the Cattle Market; a major market site for many years.
Sadly with the 2001 Foot and Mouth outbreak the market suffered and trade reduced. Established by Act of Parliament, the market must be provided, and so a new Bill was introduced in 2003 to move the site to the outskirts of the city. The inner city site would then be available for redevelopment, a process that has now finished.
The new Hereford Cattle Market opened its doors in August 2011 on a new site just outside the city and has already proved so successful that trading and business is up on the previous sites record.
Music.
The annual Three Choirs Festival, originating in the 18th century and one of the oldest music festivals in the British Isles, is held in Hereford every third year, the other venues being Gloucester and Worcester.
Composer Sir Edward Elgar lived at Plas Gwyn in Hereford between 1904 and 1911, writing some of his most famous works during that time. He is commemorated with a statue on the Cathedral Close. One of his Enigma Variations was inspired by a bulldog named Dan falling into the River Wye at Hereford, and the dog is similarly honoured with a wooden statue beside the river.
Hereford is home to the Hereford Police Male Voice Choir who competed on the BBC TV show "Last Choir Standing", and the Railway Choir.
A charitable music school is also based in Hereford.
Art.
H.Art, or Herefordshire Art Week, is an annual county-wide exhibition held in September, displaying the work of local artists. Many places usually closed to the public are opened during this week, such as the Bishop's Palace at the Cathedral.
There are numerous little galleries and places to find artworks in Hereford.
Polish-born sculptor Walenty Pytel has had studios in Hereford since 1963 after training at Hereford College of Art.
There is a statue of a Bronze Hereford bull designed by Brian Alabaster ARBS in front of The Old House
There is also a sculpture of Edward Elgar outside Hereford Cathedral.
Literature.
The troops of the fictional commando squad Rainbow were based at RAF Hereford, as detailed in the novel "Rainbow Six".
The action of the fictional novels and The last Dragonslayer by Jasper Fforde take place in Hereford.
Media.
The local radio stations are Free Radio (formerly known as Wyvern FM) which broadcasts on 97.6-96.7-102.8 FM, Sunshine Radio on 106.2 FM, and BBC Hereford and Worcester which broadcasts on 94.7FM.
Hereford is briefly mentioned, though mispronounced, in "Ronin" as a ploy by Sam (Robert De Niro) to expose Spence (Sean Bean) as a liar.
The City is mentioned in the video game Rainbow Six Vegas 2 and in the television show Peep Show.
The Hereford Times is the major weekly newspaper. The 'Hereford Journal' ceased publication on June 11, 2014 and before that the 'Hereford Admag' also ceased publication. The Council produces a regular magazine called "Herefordshire Matters".
Local TV content is currently provided by BBC Midlands Today and ITV Central News. Hereford has been selected as a viable location for a new local TV station by the UK Government.
Entertainment.
The city's main theatre and cultural venue is the Courtyard Centre for the Arts which was opened in 1998, replacing the New Hereford Theatre.
There is also a multi screen Odeon cinema in the old market precinct.
MFA Bowl (formerly known as TGS, home to a Ten Pin Bowling alley and Adventure Golf course is located near the railway station.
There is also a dedicated Skatepark on Holmer Road.
Famous people.
John Kemble, Catholic priest and martyr, was born near Hereford.
Nell Gwyn, and David Garrick, and Sarah Siddons, actors and actresses, are all historical figures popularly associated with Hereford.
Major-General Stringer Lawrence, first commander-in-chief of British troops in India, under whose command Robert Clive served, was born in Hereford.
Broadcaster Gilbert Harding was born there when his father was master of the local workhouse, as was contemporary actress Beryl Reid.
The original lineup of The Pretenders, with the exception of lead singer Chrissie Hynde, were from Hereford, as were the rock band Mott the Hoople. Singer Ellie Goulding was born in Hereford.
Frank Oz, puppeteer for The Muppets was born in Hereford and lived there for the first five years of his life.
Footballers Aaron Wildig and Connor Wickham were born in the town.
Hereford is the current home of television personality, Wincey Willis.
Tourism and attractions.
The Old House, Hereford is an historic black and white house in the centre of High Town in Hereford City. It is now a museum about life in the Jacobean era of the 1600s when it was built.
The Hereford Museum and Art Gallery, housed in a Victorian Gothic building and opened in 1874, presents artefacts, fine art, and decorative art associated with the local area.
The Hereford Cider Museum is located in the City, with a shop and a fully interactive guide to how to produce the drink. H C.Mus. is a reg Charity Trust founded in the early 1970s by people who wanted to record the past and rapidly disappearing traditional art of cider making as had been done for generations on the farms in the "Cider Counties". Situated in an old cider factory, it opened in 1980 and 1981. Annually it holds a large display of named cider apples, during the cider festival, when the apples are pressed in the old way. In the spring/summer there is the International Cider festival, started in the mid-1980s, by the Friends of the Museum with the advice of Long Ashton Research station Nr Bristol (sadly no longer in existence). The Museum also holds in its recent Pomological Archive, a number of records pertaining to apples and cider.
Hereford Cathedral dates from 1079 and contains the "Mappa Mundi", a medieval map of the world dating from the 13th century which was restored in the late 20th century. It also contains the world famous Chained Library.
Holme Lacy House is now a hotel for a major national chain was built by John Scudamore in the 1500s. It has played host to many famous historical figures in its time.

</doc>
<doc id="49037" url="http://en.wikipedia.org/wiki?curid=49037" title="Ecological selection">
Ecological selection

Ecological selection (or environmental selection or survival selection or individual selection or asexual selection) refers to natural selection minus sexual selection, i.e. strictly ecological processes that operate on a species' inherited traits without reference to mating or secondary sex characteristics. The variant names describe varying circumstances where sexual selection is wholly suppressed as a mating factor.
Ecologists often study ecological selection when examining the abundance of individuals per population across regions, and what governs such abundances.
Circumstances in which it occurs.
Ecological selection can be said to be taking place in any circumstance where inheritance of specific traits is determined by ecology alone without direct sexual competition, when e.g. sexual competition is strictly ecological or economic, there is little or no mate choice, females do not resist any male who wishes to mate, all traits will be equally propagated regardless of mating, or the species is hermaphroditic or asexually reproducing, an ecological selection is taking place. For example, environmental pressures are largely responsible for the evolution of different life history strategies between the African honey bee, "A. m. scutellata", and the European honey bee.
In sexually reproducing species, it is applicable mostly to situations where ecological pressures prevent most competitors from reaching maturity, or where crowding or pair-bonding or an extreme suppression of sexual selection factors prevents the normal sexual competition rituals and selection from taking place, but which also prevent artificial selection from operating, e.g. arranged marriages, where parents rather than the young select the mate based on economic or even astrological factors, and where the sexual desires of the mated pair are often subordinated to these factors, are artificial unless "wholly" based on an ecological factor such as control of land which is held by their own force.
In forests, ecological selection can be witnessed involving many factors such as available sunlight, soil quality, and the surrounding biota. During forest growth, tree seedlings in particular, are ecosystem pioneers, and different tree seedlings can often react to a number of members in their ecological community in completely different ways, thus providing a spectrum of ecological occupations. On the other hand, adult trees can heavily impact their ecological communities, reversing the roles of ecological selection. Elements of the soil are an extremely influential selective factor in forest growth. Throughout time, every species of tree has evolved to grow under specific soil conditions, whether it is dependent on the pH levels, the mineral contents, or the drainage levels. Each of these is a vehicle for ecological selection to do its work in the course of evolution. However, ecological selection can be much more specific, not only working within species but within populations, even populations in the same region. For example, scientists in Quebec recently examined how tree seedlings react to different nitrate levels. What they found was that areas with higher nitrate levels contained plants that could much more efficiently metabolize nitrogen. Such plants could perform photosynthesis and respiration at a much faster rate than their nitrogen lacking peers, and also had longer root lengths on average, giving them an evolutionary advantage for their habitat. Nitrogen levels that are unexpectedly too high could harm some tree species, but these particular specimens created a niche for themselves, and could outcompete others around them. A site of tree growth can also be influenced by slope, rockiness, climate, and available sunlight. Space is initially available to everything, but seedlings that can most quickly inhabit the soil and take advantage of the available nutrients are usually most successful. Generally, one of the first factors to control which species grow best in the soil is the amount of sunlight. Soil and water themselves are both very important (For instance, a dry hardwood such as a white oak will not grow in a swamp), but sunlight is the initial decider in forest succession. Shade intolerant trees can immediately grow impressively. They need the sunlight that is offered by an open canopy found in a bare environment. Selection weeds out the seedlings that can not handle full sun, thus tall, straight trees will eventually grow and develop a full, lush canopy. However, these behaviors will soon be reversed. Seedlings that were once removed by ecological selection now become favored, because the shaded forest floor has become ideal for such shade tolerant species. This is a great example of how ecological selection can create niches for different species by performing the same function with different outcomes.
Ecological selection vs sexual selection.
In cases where ecological and sexual selection factors are strongly at odds, simultaneously encouraging and discouraging the same traits, it may also be important to distinguish them as sub-processes within natural selection.
For instance, "Ceratogaulus", the Oligocene horned gopher, left in the fossil record a series of individuals with successively longer and longer horns, that seemed to be unrelated or maladaptive to its ecological niche. Some modern scientists have theorized that the horns were useful or impressive in mating rituals among males (although other scientists dispute this theory, pointing out that the horns were not sexually dimorphic) and that it was an example of runaway evolution. The species seems to have suddenly died out when horns reached approximately the body length of the animal itself, possibly because it could no longer run or evade predators - thus ecological selection seems to have ultimately trumped sexual.
It is also important to distinguish ecological selection in cases of extreme ecological abundance, e.g. the human built environment, cities or zoos, where sexual selection must generally predominate, as there is no threat of the species or individuals losing their ecological niche. Even in these situations, however, where survival is not in question, the variety and the quality of food, e.g. as presented by male to female monkeys in exchange for sex in some species, still has an impact on reproduction, however it becomes a sexual selection factor. Similar phenomena can be said to exist in humans e.g. the "mail order bride" who primarily mates for economic advantage.
Differentiating ecological selection from sexual is useful especially in such extreme cases; Above examples demonstrate exceptions rather than a typical selection in the wild. In general, ecological selection is assumed to be the dominant process in natural selection, except in highly cognitive species that do not, or do not always, pair bond, e.g. walrus, gorilla, human. But even in these species, one would distinguish cases where isolated populations had no real choice of mates, or where the vast majority of individuals died before sexual maturity, leaving only the ecologically selected survivor to mate - regardless of its sexual fitness under normal sexual selection processes for that species.
For example, if only a few closely related males survive a natural disaster, and all are able to mate very widely due to lack of males, sexual selection has been suppressed by an ecological selection (the disaster). Such situations are usually temporary, characteristic of populations under extreme stress, for relatively short terms. However, they can drastically affect populations in that short time, sometimes eliminating all individuals susceptible to a pathogen, and thereby rendering all survivors immune. A few such catastrophic events where ecological selection predominates can lead to a population with specific advantages, e.g. in colonization when invading populations from more crowded disease-prone conditions arrive with antibodies to diseases, and the diseases themselves, which proceed to wipe out natives, clearing the way for the colonists.
In humans, the intervention of artificial devices such as ships or blankets may be enough to make some consider this an example of artificial selection. However it is clearly observed in other species, it seems unreasonable to differentiate colonization by ship from colonization by walking, and even the word "colony" is not specific to humans but refers generically to an intrusion of one species on an ecology to which it has not wholly adapted. So, despite the potential controversy, it may be better to consider all examples of colonist-borne diseases to be ecological selection.
For another example, in a region devastated by nuclear radiation, such as the Bikini Atoll, capacity to survive gamma rays to sexual maturity and (for the female) to term is a key ecological selection factor, although it is neither "natural" nor sexual. Some would call this too artificial selection, not natural or ecological, as the radiation does not enter the ecology as a factor save due to man's effort. Ambiguous artificial-plus-ecological factors may reasonably be called "environmental", and the term environmental selection may be preferable in these cases.

</doc>
<doc id="49040" url="http://en.wikipedia.org/wiki?curid=49040" title="Magnus Maximus">
Magnus Maximus

Magnus Maximus (Latin: "Flavius Clemens Magnus Maximus Augustus", Welsh: "Macsen Wledig") (ca. 335 – August 28, 388), also known as Maximianus (not to be confused with the emperor Maximian), was Western Roman Emperor from 383 to 388.
In 383 as commander of Britain, he usurped the throne against emperor Gratian; and through negotiation with emperor Theodosius I the following year he was made emperor in Britannia and Gaul – while Gratian's brother Valentinian II retained Italy, Pannonia, Hispania, and Africa. In 387 Maximus' ambitions led him to invade Italy, resulting in his defeat by Theodosius I at the Battle of the Save in 388. In the view of some historians his death marked the end of direct imperial presence in Northern Gaul and Britain.
Life.
Maximus was born in Gallaecia, on the estates of Count Theodosius (the Elder) to who was a nephew, Flavius Iulius Eucherius son. According near-contemporaries described his dignity as offended when lesser men were promoted to high positions.
Maximus was a distinguished general, who served under Count Theodosius in Africa in 373 and on the Danube in 376. It is likely he also may have been a junior officer in Britain in 368, during the quelling of the Great Conspiracy. Assigned to Britain in 380, he defeated an incursion of the Picts and Scots in 381.
The western emperor Gratian had become unpopular because of perceived favouritism toward Alans over Roman citizens. The Alans are an Iranian speaking people (see also Sarmatians and Ossetians) who were early adopters of Christianity and migrated both east and west from their homeland.
In 383 Maximus was proclaimed emperor by his troops. He went to Gaul to pursue his imperial ambitions, taking a large portion of the British garrison troops with him.
Following his landing in Gaul, Maximus went out to meet his main opponent, emperor Gratian, whom he defeated near Paris. Gratian, after fleeing, was killed at Lyon on August 25, 383. Continuing his campaign into Italy, Maximus was stopped from overthrowing Valentinian II, who was only twelve, when Theodosius I, the Eastern Roman Emperor, sent Flavius Bauto with a powerful force to stop him. Negotiations followed in 384 including the intervention of Ambrose, Bishop of Milan, leading to an accord with Valentinian II and Theodosius I in which Maximus was recognized as Augustus in the west.
Maximus made his capital at Augusta Treverorum (Treves, Trier) in Gaul, and ruled Britain, Gaul, Spain and Africa. He issued coinage and a number of edicts reorganizing Gaul's system of provinces. Some scholars believe Maximus may have founded the office of the Comes Britanniarum as well. He became a popular emperor, Quintus Aurelius Symmachus delivered a panegyric on Maximus' virtues. He used "foederati" forces such as the Alamanni to great effect. He was also a stern persecutor of heretics. It was on his orders that Priscillian and six companions became the first people in the history of Christianity to be executed for heresy, in this case of Priscillianism, by other Christians (though the civil charges laid by Maximus himself were for the practice of magic or "witchcraft", technically spiritual fraud by use of ventriloquism, and their property was confiscated. These executions went ahead despite the wishes of prominent men such as St. Martin of Tours. Maximus' edict of 387 or 388 which censured Christians at Rome for burning down a Jewish synagogue, was condemned by bishop Ambrose, who said people exclaimed: ‘the emperor has become a Jew’ 
In 387 Maximus managed to force emperor Valentinian II out of Milan, after which he fled to Theodosius I.
Theodosius I and Valentinian II then invaded from the east, and campaigned against Magnus Maximus in July–August 388, their troops being led by Richomeres and other generals. Maximus was defeated in the Battle of the Save, and retreated to Aquileia. Meanwhile the Franks under Marcomer had taken the opportunity to invade northern Gaul, at the same time further weakening Maximus' position.
Andragathius, "magister equitum" of Maximus and the killer of emperor Gratian, was defeated near Siscia while Maximus' brother, Marcellinus, fell in battle at Poetovio. Maximus surrendered in Aquileia, and although he pleaded for mercy was executed. The Senate passed a decree of "Damnatio memoriae" against him. However, his mother and at least two daughters were spared. Theodosius' trusted general Arbogast strangled Maximus' son, Flavius Victor, at Trier in the fall of the same year.
What exactly happened to Maximus' family after his downfall is not recorded. He is known to have had a wife, who is recorded as having sought spiritual counsel from St. Martin of Tours during his time at Trier. Her ultimate fate, and even her name (but see the Welsh tradition below), have not been preserved in definitive historic records. The same is true of Maximus' mother and daughters, other than that they were spared by Theodosius I.
One of Maximus' daughters may have been married to Ennodius, proconsul Africae (395). Ennodius' grandson was Petronius Maximus, another ill-fated emperor, who ruled in Rome for but 77 days before he was stoned to death while fleeing from the Vandals on May 24, 455. Other descendants of Ennodius, and thus possibly of Maximus, included Anicius Olybrius, emperor in 472, but also several consuls and bishops such as St. Magnus Felix Ennodius (Bishop of Pavia c. 514-21). We also encounter an otherwise unrecorded daughter of Magnus Maximus, Sevira, on the Pillar of Eliseg, an early medieval inscribed stone in Wales which claims her marriage to Vortigern, king of the Britons.
Role in British and Breton history.
Maximus' bid for imperial power in 383 coincides with the last date for any evidence of a Roman military presence in Wales, the western Pennines, and the fortress of Deva. Coins dated later than 383 have been found in excavations along Hadrian's Wall, suggesting that troops were not stripped from it, as was once thought. In the "De Excidio et Conquestu Britanniae" written c. 540, Gildas says that Maximus left Britain not only with all of its Roman troops, but also with all of its armed bands, governors, and the flower of its youth, never to return.
Having left with the troops and senior administrators, and planning to continue as the ruler of Britain in the future, his practical course was to transfer local authority to local rulers. Welsh legend supports that this happened, with stories such as "Breuddwyd Macsen Wledig" (English: The Dream of Emperor Maximus), where he not only marries a wondrous British woman (thus making British descendants probable), but also gives her father sovereignty over Britain (thus formally transferring authority from Rome back to the Britons themselves).
The earliest Welsh genealogies give Maximus (referred to as "Macsen/Maxen Wledig", or "Emperor Maximus") the role of founding father of the dynasties of several medieval Welsh kingdoms, including those of Powys and Gwent. He is given as the ancestor of a Welsh king on the Pillar of Eliseg, erected nearly 500 years after he left Britain, and he figures in lists of the Fifteen Tribes of Wales.
After he became emperor of the Western Roman Empire, Maximus would return to Britain to campaign against the Picts and Scots (i.e., Irish), probably in support of Rome's long-standing allies the Damnonii, Votadini, and Novantae (all located in modern Scotland). While there he likely made similar arrangements for a formal transfer of authority to local chiefs—the later rulers of Galloway, home to the Novantae, would claim Maximus as the founder of their line, the same as did the Welsh kings.
The ninth century "Historia Brittonum" gives another account of Maximus and assigns him an important role:
The seventh emperor was Maximianus. He withdrew from Britain with all its military force, slew Gratianus the king of the Romans, and obtained the sovereignty of all Europe. Unwilling to send back his warlike companions to their wives, families, and possessions in Britain, he conferred upon them numerous districts from the lake on the summit of Mons lovis, to the city called Cant Guic, and to the western Tumulus, that is Cruc Occident. These are the Armoric Britons, and they remain there to the present day. In consequence of their absence, Britain being overcome by foreign nations, the lawful heirs were cast out, till God interposed with his assistance.
Modern historians believe that this idea of mass British troop settlement in Brittany by Maximus may very well reflect some reality, as it accords with archaeological and other historical evidence and later Breton traditions.
Armorica declared independence from the Roman Empire in 407 CE, but contributed archers for Flavius Aetius's defence against Attila the Hun, and its king Riothamus was subsequently mentioned in contemporary documents as an ally of Rome's against the Goths. Despite its continued usage of two distinct languages, Breton and Gallo, and extensive invasions and conquests by Franks and Vikings, Armorica retained considerable cultural cohesion into the 13th century.
Maximus also established a military base in his native Gallaecia, i.e. Galicia (Spain), which persisted as a cultural entity despite occupation by the Suebi in 409, see Kingdom of Galicia. This kingdom successfully resisted the Moors and subsequently initiated the Spanish Reconquista.
Aetius sent large numbers of Alans to both Armorica and Galicia following the defeat of Attila at the Battle of the Catalunian Plains. The Alans evidently assimilated quickly into the local Celtic cultures, contributing their own legends, e.g. to the Arthurian Cycle of romances.
Welsh legend.
Legendary versions of Maximus' career in which he marries the Welsh princess Elen may have circulated in popular tradition in Welsh-speaking areas from an early date. Although the story of Helen and Maximus's meeting is almost certainly fictional, there is some evidence for the basic claims. He is certainly given a prominent place in the earliest version of the Welsh Triads which are believed to date from c. 1100 and which reflect far older traditions. Welsh poetry also frequently refers to Macsen as a figure of comparison with later Welsh leaders. These legends come down to us in two separate versions.
Geoffrey of Monmouth.
According to Geoffrey of Monmouth's fictional "Historia Regum Britanniae" ("ca." 1136), the basis for many English and Welsh legends, Maximianus as he calls him, was a Roman senator, a nephew of Coel Hen through his brother Ioelinus, and king of the Brythons following the death of Octavius. Geoffrey writes this came about because Octavius, the king of the Britons, wanted to wed his daughter to such a powerful half-Roman, half-Briton and to give the kingship of Britain as a dowry to that husband so he sent a message to Rome offering his daughter to Maximian.
Caradocus, the Duke of Cornwall, had suggested and supported the marriage between Octavius's daughter and Maximian. Maximian accepted the offer and left Rome for Britain. Geoffrey claims further that Maximian gathered an army as he sacked Frankish towns along the way. He invaded Clausentum (modern Southampton) unintentionally and nearly fought the army of the Britons under Conanus before agreeing to a truce. Following further negotiations, Maximian was given the kingship of Britain and Octavius retired. Five years into his kingship, Magnus Maximus assembled a vast fleet and invaded Gaul, leaving Britain in the control of Caradocus. Upon reaching the kingdom of Armorica (historically, the region between the Loire and Seine rivers, later comprising Brittany, Normandy, Anjou, Maine and Touraine), he defeated the king and killed thousands of inhabitants. Before departing to Rome, he summoned Conanus, the rebellious nephew of Octavius, and asked him to rule as king of the land, which was renamed Brittany, or "Little Britain". Conan's men married native women after cutting out their tongues to preserve the purity of their language. Geoffrey of Monmouth presents this legend to explain the Welsh name for Brittany, Llydaw, as originating from "lled-taw" or "half-silent". Given that Conan was well established in genealogies as the founder of Brittany, this account is certainly connected to an older tradition than Geoffrey.
Following the death of Caradocus, rule of Britain as regent passed to Dionotus, who - facing a foreign invasion - appealed to Maximus, who finally sent a man named Gracianus Municeps with two legions to stop the attack. He killed many thousands before the invaders fled to Ireland. Maximus died in Rome soon after and Dionotus became the official king of the Britons. Unfortunately, before he could begin his reign, Gracianus took hold of the crown and made himself king over Dionotus.
The Dream of Macsen Wledig.
Although the "Mabinogion" tale "The Dream of Macsen Wledig" is written in later manuscripts than Geoffrey's version, the two accounts are so different that scholars agree the Dream cannot be based purely on Geoffrey's version. The Dream's account also seems to accord better with details in the Triads, so it perhaps reflects an earlier tradition.
Macsen Wledig, the Emperor of Rome, dreams one night of a lovely maiden in a wonderful, far-off land. Awakening, he sends his men all over the earth in search of her. With much difficulty they find her in a rich castle in Wales, daughter of a chieftain based at Segontium (Caernarfon), and lead the Emperor to her. Everything he finds is exactly as in his dream. The maiden, whose name is Helen or Elen, accepts and loves him. Because Elen is found a virgin, Macsen gives her father sovereignty over the island of Britain and orders three castles built for his bride. In Macsen's absence, a new emperor seizes power and warns him not to return. With the help of men from Britain led by Elen's brother Conanus ("Welsh": Kynan Meriadoc, "Breton": Conan Meriadeg), Macsen marches across Gaul and Italy and recaptures Rome. In gratitude to his British allies, Macsen rewards them with a portion of Gaul that becomes known as Brittany.
Later literature.
The prominent place of Macsen in history, Welsh legend and in the Matter of Britain means he is often a character or referred to in historical and Arthurian fiction. Such stories include Stephen R. Lawhead's Pendragon Cycle, Mary Stewart's "The Hollow Hills", Jack Whyte's Camulod Chronicles, Nancy McKenzie's "Queen of Camelot" and Rudyard Kipling's "Puck of Pook's Hill". The popular Welsh folk song "Yma o Hyd", recorded by Dafydd Iwan in 1981, recalls Macsen Wledig and celebrates the continued survival of the Welsh people since his days.
Primary sources.
Magnus Maximus is mentioned in a number of ancient and Medieval sources.

</doc>
<doc id="49044" url="http://en.wikipedia.org/wiki?curid=49044" title="Bokononism">
Bokononism

Bokononism is a religion invented by Kurt Vonnegut and practiced by many of the characters in his novel "Cat's Cradle". Many of the sacred texts of Bokononism were written in the form of calypsos.
Bokononism is based on the concept of "foma", which are defined as harmless untruths. A foundation of Bokononism is that the religion, including its texts, is formed entirely of lies; however, one who believes and adheres to these lies will have peace of mind, and perhaps live a good life. The primary tenet of Bokononism is to "Live by the foma that make you brave and kind and healthy and happy."
Background.
Bokonon, a character in the novel, is the founder of the religion. He was born Lionel Boyd Johnson in 1891 and attended the London School of Economics to study Political Science, only to have his education cut short by World War I. "Bokonon" was the way the natives of San Lorenzo, the fictional Caribbean island-nation where the ship-wrecked Johnson started his religion, pronounced his family name in their unique dialect of English. 
Bokonon established Bokononism with Earl McCabe, his partner in ruling the island, when all the duo's efforts to raise the standard of living on the island failed, as a means to help the poor islanders escape their miserable reality by practicing a simple, useful religion. Arranging with McCabe that Bokononism be outlawed and eternally persecuted by the government, he went to live in the jungle, supposedly hiding, thus trying to lure the population into Bokononism as a kind of forbidden fruit.
Terminology.
Bokononism encompasses a number of unique concepts expressed in the San Lorenzan dialect:

</doc>
<doc id="49048" url="http://en.wikipedia.org/wiki?curid=49048" title="Sampler (needlework)">
Sampler (needlework)

A (needlework) sampler is a piece of embroidery produced as a demonstration or test of skill in needlework. It often includes the alphabet, figures, motifs, decorative borders and sometimes the name of the person who embroidered it and the date. The word sampler is derived from the Latin ‘exemplum’ - an example.
History.
The earliest sampler extant is a spot sampler, i.e. one having randomly scattered motifs, of the Nazca culture in Peru formerly in the Museum of Primitive Art, New York City. It is estimated to date from ca. 200 BC –300 AD and is worked in cotton and wool pattern darning on a woven cotton ground. It has seventy-four figures of birds, plants and mythological beings.
Coptic sampler fragments of silk on linen in double running stitch and pattern darning have been found in Egyptian burial grounds of 400–500 AD. These are pattern samplers having designs based on early Christian symbols.
Samplers were known to be used by stitchers in Europe as early as the beginning of the 16th century, although none that early have been found. A collection of fifty "dechados" (samplers) was listed in the 1509 inventory of the possessions of Queen Joanna (Juana I) (1479-1555) of Castile (Spain). They were described as stitchery and "deshilado" (drawn thread work), some in silk and others in gold thread. At the time of the inventory they were in the care of her chamberlain Diego de Rivera and his son Alonso, but they have all disappeared.
The oldest surviving European samplers were made in the 16th and 17th centuries. As there were few pre-printed patterns available for needleworkers, a stitched model was needed. Whenever a needlewoman saw a new and interesting example of a stitching pattern, she would quickly sew a small sample of it onto a piece of cloth - her 'sampler'. The patterns were sewn randomly onto the fabric as a reference for future use, and the woman would collect extra stitches and patterns throughout her lifetime.
The first printed pattern book "Furm oder Modelbüchlein" was published by Johann Schönsperger the Younger of Augsburg in 1523, but it was not easily obtainable and a sampler was the most common form of reference available to many women. Pattern books were widely copied and issued by other publishers. Some are still available in reprint today.
The earliest British dated surviving sampler, housed in the Victoria and Albert Museum in London, was made by Jane Bostocke who included her name and the date 1598 in the inscription. Stitched with silk and metal thread on linen it has pictorial figures above with border and all-over patterns below. The inscription reads: ″IANE:BOSTOCKE 1598 ALICE:LEE:WAS:BORNE:THE:23:OF:NOVEMBER:BEING:TWESDAY:IN:THE:AFTER:NOONE:1596″ The museum has two other samplers believed to date from the 16th century, one from Germany with religious motifs and one from Italy with floral patterns and grotesques. Both are worked in silk and linen.
A Dutch sampler dated 1585 survives in the Nederlands Openluchtmuseum in Arnhem.
A sampler in the Museum of London has two cutwork bands stitched in silk, gold, and silver threads and thirteen bands of "reticella" whitework in white linen thread. The fourth band from the top has the initials E R, the royal arms of Queen Elizabeth I, and the maker's name SUSAN NEGABRI in bold letters. It is believed to date before the queen's death in 1603.
Because very few samplers from the 16th century have been found it is not possible to generalize about their style and appearance. By the middle of the 17th century English, Dutch, and German samplers were being stitched on a narrow band of fabric 6 - wide. As fabric was very expensive, these samplers were totally covered with neat rows of stitches. They were known as band samplers and valued highly, often being mentioned in wills and passed down through the generations. These samplers were stitched using a variety of needlework styles, threads, and ornament. Many of them were exceedingly elaborate, incorporating subtly shaded colours, silk and metallic embroidery threads, and using stitches such as Hungarian, Florentine, tent, cross, long-armed cross, two-sided Italian cross, rice, running, Holbein, Algerian eye and buttonhole stitches. The samplers also incorporated small designs of flowers and animals, and geometric designs stitched using as many as 20 different colors of thread. Some were stitched partially or entirely in whitework.
Band samplers were more decorative than the utilitarian random motif spot samplers, and stitchers finished them by adding their names, dates of completion, and sometimes teacher or school names. As the work of sampler making moved into schools in the late 17th and early 18th centuries design styles changed. Alphabets and verses were added along with pictorial elements such as architectural motifs, landscapes, and large potted plants. Educational themes included maps, multiplication tables, perpetual calendars, and acrostic puzzles.
By the 18th century, samplers were a complete contrast to the scattered samples sewn earlier on. They became wider and more square, eventually with borders on all four sides. Samplers were mainly school exercises during the 18th and 19th centuries, and were almost entirely worked in cross stitch. Design styles were increasingly influenced by Berlin woolwork which became popular worldwide. These samplers were stitched more to demonstrate knowledge than to preserve skill. The stitching of samplers was believed to be a sign of virtue, achievement and industry, and girls were taught the art from a young age.
Berlin woolwork designs had naturalistic shading and more depth of perspective than the flat two-dimensional objects on traditional needlework. By mid-19th century adult needleworkers were devising long and narrow stitch samplers having geometric patterns done in woolwork. The Art Needlework movement and elimination of samplers from female education brought about the decline in traditional sampler making that continued into the 20th century.
Modern samplers.
Samplers are widely stitched today, some using kits purchased from needlework shops, some from chart-packs, and many from patterns available on the Internet or through e-mail from designers. Patterns range from simple using only one stitch, to complex, using 15 to 20 and more stitches. Designs range widely in style, from accurate reproductions of historic pieces to much more contemporary and modern styles. Popular topics include designs commemorating births and marriages, family trees, and mottoes of all kinds. Map charts are widely available in English-speaking countries and Denmark. These are often pictorial maps of local areas, whole countries, or even the imaginary realms of Tolkien's Middle-earth. Many sampler reproductions are also available, copying colors and imperfect stitches from the originals.
The word "sampler" is sometimes inaccurately applied to any piece of needlework meant for display. However the genre may broadly be said to include any needlework in sampler style with or without lettering.
Materials used include aida cloth, evenweave, and linen fabrics, in cotton, linen, and man-made materials combined in more and more ways; and fibers from cotton floss to silk, rayon, viscose, and metallic.

</doc>
<doc id="49053" url="http://en.wikipedia.org/wiki?curid=49053" title="Carlos Filipe Ximenes Belo">
Carlos Filipe Ximenes Belo

Carlos Filipe Ximenes Belo SDB, GCL (born 3 February 1948) is an East Timorese Roman Catholic bishop. Along with José Ramos-Horta, he received the 1996 Nobel Peace Prize for work "towards a just and peaceful solution to the conflict in East Timor."
Early life and religious vocation.
The fifth child of Domingos Vaz Filipe and Ermelinda Baptista Filipe, Carlos Filipe Ximenes Belo was born in the village of Wailakama, near Vemasse, on the north coast of East Timor. His father, a schoolteacher, died two years later. His childhood years were spent in Catholic schools at Baucau and Ossu, before he proceeded to the Dare minor seminary outside Dili, from which he graduated in 1968. From 1969 until 1981, apart from periods of practical training (1974–1976) in East Timor and in Macau, he was in Portugal and Rome where, having become a member of the Salesian Society, he studied philosophy and theology before being ordained a priest in 1980.
Returning to East Timor in July 1981 he became a teacher for 20 months, then director for two months, at the Salesian College at Fatumaca.
Pastoral leadership.
On the resignation of Martinho da Costa Lopes in 1983, Carlos Filipe Ximenes Belo was appointed Apostolic Administrator of the Dili diocese, becoming head of the East Timor church and directly responsible to the Pope. On 6 February 1989, he was consecrated titular Bishop of Lorium.
Father Belo was the choice of the Vatican's Pro Nuncio in Jakarta and the Indonesian leaders because of his supposed submissiveness, but he was not the choice of the Timorese priests who did not attend his inauguration. However within only five months of his assuming office, he protested vehemently, in a sermon in the cathedral, against the brutalities of the Kraras massacre (1983) and condemned the many Indonesian arrests. The church was the only institution capable of communicating with the outside world, so with this in mind the new Apostolic Administrator started writing letters and building up overseas contacts, in spite of the isolation arising from the opposition of the Indonesians and the disinterest of most of the world.
In February 1989 he wrote to the President of Portugal, the Pope, and the UN Secretary-General, calling for a UN referendum on the future of East Timor and for international help for the East Timorese, who were "dying as a people and a nation", but when the UN letter became public in April, he became even more of a target of the Indonesians. This precariousness increased when Bishop Belo gave sanctuary in his own home, as he did on various occasions, to youths escaping the Santa Cruz massacre (1991), and endeavoured to expose the numbers of victims killed.
Bishop Belo's labours on behalf of the East Timorese and in pursuit of peace and reconciliation were internationally recognised when, along with José Ramos-Horta, he was awarded the Nobel Peace Prize in December 1996. Bishop Belo capitalised upon this honour through meetings with Bill Clinton of the United States and Nelson Mandela of South Africa. In 1995, he also won the John Humphrey Freedom Award from the Canadian human rights group Rights & Democracy.
Resignation as Apostolic Administrator and new pastoral activity.
In the aftermath of East Timorese independence on 20 May 2002, the pressure of events and the ongoing stress he endured began to show their effects on Bishop Belo's health. Pope John Paul II accepted his resignation as Apostolic Administrator of Dili on November 26, 2002.
Following his resignation Bishop Belo traveled to Portugal for medical treatment. By the beginning of 2004, there were repeated calls for him to return to East Timor and to run for the office of president. However, in May 2004 he told Portuguese state-run television RTP, that he would not allow his name to be put up for nomination. "I have decided to leave politics to politicians," he stated. One month later, on 7 June 2004, Pascuál Chavez, rector major of the Salesian Society, announced from Rome that Bishop Belo, returned to health, would take up a new assignment. In agreement with the Holy See, he would go to Mozambique as a missionary, and live as a member of the Salesian Society in that country.
In a statement released on 8 June, Bishop Belo said that, following two meetings in 2003 and in 2004 with the Prefect of the Congregation for the Evangelization of Peoples, he would go on a mission to the Diocese of Maputo, the capital of Mozambique, as he had wanted to since his youth. He started in July 2004; the same year he was awarded an Honorary Doctorate from CEU Cardinal Herrera University.
In February 2011 Belo received the Prize for Lusophonic Personality of the Year, given by in the Lisbon Academy of Sciences.

</doc>
<doc id="49054" url="http://en.wikipedia.org/wiki?curid=49054" title="Tupac Katari Guerrilla Army">
Tupac Katari Guerrilla Army

The Tupac Katari Guerrilla Army "(Ejército Guerrillero Túpac Katari)" was a Maoist guerrilla movement in Bolivia. Albeit of indigenist inspiration, the movement had a multirracial membership. The organization descended directly from the original revolutionaries trained by Che Guevara in the 1960s. Their objective was to fight for social equality in Bolivia and amongst its indigenous population. They carried out their first attack on July 5, 1991, destroying an electric power pylon in El Alto, a major city which adjoins La Paz, Bolivia's administrative capital. Most of the group's attacks have been similarly small-scale and they had confined their activities largely to Bolivia. The group suffered a major setback in a crackdown in 1992, when much of its leadership was neutralized through incarceration.
The group was named after Tupac Katari, a colonial revolutionary. One of their former members, Álvaro García Linera, is currently the vice-president of Bolivia.

</doc>
<doc id="49057" url="http://en.wikipedia.org/wiki?curid=49057" title="Henry Barnes (traffic engineer)">
Henry Barnes (traffic engineer)

Henry A. Barnes (December 16, 1906 – September 1968) was an American traffic engineer and commissioner who served in many cities, including Flint, Michigan; Denver, Colorado; Baltimore, Maryland; and New York City. Barnes was responsible for many innovations in applied traffic engineering, including the Green Wave of coordinated traffic signals, the application of actuated traffic signals (signals set off by the presence of an automobile or a pedestrian pushing a button), and the introduction of bus lanes.
Barnes Dance.
The Barnes Dance is a street-crossing system that stops all traffic and allows pedestrians to cross intersections in every direction at the same time. This system was first used in Kansas City and Vancouver in the late 1940s. Subsequently it was adopted in other cities such as Denver, Colorado, New York, San Diego, Baltimore, Maryland, and Washington, D.C.. Barnes stated that he did not invent the concept but promoted its widespread use. The phrase originated from City Hall reporter, John Buchanan, who wrote, "Barnes has made people so happy, they're dancing in the street."
Traffic Engineer / Commissioner.
Flint, Michigan.
Barnes served in Flint until 1947.
Denver.
Denver city's first professional traffic engineer from 1947 to 1953, where he oversaw the conversion of Denver Tramways to bus and trolley coach.
Baltimore.
Barnes came to Baltimore in 1953 for a one-month job as consultant traffic engineer, but Mayor Thomas D'Alesandro, Jr. (father of Nancy Pelosi) was so impressed he hired Barnes as traffic commissioner. At Baltimore, Barnes installed a traffic-control computer that was, in 1957, the largest of its kind in the world. He had the pedestal honoring Johns Hopkins moved to Charles and 33rd Street; previously it had been responsible for a number of fatalities. Barnes asked the mayor for a raise from $18,000 to 20,000 and took the traffic commissioner position at New York City when the request was turned down.
New York City.
Barnes was appointed traffic commissioner to New York City on January 15, 1962, by Mayor Robert F. Wagner, and kept on by Mayor John V. Lindsay. In 1962, Barnes fought with domineering city planner Robert Moses and killed the planned elevated Lower Manhattan Expressway. In 1963, he had an idea for expanding the Long Island Expressway capacity in Queens by adding three more lanes in each direction plus a second, four-lane deck above it. The upper deck would have no exits and run inbound in the morning and outbound in the evening. The "semi-actuated signal" that allows pedestrians to influence the change of traffic lights was another idea of Barnes, thought to have been introduced in 1964. With regards to the city's traffic signals, Barnes declared to repaint all of the traffic signals throughout the city of New York in the early 1960s. Prior to the early 1960s, traffic signals throughout the city were originally dark olive green. They were repainted yellow in the early 1960s. He was also involved in the completion of converting major avenues to one-way in New York City, a project started in 1949. He reportedly suggested solving Manhattan's traffic problems by making all avenues one-way, north. Barnes endorsed the use of seat belts, built municipal parking garages and implemented widespread use of parking meters.
Recognition.
Barnes was featured in "Life Magazine" November 13, 1964 edition – "New York's traffic jam doctor". He also appeared as a "Mystery Guest #1" on the television show "What's My Line?" which first aired on February 18, 1962. Barnes was also featured in "Popular Mechanics Magazine" in January 1953.
The Theodore M. Matson Memorial Award was bestowed on him in 1968.
Death.
Barnes died in September 1968 at age 61, suffering a heart attack on the job.

</doc>
<doc id="49060" url="http://en.wikipedia.org/wiki?curid=49060" title="Trainer">
Trainer

Trainer may refer to:

</doc>
<doc id="49062" url="http://en.wikipedia.org/wiki?curid=49062" title="Moral absolutism">
Moral absolutism

Moral absolutism is an ethical view that particular actions are intrinsically right or wrong. Stealing, for instance, might be considered to be always immoral, even if done for the well-being of others (e.g., stealing food to feed a starving family), and even if it does in the end promote such a good. Moral absolutism stands in contrast to other categories of normative ethical theories such as consequentialism, which holds that the morality (in the wide sense) of an act depends on the consequences or the context of the act.
Moral absolutism is not the same as moral universalism (also called "moral objectivism"). Universalism holds merely that what is right or wrong is independent of custom or opinion (as opposed to moral relativism), but not necessarily that what is right or wrong is independent of context or consequences (as in absolutism). Moral universalism is compatible with moral absolutism, but also positions such as consequentialism. Louis Pojman gives the following definitions to distinguish the two positions of moral absolutism and universalism:
Ethical theories which place strong emphasis on rights and duty, such as the deontological ethics of Immanuel Kant, are often forms of moral absolutism, as are many religious moral codes.
Moral absolutism and religion.
Moral absolutism may be understood in a strictly secular context, as in many forms of deontological moral rationalism. However, many religions have morally absolutist positions as well, regarding their system of morality as deriving from divine commands. Therefore, they regard such a moral system as absolute, (usually) perfect, and unchangeable. Many secular philosophies also take a morally absolutist stance, arguing that absolute laws of morality are inherent in the nature of human beings, the nature of life in general, or the universe itself. For example, someone who believes absolutely in nonviolence considers it wrong to use violence even in self-defense.

</doc>
<doc id="49063" url="http://en.wikipedia.org/wiki?curid=49063" title="Airco">
Airco

The Aircraft Manufacturing Company Limited (Airco) was a British aircraft manufacturer operating from 1912 to 1920. Airco produced thousands of aircraft for the British military during the First World War, most of which were designed by their chief designer, Geoffrey de Havilland. Advertised in 1918 as the largest aircraft company in the world, Airco established the first airline in the United Kingdom, Aircraft Transport and Travel Limited, as a subsidiary. A glut of war surplus aircraft and a lack of government interest in aviation caused the company to become unprofitable, and in 1920 it was sold to Birmingham Small Arms Company, who had its operations liquidated later that year.
Geoffrey de Havilland.
Airco was established in 1912 by George Holt Thomas at The Hyde in Hendon, north London, England. Two years later, learning that Geoffrey de Havilland, who was then at the Royal Aircraft Factory in Farnborough, might be available, Holt Thomas invited de Havilland to join Airco as chief designer. De Havilland's Airco designs were to provide around 30% of all trainers, fighters and bombers used by Britain and the United States during the First World War.
De Havilland's designs for Airco were marked with his initials "DH". Their pusher configuration fighter DH.2 of 1916 helped to end the "Fokker scourge" of 1915. Later notable aircraft designed and built by Airco during the war included the DH.6 trainer, of which more than 2,280 examples were built, and the DH.4 and DH.9 light bombers. These types, and the DH.9A, a developed version that served for many years with the postwar Royal Air Force, formed the basis of early de Havilland designed airliners, including the company's DH.16 and DH.18 types which were operated by Aircraft Transport and Travel Limited, the first airline established in the United Kingdom, also owned by George Holt Thomas.
Wartime production.
Flight Magazine reported in 1920 that during the period from August 1914 to November 1918 the D. H. Airco machines built in Great Britain and the United States of America comprised approximately 30 per cent of the total output of aeroplanes of these two allies.
By December 1918, Holt Thomas claimed in an advertisement that Airco was the largest aircraft company in the world and was building aeroplanes, engines and propellors in large numbers, as well as airships and flying boats. The engines being built included licenced production of Gnome and Le Rhone rotary engines. The company's works at Hendon employed between 7,000 and 8,000 people and had the latest metal-working machinery, a materials testing laboratory, and a wind tunnel. Airco was completing on average a new aircraft every 45 minutes.
First daily international flights.
Aircraft Transport and Travel, a subsidiary of Airco, started the world's first regular daily international service on 25 August 1919, between Hounslow Heath Aerodrome and Le Bourget. DH.16s were used for this service.
Sale to BSA and liquidation.
Following the cessation of hostilities, the large number of war-surplus machines, sharp fluctuations in business confidence, and the government's failure, unlike those of USA and France, to provide any form of support, Airco became unprofitable. Thomas endeavoured to sell Airco to a car manufacturer. Airco and BSA, parent company of Daimler, announced on 1 March 1920 that Airco had amalgamated with Birmingham Small Arms Company.
Within days BSA discovered Airco was in a far more serious financial state than Thomas had revealed. Thomas was immediately removed from his new seat on the BSA board and all BSA's new acquisitions were placed in liquidation. BSA failed to pay a dividend for the following four years.
With help from Thomas, de Havilland bought the group's assets he needed to form the de Havilland Aircraft Company in 1920.
Aircraft Transport and Travel had been allowed to continue to operate until December 1920. BSA then bought Aircraft Transport and Travel's aircraft from the liquidator and, in early 1921, established Daimler Airway and Daimler Air Hire under Daimler Hire Limited's Frank Searle.

</doc>
<doc id="49064" url="http://en.wikipedia.org/wiki?curid=49064" title="People's Party for Freedom and Democracy">
People's Party for Freedom and Democracy

The People's Party for Freedom and Democracy (Dutch: "Volkspartij voor Vrijheid en Democratie", VVD) (] is a conservative-liberal political party in the Netherlands.
The VVD supports private enterprise in the Netherlands and is an economic liberal party. After the fourth Balkenende cabinet was formed (22 February 2007), the VVD was the second-largest opposition party in the Dutch House of Representatives. During the Dutch general-election of 2010 the VVD obtained the highest number of votes cast and came to occupy 31 of the 150 seats in the House of Representatives. The VVD was the senior party in a centre-right minority government coalition together with the Christian Democratic Appeal (CDA) under the leadership of VVD party leader Mark Rutte. Rutte has been the leader of the VVD since 31 May 2006.
The First Rutte cabinet's parliamentary majority was provided by Geert Wilders' anti-Islam, -immigration, and -EU Party for Freedom (PVV), but this majority became unstable when, as was to be expected, Wilders' party refused to support austerity measures in connection with the Euro crisis. Therefore, elections for the House of Representatives were held on 12 September 2012. The VVD remained the largest party, with 41 seats. Since 5 November 2012, the VVD has been the senior partner in the second Rutte cabinet, a "purple" coalition with the Labour Party (PvdA).
History.
1948–1971.
The VVD was founded in 1948 as a continuation of the Freedom Party, which was a continuation of the interbellum Liberal State Party, which in turn was a continuation of Liberal Union. They were joined by the Comité-Oud, a group of liberal members of the Labour Party (PvdA), led by Pieter Oud. The liberals within the Labour Party were primarily members of the pre-war social-liberal Free-thinking Democratic League (VDB), who went on to join the Labour Party in the post-war Doorbraak (breakthrough) movement. They were unhappy with the social-democratic orientation of the Labour Party.
Between 1948 and 1952 the VVD took part in the broad cabinets led by the Labour Party Prime Minister Willem Drees. The party was a junior partner with only eight seats to the Catholic People's Party (KVP) and Labour Party, which both had around thirty seats (out of 100). The party's leadership was in the hands of the respected former Labour Party member Oud. The Drees cabinet laid the foundation for the welfare state and decolonization of the Dutch East Indies. In the Dutch general election of 1952 the VVD gained one seat, but did not join the government. In the Dutch general election of 1956 they increased their total, receiving thirteen seats, but were still kept out of government until the Dutch general election of 1959, which were held early because of cabinet crisis. This time they gained nineteen seats and the party entered government alongside the Protestant Anti Revolutionary Party (ARP), Christian Historical Union CHU and the Roman Catholic KVP.
In 1963, Oud left politics, and was succeeded by the minister of Home Affairs Edzo Toxopeus. With the lead of Toxopeus VVD lost three seats in the 1963 elections, but remained in government. In 1962, a substantial group of disillusioned VVD-members founded the Liberal Democratic Centre ("Liberaal Democratisch Centrum", LDC) which was intended to introduce a more twentieth-century liberal direction pointing to the classical liberal VVD. In 1966, frustrated with their hopeless efforts, LDC-members departed the VVD altogether and went on now to form an entirely political party -- Democrats 66 (D66).
In 1965, there also occurred a conflict between VVD-ministers and their counterparts from KVP and ARP in Cabinet Marijnen. The cabinet fell and without elections it was replaced by the KVP-ARP-Labour Party cabinet under Cals, which itself also fell the next year. In the following 1967 elections the VVD remained relatively stable and entered yet again the cabinet under Prime Minister Piet de Jong.
During this period the VVD had loose ties with other liberal organisations and together they formed the neutral pillar. This included the liberal papers Nieuwe Rotterdamsche Courant and the Algemeen Handelsblad, the broadcaster AVRO and the employers' organization VNO.
1971–1994.
In the Dutch general election of 1971 the VVD lost one seat and the cabinet lost its majority. A cabinet was formed by the Christian-democratic parties, the VVD and the Labour Party offshoot Democratic Socialists '70. This cabinet fell after a few months. Meanwhile the charismatic young MP Hans Wiegel had attracted considerable attention. He became the new leader of the VVD: in 1971 he became the new Parliamentary group leader, and in 1972 he was appointed lijsttrekker. With Wiegel the VVD oriented towards a new political course, reforming the welfare state, cutting taxes etc. Wiegel did not shrink from conflict with the Labour Party and the trade unions. With this new course came a new electorate: working class and middle class voters, who because of individualization and depillarization were more easy to attract.
The course proved to be profitable: in the heavily polarized Dutch general election of 1972 the VVD gained six seats. The VVD was kept out of government by the social-democratic/Christian-democratic cabinet led by Den Uyl. Although the ties between the VVD and other organizations within the neutral pillar became ever looser, the number of neutral organizations, friendly to the VVD, expanded. The TROS and later Veronica, new broadcasters which entered the Netherlands Public Broadcasting were friendly to the VVD. In 1977 the VVD again won six seats bringing its total to twenty-eight seats. When lengthy formation talks between the Social-democrats and Christian-democrats eventually led to a final break between the two parties, the VVD formed cabinet with the Christian Democratic Appeal (CDA), with a majority of only two seats.
In the Dutch general election of 1981 the VVD lost two seats and its partner the CDA lost even more. The cabinet was without a majority and a CDA-Labour Party-D66 cabinet was formed, falling after only a few months. In 1982 Hans Wiegel left Parliament to become Queen's Commissioner in Friesland and was succeeded by Ed Nijpels. In the Dutch general election of 1982 Nijpels' VVD won ten seats, bringing its total up to 36. It entered again cabinet with the CDA under CDA-leader Ruud Lubbers. The cabinet began a program of radical reform to the welfare state, which is still in place today. The VVD lost nine seats in the 1986 elections but the cabinet nonetheless retained its majority. The losses were blamed on Nijpels, who stood down as leader of the VVD. He was succeeded by Joris Voorhoeve. In 1989 the CDA-VVD cabinet fell over a minor point. In the subsequent elections the VVD lost five seats, leaving only twenty-two. The VVD was kept out of government. Voorhoeve was replaced by the charismatic intellectual Frits Bolkestein.
1994–present.
Bolkestein's VVD was one of the winners of the Dutch general election of 1994: they won nine seats. It formed an unprecedented government with the Labour Party (PvdA) and the social-liberal Democrats 66. The so-called "purple cabinet" led by Wim Kok was the first Dutch government without any confessional parties since 1918. Like many of his predecessors, the VVD-leader Bolkestein remained in parliament. His political style was characterized by some as "opposition to one's own government". This style was very successful and the VVD won seven seats in the 1998 elections becoming the second largest party in parliament with thirty-eight seats. The VVD formed a second Purple cabinet with the Labour Party and D66. Bolkestein left Dutch politics in 1999 to become European Commissioner. He was replaced by the more technocratic and socially liberal Hans Dijkstal.
In the heavily polarized Dutch general election of 2002, dominated by the rise and murder of Pim Fortuyn, the VVD lost fourteen seats, leaving only twenty-four. The VVD nonetheless entered a cabinet with the CDA and the Pim Fortuyn List (LPF). Dijkstal stood down, and was replaced by the popular former minister of finance Gerrit Zalm. After a few months Zalm "pulled the plug" on the VVD-CDA-LPF-cabinet, after infighting between LPF ministers Bomhoff and Heinsbroek.
In the Dutch general election of 2003 the VVD gained four seats, making a total of twenty-eight. The party had expected to do much better, having adopted most of Fortuyn's proposals on immigration and integration. The VVD unwillingly entered the VVD-CDA-D66-cabinet with Zalm returning to the ministry of Finance. He was replaced as party leader by Jozias van Aartsen, former foreign minister. On September 2, 2004, VVD MP Geert Wilders left the party after a dispute with parliamentary leader Van Aartsen. He has chosen to continue as an independent in the House of Representatives.
In 2006 the party lost a considerable number of seats in the municipal elections, prompting parliamentary leader Jozias van Aartsen to step down. Willibrord van Beek was subsequently appointed parliamentary leader ad interim. In the subsequent party leadership run-off Mark Rutte was elected as the leader, beating Rita Verdonk and Jelleke Veenendaal.
The Dutch general election of 2006 did not start off well for the VVD: top candidate Mark Rutte was criticized by his own parliamentary party for being invisible in the campaign, and he was unable to break the attention away from the duel between current Christian-Democratic Prime Minister Jan Peter Balkenende and Wouter Bos of the Labour Party. However, the VVD's campaign started relatively late. The election polls showed losses for the VVD; the former VVD deputy Prime Minister Hans Wiegel blamed a poor VVD campaign for this, caused by the heavily contested VVD leadership run-off between Mark Rutte and Rita Verdonk earlier in the year. Verdonk had her eyes on the deputy-minister post, while cabinet posts are normally decided upon by the political leader of the VVD. On election day, the party received enough votes for twenty-two seats, a loss of six seats. When the official election results were announced on Monday 27 November 2006, preferential votes became known as well, showing that the second candidate on the list Rita Verdonk obtained more votes than the VVD's top candidate, Mark Rutte. Rutte received 553,200 votes, Verdonk 620,555. This lead Verdonk to call for a party commission that would investigate the party leadership position, as a consequence of the situation of her obtaining more votes in the general election than Rutte, creating a shortly-lived crisis in the party. A crisis was averted when Rutte called for an ultimatum on his leadership, which Verdonk had reconcile to, by rejecting her proposal for a party commission.
During 2007, signs of VVD infighting continued to play in the media. In June 2007, the former VVD minister Dekker presented a report on the previous elections, showing that the VVD lacked clear leadership roles, however the report did not single out individuals for blame for the party's losses.
After Verdonk renewed her criticism of the party in September 2007, she was expelled from the parliamentary faction, and subsequently relinquished her membership of the party, after reconciliation attempts proved futile. Verdonk started her own political movement, Proud of the Netherlands, subsequently. In opinion polls held after Verdonk's exit, the VVD is set to lose close to ten parliamentary seats in the next elections.
Jan van Zanen, chairman of the VVD's party board, announced in November 2007 that he would step down in May 2008, a year before his term would end. The rest of the board also announced that they would step down. On the same day of his announcement, honorary member Hans Wiegel called for the resignation of the board, because it could not keep Verdonk in the party. Wiegel also opinioned that the VVD should become part of a larger liberal movement, that would encompass the social-liberals Democrats 66, the Party for Freedom of Geert Wilders and Rita Verdonk's Proud of the Netherlands movement, although he found little resonance for this ideas from others.
In 2008, the VVD chose a new party chairman, Ivo Opstelten, the outgoing mayor of Rotterdam. Mark Rutte announced at the celebration of the party's sixth decennial that he would rewrite the foundational program of the party that was enacted in the early 1980s, and offer the new principles for consideration to the party's members in the fall congress.
After the Dutch general election of 2010 the VVD became the largest party with 31 seats and was the senior party in a centre-right minority First Rutte cabinet with the Christian Democratic Appeal supported by the Party for Freedom of Geert Wilders to obtain a majority. Rutte was sworn in as Prime Minister on 21 October 2010, becoming both the first VVD Prime Minister and the first liberal to hold the post in 92 years. However, on 21 April 2012, after failed negotiations with the Party for Freedom on renewed budget cuts, the government became unstable and Mark Rutte deemed it likely that new elections would be held in 2012.
On election day, 12 September 2012, the VVD became once more the biggest party within Dutch Parliament, winning 41 seats, a gain of 10 seats. Mark Rutte seems to be, in September 2012, the only European political leader who hasn't been voted out of office by the electorate during the euro-crisis.
Name.
The VVD was originally a merger of the Party of Freedom and Freethinking Democratic dissenters within the Labour Party. In this name, both tendencies, classical liberalism ("Freedom") and social liberalism ("People's Party"; "Democracy") are represented. Despite being a liberal party, the VVD did not openly call itself "liberal", mainly because of the still lingering "negative" connotations of liberalism developed during the Great Depression and Second World War.<br> The most common English translation of the name is the literal translation (People's Party for Freedom and Democracy).
Ideology and issues.
The VVD is a party founded on liberal philosophy, traditionally being the most ardent supporter of 'free markets' of all Dutch political parties, promoting political, economic liberalism, classical liberalism, cultural liberalism, but also (in contrast to this) committed to the idea of the welfare state.
Post 1971, the party became more populist, although some conservative liberal elements remain. The 2006 leadership election was interpreted by many as a conflict between a liberal group and a conservative group within the VVD, with the distinctly liberal Rutte beating conservative Verdonk. The results were, with 52% voting for Rutte and 46% for Verdonk.
Liberal Manifesto.
The principles of the People's Party for Freedom and Democracy (VVD) are outlined in the "Liberal Manifesto" ("Liberaal Manifest") and the election programs. The "Liberal Manifesto" is a general outlook on the direction of the party would like to mirror itself and is an expansion of the party's foundational principles. The election programs are more oriented to practical politics, for example, winning the elections on-the-day and by any means possible.
The last "Liberal Manifesto" of the VVD was published in September 2005. It develops a broad outline around the themes of democracy, security, freedom and citizenship, along with a vision of the future of party's internal structure. Below some of the points from the Manifesto are presented:
Representation.
Members of the cabinet.
Ten members of the Second Rutte cabinet (since 2012):
Members of the House of Representatives.
Current members.
Current members of the House of Representatives since the general election of 2012:
Members of the Senate.
Current members.
Current members of the Senate since the election of 2011:
Seats.
Seats in the Senate:
Members of the European Parliament.
Current members.
Current members of the European Parliament since the election of 2014:
Seats.
Seats in the European Parliament:
VVD MEPs are part of the Alliance of Liberals and Democrats for Europe Party and Alliance of Liberals and Democrats for Europe Group in the European parliament.
Municipal and provincial government.
Provincial government.
The VVD provides four of twelve King's Commissioners. The VVD is part of every college of Gedeputeerde Staten (provincial executive) except for Friesland.
In the following figure one can see the election results of the provincial election of 2003, 2007 and 2011 per province. It shows the areas where the VVD is strong, namely the Randstad urban area that consists out of the provinces North and South Holland, Utrecht and (parts of) Flevoland. The party is weak in peripheral provinces like Friesland, Overijssel, Zeeland and Limburg.
Municipal government.
109 of the 414 Dutch mayors are member of the VVD. Furthermore the party has about 250 aldermen and 1100 members of municipal councils.
Electorate.
Historically the VVD electorate consisted mainly of secular middle-class and upper-class voters, with a strong support from entrepreneurs. Under the leadership of Wiegel, the VVD started to expand its appeal to working class voters.
Organization.
Organizational structure.
The highest organ of the VVD is the General Assembly, in which all members present have a single vote. It convenes usually twice every year. It appoints the party board and decides on the party program.
The order of the First Chamber, Second Chamber and European Parliament candidates list is decided by a referendum under all members voting by internet, phone or mail. If contested, the lijsttrekker of a candidates lists is appointed in a separate referendum in advance. Since 2002 the General Assembly can call for a referendum on other subjects too. The present chairman of the board was elected this way.
About 90 members elected by the members in meetings of the regional branches form the Party Council, which advises the Party Board in the months that the General Assembly does not convene. This is important forum within the party. The party board handles the daily affairs of the party.
Linked organisations.
The independent youth-organization that has a partnership agreement with the VVD is the Youth Organisation Freedom and Democracy (Jongeren Organisatie Vrijheid en Democratie; JOVD), which as a member of the Liberal Youth Movement of the European Union and the International Federation of Liberal and Radical Youth.
The education institute of the VVD is the "Haya van Someren Foundation". The Scientific institute "Telders Foundation" publishes the magazine "Liberaal Reveil" every two months. The party published the magazine "Liber" bi-monthly.
International organisations.
The VVD is a member of the Alliance of Liberals and Democrats for Europe Party and Liberal International.
Relationships to other parties.
The VVD has always been a very independent party. The VVD cooperates on the European and the international level with the social-liberal Democraten 66. It has a long history of coalitions with the Christian Democratic Appeal and its Christian-democratic predecessors, but was in government with the social-democratic Labour Party from 1994 to 2002 and again since 2012.
The VVD participates in the Netherlands Institute for Multiparty Democracy, a democracy assistance organisation of seven Dutch political parties.

</doc>
<doc id="49066" url="http://en.wikipedia.org/wiki?curid=49066" title="Labour Party (Netherlands)">
Labour Party (Netherlands)

The Labour Party (Dutch: "Partij van de Arbeid"; ], shortened PvdA ]) is a social-democratic political party in the Netherlands. Since 5 November 2012, the PvdA has governed in coalition with the People's Party for Freedom and Democracy (VVD) in the second Rutte cabinet.
Party history.
1945–1965.
The Labour Party (PvdA) was founded on 9 February 1946, through a merger of three parties: the Social Democratic Workers' Party (SDAP), the social-liberal Free-thinking Democratic League (VDB) and progressive-Protestant Christian Democratic Union (CDU). They were joined by individuals from Catholic resistance group "Christofoor" and the Protestant parties Christian Historical Union (CHU) and Anti Revolutionary Party (ARP).
The founders of the PvdA wanted to create a broad party, breaking with the historic tradition of Pillarisation. This desire to come to a new political system was called the "Doorbraak". The party combined socialists with liberal democrats and progressive Christians. However, the party was unable to break Pillarisation. Instead the new party renewed the close ties that SDAP had with other socialist organisations (see linked organisations). In 1948 some liberal members, led by former VDB leader Pieter Oud, left the PvdA because they were unhappy with the socialist course of the PvdA. Together with the Freedom Party, they formed the People's Party for Freedom and Democracy (VVD), a conservative-liberal party.
Between 1946 and 1958, the PvdA formed coalition governments with the Catholic People's Party (KVP), and combinations of VVD, ARP and CHU. The KVP and the PvdA together had a large majority in parliament. Since 1948, these cabinets were led by PvdA Prime Minister Willem Drees. Under his leadership the Netherlands recovered from the war, began to build its welfare state and Indonesia became independent.
After the cabinet crisis of 1958, the PvdA was replaced by the VVD. The PvdA was in opposition until 1965. The electoral support of PvdA voters began to decline.
1965–1989.
In 1965 a conflict in the KVP-ARP-CHU-VVD cabinet made continuation of the government impossible. The three confessional, Christian-influenced parties turned towards the PvdA. Together they formed the Cals cabinet. This cabinet was also short lived and conflict ridden. The conflicts culminated in the fall of the Cals cabinet over economic policy.
Meanwhile, a younger generation was attempting to gain control of the PvdA. A group of young PvdA members, calling themselves the New Left, changed the party. The New Left wanted to reform the PvdA: they believed the party should become oriented towards the new social movements, adopting their anti-parliamentary strategies and their issues, such as women's liberation, environmental conservation and Third World development. Prominent New Left members were Jan Nagel, André van der Louw and Bram Peper. One of their early victories followed the fall of the Cals cabinet. The party Congress adopted a motion that made it impossible for the PvdA to govern with the KVP and its Protestant allies. In response to the growing power of the New Left group, a group of older, centrist party members, led by Willem Drees' son, Willem Drees, Jr. founded the New Right. In 1970, it was clear that they lost the conflict within the party and left, founding the party Democratic Socialists '70 (DS70).
Under the New Left, the PvdA started a strategy of polarisation, striving for a cabinet based on a progressive majority in parliament. In order to form that cabinet the PvdA allied itself with the social-liberal party Democrats 66 (D66) and the progressive Christian Political Party of Radicals (PPR). The alliance was called the Progressive Accord (PAK). In the 1971 and 1972 elections, these three parties promised to form a cabinet with a radical common program after the elections. They were unable to gain a majority in either election. In 1971, they were kept out of cabinet, and the party of former PvdA members, DS70, became a partner of the First Biesheuvel cabinet.
In the 1972 elections, neither the PvdA and its allies or the KVP and its allies were able to gain a majority. The two sides were forced to work together. Joop den Uyl, the leader of the PvdA, led the cabinet. The cabinet was an extra-parliamentary cabinet and it was composed of members of the three progressive parties and members of the KVP and the ARP. The cabinet attempted to radically reform government, society and the economy, and a wide range of progressive social reforms were enacted during its time in office, such as significant increases in welfare payments and the indexation of benefits and the minimum wage to the cost of living.
However, it also faced economic decline and was riddled with personal and ideological conflicts. Especially, the relationship between Prime Minister Den Uyl and the KVP Deputy Prime Minister, Van Agt was very problematic. The conflict culminated just before the 1977 elections, the cabinet fell. The 1977 general election were won by the PvdA, but the ideological and personal conflict between Van Agt and Den Uyl prevented the formation of a new centre-left cabinet. After very long cabinet formation talks, the Christian Democratic Appeal (CDA), itself a new Christian democratic political formation composed of KVP, CHU and ARP, formed government with the VVD, based on a very narrow majority. The PvdA was left in opposition.
In the 1981 general election, the incumbent CDA-VVD cabinet lost their majority. The CDA remained the largest party, but it was forced to co-operate with the PvdA and D66 (the PPR had left the alliance, after losing the 1977 elections). In the new cabinet led by Van Agt, Den Uyl returned to cabinet, now as Deputy Prime Minister. The personal and ideological conflict between Van Agt and Den Uyl culminated in the fall of the cabinet just months after it was formed. The VVD and the CDA regained their majority in the 1982 general election and retained it in the 1986 general election. The PvdA was left in opposition. During this period, the party began to reform. In 1986, Den Uyl left politics, appointing former trade union leader Wim Kok as his successor.
1989–2010.
After the 1989 general election, the PvdA returned to cabinet together with the CDA. Kok became Deputy Prime Minister to CDA leader Ruud Lubbers. The PvdA accepted the major economic reforms the previous Lubbers cabinets made, including privatisation of public enterprises and reform of the welfare state. They continued these policies in this cabinet. The cabinet faced heavy protest from the unions and saw major political conflict within the PvdA itself.
In the 1994 general election, the PvdA and CDA coalition lost its majority in parliament. The PvdA however emerged as the biggest party. Kok formed a government together with the conservative-liberal VVD and social-liberal D66. The so-called "purple government" was a political novelty, because the Christian Democrats had been in government since 1918. The first Kok cabinet continued the economic reforms, but combined this with a progressive outlook on ethical questions and promises of political reform. Kok became a very popular prime minister. Kok was not a partisan figure, but combined successful technocratic policies with the charisma of a national leader. In the 1998 general election, the cabinet was rewarded for its stewardship of the economy. The PvdA and the VVD increased their seats, at the cost of D66.
The PvdA was expected to perform very well in the 2002 general election. Kok left politics leaving the leadership of the party to his preferred successor Ad Melkert. But the political rise of Pim Fortuyn frustrated these hopes. The PvdA lost the 2002 elections, and the party's parliamentary representation fell from 45 seats to 23. The loss was blamed on the uncharismatic new leader Melkert, the perceived arrogance of the PvdA and the inability to answer to the right-wing populist issues Fortuyn raised, especially immigration and integration. Melkert resigned as party leader and was replaced by Jeltje van Nieuwenhoven. The PvdA was kept out of cabinet. The government formed by CDA, VVD and the Pim Fortuyn List (LPF) fell after a very short period.
Meanwhile, Wouter Bos, State Secretary in the second purple cabinet, was elected leader of the PvdA in a referendum among PvdA members, being elected closely to Jouke de Vries. He started to democratise the party organisation and began an ideological reorientation. In the 2003 general election, Wouter Bos managed to regain almost all seats lost in the previous election, and the PvdA was once again the second largest party in the Netherlands, only slightly smaller than the CDA. Personal and ideological conflicts between Bos and the CDA leader Jan Peter Balkenende prevented the formation of a CDA-PvdA cabinet. Instead, the PvdA was kept out of government by the formation of cabinet of the CDA, the VVD, and D66, the latter being former allies of PvdA. In the 2006 municipal elections, the renewed PvdA performed very well. The PvdA became by far the largest party nationally, while the three governing parties lost a considerable number of seats in municipal councils.
It was expected that the PvdA would do well in the upcoming 2006 general election, but the party lost the race for Prime Minister to the CDA after suffering a loss of 9 seats. The PvdA now held only 33 seats, losing many votes to the Socialist Party (SP). The PvdA had previously distanced themselves from the idea of a voting bloc on the left. It did however join the fourth Balkenende cabinet o 22 February 2007, in which Wouter Bos became minister of Finance. In the aftermath of the lost elections the entire party executive stepped down on 26 April 2007. On Saturday 20 February 2010, the Labour Party withdrew from the government after arguments over the Dutch role in Afghanistan.
2010–present.
After withdrawing from the government, Wouter Bos announced he would leave politics to spend more time with his wife and two daughters. Then mayor of Amsterdam, Job Cohen, took his place as leader of the PvdA. In the 2010 election, the PvdA won 30 seats, a loss of three, and were narrowly overtaken by the VVD. After the election, a "Purple Coalition" was considered - it would have required a fourth party in addition to the VVD, PvdA and D66 - but talks broke down and the PvdA entered opposition.
Cohen resigned as leader in February 2012. Diederik Samsom was subsequently elected the party leader. In the 2012 general election, the Labour Party won 38 seats, a gain of 8, defying initial predictions that the Socialist Party would overtake them. Following the election the party entered a governing coalition with the VVD under Mark Rutte, with Labour's Lodewijk Asscher becoming Deputy Prime Minister.
Ideology and issues.
The PvdA began as a traditional social-democratic party, committed to building a welfare state. During the 1970s, it radicalised its program and included new issues, such as women's liberation, environmental conservation and Third World development. During the 1990s, it moderated its program, including reform of the welfare state and privatisation of public enterprise. In 2005, the party adopted a new program of principles, expressing a centre-left ideology. Its core issues are employment, social security and welfare, and investing in public education, public safety and health care.
Representation.
1: In combined PvdA/PPR groups (estimate).
Members of the cabinet.
Ten members of the second Rutte cabinet (2012-):
Members of the House of Representatives.
After the 2012 election, the party has 38 representatives in the House of Representatives. In November 2014, two members left the party:
Members of the Senate.
Following the 2011 Senate election, the party has 14 representatives in the Senate:
Members of the European Parliament.
PvdA MEPs sit as part of the Progressive Alliance of Socialists and Democrats, the parliamentary group of the Party of European Socialists.
After the 2014 European Parliament elections, the party has 3 representatives in the European Parliament:
Municipal and provincial government.
Provincial government.
Three of the twelve King's Commissioners are members of the PvdA (Drenthe, Flevoland and Groningen). The party cooperates in eight States Deputed (Groningen, Friesland, Drenthe, Gelderland, Flevoland, North Holland, Limburg and Zeeland).
Municipal government.
100 of the 379 mayors of the Netherlands are members of the PvdA (September 2010). The best known of them is Ahmed Aboutaleb, mayor of Rotterdam. The party cooperates in many municipal executives, among others the big four (Amsterdam, Rotterdam, The Hague and Utrecht). The PvdA obtained 799 seats in the 2014 municipal elections.
Electorate.
Historically, the PvdA was supported by the working class. Currently the party is supported relatively well by civil servants, migrants, and the working class. The party has historically been very strong in the major cities, such as Amsterdam and Rotterdam, and in the northern provinces of Groningen, Friesland and Drenthe.
Organisation.
Organisational structure.
The highest organ of the PvdA is the Congress, formed by delegates from the municipal branches. It convenes once every year. It appoints the party board, decides the order of candidates on electoral lists for the Senate, House of Representatives and European Parliament and has the final say over the party program. Since 2002, a referendum of all members has partially replaced the Congress. Both the lijsttrekker of the House of Representatives candidate list, who is the political leader of the party, and the party chairman, who leads the party organisation, are selected by such a referendum. In 2002, Wouter Bos won the PvdA leadership election.
Members.
The PvdA currently has 62,000 members. They are organised in over 500 municipal branches.
Linked organisations.
The Young Socialists in the PvdA is the youth organisation of the PvdA. It is a member of Young European Socialists and the International Union of Socialist Youth. They publish the periodical "Lava".
"Rood" is the party periodical. It appears eight times a year.
The scientific institute (or think tank) of the PvdA is the Wiardi Beckman Foundation. It publishes the periodical "Socialisme & Democratie".
The PvdA participates in the Netherlands Institute for Multiparty Democracy, a democracy assistance organisation of seven Dutch political parties.
International organisations.
The PvdA is a full member of the Party of European Socialists and was formerly an observer member of the Socialist International until December 2014, having previously downgraded their membership in December 2012. The PvdA joined the Progressive Alliance, a new international network for social-democratic political parties, at its founding event on 22 May 2013.
Pillarised organisations.
During the period of strong pillarisation the PvdA had strong links with the social-democratic broadcasting organisation VARA Broadcasting Association, the Dutch Association of Trade Unions, and the paper "Het Vrije Volk".
Relationships to other parties.
Historically, the PvdA has co-operated in cabinets with the Christian-democratic Christian Democratic Appeal (CDA), Political Party of Radicals (PPR), Catholic People's Party (KVP), Anti Revolutionary Party (ARP), Christian Historical Union (CHU) and ChristianUnion (CU) parties and the liberal parties Democrats 66 (D66) and People's Party for Freedom and Democracy (VVD). Between 1971 and 1977, PvdA was allied with D66 and the PPR. After 1977 until 1989, it was closely allied to D66. Since 2003, the relationship between the PvdA and D66 has considerably worsened, at first because PvdA was in opposition to the Second Balkenende cabinet which D66 had co-operated in.
During the governance of the second and third Balkenende cabinet, the Socialist Party and the GreenLeft were calling for closer cooperation with the PvdA, calling to form a shadow government against the Balkenende cabinet, PvdA leader Bos held this off.

</doc>
<doc id="49069" url="http://en.wikipedia.org/wiki?curid=49069" title="Pope Eugene IV">
Pope Eugene IV

Pope Eugene IV (Latin: "Eugenius IV"; 1383 – 23 February 1447), born Gabriele Condulmer, was Pope from 3 March 1431 to his death in 1447. He is the last pope to take the name "Eugene" upon his election.
Biography.
Early life.
Condulmaro was born in Venice to a rich merchant family. He entered a community of Canons Regular of San Giorgio in Alga in his native city. At the age of twenty-four he was appointed by his maternal uncle, Pope Gregory XII, as Bishop of Siena. In Siena, the political leaders objected to a bishop who was not only 24, but also a foreigner. Therefore, he resigned the appointment, becoming instead his uncle's papal treasurer, protonotary and Cardinal Priest of the Basilica of San Clemente. Pope Martin V named him Cardinal Priest of the Basilica di Santa Maria in Trastevere.
Papacy.
Condulmer made himself useful to Pope Martin as legate in Picenum and was quickly elected to succeed him in the papal conclave of 1431. He was crowned as Eugene IV at St. Peter's Basilica on 11 March 1431. By a written agreement made before his election he pledged to distribute to the cardinals one-half of all the revenues of the Church and promised to consult with them on all questions of importance, both spiritual and temporal. He is described as tall, thin, with a winning countenance, although many of his troubles were owing to his own want of tact, which alienated parties from him. Upon assuming the papal chair, Eugene IV took violent measures against the numerous Colonna relatives of his predecessor Martin V, who had rewarded them with castles and lands. This at once involved him in a serious contest with the powerful house of Colonna that nominally supported the local rights of Rome against the interests of the Papacy. A truce was soon arranged.
Conciliar reform and papal misfortunes.
By far the most important feature of Eugene IV's pontificate was the great struggle between the Pope and the Council of Basel (1431–39), the final embodiment of the Conciliar movement. On 23 July 1431, his legate Giuliano Cesarini opened the council, which had been convoked by Martin V, but, distrustful of its purposes and emboldened by the small attendance, the Pope issued a bull on 18 December 1431 that dissolved the council and called a new one to meet in eighteen months at Bologna. The council resisted this expression of papal prerogative. Eugene IV's action gave some weight to the contention that the Curia was opposed to any authentic measures of reform. The council refused to dissolve; instead they renewed the resolutions by which the Council of Constance had declared a council superior to the Pope and ordered Eugene IV to appear at Basel. A compromise was arranged by the Holy Roman Emperor Sigismund, who had been crowned emperor at Rome on 31 May 1433. By its terms, the Pope recalled his bull of dissolution, and, reserving all the rights of the Holy See, acknowledged the council as ecumenical on 15 December 1433 except for the initial unapproved sessions that contained canons which exalted conciliar authority above that of the pope.
These concessions also were due to the invasion of the Papal States by the former Papal condottiero Niccolò Fortebraccio and the troops of Filippo Maria Visconti led by Niccolò Piccinino in retaliation for Eugene's support of Florence and Venice against Milan (see also Wars in Lombardy). This situation led also to establishment of an insurrectionary republic at Rome controlled by the Colonna family. In early June 1434, disguised in the robes of a Benedictine monk, Eugene was rowed down the center of the Tiber, pelted by stones from either bank, to a Florentine vessel waiting to pick him up at Ostia. The city was restored to obedience by Giovanni Vitelleschi, the militant Bishop of Recanati, in the following October. In August 1435 a peace treaty was signed at Ferrara by the various belligerents. The Pope moved to Bologna in April 1436. His condottieri Francesco Sforza and Vitelleschi in the meantime reconquered much of the Papal States. Traditional Papal enemies such as the Prefetti di Vico were destroyed, while the Colonna were reduced to obedience after the destruction of their stronghold in Palestrina in August 1436.
Eugenius resurgent.
Meanwhile the struggle with the council sitting at Basel broke out anew. Eugene IV at length convened a rival council at Ferrara on 8 January 1438 and excommunicated the prelates assembled at Basel. King Charles VII of France had forbidden members of the clergy in his kingdom from attending the counsel in Ferrara, and introduced the decrees of the Council of Basel, with slight changes, into France through the Pragmatic Sanction of Bourges (7 July 1438). The King of England and the Duke of Burgundy, who felt that the council was partial to France, decided not to recognize the council at Basel. Castile, Aragon, Milan, and Bavaria withdrew support. 
The Council of Basel suspended Eugene on 24 January 1438, then formally deposed him as a heretic on 25 June 1439. In the following November the council elected the ambitious Amadeus VIII, Duke of Savoy, as antipope under the name of Felix V. The Diet of Mainz had deprived the Pope of most of his rights in the Empire (26 March 1439).
At Florence, where the council of Ferrara had been transferred as a result of an outbreak of the plague, a union with the Eastern Orthodox Church was effected in July 1439, which, as the result of political necessities, proved but a temporary bolster to the papacy's prestige. This union was followed by others of even less stability. Eugene IV signed an agreement with the Armenians on 22 November 1439, and with a part of the Jacobites of Syria in 1443, and in 1445 he received the Nestorians and the Maronites. He did his best to stem the Turkish advance, pledging one-fifth of the papal income to a crusade which set out in 1443, but which met with overwhelming defeat at the Battle of Varna. Cardinal Cesarini, the papal legate, perished in the rout.
Eugene's rival Felix V in the meantime obtained scant recognition, even in the Empire. Eventually Holy Roman Emperor Frederick III moved toward acceptance of Eugene. One of the king's ablest advisers, the humanist Aeneas Sylvius Piccolomini, who was later to be Pope Pius II, made peace with Eugene in 1442. The Pope's recognition of the claim to Naples of King Alfonso V of Aragon (in the treaty of Terracina, approved by Eugenius at Siena somewhat later) withdrew the last important support in Italy from the Council of Basel. In 1442 Eugene, Alfonso and Visconti sent Niccolò Piccinino to reconquer the March of Ancona from Francesco Sforza; but the defeat of the allied army at the Battle of Montolmo pushed the Pope to reconcile with Sforza.
So enabled, Eugene IV made a victorious entry into Rome on 28 September 1443 after an exile of nearly ten years.
His protests against the Pragmatic Sanction of Bourges were ineffectual, but by means of the Concordat of the Princes, negotiated by Piccolomini with the electors in February 1447, the whole of Germany declared against the antipope. This agreement was completed only after Eugene's death.
Eugenius on slavery.
Christianity had gained many converts in the Canary Islands by the early 1430s. However, the ownership of the lands had been the subject of dispute between Portugal and the Kingdom of Castille. The lack of effective control had resulted in periodic raids on the islands to procure slaves. As early as the Council of Koblenz in 922, the capture of Christians as slaves by other Christians had been condemned.
Acting on a complaint by Fernando Calvetos, bishop of the islands, Pope Eugene IV issued a Papal bull, "Creator Omnium", on 17 December 1434, annulling previous permission granted to Portugal to conquer those islands still pagan. Eugene excommunicated anyone who enslaved newly converted Christians, the penalty to stand until the captive was restored to their liberty and possessions.
Portuguese soldiers continued to raid the islands during 1435, and Eugene issued a further edict "Sicut Dudum" that prohibited wars being waged against the islands and affirming the ban on enslavement. Eugene condemned the enslavement of the peoples of the newly colonized Canary Islands and, under pain of excommunication, ordered all such slaves to be immediately set free. 
Eugene tempered "Sicut Dudum" in September 1436 with the issuance of a papal bull in response to complaints made by King Edward of Portugal that allowed the Portuguese to conquer any unconverted parts of the Canary Islands. According to Raiswell (1997), any Christian would be protected by the earlier edict but the un-baptized were implicitly allowed to be enslaved.
Following the arrival of the first African slaves in Lisbon during the year 1441, Prince Henry asked Eugene to designate Portugal's raids along the West African coast as a crusade, a consequence of which would be the legitimization of enslavement for captives taken during the crusade. On 19 December 1442, Eugene replied by issuing the bull "Illius qui", in which he granted full remission of sins to those who took part in any expeditions against the Saracens. Davidson (1961) asserts that "In Christianity as in Islam...the heathen was expendable".
Richard Raiswell argues that the bulls of Eugene helped in some way the development of thought which perceived the enslavement of Africans by the Portuguese and later Europeans "as dealing a blow for Christendom". Joel S Panzer views "Sicut Dudum" as a significant condemnation of slavery, issued sixty years before the Europeans found the New World. 
Death and legacy.
Although his pontificate had been so stormy and unhappy that he is said to have regretted on his deathbed that he ever left his monastery, Eugene IV's victory over the Council of Basel and his efforts on behalf of church unity nevertheless contributed greatly to the breakdown of the conciliar movement and restored the papacy to a semblance of the dominant position it had held before the Western Schism (1378–1417). This victory had been gained, however, by making concessions to the princes of Europe. Thereafter, the papacy had to depend more for its revenues on the Papal States.
Eugene IV was dignified in demeanour, but inexperienced and vacillating in action and excitable in temper. Bitter in his hatred of heresy, he nevertheless displayed great kindness to the poor. He laboured to reform the monastic orders, especially the Franciscans, and was never guilty of nepotism. Although austere in his private life, he was a sincere friend of art and learning, and in 1431 he re-established the university at Rome. He also consecrated Florence Cathedral on 25 March 1436. Eugene was buried at Saint Peter's by the tomb of Pope Eugene III. Later his tomb was transferred to San Salvatore in Lauro, a parish church on the other bank of the Tiber River.

</doc>
<doc id="49070" url="http://en.wikipedia.org/wiki?curid=49070" title="Avalanche chess">
Avalanche chess

Avalanche chess is a chess variant designed by Ralph Betza in 1977. After moving one of your own pieces, you "must" move one of your opponent's pawns forward one space.
Rules.
Rules are as normal chess except for the following. After you move one of your own pieces, you "must" move one of your opponent's pawns one space forward toward you. You cannot use your opponent's pawn to capture and you cannot move your opponent's pawn two spaces forward. If none of opponent's pawns can be moved, then that part of the turn is skipped. If you must move your opponent's pawn to promotion, then your opponent chooses to what piece it promotes. If moving an opponent's pawn gives check to you, then your opponent wins the game immediately.

</doc>
<doc id="49071" url="http://en.wikipedia.org/wiki?curid=49071" title="Oceanus">
Oceanus

Oceanus (; Greek: Ὠκεανός "Ōkeanós", ]) was a divine figure in classical antiquity, believed by the ancient Greeks and Romans to be the divine personification of the sea, an enormous river encircling the world.
Strictly speaking, Oceanus was the ocean-stream at the Equator in which floated the habitable hemisphere (οἰκουμένη, "oikoumene"). In Greek mythology, this world-ocean was personified as a Titan, a son of Uranus and Gaea. In Hellenistic and Roman mosaics, this Titan was often depicted as having the upper body of a muscular man with a long beard and horns (often represented as the claws of a crab) and the lower body of a serpent (cf. "Typhon"). On a fragmentary archaic vessel of circa 580 BC (British Museum 1971.11-1.1), among the gods arriving at the wedding of Peleus and the sea-nymph Thetis, is a fish-tailed Oceanus, with a fish in one hand and a serpent in the other, gifts of bounty and prophecy. In Roman mosaics, such as that from Bardo he might carry a steering-oar and cradle a ship.
Some scholars believe that Oceanus originally represented all bodies of salt water, including the Mediterranean Sea and the Atlantic Ocean, the two largest bodies known to the ancient Greeks. However, as geography became more accurate, Oceanus came to represent the stranger, more unknown waters of the Atlantic Ocean (also called the "Ocean Sea"), while the newcomer of a later generation, Poseidon, ruled over the Mediterranean.
Oceanus' consort is his sister Tethys, and from their union came the ocean nymphs, also known as the three-thousand Oceanids, and all the rivers of the world, fountains, and lakes. From Cronus, of the race of Titans, the Olympian gods have their birth, and Hera mentions twice in "Iliad" book XIV her intended journey "to the ends of the generous earth on a visit to Oceanus, whence the gods have risen, and Tethys our mother who brought me up kindly in their own house."
In most variations of the war between the Titans and the Olympians, or Titanomachy, Oceanus, along with Prometheus and Themis, did not take the side of his fellow Titans against the Olympians, but instead withdrew from the conflict. In most variations of this myth, Oceanus also refused to side with Cronus in the latter's revolt against their father, Uranus.
Etymology.
R. S. P. Beekes has suggested a Pre-Greek proto-form *"-kay-an-".
In ancient literature and legend.
This excerpt tells of the role of Oceanus in the Titanomachy:
After the first Dionysus [Zagreus] had been slaughtered, Father Zeus ... attacked the mother of the Titanes [Gaia the Earth] with avenging brand, and shut up the murderers of horned Dionysus [the Titans dismembered the godling Zagreus] within the gate of Tartarus [after a long war]: the trees blazed, the hair of suffering Gaea (Earth) was scorched with heat . . . Now Okeanos poured rivers of tears from his watery eyes, a libation of suppliant prayer. Then Zeus claimed his wrath at the sight of the scorched earth; he pitied her, and wished to wash with water the ashes of ruin and the fiery wounds of the land. Then Rainy Zeus covered the whole sky with clouds and flooded all the earth [in the Great Deluge of Deukalion].—Nonnus, Dionysiaca 6. 155 ff
In the "Iliad", the rich iconography of Achilles' shield, as fashioned by Hephaestus, is enclosed, as the world itself was believed to be, by Oceanus:
Then, running round the shield-rim, triple-ply,
he pictured all the might of the Ocean stream.
When Odysseus and Nestor walk together along the shore of the sounding sea ("Iliad" IX.182) they address their prayers "to the great Sea-god who girdles the world". It is to Oceanus, not to Poseidon, that their thoughts are directed.
Invoked in passing by poets and figured as the father of rivers and streams, and thus the progenitor of river gods, Oceanus appears only once in myth, as a representative of the archaic world that Heracles constantly threatened and bested.
Heracles forced Helios to lend him his golden bowl, in order to cross the wide expanse of the Ocean on his trip to the Hesperides. When Oceanus tossed the bowl about, Heracles threatened him and stilled his waves. The journey of Heracles in the sun-bowl upon Oceanus became a favored theme among painters of Attic pottery.
In cosmography.
Oceanus appears in Hellenic cosmography as well as myth. Cartographers continued to represent the encircling equatorial stream much as it had appeared on Achilles' shield.
Though Herodotus was skeptical about the physical existence of Oceanus, he rejected snowmelt as a cause of the annual flood of the Nile river; according to his translator and interpreter, Livio Catullo Stecchini, he left unsettled the question of an equatorial Nile, since the geography of Sub-Saharan Africa was unknown to him.
Apollonius of Rhodes calls the lower Danube the "Keras Okeanoio" (Gulf or Horn of Oceanus) in "Argonautica" (IV. 282).
"Accion" (Ocean) in the fourth century Gaulish Latin of Rufus Avienus', "Ora maritima", was applied to great lakes.
Both Homer ("Odyssey", XII. 1) and Hesiod ("Theogonia", v.242. 959) refer to "Okeanos Potamos", the "Ocean Stream",
Hecateus of Abdera writes that the Oceanus of the Hyperboreans is neither the Arctic Ocean nor Western Ocean, but the sea located to the north of the ancient Greek world, called "the most admirable of all seas" by Herodotus (lib. IV 85), called the "immense sea" by Pomponius Mela (lib. I. c. 19) and by Dionysius Periegetes ("Orbis Descriptio", v. 165), and which is named "Mare majus" on medieval geographic maps.
At the end of the "Okeanos Potamos", is the holy island of Alba (Leuke, Pytho Nisi, Isle of Snakes), sacred to the Pelasgian (and later, Greek) Apollo, greeting the sun rising in the east. Hecateus of Abdera refers to Apollo's island from the region of the Hyperboreans, in the Oceanus. It was on Leuke, in one version of his legend, that the hero Achilles, in a hilly tumulus, was buried (to this day, one of the mouths of the Danube is called Chilia). Leto, the Hyperborean goddess, after nine days and nine nights of labour on the island of Delos (Pelasgian for hill, related to tell) "gave birth to the great god of the antique light" (Pseudo-Apollodorus, "Bibliotheca", I. 4.1). Old Romanian folk songs sing of a white monastery on a white island with nine priests, nine singers, nine altars, on a part of the Black Sea known as the White Sea.

</doc>
<doc id="49072" url="http://en.wikipedia.org/wiki?curid=49072" title="Francis Galton">
Francis Galton

Sir Francis Galton, FRS (; 16 February 1822 – 17 January 1911) was an English Victorian progressive, polymath, sociologist, psychologist, anthropologist, eugenicist, tropical explorer, geographer, inventor, meteorologist, proto-geneticist, psychometrician, and statistician. He was knighted in 1909.
Galton produced over 340 papers and books. He also created the statistical concept of correlation and widely promoted regression toward the mean. He was the first to apply statistical methods to the study of human differences and inheritance of intelligence, and introduced the use of questionnaires and surveys for collecting data on human communities, which he needed for genealogical and biographical works and for his anthropometric studies.
He was a pioneer in eugenics, coining the term itself and the phrase "nature versus nurture". His book "Hereditary Genius" (1869) was the first social scientific attempt to study genius and greatness.
As an investigator of the human mind, he founded psychometrics (the science of measuring mental faculties) and differential psychology and the lexical hypothesis of personality. He devised a method for classifying fingerprints that proved useful in forensic science. He also conducted research on the power of prayer, concluding it had none by its null effects on the longevity of those prayed for.
As the initiator of scientific meteorology, he devised the first weather map, proposed a theory of anticyclones, and was the first to establish a complete record of short-term climatic phenomena on a European scale. He also invented the Galton Whistle for testing differential hearing ability.
He was cousin of Douglas Strutt Galton and half-cousin of Charles Darwin.
Biography.
Early life.
Galton was born at "The Larches", a large house in the Sparkbrook area of Birmingham, England, built on the site of "Fair Hill", the former home of Joseph Priestley, which the botanist William Withering had renamed. He was Charles Darwin's half-cousin, sharing the common grandparent Erasmus Darwin. His father was Samuel Tertius Galton, son of Samuel "John" Galton. The Galtons were famous and highly successful Quaker gun-manufacturers and bankers, while the Darwins were distinguished in medicine and science.
Both families boasted Fellows of the Royal Society and members who loved to invent in their spare time. Both Erasmus Darwin and Samuel Galton were founding members of the famous Lunar Society of Birmingham, whose members included Boulton, Watt, Wedgwood, Priestley, Edgeworth, and other distinguished scientists and industrialists. Likewise, both families were known for their literary talent: Erasmus Darwin composed lengthy technical treatises in verse; Galton's aunt Mary Anne Galton wrote on aesthetics and religion, and her notable autobiography detailed the unique environment of her childhood populated by Lunar Society members.
Galton was by many accounts a child prodigy – he was reading by the age of two; at age five he knew some Greek, Latin and long division, and by the age of six he had moved on to adult books, including Shakespeare for pleasure, and poetry, which he quoted at length . Later in life, Galton would propose a connection between genius and insanity based on his own experience. He stated, "Men who leave their mark on the world are very often those who, being gifted and full of nervous power, are at the same time haunted and driven by a dominant idea, and are therefore within a measurable distance of insanity"
Galton attended King Edward's School, Birmingham, but chafed at the narrow classical curriculum and left at 16. His parents pressed him to enter the medical profession, and he studied for two years at Birmingham General Hospital and King's College London Medical School. He followed this up with mathematical studies at Trinity College, University of Cambridge, from 1840 to early 1844.
According to the records of the United Grand Lodge of England, it was in February 1844 that Galton became a freemason at the so-called "Scientific" lodge, held at the Red Lion Inn in Cambridge, progressing through the three masonic degrees as follows: Apprentice, 5 February 1844; Fellow Craft, 11 March 1844; Master Mason, 13 May 1844. A curious note in the record states: "Francis Galton Trinity College student, gained his certificate 13 March 1845". One of Galton's masonic certificates from "Scientific" lodge can be found among his papers at University College, London.
A severe nervous breakdown altered Galton's original intention to try for honours. He elected instead to take a "poll" (pass) B.A. degree, like his half-cousin Charles Darwin . (Following the Cambridge custom, he was awarded an M.A. without further study, in 1847.) He then briefly resumed his medical studies. The death of his father in 1844 had left him financially independent but emotionally destitute, and he terminated his medical studies entirely, turning to foreign travel, sport and technical invention.
In his early years Galton was an enthusiastic traveller, and made a notable solo trip through Eastern Europe to Constantinople, before going up to Cambridge. In 1845 and 1846 he went to Egypt and travelled down the Nile to Khartoum in the Sudan, and from there to Beirut, Damascus and down the Jordan.
In 1850 he joined the Royal Geographical Society, and over the next two years mounted a long and difficult expedition into then little-known South West Africa (now Namibia). He wrote a successful book on his experience, "Narrative of an Explorer in Tropical South Africa". He was awarded the Royal Geographical Society's gold medal in 1853 and the Silver Medal of the French Geographical Society for his pioneering cartographic survey of the region . This established his reputation as a geographer and explorer. He proceeded to write the best-selling "The Art of Travel", a handbook of practical advice for the Victorian on the move, which went through many editions and is still in print.
In January 1853 Galton met Louisa Jane Butler (1822–1897) at his neighbour's home and they were married on 1 August 1853. The union of 43 years proved childless.
Middle years.
Galton was a polymath who made important contributions in many fields of science, including meteorology (the anti-cyclone and the first popular weather maps), statistics (regression and correlation), psychology (synaesthesia), biology (the nature and mechanism of heredity), and criminology (fingerprints). Much of this was influenced by his penchant for counting or measuring. Galton prepared the first weather map published in The Times (1 April 1875, showing the weather from the previous day, 31 March), now a standard feature in newspapers worldwide.
He became very active in the British Association for the Advancement of Science, presenting many papers on a wide variety of topics at its meetings from 1858 to 1899 . He was the general secretary from 1863 to 1867, president of the Geographical section in 1867 and 1872, and president of the Anthropological Section in 1877 and 1885. He was active on the council of the Royal Geographical Society for over forty years, in various committees of the Royal Society, and on the Meteorological Council.
James McKeen Cattell, a student of Wilhelm Wundt who had been reading Galton's articles, decided he wanted to study under him. He eventually built a professional relationship with Galton, measuring subjects and working together on research.
In 1888, Galton established a lab in the science galleries of the South Kensington Museum. In Galton's lab, participants could be measured to gain knowledge of their strengths and weaknesses. Galton also used these data for his own research. He would typically charge people a small fee for his services.
During this time, Galton wrote a controversial letter to the Times titled 'Africa for the Chinese', where he argued that the Chinese, as a race capable of high civilisation and (in his opinion) only temporarily stunted by the recent failures of Chinese dynasties, should be encouraged to immigrate to Africa and displace the supposedly inferior aboriginal blacks.
Heredity and eugenics.
The publication by his cousin Charles Darwin of "The Origin of Species" in 1859 was an event that changed Galton's life. He came to be gripped by the work, especially the first chapter on "Variation under Domestication," concerning the breeding of domestic animals.
Galton devoted much of the rest of his life to exploring variation in human populations and its implications, at which Darwin had only hinted. In so doing, he established a research program which embraced multiple aspects of human variation, from mental characteristics to height; from facial images to fingerprint patterns. This required inventing novel measures of traits, devising large-scale collection of data using those measures, and in the end, the discovery of new statistical techniques for describing and understanding the data.
Galton was interested at first in the question of whether human ability was hereditary, and proposed to count the number of the relatives of various degrees of eminent men. If the qualities were hereditary, he reasoned, there should be more eminent men among the relatives than among the general population. To test this, he invented the methods of historiometry. Galton obtained extensive data from a broad range of biographical sources which he tabulated and compared in various ways. This pioneering work was described in detail in his book "Hereditary Genius" in 1869. Here he showed, among other things, that the numbers of eminent relatives dropped off when going from the first degree to the second degree relatives, and from the second degree to the third. He took this as evidence of the inheritance of abilities.
Galton recognised the limitations of his methods in these two works, and believed the question could be better studied by comparisons of twins. His method envisaged testing to see if twins who were similar at birth diverged in dissimilar environments, and whether twins dissimilar at birth converged when reared in similar environments. He again used the method of questionnaires to gather various sorts of data, which were tabulated and described in a paper "The history of twins" in 1875. In so doing he anticipated the modern field of behaviour genetics, which relies heavily on twin studies. He concluded that the evidence favoured nature rather than nurture. He also proposed adoption studies, including trans-racial adoption studies, to separate the effects of heredity and environment.
Galton recognised that cultural circumstances influenced the capability of a civilisation's citizens, and their reproductive success. In "Hereditary Genius", he envisaged a situation conducive to resilient and enduring civilisation as follows:
The best form of civilization in respect to the improvement of the race, would be one in which society was not costly; where incomes were chiefly derived from professional sources, and not much through inheritance; where every lad had a chance of showing his abilities, and, if highly gifted, was enabled to achieve a first-class education and entrance into professional life, by the liberal help of the exhibitions and scholarships which he had gained in his early youth; where marriage was held in as high honour as in ancient Jewish times; where the pride of race was encouraged (of course I do not refer to the nonsensical sentiment of the present day, that goes under that name); where the weak could find a welcome and a refuge in celibate monasteries or sisterhoods, and lastly, where the better sort of emigrants and refugees from other lands were invited and welcomed, and their descendants naturalised. ()
Galton invented the term "eugenics" in 1883 and set down many of his observations and conclusions in a book, "Inquiries into Human Faculty and Its Development". He believed that a scheme of 'marks' for family merit should be defined, and early marriage between families of high rank be encouraged by provision of monetary incentives. He pointed out some of the tendencies in British society, such as the late marriages of eminent people, and the paucity of their children, which he thought were "dysgenic". He advocated encouraging eugenic marriages by supplying able couples with incentives to have children. On 29 October 1901, Galton chose to address eugenic issues when he delivered the second Huxley lecture at the Royal Anthropological Institute
The "Eugenics Review", the journal of the Eugenics Education Society, commenced publication in 1909. Galton, the Honorary President of the society, wrote the foreword for the first volume. The First International Congress of Eugenics was held in July 1912. Winston Churchill and Carls Elliot were among the attendees.
Empirical test of pangenesis and Lamarckism.
Galton conducted wide-ranging inquiries into heredity which led him to challenge Charles Darwin's hypothetical theory of pangenesis. Darwin had proposed as part of this hypothesis that certain particles, which he called "gemmules" moved throughout the body and were also responsible for the inheritance of acquired characteristics. Galton, in consultation with Darwin, set out to see if they were transported in the blood. In a long series of experiments in 1869 to 1871, he transfused the blood between dissimilar breeds of rabbits, and examined the features of their offspring. He found no evidence of characters transmitted in the transfused blood .
Darwin challenged the validity of Galton's experiment, giving his reasons in an article published in "Nature" where he wrote:
Now, in the chapter on Pangenesis in my "Variation of Animals and Plants under Domestication" I have not said one word about the blood, or about any fluid proper to any circulating system. It is, indeed, obvious that the presence of gemmules in the blood can form no necessary part of my hypothesis; for I refer in illustration of it to the lowest animals, such as the Protozoa, which do not possess blood or any vessels; and I refer to plants in which the fluid, when present in the vessels, cannot be considered as true blood." He goes on to admit: "Nevertheless, when I first heard of Mr. Galton's experiments, I did not sufficiently reflect on the subject, and saw not the difficulty of believing in the presence of gemmules in the blood.
Galton explicitly rejected the idea of the inheritance of acquired characteristics (Lamarckism), and was an early proponent of "hard heredity" through selection alone. He came close to rediscovering Mendel's particulate theory of inheritance, but was prevented from making the final breakthrough in this regard because of his focus on continuous, rather than discrete, traits (now known as polygenic traits). He went on to found the biometric approach to the study of heredity, distinguished by its use of statistical techniques to study continuous traits and population-scale aspects of heredity.
This approach was later taken up enthusiastically by Karl Pearson and W.F.R. Weldon; together, they founded the highly influential journal "Biometrika" in 1901. (R.A. Fisher would later show how the biometrical approach could be reconciled with the Mendelian approach. ) The statistical techniques that Galton invented (correlation, regression—see below) and phenomena he established (regression to the mean) formed the basis of the biometric approach and are now essential tools in all the social sciences.
Innovations in statistics and psychological theory.
Historiometry.
The method used in "Hereditary Genius" has been described as the first example of historiometry. To bolster these results, and to attempt to make a distinction between 'nature' and 'nurture' (he was the first to apply this phrase to the topic), he devised a questionnaire that he sent out to 190 Fellows of the Royal Society. He tabulated characteristics of their families, such as birth order and the occupation and race of their parents. He attempted to discover whether their interest in science was 'innate' or due to the encouragements of others. The studies were published as a book, "English men of science: their nature and nurture", in 1874. In the end, it promoted the nature versus nurture question, though it did not settle it, and provided some fascinating data on the sociology of scientists of the time.
The Lexical Hypothesis.
Sir Francis was the first scientist to recognise what is now known as the Lexical hypothesis. This is the idea that the most salient and socially relevant personality differences in people's lives will eventually become encoded into language. The hypothesis further suggests that by sampling language, it is possible to derive a comprehensive taxonomy of human personality traits.
The questionnaire.
Galton's inquiries into the mind involved detailed recording of people's subjective accounts of whether and how their minds dealt with phenomena such as mental imagery. To better elicit this information, he pioneered the use of the questionnaire. In one study, he asked his fellow members of the Royal Society of London to describe mental images that they experienced. In another, he collected in-depth surveys from eminent scientists for a work examining the effects of nature and nurture on the propensity toward scientific thinking.
Variance and standard deviation.
Core to any statistical analysis is the concept that measurements vary: they have both a central tendency, or mean, and a spread around this central value, or variance. In the late 1860s, Galton conceived of a measure to quantify normal variation: the standard deviation.
Galton was a keen observer. In 1906, visiting a livestock fair, he stumbled upon an intriguing contest. An ox was on display, and the villagers were invited to guess the animal's weight after it was slaughtered and dressed. Nearly 800 participated, but not one person hit the exact mark: 1,198 pounds. Galton stated that "the middlemost estimate expresses the "vox populi", every other estimate being condemned as too low or too high by a majority of the voters", and calculated this value (in modern terminology, the median) as 1,207 pounds. To his surprise, this was within 0.8% of the weight measured by the judges. Soon afterwards, he acknowledged that the mean of the guesses, at 1,197 pounds, was even more accurate.
The same year, Galton suggested in a letter to the journal "Nature" a better method of cutting a round cake by avoiding making radial incisions.
Experimental derivation of the normal distribution.
Studying variation, Galton invented the quincunx, a pachinko-like device, also known as the bean machine, as a tool for demonstrating the law of error and the normal distribution .
Bivariate normal distribution.
He also discovered the properties of the bivariate normal distribution and its relationship to regression analysis.
Correlation and regression.
In 1846, the French physicist Auguste Bravais (1811–1863) first developed what would become the correlation coefficient. After examining forearm and height measurements, Galton independently rediscovered the concept of correlation in 1888 and demonstrated its application in the study of heredity, anthropology, and psychology. Galton's later statistical study of the probability of extinction of surnames led to the concept of Galton–Watson stochastic processes . This is now a core of modern statistics and regression.
Galton invented the use of the regression line , and was the first to describe and explain the common phenomenon of regression toward the mean, which he first observed in his experiments on the size of the seeds of successive generations of sweet peas. He is responsible for the choice of r (for reversion or regression) to represent the correlation coefficient. In the 1870s and 1880s he was a pioneer in the use of normal distribution to fit histograms of actual tabulated data.
Theories of perception.
Galton went beyond measurement and summary to attempt to explain the phenomena he observed. Among such developments, he proposed an early theory of ranges of sound and hearing, and collected large quantities of anthropometric data from the public through his popular and long-running Anthropometric Laboratory, which he established in 1884, and where he studied over 9,000 people. It was not until 1985 that these data were analysed in their entirety.
Differential psychology.
Galton's study of human abilities ultimately led to the foundation of differential psychology and the formulation of the first mental tests. He was interested in measuring humans in every way possible. This included measuring their ability to make sensory discrimination which he assumed was linked to intellectual prowess. Galton suggested that individual differences in general ability are reﬂected in performance on relatively simple sensory capacities and in speed of reaction to a stimulus, variables that could be objectively measured by tests of sensory discrimination and reaction
time. He also measured how quickly people reacted which he later linked to internal wiring which ultimately limited intelligence ability. Throughout his research Galton assumed that people who reacted faster were more intelligent than others.
Composite photography.
Galton also devised a technique called "composite portraiture" (produced by superimposing multiple photographic portraits of individuals' faces registered on their eyes) to create an average face (see averageness). In the 1990s, a hundred years after his discovery, much psychological research has examined the attractiveness of these faces, an aspect that Galton had remarked on in his original lecture. Others, including Sigmund Freud in his work on dreams, picked up Galton's suggestion that these composites might represent a useful metaphor for an Ideal type or a concept of a "natural kind" (see Eleanor Rosch)—such as Jewish men, criminals, patients with tuberculosis, etc.—onto the same photographic plate, thereby yielding a blended whole, or "composite", that he hoped could generalise the facial appearance of his subject into an "average" or "central type". (See also entry "Modern physiognomy" under Physiognomy).
This work began in the 1880s while the Jewish scholar Joseph Jacobs studied anthropology and statistics with Francis Galton. Jacobs asked Galton to create a composite photograph of a Jewish type. One of Jacobs' first publications that used Galton's composite imagery was "The Jewish Type, and Galton's Composite Photographs," "Photographic News", 29, (24 April 1885): 268–269.
Galton hoped his technique would aid medical diagnosis, and even criminology through the identification of typical criminal faces. However, his technique did not prove useful and fell into disuse, although after much work on it including by photographers Lewis Hine and John L. Lovell and Arthur Batut.
Fingerprints.
In a Royal Institution paper in 1888 and three books ("Finger Prints", 1892; "Decipherment of Blurred Finger Prints", 1893; and "Fingerprint Directories", 1895), Galton estimated the probability of two persons having the same fingerprint and studied the heritability and racial differences in fingerprints. He wrote about the technique (inadvertently sparking a controversy between Herschel and Faulds that was to last until 1917), identifying common pattern in fingerprints and devising a classification system that survives to this day.
The method of identifying criminals by their fingerprints had been introduced in the 1860s by Sir William James Herschel in India, and their potential use in forensic work was first proposed by Dr Henry Faulds in 1880, but Galton was the first to place the study on a scientific footing, which assisted its acceptance by the courts . Galton pointed out that there were specific types of fingerprint patterns. He described and classified them into eight broad categories: 1: plain arch, 2: tented arch, 3: simple loop, 4: central pocket loop, 5: double loop, 6: lateral pocket loop, 7: plain whorl, and 8: accidental.
Final years.
In an effort to reach a wider audience, Galton worked on a novel entitled "Kantsaywhere" from May until December 1910. The novel described a utopia organised by a eugenic religion, designed to breed fitter and smarter humans. His unpublished notebooks show that this was an expansion of material he had been composing since at least 1901. He offered it to Methuen for publication, but they showed little enthusiasm. Galton wrote to his niece that it should be either "smothered or superseded". His niece appears to have burnt most of the novel, offended by the love scenes, but large fragments survived.
Honours and impact.
Over the course of his career Galton received many major awards, including the Copley Medal of the Royal Society (1910). He received in 1853 the highest award from the Royal Geographical Society, one of two gold medals awarded that year, for his explorations and map-making of southwest Africa. He was elected a member of the prestigious Athenaeum Club in 1855 and made a Fellow of the Royal Society in 1860. His autobiography also lists the following:
Galton was knighted in 1909. His statistical heir Karl Pearson, first holder of the Galton Chair of Eugenics at University College London (now Galton Chair of Genetics), wrote a three-volume biography of Galton, in four parts, after his death . The eminent psychometrician Lewis Terman estimated that his childhood IQ was on the order of 200, based on the fact that he consistently performed mentally at roughly twice his chronological age . (This follows the original definition of IQ as mental age divided by chronological age, rather than the modern definition based on the standard distribution and standard deviation.)
The flowering plant genus "Galtonia" was named in his honour.

</doc>
<doc id="49073" url="http://en.wikipedia.org/wiki?curid=49073" title="Pablo Casals">
Pablo Casals

Pau Casals i Defilló (]; December 29, 1876 – October 22, 1973), known during his professional career as Pablo Casals, was a Spanish Catalan cellist and conductor. He is generally regarded as the pre-eminent cellist of the first half of the 20th century, and one of the greatest cellists of all time. He made many recordings throughout his career, of solo, chamber, and orchestral music, also as conductor, but he is perhaps best remembered for the recordings of the Bach Cello Suites he made from 1936 to 1939.
Biography.
Childhood and early years.
Casals was born in El Vendrell, Catalonia, Spain. His father, Carles Casals i Ribes (1852–1908), was a parish organist and choirmaster. He gave Casals instruction in piano, song, violin, and organ. He was also a very strict disciplinarian. When Casals was young his father would pull the piano out from the wall and have him and his brother, Artur, stand behind it and name the notes and the scales that his father was playing. At the age of four Casals could play the violin, piano and flute; at the age of six he played the violin well enough to perform a solo in public. His first encounter with a cello-like instrument was from witnessing a local traveling Catalan musician, who played a cello-strung broom handle. Upon request, his father built him a crude cello, using a gourd as a sound-box. When Casals was eleven, he first heard the real cello performed by a group of traveling musicians, and decided to dedicate himself to the instrument. 
In 1888 his mother, Pilar Defilló de Casals, who was born in Mayagüez, Puerto Rico of Catalonian ancestry, took him to Barcelona, where he enrolled in the Escola Municipal de Música. There he studied cello, theory, and piano. In 1890, when he was 13, he found in a second-hand sheet music store in Barcelona a tattered copy of Bach's six cello suites. He spent the next 13 years practicing them every day before he would perform them in public for the first time. Casals would later make his own version of the six suites.
He made prodigious progress as a cellist; on February 23, 1891 he gave a solo recital in Barcelona at the age of fourteen. He graduated from the "Escola" with honours five years later.
Youth and studies.
In 1893, Catalan composer Isaac Albéniz heard him playing in a trio in a café and gave him a letter of introduction to the Count Guillermo Morphy, the private secretary to María Cristina, the Queen Regent. Casals was asked to play at informal concerts in the palace, and was granted a royal stipend to study composition at the "Real Conservatorio de Música y Declamación" in Madrid with Víctor Mirecki. He also played in the newly organized Quartet Society.
In 1895 he went to Paris, where, having lost his stipend from Catalonia, he earned a living by playing second cello in the theater orchestra of the "Folies Marigny". In 1896, he returned to Catalonia and received an appointment to the faculty of the "Escola Municipal de Música" in Barcelona. He was also appointed principal cellist in the orchestra of Barcelona's opera house, the Liceu. In 1897 he appeared as soloist with the Madrid Symphony Orchestra, and was awarded the Order of Carlos III from the Queen. 
International career.
In 1899, Casals played at The Crystal Palace in London, and later for Queen Victoria at Osborne House, her summer residence, accompanied by Ernest Walker. On November 12, and December 17, 1899, he appeared as a soloist at Lamoureux Concerts in Paris, to great public and critical acclaim. He toured Spain and the Netherlands with the pianist Harold Bauer from 1900 to 1901; in 1901/02 he made his first tour of the United States; and in 1903 toured South America.
On January 15, 1904, Casals was invited to play at the White House for President Theodore Roosevelt. On March 9, of that year he made his debut at Carnegie Hall in New York, playing Richard Strauss's "Don Quixote" under the baton of the composer. In 1906 he became associated with the talented young Portuguese cellist Guilhermina Suggia, who studied with him and began to appear in concerts as Mme. P. Casals-Suggia, although they were not legally married. Their relationship ended in 1912.
The "New York Times" of April 9, 1911, announced that Casals would perform at the London Musical Festival to be held at the Queen's Hall on the second day of the Festival (May 23). The piece chosen was Haydn's Cello Concerto in D and Casals would later join Fritz Kreisler for Brahms's Double Concerto for Violin and Cello.
In 1914 Casals married the American socialite and singer Susan Metcalfe; they were separated in 1928, but did not divorce until 1957.
Although Casals made his first recordings in 1915 (a series for Columbia), he would not release another recording until 1926 (on the Victor label).
Back in Paris, Casals organized a trio with the pianist Alfred Cortot and the violinist Jacques Thibaud; they played concerts and made recordings until 1937. Casals also became interested in conducting, and in 1919 he organized, in Barcelona, the Pau Casals Orchestra and led its first concert on October 13, 1920. With the outbreak of the Spanish Civil War in 1936, the Orquesta Pau Casals ceased its activities.
Casals was an ardent supporter of the Spanish Republican government, and after its defeat vowed not to return to Spain until democracy was restored. Casals performed at the Gran Teatre del Liceu on October 19, 1938, possibly his last performance in Catalonia before his exile.
He settled in the French Catalan village of Prada de Conflent, near the Spanish Catalan border; between 1939 and 1942 he made sporadic appearances as a cellist in the unoccupied zone of southern France and in Switzerland. So fierce was his opposition to the dictatorial regime of Francisco Franco in Spain that he refused to appear in countries that recognized the authoritarian Spanish government. He made a notable exception when he took part in a concert of chamber music in the White House on November 13, 1961, at the invitation of President John F. Kennedy, whom he admired. On December 6, 1963, Casals was awarded the U.S. Presidential Medal of Freedom.
Throughout most of his professional career, he played on a cello that was labeled and attributed to "Carlo Tononi ... 1733" but after he had been playing it for 50 years it was discovered to have been created by the Venetian luthier Matteo Goffriller around 1700. It was acquired by Casals in 1913. He also played another cello by Goffriller dated 1710, and a Tononi from 1730.
Prades Festivals.
In 1950 he resumed his career as conductor and cellist at the Prades Festival in Conflent, organized in commemoration of the bicentenary of the death of Johann Sebastian Bach; Casals agreed to participate on condition that all proceeds were to go to a refugee hospital in nearby Perpignan.
In 1952, Casals met Marta Angélica Montañez y Martinez, a 15-year-old Puerto Rican student who had gone to Spain to participate in the Festival. Casals was very impressed with her and encouraged her to return to Mannes College of Music in New York to continue her studies. He continued leading the Prades Festivals until 1966. 
Puerto Rico.
Casals traveled extensively to Puerto Rico in 1955, inaugurating the annual Casals Festival the next year. In 1955 Casals married as his second wife long-time associate Francesca Vidal de Capdevila, who died that same year. In 1957, at age 80, Casals married 20-year-old Marta Montañez y Martinez. He is said to have dismissed concerns that marriage to someone 60 years his junior might be hazardous to his health by saying, "I look at it this way: if she dies, she dies." Pablo and Marta made their permanent residence in the town of Ceiba, and lived in a house called "El Pessebre" (The Manger). He made an impact in the Puerto Rican music scene, by founding the Puerto Rico Symphony Orchestra in 1958, and the Conservatory of Music of Puerto Rico in 1959.
Later years.
Casals appeared in the 1958 documentary film "Windjammer". In the 1960s, Casals gave many master classes throughout the world in places such as Gstaad, Zermatt, Tuscany, Berkeley, and Marlboro. Several of these master classes were televised.
In 1961, he performed at the White House by invitation of President Kennedy. This performance was recorded and released as an album.
Casals was also a composer. Perhaps his most effective work is "La Sardana", for an ensemble of cellos, which he composed in 1926. His oratorio "El Pessebre" was performed for the first time in Acapulco, Mexico, on December 17, 1960. He also presented it to the United Nations during their anniversary in 1963. He was initiated as an honorary member of the Epsilon Iota Chapter of Phi Mu Alpha Sinfonia music fraternity at The Florida State University in 1963. He was later awarded the fraternity's Charles E. Lutton Man of Music Award in 1973.
One of his last compositions was the "Hymn of the United Nations". He conducted its first performance in a special concert at the United Nations on October 24, 1971, two months before his 95th birthday. On that day, the Secretary-General of the United Nations, U Thant, awarded Pablo Casals the U.N. Peace Medal in recognition of his stance for peace, justice and freedom. Casals accepted the medal and made his famous "I Am a Catalan" speech, where he stated that Catalonia had the first democratic parliament, long before England did.
In 1973, invited by his friend Isaac Stern, Casals arrived at Jerusalem to conduct the youth orchestra and the Jerusalem Symphony Orchestra. The concert he conducted with the youth orchestra at the Jerusalem Khan Theater was the last concert he conducted in his life.
Casals' memoirs were taken down by Albert E. Kahn, and published as "Joys and Sorrows: Pablo Casals, His Own Story" (1970).
Death.
Casals died in 1973 at Auxilio Mutuo Hospital in San Juan, Puerto Rico, at the age of 96, from complications of a heart attack he had three weeks earlier. He was buried at the Puerto Rico National Cemetery. He did not live to see the end of the Franco dictatorial regime, but he was posthumously honoured by the Spanish government under King Juan Carlos I which in 1976 issued a commemorative postage stamp depicting Casals, in honour of the centenary of his birth. In 1979 his remains were interred in his hometown of El Vendrell, Catalonia. In 1989, Casals was posthumously awarded a Grammy Lifetime Achievement Award.
Legacy.
The southern part of the highway C-32 in Catalonia is named Autopista de Pau Casals.
The International Pablo Casals Cello Competition is held in Kronberg and Frankfurt am Main, Germany, under the auspices of the Kronberg Academy once every four years, starting in 2000, to discover and further the careers of the future cello elite, and is supported by the Pau Casals Foundation, under the patronage of Marta Casals Istomin. One of the prizes is the use of one of the Gofriller cellos owned by Casals. The first top prize was awarded in 2000 to Claudio Bohórquez.
American comedian George Carlin, in his interview for the Archive of American Television, refers to Casals when discussing the restless nature of an artist's persona. As Carlin states, when Casals (then aged 93) was asked why he continued to practice the cello three hours a day, Casals replied, "'I'm beginning to notice some improvement...' [A]nd that's the thing that's in me. I notice myself getting better at this," Carlin continued.
In Puerto Rico, the Casals Festival is still celebrated annually. There is also a museum dedicated to the life of Casals located in Old San Juan. On October 3, 2009, Sala Sinfonica Pablo Casals, a symphony hall named in Casals' honor, opened in San Juan, Puerto Rico. The $34 million building, designed by Rodolfo Fernandez, is the latest addition to the Centro de Bellas Artes complex. It is the new home of the Puerto Rico Symphony Orchestra.
In Tokyo, the Casals Hall opened in 1987 as a venue for chamber music. Pablo Casals Elementary School in Chicago is named in his honor.
Casals' motet "O vos omnes", composed in 1932, is frequently performed today.

</doc>
<doc id="49076" url="http://en.wikipedia.org/wiki?curid=49076" title="Airspeed Ltd.">
Airspeed Ltd.

Airspeed Limited was established to build aeroplanes in 1931 in York, England, by A. H. Tiltman and Nevil Shute Norway (the aeronautical engineer and famous writer, who used his forenames as his pen-name). The other directors were A. E. Hewitt, Lord Grimthorpe and Alan Cobham. Amy Johnson was also one of the initial subscribers for shares.
Early operations.
After a short production run of the AS.1 Tern glider, Airspeed produced the AS.4 Ferry, a three-engined, ten-passenger biplane, concentrating on transport monoplanes thereafter. In March 1933, the firm moved to Portsmouth and, in the following year, became associated with the Tyneside ship builder Swan Hunter & Wigham Richardson Limited and became Airspeed (1934) Limited in August 1934. During this period, it developed the AS.8 Viceroy for an intercontinental air race.
Wolseley engine.
All Airspeed aeroplanes under manufacture or development in 1936 were to use a Wolseley radial aero engine of about 250 hp which was under development by Nuffield, the Wolseley Scorpio. The project was abandoned in September 1936 after the expenditure of about two hundred thousand pounds when Lord Nuffield got the fixed price I.T.P. (Intention to Proceed) contract papers (which would have required re-orientation of their offices with an army of chartered accountants) and decided to deal only with the War Office and the Admiralty, not the Air Ministry.
According to Nevil Shute Norway it was a very advanced engine (and the price struck Shute as low; much lower than competing engines on the basis of power-to-weight ratio), so its loss was a major disaster for Airspeed (and Britain). But when he asked Lord Nuffield to retain the engine, Nuffield said "I tell you, Norway ... I sent that I.T.P. thing back to them, and I told them they could put it where the monkey put the nuts!" Shute wrote that the loss of the Wolseley engine due to the over-cautious high civil servants of the Air Ministry was a great loss to Britain. Shute said that "admitting Air Ministry methods of doing business ... would be like introducing a maggot into an apple .. Better to stick to selling motor vehicles for cash to the War Office and the Admiralty who retained the normal methods of buying and selling."
Second World War.
In June, 1940, formal announcement was made that the de Havilland Aircraft Co., Ltd., had completed negotiations for the purchase from Swan, Hunter and Wigham Richardson, Ltd., of that firm's holding of Airspeed ordinary shares. Airspeed retained its identity as a separate company though as a wholly owned subsidiary of de Havilland.
Around 1943, presumably to reduce the risk of Luftwaffe bombing, a new dispersed design office was opened at Fairmile Manor in Cobham, Surrey; little is known of this establishment and nothing survives there today.
Airspeed's most productive period was during the Second World War. The graceful, twin-engined trainer-cum-light transport aircraft known as the AS.10 Oxford had a production run exceeding 8,500.
3,800 AS51 and AS58 Horsa military gliders were built for the Royal Air Force and its allies. Many of these made one-way journeys into occupied France as part of the D-Day landings, and later the Netherlands for the Arnhem landing, towed from England behind aircraft such as the Douglas Dakota and Handley Page Halifax.
Postwar operations.
The company reverted to the company name of Airspeed Limited on 25 January 1944. Postwar it became involved in adapting some surplus ex-RAF Oxford aircraft as AS65 Consuls for the commercial market. Airspeed went on to produce the superbly streamlined pressurised twin-engined piston airliner called the AS57 Ambassador. This served successfully for some years with British European Airways as their "Elizabethan Class". In 1951 Airspeed Limited completely merged with de Havilland.

</doc>
<doc id="49080" url="http://en.wikipedia.org/wiki?curid=49080" title="Georg Ohm">
Georg Ohm

Georg Simon Ohm (]; 16 March 1789 – 6 July 1854) was a German physicist and mathematician. As a school teacher, Ohm began his research with the new electrochemical cell, invented by Italian scientist Alessandro Volta. Using equipment of his own creation, Ohm found that there is a direct proportionality between the potential difference (voltage) applied across a conductor and the resultant electric current. This relationship is known as Ohm's law.
Biography.
Early years.
Georg Simon Ohm was born into a Protestant family in Erlangen, Brandenburg-Bayreuth (then a part of the Holy Roman Empire), son to Johann Wolfgang Ohm, a locksmith and Maria Elizabeth Beck, the daughter of a tailor in Erlangen. Although his parents had not been formally educated, Ohm's father was a respected man who had educated himself to a high level and was able to give his sons an excellent education through his own teachings. Of the seven children of the family only three survived to adulthood: Georg Simon, his younger brother Martin, who later became a well-known mathematician, and his sister Elizabeth Barbara. His mother died when he was ten.
From early childhood, Georg and Martin were taught by their father who brought them to a high standard in mathematics, physics, chemistry and philosophy. Georg Simon attended Erlangen Gymnasium from age eleven to fifteen where he received little in the area of scientific training, which sharply contrasted with the inspired instruction that both Georg and Martin received from their father. This characteristic made the Ohms bear a resemblance to the Bernoulli family, as noted by Karl Christian von Langsdorf, a professor at the University of Erlangen.
Life in university.
Georg Ohm's father, concerned that his son was wasting his educational opportunity, sent Ohm to Switzerland. There in September 1806 Ohm accepted a position as a mathematics teacher in a school in Gottstadt bei Nidau.
Karl Christian von Langsdorf left the University of Erlangen in early 1809 to take up a post in the University of Heidelberg and Ohm would have liked to have gone with him to Heidelberg to restart his mathematical studies. Langsdorf, however, advised Ohm to continue with his studies of mathematics on his own, advising Ohm to read the works of Euler, Laplace and Lacroix. Rather reluctantly Ohm took his advice but he left his teaching post in Gottstatt Monastery in March 1809 to become a private tutor in Neuchâtel. For two years he carried out his duties as a tutor while he followed Langsdorf's advice and continued his private study of mathematics. Then in April 1811 he returned to the University of Erlangen.
Teaching career.
Ohm's own studies prepared him for his doctorate which he received from the University of Erlangen on October 25, 1811. He immediately joined the faculty there as a lecturer in mathematics but left after three semesters because of unpromising prospects. He could not survive on his salary as a lecturer. The Bavarian government offered him a post as a teacher of mathematics and physics at a poor quality school in Bamberg which Ohm accepted in January 1813. Unhappy with his job, Georg began writing an elementary textbook on geometry as a way to prove his abilities. Ohm's school was closed down in February 1816. The Bavarian government then sent him to an overcrowded school in Bamberg to help out with the teaching of mathematics.
After his assignment in Bamberg, Ohm sent his completed manuscript to King Wilhelm III of Prussia. The King was satisfied with Ohm's book, and offered Ohm a position at the Jesuit Gymnasium of Cologne on 11 September 1817. This school had a reputation for good science education and Ohm was required to teach physics in addition to mathematics. The physics laboratory was well-equipped, allowing Ohm to begin experiments in physics. As the son of a locksmith, Ohm had some practical experience with mechanical devices.
Ohm published "Die galvanische Kette, mathematisch bearbeitet" ("The Galvanic Circuit Investigated Mathematically") in 1827. Ohm's college did not appreciate his work and Ohm resigned from his position. He then made an application to, and was employed by, the Polytechnic School of Nuremberg. Ohm arrived at the Polytechnic School of Nuremberg in 1833, and in 1852 he became a professor of experimental physics at the University of Munich.
In 1849, Ohm published "Beiträge zur Molecular-Physik", (in English: Molecular Physics). In the preface of this work he stated he hoped to write a second and third volume "and if God gives me length of days for it, a fourth". However, on finding that an original discovery recorded in it was being anticipated by a Swedish scientist he did not publish it, stating: "The episode has given a fresh and deep sense for my mind to the saying 'Man proposes, and God disposes'. The project that gave the first impetus to my inquiry has been dissipated into mist, and a new one, undesigned by me, has been accomplished in its place."
He died in Munich in 1854, and is buried in the Alter Südfriedhof. A collection of his family letters would be compiled in a German book, which shows that he used to sign some of his letters with the expression "Gott befohlen, G S Ohm," meaning "Commended to God".
The discovery of Ohm's law.
Ohm's law first appeared in the famous book Die galvanische Kette, mathematisch bearbeitet (tr., The Galvanic Circuit Investigated Mathematically) (1827) in which he gave his complete theory of electricity. 
In this work, he stated his law for electromotive force acting between the extremities of any part of a circuit is the product of the strength of the current, and the resistance of that part of the circuit.
The book begins with the mathematical background necessary for an understanding of the rest of the work. While his work greatly influenced the theory and applications of current electricity, it was coldly received at that time. It is interesting that Ohm presents his theory as one of contiguous action, a theory which opposed the concept of action at a distance. Ohm believed that the communication of electricity occurred between "contiguous particles" which is the term he himself used. The paper is concerned with this idea, and in particular with illustrating the differences in this scientific approach of Ohm's and the approaches of Joseph Fourier and Claude-Louis Navier.
A detailed study of the conceptual framework used by Ohm in producing Ohm's law has been presented by Archibald. The work of Ohm marked the early beginning of the subject of circuit theory, although this did not become an important field until the end of the century.
Ohm's acoustic law.
Ohm's acoustic law, sometimes called the acoustic phase law or simply Ohm's law, states that a musical sound is perceived by the ear as a set of a number of constituent pure harmonic tones. It is well known to be not quite true.
Study and publications.
His writings were numerous. The most important was his pamphlet published in Berlin in 1827, with the title "Die galvanische Kette mathematisch bearbeitet". This work, the germ of which had appeared during the two preceding years in the journals of Schweigger and Poggendorff, has exerted an important influence on the development of the theory and applications of electric current. Ohm's name has been incorporated in the terminology of electrical science in Ohm's Law (which he first published in "Die galvanische Kette"...), the proportionality of current and voltage in a resistor, and adopted as the SI unit of resistance, the ohm (symbol Ω).
Although Ohm's work strongly influenced theory, at first it was received with little enthusiasm. However, his work was eventually recognized by the Royal Society with its award of the Copley Medal in 1841. He became a foreign member of the Royal Society in 1842, and in 1845 he became a full member of the Bavarian Academy of Sciences and Humanities. At some extent, Charles Wheatstone drew attention to the definitions which Ohm had introduced in the field of physics.

</doc>
<doc id="49082" url="http://en.wikipedia.org/wiki?curid=49082" title="Supermodel">
Supermodel

A supermodel (also spelled super-model and super model) is a highly paid fashion model who usually has a worldwide reputation and often a background in haute couture and commercial modeling. The term "supermodel" became prominent in the popular culture of the 1980s.
Supermodels usually work for top fashion designers and famous clothing brands. They have multi-million dollar contracts, endorsements and campaigns. They have branded themselves as household names and worldwide recognition is associated with their modeling careers. They have been on the covers of prestigious magazines such as French, British, American and Italian "Vogue". Claudia Schiffer stated, "In order to become a supermodel one must be on all the covers all over the world at the same time so that people can recognise the girls."
History.
Origins.
An early use of the term "supermodel" appeared in 1891 in an interview with artist Henry Stacy Marks for "The Strand Magazine", in which Marks told journalist Harry How, "A good many models are addicted to drink, and, after sitting a while, will suddenly go to sleep. Then I have had what I call the 'super' model. You know the sort of man; he goes in for theatrical effect ..."
On 6 October 1942, a writer named Judith Cass had used the term "super model" for her article in the "Chicago Tribune", which headlined "Super Models are Signed for Fashion Show". Later in 1943, an agent named Clyde Matthew Dessner used the term in a "how-to" book about modeling entitled "So You Want to Be a Model!" in which Dessner wrote, "She will be a super-model, but the girl in her will be like the girl in you—quite ordinary, but ambitious and eager for personal development." According to "Model: The Ugly Business of Beautiful Women" by Michael Gross, Gross stated the term "supermodel" was first used by Dessner. However, etymologist Barry Popik has disputed Gross' claim that Dessner coined the term. In 1947, anthropologist Harold Sterling Gladwin wrote "supermodel" in his book "Men Out of Asia". In 1949, the magazine "Hearst's International Combined with Cosmopolitan" referred to Anita Colby, the highest paid model at the time, as a "super model": "She's been super model, super movie saleswoman, and top brass at Selznick and Paramount." On 18 October 1959, Vancouver's "Chinatown News" described Susan Chew as a "super model".
The term "supermodel" had been used several times in the media in the 1960s and 1970s. In 1965, the encyclopedic guide "American Jurisprudence Trials" used the term "super model" ("...at issue was patient's belief that her husband was having an affair with a super model"). On 21 March 1967, the "New York Times" referred to Twiggy as a supermodel; the February 1968 article of "Glamour magazine" listed all 19 "supermodels"; the Chicago "Daily Defender" wrote "New York Designer Turns Super Model" in January 1970; "The Washington Post" and Mansfield "News Journal" used the term in 1971; and in 1974 both the "Chicago Tribune" and "The Advocate" also used the term "supermodel" in their articles. American "Vogue" used the term "super-model" to describe Jean Shrimpton in the 15 October 1965 edition and "supermodel" on the cover page to describe Margaux Hemingway in the 1 September 1975 edition. Margaux Hemingway was again described as a 'supermodel' in the 25 July 1977 edition of "Time". "Jet" also described Beverly Johnson as a "supermodel" in the 22 December 1977 edition.
Model Janice Dickinson has incorrectly stated that she coined the term "supermodel" in 1979, as a compound of Superman and model. During an interview with "Entertainment Tonight", Dickinson stated that her agent Monique Pilar of Elite Model Management asked her, "Janice, who do you think you are, Superman?" She replied, "No ... I'm a supermodel, honey, and you will refer to me as a supermodel and you will start a supermodel division." Dickinson also claims to be the first supermodel.
Lisa Fonssagrives is widely considered the world's first supermodel. She was in most of the major fashion magazines and general interest magazines from the 1930s to the 1950s, including "Town & Country", "Life", "Vogue", the original "Vanity Fair", "Harper's Bazaar", and "Time". Dorian Leigh has also been called the world's first supermodel, as well as Gia Carangi, and Jean Shrimpton.
1960s–1970s.
In February 1968, an article in "Glamour" described 19 models as "supermodels", of whom were: Cheryl Tiegs, Verushka, Lisa Palmer, Peggy Moffitt, Susan (Sue) Murray, Twiggy, Sunny Harnett, Marisa Berenson, Gretchen Harris, Heide Wiedeck, Irish Bianchi, Hiroko Matsumoto, Anne de Zogheb, Kathy Carpenter, Jean Shrimpton, Jean Patchett, Benedetta Barzini, Claudia Duxbury, and Agneta Frieberg.
In the 1970s, some models became more prominent as their names became more recognizable to the general public. "Sports Illustrated" editor Jule Campbell abandoned then-current modeling trends for its fledgling "Sports Illustrated Swimsuit Issue" by photographing "bigger and healthier" California models and printing their names by their photos, thus turning many of them into household names and establishing the issue as a cornerstone of supermodel status.
In 1975, Margaux Hemingway landed a then-unprecedented million-dollar contract as the face of Fabergé's Babe perfume and the same year appeared on the cover of "Time" magazine, labelled one of the "New Beauties", giving further name recognition to fashion models.
Lauren Hutton became the first model to receive a huge contract from a cosmetics company and appeared on cover of "Vogue" 25 times. Iman is considered to have been the first supermodel of color.
Donyale Luna was the first black model to appear in British "Vogue", Naomi Sims, who is sometimes regarded as the first black supermodel, became the first African American to feature on the cover of "Ladies' Home Journal" in 1968. The first African American model to be on the cover of American "Vogue" was Beverly Johnson in 1974.
Other notable "supermodels" of the time were Iman, Cybill Shepherd, Patti Hansen, Penelope Tree, Grace Jones, Lauren Hutton, Janice Dickinson, Rene Russo, Gia Carangi, Jerry Hall, Wilhelmina Cooper, Christie Brinkley, Edie Sedgewick and Kelly Emberg.
1980s.
In October 1981, "Life" cited Shelley Hack, Lauren Hutton, and Iman for Revlon, Margaux Hemingway for Fabergé, Karen Graham for Estee Lauder, Christina Ferrare for Max Factor, and Cheryl Tiegs for Cover Girl by proclaiming them the "million dollar faces" of the beauty industry. These supermodels negotiated previously unheard of lucrative and exclusive deals with giant cosmetics companies, were instantly recognizable, and their names became well known to the public.
In the early 1980s, Inès de la Fressange was the first model to sign an exclusive modeling contract with an haute couture fashion house, Chanel. During the early 1980s, fashion designers began advertising on television and billboards. Catwalk regulars like Gia Carangi, Cheryl Tiegs, Christie Brinkley, Kim Alexis, Paulina Porizkova, Yasmin Le Bon, Kathy Ireland, Brooke Shields, Carol Alt, and Elle Macpherson began to endorse products with their names, as well as their faces, through the marketing of brands such as the beverage Diet Pepsi to the extension of car title Ford Trucks.
As the models began to embrace old-style glamour, they were starting to replace film stars as symbols of luxury and wealth. In this regard, supermodels were viewed not so much as individuals but as images.
1990s.
By the 1990s, the supermodel became increasingly prominent in the media. The title became tantamount to superstar, to signify a supermodel's fame having risen simply from "personality." Supermodels did talk shows, were cited in gossip columns, partied at the trendiest nightspots, landed movie roles, inspired franchises, dated or married film stars, and earned themselves millions. Fame empowered them to take charge of their careers, to market themselves, and to command higher fees.
The new era began in 1990, with the era-defining British Vogue cover of Naomi Campbell, Cindy Crawford, Christy Turlington, Linda Evangelista, and Tatjana Patitz, which created such an impression on the fashion world that they came to embody the term supermodel. Individually and as an elite group, it seemed as if the idea of the supermodel had been coined just for them. Each model had gradually attained fame since the mid-1980s and were now among the industry's top stars. Handpicked by photographer Peter Lindbergh for the January cover of Vogue, the now famous cover inspired pop star George Michael to cast the same five models in the music video for his international hit song, "Freedom! '90", directed by David Fincher. Other notable photographs capturing this new generation of models, including the famous nude taken by Herb Ritts for Rolling Stone that included Tatjana, Cindy, Naomi, Christy and Stephanie Seymour, helped each supermodel attain world-wide fame and fortune, by sharing covers of all the international editions of "Vogue", walking the catwalks for the world's top designers, and becoming known by their first names alone.
In 1991, Christy Turlington signed a contract with Maybelline that paid her $800,000 for twelve days' work each year. Four years later, Claudia Schiffer reportedly earned $12 million for her various modeling assignments. Authorities ranging from Karl Lagerfeld to "Time" had declared the supermodels more glamorous than movie stars.
Naomi Campbell, Linda Evangelista and Christy Turlington became known as "The Trinity", a term first used by journalist Michael Gross. Evangelista was known as the "Chameleon", for her ability to transform her look and reinvent herself. Turlington was known as the "insurance model", saying "clients know that if they hire me, nothing will go wrong". Campbell was the first black model to appear on the front cover of Time Magazine, French Vogue, British Vogue, and the September issue of American Vogue, traditionally the years biggest and most important issue. Two now iconic images of the three women saw them drinking champagne in a bathtub in one, and portraying the see no evil, hear no evil, speak no evil metaphor in the other.
As the 1990s phenomenon progressed, the supermodels were joined by Kate Moss. They were the most heavily in demand, collectively dominating magazine covers, fashion runways, editorial pages, and both print and broadcast advertising. Naomi Campbell, Cindy Crawford, Linda Evangelista, Claudia Schiffer and Christy Turlington became known as the Big Five supermodels, and later, with the addition of Kate Moss, as the Big Six.
In the 2006 book, "In Vogue: The Illustrated History of the World's Most Famous Fashion Magazine" (Rizzoli), the editors cite the "original supermodels" and Claudia Schiffer when quoting "Vogue" Magazine Editor-In-Chief, Anna Wintour, who said, "Those girls were so fabulous for fashion and totally reflected that time ... [They] were like movie stars." The editors name famous models from previous decades, but explain that, "None of them attained the fame and world-wide renown bestowed on Linda Evangelista, Christy Turlington, Cindy Crawford, Naomi Campbell, Tatjana Patitz, Stephanie Seymour, Claudia Schiffer, Amber Valletta, Yasmeen Ghauri, and Karen Mulder in the late 1980s and early 1990s. These models burst out beyond the pages of the magazines. Many became the faces of cosmetics brands and perfumes, had their own television programs and physical-fitness videos and their own lines of lingerie ... Their lives, activities, influences, and images were the subjects of all types of sociological and historical analysis."
In the mid-1990s, the initial era of the supermodel ended and a new era for the supermodel began driven by heroin chic. By the late 1990s, actresses, pop singers, and other entertainment celebrities began gradually replacing models on fashion magazine covers and ad campaigns. The pendulum of limelight left many models in anonymity. A popular "conspiracy theory" explaining the supermodel's disappearance is that designers and fashion editors grew weary of the "I won't get out of bed for less than $10,000 a day" attitude and made sure no small group of models would ever again have the power of the Big Six.
Charles Gandee, associate editor at "Vogue", has said that high prices and poor attitudes contributed less to the decline of the supermodel. As clothes became less flashy, designers turned to models who were less glamorous, so they wouldn't overpower the clothing. Whereas many supermodels of the previous era were American-born, their accents making for an easier transition to stardom, the majority of models began coming from non-English speaking countries and cultures, making the crossover to mainstream spokesperson and cover star difficult. However, the term continued to be applied to notable models such as Laetitia Casta, Eva Herzigová, Carla Bruni, Tatiana Sorokko, Yasmin Le Bon, Shalom Harlow, Nadja Auermann, Helena Christensen, Patricia Velásquez, Adriana Karembeu, Milla Jovovich. and Valeria Mazza.
2000s and present day.
Emerging in the late 1990s, Gisele Bündchen became the first in a wave of Brazilian models to gain popularity in the industry and with the public. With numerous covers of "Vogue" under her belt, including an issue that dubbed her the "Return of the "Sexy" Model," Bündchen was credited with ending the "heroin chic" era of models. Following in her footsteps by signing contracts with Victoria's Secret, fellow Brazilians Adriana Lima and Alessandra Ambrosio rose to prominence; however, this "new trinity" were unable to cross over into the world of TV, movies and talk shows as easily as their predecessors due to their foreign accents.
Several seasons later, they were followed by Eastern Europeans barely into their teens, pale, and "bordering on anorexic. They were too young to become movie stars or date celebrities; too skeletal to bag Victoria's Secret contracts; and a lack of English didn't bode well for a broad media career". The opportunities for super-stardom were waning in the modeling world, and models like Heidi Klum and Tyra Banks took to television with reality shows like "Project Runway" , "America's Next Top Model" and "Germany's Next Topmodel" , respectively, to not only remain relevant but establish themselves as media moguls.
Contrary to the fashion industry's celebrity trend of the previous decade, lingerie retailer Victoria's Secret continues to groom and launch young talents into supermodel status, awarding their high-profile "Angels" multi-year, multi-million-dollar contracts. In addition to Klum, Banks, Bündchen, Lima, and Ambrosio, these models have included Karolína Kurková, Miranda Kerr, Izabel Goulart, Selita Ebanks, Rosie Huntington-Whiteley and Marisa Miller. However, some, such as Claudia Schiffer, argued that Bündchen is the only model who comes close to earning the supermodel title.
American "Vogue" dubbed ten models (Doutzen Kroes, Agyness Deyn, Hilary Rhoda, Raquel Zimmermann, Coco Rocha, Lily Donaldson, Chanel Iman, Sasha Pivovarova, Caroline Trentini, and Jessica Stam) as the new crop of supermodels in their May 2007 cover story, while the likes of Christie Brinkley, Christy Turlington, and Linda Evangelista returned to reclaim prominent contracts from celebrities and younger models.
Perception.
Critical perception of the supermodel as an industry has been frequent inside and outside the fashion press, from complaints that women desiring this status become unhealthily thin to charges of racism, where the "supermodel" generally has to conform to a Northern European standard of beauty. According to fashion writer Guy Trebay of "The New York Times", in 2007, the "android" look is popular, a vacant stare and thin body serving, according to some fashion industry conventions, to set off the couture. This was not always the case. In the 1970s, black, heavier and "ethnic" models dominated the runways but social changes since that time have made the power players in the fashion industry shun suggestions of "otherness".

</doc>
<doc id="49084" url="http://en.wikipedia.org/wiki?curid=49084" title="Glans penis">
Glans penis

In male human anatomy, the glans penis (or simply glans, ) is the sensitive bulbous structure at the distal end of the penis. The glans is anatomically homologous to the clitoral glans of the human female. Typically, the glans is completely or partially covered by the foreskin, except in men who have been circumcised, though the foreskin can generally be retracted over and past the glans.
The glans is more commonly known as the "head of the penis". The medical name comes from the Latin word "glans" ('acorn') and "penis" ('of the penis') – the Latin genitive of this word has the same form as the nominative.
Medical considerations.
The meatus (opening) of the urethra is located at the tip of the glans penis. In circumcised infants, the foreskin no longer protects the meatal area of the glans; consequently, when wearing diapers, there may be greater risk of developing meatitis, meatal ulceration, or meatal stenosis.
The epithelium of the glans penis is mucocutaneous tissue. Birley "et al." report that excessive washing with soap may dry the mucous membrane that covers the glans penis and cause non-specific dermatitis.
Inflammation of the glans penis is known as balanitis. It occurs in 3–11% of males, and up to 35% of diabetic males. Edwards reported that it is generally more common in males who have poor hygiene habits or have not been circumcised. It has many causes, including irritation, or infection with a wide variety of pathogens. Careful identification of the cause with the aid of patient history, physical examination, swabs and cultures, and biopsy are essential in order to determine the proper treatment.
Anatomical details.
The glans penis is the expanded cap of the corpus spongiosum. It is moulded on the rounded ends of the corpora cavernosa penis, extending farther on their upper than on their lower surfaces. At the summit of the glans is the slit-like vertical external urethral orifice. The circumference of the base of the glans forms a rounded projecting border, the corona glandis, overhanging a deep retroglandular sulcus (the "coronal sulcus"), behind which is the neck of the penis. The proportional size of the glans penis can vary greatly. On some penises it is much wider in circumference than the shaft, giving the penis a mushroom-like appearance, and on others it is narrower and more akin to a probe in shape.
The foreskin maintains the mucosa in a moist environment. In males who have been circumcised, the glans is permanently exposed and dry. Szabo and Short found that the glans of a circumcised penis does not develop a thicker keratinization layer. Several studies have suggested that the glans is equally sensitive in circumcised and uncircumcised males, while others have reported that it is more sensitive in males who are not circumcised.
Halata & Munger (1986) report that the density of genital corpuscles is greatest in the corona glandis, while Yang & Bradley (1998) report that their study "showed no areas in the glans to be more densely innervated than others."
Halata & Spathe (1997) reported that "the glans penis contains a predominance of free nerve endings, numerous genital end bulbs and rarely Pacinian and Ruffinian corpuscles. Merkel nerve endings and Meissner's corpuscles are not present."
Yang & Bradley argue that "the distinct pattern of innervation of the glans emphasizes the role of the glans as a sensory structure".
Evolutionary significance.
It has been suggested that the unique and unusual shape of the glans in humans has evolved to serve the function of "scooping" any remnant semen deposited by other rival males out of the deeper part of the vagina of a female who may have recently copulated, and thereby decreasing the chance of the rival male impregnating the female. Other theorists suggest that its distinctive shape evolved to heighten the sexual pleasure experienced by the female during vaginal intercourse. In this theory, the glans increases friction and tension at the mouth of the vagina by its additional girth and the dilating properties of its probe-like shape.
In other animals.
The glans of a fossa's penis extends about halfway down the shaft and is spiny except at the tip. In comparison, the glans of felids is short and spiny, while that of viverrids is smooth and long. Male felids urinate backwards by curving the tip of the glans penis backward.
The shape of the glans varies among different marsupial species.
The glans penis of the marsh rice rat is long and robust, averaging 7.3 mm (0.29 in) long and 4.6 mm (0.18 in) broad.
In "Thomasomys ucucha" the glans penis is rounded, short, and small and is superficially divided into left and right halves by a trough at the top and a ridge at the bottom. Most of the glans is covered with spines, except for an area near the tip.
Winkelmann's mouse can most readily be distinguished from its close relatives by its partially corrugated glans penis.
When erect, the glans of a horse's penis increases by 3 to 4 times. The urethra opens within the urethral fossa, a small pouch at the distal end of the glans. Unlike the human glans, the glans of a horse's penis extends backwards on its shaft.
Males of Racey's pipistrelle bat have a narrow, egg-shaped glans penis.
The glans penis of a male cape ground squirrel is large with a prominent baculum.

</doc>
<doc id="49085" url="http://en.wikipedia.org/wiki?curid=49085" title="Palette knife">
Palette knife

A palette knife is a blunt tool used for mixing or applying paint, with a flexible steel blade. It is primarily used for mixing paint colors, paste, etc., or for marbling, decorative endpapers, etc. The "palette" in the name is a reference to an artist's palette which is used for mixing oil paint and acrylic paints.
Art knives come primarily in two types:
While palette knives are manufactured without sharpened cutting edges, with prolonged use they may become "sharpened" by the action of abrasive pigments such as earth colors.
Palette knives are also used in cooking, where their flexibility allows them to easily slide underneath pastries or other items. See frosting spatula.

</doc>
<doc id="49086" url="http://en.wikipedia.org/wiki?curid=49086" title="Libanius">
Libanius

Libanius (Greek: Λιβάνιος, "Libanios"; c. 314 – 392 or 393) was a Greek-speaking teacher of rhetoric of the Sophist school. During the rise of Christian hegemony in the later Roman Empire, he remained unconverted and in religious matters was a pagan Hellene. 
Life.
He was born into a once-influential, deeply cultured family of Antioch that had recently come into diminished circumstances. When fourteen years old, Libanius fell in love with rhetoric and focused his whole life on it. He withdrew from public life and devoted himself to philosophy. Unfamiliar with Latin literature, he deplored its influence. He also attacked the increasing imperial pressures on the traditional city-oriented culture that had been supported and dominated by the local upper classes. Libanius used his arts of rhetoric to advance various private and political causes. Despite his own religious views and his friendship with the Emperor Julian, called "the Apostate" for attempting to restore the traditional religions of the empire, Libanius cultivated long-lasting friendships with Christians, both as private individuals and as imperial officials.
He studied in Athens and began his career in Constantinople as a private tutor, but was soon exiled to Nicomedia. Before his exile, Libanius was a friend of the emperor Julian, with whom some correspondence survives, and in whose memory he wrote a series of orations; they were composed between 362 and 365.
The works of Libanius are valuable as a historical source for the changing world of the later 4th century. His first "Oration I" is an autobiographical narrative, first written in 374 and revised throughout his life, a scholar's account that ends as an old exile's private journal. In 354, he accepted the chair of rhetoric in Antioch, where he stayed until his death. Although Libanius was not a Christian, his students included such notable Christians as John Chrysostom and Theodore of Mopsuestia. Despite friendship with the restorationist Emperor Julian, he was made an honorary "praetorian prefect" by the Christian Emperor Theodosius I.

</doc>
<doc id="49088" url="http://en.wikipedia.org/wiki?curid=49088" title="College Bowl">
College Bowl

College Bowl (also known as General Electric (G.E.) College Bowl) was a radio television and student quiz show. "College Bowl" first aired on US radio stations in 1953. It then moved to US television broadcast networks, airing 1959 to 1963 on CBS and from 1963 to 1970 on NBC. In 1977, the game resurfaced as an activity on college campuses through an affiliation with the Association of College Unions International (ACUI) that lasted for 31 years. In 2008, the College Bowl Company announced its suspension of the College Bowl program, citing increased costs and financial infeasibility of continuing to work with ACUI.
History.
"College Bowl" originated as a USO activity created by Canadian Don Reid for soldiers serving in World War II. The game was then developed into a radio show by Reid and John Moses. Grant Tinker, later President of NBC and MTM Enterprises, got his start as an assistant on the show.
Two four-member teams representing various colleges and universities competed; one member of each team was its captain. The game began with a "toss-up" question for ten points; the first player to buzz in got the right to answer, but if (s)he was wrong, the other team could try to answer (if a player buzzed in before the host finished reading the question and was wrong, the team was penalized five points). Answering a "toss-up" correctly earned the team the right to answer a multi-part "bonus" question worth up to thirty points; the team members could collaborate, but only the captain was allowed to actually give the answer. The game continued in this manner, and was played in halves. During halftime, the players were allowed to show a short promotional film of their school; or they might talk about career plans or the like.
The first "College Quiz Bowl" match was played on NBC radio on October 10, 1953, when Northwestern University defeated Columbia University, 135-60. Twenty-six episodes ran in that first season, with winning teams receiving $500 grants for their school. "Good Housekeeping" magazine became the sponsor for the 1954-55 season, and a short third season in the autumn of 1955 finished the run. The most dominant team was the University of Minnesota, which had teams appear in 23 of the 68 broadcast matches. The 1953-55 series had a powerful appeal because it used remote broadcasts; each team was located at their own college where they were cheered on by their wildly enthusiastic classmates. The effect was akin to listening to a football game, but this type of excitement evaporated in later versions, in which both teams competed in the same room.
Television.
Though a pilot was shot in the spring of 1955, the game did not move to television until 1959. As "G.E. College Bowl" with General Electric as the primary sponsor, the show ran on CBS from 1959 to 1963, and moved back to NBC from 1963 to 1970. Allen Ludden was the original host, but left to do "Password" full-time in 1962. Robert Earle was moderator for the rest of the run. The norm developed in the Ludden-Earle era of undefeated teams retiring after winning five games. Each winning team earned $1,500 in scholarship grants from General Electric with runner-up teams receiving $500. A team's fifth victory awarded $3,000 from General Electric plus $1,500 from Gimbel's department stores for a grand total of $10,500. On April 16, 1967, "Seventeen" magazine matched GE's payouts so that each victory won $3,000 and runners-up earned $1,000. The payouts from Gimbel department stores remained the same so that five-time champions retired with a grand total of $19,500.
Colgate University was the first team to win five consecutive contests and become "retired undefeated champions," defeating New York University in Colgate's first appearance in April 1960 when NYU was going for its fifth win. Rutgers was the second college to win five contests and be retired. Colgate later defeated Rutgers in a special one-time playoff contest to become the only six-time winner in a "five-win-limit" competition. An upset occurred in 1961, when the small liberal arts colleges of Hobart and William Smith in Geneva, New York, defeated Baylor University to become the third college to retire undefeated. In another example, Lafayette College retired undefeated in fall 1962 after beating the University of California, Berkeley for its fifth victory, a David and Goliath event.
The show licensed and spun off three other academic competitions in the U.S.:
International versions.
"University Challenge".
A British version of the televised College Bowl competition was launched as "University Challenge" in 1962. The program, presented by Bamber Gascoigne, produced by Granada Television and broadcast across the ITV network, was very popular and ran until it was taken off the air in 1987. In 1994 the show was resurrected by the BBC with Jeremy Paxman as the new quizmaster. It remains very popular in Britain. The show, and the catch phrase used by Gascoigne (and later Paxman) before each toss-up question, "Your starter for 10," was the inspiration for the novel "Starter for 10", and the subsequent film.
"Challenging Times".
An Irish version of the competition called "Challenging Times" ran between 1991 and 2002. It was sponsored by The Irish Times newspaper, and presented by Kevin Myers, then a columnist with that newspaper. Over the course of the show, University College Cork had the most wins, with three, while National University of Ireland, Galway qualified for the most finals, winning twice and placing second twice.
Later history.
The game returned to radio from 1979 to 1982, hosted by Art Fleming (the host of the original Jeopardy!), with the 1978 and 1979 national tournament semi-finals and finals appearing on syndicated television. The two champions from those years competed against teams from the UK for the "College Bowl World Championship," which were also televised; in 1978, Stanford University played a team of UK all-stars under College Bowl rules, and in 1979, Davidson College played Sidney Sussex College, Cambridge University under University Challenge rules. (The UK teams won in both years.) There have been two television appearances since then; the 1984 tournament semi-finals and finals aired on NBC, hosted by Pat Sajak (of Wheel of Fortune fame), and the entire 1987 tournament on Disney Channel, hosted by Dick Cavett. The University of Minnesota won both iterations.
In 1970, modern quiz bowl invitational tournaments began with the Southeastern Invitational Tournament, and the circuit expanded through the 1970s, 1980s, and 1990s. These tournaments increasingly made various modifications to the College Bowl format, and came to be known as quiz bowl. Earlier invitational tournaments, such as the "Syra-quiz" at Syracuse University, had occurred in the 1950s and 1960s. 
In 1976, the program became affiliated with the Association of College Unions International (ACUI) , which continued to promote the competition as a non-broadcast event after the demise of the radio and television experiments. That affiliation ended in 2008, and the College Bowl program is no longer active. The College Bowl Company continues to administer the Honda Campus All-Star Challenge at historically black colleges and universities.
In the 1990s with the rise of the Academic Competition Federation and National Academic Quiz Tournaments, both with their own national championships, a number of schools (such as the University of Maryland, the University of Chicago, both former national champions, and recent runner up Georgia Tech) "de-affiliated" from College Bowl. Factors which contributed to this process included, among other issues, eligibility rules for College Bowl (which limited the number of graduate students who could compete and required a minimum courseload), higher participation costs for College Bowl relative to these other formats, and concerns regarding the quality and difficulty of the questions used in College Bowl competitions.
Criticism.
In the 1987 and 1988 regional tournaments, College Bowl was accused of recycling questions from previous tournaments, thereby possibly compromising the integrity of results. Questions for tournaments need to be new for all teams involved, or certain teams could have a competitive advantage from having heard some questions previously. The 1987 National Tournament on the Disney Channel saw additional controversy, as a number of protested matches proved to strain the television format. Especially in the early 1990s, The College Bowl Company attempted to collect licensing fees based on copyright and trade dress claims from invitational tournaments that employed formats that it claimed were similar to College Bowl, and threatened not to allow schools that failed to pay these fees to compete in College Bowl events. As it was, the company's intellectual property claims were never tested in court and these events along with the growing Internet community of quiz bowl players led to a great increase in teams, tournaments, and formats.
Top four finishers of CBI National Championship Tournament (1978-2008).
No tournament was held in 1983 or 1985, though regional tournaments were held in each year.
†Tied for third (lost in semifinals, no playoff for third place).
††In 1994, Brigham Young University finished second in the round-robin, qualifying for the final series. However, as the final best-two-out-of-three series was held on Sunday, the team declined to participate, and the University of Virginia took their place instead. Brigham Young was awarded third place.

</doc>
<doc id="49089" url="http://en.wikipedia.org/wiki?curid=49089" title="Cox's theorem">
Cox's theorem

Cox's theorem, named after the physicist Richard Threlkeld Cox, is a derivation of the laws of probability theory from a certain set of postulates. This derivation justifies the so-called "logical" interpretation of probability. As the laws of probability derived by Cox's theorem are applicable to any proposition, logical probability is a type of Bayesian probability. Other forms of Bayesianism, such as the subjective interpretation, are given other justifications.
Cox's assumptions.
Cox wanted his system to satisfy the following conditions:
The postulates as stated here are taken from Arnborg and Sjödin.
"Common sense" includes consistency with Aristotelian logic when
statements are completely plausible or implausible.
The postulates as originally stated by Cox were not mathematically
rigorous (although better than the informal description above), e.g.,
as noted by Halpern. However it appears to be possible
to augment them with various mathematical assumptions made either
implicitly or explicitly by Cox to produce a valid proof.
Cox's axioms and functional equations are:
Cox's theorem implies that any plausibility model that meets the
postulates is equivalent to the subjective probability model, i.e.,
can be converted to the probability model by rescaling.
Implications of Cox's postulates.
The laws of probability derivable from these postulates are the following. Here "w"("A"|"B") is the "plausibility" of the proposition "A" given "B", and "m" is some positive number. Further, "A""C" represents the absolute complement of "A".
It is important to note that the postulates imply only these general properties. We may recover the usual usual laws of probability by setting a new function, conventionally denoted "P" or Pr, equal to "w""m". Then we obtain the laws of probability in a more familiar form:
Rule 2 is a rule for negation, and rule 3 is a rule for conjunction. Given that any proposition containing conjunction, disjunction, and negation can be equivalently rephrased using conjunction and negation alone (the conjunctive normal form), we can now handle any compound proposition.
The laws thus derived yield finite additivity of probability, but not countable additivity. The measure-theoretic formulation of Kolmogorov assumes that a probability measure is countably additive. This slightly stronger condition is necessary for the proof of certain theorems.
Interpretation and further discussion.
Cox's theorem has come to be used as one of the justifications for the
use of Bayesian probability theory. For example, in Jaynes it is
discussed in detail in chapters 1 and 2 and is a cornerstone for the
rest of the book. Probability is interpreted as a formal system of
logic, the natural extension of Aristotelian logic (in which every
statement is either true or false) into the realm of reasoning in the
presence of uncertainty.
It has been debated to what degree the theorem excludes alternative models for reasoning about uncertainty. For example, if certain "unintuitive" mathematical assumptions were dropped then alternatives could be devised, e.g., an example provided by Halpern. However Arnborg and Sjödin suggest additional
"common sense" postulates, which would allow the assumptions to be relaxed in some cases while still ruling out the Halpern example. Other approaches were devised by Hardy or Dupré and Tipler.
The original formulation of Cox's theorem is in which is extended with additional results and more discussion in . Jaynes cites Abel for the first known use of the associativity functional equation. Aczél provides a long proof of the "associativity equation" (pages 256-267). Jaynes (p27) reproduces the shorter proof by Cox in which differentiability is assumed. A guide to Cox's theorem by Van Horn aims at comprehensively introducing the reader to all these references.
References.
</dl>

</doc>
<doc id="49090" url="http://en.wikipedia.org/wiki?curid=49090" title="Ohm's law">
Ohm's law

Ohm's law states that the current through a conductor between two points is directly proportional to the potential difference across the two points. Introducing the constant of proportionality, the resistance, one arrives at the usual mathematical equation that describes this relationship:
where "I" is the current through the conductor in units of amperes, "V" is the potential difference measured "across" the conductor in units of volts, and "R" is the resistance of the conductor in units of ohms. More specifically, Ohm's law states that the "R" in this relation is constant, independent of the current.
The law was named after the German physicist Georg Ohm, who, in a treatise published in 1827, described measurements of applied voltage and current through simple electrical circuits containing various lengths of wire. He presented a slightly more complex equation than the one above (see History section below) to explain his experimental results. The above equation is the modern form of Ohm's law.
In physics, the term "Ohm's law" is also used to refer to various generalizations of the law originally formulated by Ohm. The simplest example of this is:
where J is the current density at a given location in a resistive material, E is the electric field at that location, and "σ" (Sigma) is a material-dependent parameter called the conductivity. This reformulation of Ohm's law is due to Gustav Kirchhoff.
History.
In January 1781, before Georg Ohm's work, Henry Cavendish experimented with Leyden jars and glass tubes of varying diameter and length filled with salt solution. He measured the current by noting how strong a shock he felt as he completed the circuit with his body. Cavendish wrote that the "velocity" (current) varied directly as the "degree of electrification" (voltage). He did not communicate his results to other scientists at the time, and his results were unknown until Maxwell published them in 1879.
Ohm did his work on resistance in the years 1825 and 1826, and published his results in 1827 as the book "Die galvanische Kette, mathematisch bearbeitet" ("The galvanic circuit investigated mathematically").
He drew considerable inspiration from Fourier's work on heat conduction in the theoretical explanation of his work. For experiments, he initially used voltaic piles, but later used a thermocouple as this provided a more stable voltage source in terms of internal resistance and constant potential difference. He used a galvanometer to measure current, and knew that the voltage between the thermocouple terminals was proportional to the junction temperature. He then added test wires of varying length, diameter, and material to complete the circuit. He found that his data could be modeled through the equation
where "x" was the reading from the galvanometer, "l" was the length of the test conductor, "a" depended only on the thermocouple junction temperature, and "b" was a constant of the entire setup. From this, Ohm determined his law of proportionality and published his results.
Ohm's law was probably the most important of the early quantitative descriptions of the physics of electricity. We consider it almost obvious today. When Ohm first published his work, this was not the case; critics reacted to his treatment of the subject with hostility. They called his work a "web of naked fancies" and the German Minister of Education proclaimed that "a professor who preached such heresies was unworthy to teach science." The prevailing scientific philosophy in Germany at the time asserted that experiments need not be performed to develop an understanding of nature because nature is so well ordered, and that scientific truths may be deduced through reasoning alone. Also, Ohm's brother Martin, a mathematician, was battling the German educational system. These factors hindered the acceptance of Ohm's work, and his work did not become widely accepted until the 1840s. Fortunately, Ohm received recognition for his contributions to science well before he died.
In the 1850s, Ohm's law was known as such and was widely considered proved, and alternatives, such as "Barlow's law", were discredited, in terms of real applications to telegraph system design, as discussed by Samuel F. B. Morse in 1855.
While the old term for electrical conductance, the mho (the inverse of the resistance unit ohm), is still used, a new name, the siemens, was adopted in 1971, honoring Ernst Werner von Siemens. The siemens is preferred in formal papers.
In the 1920s, it was discovered that the current through a practical resistor actually has statistical fluctuations, which depend on temperature, even when voltage and resistance are exactly constant; this fluctuation, now known as Johnson–Nyquist noise, is due to the discrete nature of charge. This thermal effect implies that measurements of current and voltage that are taken over sufficiently short periods of time will yield ratios of V/I that fluctuate from the value of R implied by the time average or ensemble average of the measured current; Ohm's law remains correct for the average current, in the case of ordinary resistive materials.
Ohm's work long preceded Maxwell's equations and any understanding of frequency-dependent effects in AC circuits. Modern developments in electromagnetic theory and circuit theory do not contradict Ohm's law when they are evaluated within the appropriate limits.
Scope.
Ohm's law is an empirical law, a generalization from many experiments that have shown that current is approximately proportional to electric field for most materials. It is less fundamental than Maxwell's equations and is not always obeyed. Any given material will break down under a strong-enough electric field, and some materials of interest in electrical engineering are "non-ohmic" under weak fields.
Ohm's law has been observed on a wide range of length scales. In the early 20th century, it was thought that Ohm's law would fail at the atomic scale, but experiments have not borne out this expectation. As of 2012, researchers have demonstrated that Ohm's law works for silicon wires as small as four atoms wide and one atom high.
Microscopic origins.
The dependence of the current density on the applied electric field is essentially quantum mechanical in nature; (see Classical and quantum conductivity.) A qualitative description leading to Ohm's law can be based upon classical mechanics using the Drude model developed by Paul Drude in 1900.
The Drude model treats electrons (or other charge carriers) like pinballs bouncing among the ions that make up the structure of the material. Electrons will be accelerated in the opposite direction to the electric field by the average electric field at their location. With each collision, though, the electron is deflected in a random direction with a velocity that is much larger than the velocity gained by the electric field. The net result is that electrons take a zigzag path due to the collisions, but generally drift in a direction opposing the electric field.
The drift velocity then determines the electric current density and its relationship to E and is independent of the collisions. Drude calculated the average drift velocity from p = −"eEτ" where p is the average momentum, −"e" is the charge of the electron and τ is the average time between the collisions. Since both the momentum and the current density are proportional to the drift velocity, the current density becomes proportional to the applied electric field; this leads to Ohm's law.
Hydraulic analogy.
A hydraulic analogy is sometimes used to describe Ohm's law. Water pressure, measured by pascals (or PSI), is the analog of voltage because establishing a water pressure difference between two points along a (horizontal) pipe causes water to flow. Water flow rate, as in liters per second, is the analog of current, as in coulombs per second. Finally, flow restrictors—such as apertures placed in pipes between points where the water pressure is measured—are the analog of resistors. We say that the rate of water flow through an aperture restrictor is proportional to the difference in water pressure across the restrictor. Similarly, the rate of flow of electrical charge, that is, the electric current, through an electrical resistor is proportional to the difference in voltage measured across the resistor.
Flow and pressure variables can be calculated in fluid flow network with the use of the hydraulic ohm analogy. The method can be applied to both steady and transient flow situations. In the linear laminar flow region, Poiseuille's law describes the hydraulic resistance of a pipe, but in the turbulent flow region the pressure–flow relations become nonlinear.
The hydraulic analogy to Ohm's law has been used, for example, to approximate blood flow through the circulatory system.
Circuit analysis.
In circuit analysis, three equivalent expressions of Ohm's law are used interchangeably:
Each equation is quoted by some sources as the defining relationship of Ohm's law,
or all three are quoted, or derived from a proportional form,
or even just the two that do not correspond to Ohm's original statement may sometimes be given.
The interchangeability of the equation may be represented by a triangle, where V (voltage) is placed on the top section, the I (current) is placed to the left section, and the R (resistance) is placed to the right. The line that divides the left and right sections indicate multiplication, and the divider between the top and bottom sections indicates division (hence the division bar).
Resistive circuits.
Resistors are circuit elements that impede the passage of electric charge in agreement with Ohm's law, and are designed to have a specific resistance value "R". In a schematic diagram the resistor is shown as a zig-zag symbol. An element (resistor or conductor) that behaves according to Ohm's law over some operating range is referred to as an "ohmic device" (or an "ohmic resistor") because Ohm's law and a single value for the resistance suffice to describe the behavior of the device over that range.
Ohm's law holds for circuits containing only resistive elements (no capacitances or inductances) for all forms of driving voltage or current, regardless of whether the driving voltage or current is constant (DC) or time-varying such as AC. At any instant of time Ohm's law is valid for such circuits.
Resistors which are in "series" or in "parallel" may be grouped together into a single "equivalent resistance" in order to apply Ohm's law in analyzing the circuit.
Reactive circuits with time-varying signals.
When reactive elements such as capacitors, inductors, or transmission lines are involved in a circuit to which AC or time-varying voltage or current is applied, the relationship between voltage and current becomes the solution to a differential equation, so Ohm's law (as defined above) does not directly apply since that form contains only resistances having value R, not complex impedances which may contain capacitance ("C") or inductance ("L").
Equations for time-invariant AC circuits take the same form as Ohm's law, however, the variables are generalized to complex numbers and the current and voltage waveforms are complex exponentials.
In this approach, a voltage or current waveform takes the form formula_5, where "t" is time, "s" is a complex parameter, and "A" is a complex scalar. In any linear time-invariant system, all of the currents and voltages can be expressed with the same "s" parameter as the input to the system, allowing the time-varying complex exponential term to be canceled out and the system described algebraically in terms of the complex scalars in the current and voltage waveforms.
The complex generalization of resistance is impedance, usually denoted "Z"; it can be shown that for an inductor,
and for a capacitor,
We can now write,
where V and I are the complex scalars in the voltage and current respectively and Z is the complex impedance.
This form of Ohm's law, with "Z" taking the place of "R", generalizes the simpler form. When "Z" is complex, only the real part is responsible for dissipating heat.
In the general AC circuit, "Z" varies strongly with the frequency parameter "s", and so also will the relationship between voltage and current.
For the common case of a steady sinusoid, the "s" parameter is taken to be formula_9, corresponding to a complex sinusoid formula_10. The real parts of such complex current and voltage waveforms describe the actual sinusoidal currents and voltages in a circuit, which can be in different phases due to the different complex scalars.
Linear approximations.
Ohm's law is one of the basic equations used in the analysis of electrical circuits. It applies to both metal conductors and circuit components (resistors) specifically made for this behaviour. Both are ubiquitous in electrical engineering. Materials and components that obey Ohm's law are described as "ohmic" which means they produce the same value for resistance (R = V/I) regardless of the value of V or I which is applied and whether the applied voltage or current is DC (direct current) of either positive or negative polarity or AC (alternating current).
In a true ohmic device, the same value of resistance will be calculated from R = V/I regardless of the value of the applied voltage V. That is, the ratio of V/I is constant, and when current is plotted as a function of voltage the curve is "linear" (a straight line). If voltage is forced to some value V, then that voltage V divided by measured current I will equal R. Or if the current is forced to some value I, then the measured voltage V divided by that current I is also R. Since the plot of I versus V is a straight line, then it is also true that for any set of two different voltages V1 and V2 applied across a given device of resistance R, producing currents I1 = V1/R and I2 = V2/R, that the ratio (V1-V2)/(I1-I2) is also a constant equal to R. The operator "delta" (Δ) is used to represent a difference in a quantity, so we can write ΔV = V1-V2 and ΔI = I1-I2. Summarizing, for any truly ohmic device having resistance R, V/I = ΔV/ΔI = R for any applied voltage or current or for the difference between any set of applied voltages or currents.
There are, however, components of electrical circuits which do not obey Ohm's law; that is, their relationship between current and voltage (their I–V curve) is "nonlinear" (or non-ohmic). An example is the p-n junction diode (curve at right). As seen in the figure, the current does not increase linearly with applied voltage for a diode. One can determine a value of current (I) for a given value of applied voltage (V) from the curve, but not from Ohm's law, since the value of "resistance" is not constant as a function of applied voltage. Further, the current only increases significantly if the applied voltage is positive, not negative. The ratio "V"/"I" for some point along the nonlinear curve is sometimes called the "static", or "chordal", or DC, resistance, but as seen in the figure the value of total "V" over total "I" varies depending on the particular point along the nonlinear curve which is chosen. This means the "DC resistance" V/I at some point on the curve is not the same as what would be determined by applying an AC signal having peak amplitude ΔV volts or ΔI amps centered at that same point along the curve and measuring ΔV/ΔI. However, in some diode applications, the AC signal applied to the device is small and it is possible to analyze the circuit in terms of the "dynamic", "small-signal", or "incremental" resistance, defined as the one over the slope of the V–I curve at the average value (DC operating point) of the voltage (that is, one over the derivative of current with respect to voltage). For sufficiently small signals, the dynamic resistance allows the Ohm's law small signal resistance to be calculated as approximately one over the slope of a line drawn tangentially to the V-I curve at the DC operating point.
Temperature effects.
Ohm's law has sometimes been stated as, "for a conductor in a given state, the electromotive force is proportional to the current produced." That is, that the resistance, the ratio of the applied electromotive force (or voltage) to the current, "does not vary with the current strength ." The qualifier "in a given state" is usually interpreted as meaning "at a constant temperature," since the resistivity of materials is usually temperature dependent. Because the conduction of current is related to Joule heating of the conducting body, according to Joule's first law, the temperature of a conducting body may change when it carries a current. The dependence of resistance on temperature therefore makes resistance depend upon the current in a typical experimental setup, making the law in this form difficult to directly verify. Maxwell and others worked out several methods to test the law experimentally in 1876, controlling for heating effects.
Relation to heat conductions.
Ohm's principle predicts the flow of electrical charge (i.e. current) in electrical conductors when subjected to the influence of voltage differences; Jean-Baptiste-Joseph Fourier's principle predicts the flow of heat in heat conductors when subjected to the influence of temperature differences.
The same equation describes both phenomena, the equation's variables taking on different meanings in the two cases. Specifically, solving a heat conduction (Fourier) problem with "temperature" (the driving "force") and "flux of heat" (the rate of flow of the driven "quantity", i.e. heat energy) variables also solves an analogous electrical conduction (Ohm) problem having "electric potential" (the driving "force") and "electric current" (the rate of flow of the driven "quantity", i.e. charge) variables.
The basis of Fourier's work was his clear conception and definition of thermal conductivity. He assumed that, all else being the same, the flux of heat is strictly proportional to the gradient of temperature. Although undoubtedly true for small temperature gradients, strictly proportional behavior will be lost when real materials (e.g. ones having a thermal conductivity that is a function of temperature) are subjected to large temperature gradients.
A similar assumption is made in the statement of Ohm's law: other things being alike, the strength of the current at each point is proportional to the gradient of electric potential. The accuracy of the assumption that flow is proportional to the gradient is more readily tested, using modern measurement methods, for the electrical case than for the heat case.
Other versions.
Ohm's law, in the form above, is an extremely useful equation in the field of electrical/electronic engineering because it describes how voltage, current and resistance are interrelated on a "macroscopic" level, that is, commonly, as circuit elements in an electrical circuit. Physicists who study the electrical properties of matter at the microscopic level use a closely related and more general vector equation, sometimes also referred to as Ohm's law, having variables that are closely related to the V, I, and R scalar variables of Ohm's law, but which are each functions of position within the conductor. Physicists often use this continuum form of Ohm's Law:
where "E" is the electric field vector with units of volts per meter (analogous to "V" of Ohm's law which has units of volts), "J" is the current density vector with units of amperes per unit area (analogous to "I" of Ohm's law which has units of amperes), and "ρ" (Greek "rho") is the resistivity with units of ohm·meters (analogous to "R" of Ohm's law which has units of ohms). The above equation is sometimes written as J = formula_12E where "σ" (Greek "sigma") is the conductivity which is the reciprocal of ρ.
The potential difference between two points is defined as:
with formula_14 the element of path along the integration of electric field vector E. If the applied E field is uniform and oriented along the length of the conductor as shown in the figure, then defining the voltage V in the usual convention of being opposite in direction to the field (see figure), and with the understanding that the voltage V is measured differentially across the length of the conductor allowing us to drop the Δ symbol, the above vector equation reduces to the scalar equation:
Since the E field is uniform in the direction of wire length, for a conductor having uniformly consistent resistivity ρ, the current density J will also be uniform in any cross-sectional area and oriented in the direction of wire length, so we may write:
Substituting the above 2 results (for "E" and "J" respectively) into the continuum form shown at the beginning of this section:
The electrical resistance of a uniform conductor is given in terms of resistivity by:
where "l" is the length of the conductor in SI units of meters, "a" is the cross-sectional area (for a round wire "a" = "πr"2 if "r" is radius) in units of meters squared, and ρ is the resistivity in units of ohm·meters.
After substitution of "R" from the above equation into the equation preceding it, the continuum form of Ohm's law for a uniform field (and uniform current density) oriented along the length of the conductor reduces to the more familiar form:
A perfect crystal lattice, with low enough thermal motion and no deviations from periodic structure, would have no resistivity, but a real metal has crystallographic defects, impurities, multiple isotopes, and thermal motion of the atoms. Electrons scatter from all of these, resulting in resistance to their flow.
The more complex generalized forms of Ohm's law are important to condensed matter physics, which studies the properties of matter and, in particular, its electronic structure. In broad terms, they fall under the topic of constitutive equations and the theory of transport coefficients.
Magnetic effects.
If an external B-field is present and the conductor is not at rest but moving at velocity v, then an extra term must be added to account for the current induced by the Lorentz force on the charge carriers.
In the rest frame of the moving conductor this term drops out because v= 0. There is no contradiction because the electric field in the rest frame differs from the E-field in the lab frame: E ' = E + v×B.
Electric and magnetic fields are relative, see Lorentz transform.
If the current J is alternating because the applied voltage or E-field varies in time, then reactance must be added to resistance to account for self-inductance, see electrical impedance. The reactance may be strong if the frequency is high or the conductor is coiled.
See Hall effect for some other implication of a magnetic field.
Conductive fluids.
In a conductive fluid, such as a plasma, there is a similar effect. Consider a fluid moving with the velocity formula_21 in a magnetic field formula_22. The relative motion induces an electric field formula_23 which exerts electric force on the charged particles giving rise to an electric current formula_24. The equation of motion for the electron gas, with a number densityformula_25, is written as
where formula_27, formula_28 and formula_29 are the charge, mass and velocity of the electrons, respectively. Also, formula_30 is the frequency of collisions of the electrons with ions which have a velocity field formula_31. Since, the electron has a very small mass compared with that of ions, we can ignore the left hand side of the above equation to write
where we have used the definition of the current density, and also put formula_33 which is the electrical conductivity. This equation can also be equivalently written as
where formula_35 is the electrical resistivity. It is also common to write formula_36 instead of formula_37 which can be confusing since it is the same notation used for the magnetic diffusivity defined as formula_38.

</doc>
<doc id="49091" url="http://en.wikipedia.org/wiki?curid=49091" title="Optical character recognition">
Optical character recognition

Optical character recognition (OCR) is the mechanical or electronic conversion of images of typewritten or printed text into machine-encoded text. It is widely used as a form of data entry from printed paper data records, whether passport documents, invoices, bank statements, computerized receipts, business cards, mail, printouts of static-data, or any suitable documentation. It is a common method of digitizing printed texts so that it can be electronically edited, searched, stored more compactly, displayed on-line, and used in machine processes such as machine translation, text-to-speech, key data and text mining. OCR is a field of research in pattern recognition, artificial intelligence and computer vision.
Early versions needed to be trained with images of each character, and worked on one font at a time. Advanced systems capable of producing a high degree of recognition accuracy for most fonts are now common. Some systems are capable of reproducing formatted output that closely approximates the original page including images, columns, and other non-textual components.
History.
Early optical character recognition may be traced to technologies involving telegraphy and creating reading devices for the blind. In 1914, Emanuel Goldberg developed a machine that read characters and converted them into standard telegraph code. Concurrently, Edmund Fournier d'Albe developed the Optophone, a handheld scanner that when moved across a printed page, produced tones that corresponded to specific letters or characters.
In the late 1920s and into the 1930s Emanuel Goldberg developed what he called a "Statistical Machine" for searching microfilm archives using an optical code recognition system. In 1931 he was granted USA Patent number 1,838,389 for the invention. The patent was acquired by IBM.
Blind and visually impaired users.
In 1974, Ray Kurzweil started the company Kurzweil Computer Products, Inc. and continued development of omni-font OCR, which could recognize text printed in virtually any font (Kurzweil is often credited with inventing omni-font OCR, but it was in use by companies, including CompuScan, in the late 1960s and 1970s.) Kurzweil decided that the best application of this technology would be to create a reading machine for the blind, which would allow blind people to have a computer read text to them out loud. This device required the invention of two enabling technologies – the CCD flatbed scanner and the text-to-speech synthesizer. On January 13, 1976, the successful finished product was unveiled during a widely reported news conference headed by Kurzweil and the leaders of the National Federation of the Blind. In 1978, Kurzweil Computer Products began selling a commercial version of the optical character recognition computer program. LexisNexis was one of the first customers, and bought the program to upload legal paper and news documents onto its nascent online databases. Two years later, Kurzweil sold his company to Xerox, which had an interest in further commercializing paper-to-computer text conversion. Xerox eventually spun it off as Scansoft, which merged with Nuance Communications.. The research group headed by Prof. A. G. Ramakrishnan at the Medical intelligence and language engineering lab, Indian Institute of Science, has developed PrintToBraille tool, an open source GUI frontend that can be used by any OCR to convert scanned images of printed books to Braille books.
In the 2000s, OCR was made available online as a service (WebOCR), in a cloud computing environment, and in mobile applications like real-time translation of foreign-language signs on a smartphone.
 are available for most common writing systems, including Latin, Cyrillic, Arabic, Hebrew, Indic, Bengali (Bangla), Devanagari, Tamil, Chinese, Japanese, and Korean characters.
Applications.
OCR engines have been developed into many kinds of object-oriented OCR applications, such as receipt OCR, invoice OCR, check OCR, legal billing document OCR.
They can be used for:
Types.
OCR is generally an "offline" process, which analyzes a static document. Handwriting movement analysis can be used as input to handwriting recognition. Instead of merely using the shapes of glyphs and words, this technique is able to capture motions, such as the order in which segments are drawn, the direction, and the pattern of putting the pen down and lifting it. This additional information can make the end-to-end process more accurate. This technology is also known as "on-line character recognition", "dynamic character recognition", "real-time character recognition", and "intelligent character recognition".
Techniques.
Pre-processing.
OCR software often "pre-processes" images to improve the chances of successful recognition. Techniques include:
Segmentation of fixed-pitch fonts is accomplished relatively simply by aligning the image to a uniform grid based on where vertical grid lines will least often intersect black areas. For proportional fonts, more sophisticated techniques are needed because whitespace between letters can sometimes be greater than that between words, and vertical lines can intersect more than one character.
Character recognition.
There are two basic types of core OCR algorithm, which may produce a ranked list of candidate characters.
Matrix matching involves comparing an image to a stored glyph on a pixel-by-pixel basis; it is also known as "pattern matching", "pattern recognition", or "image correlation". This relies on the input glyph being correctly isolated from the rest of the image, and on the stored glyph being in a similar font and at the same scale. This technique works best with typewritten text and does not work well when new fonts are encountered. This is the technique the early physical photocell-based OCR implemented, rather directly.
Feature extraction decomposes glyphs into "features" like lines, closed loops, line direction, and line intersections. These are compared with an abstract vector-like representation of a character, which might reduce to one or more glyph prototypes. General techniques of feature detection in computer vision are applicable to this type of OCR, which is commonly seen in "intelligent" handwriting recognition and indeed most modern OCR software. Nearest neighbour classifiers such as the k-nearest neighbors algorithm are used to compare image features with stored glyph features and choose the nearest match.
Software such as Cuneiform and Tesseract use a two-pass approach to character recognition. The second pass is known as "adaptive recognition" and uses the letter shapes recognized with high confidence on the first pass to recognize better the remaining letters on the second pass. This is advantageous for unusual fonts or low-quality scans where the font is distorted (e.g. blurred or faded).
Post-processing.
OCR accuracy can be increased if the output is constrained by a lexicon – a list of words that are allowed to occur in a document. This might be, for example, all the words in the English language, or a more technical lexicon for a specific field. This technique can be problematic if the document contains words not in the lexicon, like proper nouns. Tesseract uses its dictionary to influence the character segmentation step, for improved accuracy.
The output stream may be a plain text stream or file of characters, but more sophisticated OCR systems can preserve the original layout of the page and produce, for example, an annotated PDF that includes both the original image of the page and a searchable textual representation.
"Near-neighbor analysis" can make use of co-occurrence frequencies to correct errors, by noting that certain words are often seen together. For example, "Washington, D.C." is generally far more common in English than "Washington DOC".
Knowledge of the grammar of the language being scanned can also help determine if a word is likely to be a verb or a noun, for example, allowing greater accuracy.
Application-specific optimizations.
In recent years, the major OCR technology providers began to tweak OCR systems to better deal with specific types of input. Beyond an application-specific lexicon, better performance can be had by taking into account business rules, standard expression, or rich information contained in color images. This strategy is called "Application-Oriented OCR" or "Customized OCR", and has been applied to OCR of license plates, business cards, invoices, screenshots, ID cards, driver licenses, and automobile manufacturing.
Workarounds.
There are several techniques for solving the problem of character recognition by means other than improved OCR algorithms.
Forcing better input.
Special fonts like OCR-A, OCR-B, or MICR fonts, with precisely specified sizing, spacing, and distinctive character shapes, allow a higher accuracy rate during transcription. These were often used in early matrix-matching systems.
"Comb fields" are pre-printed boxes that encourage humans to write more legibly – one glyph per box. These are often printed in a "dropout color" which can be easily removed by the OCR system.
Palm OS used a special set of glyphs, known as "Grafitti" which are similar to printed English characters but simplified or modified for easier recognition on the platform's computationally limited hardware. Users would need to learn how to write these special glyphs.
Zone-based OCR restricts the image to a specific part of a document. This is often referred to as "Template OCR".
Crowdsourcing.
Crowdsourcing humans to perform the character recognition can quickly process images like computer-driven OCR, but with higher accuracy for recognizing images than is obtained with computers. Practical systems include the Amazon Mechanical Turk and reCAPTCHA.
Accuracy.
Commissioned by the U.S. Department of Energy (DOE), the Information Science Research Institute (ISRI) had the mission to foster the improvement of automated technologies for understanding machine printed documents, and it conducted the most authoritative of the "Annual Test of OCR Accuracy" from 1992 to 1996.
Recognition of Latin-script, typewritten text is still not 100% accurate even where clear imaging is available. One study based on recognition of 19th- and early 20th-century newspaper pages concluded that character-by-character OCR accuracy for commercial OCR software varied from 81% to 99%; total accuracy can be achieved by human review or Data Dictionary Authentication. Other areas—including recognition of hand printing, cursive handwriting, and printed text in other scripts (especially those East Asian language characters which have many strokes for a single character)—are still the subject of active research. The MNIST database is commonly used for testing systems' ability to recognize handwritten digits.
Accuracy rates can be measured in several ways, and how they are measured can greatly affect the reported accuracy rate. For example, if word context (basically a lexicon of words) is not used to correct software finding non-existent words, a character error rate of 1% (99% accuracy) may result in an error rate of 5% (95% accuracy) or worse if the measurement is based on whether each whole word was recognized with no incorrect letters.
Web based OCR systems for recognizing hand-printed text on the fly have become well known as commercial products in recent years (see Tablet PC history). Accuracy rates of 80% to 90% on neat, clean hand-printed characters can be achieved by pen computing software, but that accuracy rate still translates to dozens of errors per page, making the technology useful only in very limited applications.
Recognition of cursive text is an active area of research, with recognition rates even lower than that of hand-printed text. Higher rates of recognition of general cursive script will likely not be possible without the use of contextual or grammatical information. For example, recognizing entire words from a dictionary is easier than trying to parse individual characters from script. Reading the "Amount" line of a cheque (which is always a written-out number) is an example where using a smaller dictionary can increase recognition rates greatly. The shapes of individual cursive characters themselves simply do not contain enough information to accurately (greater than 98%) recognise all handwritten cursive script.
Unicode.
Characters to support OCR were added to the Unicode Standard in June, 1993 with the release of version 1.1.
Some of these characters are mapped from fonts specific to MICR or OCR-A.

</doc>
<doc id="49093" url="http://en.wikipedia.org/wiki?curid=49093" title="Tom Clancy's Rainbow Six (video game)">
Tom Clancy's Rainbow Six (video game)

Tom Clancy's Rainbow Six is a tactical shooter computer game and the first in the Rainbow Six series. It was developed and published by Red Storm Entertainment for the PC in 1998. It was later ported to Mac OS, Nintendo 64, PlayStation, Dreamcast and Game Boy Color. An expansion pack, "Tom Clancy's Rainbow Six Mission Pack: Eagle Watch", was released on January 31, 1999. The PC version is available for purchase on GOG.com, while the PlayStation version is now available for download from the PlayStation Store.
Gameplay.
"Rainbow Six" is a tactical shooter, which focuses more on stealth and tactics than on sheer firepower, exploring the lethality of a single bullet. To add to the realism, all in-game characters, terrorists, hostages and Rainbow operatives, can be wounded or dispatched in just fractions of a second with only one or two bullets. Tools such as thicker body armor, automatic rifles, and grenades have little value before the player grows accustomed to the gameplay.
Before each mission is a planning stage, during which the player is given a briefing, and then chooses the operatives to be involved in the mission, their weapons, equipment and uniform/camouflage. In earlier games, the player pre-established orders and waypoints during this step. The planning stage determined elements such as the path the AI-controlled squads would follow during the mission, as well as where they will deploy devices such as flashbangs or door breaching charges.
Successful missions often last just minutes, but may require dozens of repetitions and planning changes (many more for beginners). During gameplay, the player controls only one team member directly, and can see stats for that member and all units on the Heads-Up Display. Teams not under player control follow the orders given to them in the planning stage. The player can take control of any living operative at will, making them the leader.
The game forms a campaign that is a series of scenarios, with the plot being advanced in the mission briefing of each scenario. Any casualties that occur during a mission are permanent, so the deceased cannot be used in future missions. Consequently, many players replay missions that are technically successful merely to reduce the number of casualties.
Online multiplayer gaming was popular on the Mplayer.com and Zone.com services and for a time featured a thriving competitive clan based community with numerous independent ladder style leagues.
Unlike the other versions, the PlayStation version actually showed the gun being held in the player's hands.
Plot.
Rainbow Six is set in the year 1999. RAINBOW is a newly created multinational counter-terrorism unit, composed of elite soldiers from NATO countries, formed to address the growing problem of international terrorism. The organization's director is John Clark, and the team leader is Ding Chavez. The term "Rainbow Six" refers to the director of the organization, John Clark.
Soon after its inauguration, RAINBOW finds itself responding to a series of seemingly unrelated terrorist attacks by the Phoenix Group, a radical eco-terrorist organization. Throughout its investigation, RAINBOW is assisted and advised by John Brightling, chairman of the powerful biotechnology corporation Horizon Inc.
However, RAINBOW eventually learns that the Phoenix Group is actually a front for Horizon Inc itself. Brightling's company is developing a highly contagious strain of the Ebola virus, called "Shiva", with the ability to kill every human being on the planet. In order to protect "mother nature", John Brightling is planning to kill most of the human race, sparing only Brightling's chosen few, who will re-emerge and rebuild the planet into a scientific and environmentally-friendly utopia. To achieve this goal, he has used the scattered terrorist attacks to create fear of terrorism, which he then exploited in order to get a security contract for his own private security firm at the Olympic Games. Brightling's plan is for his "security personnel" to unleash the virus at the games, spreading it to all the countries of the world.
RAINBOW succeeds in preventing the release of the virus at the Olympics, and Brightling and his collaborators retreat to their Horizon Ark facility in the Brazilian jungle, from which they had originally planned to weather out the global holocaust. RAINBOW infiltrates the facility, killing all of Brightling's collaborators and capturing Brightling himself.
Development.
Red Storm Entertainment (founded by author Tom Clancy) had originally planned to do a special operations game featuring first-person action, and a team of operators rescuing hostages and taking out terrorists. Their first concept was modeled after the FBI Hostage Rescue Team. Later they decided to make the concept more international, as HRT would only operate in the United States, and renamed it "Black Ops" and incorporated operators from all over the world. It was then they found that Clancy was writing a book about terrorism and a special team to combat it, so they rewrote some of the missions to fit within the book plot and Clancy shared his research with the game development team. The book was "Rainbow Six" so the game was renamed "Rainbow Six". However, by the time they finished the game, the book was not yet finished. Thus, the plot of the game does not completely match the plot of the book.
Release.
"Eagle Watch".
"Tom Clancy's Rainbow Six Mission Pack: Eagle Watch" was released on January 31, 1999 as an expansion pack to the original game. It adds five new missions, four new operatives, three new weapons and new multiplayer modes. The expansion was packaged with the original game as "Tom Clancy's Rainbow Six Gold Pack Edition" in 1999.
Reception.
"Rainbow Six" was met with mostly positive reviews on PC. However, the console versions received average to negative response upon release. GameRankings gave it a score of 81.81% for the PC version; 73.71% for the Nintendo 64 version; 72.28% for the Dreamcast version; 53.83% for the Game Boy Color version; and 48.07% for the PlayStation version. Metacritic gave only the PC version a score of 85 out of 100.
GameSpot described the PC version as "actually a pretty good game, albeit very hard and extremely frustrating", and its "audio cues, background sounds, and other various noises are also represented very well; the immersive feeling of "Rainbow Six" is perhaps one of the best seen in a game." In 2001-2002, "Rainbow Six" and "" together sold 450,000 copies. In 1998, CNN working in partnership with Games.Net named Tom Clancy's Rainbow Six as one of the "Top 25 game downloads of 1998"..

</doc>
<doc id="49096" url="http://en.wikipedia.org/wiki?curid=49096" title="Danny Kass">
Danny Kass

Daniel "Danny" Kass (born September 21, 1982) is an American professional snowboarder who has competed at Olympic level.
Kass was born in Pompton Plains section of Pequannock Township, New Jersey. He grew up in New Jersey where he began snowboarding at the age of twelve. His home slopes are what are now known as Hidden Valley and Mountain Creek ski resorts both located in the mountains of Vernon Township and Highland Lakes. After several years of shredding around the Mid-Atlantic States, Kass entered into the Okemo Mountain School to focus more of his time on riding some of the better pipe and park in the East. In 2001, Danny broke out into what has become one of the most successful contest streaks in snowboarding with four US Open Championships, seven Winter X Games medals and two Olympic silver medals.
During the winter of 2006, Kass fought his way back from a slow start in the US Grand Prix Circuit to qualify for the Olympics in Torino for the second time in a row. And just like in 2002, Kass took home another silver medal with back to back 1080s and his signature smooth style.
Kass and his brother Matt Kass were the founders of Grenade Gloves, which specializes in snowboarding gloves, accessories, and other extreme sporting gear.
Kass is one of the stars of "The Adventures of Danny and The Dingo" on Fuel TV.
References.
http://www.yobeat.com/2012/02/22/matt-kasss-hump-day/

</doc>
<doc id="49104" url="http://en.wikipedia.org/wiki?curid=49104" title="University of Minnesota system">
University of Minnesota system

The University of Minnesota System is a university system with several coordinate campuses spread throughout the U.S. state of Minnesota. There are five primary campuses in the Twin Cities, Duluth, Crookston, Morris, and Rochester. A campus was open in Waseca, but was closed and converted into a federal prison for women. The university also operates several research facilities around the state, including some large tracts of land. The other collegiate system of higher education in the state is the larger Minnesota State Colleges and Universities System also known by it's acronym MnSCU.
The University of Minnesota Twin Cities and the Crookston, Duluth, and Morris coordinate campuses are accredited by the Higher Learning Commission (HLC) of the North Central Association of Colleges and Schools.
Campuses.
The flagship Twin Cities campus is by far the largest in the system, with a total enrollment of 51,853 students (undergraduate, graduate, professional, and non-degree included); Duluth reported 11,491; Crookston had 2,764; Morris had 1,896; and Rochester had 414, bringing the system-wide total to 68,418 for fall semester 2012.
The colors of the university, which are used system-wide, are maroon and gold.
Twin Cities.
Because of its size and several decades of history prior to the addition of other campuses, the University of Minnesota Twin Cities (sometimes abbreviated UMTC) is what most people think of upon hearing "University of Minnesota." It can actually be subdivided into multiple parts. Most significantly, Minneapolis and neighboring Saint Paul (actually, the suburb of Falcon Heights) each have distinct campuses. The Minneapolis and St. Paul campuses are connected via a dedicated bus transitway. The buildings on each campus are connected by a series of underground tunnels and above-ground skyways called The Gopher Way. The campus had 51,721 students enrolled for fall 2010, making it the fourth largest public university in the United States.
The Minneapolis portion is the largest and has a number of colleges dedicated to a variety of subjects. Minneapolis' campus can be further subdivided into the East Bank (main portion) and West Bank, as the Mississippi River flows through it. Students become well-acquainted with the double-decker Washington Avenue Bridge that connects the two sections. There are a number of distinguished graduate and professional schools on the Minneapolis campus, notably the University of Minnesota Law School, Medical School, Carlson School of Management, School of Public Health, and Hubert H. Humphrey Institute of Public Affairs. In addition, Minneapolis houses many research facilities such as The Cancer Center.
The Saint Paul campus is more focused on agriculture, though several other subjects are taught there. Due to the workings of the U of M phone system, both campuses have 612 area code (Minneapolis) telephone numbers instead of the 651 code that would be expected for the Saint Paul portion. The Minnesota State Fairgrounds is also located in Falcon Heights.
The mascot for the Twin Cities campus is Goldy the Gopher, and the sports teams are called the Minnesota Golden Gophers. They participate in the NCAA's Division I-A and in the Big Ten Conference. Its hockey program competes in the Western Collegiate Hockey Association.
Among the graduates from this campus are two former U.S. Vice Presidents, Hubert Humphrey and Walter Mondale, former NAACP president Roy Wilkins, British Deputy Prime Minister Nick Clegg, several Nobel prize winners, several athletes such as Ric Flair, Kevin McHale, Dave Winfield, Patty Berg, Brock Lesnar, Curt Hennig, Shelton Benjamin, Bobby Jackson of the NBA, and composer Yanni. Folksinger Bob Dylan famously attended the University and was a part of its thriving "West Bank" music scene, but did not graduate. A wide variety of medical and technological innovations have taken place there as well. For instance, the Internet Gopher protocol was created at the Twin Cities campus. A predecessor of sorts to the World Wide Web, it was named after the school mascot.
Campus media includes the "Minnesota Daily" newspaper, "The Wake Student Magazine", and 770 Radio K (KUOM), an AM radio station that is probably the oldest in the state.
Duluth.
The University of Minnesota Duluth (UMD) became part of the system in 1947, though the campus has a history stretching back to 1895 when it was formed as the Normal School at Duluth. Their teams are nicknamed Bulldogs. Campus media includes the KUMD-FM radio station.
Morris.
The University of Minnesota Morris (UMM) joined the system in 1960 as the system's public liberal arts college. Their teams are nicknamed Cougars. The school operates an FM radio station, KUMM.
Crookston.
The University of Minnesota Crookston (UMC) joined the university system in 1966. At that time it was known as the University of Minnesota Technical Institute at Crookston. Since 1993 the Crookston campus has offered bachelor's degrees, and it has grown to be a more comprehensive regional college campus. It is known for its focus on experiential learning and technology, and through fees each student is provided a laptop computer as part of their experience. The campus mascot is Regal the Eagle, and the athletics teams are known as the Golden Eagles.
Rochester.
The University of Minnesota Rochester (UMR) is the newest campus of the University of Minnesota system, having been formally established in December 2006 (although the University of Minnesota has offered classes in Rochester as a satellite site since as early as 1966). The program offerings at UMR are exclusively within the areas of health science and biotechnology, the largest of which is the Bachelor of Science Health Science (BSHS) program. It is currently housed in University Square in downtown Rochester, near the Mayo Clinic. Plans are being developed for a larger campus of its own. It has no NCAA athletic teams, but it does have a mascot, the Rochester Raptors.
Waseca.
The Waseca campus opened in 1971, but was closed in 1992. Their mascot was the ram. The University still operates an agricultural outreach program in the city. Campus buildings became part of a low-security federal prison (see Federal Correctional Institution, Waseca).
History.
The University of Minnesota was founded in Minneapolis in 1851 as a college preparatory school, seven years prior to Minnesota's statehood. As such, the U enjoys much autonomy from other operations of the state government. The school was closed during the American Civil War, but reopened in 1867. Minneapolis businessman John Sargent Pillsbury is known today as the "Father of the University", and aided the campus through financial troubles as a regent, state senator, and governor. The Morrill Land Grant Colleges Act also helped provide funding for the U.
In 1869 the school reorganized and became an institution of higher education. William Watts Folwell served as the U's first president. An official residence known as Eastcliff has been used by six university presidents since 1958. The 20-room house, originally built by lumber baron Edward Brooks, Sr., was added to the National Register of Historic Places in 2000.
During the traditional autumn through spring year, classes are not held on Thanksgiving Day or the Friday after, and the school traditionally has an extended break covering Christmas and New Year's Day. Classes don't resume in January until the day after Martin Luther King, Jr. Day. A week-long spring break occurs after the eighth week of the spring term, which sometimes coincides with Easter.
Sources of funding.
The University of Minnesota system has one of the largest endowments among public universities in the country. As of 2007, the University of Minnesota maintained an endowment of $2.8 billion. Also, as a public university, the system received an estimated $641 million from the State of Minnesota. The system's total budget for FY 2006 was $2.36 billion.
Additional properties.
There are several other research and outreach centers across the state operated by the University of Minnesota - Twin Cities or by the university system. As of September 2004, these areas plus the campuses are spread across 28,300 acres (44 miles² or 115 km²). Other areas owned by the state and university bring this up to a total of 57,200  acres (89 miles² or 231 km²)

</doc>
<doc id="49105" url="http://en.wikipedia.org/wiki?curid=49105" title="Vacuum cleaner">
Vacuum cleaner

A vacuum cleaner is a device that uses an air pump (a centrifugal fan in all but some of the very oldest models), to create a partial vacuum to suck up dust and dirt, usually from floors, and from other surfaces as well. The dirt is collected by either a dustbag or a cyclone for later disposal. Vacuum cleaners, which are used in homes as well as in industry, exist in a variety of sizes and models—small battery-powered hand-held devices, domestic central vacuum cleaners, huge stationary industrial appliances that can handle several hundred litres of dust before being emptied, and self-propelled vacuum trucks for recovery of large spills or removal of contaminated soil.
Name.
Although "vacuum cleaner" is a neutral name, in some countries (UK, Ireland) "hoover" is used instead as a genericized trademark, after one of the first and more influential companies in the development of the device, but actually irrespective of the brand used (like kleenex for facial tissue).
History.
The vacuum cleaner evolved from the carpet sweeper via manual vacuum cleaners. The first manual models, using bellows, were developed in the 1860s, and the first motorized designs appeared at the turn of the 20th century, with the first decade being the boom decade.
Manual predecessors.
In 1860 a carpet sweeper was invented by Daniel Hess of West Union, Iowa that gathered dust with a rotating brush and a bellows for generating suction.
Another early model (1869) was the "Whirlwind", invented in Chicago in 1868 by Ives W. McGaffey. The bulky device worked with a belt driven fan cranked by hand that made it awkward to operate, although it was commercially marketed with mixed success.
A similar model was constructed by Melville R. Bissell of Grand Rapids, Michigan in 1876. The company later added portable vacuum cleaners to its line of cleaning tools.
The next improvement came in 1898, when John S. Thurman of St. Louis, Missouri, submitted a patent (US No. 634,042) for a "pneumatic carpet renovator". This was a gasoline powered cleaner although the dust was blown into a receptacle rather than being sucked in, as in the machine now used. In a newspaper advertisement from the "St. Louis Dispatch", Thurman offered his invention of the horse-drawn (which went door to door) motorized cleaning system in St. Louis. He offered cleaning services at $4 per visit. By 1906 Thurman was offering built-in central cleaning systems that used compressed air, yet featured no dust collection. In later patent litigation, Judge Augustus Hand ruled that Thurman "does not appear to have attempted to design a vacuum cleaner, or to have understood the process of vacuum cleaning".
Vacuum cleaner.
The motorized vacuum cleaner was invented by Hubert Cecil Booth of England in 1901. As Booth recalled decades later, that year he attended "a demonstration of an American machine by its inventor" at the Empire Music Hall in London. The inventor is not named, but Booth's description of the machine conforms fairly closely to Thurman's design, as modified in later patents. Booth watched a demonstration of the device, which blew dust off the chairs, and thought that "...if the system could be reversed, and a filter inserted between the suction apparatus and the outside air, whereby the dust would be retained in a receptacle, the real solution of the hygienic removal of dust would be obtained." He tested the idea by laying a handkerchief on the seat of a restaurant chair, putting his mouth to the handkerchief, and then trying to suck up as much dust as he could onto the handkerchief. Upon seeing the dust and dirt collected on the underside of the handkerchief, he realized the idea could work.
Booth created a large device, driven by an internal combustion engine. Nicknamed the "Puffing Billy", Booth's first petrol-powered, horse-drawn vacuum cleaner relied upon air drawn by a piston pump through a cloth filter. It did not contain any brushes; all the cleaning was done by suction through long tubes with nozzles on the ends. Although the machine was too bulky to be brought into the building, its principles of operation were essentially the same as the vacuum cleaners of today. He followed this up with an electric-powered model, but both designs were extremely bulky, and had to be transported by horse and carriage. The term "vacuum cleaner" was first used by the company set up to market Booth's invention, in its first issued prospectus of 1901.
Booth initially did not attempt to sell his machine, but rather sold cleaning services. The vans of the British Vacuum Cleaner Company (BVCC) were bright red; uniformed operators would haul hose off the van and route it through the windows of a building to reach all the rooms inside. Booth was harassed by complaints about the noise of his vacuum machines and was even fined for frightening horses. Gaining the royal seal of approval, Booth's motorized vacuum cleaner was used to clean the carpets of Westminster Abbey prior to Edward VII's coronation in 1901. The device was used by the Royal Navy to improve the level of sanitation in the naval barracks. It was also used in businesses such as theatres and shops, although the device was too large to be feasibly used as a domestic appliance.
Booth received his first patents on 18 February and 30 August 1901. Booth started the BVCC and refined his invention over the next several decades. Though his "Goblin" model lost out to competition from Hoover in the household vacuum market, his company successfully turned its focus to the industrial market, building ever-larger models for factories and warehouses. Booth's company, now BVC, lives on today as a unit of pneumatic tube system maker Quirepace Ltd.
The American industry was established by the New Jersey inventor David T. Kenney between 1903 and 1913. Membership in the Vacuum Cleaner Manufacturers' Association, formed in 1919, was limited to licensees under his patents.
Domestic vacuum cleaner.
The first vacuum-cleaning device to be portable and marketed at the domestic market was built in 1905 by Walter Griffiths, a manufacturer in Birmingham, England. His "Griffith's Improved Vacuum Apparatus for Removing Dust from Carpets" resembled modern-day cleaners; – it was portable, easy to store, and powered by "any one person (such as the ordinary domestic servant)", who would have the task of compressing a bellows-like contraption to suck up dust through a removable, flexible pipe, to which a variety of shaped nozzles could be attached.
In 1906 James B. Kirby developed his first of many vacuums called the "Domestic Cyclone" It used water for dirt separation. He held over 60 patents on everything from a wringerless washing machine to ironing and dry cleaning equipment.
In 1907 department store janitor James Murray Spangler (1848-1915) of Canton, Ohio invented the first portable electric vacuum cleaner, obtaining a patent for the Electric Suction Sweeper on June 2, 1908. Crucially, in addition to suction from an electric fan that blew the dirt and dust into a soap box and one of his wife's pillow cases, Spangler's design utilized a rotating brush to loosen debris. Unable to produce the design himself due to lack of funding, he sold the patent in 1908 to local leather goods manufacturer William Henry Hoover (1849-1932), who had Spangler's machine redesigned with a steel casing, casters, and attachments, founding the company that in 1922 was renamed the Hoover Company. Their first vacuum was the 1908 Model O, which sold for $60. Subsequent innovations included the beater bar in 1919 ("It beats as it sweeps as it cleans"), disposal filter bags in the 1920s, and an upright vacuum cleaner in 1926.
In Continental Europe, the Fisker and Nielsen company in Denmark was the first to sell vacuum cleaners in 1910. The design weighed just 17.5 kg and could be operated by a single person.
The Swedish company Electrolux launched the innovative Model V in 1921 that was designed to lie on the floor on two thin metal runners. This innovation, conceived by Electrolux founder Axel Wenner-Gren, became a standard feature on generations of future vacuum cleaners. There is a recorded example of a 1930s Electrolux vacuum cleaner surviving in use for over 70 years, finally breaking in 2008.
Post-Second World War.
For many years after their introduction, vacuum cleaners remained a luxury item, but after the Second World War, they became common among the middle classes. Vacuums tend to be more common in Western countries because in most other parts of the world, wall-to-wall carpeting is uncommon and homes have tile or hardwood floors, which are easily swept, wiped or mopped manually without power assist.
The last decades of the 20th century saw the more widespread use of technologies developed earlier, including filterless cyclonic dirt separation, central vacuum systems and rechargeable hand-held vacuums. In addition, miniaturized computer technology and improved batteries allowed the development of a new type of machine — the autonomous robotic vacuum cleaner. In 1997 Electrolux of Sweden demonstrated the Electrolux Trilobite, the first autonomous cordless robotic vacuum cleaner on the BBC-TV program "Tomorrow' World", introducing it to the consumer market in 2001.
Recent developments.
In 2004 a British company released Airider, a hovering vacuum cleaner that floats on a cushion of air. It has claimed to be light-weight and easier to maneuver (compared to using wheels), although it is not the first vacuum cleaner to do this — the Hoover Constellation predated it by at least 35 years.
A British inventor has developed a new cleaning technology known as Air Recycling Technology, which, instead of using a vacuum, uses an air stream to collect dust from the carpet. This technology was tested by the Market Transformation Programme (MTP) and shown to be more energy-efficient than the vacuum method. Although working prototypes exist, Air Recycling Technology is not currently used in any production cleaner.
Modern configurations.
A wide variety of technologies, designs, and configurations are available for both domestic and commercial cleaning jobs.
Upright.
Upright vacuum cleaners are popular in the United States, Britain and numerous Commonwealth countries, but very unusual in Continental Europe . They take the form of a cleaning head, onto which a handle and bag are attached. Upright designs generally employ a rotating brushroll or beater bar, which removes dirt through a combination of sweeping and vibration. There are two types of upright vacuums; dirty-air/direct fan (found mostly on commercial vacuums), or clean-air/fan-bypass (found on most of today's domestic vacuums).
The older of the two designs, direct-fan cleaners have a large impeller (fan) mounted close to the suction opening, through which the dirt passes directly, before being blown into a bag. The motor is often cooled by a separate cooling fan. Because of their large-bladed fans, and comparatively short airpaths, direct-fan cleaners create a very efficient airflow from a low amount of power, and make effective carpet cleaners. Their "above-floor" cleaning power is less efficient, since the airflow is lost when it passes through a long hose, and the fan has been optimized for airflow volume and not suction.
Fan-bypass uprights have their motor mounted after the filter bag. Dust is removed from the airstream by the bag, and usually a filter, before it passes through the fan. The fans are smaller, and are usually a combination of several moving and stationary turbines working in sequence to boost power. The motor is cooled by the airstream passing through it. Fan-bypass vacuums are good for both carpet and above-floor cleaning, since their suction does not significantly diminish over the distance of a hose, as it does in direct-fan cleaners. However, their air-paths are much less efficient, and can require more than twice as much power as direct-fan cleaners to achieve the same results.
The most common upright vacuum cleaners use a drive-belt powered by the suction motor to rotate the brush-roll. However, a more common design of dual motor upright is available. In these cleaners, the suction is provided via a large motor, while the brushroll is powered by a separate, smaller motor, which does not create any suction. The brush-roll motor can sometimes be switched off, so hard floors can be cleaned without the brush-roll scattering the dirt. It may also have an automatic cut-off feature which shuts the motor off if the brush-roll becomes jammed, protecting it from damage.
Canister.
Canister models (in the UK also often called cylinder models) dominate the European market. They have the motor and dust collector (using a bag or bagless) in a separate unit, usually mounted on wheels, which is connected to the vacuum head by a flexible hose. Their main advantage is flexibility, as the user can attach different heads for different tasks, and maneuverability (the head can reach under furniture and makes it very easy to vacuum stairs and vertical surfaces). Many cylinder models have power heads as standard or add-on equipment containing the same sort of mechanical beaters as in upright units, making them as efficient on carpets as upright models. Such beaters are driven by a separate electric motor or a turbine which uses the suction power to spin the brushroll via a drive belt.
Drum.
Drum or shop vac models are essentially heavy-duty industrial versions of cylinder vacuum cleaners, where the canister consists of a large vertically positioned drum which can be stationary or on wheels. Smaller versions, for use in garages or small workshops, are usually electrically powered. Larger models, which can store over 200 litres (53 US gallons), are often hooked up to compressed air, utilizing the Venturi effect to produce a partial vacuum. Built-in dust collection systems are also used in many workshops.
Wet/dry.
Wet or wet/dry vacuum cleaners (commonly known by the generic trademark Shop-Vac) are a specialized form of the cylinder/drum models that can be used to clean up wet or liquid spills. They are generally designed to be used both indoors and outdoors and to accommodate both wet and dry debris; some are also equipped with an exhaust port or detachable blower for reversing the airflow, a useful function for everything from clearing a clogged hose to blowing dust into a corner for easy collection.
Pneumatic.
Pneumatic or pneumatic wet/dry vacuum cleaners are a specialized form of wet/dry models that hook up to compressed air. They commonly can accommodate both wet and dry soilage, a useful feature in industrial plants and manufacturing facilities.
Backpack.
Backpack vacuum cleaners are commonly used for commercial cleaning: they allow the user to move rapidly about a large area. They are essentially small canister vacuums strapped onto the user's back.
Hand-held.
Lightweight hand-held vacuum cleaners, either powered from rechargeable batteries or mains power, are also popular for cleaning up smaller spills. Frequently seen examples include the Black & Decker DustBuster, which was introduced in 1979, and numerous handheld models by Dirt Devil, which were first introduced in 1984. Some battery-powered handheld vacuums are wet/dry rated; the appliance must be partially disassembled and cleaned after picking up wet materials, to avoid developing unpleasant odors.
Robotic.
In the late 1990s and early 2000s, several companies developed robotic vacuum cleaners, a form of carpet sweeper usually equipped with limited suction power. Some prominent brands are Roomba, Neato, and bObsweep. These machines move autonomously while collecting surface dust and debris into a dustbin. They can usually navigate around furniture and come back to a docking station to charge their batteries, and a few are able to empty their dust containers into the dock as well. Most models are equipped with motorized brushes and a vacuum motor to collect dust and debris. While most robotic vacuum cleaners are designed for home use, some models are appropriate for operation in offices, hotels, hospitals, etc.
The Dyson robotic vacuum cleaner (DC06) was too expensive for home use, due to its high technical specifications. Thus, it was never released, although it is claimed that it would have been the first robotic vacuum cleaner sold.
In December 2009, Neato Robotics launched the world's first robotic vacuum cleaner which uses a rotating laser-based range-finder (a form of lidar) to scan and map its surrounding. It uses this map to clean the floor methodically, even if it requires the robot to return to its base multiple times to recharge itself. In many cases it will notice when an area of the floor that was previously inaccessible becomes reachable, such as when a dog wakes up from a nap, and return to vacuum that area. It also has the strongest impeller among robotic vacuum cleaners, pulling in 35CFM of air.
Cyclonic.
Portable vacuum cleaners working on the cyclonic separation principle became popular in the 1990s. This dirt separation principle was well known and often used in central vacuum systems. Cleveland's P.A. Geier Company had obtained a patent on a cyclonic vacuum cleaner as early as 1928, which was later sold to Health-Mor in 1939, introducing the Filter Queen cyclonic canister vacuum cleaner.
In 1979, James Dyson introduced a portable unit with cyclonic separation, adapting this design from industrial saw mills. He launched his cyclone cleaner first in Japan in the 1980s at a cost of about US$1800 and in 1993 released the Dyson DC01 upright in the UK for £200. Critics expected that people would not buy a vacuum cleaner at twice the price of a conventional unit, but the Dyson design later became the most popular cleaner in the UK.
Cyclonic cleaners do not use filtration bags. Instead, the dust is separated in a detachable cylindrical collection vessel or bin. Air and dust are sucked at high speed into the collection vessel at a direction tangential to the vessel wall, creating a fast-spinning vortex. The dust particles and other debris move to the outside of the vessel by centrifugal force, where they fall due to gravity.
In fixed-installation central vacuum cleaners, the cleaned air may be exhausted directly outside without need for further filtration. A well-designed cyclonic filtration system loses suction power due to airflow restriction only when the collection vessel is almost full. This is in marked contrast to filter bag systems, which lose suction when pores in the filter become clogged as dirt and dust are collected.
In portable cyclonic models, the cleaned air from the center of the vortex is expelled from the machine after passing through a number of successively finer filters at the top of the container. The first filter is intended to trap particles which could damage the subsequent filters that remove fine dust particles. The filters must regularly be cleaned or replaced to ensure that the machine continues to perform efficiently.
Since Dyson's success in raising public awareness of cyclonic separation, several other companies have introduced cyclone models. Competing manufacturers include Hoover, Bissell, Shark, Eureka, Electrolux, Filter Queen, etc., and the cheapest models are no more expensive than a conventional cleaner.
Central.
Central vacuum cleaners, also known as built-in or ducted, are a type of canister/cylinder model which has the motor and dirt filtration unit located in a central location in a building, and connected by pipes to fixed vacuum inlets installed throughout the building. Only the hose and cleaning head need be carried from room to room, and the hose is commonly 8 m (25 ft) long, allowing a large range of movement without changing vacuum inlets. Plastic or metal piping connects the inlets to the central unit. The vacuum head may be unpowered, or have beaters operated by an electric motor or by an air-driven turbine.
The dirt bag or collection bin in a central vacuum system is usually so large that emptying or changing needs to be done less often, perhaps a few times per year for an ordinary household. The central unit usually stays in stand-by, and is turned on by a switch on the handle of the hose. Alternately, the unit powers up when the hose is plugged into the wall inlet, when the metal hose connector makes contact with two prongs in the wall inlet and control current is transmitted through low voltage wires to the main unit.
A central vacuum typically produces greater suction than common portable vacuum cleaners because a larger fan and more powerful motor can be used when they are not required to be portable. A cyclonic separation system, if used, does not lose suction as the collection container fills up, until the container is nearly full. This is in marked contrast to filter-bag designs, which start losing suction immediately as pores in the filter become clogged by accumulated dirt and dust.
A benefit to allergy sufferers is that unlike a standard vacuum cleaner, which must blow some of the dirt collected back into the room being cleaned (no matter how efficient its filtration), a central vacuum removes all the dirt collected to the central unit. Since this central unit is usually located outside the living area, no dust is recirculated back into the room being cleaned. Also it is possible on most newer models to vent the exhaust entirely outside, even with the unit inside the living quarters.
Another benefit of the central vacuum is, because of the remote location of the motor unit, there is much less noise in the room being cleaned than with a standard vacuum cleaner.
Constellation.
The Hoover Company marketed an unusual vacuum cleaner, called the "Constellation", in the 1960s. The cylinder type lacked wheels, and instead the vacuum cleaner floated on its exhaust, operating as a hovercraft, although this is not true of the earliest models. They had a rotating hose with the intention being that the user would place the unit in the center of the room, and work around the cleaner. Introduced in 1954, they are collectible, and are easily identified by their spherical shape. But they remain an interesting machine; restored, they work well in homes with lots of hardwood floors.
The Constellations were changed and updated over the years until discontinued in 1975. These Constellations route all of the exhaust under the vacuum using a different airfoil. The updated design is quiet even by modern standards, particularly on carpet as it muffles the sound. These models float on carpet or bare floor—although on hard flooring, the exhaust air tends to scatter any fluff or debris around.
Hoover re-released an updated version of this later model Constellation in the US (model # S3341 in Pearl White and # S3345 in stainless steel). Changes include a HEPA filtration bag, a 12-amp motor, a turbine-powered brush roll, and a redesigned version of the handle. This same model was marketed in the UK under the Maytag brand as the "Satellite" because of licensing restrictions. It was sold from 2006 to 2009.
Vehicles.
See vacuum truck for very big vacuum cleaners mounted on vehicles.
Other.
Some other vacuum cleaners include an electric mop in the same machine: for a dry and a later wet clean.
The iRobot company developed the Scooba, a robotic wet vacuum cleaner that carries its own cleaning solution, applies it and scrubs the floor, and vacuums the dirty water into a collection tank.
Technology.
A vacuum's suction is caused by a difference in air pressure. A fan driven by an electric motor (often a universal motor) reduces the pressure inside the machine. Atmospheric pressure then pushes the air through the carpet and into the nozzle, and so the dust is literally pushed into the bag.
Tests have shown that vacuuming can kill 100% of young fleas and 96% of adult fleas.
Exhaust filtration.
Vacuums by their nature cause dust to become airborne, by exhausting air that is not completely filtered. This can cause health problems since the operator ends up inhaling respirable dust, which is also redeposited into the area being cleaned. There are several methods manufacturers use to control this problem, some of which may be combined together in a single appliance. Typically a filter is positioned so that the incoming air passes through it before it reaches the motor, and then the filtered air passes through the motor for cooling purposes. Some other designs use a completely separate air intake for cooling.
It is nearly impossible for a practical air filter to completely remove all ultrafine particles from a dirt-laden airstream. An ultra-efficient air filter will immediately clog up and become ineffective during everyday use, and practical filters are a compromise between filtering effectiveness and restriction of airflow. One way to sidestep this problem is to exhaust partially filtered air to the outdoors, which is a design feature of some central vacuum systems. Specially engineered portable vacuums may also utilize this design, but are more awkward to set up and use, requiring temporary installation of a separate exhaust hose to an exterior window.
Ordinary vacuum cleaners should never be used to clean up asbestos fibers, even if fitted with a HEPA filter. Specially-designed machines are required to safely clean up asbestos.
Attachments.
Most vacuum cleaners are supplied with numerous specialized attachments, such as tools, brushes and extension wands, which allow them to reach otherwise inaccessible places or to be used for cleaning a variety of surfaces. The most common of these tools are:
Specifications.
The performance of a vacuum cleaner can be measured by several parameters:
Other specifications of a vacuum cleaner are:
Suction.
The suction is the maximum pressure difference that the pump can create. For example, a typical domestic model has a suction of about negative 20 kPa. This means that it can lower the pressure inside the hose from normal atmospheric pressure (about 100 kPa) by 20 kPa. The higher the suction rating, the more powerful the cleaner. One inch of water is equivalent to about 249 Pa; hence, the typical suction is 80 in of water.
Input power.
The power consumption of a vacuum cleaner, in watts, is often the only figure stated. Many North American vacuum manufacturers give the current only in amperes (e.g. "6 amps"), and the consumer is left to multiply that by the line voltage of 120 volts to get the approximate power ratings in watts. The rated input power does not indicate the effectiveness of the cleaner, only how much electricity it consumes.
After 1 September 2014, due to EU rules, manufacture of vacuum cleaners with a power consumption greater than 1600 watts will be banned, and from 2017 no vacuum cleaner with a wattage greater than 900 watts will be permitted.
Output power.
The amount of input power that is converted into airflow at the end of the cleaning hose is sometimes stated, and is measured in "airwatts": the measurement units are simply watts. The word "air" is used to clarify that this is output power, not input electrical power.
The airwatt is derived from English units. ASTM International defines the airwatt as 0.117354 × F × S, where F is the rate of air flow in ft3/min and S is the pressure in inches of water. This makes one airwatt equal to 0.9983 watts.
Automotive vacuums in Bayonet Point, Florida. Although the sign says "free", a paid car wash is required first.
External links.
 at DMOZ

</doc>
<doc id="49107" url="http://en.wikipedia.org/wiki?curid=49107" title="Willie Dixon">
Willie Dixon

William James "Willie" Dixon (July 1, 1915 – January 29, 1992) was an American blues musician, vocalist, songwriter, arranger and record producer. A Grammy Award winner who was proficient on both the upright bass and the guitar and as a vocalist, Dixon is perhaps best known as one of the most prolific songwriters of his time. Next to Muddy Waters, Dixon is recognized as the most influential person in shaping the post-World War II sound of the Chicago blues.
Dixon's songs have been recorded by countless musicians in many genres as well as by various ensembles in which he participated. A short list of his most famous compositions includes "Hoochie Coochie Man", "I Just Want to Make Love to You", "Little Red Rooster", "My Babe", "Spoonful", and "You Can't Judge a Book by the Cover". These tunes were written during the peak of Chess Records, 1950–1965, and performed by Muddy Waters, Howlin' Wolf, Little Walter, and Bo Diddley; they influenced a worldwide generation of musicians.
Dixon also was an important link between the blues and rock and roll, working with Chuck Berry and Bo Diddley in the late 1950s. His songs were covered by some of the biggest artists of more recent times, such as Bob Dylan, Cream, Jeff Beck, the Doors, Jimi Hendrix, Led Zeppelin, and the Rolling Stones.
Biography.
Early life.
Dixon was born in Vicksburg, Mississippi on July 1, 1915. His mother Daisy often rhymed the things she said, a habit her son imitated. At the age of seven, young Dixon became an admirer of a band that featured pianist Little Brother Montgomery. Dixon was first introduced to blues when he served time on prison farms in Mississippi as an early teenager. He later learned how to sing harmony from local carpenter Leo Phelps. Dixon sang bass in Phelps' group The Jubilee Singers, a local gospel quartet that regularly appeared on the Vicksburg radio station WQBC. Dixon began adapting poems he was writing as songs, and even sold some tunes to local music groups.
Adulthood.
Dixon left Mississippi for Chicago in 1936. A man of considerable stature, at 6 and a half feet and weighing over 250 pounds, he took up boxing; he was so successful that he won the Illinois State Golden Gloves Heavyweight Championship (Novice Division) in 1937. Dixon turned professional as a boxer and worked briefly as Joe Louis' sparring partner. After four fights, Dixon left boxing after getting into a fight with his manager over being cheated out of money.
Dixon met Leonard Caston at the boxing gym where they would harmonize at times. Dixon performed in several vocal groups in Chicago but it was Caston that got him to pursue music seriously. Caston built him his first bass, made of a tin can and one string. Dixon's experience singing bass made the instrument familiar. He also learned the guitar.
In 1939, Dixon was a founding member of the Five Breezes, with Caston, Joe Bell, Gene Gilmore and Willie Hawthorne. The group blended blues, jazz, and vocal harmonies, in the mode of the Ink Spots. Dixon's progress on the Upright bass came to an abrupt halt during the advent of World War II when he resisted the draft as a conscientious objector and was imprisoned for ten months. After the war, he formed a group named the Four Jumps of Jive and then reunited with Caston, forming the Big Three Trio, who went on to record for Columbia Records.
Pinnacle of career.
Dixon signed with Chess Records as a recording artist, but began performing less, being more involved with administrative tasks for the label. By 1951, he was a full-time employee at Chess, where he acted as producer, talent scout, session musician and staff songwriter. He was also a producer for Chess subsidiary Checker Records. His relationship with Chess was sometimes strained, although he stayed with the label from 1948 to the early 1960s. During this time Dixon's output and influence were prodigious. From late 1956 to early 1959, he worked in a similar capacity for Cobra Records, where he produced early singles for Otis Rush, Magic Sam, and Buddy Guy. He later recorded on Bluesville Records. From the late 1960s until the middle 1970s, Dixon ran his own record label, Yambo Records, along with two subsidiary labels, Supreme and Spoonful. He released his 1971 album "Peace?" on Yambo, as well as singles by McKinley Mitchell, Lucky Peterson and others.
Dixon is considered one of the key figures in the creation of Chicago blues. He worked with Chuck Berry, Muddy Waters, Howlin' Wolf, Otis Rush, Bo Diddley, Joe Louis Walker, Little Walter, Sonny Boy Williamson, Koko Taylor, Little Milton, Eddie Boyd, Jimmy Witherspoon, Lowell Fulson, Willie Mabon, Memphis Slim, Washboard Sam, Jimmy Rogers, Sam Lay and others.
In December 1964, The Rolling Stones reached No. 1 in the UK Singles Chart with their cover version of Dixon's "Little Red Rooster".
Copyright battles.
In his later years, Willie Dixon became a tireless ambassador for the blues and a vocal advocate for its practitioners, founding the Blues Heaven Foundation. The organization works to preserve the blues' legacy and to secure copyrights and royalties for blues musicians who were exploited in the past. Speaking with the simple eloquence that was a hallmark of his songs, Dixon claimed, "The blues are the roots and the other musics are the fruits. It's better keeping the roots alive, because it means better fruits from now on. The blues are the roots of all American music. As long as American music survives, so will the blues."
In 1977, unhappy with the royalties rate from ARC Music, he and Muddy Waters sued the Chess-owned publishing company, and with the proceeds from the lawsuit set up Hoochie Coochie Music.
In 1987, Dixon received an out-of-court settlement from Led Zeppelin after suing them for plagiarism, in relation to their use of his music for "Bring It On Home" and his lyrics from his composition "You Need Love" (1962) for their track "Whole Lotta Love".
Dixon's health deteriorated increasingly during the seventies and the eighties, primarily due to long-term diabetes. Eventually one of his legs had to be amputated. Dixon was inducted at the inaugural session of the Blues Foundation's ceremony, and into the Blues Hall of Fame in 1980. In 1989 he was also the recipient of a Grammy Award for his album, "Hidden Charms".
Death and legacy.
Dixon died of heart failure in Burbank, California on January 29, 1992, and was buried in the Burr Oak Cemetery in Alsip, Illinois. Dixon was posthumously inducted into the Rock and Roll Hall of Fame in the "early influences" (pre-rock) category in 1994. On April 28, 2013, Dixon's grandson, Alex Dixon, was inducted into the Chicago Blues Hall of Fame along with his grandfather.
Actor and comedian Cedric the Entertainer portrayed Dixon in "Cadillac Records", a 2008 film based on the early history of Chess Records.
Discography.
As sideman.
With Sam Lazar

</doc>
<doc id="49109" url="http://en.wikipedia.org/wiki?curid=49109" title="Gustav I of Sweden">
Gustav I of Sweden

Gustav I, born Gustav Eriksson of the Vasa noble family and later known as Gustav Vasa (12 May 1496 – 29 September 1560), was King of Sweden from 1523 until his 1560 death, previously self-recognised Protector of the Realm ("Rikshövitsman") from 1521, during the ongoing Swedish War of Liberation against King Christian II of Denmark, Norway and Sweden. Initially of low standing, Gustav rose to lead the rebel movement following the Stockholm Bloodbath, in which his father perished. Gustav's election as King on 6 June 1523 and his triumphant entry into Stockholm eleven days later meant the end of Medieval Sweden's elective monarchy and the Kalmar Union, and the birth of a hereditary monarchy under the House of Vasa and its successors, including the current House of Bernadotte.
As King, Gustav proved an enigmatic administrator with a ruthless streak not inferior to his predecessor's, brutally suppressing subsequent uprisings (three in Dalarna – which had once been the first region to support his claim to the throne - one in Västergötland, and one in Småland). He worked to raise taxes, end Feudalism and bring about a Swedish Reformation, replacing the prerogatives of local landowners, noblemen and clergy with centrally appointed governors and bishops. His 37-year rule, which was the longest of a mature Swedish king to that date (subsequently passed by Gustav V and Carl XVI Gustav) saw a complete break with not only the Danish supremacy but also the Roman Catholic Church, whose assets were nationalised, with the Lutheran Church of Sweden established under his personal control. He became the first truly autocratic native Swedish sovereign and was a skilled propagandist and bureaucrat, with his main opponent, Christian's, infamous mark as the "tyrant king" and his largely fictitious adventures during the liberation struggle still widespread to date. Due to a vibrant dynastic succession, his three sons, Erik, Johan and Karl IX, all held the kingship at different points.
Gustav I has subsequently been labelled the founder of modern Sweden, and the "father of the nation". Gustav liked to compare himself to Moses, whom he believed to have also liberated his people and established a sovereign state. As a person, Gustav was known for ruthless methods and a bad temper, but also a fondness for music and had a certain sly wit and ability to outmaneuver and annihilate his opponents. He founded one of the now oldest orchestras of the world, the "Kungliga Hovkapellet" (Royal Court Orchestra). Royal housekeeping accounts from 1526 mention twelve musicians including wind players and a timpanist but no string players. Today the "Kungliga Hovkapellet" is the orchestra of the Royal Swedish Opera.
Early life.
Gustav Eriksson, a son of Cecilia Månsdotter Eka and Erik Johansson Vasa, was probably born in 1496. The birth most likely took place in Rydboholm Castle, northeast of Stockholm, the manor house of the father, Erik. The newborn got his name, Gustav, from Erik's grandfather Gustav Anundsson.
Erik Johansson's parents were Johan Kristersson and Birgitta Gustafsdotter of the dynasties Vasa and Sture respectively, both dynasties of high nobility. Birgitta Gustafsdotter was the sister of Sten Sture the Elder, regent of Sweden. Being a relative and ally of uncle Sten Sture, Erik inherited the regent's estates in Uppland and Södermanland when the latter died in 1503. Although a member of a family with considerable properties since childhood, Gustav Eriksson would later be the holder of possessions of a much greater dimension.
According to genealogical research, Birgitta Gustafsdotter and Sten Sture (and consequently also Gustav Vasa) were descended from King Sverker II of Sweden, through King Sverker's granddaughter Benedikte Sunesdotter (who was married to Svantepolk Knutsson, son of Duke of Reval). One of King Gustav's great-grandmothers was a half-sister of King Charles VIII of Sweden.
Resistance against the Danish.
Supporting the Sture party.
Since the end of the 14th century, Sweden had been a part of the Kalmar Union with Denmark and Norway. The Danish dominance in this union occasionally led to uprisings in Sweden. During Gustav's childhood, parts of the Swedish nobility tried to make Sweden independent. Gustav and his father Erik supported the party of Sten Sture the Younger, regent of Sweden from 1512, and its struggle against the Danish King Christian II. Following the battle of Brännkyrka in 1518, where Sten Sture's troops beat the Danish forces, it was decided that Sten Sture and King Christian would meet in Österhaninge for
negotiations. To guarantee the safety of the king, the Swedish side sent six men as hostages to be kept by the Danes for as long as the negotiations lasted. However, Christian did not show up for the negotiations, violated the deal with the Swedish side and took the hostages aboard ships carrying them to Copenhagen. The six members of the kidnapped hostage were Hemming Gadh, Lars Siggesson (Sparre), Jöran Siggesson (Sparre), Olof Ryning, Bengt Nilsson (Färla) – and Gustav Eriksson. Gustav was held in Kalø slot where he was treated very well after promising he would not make attempts to escape. A reason for this gentle treatment was King Christian's hope to convince the six men to switch sides, and turn against their leader Sten Sture. This strategy was successful regarding all men but Gustav, who stayed loyal to the Sture party.
In 1519, Gustav Eriksson escaped from Kalø. He fled to the Hanseatic city of Lübeck where he arrived on 30 September. How he managed to escape is not certain, but according to a somewhat likely story, he disguised himself as a bullocky. For this, Gustav Eriksson got the nicknames "King Oxtail" and "Gustav Cow Butt", something he indeed disliked. When a swordsman drank to His Majesty "Gustav Cow Butt" in Kalmar in 1547, the swordsman was killed.
While staying in Lübeck, Gustav could hear about developments in his native Sweden. While he was there, Christian II mobilised to attack Sweden in an effort to seize power from Sten Sture and his supporters. In 1520, the forces of King Christian were triumphant. Sten Sture died in March, but some strongholds, including the Swedish capital Stockholm, were still able to withstand the Danish forces. Gustav left Lübeck on a ship, and was put ashore south of Kalmar on 31 May.
It seems Gustav stayed largely inactive during his first months back on Swedish soil. According to some sources, Gustav received an invitation to the coronation of Christian. This was to take place in the newly captured Stockholm in November. Even though King Christian had promised amnesty to his enemies within the Sture party, including Gustav Eriksson, the latter chose to decline the invitation. The coronation took place on 4 November and days of festivities in a friendly spirit followed. When the celebration had lasted a few days, the castle was locked and the former enemies of King Christian were imprisoned. Accusations against the old supporters of Sten Sture regarding heresy were brought forward. The following day the sentences were announced. During the Stockholm Bloodbath, close to 100 people were executed on Stortorget, among them Gustav Eriksson's father, Erik Johansson, and nephew, Joakim Brahe. Gustav himself was at the time staying at Räfsnäs, close to Gripsholm Castle.
In Dalarna.
Gustav Eriksson had reasons to fear for his life and left Räfsnäs. He travelled to the province of Dalarna, in what was then northwestern Sweden. What happened there has been described in Peder Svart's chronicle, which can be described as a strongly biased heroic tale about Gustav Eriksson. The Dalarna adventures of Gustav that could be described as a part of the national heritage of Sweden, can therefore not be verified in a satisfying way. He is supposed to have tried to gather troops among the peasantry in the province, but with little success initially. Being chased by men loyal to king Christian and failing at creating an army to challenge the king, Gustav Eriksson had no other alternative but to flee to Norway. While he made his way from Mora via Lima to Norway, people that had recently turned down Gustav's call for support against the king changed their minds. Representatives of that group caught up with Gustav before he had reached Norway and convinced him to follow them back to Mora. Gustav Eriksson's run towards Norway and back has formed the background to the famous cross-country ski race Vasaloppet.
Swedish War of Liberation.
Gustav Eriksson was appointed "hövitsman". The rebel force he led grew. In February 1520 it consisted of 400 men, mainly from the area around Lake Siljan. The first big clash in the Dissolution of Kalmar Union that now started, took place at Brunnbäck's Ferry in April, where the rebels defeated an army loyal to the king. Neither in this battle nor in later battles during the war, did Gustav take part in leading the military operations. The rebel insurgents had further successes. Other parts of Sweden, for example the Götaland provinces of Småland and Västergötland, also saw rebellions. The leading noblemen of Götaland joined Gustav Eriksson's forces and, in Vadstena in August, they declared Gustav regent of Sweden.
The election of Gustav Eriksson as a regent made many Swedish nobles, who had so far stayed loyal to King Christian, switch sides. Some noblemen, still loyal to the king, chose to leave Sweden, while others were killed. As a result, the Swedish Privy Council lost old members who were replaced by supporters of Gustav Eriksson. Most fortified cities and castles were conquered by Gustav's rebels, but the strongholds with the best defences, including Stockholm, were still under Danish control. In 1522, after negotiations between Gustav Eriksson's people and Lübeck, the Hanseatic city joined the war against Denmark. The winter of 1523 saw the joint forces attack the Danish and Norwegian areas of Scania, Halland, Blekinge and Bohuslän. During this winter, Christian II was overthrown and replaced by Frederick I. The new king openly claimed the Swedish throne and had hopes Lübeck would abandon the Swedish rebels. The German city, preferring an independent Sweden to a strong Kalmar Union dominated by Denmark, took advantage of the situation and put pressure on the rebels. The city wanted privileges on future trade as well as guarantees regarding the loans they had granted the rebels. The Privy Council and Gustav Eriksson knew the support from Lübeck was absolutely crucial. As a response, the council decided to appoint Gustav Eriksson king.
Election as king.
The ceremonial election of the regent Gustav Eriksson as king of Sweden took place when the leading men of Sweden got together in Strängnäs in June 1523. 
When the councillors of Sweden had chosen Gustav as king, he met with the two visiting councillors of Lübeck. The German representatives supported the appointment without hesitation and declared it an act of God. Gustav stated he had to bow to what was described as the will of God. In a meeting with the Privy Council, Gustav Eriksson announced his decision to accept. In the following ceremony, led by the deacon of Strängnäs, Laurentius Andreae, Gustav swore the royal oath. The next day, bishops and priests joined Gustav in Roggeborgen where Laurentius Andreae raised the holy sacrament above a kneeling Gustav Eriksson. Flanked by the councillors of Lübeck, Gustav Eriksson was brought to Strängnäs Cathedral where the king sat down in the choir with the Swedish privy councillors on one side, and the Lübeck representatives on the other. After the hymn "Te Deum", Laurentius Andreae proclaimed Gustav Eriksson king of Sweden. He was, however, still not crowned. In 1983, in remembrance of the election of Gustav as Swedish king on 6 June, that date was declared the National Day of Sweden.
The capture of Stockholm.
Shortly after the events of 1523 in Strängnäs, a letter patent was issued to Lübeck and its allied Hanseatic cities, who now were freed from tolls when trading in Sweden. An agreement, designed by Lübeck negotiators, was made with the Danish defenders in Stockholm. On 17 June the rebels could enter the capital city. At Midsummer, a grand entrance of king Gustav was arranged at Söderport, the southern gate of Stockholm. Celebrations followed, including a mass of thanksgiving in Storkyrkan (also known as Stockholm Cathedral) led by Peder Jakobsson. Gustav could now install himself in the Tre Kronor palace.
The war ends.
Bailiffs, loyal to the old king Christian, were still holding castles in Finland, a part of Sweden at the time. During the summer and fall of 1523 they all surrendered. The next year, on 24 August 1524, Gustav arrived in Malmö in order to reach a settlement with Denmark and its king Frederick. The Treaty of Malmö (in Swedish: "Malmö recess") had both positive and negative sides to it, from king Gustav's perspective. The treaty meant that Denmark-Norway acknowledged the independence of Sweden. The hopes Gustav had carried of winning further provinces (Gotland and Blekinge) were however scuttled. The treaty marked the end of the Swedish War of Liberation.
The Reformation.
After Gustav seized power, the previous Archbishop, Gustav Trolle, who at the time held the post of a sort of chancellor, was exiled from the country. Gustav sent a message to Pope Clement VII requesting the acceptance of a new archbishop selected by Gustav himself: Johannes Magnus.
The Pope sent back his decision demanding that unlawful expulsion of Archbishop Gustav Trolle be rescinded, and that the archbishop be reinstated. Here Sweden's remote geographical location proved to have a marked impact – for the former Archbishop had been allied with the Danish king, or at least was considered to have been so allied in contemporary Stockholm, and to reinstate him would be close to impossible for Gustav.
The king let the Pope know the impossibility of the request, and the possible results if the Pope persisted, but – for better or worse – the Pope did persist, and refused to accept the king's suggestions of archbishops. At the time, incidentally and for different reasons, there were also four other unoccupied bishop's seats, where the king made suggestions to the Pope about candidates, but the Pope only accepted one of the candidates. Because the Pope refused to budge on the issue of Gustav Trolle, the king, influenced by Lutheran scholar Olaus Petri, in 1531 took it upon himself to appoint yet another archbishop, namely the brother of Olaus, Laurentius Petri. With this royal act, the Pope lost any influence over the Swedish Church.
In the 1520s, the Petri brothers led a campaign for the introduction of Lutheranism. The decade saw many events which can be seen as gradual introductions of Protestantism, for instance the marriage of Olaus Petri – a consecrated priest – and several texts published by him, advocating Lutheran dogmas. A translation of the New Testament had also been published in 1526. After the reformation, a full translation was published in 1540–41, called the Gustav Vasa Bible. However, knowledge of Greek and Hebrew among Swedish clergymen was not sufficient for a translation from the original sources; instead the work followed the German translation by Martin Luther in 1534.
Gustav I's breaking with the Catholic Church is virtually simultaneous with Henry VIII doing the same in England; both kings acted following a similar pattern, i.e., a prolonged confrontation with the Pope culminating with the king deciding to take his own decisions independently of Rome.
Further reign.
Gustav encountered resistance from some areas of the country. People from Dalarna rebelled three times in the first ten years of Gustav's reign, as they considered the king to have been too harsh on everyone he perceived as a supporter of the Danish, and as they resented his introduction of Protestantism. Many of those who had helped Gustav in his war against the Danes became involved in these rebellions and paid for this, several of them with their lives.
Peasants in Småland rebelled in 1542, fuelled by grievances over taxes, church reform and confiscation of church bells and vestments. For several months this uprising caused Gustav severe difficulties in the dense forests. The king sent a letter to the people of the province of Dalarna, requesting that they should circulate letters to every Swedish province, stating their support for the king with their troops, and urging every other province to do the same. Gustav got his troops, with whose help – and, not least, with paid German mercenaries – he managed to defeat the rebels in the spring of 1543.
The leader of the rebels, Nils Dacke, has traditionally been seen as a traitor to Sweden. His own letters and proclamations to fellow peasants focused on the suppression of Roman Catholic customs of piety, the King's requisitions of church bells and church plate to be smelted down for money and the general discontent with Gustav's autocratic measures, and the King's letters indicate that Dacke had considerable military success for several months. Historical records state that Nils was seriously wounded during a battle, taking bullet wounds to both legs; if this is true, his survival may have been surprising in view of contemporary medical techniques. Some sources state that Nils was executed by quartering; others that he was reduced to the state of an outlaw after recovering from his wounds, and killed while trying to escape through the woods on the border between Småland and then Danish Blekinge. It is said that his body parts were displayed throughout Sweden as a warning to other would-be rebels; this is uncertain though his head was likely mounted on a pole at Kalmar. Modern Swedish scholarship has toned down criticism of Nils Dacke, sometimes making him into a hero in the vein of Robin Hood, particularly in Småland.
Difficulties with the continuation of the Church also troubled Gustav Vasa. The 1540s saw him imposing death sentences upon both the Petri brothers, as well as his former chancellor Laurentius Andreae. All of them were however granted amnesty, after spending several months in jail. In 1554–1557, he waged an inconclusive war against Ivan the Terrible of Russia.
End of reign.
In the late 1550s, Gustav's health declined. When his grave was opened in 1945, an examination of his corpse revealed that he had suffered chronic infections of a leg and in his jaw.
He gave a so-called "last speech" in 1560 to the chancellors, his children and other noblemen, whereby he encouraged them to remain united. On 29 September 1560, Gustav died and was buried (together with three of his wives, while only two are engraved) in the Cathedral of Uppsala.
Heritage.
Gustav's heritage has been disputed. In 19th-century Swedish history a folklore developed wherein Gustav was supposed to have had all kinds of adventures when he liberated Sweden from the Danes. The memory of Gustav has been honored greatly, resulting in embroidered history books, commemorative coins, and the annual ski event Vasaloppet (the largest ski event in the world with 15,000 participants). The city of Vaasa in Finland was named after the royal house of Vasa in 1606. Gustav is currently portrayed on the 1000 kronor note. Today most of these stories are considered to have no other foundation than legend and skilful propaganda by Gustav himself during his time.
Here is an example of one of his better known adventures among the Swedish people. While Gustav was in exile from the Danish army, he was staying at a farm owned by a close friend for a day's rest. As he was warming himself in the common room, the Danish soldiers got a tip from one of the farm hands that Gustav was in his landlord's farm house. The Danish soldiers burst into the farm house and began searching in the common room for someone that would fit Gustav's description. As one of the soldiers came close to check Gustav Vasa, all of a sudden the landlady took out a bakery spade and started to hit Gustav and scolded him as a "lazy farmboy" and ordered him to go out and work. The Danish soldier found it amusing and did not realise this "lazy farmboy" was in fact Gustav Vasa himself who managed to slip away from danger and escape death. There are many other stories about Gustav's close encounters with death, however it is questionable if any of his adventures really did happen or were dramatised by Gustav himself; regardless of whether they happened or not, his adventures are still told to this day in Sweden.
Gustav has been regarded by some as a power-hungry man who wished to control everything: the Church, the economy, the army and all foreign affairs. But in doing this, he also did manage to unite Sweden, a country that previously had no standardised language, and where individual provinces held a strong regional power. He also laid the foundation for Sweden's professional army that was to make Sweden into a regional superpower in the 17th century.
18th century references in Britain.
In 1739, the English playwright Henry Brooke wrote the play "Gustavus Vasa", dealing with the liberation of Sweden from Danish rule. However, Robert Walpole, British Prime Minister at the time, believed that the play's villain was intended to represent him, and had the play banned – the first English play to be so banned under the Licensing Act 1737.
Later in the 18th century, the name "Gustavus Vasa" was given to Olaudah Equiano, a prominent African ex-slave living in Britain and involved in the struggle to abolish slavery.
Gallery.
Gustav Vasa had a series of paintings made during his reign. The originals are lost but watercolor reproductions of unknown date remain. These paintings show Gustav's triumphs, showing what Gustav himself considered important to depict.
Family.
Gustav's first wife was Catherine of Saxe-Lauenburg (1513–1535), whom he married on 24 September 1531. They had a son:
On 1 October 1536, he married his second wife, Margareta Leijonhufvud (1514–1551). Their children were:
At Vadstena Castle on 22 August 1552 he married his third wife, Katarina Stenbock (1535–1621).

</doc>
<doc id="49110" url="http://en.wikipedia.org/wiki?curid=49110" title="List of cities in Sardinia">
List of cities in Sardinia

Sardinian towns
Most important towns of Sardinia.
With "local names".
Metropolitan areas.
"Including Quartu Sant'Elena, Selargius, Assemini, Capoterra, Monserrato, Sestu, Sinnai, Quartucciu, Maracalagonis and Elmas.
"Including Alghero, Castelsardo, Porto Torres, Sennori, Sorso and Stintino."

</doc>
<doc id="49112" url="http://en.wikipedia.org/wiki?curid=49112" title="Potsdam">
Potsdam

Potsdam (]), is the capital city of the German federal state of Brandenburg. It directly borders the German capital Berlin and is part of the Berlin/Brandenburg Metropolitan Region. It is situated on the River Havel, 24 km southwest of Berlin's city center.
Potsdam was a residence of the Prussian kings and the German Kaiser, until 1918. Around the city there are a series of interconnected lakes and cultural landmarks, in particular the parks and palaces of Sanssouci, the largest World Heritage Site in Germany. The Potsdam Conference, the major post-World War II conference between the victorious Allies, was held at another palace in the area, the Cecilienhof.
Babelsberg, in the south-eastern part of Potsdam, was a major film production studio before the war and has enjoyed success as a major center of European film production since the fall of the Berlin Wall. The Filmstudio Babelsberg is the oldest large-scale film studio in the world.
Potsdam developed into a centre of science in Germany from the 19th century. Today, there are three public colleges, the University of Potsdam and more than 30 research institutes in the city.
Etymology.
The name "Potsdam" originally seems to have been "Poztupimi". A common theory is that it derives from an old West Slavonic term meaning "beneath the oaks", i.e., the corrupted "pod dubmi/dubimi" ("pod" "beneath, "dub" "oak"). However some question this explanation.
Geography.
The area was formed from a series of large moraines left after the last glacial period. Today, the city is three-quarters green space, with just a quarter as urban area. There are about 20 lakes and rivers in and around Potsdam, such as the Havel, the Griebnitzsee, Templiner See, Tiefer See, Jungfernsee, Teltowkanal, Heiliger See and the Sacrower See. The highest point is the 114 m high "Kleiner Ravensberg".
Potsdam is divided into seven historic city districts and nine new "Ortsteile" (villages), which joined the city in 2003. The appearances of the city districts are quite different. The districts in the north and in the centre consist mainly of historical buildings, the south of the city is dominated by larger areas of newer buildings.
Climate.
Potsdam has a humid continental climate with cold and snowy winters and warm summers. Winters are cold with a high of 0°C and a low of -5°C. Snow is common in the winter. Spring and fall are short. Summers are quite warm with a high of 24°C and a low of 14°C. Since Potsdam is not as built up as Berlin, it doesn't have the microclimate effect that Berlin has. Winters are a couple degrees colder than in Berlin and Potsdam has more snow then Berlin because Berlin is usually a couple degrees warmer than surrounding areas due to its microclimate effect.
Demographics.
Largest groups of foreign residents as of 2012:
History.
The area around Potsdam shows occupancy since the Bronze Age and was part of "Magna Germania" as described by Tacitus. After the great migrations of the Germanic peoples, Slavs moved in and Potsdam was probably founded after the 7th century as a settlement of the Hevelli tribe centred on a castle. It was first mentioned in a document in 993 AD as "Poztupimi", when Emperor Otto III gifted the territory to the Quedlinburg Abbey, then led by his aunt Matilda. By 1317, it was mentioned as a small town. It gained its town charter in 1345. In 1573, it was still a small market town of 2,000 inhabitants. Potsdam lost nearly half of its population due to the Thirty Years' War (1618–1648).
A continuous Hohenzollern possession since 1415, Potsdam became prominent, when it was chosen in 1660 as the hunting residence of Frederick William I, Elector of Brandenburg, the core of the powerful state that later became the Kingdom of Prussia. It also housed Prussian barracks.
After the Edict of Potsdam in 1685, Potsdam became a centre of European immigration. Its religious freedom attracted people from France (Huguenots), Russia, the Netherlands and Bohemia. The edict accelerated population growth and economic recovery.
Later, the city became a full residence of the Prussian royal family. The buildings of the royal residences were built mainly during the reign of Frederick the Great. One of these is the Sanssouci Palace (French: "without cares", by Georg Wenzeslaus von Knobelsdorff, 1744), famed for its formal gardens and Rococo interiors. Other royal residences include the New Palace and the Orangery.
In 1815, at the formation of the Province of Brandenburg, Potsdam became the provincial capital until 1918, however, interrupted and succeeded by Berlin as provincial capital between 1827 and 1843, and after 1918. The province comprised two governorates named after their capitals Potsdam and Frankfurt (Oder).
Governorate of Potsdam.
Between 1815 and 1945 the city of Potsdam served as capital of the governorate of Potsdam (German: ""). The "Regierungsbezirk" encompassed the former districts of Uckermark, the Mark of Priegnitz, and the greater part of the Middle March. It was situated between Mecklenburg and the Province of Pomerania on the north, and the Province of Saxony on the south and west (Berlin, with a small surrounding district, was an urban governorate and enclave within the governorate of Potsdam between 1815 and 1822, then it merged as urban district into the governorate only to be disentangled again from Potsdam governorate in 1875, becoming an own distinct province-like entity on 1 April 1881). Towards the north west the governorate was bounded by the rivers Elbe and the Havel, and on the north east by the Oder. The southeastern boundary was to the neighbouring governorate of Frankfurt (Oder). About 500,000 inhabitants lived in the Potsdam governorate, which covered an area of about 20700 km2, divided into thirteen rural districts, partially named after their capitals:
The traditional towns in the governorate were small, however, in the course of the industrial labour migration some reached the rank as urban districts. The principal towns were Brandenburg upon Havel, Köpenick, Potsdam, Prenzlau, Spandau and Ruppin. Until 1875 also Berlin was a town within the governorate. After its disentanglement a number of its suburbs outside Berlin's municipal borders grew to cities, many forming urban within the governorate of Potsdam such as Charlottenburg, Lichtenberg, Rixdorf (after 1912 Neukölln), and Schöneberg (all of which, as well as Köpenick and Spandau, incorporated into Greater Berlin in 1920). The urban districts were (years indicating the elevation to rank of urban district or affiliation with Potsdam governorate, respectively):
Potsdam in the 20th century.
Berlin was the official capital of Prussia and later of the German Empire, but the court remained in Potsdam, where many government officials settled. In 1914, the Emperor Wilhelm II signed the Declaration of War in the Neues Palais. The city lost its status as a second capital in 1918, when Wilhelm II abdicated at the end of World War I.
At the start of the Third Reich in 1933 there was a ceremonial handshake between President Paul von Hindenburg and the new Chancellor Adolf Hitler on 21 March 1933 in Potsdam's Garrison Church in what became known as the "Day of Potsdam". This symbolised a coalition of the military ("Reichswehr") and Nazism. Potsdam was severely damaged in bombing raids during World War II.
The Cecilienhof Palace was the scene of the Potsdam Conference from 17 July, to 2 August 1945, at which the victorious Allied leaders (Harry S. Truman; Winston Churchill and his successor, Clement Attlee; and Joseph Stalin) met to decide the future of Germany and postwar Europe in general. The conference ended with the Potsdam Agreement and the Potsdam Declaration.
The government of East Germany (formally known as the German Democratic Republic (German: Deutsche Demokratische Republik, DDR)) tried to remove symbols of Prussian militarism. Many historic buildings, some of them badly damaged in the war, were demolished.
When in 1946 the remainder of the Province of Brandenburg west of the Oder-Neiße line was constituted as the state of Brandenburg, Potsdam became its capital. In 1952 the GDR disestablished its federative states and replaced them by smaller new East German administrative districts. Potsdam became the capital of the new Bezirk Potsdam until 1990.
Potsdam, south-west of Berlin, lay just outside West Berlin after the construction of the Berlin Wall. The walling off of West Berlin not only isolated Potsdam from West Berlin, but also doubled commuting times to East Berlin. The Glienicke Bridge across the Havel connected the city to West Berlin and was the scene of some Cold War exchanges of spies.
After German reunification, Potsdam became the capital of the newly re-established state of Brandenburg. There are many ideas and efforts to reconstruct the original appearance of the city, most remarkably the Potsdam City Palace and the Garrison Church.
Politics.
Administration.
Potsdam has had a mayor ("Bürgermeister") and city council since the 15th century. From 1809 the city council was elected, with a mayor ("Oberbürgermeister") at its head. During the Third Reich the mayor was selected by the NSDAP and the city council was dissolved; it was reconstituted in token form after the Second World War, but free elections did not take place until after reunification.
Today, the city council is the city's central administrative authority. Local elections took place on 26 October 2003 and again in 2008. Between 1990 and 1999, the Chairman of the City Council was known as the "Town President" but today the post is the "Chairman of the City Council". The mayor is elected directly by the population. In the mayoral election on 22 September 2002, no candidate gained an overall majority, and a run-off election was held between Jann Jakobs (SPD) and Hans-Jürgen Scharfenberg (PDS), with Jann Jakobs gaining the narrowest of victories, with 50.1%.
The Landtag Brandenburg, the parliament of the federal state of Brandenburg is in Potsdam. It is planned to move into the Potsdam City Palace in 2014, after its reconstruction.
International relations.
Twin towns — sister cities.
Potsdam is twinned with the following cities:
Education and research.
Potsdam is a university town. The University of Potsdam was founded in 1991 as a university of the State of Brandenburg. Its predecessor was the "Akademie für Staats- und Rechtswissenschaften der DDR "Walter Ulbricht"", a college of education founded in 1948 which was one of the GDR's most important colleges. There are about 20,000 students enrolled at the university.
In 1991 the Fachhochschule was founded as the second college; it now has 2,400 students.
In addition there is a College of Film and Television ("Hochschule für Film und Fernsehen "Konrad Wolf" HFF"), founded in 1954 in Babelsberg, the foremost centre of the German film industry since its birth, with currently 600 students.
There are also several research foundations, including Fraunhofer Institutes for and , Max Planck Institute for Gravitational Physics ("Albert Einstein Institute"), Max Planck Institute of , and Max Planck Institute for , the , the , the and the , which employs 340 people in researching climate change.
As well as universities, Potsdam is home to reputable secondary schools. Montessori Gesamtschule Potsdam, in western Potsdam, attracts 400 students from the Brandenburg and Berlin region.
Main sights.
Potsdam was historically a centre of European immigration. Its religious tolerance attracted people from France, Russia, the Netherlands and Bohemia. This is still visible in the culture and architecture of the city.
The most popular attraction in Potsdam is Sanssouci Park, 2 km west of the city centre. In 1744 King Frederick the Great ordered the construction of a residence here, where he could live "sans souci" ("without worries", in the French spoken at the court). The park hosts a botanical garden (Botanical Garden, Potsdam) and many buildings:
Three gates from the original city wall remain today. The oldest is the Hunters' Gate ("Jägertor"), built in 1733. The Nauener Tor was built in 1755 and close to the historic Dutch Quarter. The ornate Brandenburg Gate (built in 1770, not to be confused with the Brandenburg Gate in Berlin) is situated on the Luisenplatz at the western entrance to the old town.
The Old Market Square ("Alter Markt") is Potsdam's historical city centre. For three centuries this was the site of the City Palace ("Stadtschloß"), a royal palace built in 1662. Under Frederick the Great, the palace became the winter residence of the Prussian kings. The palace was severely damaged by bombing in 1945 and demolished in 1961 by the Communist authorities. In 2002 the Fortuna Gate ("Fortunaportal") was rebuilt in its original historic position, which marks the first step in the reconstruction of the palace.
The Old Market Square is dominated today by the dome of St. Nicholas' Church ("Nikolaikirche"), built in 1837 in the classical style. It was the last work of Karl Friedrich Schinkel, who designed the building but did not live to see its completion. It was finished by his disciples Friedrich August Stüler and Ludwig Persius. The eastern side of the Market Square is dominated by the Old City Hall ("Altes Rathaus"), built in 1755 by the Dutch architect Jan Bouman (1706–1776). It has a characteristic circular tower, crowned with a gilded Atlas bearing the world on his shoulders.
North of the Old Market Square is the oval French Church ("Französische Kirche"), erected in the 1750s by Boumann for the Huguenot community.
Another landmark of Potsdam is the two-street Dutch Quarter ("Holländisches Viertel"), an ensemble of buildings that is unique in Europe, with about 150 houses built of red bricks in the Dutch style. It was built between 1734 and 1742 under the direction of Jan Bouman to be used by Dutch artisans and craftsmen who had been invited to settle here by King Frederick Wilhelm I. Today, this area is one of Potsdam's most visited districts.
North of the city centre is the Russian colony of Alexandrowka, a small enclave of Russian architecture (including an Orthodox chapel) built in 1825 for a group of Russian immigrants. Since 1999, the colony has been a UNESCO World Heritage Site.
East of the Alexandrowka colony is a large park, the New Garden ("Neuer Garten"), which was laid out from 1786 in the English style. The site contains two palaces; one of them, the Cecilienhof, was where the Potsdam Conference was held in July and August 1945. The "Marmorpalais" (Marble Palace) was built in 1789 in the style of classicism. Nearby is the Biosphäre Potsdam, a tropical botanical garden.
Another district of Potsdam is Babelsberg, a quarter south-east of the centre, housing the UFA film studios (Babelsberg Studios), and an extensive park with some historical buildings, including the Babelsberg Palace (Schloß Babelsberg, a neo-Gothic palace designed by Schinkel).
The Einstein Tower is located within the Albert Einstein Science Park, which is on the top of the "Telegraphenberg" within an astronomy compound.
There are many parks in Potsdam, most of them included in UNESCO World Heritage Sites. Some of them are:
Potsdam also includes a memorial centre in the former KGB prison in Leistikowstraße.

</doc>
<doc id="49114" url="http://en.wikipedia.org/wiki?curid=49114" title="Rouen">
Rouen

Rouen (]) is a city on the River Seine in the north of France. It is the capital of the region of Upper Normandy and the historic capital city of Normandy. One of the largest and most prosperous cities of medieval Europe, it was the seat of the Exchequer of Normandy in the Middle Ages. It was one of the capitals of the Anglo-Norman dynasties, which ruled both England and large parts of modern France from the 11th to the 15th centuries. It was here that Joan of Arc was executed in 1431. People from Rouen are called "Rouennais".
The population of the metropolitan area (in French: "agglomération") at the 1999 census was 518,316, and 532,559 at the 2007 estimate. The city proper had an estimated population of 110,276 in 2007.
Administration.
Haute-Normandie (Upper Normandy) "région", as well as a commune and the "préfecture" (capital) of the Seine-Maritime "département".
Rouen and 70 suburban communes of the metropolitan area form the Agglomeration community of Rouen-Elbeuf-Austreberthe (CREA), with 494,382 inhabitants at the 2010 census. In descending order of population, the largest of these suburbs are Sotteville-lès-Rouen, Saint-Étienne-du-Rouvray, Le Grand-Quevilly, Le Petit-Quevilly, and Mont-Saint-Aignan, each with a population exceeding 20,000.
History.
Unknown to Julius Caesar, Rouen was founded by the Gaulish tribe of the Veliocasses, who controlled a large area in the lower Seine valley, which retains a trace of their name as the Vexin. They called it "Ratumacos"; the Romans called it "Rotomagus". Roman Rotomagus was the second city of Gallia Lugdunensis after Lugdunum (Lyon) itself. Under the reorganization of the empire by Diocletian, Rouen became the chief city of the divided province of Gallia Lugdunensis II and reached the apogee of its Roman development, with an amphitheatre and "thermae" of which the foundations remain. In the 5th century, it became the seat of a bishopric and later a capital of Merovingian Neustria.
The Middle Ages.
From their first incursion into the lower valley of the Seine in 841, the Vikings overran Rouen until some of them finally settled and founded a colony led by Rollo (Hrolfr), who was nominated count of Rouen by the king of the Franks in 911. In the 10th century Rouen became the capital of the Duchy of Normandy and residence of the dukes, until William the Conqueror established his castle at Caen.
In the early 1100s the city's population reached the size of 30,000. In 1150, Rouen received its founding charter, which permitted self-government. During the 12th century, Rouen was probably the site of a Jewish yeshiva. At that time, about 6,000 Jews lived in the town, comprising about 20% of the total population. In addition, there were a large number of Jews scattered about another 100 communities in Normandy. The well-preserved remains of a medieval Jewish building, that could be a yeshiva, were discovered in the 1970s under the Rouen Law Courts.
In 1200, a fire destroyed part of the old Romanesque cathedral, leaving St Romain's tower, the side porches of the front, and part of the nave.
New work on the present Gothic cathedral of Rouen was begun, in the nave, transept, choir, and the lowest section of the lantern tower. On 24 June 1204, Philip II Augustus of France entered Rouen and annexed Normandy to the French Kingdom. The fall of Rouen meant the end of Normandy's sovereign status. He demolished the Norman castle and replaced it with his own, the Château Bouvreuil, built on the site of the Gallo-Roman amphitheatre.
A textile industry developed based on wool imported from England, for which the northern County of Flanders and Duchy of Brabant were constantly fierce but worthy competitors, and finding its market in the Champagne fairs. Rouen also depended for its prosperity on the river traffic of the Seine, on which it enjoyed a monopoly that reached as far upstream as Paris. Wine and wheat were exported to England, with tin and wool received in return.
In the 14th century urban strife threatened the city: in 1291, the mayor was assassinated and noble residences in the city were pillaged. Philip IV reimposed order and suppressed the city's charter and the lucrative monopoly on river traffic, but he was quite willing to allow the Rouennais to repurchase their former liberties in 1294. In 1306, he decided to expel the Jewish community of Rouen, which then numbered some five or six thousand in the city of 40,000 people.
In 1389, another urban revolt of the underclass broke out, the "Harelle". It was part of a widespread rebellion in France that year and was suppressed with the withdrawal of Rouen's charter and river-traffic privileges once more.
During the Hundred Years' War, on 19 January 1419, Rouen and its population of 70,000 surrendered to Henry V of England, who annexed Normandy once again to the Plantagenet domains. But Rouen did not go quietly: Alain Blanchard hung English prisoners from the walls, for which he was summarily executed; the Canon and Vicar General of Rouen, Robert de Livet, became a hero for excommunicating the English king, resulting shortly after in de Livet's himself imprisonment for five years in England.
Rouen became the capital city of English power in occupied France and when the Duke of Bedford, John of Lancaster bought Joan of Arc from his ally, the Duke of Burgundy who had been keeping her in jail since May 1430, she was sent to be tried in this city on Christmas 1430. After a long trial by a church court, sentenced to be burned at the stake. The sentence was carried out on 30 May 1431 in this city, where most inhabitants supported the Duke of Burgundy, Joan of Arc's royal enemy.
The king of France Charles VII recaptured the town in 1449, 18 years after the death of Joan of Arc and after 30 years of English occupation.
In that same year the young Henry VI was crowned king of England and France in Paris before coming to Rouen where he was acclaimed by the crowds.
The Renaissance Period.
The naval dockyards, where activity had been slowed down by the 100 years war, developed again as did the church of Saint-Maclou which had been started under the English occupation, and was finally finished during the Renaissance period. The nave of the church of Saint Ouen was completed at last, but without the façade flanked by twin towers. The salle des pas-perdus (a sort of waiting room or ante-room) of the present law courts was built during this time. The whole building was built in a flamboyant style into which the first decorative elements typical of the Renaissance style right at the beginning of the 16th century had been incorporated.
At that time Rouen was the most populous city in the realm after Paris, Marseille and Lyon. Rouen was also one of the Norman cradles of the artistic Renaissance, in particular the one under the patronage of the archbishops and financiers of the town.
The economic upturn of the town at the end of the 15th century was mainly due to the cloth industry, but also to the development of the silk industry and metallurgy. The fishermen of Rouen went as far afield as the Baltic to fish for herrings. Salt was imported from Portugal and Guérande. Cloth was sold in Spain which also provided wool, and the Medici family made Rouen into the main port for the resale of Roman alum.
At the beginning of the 16th century Rouen became the main French port through which trade was conducted with Brasil, principally for the import of cloth dyes. By 1500 ten printing presses had been installed in the town following the installation of the first one sixteen years earlier.
The Wars of Religion.
In the years following 1530, part of the population of Rouen embraced Calvinism. The members of the Reformed Church who represented a quarter to a third of the total population, a significant part but still a minority.
In 1550, King Henri II staged a triumphant entry into Rouen, modeled on the ancient Roman triumph and specifically compared to Pompey's third triumph of 61 BC at Rome: "No less pleasing and delectable than the third triumph of Pompey... magnificent in riches and abounding in the spoils of foreign nations". It was not enough, however, to long sustain royal authority in the city.
From 1560 onwards tensions rose between the Protestant and Catholic communities, when the Massacre of Vassy set off the first of the French Wars of Religion. On 15 April 1562 the Protestants entered the town hall and ejected the King's personal representative. In May there was an outbreak of Iconoclasm (statue smashing). On 10 May the Catholic members of the town council fled Rouen. The Catholics captured, however, the Fort of Saint Catherine which overlooked the town. Both sides resorted to terror tactics.
At this juncture the Protestant town authorities requested help from Queen Elizabeth I of England. In accordance with the Hampton Court Treaty which they had signed with Condé on 20 September 1562, the English sent troops to support the Protestants, and these occupied Le Havre. On 26 October 1562 French Royalist troops retook Rouen and pillaged it for three days.
The news of the Massacre of St. Bartholomew's Day reached Rouen at the end of August 1572. Hennequier tried to avoid a massacre of the Protestants by shutting them up in various prisons. But between 17 and 20 September the crowds forced the gates of the prisons and murdered the Protestants that they found inside.
The town was attacked on several occasions by Henry IV, but it resisted, notably during the siege of December 1591 to May 1592, with the help of a Spanish army led by the Duke of Parma (see Siege of Rouen (1591)).
The Classical Age.
The permanent exchequer of Normandy, which had been installed in Rouen in 1499 by George of Amboise, was transformed into a regional administrative assembly by Francis I in 1515 and up to the time of the Revolution was the administrative centre of the region. It had judicial, legislative and executive powers in Norman affairs and was only subordinate to the Privy Council. It also had power to govern French Canada.
The 16th and the 18th centuries brought prosperity to the town through the textile trade and the increased use of the port facilities. In 1703 the Norman Chamber of Commerce was formed.
Although it did not have a university, Rouen became an important intellectual centre by reason of its reputed schools of higher learning. In 1734, a school of surgery (second only to that of Paris founded in 1724) was founded. In 1758 a new hospital was opened to the West of the town which replaced the old medieval one which had grown too small, and which had been situated on the south side of the cathedral.
The Modern Period.
During the Franco-Prussian War of 1870, Rouen was occupied by the Prussians.
During the First World War the British used Rouen as a supply base and there were many military hospitals.
The city was heavily damaged during World War II - approximately 45% was destroyed. In June 1940 the area between the Notre-Dame Cathedral and the Seine river burned for 48 hours because the Germans did not allow firemen access to the fire. Other areas were destroyed between March and August 1944 just before and during the Battle of Normandy, which ended on the left bank of the Seine with the destruction of several regiments belonging to the German 7th Army. Rouen's cathedral and several significant monuments were damaged by Allied bombing. During the German occupation, Nazi Germany's "Kriegsmarine" had its headquarters located in a château on what is now the Rouen Business School (École Supérieure de Commerce de Rouen). The city was liberated by the Canadians on 30 August 1944 after the breakout from Normandy.
Main sights.
Rouen is known for its Notre Dame cathedral, with its "Tour de Beurre" ("butter tower") financed by the sale of indulgences for the consumption of butter during Lent. The cathedral's gothic façade (completed in the 1500s) was the subject of a series of paintings by Claude Monet, some of which are exhibited in the Musée d'Orsay in Paris.
The "Gros Horloge" is an astronomical clock dating back to the 16th century , though the movement is considerably older (1389). It is located in the "Gros Horloge" street.
Other famous structures include Rouen Castle, whose keep is known as the "Tour Jeanne d'Arc", where Joan of Arc was brought in 1431 to be threatened with torture (contrary to popular belief, she was not imprisoned there but in the since destroyed "tour de la Pucelle"); the "Church of Saint Ouen" (12th–15th century); the "Palais de Justice", which was once the seat of the "Parlement" (French court of law) of Normandy; the Gothic "Church of St Maclou" (15th century); and the Museum of Fine Arts and Ceramics which contains a splendid collection of faïence and porcelain for which Rouen was renowned during the 16th to 18th centuries. Rouen is also noted for its surviving half-timbered buildings.
There are many museums in Rouen: Musée des Beaux-Arts de Rouen, an art museum with pictures of well-known painters such as Claude Monet and Géricault; Musée maritime fluvial et portuaire, a museum on the history of the port of Rouen and navigation; Musée des antiquités, an art and history museum with local works from the Bronze Age through the Renaissance; Musée de la céramique, Musée Le Secq des Tournelles...
The Jardin des Plantes de Rouen is a notable botanical garden once owned by Scottish banker John Law and dating to 1840 in its present form. It was the site of Élisa Garnerin's parachute jump from a balloon in 1817.
In the centre of the Place du Vieux Marché (the site of Joan of Arc's pyre) is the modern church of Saint Joan of Arc. This is a large, modern structure which dominates the square. The form of the building represents an upturned viking boat and fish shape.
Rouen was also home to the French Grand Prix, hosting the race at the nearby Rouen-Les-Essarts track sporadically between 1952 and 1968. In 1999 Rouen authorities demolished the grandstands and other remnants of Rouen's racing past. Today, little remains beyond the public roads that formed the circuit.
Transport.
Mainline trains operate from Gare de Rouen-Rive-Droite to Le Havre and Paris, and regional trains to Caen, Dieppe and other local destinations in Normandy. Daily direct trains operate to Amiens and Lille, and direct TGVs (high-speed trains) connect daily with Lyon and Marseille.
City transportation in Rouen consists of a tram and a bus system. The tramway branches into two lines out of a tunnel under the city centre. Rouen is also served by TEOR (Transport Est-Ouest Rouennais) and by buses run in conjunction with the tramway by TCAR (Transports en commun de l'agglomération rouennaise), a subsidiary of Veolia Transport.
Rouen has its own airport, serving major domestic destinations as well as international destinations in Europe.
The Seine is a major axis for maritime cargo links in the Port of Rouen. The Cross-Channel ferry ports of Caen, Le Havre, Dieppe (50 minutes) and Calais, and the Channel Tunnel are within easy driving distance (two and a half hours or less).
Education.
The University of Rouen, the École Supérieure de Commerce de Rouen (Rouen Business School), (agronomy and agriculture) – all centred or located at nearby Mont-Saint-Aignan, and INSA Rouen, and CESI – both at nearby Saint-Étienne-du-Rouvray are schools of higher education located in the Rouen area.
Performing arts.
The main opera company in Rouen is the Opéra de Rouen - Haute-Normandie. The company performs in the Théâtre des Arts, 7 rue du Docteur Rambert. The company presents opera, classical and other types of music, both vocal and instrumental, as well as dance performances.
L'Armada, one of the biggest events in Rouen.
Every 5 years, millions of visitors come to visit Rouen for the prestigious event 'L'Armada'. Along the quays of the port of Rouen, the finest and largest sailing ships, modern warships and many other outstanding ships (around 50) come from all around the world and sail up the river 'La Seine' towards Rouen. Visitors can also enjoy concerts, fireworks and plenty other entertainments, especially at night time.
Notable people.
Rouen was the birthplace of:
International relations.
Twin towns – Sister cities.
Rouen is twinned with:
In fiction and popular culture.
Fine art.
Rouen Cathedral is the subject of a series of paintings by the Impressionist painter Claude Monet, who painted the same scene at different times of the day. Two paintings are in the National Gallery of Art in Washington, D.C.; two are in the Pushkin Museum of Fine Arts in Moscow; one is in the National Museum of Serbia in Belgrade. The estimated value of one painting is over $40 million.
Sculpture.
During the second half of the 20th century, several sculptures by Jean-Yves Lechevallier were erected in the city.
Literature.
The Rouen area is an integral part of the work of French writer Annie Ernaux.
Film.
The 2000 film "The Taste of Others" was filmed and set in Rouen. In the 2001 movie "A Knight's Tale", the protagonist William Thatcher (played by Heath Ledger) poses as a noble and competes in his first jousting tournament at Rouen.
The 1952 film "The Snows of Kilimanjaro" references the memoirs of Harry Street titled "The Road to Rouen" in the scene with Harry and Uncle Bill.

</doc>
<doc id="49115" url="http://en.wikipedia.org/wiki?curid=49115" title="Solothurn">
Solothurn

The city of Solothurn (German: "Solothurn" ]; French: "Soleure"; Italian: "Soletta"; Romansh: "Soloturn") is the capital of the Canton of Solothurn in Switzerland. The city also is the only municipality of the district of the same name.
The official language of Solothurn is (the Swiss variety of Standard) German, but the main spoken language is the local variant of the Alemannic Swiss German dialect.
History.
Pre-Roman settlement.
The oldest finds from Solothurn city probably date from the Paleolithic era. The remains of a Mesolithic camp were discovered in 1986 during renovations of the former " Kino Elite" building. From the Neolithic, Bronze and Iron Age, only a few scattered items have been discovered.
Roman settlement.
The Roman settlement at Solothurn was probably built around AD 15-25 as a road station and bridge head on the road from Aventicum to Augusta Raurica or Vindonissa. A small vicus or settlement quickly developed around the castrum. Solothurn is first mentioned in 219 as "vico salod[uro]" on the so-called Eponastein. The name may indicate either that a Celtic settlement existed on the site before or just be a testimony to the mixed Gallo-Roman culture in the north-west provinces of the Roman Empire. It came to be known as "Salodurum". Its strategical importance lay in the position at the approach to the Rhine from southeast. In the 2nd-3rd Century AD, the vicus expanded rapidly to fill almost all of what is now the old city of Solothurn, including a portion of today's suburb south of the Aare.
The Roman bridge was probably somewhat above the current Wengibrücke. The Roman era river bed was 40 - north of the present Aare river. The main street of the Vicus was well below the present main street. In addition to the normal government of the settlement, there were two mayors (magistri), and a six-member college (seviri Augustales), which was entrusted with supporting the imperial cult. Salodurum was also home to a guard detachment of the XXII Legion, whose high command was stationed in Mainz in Germany. According to inscriptions, there was a temple of Jupiter, a temple of Apollo Augustus and an altar to the goddess of horses Epona, who was popular in the Roman military and of Celtic origin. However, the locations of those three temples is not known. There was bath house on the main street and a pottery district in the northwest of the town which have been documented archaeologically. A cemetery with urns and cremation burials on the eastern end of the Vicus was discovered in 1762-63 during the demolition of the old church of St. Ursus. In addition, two Roman tombs were discovered in the same area.
Around 325-350, the unfortified settlement along the road was transformed into a fortified camp or castrum, which covered only half of the former settlement area. A 2 - thick and 9 m high wall was built around the settlement. The new, fortified town was bell-shaped, and is still visible in the cadastral map of the city. At various points in the city, large and small pieces of the old Roman wall are still visible in the houses of the old town. The location of a gate in the north and a tower in the south-east corner are known and it is likely that there were additional gates and towers. Almost nothing is known about the buildings inside the walls.
Early Middle Ages.
In the Early Middle Ages there were two settlement centres, a secular settlement in the former castrum and a religious settlement on the grounds of the late-Roman cemeteries outside the walls. Both the religious histories and archeological discoveries indicate that both areas remained inhabited continuously into the Early Middle Ages. The former chapel of St. Stephen inside the castrum was built on the foundation of an earlier, late-Roman building. A burial memorial in the cemetery of the nearby St. Peter's Chapel dates to around the collapse of the Roman Empire. By the middle of the 5th Century, St. Eucherius of Lyon mentions the martyrdom of St. Ursus and St. Victor and a cult of saints in Solothurn. About 500 AD, the Burgundian Princess Sedeleuba took the bones of St. Victor to Geneva, while the bones of St. Ursus remained in Solothurn. The church dedicated to the veneration of Saint Ursus is first mentioned in 870.
Medieval city.
During the Early Middle Ages, Solothurn was part of the Kingdom of Lotharingia (Lorraine). After the collapse of Lotharingia, it became part of the Second Kingdom of Burgundy. In 1033, the Kingdom of Burgundy became part of the Holy Roman Empire and Solothurn gained some independence. In 1038, Emperor Conrad II held court at Solothurn and there crowned his son, Henry III King of Burgundy. The royal court resided in Solothurn on several occasions until 1052, however, there is no evidence of a permanent royal palace. In 1127, it was acquired by the dukes of Zähringen. Under the rule of the Zähringens, in 1146, Solothurn's coins are first mentioned. In 1182, "causidicus" or Zähringen appointed judges first appeared in Solothurn. After the extinction of the Zähringer line in 1218 it became a free imperial city under the Holy Roman Emperor. In 1252, the town council and Schultheiss or mayor became mostly independent and had their own town seals. In 1251 it was mentioned as "saluerre" and in 1275 as "Solotren". Starting around 1200, there was a council of nobles in the town.
In 1252, a group of nobles that could witness and support deeds, known as "consuls et cives Solodorenses", first appears in the town. Initially the nobles exercised power over the entire town. However, the guild movement of the 14th Century resulted in a reduction in the power of the nobles and also a restricted guild system in Solothurn. By around 1350, an eleven-member "Altrat" (Council of Elders) and a 22-member "Jungrat" (Younger Council) existed in the city. Each of the eleven guilds were represented by a member of the "Altrat" and two members of the "Jungrat". These 33 councillors exercised, together with the mayor, the power of government and helped appoint law makers. The members of the two councils were elected each year by the citizens of the city, after which the councils and mayor appointed many of the government officials. The noble families retained some power as the guilds became part of the town council. However in 1459 the last noble family died out and positions on the council fell to wealthy farmers, butchers and millers.
Until the pogrom on 1348 during an outbreak of the plague, there was a small Jewish community in Solothurn.
Over the 13th to 15th centuries, the citizens of the city slowly emancipated themselves from the higher nobility. In 1276 and 1280 Emperor Rudolf I codified the previously poorly defined rights of the city and granted it the privilege "de non Evocando" or the right that their citizens were protected from trial in foreign courts. In 1344 Solothurn acquired the right to appoint their own Schultheiss from the Count of Buchegg, which was confirmed by Emperor Charles IV in 1360. In 1409, Emperor Ruprecht extended the "de non Evocando" privilege to include the royal High Court as well.
As the city grew in power, it bound the Monastery of St. Ursus more closely to the city. In 1251 the city defeated claims made by the Monastery on the right to appoint the Schultheiss. Shortly after the acquisition of the right to the Schultheiss office in 1344, the city came into possession of the vogt right over the Monastery by granting citizenship rights to the former vogt (bailiff), Burkhard Senn the Elder. In 1512-20 the city received the right to appoint canons and provosts from the Pope.
After the alliance with Bern in 1295, it became part of the Swiss Confederation. In 1382 the Habsburgs attacked the city, involving Solothurn in the Battle of Sempach. By the treaty of two years later, the Habsburgs renounced all claims to the territory of the city. The latter was expanded by acquisition of neighbouring lands in the 15th century, roughly up to the today's canton area.
In 1481, it obtained full membership in the Swiss Confederation.
From 1530 to 1792 it was the seat of the French ambassador to Switzerland.
Buildings in the medieval city.
Before 1200 there was a Zähringer fortified tower north of the Monastery of St. Ursus. In the first half of the 13th Century, a city wall was built around the area of the former castrum as well as the adjoining industrial area to the east and the churches of St. Peter and St. Urs. Near the Monastery of St. Ursus, a Franciscan monastery was built, and after 1280 it formed the northern city wall on the eastern part of the city. In 1532, the French embassy with a church and stately home was built in the eastern half of the city. In the western part of Solothurn, the town hall was built. First it was along the main street and in 1476 it moved south of the Franciscan monastery. A main market place grew up along the main street, and in the first half of the 17th century it moved to the northern banks of the Aare river. The town hall, market place and clock tower formed the political and economic centre of city life.
Early Modern Solothurn.
The medieval cooperative election of the mayor and councillors led to the creation of a nearly hereditary oligarchy by the 15th Century. By the second half of the 16th Century, the political voice of citizens was nearly totally suppressed. By the second half of the 17th Century, the government was run by a small group of patricians. The oligarchs were weakened in the 18th Century, when in 1718-21 the city council managed to regain some powers. However, in 1682, a new citizenship law prevented wealthy families who had moved into Solothurn from becoming members of the council. While this law reduced the number of people who could be on the city council, the introduction of a secret ballot procedure in 1764 and measures against vote-buying in 1774 allowed more and more non-patrician burghers to join the council.
During the heyday of the patricians in the 17th and 18th Centuries, a number of elegant town houses (Reinert House 1692-93, Palais Besenval 1703-06) and summer residences outside the city (Sommerhaus Vigier 1648-50, Waldegg Castle 1682-86, Steinbrugg Castle 1665-68 and Blumenstein Castle 1725-28) were built. A number of new public buildings were also added including; the Arsenal (1610–19), the town hall with its north staircase tower (1632–34) and its eastern façade (Archive tower 1624, completed 1703-14), the Jesuit church (1680–89), the new Ambassadorenhof (1717–24), the Holy Spirit Hospital in a suburb (1735–1800) and the new classicist Church of St. Ursus (1763–90). In the 16th Century the town walls were reinforced with the Basel gate and three round towers.
Between 1667-1727, following plans by Francesco Polatta, Jacques Le Prestre Tarade and Sébastien de Vauban, the city built fortifications with eleven full and half bastions. The new city wall increased the size of the city by including the eastern suburb of Kreuzacker. Until the 18th Century, prisoners were housed in the towers of the medieval and early modern fortifications store. Between 1753-61 a new prison was built outside the city walls, which remained in use into the 20th Century. A gallows was first mentioned in 1460 and was located northeast of the city near Feldbrunnen. A second gallows was located to the southwest of the city.
The Early Modern Period in Solothurn ended, as in the rest of Switzerland, with the French invasion in 1798.
Modern Solothurn.
Following the capitulation of Solothurn on 2 March 1798, the French General Balthazar Alexis Henri Schauenburg set up a provisional government on the following day. The new government met in April to set up the new constitution. The eleven old "Vogtei" (baillywicks) were replaced by five districts: Solothurn, Biberist, Balsthal, Olten, and Dornach. The municipal Bürgergemeinde laid claim to the assets of the defunct city-state and in 1801 it received the "Sönderungsconvention", large estates and extensive forest land outside the city. In 1831 the cantonal parliament withdrew all political power from the eleven city guilds. Over the following years (1831–1842) all the guilds were dissolved. Due to the municipal law of 1859, the enforcement of the Federal Constitution of 1874 and the Cantonal Constitution of 1875, an "Einwohnergemeinde" was created. The "Einwohnergemeinde" included all residents of the city, as opposed to the more limited "Bürgergemeinde". The division of property between residents and the Bürgergemeinde proved to be lengthy and could not be completed until 1978 and then only with the help of the Executive Council.
On 15 October 1817, Tadeusz Kościuszko, the national hero of Poland and United States, died in Solothurn and was initially interred in the local cemetery.
In 1828 Solothurn became the seat of the Bishop of Basel.
Since 1897, the municipal council has been elected by proportional voting and consists of 30 members and 15 alternate members. As the executive body, it elects the council commission (seven members). Mayor and Vice-Mayor are elected by the people. The municipal assembly is the legislative body. The composition of the council remained remarkably stable between 1917-73. The Liberals held an average of 60% of the seats, the Social Democrats and the Conservative People's Party (CVP today), about 20% each. In 1970, the municipality granted voting rights for women. With the emergence of new parties, the Liberals lost its dominant position. 2009, the FDP 30%, SP 23%, CVP 23%, the Greens 17% and 7% of the votes go to the SVP.
Rock band Krokus was formed in Solothurn in 1974.
Geography.
Solothurn has an area, as of 2009[ [update]], of 6.28 km2. Of this area, 1.42 km2 or 22.6% is used for agricultural purposes, while 0.17 km2 or 2.7% is forested. Of the rest of the land, 4.37 km2 or 69.6% is settled (buildings or roads), 0.33 km2 or 5.3% is either rivers or lakes.
Of the built up area, industrial buildings made up 4.1% of the total area while housing and buildings made up 38.5% and transportation infrastructure made up 17.5%. Power and water infrastructure as well as other special developed areas made up 1.9% of the area while parks, green belts and sports fields made up 7.5%. Out of the forested land, 0.5% of the total land area is heavily forested and 2.2% is covered with orchards or small clusters of trees. Of the agricultural land, 14.2% is used for growing crops and 7.0% is pastures, while 1.4% is used for orchards or vine crops. All the water in the municipality is flowing water.
Solothurn is located in the north-west of Switzerland on the banks of the Aare and on the foot of the Weissenstein Jura mountains.
The municipalities of Biberist, Derendingen, Luterbach, Bellach, Langendorf and Solothurn are considering a merger at a date in the future into the new municipality of with an, as of 2011[ [update]], undetermined name.
Coat of arms.
The blazon of the municipal coat of arms is "Per fess Gules and Argent."
Demographics.
Solothurn has a population (as of December 2013[ [update]]) of . s of 2008[ [update]], 21.1% of the population are resident foreign nationals. Over the last 10 years (1999–2009 ) the population has changed at a rate of 4.4%.
Most of the population (as of 2000[ [update]]) speaks German (13,270 or 85.7%), with Italian being second most common (469 or 3.0%) and Albanian being third (261 or 1.7%). There are 193 people who speak French and 19 people who speak Romansh.
s of 2008[ [update]], the gender distribution of the population was 48.1% male and 51.9% female. The population was made up of 5,891 Swiss men (37.0% of the population) and 1,775 (11.1%) non-Swiss men. There were 6,669 Swiss women (41.8%) and 1,604 (10.1%) non-Swiss women. Of the population in the municipality 3,864 or about 24.9% were born in Solothurn and lived there in 2000. There were 3,630 or 23.4% who were born in the same canton, while 4,135 or 26.7% were born somewhere else in Switzerland, and 3,193 or 20.6% were born outside of Switzerland.
In 2008[ [update]] there were 115 live births to Swiss citizens and 27 births to non-Swiss citizens, and in same time span there were 190 deaths of Swiss citizens and 10 non-Swiss citizen deaths. Ignoring immigration and emigration, the population of Swiss citizens decreased by 75 while the foreign population increased by 17. There were 8 Swiss men and 13 Swiss women who immigrated back to Switzerland. At the same time, there were 91 non-Swiss men and 78 non-Swiss women who immigrated from another country to Switzerland. The total Swiss population change in 2008 (from all sources, including moves across municipal borders) was an increase of 98 and the non-Swiss population increased by 161 people. This represents a population growth rate of 1.7%.
The age distribution, as of 2000[ [update]], in Solothurn is; 913 children or 5.9% of the population are between 0 and 6 years old and 2,013 teenagers or 13.0% are between 7 and 19. Of the adult population, 888 people or 5.7% of the population are between 20 and 24 years old. 4,832 people or 31.2% are between 25 and 44, and 3,678 people or 23.7% are between 45 and 64. The senior population distribution is 2,068 people or 13.4% of the population are between 65 and 79 years old and there are 1,097 people or 7.1% who are over 80.
s of 2000[ [update]], there were 6,784 people who were single and never married in the municipality. There were 6,403 married individuals, 1,144 widows or widowers and 1,158 individuals who are divorced.
s of 2000[ [update]], there were 7,447 private households in the municipality, and an average of 1.9 persons per household. There were 3,468 households that consist of only one person and 303 households with five or more people. Out of a total of 7,625 households that answered this question, 45.5% were households made up of just one person and there were 49 adults who lived with their parents. Of the rest of the households, there are 1,907 married couples without children, 1,455 married couples with children There were 405 single parents with a child or children. There were 163 households that were made up of unrelated people and 178 households that were made up of some sort of institution or another collective housing.
In 2000[ [update]] there were 1,311 single family homes (or 44.3% of the total) out of a total of 2,957 inhabited buildings. There were 838 multi-family buildings (28.3%), along with 441 multi-purpose buildings that were mostly used for housing (14.9%) and 367 other use buildings (commercial or industrial) that also had some housing (12.4%). Of the single family homes 161 were built before 1919, while 62 were built between 1990 and 2000. The greatest number of single family homes (443) were built between 1919 and 1945.
In 2000[ [update]] there were 8,586 apartments in the municipality. The most common apartment size was 3 rooms of which there were 2,954. There were 728 single room apartments and 1,634 apartments with five or more rooms. Of these apartments, a total of 7,272 apartments (84.7% of the total) were permanently occupied, while 794 apartments (9.2%) were seasonally occupied and 520 apartments (6.1%) were empty. s of 2009[ [update]], the construction rate of new housing units was 3.3 new units per 1000 residents. s of 2003[ [update]] the average price to rent an average apartment in Solothurn was 980.18 Swiss francs (CHF) per month (US$780, £440, €630 approx. exchange rate from 2003). The average rate for a one room apartment was 568.85 CHF (US$460, £260, €360), a two room apartment was about 725.13 CHF (US$580, £330, €460), a three room apartment was about 904.51 CHF (US$720, £410, €580) and a six or more room apartment cost an average of 1564.78 CHF (US$1250, £700, €1000). The average apartment price in Solothurn was 87.8% of the national average of 1116 CHF. The vacancy rate for the municipality, in 2010[ [update]], was 0.45%.
Historic Population.
The historical population is given in the following chart:
Main sights.
The old town was built between 1530 and 1792 and shows an architectural combination of Italian Grandezza, French style and Swiss ideas.
In 1980, Solothurn was awarded the Wakker Prize for the development and preservation of its architectural heritage.
Solothurn is home to 18 structures that are listed as Swiss heritage sites of national significance. The religious buildings on the list are; the Visitation Convent, the Jesuit Church with "Kollegium" (Lapidarium), the Swiss Reformed Church on Westringstrasse and the St. Ursen Cathedral. There are four civic buildings on the list; the old Armory which is now the Cantonal Museum, the Rathaus (town council house), the State Archives at Bielstrasse 41 and the nearby Central Library at Bielstrasse 39. Two other museums are on the list, the Art Museum and the Naturmuseum. There are two houses and two public objects on the list; the Haller-Haus (former Bishops Palace) at Baselstrasse 61, the Sommerhaus Vigier at Untere Steingrubenstrasse 21, the Mauritius Fountain and the city clock tower. Two castles are listed; the former Blumenstein Castle and Steinbrugg Castle. Finally, the list includes the old city of Salodurum which was a Roman era Vicus and the medieval and early modern city as well as the city walls. The entire old city of Solothurn is part of the Inventory of Swiss Heritage Sites.
Sights include:
Politics.
In the 2007 federal election the most popular party was the SP which received 24.09% of the vote. The next three most popular parties were the FDP (23.53%), the Green Party (18.56%) and the CVP (17.19%). In the federal election, a total of 5,767 votes were cast, and the voter turnout was 53.8%.
Economy.
s of 2010[ [update]], Solothurn had an unemployment rate of 4.6%. s of 2008[ [update]], there were 22 people employed in the primary economic sector and about 4 businesses involved in this sector. 2,587 people were employed in the secondary sector and there were 178 businesses in this sector. 14,381 people were employed in the tertiary sector, with 1,226 businesses in this sector. There were 8,023 residents of the municipality who were employed in some capacity, of which females made up 46.9% of the workforce.
In 2008[ [update]] the total number of full-time equivalent jobs was 13,378. The number of jobs in the primary sector was 16, of which 7 were in agriculture and 9 were in forestry or lumber production. The number of jobs in the secondary sector was 2,430 of which 1,398 or (57.5%) were in manufacturing and 813 (33.5%) were in construction. The number of jobs in the tertiary sector was 10,932. In the tertiary sector; 1,537 or 14.1% were in wholesale or retail sales or the repair of motor vehicles, 454 or 4.2% were in the movement and storage of goods, 610 or 5.6% were in a hotel or restaurant, 583 or 5.3% were in the information industry, 975 or 8.9% were the insurance or financial industry, 1,095 or 10.0% were technical professionals or scientists, 614 or 5.6% were in education and 2,612 or 23.9% were in health care.
In 2000[ [update]], there were 13,529 workers who commuted into the municipality and 3,598 workers who commuted away. The municipality is a net importer of workers, with about 3.8 workers entering the municipality for every one leaving. Of the working population, 20.1% used public transportation to get to work, and 40.3% used a private car.
Religion.
From the 2000 census[ [update]], 5,463 or 35.3% were Roman Catholic, while 4,358 or 28.1% belonged to the Swiss Reformed Church. Of the rest of the population, there were 278 members of an Orthodox church (or about 1.79% of the population), there were 182 individuals (or about 1.18% of the population) who belonged to the Christian Catholic Church, and there were 248 individuals (or about 1.60% of the population) who belonged to another Christian church. There were 27 individuals (or about 0.17% of the population) who were Jewish, and 915 (or about 5.91% of the population) who were Islamic. There were 78 individuals who were Buddhist, 173 individuals who were Hindu and 27 individuals who belonged to another church. 3,139 (or about 20.27% of the population) belonged to no church, are agnostic or atheist, and 601 individuals (or about 3.88% of the population) did not answer the question.
Education.
In Solothurn about 5,724 or (37.0%) of the population have completed non-mandatory upper secondary education, and 2,815 or (18.2%) have completed additional higher education (either university or a "Fachhochschule"). Of the 2,815 who completed tertiary schooling, 58.0% were Swiss men, 28.0% were Swiss women, 8.1% were non-Swiss men and 5.9% were non-Swiss women.
During the 2010-2011 school year there were a total of students in the Solothurn school system. The education system in the Canton of Solothurn allows young children to attend two years of non-obligatory Kindergarten. During that school year, there were Schülerbestand children in kindergarten. The canton's school system requires students to attend six years of primary school, with some of the children attending smaller, specialized classes. In the municipality there were 2010-2011 students in primary school. The secondary school program consists of three lower, obligatory years of schooling, followed by three to five years of optional, advanced schools. All the lower secondary students from Solothurn attend their school in a neighboring municipality. s of 2000[ [update]], there were 2,517 students in Solothurn who came from another municipality, while 188 residents attended schools outside the municipality.
Solothurn is home to 2 libraries. These libraries include; the Zentralbibliothek Solothurn and the "Fachhochschule Nordwestschweiz, Pädagogische Hochschule, Standort Solothurn" (a library of the Fachhochschule Nordwestschweiz). There was a combined total (as of 2008[ [update]]) of 1,195,394 books or other media in the libraries, and in the same year a total of 522,650 items were loaned out.
The number 11.
Solothurn has a special affinity for the number eleven.
The Canton of Solothurn was the eleventh to become part of the Swiss Confederation. There are eleven churches and chapels, as well as eleven historical fountains and eleven towers. The St. Ursus cathedral has eleven altars and eleven bells, and the stairs in front of the cathedral have levels between every eleven steps.
A local brewery has named itself "Öufi", which is Swiss German for eleven, and produces a beer with the same name
International relations.
Twin towns — Sister cities.
Solothurn is twinned with:

</doc>
<doc id="49117" url="http://en.wikipedia.org/wiki?curid=49117" title="Regensburg">
Regensburg

Regensburg (]) is a city in south-east Germany, situated at the confluence of the Danube, Naab and Regen River. With over 140,000 inhabitants, Regensburg is the fourth-largest town in the State of Bavaria after Munich, Nuremberg and Augsburg. The city is the political, economic and cultural centre of Eastern Bavaria and the capital of the Bavarian administrative region Upper Palatinate.
The medieval centre of the city is a UNESCO World Heritage Site and a testimony of the city's status as cultural centre of southern Germany in the middle ages. In 2014, Regensburg was among the top sights and travel attractions in Germany. Generally known in English as Ratisbon until well into the twentieth century, the city is known as Ratisbonne in French and as Ratisbona in Spanish and Italian.
History.
Early history.
The first settlements in Regensburg date to the Stone Age. The Celtic name Radasbona was the oldest name given to a settlement near the present city. Around AD 90, the Romans built a fort there.
In 179, the Roman fort Castra Regina ("fortress by the river Regen") was built for Legio III "Italica" during the reign of Emperor Marcus Aurelius. It was an important camp on the most northern point of the Danube: it corresponds to what is today the core of Regensburg's "Altstadt" ("Old City") east of the Obere and Untere Bachgasse and West of the Schwanenplatz. It is believed that even in late Roman times the city was the seat of a bishop, and St Boniface re-established the Bishopric of Regensburg in 739.
From the early 6th century, Regensburg was the seat of the Agilolfing ruling family. From about 530 to the first half of the 13th century, it was the capital of Bavaria. Regensburg remained an important city during the reign of Charlemagne. In 792, Regensburg hosted the ecclesiastical section of Charlemagne's General Assembly. The bishops in council condemned the heresy of Adoptionism taught by the Spanish bishops, Elipandus of Toledo and Felix of Urgel. After the partition of the Carolingian Empire, the city became the seat of the Eastern Frankish ruler, Louis II the German in 843. Two years later, fourteen Bohemian princes came to Regensburg to receive baptism there. This was the starting point of Christianization of the Czech people, and the diocese of Regensburg became the mother diocese of Prague. These events had a wide impact on the cultural history of Czech lands, as they were consequently incorporated into the Roman Catholic and not into the Slavic-Orthodox world. The fact is well remembered, and a memorial plate at St John's Church (the alleged place of the baptism) was unveiled a few years ago, commemorating the incident in the Czech and German languages.
On 8 December 899 Arnulf of Carinthia, descendant of Charlemagne, died at Regensburg (known as Ratisbon at the time), Bavaria, Germany.
In 800 AD the city had 23,000 inhabitants and by 1000 AD this almost doubled to 40,000 people.
In 1096, on the way to the First Crusade, Peter the Hermit led a mob of Crusaders that attempted to force the mass conversion of the Jews of Regensburg and killed all those who resisted.
Between 1135 and 1146, the Stone Bridge across the Danube was built at Regensburg. This bridge opened major international trade routes between northern Europe and Venice, and this began Regensburg's golden age as a residence of wealthy trading families. Regensburg became the cultural centre of southern Germany and was celebrated for its gold work and fabrics.
Middle Ages.
In 1245 Regensburg became a Free Imperial City and was a trade centre before the shifting of trade routes in the late Middle Ages. At the end of the 15th century in 1486, Regensburg became part of the Duchy of Bavaria, but its independence was restored by the Holy Roman Emperor ten years later. The city adopted the Protestant Reformation in 1542 and its Town Council remained entirely Lutheran. From 1663 to 1806, the city was the permanent seat of the Imperial Diet of the Holy Roman Empire, which became known as the Perpetual Diet of Regensburg. Thus, Regensburg was one of the central towns of the Empire, attracting visitors in large numbers.
A minority of the population remained Roman Catholic, and Roman Catholics were denied civil rights ("Bürgerrecht"). But the town of Regensburg must not be confused with the Bishopric of Regensburg. Although the Imperial city had adopted the Reformation, the town remained the seat of a Roman Catholic bishop and several abbeys. Three of the latter, St. Emmeram, Niedermünster and Obermünster, were estates of their own within the Holy Roman Empire, meaning that they were granted a seat and a vote at the Imperial Diet "(Reichstag)". So there was the unique situation that the town of Regensburg comprised five independent "states" (in terms of the Holy Roman Empire): the Protestant city itself, the Roman Catholic bishopric, and the three monasteries (mentioned previously). In addition, it was seen as the traditional capital of the region Bavaria (not the state), acted as functional co-capital of the Empire (second to the Emperor's court at Vienna) due to the presence of the Perpetual Diet, and it was residence of the Emperor's Commissary-Principal to the same diet, who with one very brief exception was a prince himself (longstandingly the Prince Thurn and Taxis, still resident in the town).
Modern history.
In 1803 the city lost its status as a free city, following its incorporation into the Principality of Regensburg. It was handed over to the Archbishop of Mainz and Archchancellor of the Holy Roman Empire Carl von Dalberg in compensation for Mainz, which had become French under the terms of the Treaty of Lunéville in 1801. The archbishopric of Mainz was formally transferred to Regensburg. Dalberg united the bishopric, the monasteries, and the town itself, making up the Principality of Regensburg ("Fürstentum Regensburg"). Dalberg strictly modernized public life. Most importantly, he awarded equal rights to Protestants and Roman Catholics alike. In 1810 Dalberg ceded Regensburg to the Kingdom of Bavaria, he himself being compensated by the award of Fulda and Hanau to him under the title of "Grand Duke of Frankfurt".
Between April 19 and April 23, 1809, Regensburg was the scene of the Battle of Ratisbon between forces commanded by Baron de Coutaud (the 65th Ligne) and retreating Austrian forces. The city was eventually overrun, after supplies and ammunition ran out. The city suffered severe damage during the fight, with about 150 houses being burnt and others being looted.
Nazism and World War II.
Regensburg was home to both a Messerschmitt Bf 109 aircraft factory and an oil refinery, which were bombed by the Allies on August 17, 1943, by the Schweinfurt-Regensburg mission, and on February 5, 1945, during the Oil Campaign of World War II. Although both targets were badly damaged, Regensburg itself suffered little damage from the Allied strategic bombing campaign, and the nearly intact medieval city centre is listed as a UNESCO World Heritage Site. The city's most important cultural loss was that of the Romanesque church of Obermünster, which was destroyed in a March 1945 air raid and was never rebuilt (the belfry survived). Also, Regensburg's slow economic recovery after the war ensured that historic buildings were not torn down, to be replaced by newer ones. When the upswing in restoration reached Regensburg in the late 1960s, the prevailing mindset had turned in favour of preserving the city's heritage.
History after 1945.
Between 1945 and 1949, Regensburg was the site of the largest Displaced persons (DP) camp in Germany. At its peak in 1946–1947, the workers' district of Ganghofersiedlung housed almost 5,000 Ukrainian and 1,000 non-Ukrainian refugees and displaced persons. With the approval of U.S. Military Government in the American Allied Occupation Zone, Regensburg and other DP camps organised their own camp postal service. In Regensburg, the camp postal service began operation on December 11, 1946.
At the beginning of the 1960s, Regensburg invested a lot in technical and social infrastructure to attract industry. Siemens was the first multinational company to come to Regensburg, a milestone in the city's development after WWII. In 1965, Regensburg University was founded, Regensburg University of Applied Sciences was established in 1971. The second multinational company, BMW, came in 1986 to build up a large production plant. Since the 1990s, several well-known hightech companies are located in Regensburg, such as Infineon and OSRAM, contributing to the city's current wealth.
In 1997, Regensburg was awarded the Europe Prize for its outstanding achievements in european integration.
The World Heritage Committee listed Regensburg's Old Town a UNESCO World Heritage Site in July 2006. It is the largest medieval old town north of the Alps and very well preserved, dubbing it "Italy's most northern city". Close to the Stone Bridge, the city of Regensburg established a World Heritage Centre in the historic Salzstadl in 2007, where detailed information on Regensburg's 2000 year old history is given.
Geography.
Topography.
Regensburg is situated on the northernmost part of the Danube river at the geological crossroads of four distinct landscapes:
Climate.
The climate in Regensburg is categorized in the Köppen climate classification as Dfb (humid continental). The average temperature of 8.5 °C is slightly above the German average (7.8 °C), the average precipitation of 636 mm per year below the German average (approximately 700 mm). With a total of 1670 sunshine hours per year, Regensburg is roughly 120 hours above German average.
The warmest month of the year, on average, is July. The coolest month of the year, on average, is January.
Main sights.
The city.
Regensburg owns the largest medieval old town north of the Alps with nearly 1,500 listed buildings and a picturesque cityscape. Its most famous sights are located mainly in the Old Town, such as:
The surrounding.
Near Regensburg there are two very imposing Classical buildings, erected by Ludwig I of Bavaria as national monuments to German patriotism and greatness: 
Besides, there is the famous Weltenburg Abbey (Kloster Weltenburg), a Benedictine monastery in Weltenburg near Kelheim on the Danube. The abbey is situated on a peninsula in the Danube, on the so-called "Weltenburg Narrows" or the "Danube Gorge". The monastery, founded by Irish or Scottish monks in about 620, is held to be the oldest monastery in Bavaria.
To the east of Regensburg lies the Bavarian Forest with its National Park, one of the most visited protected areas in Germany.
Culture.
Museums and Exhibitions.
Altogether Regensburg is home to 20 museums. Among the most prominent museums are for instance the Regensburg Museum of History which shows history, culture and arts of Regensburg and Eastern Bavaria from stone age to present. Then there is the Imperial diet museum (Reichstagsmuseum) in the Old Town Hall describing the life during the Holy Roman Empire. Its main attractions are an original torture chamber and the Reichssaal, the rooms occupied by the Imperial diet from 1663 to 1806. The Kepler Memorial House (Keplergedächtnishaus) illustrates the life of the famous astronomer and mathematician Johannes Kepler. The Municipal Art Gallery "Leerer Beutel" offers art collections, film events and cultural festivals. Over the last years, the city added several outdoor museums to its cultural landscape, the so-called "document" sites. These give an overview on specific topics such as Roman, Jewish and Bavarian history.
Besides, there are the diocese museums (Bistumsmuseen) of Regensburg and a branch of the Bavarian National Museum located in the St. Emmeram's Abbey, which contains the Princely Treasure Chamber of the family Thurn and Taxis. The Domschatzmuseum where church treasures, monstrances and tapestries are displayed is in St. Peter's Cathedral. Other museums are the Kunstforum Ostdeutsche Galerie, the Naturkundemuseum Ostbayern, the reptile zoo, the Regensburg Museum of Danube Shipping (Donau-Schiffahrts-Museum), the Public Observatory Regensburg as well as the watch museum (Uhrenmuseum), the golf museum, the post museum and the Dinoraeum. To celebrate its centenary, the State of Bavaria will open the museum of Bavarian history in Regensburg in mai 2018. Besides, there are guided tours in most of the historical monuments of Regensburg, as well as organized tourist tours through the city available in several languages.
Theaters.
The Regensburg Theater at the Bismarckplatz is 200 years old and is the most important theater of the city. Operas, operettas, musicals and ballets are shown. In summer, open-air performances are carried out as well. With the theater at the Bismarckplatz as the oldest and largest one, the Regensburg theater has four other stages with programmes that complement each other: in the "Neuhaussaal" of the theater at the Bismarckplatz, concerts by the Philharmonic Orchestra Regensburg take place. The Velodrom Theater presents musicals and plays. In the Haidplatz Theater mainly literary and modern plays are performed, whereas the Turmtheater at the Goliathplatz shows modern plays as well, but also cabarets, musicals and plays for children.
Music.
Regensburg is home to the famous Regensburger Domspatzen. Since 2003 there are the Regensburger Schlossfestspiele in the inner courtyard of the St. Emmeram's Abbey every July, sponsored by the Princely Family of Thurn und Taxis. Meanwhile, those were attracting musicians like Elton John, David Garrett, Tom Jones or Plácido Domingo. Modern music styles, especially Jazz, are presented every summer during the Bavarian Jazz weekend. All over the Old Town, over hundred bands, combos and soloists are performing. In 2015, the House of Music was opened, giving home to skilled musicians and their education.
Film and Cinema.
The international short film season is hosted annually in Regensburg. It is a non-profit event and takes place every march, being meanwhile one of the most important of its type in Germany. Aside, there a several cinemas such as CinemaxX, the largest one showing blockbusters and arthouse films, and smaller independent cinemas such as Garbo, Ostentor Kino and Regina Filmtheater. Regensburg has two open air cinemas as well.
Buildings.
The Old Town of Regensburg with nearly 1,500 listed buildings offers a huge cultural diversity from Roman to modern times.
Recreation.
The Old Town of Regensburg is surrounded completely by a green belt. Numerous inner-city parks like the City Park ("Stadtpark"), the "Herzogspark", the "Dörnbergpark", the "Villapark" or the university's botanical garden are a source for recreation and leisure.
Memorial Sites.
The city of Regensburg erected several memorials to combat racism, intolerance towards minorities and all other forms of contempt for human dignity: 
A specific in Regensburg are the so-called "Stolpersteine" (stumbling blocks) in honor of deported Jews during Nazism.
Events.
Twice a year takes place the Regensburg Dult, the city's Volksfest, which is Bavaria's fourth largest. The Bürgerfest (citizen celebration) in the Old Town is every two years, attracting over 100,000 visitors. Every second weekend in July, knights and other medieval people come together at the Regensburg Spectaculum, a medieval market, on the Stone Bridge. Every December, there are several Christmas markets all over the city.
Nightlife.
With over 500 bars, restaurants, clubs and other locations merely in the inner city, Regensburg provides a rich and diverse nightlife due to its young population.
Demographics.
Population.
In 2013, Regensburg had 140, 276 inhabitants, making it the fourth largest city in Bavaria. Over the last hundred years, the city has experienced a strong increase in population, surpassing 100,000 inhabitants in 1945 due to German refugees from eastern parts of the Third Reich, especially from the Sudetenland. Today, Regensburg is one of fastest growing cities in Germany and is supposed to reach 150,000 inhabitants in the near future. 
International communities.
Nearly 12% of the total population are foreign residents. Most of them come from Turkey and Central and Eastern Europe:
Religion.
Regensburg's population is mostly roman catholic. In 2013, about 56,5 % of the city's inhabitants identified with the Roman Catholic Church, 14,0 % were registered Protestants and about 29,5 % identified with other religions or did not have any registered religious affiliation.
Politics.
Government.
The Lord Mayor and the City Council are elected for a period of six years. Both elections take place at the same time. The City Council is composed of 51 members and includes the Lord Mayor, two deputy mayors, five counsellors and the other council members.
The municipal elections in Bavaria of 2014 delivered the following results: 
After 18 years of a City Council with conservative majority, the social-democratic candidate, Joachim Wolbergs, became Lord Mayor in Mai 2014.
Boroughs.
Regensburg is subdivided into 18 boroughs (Stadtbezirke). Each borough contains a number of localities (Ortsteile), which can have historic roots in older municipalities that became urbanized and incorporated into the city.
Sister cities.
Regensburg is twinned with:
Economy.
Regensburg's economy counts among the most dynamic and fastest growing in Germany. Focus is on manufacturing industries, such as automotive, industrial and electrical engineering.
Companies.
There are several multinational corporations located in Regensburg, such as BMW, Continental, E.ON, General Electric, Infineon, Osram, Schneider Electric, Siemens, Telekom and Toshiba as well as hidden champions (Krones, MR).
BMW operates an automobile production plant in Regensburg; the Regensburg BMW plant produces 3-series, 1-series and Z4 vehicles. Continental AG, with the headquarters of its car component business, Osram Opto-Semiconductors and Siemens as well as Infineon, the former Siemens semiconductor branch, provide a high level of innovation and technical development in Regensburg. Other well known international companies, such as AREVA, Schneider Electric and Toshiba, have built plants in or near Regensburg. GE Aviation founded a greenfield site to innovate, develop and produce turbinemachinery components with a new manufacturing casting technology. Amazon.com located its first German customer service centre in Regensburg. The hidden champions Maschinenfabrik Reinhausen (MR) and Krones both are headquartered in Regensburg and are among the major employeurs.
Aside from the industrial sector, tourism contributes a lot to Regensburg's economical growth, especially since 2006, when the city gained status as UNESCO World Heritage site. The University of Regensburg, the Regensburg University of Applied Sciences and mercantile trade also play major roles in Regensburg's economy. Increasingly, biotech companies were founded in Regensburg over the last two decades and have their headquarters and laboratories in the city's "BioPark". Another focus is on information technology, with the city running a start-up centre for IT firms. One of these former start-ups, CipSoft GmbH, now is a known video game company still based in Regensburg.
OTTI, the Eastern Bavaria Technology Transfer-Institut e.V., is headquartered in Regensburg.
Tourism.
The city recorded 912.238 overnight hotel stays and 531.943 hotel guests in 2012. Tourism figures have nearly doubled within the last 15 years and Regensburg has become one of the most-visited German cities from 100,000 to 500,000 residents. In 2014, Regensburg was ranked as a Top-30 travel attraction in Germany by international tourists.
Infrastructure.
Transport.
Regensburg Hauptbahnhof (central station) is connected to lines to Munich, Nuremberg, Passau, Hof and Ingolstadt and Ulm. It can easily be reached from Munich by train, which takes about 1 hour 30 mins. The city lies also on two motorways, the A3 from Cologne and Frankfurt to Vienna, and the A93 from Munich to Dresden. The city is also connected by "Bundesstraßen", namely the B8, B15, and B16.
The local transport is provided by an intensive bus network run by the .
Energy.
Regensburg's energy is mainly supplied by the German company E.ON, one of the world's largest electric utility service providers. Its subsidiary Bayernwerk runs the local hydropower station in the Danube River. In 2012, about 9,1 % of the total electricity consumption was generated by renewable energy sources, about 5,1 % of the total heat consumption were generated by renewables. Both figures show, that Regensburg is behind other Bavarian cities in this context. Therefore the municipal government presented an energy plan in 2014, which should enhance the transformation towards renewable energy sources over the next decade.
Health.
Regensburg hosts one of the most modern university hospitals in Europe, the "Universitätsklinikum Regensburg". Aside, there are several other renowned hospitals such as the "Krankenhaus Barmherzige Brüder" and the "St. Josef-Krankenhaus". In the "Bezirksklinikum", mental diseases are treated. With 19,4 hospital beds per 1000 residents, Regensburg owns the fourth highest density of beds per residents in Germany. Concerning medical doctors per residents, Regensburg obtains the third place in Germany (339 per 100,000 residents).
The city's BioPark, representing Bavaria's second largest biotech cluster, hosts numerous research institutions and biotech companies.
Education.
Universities and academia.
Regensburg is known for its institutions of higher education. The biggest of those is the University of Regensburg. Founded in 1962, it is one of Germany's youngest institutions and ranked among the Top 400 universities worldwide. Among the prominent thinkers associated with the institution are Pope Benedict XVI, Udo Steiner and Wolfgang Wiegard. The campus is situated in one area together with the Regensburg University of Applied Sciences.
Since 1874 there has been a College of Catholic Music, the Hochschule für Katholische Kirchenmusik und Musikpädagogik Regensburg.
Research.
In addtition to the research centres and institutes of the universities, there are several research institutions situated in the city of Regensburg. Among them are the Institute for East and Southeast European Studies (IOS), the Regensburg Centre for Interventional Immunology (RCI), the Fraunhofer Institute for Toxicology and Experimental Medicine (ITEM) and the BioPark, the Bavarian biotech cluster.
Schools.
Regensburg is home to 18 elementary schools. There are several institutions of secondary education, both public and private, representing all levels of the German school system. There are eight Gymnasiums in Regensburg, five Realschule, six Hauptschule and four vocational schools (the so-called Berufsschule). In addition, there are several folk high schools with different specialisations. Aside, there is the Regensburg International School (RIS) for offering families an international educational infrastructure.
Sports.
Football.
SSV Jahn Regensburg is the local football club and attracts a fairly large local following. The team was part of a larger sports club founded in 1889 as "Turnerbund Jahn Regensburg" which took its name from Friedrich Ludwig Jahn, whose ideas of gymnastics greatly influenced German sport in the 19th century. The football department was created in 1907. The footballers and swimmers left their parent club in 1924 to form "Sportbund Jahn Regensburg".
Ice Hockey.
EV Regensburg is the local ice hockey club, currently playing in the Oberliga Süd, Germany's third highest professional league.
Baseball.
Regensburg Legionäre is the baseball and softball club from Regensburg. The team is also known as Buchbinder Legionäre, following a sponsorship of the Buchbinder company. The club is playing in the German Bundesliga and is one of the most famous and most successful baseball clubs in Germany. Several players now playing in the MLB formerly played at the club. Its arena, Armin-Wolf-Arena, was built in 1996 and has a capacity of 10,000 spectators, making it to Germany's largest baseball stadium.
Athletics.
The local athletics club, LG TELIS FINANZ Regensburg, offers a wide range of different competitions and is counted among the most successful clubs in Germany.

</doc>
<doc id="49120" url="http://en.wikipedia.org/wiki?curid=49120" title="Carthusians">
Carthusians

The Carthusian Order, also called the Order of Saint Bruno, is a Roman Catholic religious order of enclosed monastics. The order was founded by Saint Bruno of Cologne in 1084 and includes both monks and nuns. The order has its own Rule, called the "Statutes", rather than the Rule of Saint Benedict, and combines eremitical and cenobitic life.
The name "Carthusian" is derived from the Chartreuse Mountains; Saint Bruno built his first hermitage in the valley of these mountains in the French Alps. The word "charterhouse", which is the English name for a Carthusian monastery, is derived from the same source. The same mountain range lends its name to the alcoholic cordial Chartreuse produced by the monks since 1737 which itself gives rise to the name of the colour. The motto of the Carthusians is "Stat crux dum volvitur orbis", Latin for "The Cross is steady while the world is turning."
Character.
There are no Carthusian abbeys as they have no abbots, and each charterhouse is headed by a prior and is populated by choir monks, referred to as hermits, and lay brothers.
Each hermit —  that is, a monk who is or who will be a priest  — has his own living space, called a cell, usually consisting of a small dwelling. Traditionally there is a one-room lower floor for the storage of wood for a stove and a workshop as all monks engage in some manual labour. A second floor consists of a small entryway with an image of the Virgin Mary as a place of prayer and a larger room containing a bed, a table for eating meals, a desk for study, a choir stall, and a kneeler for prayer. Each cell has a high walled garden wherein the monk may meditate as well as grow flowers for himself and/or vegetables for the common good of the community, as a form of physical exercise.
The individual cells are organised so that the door of each cell comes off a large corridor. Next to the door is a small revolving compartment — called a "turn" — so that meals and other items may be passed in and out of the cell without the hermit having to meet the bearer. Most meals are provided in this manner, which the hermit then eats in the solitude of his cell. There are two meals provided for much of the year: lunch and supper. During seasons or days of fasting, just one meal is provided. The hermit makes his needs known to the lay brother by means of a note, requesting items such as a fresh loaf of bread, which will be kept in the cell for eating with several meals.
The hermit spends most of his day in the cell: he meditates, prays the minor hours of the Liturgy of the Hours on his own, eats, studies and writes (Carthusian monks have published scholarly and spiritual works), and works in his garden or at some manual trade. Unless required by other duties, the Carthusian hermit leaves his cell daily only for three prayer services in the monastery chapel, including the community Mass, and occasionally for conferences with his superior. Additionally, once a week, the community members take a long walk in the countryside during which they may speak; on Sundays and feastdays a community meal is taken in silence. Twice a year there is a day-long community recreation, and the monk may receive an annual visit from immediate family members.
The Carthusians do not engage in work of a pastoral or missionary nature. Unlike most monasteries, they do not have retreatants and those who visit for a prolonged period are people who are contemplating entering the monastery. As far as possible, the monks have no contact with the outside world. Their contribution to the world is their life of prayer, which they undertake on behalf of the whole Church and the human race.
In addition to the choir monks there are lay brothers, monks under slightly different types of vows who spend less time in prayer and more time in manual labour; they live a slightly more communal life, sharing a common area of the charterhouse. The lay brothers provide material assistance to the choir monks: cooking meals, doing laundry, undertaking physical repairs, providing the choir monks with books from the library and managing supplies. All of the monks live lives of silence.
Carthusian nuns live a life similar to the monks, but with some differences. Choir nuns tend to lead somewhat less eremitical lives, while still maintaining a strong commitment to solitude and silence.
Today, Carthusians live very much as they originally did, without any relaxing of their rules.
Carthusians in Britain.
The first Carthusian monastery or 'Charterhouse' in England was founded by Henry II in Witham Friary, Somerset as penance for the murder of St Thomas Becket.
The best preserved remains of a medieval Charterhouse in the UK are at Mount Grace Priory near Osmotherley, North Yorkshire. One of the cells has been reconstructed to illustrate how different the lay-out is to monasteries of most other Christian orders, which are normally designed with communal living in mind.
The third Charterhouse built in Britain was Beauvale Charterhouse remains of which can still be seen in Beauvale, Greasley parish, Nottinghamshire.
The London Charterhouse gave its name to a square and several streets in the City of London, as well as to the Charterhouse School which used part of its site before moving out to Godalming, Surrey.
A few fragments remain of the Charterhouse in Coventry, mostly dating from the 15th century, and consisting of a sandstone building that was probably the prior's house. The area, about a mile from the centre of the city, is a conservation area, but the buildings are in use as part of a local college. Inside the building is a medieval wall painting, alongside many carvings and wooden beams. Nearby is the river Sherbourne that runs underneath the centre of the city.
A single Carthusian Priory was founded in Scotland during the Middle Ages, at Perth. It stood just west of the medieval town and was founded by James I (1406–1437) in the early 15th century. James I and his queen Joan Beaufort (died 1445) were both buried in the priory church, as was Queen Margaret Tudor (died 1541), widow of James IV of Scotland. The Priory, said to have been a building of 'wondrous cost and greatness' was sacked during the Scottish Protestant Revolution in 1559, and swiftly fell into decay. No remains survive above ground, though a Victorian monument marks the site. The Perth names Charterhouse Lane and Pomarium Flats (built on the site of the Priory's orchard) recall its existence.
St. Hugh's Charterhouse, Parkminster, West Sussex has cells running around a square cloister approximately 400 m (one quarter-mile) on a side, making it the largest cloister in Europe.
Modern Carthusians.
The Carthusians were, as with all Catholic religious orders, variously persecuted and banned during the Protestant Reformation, owing to their number particularly reducing was the abolition of their priories which were sources of charity in England, followed by the same during the French Revolution and after in France.
Today, the monastery of the Grande Chartreuse is still the Motherhouse of the Order. There is a museum illustrating the history of Carthusian order next to Grande Chartreuse; the monks of that monastery are also involved in producing Chartreuse liqueur. Visits are not possible into the Grande Chartreuse itself, but the 2005 documentary "Into Great Silence" gave unprecedented views of life within the hermitage. In the 21st century, the Sélignac Charterhouse was converted into a house in which lay people could come and experience Carthusian retreats, living the Carthusian life for shorter periods (an eight-day retreat being fixed as the absolute minimum, in order to enter at least somewhat into the silent rhythm of the charterhouse).
There is one Carthusian monastery in North America, the Charterhouse of the Transfiguration, on Mount Equinox (near Arlington, Vermont). It was founded in the 1950s.
Liturgy.
Before the Council of Trent in the 16th century, the Catholic Church in Western Europe had a wide variety of rituals for the celebration of Mass. Although the essentials were the same, there were variations in prayers and practices from region to region or among the various religious orders.
When Pope Pius V made the Roman Missal mandatory for all Catholics of the Latin Rite, he permitted the continuance of other forms of celebrating Mass that had an antiquity of at least two centuries. The rite used by the Carthusians was one of these, and still continues in use in a version revised in 1981. Apart from the new elements in this revision, it is substantially the rite of Grenoble in the 12th century, with some admixture from other sources. According to current Catholic legislation, however, priests can celebrate the traditional rites of their order without further authorization.
A feature unique to Carthusian liturgical practice is that the bishop bestows on Carthusian nuns, in the ceremony of their profession, a stole and a maniple. This is interpreted by some as a relic of the former rite of ordination of women deacons. The nun is also invested with a crown and a ring. The nun wears these ornaments again only on the day of her monastic jubilee, and after her death on her bier. At Matins, if no priest is present, a nun assumes the stole and reads the Gospel; and although in the time of the Tridentine Mass the chanting of the Epistle was reserved to an ordained subdeacon, a consecrated nun sang the Epistle at the conventual Mass, though without wearing the maniple. For centuries Carthusian nuns retained this rite, administered by the diocesan bishop four years after the nun took her vows. It is no longer unique, since the liturgical reforms that followed Second Vatican Council made the rite of the consecration of virgins more widely available.
Locations of monasteries.
There are 25 active Charterhouses around the world, five of which are for nuns; altogether, there are around 370 monks and 75 nuns. They can be found in Argentina (1), Brazil (1), France (6), Germany (1), Italy (4), Portugal (1), Slovenia (1), South Korea (2), Spain (5), Switzerland (1), the United Kingdom (1) and the USA (1). The two in South Korea, one of monks and one of nuns, are of recent construction.
References.
Notes
Further reading

</doc>
