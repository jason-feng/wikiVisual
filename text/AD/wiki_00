<doc id="42661" url="http://en.wikipedia.org/wiki?curid=42661" title="Myth (series)">
Myth (series)

Myth is a series of real-time tactics video games for Microsoft Windows and Mac OS. The games are:
"Myth" was developed by Bungie and published in 1997 by Eidos in Europe and Bungie in North America. "Myth II" was also developed by Bungie and self-published in North America in 1998. It was published by GT Interactive Software in Europe. As a result of Bungie's sale to Microsoft in 2000, the company lost the franchise rights to Take-Two Interactive. "Myth III: The Wolf Age" was developed by MumboJumbo and published by Take-Two in 2001.
All three games have received good reviews, especially the first and second game. Although the third game also received a generally positive reception, many reviewers cited a number of bugs in the initial release, and there was a general feeling that Take-Two had not given MumboJumbo enough time to complete the game.
The "Myth" games are categorized as real-time tactics, representing a departure from established real-time strategy titles such as "Warcraft" and "Command & Conquer"; resource micromanagement and the gradual building up of armies are not part of the gameplay, which instead focuses entirely on squad and soldier-level tactics. Some critics have argued that this style of gameplay allows the games a far greater sense of realism than their real-time strategy contemporaries.
Release dates.
† indicates user created content bundled with re-releases of the games.
Gameplay.
General.
Players control small forces made up of a number of different units, each possessing their own strengths and weaknesses. In single-player mode, only units representing "The Light" are playable, but in multiplayer mode, the player can control both light and dark units.
The "Myth" games are real-time tactics games, meaning that unlike the gameplay of real-time strategy games, the player does not have to worry about resource micromanagement and the gradual building up of their army; each level begins with the player's army already assembled and ready to go into combat straight away. Also unlike real-time strategy games, where the emphasis tends to be on producing as many soldiers as possible, in the "Myth" games, it is possible for a skilled player to defeat a much larger force with few or no casualties through tactical play. This is largely due to the advanced physics engine the games employ, as physically modelled environments, unit interactions, and diverse unit behaviours combine to create a gameplay experience in which realistic battlefield interactions can and do occur.
Nearly all objects on the maps, even the remains of dead units, are potential projectiles. These objects react with one another, units on the map, and terrain, with the expected physical behaviour, including rolling, bouncing, and crashing. Projectiles, including those fired by ranged units, have no guarantee of hitting any target; they are merely propelled in the directions instructed by the physics engine, based on the actions of the players. Arrows may miss their targets due to a small degree of simulated aiming error that becomes significant at long range, or the target may simply move out of the way before the arrow reaches them. This aiming error may cause the arrow to hit the attacker's own melee unit instead, causing the same amount of damage. As such, friendly fire is a prominent aspect of the game and can be used to the player's advantage when facing certain enemies.
Unit formations are important in all of the games, where a real battlefield is simulated accurately enough for maneuvers such as flanking and encirclement to be effective. When placed together in formation, units can provide an effective defensive front, block an enemy force's escape route, or exploit bad positioning of an enemy force by surrounding it. As healing is a very rare and extremely limited ability, units do not regenerate health, and there is no way to construct new units (although in some single-player missions, reinforcements are automatically received at a predetermined point), hit-and-run skirmishes are very effective, and unit conservation is essential.
Terrain and environmental factors are also important. Rain or standing water will put out some fire and explosive-based attacks. Archers on high ground are able to shoot farther than those on level ground. Most units will flinch when damaged, interrupting actions such as movement and attacks. This has many strategic implications; for example, if two or three melee units gang up to attack one enemy melee unit, it may flinch too frequently to have a chance to attack or escape.
Single-player.
In the single-player campaigns, the player starts each mission with an army which must be used to accomplish specific goals. These goals range from defending a location, reaching a certain point on the map, escorting a unit safely, or destroying an object of strategic significance.
The focus of the single-player campaigns is on a smaller force outmaneuvering and outthinking a much larger enemy force. For this reason, the importance of terrain and unit formation is particularly important. Using high ground to further the range of archers, creating bottle necks, and whittling down an enemy with hit-and-run tactics all become crucial strategies in the single-player game, especially on higher difficulty levels.
Units in the single-player campaign acquire experience with each kill. As they acquire experience, they become more resilient, attack faster, and deal more damage. Units retain this experience until killed or until a unit of their type does not appear in a given scenario.
Multiplayer.
In multiplayer, the player starts with an army and can usually customize it by trading units, using point values that approximate the value of the units. Proper selection of units is an important strategy given the goal of each multiplayer game. For example, if the goal of the game is to guard a flag as long as possible (such as "King of the Hill"), customizing the army with only ranged units would not be wise as there would be no melee units to guard the flag in close combat.
Multiplayer games generally are either "Free-For-All" (FFA), where each player has their own army and competes with everyone else, or "Team," where each army is controlled by a group of players with a captain who disperses units for his teammates to control. There are many different types of multiplayer games within this, ranging from simple "Body Count" to more complicated games involving flags, balls, or animals.
"Myth" development history.
Bungie.
"Myth: The Fallen Lords" and "Myth II: Soulblighter" were developed by Bungie in 1997 and 1998. Some reviewers felt that the games were influenced by Glen Cook's book series "The Black Company". Due to mapmaking tools released to the public by Bungie, and additional tools created by fans, new maps, units, 3D objects, and other plug-ins were created for "Myth II". Some of the better known mods were released in official bundles of the game. The expansion pack "Myth II: Chimera" was jointly developed by Bungie and Badlands Games, and was released in the bundle "Myth: The Total Codex", along with "Myth", and "Myth II" itself. "Myth: The Total Codex" was the last official release by Bungie for the "Myth" franchise.
Take-Two acquires "Myth" franchise.
In 2000, Microsoft purchased Bungie Studios, which had previously developed games for Microsoft's main competitor's platform, Apple's Macintosh, in order to have the studio develop "" exclusively for Microsoft's new Xbox game console. Early development versions of "Halo" resembled a sci-fi clone of "Myth". As part of the sale of Bungie, the rights to the "Oni" and the "Myth" series went to the video game publisher Take-Two Interactive who held a large share of the studio's stocks at the time of the sale. Take-Two initially released two "Myth" related titles; "Myth II: Worlds" (which included two disks of fan-created add-ons) and "Myth II: Green Berets" (a conversion from the medieval setting to a Vietnam war era setting).
Tournaments and online servers.
Bungie.net was the original "Myth" series server. The "Myth: The Fallen Lords" server closed in November 2001, and the "Myth II: Soulblighter" server closed in March, 2002. After Bungie.net closed, Bungie open sourced the online server code, and multiple other online servers appeared, the most famous being PlayMyth.net. PlayMyth.net was taken offline in 2007.
MumboJumbo develops "Myth III".
Take-Two hired the startup company MumboJumbo to develop "Myth III" based on "Myth II"‍‍ '​‍s source code. MumboJumbo had been founded by employees that left Ritual Entertainment, and "Myth III" would be their first game. Take-Two also hired many members of "Myth II"‍ '​s modding community to work on both expansions for "Myth II" and the new "Myth III".
The developers made significant changes to the existing code to improve the game's visual aesthetics, such as increasing the game's texture quality. The new engine also sported full 3D characters, each with 300 to 800 polygons and at least 13 different animations, unlike its predecessor which relied on sprites for characters and animations. Soon before the game's release, "PC Gamer"‍ '​s staff writer Jim Preston wrote that he was skeptical as to whether the developer had been given enough time to satisfactorily complete the game.
The developers worked to support the modding community by taking "Fear & Loathing", the application used to create mods for the previous games, and creating a new, easier to use application known as "Vengeance".
Shortly after the release of "Myth III", Take-Two cancelled all development and technical support for all three "Myth" games, and the complete MumboJumbo "Myth III" team was laid off.
Game-community support.
Lead developer Andrew Meggs joined with a group of fans who called themselves "MythDevelopers" with the aim of continuing to support the games. Given access to the source code for all three games by Take-Two, they have continued to work to update the series. This group, and successor groups, have continued to support and improve all three games, with software updates for the latest operating systems, bug fixes, and the addition of enhancements and features to both the games and modding tools.
In December 2003, MythDevelopers had internal struggles and disbanded, forming two smaller groups: Project Magma, and FlyingFlip. FlyingFlip went offline in 2007, leaving the currently active (as of October 2013) "Myth" development group as Project Magma.
Reception.
All three main games in the "Myth" franchise have been critically acclaimed, especially the first two.
Game Revolution's Calvin Hubble called "The Fallen Lords" "one of the most impressive looking strategy games to hit the market," whilst GameSpot's Michael E. Ryan argued that it "can rightfully claim its place among the best strategy games on the market." Hubble called "Myth II" "both one of the best sequels to hit the scene and one of the finest titles on the RTS market." Ryan wrote that ""Myth II" is about as good as a computer game can possibly be." IGN's Tal Blevins said ""Myth II" lives up to (and surpasses) all of the hype surrounding this long-awaited title."
"Myth III" did not get quite as good reviews, but it was still well received. GameSpot's Sam Parker wrote that ""Myth III"‍ '​s single-player game represents the best the "Myth" series has to offer. Featuring great graphics, a memorable story, and plenty of diverse missions, "Myth III"‍ '​s campaign will present a welcome challenge for veterans and newcomers alike." He also expressed some concern as to how long Take-Two would provide technical support for the game. IGN's Dan Adams found several faults, but still enjoyed the game, writing "some unfortunate set backs, whether they were by design or bug dulled the experience a little bit, but not enough to hamper my enjoyment. Fans of the series shouldn't be disappointed by MumboJumbo's effort to follow in the mighty footsteps that Bungie left behind."

</doc>
<doc id="42673" url="http://en.wikipedia.org/wiki?curid=42673" title="How Green Was My Valley">
How Green Was My Valley

How Green Was My Valley is a 1939 novel by Richard Llewellyn, narrated by Huw, the main character, about his Welsh family and the mining community in which they live. The author had claimed that he based the book on his own personal experiences but this was found to be untrue after his death; Llewellyn was English-born and spent little time in Wales, though he was of Welsh descent. Llewellyn gathered material for the novel from conversations with local mining families in Gilfach Goch.
The title of the novel appears in two sentences. It is first used in Chapter Thirty, after the narrator has had his first sexual experience. He sits up to "... look down in the valley." He then reflects: "How green was my Valley that day, too, green and bright in the sun." The phrase is used again in the novel's last sentence: "How green was my Valley then, and the Valley of them that have gone."
In the United States, Llewellyn won the National Book Award for favorite novel of 1940, voted by members of the American Booksellers Association.
Plot summary.
The novel is set in South Wales during the reign of Queen Victoria. It tells the story of the Morgans, a respectable mining family of the South Wales Valleys, through the eyes of the youngest son, Huw Morgan. 
Huw's academic ability sets him apart from his elder brothers and enables him to consider a future away from the dangerous coal mines. His five brothers and his father are miners; after the eldest brother, Ivor, is killed in a mining accident, Huw moves in with his sister-in-law, Bronwen, with whom he has always been in love. 
One of Huw's three sisters, Angharad, marries the wealthy mine owner's son - whom she does not love - and the marriage is an unhappy one. She never overcomes her clandestine relationship with the local minister. 
Huw's father is later killed in a mine explosion. After everyone Huw has known either dies or moves away, and the town is reduced to a contaminated shell, he decides to leave, and tells the story of his life just before going away.
Characters.
The Older Morgans:
The Middle Brothers:
These are Huw's young adult brothers. Ianto goes to London to find work early in the book, but returns unhappily; Owen and Gwilym do the same later. 
The Younger Morgans: 
Other Characters:
First printing.
The first edition was published in 1939 by Michael Joseph Ltd, London, set and printed in Great Britain by William Brendon & Son, Ltd., at the Mayflower Press, Plymouth, in Walbaum type, twelve point, leaded, on a toned opaque-wove paper made by John Dickinson, and bound by James Burn. It was published in 8vo size. The first printing included a limited edition run of 200, numbered and signed by Richard Llewellyn. The original print run also included a glossary covering Welsh words and terms at the end of the book. 
Sequels.
The author continued the story of Huw Morgan's life in three sequels:
Adaptations.
The 1941 Hollywood film adaptation, which was highly successful, had a cast that included Walter Pidgeon, Maureen O'Hara, Anna Lee, Roddy McDowall (as Huw), Donald Crisp, and Barry Fitzgerald. None of the leading players was Welsh (though Welsh actor Rhys Williams made his screen debut in the film in a minor role). Directed by John Ford, "How Green Was My Valley" was selected for preservation in the United States National Film Registry. "How Green Was My Valley" is available on DVD from 20th Century Fox as part of their 20th Century Fox Studio Classics collection.
The book has twice been adapted by the BBC for television, in 1960 and 1975. The 1975 production– scripted by Elaine Morgan – starred Stanley Baker, Siân Phillips, and Nerys Hughes.
The novel was adapted as a Broadway musical, called "A Time for Singing", which opened at the Broadway Theatre, New York, on 21 May 1966. The music was by John Morris; book and lyrics were by Gerald Freedman and John Morris. The production was directed by Mr. Freedman, and it starred Ivor Emmanuel, Tessie O'Shea, Shani Wallis, and Laurence Naismith.
A stage version, adapted by Shaun McKenna was performed at the Theatre Royal in Northampton in 1990. It marked the stage debut of Aled Jones as the teenage Huw. It was directed by Michael Napier Brown and designed by Ray Lett.

</doc>
<doc id="42674" url="http://en.wikipedia.org/wiki?curid=42674" title="Shor's algorithm">
Shor's algorithm

Shor's algorithm, named after mathematician Peter Shor, is a quantum algorithm (an algorithm that runs on a quantum computer) for integer factorization formulated in 1994. Informally it solves the following problem: Given an integer "N", find its prime factors.
On a quantum computer, to factor an integer "N", Shor's algorithm runs in polynomial time (the time taken is polynomial in log "N", which is the size of the input). Specifically it takes time and quantum gates of order O((log "N")2(log log "N")(log log log "N")) using fast multiplication, demonstrating that the integer factorization problem can be efficiently solved on a quantum computer and is thus in the complexity class BQP. This is substantially faster than the most efficient known classical factoring algorithm, the general number field sieve, which works in sub-exponential time — about O(e1.9 (log N)1/3 (log log N)2/3). The efficiency of Shor's algorithm is due to the efficiency of the quantum Fourier transform, and modular exponentiation by repeated squarings.
If a quantum computer with a sufficient number of qubits could operate without succumbing to noise and other quantum decoherence phenomena, Shor's algorithm could be used to break public-key cryptography schemes such as the widely used RSA scheme. RSA is based on the assumption that factoring large numbers is computationally intractable. So far as is known, this assumption is valid for classical (non-quantum) computers; no classical algorithm is known that can factor in polynomial time. However, Shor's algorithm shows that factoring is efficient on an ideal quantum computer, so it may be feasible to defeat RSA by constructing a large quantum computer. It was also a powerful motivator for the design and construction of quantum computers and for the study of new quantum computer algorithms. It has also facilitated research on new cryptosystems that are secure from quantum computers, collectively called post-quantum cryptography.
In 2001, Shor's algorithm was demonstrated by a group at IBM, who factored 15 into 3 × 5, using an NMR implementation of a quantum computer with 7 qubits. Since IBM's implementation, several other groups have implemented Shor's algorithm using photonic qubits, emphasizing that entanglement was observed. In 2012, the factorization of 15 was repeated. Also in 2012, the factorization of 21 was achieved, setting the record for the largest number factored with a quantum computer. In April 2012, the factorization of 143 was achieved, although this used adiabatic quantum computation rather than Shor's algorithm . It was discovered in November 2014 that this adiabatic quantum computation in 2012 had in fact also factored larger numbers, the largest being 56153, which is currently the record for the largest integer factored on a quantum device .
Procedure.
The problem we are trying to solve is: given an odd composite number formula_1, find an integer formula_2, strictly between formula_3 and formula_1, that divides formula_1. We are interested in odd values of formula_1 because any even value of formula_1 trivially has the number formula_8 as a prime factor. We can use a primality testing algorithm to make sure that formula_1 is indeed composite.
Moreover, for the algorithm to work, we need formula_1 not to be the power of a prime. This can be tested by taking square, cubic, ..., formula_11-roots of formula_1, for formula_13, and checking that none of these is an integer. (This actually excludes that formula_14 for some integer formula_15 and formula_16.)
Since formula_1 is not a power of a prime, it is the product of two coprime numbers greater than formula_3. As a consequence of the Chinese remainder theorem, the number formula_3 has at least four distinct square roots modulo formula_1, two of them being formula_3 and formula_22. The aim of the algorithm is to find a square root formula_23 of one, other than formula_3 and formula_22; such a formula_23 will lead to a factorization of formula_1, as in other factoring algorithms like the quadratic sieve.
In turn, finding such a formula_23 is reduced to finding an element formula_29 of even period with a certain additional property (as explained below, it is required that the condition of Step 6 of the classical part does not hold). The quantum algorithm is used for finding the period of randomly chosen elements formula_29, as order-finding is a hard problem on a classical computer.
Shor's algorithm consists of two parts:
Classical part.
For example: formula_31, formula_32, where formula_33, and formula_34.
Quantum part: Period-finding subroutine.
The quantum circuits used for this algorithm are custom designed for each choice of "N" and each choice of the random "a" used in "f"("x") = "a""x" mod "N". Given "N", find "Q" = 2"q" such that formula_35, which implies formula_36. The input and output qubit registers need to hold superpositions of values from 0 to "Q" − 1, and so have "q" qubits each. Using what might appear to be twice as many qubits as necessary guarantees that there are at least "N" different "x" which produce the same "f"("x"), even as the period "r" approaches "N"/2.
Proceed as follows:
Explanation of the algorithm.
The algorithm is composed of two parts. The first part of the algorithm turns the factoring problem into the problem of finding the period of a function, and may be implemented classically. The second part finds the period using the quantum Fourier transform, and is responsible for the quantum speedup.
Obtaining factors from period.
The integers less than "N" and coprime with "N" form a finite Abelian group formula_37 under multiplication modulo "N". The size is given by Euler's totient function formula_38.
By the end of step 3, we have an integer "a" in this group. Since the group is finite, "a" must have a finite order "r", the smallest positive integer such that
Therefore, "N" divides (also written | ) "a" "r" − 1 . Suppose we are able to obtain "r", and it is even. (If "r" is odd, see step 5.) Now formula_40 is a square root of 1 modulo formula_1, different from 1. This is because formula_42 is the order of formula_29 modulo formula_1, so formula_45, else the order of formula_29 in this group would be formula_47. If formula_48, by step 6 we have to restart the algorithm with a different random number formula_29.
Eventually, we must hit an formula_29, of order formula_42 in formula_37, such that formula_53. This is because such a formula_23 is a square root of 1 modulo formula_1, other than 1 and formula_22, whose existence is guaranteed by the Chinese remainder theorem, since formula_1 is not a prime power.
We claim that formula_58 is a proper factor of formula_1, that is, formula_60. In fact if formula_61, then formula_1 divides formula_63, so that formula_64, against the construction of formula_23. If on the other hand formula_66, then by Bézout's identity there are integers formula_67 such that
Multiplying both sides by formula_69 we obtain
Since formula_1 divides formula_72, we obtain that formula_1 divides formula_69, so that formula_75, again contradicting the construction of formula_23.
Thus formula_2 is the required proper factor of formula_1.
Finding the period.
Shor's period-finding algorithm relies heavily on the ability of a quantum computer to be in many states simultaneously.
Physicists call this behavior a "superposition" of states. To compute the period of a function "f", we evaluate the function at all points simultaneously.
Quantum physics does not allow us to access all this information directly, though. A measurement will yield only one of all possible values, destroying all others. If not for the no cloning theorem, we could first measure "f"("x") without measuring "x", and then make a few copies of the resulting state (which is a superposition of states all having the same "f"("x")). Measuring "x" on these states would provide different "x" values which give the same "f"("x"), leading to the period. Because we cannot make exact copies of a quantum state, this method does not work. Therefore we have to carefully transform the superposition to another state that will return the correct answer with high probability. This is achieved by the quantum Fourier transform.
Shor thus had to solve three "implementation" problems. All of them had to be implemented "fast", which means that they can be implemented with a number of quantum gates that is polynomial in formula_79.
After all these transformations a measurement will yield an approximation to the period "r".
For simplicity assume that there is a "y" such that "yr/Q" is an integer.
Then the probability to measure "y" is 1.
To see that we notice that then
for all integers "b". Therefore the sum whose square gives us the probability to measure "y" will be "Q/r" since "b" takes roughly "Q/r" values and thus the probability is formula_82. There are "r" "y" such that "yr/Q" is an integer and also "r" possibilities for formula_83, so the probabilities sum to 1.
Note: another way to explain Shor's algorithm is by noting that it is just the quantum phase estimation algorithm in disguise.
The bottleneck.
The runtime bottleneck of Shor's algorithm is quantum modular exponentiation, which is by far slower than the quantum Fourier transform and classical pre-/post-processing. There are several approaches to constructing and optimizing circuits for modular exponentiation. The simplest and (currently) most practical approach is to mimic conventional arithmetic circuits with reversible gates, starting with ripple-carry adders. Knowing the base and the modulus of exponentiation facilitates further optimizations. Reversible circuits typically use on the order of formula_84 gates for formula_85 qubits. Alternative techniques asymptotically improve gate counts by using quantum Fourier transforms, but are not competitive with less than 600 qubits due to high constants.
Discrete logarithms.
Given prime formula_86 with generator formula_87 where formula_88, suppose we know that formula_89, for some "r", and we wish to compute "r", which is the discrete logarithm: formula_90. Consider the Abelian group formula_91 where each factor corresponds to modular multiplication of nonzero values, assuming p is prime. Now, consider the function
This gives us an Abelian hidden subgroup problem, as "f" corresponds to a group homomorphism. The kernel corresponds to modular multiples of ("r",1). So, if we can find the kernel, we can find "r".
In popular culture.
On the television show "Stargate Universe", the lead scientist, Dr. Nicholas Rush, hoped to use Shor's algorithm to crack "Destiny"'s master code. He taught a quantum cryptography class at the University of California, Berkeley, in which Shor's algorithm was studied.
Shor's algorithm was also a correct answer to a question in a Physics Bowl competition in the episode "The Bat Jar Conjecture" of the TV series "The Big Bang Theory".

</doc>
<doc id="42676" url="http://en.wikipedia.org/wiki?curid=42676" title="Mold health issues">
Mold health issues

Mold health issues are potentially harmful effects of molds.
Molds (also spelled "moulds") are ubiquitous in the biosphere, and mold spores are a common component of household and workplace dust. The United States Centers for Disease Control and Prevention reported in its June 2006 report, 'Mold Prevention Strategies and Possible Health Effects in the Aftermath of Hurricanes and Major Floods,' that "excessive exposure to mold-contaminated materials can cause adverse health effects in susceptible persons regardless of the type of mold or the extent of contamination." When mold spores are present in abnormally high quantities, they can present especially hazardous health risks to humans, including allergic reactions or poisoning by mycotoxins, or causing fungal infection (mycosis).
Health effects.
Studies have shown that people who are healthy, atopic (sensitive), already suffer from allergies, asthma, or compromised immune systems and occupy damp or moldy buildings are at an increased risk of health problems such as inflammatory and toxic responses to mold spores, metabolites and other components. The most common health problem is an allergic reaction. Other problems are respiratory and/or immune system responses including respiratory symptoms, respiratory infections, exacerbation of asthma, and rarely hypersensitivity pneumonitis, allergic alveolitis, chronic rhinosinusitis and allergic fungal sinusitis. Severe reactions are rare but possible. A persons reaction to mold depends on their sensitivity and other health conditions, the amount of mold present, length of exposure and the type of mold or mold products.
Some molds also produce mycotoxins that can pose serious health risks to humans and animals. The term "toxic mold" refers to molds that produce mycotoxins, such as "Stachybotrys chartarum", not to all molds. Exposure to high levels of mycotoxins can lead to neurological problems and in some cases death. Prolonged exposure, e.g., daily workplace exposure, can be particularly harmful.
The four most common genera of indoor molds are "Cladosporium", "Penicillium", "Aspergillus", and "Alternaria". 
Damp environments which allow mold to grow can also produce bacteria and help release volatile organic compounds.
Symptoms of mold exposure.
Symptoms of mold exposure can include:
Health effects linking to asthma.
Infants may develop respiratory symptoms as a result of exposure to a specific type of fungal mold, called Penicillium. Signs that an infant may have mold-related respiratory problems include (but are not limited to) a persistent cough and/or wheeze. Increased exposure increases the probability of developing respiratory symptoms during their first year of life. Studies have shown that a correlation exists between the probability of developing asthma and increased exposure "Penicillium". The levels are deemed no mold to low level, from low to intermediate, from intermediate to high.
Mold exposures have a variety of health effects depending on the person. Some people are more sensitive to mold than others. Exposure to mold can cause a number of health issues such as; throat irritation, nasal stuffiness, eye irritation, cough and wheezing, as well as skin irritation in some cases. Exposure to mold may also cause heightened sensitivity depending on the time and nature of exposure. People at higher risk for mold allergies are people with chronic lung illnesses, which will result in more severe reactions when exposed to mold.
There has been sufficient evidence that damp indoor environments are correlated with upper respiratory tract symptoms such as coughing, and wheezing in people with asthma.
Mold-associated conditions.
Health problems associated with high levels of airborne mold spores include allergic reactions, asthma episodes, irritations of the eye, nose and throat, sinus congestion, and other respiratory problems, although it should be noted that mold spores won't actually cause asthma, just irritate existing conditions. For example, residents of homes with mold are at an elevated risk for both respiratory infections and bronchitis. When mold spores are inhaled by an immunocompromised individual, some mold spores may begin to grow on living tissue, attaching to cells along the respiratory tract and causing further problems. Generally, when this occurs, the illness is an epiphenomenon and not the primary pathology. Also, mold may produce mycotoxins, either before or after exposure to humans, potentially causing toxicity.
Fungal infection.
A serious health threat from mold exposure for immunocompromised individuals is systemic fungal infection (systemic mycosis). Immunocompromised individuals exposed to high levels of mold, or individuals with chronic exposure may become infected. Sinuses and digestive tract infections are most common; lung and skin infections are also possible. Mycotoxins may or may not be produced by the invading mold.
Dermatophytes are the parasitic fungi that cause skin infections such as athlete's foot and tinea cruris. Most dermataphyte fungi take the form of a mold, as opposed to a yeast, with appearance (when cultured) that is similar to other molds.
Opportunistic infection by molds such as "Penicillium marneffei" and "Aspergillus fumigatus" is a common cause of illness and death among immunocompromised people, including people with AIDS or asthma.
Mold-induced hypersensitivity.
The most common form of hypersensitivity is caused by the direct exposure to inhaled mold spores that can be dead or alive or hyphal fragments which can lead to allergic asthma or allergic rhinitis. The most common effects are rhinorrhea (runny nose), watery eyes, coughing and asthma attacks. Another form of hypersensitivity is hypersensitivity pneumonitis. Exposure can occur at home, at work or in other settings. It is predicted that about 5% of people have some airway symptoms due to allergic reactions to molds in their lifetimes.
Hypersensitivity may also be a reaction toward an established fungal infection in allergic bronchopulmonary aspergillosis.
Mycotoxin toxicity.
Molds excrete toxic compounds called mycotoxins, secondary metabolites produced by fungi under certain environmental conditions. These environmental conditions affect the production of mycotoxins at the transcription level. Temperature, water activity and pH, strongly influence mycotoxin biosynthesis by increasing the level of transcription within the fungal spore. It has also been found that low levels of fungicides can boost mycotoxin synthesis. Certain mycotoxins can be harmful or lethal to humans and animals when exposure is high enough.
Extreme exposure to very high levels of mycotoxins can lead to neurological problems and in some cases death; fortunately, such exposures rarely to never occur in normal exposure scenarios, even in residences with serious mold problems. Prolonged exposure, such as daily workplace exposure, can be particularly harmful.
The health hazards produced by mold have been associated with sick building syndrome, but no validated studies have been able to demonstrate that normal indoor exposures to these common organisms pose a significant threat.
It is thought that all molds may produce mycotoxins and thus all molds may be potentially toxic if large enough quantities are ingested, or the human becomes exposed to extreme quantities of mold. Mycotoxins are not produced all the time, but only under specific growing conditions. Mycotoxins are harmful or lethal to humans and animals only when exposure is high enough.
Mycotoxins can be found on the mold spore and mold fragments, and therefore they can also be found on the substrate upon which the mold grows. Routes of entry for these insults can include ingestion, dermal exposure and inhalation.
Some mycotoxins cause immune system responses that vary considerably, depending on the individual. The duration of exposure, the frequency of exposure and the concentration of the insult (exposure) are elements in triggering immune system response.
Aflatoxin is an example of a mycotoxin. It is a cancer-causing poison produced by certain fungi in or on foods and feeds, especially in field corn and peanuts.
Originally, toxic effects from mold were thought to be the result of exposure to the mycotoxins of some mold species, such as "Stachybotrys chartarum". However, studies are suggesting that the so-called toxic effects are actually the result of chronic activation of the immune system, leading to chronic inflammation. Studies indicate that up to 25% of the population have the genetic capability of experiencing chronic inflammation to mold exposure, but it is unknown how many actually experience such symptoms due to frequent misdiagnosis. A 1993–94 case study based on cases of pulmonary hemorrhage in infants in Cleveland, Ohio originally concluded there was causal relationship between the exposure and the disease. The investigators revisited the cases and established that there was no link to the exposure to "S. chartrum" and the infants in their homes.
Exposure sources and prevention.
The main sources of mold exposure are from the indoor air in buildings with substantial mold growth, and from ingestion of food with mold growths.
Air.
Prevention of mold exposure and its ensuing health issues begins with prevention of mold growth in the first place by avoiding a mold-supporting environment such as humid air. Extensive flooding and water damage can support extensive mold growth. Following hurricanes, homes with greater flood damage, especially those with more than 3 ft of indoor flooding, demonstrated higher levels of mold growth compared with homes with little or no flooding. The aftermath of a hurricane is the worst-case scenario, but the concept of water damage supporting widespread mold growth is more generally applicable.
It is useful to perform an assessment of the location and extent of the mold hazard in a structure. Various practices of remediation can be followed to mitigate mold issues in buildings, the most important of which is to reduce moisture levels. Removal of affected materials after the source of moisture has been reduced and/or eliminated may be necessary. Thus, the concept of mold growth, assessment, and remediation is essential in prevention of mold health issues.
A common issue with mold hazards in the household is the placement of furniture, and the lack of ventilation which this causes to certain parts of the wall. The simplest method of avoiding mold in a home so affected is to move the furniture in question.
Adverse respiratory health effects are associated with occupancy in buildings with moisture and mold damage.
Molds may excrete liquids or low-volatility gases, but the concentrations are so low that frequently they cannot be detected even with sensitive analytical sampling techniques. Sometimes these by-products are detectable by odor, in which case they are referred to as "ergonomic odors" meaning the odors are detectable, but do not indicate toxicologically significant exposures.
Food.
Molds that are often found on meat and poultry include members of the genera "Alternaria", "Aspergillus", "Botrytis", "Cladosporium", "Fusarium", "Geotrichum", "Mortierella", "Mucor", "Neurospora","Paecilomyces", "Penicillium" and "Rhizopus". Grain crops in particular incur considerable losses both in field and storage due to pathogens, post-harvest spoilage and insect damage. A number of common microfungi are important agents of post-harvest spoilage, notably members of the genera "Aspergillus", "Fusarium" and "Penicillium". A number of these produce mycotoxins (soluble, non-volatile toxins produced by a range of microfungi that demonstrate specific and potent toxic properties on human and animal cells) that can render foods unfit for consumption. When ingested, inhaled, or absorbed through skin, mycotoxins may cause or contribute to a range of effects from reduced appetite and general malaise to acute illness or death in rare cases. Mycotoxins may also contribute to cancer. Dietary exposure to the mycotoxin aflatoxin B1, commonly produced by growth of the fungus "Aspergillus flavus" on improperly stored ground nuts in many areas of the developing world, is known to independently (and synergistically with Hepatitis B virus) induce liver cancer. Mycotoxin-contaminated grain and other food products have a significantly impact on human and animal health globally. According to the World Health Organization, roughly 25% of the world's food may be contaminated by mycotoxins.
Prevention of mold exposure from food is generally to consume food that has no mold growths on it. Also, mold growth in the first place can be prevented by the same concept of mold growth, assessment, and remediation that prevents air exposure. In addition, it is especially useful to clean the inside of the refrigerator, and to ensure dishcloths, towels, sponges and mops are clean.
Ruminants are considered to have increased resistance to some mycotoxins, presumably due to the superior mycotoxin-degrading capabilities of their gut microbiota. The passage of mycotoxins through the food chain may also have important consequences on human health. For example, in China in December 2011, high levels of carcinogen aflatoxin M1 in Mengniu brand milk were found to be associated with the consumption of mold-contaminated feed by dairy cattle.
History.
In the 1930s, mold was identified as the cause behind the mysterious deaths of farm animals in Russia and other countries. "Stachybotrys chartarum" was found growing on wet grain used for animal feed. Illness and death also occurred in humans when starving peasants ate large quantities of rotten food grains and cereals that were heavily overgrown with the "Stachybotrys" mold.
In the 1970s, building construction techniques changed in response to changing economic realities including the energy crisis. As a result, homes and buildings became more airtight. Also, cheaper materials such as drywall came into common use. The newer building materials reduced the drying potential of the structures making moisture problems more prevalent. This combination of increased moisture and suitable substrates contributed to increased mold growth inside buildings.
Today, the US Food and Drug Administration and the agriculture industry closely monitor mold and mycotoxin levels in grains and foodstuffs in order to keep the contamination of animal feed and human food supplies below specific levels. In 2005 Diamond Pet Foods, a US pet food manufacturer, experienced a significant rise in the number of corn shipments containing elevated levels of aflatoxin. This mold toxin eventually made it into the pet food supply, and dozens of dogs and cats died before the company was forced to recall affected products.
Litigation.
In newer buildings where wallboard is often used for interior walls, the paper backing provides a receptive environment for mold if the paper becomes wet. Mold can also feed on damp sheetrock, wallpaper glue, wood, and other substances. The worst possible outcome is that people can develop an allergic respiratory disease.
In 1999, an Austin, Texas, woman was awarded $32 million when she sued her insurer over mold damage in her 22-room mansion.
In 2001, a jury awarded a couple and their eight-year-old son $2.7 million, plus attorney’s fees and costs, in a toxic mold-related personal injury lawsuit against the owners and managers of their apartment in Sacramento, California. In 2002, the U.S. International Trade Commission reported that according to one estimate, US insurers paid over $3 billion in claims relating to mold lawsuits, an increase from the $1.3 billion paid the prior year.
In 2003, comedian, game show host, and Johnny Carson's sidekick on "The Tonight Show" Ed McMahon received $7.2 million from insurers and others to settle his lawsuit alleging that toxic mold in his Beverly Hills home sickened him and his wife, and killed their dog. That same year environmental activist Erin Brockovich received settlements of $430,000 from two parties and an undisclosed amount from a third party to settle her lawsuit alleging toxic mold in her Agoura Hills, California, home. That year, according to the Insurance Information Institute, a US industry organization in New York, there were over 10,000 mold-related lawsuits pending in state courts across the United States. Most were filed in states such as Florida, California, and Texas, but some New York lawyers said that multimillion dollar jury awards in some cases seemed to be inspiring an increasing number of mold-related lawsuits in New York.
By 2004, many mold litigation settlements were for amounts well past $100,000. In 2005, the U.S. International Trade Commission reported that toxic mold showed signs of being the "new asbestos" in terms of claims paid by the environmental insurance industry.
In 2006, a Manhattan Beach, California family received a $22.6 million settlement in a toxic mold case. The family had asserted that that moldy lumber had caused severed medical problems in their child. That same year, Hilton Hotels received $25 million in settlement of its lawsuit over mold growth in the Hilton Hawaiian Village's Kalia Tower. 
In 2010, a jury awarded a father $1.2 million in damages in a lawsuit against his landlord for neglecting to repair his mold-infested Laguna Beach, California, home. According to the lawsuit, the mold caused the man's daughter to develop severe respiratory problems.
In 2011, New York City housing inspectors issued 15,942 violations for mold-related conditions, a 19% increase from 2007. The most serious violations rose by 67% during the same period. That year, in North Pocono, Pennsylvania, a jury awarded two homeowners $4.3 million in a toxic mold verdict (consisting of $3.4 million for post-traumatic stress disorder and loss of life's pleasures in the use of the property, and $900,000 for loss of their property and medical expenses).
In 2012, a key appellate court decision in Manhattan by a five-judge panel found held the scientific literature on mold causing illness was now "indicative of a causal relationship."

</doc>
<doc id="42677" url="http://en.wikipedia.org/wiki?curid=42677" title="Celtic Tiger">
Celtic Tiger

Celtic Tiger (Irish: "An Tíogar Ceilteach") refers to the economy of the Republic of Ireland between 1995 and 2000, a period of rapid real economic growth fuelled by foreign direct investment, and a subsequent property price bubble which rendered the real economy uncompetitive. The Irish economy expanded at an average rate of 9.4% between 1995 and 2000 and continued to grow at an average rate of 5.9% during the following decade until 2008, when it fell into recession.
Many commentators fail to distinguish between the period of real growth in the export-oriented technology and pharmaceutical sectors which created the "Celtic Tiger", and the bubble period of property-price inflation that yielded high levels of transaction-based tax revenue and triggered unsustainable levels of current budget expenditure.
The economy underwent a dramatic reversal from 2008, with GDP contracting by 14% and unemployment levels rising to 14% by 2011.
Term.
The colloquial term "Celtic Tiger" has been used to refer to the country itself, and to the years associated with the boom. The first recorded use of the phrase is in a 1994 Morgan Stanley report by Kevin Gardiner. The term refers to Ireland's similarity to the East Asian Tigers: Hong Kong, Singapore, South Korea, and Taiwan during their periods of rapid growth in the early 1960s and late 1990s. "An Tíogar Ceilteach", the Irish language version of the term, appears in the official terminology database and has been used regularly in government and administrative contexts since at least 2005.
The Celtic Tiger period has also been called "The Boom" or "Ireland's Economic Miracle". During that time, the country experienced a period of economic growth that transformed it from one of Europe's poorer countries into one of its wealthiest. The causes of Ireland's growth are the subject of some debate, but credit has been primarily given to state-driven economic development; social partnership among employers, government and unions; increased participation by women in the labour force; decades of investment in domestic higher education; targeting of foreign direct investment; a low corporation tax rate; an English-speaking workforce; and membership of the European Union which provided transfer payments and export access to the Single Market.
By mid-2007, in the wake of the growing global financial crisis, the Celtic Tiger had all but died. Some critics, such as David McWilliams, who had been warning about impending collapse for some time, concluded: "The case is clear: an economically challenged government, perniciously influenced by the interests of the housing lobby, blew it. The entire Irish episode will be studied internationally in years to come as an example of how not to do things."
Historian Richard Aldous stated the Celtic Tiger has now gone the "way of the dodo". In early 2008, many commentators thought a soft landing was likely, but by January 2009, it seemed possible the country could experience a depression. In early January 2009, "The Irish Times", in an editorial, declared:
"We have gone from the Celtic Tiger to an era of financial fear with the suddenness of a Titanic-style shipwreck, thrown from comfort, even luxury, into a cold sea of uncertainty." In February 2010, a report by Davy Research concluded that Ireland had "largely wasted” its years of high income during the boom, with private enterprise investing its wealth "in the wrong places", and compared Ireland's growth to other small Euro zone countries such as Finland and Belgium – noting that the physical wealth of those countries exceeds that of Ireland because of their "vastly superior" transport infrastructure, telecommunications network, and public services.
Tiger economy.
From 1995 to 2000, GDP growth rate ranged between 7.8 and 11.5%; it then slowed to between 4.4 and 6.5% from 2001 to 2007. During that period, the Irish GDP per capita rose dramatically to equal, then eventually surpass, that of all but one state in Western Europe. Although GDP does not represent the standard of living, and the GNP remained lower than the GDP, in 2007, the GNP achieved the same level as of some other Western European countries'.
Causes.
Tax policy.
Many economists credit Ireland's growth to a low corporate taxation rate (10 to 12.5% throughout the late 1990s). Since 1956, successive Irish governments have pursued low-taxation policies.
European Union Structural and Cohesion Funds.
Since joining the EU in 1973, Ireland has received over €17 billion in EU Structural and Cohesion Funds. These are made up of the European Regional Development Fund (ERDF) and the European Social Fund (ESF) and were used to increase investment in the education system and to build physical infrastructure. These transfer payments from members of the European Union, such as Germany and France, were as high as 4% of Ireland's gross national product (GNP). Ireland is unique among cohesion countries, having allocated up to 35% of its Structural Funds to human resource investments, compared with an average of around 25% for other cohesion fund recipients. The Irish economy's increased productive capacity is sometimes attributed to these investments, which made Ireland more attractive to high-tech businesses, though the libertarian Cato Institute has suggested that the EU transfer payments were economically inefficient and may have actually slowed growth. The conservative Heritage Foundation also attributed to transfer payments no significant role in causing growth.
Trade within the European Union.
Ireland's membership in the EU since 1973 helped the country gain access to Europe's large markets. Ireland's trade had previously been predominantly with the United Kingdom.
Industrial policies.
In the 1990s, the provision of subsidies and investment capital by Irish state organisations (such as IDA Ireland) encouraged high-profile companies, such as Dell, Intel, and Microsoft, to locate in Ireland; these companies were attracted to Ireland because of its EU membership, relatively low wages, government grants, and low tax rates. Enterprise Ireland, a state agency, provides financial, technical, and social support to start-up businesses. Additionally, the building of the International Financial Services Centre in Dublin led to the creation of 14,000 high-value jobs in the accounting, legal, and financial management sectors.
In July 2003, the government established the Science Foundation Ireland on a statutory basis to promote education for highly skilled careers, particularly in biotechnology and information and communications technology, with the additional purpose to invest in science initiatives that aim to further Ireland's knowledge economy.
Geography and demographics.
A favourable time zone difference allows Irish and British employees to work the first part of each day while US workers sleep. US firms were drawn to Ireland by cheap wage costs compared to the UK, and by the limited government intervention in business compared to other EU members, and particularly to countries in Eastern Europe. Growing stability in Northern Ireland brought about by the Good Friday Agreement further established Ireland's ability to provide a stable business environment.
Irish workers can communicate effectively with Americans – especially compared to those in other low-wage, non-English-speaking EU nations, such as Portugal and Spain; this factor was vital to U.S. companies' choosing Ireland for their European headquarters. It has also been argued that the demographic dividend from the rising ratio of workers to dependents due to falling fertility, and increased female labour market participation, increased income per capita.
Impact of economic growth.
Ireland was transformed from one of the poorest countries in Western Europe to one of the wealthiest. Disposable income soared to record levels, enabling a huge rise in consumer spending with foreign holidays accounting for over 91% of total holiday expenditure in 2004. However, the gap between the highest and lowest income households widened in the five-year period to 2004-2005; in response, the Economic and Social Research Institute (ESRI) stated in 2002: "On balance, budgets over the past 10 to 20 years have been more favourable to high income groups than low income groups, but particularly so during periods of high growth". Unemployment fell from 18% in the late 1980s to 4.5% by the end of 2007, and average industrial wages grew at one of the highest rates in Europe. Inflation brushed 5% per annum towards the end of the "Tiger" period, pushing Irish prices up to those of Nordic Europe, even though wage rates are roughly the same as in the UK. The national debt had remained constant during the boom, but the GDP to debt ratio rose, due to the dramatic rise in GDP.
The new wealth resulted in large investments in modernising Irish infrastructure and cities. The National Development Plan led to improvements in roads, and new transport services were developed, such as the Luas light rail lines, the Dublin Port Tunnel, and the extension of the Cork Suburban Rail. Local authorities enhanced city streets and built monuments such as the Spire of Dublin.
Ireland's trend of net emigration was reversed as the republic became a destination for immigrants. This significantly changed Irish demographics and resulted in expanding multiculturalism, particularly in the Dublin, Cork, Limerick, and Galway areas. It was estimated in 2007 that 10% of Irish residents were foreign-born; most of the new arrivals were citizens of Poland and the Baltic states, many of whom found work in the retail and service sectors. A study conducted in 2006 found that many Irish people regarded immigration as an important factor for economic progress. Within Ireland, many young people left the rural countryside to live and work in urban centres.
Many people in Ireland believe that the growing consumerism during the boom years eroded the country's culture, with the adoption of American capitalist ideals. While Ireland's historical economic ties to the UK had often been the subject of criticism, Peader Kirby argued that the new ties to the US economy were met with a "satisfied silence". Nevertheless, voices on the political left have decried the "closer to Boston than Berlin" philosophy of the Fianna Fail-Progressive Democrat government. Writers such as William Wall, Mike McCormick, and Gerry Murphy have satirised these developments. Growing wealth was blamed for rising crime levels among youths, particularly alcohol-related violence resulting from increased spending power. However, it was also accompanied by rapidly increased life expectancy and very high quality of life ratings; the country ranked first in "The Economist"'s 2005 quality of life index.
The growing success of Ireland's economy encouraged entrepreneurship and risk-taking, qualities that had been dormant during poor economic periods. However, whilst some semblance of a culture of entrepreneurship exists, foreign-owned companies account for 93% of Ireland's exports.
Downturn, 2001–2003.
The Celtic Tiger's momentum slowed sharply in 2002, after seven years of high growth. The Irish economic downturn was in line with the worldwide downturn.
The economy was impacted by a large reduction in investment in the worldwide information technology (IT) industry. The industry had over-expanded in the late 1990s, and its stock market equity declined sharply. Ireland was a major player in the IT industry: in 2002, it had exported US$10.4 billion worth of computer services, compared to $6.9 billion from the US. Ireland accounted for approximately 50% of all mass-market packaged software sold in Europe in 2002 (OECD, 2002; OECD, 2004).
Foot and mouth disease and the 11 September 2001 attacks damaged Ireland's tourism and agricultural sectors, deterring U.S. and British tourists. Several companies moved operations to Eastern Europe and the People's Republic of China because of a rise in Irish wage costs, insurance premiums, and a general reduction in Ireland's economic competitiveness. The rising value of the Euro hit non-EMU exports, particularly those to the U.S. and the United Kingdom.
At the same time, economies globally experienced a slowdown. The US economy grew only 0.3% in April, May, and June 2002 from a year earlier, and the Federal Reserve made 11 rate cuts that year in an attempt to stimulate the US economy. The EU scarcely grew throughout the whole of 2002, and many members' governments (notably in Germany and France) lost control of public finances, causing large deficits that broke the terms of the EMU Stability and Growth Pact.
The economic downturn in Ireland was not a recession but a slowdown in the rate of economic expansion. Signs of a recovery became evident in late 2003, as US investment levels increased once again. Many senior economists have heavily criticised the government for the economic imbalance in favour of the construction industry, and the prospect of sustaining economic growth in the future.
Post-2003 resurgence.
After the slowdown in 2001 and 2002, Irish economic growth began to accelerate again in late 2003 and 2004. Some of the media considered that an opportunity to document the return of the Celtic Tiger – occasionally referred to in the press as the "Celtic Tiger 2" and "Celtic Tiger Mark 2". In 2004, Irish growth was the highest, at 4.5%, of the EU-15 states, and a similar figure was forecast for 2005. Those rates contrast with growth rates of 1% to 3% for many other European economies, including France, Germany, and Italy. The pace of expansion in lending to households from 2003-2007 was among the highest in the euro area
In 2006, there was a surge in Foreign Direct Investment and a net increase of 3,795 in IDA supported jobs, with International and Financial Services having the highest growth rate. The reasons for the continuation of the Irish economic boom were somewhat controversial within Ireland. Some Economists, Civil Rights Activists and Social Commentators have said that the growth throughout this period was merely due to a great increase in property values, and to catch-up growth in employment in the construction sector.
Globally, the U.S. recovery boosted Ireland's economy due to Ireland's close economic ties to the US. The decline in tourism as a result of foot and mouth disease and the 11 September 2001 attacks had reversed itself. The recovery of the global information technology industry was also a factor; Ireland produced 25% of all European PCs, and Apple, Dell (whose major European manufacturing plant was in Limerick), HP, and IBM all had sizeable Irish operations.
There had been a renewed investment by multinational firms. Intel had resumed its Irish expansion, Google created an office in Dublin, Abbott Laboratories was building a new Irish facility, and Bell Labs planned to open a future facility.
Domestically, a new state body, Science Foundation Ireland, was established to promote new science companies in Ireland Maturing funds from the SSIA government savings scheme relaxed consumers' concerns about spending and thus fueled retail sales growth.
In September 2009, Tánaiste Mary Coughlan said Ireland had lost ground in international competitiveness every year since 2000.
Challenges.
Property market.
The return of the boom in 2004 was claimed to be primarily the result of the large construction sector's catching up with the demand caused by the first boom. The construction sector represented nearly 12% of GDP and a large proportion of employment among young, unskilled men. A number of sources, including "The Economist," warned of excessive Irish property values. 2004 saw the construction of 80,000 new homes, compared to the UK's 160,000 – a nation that has 15 times Ireland's population. House prices doubled between 2000 and 2006; tax incentives were a key driver of this price rise, and the Fianna Fáil-Progressive Democrats government subsequently received substantial criticism for these policies.
In January 2009, UCD economist Morgan Kelly predicted that house prices would fall by 80% from peak to trough in real terms.
Loss of competitiveness.
Rising wages, inflation, and excessive public spending led to a loss of competitiveness in the Irish economy. Irish wages were substantially above the EU average, particularly in the Dublin region, though many poorer Eastern European states had joined the EU since 2004, substantially lowering the average EU wage below its 1995 level. Low-paid sectors, such as retail and hospitality, remained below the EU-15 average, however. The pressures primarily affect unskilled, semi-skilled, and manufacturing jobs. Outsourcing of professional jobs also increased, with Poland in 2008 gaining several hundred former Irish jobs from the accountancy divisions of Philips and Dell.
Promotion of indigenous industry.
One of the major challenges facing Ireland is the successful promotion of indigenous industry. Although Ireland boasted a few large international companies, such as AIB, CRH, Élan, Kerry Group, Ryanair, and Smurfit Kappa Group, there are few companies with over one billion euros in annual revenue. The government has charged Enterprise Ireland with the task of boosting Ireland's indigenous industry and launched a website in 2003 with the objective of streamlining and marketing the process of starting a business in Ireland.
Reliance on foreign energy sources.
Ireland relies on imported fossil fuels for over 80% of its energy. Ireland for many years in the middle twentieth century limited its dependence on external energy sources by developing its peat bogs, building various hydroelectric projects, including a dam at Ardnacrusha on the River Shannon in 1928, developing offshore gas fields, and diversifying into coal in the 1970s. As gas, peat, and hydroelectric power have been almost fully exploited in Ireland, there is a continuously increasing need for imported fossil fuels at a time of increasing concerns about security of supply and global warming. One solution is to develop alternative energy sources, including wind power and, to a lesser extent, wave power. Wind,however, is not a panacea as it needs to have conventional plants to augment it. An offshore wind farm is currently under construction off the east coast near Arklow, and many remote locations in the west show potential for wind farm development. A report by Sustainable Energy Ireland indicated that if wind power were properly developed, Ireland could one day be exporting excess wind power if the natural difficulties of integrating wind power into the national grid are solved. Wind power by November 2009 already accounted for 15.4% of total installed generating capacity in the state. By 2020, the Irish government forecasts that 40% of the country's energy needs will come from renewable sources, well above the EU average.
Distribution of wealth.
Ireland's new wealth is unevenly distributed. The United Nations reported in 2004 that Ireland was second only to the US in inequality among Western nations. There is some opposition to the theory that Ireland's wealth has been unusually unevenly distributed, among them economist and journalist David McWilliams. He cites Eurostat figures which indicate that Ireland is just above average in terms equality by one type of measurement. However, while it is better off by this measurement than generally less developed or more free market countries like Britain, the Mediterranean, and the new accession states, Ireland is still more unequal than France, Germany, and the Scandinavan countries. Moreover, Ireland's inequality persists by other measurements. According to an ESRI report published in December 2006, Ireland's child poverty level ranks 22nd out of the 26 richest countries, and it is the 2nd most unequal country in Europe.
Banking scandals.
The "New York Times" in 2005 described Ireland as the "Wild West of European finance", a perception that helped prompt the creation of the Irish Financial Services Regulatory Authority. Despite its mandate for stricter oversight, the agency never imposed major sanctions on any Irish institution, even though Ireland had experienced several major banking scandals in overcharging of their customers. Industry representatives disputed the idea that Ireland may be home to unchecked financial frauds. In December 2008, irregularities in directors loans that had been kept off one bank's balance sheet for eight years forced the resignation of the financial regulator. Economic commentator David McWilliams has described the collapse of Anglo Irish Bank as Ireland's Enron.
Death of the Tiger.
In an economic analysis, the Economic and Social Research Institute (ESRI) on 24 June 2008 forecasted the possibility the Irish economy would experience marginal negative growth in 2008. This would be the first time since 1983. Outlining possible prospects for the economy for 2008, the ESRI said output of goods and services might fall that year—which would have been the Irish definition of a mild recession. It also predicted a recovery in 2009 and 2010.
In September 2008, Ireland became the first eurozone country to officially enter recession. The recession was confirmed by figures from the Central Statistics Office showing the bursting of the property bubble and a collapse in consumer spending that terminated the boom that was the Celtic Tiger. The figures show the gross domestic product (GDP), which measures the value of all the goods and services produced in the State, fell 0.8% in the second three months of 2008 compared with the same quarter of 2007. That was the second successive quarter of negative economic growth, which is the definition of a recession.
In a November 2008 interview in" Hot Press," in a grim assessment of where Ireland stood, then Taoiseach Brian Cowen said many people still did not realise how badly shaken the public finances were.
By 30 January 2009, Ireland’s government debt had become the riskiest in the euro zone, surpassing Greece’s sovereign bonds, according to credit-default swap prices. In February 2009, Taoiseach Brian Cowen said that Ireland's economy appeared on course to contract by 6.5% in 2009.
Aftermath.
Former Taoiseach Garret FitzGerald has blamed Ireland's dire economic state in 2009 on a series of "calamitous" government policy errors. Between the years of 2000 and 2003 the then Finance Minister Charlie McCreevy boosted public spending by 48% while cutting income tax. A second problem occurred when government policies allowed, or even encouraged, a housing bubble to develop, "on an immense scale". However, he wrote nothing of the impact of the European Central Bank's low interest rates which funded the property bubble and further exacerbated the overheating economy
Nobel laureate Paul Krugman had a bleak prediction,
"“As far as responding to the recession goes, Ireland appears to be really, truly without options, other than to hope for an export-led recovery, if and when the rest of the world bounces back.”"
The International Monetary Fund in mid-April 2009 forecast a very poor outlook for Ireland. It projected that the Irish economy would contract by 8 per cent in 2009 and by 3 per cent in 2010 – and that might be on the optimistic side.
Unemployment in Ireland was forecasted to rise almost 17 percent in 2010, the Economic and Social Research Institute (ESRI) stated in a report published on 28 April 2009, however the unemployment rate in 2010 steadied at 14%. In 2012, the unemployment rate was at 14.8 percent, and in order to escape economic downfall, Ireland requested €67.5 billion ($85.7 billion) from the International Monetary Fund and members of the euro area. Taking the money meant accepting austerity: The government has cut expenditure by 15 percent over three years, consumer spending has dropped for six straight quarters, and young Irish by the thousands have emigrated to Australia and elsewhere.
On 19 November 2010, The Irish Government had begun talks on a multi-billion-dollar economic assistance package with experts from the International Monetary Fund (IMF) and the European Union.
In mid-2011 the ratings agency Moody's proceeded to downgrade Ireland's government bond ratings to junk.
Celtic Phoenix.
From 2014, the Irish economy, including the construction sector, started to show signs of recovery. The Irish economy grew by 4.8% in 2014, and the "Irish Independent" described the economic recovery as a "Celtic Phoenix" - referring to a phoenix rising from the ashes of the Celtic Tiger. Following this growth, the Irish national debt fell to 109% of GDP, and the budget deficit fell to 3.1% in the fourth quarter of 2014.
References.
http://www.economist.com/media/pdf/QUALITY_OF_LIFE.pdf

</doc>
<doc id="42678" url="http://en.wikipedia.org/wiki?curid=42678" title="1630s BC">
1630s BC


</doc>
<doc id="42679" url="http://en.wikipedia.org/wiki?curid=42679" title="1640s BC">
1640s BC


</doc>
<doc id="42680" url="http://en.wikipedia.org/wiki?curid=42680" title="1690s BC">
1690s BC


</doc>
<doc id="42681" url="http://en.wikipedia.org/wiki?curid=42681" title="1680s BC">
1680s BC


</doc>
<doc id="42682" url="http://en.wikipedia.org/wiki?curid=42682" title="1670s BC">
1670s BC


</doc>
<doc id="42683" url="http://en.wikipedia.org/wiki?curid=42683" title="1660s BC">
1660s BC


</doc>
<doc id="42684" url="http://en.wikipedia.org/wiki?curid=42684" title="1610s BC">
1610s BC


</doc>
<doc id="42685" url="http://en.wikipedia.org/wiki?curid=42685" title="1620s BC">
1620s BC


</doc>
<doc id="42686" url="http://en.wikipedia.org/wiki?curid=42686" title="1600s BC (decade)">
1600s BC (decade)


</doc>
<doc id="42687" url="http://en.wikipedia.org/wiki?curid=42687" title="1650s BC">
1650s BC


</doc>
<doc id="42691" url="http://en.wikipedia.org/wiki?curid=42691" title="André Malraux">
André Malraux

André Malraux DSO (]; 3 November 1901 – 23 November 1976) was a French novelist, art theorist and Minister of Cultural Affairs. Malraux's novel "La Condition Humaine" (Man's Fate) (1933) won the Prix Goncourt. He was appointed by President Charles de Gaulle as Minister of Information (1945–1946) and subsequently as France's first Minister of Cultural Affairs during de Gaulle's presidency (1959–1969).
Early years.
Malraux was born in Paris in 1901, the son of Fernand-Georges Malraux and Berthe Lamy (Malraux). His parents separated in 1905 and eventually divorced. There are suggestions that Malraux's paternal grandfather committed suicide in 1909.
Malraux was raised by his mother, maternal aunt Marie and maternal grandmother, Adrienne Lamy-Romagna, who had a grocery store in the small town of Bondy. His father, a stockbroker, committed suicide in 1930 after the international crash of the stock market and onset of the Great Depression. From his childhood, associates noticed that André had marked nervousness and motor and vocal tics. The recent biographer Olivier Todd, who published a book on Malraux in 2005, suggests that he had Tourette's syndrome, although that has not been confirmed. Either way, most critics have not seen this as a significant factor in Malraux's life or literary works.
The young Malraux left formal education early, but he followed his curiosity through the booksellers and museums in Paris, and explored its rich libraries as well.
Marriage and family.
In 1922, Malraux married Clara Goldschmidt. Malraux and his first wife separated in 1938 but didn't divorce until 1947. His daughter from this marriage, Florence (b. 1933), married the filmmaker Alain Resnais.
After the breakdown of his marriage with Clara, Malraux lived with journalist and novelist Josette Clotis, starting in 1933. Malraux and Josette had two sons: Pierre-Gauthier (1940–1961) and Vincent (1943–1961). During 1944, while Malraux was fighting in Alsace, Josette died, aged 34, when she slipped while boarding a train. His two sons died together in 1961 in an automobile accident.
In 1948, Malraux married a second time, to Marie-Madeleine Lioux, a concert pianist and the widow of his half-brother, Roland Malraux. They separated in 1966.
Subsequently, Malraux lived with Louise de Vilmorin in the Vilmorin family château at Verrières-le-Buisson, Essonne, a suburb southwest of Paris. Vilmorin was best known as a writer of delicate but mordant tales, often set in aristocratic or artistic milieu. Her most famous novel was "Madame de...", published in 1951, which was adapted into the celebrated film "The Earrings of Madame de..." (1953), directed by Max Ophüls and starring Charles Boyer, Danielle Darrieux and Vittorio de Sica. Vilmorin's other works included "Juliette", "La lettre dans un taxi", "Les belles amours", "Saintes-Unefois", and "Intimités". Her letters to Jean Cocteau were published after the death of both correspondents. After Louise's death, Malraux spent his final years with her relative, Sophie de Vilmorin.
Career.
Early years.
Malraux's first published work, an article entitled "The Origins of Cubist Poetry", appeared in the magazine "Action" in 1920. This was followed in 1921 by three semi-surrealist tales, one of which, "Paper Moons", was illustrated by Fernand Léger. Malraux also frequented the Parisian artistic and literary milieux of the period, meeting figures such as Demetrios Galanis, Max Jacob, François Mauriac, Guy de Pourtalès, André Salmon, Jean Cocteau, Raymond Radiguet, Florent Fels, Pascal Pia, Marcel Arland, Edmond Jaloux, and Pierre Mac Orlan.
Indochina.
In 1923, aged 22, Malraux left for Cambodia with Clara. There, together with Clara and a friend, Louis Chevasson, he undertook a small expedition into unexplored areas of the Cambodian jungle in search of lost Khmer temples, hoping to recover items that might be sold to art museums. On his return, he was arrested by French colonial authorities for removing a "bas-relief" from "Banteay Srei". Malraux, who believed he had acted within the law as it then stood, contested the charges but was unsuccessful.
Malraux's experiences in Indochina led him to become highly critical of the French colonial authorities there. In 1925, with Paul Monin, a progressive lawyer, he helped to organize the Young Annam League and founded a newspaper "L'Indochine".
On his return to France, Malraux published "The Temptation of the West" (1926). The work was in the form of an exchange of letters between a Westerner and an Asian, comparing aspects of the two cultures. This was followed by his first novel "The Conquerors" (1928), and then by "The Royal Way" (1930) which reflected some of his Cambodian experiences. In 1933 Malraux published "Man's Fate" ("La Condition Humaine"), a novel about the 1927 failed Communist rebellion in Shanghai. The work was awarded the 1933 Prix Goncourt.
Spanish Civil War.
During the 1930s, Malraux was active in the anti-fascist Popular Front in France. At the beginning of the Spanish Civil War he joined the Republican forces in Spain, serving in and helping to organize the small Spanish Republican Air Force. (Curtis Cate, one of his biographers, claims that Malraux was slightly wounded twice during efforts to stop the Falangists' takeover of Madrid, but the historian Hugh Thomas claims otherwise.)
The French government sent aircraft to Republican forces in Spain, but they were obsolete by the standards of 1936. They were mainly Potez 540 bombers and Dewoitine D.372 fighters. The slow Potez 540 rarely survived three months of air missions, moving some 80 knots against enemy fighters flying at more than 250 knots. Few of the fighters proved to be airworthy, and they were delivered intentionally without guns or gunsights. (The Ministry of Defense of France had feared that modern types of planes would easily be captured by the Germans fighting for Francisco Franco, and the lesser models were a way of maintaining official "neutrality".) The planes were surpassed by more modern types introduced by the end of 1936 on both sides.
The Republic circulated photos of Malraux standing next to some Potez 540 bombers suggesting that France was on their side, at a time when France and the United Kingdom had declared official neutrality. But Malraux's commitment to the Republicans was personal, like that of many other foreign volunteers, and there was never any suggestion that he was there at the behest of the French Government. Malraux himself was not a pilot, and never claimed to be one, but his leadership qualities seem to have been recognized because he was made Squadron Leader of the 'España' squadron. Acutely aware of the Republicans' inferior armaments, of which outdated aircraft were just one example, he toured the United States to raise funds for the cause. In 1938 he published "L'Espoir" (Man's Hope), a novel influenced by his Spanish war experiences.
Malraux's participation in major historical events such as the Spanish Civil War inevitably brought him determined adversaries as well as strong supporters, and the resulting polarization of opinion has colored, and rendered questionable, much that has been written about his life. Fellow combatants praised Malraux's leadership and sense of camaraderie, although Antony Beevor says he was criticized by the representative of the Comintern who described him as an "adventurer" for his high profile and demands on the Spanish Republican government. (This criticism is not surprising, however, since the Comintern regularly criticized anyone who did not toe the party line, and despite his support for the Republicans, Malraux was never a member of the Communist Party.) Antony Beevor also claims that "Malraux stands out, not just because he was a mythomaniac in his claims of martial heroism – in Spain and later in the French Resistance – but because he cynically exploited the opportunity for intellectual heroism in the legend of the Spanish Republic." These statements are, however, very questionable since Malraux nowhere makes "claims of martial heroism". In fact, he rarely wrote about his own military experience and when he did (e.g. in his "Antimemoirs") placed very little emphasis on his own role.
Malraux's participation in events such as the Spanish Civil War has tended to distract attention from his important literary achievement. Malraux saw himself first and foremost as a writer and thinker (and not "man of action" as biographers so often portray him) but his extremely eventful life – a far cry from the stereotype of the French intellectual confined to his study or a Left Bank café – has tended to obscure this fact. As a result, his literary works, including his important works on the theory of art, have received less attention than one might expect, especially in Anglophone countries.
World War II.
At the beginning of the Second World War, Malraux joined the French Army. He was captured in 1940 during the Battle of France but escaped and later joined the French Resistance. In 1944, he was captured by the Gestapo. He later commanded the tank unit Brigade Alsace-Lorraine in defence of Strasbourg and in the attack on Stuttgart.
After the war, Malraux was awarded the "Médaille de la Résistance" and the Croix de guerre. The British awarded him the Distinguished Service Order, for his work with British liaison officers in Corrèze, Dordogne and Lot. After Dordogne was liberated, Malraux led a battalion of former resistance fighters to Alsace-Lorraine, where they fought alongside the First Army.
During the war, he worked on his last novel, "The Struggle with the Angel", the title drawn from the story of the Biblical Jacob. The manuscript was destroyed by the Gestapo after his capture in 1944. A surviving first section, titled "The Walnut Trees of Altenburg", was published after the war.
After the war.
Shortly after the war, General Charles de Gaulle appointed Malraux as his Minister for Information (1945–1946). Soon after, he completed his first book on art, "The Psychology of Art", published in three volumes (1947–1949). The work was subsequently revised and republished in one volume as "The Voices of Silence" ("Les Voix du Silence"), the first part of which has been published separately as "The Museum without Walls". Other important works on the theory of art were to follow. These included the three-volume "Metamorphosis of the Gods" and "Precarious Man and Literature", the latter published posthumously in 1977.
When de Gaulle returned to the French presidency in 1958, Malraux became France's first Minister of Cultural Affairs, a post he held from 1958 to 1969. Among many initiatives, he launched an innovative (and subsequently widely-imitated) program to clean the blackened facades of notable French buildings, revealing the natural stone underneath. He also created a number of "maisons de la culture" in provincial cities and worked to preserve France's national heritage.
In 1957, Malraux published the first volume of his trilogy on art entitled "The Metamorphosis of the Gods". The second two volumes (not yet translated into English) were published shortly before he died. They are entitled "L’Irréel" and "L'Intemporel" and discuss artistic developments from the Renaissance to modern times. Malraux also initiated the series "Arts of Mankind", an ambitious survey of world art that generated more than thirty large, illustrated volumes.
Malraux was an outspoken supporter of the Bangladesh liberation movement during the 1971 Pakistani Civil War and despite his age seriously considered joining the struggle.
During this post-war period, Malraux also published a series of semi-autobiographical works, the first entitled "Antimémoires" (1967). A later volume in the series, "Lazarus", is a reflection on death occasioned by his experiences during a serious illness. "La Tête d'obsidienne" (1974) (translated as "Picasso's Mask") concerns Picasso, and visual art more generally.
Malraux died in Créteil, near Paris, on 23 November 1976. He was buried in the Verrières-le-Buisson (Essonne) cemetery. In recognition of his contributions to French culture, his ashes were moved to the Panthéon in Paris during 1996, on the twentieth anniversary of his death.
Legacy and honours.
There is now a large and steadily growing body of critical commentary on Malraux's literary "œuvre", including his very extensive writings on art. Unfortunately, some of his works, including the last two volumes of "The Metamorphosis of the Gods" ("L'Irréel" and "L'Intemporel") are not yet available in English translation. Malraux's works on the theory of art contain a revolutionary approach to art that challenges the Enlightenment tradition that treats art simply as a source of "aesthetic pleasure". However, as French writer André Brincourt has commented, Malraux's books on art have been "skimmed a lot but very little read" (this is especially true in Anglophone countries) and the radical implications of his thinking are often missed. A particularly important aspect of Malraux's thinking about art is his explanation of the capacity of art to transcend time. In contrast to the traditional notion that art endures because it is timeless ("eternal"), Malraux argues that art lives on through metamorphosis – a process of resuscitation (where the work had fallen into obscurity) and transformation in meaning.
Quotations.
""Man is dead", after God". Malraux, "The Temptation of the West". (1926)
‘The artist is not the transcriber of the world, he is its rival.’ Malraux, "L'Intemporel" (3rd volume of "The Metamorphosis of the Gods".)
'In a world in which everything is subject to the passing of time, art alone is both subject to time and yet victorious over it'. "Malraux in a television program about art, 1975".
"Art is an object lesson for the gods." "The Voices of Silence"
"The art museum is one of the places that give us the highest idea of man." "The Voices of Silence"
"Humanism does not consist in saying: ‘No animal could have done what I have done,’ but in declaring: ‘We have refused what the beast within us willed to do, and we seek to reclaim man wherever we find that which crushes him.’" "The Voices of Silence"
"The greatest mystery is not that we have been flung at random between this profusion of matter and the stars, but that within this prison we can draw from ourselves images powerful enough to deny our nothingness." "Les Noyers de l'Altenburg"
Bibliography.
For a more complete bibliography, see site littéraire André Malraux.

</doc>
<doc id="42693" url="http://en.wikipedia.org/wiki?curid=42693" title="Upper and lower bounds">
Upper and lower bounds

In mathematics, especially in order theory, an upper bound of a subset "S" of some partially ordered set ("K", ≤) is an element of "K" which is greater than or equal to every element of "S". The term lower bound is defined dually as an element of "K" which is less than or equal to every element of "S". A set with an upper bound is said to be bounded from above by that bound, a set with a lower bound is said to be bounded from below by that bound. The terms bounded above (bounded below) are also used in the mathematical literature for sets that have upper (respectively lower) bounds.
Properties.
A subset "S" of a partially ordered set "P" may fail to have any bounds or may have many different upper and lower bounds. By transitivity, any element greater than or equal to an upper bound of "S" is again an upper bound of "S", and any element less than or equal to any lower bound of "S" is again a lower bound of "S". This leads to the consideration of least upper bounds (or "suprema") and greatest lower bounds (or "infima").
The bounds of a subset "S" of a partially ordered set "K" may or may not be elements of "S" itself. If "S" contains an upper bound then that upper bound is unique and is called the greatest element of "S". The greatest element of "S" (if it exists) is also the least upper bound of "S". A special situation does occur when a subset is equal to the set of lower bounds of its own set of upper bounds. This observation leads to the definition of Dedekind cuts.
The empty subset ∅ of a partially ordered set "K" is conventionally considered to be both bounded from above and bounded from below with every element of "P" being both an upper and lower bound of ∅.
Examples.
5 is a lower bound for the set { 5, 10, 34, 13934 }, but 8 is not. 42 is both an upper and a lower bound for the set { 42 }; all other numbers are either an upper bound or a lower bound for that set.
Every subset of the natural numbers has a lower bound, since the natural numbers have a least element (0, or 1 depending on the exact definition of natural numbers). An infinite subset of the natural numbers cannot be bounded from above. An infinite subset of the integers may be bounded from below or bounded from above, but not both. An infinite subset of the rational numbers may or may not be bounded from below and may or may not be bounded from above.
Every finite subset of a non-empty totally ordered set has both upper and lower bounds.
Bounds of functions.
The definitions can be generalised to functions and even sets of functions.
Given a function f with domain D and a partially ordered set ("K", ≤) as codomain, an element "y" of K is an upper bound of f if "y" ≥ "f"("x") for each x in D. The upper bound is called "sharp" if equality holds for at least one value of x.
Function g defined on domain D and having the same codomain ("K", ≤) is an upper bound of f if "g"("x") ≥ "f"("x") for each x in D.
Function g is further said to be an upper bound of a set of functions if it is an upper bound of each function in that set.
The notion of lower bound for (sets of) functions is defined analogously, with ≤ replacing ≥.
Tight bounds.
An upper bound is said to be a "tight upper bound", a "least upper bound", or a "supremum" if no smaller value is an upper bound.
Similarly a lower bound is said to be a "tight lower bound", a "greatest lower bound", or an "infimum" if no greater value is a lower bound.

</doc>
<doc id="42694" url="http://en.wikipedia.org/wiki?curid=42694" title="Nabokov (surname)">
Nabokov (surname)

Nabokov is a surname. Notable people with the surname include:

</doc>
<doc id="42702" url="http://en.wikipedia.org/wiki?curid=42702" title="Gloster Meteor">
Gloster Meteor

The Gloster Meteor was the first British jet fighter and the Allies' first operational jet aircraft during the Second World War. The Meteor's development was heavily reliant on its ground-breaking turbojet engines, pioneered by Sir Frank Whittle and his company, Power Jets Ltd. Development of the aircraft itself began in 1940, although work on the engines had been under way since 1936. The Meteor first flew in 1943 and commenced operations on 27 July 1944 with No. 616 Squadron RAF. Nicknamed the "Meatbox", the Meteor was not a sophisticated aircraft in its aerodynamics, but proved to be a successful combat fighter.
Several major variants of the Meteor incorporated technological advances during the 1940s and 1950s. Thousands of Meteors were built to fly with the RAF and other air forces and remained in use for several decades. The Meteor saw limited action in the Second World War. Meteors of the Royal Australian Air Force (RAAF) provided a significant contribution in the Korean War. Several other operators such as Argentina, Egypt and Israel flew Meteors in later regional conflicts. Specialised variants of the Meteor were developed for use in photo-reconnaissance and as night fighters.
The Meteor was also used for research and development purposes and to break several aviation records. On 7 November 1945, the first official air speed record by a jet aircraft was set by a Meteor F.3 of 606 miles per hour (975 km/h). In 1946, this record was broken when a Meteor F.4 reached a speed of 616 mph (991 km/h). Other performance-related records were broken in categories including flight time endurance, rate of climb, and speed. On 20 September 1945, a heavily modified Meteor I, powered by two Rolls-Royce Trent turbine engines driving propellers, became the first turboprop aircraft to fly. On 10 February 1954, a specially adapted Meteor F.8, the "Meteor Prone Pilot", which placed the pilot into a prone position to counteract inertial forces, took its first flight.
In the 1950s, the Meteor became increasingly obsolete as more nations introduced jet fighters, many of these newcomers having adopted a swept wing instead of the Meteor's conventional straight wing; in RAF service, the Meteor was replaced by newer types such as the Hawker Hunter and Gloster Javelin. As of 2013, two Meteors, "WL419" and "WA638", remain in active service with the Martin-Baker company as ejection seat testbeds. Two further aircraft in the UK remain airworthy, as does another in Australia.
Development.
Origins.
The development of the turbojet-powered Gloster Meteor was a collaboration between the Gloster Aircraft Company and Sir Frank Whittle's firm, Power Jets Ltd. Frank Whittle formed Power Jets Ltd in March 1936 to develop his ideas of jet propulsion, Whittle himself serving as the company's chief engineer. For several years, attracting financial backers and aviation firms prepared to take on Whittle's radical ideas was difficult; in 1931, Armstrong-Siddeley had evaluated and rejected Whittle's proposal, finding it to be technically sound but at the limits of engineering capability. Securing funding was a persistently worrying issue throughout the early development of the engine. The first Whittle prototype jet engine, the Power Jets WU, began running trials in early 1937; shortly afterwards, both Sir Henry Tizard, chairman of the Aeronautical Research Committee, and the Air Ministry gave the project their support.
On 28 April 1939, Whittle made a visit to the premises of the Gloster Aircraft Company, where he met several key figures, such as George Carter, Gloster's chief designer. Carter took a keen interest in Whittle's project, particularly when he saw the operational Power Jets W.1 engine; Carter quickly made several rough proposals of various aircraft designs powered by the engine. Independently, Whittle had also been producing several proposals for a high-altitude jet-powered bomber; following the start of the Second World War and the Battle for France, a greater national emphasis on fighter aircraft arose. Power Jets and Gloster quickly formed a mutual understanding around mid-1939.
In spite of ongoing infighting between Power Jets and several of its stakeholders, the Air Ministry contracted Gloster to manufacture a prototype aircraft powered by one of Whittle's new turbojet engines in late 1939. The single-engined proof-of-concept Gloster E28/39, the first British jet-powered aircraft, conducted its maiden flight on 15 May 1941, flown by Gloster's Chief Test Pilot, Flight Lieutenant Philip "Gerry" Sayer. The success of the smaller E.28/39 proved the viability of jet propulsion, and Gloster pressed ahead with designs for a production fighter aircraft. Due to the limited thrust available from early jet engines, it was decided that subsequent production aircraft would be powered by a pair of turbojet engines.
In 1940, for a "military load" of 1500 lb, the RAE had advised that work on an aircraft of 8500 lb all-up weight, with a static thrust of 3,200 lb (14.2 kN) should be started, with an 11,000 lb (4,990 kg) design for the expected more powerful W.2 and axial engine designs. George Carter's calculations based on the RAE work and his own investigations was that a 8700 - aircraft with two or four 20 mm cannon and six 0.303 machine guns would have a top speed of 400–431 mph at sea level and 450–470 mph at 30,000 ft. In January 1941 Gloster were told by Lord Beaverbrook that the twin jet fighter was of "unique importance", and that the company was to stop work on a night-fighter being developed to Specification F.18/40.
Prototypes.
In August 1940, Carter presented Gloster's initial proposals for a twin-engined jet fighter with a nosewheel undercarriage. On 7 February 1941, Gloster received an order for twelve prototypes (later reduced to eight) under Specification F9/40. A letter of intent for the production of 300 of the new fighter, initially to be named "Thunderbolt," was issued on 21 June 1941; to avoid confusion with the USAAF Republic P-47 Thunderbolt which had been issued with the same name to the RAF in 1944, the aircraft's name was quickly changed to "Meteor." During the aircraft's secretive development, employees and officials made use of the codename "Rampage" to refer to the Meteor. Test locations and other key project information was similarly obscured.
Although taxiing trials were carried out in 1942, it was not until the following year that any flights took place due to production and approval holdups with the Power Jets W.2 engine powering the Meteor. Due to the delays at subcontractor Rover, who was struggling to manufacture the W.2 engines on schedule, on 26 November 1942, production of the Meteor was ordered to stop; considerable interest was shown in Gloster's E.1/44 proposal for a single-engine fighter, unofficially named Ace. Gloster continued development work on the Meteor and the production-stop order was itself turned over in favour of the construction of six (eventually increased to eight) F9/40 prototypes alongside three E.1/44 prototypes. Rover's responsibilities for development and production of the W.2B engine were also transferred to Rolls-Royce that year.
On 5 March 1943, the fifth prototype, serial "DG206", powered by two substituted de Havilland Halford H.1 engines owing to problems with the intended W.2 engines, became the first Meteor to become airborne at RAF Cranwell, piloted by Michael Daunt. On the initial flight, an uncontrollable yawing motion was discovered, which led to a redesigned larger rudder; however, no difficulties had been attributed to the groundbreaking turbojet propulsion. Only two prototypes flew with de Havilland engines because of the low flight endurance they were capable of providing. Before the first prototype aircraft had even undertaken its first flight, an extended order for 100 production-standard aircraft had already been placed by the RAF.
The first Whittle-engined aircraft, "DG205/G", flew on 12 June 1943 (later crashing during takeoff on 27 April 1944) and was followed by "DG202/G" on 24 July. "DG202/G" was later used for deck handling tests aboard aircraft carrier HMS "Pretoria Castle". "DG203/G" made its first flight on 9 November 1943, later becoming a ground instructional model. "DG204/G", powered by Metrovick F.2 engines, first flew on 13 November 1943; "DG204/G" was lost in an accident on 4 January 1944, the cause believed to have been an engine compressor failure due to overspeed. "DG208/G" made its debut on 20 January 1944, by which time the majority of design problems had been overcome and a production design had been approved. "DG209/G" was used as an engine testbed by Rolls-Royce, first flying on 18 April 1944. "DG207/G" was intended to be the basis for the Meteor F.2 with de Havilland engines, but it did not fly until 24 July 1945, at which time the Meteor 3 was in full production and de Havilland's attention was being redirected to the incoming de Havilland Vampire, thus the F.2 was cancelled.
Into production.
On 12 January 1944, the first Meteor F.1, serial "EE210/G", took to the air from Moreton Valence. It was essentially identical to the F9/40 prototypes except for the addition of four nose-mounted 20 mm (.79 in) Hispano Mk V cannons and some changes to the canopy to improve all-round visibility. Due to the F.1's similarity to the prototypes, they were frequently operated in the test program to progress British understanding of jet propulsion, and it took until July 1944 for the aircraft to enter squadron service. "EE210/G" was later sent to the U.S. for evaluation, where it was first flown at Muroc Army Airfield on 15 April 1944.
Originally 300 F.1s were ordered, but the total produced was reduced to 20 aircraft as the follow-on orders had been converted to the more advanced models. Some of the last major refinements to the Meteor's early design were trialed using this first production batch, and what was to become the long-term design of the engine nacelles was introduced upon "EE211". "EE215" was the first Meteor to be fitted with guns; "EE215" was also used in engine reheat trials, and was later converted into the first two-seat Meteor. Due to the radical differences between jet-powered aircraft and those that preceded, a special "Tactical Flight" or "T-Flight" unit was established to prepare the Meteor for squadron service, led by Group Captain Hugh Joseph Wilson. The Tactical Flight was formed at Farnborough in May 1944, the first Meteors arriving the following month, upon which both tactical applications and limitations were extensively explored.
On 17 July 1944, the Meteor F.1 was cleared for service use. Shortly afterwards, elements of the Tactical flight and their aircraft were transferred to operational RAF squadrons. The first deliveries to No. 616 Squadron RAF, the first operational squadron to receive the Meteor, began in July 1944. When the F.2 was cancelled, the Meteor F.3 became the immediate successor to the F.1 and alleviated some of the shortcomings of the F.1. In August 1944, the first F.3 prototype flew, early F.3 production aircraft were still fitted with the Welland engine as the Derwent engine's production line was only just starting at this point. A total of 210 F.3 aircraft were produced before they were in turn superseded by production of the Meteor F.4 in 1945.
Several Meteor F.3s were converted into navalised Meteors. The adaptations included a strengthened undercarriage and arrester hook. Operational trials of the type took place aboard HMS "Implacable". The trials included carrier landings and takeoffs. Performance of these naval prototype Meteors proved to be favorable, including takeoff performance, leading to further trials with a modified Meteor F.4 fitted with folding wings; a 'clipped wing' was also adopted. The Meteor later entered service with the Royal Navy, but only as a land-based trainer, the Meteor T.7, to prepare pilots of the Fleet Air Arm for flying other jet aircraft such as the de Havilland Sea Vampire.
While various marks of Meteor had been introduced by 1948, they had remained very similar to the prototypes of the Meteor; consequently, the performance of the Meteor F.4 was beginning to be eclipsed by new jet designs. Gloster therefore embarked on a redesign programme to produce a new version of the Meteor with better performance. Designated Meteor F.8, this upgraded variant was a potent fighter aircraft, forming the bulk of RAF Fighter Command between 1950 and 1955. The Meteor continued to be operated in a military capacity by several nations into the 1960s.
Night fighter.
In order to replace the increasingly obsolete de Havilland Mosquito as a night fighter, the Meteor was adapted to serve in the role as an interim aircraft. Gloster had initially proposed a night fighter design to meet the Air Ministry specification for the Mosquito replacement, based on the two seater trainer variant of the Meteor, with the pilot in the front seat and the navigator in the rear. Once accepted however, work on the project was swiftly transferred to Armstrong Whitworth to perform both the detailed design process and production of the type; the first prototype flew on 31 May 1950. Although based on the T.7 twin seater, it used the fuselage and tail of the F.8, and the longer wings of the F.3. An extended nose contained the AI Mk 10 (the 1940s Westinghouse SCR-720) Air Intercept radar. As a consequence the 20 mm cannons were moved into the wings, outboard of the engines. A ventral fuel tank and wing mounted drop tanks completed the "Armstrong Whitworth Meteor NF.11".
As radar technology developed, a new Meteor night fighter was developed to use the improved US-built APS-21 system. The "NF.12" first flew on 21 April 1953. It was similar to the NF 11 but had a nose section 17 inches (43.2 cm) longer; the fin was enlarged to compensate for the greater keel area of the enlarged nose and to counter the airframe reaction to the "wig-wag" scan of the radar which affected the gunsighting, an anti-tramp motor operating on the rudder was fitted midway up the front leading edge of the fin. The NF.12 also had the new Rolls-Royce Derwent 9 engines and the wings were reinforced to handle the new engine. Deliveries of the NF.12 started in 1953, with the type entering squadron service in early 1954, equipping seven squadrons (Nos 85, 25, 152, 46, 72, 153 and 64); the aircraft was replaced over 1958–59.
The final Meteor night fighter was the "NF.14". First flown on 23 October 1953, the NF.14 was based on the NF.12 but had an even longer nose, extended by a further 17 inches to accommodate new equipment, increasing the total length to 51 ft 4 in (15.65 m) and a larger bubble canopy to replace the framed T.7 version. Just 100 NF.14s were built; they first entered service in February 1954 beginning with No. 25 Squadron and were being replaced as early as 1956 with the Gloster Javelin. Overseas, they remained in service a little longer, serving with No. 60 Squadron at Tengah, Singapore until 1961. As the NF.14 was replaced, some 14 were converted to training aircraft as the "NF(T).14" and given to No. 2 Air Navigation School on RAF Thorney Island where they served until 1965.
Design.
Overview.
The first operational version of the Meteor, designated as the Meteor F.1, apart from the minor airframe refinements, was a straightforward 'militarisation' of the earlier F9/40 prototypes. The dimensions of the standard Meteor F.1 were 41 ft 3 in (12.58 m) long with a span of 43 ft 0 in (13.11 m), with an empty weight of 8,140 lb (3,823 kg) and a maximum takeoff weight of 13,795 lb (6,270 kg). Despite the revolutionary turbojet propulsion used, the design of the Meteor was relatively orthodox and did not take advantage of many aerodynamic features utilised on other jet fighters, such as swept wings; the Meteor shared a broadly similar basic configuration to its German equivalent, the Messerschmitt Me 262.
It was an all-metal aircraft with a tricycle undercarriage and conventional low, straight wings with mid-mounted turbojet engines and a high-mounted tailplane clear of the jet exhaust. The Meteor F.1 exhibited some problematic flying characteristics typical of early jet aircraft; it suffered from stability problems at high transonic speeds, large trim changes, high stick forces and self-sustained yaw instability (snaking) caused by airflow separation over the thick tail surfaces. The longer fuselage of the Meteor T.7, a two-seater trainer, significantly reduced the aerodynamic instability that the early Meteors were known for.
Later Meteor variants would see a large variety of changes from the initial Meteor F.1 introduced to service in 1944. Much attention was given to raising the aircraft's top speed, often by improving the airframe's aerodynamic qualities, incorporating the latest engine developments, and increasing the strength of the airframe. The Meteor F.8, which emerged in the late 1940s, was considered to have substantially improved performance over prior variants; the F.8 was reportedly the most powerful single-seat aircraft flying in 1947, capable of ascending to 40,000 feet within five minutes.
Construction.
From the outset, each Meteor was constructed from several modular sections or separately produced units; this was a deliberate design choice to allow for production to be dispersed and for easy disassembly for transport. Each aircraft comprised five main sections: nose, forward fuselage, central section, rear fuselage and tail units; the wings were also built out of lengthwise sections. The forward section contained the pressure cabin, gun compartments, and forward undercarriage. The center section incorporated much of the structural elements, including the inner wing, engine nacelles, fuel tank, ammunition drums, and main undercarriage. The rear fuselage was of a conventional semi-monocoque structure. Various aluminium alloys were the primary materials used throughout the structure of the Meteor, such as the stressed duralumin skin.
Across the Meteor's production life, various different companies were subcontracted to manufacture aircraft sections and major components; due to the wartime workload on producing fighter aircraft such as the Hawker Hurricane and Hawker Typhoon, neither Gloster nor the wider Hawker Siddeley Group were able to internally meet the production demand of 80 aircraft per month. Bristol Tramways produced the forward fuselage of the aircraft, the Standard Motor Company manufactured the central fuselage and inner wing sections, the Pressed Steel Company produced the rear fuselage, and Parnall Aircraft made the tail unit. Other main subcontractors included Boulton Paul Aircraft, Excelsior Motor Radiator Company, Bell Punch, Turner Manufacturing Company, and Charlesworth Bodies; many of these firms had little or no experience producing aircraft, both quality and interchangeability of components were maintained by contractually enforced adherence to Gloster's original drawings.
From the Meteor F.4 onwards, Armstrong Whitworth began completing whole units at their Coventry facility in addition to Gloster's own production line. Belgian aviation firm Avions Fairey would also produce the Meteor F.8 under license from Gloster for the Belgian Air Force; a similar license manufacturing arrangement was made with Dutch company Fokker to meet the Royal Netherlands Air Force's order.
Engines.
The "Meteor F.1" was powered by two Rolls-Royce Welland turbojet engines, Britain's first production jet engines, which were built under license from Whittle's designs. The Meteor embodied the advent of practical jet propulsion; in the type's service life, both military and civil aviation manufacturers would rapidly integrate turbine engines into their designs, favouring its advantages such as smoother running and greater power output. The Meteor's engines were considerably more practical than those of the German Me 262, having both a longer service life and being more efficient; unlike the Me 262, the engines were embedded into the wing in nacelles between the front and rear spars rather than underslung.
The W.2B/23C engines produced 1,700 lbf (7.58 kN) of thrust each, giving the aircraft a maximum speed of 417 mph (670 km/h) at 3,000 m and a range of 1,006 miles (1,610 km). It incorporated a hydraulically driven engine starter developed by Rolls-Royce, which was automated following the press of a starter button in the cockpit. The engines also drove hydraulic and vacuum pumps as well as a generator via a Rotol gearbox fixed on the forward wing spar; the cockpit was also heated by bleed air from one of the engines. The acceleration rate of the engines was manually controlled by the pilot; rapid engine acceleration would frequently induce compressor stalls early on; the likelihood of compressor stalls was effectively eliminated upon further design refinements of both the Welland engine and the Meteor itself. At high speeds, the Meteor had an unfortunate tendency to lose directional stability, often during unfavourable weather conditions, leading to a 'snaking' motion; this could be easily resolved by throttling back to reduce speed.
Based upon designs produced by Power Jets, Rolls-Royce produced more advanced and powerful turbojet engines. Beyond numerous improvements made to the Welland engine that powered the early Meteors; Rolls-Royce and Power Jets collaborated to develop the more capable Derwent engine. The Derwent engine was installed on many of the later production Meteors, the adoption of this new powerplant let to considerable performance increases. The Meteor often served as the basis for the development of other early turbojet designs; a pair of Meteor F.4s were sent to Rolls-Royce to aid in their experimental engine trials, "RA435" being used for reheat testing, and "RA491" being fitted with the Rolls-Royce Avon, an axial-flow engine. From their involvement in the development of the Meteor's engines, Armstrong-Siddeley, Bristol Aircraft, Metropolitan-Vickers, and de Havilland would also independently develop their own gas turbine engines.
Performance.
During development, skeptical elements of the Air Ministry had expected mature piston-powered aircraft types to exceed the capabilities of the Meteor in all respects except that of speed; thus, the performance of early Meteors was considered favourable for the interceptor mission, being capable of out-diving the majority of enemy aircraft. The conclusion of in-service trials conducted between the Meteor F.3. and the Hawker Tempest V was that the performance of the Meteor exceeded the Tempest in almost all respects and that, barring some manoeuvrability issues, the Meteor could be considered a capable all-round fighter. Pilots formerly flying piston-engine aircraft often described the Meteor as being exciting to fly. Ex-RAF pilot Norman Tebbit stated of his experience of the Meteor: "Get airborne, up with the wheels, hold it low until you were about 380 knots, pull it up and she would go up, well we thought then, like a rocket".
As a general rule, the jet engine consumes more fuel than its piston-engine counterparts; the fuel-hungry Welland engines imposed considerable limitations on the Meteor F.1, leading to the type being used for local interception duties only. In the post-war environment, there was considerable pressure to increase the range of interceptors to counter the threat of bombers armed with nuclear weapons. The long term answer to this question was the in-flight refuelling; several Meteors were provided to Flight Refuelling Limited for trials of the newly developed probe-and-drogue refuelling techniques. This capability was not rolled out to service Meteors however, having already been supplanted by more modern interceptor aircraft at this point.
A total of 890 Meteors were lost in RAF service (145 of these crashes occurred in 1953 alone), resulting in the deaths of 450 pilots. Contributory factors in the number of crashes were the high fuel consumption and consequent short flight endurance (less than one hour), causing pilots to run out of fuel, and difficult handling with one engine out due to the widely set engines. In May 1951 it was reported that the Meteor 4 tail unit lost half its strength when the skin tore, the skin tearing was found to originate round rivet holes, access panels or discontinuous stringers (stress risers). The casualty rate was exacerbated by the lack of ejection seats in early series Meteors; ejection seats would be fitted in the later F.8, FR.9, PR.10 and some experimental Meteors. The difficulty of bailing out of the Meteor has been noted by pilots during development, reporting several contributing design factors such as the limited size and relative position of the cockpit to the rest of the aircraft, and difficulty in using the two-lever jettisonable hood mechanism.
Operational service.
Second World War.
No. 616 Squadron RAF was the first to receive operational Meteors, a total of 14 aircraft were initially delivered. The squadron was based at RAF Culmhead, Somerset and had been previously equipped with the Spitfire VII. The conversion to the Meteor was initially a matter of great secrecy. Following a conversion course at Farnborough attended by the squadron's six leading pilots, the first aircraft was delivered to Culmhead on 12 July 1944. The squadron and its seven Meteors moved on 21 July 1944 to RAF Manston on the east Kent coast and, within a week, 32 pilots had been converted to the type.
The Meteor was initially used to counter the V-1 flying bomb threat. 616 Squadron Meteors saw action for the first time on 27 July 1944, when three aircraft were active over Kent. These were the first operational jet combat missions for the Meteor and for the Royal Air Force. After some problems, especially with jamming guns, the first two V1 "kills" were made on 4 August. By war's end, Meteors accounted for 14 flying bombs. After the end of the V-1 threat, and the introduction of the ballistic V-2 rocket, the RAF was forbidden to fly the Meteor on combat missions over German-held territory for fear of an aircraft being shot down and salvaged by the Germans.
No. 616 Squadron briefly moved to RAF Debden to allow USAAF bomber crews to gain experience and create tactics in facing jet-engined foes before moving to Colerne, Wiltshire. For a week from 10 October 1944 a series of exercises were carried out in which a flight of Meteors made mock attacks on a formation of 100 B-24s and B-17s escorted by 40 Mustangs and Thunderbolts. These suggested if the jet fighter attacked the formation from above it could take advantage of its superior speed in the dive to attack the bombers and then escape by diving through the formation before the escorts could react. The best tactic to counter this was to place a fighter screen 5,000 ft above the bombers and attempt to intercept the jets early in the dive. The exercise was also useful from No. 616 Squadron's perspective, gaining valuable practical experience in Meteor operations.
No. 616 Squadron exchanged its F.1s for the first "Meteor F.3"s on 18 December 1944. These first 15 F.3s differed from the F.1 in having a sliding canopy in place of the sideways hinging canopy, increased fuel capacity and some airframe refinements. They were still powered by Welland I engines. Later F.3s were equipped with the Derwent I engines. This was a substantial improvement over the earlier mark, although the basic design still had not reached its potential. Wind tunnel and flight tests demonstrated that the original short nacelles, which did not extend far fore and aft of the wing, contributed heavily to compressibility buffeting at high speed. New, longer nacelles not only cured some of the compressibility problems but added 120 km/h (75 mph) at altitude, even without upgraded powerplants. The last batch of Meteor F.3s featured the longer nacelles; other F.3s were retrofitted in the field with the new nacelles. The F.3 also had the new Rolls-Royce Derwent engines, increased fuel capacity, and a new larger, more strongly raked bubble canopy.
Judging the "Meteor F.3"s were ready for combat over Europe, the RAF finally decided to deploy them on the continent. On 20 January 1945, four Meteors from 616 Squadron were moved to Melsbroek in Belgium and attached to the Second Tactical Air Force, just under three weeks after the Luftwaffe's surprise Unternehmen Bodenplatte attack on New Year's Day, in which Melsbroek's RAF base, designated as Allied Advanced Landing Ground "B.58", had been struck by the piston-engined fighters of JG 27 and JG 54. The 616 Squadron Meteor F.3s' initial purpose was to provide air defence for the airfield, but their pilots hoped that their presence might provoke the Luftwaffe into sending Me 262s against them. At this point the Meteor pilots were still forbidden to fly over German-occupied territory, or to go east of Eindhoven, to prevent a downed aircraft being captured by the Germans or the Soviets.
In March, the entire squadron was moved to Gilze-Rijen and then in April, to Nijmegen. The Meteors flew armed reconnaissance and ground attack operations without encountering any German jet fighters. By late April, the squadron was based at Faßberg, Germany and suffered its first losses when two aircraft collided in poor visibility. The war ended with the Meteors having destroyed 46 German aircraft through ground attack. Friendly fire through misidentification as Messerschmitt Me 262s by Allied anti-aircraft gunners was more of a threat than the already-diminished forces of the Luftwaffe; to counter this, continental-based Meteors were given an all-white finish as a recognition aid. The nearest No.616 squadron came to a jet-to-jet battle came on 19 March, when a force of Arado Ar 234 jet bombers attacked their airfield.
Post-war.
The next-generation "Meteor F.4" prototype first flew on 17 May 1945, and went into production in 1946 when 16 RAF squadrons were already operating Meteors. Equipped with Rolls-Royce Derwent 5 engines, the smaller version of the Nene, the F.4 was 170 mi/h faster than the F.1 at sea level (585 against 415), but the reduced wings impaired its rate of climb.The F.4 wingspan was 86.4 cm shorter than the F.3 and with blunter wing tips (derived from the world speed record prototypes). Improvements included a strengthened airframe, fully pressurized cockpit, lighter ailerons (to improve manoeuvrability), and rudder trim adjustments to reduce snaking. The F.4 could be fitted with a drop tank under each wing, and experiments were carried out with carriage of underwing stores and also in lengthened fuselage models.
Because of increased demand, F.4 production was divided between Gloster and Armstrong Whitworth. The majority of early F.4s did not go to the RAF: 100 were exported to Argentina, seeing action on both sides in the 1955 revolution; in 1947, only RAF Nos. 74 and 222 Squadrons were fully equipped with the F.4. Nine further RAF squadrons converted from 1948 onwards. From 1948, 38 F.4s were exported to the Dutch, equipping four squadrons (322, 323, 326 and 327) split between bases in Soesterberg and Leeuwarden until the mid-1950s. In 1949, only two RAF squadrons were converted to the F.4, Belgium was sold 48 aircraft in the same year (going to 349 and 350 Squadrons at Beauvechain) and Denmark received 20 over 1949–50. In 1950, three more RAF squadrons were upgraded, including No. 616 and, in 1951, six more.
A modified two-seater F.4 for jet-conversion and advanced training was tested in 1949 as the "T.7". It was accepted by the RAF and the Fleet Air Arm and became a common addition to the various export packages (for example 43 to Belgium 1948–57, a similar number to the Netherlands over the same period, two to Syria in 1952, six to Israel in 1953, etc.). Despite its limitations—unpressurised cockpit, no armament, limited instructor instrumentation—over 650 T.7s were manufactured. The T.7 remained in RAF service into the 1970s.
As improved jet fighters emerged, Gloster decided to modernise the F.4 while retaining as much of the manufacturing tooling as possible. The result was the definitive production model, the "Meteor F.8" (G-41-K), serving as a major RAF fighter until the introduction of the Hawker Hunter and the Supermarine Swift. The first prototype F.8 was a modified F.4, followed by a true prototype, "VT150", that flew on 12 October 1948 at Moreton Valence. Flight testing of the F.8 prototype led to the discovery of an aerodynamic problem: when ammunition was expended, the aircraft became tail heavy and unstable around the pitch axis due to the weight of fuel in fuselage tanks no longer being balanced by the ammunition. Gloster solved the problem by substituting the tail of the abortive "G 42" single-engined jet fighter. The F.8 and other production variants successfully used the new tail design, giving the later Meteors a distinctive appearance, with taller straighter edges compared with the rounded tail of the F.4s and earlier marks.
The F.8 also featured a fuselage stretch of 76 centimetres (30 inches), intended to shift the aircraft's centre of gravity and also eliminate the use of ballast former necessary in earlier marks. The F.8 incorporated uprated engines, Derwent 8s, with 16 kN (1,633 kgp / 3,600 lbf) thrust each combined with structural strengthening, a Martin Baker ejection seat and a "blown" teardrop cockpit canopy that provided improved pilot visibility. Between 1950 and 1955, the Meteor F.8 was the mainstay of RAF Fighter Command, and served with distinction in combat in Korea with the RAAF as well as operating with many air forces worldwide, although it was clear that the original design was obsolete compared with contemporary swept-wing fighters such as the North American F-86 Sabre and the Soviet MiG-15.
Initial deliveries of the F.8 to the RAF were in August 1949, with the first squadron receiving its fighters in late 1950. Like the F.4, there were strong export sales of the F.8. Belgium ordered 240 aircraft, the majority assembled in The Netherlands by Fokker. The Netherlands had 160 F.8s, equipping seven squadrons until 1955. Denmark had 20, ordered in 1951; they were to be the last F.8s in front line service in Europe. The RAAF ordered 94 F.8s, which served in the Korean War. Despite arms embargoes, both Syria and Egypt received F.8s from 1952, as did Israel, each using their respective Meteors during the Suez Crisis. Brazil ordered 60 new Meteor F.8s and 10 T.7 trainers in October 1952, paying with 15,000 tons of raw cotton.
In the 1950s, Meteors were developed into effective photo-reconnaissance, training and night fighter versions. The fighter reconnaissance (FR) versions were the first to be built, replacing the ageing Spitfires and Mosquitos then in use. Two "FR.5"s were built on the F.4 body; one was used for nose section camera tests, the other broke up in midair while in testing over Moreton Valence. On 23 March 1950, the first "FR.9" flew. Based on the F.8, it was 20 cm longer with a new nose incorporating a remote control camera and window and was also fitted with additional external ventral and wing fuel tanks. Production of the FR.9 began in July. No. 208 Squadron, then based at Fayid, Egypt was the first to be upgraded followed by the 2nd Tactical Air Force in West Germany, No. 2 Squadron RAF at Bückeburg and No. 79 Squadron RAF at RAF Gutersloh flew the FR.9 from 1951 until 1956. In Aden, No. 8 Squadron RAF was given the FR.9 in November 1958 and used them until 1961. Ecuador (12), Israel (7) and Syria (2) were foreign customers for the FR.9.
In 1951, Nos. 29, 141, 85 and 264 Squadrons each received a number of NF.11 aircraft, the first of the Meteor night fighters. It was rolled out across the RAF until the final deliveries in 1954. A "tropicalised" version of the NF.11 for the Middle East was developed; first flying on 23 December 1952 as the "NF.13". The aircraft equipped No. 219 Squadron RAF at Kabrit and No. 39 Squadron at Fayid, both in Egypt. The aircraft served during the Suez crisis and remained with No. 39 Squadron after they were withdrawn to Malta until 1958. Several problems were encountered: the heavily framed T.7 canopy made landings tricky due to limited visibility, the under-wing external fuel tanks tended to break up when the wing cannons were fired, and gun harmonisation, normally set to about 400 yards, was poor due to the wings flexing in flight. Belgium (24), Denmark (20) and France (41) were foreign customers for the NF.11. Ex-RAF NF.13s were sold to France (two), Syria (six), Egypt (six) and Israel (six).
In addition to the armed, low altitude operation, tactical FR.9 variant, Gloster also developed the "PR.10" for high altitude missions. The first prototype flew on 29 March 1950 and was actually converted into the first production aircraft. Based on the F.4, it had the F.4-style tail and the longer wings of the earlier variant. All the cannons were removed and a single camera placed in the nose with two more in the rear fuselage; the canopy was also changed. The PR.10 was delivered to the RAF in December 1950 and were given to No. 2 and No. 541 Squadrons in Germany and No. 13 Squadron RAF in Cyprus. The PR.10 was rapidly phased out from 1956; rapid improvements in surface-to-air missile technology and the introduction of newer aircraft capable of flying at greater altitudes and speeds had rendered the aircraft obsolete.
Australia.
The Royal Australian Air Force (RAAF) acquired 113 Meteors between 1946 and 1952, 94 of which were the F.8 variant. The first RAAF Meteor was a F.3 delivered for evaluation in June 1946.
Australia's F.8s saw extensive service during the Korean War with No. 77 Squadron RAAF, part of British Commonwealth Forces Korea, and had personnel from other Commonwealth air forces attached to it. The squadron had arrived in Korea equipped with piston engine aircraft, the F-51D Mustangs. In order to match the threat posed by Communist MiG-15 jet fighters, it was decided to reequip the squadron with Meteors. Jet conversion training was conducted at Iwakuni, Japan, after which the squadron returned to the Korean theatre in April 1951 with about 30 Meteor F.8s and T.7s. The squadron moved to Kimpo Air Base in June, and was declared combat ready the following month. Other aircraft, such as the F-86 Sabre and the Hawker Hunter, were considered but were determined to be unavailable; the Meteor proved to be considerably inferior in combat against the MiG-15 in several respects, including speed and maneuverability at high altitude.
On 29 July 1951, 77 Squadron began operating their Meteors on combat missions. The squadron had mainly been trained in the ground attack role, and had difficulties when assigned to bomber escort duty at sub optimum altitudes. On 29 August 1951, eight Meteors were on escort duty in "MiG Alley" when they were engaged by six MiG-15s; one Meteor was lost and two damaged, and 77 Squadron did not officially destroy any enemy aircraft on this occasion. On 27 October, the squadron achieved its first probable followed by two probables six days later. On 1 December, during the air battle of Sunchon between 12 Meteors and some 40 MiG-15s, the squadron had its first two confirmed victories: Flying Officer Bruce Gogerly made the first kill. However, in the course of the same dogfight, four Meteors were also destroyed.
At the end of 1951, 77 Squadron and its Meteors were assigned to ground attack duties due to their favourable low-level performance and sturdy construction. In February 1952, over a thousand sorties were flown in the ground attack role; these sorties continued until May 1952, when 77 Squadron switched to fighter sweep operations. The last encounter between the Meteor and the MiG-15 was in March 1953, during which a Meteor piloted by Sergeant John Hale recorded a victory. By the end of the conflict, the squadron had flown 4,836 missions, destroying six MiG-15s, over 3,500 structures and some 1,500 vehicles. About 30 Meteors were lost to enemy action in Korea—the vast majority had been shot down by anti-aircraft fire while serving in a ground attack capacity.
The RAAF began introducing the domestically produced CAC Sabre in 1955, which progressively relegated the older Meteor to training and secondary duties. A number of Meteors would be assigned to the Citizen Air Force, while others were configured as pilotless drone aircraft or for target towing. No. 75 Squadron RAAF was the last Australian squadron to operate the Meteor; notably, it had operated a three-unit aerobatic team, named "The Meteorites".
Argentina.
Argentina became the first overseas operator of the Meteor, ordering 100 F Mk.4s in May 1947. This made Argentina's the second air force in the Americas to operate jets. The Argentine Meteors first saw combat during the 16 June 1955 rebellion when, in an attempt to kill Juan Perón, rebel-flown aircraft bombed the Casa Rosada. A loyalist Meteor shot down a rebel AT-6, while another strafed rebel-held Ezeiza airport. The rebels seized Morón Airport and Air Base, base of the Meteors, and flew the captured aircraft in several attacks against loyalist forces and the Casa Rosada before the rebellion was defeated by day's end. A second revolt, the Revolución Libertadora broke out on 16 September 1955, with, again, both sides operating the Meteor. The rebels seized three Meteors. Government Meteors flew strafing attacks against the rebel held destroyers "Rioja" and "Cervantes", and several landing ships near Rio Santiago on 16 September and attacking Pajas Blancas airport near the city of Córdoba, damaging several Avro Lincoln bombers. The rebel flown Meteors were used to attack loyalist forces attacking Córdoba, losing one of their number on 19 September to an engine failure caused by use of automobile petrol instead of Jet fuel.
The acquisition of North American F-86 Sabres in 1960 allowed the remaining Meteors to be transferred to the ground attack role, with the aircraft being fitted with bomb pylons and rocket rails and discarding the bare metal colour scheme for a camouflage scheme. Argentine Meteors were used to attack rebels during attempted uprisings in September 1962 and April 1963, with the type being withdrawn from service in 1970.
Egypt.
Although Egypt's first order for the Meteor was placed in 1948, the rising tension in the region led to the imposition of a series of arms embargoes. Twelve F Mk.4s were eventually delivered between October 1949 and May 1950, along with three T Mk.7s. Twenty-four F Mk.8s were ordered in 1949, but this order was stopped by an embargo. A further order for 12 ex-RAF F.8s was placed in December 1952, of which four were delivered before the order was cancelled, with the final eight being delivered in 1955, along with three more T Mk.7s. and six NF Mk.13s, all ex-RAF aircraft. Britain had allowed the Meteor sales as part of an effort to foster and support good relations; tensions over the Suez Canal would lead to arms sales being suspended once again.
Egyptian Meteors participated in the fighting during the Suez Crisis of 1956, typically being used in ground attack missions against Israeli forces. In one incident, an Egyptian Meteor NF Mk.13 claimed to have damaged an RAF Vickers Valiant bomber. An aerial bombing campaign of Egyptian airfields by Anglo-French forces resulted in several aircraft being destroyed on the ground, the Egyptian Air Force subsequently decided to withdraw from combat within the Sinai region.
Syria.
Meteors were the fledgling Syrian Air Force's first jet aircraft. It would acquire 25 of them between 1952 and 1956. Although the British were willing to supply aircraft, they did not supply combat training or radar. As Syria became more aligned with Gamal Abdel Nasser's Egypt, British support for Meteor operations was withdrawn and Syrian pilots began training with their Egyptian counterparts. During the Suez Crisis, the RAF performed multiple high altitude reconnaissance flights over Syria via Canberra aircraft from bases in Cyprus. Lacking radar to track the aircraft, the Syrians developed a ground spotter network that reported information by telephone in an attempt to intercept these flights. On 6 November 1956, Syrian Meteors acting on spotter information successfully shot down a Canberra reconnaissance flight over Homs. In 1957, Syria began to replace its Meteors with newly procured MiG-17s from the Soviet Union.
France.
The French Air Force was keen to acquire jet aircraft as part of its re-equipment program following the Second World War; in response to French interest in the Meteor, a pair of F Mk.IVs were sent to France for evaluation purposes in 1948. In 1953, 25 new-build aircraft were diverted from RAF orders to fulfill a French order, a further 16 ex-RAF NF.11s were purchased in 1954 and delivered between September 1954 and April 1955, these being supplemented by about 14 T Mk.7s. The NF Mk.11s replaced the Mosquito night fighter with the Escadre de Chasse (EC) 30, serving with that Wing until replaced by the Sud Aviation Vautour in 1957. Several Meteors were then transferred to ECN 1/7 in Algeria, which saw combat in the Algerian War, operating from Bône, while others were used for training Vautour night fighter crews. The Vautour was retired from French Air Force service in 1964.
Five Meteor NF.11s were transferred to the Centre d’Essais en Vol (Flight Test Centre) in 1958, where they were used as equipment testbeds and chase planes, and were later joined by two NF Mk.13s and two NF Mk.14s. The test aircraft were used in a wide variety of experiments, including radar and missile tests and during the development of Concorde.
Israel.
Due to tensions between the newly formed nation of Israel and its neighbors, both sides had commenced an arms race which led to jet aircraft being vigorously purchased by various countries in the region. In 1953 Israel ordered four T Mk.7s and 11 F Mk.8s, with delivery continuing until early 1954. The F Mk.8s were modified to carry American HVAR rockets but were otherwise identical to RAF aircraft. A second batch of seven refurbished FR Mk.9s and two more T Mk.7s was delivered in 1955. In 1956, Israel purchased six NF Mk.13s, with three delivered that year, and the remaining three, delayed by an arms embargo, in 1958. Five more T Mk.7s were later purchased, these were converted from ex-Belgian F Mk.4s and were fitted with the Mk.8 tail.
On 1 September 1955, an Israeli Meteor shot down an Egyptian de Havilland Vampire, the first jet aircraft to be shot down in the theatre. The Meteor played a key role during the Suez Crisis; on 28 October 1956, an Israeli NF.13 took part in Operation Tarnegol, in which it successfully located and shot down an Egyptian Ilyushin Il-14 that had been carrying several high-ranking Egyptian Military officers on the eve of the crisis. The operation had intended to shoot down the Il-14 that was supposed to be carrying the supreme commander of the Egyptian armed forces, Abdel Hakim Amer, however a different aircraft had been inadvertently attacked and destroyed instead. After deploying paratroopers east of the Suez Canal, the Israeli Air Force continued to support them on the ground predominantly using its jet aircraft, fearing its propeller-driven aircraft would be vulnerable against Egypt's own jet fighters.
While initially flying combat air patrol missions, the Meteors and other Israeli aircraft could not prevent effective attacks by Egyptian aircraft on the ground forces. Israeli officers came to recognize that the Meteor was outclassed by Egyptian MiG-15s, and would subsequently limit the Meteor's employment as a fighter against other aerial adversaries. Following the start of the Anglo-French bombing campaign against Egypian airbases, the Egyptian Air Force mostly withdrew from combat in the Sinai, allowing Israeli aircraft to operate unhindered.
The Mk.8s remained in front line service until 1956, and were then used as training aircraft. The NF Mk.13s remained in operational use until 1962.
Record setting.
Late in 1945, two F.3 Meteors were modified for an attempt on the world air speed record. On 7 November 1945 at Herne Bay in Kent, UK, Group Captain Hugh "Willie" Wilson set the first official air speed record by a jet aircraft of 606 mph TAS. In 1946, Group Captain Edward "Teddy" Donaldson broke this record with a speed of 616 mph (991 km/h) TAS, in "EE549", a Meteor F.4.
Neither of these records, however, exceeded Heini Dittmar's 623 mph (1,004 km/h) unofficial record velocity in one of the Me 163A rocket fighter prototypes, set on October 2, 1941. Test pilot Roland Beamont had previously taken the same aircraft to its compressibility limit at 632 mi/h, but not under official record conditions, and outside its official safety limits.
In 1947, Sqn Ldr Janusz Żurakowski set an international speed record: London-Copenhagen-London, 4–5 April 1950 in a production standard F.8 ("VZ468"). Suitably impressed, the Danes later purchased the type.
Another "claim to fame" was the Meteor's ability to perform the "Żurabatic Cartwheel", an aerobatics manoeuvre named after Gloster's acting Chief Test Pilot, it was first demonstrated by Meteor G-7-1 "G-AMCJ" prototype at the 1951 Farnborough Air Show; the Meteor, due to its widely set engines, could have individual engines throttled back and forward to achieve a seemingly stationary vertical cartwheel. Many Meteor pilots went on to "prove their mettle" by attempting the same feat.
On 7 August 1949, the Meteor III, "EE397", on loan from the RAF and flown by Flight Refuelling Ltd (FRL) test pilot Patrick Hornidge, took off from Tarrant Rushton and, refuelled 10 times by the Lancaster tanker, remained airborne for 12 hours and 3 minutes, receiving 2,352 gallons of fuel from the tanker in ten tanker contacts and flying an overall distance of 3600 mi, achieving a new jet endurance record.
Meteor F.8 "WA820" was adapted during 1948 to take two Armstrong Siddeley Sapphire turbojets, and from Moreton Valence, on August 31, 1951, established a time-to-height climb record. The pilot was Flt Lt Tom Prickett, of Armstrong Siddeley. A height of 9,843 ft was reached in 1 min 16 sec, 19,685 ft in 1 min 50 sec, 29,500 ft in 2 min 29 sec, and 39,370 ft in 3 min 7 sec. Air Service Training Ltd were responsible for the conversion.
Survivors.
Although many Meteors survive in museums, collections and on pylons in public spaces, only five remain airworthy. 
References.
Bibliography.
</dl>

</doc>
<doc id="42704" url="http://en.wikipedia.org/wiki?curid=42704" title="Transatlantic communications cable">
Transatlantic communications cable

A transatlantic telecommunications cable is a submarine communications cable connecting one side of the Atlantic Ocean to the other.
History.
When the first transatlantic telegraph cable was laid in 1858 by businessman Cyrus West Field, it operated for only three weeks; subsequent attempts in 1865 and 1866 were more successful. Although a telephone cable was discussed starting in the 1920s, to be practical it needed a number of technological advances which did not arrive until the 1940s. Starting in 1927, transatlantic telephone service was radio-based.
TAT-1 (Transatlantic No. 1) was the first transatlantic telephone cable system. It was laid between Gallanach Bay, near Oban, Scotland and Clarenville, Newfoundland between 1955 and 1956 by the cable ship "Monarch". It was inaugurated on September 25, 1956, initially carrying 36 telephone channels. In the first 24 hours of public service there were 588 London–U.S. calls and 119 from London to Canada. The capacity of the cable was soon increased to 48 channels. TAT-1 was finally retired in 1978. Later coaxial cables, installed through the 1970s, used transistors and had higher bandwidth.
Current technology.
All cables presently in service use fiber optic technology. Many cables terminate in Newfoundland and Ireland, which lie on the great circle route (the shortest route) from London, UK to New York City, USA.
There have been a succession of newer transatlantic cable systems. All recent systems have used fiber optic transmission, and a self-healing ring topology. Late in the 20th century, communications satellites lost most of their North Atlantic telephone traffic to these low cost, high capacity, low latency cables. This advantage only increases over time as tighter cables provide higher speed – the 2012 generation of cables drop the transatlantic latency to under 60 milliseconds, according to Hibernia Atlantic, deploying such a cable that year.
Some new cables are being announced on the South Atlantic: SACS(South Atlantic Cable System) and SAex(South Atlantic Express) 
TAT cable routes.
The TAT series of cables constitute a large percentage of all North Atlantic cables. All TAT cables are joint ventures between a number of telecommunications companies, e.g. British Telecom. CANTAT cables terminate in Canada rather than in the USA.
Private cable routes.
There are a number of private non-TAT cables.

</doc>
<doc id="42706" url="http://en.wikipedia.org/wiki?curid=42706" title="Dreams (TV series)">
Dreams (TV series)

Dreams is an American television series that aired in 1984-1985 for one season on CBS. It follows the story of a fictional rock band that tries to get a recording contract.

</doc>
<doc id="42707" url="http://en.wikipedia.org/wiki?curid=42707" title="Frank Whittle">
Frank Whittle

Air Commodore Sir Frank Whittle, OM, KBE, CB, FRS, Hon FRAeS (1 June 1907 – 9 August 1996) was an English Royal Air Force (RAF) engineer air officer. He is credited with single-handedly inventing the turbojet engine. A patent was submitted by Maxime Guillaume in 1921 for a similar invention; however, this was technically unfeasible at the time. Whittle's engines were developed some years earlier than those of Germany's Dr. Hans von Ohain who was the designer of the first "operational" jet engine.
From an early age, Whittle demonstrated an aptitude for engineering and an interest in flying. At first he was turned down by the RAF but, determined to join the Royal Air Force, he overcame his physical limitations and was accepted and sent to No. 2 School of Technical Training to join No 1 Squadron of Cranwell Aircraft Apprentices. He was taught the theory of aircraft engines and gained practical experience in the engineering workshops. His academic and practical abilities as an Aircraft Apprentice earned him a place on the officer training course at Cranwell. He excelled in his studies and became an accomplished pilot. While writing his thesis there he formulated the fundamental concepts that led to the creation of the turbojet engine, taking out a patent on his design in 1930. His performance on an officers' engineering course earned him a place on a further course at the Peterhouse (College) of the University of Cambridge where he graduated with a First.
Without Air Ministry support, he and two retired RAF servicemen formed Power Jets Ltd to build his engine with assistance from the firm of British Thomson-Houston. Despite limited funding, a prototype was created, which first ran in 1937. Official interest was forthcoming following this success, with contracts being placed to develop further engines, but the continuing stress seriously affected Whittle's health, eventually resulting in a nervous breakdown in 1940. In 1944 when Power Jets was nationalised he again suffered a nervous breakdown, and resigned from the board in 1946.
In 1948, Whittle retired from the RAF and received a knighthood. He joined BOAC as a technical advisor before working as an engineering specialist in one of Shell Oil's subsidiaries followed by a position with Bristol Aero Engines. After emigrating to the U.S. in 1976 he accepted the position of NAVAIR Research Professor at the United States Naval Academy from 1977–1979. In August 1996, Whittle died of lung cancer at his home in Columbia, Maryland. In 2002, Whittle was ranked number 42 in the BBC poll of the 100 Greatest Britons.
Early life.
Whittle was born in a terraced house in Newcombe Road, Earlsdon, Coventry, England on 1 June 1907, the eldest son of Moses Whittle and Sara Alice Garlick. When he was nine years old, the family moved to the nearby town of Royal Leamington Spa where his father, a highly inventive practical engineer and mechanic, purchased the Leamington Valve and Piston Ring Company, which comprised a few lathes and other tools and a single-cylinder gas engine, on which Whittle became an expert. Whittle developed a rebellious and adventurous streak, together with an early interest in aviation.
After two years attending Milverton School, Whittle won a scholarship to a secondary school which in due course became Leamington College for Boys, but when his father's business faltered there was not enough money to keep him there. He quickly developed practical engineering skills while helping in his father's workshop, and being an enthusiastic reader spent much of his spare time in the Leamington reference library, reading about astronomy, engineering, turbines, and the theory of flight. At the age of 15, determined to be a pilot, Whittle applied to join the RAF.
Entering the RAF.
In January 1923, having passed the RAF entrance examination with a high mark, Whittle reported to RAF Halton as an Aircraft Apprentice. He lasted only two days: just five feet tall and with a small chest measurement, he failed the medical. He then put himself through a vigorous training programme and special diet devised by a physical training instructor at Halton to build up his physique, only to fail again six months later, when he was told that he could not be given a second chance, despite having added three inches to his height and chest. Undeterred, he applied again under an assumed name and presented himself as a candidate at the No 2 School of Technical Training RAF Cranwell. This time he passed the physical and, in September that year, 364365 Boy Whittle, F started his three-year training as an aircraft mechanic in No. 1 Squadron of No. 4 Apprentices Wing, RAF Cranwell, because RAF Halton No. 1 School of Technical Training was unable to accommodate all the aircraft apprentices at that time.
Whittle hated the strict discipline imposed on apprentices and, convinced there was no hope of ever becoming a pilot he at one time seriously considered deserting. However, throughout his early days as an aircraft apprentice (and at the Royal Air Force College Cranwell), he maintained his interest in model aircraft and joined the Model Aircraft Society, where he built working replicas. The quality of these attracted the eye of the Apprentice Wing commanding officer, who noted that Whittle was also a mathematical genius. He was so impressed that in 1926 he recommended Whittle for officer training at RAF College Cranwell.
For Whittle, this was the chance of a lifetime, not only to enter the commissioned ranks but also because the training included flying lessons on the Avro 504. While at Cranwell he lodged in a bungalow at Dorrington. Being an ex-apprentice amongst a majority of ex-public schoolboys, life as an officer cadet was not easy for him, but he nevertheless excelled in the courses and went solo in 1927 after only 13.5 hours instruction, quickly progressing to the Bristol Fighter and gaining a reputation for daredevil low flying and aerobatics.
A requirement of the course was that each student had to produce a thesis for graduation: Whittle decided to write his on potential aircraft design developments, notably flight at high altitudes and speeds over 500 mph (800 km/h). In "Future Developments in Aircraft Design" he showed that incremental improvements in existing propeller engines were unlikely to make such flight routine. Instead he described what is today referred to as a motorjet; a motor using a conventional piston engine to provide compressed air to a combustion chamber whose exhaust was used directly for thrust – essentially an afterburner attached to a propeller engine. The idea was not new and had been talked about for some time in the industry, but Whittle's aim was to demonstrate that at increased altitudes the lower outside air pressure would increase the design's efficiency. For long-range flight, using an Atlantic-crossing mailplane as his example, the engine would spend most of its time at high altitude and thus could outperform a conventional powerplant.
Of the few apprentices accepted into the Royal Air Force College, Whittle graduated in 1928 at the age of 21 and was commissioned as a Pilot Officer in July. He ranked second in his class in academics, won the Andy Fellowes Memorial Prize for Aeronautical Sciences for his thesis, and was described as an "exceptional to above average" pilot. However, his flight logbook also showed numerous red ink warnings about showboating and overconfidence, and because of dangerous flying in an Armstrong Whitworth Siskin he was disqualified from the end of term flying contest.
Development of the turbojet engine.
Whittle continued working on the motorjet principle after his thesis work but eventually abandoned it when further calculations showed it would weigh as much as a conventional engine of the same thrust. Pondering the problem he thought: "Why not substitute a turbine for the piston engine?" Instead of using a piston engine to provide the compressed air for the burner, a turbine could be used to extract some power from the exhaust and drive a similar compressor to those used for superchargers. The remaining exhaust thrust would power the aircraft.
On 27 August 1928 Pilot Officer Whittle joined No. 111 Squadron, Hornchurch, flying Siskin IIIs. His continuing reputation for low flying and aerobatics provoked a public complaint that almost led to his being court-martialled. Within a year he was posted to Central Flying School, Wittering, for a flying instructor's course. He became a popular and gifted instructor, and was selected as one of the entrants in a competition to select a team to perform the "crazy flying" routine in the 1930 Royal Air Force Air Display at RAF Hendon. He destroyed two aircraft in accidents during rehearsals but remained unscathed on both occasions. After the second incident an enraged Flight Lieutenant Harold W. Raeburn said furiously, "Why don't you take all my bloody aeroplanes, make a heap of them in the middle of the aerodrome and set fire to them – it's quicker!"
Whittle showed his engine concept around the base, where it attracted the attention of Flying Officer Pat Johnson, formerly a patent examiner. Johnson, in turn, took the concept to the commanding officer of the base. This set in motion a chain of events that almost led to the engines being produced much sooner than actually occurred.
Earlier, in July 1926, A. A. Griffith had published a paper on compressors and turbines, which he had been studying at the Royal Aircraft Establishment (RAE). He showed that such designs up to this point had been flying "stalled", and that by giving the compressor blades an aerofoil-shaped cross-section their efficiency could be dramatically improved. The paper went on to describe how the increased efficiency of these sorts of compressors and turbines would allow a jet engine to be produced, although he felt the idea was impractical, and instead suggested using the power as a turboprop. At the time most superchargers used a centrifugal compressor, so there was limited interest in the paper.
Encouraged by his Commanding Officer, in late 1929 Whittle sent his concept to the Air Ministry to see if it would be of any interest to them. With little knowledge of the topic they turned to the only other person who had written on the subject and passed the paper on to Griffith. Griffith appears to have been convinced that Whittle's "simple" design could never achieve the sort of efficiencies needed for a practical engine. After pointing out an error in one of Whittle's calculations, he went on to comment that the centrifugal design would be too large for aircraft use and that using the jet directly for power would be rather inefficient. The RAF returned his comment to Whittle, referring to the design as being "impracticable".
Pat Johnson remained convinced of the validity of the idea, and had Whittle patent the idea in January 1930. Since the RAF was not interested in the concept they did not declare it secret, meaning that Whittle was able to retain the rights to the idea, which would have otherwise been their property. Johnson arranged a meeting with British Thomson-Houston (BTH), whose chief turbine engineer seemed to agree with the basic idea. However, BTH did not want to spend the ₤60,000 it would cost to develop it, and this potential brush with early success went no further.
In January 1930, Whittle was promoted to Flying Officer. In Coventry, on 24 May 1930, Whittle married his fiancée, Dorothy Mary Lee, with whom he later had two sons, David and Ian. Then, in 1931, he was posted to the Marine Aircraft Experimental Establishment at Felixstowe as an armament officer and test pilot of seaplanes, where he continued to publicize his idea. This posting came as a surprise for he had never previously flown a seaplane, but he nevertheless increased his reputation as a pilot by flying some 20 different types of floatplanes, flying boats, and amphibians. Every officer with a permanent commission was expected to take a specialist course, and as a result Whittle attended the Officers’ Engineering Course at RAF Henlow, Bedfordshire in 1932. He obtained an aggregate of 98% in all subjects in his exams, completing the course in 18 months instead of the more normal two years.
His performance in the course was so exceptional that in 1934 he was permitted to take a two-year engineering course as a member of Peterhouse, the oldest college of Cambridge University, graduating in 1936 with a First in the Mechanical Sciences Tripos. In February 1934, he had been promoted to the rank of Flight Lieutenant.
Power Jets Ltd.
Still at Cambridge, Whittle could ill afford the £5 renewal fee for his jet engine patent when it became due in January 1935, and because the Air Ministry refused to pay it the patent was allowed to lapse. Shortly afterwards, in May, he received mail from Rolf Dudley-Williams, who had been with him at Cranwell in the 1920s and Felixstowe in 1930. Williams arranged a meeting with Whittle, himself, and another now-retired RAF serviceman, James Collingwood Tinling. The two proposed a partnership that allowed them to act on Whittle's behalf to gather public financing so that development could go ahead.
The agreement soon bore fruit, and in September 1935 the pair introduced Whittle to two investment bankers at O.T. Falk & Partners, Sir Maurice Bonham-Carter and Lancelot Law Whyte. The firm had an interest in developing speculative projects that conventional banks would not touch. Whyte was impressed by the 28-year-old Whittle and his design when they met on 11 September 1935:
The impression he made was overwhelming, I have never been so quickly convinced, or so happy to find one's highest standards met... This was genius, not talent. Whittle expressed his idea with superb conciseness: 'Reciprocating engines are exhausted. They have hundreds of parts jerking to and fro, and they cannot be made more powerful without becoming too complicated. The engine of the future must produce 2,000 hp with one moving part: a spinning turbine and compressor.'—Lancelot Law Whyte
Falk & Partners financed an independent engineering review that was favourable, and with that the jet engine was finally on its way to becoming a reality.
On 27 January 1936, the principals signed the "Four Party Agreement", creating "Power Jets Ltd." The parties were O.T. Falk, the Air Ministry, Whittle and, together, Williams and Tinling. Falk was represented on the board of Power Jets by Whyte as Chairman, and Bonham-Carter as a director. Whittle, Williams and Tinling retained a 49% share of the company in exchange for Falk and Partners putting in £2,000 with the option of a further £18,000 within 18 months. As Whittle was still a full-time RAF officer and currently at Cambridge, he was given the title "Honorary Chief Engineer and Technical Consultant". Needing special permission to work outside the RAF, he was placed on the Special Duty List and allowed to work on the design as long as it was for no more than six hours a week.
The Air Ministry still saw no value in the effort, and having no production facilities of its own, Power Jets entered into an agreement with steam turbine specialists British Thomson-Houston to build an experimental engine facility at a BTH factory in Rugby, Warwickshire. Work progressed quickly, and by the end of the year the prototype detail design was finalised and parts for it were well on their way to being completed, all within the original £2,000 budget.
Financial difficulty.
Earlier, in January, when the company formed, Henry Tizard, the rector of Imperial College London and chairman of the Aeronautical Research Committee (ARC), had prompted the Air Ministry's Director of Scientific Research to ask for a write-up of the design. The report was once again passed on to Griffith for comment, but was not received back until March 1937 by which point Whittle's design was well along. Griffith had already started construction of his own turbine engine design and, perhaps to avoid tainting his own efforts, he returned a somewhat more positive review. However, he remained highly critical of some features, notably the use of jet thrust. The Engine Sub-Committee of ARC studied Griffith's report, and decided to fund his effort instead.
Given this astonishing display of official indifference, Falk and Partners gave notice that they could not provide funding beyond £5,000. Nevertheless the team pressed ahead, and the W.U. (Whittle Unit) engine ran successfully on 12 April 1937. Tizard pronounced it "streaks ahead" of any other advanced engine he had seen, and managed to interest the Air Ministry enough to fund development with a contract for £5,000 to develop a flyable version. However, it was a year before the funds were made available, greatly delaying development.
In July, when Whittle's stay at Cambridge was over, he was released to work full-time on the engine. On 8 July Falk gave the company an emergency loan of £250, and on the 15th they agreed to find £4,000 to £14,000 in additional funding. The money never arrived and, entering into default, Falk's shares were returned to Williams, Tinling and Whittle on 1 November. Nevertheless, Falk arranged another loan of £3,000, and work continued. Whittle was promoted to Squadron Leader in December.
Testing continued with the W.U., which showed an alarming tendency to race out of control. Because of the dangerous nature of the work being carried out, development was largely moved from Rugby to BTH's lightly used Ladywood foundry at nearby Lutterworth in Leicestershire in 1938, where there was a successful run of the W.U. in March that year. BTH had decided to put in £2,500 of their own in January, and in March 1938 the Air Ministry funds finally arrived. This proved to be a mixed blessing – the company was now subject to the Official Secrets Act, which made it extremely difficult to gather more private equity.
These delays and the lack of funding slowed the project. In Germany, Hans von Ohain had started work on a prototype in 1935, and had by this point passed the prototype stage and was building the world's first flyable Jet aircraft, the Heinkel HeS 3. There is little doubt that Whittle's efforts would have been at the same level or even more advanced had the Air Ministry taken a greater interest in the design. When war broke out in September 1939, Power Jets had a payroll of only 10 and Griffith's operations at the RAE and Metropolitan-Vickers were similarly small.
The stress of the continual on-again-off-again development and problems with the engine took a serious toll on Whittle.
The responsibility that rests on my shoulders is very heavy indeed. ... either we place a powerful new weapon in the hands of the Royal Air Force or, if we fail to get our results in time, we may have falsely raised hopes and caused action to be taken which may deprive the Royal Air Force of hundreds of [conventional] aircraft that it badly needs. ... I have a good crowd round me. They are all working like slaves, so much so, that there is a risk of mistakes through physical and mental fatigue.—Frank Whittle
He suffered from stress-related ailments such as eczema and heart palpitations, while his weight dropped to nine stone (126 lb / 57 kg). In order to keep to his 16-hour workdays, he sniffed Benzedrine during the day and then took tranquillizers and sleeping pills at night to offset the effects and allow him to sleep. Over this period he became irritable and developed an "explosive" temper.
Changing fortunes.
By June 1939 Power Jets could barely afford to keep the lights on when yet another visit was made by Air Ministry personnel. This time Whittle was able to run the W.U. at high power for 20 minutes without any difficulty. One of the members of the team was the Director of Scientific Research, David Randall Pye, who walked out of the demonstration utterly convinced of the importance of the project. The Ministry agreed to buy the W.U. and then loan it back to them, injecting cash, and placed an order for a flyable version of the engine.
Whittle had already studied the problem of turning the massive W.U. into a flyable design, and with the new contract work started in earnest on the "Whittle Supercharger Type W.1". It featured a reverse-flow design; compressed air from the outer rim of the compressor was fed into the burners and ignited, then piped back towards the front of the engine, reversing again, then finally into the turbine area. This design allowed the flame cans to be folded in length, reducing the length of the engine, and the length of the drive shaft connecting the compressor and turbine, thus reducing weight.
In January 1940, the Ministry placed a contract with the Gloster Aircraft Company for a simple aircraft specifically to flight-test the W.1, the Gloster E.28/39. They also placed a second engine contract, this time for a larger design that developed into the otherwise similar W.2. In February work started on a third design, the W.1A, which was the size of the W.1 but used the W.2's mechanical layout. The W.1A allowed them to flight test the W.2's basic mechanical design in the E.28/39. Power Jets also spent some time in May 1940 drawing up the W.2Y, a similar design with a "straight-through" airflow that resulted in a longer engine and (more critically) driveshaft but having a somewhat simpler layout. To reduce the weight of the driveshaft as much as possible, the W.2Y used a large cylindrical shaft almost as large as the turbine disc, "necked down" at either end where it connected to the turbine and compressor.
In April the Air Ministry issued contracts for W.2 production lines with a capacity of up to 3,000 engines a month in 1942, asking BTH, Vauxhall and the Rover Company to join. However, the contract was eventually taken up by Rover only. In June, Whittle received a promotion to Wing Commander.
Rover.
Meanwhile work continued with the W.U., which eventually went through nine rebuilds in an attempt to solve the combustion problems that caused the engines to race and surge. On 9 October the W.U. ran once again, this time equipped with Lubbock ("Shell" type) atomizing burners which solved the racing problems, but surging continued.
By this point it was clear that Gloster's first airframe would be ready long before Rover could deliver an engine. Unwilling to wait, Whittle cobbled together an engine from spare parts, creating the W.1X ("X" standing for "experimental") which ran for the first time on 14 December 1940. On 10 December Whittle suffered a nervous breakdown, and left work for a month. This engine powered the E.28/39 for taxi testing on 7 April 1941 near the factory in Gloucester, where it took to the air for two or three short hops of several hundred yards at about six feet from the ground.
The definitive W.1 of 850 lbf (3.8 kN) thrust ran on 12 April 1941, and on 15 May the W.1-powered E.28/39 took off from Cranwell at 7:40 pm, flying for 17 minutes and reaching a maximum speed of around 340 mph (545 km/h). At the end of the flight, Pat Johnson, who had encouraged Whittle for so long said to him, "Frank, it flies." Whittle replied, "Well, that's what it was bloody well designed to do, wasn't it?"
Within days the aircraft was reaching 370 mph (600 km/h) at 25,000 feet (7,600 m), exceeding the performance of the contemporary Spitfires. Success of the design was now evident; the first example of what was a purely experimental and entirely new engine design was already outperforming one of the best piston engines in the world, an engine that had five years of development and production behind it, and decades of basic engineering. Nearly every engine company in Britain then started their own crash efforts to catch up with Power Jets.
In 1941 Rover set up a new laboratory for Whittle's team along with a production line at their unused Barnoldswick factory, but by late 1941 it was obvious that the arrangement between Power Jets and Rover was not working. Whittle was frustrated by Rover's inability to deliver production-quality parts, as well as with their attitude of engineering superiority, and became increasingly outspoken about the problems. Rover decided to set up secretly a parallel effort with their own engineers at Waterloo Mill, in nearby Clitheroe. Here Adrian Lombard started work developing the W.2B into Rover's own production-quality design, dispensing with Whittle's "reverse-flow" burners and developing a longer but simpler "straight-through" engine instead. This was encouraged by the Air Ministry, who gave Whittle's design the name "B.23", and Rover's became the "B.26".
Work on all of the designs continued over the winter of 1941–42. The first W.1A was completed soon after, and on 2 March 1942 the second E.28/39 reached 430 mph (690 km/h) at 15,000 feet (4,600 m) on this engine. The next month work on an improved W.2B started under the new name, "W2/500". In April Whittle learned of Rover's parallel effort, creating discontentment and causing a major crisis in the programme. Work continued, however, and in September the first W2/500 ran for the first time, generating its full design thrust of 1,750 lbf (7.8 kN) the same day. Work started on a further improvement, the W2/700.
Rolls-Royce.
Earlier, in January 1940, Whittle had met Dr Stanley Hooker of Rolls-Royce, who in turn introduced Whittle to Rolls-Royce board member and manager of their Derby factory, Ernest Hives (later Lord Hives). Hooker was in charge of the supercharger division at Rolls-Royce Derby and was a specialist in the mathematics of "fluid flow". He had already increased the power of the Merlin piston engine by improving its supercharger. Such a speciality was naturally suited to the dynamics of jet engines in which the optimisation of airflow in compressor, flame cans, turbine and jet pipe, is fundamental. Hives agreed to supply key parts to help the project and it was Rolls-Royce engineers who helped solve surging problems (unstable airflow in the compressor) experienced in the early engines. In early 1942 Whittle contracted Rolls-Royce for six engines, known as the WR.1, identical to the existing W.1.
When Rolls-Royce became involved, Ray Dorey, the manager of the company's Flight Centre at Hucknall airfield on the north side of Nottingham, had a Whittle engine installed in the rear of a Vickers Wellington bomber. This enabled testing to be carried out in a real flight environment without the aircraft depending on the jet engine for its own propulsion and safety. This was the first flying "test bed" for testing jet engines before they were used for primary propulsion in their own right.
The problems between Rover and Power Jets became a "public secret" and late in 1942 Spencer Wilks of Rover met with Hives and Hooker at the "Swan and Royal" pub, in Clitheroe, near the Barnoldswick factory. They decided to trade the jet factory at Barnoldswick for Rolls-Royce's tank engine factory in Nottingham, sealing the deal with a handshake. The official handover took place on 1 January 1943, although the W.2B contract had already been signed over in December. Rolls-Royce closed Rover's secret parallel plant at Clitheroe soon after; however, they continued the development of the W.2B/26 that had begun there.
Testing and production ramp-up was immediately accelerated. In December 1942 Rover had tested the W.2B for a total of 37 hours, but within the next month Rolls-Royce tested it for 390 hours. The W.2B passed its first 100-hour test at full performance of 1,600 lbf (7.1 kN) on 7 May 1943. The prototype Meteor airframe was already complete and took to the air on 12 June 1943. Production versions of the engine started rolling off the line in October, first known as the W.2B/23, then the RB.23 (for "Rolls-Barnoldswick") and eventually became known as the Rolls-Royce Welland. Barnoldswick was too small for full-scale production and turned back into a pure research facility under Hooker's direction, while a new factory was set up in Newcastle-under-Lyme. Rover's W.2B/26, as the Rolls-Royce Derwent, opened the new line and soon replaced the Welland, allowing the production lines at Barnoldswick to shut down in late 1944.
Despite lengthy delays in their own programme, the Luftwaffe beat the British efforts into the air by nine months. A lack of cobalt for high-temperature steel alloys meant the German designs were always at risk of overheating and damaging their turbines. The low-grade alloy production versions of the Junkers Jumo 004, designed by Dr. Anselm Franz, would typically last only 10–25 hours (longer with an experienced pilot) before burning out, and sometimes exploded on their first startup. Whittle's designs were more basic, with centrifugal compressors rather than the more complicated axial designs. The latter, having several stages of rotating blades, each stage increasing the compression ratio, were potentially more efficient but were much more difficult to develop. The UK designs also had better materials such as the Nimonic alloys for turbine blades. Early UK jet engines would run for 150 hours between overhauls and had better power-to-weight ratio and specific fuel consumption compared to the German designs. By the end of the Second World War, other UK engine companies were working on jet designs based on the Whittle pattern, such as the de Havilland Goblin and Ghost engines. However, the advantages of axial-flow compressors with their higher compression ratios compared to simpler centrifugal designs led to a transition to axial compressors in the late 1940s, epitomised by the Rolls-Royce Avon series, Armstrong Siddeley Sapphire, Bristol Olympus, and so forth.
Continued development.
With the W.2 design proceeding smoothly, Whittle was sent to Boston, Massachusetts in mid-1942 to help the General Electric jet programme. GE, the primary supplier of turbochargers in the U.S., was well suited to starting jet production quickly. A combination of the W.2B design and a simple airframe from Bell Aircraft flew in autumn of 1942 as the Bell XP-59A Airacomet.
Whittle's developments at Power Jets continued, the W.2/700 later being fitted with an afterburner ("reheat" in British terminology), as well as experimental water injection to cool the engine and allow higher power settings without melting the turbine. Whittle also turned his attention to the axial-flow (straight-through) engine type as championed by Griffith, designing the L.R.1. Other developments included the use of fans to provide greater mass-flow, either at the front of the engine as in a modern turbofan or at the rear, which is much less common but somewhat simpler.
Whittle's work had caused a minor revolution within the British engine manufacturing industry, and even before the E.28/39 flew most companies had set up their own research efforts. In 1939, Metropolitan-Vickers set up a project to develop an axial-flow design as a turboprop but later re-engineered the design as a pure jet known as the Metrovick F.2. Rolls-Royce had already copied the W.1 to produce the low-rated WR.1 but later stopped work on this project after taking over Rover's efforts. In 1941, de Havilland started a jet fighter project, the Spider Crab — later called Vampire — along with their own engine to power it; Frank Halford's Goblin (Halford H.1). Armstrong Siddeley also developed an axial-flow design, the ASX but reversed Vickers' thinking and later modified it into a turboprop instead, the Python.
Nationalisation.
During a demonstration of the E.28/39 to Winston Churchill in April 1943, Whittle proposed to Stafford Cripps, Minister of Aircraft Production, that all jet development be nationalised. He pointed out that the company had been funded by private investors who helped develop the engine successfully, only to see production contracts go to other companies. Nationalisation was the only way to repay those debts and ensure a fair deal for everyone, and he was willing to surrender his shares in Power Jets to make this happen. In October, Cripps told Whittle that he decided a better solution would be to nationalise Power Jets only.
Whittle believed that he had triggered this decision, but Cripps had already been considering how best to maintain a successful jet programme and act responsibly regarding the state's substantial financial investment, while at the same time wanting to establish a research centre that could utilise Power Jets' talents, and had come to the conclusion that national interests demanded the setting up of a Government-owned establishment. On 1 December Cripps advised Power Jets' directors that the Treasury would not pay more than £100,000 for the company.
In January 1944 Whittle was awarded the CBE in the New Year Honours. By this time he was a Group Captain, having been promoted from Wing Commander in July 1943. Later that month after further negotiations the Ministry made another offer of £135,500 for Power Jets, which was reluctantly accepted after the Ministry refused arbitration on the matter. Since Whittle had already offered to surrender his shares he would receive nothing at all, while Williams and Tinling each received almost £46,800 for their stock, and investors of cash or services had a threefold return on their original investment. Whittle met with Cripps to object personally to the nationalisation efforts and how they were being handled, but to no avail. The final terms were agreed on 28 March, and Power Jets officially became Power Jets (Research and Development) Ltd, with Roxbee Cox as Chairman, Constant of RAE Head of Engineering Division, and Whittle as Chief Technical Advisor. On 5 April 1944, the Ministry sent Whittle an award of only £10,000 for his shares.
From the end of March, Whittle spent six months in hospital recovering from nervous exhaustion, and resigned from Power Jets (R and D) Ltd in January 1946. In July the company was merged with the gas turbine division of the RAE to form the National Gas Turbine Establishment (NGTE) at Farnborough, and 16 Power Jets engineers, following Whittle's example, also resigned.
After the war.
As part of his socialist ideals, he proposed that Power Jets be nationalised; in part because he saw that private companies would profit from the technology freely given during the war.
In 1946 Whittle accepted a post as Technical Advisor on Engine Design and Production to Controller of Supplies (Air); was made Commander, the U.S. Legion of Merit; and was awarded the Order of the Bath (CB) in 1947. During May 1948 Whittle received an ex-gratia award of £100,000 from the Royal Commission on Awards to Inventors in recognition of his work on the jet engine, and two months later he was made a Knight Commander of the Order of the British Empire (KBE), Military Division.
During a lecture tour in the U.S. he again broke down and retired from the RAF on medical grounds on 26 August 1948, leaving with the rank of Air Commodore. He joined BOAC as a technical advisor on aircraft gas turbines and travelled extensively over the next few years, viewing jet engine developments in the United States, Canada, Africa, Asia and the Middle East. He left BOAC in 1952 and spent the next year working on a biography, "Jet: The Story of a Pioneer". He was awarded the Royal Society of Arts' Albert Medal that year.
Returning to work in 1953, he accepted a position as a Mechanical Engineering Specialist in one of Shell Oil's subsidiaries, where he developed a new type of self-powered drill, driven by a turbine running on the lubricating mud that is pumped into the borehole during drilling. Normally a well is drilled by attaching rigid sections of pipe together and powering the cutting head by spinning the pipe, but Whittle's design removed the need for a strong mechanical connection between the drill and the head frame, allowing for much lighter piping to be used. He gave the Royal Institution Christmas Lectures in 1954 on "The Story of Petroleum".
Whittle left Shell in 1957 to work for Bristol Aero Engines who picked up the project in 1961, setting up "Bristol Siddeley Whittle Tools" to further develop the concept. In 1966 Rolls-Royce purchased Bristol Siddeley, but the financial pressures and eventual bankruptcy because of cost overruns of the RB211 project led to the slow wind-down and eventual disappearance of Whittle's "turbo-drill". The design eventually appeared only in the late 1990s, when it was combined with a continuous coiled pipe to allow uninterrupted drilling at any angle. "Continuous-coil drilling" has the ability to drill straight down into a pocket of oil and then sideways through the pocket to allow the oil to flow out faster.
By 1964 he had deserted his previously socialist beliefs, going so far as to launch a fierce attack on the Labour candidate in Smetwick.
In 1967, he was awarded an Honorary Degree (Doctor of Science) by the University of Bath.
Later life.
Whittle received the "Tony Jannus Award" in 1969 for his distinguished contributions to commercial aviation.
In 1976, his marriage to Dorothy was dissolved and he married American Hazel S Hall ("Tommie"). He emigrated to the U.S. and the following year accepted the position of NAVAIR Research Professor at the United States Naval Academy (Annapolis, Maryland). His research concentrated on the boundary layer before his professorship became part-time from 1978 to 1979. The part-time post enabled him to write a textbook entitled "Gas turbine aero-thermodynamics: with special reference to aircraft propulsion", published in 1981. Having first met Hans von Ohain in 1966, Whittle again met him at Wright-Patterson Air Force Base in 1978 while von Ohain was working there as the Aero Propulsion Laboratory's Chief Scientist. Initially upset because he believed von Ohain's engine had been developed after seeing Whittle's patent, he eventually became convinced that von Ohain's work was, in fact, independent. The two became good friends and often toured the U.S. giving talks together.
In 1986 Whittle was appointed a member of the Order of Merit (Commonwealth). He was made a Fellow of the Royal Society, and of the Royal Aeronautical Society, and in 1991 he and von Ohain were awarded the Charles Stark Draper Prize for their work on turbojet engines.
Whittle was an atheist.
Whittle died of lung cancer on 9 August 1996, at his home in Columbia, Maryland. He was cremated in America and his ashes were flown to England where they were placed in a memorial in a church in Cranwell.
In a conversation with Whittle after the war, Von Ohain stated that "If you had been given the money you would have been six years ahead of us. If Hitler or Goering had heard that there is a man in England who flies 500mph in a small experimental plane and that it is coming into development, it is likely that World War II would not have come into being"
Bibliography.
</dl>

</doc>
<doc id="42708" url="http://en.wikipedia.org/wiki?curid=42708" title="Bion of Smyrna">
Bion of Smyrna

Bion (; Greek: Βίων, "gen".: Βίωνος) was a Greek bucolic poet.
Life.
He was a native of the city of Smyrna and flourished about 100 BC. Most of his work is lost. There remain 17 fragments (preserved in ancient anthologies) and the "Epitaph of Adonis", a mythological poem on the death of Adonis and the lament of Aphrodite (preserved in several late medieval manuscripts of bucolic poetry). Some of the fragments show the pastoral themes that were typical of ancient Greek bucolic poetry, while others attest the broader thematic interpretation of the bucolic form that prevailed in the later Hellenistic period. They are often concerned with love, mainly homosexual. Besides Adonis, other myths that appear in his work are those of Hyacinthus and the Cyclops; to judge from references in the "Epitaph on Bion", which frequently alludes to Bion's work, he also wrote a poem on Orpheus, to which some of the extant fragments may have belonged. The Greek texts of Bion's poems are generally included in the editions of Theocritus. There is no particular reason to think that the "Epithalamium of Achilles and Deidameia", preserved in bucolic manuscripts and usually included under his name in modern editions, is Bion's work.
Bion's influence can be seen in numerous ancient Greek and Latin poets and prose authors, including Virgil and Ovid. His treatment of the myth of Adonis in particular has influenced European and American literature since the Renaissance.
Almost nothing is known of Bion's life. The account formerly given of him, that he was the contemporary of Theocritus and a friend and teacher of Moschus, and lived about 280 BC, is now regarded as incorrect: it rests on a misreading of the "Epitaph of Bion", a poem commemorating his death, which in early modern times was erroneously attributed to Moschus. The Suda lists the ancient canon of Greek bucolic poets as Theocritus, Moschus, and Bion, which should reflect chronogical order, and Moschus flourished in the mid 2nd century BC. Probable and certain imitations of Bion by Greek and Latin poets begin to be seen in the early 1st century. Some information concerning Bion's life comes from the "Epitaph on Bion". Its anonymous author calls himself Bion's heir and an "Ausonian" (= Italian), which may mean that Bion traveled to Italy at some point, perhaps for patronage in Rome (as Greek poets were beginning to do in his lifetime). It may, however, mean only that the author considered himself Bion's poetic heir. The poem also asserts that Bion was poisoned, which may or may not be a poetic metaphor.
One ancient text gives his place of origin as "a little place called Phlossa," which is otherwise unknown; it was presumably a district under the administration of Smyrna, perhaps one of the villages out of which Smyrna was reconstituted during the Hellenistic period. The appellation "Bion of Phlossa," under which he is sometimes known (for example, by the Library of Congress), is therefore a pedantic solecism: outside of Smyrna itself he would have been known as Bion of Smyrna.
Works.
Recent editions are:
Bion and Moschus have been edited separately by 
Sources.
On the date of Bion:

</doc>
<doc id="42709" url="http://en.wikipedia.org/wiki?curid=42709" title="Pendulum">
Pendulum

A pendulum is a weight suspended from a pivot so that it can swing freely. When a pendulum is displaced sideways from its resting equilibrium position, it is subject to a restoring force due to gravity that will accelerate it back toward the equilibrium position. When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth. The time for one complete cycle, a left swing and a right swing, is called the period. The period depends on the length of the pendulum, and also to a slight degree on the amplitude, the width of the pendulum's swing.
From its examination in around 1602 by Galileo Galilei, the regular motion of pendulums was used for timekeeping, and was the world's most accurate timekeeping technology until the 1930s. Pendulums are used to regulate pendulum clocks, and are used in scientific instruments such as accelerometers and seismometers. Historically they were used as gravimeters to measure the acceleration of gravity in geophysical surveys, and even as a standard of length. The word 'pendulum' is new Latin, from the Latin "pendulus", meaning 'hanging'.
The "simple gravity pendulum" is an idealized mathematical model of a pendulum. This is a weight (or bob) on the end of a massless cord suspended from a pivot, without friction. When given an initial push, it will swing back and forth at a constant amplitude. Real pendulums are subject to friction and air drag, so the amplitude of their swings declines.
Period of oscillation.
The period of swing of a simple gravity pendulum depends on its length, the local strength of gravity, and to a small extent on the maximum angle that the pendulum swings away from vertical, "θ0", called the amplitude. It is independent of the mass of the bob. If the amplitude is limited to small swings, the period "T" of a simple pendulum, the time taken for a complete cycle, is:
where L is the length of the pendulum and g is the local acceleration of gravity.
For small swings the period of swing is approximately the same for different size swings: that is, "the period is independent of amplitude". This property, called isochronism, is the reason pendulums are so useful for timekeeping. Successive swings of the pendulum, even if changing in amplitude, take the same amount of time.
For larger amplitudes, the period increases gradually with amplitude so it is longer than given by equation (1). For example, at an amplitude of "θ0" = 23° it is 1% larger than given by (1). The period increases asymptotically (to infinity) as "θ0" approaches 180°, because the value "θ0" = 180° is an unstable equilibrium point for the pendulum. The true period of an ideal simple gravity pendulum can be written in several different forms (see Pendulum (mathematics) ), one example being the infinite series:
The difference between this true period and the period for small swings (1) above is called the "circular error". In the case of a typical grandfather clock whose pendulum has a swing of 6° and thus an amplitude of 3° (0.05 radians), the difference between the true period and the small angle approximation (1) amounts to about 15 seconds per day.
For small swings the pendulum approximates a harmonic oscillator, and its motion as a function of time, t, is approximately simple harmonic motion:
where formula_4 is a constant value, dependent on initial conditions. 
For real pendulums, corrections to the period may be needed to take into account the presence of air, the mass of the string, the size and shape of the bob and how it is attached to the string, flexibility and stretching of the string, motion of the support, and local gravitational gradients.
Compound pendulum.
The length "L" of the ideal simple pendulum discussed above is the distance from the pivot point to the center of mass of the bob. Any swinging rigid body free to rotate about a fixed horizontal axis is called a compound pendulum or physical pendulum. The appropriate equivalent length "L" for calculating the period of any such pendulum is the distance
from the pivot to the "center of oscillation". This point is located under the center of mass at a distance from the
pivot traditionally called the radius of oscillation, which depends on the mass distribution of the pendulum. If most of the mass is concentrated in a relatively small bob compared to the pendulum length, the center of oscillation is close to the center of mass.
The radius of oscillation or equivalent length "L" of any physical pendulum can be shown to be
where "I" is the moment of inertia of the pendulum about the pivot point,
"m" is the mass of the pendulum, and "R" is the distance between the pivot point and the center of mass.
Substituting this expression in (1) above, the period "T" of a compound pendulum is given by
for sufficiently small oscillations.
A rigid uniform rod of length "L" pivoted about either end has moment of inertia "I" = (1/3)"mL"2.
The center of mass is located at the center of the rod, so "R" = "L"/2. Substituting these values into the above equation gives "T" = 2π√2"L"/3"g". This shows that a rigid rod pendulum has the same period as a simple pendulum of 2/3 its length. 
Christiaan Huygens proved in 1673 that the pivot point and the center of oscillation are interchangeable. This means if any pendulum is turned upside down and swung from a pivot located at its previous center of oscillation, it will have the same period as before and the new center of oscillation will be at the old pivot point. In 1817 Henry Kater used this idea to produce a type of reversible pendulum, now known as a Kater pendulum, for improved measurements of the acceleration due to gravity.
History.
One of the earliest known uses of a pendulum was a 1st-century seismometer device of Han Dynasty Chinese scientist Zhang Heng. Its function was to sway and activate one of a series of levers after being disturbed by the tremor of an earthquake far away. Released by a lever, a small ball would fall out of the urn-shaped device into one of eight metal toad's mouths below, at the eight points of the compass, signifying the direction the earthquake was located.
Many sources claim that the 10th-century Egyptian astronomer Ibn Yunus used a pendulum for time measurement, but this was an error that originated in 1684 with the British historian Edward Bernard.
During the Renaissance, large pendulums were used as sources of power for manual reciprocating machines such as saws, bellows, and pumps. Leonardo da Vinci made many drawings of the motion of pendulums, though without realizing its value for timekeeping.
1602: Galileo's research.
Italian scientist Galileo Galilei was the first to study the properties of pendulums, beginning around 1602. The earliest extant report of his research is contained in a letter to Guido Ubaldo dal Monte, from Padua, dated November 29, 1602. His biographer and student, Vincenzo Viviani, claimed his interest had been sparked around 1582 by the swinging motion of a chandelier in the Pisa cathedral. Galileo discovered the crucial property that makes pendulums useful as timekeepers, called isochronism; the period of the pendulum is approximately independent of the amplitude or width of the swing. He also found that the period is independent of the mass of the bob, and proportional to the square root of the length of the pendulum. He first employed freeswinging pendulums in simple timing applications. A physician friend invented a device which measured a patient's pulse by the length of a pendulum; the "pulsilogium". In 1641 Galileo conceived and dictated to his son Vincenzo a design for a pendulum clock; Vincenzo began construction, but had not completed it when he died in 1649. The pendulum was the first harmonic oscillator used by man.
1656: The pendulum clock.
In 1656 the Dutch scientist Christiaan Huygens built the first pendulum clock. This was a great improvement over existing mechanical clocks; their best accuracy was increased from around 15 minutes deviation a day to around 15 seconds a day. Pendulums spread over Europe as existing clocks were retrofitted with them.
The English scientist Robert Hooke studied the conical pendulum around 1666, consisting of a pendulum that is free to swing in two dimensions, with the bob rotating in a circle or ellipse. He used the motions of this device as a model to analyze the orbital motions of the planets. Hooke suggested to Isaac Newton in 1679 that the components of orbital motion consisted of inertial motion along a tangent direction plus an attractive motion in the radial direction. This played a part in Newton's formulation of the law of universal gravitation. Robert Hooke was also responsible for suggesting as early as 1666 that the pendulum could be used to measure the force of gravity.
During his expedition to Cayenne, French Guiana in 1671, Jean Richer found that a pendulum clock was 2 1⁄2 minutes per day slower at Cayenne than at Paris. From this he deduced that the force of gravity was lower at Cayenne. In 1687, Isaac Newton in "Principia Mathematica" showed that this was because the Earth was not a true sphere but slightly oblate (flattened at the poles) from the effect of centrifugal force due to its rotation, causing gravity to increase with latitude. Portable pendulums began to be taken on voyages to distant lands, as precision gravimeters to measure the acceleration of gravity at different points on Earth, eventually resulting in accurate models of the shape of the Earth.
1673: Huygens' "Horologium Oscillatorium".
In 1673, Christiaan Huygens published his theory of the pendulum, "Horologium Oscillatorium sive de motu pendulorum". Marin Mersenne and René Descartes had discovered around 1636 that the pendulum was not quite isochronous; its period increased somewhat with its amplitude. Huygens analyzed this problem by determining what curve an object must follow to descend by gravity to the same point in the same time interval, regardless of starting point; the so-called "tautochrone curve". By a complicated method that was an early use of calculus, he showed this curve was a cycloid, rather than the circular arc of a pendulum, confirming that the pendulum was not isochronous and Galileo's observation of isochronism was accurate only for small swings. Huygens also solved the problem of how to calculate the period of an arbitrarily shaped pendulum (called a "compound pendulum"), discovering the "center of oscillation", and its interchangeability with the pivot point.
The existing clock movement, the verge escapement, made pendulums swing in very wide arcs of about 100°. Huygens showed this was a source of inaccuracy, causing the period to vary with amplitude changes caused by small unavoidable variations in the clock's drive force. To make its period isochronous, Huygens mounted cycloidal-shaped metal 'chops' next to the pivots in his clocks, that constrained the suspension cord and forced the pendulum to follow a cycloid arc. This solution didn't prove as practical as simply limiting the pendulum's swing to small angles of a few degrees. The realization that only small swings were isochronous motivated the development of the anchor escapement around 1670, which reduced the pendulum swing in clocks to 4°–6°.
1721: Temperature compensated pendulums.
During the 18th and 19th century, the pendulum clock's role as the most accurate timekeeper motivated much practical research into improving pendulums. It was found that a major source of error was that the pendulum rod expanded and contracted with changes in ambient temperature, changing the period of swing. This was solved with the invention of temperature compensated pendulums, the mercury pendulum in 1721 and the gridiron pendulum in 1726, reducing errors in precision pendulum clocks to a few seconds per week.
The accuracy of gravity measurements made with pendulums was limited by the difficulty of finding the location of their center of oscillation. Huygens had discovered in 1673 that a pendulum has the same period when hung from its center of oscillation as when hung from its pivot, and the distance between the two points was equal to the length of a simple gravity pendulum of the same period. In 1818 British Captain Henry Kater invented the reversible Kater's pendulum which used this principle, making possible very accurate measurements of gravity. For the next century the reversible pendulum was the standard method of measuring absolute gravitational acceleration.
1851: Foucault pendulum.
In 1851, Jean Bernard Léon Foucault showed that the plane of oscillation of a pendulum, like a gyroscope, tends to stay constant regardless of the motion of the pivot, and that this could be used to demonstrate the rotation of the Earth. He suspended a pendulum free to swing in two dimensions (later named the Foucault pendulum) from the dome of the Panthéon in Paris. The length of the cord was 67 m. Once the pendulum was set in motion, the plane of swing was observed to precess or rotate 360° clockwise in about 32 hours.
This was the first demonstration of the Earth's rotation that didn't depend on celestial observations, and a "pendulum mania" broke out, as Foucault pendulums were displayed in many cities and attracted large crowds.
1930: Decline in use.
Around 1900 low-thermal-expansion materials began to be used for pendulum rods in the highest precision clocks and other instruments, first invar, a nickel steel alloy, and later fused quartz, which made temperature compensation trivial. Precision pendulums were housed in low pressure tanks, which kept the air pressure constant to prevent changes in the period due to changes in buoyancy of the pendulum due to changing atmospheric pressure. The accuracy of the best pendulum clocks topped out at around a second per year.
The timekeeping accuracy of the pendulum was exceeded by the quartz crystal oscillator, invented in 1921, and quartz clocks, invented in 1927, replaced pendulum clocks as the world's best timekeepers. Pendulum clocks were used as time standards until World War 2, although the French Time Service continued using them in their official time standard ensemble until 1954. Pendulum gravimeters were superseded by "free fall" gravimeters in the 1950s, but pendulum instruments continued to be used into the 1970s.
Use for time measurement.
For 300 years, from its discovery around 1581 until development of the quartz clock in the 1930s, the pendulum was the world's standard for accurate timekeeping. In addition to clock pendulums, freeswinging seconds pendulums were widely used as precision timers in scientific experiments in the 17th and 18th centuries. Pendulums require great mechanical stability: a length change of only 0.02%, 0.2 mm in a grandfather clock pendulum, will cause an error of a minute per week.
Clock pendulums.
Pendulums in clocks (see example at right) are usually made of a weight or bob "(b)" suspended by a rod of wood or metal "(a)". To reduce air resistance (which accounts for most of the energy loss in clocks) the bob is traditionally a smooth disk with a lens-shaped cross section, although in antique clocks it often had carvings or decorations specific to the type of clock. In quality clocks the bob is made as heavy as the suspension can support and the movement can drive, since this improves the regulation of the clock (see Accuracy below). A common weight for seconds pendulum bobs is 15 lb. Instead of hanging from a pivot, clock pendulums are usually supported by a short straight spring "(d)" of flexible metal ribbon. This avoids the friction and 'play' caused by a pivot, and the slight bending force of the spring merely adds to the pendulum's restoring force. A few precision clocks have pivots of 'knife' blades resting on agate plates. The impulses to keep the pendulum swinging are provided by an arm hanging behind the pendulum called the "crutch", "(e)", which ends in a "fork", "(f)" whose prongs embrace the pendulum rod. The crutch is pushed back and forth by the clock's escapement, "(g,h)".
Each time the pendulum swings through its centre position, it releases one tooth of the "escape wheel" "(g)". The force of the clock's mainspring or a driving weight hanging from a pulley, transmitted through the clock's gear train, causes the wheel to turn, and a tooth presses against one of the pallets "(h)", giving the pendulum a short push. The clock's wheels, geared to the escape wheel, move forward a fixed amount with each pendulum swing, advancing the clock's hands at a steady rate.
The pendulum always has a means of adjusting the period, usually by an adjustment nut "(c)" under the bob which moves it up or down on the rod. Moving the bob up decreases the pendulum's length, causing the pendulum to swing faster and the clock to gain time. Some precision clocks have a small auxiliary adjustment weight on a threaded shaft on the bob, to allow finer adjustment. Some tower clocks and precision clocks use a tray attached near to the midpoint of the pendulum rod, to which small weights can be added or removed. This effectively shifts the centre of oscillation and allows the rate to be adjusted without stopping the clock.
The pendulum must be suspended from a rigid support. During operation, any elasticity will allow tiny imperceptible swaying motions of the support, which disturbs the clock's period, resulting in error. Pendulum clocks should be attached firmly to a sturdy wall.
The most common pendulum length in quality clocks, which is always used in grandfather clocks, is the seconds pendulum, about 1 m long. In mantel clocks, half-second pendulums, 25 cm long, or shorter, are used. Only a few large tower clocks use longer pendulums, the 1.5 second pendulum, 2.25 m long, or occasionally the two-second pendulum, 4 m as is the case of Big Ben.
Temperature compensation.
The largest source of error in early pendulums was slight changes in length due to thermal expansion and contraction of the pendulum rod with changes in ambient temperature. This was discovered when people noticed that pendulum clocks ran slower in summer, by as much as a minute per week (one of the first was Godefroy Wendelin, as reported by Huygens in 1658). Thermal expansion of pendulum rods was first studied by Jean Picard in 1669. A pendulum with a steel rod will expand by about 11.3 parts per million (ppm) with each degree Celsius increase, causing it to lose about 0.27 seconds per day for every degree Celsius increase in temperature, or 9 seconds per day for a 33 C-change change. Wood rods expand less, losing only about 6 seconds per day for a 33 C-change change, which is why quality clocks often had wooden pendulum rods. The wood had to be varnished to prevent water vapor from getting in, because changes in humidity also affected the length.
Mercury pendulum.
The first device to compensate for this error was the mercury pendulum, invented by George Graham in 1721. The liquid metal mercury expands in volume with temperature. In a mercury pendulum, the pendulum's weight (bob) is a container of mercury. With a temperature rise, the pendulum rod gets longer, but the mercury also expands and its surface level rises slightly in the container, moving its centre of mass closer to the pendulum pivot. By using the correct height of mercury in the container these two effects will cancel, leaving the pendulum's centre of mass, and its period, unchanged with temperature. Its main disadvantage was that when the temperature changed, the rod would come to the new temperature quickly but the mass of mercury might take a day or two to reach the new temperature, causing the rate to deviate during that time. To improve thermal accommodation several thin containers were often used, made of metal. Mercury pendulums were the standard used in precision regulator clocks into the 20th century.
Gridiron pendulum.
The most widely used compensated pendulum was the gridiron pendulum, invented in 1726 by John Harrison. This consists of alternating rods of two different metals, one with lower thermal expansion (CTE), steel, and one with higher thermal expansion, zinc or brass. The rods are connected by a frame, as shown in the drawing at the right, so that an increase in length of the zinc rods pushes the bob up, shortening the pendulum. With a temperature increase, the low expansion steel rods make the pendulum longer, while the high expansion zinc rods make it shorter. By making the rods of the correct lengths, the greater expansion of the zinc cancels out the expansion of the steel rods which have a greater combined length, and the pendulum stays the same length with temperature.
Zinc-steel gridiron pendulums are made with 5 rods, but the thermal expansion of brass is closer to steel, so brass-steel gridirons usually require 9 rods. Gridiron pendulums adjust to temperature changes faster than mercury pendulums, but scientists found that friction of the rods sliding in their holes in the frame caused gridiron pendulums to adjust in a series of tiny jumps. In high precision clocks this caused the clock's rate to change suddenly with each jump. Later it was found that zinc is subject to creep. For these reasons mercury pendulums were used in the highest precision clocks, but gridirons were used in quality regulator clocks.
Gridiron pendulums became so associated with good quality that, to this day, many ordinary clock pendulums have decorative 'fake' gridirons that don't actually have any temperature compensation function.
Invar and fused quartz.
Around 1900 low thermal expansion materials were developed which, when used as pendulum rods, made elaborate temperature compensation unnecessary. These were only used in a few of the highest precision clocks before the pendulum became obsolete as a time standard. In 1896 Charles Édouard Guillaume invented the nickel steel alloy Invar. This has a CTE of around 0.5 µin/(in·°F), resulting in pendulum temperature errors over 71 °F of only 1.3 seconds per day, and this residual error could be compensated to zero with a few centimeters of aluminium under the pendulum bob (this can be seen in the Riefler clock image above). Invar pendulums were first used in 1898 in the Riefler regulator clock which achieved accuracy of 15 milliseconds per day. Suspension springs of Elinvar were used to eliminate temperature variation of the spring's restoring force on the pendulum. Later fused quartz was used which had even lower CTE. These materials are the choice for modern high accuracy pendulums.
Atmospheric pressure.
The effect of the surrounding air on a moving pendulum is complex and requires fluid mechanics to calculate precisely, but for most purposes its influence on the period can be accounted for by three effects:
So increases in barometric pressure increase a pendulum's period slightly due to the first two effects, by about 0.11 seconds per day per kilopascal (0.37 seconds per day per inch of mercury or 0.015 seconds per day per torr). Researchers using pendulums to measure the acceleration of gravity had to correct the period for the air pressure at the altitude of measurement, computing the equivalent period of a pendulum swinging in vacuum. A pendulum clock was first operated in a constant-pressure tank by Friedrich Tiede in 1865 at the Berlin Observatory, and by 1900 the highest precision clocks were mounted in tanks that were kept at a constant pressure to eliminate changes in atmospheric pressure. Alternatively, in some a small aneroid barometer mechanism attached to the pendulum compensated for this effect.
Gravity.
Pendulums are affected by changes in gravitational acceleration, which varies by as much as 0.5% at different locations on Earth, so pendulum clocks have to be recalibrated after a move. Even moving a pendulum clock to the top of a tall building can cause it to lose measurable time from the reduction in gravity.
Accuracy of pendulums as timekeepers.
The timekeeping elements in all clocks, which include pendulums, balance wheels, the quartz crystals used in quartz watches, and even the vibrating atoms in atomic clocks, are in physics called harmonic oscillators. The reason harmonic oscillators are used in clocks is that they vibrate or oscillate at a specific resonant frequency or period and resist oscillating at other rates. However, the resonant frequency is not infinitely 'sharp'. Around the resonant frequency there is a narrow natural band of frequencies (or periods), called the resonance width or bandwidth, where the harmonic oscillator will oscillate. In a clock, the actual frequency of the pendulum may vary randomly within this bandwidth in response to disturbances, but at frequencies outside this band, the clock will not function at all.
"Q" factor.
The measure of a harmonic oscillator's resistance to disturbances to its oscillation period is a dimensionless parameter called the "Q" factor equal to the resonant frequency divided by the bandwidth. The higher the "Q", the smaller the bandwidth, and the more constant the frequency or period of the oscillator for a given disturbance. The reciprocal of the Q is roughly proportional to the limiting accuracy achievable by a harmonic oscillator as a time standard.
The "Q" is related to how long it takes for the oscillations of an oscillator to die out. The "Q" of a pendulum can be measured by counting the number of oscillations it takes for the amplitude of the pendulum's swing to decay to 1/"e" = 36.8% of its initial swing, and multiplying by 2"π".
In a clock, the pendulum must receive pushes from the clock's movement to keep it swinging, to replace the energy the pendulum loses to friction. These pushes, applied by a mechanism called the escapement, are the main source of disturbance to the pendulum's motion. The "Q" is equal to 2"π" times the energy stored in the pendulum, divided by the energy lost to friction during each oscillation period, which is the same as the energy added by the escapement each period. It can be seen that the smaller the fraction of the pendulum's energy that is lost to friction, the less energy needs to be added, the less the disturbance from the escapement, the more 'independent' the pendulum is of the clock's mechanism, and the more constant its period is. The "Q" of a pendulum is given by:
where "M" is the mass of the bob, "ω" = 2"π"/"T" is the pendulum's radian frequency of oscillation, and "Γ" is the frictional damping force on the pendulum per unit velocity.
"ω" is fixed by the pendulum's period, and "M" is limited by the load capacity and rigidity of the suspension. So the "Q" of clock pendulums is increased by minimizing frictional losses ("Γ"). Precision pendulums are suspended on low friction pivots consisting of triangular shaped 'knife' edges resting on agate plates. Around 99% of the energy loss in a freeswinging pendulum is due to air friction, so mounting a pendulum in a vacuum tank can increase the "Q", and thus the accuracy, by a factor of 100.
The "Q" of pendulums ranges from several thousand in an ordinary clock to several hundred thousand for precision regulator pendulums swinging in vacuum. A quality home pendulum clock might have a "Q" of 10,000 and an accuracy of 10 seconds per month. The most accurate commercially produced pendulum clock was the Shortt-Synchronome free pendulum clock, invented in 1921. Its Invar master pendulum swinging in a vacuum tank had a "Q" of 110,000 and an error rate of around a second per year.
Their Q of 103–105 is one reason why pendulums are more accurate timekeepers than the balance wheels in watches, with "Q" around 100-300, but less accurate than the quartz crystals in quartz clocks, with "Q" of 105–106.
Escapement.
Pendulums (unlike, for example, quartz crystals) have a low enough "Q" that the disturbance caused by the impulses to keep them moving is generally the limiting factor on their timekeeping accuracy. Therefore the design of the escapement, the mechanism that provides these impulses, has a large effect on the accuracy of a clock pendulum. If the impulses given to the pendulum by the escapement each swing could be exactly identical, the response of the pendulum would be identical, and its period would be constant. However, this is not achievable; unavoidable random fluctuations in the force due to friction of the clock's pallets, lubrication variations, and changes in the torque provided by the clock's power source as it runs down, mean that the force of the impulse applied by the escapement varies.
If these variations in the escapement's force cause changes in the pendulum's width of swing (amplitude), this will cause corresponding slight changes in the period, since (as discussed at top) a pendulum with a finite swing is not quite isochronous. Therefore, the goal of traditional escapement design is to apply the force with the proper profile, and at the correct point in the pendulum's cycle, so force variations have no effect on the pendulum's amplitude. This is called an "isochronous escapement".
The Airy condition.
In 1826 British astronomer George Airy proved what clockmakers had known for centuries; that the disturbing effect of a drive force on the period of a pendulum is smallest if given as a short impulse as the pendulum passes through its bottom equilibrium position. Specifically, he proved that if a pendulum is driven by an impulse that is symmetrical about its bottom equilibrium position, the pendulum's amplitude will be unaffected by changes in the drive force. The most accurate escapements, such as the deadbeat, approximately satisfy this condition.
Gravity measurement.
The presence of the acceleration of gravity "g" in the periodicity equation (1) for a pendulum means that the local gravitational acceleration of the Earth can be calculated from the period of a pendulum. A pendulum can therefore be used as a gravimeter to measure the local gravity, which varies by over 0.5% across the surface of the Earth. The pendulum in a clock is disturbed by the pushes it receives from the clock movement, so freeswinging pendulums were used, and were the standard instruments of gravimetry up to the 1930s.
The difference between clock pendulums and gravimeter pendulums is that to measure gravity, the pendulum's length as well as its period has to be measured. The period of freeswinging pendulums could be found to great precision by comparing their swing with a precision clock that had been adjusted to keep correct time by the passage of stars overhead. In the early measurements, a weight on a cord was suspended in front of the clock pendulum, and its length adjusted until the two pendulums swung in exact synchronism. Then the length of the cord was measured. From the length and the period, "g" could be calculated from equation (1).
The seconds pendulum.
 The seconds pendulum, a pendulum with a period of two seconds so each swing takes one second, was widely used to measure gravity, because most precision clocks had seconds pendulums. By the late 17th century, the length of the seconds pendulum became the standard measure of the strength of gravitational acceleration at a location. By 1700 its length had been measured with submillimeter accuracy at several cities in Europe. For a seconds pendulum, "g" is proportional to its length:
Kater's pendulum.
The precision of the early gravity measurements above was limited by the difficulty of measuring the length of the pendulum, "L" . "L" was the length of an idealized simple gravity pendulum (described at top), which has all its mass concentrated in a point at the end of the cord. In 1673 Huygens had shown that the period of a real pendulum (called a "compound pendulum") was equal to the period of a simple pendulum with a length equal to the distance between the pivot point and a point called the center of oscillation, located under the center of gravity, that depends on the mass distribution along the pendulum. But there was no accurate way of determining the center of oscillation in a real pendulum.
To get around this problem, the early researchers above approximated an ideal simple pendulum as closely as possible by using a metal sphere suspended by a light wire or cord. If the wire was light enough, the center of oscillation was close to the center of gravity of the ball, at its geometric center. This "ball and wire" type of pendulum wasn't very accurate, because it didn't swing as a rigid body, and the elasticity of the wire caused its length to change slightly as the pendulum swung.
However Huygens had also proved that in any pendulum, the pivot point and the center of oscillation were interchangeable. That is, if a pendulum were turned upside down and hung from its center of oscillation, it would have the same period as it did in the previous position, and the old pivot point would be the new center of oscillation.
British physicist and army captain Henry Kater in 1817 realized that Huygens' principle could be used to find the length of a simple pendulum with the same period as a real pendulum. If a pendulum was built with a second adjustable pivot point near the bottom so it could be hung upside down, and the second pivot was adjusted until the periods when hung from both pivots were the same, the second pivot would be at the center of oscillation, and the distance between the two pivots would be the length of a simple pendulum with the same period.
Kater built a reversible pendulum (shown at right) consisting of a brass bar with two opposing pivots made of short triangular "knife" blades "(a)" near either end. It could be swung from either pivot, with the knife blades supported on agate plates. Rather than make one pivot adjustable, he attached the pivots a meter apart and instead adjusted the periods with a moveable weight on the pendulum rod "(b,c)". In operation, the pendulum is hung in front of a precision clock, and the period timed, then turned upside down and the period timed again. The weight is adjusted with the adjustment screw until the periods are equal. Then putting this period and the distance between the pivots into equation (1) gives the gravitational acceleration "g" very accurately.
Kater timed the swing of his pendulum using the "method of coincidences" and measured the distance between the two pivots with a micrometer. After applying corrections for the finite amplitude of swing, the buoyancy of the bob, the barometric pressure and altitude, and temperature, he obtained a value of 39.13929 inches for the seconds pendulum at London, in vacuum, at sea level, at 62 °F. The largest variation from the mean of his 12 observations was 0.00028 in. representing a precision of gravity measurement of 7×10−6 (7 mGal or 70 µm/s2). Kater's measurement was used as Britain's official standard of length (see below) from 1824 to 1855.
Reversible pendulums (known technically as "convertible" pendulums) employing Kater's principle were used for absolute gravity measurements into the 1930s.
Later pendulum gravimeters.
The increased accuracy made possible by Kater's pendulum helped make gravimetry a standard part of geodesy. Since the exact location (latitude and longitude) of the 'station' where the gravity measurement was made was necessary, gravity measurements became part of surveying, and pendulums were taken on the great geodetic surveys of the 18th century, particularly the Great Trigonometric Survey of India.
Relative pendulum gravimeters were superseded by the simpler LaCoste zero-length spring gravimeter, invented in 1934 by Lucien LaCoste. Absolute (reversible) pendulum gravimeters were replaced in the 1950s by free fall gravimeters, in which a weight is allowed to fall in a vacuum tank and its acceleration is measured by an optical interferometer.
Standard of length.
Because the acceleration of gravity is constant at a given point on Earth, the period of a simple pendulum at a given location depends only on its length. Additionally, gravity varies only slightly at different locations. Almost from the pendulum's discovery until the early 19th century, this property led scientists to suggest using a pendulum of a given period as a standard of length.
Until the 19th century, countries based their systems of length measurement on prototypes, metal bar primary standards, such as the standard yard in Britain kept at the Houses of Parliament, and the standard "toise" in France, kept at Paris. These were vulnerable to damage or destruction over the years, and because of the difficulty of comparing prototypes, the same unit often had different lengths in distant towns, creating opportunities for fraud. Enlightenment scientists argued for a length standard that was based on some property of nature that could be determined by measurement, creating an indestructible, universal standard. The period of pendulums could be measured very precisely by timing them with clocks that were set by the stars. A pendulum standard amounted to defining the unit of length by the gravitational force of the Earth, for all intents constant, and the second, which was defined by the rotation rate of the Earth, also constant. The idea was that anyone, anywhere on Earth, could recreate the standard by constructing a pendulum that swung with the defined period and measuring its length.
Virtually all proposals were based on the seconds pendulum, in which each swing (a half period) takes one second, which is about a meter (39 inches) long, because by the late 17th century it had become a standard for measuring gravity (see previous section). By the 18th century its length had been measured with sub-millimeter accuracy at a number of cities in Europe and around the world.
The initial attraction of the pendulum length standard was that it was believed (by early scientists such as Huygens and Wren) that gravity was constant over the Earth's surface, so a given pendulum had the same period at any point on Earth. So the length of the standard pendulum could be measured at any location, and would not be tied to any given nation or region; it would be a truly democratic, worldwide standard. Although Richer found in 1672 that gravity varies at different points on the globe, the idea of a pendulum length standard remained popular, because it was found that gravity only varies with latitude. Gravitational acceleration increases smoothly from the equator to the poles, due to the oblate shape of the Earth. So at any given latitude (east-west line), gravity was constant enough that the length of a seconds pendulum was the same within the measurement capability of the 18th century. So the unit of length could be defined at a given latitude and measured at any point at that latitude. For example, a pendulum standard defined at 45° north latitude, a popular choice, could be measured in parts of France, Italy, Croatia, Serbia, Romania, Russia, Kazakhstan, China, Mongolia, the United States and Canada. In addition, it could be recreated at any location at which the gravitational acceleration had been accurately measured.
By the mid 19th century, increasingly accurate pendulum measurements by Edward Sabine and Thomas Young revealed that gravity, and thus the length of any pendulum standard, varied measurably with local geologic features such as mountains and dense subsurface rocks. So a pendulum length standard had to be defined at a single point on Earth and could only be measured there. This took much of the appeal from the concept, and efforts to adopt pendulum standards were abandoned.
Early proposals.
One of the first to suggest defining length with a pendulum was Flemish scientist Isaac Beeckman who in 1631 recommended making the seconds pendulum "the invariable measure for all people at all times in all places". Marin Mersenne, who first measured the seconds pendulum in 1644, also suggested it. The first official proposal for a pendulum standard was made by the British Royal Society in 1660, advocated by Christiaan Huygens and Ole Rømer, basing it on Mersenne's work, and Huygens in "Horologium Oscillatorium" proposed a "horary foot" defined as 1/3 of the seconds pendulum. Christopher Wren was another early supporter. The idea of a pendulum standard of length must have been familiar to people as early as 1663, because Samuel Butler satirizes it in "Hudibras":
In 1671 Jean Picard proposed a pendulum defined 'universal foot' in his influential "Mesure de la Terre". Gabriel Mouton around 1670 suggested defining the "toise" either by a seconds pendulum or a minute of terrestrial degree. A plan for a complete system of units based on the pendulum was advanced in 1675 by Italian polymath Tito Livio Burratini. In France in 1747, geographer Charles Marie de la Condamine proposed defining length by a seconds pendulum at the equator; since at this location a pendulum's swing wouldn't be distorted by the Earth's rotation. British politicians James Steuart (1780) and George Skene Keith were also supporters.
By the end of the 18th century, when many nations were reforming their weight and measure systems, the seconds pendulum was the leading choice for a new definition of length, advocated by prominent scientists in several major nations. In 1790, then US Secretary of State Thomas Jefferson proposed to Congress a comprehensive decimalized US 'metric system' based on the seconds pendulum at 38° North latitude, the mean latitude of the United States. No action was taken on this proposal. In Britain the leading advocate of the pendulum was politician John Riggs Miller. When his efforts to promote a joint British–French–American metric system fell through in 1790, he proposed a British system based on the length of the seconds pendulum at London. This standard was adopted in 1824 (below).
The metre.
In the discussions leading up to the French adoption of the metric system in 1791, the leading candidate for the definition of the new unit of length, the metre, was the seconds pendulum at 45° North latitude. It was advocated by a group led by French politician Talleyrand and mathematician Antoine Nicolas Caritat de Condorcet. This was one of the three final options considered by the French Academy of Sciences committee. However, on March 19, 1791 the committee instead chose to base the metre on the length of the meridian through Paris. A pendulum definition was rejected because of its variability at different locations, and because it defined length by a unit of time. (However, since 1983 the metre has been officially defined in terms of the length of the second and the speed of light.) A possible additional reason is that the radical French Academy didn't want to base their new system on the second, a traditional and nondecimal unit from the "ancien regime".
Although not defined by the pendulum, the final length chosen for the metre, 10−7 of the pole-to-equator meridian arc, was very close to the length of the seconds pendulum (0.9937 m), within 0.63%. Although no reason for this particular choice was given at the time, it was probably to facilitate the use of the seconds pendulum as a secondary standard, as was proposed in the official document. So the modern world's standard unit of length is certainly closely linked historically with the seconds pendulum.
Britain and Denmark.
Britain and Denmark appear to be the only nations that (for a short time) based their units of length on the pendulum. In 1821 the Danish inch was defined as 1/38 of the length of the mean solar seconds pendulum at 45° latitude at the meridian of Skagen, at sea level, in vacuum. The British parliament passed the "Imperial Weights and Measures Act" in 1824, a reform of the British standard system which declared that if the prototype standard yard was destroyed, it would be recovered by defining the inch so that the length of the solar seconds pendulum at London, at sea level, in a vacuum, at 62 °F was 39.1393 inches. This also became the US standard, since at the time the US used British measures. However, when the prototype yard was lost in the 1834 Houses of Parliament fire, it proved impossible to recreate it accurately from the pendulum definition, and in 1855 Britain repealed the pendulum standard and returned to prototype standards.
Other uses.
Seismometers.
A pendulum in which the rod is not vertical but almost horizontal was used in early seismometers for measuring earth tremors. The bob of the pendulum does not move when its mounting does, and the difference in the movements is recorded on a drum chart.
Schuler tuning.
As first explained by Maximilian Schuler in a 1923 paper, a pendulum whose period exactly equals the orbital period of a hypothetical satellite orbiting just above the surface of the earth (about 84 minutes) will tend to remain pointing at the center of the earth when its support is suddenly displaced. This principle, called Schuler tuning, is used in inertial guidance systems in ships and aircraft that operate on the surface of the Earth. No physical pendulum is used, but the control system that keeps the inertial platform containing the gyroscopes stable is modified so the device acts as though it is attached to such a pendulum, keeping the platform always facing down as the vehicle moves on the curved surface of the Earth.
Coupled pendulums.
In 1665 Huygens made a curious observation about pendulum clocks. Two clocks had been placed on his mantlepiece, and he noted that they had acquired an opposing motion. That is, their pendulums were beating in unison but in the opposite direction; 180° out of phase. Regardless of how the two clocks were started, he found that they would eventually return to this state, thus making the first recorded observation of a coupled oscillator.
The cause of this behavior was that the two pendulums were affecting each other through slight motions of the supporting mantlepiece. This process is called entrainment or mode locking in physics and is observed in other coupled oscillators. Synchronized pendulums have been used in clocks and were widely used in gravimeters in the early 20th century. Although Huygens only observed out-of-phase synchronization, recent investigations have shown the existence of in-phase synchronization, as well as "death" states wherein one or both clocks stops.
Religious practice.
Pendulum motion appears in religious ceremonies as well. The swinging incense burner called a censer, also known as a thurible, is an example of a pendulum. Pendulums are also seen at many gatherings in eastern Mexico where they mark the turning of the tides on the day which the tides are at their highest point. See also pendulums for divination and dowsing.
Execution.
During the Middle Ages, pendulums were used as a method of torture by the Spanish Inquisition. Using the basic principle of the pendulum, the weight (bob) is replaced by an axe head. The victim is strapped to a table below, the device is activated, and the axe begins to swing back and forth through the air. With each pass, or return, the pendulum is lowered, gradually coming closer to the victim's torso, until finally cleaved. Because of the time required before the mortal action of the axe is complete, the pendulum is considered a method of torturing the victim before his or her demise.
Notes.
The value of g reflected by the period of a pendulum varies from place to place. The gravitational force varies with distance from the center of the Earth, i.e. with altitude - or because the Earth's shape is oblate, g varies with latitude.
A more important cause of this reduction in g at the equator is because the equator is spinning at one revolution per day, reducing the gravitational force there.
References.
Note: most of the sources below, including books, can be viewed online through the links given.
Further reading.
</dl>

</doc>
<doc id="42713" url="http://en.wikipedia.org/wiki?curid=42713" title="Special Olympics">
Special Olympics

Special Olympics is the world's largest sports organization for children and adults with intellectual disabilities, providing year-round training and competitions to more than 4.4 million athletes in 170 countries.
Special Olympics competitions are held every day, all around the world—including local, national and regional competitions, adding up to more than 70,000 events a year.
These competitions include the Special Olympics World Games, which alternate between summer and winter games. Special Olympics World Games are held every four years. The most recent World Summer Games were the Special Olympics World Summer Games, held in Athens, Greece (The birthplace of the modern Olympic Games), from June 25, 2011 to July 4, 2011.
The most recent Special Olympics World Winter Games were held in Pyeongchang, South Korea from January 29 to February 5, 2013. At the same time, the first Special Olympics Global Development Summit was held on "Ending the Cycle of Poverty and Exclusion for People with Intellectual Disabilities," gathering government officials, activists and business leaders from around the world 
The next World Games will be the 2015 Special Olympics World Summer Games in Los Angeles, California from July 25 to August 2, 2015. Graz and Schladming, Austria will host the next Special Olympics World Winter Games from March 14–24, 2017.
History.
In June 1962, Eunice Kennedy Shriver started a day camp for children with intellectual disabilities at her home in Potomac, Maryland. She started this camp because she was concerned about children with intellectual disabilities having nowhere to play. Using Camp Shriver as an example, Shriver, who was head of the Joseph P. Kennedy Jr. Foundation and part of President Kennedy's Panel on Mental Retardation, promoted the concept of involvement in physical activity and competition opportunities for people with intellectual disabilities. Camp Shriver became an annual event, and the Kennedy Foundation (of which Shriver was executive vice president) gave grants to universities, recreation departments and community centers to hold similar camps.
Shriver and Kennedy’s oldest sister, Rosemary Kennedy, underwent a lobotomy in an effort by their father to cure her mental disability. The brain damage inflicted by the operation caused her to be permanently incapacitated. This disability is often credited as Shriver's inspiration to form Special Olympics, but Shriver told "The New York Times" in 1995 that was not exactly the case.
The first International Special Olympics Summer Games were held in 1968 at Soldier Field in Chicago. About 1500 athletes from the U.S. and Canada took part in the one-day event, which was a joint venture by the Kennedy Foundation and the Chicago Park District. Anne McGlone Burke, a physical education teacher with the Chicago Park District and recipient of a Kennedy Foundation grant, began with the idea for a one-time Olympic-style athletic competition for people with special needs. Burke then approached Eunice Kennedy Shriver to fund the event. Shriver encouraged Burke to expand on the idea and the JPK Jr. Foundation provided a grant of $25,000.
The advisory committee to the Chicago Special Olympics included Dr. William Freeberg, Southern Illinois University; Dr. Frank J. Hayden, Joseph P. Kennedy Foundation; Dr. Arthur Peavy; William McFetridge, Anne McGlone Burke and Stephen Kelly of the Chicago Park District; and Olympic decathlon champion Rafer Johnson. Eunice Kennedy Shriver was honorary chairman. At the July 1968 games, Shriver announced the formation of Special Olympics and that more games would be held every two years as a "Biennial International Special Olympics.".
In 1971, The U.S. Olympic Committee gave the Special Olympics official approval to use the name “Olympics”.
The first 1977 Special Olympics World Winter Games were held in February 1977 in Steamboat Springs, Colorado, U.S.
In 1988, the Special Olympics was officially recognized by the International Olympic Committee (IOC).
In 1997, Healthy Athletes became an official Special Olympics initiative, offering health information and screenings to Special Olympics athletes worldwide. By 2010, the Healthy Athletes program had given free health screenings and treatment to more than 1 million people with intellectual disabilities.
In 2003 the first Special Olympics World Summer Games to be held outside of the United States took place in Dublin, Ireland. Approximately 7,000 athletes from 150 countries competed over 18 disciplines. The Dublin games were also the first to have their own opening and closing ceremonies broadcast live, performed by the President of Ireland, Mary McAleese.
Most significantly the 2003 games dramatically changed the perceptions and attitudes of society regarding the abilities and limitations of people with intellectual disabilities. The opening ceremony of the 2003 Games has been described by President McAleese as "a time when Ireland was at its superb best".
On October 30, 2004, President George W. Bush signed into law the "Special Olympics Sport and Empowerment Act," Public Law 108-406. The bill authorized funding for its Healthy Athletes, Education, and Worldwide Expansion programs. Co-sponsored by Representatives Roy Blunt (R-MO), and Steny Hoyer (D-MD), and Senators Rick Santorum (R-PA) and Harry Reid (D-NV), the bills were passed by unanimous consent in both chambers.
In July 2006, the first Special Olympics USA Games were held at Iowa State University. Teams from all 50 states and the District of Columbia participated.
In 2008, Special Olympics and Best Buddies International launched the Spread the Word to End the Word campaign to encourage individuals to stop using the word "retard" in everyday speech.
In 2011, Senators Tom Harkin and Roy Blunt and Representatives Steny Hoyer and Peter King introduced the Eunice Kennedy Shriver Act to authorize federal funding for Special Olympics Programs and Best Buddies Programs.
Symbols.
The Special Olympics logo has gone through several changes in its lifetime. The "stick figure" is an abstract but humanistic form designed to convey the impression of movement and activity. The logo is a symbol of growth, confidence and joy among children and adults with disabilities who are learning coordination, mastering skills, participating in competitions and preparing themselves for richer, more productive lives. The spherical appearance of the logo is a representation of Special Olympics' global outreach.
Participation.
Special Olympics programs are available for athletes free of charge. More than 4 million athletes are involved in Special Olympics sports training and competition in 180 countries. The organization offers year-round training and competition in 32 Olympic-style summer and winter sports.
People with intellectual disabilities are encouraged to join Special Olympics for the physical activity, which helps lower the rate of cardiovascular disease and obesity, among other health benefits. Also, they gain many emotional and psychological benefits, including self-confidence, social competence, building greater athletic skills and higher self-esteem. The motivations for joining the Special Olympics vary from one individual to the next; yet, there are common themes among individuals and their families that encourage them to either participate or abstain from the Special Olympics.
Special Olympics competitions are open to athletes ages 8 and up. For young people with intellectual disabilities ages 2–7, Special Olympics has a Young Athletes program—a sport and play program with a focus on fun activities that are important to mental and physical growth. Children engage in games and activities that develop motor skills and hand-eye coordination. Parents say their children in Young Athletes also develop better social skills. The confidence boost makes it easier for them to play and talk with other children on the playground and elsewhere . A study by the Center for Social Development and Education (University of Massachusetts, Boston) found that the activities also had the effect of helping children with intellectual disabilities learn routines and approaches to learning, along with how to follow rules and directions.
Families can also get involved with the Special Olympics experience. Family members support their athletes to the best of their ability, which may involve attending or volunteering at the events. By being involved they can boost their athlete's self-esteem and will be looked at as a constant source of encouragement.
Volunteers and supporters are an integral part of Special Olympics—and millions of people around the world are committed to its programs. Some are sponsors or donors. Many others are coaches, event volunteers and fans.
Coaches help the athletes be the best they can be regardless of ability—or disability. Special Olympics trains coaches through the Coaching Excellence program, which includes partnering with sports organizations. Special Olympics volunteers are introduced to lifetime friendships and great rewards.
There are many events that families and volunteers can get involved with, but the biggest event is the Law Enforcement Torch Run. The Torch Run involves police chiefs, police officers, secret service, FBI agents, military police, sheriffs, state troopers, prison guards, and other law enforcement personnel. They all get together to raise awareness and funds for Special Olympics. Ahead of a Special Olympics competition, law enforcement officers carry the torch in intervals along a planned route covering most of the state or country to the site of the opening ceremonies of the chapter or Special Olympics World Summer or Winter Games. Then they pass the torch to a Special Olympics athlete and together they run up to the cauldron and light it, signifying the beginning of the games.
The Special Olympics athlete's oath is "Let me win. But if I cannot win, let me be brave in the attempt." It was first introduced by Eunice Kennedy Shriver at the inaugural Special Olympics international games in Chicago in 1968.
Sports offered.
Special Olympics has more than 32 Olympic-type individual and team sports that provide meaningful training and competition opportunities for people with intellectual disabilities. A few are listed below:
The above list shows a few of the 32 sports that Special Olympics offers; there are several more recognized and demonstration sports, including Open Water Swimming, Kayaking, Floorball, Cricket, Netball and Beach Volleyball. Availability of sports can depend on location and season.
A key difference between Special Olympics competitions and those of other sports organizations is that athletes of all ability levels are encouraged to participate. Competitions are structured so that athletes compete with other athletes of similar ability in equitable divisions. An athlete's ability is the primary factor in divisioning Special Olympics competitions. The ability of an athlete or team is determined by an entry score from a prior competition or the result of a seeding round or preliminary event at the competition itself. Other factors that are significant in establishing competitive divisions are age and sex.
At competitions, medals are awarded to the first, second and third-place winners in each event and ribbons are awarded to athletes who finish in fourth through eighth place.
In the Young Athletes program, children ages 2–7 play simple sports and games. The focus is on fun activities that are important to mental and physical growth.
In 1968, track and field and swimming were the first two official sports offered by Special Olympics. As in the Olympics, events are introduced in training and then added to the competitive schedule, and from there the list of sports and events continued to grow.
Famous supporters.
The Special Olympics movement has attracted the support of a number of international sportsmen and other celebrities, including Rafer Johnson, Avril Lavigne, Bono, Joe Jonas, Dikembe Mutombo, Derek Poundstone, Pádraig Harrington, Jackie Chan, Zhang Ziyi, Yao Ming, Nadia Comaneci, Bart Conner, Vanessa Williams, Mary Alice Pearce DeVane, Colin Farrell and Arnold Schwarzenegger.
Nelson Mandela, Muhammad Ali and Quincy Jones took part in a 2003 Global Youth Summit at the Special Olympics World Summer Games in Dublin, Ireland.
U.S. President Bill Clinton took part in a Global Youth Summit during the 2005 Special Olympics World Winter Games in Nagano, Japan.
In 2011, Princess Charlene of Monaco, herself a former Olympian, was named as a Global Ambassador for Special Olympics. Olympic swimming legend Michael Phelps was also named a Global Ambassador and has taken part in aquatics clinics for Special Olympics swimmers in Shanghai, China and elsewhere. Other celebrity supporters include Olympic stars Michelle Kwan, Apolo Ohno, Kim Yuna, Yang Yang (A), and Scott Hamilton, and other sport greats Dikembe Mutombo, Pádraig Harrington, IK Kim, Dani Alves and Kaká, along with celebrities including Zhang Ziyi, Yang Lan, Brooklyn Decker, Lauren Alaina, Nicole Sherzinger, and the Wonder Girls. More recently, Olympic snowboarder Hannah Teter, Japanese football legend Hidetoshi Nakata, and WNBA player Elena Delle Donne, whose older sister Lizzie has multiple disabilities, were named Global Ambassadors in 2014.
Unified Sports.
In recent years, Special Olympics has pioneered the concept of Unified Sports, bringing together athletes with and without intellectual disabilities as teammates. The basic concept is that training together and playing together can create a path to friendship and understanding. The program has expanded beyond the U.S. and North America: a half-million people worldwide now take part in Special Olympics Unified Sports, breaking down stereotypes about people with intellectual disabilities.
A recent study of Special Olympics Unified Sports in Serbia, Poland, Ukraine, Germany and Hungary documented the program's benefits, including the effect of changing attitudes toward people with intellectual disabilities. As one Unified Sports partner said, "I am ashamed to say that I used to laugh at these people (people with intellectual disabilities), now I will tell anybody to stop laughing if I see it and I will stand up for people if I can." Other evaluations have also shown Unified Sports to be successful in building self-esteem and confidence in people with intellectual disabilities and also as a way to improve understanding and acceptance of people with intellectual disabilities among their non-disabled peers.
The Special Olympics Europe Eurasia Regional Research centre is based at the University of Ulster Jordanstown and is jointly led by Professor Roy McConkey and Professor David Hassan.
Healthy Athletes.
As Special Olympics began to grow, staffers and volunteers began to notice that athletes—children and adults with intellectual disabilities—also had many untreated health problems. In 1997, Special Olympics began an initiative called Healthy Athletes, which offers health screenings to athletes in need.
Healthy Athletes currently offers health screenings in seven areas: Fit Feet (podiatry), FUNfitness (physical therapy), Health Promotion (better health and well-being), Healthy Hearing (audiology), MedFest (sports physical exam), Opening Eyes (vision) and Special Smiles (dentistry). Screenings educate athletes on healthy and healthy choices and also identify problems that may need additional follow-up.
Since the Healthy Athletes program began, Special Olympics has become the largest global public health organization dedicated to serving people with intellectual disabilities. So far, more than 1.4 million Healthy Athletes screenings have been conducted for people with intellectual disabilities all around the world.
The Special Olympics health initiative has attracted high-profile partners, including the Hear the World Foundation, which screened more than 1,000 athletes during the most recent World Winter Games in Korea; more than 200 of them were found to have hearing loss.
In 2012, the Special Olympics Healthy Communities initiative launched in eight countries—Kazakhstan, Malawi, Malaysia, Mexico, Peru, Romania, South Africa and Thailand, as well as six U.S. states. The goal is to improve the health and well-being of people with intellectual disabilities and allow them to reach their full potential.
Criticism.
The Special Olympics program has occasionally been the subject of criticism. Scholar Keith Storey summarized many such objections in a 2004 article. 
In previous years, some have felt that the Special Olympics in itself are a form of segregation. This is because of the necessity to have a disability to participate. Storey argues that since some studies have shown that Special Olympic events do not lead to the reduction of prejudice and also reinforces negative stereotypes of people with intellectual disabilities. One of the main arguments against the Special Olympics organization, according to Storey, is that there is a lack of normalization and promotion of negative images. The lack of normalization comes from the differences in reaction to events. For example in previous years the Special Olympics would find people to stand at the finish line to hug the athletes once they've completed a race. Also, the Special Olympics do not announce anyone who's lost a race.
There are many other reasons that people object to Special Olympics, such as perceived promotion of Handicapism, the alleged promotion of corporations, and degrading paternalism, athletic ability. The promotion of Handicappism is a theory that when a set of practice is put in place to promote unequal treatment of people because of assumed intellectual disability it creates two classes of people, "normal" and "disabled". The integration of Corporations within the Special Olympics does help with fundraising and creates a large sum of donations to make these games possible. Yet, critics argue, such corporate involvement in Special Olympics is shallow public relations strategy that does little or nothing to integrate those with intellectual disabilities into the workforce at companies that sponsor Special Olympics. The term Paternalism, in this context, is used to describe posited problems with how the Special Olympics Organization is run. The board of directors have recognized only two of their board to have developmental disabilities. Therefore, the people doing the decision making and have the power of running this program are the people without disabilities. This double-standard, it is argued, reflects poorly on on the Disability rights movement where people with disabilities control the service delivery system rather than relying on people without disabilities.

</doc>
<doc id="42715" url="http://en.wikipedia.org/wiki?curid=42715" title="Pulse (legume)">
Pulse (legume)

A pulse (from Latin: "puls", from Ancient Greek πόλτος: "poltos" "porridge"), sometimes called a "grain legume", is an annual leguminous crop yielding from one to twelve seeds of variable size, shape, and color within a pod. Pulses are used for food for humans and other animals. Included in the pulses are: dry beans like pinto beans, kidney beans and navy beans; dry peas; lentils; and others.
Like many leguminous crops, pulses play a key role in crop rotation due to their ability to fix nitrogen. To support the awareness on this matter, the United Nations declared 2016 the UN International Year of Pulses.
The words "bean", "lentil", and "pulse" may refer to just the seed or to the entire plant.
Interpretations.
The term "pulse", as used by the United Nations' Food and Agricultural Organization (FAO), is reserved for crops harvested solely for the dry seed. This excludes green beans and green peas, which are considered vegetable crops. Also excluded are crops that are mainly grown for oil extraction (oilseeds like soybeans and peanuts), and crops which are used exclusively for sowing (clovers, alfalfa). However, in common use, these distinctions are not clearly made, and many of the varieties so classified and given below are also used as vegetables, with their beans in pods while young; cooked in whole cuisines; and sold for the purpose; for example, black-eyed beans, lima beans and Toor or pigeon peas are thus eaten as fresh green beans, or cooked as part of a meal.
History.
Archaeologists have discovered traces of pulse production around Ravi River (Punjab), the seat of the Indus Valley civilization, dating circa 3300 BC. Meanwhile, evidence of lentil cultivation has also been found in Egyptian pyramids and dry pea seeds have been discovered in a Swiss village that are believed to date back to the Stone Age. Archaeological evidence suggests that these peas must have been grown in the eastern Mediterranean and Mesopotamia regions at least 5,000 years ago and in Britain as early as the 11th century.
World economy.
India is the world's largest producer and the largest consumer of pulses. Pakistan, Canada, Burma, Australia and the United States, in that order, are significant exporters and are India's most significant suppliers. Canada now accounts for approximately 35% of global pulse trade each year.
The global pulse market is estimated at 60 million tonnes.
Classification.
FAO recognizes 11 primary pulses.
Nutrients.
Pulses provide protein, complex carbohydrates, and several vitamins and minerals. Like other plant-based foods, they contain no cholesterol and little fat or sodium. Pulses also provide iron, magnesium, phosphorus, zinc and other minerals, which play a variety of roles in maintaining good health.
Pulses are 20 to 25% protein by weight, which is double the protein content of wheat and three times that of rice. While pulses are generally high in protein, and the digestibility of that protein is also high, they are often relatively poor in methionine, an essential amino acid. Grains (which are themselves deficient in lysine) are commonly consumed along with pulses to form a complete diet of protein. Indian cuisine also includes sesame seeds, which contain high levels of methionine.
Health.
There is evidence that a portion of pulses (roughly one cup daily) in a diet may help lower blood pressure and reduce LDL cholesterol levels, though there is a concern about the quality of the supporting data.

</doc>
<doc id="42716" url="http://en.wikipedia.org/wiki?curid=42716" title="Nelly Furtado">
Nelly Furtado

Nelly Kim Furtado CIH (born December 2, 1978) is a Canadian singer and songwriter. She has sold 20 million albums worldwide and more than 20 million singles, bringing her total sales to over 40 million records around the world. Furtado first gained fame with her debut album, "Whoa, Nelly!", which spawned two successful singles, "I'm Like a Bird" and "Turn Off the Light". "I'm Like A Bird" won a 2001 Juno Award for Single of the Year and a 2002 Grammy Award for Best Female Pop Vocal Performance. In 2003, Furtado released "Folklore", which produced three international singles— "Powerless (Say What You Want)", "Try" and "Força". 
Three years later she released "Loose", a worldwide commercial success with 10 million copies sold. The album spawned four number-one hits: "Promiscuous", "Maneater", "Say It Right" and "All Good Things (Come to an End)". After a three-year break, she released her first full-length Spanish album, "Mi Plan", and Furtado received a Latin Grammy for Best Female Pop Vocal Album. In 2012, Furtado's fourth English-language studio album, "The Spirit Indestructible", was released. 
Furtado's work has earned her numerous awards and accolades, including two Grammy Awards, 10 Juno Awards, three MuchMusic Video Awards and a star on Canada's Walk of Fame. Furtado was awarded "Commander of the Order of Prince Henry" on February 28, 2014, in Toronto by Aníbal Cavaco Silva, the President of Portugal.
Early life.
Furtado was born on December 2, 1978, in Victoria, British Columbia, Canada. Her Portuguese parents, Maria Manuela and António José Furtado, were both from São Miguel Island in the Azores who had emigrated to Canada in the late 1960s. Nelly was named after Soviet gymnast Nellie Kim. Her siblings are Michael Anthony and Lisa Anne. They were raised Roman Catholic. At age four, she began performing and singing in Portuguese. Furtado's first public performance was when she sang a duet with her mother at a church on Portugal Day. She began playing musical instruments at the age of nine, learning the trombone, ukulele and – in later years – the guitar and keyboards. At the age of 12, she began writing songs, and as a teenager, she performed in a Portuguese marching band.
Furtado has acknowledged her family as the source of her strong work ethic; she spent eight summers working as a chambermaid with her mother, along with her brother and sister, who was a housekeeper in Victoria. She has stated that coming from a working-class background has shaped her identity in a positive way.
Career.
Career beginnings.
During a visit with her sister Lisa Anne in Toronto, the summer after grade eleven, Furtado met Tallis Newkirk, member of the hip hop group Plains of Fascination. She contributed vocals to their 1996 album, "Join the Ranks," on the track "Waitin' 4 The Streets." After graduating from Mount Douglas Secondary School in 1996, she moved to Toronto to reside with her sister Lisa Anne. The following year, she formed Nelstar, a trip hop duo with Newkirk. Ultimately, Furtado felt the trip-hop style of the duo was "too segregated," and believed it did not represent her personality or allow her to showcase her vocal ability. She left the group and planned to move back home.
In 1997, she performed at the Honey Jam talent show. Her performance attracted the attention of The Philosopher Kings singer Gerald Eaton, who then approached her to write with him. He and fellow Kings member Brian West helped Furtado produce a demo. She left Toronto, but returned again to record more material with Eaton and West. The material recorded during these sessions which were shopped to record companies by her attorney Chris Taylor and led to her 1999 record deal with DreamWorks Records, signed by A&R executive Beth Halper, partner of Garbage drummer and record producer Butch Vig. Furtado's first single, "Party's Just Begun (Again)", was released that year on the "".
2000–05: "Whoa, Nelly!" and "Folklore".
Furtado continued the collaboration with Eaton and West, who co-produced her debut album, "Whoa, Nelly!", which was released in October 2000. The album was an international success, supported by three international singles: "I'm like a Bird", "Turn off the Light", and "...On the Radio (Remember the Days)". It received four Grammy nominations in 2002, and her debut single won for Best Female Pop Vocal Performance. Furtado's work was also critically acclaimed for her innovative mixture of various genres and sounds. "Slant Magazine" called the album "a delightful and refreshing antidote to the army of 'pop princesses' and rap-metal bands that had taken over popular music at the turn of the millennium". The sound of the album was strongly influenced by musicians who had traversed cultures and "the challenge of making heartfelt, emotional music that's upbeat and hopeful". According to "Maclean's" magazine, "Whoa, Nelly!" had sold six million copies worldwide as of August 2006. Portions of the song "Scared of You" are in Portuguese, while "Onde Estás" is entirely in Portuguese, reflecting Furtado's Portuguese heritage. Following the release of the album, Furtado headlined the "Burn in the Spotlight Tour" and also appeared on Moby's "Area:One" tour.
In 2002, Furtado appeared on the song "Thin Line", on underground hip hop group Jurassic 5's album "Power in Numbers". The same year, Furtado provided her vocals to the Paul Oakenfold's song "The Harder They Come" from the album "Bunkka" and also made the song "These words are my own". She also had a collaboration with Colombian artist Juanes, in the song "Fotografia" where she showed her diversity of yet another language. Furtado was also featured in "Breathe" from Swollen Members "Monsters in the Closet" release; the video for "Breathe," directed by Spawn creator Todd McFarlane, won the 2003 Western Canadian Music Awards Outstanding Video and MuchVIBE Best Rap Video.
Furtado's second album, "Folklore", was released in November 2003. The final track on the album, "Childhood Dreams", was dedicated to her daughter, Nevis. The album includes the single "Força" (meaning "strength"/ "power" or "you can do it!" in Portuguese), the official anthem of the 2004 European Football Championship. Furtado performed this song in Lisbon at the championship's final, in which the Portugal national team played. The lead single is "Powerless (Say What You Want)" and the second single is the ballad "Try". The album was not as successful as her debut, partly due to the album's less "poppy" sound, as well as underpromotion from her label DreamWorks Records. DreamWorks had just been sold to Universal Music Group. In 2005, DreamWorks Records, along with many of its artists including Furtado, was absorbed into Geffen Records. "Powerless (Say What You Want)" was later remixed, featuring Colombian rocker Juanes, who had previously worked with Furtado on his track "Fotografía" ("Photograph"). The two would collaborate again on "Te Busqué" ("I searched for you"), a single from Furtado's 2006 album "Loose".
2006–09: "Loose".
Furtado's third album, named "Loose", after the spontaneous, creative decisions she made while creating the album, was released in June 2006. In this album, primarily produced by Timbaland, Furtado experiments with sounds from R&B, hip hop, and 1980s music. Furtado herself describes the album's sound as "punk-hop", described as "modern, poppy, spooky" and as having "a mysterious, after-midnight vibe... extremely visceral". She attributed the youthful sound of the album to the presence of her two-year-old daughter. The album received generally positive reviews from critics, with some citing the "revitalising" effect of Timbaland on Furtado's music, and others calling it "slick, smart and surprising".
"Loose" has become the most successful album of Furtado's career so far, as it reached number one, not only in Canada and the United States, but also several countries worldwide. The album produced her first number-one hit in the United States, "Promiscuous", as well as her first number-one hit in the United Kingdom, "Maneater". The single "Say It Right" eventually became Furtado's most successful song worldwide, due to its huge success in Europe and in the United States, where it became her second number-one hit. "All Good Things (Come to an End)" became her most successful song in Europe, topping single charts in numerous countries there.
On February 16, 2007, Furtado embarked on the "Get Loose Tour". She returned in March 2007 to her hometown of Victoria to perform a concert at the Save-On Foods Memorial Centre. In honour of her visit, local leaders officially proclaimed March 21, 2007, the first day of spring, as Nelly Furtado Day. After the tour, she released her first live DVD/CD named "Loose the Concert". On April 1, 2007, Furtado was a performer and host of the 2007 Juno Awards in Saskatoon, Saskatchewan. She won all five awards for which she was nominated, including Album of the Year and Single of the Year. She also appeared on stage at the Concert for Diana at Wembley Stadium in London on July 1, 2007, where she performed "Say It Right", "Maneater", and "I'm like a Bird". In 2007, Furtado and Justin Timberlake were featured on Timbaland's single "Give It to Me", which became her third number-one single in the U.S. and second in the UK. In late 2008, Furtado collaborated with James Morrison on a song called "Broken Strings" for his album "Songs for You, Truths for Me". The single was released on December 8 and peaked at No.2 on the UK Singles Chart in early January. In 2008, she sang with the Italian group "Zero Assoluto" the ballad Win or Lose – Appena prima di partire, released in Italy, France and Germany and whose video was shot in Barcelona. Furtado made a guest appearance on Flo Rida's new album, "R.O.O.T.S.". Furtado also made a guest appearance on Divine Brown's "Love Chronicles", co-writing and singing on the background of the song "Sunglasses". Furtado married Cuban sound engineer Demacio "Demo" Castellón, with whom she had worked on the "Loose" album, on July 19, 2008.
2009–12: "Mi Plan" and "The Best of Nelly Furtado".
Furtado's debut Spanish album, "Mi Plan" was released with the first single, "Manos Al Aire" ("Hands in the Air"). She had formed her own record label, Nelstar, in conjunction with Canadian independent label group Last Gang Labels. The first act signed to Nelstar is Fritz Helder & the Phantoms. "Manos al Aire" was released on the new label. The second, third and fourth singles were "Más", "Mi Plan" and "Bajo Otra Luz" respectively. Furtado won the Latin Grammy Award for Best Female Pop Vocal Album for "Mi Plan". She is the first Canadian to win a Latin Grammy award. Furtado also recorded "Manos al Aire" in Simlish for the new Sims 3 expansion, World Adventures. "Lifestyle", her planned fourth English studio album, was not released during the summer of 2010 in favor a second leg of her "Mi Plan Tour". To promote the tour in Brazil, on March 24, 2010, Furtado made a "VIP Pocket Show" in reality show program Big Brother Brasil 10 from Rede Globo, the country's leading channel. Furtado participated in the live DVD recording of the Brazilian singer Ivete Sangalo in Madison Square Garden on September 4, 2010. Furtado released "Mi Plan Remixes" featuring 12 tracks of remixed hits from "Mi Plan." This album included the Original Spanglish Version of "Fuerte", her final release from "Mi Plan". Furtado made a guest appearance on Canadian singer k-os's new album "Yes!", collaborating alongside Saukrates on the song "I Wish I Knew Natalie Portman," released in early July 2009. Nelly Furtado also made a guest appearance on Tiësto's single "Who Wants to Be Alone" on his new album "Kaleidoscope". Furtado sang in a duet with Bryan Adams at the opening ceremonies of the 2010 Vancouver Winter Olympic Games. The song was called "Bang The Drum" released on EMI album "Sounds Of Vancouver 2010" (a commemorative album). Furtado is featured in a new song by N.E.R.D. called "Hot N Fun". She also participated in the Young Artists for Haiti song, in which many Canadian artists came together and sang K'naan's inspirational song "Wavin' Flag" to raise money for the victims of the Haiti Earthquake. Furtado was honoured with a star on Canada's Walk of Fame in October 2010.
Furtado released her first greatest hits album titled "The Best of Nelly Furtado" on November 16, 2010. Three new songs were included on the greatest hits album, including "Night Is Young", "Girlfriend in the City", and the Lester Mendez produced track, left over from the "Loose" sessions, "Stars". The album's first single, "Night Is Young", was released on October 12, 2010. Furtado had previously sung two of the new songs: "Girlfriend in the City" and "Night Is Young" at her concert in Warsaw, Poland.
Furtado came under fire after 2011 reports from the "New York Times" and a WikiLeaks document revealed she had accepted payment of one million dollars to perform for the family of Libyan ruler Muammar Gaddafi. Only after the story broke did she promise to donate to charity the $1 million she received for a 2007 concert, which ended up going to Free the Children.
Furtado publicly endorsed Green Party leader Elizabeth May in Saanich-Gulf Islands during the federal election in 2011. Furtado was featured on one of the Game's The R.E.D. Album tracks, titled "Mamma Knows" (produced by The Neptunes). For the Canadian film The Year Dolly Parton Was My Mom, Furtado lent her vocals for the Dolly Parton gospel cover "The Seeker" featured during the credits of the film. Furtado collaborated with recording artist Alex Cuba and K'naan once again. The duet with K'naan "Is Anybody Out There", was released as the first single from his extended play "More Beautiful than Silence".
2012–present: "The Spirit Indestructible".
"The Spirit Indestructible" was released in September 2012. Furtado previously proclaimed that the album was most like her 2000 debut "Whoa, Nelly!", but containing elements from urban, alternative, and reggae. The influences for the album range from Janelle Monae, The xx, to Florence + the Machine. The album has attracted producers such as The Neptunes, Tiësto, Timbaland, Rick Nowels, Ryan Tedder and Rodney Jerkins. The first single from "The Spirit Indestructible", "Big Hoops (Bigger the Better)", was released digitally on April 17, 2012 and was sent to North American radio stations on May 1, 2012. Furtado continued to collaborate with hip-hop producer Salaam Remi, who previously worked on the 2010 single "Night Is Young", on "The Edge". The lyrics for the Salaam Remi produced track are reported to be influenced by the Tiger Woods cheating scandal, in which was originally referred to as "Elin's Song". On July 2, 2013 Furtado performed a new track, the acoustic ballad "Mystery", from her upcoming studio album. Furtado promoted the album on her The Spirit Indestructible Tour.
Personal life.
Furtado gave birth to daughter Nevis Chetan () in Toronto with then-boyfriend Jasper Gahunia. Furtado and Gahunia, who had been good friends for several years, remained together for four years until their breakup in 2005. Furtado told "Blender" magazine that they continue to be good friends and jointly share responsibility of raising Nevis. In a June 2006 interview with "Genre" magazine, when asked if she had "ever felt an attraction to women", Furtado replied "Absolutely. Women are beautiful and sexy". Some considered this an announcement of bisexuality, but in August 2006, she stated that she was "straight, but very open-minded". In November 2006, Furtado revealed that she once turned down $500,000 to pose fully clothed in "Playboy". On July 19, 2008, Furtado married sound engineer Demacio Castellon.
Philanthropy.
Furtado hosted a program about AIDS on MTV, which also featured guests Alicia Keys and Justin Timberlake. On September 27, 2011, Furtado announced during Free the Children's We Day Toronto, that she was giving $1,000,000 to Free the Children's effort to build girls' schools in the Maasai region of Kenya.
Artistry.
During her pre-teenage to teenage years, Furtado embraced many musical genres, listening heavily to mainstream R&B, hip hop, alternative hip hop, drum and bass, trip hop, world music (including Portuguese fado, Brazilian bossa nova and Indian music), and a variety of others. Her biggest influence when growing up was Ani DiFranco, she explained that "[w]hen I was a teenager, I wanted to be (the feminist punk-folk singer) Ani DiFranco. I never wanted to be part of corporate music." She cites diverse influences, which include soul-trip/hip hop artists such as De La Soul, TLC, world music artists Nusrat Fateh Ali Khan, Amalia Rodrigues, as well as Caetano Veloso, Juanes, Jeff Buckley, Esthero, Björk, Cornershop, Oasis, Radiohead, The Smashing Pumpkins and Beck. Furtado's music has also been influenced by her current residence, Toronto, which she calls "the most multicultural city in the entire world" and a place where she "can be any culture". Regarding Toronto's cultural diversity, she has said that she did not have to wait for the Internet revolution to learn about world music; she began listening to it at the age of five and continues to discover new genres.

</doc>
<doc id="42719" url="http://en.wikipedia.org/wiki?curid=42719" title="Do it yourself">
Do it yourself

Do it yourself, also known as DIY, is the method of building, modifying, or repairing something without the aid of experts or professionals. Academic research describes DIY as behaviors where "individuals engage raw and semi-raw materials and component parts to produce, transform, or reconstruct material possessions, including those drawn from the natural environment (e.g. landscaping)". DIY behavior can be triggered by various motivations previously categorized as marketplace motivations (economic benefits, lack of product availability, lack of product quality, need for customization), and identity enhancement (craftsmanship, empowerment, community seeking, uniqueness)
The term "do-it-yourself" has been associated with consumers since at least 1912 primarily in the domain of home improvement and maintenance activities. The phrase "do it yourself" had come into common usage (in standard English) by the 1950s, in reference to the emergence of a trend of people undertaking home improvement and various other small craft and construction projects as both a creative-recreational and cost-saving activity.
Subsequently, the term DIY has taken on a broader meaning that covers a wide range of skill sets. DIY is associated with the international alternative rock, punk rock, and indie rock music scenes; indymedia networks, pirate radio stations, and the zine community. In this context, DIY is related to the Arts and Crafts movement, in that it offers an alternative to modern consumer culture's emphasis on relying on others to satisfy needs. The abbreviation DIY is also widely used in the military as a way to teach commanders or other types of units to take responsibility, so that they'd be able to do things themselves just as a preparation for their own future.
History.
Italian archaeologists unearthed the ruins of a 6th-century BC Greek structure in southern Italy that came with detailed assembly instructions and is being called an "ancient IKEA building". The structure was a temple-like building discovered at Torre Satriano, near the southern city of Potenza, in Basilicata, a region where local people mingled with Greeks who settled along the southern coast known as Magna Graecia and in Sicily from the 8th century BC onwards. Professor Christopher Smith, director of the British School at Rome, said that the discovery was "the clearest example yet found of mason's marks of the time. It looks as if someone was instructing others how to mass-produce components and put them together in this way". Much like the instruction booklets, various sections of the luxury building were inscribed with coded symbols showing how the pieces slotted together. The characteristics of these inscriptions indicate they date back to around the 6th century BC, which tallies with the architectural evidence suggested by the decoration. The building was built by Greek artisans coming from the Spartan colony of Taranto in Apulia.
Home improvement.
The DIY movement is a re-introduction (often to urban and suburban dwellers) of the old pattern of personal involvement and use of skills in upkeep of a house or apartment, making clothes; maintenance of cars, computers, websites; or any material aspect of living. The philosopher Alan Watts (from the "Houseboat Summit" panel discussion in a 1967 edition of the "San Francisco Oracle") reflected a growing sentiment:
 Our educational system, in its entirety, does nothing to give us any kind of material competence. In other words, we don't learn how to cook, how to make clothes, how to build houses, how to make love, or to do any of the absolutely fundamental things of life. The whole education that we get for our children in school is entirely in terms of abstractions. It trains you to be an insurance salesman or a bureaucrat, or some kind of cerebral character.
In the 1970s, DIY spread through the North American population of college- and recent-college-graduate age groups. In part, this movement involved the renovation of affordable, rundown older homes. But it also related to various projects expressing the social and environmental vision of the 1960s and early 1970s. The young visionary Stewart Brand, working with friends and family, and initially using the most basic of typesetting and page-layout tools, published the first edition of "The Whole Earth Catalog" (subtitled "Access to Tools") in late 1968.
The first "Catalog", and its successors, used a broad definition of the term "tools". There were informational tools, such as books (often technical in nature), professional journals, courses, classes, and the like. There were specialized, designed items, such as carpenters' and masons' tools, garden tools, welding equipment, chainsaws, fiberglass materials and so on; even early personal computers. The designer J. Baldwin acted as editor to include such items, writing many of the reviews. The "Catalog"'s publication both emerged from and spurred the great wave of experimentalism, convention-breaking, and do-it-yourself attitude of the late 1960s. Often copied, the "Catalog" appealed to a wide cross-section of people in North America and had a broad influence.
For decades, magazines such as "Popular Mechanics" and "Mechanix Illustrated" offered a way for readers to keep current on useful practical skills and techniques. DIY home improvement books began to flourish in the 1970s, first created as collections of magazine articles. An early, extensive line of DIY how-to books was created by Sunset Books, based upon previously published articles from their magazine, "Sunset", based in California. Time-Life, Better Homes and Gardens, and other publishers soon followed suit.
In the mid-1990s, DIY home-improvement content began to find its way onto the World Wide Web. HouseNet was the earliest bulletin-board style site where users could share information. HomeTips.com, established in early 1995, was among the first Web-based sites to deliver free extensive DIY home-improvement content created by expert authors. Since the late 1990s, DIY has exploded on the Web through thousands of sites.
In the 1970s, when home video (VCRs) came along, DIY instructors quickly grasped its potential for demonstrating processes by audio-visual means. In 1979, the PBS television series "This Old House", starring Bob Vila, premiered and this spurred a DIY television revolution. The show was immensely popular, educating people on how to improve their living conditions (and the value of their house) without the expense of paying someone else to do (as much of) the work. In 1994, the HGTV Network cable television channel was launched in the United States and Canada, followed in 1999 by the DIY Network cable television channel. Both were launched to appeal to the growing percentage of North Americans interested in DIY topics, from home improvement to knitting. Such channels have multiple shows showing how to stretch one's budget to achieve professional-looking results ("Design Cents", "Design on a Dime", etc.) while doing the work yourself. "Toolbelt Diva" specifically caters to female DIYers.
Beyond magazines and television, the scope of home improvement DIY continues to grow online where most mainstream media outlets now have extensive DIY-focused informational websites such as "This Old House", Martha Stewart, Hometalk, and the DIY Network. These are often extensions of their magazine or television brand. The growth of independent online DIY resources is also spiking. The number of homeowners who blog about their experiences continues to grow, along with DIY websites from smaller organizations.
Fashion.
DIY amongst the fashion community has become very popular. With the use of social media such as YouTube, a great number of people watch videos on a daily basis. YouTube has an array of DIY fashion videos from distressing jeans, bleaching jeans, redesigning an old shirt, and studding denim, just to name a few. This new trend is increasingly becoming more and more popular. There over 1,000 videos that individuals have posted demonstrating how to do those things. There are also other DIY videos that individuals could look up such as DIY jewelry, DIY room decor, and DIY hairstyles.
Subculture.
The terms "DIY" and "do-it-yourself" are also used to describe:
DIY as a subculture could be said to have begun with the punk movement of the 1970s. Instead of traditional means of bands reaching their audiences through large music labels, bands began recording, manufacturing albums and merchandise, booking their own tours, and creating opportunities for smaller bands to get wider recognition and gain cult status through repetitive low-cost DIY touring. The burgeoning zine movement took up coverage of and promotion of the underground punk scenes, and significantly altered the way fans interacted with musicians. Zines quickly branched off from being hand-made music magazines to become more personal; they quickly became one of the youth culture's gateways to DIY culture. This led to tutorial zines showing others how to make their own shirts, posters, zines, books, food, etc.
Groups and publications.
Publications and websites that focus on DIY and/or crafting content include:

</doc>
<doc id="42720" url="http://en.wikipedia.org/wiki?curid=42720" title="Second Boer War">
Second Boer War

The Second Boer War (Dutch: "Tweede Boerenoorlog", Afrikaans: "Tweede Vryheidsoorlog", literally "Second Freedom War") was fought from 11 October 1899 until 31 May 1902 between the United Kingdom and the South African Republic (Transvaal Republic) and Orange Free State. The British war effort was supported by troops from several regions in the British Empire, including Southern Africa, the Australian colonies, Canada, India and New Zealand. The war ended in victory for Britain and the annexation of both republics. Both would eventually be incorporated into the Union of South Africa in 1910.
Name.
The conflict is commonly referred to as simply the Boer War, since the First Boer War (December 1880 to March 1881) is much less well known. "Boers" was the common term for Afrikaans-speaking settlers in southern Africa at the time. It is also known as the South African War outside South Africa and as the (Second) Anglo-Boer War among South Africans. In Afrikaans it may be called the "Anglo-Boereoorlog" ("Anglo-Boer War"), "Tweede Boereoorlog" ("Second Boer War"), "Tweede Vryheidsoorlog" ("Second Freedom War", i.e. a war of liberation) or "Engelse oorlog" ("English War").
Origins.
The complex origins of the war resulted from more than a century of conflict between the Boers and the British Empire, but of particular immediate importance was the question as to which nation would control and benefit most from the very lucrative Witwatersrand gold mines. During the Napoleonic Wars, a British military expedition landed in the Cape Colony and defeated the defending Dutch forces at the Battle of Blaauwberg (1806). After the war, the British formally acquired the colony (1814), and encouraged immigration by British settlers who were largely at odds with the Dutch settlers. Many Boers who were dissatisfied with aspects of British administration, in particular with Britain's abolition of slavery on 1 December 1834, elected to migrate away from British rule in what became known as the Great Trek.
The Trekkers initially followed the eastern coast towards Natal and then, after Britain annexed the Natal in 1843, journeyed northwards towards the interior. There they established two independent Boer republics: the South African Republic (1852; also known as the Transvaal Republic) and the Orange Free State (1854). The British recognised the two Boer republics in 1852 and 1854, but attempted British annexation of the Transvaal in 1877 led to the First Boer War in 1880–81. After the British suffered defeats, particularly at the Battle of Majuba Hill (1881), the independence of the two republics was restored subject to certain conditions; relations, however, remained uneasy.
In 1866 Erasmus Jacobs discovered diamonds at Kimberley, prompting a diamond rush and a massive influx of foreigners to the borders of the Orange Free State. Then in 1886, an Australian discovered gold in the Witwatersrand area of the South African Republic. Gold made the Transvaal the richest and potentially the most powerful nation in southern Africa; however, the country had neither the manpower nor the industrial base to develop the resource on its own. As a result, the Transvaal reluctantly acquiesced to the immigration of "uitlanders" (foreigners), mainly from Britain, who came to the Boer region in search of fortune and employment. This resulted in the number of uitlanders in the Transvaal potentially exceeding the number of Boers, and precipitated confrontations between the earlier-arrived Boer settlers and the newer, non-Boer arrivals.
British expansionist ideas (notably propagated by Cecil Rhodes) as well as disputes over uitlander political and economic rights resulted in the failed Jameson Raid of 1895. Dr. Leander Starr Jameson, who led the raid, intended to encourage an uprising of the uitlanders in Johannesburg. However, the uitlanders did not take up arms in support, and Transvaal government forces surrounded the column and captured Jameson's men before they could reach Johannesburg.
As tensions escalated, political manoeuvrings and negotiations attempted to reach compromise on the issues of the rights of the uitlanders within the South African Republic, control of the gold mining industry, and the British desire to incorporate the Transvaal and the Orange Free State into a federation under British control. Given the British origins of the majority of uitlanders and the ongoing influx of new uitlanders into Johannesburg, the Boers recognised that granting full voting rights to the uitlanders would eventually result in the loss of ethnic Boer control in the South African Republic.
To the satisfaction of Lord Milner, British High Commissioner for South Africa, the June 1899 negotiations in Bloemfontein failed, and in September 1899 British Colonial Secretary Joseph Chamberlain demanded full voting rights and representation for the uitlanders residing in the Transvaal. Paul Kruger, the President of the South African Republic, issued an ultimatum on 9 October 1899, giving the British government 48 hours to withdraw all their troops from the borders of both the Transvaal and the Orange Free State, failing which the Transvaal, allied to the Orange Free State, would declare war on the British government. The British government rejected the South African Republic's ultimatum, resulting in the South African Republic and Orange Free State declaring war on Britain.
Phases.
The war had three distinct phases. In the first phase, the Boers mounted pre-emptive strikes into British-held territory in Natal and the Cape Colony, besieging the British garrisons of Ladysmith, Mafeking and Kimberley. The Boers then won a series of tactical victories at Colenso, Magersfontein and Spionkop
In the second phase, after the introduction of greatly increased British troop numbers under the command of Lord Roberts, the British launched another offensive in 1900 to relieve the sieges, this time achieving success. After Natal and the Cape Colony were secure, the British were able to invade the Transvaal, and the republic's capital, Pretoria, was ultimately captured in June 1900.
In the third and final phase, beginning in March 1900, the Boers launched a protracted hard-fought guerrilla war against the British forces, lasting a further two years, during which the Boers raided targets such as British troop columns, telegraph sites, railways and storage depots. In an effort to cut off supplies to the raiders, the British, now under the leadership of Lord Kitchener, responded with a scorched earth policy of destroying Boer farms and moving civilians into concentration camps.
Some parts of the British press and British government expected the campaign to be over within months, and the protracted war gradually became less popular, especially after revelations about the conditions in the concentration camps (where as many as 26,000 Afrikaner women and children died of disease and malnutrition). The Boer forces finally surrendered on Saturday, 31 May 1902, with 54 of the 60 delegates from the Transvaal and Orange Free State voting to accept the terms of the peace treaty. This was known as the Treaty of Vereeniging, and under its provisions, the two republics were absorbed into the British Empire, with the promise of self-government in the future. This promise was fulfilled with the creation of the Union of South Africa in 1910.
The war had a lasting effect on the region and on British domestic politics. For Britain, the Second Boer War was the longest, the most expensive (£200 million), and the bloodiest conflict between 1815 and 1914, lasting three months longer and resulting in higher British casualties than the Crimean War (1853–56) (although more soldiers died from disease in the Crimean War).
Background.
The southern part of the African continent was dominated in the 19th century by a set of struggles to create within it a single unified state. While the Berlin Conference of 1884–5 sought to draw boundaries between the European powers' African possessions, it also set the stage for further scrambles. The British attempted to annex first the South African Republic in 1880, and then, in 1899, both the South African Republic and the Orange Free State. In 1868, the British annexed Basutoland in the Drakensberg Mountains following an appeal from Moshesh, the leader of a mixed group of African refugees from the Zulu wars, who sought British protection against the Boers.
In the 1880s, Bechuanaland (modern Botswana, located north of the Orange River) became the object of a dispute between the Germans to the west, the Boers to the east, and the British Cape Colony to the south. Although Bechuanaland had no economic value, the "Missionaries Road" passed through it towards territory farther north. After the Germans annexed Damaraland and Namaqualand (modern Namibia) in 1884, the British annexed Bechuanaland in 1885.
In the First Boer War of 1880–81 the Boers of the Transvaal Republic had proved skillful fighters in resisting the British attempt at annexation, causing a series of British defeats. The British government of William Ewart Gladstone had been unwilling to become mired in a distant war, requiring substantial troop reinforcement and expense, for what was at the time perceived to be a minimal return. An armistice followed, ending the war, and subsequently a peace treaty was signed with the Transvaal President Paul Kruger.
However, when, in 1886, a major gold field was discovered at an outcrop on a large ridge some sixty kilometres south of the Boer capital at Pretoria, it reignited British imperial interests. The ridge, known locally as the "Witwatersrand" (literally "white water ridge"–a watershed) contained the world's largest deposit of gold-bearing ore. Although it was not as rich as gold finds in Canada and Australia, its consistency made it especially well-suited to industrial mining methods. With the 1886 discovery of gold in the Transvaal, the resulting gold rush brought thousands of British and other prospectors and settlers from across the globe and over the border from the Cape Colony (under British control since 1806).
The city of Johannesburg sprang up as a shanty town nearly overnight as the "uitlanders" ("foreigners," meaning non-Boer whites) poured in and settled around the mines. The influx was such that the uitlanders quickly outnumbered the Boers in Johannesburg and along the Rand, although they remained a minority in the Transvaal as a whole. The Boers, nervous and resentful of the uitlanders' growing presence, sought to contain their influence through requiring lengthy residential qualifying periods before voting rights could be obtained, by imposing taxes on the gold industry, and by introducing controls through licensing, tariffs and administrative requirements. Among the issues giving rise to tension between the Transvaal government on the one hand, and the uitlanders and British interests on the other, were:
Certain self-appointed "uitlanders" representatives and British mine owners became increasingly angered and frustrated by their dealings with the Transvaal government. A Reform Committee (Transvaal) was formed to represent the uitlanders.
Jameson Raid.
In 1895, a plan was hatched with the connivance of the Cape Prime Minister Cecil Rhodes and Johannesburg gold magnate Alfred Beit to take Johannesburg, ending the control of the Transvaal government. A column of 600 armed men (mainly made up of his Rhodesian and Bechuanaland policemen) was led by Dr. Leander Starr Jameson (the Administrator in Rhodesia of the British South Africa Company (or "Chartered Company") of which Cecil Rhodes was the Chairman) over the border from Bechuanaland towards Johannesburg. The column was equipped with Maxim machine guns, and some artillery pieces.
The plan was to make a three-day dash to Johannesburg before the Boer commandos could mobilise, and once there, trigger an uprising by the primarily British expatriate workers (uitlanders) organised by the Reform Committee. However, the Transvaal authorities had advance warning of the Jameson Raid and tracked it from the moment it crossed the border. Four days later, the weary and dispirited column was surrounded near Krugersdorp within sight of Johannesburg. After a brief skirmish in which the column lost 65 killed and wounded—while the Boers lost but one man—Jameson's men surrendered and were arrested by the Boers.
The botched raid resulted in repercussions throughout southern Africa and in Europe. In Rhodesia, the departure of so many policemen enabled the Matabele and Mashona tribes to rise up against the Chartered Company, and the rebellion, known as the Second Matabele War, was suppressed only at great cost.
A few days after the raid, the German Kaiser sent a telegram ("Kruger telegram") congratulating President Kruger and the government of the South African Republic on their success, and when the text of this telegram was disclosed in the British press, it generated a storm of anti-German feeling. In the baggage of the raiding column, to the great embarrassment of the British, the Boers found telegrams from Cecil Rhodes and the other plotters in Johannesburg. Joseph Chamberlain, the British Colonial Secretary, quickly moved to condemn the raid, despite previously having approved Rhodes' plans to send armed assistance in the case of a Johannesburg uprising. Subsequently, Rhodes was severely censured at the Cape inquiry and the London parliamentary inquiry, and forced to resign as Prime Minister of the Cape and as Chairman of the Chartered Company for having sponsored the failed coup d'état.
The Boer government handed their raid prisoners over to the British for trial. Dr. Jameson was tried in England for leading the raid. However, the British press and London society inflamed by anti-Boer and anti-German feeling and in a frenzy of jingoism, lionised Dr. Jameson and treated him as a hero. Although sentenced to 15 months imprisonment (which he served in Holloway), Jameson was later rewarded by being named Prime Minister of the Cape Colony (1904–08) and ultimately anointed as one of the founders of the Union of South Africa. For conspiring with Jameson, the uitlander members of the Reform Committee (Transvaal) were tried in the Transvaal courts and found guilty of high treason. The four leaders were sentenced to death by hanging, but this sentence was next day commuted to 15 years' imprisonment; and in June 1896, the other members of the Committee were released on payment of £2,000 each in fines, all of which were paid by Cecil Rhodes. One Reform Committee member, Frederick Gray, had committed suicide while in Pretoria gaol, on 16 May, and his death was a factor in softening the Transvaal government's attitude to the remaining prisoners.
Jan C. Smuts wrote in 1906, "The Jameson Raid was the real declaration of war ... And that is so in spite of the four years of truce that followed ... [the] aggressors consolidated their alliance ... the defenders on the other hand silently and grimly prepared for the inevitable."
Escalation and war.
The Jameson Raid alienated many Cape Afrikaners from the British, and united the Transvaal Boers behind President Kruger and his government. It also had the effect of drawing the Transvaal and the Orange Free State (led by President Martinus Theunis Steyn) together in opposition to perceived British imperialism. In 1897, a military pact was concluded between the two republics. President Paul Kruger proceeded to re-equip the Transvaal army, and imported 37,000 of the latest Mauser Model 1895 rifles, and some 40 to 50 million rounds of ammunition. The best modern European artillery was also purchased.
By October 1899 the Transvaal State Artillery had 73 guns, of which 59 were new, including four 155-mm Creusot fortress guns, and 25 37mm Maxim Nordenfeldt guns. The Transvaal army had been transformed; approximately 25,000 men equipped with modern rifles and artillery could mobilise within two weeks. However, President Kruger's victory in the Jameson Raid incident did nothing to resolve the fundamental problem; the impossible dilemma continued, namely how to make concessions to the uitlanders without surrendering the independence of the Transvaal.
The failure to gain improved rights for uitlanders became a pretext for war and a justification for a major military buildup in the Cape Colony. The case for war was developed and espoused as far away as the Australian colonies. Several key British colonial leaders favoured annexation of the independent Boer republics. These figures included Cape Colony Governor Sir Alfred Milner, Cape Prime Minister Cecil Rhodes, British Colonial Secretary Joseph Chamberlain, and mining syndicate owners or Randlords (nicknamed the "gold bugs"), such as Alfred Beit, Barney Barnato, and Lionel Phillips. Confident that the Boers would be quickly defeated, they planned and organised a short war, citing the uitlanders' grievances as the motivation for the conflict.
Their influence with the British government was, however, limited. Lord Salisbury, the Prime Minister, despised jingoism and jingoists. He also distrusted the abilities of the British Army. Yet he led Britain into war for three main reasons: because he believed the British government had an obligation to British South Africans; because he thought that the Transvaal, the Orange Free State, and the Cape Boers aspired to a Dutch South Africa, and that the achievement of such a state would damage Britain's imperial prestige around the world; and because of the Boers' treatment of black South Africans (Salisbury had referred to the London Convention of 1884, after the British defeat, as an agreement 'really in the interest of slavery'). Salisbury was not alone in this concern over the treatment of black South Africans; Roger Casement, already well on the way to becoming an Irish Nationalist, was nevertheless happy to gather intelligence for the British against the Boers because of their treatment of black Africans.
Given this sense of caution among key members of the British cabinet and of the army, it is even harder to understand why the British government went against the advice of its generals (such as Wolseley) to send substantial reinforcements to South Africa before war broke out. One strong argument is that Lansdowne, Secretary of State for War, did not believe the Boers were preparing for war, and also believed that if Britain were to send large numbers of troops, it would strike too aggressive a posture and so prevent a negotiated settlement being reached or even encourage a Boer attack.
President Steyn of the Orange Free State invited Milner and Kruger to attend a conference in Bloemfontein. The conference started on 30 May 1899, but negotiations quickly broke down, despite Kruger's offer of concessions. In September 1899, Chamberlain sent an ultimatum demanding full equality for British citizens resident in Transvaal. Kruger, seeing that war was inevitable, simultaneously issued his own ultimatum prior to receiving Chamberlain's. This gave the British 48 hours to withdraw all their troops from the border of Transvaal; otherwise the Transvaal, allied with the Orange Free State, would declare war.
News of the ultimatum reached London on the day it expired. Outrage and laughter were the main responses. The editor of the "Times" laughed out loud when he read it, saying 'an official document is seldom amusing and useful yet this was both.' "The Times" denounced the ultimatum as an 'extravagant farce.' "The Globe" denounced this 'trumpery little state.' Most editorials were similar to the "Daily Telegraph", which declared: 'of course there can only be one answer to this grotesque challenge. Kruger has asked for war and war he must have!'
Such views were far from those of the British government, and from those in the army. To most sensible observers, army reform had been a matter of pressing concern from the 1870s, constantly put off because the British public did not want the expense of a larger, more professional army, and because a large home army was not politically welcome. Lord Salisbury, the Prime Minister, then had to explain to a surprised Queen Victoria that: 'We have no army capable of meeting even a second-class Continental Power.'
First phase: The Boer offensive (October – December 1899).
War was declared on 11 October 1899 with a Boer offensive into the British-held Natal and Cape Colony areas. The Boers had no problems with mobilisation, since the fiercely independent Boers had no regular army units, apart from the "Staatsartillerie" (Afrikaans for 'States Artillery') of both republics. As with the First Boer War, since the Boers were civilian militia, each man wore what he wished, usually his everyday dark-grey, light-grey, neutral-coloured, or earthtone khaki farming clothes—often a jacket, trousers and slouch hat. Only the members of the "Staatsartillerie" wore light green uniforms.
When danger loomed, all the "burghers" (citizens) in a district would form a military unit called a "commando" and would elect officers. A full-time official titled a "Veldkornet" maintained muster rolls, but had no disciplinary powers. Each man brought his own weapon, usually a hunting rifle, and his own horse. Those who could not afford a gun were given one by the authorities. (See also the arms procurement mentioned above.) The Presidents of the Transvaal and Orange Free State simply signed decrees to concentrate within a week and the Commandos could muster between 30,000–40,000 men.
The average Boer nevertheless was not thirsty for war. Many did not look forward to fighting against fellow Christians and, by and large, fellow Christian Protestants. Many may have had an overly optimistic sense of what the war would involve, imagining that victory could be won as easily as in the First South African War. Many, including many generals, also had a sense that their cause was holy and just, and blessed by God.
It rapidly became clear that the Boer forces presented the British forces with a severe tactical challenge. What the Boers presented was a mobile and innovative approach to warfare, drawing on their experiences from the First Boer War. The average Boers who made up their Commandos were farmers who had spent almost all their working life in the saddle, both as farmers and hunters. They depended on the pot, horse and rifle were skilled stalkers and marksmen. As hunters they had learned to fire from cover, from a prone position and to make the first shot count, knowing that if they missed, the game would either be long gone or could charge and potentially kill them.
At community gatherings, target shooting was a major sport, and they practised shooting at targets such as hens' eggs perched on posts 100 m away. They made expert mounted infantry, using every scrap of cover, from which they could pour in a destructive fire using their modern, smokeless, Mauser rifles. Furthermore, in preparation for hostilities, the Boers had acquired around one hundred of the latest Krupp field guns, all horse-drawn and dispersed among the various Commando groups, and several Le Creusot "Long Tom" siege guns. The Boers' skill in adapting themselves to becoming first-rate artillerymen shows them to have been a versatile adversary. The Transvaal also had an intelligence service that stretched across South Africa, and of whose extent and efficiency the British were unaware.
The Boers struck first on 12 October at Kraaipan, an attack that heralded the invasion of the Cape Colony and Colony of Natal between October 1899 and January 1900. With elements of both speed and surprise the Boer drove quickly towards the major British garrison at Ladysmith and the smaller ones at Mafeking and Kimberley. The quick Boer mobilisation resulted in early military successes against the scattered British forces.
Sir George Stuart White, commanding the British division at Ladysmith, had unwisely allowed Major-General Penn Symons to throw a brigade forward to the coal-mining town of Dundee (also reported as Glencoe), which was surrounded by hills. This became the site of the first engagement of the war, the Battle of Talana Hill. Boer guns began shelling the British camp from the summit of Talana Hill at dawn on 20 October. Penn Symons immediately counter-attacked. His infantry drove the Boers from the hill, but at the cost of 446 British casualties including Penn Symons himself.
Another Boer force occupied Elandslaagte, which lay between Ladysmith and Dundee. The British under Major General John French and Colonel Ian Hamilton attacked to clear the line of communications to Dundee. The resulting Battle of Elandslaagte was a clear-cut British tactical victory, but Sir George White feared that more Boers were about to attack his main position and ordered a chaotic retreat from Elandslaagte, throwing away any advantage gained. The detachment from Dundee was compelled to make an exhausting cross-country retreat to rejoin White's main force.
As Boers surrounded Ladysmith and opened fire on the town with siege guns, White ordered a major sortie against the Boer artillery positions. The result was a disaster, with 140 men killed and over 1,000 captured. The Siege of Ladysmith began, and was to last several months.
Meanwhile to the north-west at Mafeking, on the border with Transvaal, Colonel Robert Baden-Powell had raised two regiments of local forces amounting to some 1,200 men in order to attack and create diversions if things further south went amiss. Mafeking, being a railway junction, provided good supply facilities and was the obvious place for Baden-Powell to fortify in readiness for such attacks. However, instead of being the aggressor Baden-Powell and Mafeking were forced to defend when 6,000 Boer, commanded by Piet Cronje, attempted a determined assault on the town. But this quickly subsided into a desultory affair with the Boers prepared to starve the stronghold into submission, and so, on 13 October, began the 217-day Siege of Mafeking.
Lastly, over 360 km to the south of Mafeking lay the diamond mining city of Kimberley, which was also subjected to a siege. Although not militarily significant, it nonetheless represented an enclave of British imperialism on the borders of the Orange Free State and was hence an important Boer objective. From early November about 7,500 Boer began their siege, again content to starve the town into submission. Despite Boer shelling, the 40,000 inhabitants, of which only 5,000 were armed, were under little threat as the town was well-stocked with provisions. The garrison was commanded by Lieutenant Colonel Robert Kekewich, although Cecil Rhodes was also a prominent figure in the defence.
Siege life took its toll on both the defending soldiers and the civilians in the cities of Mafeking, Ladysmith, and Kimberley as food began to grow scarce after a few weeks. In Mafeking, Sol Plaatje wrote, "I saw horseflesh for the first time being treated as a human foodstuff." The cities under siege also dealt with constant artillery bombardment, making the streets a dangerous place. Near the end of the siege of Kimberley, it was expected that the Boers would intensify their bombardment, so Rhodes displayed a notice encouraging people to go down into shafts of the Kimberley Mine for protection. The townspeople panicked, and people surged into the mine-shafts constantly for a 12-hour period. Although the bombardment never came, this did nothing to diminish the distress of the civilians. The most well-heeled of the townspeople, such as Cecil Rhodes, sheltered in the Sanatorium, site of the present-day McGregor Museum; the poorer residents, notably the black population, did not have any shelter from the shelling.
In retrospect, the Boer decision to commit themselves to sieges (Sitzkrieg) was a mistake, and one of the best illustrations of the Boers' lack of strategic vision. Historically, it had little in its favour. Of the seven sieges in the First Boer War, the Boers had won none. More importantly, it handed the initiative back to the British and allowed them time to recover, which they then did. Generally speaking, throughout the campaign, the Boers were too defensive and passive, wasting the opportunities they had for victory. Yet that passiveness also testified to the fact that they had no desire to conquer British territory, but only to preserve their ability to rule in their own territory.
First British relief attempts.
It was at this point that General Sir Redvers Henry Buller, a much respected commander, arrived in South Africa with major British reinforcements (including an army corps of three divisions). Buller originally intended an offensive straight up the railway line leading from Cape Town through Bloemfontein to Pretoria. Finding on arrival that the British troops already in South Africa were under siege, he split his army corps into several widely spread detachments, to relieve the besieged garrisons. One division, led by Lieutenant General Lord Methuen, was to follow the Western Railway to the north and relieve Kimberley and Mafeking. A smaller force of about 3,000 led by Major General William Gatacre, was to push north toward the railway junction at Stormberg, to secure the Cape Midlands district from Boer raids and local rebellions by Boer inhabitants. Finally, Buller himself would lead the major part of the army corps to relieve Ladysmith to the east.
The initial results of this offensive were mixed, with Methuen winning several bloody skirmishes at Belmont on 23 November, at Graspan on 25 November, and at a larger conflict, Modder River on 28 November resulting in British losses of 71 dead and over 400 wounded. British commanders had trained on the lessons of the Crimean War, and were adept at battalion and regimental set pieces with columns manoeuvring in jungles, deserts and mountainous regions. What they entirely failed to comprehend, however, was both the impact of destructive fire from trench positions and the mobility of cavalry raids, both of which had been developed in the American Civil War. The British troops went to war with what would prove to be antiquated tactics, and in some cases antiquated weapons, against the mobile Boer forces with the destructive fire of their modern Mausers, the latest Krupp field guns, and their innovative tactics.
The middle of December was disastrous for the British Army. In a period known as Black Week (10 – 15 December 1899), the British suffered a series of losses on each of the three major fronts.
On 10 December, General Gatacre tried to recapture Stormberg railway junction about 50 mi south of the Orange River. Gatacre's attack was marked by administrative and tactical blunders, and the Battle of Stormberg ended in a British defeat, with 135 killed and wounded, and two guns and over 600 troops captured.
At the Battle of Magersfontein on 11 December, Methuen's 14,000 British troops attempted to capture a Boer position in a dawn attack to relieve Kimberley. This too turned into a disaster when the Highland Brigade became pinned down by accurate Boer fire. After suffering from intense heat and thirst for nine hours, they eventually broke in ill-disciplined retreat. The Boer commanders, Koos de la Rey and Piet Cronjé, had ordered trenches to be dug in an unconventional place to fool the British and to give their riflemen a greater firing range. The plan worked and this tactic helped write the doctrine of the supremacy of the defensive position, using modern small arms and trench fortifications. The British lost 120 killed and 690 wounded and were prevented from relieving Kimberley and Mafeking. A British soldier encapsulated the soldiers' view of the defeat:
""Such was the day for our regiment"<br>"Dread the revenge we will take. "<br>"Dearly we paid for the blunder -"<br>"A drawing-room General's mistake. "<br>"Why weren't we told of the trenches?"<br>"Why weren't we told of the wire? "<br>"Why were we marched up in column, "<br>"May Tommy Atkins enquire ..." "
However, the nadir of Black Week was the Battle of Colenso on 15 December where 21,000 British troops commanded by Buller himself, attempted to cross the Tugela River to relieve Ladysmith where 8,000 Transvaal Boers, under the command of Louis Botha, were awaiting them. Through a combination of artillery and accurate rifle fire, and a better use of the ground, the Boers repelled all British attempts to cross the river. After his first attacks failed, Buller broke off the battle and ordered a retreat, abandoning many wounded men, several isolated units and ten field guns to be captured by Botha's men. Buller's forces lost 145 men killed and 1,200 missing or wounded. The Boers suffered 40 casualties, including only 8 killed.
Second phase: The British offensive of January to September 1900.
The British government took these defeats badly and with the sieges still continuing was compelled to send two more divisions plus large numbers of colonial volunteers. By January 1900 this would become the largest force Britain had ever sent overseas, amounting to some 180,000 men with further reinforcements being sought.
While watching for these reinforcements, Buller made another bid to relieve Ladysmith by crossing the Tugela west of Colenso. Buller's subordinate, Major General Charles Warren, successfully crossed the river, but was then faced with a fresh defensive position centred on a prominent hill known as Spion Kop. In the resulting Battle of Spion Kop, British troops captured the summit by surprise during the early hours of 24 January 1900, but as the early morning fog lifted they realised too late that they were overlooked by Boer gun emplacements on the surrounding hills. The rest of the day resulted in a disaster caused by poor communication between Buller and his commanders. Between them they issued contradictory orders, on the one hand ordering men off the hill, while other officers ordered fresh reinforcements to defend it. The result was 350 men killed and nearly 1,000 wounded and a retreat back across the Tugela River into British territory. There were nearly 300 Boer casualties.
Buller attacked Louis Botha again on 5 February at Vaal Krantz and was again defeated. Buller withdrew early when it appeared that the British would be isolated in an exposed bridgehead across the Tugela, and was nicknamed "Sir Reverse" by some of his officers.
By taking command in person in Natal, Buller had allowed the overall direction of the war to drift. Because of concerns about his performance and negative reports from the field, he was replaced as Commander in Chief by Field Marshal Lord Roberts. Roberts quickly assembled an entirely new team for headquarters staff and he chose military men from far and wide: Lord Kitchener (Chief of Staff) from the Sudan; Frederick Russell Burnham (Chief of Scouts), the American scout, from the Klondike; David Henderson from the Staff College; Neville Bowles Chamberlain from Afghanistan; and William Nicholson (Military Secretary) from Calcutta Like Buller, Roberts first intended to attack directly along the Cape Town – Pretoria railway but, again like Buller, was forced to relieve the beleaguered garrisons. Leaving Buller in command in Natal, Roberts massed his main force near the Orange River and along the Western Railway behind Methuen's force at the Modder River, and prepared to make a wide outflanking move to relieve Kimberley.
Except in Natal, the war had stagnated. Other than a single attempt to storm Ladysmith, the Boers made no attempt to capture the besieged towns. In the Cape Midlands, the Boers did not exploit the British defeat at Stormberg, and were prevented from capturing the railway junction at Colesberg. In the dry summer, the grazing on the veld became parched, weakening the Boers' horses and draught oxen, and many Boer families joined their menfolk in the siege lines and "laagers" (encampments), fatally encumbering Cronje's army.
Roberts launched his main attack on 10 February 1900 and although hampered by a long supply route, managed to outflank the Boers defending Magersfontein. On 14 February, a cavalry division under Major General John French launched a major attack to relieve Kimberley. Although encountering severe fire, a massed cavalry charge split the Boer defences on 15 February, opening the way for French to enter Kimberley that evening, ending its 124 days' siege.
Meanwhile, Roberts pursued Piet Cronje's 7,000-strong force, which had abandoned Magersfontein to head for Bloemfontein. General French's cavalry was ordered to assist in the pursuit by embarking on an epic 50 km drive towards Paardeberg where Cronje was attempting to cross the Modder River. At the Battle of Paardeberg from 18 to 27 February, Roberts then surrounded General Piet Cronje's retreating Boer army. On 17 February, a pincer movement involving both French's cavalry and the main British force attempted to take the entrenched position, but the frontal attacks were uncoordinated and so were easily repulsed by the Boers. Finally, Roberts resorted to bombarding Cronje into submission, but it took a further ten precious days and with the British troops using the polluted Modder River as water supply, resulting in a typhoid epidemic killing many troops. General Cronje was forced to surrender at Surrender Hill with 4000 men.
In Natal, the Battle of the Tugela Heights, which started on 14 February was Buller's fourth attempt to relieve Ladysmith. Despite reinforcements his progress was painfully slow against stiff opposition. However, on 26 February, after much deliberation, Buller used all his forces in one all-out attack for the first time and at last succeeded in forcing a crossing of the Tugela, and defeated Botha's outnumbered forces north of Colenso. After a siege lasting 118 days, the Relief of Ladysmith was effected, the day after Cronje surrendered, but at a total cost of 7,000 British casualties.
After a succession of defeats, the Boers realised that against such overwhelming superiority of troops, they had little chance of defeating the British and so became demoralised. Roberts then advanced into the Orange Free State from the west, putting the Boers to flight at the Battle of Poplar Grove and capturing Bloemfontein, the capital, unopposed on 13 March with the Boer defenders escaping and scattering. Meanwhile, he detached a small force to relieve Baden-Powell, and the Relief of Mafeking on 18 May 1900 provoked riotous celebrations in Britain.
On 28 May, the Orange Free State was annexed and renamed the Orange River Colony.
After being forced to delay for several weeks at Bloemfontein due to a shortage of supplies and enteric (typhoid) fever, caused by poor hygiene, drinking bad water at Paardeburg and appalling medical care, Roberts resumed his advance. He was forced to halt again at Kroonstad for 10 days, due once again to the collapse of his medical and supply systems, but finally captured Johannesburg on 31 May and the capital of the Transvaal, Pretoria, on 5 June. The first into Pretoria, was Lt. William Watson of the New South Wales Mounted Rifles, who persuaded the Boers to surrender the capital. (Before the war, the Boers had constructed several forts south of Pretoria, but the artillery had been removed from the forts for use in the field, and in the event the Boers abandoned Pretoria without a fight).
This allowed Roberts to declare the war over, having won the principal cities and so, on 3 September 1900, the South African Republic was formally annexed.
British observers believed the war to be all but over after the capture of the two capital cities. However, the Boers had earlier met at the temporary new capital of the Orange Free State, Kroonstad, and planned a guerrilla campaign to hit the British supply and communication lines. The first engagement of this new form of warfare was at Sanna's Post on 31 March where 1,500 Boers under the command of Christiaan De Wet attacked Bloemfontein's waterworks about 23 mi east of the city, and ambushed a heavily escorted convoy, which caused 155 British casualties and the capture of seven guns, 117 wagons, and 428 British troops.
After the fall of Pretoria, one of the last formal battles was at Diamond Hill on 11 – 12 June, where Roberts attempted to drive the remnants of the Boer field army beyond striking distance of Pretoria. Although Roberts drove the Boers from the hill, the Boer commander, Louis Botha, did not regard it as a defeat, for he inflicted more casualties on the British (totalling 162 men) while suffering around 50 casualties.
The set-piece period of the war now largely gave way to a mobile guerrilla war, but one final operation remained. President Kruger and what remained of the Transvaal government had retreated to eastern Transvaal. Roberts, joined by troops from Natal under Buller, advanced against them, and broke their last defensive position at Bergendal on 26 August. As Roberts and Buller followed up along the railway line to Komatipoort, Kruger sought asylum in Portuguese East Africa (modern Mozambique). Some dispirited Boers did likewise, and the British gathered up much war material. However, the core of the Boer fighters under Botha easily broke back through the Drakensberg Mountains into the Transvaal highveld after riding north through the bushveld. Under the new conditions of the war, heavy equipment was no use to them, and therefore no great loss.
As Roberts's army occupied Pretoria, the Boer fighters in the Orange Free State had been driven into a fertile area known as the Brandwater Basin in the north east of the Republic. This offered only temporary sanctuary, as the mountain passes leading to it could be occupied by the British, trapping the Boers. A force under General Archibald Hunter set out from Bloemfontein to achieve this in July 1900. The hard core of the Free State Boers under Christiaan De Wet, accompanied by President Steyn, left the basin early. Those remaining fell into confusion and most failed to break out before Hunter trapped them. 4,500 Boers surrendered and much equipment was captured but as with Roberts's drive against Kruger at the same time, these losses were of relatively little consequence, as the hardcore of the Boer armies and their most determined and active leaders remained at large.
From the Basin, Christiaan De Wet headed west. Although hounded by British columns, he succeeded in crossing the Vaal into western Transvaal, to allow Steyn to travel to meet the Transvaal leaders.
There was much sympathy for the Boers on mainland Europe and in October, President Kruger and members of the Transvaal government left Portuguese East Africa on the Dutch warship "De Gelderland", sent by the Queen of the Netherlands Wilhelmina, who had simply ignored the British naval blockade of South Africa. Paul Kruger's wife, however, was too ill to travel and remained in South Africa where she died on 20 July 1901 without seeing her husband again. President Kruger first went to Marseille and then on to The Netherlands where he stayed for a while before moving finally to Clarens, Switzerland, where he died in exile on 14 July 1904.
POWs sent overseas.
The first sizeable batch of Boer prisoners of war taken by the British consisted of those captured at the Battle of Elandslaagte on 21 October 1899. At first, many were put on ships, but as numbers grew, the British decided they did not want them kept locally. The capture of 400 POWs in February 1900 was a key event, which made the British realise they could not accommodate all POWs in South Africa. The British feared they could be freed by sympathetic locals. Moreover, they already had trouble supplying their own troops in South Africa, and did not want the added burden of sending supplies for the POWs. Britain therefore chose to send many POWs overseas.
The first overseas (off African mainland) camps were opened in Saint Helena, which ultimately received about 5,000 POWs. About 5,000 POWs were sent to Ceylon. Other POWs were sent to Bermuda and India. No evidence exists of Boer POWs being sent to the Dominions of the British Empire such as Australia, Canada or New Zealand.
In all, about 26,000 POWs were sent overseas.
Third phase: Guerrilla war (September 1900 – May 1902).
By September 1900, the British were nominally in control of both Republics, with the exception of the northern part of Transvaal. However, they soon discovered that they only controlled the territory their columns physically occupied. Despite the loss of their two capital cities and half of their army, the Boer commanders adopted guerrilla warfare tactics, primarily conducting raids against infrastructure, resource and supply targets, all aimed at disrupting the operational capacity of the British Army.
Each Boer commando unit was sent to the district from which its members had been recruited, which meant that they could rely on local support and personal knowledge of the terrain and the towns within the district thereby enabling them to live off the land. Their orders were simply to act against the British whenever possible. Their tactics were to strike fast and hard causing as much damage to the enemy as possible, and then to withdraw and vanish before enemy reinforcements could arrive. The vast distances of the Republics allowed the Boer commandos considerable freedom to move about and made it nearly impossible for the 250,000 British troops to control the territory effectively using columns alone. As soon as a British column left a town or district, British control of that area faded away.
The Boer commandos were especially effective during the initial guerrilla phase of the war because Roberts had assumed that the war would end with the capture of the Boer capitals and the dispersal of the main Boer armies. Many British troops were therefore redeployed out of the area, and had been replaced by lower-quality contingents of Imperial Yeomanry and locally raised irregular corps.
From late May 1900, the first successes of the Boer strategy were at Lindley (where 500 Yeomanry surrendered), and at Heilbron (where a large convoy and its escort were captured) and other skirmishes resulting in 1,500 British casualties in less than ten days. In December 1900, De la Rey and Christiaan Beyers mauled a British brigade at Nooitgedacht. As a result of these and other Boer successes, the British, led by Lord Kitchener, mounted three extensive searches for De Wet, but without success. However, by the very nature of the Boer guerrilla war was sporadic, poorly planned, and with little overall objective in mind except to harass the British. This led to a disorganised pattern of scattered engagements throughout the region.
British response.
The British were forced to quickly revise their tactics. They concentrated on restricting the freedom of movement of the Boer commandos and depriving them of local support. The railway lines had provided vital lines of communication and supply, and as the British had advanced across South Africa, they had used armoured trains and had established fortified blockhouses at key points. They now built additional blockhouses (each housing 6–8 soldiers) and fortified these to protect supply routes against Boer raiders. Eventually some 8,000 such blockhouses were built across the two South African republics, radiating from the larger towns along principal routes. Each blockhouse cost between £800 to £1,000 and took about three months to build. However, they proved very effective. Not one bridge where one of these blockhouses was sited and manned was blown.
The blockhouse system required an enormous number of troops to garrison. Well over 50,000 British troops, or 50 battalions, were involved in blockhouse duty, greater than the approximately 30,000 Boers in the field during the guerrilla phase. In addition, up to 16,000 Africans were used both as armed guards and to patrol the line at night. The Army linked the blockhouses with barbed wire fences to parcel up the wide veld into smaller areas. "New Model" drives were mounted under which a continuous line of troops could sweep an area of veld bounded by blockhouse lines, unlike the earlier inefficient scouring of the countryside by scattered columns.
The British also implemented a "scorched earth" policy under which they targeted everything within the controlled areas that could give sustenance to the Boer guerrillas with a view to making it harder for the Boers to survive. As British troops swept the countryside, they systematically destroyed crops, burned homesteads and farms, poisoned wells, and interned Boer and African women, children and workers in concentration camps. Finally, the British also established their own mounted raiding columns in support of the sweeper columns. These were used to rapidly follow and relentlessly harass the Boers with a view to delaying them and cutting off escape, while the sweeper units caught up. Many of the 90 or so mobile columns formed by the British to participate in such drives were a mixture of British and colonial troops, but they also had a large minority of armed Africans. The total number of armed Africans serving with these columns has been estimated at approximately 20,000.
The British Army also made use of Boer auxiliaries who had been persuaded to change sides and enlist as "National Scouts". Serving under the command of General Andries Cronje, the National Scouts were despised as "hensoppers" (collaborators) but came to number a fifth of the fighting Afrikaners by the end of the War.
The British utilised armoured trains throughout the War to deliver rapid reaction forces much more quickly to incidents (such as Boer attacks on blockhouses and columns) or to drop them off ahead of retreating Boer columns.
Orange Free State.
After having conferred with the Transvaal leaders, De Wet returned to the Orange Free State, where he inspired a series of successful attacks and raids from the hitherto quiet western part of the country, though he suffered a rare defeat at Bothaville in November 1900. Many Boers who had earlier returned to their farms, sometimes giving formal parole to the British, took up arms again. In late January 1901, De Wet led a renewed invasion of Cape Colony. This was less successful, because there was no general uprising among the Cape Boers, and De Wet's men were hampered by bad weather and relentlessly pursued by British forces. They narrowly escaped across the Orange River.
From then until the final days of the war, De Wet remained comparatively quiet, partly because the Orange Free State was effectively left desolate by British sweeps. In late 1901, De Wet overran an isolated British detachment at Groenkop, inflicting heavy casualties. This prompted Kitchener to launch the first of the "New Model" drives against him. De Wet escaped the first such drive, but lost 300 of his fighters. This was a severe loss, and a portent of further attrition, although the subsequent attempts to round up De Wet were badly handled, and De Wet's forces avoided capture.
Western Transvaal.
The Boer commandos in the Western Transvaal were very active after September 1901. Several battles of importance were fought here between September 1901 and March 1902. At Moedwil on 30 September 1901 and again at Driefontein on 24 October, General Koos De La Rey's forces attacked the British, but were forced to withdraw after the British offered strong resistance.
A time of relative quiet descended thereafter on the western Transvaal. February 1902 saw the next major battle in that region. On 25 February, Koos De La Rey attacked a British column under Lieutenant-Colonel S. B. von Donop at Ysterspruit near Wolmaransstad. De La Rey succeeded in capturing many men and a large amount of ammunition. The Boer attacks prompted Lord Methuen, the British second-in-command after Lord Kitchener, to move his column from Vryburg to Klerksdorp to deal with De La Rey. On the morning of 7 March 1902, the Boers attacked the rear guard of Methuen's moving column at Tweebosch. Confusion reigned in British ranks and Methuen was wounded and captured by the Boers.
The Boer victories in the west led to stronger action by the British. In the second half of March 1902, large British reinforcements were sent to the Western Transvaal under the direction of Ian Hamilton. The opportunity the British were waiting for arose on 11 April 1902 at Rooiwal, where a commando led by General Jan Kemp and Commandant Potgieter attacked a superior force under Kekewich. The British soldiers were well positioned on the hillside and inflicted severe casualties on the Boers charging on horseback over a large distance, beating them back. This was the end of the war in the Western Transvaal and also the last major battle of the war.
Eastern Transvaal.
Two Boer forces fought in this area, one under Botha in the south east and a second under Ben Viljoen in the north east around Lydenburg. Botha's forces were particularly active, raiding railways and British supply convoys, and even mounting a renewed invasion of Natal in September 1901. After defeating British mounted infantry in the Battle of Blood River Poort near Dundee, Botha was forced to withdraw by heavy rains that made movement difficult and crippled his horses. Back on the Transvaal territory around his home district of Vryheid, Botha attacked a British raiding column at Bakenlaagte, using an effective mounted charge. One of the most active British units was effectively destroyed in this engagement. This made Botha's forces the target of increasingly large and ruthless drives by British forces, in which the British made particular use of native scouts and informers. Eventually, Botha had to abandon the high veld and retreat to a narrow enclave bordering Swaziland.
To the north, Ben Viljoen grew steadily less active. His forces mounted comparatively few attacks and as a result, the Boer enclave around Lydenburg was largely unmolested. Viljoen was eventually captured.
Cape Colony.
In parts of Cape Colony, particularly the Cape Midlands district where Boers formed a majority of the white inhabitants, the British had always feared a general uprising against them. In fact, no such uprising took place, even in the early days of the war when Boer armies had advanced across the Orange. The cautious conduct of some of the elderly Orange Free State generals had been one factor that discouraged the Cape Boers from siding with the Boer republics. Nevertheless, there was widespread pro-Boer sympathy.
After he escaped across the Orange in March 1901, De Wet had left forces under Cape rebels Kritzinger and Scheepers to maintain a guerrilla campaign in the Cape Midlands. The campaign here was one of the least chivalrous of the war, with intimidation by both sides of each other's civilian sympathizers. In one of many skirmishes, Commandant Lotter's small commando was tracked down by a much-superior British column and wiped out at Groenkloof. Several captured rebels, including Lotter and Scheepers, who was captured when he fell ill with appendicitis, were executed by the British for treason or for capital crimes such as the murder of prisoners or of unarmed civilians. Some of the executions took place in public, to deter further disaffection. Since the Cape Colony was Imperial territory, its authorities forbade the British Army to burn farms or to force Boers into concentration camps.
Fresh Boer forces under Jan Christiaan Smuts, joined by the surviving rebels under Kritzinger, made another attack on the Cape in September 1901. They suffered severe hardships and were hard pressed by British columns, but eventually rescued themselves by routing some of their pursuers at the Battle of Elands River and capturing their equipment. From then until the end of the war, Smuts increased his forces from among Cape rebels until they numbered 3,000. However, no general uprising took place, and the situation in the Cape remained stalemated.
In January 1902, Boer leader Manie Maritz was implicated in the Leliefontein massacre in the far Northern Cape.
Surgery and medicine during the war.
More than half of British casualties during the war were caused by illness, especially typhoid fever, rather than enemy action.
Concentration camps (1900–1902).
The term "concentration camp" was used to describe camps operated by the British in South Africa during this conflict, and the term grew in prominence during this period.
The camps had originally been set up by the British Army as "refugee camps" to provide refuge for civilian families who had been forced to abandon their homes for whatever reason related to the war. However, when Kitchener succeeded Roberts as commander-in-chief in South Africa on 29 November 1900, the British Army introduced new tactics in an attempt to break the guerrilla campaign and the influx of civilians grew dramatically as a result. Kitchener initiated plans to
flush out guerrillas in a series of systematic drives, organised like a sporting shoot, with success defined in a weekly 'bag' of killed, captured and wounded, and to sweep the country bare of everything that could give sustenance to the guerrillas, including women and children ... It was the clearance of civilians—uprooting a whole nation—that would come to dominate the last phase of the war.
As Boer farms were destroyed by the British under their "Scorched Earth" policy—including the systematic destruction of crops and slaughtering of livestock, the burning down of homesteads and farms, and the poisoning of wells and salting of fields—to prevent the Boers from resupplying from a home base many tens of thousands of women and children were forcibly moved into the concentration camps. This was not the first appearance of internment camps. The Spanish had used internment in the Ten Years' War that led to the Spanish–American War, and the United States had used them to devastate guerrilla forces during the Philippine–American War. But the Boer War concentration camp system was the first time that a whole nation had been systematically targeted, and the first in which some whole regions had been depopulated.
Eventually, there were a total of 45 tented camps built for Boer internees and 64 for black Africans. Of the 28,000 Boer men captured as prisoners of war, 25,630 were sent overseas. The vast majority of Boers remaining in the local camps were women and children. Over 26,000 women and children were to perish in these concentration camps.
The camps were poorly administered from the outset and became increasingly overcrowded when Kitchener's troops implemented the internment strategy on a vast scale. Conditions were terrible for the health of the internees, mainly due to neglect, poor hygiene and bad sanitation. The supply of all items was unreliable, partly because of the constant disruption of communication lines by the Boers. The food rations were meager and there was a two-tier allocation policy, whereby families of men who were still fighting were routinely given smaller rations than others (Pakenham 1979, p. 505). The inadequate shelter, poor diet, bad hygiene and overcrowding led to malnutrition and endemic contagious diseases such as measles, typhoid and dysentery to which the children were particularly vulnerable. An additional problem was the Boers' use of traditional medicines like a cow-dung poultice for skin diseases and crushed insects for convulsions. Coupled with a shortage of modern medical facilities, many of the internees died.
As the war raged across their farms and their homes were destroyed, many Africans became refugees and they, like the Boers, moved to the towns where the British Army hastily created internment camps. Subsequently, the "Scorched Earth" policy was ruthlessly applied to both Boers and Africans. Although most black Africans were not considered by the British to be hostile, many tens of thousands were also forcibly removed from Boer areas and also placed in concentration camps.
Africans were held separately from Boer internees. Eventually there were a total of 64 tented camps for Africans. Conditions were as bad as in the camps for the Boers, but even though, after the Fawcett Commission report, conditions improved in the Boer camps, "improvements were much slower in coming to the black camps."
Public opinion and political opposition.
Although the 1900 UK general election, also known as the "Khaki election," had resulted in a victory for the Conservative government on the back of recent British victories against the Boers, public support quickly waned as it became apparent that the war would not be easy and further unease developed following reports about the treatment by the British army of the Boer civilians. Public and political opposition to government policies in South Africa regarding Boer civilians was first expressed in Parliament in February 1901 in the form of an attack on the policy, the government, and the army by the radical Liberal MP David Lloyd George.
Emily Hobhouse, a delegate of the South African Women and Children's Distress Fund, visited some of the camps in the Orange Free State from January 1901, and in May 1901 she returned to England on board the ship, the "Saxon". Alfred Milner, High Commissioner in South Africa, also boarded the "Saxon" for holiday in England but, unfortunately for both the camp internees and the British government, he had no time for Miss Hobhouse, regarding her as a Boer sympathiser and "trouble maker." On her return, Emily Hobhouse did much to publicise the distress of the camp inmates. She managed to speak to the Liberal Party leader, Henry Campbell-Bannerman who professed to be suitably outraged but was disinclined to press the matter, as his party was split between the imperialists and the pro-Boer factions.
The more radical Liberals however such as David Lloyd George and John Ellis were prepared to raise the matter in Parliament and to harass the government on the issue, which they duly did. St John Brodrick, the Conservative secretary of state for war, first defended the government's policy by arguing that the camps were purely "voluntary" and that the interned Boers were "contented and comfortable," but was somewhat undermined as he had no firm statistics to back up his argument, so when his "voluntary" argument proved untenable, he resorted to the "military necessity" argument and stated that everything possible was being done to ensure satisfactory conditions in the camps.
Hobhouse published a report in June 1901 that contradicted Brodrick's claim, and Lloyd George then openly accused the government of "a policy of extermination" directed against the Boer population. The same month Liberal opposition party leader Campbell-Bannerman took up the assault and answered the rhetorical question "When is a war not a war?" with his own rhetorical answer "When it is carried on by methods of barbarism in South Africa," referring to those same camps and the policies that created them. The Hobhouse report caused uproar both domestically and in the international community. However, there was very little public sympathy for the highly reactionary Boer president Kruger.
The Fawcett Commission.
Although the government had comfortably won the parliamentary debate by a margin of 252 to 149, it was stung by the criticism and concerned by the escalating public outcry, and called on Kitchener for a detailed report. In response, complete statistical returns from camps were sent in July 1901. By August 1901, it was clear to government and opposition alike that Miss Hobhouse's worst fears were being confirmed – 93,940 Boers and 24,457 black Africans were reported to be in "camps of refuge" and the crisis was becoming a catastrophe as the death rates appeared very high, especially among the children.
The government responded to the growing clamour by appointing a commission. The Fawcett Commission, as it became known was, uniquely for its time, an all-woman affair headed by Millicent Fawcett who despite being the leader of the women's suffrage movement was a Liberal Unionist and thus a government supporter and considered a safe pair of hands. Between August and December 1901, the Fawcett Commission conducted its own tour of the camps in South Africa. While it is probable that the British government expected the Commission to produce a report that could be used to fend off criticism, in the end it confirmed everything that Emily Hobhouse had said. Indeed, if anything the Commission's recommendations went even further. The Commission insisted that rations should be increased and that additional nurses be sent out immediately, and included a long list of other practical measures designed to improve conditions in the camp. Millicent Fawcett was quite blunt in expressing her opinion that much of the catastrophe was owed to a simple failure to observe elementary rules of hygiene.
In November 1901, the Colonial Secretary Joseph Chamberlain ordered Alfred Milner to ensure that "all possible steps are being taken to reduce the rate of mortality." The civil authority took over the running of the camps from Kitchener and the British command and by February 1902 the annual death-rate in the concentration camps for white inmates dropped to 6.9 percent and eventually to 2 percent, which was a lower rate than pertained in many British cities at the time. However, by then the damage had been done. A report after the war concluded that 27,927 Boers (of whom 24,074 [50 percent of the Boer child population] were children under 16) had died of starvation, disease and exposure in the concentration camps. In all, about one in four (25 percent) of the Boer inmates, mostly children, died.
"Improvements [however] were much slower in coming to the black camps." It is thought that about 12 percent of black African inmates died (about 14,154) but the precise number of deaths of black Africans in concentration camps is unknown as little attempt was made to keep any records of the 107,000 black Africans who were interned.
The main decisions (or their absence) had been left to the soldiers, to whom the life or death of the 154,000 Boer and African civilians in the camps rated as an abysmally low priority. [It was only] ... ten months after the subject had first been raised in Parliament ... [and after public outcry and after the Fawcett Commission that remedial action was taken and] ... the terrible mortality figures were at last declining. In the interval, at least twenty thousand whites and twelve thousand coloured people had died in the concentration camps, the majority from epidemics of measles and typhoid that could have been avoided.
Somewhat higher figures for total deaths in the concentration camps are given by S.B. Spies.
Sir Arthur Conan Doyle had served as a volunteer doctor in the Langman Field Hospital at Bloemfontein between March and June 1900. In his widely distributed and translated pamphlet 'The War in South Africa: Its Cause and Conduct' he justified both the causes of the war and its conduct. He also pointed out that over 14,000 British soldiers had died of disease during the conflict (as opposed to 8000 killed in combat) and at the height of epidemics he was seeing 50–60 British soldiers dying each day in a single ill-equipped and overwhelmed military hospital.
Kitchener's policy and the post-war debate.
It has been argued that "this was not a deliberately genocidal policy; rather it was the result of [a] disastrous lack of foresight and rank incompetence on [the] part of the [British] military." British historian Niall Ferguson also argues that "Kitchener no more desired the deaths of women and children in the camps than of the wounded Dervishes after Omdurman, or of his own soldiers in the typhoid stricken hospitals of Bloemfontein."
However, to Kitchener and the British Command "the life or death of the 154,000 Boer and African civilians in the camps rated as an abysmally low priority" against military objectives. As the Fawcett Commission was delivering its recommendations, Kitchener wrote to St John Brodrick defending his policy of sweeps, and emphasising that no new Boer families were being brought in unless they were in danger of starving. This was disingenuous as the countryside had by then been devastated under the "Scorched Earth" policy (the Fawcett Commission in December 1901 in its recommendations commented that: "to turn 100,000 people now being held in the concentration camps out on the veldt to take care of themselves would be cruelty") and now that the New Model counter insurgency tactics were in full swing, it made cynical military sense to leave the Boer families in desperate conditions in the countryside.
According to writer S.B. Spies, "at [the Vereeniging negotiations in May 1902] Boer leader Louis Botha stated that he had tried to send [Boer] families to the British, but they had refused to receive them." Spies quotes a Boer commandant referring to Boer women and children made refugees by Britain's scorched-earth policy as saying, "Our families are in a pitiable condition and the enemy uses those families to force us to surrender." Spies adds, "and there is little doubt that that was indeed the intention of Kitchener when he had issued instructions that no more families were to be brought into the concentration camps." Thomas Pakenham writes of Kitchener's policy U-turn,
No doubt the continued 'hullabaloo' at the death-rate in these concentration camps, and Milner's belated agreement to take over their administration, helped change Kitchener's mind [some time at the end of 1901]. ... By mid-December at any rate, Kitchener was already circulating all column commanders with instructions not to bring in women and children when they cleared the country, but to leave them with the guerrillas. ... Viewed as a gesture to Liberals, on the eve of the new session of Parliament at Westminster, it was a shrewd political move. It also made excellent military sense, as it greatly handicapped the guerrillas, now that the drives were in full swing. ... It was effective precisely because, contrary to the Liberals' convictions, it was less humane than bringing them into camps, though this was of no great concern to Kitchener.
The end of the war.
Towards the end of the war, British tactics of containment, denial, and harassment began to yield results against the guerrillas. The sourcing and coordination of intelligence became increasingly efficient with regular reporting from observers in the blockhouses, from units patrolling the fences and conducting "sweeper" operations, and from native Africans in rural areas who increasingly supplied intelligence, as the Scorched Earth policy took effect and they found themselves competing with the Boers for food supplies. Kitchener's forces at last began to seriously affect the Boers' fighting strength and freedom of manoeuvre, and made it harder for the Boers and their families to survive.
The Boers and the British both feared the consequences of arming Africans. The memories of the Zulu and other tribal conflicts were still fresh, and they recognised that whoever won would have to deal with the consequences of a mass militarisation of the tribes. There was therefore an unwritten agreement that this war would be a "white man's war." At the outset, British officials instructed all white magistrates in the Natal Colony to appeal to Zulu ama-khosi to remain neutral, and President Kruger sent emissaries asking them to stay out of it. However, in some cases there were old scores to be settled, and some Africans, such as the Swazis, were eager to enter the war with the specific aim of reclaiming land confiscated by the Boers. As the war went on there was greater involvement of Africans, and in particular large numbers became embroiled in the conflict on the British side, either voluntarily or involuntarily. By the end of the war, many blacks had been armed and had shown conspicuous gallantry in roles such as scouts, messengers, watchmen in blockhouses, and auxiliaries.
And there were more flash-points outside of the war. On 6 May 1902 at Holkrantz in the southeastern Transvaal, a Zulu faction had their cattle stolen and their people mistreated by the Boers as a punishment for helping the British. The local Boer officer then sent an insulting message to the tribe, challenging them to take back their cattle. The Zulus attacked at night, and in a mutual bloodbath, the Boers lost 56 killed and 3 wounded, while the Africans suffered 52 killed and 48 wounded.
The official statistics of blacks who had served as combatants or non-combatants or who died in the concentration camps are unreliable. Many black combatants were dumped in unmarked graves, and most of the superintendents of the concentration camps did not record the deaths of black inmates.
After the war the British government went to great lengths to attempt to conciliate Boer opinion to the extent of refusing to officially recognise the military contribution made by blacks by issuing campaign medals. It was felt that the Boers would already feel insecure and angry at the arming of blacks, and granting medals would have prejudiced the stability of the region. Boer insecurity and the British government's favouring of Boer over African interests caused much bitterness, and did much to shape the racial politics of the region.
The British offered terms of peace on various occasions, notably in March 1901, but were rejected by Botha. The last of the Boers surrendered in May 1902 and the war ended with the Treaty of Vereeniging signed on 31 May 1902. Although the British had won, this came at a cost; the Boers were given £3,000,000 for reconstruction and were promised eventual limited self-government, which was granted in 1906 and 1907. The treaty ended the existence of the South African Republic and the Orange Free State as independent Boer republics and placed them within the British Empire. The Union of South Africa was established as a member of the Commonwealth in 1910.
In all, the war had cost around 75,000 lives; 22,000 British and allied soldiers (7,792 killed in battle, the rest through disease), between 6,000 and 7,000 Boer fighters, and, mainly in the concentration camps, between 20,000 to 28,000 Boer civilians (mainly women and children) and perhaps 20,000 black Africans (both on the battlefield and in the concentration camps). During the conflict, 78 Victoria Crosses (VC) – the highest and most prestigious award in the British armed forces for bravery in the face of the enemy – were awarded to British and colonial soldiers. See List of Boer War Victoria Cross recipients.
Aftermath and analysis.
The Second Boer War cast long shadows over the history of the South African region. The predominantly agrarian society of the former Boer republics was profoundly and fundamentally affected by the scorched earth policy of Roberts and Kitchener. The devastation of both Boer and black African populations in the concentration camps and through war and exile were to have a lasting effect on the demography and quality of life in the region. Many exiles and prisoners were unable to return to their farms at all; others attempted to do so but were forced to abandon the farms as unworkable given the damage caused by farm burning and salting of the fields in the course of the scorched earth policy. Destitute Boers and black Africans swelled the ranks of the unskilled urban poor competing with the "uitlanders" in the mines.
The postwar reconstruction administration was presided over by Lord Milner and his largely Oxford trained Milner's Kindergarten. This small group of civil servants had a profound effect on the region, eventually leading to the Union of South Africa. "In the aftermath of the war, an imperial administration freed from accountability to a domestic electorate set about reconstructing an economy that was by then predicated unambiguously on gold. At the same time, British civil servants, municipal officials, and their cultural adjuncts were hard at work in the heartland of the former Boer Republics helping to forge new identities – first as 'British South Africans' and then, later still, as 'white South Africans'." Some scholars, for good reasons, identify these new identities as partly underpinning the act of union that followed in 1910. Although challenged by a Boer rebellion only four years later, they did much to shape South African politics between the two world wars and right up to the present day."
The counterinsurgency techniques and lessons (the restriction of movement, the containment of space, the ruthless targeting of anything, everything and anyone that could give sustenance to guerrillas, the relentless harassment through sweeper groups coupled with rapid reaction forces, the sourcing and coordination of intelligence, and the nurturing of native allies) learned during the Boer War were used by the British (and other forces) in future guerrilla campaigns including to counter Malayan communist rebels during the Malayan Emergency. In World War II the British also adopted some of the concepts of raiding from the Boer commandos when, after the fall of France, they set up their special raiding forces, and in acknowledgement of their erstwhile enemies, chose the name British Commandos.
Many of the Boers referred to the war as the second of the "Freedom Wars". The most resistant of Boers wanted to continue the fight and were known as "bittereinders" (or "irreconcilables") and at the end of the war a number of Boer fighters such as Deneys Reitz chose exile rather than sign an oath, such as the following, to pledge allegiance to Britain:
The bearer, "<prisoner name>" has been released from prison of war camp "<Camp name>" on signing that he acknowledge terms of surrender and becomes a British subject.
 
 Over the following decade, many returned to South Africa and never signed the pledge. Some, like Reitz, eventually reconciled themselves to the new "status quo", but others could not.
Union of South Africa.
One of the most important events in the decade after the end of the war was the creation of the Union of South Africa (later the Republic of South Africa). It proved a key ally to Britain as a Dominion of the British Empire during the World Wars. At the start of First World War a crisis ensued when the South African government led by Louis Botha and other former Boer fighters, such as Jan Smuts, declared support for Britain and agreed to send troops to take over the German colony of German South-West Africa (Namibia).
Many Boers were opposed to fighting for Britain, especially against Germany, which had been sympathetic to their struggle. A number of bittereinders and their allies took part in a revolt known as the Maritz Rebellion. This was quickly suppressed and in 1916, the leading Boer rebels in the Maritz Rebellion got off lightly (especially compared with the fate of leading Irish rebels of the Easter Rising), with terms of imprisonment of six and seven years and heavy fines. Two years later, they were released from prison, as Louis Botha recognised the value of reconciliation. Thereafter the bittereinders concentrated on political organisation within the constitutional system and built up what later became the National Party, which took power in 1948 and dominated the politics of South Africa from the late 1940s until the early 1990s, under the apartheid system.
Effect of the war on domestic British politics.
Many Irish nationalists sympathised with the Boers, viewing them to be a people oppressed by British imperialism, much like themselves. Irish miners already in the Transvaal at the start of the war formed the nucleus of two Irish commandos. The Second Irish Brigade was headed up by an Australian of Irish parents, Colonel Arthur Lynch. In addition, small groups of Irish volunteers went to South Africa to fight with the Boers – this despite the fact that there were many Irish troops fighting in the British army. In Britain, the "Pro-Boer" campaign expanded, with writers often idealising the Boer society.
The war also highlighted the dangers of Britain's policy of non-alignment and deepened her isolation. The 1900 UK general election, also known as the "Khaki election", was called by the Prime Minister, Lord Salisbury, on the back of recent British victories. There was much enthusiasm for the war at this point, resulting in a victory for the Conservative government.
However, public support quickly waned as it became apparent that the war would not be easy and it dragged on, partially contributing to the Conservatives' spectacular defeat in 1906. There was public outrage at the use of scorched earth tactics – the forced clearance of women and children, the destruction of the countryside, burning of Boer homesteads and poisoning of wells, for example – and the conditions in the concentration camps. It also became apparent that there were serious problems with public health in Britain: up to 40% of recruits in Britain were unfit for military service, suffering from medical problems such as rickets and other poverty-related illnesses. This came at a time of increasing concern for the state of the poor in Britain.
Having taken the country into a prolonged war, the Conservative government was rejected by the electorate at the first general election after the war was over. Balfour, succeeding his uncle Lord Salisbury in 1903 immediately after the war, took over a Conservative party that had won two successive landslide majorities but led it to a landslide defeat in 1906.
Horses.
The number of horses killed in the war was at the time unprecedented in modern warfare. For example, in the Relief of Kimberley, French's cavalry rode 500 horses to their deaths in a single day. The wastage was particularly heavy among British forces for several reasons: overloading of horses with unnecessary equipment and saddlery, failure to rest and acclimatise horses after long sea voyages and, later in the war, poor management by inexperienced mounted troops and distant control by unsympathetic staffs. The average life expectancy of a British horse, from the time of its arrival in Port Elizabeth, was around six weeks.
Horses were on occasion slaughtered for their meat. During the Siege of Kimberley and Siege of Ladysmith, horses were consumed as food once the regular sources of meat were depleted. The besieged British forces in Ladysmith also produced "chevril", a Bovril-like paste, by boiling down the horse meat to a jelly paste and serving it like beef tea.
The Horse Memorial in Port Elizabeth is a tribute to the 300,000 horses that died during the conflict.
British Empire involvement.
The vast majority of troops fighting for the British army came from the United Kingdom. However, a significant number came from other parts of the British Empire. These countries had their own internal disputes over whether they should remain tied to the United Kingdom, or have full independence, which carried over into the debate around the sending of forces to assist the United Kingdom. Though not fully independent on foreign affairs, these countries did have local say over how much support to provide, and the manner it was provided. Ultimately, Australia, Canada, New Zealand and Company-ruled Rhodesia all sent volunteers to aid the United Kingdom. Australia provided the largest number of troops followed by Canada. Troops were also raised to fight with the British from the Cape Colony and the Colony of Natal. Some Boer fighters, such as Jan Smuts and Louis Botha, were technically British subjects as they came from the Cape Colony and Colony of Natal, respectively.
There were also many volunteers from the Empire who were not selected for the official contingents from their countries and travelled privately to South Africa to form private units, such as the Canadian Scouts and Doyle's Australian Scouts. There were also some European volunteer units from British India and British Ceylon, though the British Government refused offers of non-white troops from the Empire. Some Cape Coloureds also volunteered early in the war, but later some of them were effectively conscripted and kept in segregated units. As a community, they received comparatively little reward for their services. In many ways, the war set the pattern for the Empire's later involvement in the two World Wars. Specially raised units, consisting mainly of volunteers, were dispatched overseas to serve with forces from elsewhere in the British Empire.
Technically the United States stayed neutral in the conflict, but some American citizens were asked to participate. Early in the war Lord Roberts cabled the American Frederick Russell Burnham, a veteran of both Matabele wars but at that very moment prospecting in the Klondike, to serve on his personal staff as Chief of Scouts. Burnham went on to receive the highest awards of any American who served in the war, but American mercenaries participated on both sides.
Australia.
From 1899 to 1901 the six separate self-governing colonies in Australia sent their own contingents to serve in the Boer War. Much of the population of the colonies had originated from what was then the United Kingdom of Great Britain and Ireland (England, Scotland, Wales, Ireland) and the desire to support Britain during the conflict appealed to many. After the colonies formed the Commonwealth of Australia in 1901, the new Government of Australia sent "Commonwealth" contingents to the war. The Boer War was thus the first war in which the Commonwealth of Australia fought. A few Australians fought on the Boer side. The most famous and colourful character was Colonel Arthur Alfred Lynch, formerly of Ballarat, Victoria, who raised the Second Irish Brigade.
The Australian climate and geography were far closer to that of South Africa than most other parts of the empire, so Australians adapted quickly to the environment, with troops serving mostly among the army's "mounted rifles." Enlistment in all official Australian contingents totalled 16,463. Another five to seven thousand Australians served in "irregular" regiments raised in South Africa. Perhaps five hundred Australian irregulars were killed. In total, 20,000 or more Australians served and about 1,000 were killed. A total of 267 died from disease, 251 were killed in action or died from wounds sustained in battle. A further 43 men were reported missing.
When the war began some Australians, like some Britons, opposed it. As the war dragged on some Australians became disenchanted, in part because of the sufferings of Boer civilians reported in the press. In an interesting twist (for Australians), when the British missed capturing President Paul Kruger, as he escaped Pretoria during its fall in June 1900, a "Melbourne Punch", 21 June 1900, cartoon depicted how the War could be won, using the Kelly Gang.
The convictions and executions of two Australian lieutenants, Breaker Morant and Peter Handcock in 1902, and the imprisonment of a third, George Witton, had little impact on the Australian public at the time despite later legend. The controversial court-martial saw the three convicted of executing Boer prisoners under their authority. After the war, though, Australians joined an empire-wide campaign that saw Witton released from jail. Much later, some Australians came to see the execution of Morant and Handcock as instances of wrongfully executed Australians, as illustrated in the 1980 Australian film "Breaker Morant".
Canada.
Over 7,000 Canadian soldiers and support personnel were involved in the second Boer war from October 1899 to May 1902. With approximately 7,368 soldiers in a combat situation, the conflict became the largest military engagement involving Canadian soldiers from the time of Confederation until the Great War. Eventually, 270 soldiers died in the course of the Boer War. The Canadian public was initially divided on the decision to go to war as some citizens did not want Canada to become Britain's 'tool' for engaging in armed conflicts. Many Anglophone citizens were pro-Empire, and wanted the Prime Minister, Sir Wilfrid Laurier, to support the British in their conflict. On the other hand, many Francophone citizens felt threatened by the continuation of British Imperialism to their national sovereignty.
In the end, in order to appease the citizens who wanted war and avoid angering those who didn't, Laurier sent 1,000 volunteers under the command of Lieutenant Colonel William Otter to aid the confederation in its war to 'liberate' the peoples of the Boer controlled states in South Africa. The volunteers were provided to the British with the stipulation that the British pay costs of the battalion after it arrived in South Africa.
The supporters of the war claimed that it "pitted British Freedom, justice and civilization against Boer backwardness". The French Canadians' opposition to the Canadian involvement in a British 'colonial venture' eventually led to a three-day riot in various areas of Quebec.
Commonwealth involvement in the Boer War can be summarized into three parts. The first part (October 1899–December 1899) was characterized by questionable decisions and blunders from the Commonwealth leadership which affected its soldiers greatly. The soldiers of the Commonwealth were shocked at the number of Afrikaner soldiers who were willing to oppose the British. The Afrikaner troops were very willing to fight for their country, and were armed with modern weaponry and were highly mobile soldiers. This was one of the best examples of Guerilla style warfare, which would be employed throughout the twentieth century after set piece fighting was seen as a hindrance by certain groups. The Boer soldiers would evade capture and secure provisions from their enemies therefore they were able to exist as a fighting entity for an indeterminate period of time.
The end of the First part was the period in mid-December which is referred to as the "Black Week". During the week of 10–17 December 1899, the British suffered three major defeats at the hands of the Boers at the battlefields of Stormberg, Magersfontein and Colenso. Afterwards, the British called upon more volunteers to take part in the war from the Commonwealth.
The second part of the war (February–April 1900) was the opposite of the first. After the British reorganized and reinforced under new leadership, they began to experience success against the Boer soldiers. Commonwealth soldiers resorted to using blockhouses, farm burning and concentration camps to 'persuade' the resisting Boers into submission.
The final phase of the war was the guerrilla phase where many Boer soldiers turned to Guerrilla tactics such as raiding infrastructure or communications lines. Many Canadian soldiers did not actually see combat after getting shipped over to South Africa as many arrived around the time of the signing of the Treaty of Vereeniging on 31 May 1902.
New Zealand.
When the Second Boer War seemed imminent, New Zealand offered its support. On 28 September 1899, Prime Minister Richard Seddon asked Parliament to approve the offer to the imperial government of a contingent of mounted rifles, thus becoming the first British Colony to send troops to the Boer War. The British position in the dispute with the Transvaal was "moderate and righteous," he maintained. He stressed the "crimson tie" of Empire that bound New Zealand to the mother-country and the importance of a strong British Empire for the colony's security.
By the time peace was concluded two and a half years later, 10 contingents of volunteers, totalling nearly 6,500 men from New Zealand, with 8,000 horses had fought in the conflict, along with doctors, nurses, veterinary surgeons and a small number of school teachers. Some 70 New Zealanders died from enemy action, with another 158 killed accidentally or by disease.
"The New Zealanders in South Africa 1899–1902"
South Africa.
During the war, the British army also included substantial contingents from South Africa itself. There were large communities of English-speaking immigrants and settlers in Natal and Cape Colony (especially around Cape Town and Grahamstown), which formed volunteer units that took the field, or local "town guards." At one stage of the war, a "Colonial Division," consisting of five light horse and infantry units under Brigadier General Edward Brabant, took part in the invasion of the Orange Free State. Part of it withstood a siege by Christiaan De Wet at Wepener on the borders of Basutoland. Another large source of volunteers was the "uitlander" community, many of whom hastily left Johannesburg in the days immediately preceding the war.
Later during the war, Lord Kitchener attempted to form a Boer Police Force, as part of his efforts to pacify the occupied areas and effect a reconciliation with the Boer community. The members of this force were despised as traitors by the Boers still in the field. Those Boers who attempted to remain neutral after giving their parole to British forces were derided as "hensoppers" (hands-uppers) and were often coerced into giving support to the Boer guerrillas. (This was one of the reasons for the British ruthlessly scouring the countryside of people, livestock and anything else the Boer commandos might find useful.)
Like the Canadian and particularly the Australian and New Zealand contingents, many of the volunteer units formed by South Africans were "light horse" or mounted infantry, well suited to the countryside and manner of warfare. Some regular British officers scorned their comparative lack of formal discipline, but the light horse units were hardier and more suited to the demands of campaigning than the overloaded British cavalry, who were still obsessed with the charge with lance or sabre. At their peak, 24,000 South Africans (including volunteers from the Empire) served in the field in various "colonial" units. Notable units (in addition to the Imperial Light Horse) were the South African Light Horse, Rimington's Guides, Kitchener's Horse and the Imperial Light Infantry.
Notable people involved in the Boer War.
Harold Lothrop Borden was the only son of Canada's Canadian Minister of Defence and Militia, Frederick William Borden. Serving in the Royal Canadian Dragoons, he became the most famous Canadian casualty of the Second Boer War. Queen Victoria asked F. W. Borden for a photograph of his son, Prime Minister Wilfrid Laurier praised his services, tributes arrived from across Canada, and in his home town Canning, Nova Scotia, there is a monument (by Hamilton MacCarthy) erected to his memory.
Sam Hughes - Senior Militia officer and later a Federally elected cabinet minister. As a very patriotic individual, Hughes became involved in the Boer war as a member of Brigadier-General Herbert Settle's expedition after Hughes unsuccessfully tried to raise his own brigade of soldiers. Hughes was noted by his colleagues for having a dislike of professional soldiers and he was noted for being an exceptional leader of irregular soldiers, whom he preferred to lead in combat. However, Hughes was dismissed and was sent home in the summer of 1900 for; sending letters back home which were published outlining British command incompetence, his impatience and boastfulness and his providing surrendering enemies favourable conditions. When he arrived back in Canada, Hughes became very active politically, and he would eventually start his political career with the Conservatives. When he became a member of parliament, Hughes would be in the position to become the Canadian Minister of Defence and Militia in 1911, just prior the outbreak of World War I. This was a position that Hughes would be dismissed from in 1916, due once again to his impatience, among other reasons.
John McCrae - Best known as the author of the World War I poem In Flanders Fields, McCrae started his active military service in the Boer War as an artillery officer. After completing several major campaigns, McCrae's artillery unit was sent home to Canada in 1901 with what would be referred to today as an 'honourable discharge'. McCrae ended up becoming a special professor in the University of Vermont for pathology and he would later serve in World War I as a Medical officer until his death in 1918 while on active duty due to pneumonia.
Harry "Breaker" Morant - Anglo-Australian poet and soldier who participated in the summary execution of several Boer (Afrikaner) prisoners and the killing of a German missionary, Daniel Heese, who had been a witness to the shootings. Court-martialed and executed for murder.
Winston Churchill - Best known as the prime minister of Britain during the main part of the Second World War worked as a war correspondent for The Morning Post. He was captured and held prisoner in a camp in Pretoria from which he escaped and rejoined the British army. He received a commission in the South African Light Horse (still working as a correspondent) and witnessed the capture of Ladysmith and Pretoria.
Mahatma Gandhi - Best known as the preeminent leader of Indian independence movement in British-ruled India he volunteered in 1900 to form a group of ambulance drivers raising eleven hundred Indian volunteer medics. At Spion Kop Gandhi and his bearers had to carry wounded soldiers for miles to a field hospital because the terrain was too rough for the ambulances. General Redvers Buller mentioned the courage of the Indians in his dispatch. Gandhi and thirty-seven other Indians received the War Medal.
Victoria Cross recipients.
Four Canadian soldiers in the Second Boer War received a Victoria Cross, which is the highest military medal available to soldiers of the Commonwealth and former British Territories. It is awarded based on exemplary bravery and valour in the presence of danger.
Sergeant Arthur Herbert Lindsay Richardson - Soldier of Lord Strathcona's Horse, Richardson rode a wounded horse, while wounded himself, back into enemy fire to retrieve a wounded comrade whose horse had been killed at Wolve Spruit on 5 July 1900.
Lieutenant Hampden Zane Churchill Cockburn - Soldier of the Royal Canadian Dragoons, Cockburn received his Victoria Cross on 7 November 1900 when his unit was the rear guard at Leliefontein. Cockburn, along with fellow Victoria Cross recipient Lieutenant R.E.W. Turner, held off an advancing group of Boer soldiers in order to allow two Canadian Field guns to escape along with their crews. Cockburn was wounded and captured by the Boer soldiers.
Lieutenant Richard Ernest William Turner- Soldier of the Royal Canadian Dragoons, Turner received his Victoria Cross during the same portion of the conflict as Cockburn. Turner was wounded in the conflict, however unlike Cockburn, Turner escaped. Turner would later became a high-ranking officer in the Canadian army in World War I.
Sergeant Edward James Gibson Holland - Soldier of the Royal Canadian Dragoons. Holland received his Victoria Cross from the same rear guard conflict at Leliefontein on 7 November 1900 as Cockburn and Turner. However, Holland received his medal for a different reason than the two aforementioned Lieutenants. During the Boer advance, Holland kept the Boer soldiers at bay with his carriage mounted Colt machine gun despite the position becoming increasingly dangerous due to the proximity of the enemy. With his gun jammed and in danger of falling into enemy hands, Holland removed the Colt from its carriage and rode away on his horse with the gun in hand.
Final overview.
The Second Boer War was the harbinger for a new type of combat which would persevere throughout the twentieth century, guerilla warfare. After the war was over, the entire British army underwent a period of reform which was focused on lessening the emphasis placed on mounted units in combat. It was determined that the idea of Cavalry was antiquated and improperly used on the battlefield in the modern warfare of the Boer War, and that the First World War was the final proof that cavalry had no place in twentieth century combat. Yet some British soldiers held dear to the fact that cavalry was put to better use after the reforms in the theatres of the Middle East and World War I, and that the idea of mounted infantry was useful in the times where the war was more mobile. An example of this was in the First World War during the battle of Mons where the British cavalry held the Belgian town against an initial German assault.
The Canadian units of the Royal Canadian Dragoons and the Royal Canadian Mounted Rifles fought in the first world war in the same role as the Boer war. However, during, and after, the Second World War the regiments swapped their horses for mechanized vehicles. The second Boer War was also the beginning of types of conflict involving machine guns, shrapnel and observation balloons which were all used extensively in the First World War. To the Canadians however, attrition was the leading cause of death in the second Boer war, with disease being the cause of approximately half of the Canadian deaths.
Canadians ended the war with four Victoria Crosses to its soldiers and two more Victoria Crosses were given to Canadian doctors attached to British Medical Corps units, Lieutenant H.E.M. Douglas (1899, Magersfontein) and Lieutenant W.H.S. Nickerson (1900, Wakkerstroom). Not all soldiers saw action since many landed in South Africa after the hostilities ended while others (including the 3rd Special Service Battalion, The Royal Canadian Regiment) performed garrison duty in Halifax, Nova Scotia so that their British counterparts could join at the front lines. Later on, contingents of Canadians served with the paramilitary South Africa Constabulary. The war also had its fair share of controversy, as Commonwealth soldiers used a scorched Earth policy as well as concentration camps to subdue the Boers. A total of 116 000 women, children and Boer soldiers were confined to the Commonwealth concentration camps, of which at least 28 000, mainly women and children, would die.
The British saw their tactics of Scorched Earth and concentration as ways of controlling the Boers by "eliminating the decay and deterioration of the national character" and as a way of reinforcing the values, through subjugation of citizens and the destruction of the means for the Boer soldiers to continue fighting, of British society that the Boers were rejecting by engaging in a war against the Commonwealth. The Boers saw it as a British ploy designed to coerce the Boer soldiers into a surrender. With approximately 10% of their population confined, many of whom were women and children, the Boers suggested that the British were forcing the Afrikaners to return to their homes and protect their families who were in danger of internment.
Commemorations.
The Australian National Boer War Memorial Committee organises events to mark the war on 31 May each year. In Canberra, a commemorative service is usually held at the Saint John the Baptist Anglican Church in Reid. Floral tributes are laid for the dead.

</doc>
<doc id="42721" url="http://en.wikipedia.org/wiki?curid=42721" title="Mary Elizabeth Braddon">
Mary Elizabeth Braddon

Mary Elizabeth Braddon (4 October 1835 – 4 February 1915) was an English popular novelist of the Victorian era. She is best known for her 1862 sensation novel "Lady Audley's Secret".
Life and works.
Born in London, Mary Elizabeth Braddon was privately educated. Her mother Fanny separated from her father Henry in 1840, when Mary was five. When Mary was ten years old, her brother Edward Braddon left for India and later Australia, where he became Premier of Tasmania. Mary worked as an actress for three years in order to support herself and her mother.
In 1860, Mary met John Maxwell (1824–1895), a publisher of periodicals. She started living with him in 1861. However, Maxwell was already married with five children, and his wife was living in an asylum in Ireland. Mary acted as stepmother to his children until 1874, when Maxwell's wife died and they were able to get married. She had six children by him, including the novelist William Babington Maxwell.
Braddon was a prolific writer, producing more than 80 novels with inventive plots. The most famous is "Lady Audley's Secret" (1862), which won her recognition, and a fortune as a bestseller. It has remained in print since its publication and been dramatised and filmed several times. R. D. Blackmore's anonymous sensation novel "Clara Vaughan" (1864) was wrongly attributed to her by some critics.
Braddon wrote several works of supernatural fiction, including the pact with the devil story "Gerald, or the World, the Flesh and the Devil" (1891), and the ghost stories "The Cold Embrace", "Eveline's Visitant" and "At Chrighton Abbey". From the 1930s onwards, these stories were often anthologised in collections such as Montague Summers's "The Supernatural Omnibus" (1931) and 
"Fifty Years of Ghost Stories" (1935). Braddon's legacy is tied to the sensation fiction of the 1860s.
Braddon also founded "Belgravia" magazine (1866), which presented readers with serialised sensation novels, poems, travel narratives and biographies, as well as essays on fashion, history and science. The magazine was accompanied by lavish illustrations and offered readers a source of literature at an affordable cost. She also edited "Temple Bar" magazine. 
She died on 4 February 1915 in Richmond, then in Surrey and now in London, and is interred in Richmond Cemetery. Her home had been Lichfield House in the centre of then town, which was replaced by a block of flats in 1936, Lichfield Court, now listed. She has a plaque in Richmond parish church which calls her simply 'Miss Braddon'. A number of streets in the area are named after characters in her novels – her husband was a property developer in the area.
There is a critical essay on Braddon's work in Michael Sadleir's book "Things Past" (1944). In 2014 the Mary Elizabeth Braddon Association was founded to pay tribute to Braddon's life and work.
Dramatisations.
Several of Braddon's works have been dramatised, including:
References.
Bibliography.
</dl>

</doc>
<doc id="42722" url="http://en.wikipedia.org/wiki?curid=42722" title="Guar">
Guar

The Guar or cluster bean, with the botanical name Cyamopsis tetragonoloba, is an annual legume and the source of guar gum. It is also known as Gavar, Guwar or Guvar bean.
The origin of "Cyamopsis tetragonoloba" is unknown, since it has never been found in the wild. It is assumed to have developed from the African species "Cyamopsis senegalesis". It was further domesticated in India and Pakistan, where it has been cultivated for many centuries. 
Guar grows well in arid to semiarid areas, but frequent rainfall is necessary. 
This legume is a very valuable plant within a crop rotation cycle, as it lives in symbiosis with nitrogen-fixing bacteria. 
In fact, agriculturists in semi-arid regions of Rajasthan follow crop-rotation and use guar as a source to replenish the soil with essential fertilizers and nitrogen fixation, before the next crop. Guar as a plant has a multitude of different functions for human and animal nutrition but its gelling-agent-containing seeds (guar gum) are today the most important use. Demand is rising rapidly due to industrial use of guar gum in hydraulic fracturing (oil shale gas). About 80% of world production occurs in India and Pakistan, but due to strong demand, the plant is being introduced into new areas.
Biology.
"Cyamopsis tetragonoloba" grows upright, reaching a maximum height of up to 2–3 m. 
It has a main single stem with either basal branching or fine branching along the stem. 
Thanks to taproots, the guar plant can access soil moisture in low soil depths. Additionally, this legume develops root nodules with nitrogen-fixing soil bacteria rhizobia in the surface part of its rooting system.
Its leaves and stems are mostly hairy, dependent on the cultivar. Its fine leaves have an elongated oval shape (5 to 10 cm length) and of alternate position. Clusters of flowers grow in the plant axil and are of white to blueish color. The developing pods are rather flat and slim containing 5 to 12 small oval seeds of 5 mm length (TGW = 25-40 g). Usually, mature seeds are white or gray, but in case of excess moisture they can turn black and lose germination capacity. The chromosome number of guar seeds is 2n=14.
The seeds of guar beans have a very remarkable characteristic. Its kernel consists of a protein-rich germ (43-46%) and a relatively large endosperm (34-40 %), containing big amounts of the galactomannan. The latter is polysaccharide containing polymers of mannose and galactose in a ratio of 2:1 with many branches. Thanks to the latter, it exhibits a great hydrogen bonding activity having a viscosifying effect in liquids.
Cultivation.
Climate Requirements.
Guar is very drought-tolerant and sun-loving, but it is very susceptible to frost. Even though it can cope with little but regular rainfall, it requires sufficient soil moisture before planting and during maturation of seeds. Frequent drought periods can lead to delayed maturation. On the contrary, too much moisture during early phase of growth and after maturation lead to lower seed quality. Guar is also produced near to coastal areas in the Gandhidham region of Kutch, Gujarat, India. 
Soil Requirements.
"Cyamopsis tetragonoloba" (L.) can grow on a wide range of different soil types. Preferably in fertile, medium-textured and sandy loam soils that are well-drained because waterlogging decreases plant performance. In respect of soil acidity, guar grows best in moderate alkaline conditions (pH 7-8) and is tolerant of salinity. Thanks to its taproots which are inoculated with rhizobia nodules, it produces nitrogen-rich biomass and improves soil quality.
Cultivation Areas.
It is grown principally in north-western India and Pakistan with smaller crops grown in the semiarid areas of the high plains of Texas in the USA, Australia and Africa. The most important growing area centres on Jodhpur in Rajasthan, India where demand for guar for fractionation produced an agricultural boom as in 2012. 
Currently, India and Pakistan are the main producers of cluster bean, accounting for 80% production of the world's total, while Thar, Punjab Dry Areas in Pakistan and Rajasthan, Gujarat, Kutch region occupies the largest area (82.1%) under guar cultivation in India. In addition to its cultivation in India and Pakistan, the crop is also grown as a cash crop in other parts of the world. Several commercial growers have converted their crops to guar production to support the increasing demand for guar and other organic crops in the United States.
Varieties.
Pusa Naubahar and Pusa Sadabahar. Seeds at the rate of 30 kilograms/hectare (9–11 lb/acre) are planted at a spacing of 45-60 x 20–30 cm (18–24 x 8–12 in) in February–March and June–July. During rainy season, the seeds are sown 2–3 cm (~1 in) deep on ridges and in furrows during summer months. FYM is applied at the rate of 25 tonnes/ha (11.1 tons/acre). N, P2O5 and K2O recommendation for the crop is 20:60:80 kg/ha (18:53:71 lb/acre). Average yield is 5 to 6 tonnes/ha (2.2–2.6 tons/acre). Meager information is available for genetic variability in clusterbean addressing the qualitative traits (Pathak et al. 2011)
Uses.
Guar plant.
Agriculture
Domestic use
Guar gum.
As mentioned in the biology section, the seeds of the guar bean contain a very large endosperm. This endosperm consists of a very large polysaccharide of galactose and mannose. This polymer is water soluble and exhibits a viscosifying effect in water. Guar gum has a multitude of different applications:
Food.
In several food and beverages guar gum is used as additive in order to change its viscosity or as fiber source.
Partially hydrolyzed guar gum (PHGG) is produced by the partial enzymatic hydrolysis of guaran, the galactomannan of the endosperm of guar seeds (guar gum). It is a neutral polysaccharide consisting of a mannose backbone chain with single galactose side units occurring on almost two out of every three mannose units. The average molecular weight is about 25,000 Daltons. This gives a PHGG that still assays and functions as a soluble dietary fiber. 
PHGG as sold commercially is completely soluble, acid and heat stable, unaffected by ions, and will not gel at high concentrations. Commercial PHGG is approximately 75% dietary fiber and has minimal effect on taste and texture in food and beverage items. PHGG is fully fermentable in the large bowel, with a high rate of volatile fatty acid formation. The pH of the feces is lowered along with an increase in fecal bulk that mainly consists of bacterial cell mass and water. Clinical studies have demonstrated a prebiotic effect of PHGG. Studies have also shown that PHGG can be used to maintain regularity. PHGG is used in foods for particulate suspension, emulsification, antistaling, ice crystal control, and reduced fat baked goods.
Industry.
Derivatives of guar gum that has been further reacted is also used in industrial applications, such as the paper and textile industry, ore flotation, the manufacture of explosives and hydraulic fracturing (fracking) of oil and gas formations. Guar gum is often crosslinked with boron or chromium ions to make it more stable and heat-resistant. The crosslinking of guar with metal ions results in a gel that does not block the formation and helps efficiently in formation cleaning process. 
Guar and its derivatives make gel complexes with ions of Aluminium, Zirconium,Titanium, Chromium and Boron. 
The borate–guar reaction is reversible, and depends on the pH (hydrogen ion concentration) of the solution. Crosslinking of guar with borate occurs at high pH (approximately 9–10) of the solution. Guar gum has also proven a useful substitute for locust bean gum (made from carob seeds).
Feeds.
Guar meal korma and Guar meal Churi are widely used as prime raw material for Producing various kinds of Cattle feeds,Aqua feeds, Fish feeds,Poultry Feeds,Dairy feeds,Swined feeds etc.
Fracking agent.
Through the use of guar gum in the hydraulic fracturing (fracking) extraction of oil and shale gas, the demand has increased substantially. Only 10% of the Indian production stays within the country and the remaining 90% is exported for shale gas and oil industries. Consequently, many former cotton or wheat fields are converted into guar fields as production costs are also lower. But the increase of guar gum prices has also other reasons. But since prices are lower the farmers stop harvesting the Guar and returned to cotton & cumin and sesame crops sowing.

</doc>
<doc id="42725" url="http://en.wikipedia.org/wiki?curid=42725" title="Lady Audley's Secret">
Lady Audley's Secret

Lady Audley's Secret is a sensation novel by Mary Elizabeth Braddon published in 1862. It was Braddon's most successful and well-known novel. Critic John Sutherland (1989) described the work as "the most sensationally successful of all the sensation novels". The plot centres on "accidental bigamy" which was in literary fashion in the early 1860s. The plot was summarised by literary critic Elaine Showalter (1982): "Braddon's bigamous heroine deserts her child, pushes husband number one down a well, thinks about poisoning husband number two and sets fire to a hotel in which her other male acquaintances are residing". Elements of the novel mirror themes of the real-life Constance Kent case of June 1860 which gripped the nation for years. A follow-up novel, "Aurora Floyd", appeared in 1863. Braddon set the story in Ingatestone Hall, Essex, inspired by a visit there. There have been three silent film adaptations, one UK television version in 2000, and three minor stage adaptations.
History.
"Lady Audley's Secret" was partly serialised in "Robin Goodfellow" magazine July–September 1861, then entirely serialised in "Sixpenny Magazine" January–December 1862 and once again serialised in "London Journal" March–August 1863. It was published in 1862 in three volumes by William Tinsley.
Braddon initially sold the rights to the Irish publisher John Maxwell, with whom Braddon also lived and had children. Maxwell published it in his ailing magazine "Robin Goodfellow", but Braddon did not labour much, writing the final third in less than two weeks. Not until it was published as a three-volume novel by William Tinsley did it become a success and allow Braddon to be financially independent for the remainder of her life. It also enriched her publisher William Tinsley, who went on to build a villa at Barnes, 'Audley Lodge', with the profits.
Notably for the bigamous nature of the plot, Maxwell himself was married to another woman and thus Braddon was unable to marry him until his wife died in 1874. When it became public that Maxwell and Braddon had been living in an "irregular" arrangement all those years, it caused a minor scandal during which all their servants gave notice.
Plot.
The novel opens with the marriage in June 1857 of Lucy Graham, a beautiful, childlike blonde who enchants almost all who meet her, to Sir Michael Audley, a middle-aged, rich, and kind widower. Lucy was a governess for the local doctor, Mr. Dawson until her marriage. Previous to that Lucy was in service with Mrs. Vincent, but very little is known about her past before this. Around the time of the marriage, Sir Michael's nephew, the barrister Robert Audley, welcomes his old friend George Talboys back to England, after three years of gold prospecting in Australia.
George is anxious to get news of his wife, Helen, whom he left three years ago when their financial situation became desperate, to seek gold in Australia. He reads in the newspaper that she has died, and, after visiting her home to confirm this, he becomes despondent. Robert Audley cares for his friend, and, hoping to distract him, offers to take him to his wealthy uncle's country manor. George had a child, Georgey, who was left under the care of Lieutenant Maldon, George's father-in-law. Robert and George set off to visit Georgey, and George decides to make Robert little Georgey's guardian and caretaker of 20,000 pounds put into the boy's name. After settling the matter of the boy's guardianship, the two set off to visit Sir Michael.
While at the country manor Audley Court, Lady Audley avoids meeting with George. When the two seek an audience with the new Lady Audley, she makes many excuses to avoid their visit, but he and Robert are shown a portrait of her by Alicia Audley, Robert's cousin. George appears greatly struck by the portrait, unbeknownst to Robert (who credits the unfavourable reaction to that evening's storm). Shortly thereafter, George disappears upon a visit to Audley Court, much to Robert's consternation. Unwilling to believe that George has simply left suddenly and without notice, Robert begins to look into the circumstances around the strange disappearance.
While searching for his friend, Robert begins to take notes of the events as they unfold. His notes indicate the involvement of Lady Audley, much to his chagrin, and he slowly begins to collect evidence against her. One night, he reveals the evidence and notes that George was in possession of many letters that his former wife wrote. Lady Audley immediately sets off to London, where the letters were kept, and Robert follows after her. However, by the time he arrives, he discovers that George's possessions have been broken into with the help of a local locksmith and that the letters have vanished. One possession, however, remains – a book with a note written by George's wife that matches Lady Audley's handwriting. This confirms Robert's suspicion that Lady Audley is implicated in George's disappearance; it also leads Robert to conclude that Lady Audley is actually George's supposedly dead wife.
Suspecting the worst of Lady Audley and being afraid for little Georgey's life, Robert travels to Lieutenant Maldon's house and demands possession of the boy. Once Robert has Georgey under his control, he places the boy in a school run by Mr. Marchmont. Afterwards, Robert visits George's father, Mr. Harcourt Talboys, and confronts the squire with his son's death. Mr. Harcourt listens dispassionately to the story. In the course of his visit to the Talboys' manor, Robert is entranced by George's sister Clara, who looks startlingly like George. Clara's passion for finding her brother spurs Robert on.
In February 1859, Robert continues searching for evidence. He receives a notice that his uncle is ill, and he quickly returns to Audley Court. While there, Robert speaks with Mr. Dawson and receives a brief description of all that is known about Lucy's background. He hears that Lucy was employed by Mrs. Vincent at her school since 1852, and, to verify this claim, Robert tracks down Mrs. Vincent, who is in hiding because of debts. According to Miss Tonks, a teacher at Mrs. Vincent's school, Lucy actually arrived at the school in August 1854 and was secretive about her past. Miss Tonks gives Robert a travel box that used to belong to Lucy, and upon examining stickers on the box, Robert discovers both the name Lucy Graham and the name Helen Talboys.
Robert realises that Helen Talboys faked her death before creating her new identity. When Robert confronts Lucy, she tells him that he has no proof, and he leaves to find more evidence, heading to Castle Inn, which is run by Luke Marks. During the night, Lucy forces Luke's wife Phoebe to let her into the inn and Lucy sets the place on fire, with the intention of killing Robert. However, Robert survives and returns to Audley Court and again confronts Lucy. This time, she says she is insane and confesses her life's story to Robert and Sir Michael, claiming that George abandoned her originally and she had no choice but to abandon her old life and child in order to find another, wealthier husband.
Sir Michael is unhappy and leaves with Alicia to travel through Europe. Robert invites a Dr. Mosgrave to make a more astute judgment regarding Lucy's sanity, and he proclaims that she is indeed victim to latent insanity, which overpowers her in times of stress and makes her very dangerous to any and all. Lucy, under the name of Madame Taylor, enters a mental institution located somewhere in Belgium along the route between Brussels and Paris. While being committed, Lucy confesses to Robert that she killed George by pushing him down a deserted well in the garden of Audley Court.
Robert grieves for his friend George until Luke Marks, who was fatally injured in the fire, manages, before dying, to tell Robert that George survived Lady Audley's attempted murder and that George, with Luke's help, left with intent of returning to Australia. Robert is overjoyed, and he asks Clara to marry him and go with him to Australia to find George. Clara accepts, but before they set out, George returns and reveals that he actually visited New York instead. The narrative ends with the death of Lucy abroad, and Clara and Robert happily married and living in a country cottage with George and his son. Robert's formerly infatuated cousin Alicia marries her once-spurned suitor, Sir Harry Towers, and Audley Court is left abandoned along with all of its unhappy memories.
Analysis and themes.
"Lady Audley's Secret" plays on Victorian anxieties about the domestic sphere. The home was supposed to be a refuge from the dangers of the outside world, but in the novel, the seemingly perfect domestic lady turns out to be a violent criminal who has not only tried to commit murder but who has also committed bigamy and abandoned her child. This unsettled Victorian readers because it indicated that the concepts of "the perfect lady/mother" and "domestic bliss" were more idealistic than realistic. In addition, anxieties about the increasing urbanisation of Britain abound; the city gives Lady Audley the power to change her identity because it renders its citizens effectively anonymous. The small town of Audley is no longer a refuge where everyone knows the life story of every neighbour; the residents of Audley must accept Lucy Graham's account of herself since they have no other information about her past. Other anxieties about unstable identity appear throughout the novel: Robert's relationship with George has homosexual overtones, especially considered in light of his attraction to Clara, George's sister, who is described as looking identical to George. Additionally, Lady Audley's maid, Phoebe, resembles Lady Audley, thus banishing the idea of physical distinction between the upper and lower classes and therefore of any inherent superiority of the former.
"Lady Audley's Secret" is, furthermore, a story about gender and class, and Lady Audley's objectionable upward mobility suggests a threat to the paradigm of social class. Madness is also a key issue. Lady Audley and others often converse about the meaning of this word, but many readers believe that Lady Audley is not mad. In fact, many critics view Lady Audley's deception as a feminist act in which a woman takes control of the direction of her own life.
The novel mirrors many of the same themes from the real-life Constance Kent case of June 1860 that gripped the nation with headline news for years. The first instalment of "Lady Audley's Secret" came out almost exactly one year after the Kent murder. The novel, like the real-life case, featured a wicked stepmother (and former governess who married a gentleman), a mysterious and brutal murder in a country manor house, a body thrown down a well, and characters fascinated by madness. Constance Kent can be seen in many of the female characters in the novel: the murderess Lady Audley, the tomboyish Alicia Audley, the restrained Phoebe Marks and the lonely Clara Talboys. Jack Whicher, the detective and case investigator, can be seen in the character of Robert Audley.
In popular culture.
"Lady Audley's Secret" is involved in a subplot of "Betsy and Tacy Go Downtown", the fourth book in the Betsy-Tacy series by Maud Hart Lovelace. Betsy has read it and other books in the same genre, and aspires to write similar works.

</doc>
<doc id="42726" url="http://en.wikipedia.org/wiki?curid=42726" title="Cephalopod">
Cephalopod

A cephalopod (pronounced ) is any member of the molluscan class Cephalopoda (Greek plural κεφαλόποδα ("kephalópoda"); "head-feet"). These exclusively marine animals are characterized by bilateral body symmetry, a prominent head, and a set of arms or tentacles (muscular hydrostats) modified from the primitive molluscan foot. Fishermen sometimes call them inkfish, referring to their common ability to squirt ink. The study of cephalopods is a branch of malacology known as teuthology.
Cephalopods became dominant during the Ordovician period, represented by primitive nautiloids. The class now contains two, only distantly related, extant subclasses: Coleoidea, which includes octopuses, squid, and cuttlefish; and Nautiloidea, represented by "Nautilus" and "Allonautilus". In the Coleoidea, the molluscan shell has been internalized or is absent, whereas in the Nautiloidea, the external shell remains. About 800 living species of cephalopods have been identified. Two important extinct taxa are the Ammonoidea (ammonites) and Belemnoidea (belemnites).
Distribution.
There are around 800 extant species of cephalopod, although new species continue to be described. An estimated 11,000 extinct taxa have been described, although the soft-bodied nature of cephalopods means they are not easily fossilised.
Cephalopods are found in all the oceans of Earth. None of them can tolerate freshwater, but the brief squid, "Lolliguncula brevis", found in Chesapeake Bay, may be a notable exception in that it tolerates brackish water.
Cephalopods occupy most of the depth of the ocean, from the abyssal plain to the sea surface. Their diversity is greatest near the equator (~40 species retrieved in nets at 11°N by a diversity study) and decreases towards the poles (~5 species captured at 60°N).
Nervous system and behavior.
Cephalopods are widely regarded as the most intelligent of the invertebrates, and have well developed senses and large brains (larger than those of gastropods). The nervous system of cephalopods is the most complex of the invertebrates and their brain-to-body-mass ratio falls between that of endothermic and ectothermic vertebrates.:14 The brain is protected in a cartilaginous cranium. The giant nerve fibers of the cephalopod mantle have been widely used for many years as experimental material in neurophysiology; their large diameter (due to lack of myelination) makes them relatively easy to study compared with other animals.
Cephalopods are social creatures; when isolated from their own kind, they will sometimes shoal with fish.
Some cephalopods are able to fly through the air for distances of up to 50 m. While cephalopods are not particularly aerodynamic, they achieve these impressive ranges by jet-propulsion; water continues to be expelled from the funnel while the organism is in the air. The animals spread their fins and tentacles to form wings and actively control lift force with body posture. One species has been observed spreading tentacles in a flat fan shape with a mucus film between the individual tentacles while another has been observed putting the tentacles in a circular arrangement.
Senses.
Cephalopods have advanced vision, can detect gravity with statocysts, and have a variety of chemical sense organs.:34 Octopuses use their arms to explore their environment and can use them for depth perception.
Vision.
Most cephalopods rely on vision to detect predators and prey, and to communicate with one another. Consequently, cephalopod vision is acute: training experiments have shown that the common octopus can distinguish the brightness, size, shape, and horizontal or vertical orientation of objects. The morphological construction gives cephalopod eyes the same performance as sharks'; however, their construction differs, as cephalopods lack a cornea, and have an everted retina. Cephalopods' eyes are also sensitive to the plane of polarization of light. Surprisingly—given their ability to change color—all octopuses and most cephalopods are color blind. When camouflaging themselves, they use their chromatophores to change brightness and pattern according to the background they see, but their ability to match the specific color of a background may come from cells such as iridophores and leucophores that reflect light from the environment. They also produce visual pigments throughout their body, and may sense light levels directly from their body. Evidence of color vision has been found in the sparkling enope squid ("Watasenia scintillans"), which achieves color vision by the use of three distinct retinal molecules (A1, sensitive to red; A2, to purple, and A4, to yellow?) which bind to its opsin.
Unlike many other cephalopods, nautiluses do not have good vision; their eye structure is highly developed, but lacks a solid lens. They have a simple "pinhole" eye through which water can pass. Instead of vision, the animal is thought to use olfaction as the primary sense for foraging, as well as locating or identifying potential mates.
Hearing.
Some squids have been shown to detect sound using their statocysts.
Use of light.
Most cephalopods possess chromatophores - colored pigment cells that expand and contract in accordance with their counterparts to produce color and pattern - which they can use in a startling array of fashions. As well as providing camouflage with their background, some cephalopods bioluminesce, shining light downwards to disguise their shadows from any predators that may lurk below. The bioluminescence is produced by bacterial symbionts; the host cephalopod is able to detect the light produced by these organisms. Bioluminescence may also be used to entice prey, and some species use colorful displays to impress mates, startle predators, or even communicate with one another. It is not certain whether bioluminescence is actually of epithelial origin or if it is a bacterial production.
Coloration.
Cephalopods can change their colors and patterns in milliseconds, whether for signalling (both within the species and for warning) or active camouflage, as their chromatophores are expanded or contracted. Coloration is typically stronger in near-shore species than those living in the open ocean, whose functions tend to be restricted to disruptive camouflage.:2
Evidence of original coloration has been detected in cephalopod fossils dating as far back as the Silurian; these orthoconic individuals bore concentric stripes, which are thought to have served as camouflage. Devonian cephalopods bear more complex color patterns, of unknown function.
Ink.
With the exception of the Nautilidae and the species of octopus belonging to the suborder Cirrina, all known cephalopods have an ink sac, which can be used to expel a cloud of dark ink to confuse predators. This sac is a muscular bag which originated as an extension of the hind gut. It lies beneath the gut and opens into the anus, into which its contents – almost pure melanin – can be squirted; its proximity to the base of the funnel means the ink can be distributed by ejected water as the cephalopod uses its jet propulsion. The ejected cloud of melanin is usually mixed, upon expulsion, with mucus, produced elsewhere in the mantle, and therefore forms a thick cloud, resulting in visual (and possibly chemosensory) impairment of the predator, like a smokescreen. However, a more sophisticated behaviour has been observed, in which the cephalopod releases a cloud, with a greater mucus content, that approximately resembles the cephalopod that released it (this decoy is referred to as a Pseudomorph). This strategy often results in the predator attacking the pseudomorph, rather than its rapidly departing prey. For more information, see Inking behaviors.
The inking behaviour of cephalopods has led to a common name of "inkfish", primarily used in fisheries science and the fishing industry, paralleling the terms white fish, oily fish, and shellfish.
Circulatory system.
Cephalopods are the only mollusks with a closed circulatory system. Coleoids have two gill hearts (also known as branchial hearts) that move blood through the capillaries of the gills. A single systemic heart then pumps the oxygenated blood through the rest of the body.
Like most molluscs, cephalopods use hemocyanin, a copper-containing protein, rather than hemoglobin, to transport oxygen. As a result, their blood is colorless when deoxygenated and turns blue when exposed to air.
Respiration.
Cephalopods exchange gases with the seawater by forcing water through their gills, which are attached to the roof of the organism.:488 Water enters the mantle cavity on the outside of the gills, and the entrance of the mantle cavity closes. When the mantle contracts, water is forced through the gills, which lie between the mantle cavity and the funnel. The water's expulsion through the funnel can be used to power jet propulsion. The gills, which are much more efficient than those of other molluscs, are attached to the ventral surface of the mantle cavity.
There is a trade-off with gill size regarding lifestyle. To achieve fast speeds, gills need to be small—water will be passed through them quickly when energy is needed, compensating for their small size. However, organisms which spend most of their time moving slowly along the bottom do not naturally pass much water through their cavity for locomotion; thus they have larger gills, along with complex systems to ensure that water is constantly washing through their gills, even when the organism is stationary. The water flow is controlled by contractions of the radial and circular mantle cavity muscles.
The gills of cephalopods are supported by a skeleton of robust fibrous proteins; the lack of mucopolysaccharides distinguishes this matrix from cartilage. The gills are also thought to be involved in excretion, with NH4+ being swapped with K+ from the seawater.
Locomotion and buoyancy.
While most cephalopods can move by jet propulsion, this is a very energy-consuming way to travel compared to the tail propulsion used by fish. The efficiency of a propellor-driven waterjet (i.e. Froude efficiency) is a more efficient model than rocket efficiency. The relative efficiency of jet propulsion decreases further as animal size increases; paralarvae are far more efficient than juvenile and adult individuals. Since the Paleozoic era, as competition with fish produced an environment where efficient motion was crucial to survival, jet propulsion has taken a back role, with fins and tentacles used to maintain a steady velocity.
Whilst jet propulsion is never the sole mode of locomotion,:208 the stop-start motion provided by the jets continues to be useful for providing bursts of high speed - not least when capturing prey or avoiding predators. Indeed, it makes cephalopods the fastest marine invertebrates,:Preface
and they can out-accelerate most fish.
The jet is supplemented with fin motion; in the squid, the fins flap each time that a jet is released, amplifying the thrust; they are then extended between jets (presumably to avoid sinking).
Oxygenated water is taken into the mantle cavity to the gills and through muscular contraction of this cavity, the spent water is expelled through the hyponome, created by a fold in the mantle. The size difference between the posterior and anterior ends of this organ control the speed of the jet the organism can produce. The velocity of the organism can be accurately predicted for a given mass and morphology of animal. Motion of the cephalopods is usually backward as water is forced out anteriorly through the hyponome, but direction can be controlled somewhat by pointing it in different directions. Some cephalopods accompany this expulsion of water with a gunshot-like popping noise, thought to function to frighten away potential predators.
Cephalopods employ a similar method of propulsion despite their increasing size (as they grow) changing the dynamics of the water in which they find themselves. Thus their paralarvae do not extensively use their fins (which are less efficient at low Reynolds numbers) and primarily use their jets to propel themselves upwards, whereas large adult cephalopods tend to swim less efficiently and with more reliance on their fins.
Early cephalopods are thought to have produced jets by drawing their body into their shells, as "Nautilus" does today. "Nautilus" is also capable of creating a jet by undulations of its funnel; this slower flow of water is more suited to the extraction of oxygen from the water. The jet velocity in "Nautilus" is much slower than in coleoids, but less musculature and energy is involved in its production. Jet thrust in cephalopods is controlled primarily by the maximum diameter of the funnel orifice (or, perhaps, the average diameter of the funnel):440 and the diameter of the mantle cavity. Changes in the size of the orifice are used most at intermediate velocities. The absolute velocity achieved is limited by the cephalopod's requirement to inhale water for expulsion; this intake limits the maximum velocity to eight body-lengths per second, a speed which most cephalopods can attain after two funnel-blows. Water refills the cavity by entering not only through the orifices, but also though the funnel. Squid can expel up to 94% of the fluid within their cavity in a single jet thrust. To accommodate the rapid changes in water intake and expulsion, the orifices are highly flexible and can change their size by a factor of twenty; the funnel radius, conversely, changes only by a factor of around 1.5.
Some octopus species are also able to walk along the sea bed. Squids and cuttlefish can move short distances in any direction by rippling of a flap of muscle around the mantle.
While most cephalopods float (i.e. are neutrally buoyant or nearly so; in fact most cephalopods are about 2-3% denser than seawater), they achieve this in different ways.
Some, such as "Nautilus", allow gas to diffuse into the gap between the mantle and the shell; others allow purer water to ooze from their kidneys, forcing out denser salt water from the body cavity; others, like some fish, accumulate oils in the liver; and some octopuses have a gelatinous body with lighter chlorine ions replacing sulfate in the body chemistry.
Shell.
Nautiluses are the only extant cephalopods with a true external shell. However, all molluscan shells are formed from the ectoderm (outer layer of the embryo); in cuttlefish ("Sepia" spp.), for example, an invagination of the ectoderm forms during the embryonic period, resulting in a shell (cuttlebone) that is internal in the adult. The same is true of the chitinous gladius of squid and octopuses. Cirrate octopods have arch-shaped cartilaginous fin supports, which are sometimes referred to as a "shell vestige" or "gladius". The Incirrina have either a pair of rod-shaped stylets or no vestige of an internal shell, and some squid also lack a gladius. Interestingly, the shelled coleoids do not form a clade or even a paraphyletic group. The "Spirula" shell begins as an organic structure, and is then very rapidly mineralized. Shells that are "lost" may be lost by resorption of the calcium carbonate component.
Females of the octopus genus "Argonauta" secrete a specialised paper-thin eggcase in which they reside, and this is popularly regarded as a "shell", although it is not attached to the body of the animal.
The largest group of shelled cephalopods, the ammonites, are extinct, but their shells are very common as fossils.
The deposition of carbonate, leading to a mineralized shell, appears to be related to the acidity of the organic shell matrix (see Mollusc shell); shell-forming cephalopods have an acidic matrix, whereas the gladius of squid has a basic matrix.
Head appendages.
Cephalopods, as the name implies, have muscular appendages extending from their heads and surrounding their mouths. These are used in feeding, mobility, and even reproduction. In coleoids they number eight or ten. Decapods such as cuttlefish and squid have five pairs. The longer two, termed "tentacles", are actively involved in capturing prey;:225 they can lengthen rapidly (in as little as 15 milliseconds:225). In giant squid they may reach a length of 8 metres. They may terminate in a broadened, sucker-coated club.:225 The shorter four pairs are termed "arms", and are involved in holding and manipulating the captured organism.:225 They too have suckers, on the side closest to the mouth; these help to hold onto the prey.:226 Octopods only have four pairs of sucker-coated arms, as the name suggests, though developmental abnormalities can modify the number of arms expressed.
The tentacle consists of a thick central nerve cord (which must be thick to allow each sucker to be controlled independently) surrounded by circular and radial muscles. Because the volume of the tentacle remains constant, contracting the circular muscles decreases the radius and permits the rapid increase in length. Typically a 70% lengthening is achieved by decreasing the width by 23%.:227 The shorter arms lack this capability.
The size of the tentacle is related to the size of the buccal cavity; larger, stronger tentacles can hold prey as small bites are taken from it; with more numerous, smaller tentacles, prey is swallowed whole, so the mouth cavity must be larger.
Externally shelled nautilids ("Nautilus" and "Allonautilus") have on the order of 90 finger-like appendages, termed "tentacles", which lack suckers but are sticky instead, and are partly retractable.
Feeding.
All living cephalopods have a two-part beak;:7 most have a radula, although it is reduced in most octopus and absent altogether in "Spirula".:7:110 They feed by capturing prey with their tentacles, drawing it into their mouth and taking bites from it. They have a mixture of toxic digestive juices, some of which are manufactured by symbiotic algae, which they eject from their salivary glands onto their captured prey held in their mouth. These juices separate the flesh of their prey from the bone or shell. The salivary gland has a small tooth at its end which can be poked into an organism to digest it from within.
The digestive gland itself is rather short. It has four elements, with food passing through the crop, stomach and caecum before entering the intestine. Most digestion, as well as the absorption of nutrients, occurs in the digestive gland, sometimes called the liver. Nutrients and waste materials are exchanged between the gut and the digestive gland through a pair of connections linking the gland to the junction of the stomach and caecum. Cells in the digestive gland directly release pigmented excretory chemicals into the lumen of the gut, which are then bound with mucus passed through the anus as long dark strings, ejected with the aid of exhaled water from the funnel. Cephalopods tend to concentrate ingested heavy metals in their body tissue.
Radula.
The cephalopod radula consists of multiple symmetrical rows of up to nine teeth – thirteen in fossil classes. The organ is reduced or even vestigial in certain octopus species and is absent in "Spirula". The teeth may be homodont (i.e. similar in form across a row), heterodont (otherwise), or ctenodont (comb-like). Their height, width and number of cusps is variable between species. The pattern of teeth repeats, but each row may not be identical to the last; in the octopus, for instance, the sequence repeats every five rows.:79
Cephalopod radulae are known from fossil deposits dating back to the Ordovician. They are usually preserved within the cephalopod's body chamber, commonly in conjunction with the mandibles; but this need not always be the case; many radulae are preserved in a range of settings in the Mason Creek.
Radulae are usually difficult to detect, even when they are preserved in fossils, as the rock must weather and crack in exactly the right fashion to expose them; for instance, radulae have only been found in nine of the 43 ammonite genera, and they are rarer still in non-ammonoid forms: only three pre-Mesozoic species possess one.
Excretory system.
Most cephalopods possess a single pair of large nephridia. Filtered nitrogenous waste is produced in the pericardial cavity of the branchial hearts, each of which is connected to a nephridium by a narrow canal. The canal delivers the excreta to a bladder-like renal sac, and also resorbs excess water from the filtrate. Several outgrowths of the lateral vena cava project into the renal sac, continuously inflating and deflating as the branchial hearts beat. This action helps to pump the secreted waste into the sacs, to be released into the mantle cavity through a pore.
"Nautilus", unusually, possesses four nephridia, none of which are connected to the pericardial cavities.
The incorporation of ammonia is important for shell formation in terrestrial molluscs and other non-molluscan lineages. Because protein (i.e. flesh) is a major constituent of the cephalopod diet, large amounts of ammonium are produced as waste. The main organs involved with the release of this excess ammonium are the gills. The rate of release is lowest in the shelled cephalopods "Nautilus" and "Sepia" as a result of their using nitrogen to fill their shells with gas to increase buoyancy. Other cephalopods use ammonium in a similar way, storing the ions (as ammonium chloride) to reduce their overall density and increase buoyancy.
Reproduction and life cycle.
Cephalopods are a diverse group of species, but share common life history traits, for example they have a rapid growth rate and short life spans. Stearns (1992) suggested that in order to produce the largest possible number of viable offspring, spawning events depend on ecological environmental factors of the organism. The majority of cephalopods do not provide parental care to their offspring, except for example, octopus, which helps this organism increase the survival rate of their offspring.
Marine species life cycles are affected by various environmental conditions. The development of a cephalopod embryo can be greatly affected by temperature, oxygen saturation, pollution, light intensity, and salinity. These factors are important to the rate of embryonic development and the success of hatching of the embryos. Food availability also plays an important role in the reproductive cycle of cephalopods. A limitation of food influences the timing of spawning along with their function and growth. Spawning time and spawning vary among marine species; it’s correlated with temperature, though cephalopods in shallow water spawn in cold months so that the offspring would hatch at warmer temperatures. Breeding can last from several days to a month.
Sexual maturity.
 Cephalopods that are sexually mature and of adult size, begin spawning and reproducing. After the transfer of genetic material to the following generation, the adult cephalopods then die.
 Sexual maturation in male and female cephalopods can be observed internally by the enlargement of gonads and accessory glands. Mating would be a poor indicator of sexual maturation in females; they can receive sperm when not fully reproductively mature and store them until they are ready to fertilize the eggs. Most cephalopod males develop a hectocotylus, an arm tip which is capable of transferring their spermatozoa into the female mantel cavity. Though not all species use a hectocotylus; for example Nautilus releases a spadix. An indication of sexual maturity of females is the development of brachial photophores to attract mates.
Fertilization.
 Cephalopods are not broadcast spawners. During the process of fertilization, the females use sperm provided by the male via external fertilization. Internal fertilization is seen only in octopods. 
The initiation of copulation begins when the male catches a female and wrapping his arm around her; either in a ‘male to female neck’ position or mouth to mouth position, depending on the species. The males then initiate the process of fertilization by contracting their mantle several times to release the spermatozoa. Cephalopods often mate several times, which influences males to mate longer with females that have previously, nearly tripling the amount of contractions of the mantle. To insure the fertilization the eggs, female cephalopods release a sperm-attracting peptide through the gelatinous layers of the egg to direct the spermatozoa. 
Female cephalopods lay eggs in clutches; each egg is composed of a protective coat insure the safety of the developing embryo when released into the water column. Reproductive strategies differ between cephalopod species. In giant pacific octopus, large eggs are laid on den, it will often take several days to lay all of them. Once the eggs are released and attached to a sheltered substrate the females then die. In some species of cephalopods, egg clutches are anchored to substrates by a mucilaginous adhesive substance. These eggs are swelled with preivitelline fluid (PVF), a hypertonic fluid that prevents premature hatching. Fertilized egg clusters are neutrally buoyant depending at the depth that they were laid but can also be found in substrates such as sand, matrix of corals, seaweed. Because these species do not provide parental care for their offspring, egg capsules can be injected with ink by the female in order to camouflage the embryos from predators.
Male-male competition.
Aggressive spawning is an activity that most cephalopods engage in; a protein in the male capsule sheath stimulates this behavior. Male- male aggression is seen in cephalopods; smaller males tend to lose these interactions. When a female is near the males charge on another continuously and begin to flair their arms. If neither male backed away, the arms extend to the back exposing the mouth which leads to the biting of arm tips. 
During mate competition males also participate in a technique called flushing. This technique is used by the second male attempting to mate with a female. Flushing removes spermatophores in the buccal cavity that was placed there by the first mate by forcing water into the cavity. Another behavior that males engage in is sneaker mating or mimicry- smaller males adjust their behavior to that of a female in order to reduce aggression. By using this technique, they are able to fertilize the eggs while the larger male is distracted by different male. During this process, the sneaker males quickly insert drop like sperm into the seminal receptacle (Iwata et al., 2008).
Mate choice.
Mate choice is seen in cuttlefish species, where females prefer some males over the other though characteristics of the preferred males are unknown. A hypothesis states that females reject males by olfactory cues rather than visual cues. Several cephalopod species are polyandrous- accepting and storing multiple male spermatophores, which has been identified by DNA fingerprinting. Females are no longer receptive to mating attempts when holding their eggs in their arm. 
Females can store sperm in two places (1) the buccal cavity where recently mated males place their spermatophores, and (2) the internal sperm-storage receptacles where sperm packages from previous males are stored.
Spermatophore storage results in sperm competition; which states that the female controls which mate fertilizes the eggs. In order to reduce this sort of competition, males develop agonistic behaviors like mate guarding and flushing. Flushing is used by both the male and female; it is the process of removing spermatophores of other males by continuously pumping strong jets of water into the buccal cavity of the female. This behavior however, reduces the available time to mate with other females.
Sexual dimorphism.
 In a variety of marine organisms it is seen that females are larger in size compared to the males in some close related species. In some lineages, such as the blanket octopus, males become structurally smaller and smaller resembling a term, "dwarfism" dwarf males usually occurs at low densities. The blanket octopus male is an example of sexual-evolutionary dwarfism; females grow 10,000 to 40,000 times larger than the males and the sex ratio between males and females can be distinguished right after hatching of the eggs.
Embryology.
Cephalopod eggs span a large range of sizes, from 1 to 30 mm in diameter.[78] The fertilised ovum initially divides to produce a disc of germinal cells at one pole, with the yolk remaining at the opposite pole. The germinal disc grows to envelop and eventually absorb the yolk, forming the embryo. The tentacles and arms first appear at the hind part of the body, where the foot would be in other molluscs, and only later migrate towards the head.
The funnel of cephalopods develops on the top of their head, whereas the mouth develops on the opposite surface.:86 The early embryological stages are reminiscent of ancestral gastropods and extant Monoplacophora.
The shells develop from the ectoderm as an organic framework which is subsequently mineralised. In "Sepia", which has an internal shell, the ectoderm forms an invagination whose pore is sealed off before this organic framework is deposited.
Development.
The length of time before hatching is highly variable; smaller eggs in warmer waters are the fastest to hatch, and newborns can emerge after as little as a few days. Larger eggs in colder waters can develop for over a year before hatching.
The process from spawning to hatching follows a similar trajectory in all species, the main variable being the amount of yolk available to the young and when it is absorbed by the embryo.
Unlike most other molluscs, cephalopods do not have a morphologically distinct larval stage. Instead the juveniles are known as paralarvae. They quickly learn how to hunt, using encounters with prey to refine their strategies.
Growth in juveniles is usually allometric, whilst adult growth is isometric.
Evolution.
The traditional view of cephalopod evolution holds that they evolved in the Late Cambrian from a monoplacophoran-like ancestor with a curved, tapering shell, which was closely related to the gastropods (snails). The similarity of the early shelled cephalopod "Plectronoceras" to some gastropods was used in support of this view. The development of a siphuncle would have allowed the shells of these early forms to become gas-filled (thus buoyant) in order to support them and keep the shells upright while the animal crawled along the floor, and separated the true cephalopods from putative ancestors such as "Knightoconus", which lacked a siphuncle. Neutral or positive buoyancy (i.e. the ability to float) would have come later, followed by swimming in the Plectronocerida and eventually jet propulsion in more derived cephalopods.
However, some morphological evidence is difficult to reconcile with this view, and the redescription of "Nectocaris pteryx", which did not have a shell and appeared to possess jet propulsion in the manner of "derived" cephalopods, complicated the question of the order in which cephalopod features developed – provided "Nectocaris" is a cephalopod at all. Their position within the Mollusca is currently wide open to interpretation - see Mollusca#Phylogeny.
Early cephalopods were likely predators near the top of the food chain. They underwent pulses of diversification during the Ordovician period to become diverse and dominant in the Paleozoic and Mesozoic seas.
In the Early Palaeozoic, their range was far more restricted than today; they were mainly constrained to sublittoral regions of shallow shelves of the low latitudes, and usually occur in association with thrombolites. A more pelagic habit was gradually adopted as the Ordovician progressed. Deep-water cephalopods, whilst rare, have been found in the Lower Ordovician - but only in high-latitude waters.
The mid Ordovician saw the first cephalopods with septa strong enough to cope with the pressures associated with deeper water, and could inhabit depths greater than 100–200 m. The direction of shell coiling would prove to be crucial to the future success of the lineages; endogastric coiling would only permit large size to be attained with a straight shell, whereas exogastric coiling - initially rather rare - permitted the spirals familiar from the fossil record to develop, with their corresponding large size and diversity. (Endogastric mean the shell is curved so as the ventral or lower side is longitudinally concave (belly in); exogastric means the shell is curved so as the ventral side is longitudinally convex (belly out) allowing the funnel to be pointed backwards beneath the shell.)
The ancestors of coleoids (including most modern cephalopods) and the ancestors of the modern nautilus, had diverged by the Floian Age of the Early Ordovician Period, over 470 million years ago. The Bactritida, a Silurian–Triassic group of orthocones, are widely held to be paraphyletic to the coleoids and ammonoids – that is, the latter groups arose from within the Bactritida.:393 An increase in the diversity of the coleoids and ammonoids is observed around the start of the Devonian period, and corresponds with a profound increase in fish diversity. This could represent the origin of the two derived groups.
Unlike most modern cephalopods, most ancient varieties had protective shells. These shells at first were conical but later developed into curved nautiloid shapes seen in modern nautilus species.
Competitive pressure from fish is thought to have forced the shelled forms into deeper water, which provided an evolutionary pressure towards shell loss and gave rise to the modern coleoids, a change which led to greater metabolic costs associated with the loss of buoyancy, but which allowed them to recolonise shallow waters.:36 However, some of the straight-shelled nautiloids evolved into belemnites, out of which some evolved into squid and cuttlefish. The loss of the shell may also have resulted from evolutionary pressure to increase manoeuvrability, resulting in a more fish-like habit.:289
Phylogeny.
The internal phylogeny of the cephalopods is difficult to constrain; many molecular techniques have been adopted, but the results produced are conflicting. "Nautilus" tends to be considered an outgroup, with "Vampyroteuthis" forming an outgroup to other squid; however in one analysis the nautiloids, octopus and teuthids plot as a polytomy. Some molecular phylogenies do not recover the mineralized coleoids ("Spirula", "Sepia", and "Metasepia") as a clade; however, others do recover this more parsimonious-seeming clade, with "Spirula" as a sister group to "Sepia" and "Metasepia" in a clade that had probably diverged before the end of the Triassic.
Molecular estimates for clade divergence vary. One 'statistically robust' estimate has "Nautilus" diverging from "Octopus" at  ± 24 million years ago.
Taxonomy.
The classification presented here, for recent cephalopods, follows largely from (May 2001), for fossil cephalopods takes from Arkell et al. 1957, Teichert and Moore 1964, Teichert 1988, and others. The three subclasses are traditional, corresponding to the three orders of cephalopods recognized by Bather.
Class Cephalopoda († indicates extinct groups)
Other classifications differ, primarily in how the various decapod orders are related, and whether they should be orders or families.
Suprafamilial classification of the Treatise.
This is the older classification that combines those found in parts K and L of the "Treatise on Invertebrate Paleontology", which forms the basis for and is retained in large part by classifications that have come later.
Nautiloids in general (Teichert and Moore, 1964) sequence as given.
Paleozoic Ammonoidea (Miller, Furnish and Schindewolf, 1957)
Mesozoic Ammonoidea (Arkel et al., 1957)
Subsequent revisions include the establishment of three Upper Cambrian orders, the Plectronocerida, Protactinocerida and Yanhecerida; separation of the pseudorthocerids as the Pseudorthocerida, and elevating orthoceritoids as the Subclass Orthoceratoidea.
Shevyrev classification.
Shevyrev (2005) suggested a division into eight subclasses, mostly comprising the more diverse and numerous fossil forms, although this classification has been criticized as arbitrary.
Class Cephalopoda
Cladistic classification.
Another recent system divides all cephalopods into two clades. One includes nautilus and most fossil nautiloids. The other clade (Neocephalopoda or Angusteradulata) is closer to modern coleoids, and includes belemnoids, ammonoids, and many orthocerid families. There are also stem group cephalopods of the traditional Ellesmerocerida that belong to neither clade.
Monophyly of coeloids.
The coeloids have been thought to possibly represent a polyphyletic group,:289 although this has not been supported by the rising body of molecular data.
Post-mortem decay.
After death, if undisturbed, cephalopods decay relatively quickly. Their muscle softens within a couple of days, and may swell; egg sacs can swell so much that they rip through the mantle. Subsequently, the organs shrink again; at this point the organism may start to break up into fragments. The eyes retain their size while the head shrinks around them. The gills may remain swollen at this point. After around a week, the carcass collapses in on itself and begins to disintegrate. The ink sac solidifies around this point. After a fortnight little is left but a blob with eyes, arms and ink sac visible. After a couple of months, these are only recognisable as flattened dark stains — although in some cases the eye lenses can remain intact for up to a year.
In popular culture.
Cephalopods, typically octopuses and squids, have been depicted commonly in Western pop culture as creatures that enjoy hugging or latching onto objects with their limbs and refusing to release. Some of the most notable uses of cephalopods in popular culture include Cthulhu, Squidward Tentacles, and the cephalopod-like robotic arms of Doctor Octopus.

</doc>
<doc id="42728" url="http://en.wikipedia.org/wiki?curid=42728" title="Signal reflection">
Signal reflection

Signal reflection occurs when a signal is transmitted along a transmission medium, such as a copper cable or an optical fiber. Some of the signal power may be reflected back to its origin rather than being carried all the way along the cable to the far end. This happens because imperfections in the cable cause impedance mismatches and non-linear changes in the cable characteristics. These abrupt changes in characteristics cause some of the transmitted signal to be reflected. In radio frequency (RF) practice, this is often measured in a dimensionless ratio known as VSWR with a VSWR bridge. The ratio of energy bounced back depends on the impedance mismatch. Mathematically, it is defined using the reflection coefficient.
Because the principles are the same, this concept is perhaps easiest to understand when considering an optical fiber. Imperfections in the glass create mirrors that reflect the light back along the fiber.
Impedance discontinuities cause attenuation, attenuation distortion, standing waves, ringing and other effects because a portion of a transmitted signal will be reflected back to the transmitting device rather than continuing to the receiver, much like an echo. This effect is compounded if multiple discontinuities cause additional portions of the remaining signal to be reflected back to the transmitter. This is a fundamental problem with the daisy chain method of connecting electronic components. 
When a returning reflection strikes another discontinuity, some of the signal rebounds in the original signal direction, creating multiple echo effects. These forward echoes strike the receiver at different intervals making it difficult for the receiver to accurately detect data values on the signal. The effects can resemble those of jitter.
Because damage to the cable can cause reflections, an instrument called an electrical time domain reflectometer ETDR (for electrical cables) or an optical time domain reflectometer OTDR (for optical cables) can be used to locate the damaged part of a cable. These instruments work by sending a short pulsed signal into the cable and measuring how long the reflection takes to return. If only reflection magnitudes are desired, however, and exact fault locations are not required, VSWR bridges perform a similar but lesser function for RF cables.
The combination of the effects of signal attenuation and impedance discontinuities on a communications link is called insertion loss. Proper network operation depends on constant characteristic impedance in all cables and connectors, with no impedance discontinuities in the entire cable system. When a sufficient degree of impedance matching is not practical, echo suppressors or echo cancellers, or both, can sometimes reduce the problems.
The Bergeron Diagram method, valid for both linear and non-linear models, evaluates the reflection's effects in an electric line.

</doc>
<doc id="42730" url="http://en.wikipedia.org/wiki?curid=42730" title="Emory University">
Emory University

Emory University is a private research university in metropolitan Atlanta, located in the Druid Hills section of unincorporated DeKalb County, Georgia, United States. The university was founded as Emory College in 1836 in Oxford, Georgia by the Methodist Episcopal Church and was named in honor of Methodist bishop John Emory. In 1915, the college relocated to metropolitan Atlanta and was rechartered as Emory University 
Emory University has nine academic divisions: Emory College of Arts and Sciences, Oxford College, Goizueta Business School, Laney Graduate School, School of Law, School of Medicine, Nell Hodgson Woodruff School of Nursing, Rollins School of Public Health, and the Candler School of Theology. Emory University and the Georgia Institute of Technology jointly administer the Wallace H. Coulter Department of Biomedical Engineering Program with Peking University in Beijing, China. Emory University students come from all 50 U.S. states and over 100 foreign countries.
Emory Healthcare is the largest healthcare system in the state of Georgia and is composed of seven major hospitals, including the nationally renowned Emory University Hospital and Emory University Hospital Midtown. The university also operates the Winship Cancer Institute and Yerkes National Primate Research Center. The Centers for Disease Control and Prevention and the American Cancer Society are national affiliate institutions located adjacent to campus.
Emory University is 16th among the list of colleges and universities in the United States by endowment, 19th among universities in the world by endowment, and 21st in "U.S. News & World Report's" 2015 National Universities Rankings. Emory University has a Carnegie Classification of Institutions of Higher Education status of RU/VH: "very high research activity". The university is 5th among universities in the United States with licensing revenue per dollars spent on research and the 4th largest contributor in the nation to the discovery of new drugs and vaccines among public-sector research institutions. In 1995 Emory University was elected to the Association of American Universities, an association of the 62 leading research universities in the United States & Canada.
History.
Nineteenth century.
Emory College was founded in 1836 in Oxford, Georgia by the Methodist Episcopal Church. The college was named in honor of the departed Methodist bishop John Emory. Ignatius Alphonso Few was the college's first president. In 1854, the Atlanta Medical College, a forerunner of Emory University School of Medicine, was founded. On April 12, 1861, the American Civil War began. Emory College was closed in November 1861 and all of its students enlisted. In late 1863 the war came to Georgia and the college was used as hospital and later a headquarters for the Union Army. Thirty five Emory students lost their lives and much of the campus was destroyed during the war.
Emory College, as with the entire Southeastern United States, struggled to overcome financial devastation during the Reconstruction Era. In 1880, Atticus Greene Haygood, Emory College President, delivered a speech expressing gratitude for the end of slavery in the United States, which captured the attention of George I. Seney, a New York banker. Seney gave Emory College $5,000 to repay its debts, $50,000 for construction, and $75,000 to establish a new endowment. In the 1880s, the technology department was launched by Isaac Stiles Hopkins, a polymath professor at Emory College. Hopkins became the first president of the Georgia Institute of Technology in 1888. Emory University's first international student, Yun Chi-ho, graduated in 1893. Yun became an important political activist in Korea and is the author of Aegukga, the national anthem of the Republic of Korea.
Twentieth century.
On August 16, 1906, the Wesley Memorial Hospital and Training School for Nurses, later renamed the Nell Hodgson Woodruff School of Nursing, was established. In 1914, the Candler School of Theology was established. In 1915, Emory College relocated to metropolitan Atlanta and was rechartered as Emory University after accepting a land grant from Asa Griggs Candler, founder of the The Coca-Cola Company. The Emory University School of Law was established in 1916. From the 1920s through the 1970s, Emory University established its reputation as a regional institution that offered a solid education in medicine, law, theology, business, and the liberal arts. 
First and Second World Wars.
On August 6, 1917 the United States entered the First World War. Emory University organized a medical unit, composed of medical school faculty and medical alumni, that would be known as Emory Unit, Base Hospital 43. The unit served in Loir-et-Cher, France from July 1918 to January 1919. The Emory Unit, Base Hospital 43 was remobilized during the Second World War and served in the North African Campaign and Europe. To recognize Emory’s participation in the war effort, a ship was christened M.S. Emory Victory and served through World War II and in the Korean War.
Emory University students, alumni, and faculty served in the Asia-Pacific War and European theater of World War II, including Bobby Jones (golfer), who participated in the Battle of Normandy. Dr. Alfred A. Weinstein, a professor of surgery at Emory University School of Medicine, was a prisoner of war of the Empire of Japan between 1942 and 1945. His memoir "Barbed Wire Surgeon" is considered one of the finest accounts concerning allied prisoners under Japanese captivity and highlights the abuses of the war criminal Mutsuhiro Watanabe. Kiyoshi Tanimoto, who graduated from the Candler School of Theology in 1940 and is portrayed in John Hersey's Hiroshima (book), was able to organize the Hiroshima Maidens reconstructive surgery program based on the associations he made while studying in the United States. Emory helped the nation prepare for war by participating in the V-12 Navy College Training Program and Army Specialized Training Program, programs designed to supplement the force of commissioned officers in the United States Navy and United States Army. During the war, university enrollment boasted two military students for every one civilian. Emory University alumni would go on to serve in the Korean War, Second Indochina War (Vietnam War), Persian Gulf War, Yugoslav Wars, and the Global War on Terrorism.
Civil Rights Movements.
The Movements for civil rights during the 1950s and 1960s in the United States profoundly shaped the future of Emory University. Formerly an all-male school, Emory officially became a coeducational institution in 1953. Although it had previously admitted women under limited circumstances, the university had never before had a policy through which they could enroll in large numbers and as r theesident students. Shortly after, in 1959, sororities first appeared on campus. In 1962, in the midst of the African-American Civil Rights Movement, Emory embraced the initiative to end racial restrictions when it asked the courts to declare portions of the Georgia statutes unconstitutional. Previously, Georgia law denied tax-exempt status to private universities with racially integrated student bodies. The Supreme Court of Georgia ruled in Emory's favor and Emory officially became racially integrated. Marvin S. Arrington, Sr. was Emory University's first, full-time African American student and graduated from Emory University School of Law in 1967.
Emory's diversity and academic reputation flourished under the leadership of the university's fifth president, James T. Laney. In addition to leading universities in the Southeastern United States in the promotion of racial equality, Laney and many of the school's faculty and administrators were outspoken advocates of global human rights and thus were openly opposed to the military dictatorship in South Korea (1961-1987). On March 30, 1983, Laney's friend Kim Dae-jung, while in political exile in the United States, presented a speech on human rights and democracy at Emory University and accepted an honorary Doctor of Laws degree. Kim would go on to play a major role in ending authoritarianism in South Korea, served as the 8th President of South Korea from 1998 to 2003, and was awarded the Nobel Peace Prize in 2000 for his successful implementation of the Sunshine Policy. Laney would later serve as United States Ambassador to South Korea and the Emory graduate school, founded in 1919, was named in his honor in 2009.
In 2014, at Emory’s 169th Commencement, John Lewis, the only living "Big Six" leader of the African-American Civil Rights Movement, delivered the keynote address and received an honorary doctor of laws degree. In 2015, Emory University School of Law received a $1.5 million donation to help establish a John Lewis Chair in Civil Rights and Social Justice. The gift, given anonymously, funds a professorship which will enable Emory Law to conduct a national search for a scholar with an established academic profile of distinction and a demonstrated desire to promote the rule of law through the study of civil rights. The law school has committed to raise an additional $500,000 to fund the chair fully.
Expansion and modernization.
The course of Emory's history changed dramatically in November 1979 when Robert Winship Woodruff and George Waldo Woodruff presented the institution with a gift of $105 million in Coca-Cola stock. At the time this was the largest single gift to any institution of higher education in American history, and it made a profound impact on Emory's direction in the next two decades, boosting the university to the top ranks of American research universities.
Twenty-first century.
As one of the fastest-growing research universities in the United States in the 21st century, Emory University has established a national reputation on the strength of the scholarly achievements of its faculty and students, its highly ranked professional schools, a long-term commitment to the arts and sciences, and the presence of more than seventy cutting-edge research centers that are addressing major social problems. Emory has extended its ties to the community, creating close links with Atlanta's neighborhoods, clinics, hospitals, nonprofit organizations, and boardrooms To accommodate its growth, Emory has undergone a physical transformation that has increased classroom and research space. The latest additions to the campus include buildings for cancer research, biomedical research, scientific computation, mathematics and science, vaccine research, and the performing arts.
Academics.
Demographics.
Emory University's total enrollment for the 2014-2015 academic year is 14,769 students, with 7,829 undergraduates and 6,940 graduate and professional students. Students come from all 50 states and more than 65 countries. The student to faculty ratio is 7:1, with an average class size of 25 students. Of the 1,389 students in the Class of 2018, 46% are Caucasian, 31% are Asian, 10% are Black/African American, 9% are Latino/Hispanic, and 3% did not identity; 56% are female and 44% are male. 
Seventy-four percent of Emory University students come from outside the Southeastern United States. International students in the Class of 2018 come from Antigua and Barbuda, Australia, Bahamas, Belgium, Brazil, Canada, Chile, China, Hong Kong, Taiwan, Dominican Republic, Ecuador, France, Germany, Ghana, United Kingdom, Greece, Honduras, India, Indonesia, Japan, Jordan, Kazakhstan, Republic of Korea, Malaysia, Mexico, Morocco, Nigeria, Pakistan, Panama, Peru, Poland, Russia, Rwanda, Singapore, Switzerland, Thailand, Trinidad and Tobago, Turkey, Ukraine, United Arab Emirates, Vietnam, Virgin Islands, and Zimbabwe.
Undergraduate schools.
The Emory College of Arts and Sciences offers the Bachelor of Arts (B.A.) and the Bachelor of Science (B.S) undergraduate academic degrees. Academic Departments include African American Studies, African Studies, American Studies, Ancient Mediterranean Studies, Anthropology, Art History, Biology, Chemistry, Classics, Comparative Literature, East Asian Studies, Economics, Educational Studies, English, Environmental Sciences, Film & Media Studies, French and Italian Studies, German Studies, Global Health, Culture, and Society, History, Human Health, Jewish Studies, Latin American and Caribbean Studies, Linguistics, Mathematics and Computer Science, Middle Eastern and South Asian Studies, Music, Neuroscience and Behavioral Biology, Philosophy, Physics, Political Science, Psychology, Quantitative Theory and Methods, Religion, Russian and East Asian Languages and Cultures, Sociology, Spanish and Portuguese, Theater and Dance, and Women's, Gender and Sexuality Studies. Emory University offers a five-year dual-degree program in engineering, in collaboration with the Georgia Institute of Technology Emory College of Arts and Sciences, established in 1836, has over 70 majors and 50 minors for undergraduate students. The Confucius Institute a non-profit public institution affiliated with the Ministry of Education of the People's Republic of China, operates in co-operation with the university at the Emory College of Arts and Sciences. The Emory-Tibet Partnership was established in 1998. In October, 2007, the 14th Dalai Lama visited Emory and was installed as a Presidential Distinguished Professor.
Oxford College offers an Associate degree (A.A.) in liberal arts. Students that successfully complete Oxford College advance to Emory College of Arts and Sciences to complete their undergraduate education. Academic Departments include Anthropology, Art, Biology, Chemistry, Economics, English, Geology, History, Languages, Mathematics & Computer Science, Music, Political Science, Philosophy, Psychology, Physics & Astronomy, Quantitative Theory and Methods, Religion, Sociology, Theater, and Women's Studies.
Graduate and professional schools.
 
The Emory University School of Medicine offers the Doctor of Medicine (MD), Doctor of Physical Therapy, Master of Medical Science in Anesthesiology, Master of Medical Science in Human Genetics & Genetic Counseling, Master of Medical Science in Physician Assistant, and Bachelor of Medical Science in Medical Imaging. Academic Departments include Biochemistry, Biomedical Engineering, Biomedical Informatics, Cell Biology, Human Genetics, Microbiology/Immunology, Pharmacology, and Physiology. Clinical Science Departments include Anesthesiology, Dermatology, Emergency Medicine, Family & Preventive Medicine, Gynecology/Obstetrics, Hematology/Medical Oncology, Neurology, Neurosurgery, Ophthalmology, Orthopaedics, Otolaryngology, Pathology, Pediatrics, Psychiatry & Behavioral Sciences, Radiation Oncology, Radiology, Rehabilitation Medicine, Surgery, and Urology.
The Nell Hodgson Woodruff School of Nursing offers the Bachelor of Science in Nursing (BSN), Masters of Science in Nursing, and Doctor of Nursing Practice (DNP).
The Candler School of Theology offers the Master of Divinity (MDiv), Master of Religious Leadership (MRL), Master of Religion and Public Life (MRPL), Master of Theological Studies (MTS), Master of Theology (ThM), Doctor of Ministry (DMin), and Doctor of Theology in Pastoral Counseling (ThD).
The Emory University School of Law offers the Juris Doctor, Juris Master, Master of Laws, and Doctor of Juridical Science.
The Laney Graduate School offers the Master of Arts degree in Bioethics, Clinical Research, Computer Science and Informatics, Development Practice, Educational Studies, Film Studies, Mathematics, and Music. The school offers the Doctor of Philosophy in Anthropology, Art History, Behavioral Sciences and Health Education, Biochemistry, Cell and Developmental Biology (GDBBS), Biomedical Engineering, Biostatistics, Business, Cancer Biology (GDBBS), Chemistry, Clinical Psychology, Cognition and Development (Psychology), Comparative Literature, Computer Science and Informatics, Economics, Educational Studies, English, Environmental Health Sciences, Epidemiology, French, Genetics and Molecular Biology (GDBBS), Health Services Research and Health Policy, History, Immunology and Molecular Pathogenesis (GDBBS), Islamic Civilizations Studies, Mathematics, Microbiology and Molecular Genetics (GDBBS), Molecular and Systems Pharmacology (GDBBS), Neuroscience (GDBBS), Neuroscience and Animal Behavior (Psychology), Nursing, Nutrition and Health Sciences (GDBBS), Philosophy, Physics, Political Science, Population Biology, Ecology and Evolution (GDBBS), Religion, Sociology, Spanish, and Women's, Gender, and Sexuality Studies.
The Goizueta Business School offers the Bachelor of Business Administration, Master of Business Administration, Executive Master of Business Administration, and a Doctor of Philosophy in Business Administration.
The Rollins School of Public Health offers the Master of Public Health (MPH) and Master of Science in Public Health (MSPH). Academic Departments include Behavioral Sciences & Health Education, Biostatistics & Bioinformatics, Environmental Health, Epidemiology, Global Health, and Health Policy & Management.
Reputation and rankings.
Emory University is currently ranked 21st among national universities in the United States by U.S. News and World Report and 93rd among global universities in the Times Higher Education World University Rankings. The university has been named both a Hidden Ivy and Southern Ivy. Emory is considered to have one of the best writing programs in the United States and was ranked 1st among the list of the best colleges and universities for writers by the The Huffington Post and USA Today. Emory University's programs consistently rank among the most competitive in their fields by U.S. News and World Report. In 2015, the Wallace H. Coulter Department of Biomedical Engineering Program was ranked 2nd in the United States for the ninth consecutive year. The Emory University School of Medicine was ranked the 23rd Best Medical Research School in the United States in 2015. Rollins School of Public Health was ranked 7th among public health schools in the United States in 2015. The Emory University School of Medicine Physician Assistant Program was ranked 3rd among physician assistant programs in the United States in 2015. Emory University's Nell Hodgson Woodruff School of Nursing was ranked 10th among Nursing Schools in the United States in 2015.
Emory University is ranked 13th in Immunology, 22nd in Microbiology, 28th in Psychiatry, 29th in Social Sciences and Public Health, 32nd in Clinical Medicine, 37th in Neuroscience and Behavior, 45th in Pharmacology and Toxicology, 50th in Biochemistry, and 67th in Molecular Biology and Genetics in the world by U.S. News and World Report Emory University is ranked 6th among national universities in the United States in Social Psychology, 11th in Behavioral Neuroscience, 18th in Clinical Psychology, 25th in Political Science, 26th in English, 27th in History, 30th Biological Sciences, 35th in Chemistry, 35th in Sociology, 38th in Psychology, 38th in Statistics, 64th in Economics, 65th in Mathematics, 85th in Physics by U.S. News and World Report. The Emory University School of Law is ranked 19th among Law Schools in the United States by "U.S. News and World Report". The Princeton Review named the Emory University School of Law as one of best 169 law schools in the United States in 2014. Emory University's Goizueta Business School is ranked 20th among Business Schools in the United States by U.S. News and World Report. Bloomberg Businessweek ranked Goizueta Business School's BBA Program 9th in the nation in 2014. The Economist ranked Goizueta Business School's MBA program 13th in the nation in 2014.
Research.
Emory University has a Carnegie Classification of Institutions of Higher Education status of RU/VH: "very high research activity". According to The Chronicle of Higher Education, the university is 5th among universities in the United States with licensing revenue per dollars spent on research. The university is the 4th largest contributor in the nation to the discovery of new drugs and vaccines among public-sector research institutions. The Universities Allied for Essential Medicines, ranked Emory 6th among universities in the United States and Canada for global health contributions and research. In fiscal year 2014, Emory received $521.8 million in total research funding awards. In 2015, Emory University was one of four institutions selected by the National Institute of Allergy and Infectious Diseases for its seven-year, multi-million dollar Tuberculosis Research Units (TBRU) program, which aims to drive innovation in tuberculosis research and reduce the global burden of the disease. Emory University leads the nation in the number of students with Kirschstein-National Research Service Award pre-doctoral fellowships from the National Institutes of Health.
The Emory University Center for AIDS Research (CFAR) and the Emory Vaccine Center are world leaders in AIDS Vaccine Development and HIV Parthenogenesis studies are funded by nine different institutes of the National Institutes of Health and by the Georgia Research Alliance. The centers include one of the largest groups of academic vaccine scientists in the world and are currently attempting to develop an effective HIV vaccine. Emory University Researchers Dr. Dennis C. Liotta, Dr. Raymond F. Schinazi and Dr. Woo-Baeg Choi discovered Emtricitabine, a nucleoside reverse transcriptase inhibitor (NRTI) used in the treatment of HIV. The drug was named as one of the world's most important antiviral drugs by the World Health Organization and is included in their Model List of Essential Medicines.
Emory University has a strong partnership with the Centers for Disease Control and Prevention (CDC). In 1947, the university donated 15 acres of land to the United States Department of Health and Human Services for the construction of the CDC headquarters. The Emory University Prevention Research Center (EPRC) and Emory Center for Injury Control are funded by the CDC. Emory University's African Center of Excellence for Public Health Security, which seeks to improve preparedness and response to health threats in low-income countries, is a five-year, multi-million dollar cooperative program with the CDC and International Association of National Public Health Institutes (IANPHI). The Emory University Center for Global Safe Water (CGSW), which conducts applied research, evaluation, and training to promote global health equity through universal access to safe water, sanitation, and hygiene, works in collaboration with the CDC. The Emory University Global Health Institute, funded by the Bill & Melinda Gates Foundation, partners with the CDC to enhance public health infrastructure in low-resource countries. The Emory University Hospital Isolation Unit and Quarantine Station was established by the CDC following the 2003 SARS outbreak. The isolation and treatment facilities at Emory University played a crucial role in ending the 2014 Ebola virus cases in the United States. CDC scientists and administrators hold memberships and frequently speak at Emory University's Vaccine Dinner Club (VDC), an association that holds monthly academic meetings to discuss and advance vaccine research.
Student life.
Residential life.
Emory requires its students to live on campus for the first two years of undergraduate life, with defined options for freshmen and sophomores. Juniors and seniors may elect to live off-campus or continue in campus housing.
Fraternities have existed on Emory's campus as early as 1840. One early chronicler makes the case that Emory's "temple" of the Mystic Seven may have been the first chapter of a national fraternity established anywhere in the South. Today, the Greek-letter sororities and fraternities play an important part in leavening Emory's campus life. For undergraduates, Greek life comprises approximately 30% of the Emory student population. The Office of Greek Life recognizes and regulates on-campus chapters of fraternities and sororities. Fraternities have on-campus housing located on Eagle Row, and Sorority Village, a series of townhouses, faces the fraternity houses. Greek Life is an important social engagement for students, but it is not totally exclusive—students from different sororities and fraternities regularly socialize, and the college's emphasis on on-campus housing helps students make friends inside and outside the Greek system.
Community service.
The university received the 2008 Presidential Award for General Community Service, which is the highest federal recognition given to higher education institutions for their commitment to community service, service-learning and civic engagement.
About 25% of Emory students participate in Volunteer Emory, Emory's umbrella community service group. As one of the most popular groups on campus, Volunteer Emory offers dozens of ways to serve the community, working with varied organizations including the Atlanta Community Food Bank, Trees Atlanta, PAWS Atlanta, and Jones Boys and Girls Club.
Emory Cares International Service Day brings together students, alumni and other community members to volunteer at a number of projects organized by Emory and its many partners around the city of Atlanta and in cities worldwide.
Student organizations.
Hundreds of student clubs and organizations operate on Emory's campus. These include numerous student government, special interest, and service organizations.
The Student Government Association (SGA) charters and provides most of the funding for other student groups, and represents students' interests when dealing with the administration. The SGA oversees divisional councils, each coinciding with the undergraduate, graduate and professional schools of the university. Notable among these are the College Council (CC) which handles students concerns primarily for the undergraduate body of the Emory College of Arts and Sciences and annually sponsors the State of Race event, and the BBA Council which does similar activities for the Goizueta Business School BBA Program. The Student Programming Council (SPC) is the school's primary programming organization, responsible for planning five events every year: Homecoming Week, Fall Band Party, Spring Band Party, Swoopstock and Dooley's Week.
"The Emory Wheel", Emory's undergraduate student newspaper, has been continually published since 1919. It is financially independent from the university, covering its costs from self-generated advertising sales. WMRE, Emory's student operated radio station, began broadcasting in 1989. Although it was initially only available to on-campus listeners, it now enjoys a worldwide audience.
Emory also has several secret societies—the Paladin Society, the D.V.S. Senior Honor Society, Ducemus, Speculum, and The Order of Ammon.
Programs Abroad.
Through the Centers of International Programs Abroad, Emory University students can study in over 40 countries at the top academic institutions in the world including the National University of Singapore, Kyoto Consortium for Japanese Studies, Nanjing University, Oxford University, Imperial College London, the School of Oriental and African Studies, Yonsei University, Trinity College Dublin, University of St. Andrews, University of Melbourne, University of Amsterdam, University of Cape Town, and Tel Aviv University.
Arts.
Students may engage in the performing and fine arts as an area of academic study or as extracurricular activities. Undergraduates may pursue a major in the performing arts (dance, theater, or music) or in film studies, art history, visual arts, or creative writing. Graduate programs in art history, film studies, and music are offered.
There are more than 50 student organizations dedicated to the arts. Students can explore artistic interests as diverse as architecture, breakdancing, poetry, and improvisational comedy.
Emory routinely hosts arts events in the Schwartz Center for Performing Arts that are open to the Emory and Atlanta communities. Recent performances include Bang on a Can All-Stars (a side project of drummer Glenn Kotche from the rock band Wilco), jazz performer Esperanza Spalding, and New York’s Cedar Lake Dance Company. A program called Creativity Conversations brings artistic minds to campus to discuss art and the creative process. Guests have included Philip Glass, Jimmy Carter, Salman Rushdie, Seamus Heaney and Rita Dove. Rita Dove also gave the keynote address at Emory's 2013 Commencement.
Athletics.
Emory ranks among top schools in both the "U.S. News & World Report’s" rankings of the best national universities and the Directors Cup of the National Association of Collegiate Directors of Athletics for best all-around athletics program.
Emory's 18 varsity sports teams, known as the Eagles, are members of the NCAA’s Division III University Athletic Association (UAA). However, Emory does not have an intercollegiate football team.
The intramural sports program provides an athletic outlet for the entire Emory community. Emory has numerous club sports and a variety of recreational and competitive intramural teams. The Outdoor Emory Organization sponsors weekend trips of outdoor activities such as rafting, rock climbing and hiking.
Campus.
Surrounding area.
Emory's main campus is located in Druid Hills section of unincorporated DeKalb County, Georgia, a suburban community near Atlanta. Emory’s main campus is about a 15-minute drive from downtown and midtown Atlanta as well as the Buckhead area. The Atlanta metropolitan area, with more than 5.5 million people, is the third largest in the Southeastern United States and the ninth largest in the country.
Atlanta is home to the world headquarters of corporations such as The Coca-Cola Company, The Home Depot, AT&T Mobility, UPS, Delta Air Lines, and Turner Broadcasting. Atlanta has the country's fourth-largest concentration of Fortune 500 companies, and more than 75 percent of Fortune 1000 companies have business operations in the metropolitan area.
Popular attractions in the Atlanta area include, the world’s largest indoor aquarium, the Georgia Aquarium, The World of Coca-Cola, the High Museum of Art and CNN Center. Atlanta is also home to The Peachtree Road Race, the world’s largest 10k with a field capped at 60,000 runners, as well as the National Black Arts Festival, a celebration of African American music, film, visual art, dance and literature, that takes place every summer in Atlanta.
Sustainability.
The university has one of the largest inventories by square footage of Leadership in Energy and Environmental Design-certified building space among campuses in the United States. New buildings on Emory’s campus must comply with the guidelines set by U.S. Green Building Council’s Leadership in Energy and Environmental Design (LEED).
The university also has a policy to preserve more than half the campus as undeveloped green space. For every tree removed for new construction, another must be planted.
Emory is committed to having three-quarters of the food served on campus come from local or sustainable sources by 2015. Emory’s campus has several small educational gardens, where fresh produce is grown. These gardens are meant to increase awareness about local food and remind members of the community that they can reduce fossil fuel use by eating locally. The upkeep of the gardens is the responsibility of members of the Emory community. During the school year, a seasonal farmers market hosts local farmers and vendors.
The Druid Hills campus has a pedestrian-only center. The Cliff shuttle system provides transportation for students, faculty and staff. Alternative transportation is encouraged through initiatives such as Bike Emory and Zipcar, a company that rents cars for short-term use.
Students have the option of completing a minor in sustainability. This includes courses on the social, environmental and economic elements of sustainability, as well as a hands-on component, such as research or an internship.
Buildings.
The Carlos Museum houses one of the most comprehensive art collections in the Southeast, with works from ancient Egypt, Near East, Greece, Rome, ancient Americas, Africa, and Asia. The museum has been adding to its collection since 1876, when a small museum was opened on the Oxford campus. Its permanent collection includes such pieces as an influential statue of Aphrodite from the 1st century BC., which was in two parts until it was fixed by a Carlos employee. One of the most notable exhibitions that the Carlos Museum has had was an exhibition about Egyptian pharaoh Tutankhamun ("King Tut"), which was on display for the first time in 26 years. Students may visit the Carlos Museum for free. Many of the curators teach courses at the University and faculty in other departments, including dance and physics, often use the museum as part of their curriculum. 
Emory’s Robert W. Woodruff University Library has been ranked #13 in the nation, according to "The Princeton Review". The library’s tenth floor is home to MARBL, which has rare materials relating to literature, African American history and culture, and Southern and Georgia history. Notable pieces of the MARBL collection include a rare first edition of "Robinson Crusoe" by Daniel Defoe, as well as works by Flannery O'Connor, Alice Walker, Langston Hughes, W.B. Yeats, and Seamus Heaney. All students have complete access to MARBL and members of the public may also use the library. Many of these authors become subjects of exhibitions in Schatten Gallery, which is located on the third floor of Woodruff Library and houses various displays throughout the year.
Lullwater Preserve features more than 100 acres of green space including woods, walking trails and a lake. The home of the University president and his family, Lullwater House is located here. The only vehicles allowed are those that have received special permission because they are visiting the president’s house. The property was originally the estate of Walter T. Candler, son of Coca-Cola co-founder Asa Griggs Candler. 
The Yerkes National Primate Research Center is one of only eight National Institutes of Health–funded national primate research centers. Between its two locations—the main center on Emory’s Druid Hills campus and a secondary location in Lawrenceville, Ga.—the Center has nearly 3,400 nonhuman primates and 13,000 rodents. Since 1930, the Center has been conducting research in the fields of microbiology and immunology, neurologic diseases, neuropharmacology, behavioral, cognitive and developmental neuroscience, and psychiatric disorders. Current research includes developing vaccines for infectious and noninfectious diseases, treating drug addiction, and increase understanding of illnesses such as Alzheimer’s and Parkinson's diseases.
Emory is partnered with the Carter Center, a not-for-profit organization founded by former U.S. President Jimmy Carter to further human rights. Carter usually visits Emory’s campus several times throughout the year. Most notably, he hosts Carter Town Hall, an open-forum event for all first-year students.
The Donna and Marvin Schwartz Center for Performing Arts hosts professional and student-run performances throughout the year. In addition to various practice facilities and smaller performance spaces, The Schwartz Center now includes Cherry Logan Emerson Concert Hall, which has 825 seats and a large pipe organ.
Winship Cancer Institute of Emory University is Georgia’s first and only cancer center designated by the National Cancer Institute. The Winship Cancer institute was founded in 1937 with a gift from Robert Woodruff, the former president of Coca-Cola, after he lost his mother to cancer that year. For over 65 years, the mission of the Winship Cancer Institute has been to bring together researchers, physicians, epidemiologists, nurses, engineers, and social workers with the goal of preventing, treating, and curing cancer. Divisions at Winship Cancer Institute include radiation oncology, surgical oncology, hematology, and medical oncology. In 2009, Winship Cancer Institute was the first in Georgia to use a new and faster radiation system, called RapidArc, which can reduce treatment times and deliver a complete treatment in a single rotation of the machine around the patient. In 2006, the National Cancer Institute selected the Emory and Georgia Tech joint research program as one of seven National Centers of Cancer Nanotechnology.
Notable alumni and faculty.
Emory University has over 13,200 faculty and staff members and over 133,000 living alumni. Emory alumni and faculty include a President of the United States, a Vice President of the United States, a President of South Korea, a Prime Minister of South Korea, a Deputy Prime Minister of South Korea, a President of Mexico, a Vice President of the Standing Committee of the National People's Congress of the People's Republic of China, a United States Centers for Disease Control and Prevention Director, a Justice of the Supreme Court of the United States, a United States Secretary of the Interior, a Chairman of the United States Senate Committee on Armed Services, a Speaker of the United States House of Representatives, Members of Congress in the United States Senate and United States House of Representatives, Ambassadors of the United States, Governors of the States and Territories of the United States, Chief Executive Officers and Presidents of Fortune 500 corporations, a Chief Information Officer for the Federal Communications Commission, a Senior Executive with the Office of the Director of National Intelligence, a director of the The Rockefeller Institute, a President of the Republic of Korea National Red Cross, a President of the Korean Sociological Association, a President of the American Psychological Association, a United States Poet Laureate,and an Executive Vice President of the National Geographic Society. 
Awards and honors recognizing Emory alumni and faculty include the Nobel Prize, Pulitzer Prize, Bancroft Prize, National Humanities Medal, Peabody Award, Breakthrough Prize in Life Sciences, Guggenheim Fellowship, Fulbright Fellowship, American Mathematical Society Fellowship, MacArthur Fellows Program, Presidential Medal of Freedom, Rhodes Scholarship, Marshall Scholarship, and membership in the American Academy of Arts and Sciences, Carnegie Foundation for the Advancement of Teaching, Howard Hughes Medical Institute, American Society for Clinical Investigation, National Academy of Sciences, and National Research Council.
Further reading.
</dl>

</doc>
<doc id="42731" url="http://en.wikipedia.org/wiki?curid=42731" title="Dodoni">
Dodoni

Dodoni (Greek: Δωδώνη) is a village and a municipality in the Ioannina regional unit, Epirus, Greece. The seat of the municipality is the village Agia Kyriaki. Dodoni is located near the site of the ancient oracle of Dodona. "Oedipus the King" was shot here in 1967.
Municipality.
The present municipality Dodoni was formed at the 2011 local government reform by the merger of the following 4 former municipalities, that became municipal units:

</doc>
<doc id="42734" url="http://en.wikipedia.org/wiki?curid=42734" title="Transcendental Meditation">
Transcendental Meditation

Transcendental Meditation (TM) refers to a specific form of mantra meditation called the Transcendental Meditation technique,<ref name="Britannica online/TM"> </ref> and can also refer to the organizations within the Transcendental Meditation movement and to the movement itself. The TM technique and TM movement were introduced in India in the mid-1950s by Maharishi Mahesh Yogi (1918–2008).
The Maharishi taught thousands of people during a series of world tours from 1958 to 1965, expressing his teachings in spiritual and religious terms. TM became more popular in the 1960s and 1970s, as the Maharishi shifted to a more technical presentation and his meditation technique was practiced by celebrities. At this time, he began training TM teachers and created specialized organizations to present TM to specific segments of the population such as business people and students. By the late 2000s, TM had been taught to millions of people, and the worldwide TM organization had grown to include educational programs, health products, and related services.
The TM technique involves the use of a sound or mantra and is practiced for 15–20 minutes twice per day. It is taught by certified teachers through a standard course of instruction, which costs a fee that varies by country. According to the Transcendental Meditation movement, it is a method for relaxation, stress reduction and self-development. Varying views on whether the technique is religious or non-religious have been expressed including by sociologists, scholars, and a New Jersey court case.
TM is one of the most widely practiced, and is among the most widely researched meditation techniques. It is not possible to say if it has any effect on health as the research to date is of poor quality.
History.
The Transcendental Meditation (TM) program and the Transcendental Meditation movement originated with Maharishi Mahesh Yogi, founder of the organization, and continued beyond his death (2008). In 1955, "the Maharishi began publicly teaching a traditional meditation technique" learned from his master Brahmananda Saraswati that he called Transcendental Deep Meditation and later renamed Transcendental Meditation.
The Maharishi initiated thousands of people, then developed a TM teacher training program as a way to accelerate the rate of bringing the technique to more people. He also inaugurated a series of world tours which promoted Transcendental Meditation. These factors, coupled with endorsements by celebrities who practiced TM and claims that scientific research had validated the technique, helped to popularize TM in the 1960s and 1970s. By the late 2000s, TM had been taught to millions of individuals and the Maharishi was overseeing a large multinational movement. Despite organizational changes and the addition of advanced meditative techniques in the 1970s, the Transcendental Meditation technique has remained relatively unchanged.
Among the first organizations to promote TM were the Spiritual Regeneration Movement and the International Meditation Society. In modern times, the movement has grown to encompass schools and universities that teach the practice, and includes many associated programs based on the Maharishi's interpretation of the Vedic traditions. In the U.S., non-profit organizations included the Students International Meditation Society, AFSCI, World Plan Executive Council, Maharishi Vedic Education Development Corporation, Global Country of World Peace and Maharishi Foundation. The successor to Maharishi Mahesh Yogi, and leader of the Global Country of World Peace, is Tony Nader.
Technique.
The meditation practice involves the use of a mantra and is practiced for 15–20 minutes twice per day while sitting with one's eyes closed. It is reported to be one of the most widely practiced, and among the most widely researched, meditation techniques, with hundreds of published research studies. The technique is made available worldwide by certified TM teachers in a seven-step course, and fees vary from country to country. Beginning in 1965, the Transcendental Meditation technique has been incorporated into selected schools, universities, corporations, and prison programs in the US, Latin America, Europe, and India. In 1977 a US district court ruled that a curriculum in TM and the Science of Creative Intelligence (SCI) being taught in some New Jersey schools was religious in nature and in violation of the First Amendment. The technique has since been included in a number of educational and social programs around the world.
The Transcendental Meditation technique has been described as both religious and non religious, as an aspect of a new religious movement, as rooted in Hinduism, and as a non-religious practice for self-development. The public presentation of the TM technique over its 50-year history has been praised for its high visibility in the mass media and effective global propagation, and criticized for using celebrity and scientific endorsements as a marketing tool. Advanced courses supplement the TM technique and include an advanced meditation program called the TM-Sidhi program.
Movement.
The Transcendental Meditation movement refers to the programs and organizations connected with the Transcendental Meditation technique and founded by Maharishi Mahesh Yogi. Transcendental Meditation was first taught in the 1950s in India and has continued since the Maharishi's death in 2008. The organization was estimated to have 900,000 participants worldwide in 1977, a million by the 1980s, and 5 million in more recent years, including some notable practitioners.
Programs include the Transcendental Meditation technique, an advanced meditation practice called the TM-Sidhi program ("Yogic Flying"), an alternative health care program called Maharishi Ayurveda, and a system of building and architecture called Maharishi Sthapatya Ved. The TM movement's past and present media endeavors include a publishing company (MUM Press), a television station (KSCI), a radio station (KHOE), and a satellite television channel (Maharishi Channel). During its 50-year history, its products and services have been offered through a variety of organizations, which are primarily nonprofit and educational. These include the Spiritual Regeneration Movement, the International Mediation Society, World Plan Executive Council, Maharishi Vedic Education Development Corporation, the Global Country of World Peace, and the David Lynch Foundation.
The TM movement also operates a worldwide network of Transcendental Meditation teaching centers, schools, universities, health centers, herbal supplements, solar panel, and home financing companies, plus several TM-centered communities. The global organization is reported to have an estimated net worth of USD 3.5 billion. The TM movement has been characterized in a variety of ways and has been called a spiritual movement, a new religious movement, a millenarian movement, a world affirming movement, a new social movement, a guru-centered movement, a personal growth movement, a religion, and a cult. Additional sources contend that TM and its movement are not a cult. Some state that participation in TM programs does not require a belief system and is practiced by people from a diverse group of religious affiliations including atheists and agnostics. The organization has also been criticized as well as praised for its public presentation and marketing techniques throughout its 50-year history.
Health effects.
The first studies of the health effects of Transcendental Meditation appeared in the early 1970s. Robert Keith Wallace, the founding president of Maharishi University of Management, published a study in "Science" in 1970 reporting that TM induced distinct physiologic changes and a novel state of consciousness in practitioners. However, a 1976 study by independent researchers that looked at different physiological variables found that TM was biochemically similar to sitting with one's eyes closed. A second 1976 study of five subjects found that TM practitioners spent much of their meditation time napping rather than in the unique "wakeful hypometabolic state" described by Wallace. By 2004 the US government had given more than $20 million to Maharishi University of Management to study the effect of meditation on health.
It is currently not possible to say whether meditation has any effect on health, as the research to date has been of poor quality, including a high risk for bias due to the connection of researchers to the TM organization and the selection of subjects with a favorable opinion of TM. Most independent systematic reviews have not found health benefits for TM exceeding those of relaxation and health education. A 2013 statement from the American Heart Association said: "The overall evidence supports that TM modestly lowers BP [blood pressure]" and that TM could be considered as a treatment for hypertension, although other interventions such as exercise and device-guided breathing were felt to be more effective and better supported by clinical evidence. A 2014 systematic review and meta-anaylsis funded by the U.S. Agency for Healthcare Research and Quality found that mantra meditation programs such as TM had no benefit with regard to psychological stress or well-being, although the quality of scientific evidence on TM was poor as a whole. A 2015 systematic review and meta-analysis found that TM reduced systolic and diastolic blood pressure by 4.26 and 2.33 mm Hg, respectively, compared to control groups.
The American Cancer Society has stated that "available scientific evidence does not suggest that meditation is effective in treating cancer or any other disease; however, it may help to improve the quality of life for people with cancer", adding that "Research shows that meditation can help reduce anxiety, stress, blood pressure, chronic pain, and insomnia" and that "Most experts agree that the positive effects of meditation outweigh any negative reactions". A 2014 Cochrane review found limited evidence for an effect in preventing cardiovascular disease.
Maharishi Effect.
In the 1960s, Maharishi Mahesh Yogi described a paranormal effect that he believed a significant number of individuals (1% of the people in a given area) practicing the Transcendental Meditation technique (TM) could have on the local environment. This hypothetical influence was later termed the Maharishi Effect. With the introduction of the TM-Sidhi program in 1976, Maharishi proposed that the square root of one percent of the population practicing the TM-Sidhi program, together at the same time and in the same place, would increase "life-supporting trends". This was referred to as the "Extended Maharishi Effect". Evidence, which TM practitioners believe supports the existence of the effect, has been shown to lack a causal basis. The evidence was said to result from cherry-picked data and the credulity of believers.

</doc>
<doc id="42736" url="http://en.wikipedia.org/wiki?curid=42736" title="VOC">
VOC

VOC may refer to:

</doc>
<doc id="42737" url="http://en.wikipedia.org/wiki?curid=42737" title="Dutch East India Company">
Dutch East India Company

The Dutch East India Company (Dutch: Vereenigde Oostindische Compagnie, VOC, "United East India Company") was a chartered company established in 1602, when the States General of the Netherlands granted it a 21-year monopoly to carry out trade activities in Asia. It is often considered to have been the first multinational corporation in the world and it was the first company to issue stock. It was a powerful company, possessing quasi-governmental powers, including the ability to wage war, imprison and execute convicts, negotiate treaties, strike its own coins, and establish colonies.
Statistically, the VOC eclipsed all of its rivals in the Asia trade. Between 1602 and 1796 the VOC sent almost a million Europeans to work in the Asia trade on 4,785 ships, and netted for their efforts more than 2.5 million tons of Asian trade goods. By contrast, the rest of Europe combined sent only 882,412 people from 1500 to 1795, and the fleet of the English (later British) East India Company, the VOC's nearest competitor, was a distant second to its total traffic with 2,690 ships and a mere one-fifth the tonnage of goods carried by the VOC. The VOC enjoyed huge profits from its spice monopoly through most of the 17th century.
Having been set up in 1602, to profit from the Malukan spice trade, in 1619 the VOC established a capital in the port city of Jayakarta and changed the city name into Batavia (now Jakarta). Over the next two centuries the Company acquired additional ports as trading bases and safeguarded their interests by taking over surrounding territory. It remained an important trading concern and paid an 18% annual dividend for almost 200 years.
Weighed down by corruption in the late 18th century, the Company went bankrupt and was formally dissolved in 1800, its possessions and the debt being taken over by the government of the Dutch Batavian Republic. The VOC's territories became the Dutch East Indies and were expanded over the course of the 19th century to include the whole of the Indonesian archipelago, and in the 20th century would form the Republic of Indonesia.
History.
Background.
During the 16th century, the spice trade was dominated by the Portuguese who used Lisbon as a staple port. Before the Dutch Revolt, Antwerp had played an important role as a distribution centre in northern Europe. However, after 1591 the Portuguese used an international syndicate of the German Fuggers and Welsers, and Spanish and Italian firms, that used Hamburg as its northern staple port to distribute their goods, thereby cutting Dutch merchants out of the trade.
At the same time, the Portuguese trade system was unable to increase supply to satisfy growing demand, in particular the demand for pepper. Demand for spices was relatively inelastic, and therefore each lag in the supply of pepper caused a sharp rise in pepper prices.
In addition, as the Portuguese crown had been united in a personal union with the Spanish crown in 1580, with which the Dutch Republic was at war, the Portuguese Empire became an appropriate target for Dutch military incursions. These three factors motivated Dutch merchants to enter the intercontinental spice trade themselves. Further, a number of Dutchmen like Jan Huyghen van Linschoten and Cornelis de Houtman obtained first hand knowledge of the "secret" Portuguese trade routes and practices, thereby providing opportunity.
The stage was thus set for Houtman's 1595 four-ship exploratory expedition to Banten, the main pepper port of West Java, where they clashed with both the Portuguese and indigenous Indonesians. Houtman's expedition then sailed east along the north coast of Java, losing twelve crew to a Javanese attack at Sidayu and killing a local ruler in Madura. Half the crew were lost before the expedition made it back to the Netherlands the following year, but with enough spices to make a considerable profit.
In 1598, an increasing number of fleets were sent out by competing merchant groups from around the Netherlands. Some fleets were lost, but most were successful, with some voyages producing high profits. In March 1599, a fleet of eight ships under Jacob van Neck was the first Dutch fleet to reach the 'Spice Islands' of Maluku, the source of pepper, cutting out the Javanese middlemen. The ships returned to Europe in 1599 and 1600 and the expedition made a 400 percent profit.
In 1600, the Dutch joined forces with the Muslim Hituese on Ambon Island in an anti-Portuguese alliance, in return for which the Dutch were given the sole right to purchase spices from Hitu. Dutch control of Ambon was achieved when the Portuguese surrendered their fort in Ambon to the Dutch-Hituese alliance. In 1613, the Dutch expelled the Portuguese from their Solor fort, but a subsequent Portuguese attack led to a second change of hands; following this second reoccupation, the Dutch once again captured Solor, in 1636.
East of Solor on the island of Timor, Dutch advances were halted by an autonomous and powerful group of Portuguese Eurasians called the Topasses. They remained in control of the Sandalwood trade and their resistance lasted throughout the 17th and 18th century, causing Portuguese Timor to remain under the Portuguese sphere of control.
Formation (1602).
At the time, it was customary for a company to be set up only for the duration of a single voyage, and to be liquidated upon the return of the fleet. Investment in these expeditions was a very high-risk venture, not only because of the usual dangers of piracy, disease and shipwreck, but also because the interplay of inelastic demand and relatively elastic supply of spices could make prices tumble at just the wrong moment, thereby ruining prospects of profitability. To manage such risk the forming of a cartel to control supply would seem logical. The English had been the first to adopt this approach, by bundling their resources into a monopoly enterprise, the English East India Company in 1600, thereby threatening their Dutch competitors with ruin.
In 1602 the Dutch government followed suit, sponsoring the creation of a single "United East Indies Company" that was also granted monopoly over the Asian trade. The charter of the new company empowered it to build forts, maintain armies, and conclude treaties with Asian rulers. It provided for a venture that would continue for 21 years, with a financial accounting only at the end of each decade.
In 1603, the first permanent Dutch trading post in Indonesia was established in Banten, West Java and in 1611, another was established at Jayakarta (later "Batavia" and then "Jakarta"). In 1610, the VOC established the post of Governor General to more firmly control their affairs in Asia. To advise and control the risk of despotic Governors General, a Council of the Indies ("Raad van Indië") was created. The Governor General effectively became the main administrator of the VOC's activities in Asia, although the "Heeren XVII", a body of 17 shareholders representing different chambers, continued to officially have overall control.
VOC headquarters were located in Ambon during the tenures of the first three Governors General (1610–1619), but it was not a satisfactory location. Although it was at the centre of the spice production areas, it was far from the Asian trade routes and other VOC areas of activity ranging from Africa to India to Japan. A location in the west of the archipelago was thus sought; the Straits of Malacca were strategic, but had become dangerous following the Portuguese conquest and the first permanent VOC settlement in Banten was controlled by a powerful local ruler and subject to stiff competition from Chinese and English traders.
In 1604, a second English East India Company voyage commanded by Sir Henry Middleton reached the islands of Ternate, Tidore, Ambon and Banda; in Banda, they encountered severe VOC hostility, which saw the beginning of Anglo-Dutch competition for access to spices. From 1611 to 1617, the English established trading posts at Sukadana (southwest Kalimantan), Makassar, Jayakarta and Jepara in Java, and Aceh, Pariaman and Jambi in Sumatra which threatened Dutch ambitions for a monopoly on East Indies trade.
Diplomatic agreements in Europe in 1620 ushered in a period of co-operation between the Dutch and the English over the spice trade.
This ended with a notorious, but disputed incident, known as the 'Amboyna massacre', where ten Englishmen were arrested, tried and beheaded for conspiracy against the Dutch government. Although this caused outrage in Europe and a diplomatic crisis, the English quietly withdrew from most of their Indonesian activities (except trading in Bantam) and focused on other Asian interests.
Growth.
In 1619, Jan Pieterszoon Coen was appointed Governor-General of the VOC. He saw the possibility of the VOC becoming an Asian power, both political and economic. On 30 May 1619, Coen, backed by a force of nineteen ships, stormed Jayakarta driving out the Banten forces; and from the ashes established Batavia as the VOC headquarters. In the 1620s almost the entire native population of the Banda Islands was driven away, starved to death, or killed in an attempt to replace them with Dutch plantations. These plantations were used to grow cloves and nutmeg for export.
Coen hoped to settle large numbers of Dutch colonists in the East Indies, but implementation of this policy never materialised, mainly because very few Dutch were willing to emigrate to Asia.
Another of Coen's ventures was more successful. A major problem in the European trade with Asia at the time was that the Europeans could offer few goods that Asian consumers wanted, except silver and gold. European traders therefore had to pay for spices with the precious metals, and this was in short supply in Europe, except for Spain and Portugal. The Dutch and English had to obtain it by creating a trade surplus with other European countries. Coen discovered the obvious solution for the problem: to start an intra-Asiatic trade system, whose profits could be used to finance the spice trade with Europe. In the long run this obviated the need for exports of precious metals from Europe, though at first it required the formation of a large trading-capital fund in the Indies. The VOC reinvested a large share of its profits to this end in the period up to 1630.
The VOC traded throughout Asia. Ships coming into Batavia from the Netherlands carried supplies for VOC settlements in Asia. Silver and copper from Japan were used to trade with India and China for silk, cotton, porcelain, and textiles. These products were either traded within Asia for the coveted spices or brought back to Europe. The VOC was also instrumental in introducing European ideas and technology to Asia. The Company supported Christian missionaries and traded modern technology with China and Japan. A more peaceful VOC trade post on Dejima, an artificial island off the coast of Nagasaki, was for more than two hundred years the only place where Europeans were permitted to trade with Japan. When the VOC tried to military force Ming dynasty China to open up to Dutch trade, the Chinese defeated the Dutch in a war over the Penghu islands from 1623-1624 and forced the VOC to abandon Penghu for Taiwan. The Chinese defeated the VOC again at the Battle of Liaoluo Bay in 1633.
The Vietnamese Nguyen Lords defeated the VOC in a 1643 battle during the Trịnh–Nguyễn War, blowing up a Dutch ship. The Cambodians defeated the VOC in a war from 1643-44 on the Mekong River.
In 1640, the VOC obtained the port of Galle, Ceylon, from the Portuguese and broke the latter's monopoly of the cinnamon trade. In 1658, Gerard Pietersz. Hulft laid siege to Colombo, which was captured with the help of King Rajasinghe II of Kandy. By 1659, the Portuguese had been expelled from the coastal regions, which were then occupied by the VOC, securing for it the monopoly over cinnamon.
To prevent the Portuguese or the English from ever recapturing Sri Lanka, the VOC went on to conquer the entire Malabar Coast from the Portuguese, almost entirely driving them from the west coast of India. When news of a peace agreement between Portugal and the Netherlands reached Asia in 1663, Goa was the only remaining Portuguese city on the west coast.
In 1652, Jan van Riebeeck established an outpost at the Cape of Good Hope (the southwestern tip of Africa, currently in Cape Town, South Africa) to re-supply VOC ships on their journey to East Asia. This post later became a full-fledged colony, the Cape Colony, when more Dutch and other Europeans started to settle there.
VOC trading posts were also established in Persia, Bengal, Malacca, Siam, Canton, Formosa (now Taiwan), as well as the Malabar and Coromandel coasts in India. In 1662, however, Koxinga expelled the Dutch from Taiwan ("see" History of Taiwan).
In 1663, the VOC signed "Painan Treaty" with several local lords in the Painan area that were revolting against the Aceh Sultanate. The treaty resulted in VOC to build a trading post in the area and eventually monopolise the trade there, especially in gold trade.
By 1669, the VOC was the richest private company the world had ever seen, with over 150 merchant ships, 40 warships, 50,000 employees, a private army of 10,000 soldiers, and a dividend payment of 40% on the original investment.
Many of the VOC employees inter-mixed with the indigenous peoples and expanded the population of Indos in pre-colonial history 
Reorientation.
Around 1670, two events caused the growth of VOC trade to stall. In the first place, the highly profitable trade with Japan started to decline. The loss of the outpost on Formosa to Koxinga in the 1662 Siege of Fort Zeelandia and related internal turmoil in China (where the Ming dynasty was being replaced with the Qing dynasty) brought an end to the silk trade after 1666. Though the VOC substituted Bengali for Chinese silk other forces affected the supply of Japanese silver and gold. The shogunate enacted a number of measures to limit the export of these precious metals, in the process limiting VOC opportunities for trade, and severely worsening the terms of trade. Therefore, Japan ceased to function as the lynchpin of the intra-Asiatic trade of the VOC by 1685.
Even more importantly, the Third Anglo-Dutch War temporarily interrupted VOC trade with Europe. This caused a spike in the price of pepper, which enticed the English East India Company (EIC) to aggressively enter this market in the years after 1672. Previously, one of the tenets of the VOC pricing policy was to slightly over-supply the pepper market, so as to depress prices below the level where interlopers were encouraged to enter the market (instead of striving for short-term profit maximisation). The wisdom of such a policy was illustrated when a fierce price war with the EIC ensued, as that company flooded the market with new supplies from India. In this struggle for market share, the VOC (which had much larger financial resources) could wait out the EIC. Indeed by 1683, the latter came close to bankruptcy; its share price plummeted from 600 to 250; and its president Josiah Child was temporarily forced from office.
However, the writing was on the wall. Other companies, like the French East India Company and the Danish East India Company also started to make inroads on the Dutch system. The VOC therefore closed the heretofore flourishing open pepper emporium of Bantam by a treaty of 1684 with the Sultan. Also, on the Coromandel Coast, it moved its chief stronghold from Pulicat to Negapatnam, so as to secure a monopoly on the pepper trade at the detriment of the French and the Danes. However, the importance of these traditional commodities in the Asian-European trade was diminishing rapidly at the time. The military outlays that the VOC needed to make to enhance its monopoly were not justified by the increased profits of this declining trade.
Nevertheless, this lesson was slow to sink in and at first the VOC made the strategic decision to improve its military position on the Malabar Coast (hoping thereby to curtail English influence in the area, and end the drain on its resources from the cost of the Malabar garrisons) by using force to compel the Zamorin of Calicut to submit to Dutch domination. In 1710, the Zamorin was made to sign a treaty with the VOC undertaking to trade exclusively with the VOC and expel other European traders. For a brief time, this appeared to improve the Company's prospects. However, in 1715, with EIC encouragement, the Zamorin renounced the treaty. Though a Dutch army managed to suppress this insurrection temporarily, the Zamorin continued to trade with the English and the French, which led to an appreciable upsurge in English and French traffic. The VOC decided in 1721 that it was no longer worth the trouble to try to dominate the Malabar pepper and spice trade. A strategic decision was taken to scale down the Dutch military presence and in effect yield the area to EIC influence.
The 1741 Battle of Colachel by Nairs and Ezhava Chekavar warriors of Travancore under Raja Marthanda Varma defeated the Dutch. The Dutch commander Captain Eustachius De Lannoy was captured. Marthanda Varma agreed to spare the Dutch captain's life on condition that he joined his army and trained his soldiers on modern lines. This defeat in the Travancore-Dutch War is considered the earliest example of an organised Asian power overcoming European military technology and tactics; and it signalled the decline of Dutch power in India.
The attempt to continue as before as a low volume-high profit business enterprise with its core business in the spice trade had therefore failed. The Company had however already (reluctantly) followed the example of its European competitors in diversifying into other Asian commodities, like tea, coffee, cotton, textiles, and sugar. These commodities provided a lower profit margin and therefore required a larger sales volume to generate the same amount of revenue. This structural change in the commodity composition of the VOC's trade started in the early 1680s, after the temporary collapse of the EIC around 1683 offered an excellent opportunity to enter these markets. The actual cause for the change lies, however, in two structural features of this new era.
In the first place, there was a revolutionary change in the tastes affecting European demand for Asian textiles, and coffee and tea, around the turn of the 18th century. Secondly, a new era of an abundant supply of capital at low interest rates suddenly opened around this time. The second factor enabled the Company to easily finance its expansion in the new areas of commerce. Between the 1680s and 1720s, the VOC was therefore able to equip and man an appreciable expansion of its fleet, and acquire a large amount of precious metals to finance the purchase of large amounts of Asian commodities, for shipment to Europe. The overall effect was to approximately double the size of the company.
The tonnage of the returning ships rose by 125 percent in this period. However, the Company's revenues from the sale of goods landed in Europe rose by only 78 percent. This reflects the basic change in the VOC's circumstances that had occurred: it now operated in new markets for goods with an elastic demand, in which it had to compete on an equal footing with other suppliers. This made for low profit margins. Unfortunately, the business information systems of the time made this difficult to discern for the managers of the company, which may partly explain the mistakes they made from hindsight. This lack of information might have been counteracted (as in earlier times in the VOC's history) by the business acumen of the directors. Unfortunately by this time these were almost exclusively recruited from the political "regent" class, which had long since lost its close relationship with merchant circles.
Low profit margins in themselves don't explain the deterioration of revenues. To a large extent the costs of the operation of the VOC had a "fixed" character (military establishments; maintenance of the fleet and such). Profit levels might therefore have been maintained if the increase in the scale of trading operations that in fact took place, had resulted in economies of scale. However, though larger ships transported the growing volume of goods, labour productivity did not go up sufficiently to realise these. In general the Company's overhead rose in step with the growth in trade volume; declining gross margins translated directly into a decline in profitability of the invested capital. The era of expansion was one of "profitless growth".
Concretely: "[t]he long-term average annual profit in the VOC's 1630–70 'Golden Age' was 2.1 million guilders, of which just under half was distributed as dividends and the remainder reinvested. The long-term average annual profit in the 'Expansion Age' (1680–1730) was 2.0 million guilders, of which three-quarters was distributed as dividend and one-quarter reinvested. In the earlier period, profits averaged 18 percent of total revenues; in the latter period, 10 percent. The annual return of invested capital in the earlier period stood at approximately 6 percent; in the latter period, 3.4 percent."
Nevertheless, in the eyes of investors the VOC did not do too badly. The share price hovered consistently around the 400 mark from the mid-1680s (excepting a hiccup around the Glorious Revolution in 1688), and they reached an all-time high of around 642 in the 1720s. VOC shares then yielded a return of 3.5 percent, only slightly less than the yield on Dutch government bonds.
Decline.
However, from there on the fortunes of the VOC started to decline. Five major problems, not all of equal weight, can be used to explain its decline in the next fifty years to 1780.
Despite of all this, the VOC in 1780 remained an enormous operation. Its capital in the Republic, consisting of ships and goods in inventory, totalled 28 million guilders; its capital in Asia, consisting of the liquid trading fund and goods en route to Europe, totalled 46 million guilders. Total capital, net of outstanding debt, stood at 62 million guilders. The prospects of the company at this time therefore need not have been hopeless, had one of the many plans to reform it been taken successfully in hand. However, then the Fourth Anglo-Dutch War intervened. British attacks in Europe and Asia reduced the VOC fleet by half; removed valuable cargo from its control; and devastated its remaining power in Asia. The direct losses of the VOC can be calculated at 43 million guilders. Loans to keep the company operating reduced its net assets to zero.
From 1720 on, the market for sugar from Indonesia declined as the competition from cheap sugar from Brazil increased. European markets became saturated. Dozens of Chinese sugar traders went bankrupt which led to massive unemployment, which in turn led to gangs of unemployed coolies. The Dutch government in Batavia did not adequately respond to these problems. In 1740, rumours of deportation of the gangs from the Batavia area led to widespread rioting. The Dutch military searched houses of Chinese in Batavia for weapons. When a house accidentally burnt down, military and impoverished citizens started slaughtering and pillaging the Chinese community. This massacre of the Chinese was deemed sufficiently serious for the board of the VOC to start an official investigation into the Government of the Dutch East Indies for the first time in its history.
After the Fourth Anglo-Dutch War, the VOC was a financial wreck, and after vain attempts by the provincial States of Holland and Zeeland to reorganise it, was nationalised on 1 March 1796 by the new Batavian Republic. Its charter was renewed several times, but allowed to expire on 31 December 1799. Most of the possessions of the former VOC were subsequently occupied by Great Britain during the Napoleonic wars, but after the new United Kingdom of the Netherlands was created by the Congress of Vienna, some of these were restored to this successor state of the old Dutch Republic by the Anglo-Dutch Treaty of 1814.
European discovery of Australia.
In terms of world history of geography and exploration, the VOC can be credited with putting most of Australia's coast (then Hollandia Nova and other names) on the world map, between 1606 and 1756. An Australian vintner has used the VOC logo since the late 20th century, having re-registered the company's name for the purpose.
Organization.
The VOC had two types of shareholders: the "participanten", who could be seen as non-managing members, and the 76 "bewindhebbers" (later reduced to 60) who acted as managing directors. This was the usual set-up for Dutch joint-stock companies at the time. The innovation in the case of the VOC was, that the liability of not just the "participanten", but also of the "bewindhebbers" was limited to the paid-in capital (usually, "bewindhebbers" had unlimited liability). The VOC therefore was a limited liability company. Also, the capital would be "permanent" during the lifetime of the company. As a consequence, investors that wished to liquidate their interest in the interim could only do this by selling their share to others on the Amsterdam Stock Exchange.
"Confusion of confusions", a 1688 dialogue by the Sephardi Jew Joseph de la Vega analysed the workings of this one-stock exchange.
The VOC consisted of six Chambers ("Kamers") in port cities: Amsterdam, Delft, Rotterdam, Enkhuizen, Middelburg and Hoorn. Delegates of these chambers convened as the Heeren XVII (the Lords Seventeen). They were selected from the "bewindhebber"-class of shareholders.
Of the "Heeren XVII", eight delegates were from the Chamber of Amsterdam (one short of a majority on its own), four from the Chamber of Zeeland, and one from each of the smaller Chambers, while the seventeenth seat was alternatively from the Chamber of Zeeland or rotated among the five small Chambers. Amsterdam had thereby the decisive voice. The Zeelanders in particular had misgivings about this arrangement at the beginning. The fear was not unfounded, because in practice it meant Amsterdam stipulated what happened.
The six chambers raised the start-up capital of the Dutch East India Company:
The raising of capital in Rotterdam did not go so smoothly. A considerable part originated from inhabitants of Dordrecht. Although it did not raise as much capital as Amsterdam or Zeeland, Enkhuizen had the largest input in the share capital of the VOC. Under the first 358 shareholders, there were many small entrepreneurs, who dared to take the risk. The minimum investment in the VOC was 3,000 guilders, which priced the Company's stock within the means of many merchants.
Among the early shareholders of the VOC, immigrants played an important role. Under the 1,143 tenderers were 39 Germans and no fewer than 301 from the Southern Netherlands (roughly present Belgium and Luxembourg, then under Habsburg rule), of whom Isaac le Maire was the largest subscriber with ƒ85,000. VOC's total capitalisation was ten times that of its British rival.
The logo of the VOC consisted of a large capital 'V' with an O on the left and a C on the right leg. It appeared on various corporate items, such as cannons and the coin illustrated above. The first letter of the hometown of the chamber conducting the operation was placed on top (see figure for example of the Amsterdam chamber logo). The flag of the company was orange, white, blue (see Dutch flag) with the company logo embroidered on it.
The "Heeren XVII" (Lords Seventeen) met alternately 6 years in Amsterdam and 2 years in Middelburg. They defined the VOC's general policy and divided the tasks among the Chambers. The Chambers carried out all the necessary work, built their own ships and warehouses and traded the merchandise. The "Heeren XVII" sent the ships' masters off with extensive instructions on the route to be navigated, prevailing winds, currents, shoals and landmarks. The VOC also produced its own charts.
In the context of the Dutch-Portuguese War the company established its headquarters in Batavia, Java (now Jakarta, Indonesia). Other colonial outposts were also established in the East Indies, such as on the Maluku Islands, which include the Banda Islands, where the VOC forcibly maintained a monopoly over nutmeg and mace. Methods used to maintain the monopoly involved extortion and the violent suppression of the native population, including mass murder. In addition, VOC representatives sometimes used the tactic of burning spice trees to force indigenous populations to grow other crops, thus artificially cutting the supply of spices like nutmeg and cloves.
VOC outposts.
Organization and leadership structures were varied as necessary in the various VOC outposts:
"Opperhoofd" is a Dutch word (pl. "Opperhoofden") which literally means 'supreme chief'. In this VOC context, the word is a gubernatorial title, comparable to the English Chief factor, for the chief executive officer of a Dutch "factory" in the sense of trading post, as led by a factor, i.e. agent.
Council of Justice in Batavia.
The Council of Justice in Batavia was the appellate court for all the other VOC Company posts in the VOC empire.
See also.
Other trade companies of the age of the sail
Governors General of the Dutch East India Company
Further reading.
</dl>

</doc>
<doc id="42739" url="http://en.wikipedia.org/wiki?curid=42739" title="Bubble fusion">
Bubble fusion

Bubble fusion, also known as sonofusion, is the non-technical name for a nuclear fusion reaction hypothesized to occur inside extraordinarily large collapsing gas bubbles created in a liquid during acoustic cavitation. Rusi Taleyarkhan and collaborators claimed to have observed evidence of sonofusion in 2002. The claim was quickly surrounded by controversy, including allegations ranging from experimental error to academic fraud. Subsequent publications claiming independent verification of sonofusion were also highly controversial. Eventually, an investigation by Purdue University found that Taleyarkhan had engaged in falsification of independent verification, and had included a student as an author on a paper when he hadn't participated in the research. He was subsequently stripped of his professorship. One of his funders, the Office of Naval Research reviewed the report by Purdue and barred him from federal funding for 28 months.
Original experiments.
US patent 4,333,796, filed by Hugh Flynn in 1978, appears to be the earliest documented reference to a sonofusion-type reaction.
In the March 8, 2002 issue of the peer-reviewed journal "Science", Rusi P. Taleyarkhan and colleagues at the Oak Ridge National Laboratory (ORNL) reported that acoustic cavitation experiments conducted with deuterated acetone (C3D6O) showed measurements of tritium and neutron output consistent with the occurrence of fusion. The neutron emission was also reported to be coincident with the sonoluminescence pulse, a key indicator that its source was fusion caused by the heat and pressure inside the collapsing bubbles.
Oak Ridge failed replication.
The results were so startling, that the Oak Ridge National Laboratory asked two independent researchers, D. Shapira and M. J. Saltmarsh, to repeat the experiment using more sophisticated neutron detection equipment. They reported that the neutron release was consistent with random coincidence. A rebuttal by Taleyarkhan and the other authors of the original report argued that the Shapira and Saltmarsh report failed to account for significant differences in experimental setup, including over an inch of shielding between the neutron detector and the sonoluminescing acetone. According to Taleyarkhan "et al.", when properly considering those differences, the results were consistent with fusion. 
As early as 2002, while experimental work was still in progress, Aaron Galonsky of Michigan State University, in a letter to the journal "Science"
expressed doubts about the claim made by the Taleyarkhan team. In Galonsky's opinion, the observed neutrons were too high in energy to be from a deuterium-deuterium (d-d) fusion reaction. In their response (published on the same page), the Taleyarkhan team provided detailed counter-arguments and concluded that the energy was "reasonably close" to that which was expected from a fusion reaction.
In February 2005 the documentary series "Horizon" commissioned two leading sonoluminescence researchers, Seth Putterman and Kenneth S. Suslick, to reproduce Taleyarkhan's work. Using similar acoustic parameters, deuterated acetone, similar bubble nucleation, and a much more sophisticated neutron detection device, the researchers could find no evidence of a fusion reaction.
Subsequent reports of replication.
In 2004, new reports of bubble fusion were published by the Taleyarkhan group, claiming that the results of previous experiments had been replicated under more stringent experimental conditions. These results differed from the original results in that fusion was claimed to occur over longer times than previously reported. The original report only claimed neutron emission from the initial bubble collapse following bubble nucleation, whereas this report claimed neutron emission many acoustic cycles later.
In July 2005, two of Taleyarkhan's students at Purdue University published evidence confirming the previous result. They used the same acoustic chamber, the same deuterated acetone fluid and a similar bubble nucleation system. In this report, no neutron-sonoluminescence coincidence was attempted. An article in "Nature" raised issues about the validity of the research and complaints from his Purdue colleagues (see full analysis elsewhere in this page). Charges of misconduct were raised, and Purdue University opened an investigation. It concluded in 2008 that Taleyarkhan's name should have appeared in the author list because of his deep involvement in many steps of the research, that he added one author that had not really participated in the paper just to overcome the criticism of one reviewer, and that this was part of an attempt of "an effort to falsify the scientific record by assertion of independent confirmation". The investigation did not address the validity of the experimental results.
In January 2006, a paper published in the journal "Physical Review Letters" by Taleyarkhan in collaboration with researchers from Rensselaer Polytechnic Institute reported statistically significant evidence of fusion. 
In November 2006, in the midst of accusations concerning Taleyarkhan's research standards, two different scientists visited the meta-stable fluids research lab at Purdue University to measure neutrons, using Taleyarkhan's equipment. Dr. Edward R. Forringer and undergraduates David Robbins and Jonathan Martin of LeTourneau University presented two papers at the American Nuclear Society Winter Meeting that reported replication of neutron emission. Their experimental setup was similar to previous experiments in that it used a mixture of deuterated acetone, deuterated benzene, tetrachloroethylene and uranyl nitrate. Notably, however, it operated without an external neutron source and used two types of neutron detectors. They claimed a liquid scintillation detector measured neutron levels at 8 standard deviations above the background level, while plastic detectors measured levels at 3.8 standard deviations above the background. When the same experiment was performed with non-deuterated control liquid, the measurements were within one standard deviation of background, indicating that the neutron production had only occurred during cavitation of the deuterated liquid. William M. Bugg, emeritus physics professor at the University of Tennessee also traveled to Taleyarkhan's lab to repeat the experiment with his equipment. He also reported neutron emission, using plastic neutron detectors. Taleyarkhan claimed these visits counted as independent replications by experts, but Forringer later recognized that he was not an expert, and Bugg later said that Taleyarkhan performed the experiments and he had only watched.
"Nature" report.
Reports as spectacular as the above raise a lot of doubt. In March 2006, "Nature" published a special report that called into question the validity of the results of the Purdue experiments. The report quotes Brian Naranjo of the University of California, Los Angeles to the effect that neutron energy spectrum reported in the 2006 paper by Taleyarkhan, et al. was statistically inconsistent with neutrons produced by the proposed fusion reaction and instead highly consistent with neutrons produced by the radioactive decay of Californium 252, an isotope commonly used as a laboratory neutron source . 
The response of Taleyarkhan "et al.", published in "Physical Review Letters", attempts to refute Naranjo's hypothesis as to the cause of the neutrons detected.
Tsoukalas, head of the School of Nuclear Engineering at Purdue, and several of his colleagues at Purdue, had convinced Taleyarkhan to move to Purdue and attempt a joint replication. In the 2006 "Nature" report they detail several troubling issues when trying to collaborate with Taleyarkhan. He reported positive results from certain set of raw data, but his colleagues had also examined that set and it only contained negative results. He never showed his colleagues the raw data corresponding to the positive results, despite several requests. He moved the equipment from a shared laboratory to his own laboratory, thus impeding review by his colleagues, and he didn't give any advance warning or explanation for the move. Taleyarkhan convinced his colleagues that they shouldn't publish a paper with their negative results. Taleyarkhan then insisted that the university's press release presented his experiment as "peer-reviewed" and "independent", when the co-authors were working in his laboratory under his supervision, and his peers in the faculty were not allowed to review the data. In summary, Taleyarkhan's colleagues at Purdue said he placed obstacles to peer review of his experiments, and they had serious doubts about the validity of the research.
"Nature" also revealed that the process of anonymous peer-review had not been followed, and that the journal "Nuclear Engineering and Design" was not independent from the authors. Taleyarkhan was co-editor of the journal, and the paper was only peer-reviewed by his co-editor, with Taleyarkhan's knowledge.
In 2002 Taleyarkhan filed a patent application on behalf of the United States Department of Energy, while working in Oak Ridge. "Nature" reported that the patent had been rejected in 2005 by the US Patent Office. The examiner called the experiment a variation of discredited cold fusion, found that there was "no reputable evidence of record to support any allegations or claims that the invention is capable of operating as indicated", and found that there was not enough detail for others to replicate the invention. The field of fusion suffered from many flawed claims, thus the examiner asked for additional proof that the radiation was generated from fusion and not from other sources. An appeal was not filed because the Department of Energy had dropped the claim in December 2005.
Doubts prompt investigation.
Doubts among Purdue University's Nuclear Engineering faculty as to whether the positive results reported from sonofusion experiments conducted there were truthful prompted the university to initiate a review of the research, conducted by Purdue's Office of the Vice President for Research. In a March 9, 2006 article entitled "Evidence for bubble fusion called into question", "Nature" interviewed several of Taleyarkhan's colleagues who suspected something was amiss.
On February 7, 2007, the Purdue University administration determined that "the evidence does not support the allegations of research misconduct and that no further investigation of the allegations is warranted". Their report also stated that "vigorous, open debate of the scientific merits of this new technology is the most appropriate focus going forward." In order to verify that the investigation was properly conducted, House Representative Brad Miller requested full copies of its documents and reports by March 30, 2007. His congressional report concluded that "Purdue deviated from its own procedures in investigating this case and did not conduct a thorough investigation"; in response, Purdue announced that it would re-open its investigation.
In June 2008, a multi-institutional team including Taleyarkhan published a paper in Nuclear Engineering and Design to "clear up misconceptions generated by a webposting of UCLA which served as the basis for the "Nature" article of March 2006", according to a press release.
On July 18, 2008, Purdue University announced that a committee with members from five institutions had investigated 12 allegations of research misconduct against Rusi Taleyarkhan. It concluded that two allegations were founded—that Taleyarkhan had claimed independent confirmation of his work when in reality the apparent confirmations were done by Taleyarkhan's former students and was not as "independent" as Taleyarkhan implied, and that Taleyarkhan had included a colleague's name on one of his papers who had not actually been involved in the research ("the sole apparent motivation for the addition of Mr. Butt was a desire to overcome a reviewer's criticism," the report concluded).
Taleyarkhan's appeal of the report's conclusions was rejected. He said the two allegations of misconduct were trivial administrative issues and had nothing to do with the discovery of bubble nuclear fusion or the underlying science, and that "all allegations of fraud and fabrication have been dismissed as invalid and without merit — thereby supporting the underlying science and experimental data as being on solid ground". A researcher questioned by the LA Times said that the report had not clarified whether bubble fusion was real or not, but that the low quality of the papers and the doubts cast by the report had destroyed Taleyarkhan's credibility with the scientific community.
On August 27, 2008 he was stripped of his named Arden Bement Jr. Professorship, and forbidden to be a thesis advisor for graduate students for at least the next 3 years.
Despite the findings against him, Taleyarkhan received a $185,000 grant from the National Science Foundation between September 2008 and August 2009 to investigate bubble fusion. In 2009 the Office of Naval Research debarred him for 28 months, until September 2011, from receiving U.S. Federal Funding. During that period his name was listed in the 'Excluded Parties List' to prevent him from receiving further grants from any government agency.

</doc>
<doc id="42741" url="http://en.wikipedia.org/wiki?curid=42741" title="Municipal Art Society">
Municipal Art Society

The Municipal Art Society of New York, founded in 1893, is a non-profit membership organization that fights for intelligent urban planning, design and preservation through education, dialogue and advocacy in New York City.
On January 20, 2010, MAS relocated from its longtime home in the historic Villard Houses on 457 Madison Avenue to the equally famed Steinway Hall
on West 57th Street (across the street and east of Carnegie Hall). The MAS website currently lists their address as 488 Madison Avenue, Suite 1900.
History.
MAS’s advocacy efforts have shaped the city a great deal since its inception in 1893. Some of their early accomplishments include passing the city's first zoning laws, contributing input to the planning of the city’s subway line, and throughout the city.
By the 1950s, scores of notable Manhattan buildings were lost to redevelopment around the city, and the mission of MAS broadened to include historical preservation. In 1956, the Society successfully lobbied for the passage of the , which for the first time allowed cities to take aesthetics, history, and cultural associations into account for zoning laws. The law, named after longtime MAS board member and chief advocate, Albert S. Bard, provided a legal foundation for the New York City Landmarks Law, enacted in 1965.
In 1965, public outrage over the destruction of Pennsylvania Station and the helped fuel the Society's mission towards preservation. With like-minded groups, they finally succeeded in establishing New York's Landmarks Preservation Commission, and New York's Landmarks Law.
In 2001, after the demise of Trans World Airlines, the original Trans World Flight Center, completed in 1962 and designed by Eero Saarinen, fell into disuse. During this period, the Municipal Art Society succeeded in 2004 in nominating the facility to the National Trust for Historic Preservation’s list of the 11 Most Endangered Places.
In June 2007, MAS released with the Metropolitan Waterfront Alliance a new documentary about the future of the New York waterfront titled City of Water. In September 2007, the Society opened a major exhibition about Jane Jacobs sponsored by the Rockefeller Foundation.
In May 2008, MAS released a series of renderings showing what the unfinished Atlantic Yards project might look like titled "Atlantic Lots".
In October 2008, MAS launched a new initiative to develop bold new ideas for Coney Island titled ImagineConey.

</doc>
<doc id="42742" url="http://en.wikipedia.org/wiki?curid=42742" title="New York City arts organizations">
New York City arts organizations

The City of New York is home to many arts organizations. They include:

</doc>
<doc id="42745" url="http://en.wikipedia.org/wiki?curid=42745" title="Los Angeles Pierce College">
Los Angeles Pierce College

Los Angeles Pierce College, also known as Pierce College and just Pierce, is a community college that serves more than 23,000 students in the northern Chalk Hills of Woodland Hills, a community within the San Fernando Valley region of the city of Los Angeles, California.
The college began with 70 students and 18 faculty members on September 15, 1947. Originally known as the "Clarence W. Pierce School of Agriculture", the institution’s initial focus was crop cultivation and animal husbandry. Nine years later, in 1956, the school was renamed to "Los Angeles Pierce College", retaining the name of its founder, Dr. Pierce, as well as his commitment to agricultural and veterinary study. (Pierce still maintains a 225 acre working farm for hands-on training.)
Campus overview.
Pierce College offers courses on more than 100 subjects in 92 academic disciplines, and has transfer alliances with most of the universities in the state. Students at the school successfully transfer to UC and CSU schools.
Students can pursue any of the 44 Associate’s degrees or 78 Certificates of Achievement the school offers directly.
Pierce College comprises 426 acre amidst a dense metropolis, an area larger than many university campuses, including that of UCLA. The grounds are landscaped with more than 2,200 trees, thousands of roses and a 1.9 acre botanical garden. The Pierce College farm houses small herds of cattle, sheep, goats, pigs, a small poultry flock, as well as a llama and an alpaca for its students to learn from.
Student government.
The Associated Students Organization (ASO) is the official Student Government of the 23,000 students at Los Angeles Pierce College. It has been in existence since the mid-50's and has three branches: Club Council, Senate and Student Court.
John Shepard Stadium.
Besides hosting the Brahmas' football and women's soccer teams, John Shepard Stadium (current capacity 5,500) also has hosted many outdoor professional sporting events in San Fernando Valley history.
From 1976 to 1979, the San Fernando Valley's first professional sports team, the Los Angeles Skyhawks of the American Soccer League, played their home games at the Pierce College stadium.
The Los Angeles Express of the USFL played their last home game here on June 15, 1985. The stadium was expanded to 16,000-person capacity for the game.
Shepard Stadium hosts Nuts for Mutts, an annual dog show and pet fair that raises funds for the New Leash on Life Animal Rescue.
The stadium is also the former home stadium of the San Fernando Valley Quakes men's soccer team, which competed in the USL Premier Development League.
Bond construction.
Since the approval of Los Angeles County Propositions A and AA, Pierce College has been undergoing large-scale renovation.
By 2010 two new “green” complexes now under construction—a 109000 sqft. Center for the Sciences and a 50000 sqft. Student Services Building—will add increased capacity and classrooms and laboratories with enhanced technologies. Other key improvements are increased parking, new infrastructure and roadways, and renovations to existing facilities.
The 2008 passage of Measure J for Jobs will bring further enhancements to the College, with a new Library/Learning Center/Instructional Media Center, and classroom facilities that offer programs focusing on emerging green technologies and new media in the planning stages.
The 2 acre S. Mark Taper Foundation Life Science Botanic Garden, completed in 2007, is a “living classroom” in the middle of campus. It features one of the finest collections of drought-resistant plants in the region.
Pierce College prides itself as an environmentally-forward institution, with a 191-kilowatt solar generation system that has 1,274 photovoltaic panels and a 360-kilowatt, natural gas co-generation system. This project is the largest of its kind to be undertaken by a U.S. community college, yielding around 4.4 million kilowatt-hours of electricity a year and reducing Carbon dioxide emissions by more than 1,500 tons over its operating lifetime. The college also has a water retention pond beneath its soccer field, collecting run-off from the adjacent parking lot. The Los Angeles River is nearby to the north. Under propositions A and AA, a new water reclamation facility is also being planned, and the new facilities will meet rigorous Silver-level guidelines set by the U.S. Green Building Council for Leadership in Energy and Environmental Design.
The campus is home to "Old Trapper's Lodge," California Historical Landmark No. 939, an outsider art environment that pays homage to the pioneer upbringing of its creator, John Ehn. It represents the life work of John Ehn (1897–1981), a self-taught artist who wished to pass on a sense of the Old West, derived from personal experiences, myths, and tall tales. From 1951 to 1981, using his family as models, and incorporating memorabilia, the 'Old Trapper' followed his dreams and visions to create the Lodge and its 'Boot Hill.' The artwork was moved from the original site in Sun Valley, CA, and relocated to the college.
Pierce College is one of the nine colleges of the Los Angeles Community College District, and is accredited through the Western Association of Schools and Colleges, a nationally recognized accrediting agency. It is located at 6201 Winnetka Ave. near Victory Boulevard. The campus is accessible from the LACMTA Orange Line station of the same name.
In April 2010, after securing a $100,000 grant, Pierce College launched a 24-hour student-run online radio station, KPCRadio.com, airing a mix of music, sports and locally produced news and features. The station has a faculty adviser and a staff of 20.
Events.
In April of every year, the Foundation for Pierce College hosts Farmwalk, an outdoor festival including animals, activities, displays, games and music. The Farmwalk also includes face-painting, a petting-zoo and hayrides for children, all to benefit the Pierce College farm.
In October the Foundation sponsors an annual Harvest Festival, featuring tons of pumpkins grown on the Pierce farm, a five-mile (8 km) corn maze, rock climbing, games and rides for the children, a petting zoo, live music and Halloween frights for the whole family.
The College also serves as a Los Angeles County large animal emergency evacuation center. During a slew of fires in Southern California in 2007, Pierce College sheltered and fed more than 150 horses under the direction of the L.A. County Volunteer Equine Response team. The horses were taken in for free at Pierce, and a veterinarian was onsite. Trained volunteers from Pierce's equestrian program assisted the county rescue effort.
Athletics.
Pierce College currently fields 11 athletic teams, which compete in the Western State Conference.
Many athletes receive scholarships to four-year universities after playing at Pierce—and Pierce has some of the top sports facilities in the San Fernando Valley.
In 2009 the Pierce Brahmas won the American Pacific Conference, losing in the first round of bowl playoffs to the National Champs Mt. San Antonio College

</doc>
<doc id="42746" url="http://en.wikipedia.org/wiki?curid=42746" title="Los Angeles Community College District">
Los Angeles Community College District

The Los Angeles Community College District (LACCD) is the community college district serving Los Angeles, California, USA and some of its neighboring cities. Its headquarters are in Downtown Los Angeles. In addition to typical college aged students, the LACCD also serves adults of all ages. Indeed, over half of all LACCD students are older than 25 years of age, and more than a quarter are 35 or older. The Los Angeles Community College District is the largest community college district in the United States and is one of the largest in the world. The LACCD consists of nine colleges and covers an area of more than 882 sqmi.
Board of trustees.
The Los Angeles Community College District is governed by an elected Board of Trustees. s of January 2015[ [update]], the members are Scott J. Svonkin (president); Mike Eng (vice president); Mona Field; Ernest H. Moreno; Nancy Pearlman; Steve Veres.
Board members are elected by voters in the district for terms of four years. Elections are held every two years, with three members being chosen at one election and four members at the other. The President and Vice President of the Board of Trustees are elected by the Board for one-year terms at the annual organizational meeting. A Second Vice president is elected at the discretion of the Board. A student member is elected annually—the term is June 1 through May 31 of each year.
Projects.
The District is modernizing all of its facilities, including all nine of its colleges, through a $6 billion Building Program. The program is funded primarily through bond measures approved by voters in 2001, 2003, and 2008, plus additional funding from the State of California. As of its most recent report, approximately $3.1 billion of the $6 billion has been spent or committed.

</doc>
<doc id="42750" url="http://en.wikipedia.org/wiki?curid=42750" title="Enterprise JavaBeans">
Enterprise JavaBeans

Enterprise JavaBeans (EJB) is a managed, server software for modular construction of enterprise software, and one of several Java APIs. EJB is a server-side software component that encapsulates the business logic of an application. The EJB specification is a subset of the Java EE specification. An EJB web container provides a runtime environment for web related software components, including computer security, Java servlet lifecycle management, transaction processing, and other web services.
Specification.
The EJB specification was originally developed in 1997 by IBM and later adopted by Sun Microsystems (EJB 1.0 and 1.1) in 1999 and enhanced under the Java Community Process as (EJB 2.0), (EJB 2.1), (EJB 3.0), (EJB 3.1) and (EJB 3.2).
The EJB specification intends to provide a standard way to implement the server-side (also called "back-end") 'business' software typically found in enterprise applications (as opposed to 'front-end' user interface software). Such machine code addresses the same types of problems, and solutions to these problems are often repeatedly re-implemented by programmers. Enterprise JavaBeans is intended to handle such common concerns as persistence, transactional integrity, and security in a standard way, leaving programmers free to concentrate on the particular parts of the enterprise software at hand.
General responsibilities.
The EJB specification details how an application server provides the following responsibilities:
Additionally, the Enterprise JavaBean specification defines the roles played by the EJB container and the EJBs as well as how to deploy the EJBs in a container. Note that the current EJB 3.2 specification does not detail how an application server provides persistence (a task delegated to the JPA specification), but instead details how business logic can easily integrate with the persistence services offered by the application server.
Rapid adoption followed by criticism.
The vision was persuasively presented by EJB advocates such as IBM and Sun Microsystems, and Enterprise JavaBeans were quickly adopted by large companies. Problems were quick to appear with initial versions. Some developers felt that the APIs of the EJB standard were far more complex than those developers were used to. An abundance of checked exceptions, required interfaces, and the implementation of the bean class as an abstract class were unusual and counter-intuitive for programmers. The problems that the EJB standard was attempting to address, such as object-relational mapping and transactional integrity, were complex, however many programmers found the APIs to be just as difficult, leading to a perception that EJBs introduced complexity without delivering real benefits.
In addition, businesses found that using EJBs to encapsulate business logic brought a performance penalty. This is because the original specification only allowed for remote method invocation through CORBA (and optionally other protocols), even though the large majority of business applications actually do not require this distributed computing functionality. The EJB 2.0 specification addressed this concern by adding the concept of local interfaces which could be called directly without performance penalties by applications that were not distributed over multiple servers.
The complexity issue continued to hinder EJB's acceptance. Although developer tools made it easy to create and use EJBs by automating most of the repetitive tasks, these tools did not make it any easier to learn how to use the technology. Moreover, a counter-movement had grown up on the grass-roots level among programmers. The main products of this movement were the so-called 'lightweight' (i.e. in comparison to EJB) technologies of Hibernate (for persistence and object-relational mapping) and the Spring Framework (which provided an alternate and far less verbose way to encode business logic). Despite lacking the support of big businesses, these technologies grew in popularity and were adopted by businesses.
Reinventing EJBs.
Gradually an industry consensus emerged that the original EJB specification's primary virtue — enabling transactional integrity over distributed applications — was of limited use to most enterprise applications, and the functionality delivered by simpler frameworks like Spring and Hibernate was more useful.
Accordingly, the EJB 3.0 specification (JSR 220) was a radical departure from its predecessors, following this new paradigm. It shows a clear influence from Spring in its use of plain Java objects, and its support for dependency injection to simplify configuration and integration of heterogeneous systems. Gavin King, the creator of Hibernate, participated in the EJB 3.0 process and is an outspoken advocate of the technology. Many features originally in Hibernate were incorporated in the Java Persistence API, the replacement for entity beans in EJB 3.0. The EJB 3.0 specification relies heavily on the use of annotations (a feature added to the Java language with its 5.0 release) and convention over configuration to enable a much less verbose coding style.
Accordingly, in practical terms EJB 3.0 is much more lightweight and nearly a completely new API, bearing little resemblance to the previous EJB specifications.
Example.
The following shows a basic example of what an EJB looks like in code:
The above defines a service class for persisting a Customer object (via O/R mapping). The EJB takes care of managing the persistence context and the addCustomer() method is transactional and thread-safe by default. As demonstrated, the EJB focuses only on business logic and persistence and knows nothing about any particular presentation.
Such an EJB can be used by a class in e.g. the web layer as follows:
The above defines a JavaServer Faces (JSF) backing bean in which the EJB is injected by means of the @EJB annotation. Its addCustomer method is typically bound to some UI component, like a button. Contrary to the EJB, the backing bean does not contain any business logic or persistence code, but delegates such concerns to the EJB. The backing bean does know about a particular presentation, of which the EJB had no knowledge.
Types of Enterprise Beans.
An EJB container holds two major types of beans:
Session beans.
Stateful Session Beans.
Stateful Session Beans are business objects having state: that is, they keep track of which calling client they are dealing with throughout a session and thus access to the bean instance is strictly limited to only one client at a time. If concurrent access to a single bean is attempted anyway the container serializes those requests, but via the @AccessTimeout annotation the container can instead throw an exception. Stateful session beans' state may be persisted (passivated) automatically by the container to free up memory after the client hasn't accessed the bean for some time. The JPA extended persistence context is explicitly supported by Stateful Session Beans.
Stateless Session Beans.
Stateless Session Beans are business objects that do not have state associated with them. However, access to a single bean instance is still limited to only one client at a time, concurrent access to the bean is prohibited. If concurrent access to a single bean is attempted, the container simply routes each request to a different instance. This makes a stateless session bean automatically thread-safe. Instance variables can be used during a single method call from a client to the bean, but the contents of those instance variables are not guaranteed to be preserved across different client method calls. Instances of Stateless Session beans are typically pooled. If a second client accesses a specific bean right after a method call on it made by a first client has finished, it might get the same instance. The lack of overhead to maintain a conversation with the calling client makes them less resource-intensive than stateful beans. 
Singleton Session Beans.
Singleton Session Beans are business objects having a global shared state within a JVM. Concurrent access to the one and only bean instance can be controlled by the container (Container-managed concurrency, CMC) or by the bean itself (Bean-managed concurrency, BMC). CMC can be tuned using the @Lock annotation, that designates whether a read lock or a write lock will be used for a method call. Additionally, Singleton Session Beans can explicitly request to be instantiated when the EJB container starts up, using the @Startup annotation. 
Message driven beans.
Message Driven Beans are business objects whose execution is triggered by messages instead of by method calls. The Message Driven Bean is used among others to provide a high level ease-of-use abstraction for the lower level JMS (Java Message Service) specification. It may subscribe to JMS message queues or message topics, which typically happens via the activationConfig attribute of the @MessageDriven annotation. They were added in EJB to allow event-driven processing. Unlike session beans, an MDB does not have a client view (Local/Remote/No-interface), i. e. clients cannot look-up an MDB instance. An MDB just listens for any incoming message on, for example, a JMS queue or topic and processes them automatically. Only JMS support is required by the Java EE spec, but Message Driven Beans can support other messaging protocols. Such protocols may be asynchronous but can also be synchronous. Since session beans can also be synchronous or asynchronous, the prime difference between session- and message driven beans is not the synchronicity, but the difference between (object oriented) method calling and messaging. 
Execution.
EJBs are deployed in an EJB container, typically within an application server. The specification describes how an EJB interacts with its container and how client code interacts with the container/EJB combination. The EJB classes used by applications are included in the package. (The package is a service provider interface used only by EJB container implementations.)
Clients of EJBs do not instantiate those beans directly via Java's new operator, but instead have to obtain a reference via the EJB container. This reference is usually not a reference to the implementation bean itself, but to a proxy, which either dynamically implements the local or remote business interface that the client requested or dynamically implements a sub-type of the actual bean. The proxy can then be directly cast to the interface or bean. A client is said to have a 'view' on the EJB, and the local interface, remote interface and bean type itself respectively correspond with the local view, remote view and no-interface view.
This proxy is needed in order to give the EJB container the opportunity to transparently provide cross-cutting (AOP-like) services to a bean like transactions, security, interceptions, injections, remoting, etc.
E.g. a client invokes a method on a proxy, which will then first start a transaction with the help of the EJB container and then call the actual bean method. When the actual bean method returns, the proxy ends the transaction (i.e. by committing it or doing a rollback) and transfers control back to the client.
Transactions.
EJB containers must support both container managed ACID transactions and bean managed transactions.
Container-managed transactions (CMT) are by default active for calls to session beans. That is, no explicit configuration is needed. This behavior may be declaratively tuned by the bean via annotations and if needed such configuration can later be overridden in the deployment descriptor. Tuning includes switching off transactions for the whole bean or specific methods, or requesting alternative strategies for transaction propagation and starting or joining a transaction. Such strategies mainly deal with what should happen if a transaction is or isn't already in progress at the time the bean is called. The following variations are supported:
Alternatively, the bean can also declare via an annotation that it wants to handle transactions programmatically via the JTA API. This mode of operation is called Bean Managed Transactions (BMT), since the bean itself handles the transaction instead of the container.
Events.
JMS (Java Message Service) is used to send messages from beans to clients, to let clients receive asynchronous messages from these beans. 
MDBs can be used to receive messages from clients asynchronously using either a JMS 
Queue or a Topic.
Naming and directory services.
As an alternative to injection, clients of an EJB can obtain a reference to the session bean's proxy object (the EJB stub) using Java Naming and Directory Interface (JNDI). This alternative can be used in cases where injection is not available, such as in non-managed code or standalone remote Java SE clients, or when it's necessary to programmatically determine which bean to obtain.
JNDI names for EJB session beans are assigned by the EJB container via the following scheme:
"(entries in square brackets denote optional parts)"
A single bean can be obtained by any name matching the above patterns, depending on the 'location' of the client. Clients in the same module as the required bean can use the module scope and larger scopes, clients in the same application as the required bean can use the app scope and higher, etc.
E.g. code running in the same module as the CustomerService bean "(as given by the example shown earlier in this article)" could use the following code to obtain a (local) reference to it:
Remoting/distributed execution.
EJB session beans have elaborate support for remoting.
For communication with a client that's also written in the Java programming language a session bean can expose a remote-view via an @Remote annotated interface. This allows those beans to be called from clients in other JVMs which themselves may be located on other (remote) systems. From the point of view of the EJB container, any code in another JVM is remote.
Stateless- and Singleton session beans may also expose a "web service client view" for remote communication via WSDL and SOAP or plain XML. This follows the JAX-RPC and JAX-WS specifications. JAX-RPC support however is proposed for future removal. To support JAX-WS, the session bean is annotated with the @WebService annotation, and methods that are to be exposed remotely with the @WebMethod annotation.
Although the EJB specification does not mention exposure as RESTful web services in any way and has no explicit support for this form of communication, the JAX-RS specification does explicitly support EJB. Following the JAX-RS spec, Stateless- and Singleton session beans can be root resources via the @Path annotation and EJB business methods can be mapped to resource methods via the @GET, @PUT, @POST and @DELETE annotations. This however does not count as a "web service client view", which is used exclusively for JAX-WS and JAX-RPC.
Communication via web services is typical for clients not written in the Java programming language, but is also convenient for Java clients who have trouble reaching the EJB server via a firewall. Additionally, web service based communication can be used by Java clients to circumvent the arcane and ill-defined requirements for the so-called "client-libraries"; a set of jar files that a Java client must have on its class-path in order to communicate with the remote EJB server. These client-libraries potentially conflict with libraries the client may already have (for instance, if the client itself is also a full Java EE server) and such a conflict is deemed to be a very hard or impossible to resolve.
Message Driven Beans have no specific support for remoting, but being listeners to end-points (e.g. JMS queues) they are implicitly remote components by virtue of the properties of whatever type of end-point they are listening to.
Security.
The EJB Container is responsible for ensuring the client code has sufficient access rights to an EJB. Security aspects can be declaratively applied to an EJB via annotations.
Legacy.
Home interfaces and required business interface.
With EJB 2.1 and earlier, each EJB had to provide a Java implementation class and two Java interfaces. The EJB container created instances of the Java implementation class to provide the EJB implementation. The Java interfaces were used by client code of the EJB.
The two interfaces, referred to as the "Home" and the "Remote" interface, specified the signatures of the EJB's remote methods. The methods were split into two groups:
Required deployment descriptor.
With EJB 2.1 and earlier, the EJB specification required a deployment descriptor to be present. This was needed to implement a mechanism that allowed EJBs to be deployed in a consistent manner regardless of the specific EJB platform that was chosen. Information about how the bean should be deployed (such as the name of the home or remote interfaces, whether and how to store the bean in a database, etc.) had to be specified in the deployment descriptor.
The deployment descriptor is an XML document having an entry for each EJB to be deployed. This XML document specifies the following information for each EJB:
Old EJB containers from many vendors required more deployment information than that in the EJB specification. They would require the additional information as separate XML files, or some other configuration file format. An EJB platform vendor generally provided their own tools that would read this deployment descriptor, and possibly generated a set of classes that would implement the now deprecated Home and Remote interfaces.
Since EJB 3.0 (), the XML descriptor is replaced by Java annotations set in the Enterprise Bean implementation (at source level), although it is still possible to use an XML descriptor instead of (or in addition to) the annotations. If an XML descriptor and annotations are both applied to the same attribute within an Enterprise Bean, the XML definition overrides the corresponding source-level annotation, although some XML elements can also be additive (e.g., an activation-config-property in XML with a different name than already defined via an @ActivationConfigProperty annotation will be added instead of replacing all existing properties).
Container variations.
Starting with EJB 3.1, the EJB specification defines two variants of the EJB container; a full version and a limited version. The limited version adheres to a proper subset of the specification called EJB 3.1 Lite and is part of Java EE 6's web profile (which is itself a subset of the full Java EE 6 specification).
EJB 3.1 Lite excludes support for the following features: 
EJB 3.2 Lite excludes less features. Particularly it no longer excludes @Asynchronous and @Schedule/@Timeout, but for @Schedule it does not support the "persistent" attribute that full EJB 3.2 does support. The complete excluded list for EJB 3.2 Lite is:
Version history.
EJB 3.2, final release (2013-05-28).
. Enterprise JavaBeans 3.2 was a relatively minor release that mainly contained specification clarifications and lifted some restrictions that were imposed by the spec but over time appeared to serve no real purpose. A few existing full EJB features were also demanded to be in EJB 3 lite and functionality that was proposed to be pruned in EJB 3.1 was indeed pruned (made optional).
The following features were added:
EJB 3.1, final release (2009-12-10).
. The purpose of the Enterprise JavaBeans 3.1 specification is to further simplify the EJB architecture by reducing its complexity from the developer's point of view, while also adding new functionality in response to the needs of the community:
EJB 3.0, final release (2006-05-11).
 - "Major changes":
This release made it much easier to write EJBs, using 'annotations' rather than the complex 'deployment descriptors' used in version 2.x. The use of home and remote interfaces and the ejb-jar.xml file were also no longer required in this release, having been replaced with a business interface and a bean that implements the interface.
EJB 2.1, final release (2003-11-24).
 - "Major changes":
EJB 2.0, final release (2001-08-22).
 - "Major changes":
"Overall goals":
EJB 1.1, final release (1999-12-17).
"Major changes":
"Goals" for Release 1.1:
EJB 1.0 (1998-03-24).
Announced at JavaOne 1998, Sun's third Java developers conference (March 24 through 27)
"Goals" for Release 1.0:

</doc>
<doc id="42751" url="http://en.wikipedia.org/wiki?curid=42751" title="J. Michael Straczynski">
J. Michael Straczynski

Joseph Michael Straczynski (; born July 17, 1954), known professionally as J. Michael Straczynski and informally as Joe Straczynski or jms, is an American writer and producer. He works in films, television series, novels, short stories, comic books, radio dramas and other media. Straczynski is a playwright, former journalist, and author of "The Complete Book of Scriptwriting". He was the creator and showrunner for the science fiction television series "Babylon 5", its spin-off "Crusade", as well as "Jeremiah", a series loosely based on Hermann Huppen's comics. Straczynski wrote 92 out of the 110 "Babylon 5" episodes, notably including an unbroken 59-episode run through the third and fourth seasons, and all but one episode of the fifth season. He also wrote the four "Babylon 5" TV movies produced alongside the series. From 2001 to 2007, he was the writer for the long-running Marvel comic book series "The Amazing Spider-Man". He also famously wrote for "Thor", "Superman", the "" original graphic novels, "Before Watchmen" and "Wonder Woman".
In 2009, Straczynski was nominated for the BAFTA Award for his screenplay for "Changeling".
Straczynski is a long-time participant in Usenet and other early computer networks, interacting with fans through various online forums (including GEnie, CompuServe, and America Online) since 1984. He is credited as being the first TV producer ("showrunner" in Hollywood parlance) to directly engage with fans on the Internet, and allow their viewpoints to influence the look and feel of his show. (see "Babylon 5' "s use of the Internet). Two prominent areas where he had a presence were GEnie and the newsgroup rec.arts.sf.tv.babylon5.moderated.
Straczynski is a graduate of San Diego State University (SDSU), having earned a BA with a double major in psychology and sociology (with minors in philosophy and literature). While at SDSU, he wrote for the student newspaper, "The Daily Aztec," at times penning so many articles that the paper was jokingly referred to as the "Daily Joe". Straczynski resides in the Los Angeles area.
Early years.
Straczynski was born in Paterson, New Jersey, and is the son of Charles Straczynski, a manual laborer, and Evelyn Straczynski (née Pate). He was raised in Newark, New Jersey; Kankakee, Illinois; Dallas, Texas; Chula Vista, California, where he graduated from high school; and San Diego, California. Straczynski's family religion was Catholic, and he has Polish ancestry. His grandparents lived in the area which today belongs to Belarus, and fled to America from the Russian Revolution; his father was born in the US, but lived in Poland, Germany and Russia.
Straczynski cut his teeth writing plays, having several produced at Southwestern College and San Diego State University before finally publishing his adaptation of "Snow White" with Performance Publishing. Several other plays were produced around San Diego, including "The Apprenticeship" for the Marquis Public Theater. During the late 1970s, Straczynski also became the on-air entertainment reviewer for KSDO-FM and wrote several radio plays before being hired as a scriptwriter for the radio drama "Alien Worlds". He also produced his first television project in San Diego, "Marty Sprinkle" for KPBS-TV as well as worked on the XETV-TV project "Disasterpiece Theatre". While in San Diego he became a journalist for the "Los Angeles Times" as a special San Diego correspondent and also worked for "San Diego Magazine" and "The San Diego Reader". In 1981 he landed a contract with "Writer's Digest" to write a book about scriptwriting.
He and Kathryn M. Drennan, whom he met at San Diego State, moved to Los Angeles on April 1, 1981. They would marry in 1983, and separate in 2002. He worked on his book while planning a transition to television. The book's first edition was published in 1982. In Los Angeles he worked for the "Los Angeles Herald-Examiner", the "Los Angeles Times", the "Los Angeles Reader", "TV-Cable Week", and "People" magazine. He quit journalism after working for "People", and in 1983, he wrote a spec script for the show "He-Man and the Masters of the Universe" and the producers of He-Man bought it as well as other scripts and then hired Straczynski as a staff writer.
According to the jacket bio for the first edition of his scriptwriting text (see Print below), Straczynski had a play produced when he was 17, a sitcom produced when he was 21, and sold his first movie script when he was 24. It should be noted, however, that these first two credits were for volunteer public radio, and not professional script sales. By 28, his credits included television and film scripts, radio scripts for "Alien Worlds" and the Mutual Broadcasting System, a dozen plays, and more than 150 newspaper and magazine articles. He taught his craft for years at lectures and seminars in California and elsewhere.
He spent five years from 1987 to 1992 co-hosting the "Hour 25" radio talk show on KPFK-FM Los Angeles with Larry DiTillio.
Radio.
Straczynski has written for radio drama, including the series "Alien Worlds" and scripts for "Mutual Radio Theater". He wrote a script called "Where No Shadows Fall" for the company Airstage which was produced through KPBS in San Diego and aired on its radio series for the blind. The program was later aired on KPFK-FM in Los Angeles in 1982.
Straczynski has also been an on-air personality. He began by doing a weekly entertainment segment on KSDO News Radio in San Diego from 1978–1980. In Los Angeles, he put in five years as on-air host of the science fiction talk show Hour 25, which aired on KPFK 90.7 FM from 10 p.m. until midnight. During his tenure, he interviewed such luminaries as John Carpenter, Neil Gaiman, Ray Bradbury, Harlan Ellison and other writers, producers, actors and directors.
In 2000, Straczynski returned to radio drama with "The City of Dreams" for scifi.com and an original 20-part radio drama series entitled "The Adventures of Apocalypse Al" for the Canadian Broadcasting Corporation that was to debut in 2007 but has not yet aired.
Television.
Animation.
Straczynski was a fan of the cartoon, "He-Man and the Masters of the Universe". He wrote a spec script in 1984 and sent it directly to Filmation. They purchased his script, bought several others, and hired him on staff. During this time he became friends with Larry DiTillio, and when Filmation produced the He-Man spinoff, "", they both worked as story editors on the show. However, Filmation refused to give them credit on-screen and Straczynski and DiTillio both left and found work with DIC on "Jayce and the Wheeled Warriors".
Straczynski and DiTillio also worked to create an animated version of "Elfquest" but that project soon fell through when CBS attempted to retool the show to appeal to younger audiences.
While working on "Jayce", Straczynski was hired to come aboard the Len Janson and Chuck Menville project to adapt the movie "Ghostbusters" to an animated version called "The Real Ghostbusters". When Janson and Menville learned that there was not only a 13-episode order but a 65-episode syndication order as well, they decided that the workload was too much and that they would only work on their own scripts. DIC head Jean Chalopin asked Straczynski to take on the task of story editing the entire 78-episode block as well as writing his own scripts. After the show's successful first season, consultants were brought in to make suggestions for the show, including changing Janine to a more maternal character, giving every character a particular "job" (Peter is the funny one, Egon is the smart one, and Winston, the only black character, was to be the driver), and to add kids into the show. Straczynski left at this point and Janson and Menville took on the story editing job for the second network season. Straczynski then developed a show called "Spiral Zone" but left after only one script when his concept for the show was drastically altered and took his name off the series, substituting the pseudonym "Fettes Grey" (derived from the names of the grave robbers in "The Body Snatcher").
Straczynski also wrote for "CBS Storybreak", writing an adaptation of Evelyn Sibley Lampman's "The Shy Stegosaurus of Cricket Creek").
Live action and the 1988 Writer's Strike.
After leaving animation, Straczynski freelanced for "The Twilight Zone" writing an episode entitled ("What Are Friends For"), and for Shelley Duvall's "Nightmare Classics", adaptating "The Strange Case of Dr. Jekyll and Mr. Hyde", which was nominated for a Writer's Guild Award).
Straczynski was then offered the position of story editor on the syndicated live-action science fiction series "Captain Power and the Soldiers of the Future". Straczynski constructed a season long arc with lasting character changes and wrote a third of the scripts himself. After one season, the toy company Mattel demanded more input into the show, causing Straczynski to quit. He recommended DiTillio to take over the job as story editor for a second season, but the toy company financing fell through and that season was never produced.
Soon after, the 1988 Writers Guild of America strike began. Straczynski met Harlan Ellison during this time and would later become friends with him.
After the strike ended, the producers of the new "Twilight Zone" needed to create more episodes to be able to sell the series into syndication with a complete 65-episode package. They hired Straczynski as executive story editor to fill in the remaining number of needed episodes. Straczynski wrote many of the scripts himself. In addition, one episode, "Crazy as a Soup Sandwich", was written by Ellison.
Network shows and Babylon 5.
After leaving "Twilight Zone", his agent of the time asked him to pitch for the show "Jake and the Fatman". Initially wary, Straczynski finally did and was hired on as story editor under Jeri Taylor and David Moessinger. When Taylor and Moessinger left the show, Straczynski left as well in solidarity with them.
When Moessinger was hired as executive producer for "Murder, She Wrote", he offered Straczynski a job as co-producer. Straczynski joined "Murder" for two seasons and wrote 7 produced episodes. Moessinger and Straczynski moved the protagonist, Jessica Fletcher, from the sleepy Maine town of Cabot Cove to New York City to revitalize the show. The move effectively brought the show back into the top ten from the mid-thirties where it had fallen. Straczynski made Jessica an instructor in writing and criminology, and he emphasized her role as a working writer, with all the deadlines and problems involved in that profession.
Straczynski also wrote one episode of "Walker, Texas Ranger" for Moessinger between the pilot and first season of B5.
In late 1991, Warner Bros. contracted with Doug Netter and Straczynski as partners to produce the show "Babylon 5" as the flagship program for the new Prime Time Entertainment Network.
Straczynski and Netter hired many of the people from "Captain Power", as well as hiring Ellison as a consultant and DiTillio as a story editor. Babylon 5 won two Emmy Awards, back-to-back Hugo Awards, and dozens of other awards. Straczynski wrote 92 of the 110 episodes, as well as the pilot and five television movies. The show concentrated on character-driven space opera, emphasis on realism, and it pioneered extensive use of CGI for its special effects. The show ran for its planned 5-season run but its sequel "Crusade" was the victim of TNT Network Executive interference, and only 13 episodes were commissioned, ending production even before the first episode had aired.
In 2005, Straczynski began publishing his "Babylon 5" scripts. This process ended in June 2008, with the scripts no longer being available from the end of July of that year. His scripts for the television movies were published for a limited time in January 2009.
Recent television projects.
Straczynski's most recent television series was "Jeremiah", loosely based on the Belgian post-apocalyptic comic of the same name. Straczynski ran the series for two seasons but was frustrated with the conflicting directions that MGM and Showtime wanted from the show, and even used the pseudonym "Fettes Grey" for the first time since "Spiral Zone" on one of the scripts. In the second season, Straczynski decided to leave the show if things did not improve, and the show ended after 2 seasons.
In 2004, Straczynski was approached by Paramount Studios to become a producer of the "" series. He declined, believing that he would not be allowed to take the show in the direction he felt it should go. He did write a treatment for a new "Star Trek" series with colleague Bryce Zabel.
Straczynski also wrote and produced the pilot ', a pilot for the SciFi Network, and wrote, directed and produced ' as a two-hour direct-to-DVD movie.
For initial showing in late 2014 Straczynski is producing the show "Sense8" for the Netflix Internet television company using production companies Studio JMS and Georgeville Television. Fellow executive producers are Andy and Lana Wachowski.
Film.
Straczynski worked on feature film and television movies. He wrote an adaptation of Robert Louis Stevenson's "" for the Showtime network, which was nominated for a Writer's Guild of America award, and a "Murder, She Wrote" movie, "," which he produced.
In 2006, Straczynski was hired to write a feature film based on the story of King David for Universal by producers Erwin Stoff and Akiva Goldsman.
Straczynski announced on February 23, 2007 that he had been hired to write the feature film adaptation of Max Brooks's "New York Times"-bestselling novel "World War Z" for Paramount Pictures and Brad Pitt's production company, Plan B, taking screen story credit on the finished film.
In June 2007, it was announced that Straczynski had written a feature screenplay for the "Silver Surfer" movie for Fox, the production of which would depend on the success of the "". Additionally, he has written a script for Tom Hanks' Playtone Productions and Universal Pictures called "They Marched into Sunlight" based upon the Pulitzer nominated novel of the same name and an outline by Paul Greengrass, for Greengrass to direct, should it get a greenlight.
In June 2008, "Daily Variety" named Straczynski one of the top Ten Screenwriters to Watch. They announced Straczynski was writing "Lensman" for Ron Howard (to whom he had sold a screenplay entitled "The Flickering Light"), that he was selling another spec, "Proving Ground", to Tom Cruise and United Artists.
In 2008, Straczynski wrote a draft of "Ninja Assassin" for Joel Silver, which he completed in just 53 hours. The film was produced by the Wachowski Brothers and released on November 25, 2009.
In 2008, Universal Pictures and Imagine Entertainment premiered Straczynski's feature thriller "Changeling", starring Angelina Jolie. The film was directed by Clint Eastwood, since originally slated director Ron Howard declined due to scheduling conflicts.
"Changeling" was one of 20 films placed in competition at the 2008 Cannes Film Festival, and subsequently received eight nominations for the BAFTA Award, including a nomination for Best Original Screenplay.
In October 2008, it was announced that Straczynski was engaged to pen a remake of the science fiction classic "Forbidden Planet".
In the fall of 2009, it was reported that Straczynski was writing a movie titled "Shattered Union" for Jerry Bruckheimer and Disney. The screenplay, based on the video game of that name, concerns itself with a present-day American civil war.
Straczynski is credited as "story writer" along with Mark Protosevich for the 2011 film, "Thor". He also makes a cameo appearance in the film, his first appearance in a movie and his second appearance as an actor (the first being "Sleeping In Light," the final episode of "Babylon 5").
Print.
Novels, short stories and nonfiction.
Straczynski is the author of three horror novels — "Demon Night", "Othersyde", and "Tribulations" — and nearly twenty short stories, many of which are collected in two compilations — "Tales from the New Twilight Zone" and "Straczynski Unplugged". He wrote the outlines for nine of the canonical "Babylon 5" novels, supervised the three produced "B5" telefilm novelizations ("In the Beginning", "Thirdspace", and "A Call to Arms"), and is the author of four "Babylon 5" short stories published in magazines, not yet reprinted (as of 2008[ [update]]).
Straczynski has been a journalist, reviewer, and investigative reporter, publishing over 500 articles in such publications as the "Los Angeles Times", the "Los Angeles Herald-Examiner", "Writer's Digest", "Penthouse", "San Diego Magazine", "Twilight Zone Magazine", the "San Diego Reader", the "Los Angeles Reader" and "Time".
Straczynski wrote "The Complete Book of Scriptwriting" (ISBN 1-85286-882-1), often used as a text in introductory screenwriting courses, and is now in its third edition.
Comic books.
Straczynski has long been a comic fan, and began writing comics in the late 1980s. His work in comics includes the adaptations of "Captain Power and the Soldiers of the Future", "The Twilight Zone", "Star Trek" and "Babylon 5". In 1999 he created "Rising Stars" for Top Cow/Image Comics. Eventually he worked mostly under his own imprint – Joe's Comics – for which he wrote the "Midnight Nation" miniseries, and the illustrated fantasy parable "Delicate Creatures". Marvel Comics then signed him to an exclusive contract, beginning with a run on "The Amazing Spider-Man", from 2001–2007. He took over the series with issue #30 (cover dated June 2001). Straczynski and artist John Romita, Jr. crafted an acclaimed story for "The Amazing Spider-Man" #36 (Dec. 2001) in response to the September 11 attacks. He wrote or co-wrote several major Spider-Man story arcs including "", "", and "". He later wrote several other Marvel titles including "Supreme Power", "Strange", "Fantastic Four", "Thor", and mini-series featuring the Silver Surfer and a "What If" scenario, "Bullet Points". When his exclusive contract with Marvel ended, he was announced as the writer for a run on "The Brave and the Bold" for DC Comics. He collaborated with artist Shane Davis on an out-of-continuity original graphic novel starring Superman titled "". The story features a young Superman and focus on his decision about the role he want to assume in life. On March 8, 2010 it was announced he would be taking over writing duties for the monthly "Superman" title with a story arc entitled "", and the "Wonder Woman" title, beginning with issues 701 and 601 respectively. Less than a year later he was asked by DC to step away from both titles in order to concentrate on the second volume of "Superman: Earth One" and handed them over to Chris Roberson and Phil Hester to finish his Superman and Wonder Woman stories respectively. In 2012, Straczynski wrote "Before Watchmen: Dr. Manhattan" drawn by Adam Hughes and "Before Watchmen: Nite Owl" drawn by Andy Kubert and Joe Kubert. A second volume of "Superman: Earth One" was released later that same year. The Joe's Comics line was revived at Image Comics with the launch of "Ten Grand" drawn by Ben Templesmith and "Sidekick" drawn by Tom Mandrake.
Joe's Comics.
Joe's Comics was revived at Image Comics in 2013:
Awards and recognition.
His personal awards include the 1996 Hugo Award for Best Dramatic Presentation (shared with director Janet Greek) for the "Babylon 5" episode, "The Coming of Shadows" and the 1997 Hugo Award for Dramatic Presentation (shared with director David Eagle) for the "Babylon 5" episode, "Severed Dreams". Along with the "Babylon 5" cast and crew he received the 1994 Visions Of The Future Award from the Space Frontier Foundation, and in 1998 he received the Ray Bradbury Award for Outstanding Dramatic Presentation for the television series "Babylon 5".
Along with John Romita, Jr. and Scott Hanna he was the 2002 Eisner Award winner for Best Serialized Story for his work on the "Coming Home" storyline in "The Amazing Spider-Man". In 2005, he was voted Favorite Comics Writer by UK readers and received that year's Eagle Award. He was also among the recipients of the 1994 Inkpot Award. In 2008, as screenwriter, he was among the recipients of the Christopher Award issued to the movie "Changeling". In 2013 he received the prestigious International Icon Award from the San Diego Comic-Con International, only the eighth time this award has been given with past recipients including George Lucas, Neil Gaiman, Ray Bradbury, Stan Lee, and Matt Groening.
Award nominations include the 2009 BAFTA Award, for his screenplay for "Changeling". Three separate 2009 Eisner Award nominations – for Best Limited Edition ("The Twelve") along with Chris Weston, Best Continuing Series ("Thor") along with Olivier Coipel and Mark Morales, and Best Writer ("Thor"). In 1988, his novel, "Demon Night", was presented for consideration of that year's Bram Stoker award, under the category of Best First Novel. He was also nominated for a Writers Guild Award and a Cable Ace Award for his adaptation of Robert Louis Stevenson's "The Strange Case of Dr. Jekyll and Mr. Hyde", produced for Showtime Network. An asteroid, discovered in 1992 at the Kitt Peak National Observatory, was honorarily named 8379 Straczynski.
Studio JMS.
In July 2012, J. Michael Straczynski announced the launch of Studio JMS to produce TV series, movies, comics and, down the road, games and web series. On March 27, 2013 Netflix announced they will be producing the show "Sense8" with Studio JMS and the Wachowskis.

</doc>
<doc id="42752" url="http://en.wikipedia.org/wiki?curid=42752" title="Sonoluminescence">
Sonoluminescence

Sonoluminescence is the emission of short bursts of light from imploding bubbles in a liquid when excited by sound.
History.
The sonoluminescence effect was first discovered at the University of Cologne in 1934 as a result of work on sonar. H. Frenzel and H. Schultes put an ultrasound transducer in a tank of photographic developer fluid. They hoped to speed up the development process. Instead, they noticed tiny dots on the film after developing and realized that the bubbles in the fluid were emitting light with the ultrasound turned on. It was too difficult to analyze the effect in early experiments because of the complex environment of a large number of short-lived bubbles. (This experiment is also ascribed to N. Marinesco and J.J. Trillat in 1933, which also credits them with independent discovery). This phenomenon is now referred to as multi-bubble sonoluminescence (MBSL).
In 1960 Dr. Peter Jarman from Imperial College of London proposed the most reliable theory of SL phenomenon. The collapsing bubble generates an imploding shock wave that compresses and heats the gas at the center of the bubble to extremely high temperature. 
In 1989 an experimental advance was introduced by D. Felipe Gaitan and Lawrence Crum, who produced stable single-bubble sonoluminescence (SBSL). In SBSL, a single bubble trapped in an acoustic standing wave emits a pulse of light with each compression of the bubble within the standing wave. This technique allowed a more systematic study of the phenomenon, because it isolated the complex effects into one stable, predictable bubble. It was realized that the temperature inside the bubble was hot enough to melt steel. Interest in sonoluminescence was renewed when an inner temperature of such a bubble well above one million kelvins was postulated. This temperature is thus far not conclusively proven; rather, recent experiments conducted by the University of Illinois at Urbana–Champaign indicate temperatures around .
Properties.
Sonoluminescence can occur when a sound wave of sufficient intensity induces a gaseous cavity within a liquid to collapse quickly. This cavity may take the form of a pre-existing bubble, or may be generated through a process known as cavitation. Sonoluminescence in the laboratory can be made to be stable, so that a single bubble will expand and collapse over and over again in a periodic fashion, emitting a burst of light each time it collapses. For this to occur, a standing acoustic wave is set up within a liquid, and the bubble will sit at a pressure anti-node of the standing wave. The frequencies of resonance depend on the shape and size of the container in which the bubble is contained.
Some facts about sonoluminescence:
Spectral measurements have given bubble temperatures in the range from to , the exact temperatures depending on experimental conditions including the composition of the liquid and gas. Detection of very high bubble temperatures by spectral methods is limited due to the opacity of liquids to short wavelength light characteristic of very high temperatures.
Writing in "Nature", chemists David J. Flannigan and Kenneth S. Suslick describe a method of determining temperatures based on the formation of plasmas. Using argon bubbles in sulfuric acid, their data show the presence of ionized molecular oxygen O2+, sulfur monoxide, and atomic argon populating high-energy excited states, which confirms a hypothesis that the bubbles have a hot plasma core. The ionization and excitation energy of dioxygenyl cations, which they observed, is 18 electronvolts. From this they conclude the core temperatures reach at least 20,000 Kelvin.
Rayleigh–Plesset equation.
The dynamics of the motion of the bubble is characterized to a first approximation by the Rayleigh-Plesset equation (named after Lord Rayleigh and Milton Plesset):
This is an approximate equation that is derived from the incompressible Navier-Stokes equations (written in spherical coordinate system) and describes the motion of the radius of the bubble "R" as a function of time "t". Here, "μ" is the viscosity, "p" the pressure, and "γ" the surface tension. The over-dots represent time derivatives. This equation, though approximate, has been shown to give good estimates on the motion of the bubble under the acoustically driven field except during the final stages of collapse. Both simulation and experimental measurement show that during the critical final stages of collapse, the bubble wall velocity exceeds the speed of sound of the gas inside the bubble. Thus a more detailed analysis of the bubble's motion is needed beyond Rayleigh-Plesset to explore the additional energy focusing that an internally formed shock wave might produce.
Mechanism of phenomenon.
The mechanism of the phenomenon of sonoluminescence remains unsettled. Hypotheses include: hotspot, bremsstrahlung radiation, collision-induced radiation and corona discharges, nonclassical light, proton tunneling, electrodynamic jets and fractoluminescent jets (now largely discredited due to contrary experimental evidence).
In 2002, M. Brenner, S. Hilgenfeldt, and D. Lohse published a 60-page review that contains a detailed explanation of the mechanism. An important factor is that the bubble contains mainly inert noble gas such as argon or xenon (air contains about 1% argon, and the amount dissolved in water is too great; for sonoluminescence to occur, the concentration must be reduced to 20–40% of its equilibrium value) and varying amounts of water vapor. Chemical reactions cause nitrogen and oxygen to be removed from the bubble after about one hundred expansion-collapse cycles. The bubble will then begin to emit light . The light emission of highly compressed noble gas is exploited technologically in the argon flash devices.
During bubble collapse, the inertia of the surrounding water causes high pressure and high temperature, reaching around 10,000 Kelvin in the interior of the bubble, causing the ionization of a small fraction of the noble gas present. The amount ionized is small enough for the bubble to remain transparent, allowing volume emission; surface emission would produce more intense light of longer duration, dependent on wavelength, contradicting experimental results. Electrons from ionized atoms interact mainly with neutral atoms, causing thermal bremsstrahlung radiation. As the wave hits a low energy trough, the pressure drops, allowing electrons to recombine with atoms and light emission to cease due to this lack of free electrons. This makes for a 160-picosecond light pulse for argon (even a small drop in temperature causes a large drop in ionization, due to the large ionization energy relative to photon energy). This description is simplified from the literature above, which details various steps of differing duration from 15 microseconds (expansion) to 100 picoseconds (emission).
Computations based on the theory presented in the review produce radiation parameters (intensity and duration time versus wavelength) that match experimental results with errors no larger than expected due to some simplifications (e.g., assuming a uniform temperature in the entire bubble), so it seems the phenomenon of sonoluminescence is at least roughly explained, although some details of the process remain obscure.
Any discussion of sonoluminescence must include a detailed analysis of metastability. Sonoluminescence in this respect is what is physically termed a bounded phenomenon meaning that the sonoluminescence exists in a bounded region of parameter space for the bubble; a coupled magnetic field being one such parameter. The magnetic aspects of sonoluminescence are very well documented.
Other proposals.
Quantum explanations.
An unusually exotic hypothesis of sonoluminescence, which has received much popular attention, is the Casimir energy hypothesis suggested by noted physicist Julian Schwinger and more thoroughly considered in a paper by Claudia Eberlein of the University of Sussex. Eberlein's paper suggests that the light in sonoluminescence is generated by the vacuum within the bubble in a process similar to Hawking radiation, the radiation generated at the event horizon of black holes. According to this vacuum energy explanation, since quantum theory holds that vacuum contains virtual particles, the rapidly moving interface between water and gas converts virtual photons into real photons. This is related to the Unruh effect or the Casimir effect. If true, sonoluminescence may be the first observable example of quantum vacuum radiation. The argument has been made that sonoluminescence releases too large an amount of energy and releases the energy on too short a time scale to be consistent with the vacuum energy explanation, although other credible sources argue the vacuum energy explanation might yet prove to be correct.
Nuclear reactions.
Some have argued that the Rayleigh-Plesset equation described above is unreliable for predicting bubble temperatures and that actual temperatures in sonoluminescing systems can be far higher than 20,000 kelvins. Some research claims to have measured temperatures as high as 100,000 kelvins, and speculates temperatures could reach into the millions of kelvins. Temperatures this high could cause thermonuclear fusion. This possibility is sometimes referred to as bubble fusion and is likened to the implosion design used in the fusion component of thermonuclear weapons.
On January 27, 2006, researchers at Rensselaer Polytechnic Institute claimed to have produced fusion in sonoluminescence experiments.
Experiments in 2002 and 2005 by R. P. Taleyarkhan using deuterated acetone showed measurements of tritium and neutron output consistent with fusion. However, the papers were considered low quality and there were doubts cast by a report about the author's scientific misconduct. This made the report lose credibility among the scientific community.
Biological sonoluminescence.
Pistol shrimp (also called "snapping shrimp") produce a type of sonoluminescence from a collapsing bubble caused by quickly snapping its claw. The animal snaps a specialized claw shut to create a cavitation bubble that generates acoustic pressures of up to 80 kPa at a distance of 4 cm from the claw. As it extends out from the claw, the bubble reaches speeds of 60 miles per hour (97 km/h) and releases a sound reaching 218 decibels. The pressure is strong enough to kill small fish. The light produced is of lower intensity than the light produced by typical sonoluminescence and is not visible to the naked eye. The light and heat produced may have no direct significance, as it is the shockwave produced by the rapidly collapsing bubble which these shrimp use to stun or kill prey. However, it is the first known instance of an animal producing light by this effect and was whimsically dubbed "shrimpoluminescence" upon its discovery in 2001. It has subsequently been discovered that another group of crustaceans, the mantis shrimp, contains species whose club-like forelimbs can strike so quickly and with such force as to induce sonoluminescent cavitation bubbles upon impact.
External links.
Newer research papers largely ruling out the vacuum energy explanation

</doc>
<doc id="42754" url="http://en.wikipedia.org/wiki?curid=42754" title="University of Cologne">
University of Cologne

The University of Cologne (German: "Universität zu Köln") is the sixth oldest university in Central Europe and, with 38,000 students and 4,000 postgraduates, one of the largest universities in Germany. It is furthermore the German founding member of the Global Alliance in Management Education (CEMS). Since 2012 the university was awarded in the German Universities Excellence Initiative for its overall concept. As of 2014, the University of Cologne ranks between 301st-350th globally according to "Times Higher Education", 305th according to "QS World University Rankings" and between 151st-200th according to the "Academic Ranking of World Universities". 
History.
1388–1798.
The University of Cologne was established in 1388 as the fourth university in the Holy Roman Empire, after the Charles University of Prague (1348), the University of Vienna (1365) and the Ruprecht Karl University of Heidelberg (1386). The charter was signed by Pope Urban VI. The university began teaching on January 6, 1389.
In 1798, the university was abolished by the French, who had invaded Cologne in 1794, because under the new French constitution, universities were abolished all over France.The last rector Ferdinand Franz Wallraf was able to preserve the university’s Great Seal, now once more in use.
1919–today.
In 1919, the Prussian government endorsed a decision by the Cologne City Council to re-establish the university. On May 19, 1919, the Cologne Mayor Konrad Adenauer signed the charter of the modern university.
At that point, the new university was located in Neustadt-Süd, but relocated to its current campus in Lindenthal on 2 November 1934. The old premises are now being used for the Cologne University of Applied Sciences.
Initially, the university was composed of the Faculty of Commerce, Economics and Social Sciences (successor to the Institutes of Commerce and of Communal and Social Administration) and the Faculty of Medicine (successor to the Academy of Medicine). In 1920, the Faculty of Law and the Faculty of Arts were added, from which latter the School of Mathematics and Natural Sciences was split off in 1955 to form a separate Faculty. In 1980, the two Cologne departments of the Rhineland School of Education were attached to the university as the Faculties of Education and of Special Education. In 1988, the university became a founding member of the Community of European Management Schools and International Companies (CEMS), today's Global Alliance in Management Education.
The University is a leader in the area of economics and is regularly placed in top positions for law and commerce, both for national and international rankings.
Organization.
The University of Cologne is a statutory corporation (Körperschaft des öffentlichen Rechts), operated by the Federal State of North Rhine-Westphalia.
Faculties.
The university is divided into six faculties, which together offer 200 fields of study. The faculties are those of Management, Economics and Social Sciences, Law, Medicine (with the affiliated University clinic), Arts, Mathematics and Natural Sciences and Human Sciences.
Rectors.
On November 24, 2004, Axel Freimuth was elected as the Rector of the University. His four-year term began on April 1, 2005. He succeeded Tassilo Küpper and is the 49th Rector since 1919. He was previously Dean of Mathematics and Natural Sciences.
Students and faculty.
In 2005, the University enrolled 47,203 students, including 3,718 graduate students. In 2003, the number of post-doctoral students was 670.
The number of international students was 6,157 in the Summer Semester of 2005. This amounts to approximately 13% of the total students. Those from developing countries made up about 60%, representing a total of 123 nations. The largest contingents came from Bulgaria (10.5%), Russia (8.8%), Poland (7.4%), China (6.2%) and Ukraine (5.7%).
There are 508 professors at the university, including 70 women. In addition, the university employs 1,549 research assistants, with an additional 765 at the clinic, and 1,462 other assistants (3,736 at the clinic).
Partner universities.
The University of Cologne maintains twenty official partnerships with universities from ten countries. Of these, the partnerships with Clermont-Ferrand I and Pennsylvania State are the oldest partnerships. In addition, Cologne has further cooperations with more than 260 other universities.
Notable alumni and professors.
Over the centuries, scholars from Cologne have been among the most prominent in their fields, beginning with Albertus Magnus and his pupil Thomas Aquinas (both 13th century). Notable alumni of the 20th century include among others Kurt Alder (Nobel Prize in Chemistry 1950), Peter Grünberg (Nobel Prize in Physics 2007), Heinrich Böll (Nobel Prize for Literature), Karl Carstens (president of the Federal Republic of Germany 1979–1984), Gustav Heinemann (president of the Federal Republic of Germany 1969 to 1974), Karolos Papoulias (former president of the Hellenic Republic), and Erich Gutenberg (founder of modern German business studies).

</doc>
<doc id="42758" url="http://en.wikipedia.org/wiki?curid=42758" title="Murasaki Shikibu">
Murasaki Shikibu

Murasaki Shikibu (紫 式部, English: Lady Murasaki) (c. 978 – c. 1014 or 1025) was a Japanese novelist, poet and lady-in-waiting at the Imperial court during the Heian period. She is best known as the author of "The Tale of Genji", written in Japanese between about 1000 and 1012. Murasaki Shikibu is a nickname; her real name is unknown, but she may have been Fujiwara Takako, who was mentioned in a 1007 court diary as an imperial lady-in-waiting.
Heian women were traditionally excluded from learning Chinese, the written language of government, but Murasaki, raised in her erudite father's household, showed a precocious aptitude for the Chinese classics and managed to acquire fluency. She married in her mid-to late twenties and gave birth to a daughter before her husband died, two years after they were married. It is uncertain when she began to write "The Tale of Genji", but it was probably while she was married or shortly after she was widowed. In about 1005, Murasaki was invited to serve as a lady-in-waiting to Empress Shōshi at the Imperial court, probably because of her reputation as a writer. She continued to write during her service, adding scenes from court life to her work. After five or six years, she left court and retired with Shōshi to the Lake Biwa region. Scholars differ on the year of her death; although most agree on 1014, others have suggested she was alive in 1025.
Murasaki wrote "The Diary of Lady Murasaki", a volume of poetry, and "The Tale of Genji". Within a decade of its completion, "Genji" was distributed throughout the provinces; within a century it was recognized as a classic of Japanese literature and had become a subject of scholarly criticism. Early in the 20th century her work was translated; a six-volume English translation was completed in 1933. Scholars continue to recognize the importance of her work, which reflects Heian court society at its peak. Since the 13th century her works have been illustrated by Japanese artists and well-known ukiyo-e woodblock masters.
Early life.
Murasaki Shikibu was born c. 973 in Heian-kyō, Japan, into the northern Fujiwara clan descending from Fujiwara no Yoshifusa, the first 9th-century Fujiwara regent. The Fujiwara clan dominated court politics until the end of the 11th century through strategic marriages of Fujiwara daughters into the imperial family and the use of regencies. In the late 10th century and early 11th century, Fujiwara no Michinaga arranged his four daughters into marriages with emperors, giving him unprecedented power. Murasaki's great-grandfather, Fujiwara no Kanesuke, had been in the top tier of the aristocracy, but her branch of the family gradually lost power and by the time of Murasaki's birth was at the middle to lower ranks of the Heian aristocracy—the level of provincial governors. The lower ranks of the nobility were typically posted away from court to undesirable positions in the provinces, exiled from the centralized power and court in Kyoto.
Despite the loss of status, the family had a reputation among the literati through Murasaki's paternal great-grandfather and grandfather, both of whom were well-known poets. Her great-grandfather, Fujiwara no Kanesuke, had fifty-six poems included in thirteen of the Twenty-one Imperial Anthologies, the "Collections of Thirty-six Poets" and the "Yamato Monogatari" ("Tales of Yamato"). Her great-grandfather and grandfather both had been friendly with Ki no Tsurayuki, who became notable for popularizing verse written in Japanese. Her father, Fujiwara no Tametoki, attended the State Academy (Daigaku-ryō) and became a well-respected scholar of Chinese classics and poetry; his own verse was anthologized. He entered public service around 968 as a minor official and was given a governorship in 996. He stayed in service until about 1018. Murasaki's mother was descended from the same branch of northern Fujiwara as Tametoki. The couple had three children, a son and two daughters.
The names of women were not recorded in the Heian era. Murasaki's real name is not known; as was customary for women of the period, she went by a nickname, Murasaki Shikibu. Women took nicknames associated with a male relative: "Shikibu" refers to ("Shikibu-shō"), the Ministry of Ceremonials where her father was a functionary; "Murasaki" may be derived from the color violet associated with wisteria, the meaning of the word "fuji", although it is more likely that "Murasaki" was a court nickname. Michinaga mentions the names of a few ladies-in-waiting in a 1007 diary entry; one, Fujiwara Takako (Kyōshi), may be Murasaki's real name.
In Heian-era Japan, husbands and wives kept separate households; children were raised with their mothers, although the patrilineal system was still followed. Murasaki was unconventional because she lived in her father's household, most likely on Teramachi Street in Kyoto, with her younger brother Nobunori. Their mother died, perhaps in childbirth, when the children were quite young. Murasaki had at least three half-siblings raised with their mothers; she was very close to one sister who died in her twenties.
Murasaki was born at a period when Japan was becoming more isolated, after missions to China had ended and a stronger national culture was emerging. In the 9th and 10th centuries, Japanese gradually became a written language through the development of kana, a syllabary based on abbreviations of Chinese characters. In Murasaki's lifetime men continued to write in Chinese, the language of government, but kana became the written language of noblewomen, setting the foundation for unique forms of Japanese literature.
Chinese was taught to Murasaki's brother as preparation for a career in government, and during her childhood, living in her father's household, she learned and became proficient in classical Chinese. In her diary she wrote, "When my brother ... was a young boy learning the Chinese classics, I was in the habit of listening to him and I became unusually proficient at understanding those passages that he found too difficult to understand and memorize. Father, a most learned man, was always regretting the fact: 'Just my luck,' he would say, 'What a pity she was not born a man!'" With her brother she studied Chinese literature, and she probably also received instruction in more traditional subjects such as music, calligraphy and Japanese poetry. Murasaki's education was unorthodox. Louis Perez explains in "The History of Japan" that "Women ... were thought to be incapable of real intelligence and therefore were not educated in Chinese." Murasaki was aware that others saw her as "pretentious, awkward, difficult to approach, prickly, too fond of her tales, haughty, prone to versifying, disdainful, cantankerous and scornful". Asian literature scholar Thomas Inge believes she had "a forceful personality that seldom won her friends."
Marriage.
Aristocratic Heian women lived restricted and secluded lives, allowed only to speak to men when they were close relatives or household members. Murasaki's autobiographical poetry shows that she socialized with women but had limited contact with men other than her father and brother; she often exchanged poetry with women but never with men. Unlike most noblewomen of her status, she did not marry on reaching puberty; instead she stayed in her father's household until her mid-twenties or perhaps even to her early thirties.
In 996 when her father was posted to a four-year governorship in Echizen Province, Murasaki went with him, although it was uncommon for a noblewoman of the period to travel such a distance on a trip that could take as long as five days. She returned to Kyoto, probably in 998, to marry her father's friend Fujiwara no Nobutaka (c. 950 – c. 1001), a much older second cousin. Descended from the same branch of the Fujiwara clan, he was a court functionary and bureaucrat at the Ministry of Ceremonials, with a reputation for dressing extravagantly and as a talented dancer. In his late forties at the time of their marriage, he had multiple households with an unknown number of wives and offspring. Gregarious and well known at court, he was involved in numerous romantic relationships that may have continued after his marriage to Murasaki. As was customary, she would have remained in her father's household where her husband would have visited her. Nobutaka had been granted more than one governorship, and by the time of his marriage to Murasaki he was probably quite wealthy. Accounts of their marriage vary: Richard Bowring writes that the marriage was happy, but Japanese literature scholar Haruo Shirane sees indications in her poems that she resented her husband.
The couple's daughter, Kenshi (Kataiko), was born in 999. Two years later Nobutaka died during a cholera epidemic. As a married woman Murasaki would have had servants to run the household and care for her daughter, giving her ample leisure time. She enjoyed reading and had access to romances (monogatari) such as "The Tale of the Bamboo Cutter" and the "Tales of Ise." Scholars believe she may have started writing "The Tale of Genji" before her husband's death; it is known she was writing after she was widowed, perhaps in a state of grief. In her diary she describes her feelings after her husband's death: "I felt depressed and confused. For some years I had existed from day to day in listless fashion ... doing little more than registering the passage of time ... The thought of my continuing loneliness was quite unbearable".
According to legend, Murasaki retreated to Ishiyama-dera at Lake Biwa, where she was inspired to write "The Tale of Genji" on an August night while looking at the moon. Although scholars dismiss the factual basis of the story of her retreat, Japanese artists often depicted her at Ishiyama Temple staring at the moon for inspiration. She may have been commissioned to write the story and may have known an exiled courtier in a similar position to her hero Prince Genji. Murasaki would have distributed newly written chapters of "Genji" to friends who in turn would have re-copied them and passed them on. By this practice the story became known and she gained a reputation as an author.
In her early to mid-thirties, she became a lady-in-waiting, "nyōbō", at court, most likely because of her reputation as an author. Chieko Mulhern writes in "Japanese Women Writers, a Biocritical Sourcebook" that scholars have wondered why Murasaki made such a move at a comparatively late period in her life. Her diary evidences that she exchanged poetry with Michinaga after her husband's death, leading to speculation that the two may have been lovers. Bowring sees no evidence that she was brought to court as Michinaga's concubine, although he did bring her to court without following official channels. Mulhern thinks Michinaga wanted to have Murasaki at court to educate his daughter Shōshi.
Court life.
Heian culture and court life reached a peak early in the 11th century. The population of Kyoto grew to around 100,000 as the nobility became increasingly isolated at the Heian Palace in government posts and court service. Courtiers became overly refined with little to do, insulated from reality, preoccupied with the minutiae of court life, turning to artistic endeavors. Emotions were commonly expressed through the artistic use of textiles, fragrances, calligraphy, colored paper, poetry, and layering of clothing in pleasing color combinations—according to mood and season. Those who showed an inability to follow conventional aesthetics quickly lost popularity, particularly at court. Popular pastimes for Heian noblewomen—who adhered to rigid fashions of floor-length hair, whitened skin and blackened teeth—included having love affairs, writing poetry and keeping diaries. The literature that Heian court women wrote is recognized as some of the earliest and among the best literature written in the Japanese canon.
Rival courts and women poets.
When in 995 Michinaga's two brothers Fujiwara no Michitaka and Fujiwara no Michikane died leaving the regency vacant, Michinaga quickly won a power struggle against his nephew Fujiwara no Korechika (brother to Teishi, Emperor Ichijō's wife), and, aided by his sister Senshi, he assumed power. Teishi had supported her brother Korechika, who was later discredited and banished from court, causing her to lose power. Four years later Michinaga sent Shōshi, his eldest daughter, to Emperor Ichijō's harem when she was about 12. A year after placing Shōshi in the imperial harem, in an effort to undermine Teishi's influence and increase Shōshi's standing, Michinaga had her named Empress although Teishi already held the title. As historian Donald Shively explains, "Michinaga shocked even his admirers by arranging for the unprecedented appointment of Teishi (or Sadako) and Shōshi as concurrent empresses of the same emperor, Teishi holding the usual title of "Lustrous Heir-bearer" "kōgō" and Shōshi that of "Inner Palatine" ("chūgū"), a toponymically derived equivalent coined for the occasion". About five years later, Michinaga brought Murasaki to Shōshi's court, in a position that Bowring describes as a companion-tutor.
Heian Imperial court life was immensely fashionable, but also dissolute. Court women lived in seclusion, were known by nicknames and, through strategic marriages, were used to gain political power. Despite their seclusion, some women wielded considerable influence, often achieved through competitive salons, dependent on the quality of the attendants. Ichijō's mother and Michinaga's sister, Senshi, had an influential salon, and Michinaga probably wanted Shōshi to surround herself with skilled women such as Murasaki to build a rival salon.
Shōshi was 16 to 19 when Murasaki joined her court. According to Arthur Waley, Shōshi was a serious-minded young lady, whose living arrangements were divided between her father's household and her court at the Imperial Palace. She gathered around her talented women writers such as Izumi Shikibu and Akazome Emon—the author of an early vernacular history, "The Tale of Flowering Fortunes". The rivalry that existed among the women is evident in Murasaki's diary, where she wrote disparagingly of Izumi: "Izumi Shikibu is an amusing letter-writer; but there is something not very satisfactory about her. She has a gift for dashing off informal compositions in a careless running-hand; but in poetry she needs either an interesting subject or some classic model to imitate. Indeed it does not seem to me that in herself she is really a poet at all."
Sei Shōnagon, author of the "The Pillow Book", had been in service as lady-in-waiting to Teishi when Shōshi came to court; it is possible that Murasaki was invited to Shōshi's court as a rival to Shōnagon. Teishi died in 1001, before Murasaki entered service with Shōshi, so the two writers were not there concurrently, but Murasaki, who wrote about Shōnagon in her diary, certainly knew of her, and to an extent was influenced by her. Shōnagon's "The Pillow Book" may have been commissioned as a type of propaganda to highlight Teishi's court, known for its educated ladies-in-waiting. Japanese literature scholar Joshua Mostow believes Michinaga provided Murasaki to Shōshi as an equally or better educated woman, so as to showcase Shōshi's court in a similar manner.
The two writers had different temperaments: Shōnagon was witty, clever, and outspoken; Murasaki was withdrawn and sensitive. Entries in Murasaki's diary show that the two may not have been on good terms. Murasaki wrote, "Sei Shōnagon ... was dreadfully conceited. She thought herself so clever, littered her writing with Chinese characters, [which] left a great deal to be desired." Keene thinks that Murasaki's impression of Shōnagon could have been influenced by Shōshi and the women at her court because Shōnagon served Shōshi's rival empress. Furthermore, he believes Murasaki was brought to court to write "Genji" in response to Shōnagon's popular "Pillow Book". Murasaki contrasted herself to Shōnagon in a variety of ways. She denigrated the pillow book genre and, unlike Shōnagon who flaunted her knowledge of Chinese, Murasaki pretended to not know the language.
"Our Lady of the Chronicles".
Although the popularity of the Chinese language diminished in the late Heian era, Chinese ballads continued to be popular, including those written by Bai Juyi. Murasaki taught Chinese to Shōshi who was interested in Chinese art and Juyi's ballads. Upon becoming Empress, Shōshi installed screens decorated with Chinese script, causing outrage because written Chinese was considered the language of men, far removed from the women's quarters. The study of Chinese was thought to be unladylike and went against the notion that only men should have access to the literature. Women were supposed to read and write only in Japanese, which separated them through language from government and the power structure. Murasaki, with her unconventional classical Chinese education, was one of the few women available to teach Shōshi classical Chinese. Bowring writes it was "almost subversive" that Murasaki knew Chinese and taught the language to Shōshi. Murasaki, who was reticent about her Chinese education, held the lessons between the two women in secret, writing in her diary, "Since last summer ... very secretly, in odd moments when there happened to be no one about, I have been reading with Her Majesty ... There has of course been no question of formal lessons ... I have thought it best to say nothing about the matter to anybody."
Murasaki most likely earned her second nickname, "Our Lady of the Chronicles" ("Nihongi no tsubone)", for teaching Shōshi Chinese literature. A lady-in-waiting who disliked Murasaki accused her of flaunting her knowledge of Chinese and began calling her "Our Lady of the Chronicles"—an allusion to the "Chronicles of Japan"—after an incident in which chapters from "Genji" were read aloud to the Emperor and his courtiers, one of whom remarked that the author showed a high level of education. Murasaki wrote in her diary, "How utterly ridiculous! Would I, who hesitate to reveal my learning to my women at home, ever think of doing so at court?" Although meant to be insulting, Mulhern believes Murasaki was probably flattered by the nickname.
The attitude toward the Chinese language was contradictory. In Teishi's court, Chinese had been flaunted and considered a symbol of imperial rule and superiority. Yet, in Shōshi's salon there was a great deal of hostility towards the language—perhaps owing to political expedience during a period when Chinese began to be rejected in favor of Japanese—even though Shōshi herself was a student of the language. The hostility may have affected Murasaki and her opinion of the court, and forced her to hide her knowledge of Chinese. Unlike Shōnagon, who was both ostentatious and flirtatious, as well as outspoken about her knowledge of Chinese, Murasaki seems to have been humble, an attitude which possibly impressed Michinaga. Although Murasaki used Chinese and incorporated it in her writing, she publicly rejected the language, a commendable attitude during a period of burgeoning Japanese culture.
Murasaki seems to have been unhappy with court life and was withdrawn and somber. No surviving records show that she entered poetry competitions; she appears to have exchanged few poems or letters with other women during her service. In general, unlike Sei Shōnagon, Murasaki gives the impression in her diary that she disliked court life, the other ladies-in-waiting, and the drunken revelry. She did, however, become close friends with a lady-in-waiting named Lady Saishō, and she wrote of the winters that she enjoyed, "I love to see the snow here".
According to Waley, Murasaki may not have been unhappy with court life in general but bored in Shōshi's court. He speculates she would have preferred to serve with the Lady Senshi, whose household seems to have been less strict and more light-hearted. In her diary, Murasaki wrote about Shōshi's court, "[she] has gathered round her a number of very worthy young ladies ... Her Majesty is beginning to acquire more experience of life, and no longer judges others by the same rigid standards as before; but meanwhile her Court has gained a reputation for extreme dullness".
Murasaki disliked the men at court whom she thought to be drunken and stupid. However, some scholars, such as Waley, are certain she was involved romantically with Michinaga. At the least, Michinaga pursued her and pressured her strongly, and her flirtation with him is recorded in her diary as late as 1010. Yet, she wrote to him in a poem, "You have neither read my book, nor won my love." In her diary she records having to avoid advances from Michinaga—one night he snuck into her room, stealing a newly written chapter of "Genji." However, Michinaga's patronage was essential if she was to continue writing. Murasaki described his daughter's court activities: the lavish ceremonies, the complicated courtships, the "complexities of the marriage system", and in elaborate detail, the birth of Shōshi's two sons.
It is likely that Murasaki enjoyed writing in solitude. She believed she did not fit well with the general atmosphere of the court, writing of herself: "I am wrapped up in the study of ancient stories ... living all the time in a poetical world of my own scarcely realizing the existence of other people ... But when they get to know me, they find to their extreme surprise that I am kind and gentle". Inge says that she was too outspoken to make friends at court, and Mulhern thinks Murasaki's court life was comparatively quiet compared to other court poets. Mulhern speculates that her remarks about Izumi were not so much directed at Izumi's poetry but at her behavior, lack of morality and her court liaisons, of which Murasaki disapproved.
Rank was important in Heian court society and Murasaki would not have felt herself to have much, if anything, in common with the higher ranked and more powerful Fujiwaras. In her diary, she wrote of her life at court: "I realized that my branch of the family was a very humble one; but the thought seldom troubled me, and I was in those days far indeed from the painful consciousness of inferiority which makes life at Court a continual torment to me." A court position would have increased her social standing, but more importantly she gained a greater experience to write about. Court life, as she experienced it, is well reflected in the chapters of "Genji" written after she joined Shōshi. Her nickname, Murasaki, was most probably given at a court dinner in an incident she recorded in her diary: in c. 1008 the well-known court poet Fujiwara no Kintō inquired after the "Young Murasaki"—an allusion to the character named Murasaki in "Genji"—which would have been considered a compliment from a male court poet to a female author.
Later life and death.
When Emperor Ichijō died in 1011, Shōshi retired from the Imperial Palace to live in a Fujiwara mansion in Biwa, most likely accompanied by Murasaki, who is recorded as being there with Shōshi in 1013. George Aston explains that when Murasaki retired from court she was again associated with Ishiyama-dera: "To this beautiful spot, it is said, Murasaki no Shikibu retired from court life to devote the remainder of her days to literature and religion. There are sceptics, however, Motoöri being one, who refuse to believe this story, pointing out ... that it is irreconcilable with known facts. On the other hand, the very chamber in the temple where the "Genji" was written is shown—with the ink-slab which the author used, and a Buddhist Sutra in her handwriting, which, if they do not satisfy the critic, still are sufficient to carry conviction to the minds of ordinary visitors to the temple."
Murasaki may have died in 1014. Her father made a hasty return to Kyoto from his post at Echigo Province that year, possibly because of her death. Writing in "A Bridge of Dreams: A Poetics of "The Tale of Genji"", Shirane mentions that 1014 is generally accepted as the date of Murasaki Shikibu's death and 973 as the date of her birth, making her 41 when she died. Bowring considers 1014 to be speculative, and believes she may have lived with Shōshi until as late as 1025. Waley agrees given that Murasaki may have attended ceremonies with Shōshi held for her son, Emperor Go-Ichijō around 1025.
Murasaki's brother Nubonori died in around 1011, which, combined with the death of his daughter, may have prompted her father to resign his post and take vows at Miidera temple where he died in 1029. Murasaki's daughter entered court service in 1025 as a wet nurse to the future Emperor Go-Reizei (1025–68). She went on to become a well-known poet as Daini no Sanmi.
Works.
Three works are attributed to Murasaki: "The Tale of Genji", "The Diary of Lady Murasaki" and "Poetic Memoirs", a collection of 128 poems. Her work is considered important because her writing reflects the creation and development of Japanese writing during a period when Japanese shifted from an unwritten vernacular to a written language. Until the 9th century, Japanese language texts were written in Chinese characters using the man'yōgana writing system. A revolutionary achievement was the development of kana, a true Japanese script, in the mid-to late 9th century. Japanese authors began to write prose in their own language, which led to genres such as tales (monogatari) and poetic journals (Nikki Bungaku). Historian Edwin Reischauer writes that genres such as the monogatari were distinctly Japanese and that "Genji", written in kana, "was the outstanding work of the period".
Diary and poetry.
Murasaki began her diary after she entered service at Shōshi's court. Much of what we know about her and her experiences at court comes from the diary, which covers the period from about 1008 to 1010. The long descriptive passages, some of which may have originated as letters, cover her relationships with the other ladies-in-waiting, Michinaga's temperament, the birth of Shōshi's sons—at Michinaga's mansion rather than at the Imperial Palace—and the process of writing "Genji", including descriptions of passing newly written chapters to calligraphers for transcriptions. Typical of contemporary court diaries written to honor patrons, Murasaki devotes half to the birth of Shōshi's son Emperor Go-Ichijō, an event of enormous importance to Michinaga: he had planned for it with his daughter's marriage which made him grandfather and "de facto" regent to an emperor.
"Poetic Memoirs" is a collection of 128 poems Mulhern describes as "arranged in a biographical sequence". The original set has been lost. According to custom, the verses would have been passed from person to person and often copied. Some appear written for a lover—possibly her husband before he died—but she may have merely followed tradition and written simple love poems. They contain biographical details: she mentions a sister who died, the visit to Echizen province with her father and that she wrote poetry for Shōshi. Murasaki's poems were published in 1206 by Fujiwara Teika, in what Mulhern believes to be the collection that is closest to the original form; at around the same time Teika included a selection of Murasaki's works in an imperial anthology, "New Collections of Ancient and Modern Times".
"The Tale of Genji".
Murasaki is best known for her "The Tale of Genji", a three-part novel spanning 1100 pages and 54 chapters, that is thought to have taken a decade to complete. The earliest chapters were possibly written for a private patron either during her marriage or shortly after her husband's death. She continued writing while at court and probably finished while still in service to Shôshi. She would have needed patronage to produce a work of such length. Michinaga provided her with costly paper and ink, and with calligraphers. The first handwritten volumes were probably assembled and bound by ladies-in-waiting.
In his "The Pleasures of Japanese Literature", Keene claims Murasaki wrote the "supreme work of Japanese fiction" by drawing on traditions of waka court diaries, and earlier monogatari—written in a mixture of Chinese script and Japanese script—such as "The Tale of the Bamboo Cutter" or "The Tales of Ise". She drew on and blended styles from Chinese histories, narrative poetry and contemporary Japanese prose. Adolphson writes that the juxtaposition of formal Chinese style with mundane subjects resulted in a sense of parody or satire, giving her a distinctive voice. "Genji" follows the traditional format of monogatari—telling a tale—particularly evident in its use of a narrator, but Keene claims Murasaki developed the genre far beyond its bounds, and by doing so created a form that is utterly modern. The story of the "shining prince" Genji is set in the late 9th to early 10th centuries, and Murasaki eliminated from it the elements of fairy tales and fantasy frequently found in earlier monogatari.
The themes in "Genji" are common to the period, and are defined by Shively as encapsulating "the tyranny of time and the inescapable sorrow of romantic love". The main theme is that of the fragility of life, "the sorrow of human existence", "mono no aware"—she used the term over a thousand times in "Genji". Keene speculates that in her tale of the "shining prince", Murasaki may have created for herself an idealistic escape from court life, which she found less than savory. In Prince Genji she formed a gifted, comely, refined, yet human and sympathetic protagonist. Keene writes that "Genji" gives a view into the Heian period; for example love affairs flourished, although women typically remained unseen behind screens, curtains or fusuma.
Helen McCullough describes Murasaki's writing as of universal appeal and believes "The Tale of Genji" "transcends both its genre and age. Its basic subject matter and setting—love at the Heian court—are those of the romance, and its cultural assumptions are those of the mid-Heian period, but Murasaki Shikibu's unique genius has made the work for many a powerful statement of human relationships, the impossibility of permanent happiness in love ... and the vital importance, in a world of sorrows, of sensitivity to the feelings of others." Prince Genji recognizes in each of his lovers the inner beauty of the woman and the fragility of life, which according to Keene, makes him heroic. The story was popular: Emperor Ichijō had it read to him, even though it was written in Japanese. By 1021 all the chapters were known to be complete and the work was sought after in the provinces where it was scarce.
Legacy.
Murasaki's reputation and influence have not diminished since her lifetime when she, with other Heian women writers, was instrumental in developing Japanese into a written language. Her writing was required reading for court poets as early as the 12th century as her work began to be studied by scholars who generated authoritative versions and criticism. Within a century of her death she was highly regarded as a classical writer. In the 17th century, Murasaki's work became emblematic of Confucian philosophy and women were encouraged to read her books. In 1673 Kumazawa Banzan argued that her writing was valuable for its sensitivity and depiction of emotions. He wrote in his "Discursive Commentary on Genji" that when "human feelings are not understood the harmony of the Five Human Relationships is lost."
"The Tale of Genji" was copied and illustrated in various forms as early as a century after Murasaki's death. "The Genji Monogatari Emaki", is a late Heian era 12th-century handscroll, consisting of four scrolls, 19 paintings, and 20 sheets of calligraphy. The illustrations, definitively dated to between 1110 and 1120, have been tentatively attributed to Fujiwara no Takachika and the calligraphy to various well-known contemporary calligraphers. The scroll is housed at the Gotoh Museum and the Tokugawa Art Museum.
Female virtue was tied to literary knowledge in the 17th century, leading to a demand for Murasaki or "Genji" inspired artifacts, known as genji-e. Dowry sets decorated with scenes from "Genji" or illustrations of Murasaki became particularly popular for noblewomen: in the 17th century genji-e symbolically imbued a bride with an increased level of cultural status; by the 18th century they had come to symbolize marital success. In 1628, Tokugawa Iemitsu's daughter had a set of lacquer boxes made for her wedding; Prince Toshitada received a pair of silk genji-e screens, painted by Kanō Tan'yū as a wedding gift in 1649.
Murasaki became a popular subject of paintings and illustrations highlighting her as a virtuous woman and poet. She is often shown at her desk in Ishimyama Temple, staring at the moon for inspiration. Tosa Mitsuoki made her the subject of hanging scrolls in the 17th century. "The Tale of Genji" became a favorite subject of Japanese ukiyo-e woodblock artists for centuries with artists such as Hiroshige, Kiyonaga, and Utamaro illustrating various editions of the novel. While early Genji art was considered symbolic of court culture, by the middle of the Edo period the mass-produced ukiyo-e prints made the illustrations accessible for the samurai classes and commoners.
In "Envisioning the "Tale of Genji" Shirane observes that "The Tale of Genji" has become many things to many different audiences through many different media over a thousand years ... unmatched by any other Japanese text or artifact." The work and its author were popularized through its illustrations in various media: emaki (illustrated handscrolls); byōbu-e (screen paintings), ukiyo-e (woodblock prints); films, comics, and in the modern period, manga. In her fictionalized account of Murasaki's life, "The Tale of Murasaki: A Novel", Liza Dalby has Murasaki involved in a romance during her travels with her father to Echizen Province.
"The Tale of the Genji" is recognized as an enduring classic. McCullough writes that Murasaki "is both the quintessential representative of a unique society and a writer who speaks to universal human concerns with a timeless voice. Japan has not seen another such genius." Keene writes that "The Tale of Genji" continues to captivate, because, in the story, her characters and their concerns are universal. In the 1920s, when Waley's translation was published, reviewers compared "Genji" to Austen, Proust, and Shakespeare. Mulhern says of Murasaki that she is similar to Shakespeare, who represented his Elizabethan England, in that she captured the essence of the Heian court and as a novelist "succeeded perhaps even beyond her own expectations." Like Shakespeare, her work has been the subject of reams of criticism and many books.
Kyoto held a year-long celebration commemorating the 1000th anniversary of "Genji" in 2008, with poetry competitions, visits to the Tale of Genji Museum in Uji and Ishiyama-dera (where a life size rendition of Murasaki at her desk was displayed), and women dressing in traditional 12-layered Heian court Jūnihitoe and ankle-length hair wigs. The author and her work inspired museum exhibits and Genji manga spin-offs. The design on the reverse of the first 2000 yen note commemorated her and "The Tale of Genji". A plant bearing purple berries has been named after her.
A "Genji Album", only in the 1970s dated to 1510, is housed at Harvard University. The album is considered the earliest of its kind and consists of 54 paintings by Tosa Mitsunobu and 54 sheets of calligraphy on "shikishi" paper in five colors, written by master calligraphers. The leaves are housed in a case dated to the Edo period, with a silk frontispiece painted by Tosa Mitsuoki, dated to around 1690. The album contains Mitsuoki's authentication slips for his ancestor's 16th-century paintings.
Sources.
</dl>

</doc>
<doc id="42759" url="http://en.wikipedia.org/wiki?curid=42759" title="Java Transaction API">
Java Transaction API

The Java Transaction API (JTA), one of the Java Enterprise Edition (Java EE) APIs, enables distributed transactions to be done across multiple X/Open XA resources in a Java environment. JTA is a specification developed under the Java Community Process as . JTA provides for:
X/Open XA architecture.
In the X/Open XA architecture, a transaction manager or transaction processing monitor (TP monitor) coordinates the transactions across multiple resources such as databases and message queues. Each resource has its own resource manager. The resource manager typically has its own API for manipulating the resource, for example the JDBC API to work with relational databases. In addition, the resource manager allows a TP monitor to coordinate a distributed transaction between its own and other resource managers. Finally, there is the application which communicates with the TP monitor to begin, commit or rollback the transactions. The application also communicates with the individual resources using their own API to modify the resource.
JTA implementation of the X/Open XA architecture.
The JTA API consists of classes in two Java packages:
The JTA is modelled on the X/Open XA architecture, but it defines two different APIs for demarcating transaction boundaries. It distinguishes between an application server such as an EJB server and an application component. It provides an interface, , that is used by the application server itself to begin, commit and rollback the transactions. It provides a different interface, the , that is used by general client code such as a servlet or an EJB to manage the transactions.
The JTA architecture requires that each resource manager must implement the interface in order to be managed by the TP monitor. As stated previously, each resource will have its own specific API, for instance:
Java Transaction API.
The Java Transaction API consists of three elements: a high-level application transaction demarcation interface, a high-level transaction manager interface intended for an application server, and a standard Java mapping of the X/Open XA protocol intended for a transactional resource manager.
UserTransaction interface.
The interface provides the application the
ability to control transaction boundaries programmatically. This interface may be used
by Java client programs or EJB beans.
The method starts a global transaction and associates the
transaction with the calling thread. The transaction-to-thread association is managed
transparently by the Transaction Manager.
Support for nested transactions is not required. The UserTransaction.begin method
throws the NotSupportedException when the calling thread is already associated
with a transaction and the transaction manager implementation does not support nested
transactions.
Transaction context propagation between application programs is provided by the
underlying transaction manager implementations on the client and server machines.
The transaction context format used for propagation is protocol dependent and must be
negotiated between the client and server hosts. For example, if the transaction manager
is an implementation of the JTS specification, it will use the transaction context
propagation format as specified in the CORBA OTS 1.1 specification. Transaction
propagation is transparent to application programs.
@Transactional annotation.
The annotation provides the application the
ability to control transaction boundaries declaratively. This annotation can be applied to any class that the Java EE specification
defines as a managed bean (which includes CDI managed beans).
The code sample below illustrates the usage of @Transactional in a request scoped CDI managed bean:
Transactional behavior can be configured via an attribute on the annotation. The available options closely mirror those of the EJB specification.
@TransactionScoped annotation.
The annotation provides the application the
ability to declare that the scope during which a bean lives is tied to the time a given transaction is active. 
The code sample below illustrates the usage of @TransactionScoped in a request scoped CDI managed bean:
If method "foo()" is first called on a managed instance of ExampleBean and then subsequently method "bar()" is called, the number printed will be 0 and not 1. This is because each method had its own transaction and therefore its own instance of TxScopedBean. The number 1 that was set during the call to "foo()" will therefore not be seen during the call to "bar()".
UserTransaction support in EJB server.
EJB servers are required to support the UserTransaction interface for use by EJB
beans with the BEAN value in the annotation (this is called bean-managed transactions or BMT). The UserTransaction
interface is exposed to EJB components through either the EJBContext interface using the
getUserTransaction method, or directly via injection using the general codice_1 annotation. Thus, an EJB application does not interface with the
Transaction Manager directly for transaction demarcation; instead, the EJB bean relies
on the EJB server to provide support for all of its transaction work as defined in the
Enterprise JavaBeans Specification. (The underlying interaction between the EJB
Server and the TM is transparent to the application; the burden of implementing transaction management is on the EJB container and server provider.)
The code sample below illustrates the usage of UserTransaction via bean-managed transactions in an EJB session bean:
Alternatively, the UserTransaction can be obtained from the SessionContext:
Note though that in the example above if the codice_2 annotation is omitted, a JTA transaction is automatically started whenever codice_3 is called and is automatically committed or rolled back when codice_3 is exited. Making use of a UserTransaction is thus not necessary in EJB programming, but might be needed for very specialized code.
UserTransaction support in JNDI.
The UserTransaction should be available under codice_5 (if a JTA implementation is installed in the environment).
Open source JTA implementations.
There exist a number of active (as of September 2010) open source JTA implementations.
JBossTS.
JBossTS, formerly known as Arjuna Transaction Service, comes with a very robust implementation, which supports both the JTA and JTS APIs. JBossTS comes with a recovery service, which could be run as a separate process from your application processes. JBossTS is the default transaction manager for JBoss AS. It does not support out-of-the box integration with the Spring framework, but it is easy to integrate.
Atomikos TransactionsEssentials.
Atomikos TransactionsEssentials's documentation and literature on the internet show that it is a production quality implementation, which also supports recovery and some exotic features beyond the JTA API. Atomikos provides out-of-the-box Spring integration along with some nice examples. It also provides support for pooled connections for both database and JMS resources.
Bitronix JTA.
Bitronix claims to support transaction recovery as well as or even better than some of the commercial products. Bitronix also provides connection pooling and session pooling out of the box.

</doc>
<doc id="42760" url="http://en.wikipedia.org/wiki?curid=42760" title="JTA">
JTA

JTA may refer to:

</doc>
<doc id="42761" url="http://en.wikipedia.org/wiki?curid=42761" title="History of Belarus">
History of Belarus

This article describes the history of Belarus. The Belarusian ethnos is traced at least as far in time as other East Slavs.
Belarus, the name of the country comes from the archaic form in Belorussian 'Белые Росы' what means 'White Dew'.
After an initial period of independent feudal consolidation, Belarusian lands were incorporated into the Kingdom of Lithuania, Grand Duchy of Lithuania, and later in the Polish–Lithuanian Commonwealth, and the Russian Empire and eventually the Soviet Union. Belarus became an independent country in 1991 after declaring itself free from the Soviet Union.
Early history.
The history of Belarus, or more precisely of the Belarusian ethnicity, begins with the migration and expansion of the Slavic peoples throughout Eastern Europe between the 6th and 8th centuries. East Slavs settled on the territory of present-day Belarus, Russia and Ukraine, assimilating local Baltic — (Yotvingians, Dniepr Balts), Ugro-Finnic (in Russia) and steppe nomads (in Ukraine) already living there, their early ethnic integrations contributed to the gradual differentiation of the three East Slavic nations. These East Slavs, a pagan, animistic, agrarian people, had an economy which included trade in agricultural produce, game, furs, honey, beeswax and amber.
The modern Belarusian ethnos was probably formed on the basis of the three Slavic tribes — Kryvians, Drehovians, Radzimians as well as several Baltic tribes.
During the 9th and 10th centuries, Scandinavian Vikings established trade posts on the way from Scandinavia to the Byzantine Empire. The network of lakes and rivers crossing East Slav territory provided a lucrative trade route between the two civilizations. In the course of trade, they gradually took sovereignty over the tribes of East Slavs, at least to the point required by improvements in trade.
The Rus' rulers invaded the Byzantine Empire on few occasions, but eventually they allied against the Bulgars. The condition underlying this alliance was to open the country for Christianization and acculturation from the Byzantine Empire.
The common cultural bond of Eastern Orthodox Christianity and written Church Slavonic (a literary and liturgical Slavic language developed by 8th century missionaries Saints Cyril and Methodius) fostered the emergence of a new geopolitical entity, Kievan Rus' — a loose-knit network of principalities, established along preexisting trade routes, with major centers in Novgorod (currently Russia), Polatsk (in Belarus) and Kiev (currently in Ukraine) — which claimed a sometimes precarious preeminence among them.
First Belarusian states.
Between the 9th and 12th centuries, the Principality of Polotsk (northern Belarus) emerged as the dominant center of power on Belarusian territory, with a lesser role played by the Principality of Turaŭ in the south.
It repeatedly asserted its sovereignty in relation to other centers of Rus', becoming a political capital, the episcopal see of a bishopric and the controller of vassal territories among Balts in the west. The city's Cathedral of the Holy Wisdom (1044–66), though completely rebuilt over the years, remains a symbol of this independent-mindedness, rivaling churches of the same name in Novgorod and Kiev, referring to the original Hagia Sophia in Constantinople (and hence to claims of imperial prestige, authority and sovereignty). Cultural achievements of the Polatsk period include the work of the nun Euphrosyne of Polatsk (1120–73), who built monasteries, transcribed books, promoted literacy and sponsored art (including local artisan Lazarus Bohsha's famous "Cross of Euphrosyne", a national symbol and treasure stolen during World War II), and the prolific, original Church Slavonic sermons and writings of Bishop Cyril of Turau (1130–82).
Grand Duchy of Lithuania.
In the 13th century, the fragile unity of Kievan Rus' disintegrated due to nomadic incursions from Asia, which climaxed with the Mongol sacking of Kiev (1240), leaving a geopolitical vacuum in the region. The East Slavs splintered into a number of independent and competing principalities. Due to military conquest and dynastic marriages the West Ruthenian (Belarusian) principalities were acquired by the expanding Lithuania, beginning with the rule of Lithuanian King Mindaugas (1240–63). From the 13th to 15th century, Baltic and Ukrainian lands were consolidated into the Grand Duchy of Lithuania, with its initial capital unknown, but which presumably could have been either Navahrudak, Voruta, Trakai, Kernavė or Vilnius. Since the 14th century, Vilnius had been the only official capital of the state.
The Lithuanians' smaller numbers in this medieval state gave the Ruthenians (present-day Belarusians and Ukrainians) an important role in the everyday cultural life of the state. Owing to the prevalence of East Slavs and the Eastern Orthodox faith among the population in eastern and southern regions of the state, the Ruthenian language was a widely used colloquial language.
An East Slavic variety ("rus'ka mova", "Old Belarusian" or "West Russian Chancellery language"), gradually influenced by Polish, was the language of administration in the Grand Duchy of Lithuania at least since Vytautas reign until the late 17th century when it was eventually replaced by Polish language.
This period of political breakdown and reorganization also saw the rise of written local vernaculars in place of the literary and liturgical Church Slavonic language, a further stage in the evolving differentiation between the Belarusian, Russian and Ukrainian languages.
Several Lithuanian monarchs — the last being Švitrigaila in 1432–36 — relied on the Eastern Orthodox Ruthenian majority, while most monarchs and magnates increasingly came to reflect the opinions of the Roman Catholics.
Construction of Orthodox churches in some parts of present-day Belarus had been initially prohibited, as was the case of Vitebsk in 1480. On the other hand, further unification of the, mostly Orthodox, Grand Duchy with mostly Catholic Poland led to liberalization and partial solving of the religious problem. In 1511, King and Grand Duke Sigismund I the Old granted the Orthodox clergy an autonomy enjoyed previously only by Catholic clergy. The privilege was enhanced in 1531, when the Orthodox church was no longer responsible to the Catholic bishop and instead the Metropolite was responsible only to the sobor of eight Orthodox bishops, the Grand Duke and the Patriarch of Constantinople. The privilege also extended the jurisdiction of the Orthodox hierarchy over all Orthodox people.
In such circumstances, a vibrant Ruthenian culture flourished, mostly in major present-day Belarusian cities.
Despite the legal usage of the Old Ruthenian language (the predecessor of both modern Belarusian and Ukrainian languages) which was used as a chancellery language in the territory of the Grand Duchy of Lithuania, the literature was mostly non-existent, outside of several chronicles. The first Belarusian book printed with the first printing press in the Cyrillic alphabet was published in Prague in 1517, by Francysk Skaryna, a leading representative of the renaissance Belarusian culture. Soon afterwards he founded a similar printing press in Polatsk and started an extensive undertaking of publishing the Bible and other religious works there. Apart from the Bible itself, before his death in 1551 he published 22 other books, thus laying the foundations for the evolution of the Ruthenian language into the modern Belarusian language.
Polish–Lithuanian Commonwealth.
The Lublin Union of 1569 constituted the Polish–Lithuanian Commonwealth as an influential player in European politics and the largest multinational state in Europe. While Ukraine and Podlaskie became subject to the Polish Crown, present-day Belarus territory was still regarded as part of Lithuania. The new polity was dominated by much more densely populated Poland, which had 134 representatives in the Sejm as compared to 46 representatives of the Grand Duchy of Lithuania. However the Grand Duchy of Lithuania retained much autonomy, and was governed by a separate code of laws called the Lithuanian Statutes, which codified both civil and property rights. Mogilyov was the largest urban centre of the territory of present-day Belarus, followed by Vitebsk, Polotsk, Pinsk, Slutsk, and Brest, whose population exceeded 10,000. In addition, Vilna (Vilnius), the capital of the Grand Duchy of Lithuania, also had a significant Ruthenian population.
With time, the ethnic pattern did not evolve much. Throughout their existence as a separate culture, Ruthenians formed in most cases rural population, with the power held by local szlachta and boyars, often of Lithuanian, Polish or Russian descent. As in the rest of Central and Eastern Europe, the trade and commerce was mostly monopolized by Jews, who formed a significant part of the urban population. Since the Union of Horodlo of 1413, local nobility was assimilated into the traditional clan system by means of the formal procedure of adoption by the "szlachta" (Polish gentry). Eventually it formed a significant part of the szlachta. Initially mostly Ruthenian and Orthodox, with time most of them became polonized. This was especially true for major magnate families (Sapieha and Radziwiłł clans being the most notable), whose personal fortunes and properties often surpassed those of the royal families and were huge enough to be called a state within a state. Many of them founded their own cities and settled them with settlers from other parts of Europe. Indeed there were Scots, Germans and Dutch people inhabiting major towns of the area, as well as several Italian artists who had been "imported" to the lands of modern Belarus by the magnates. Contrary to Poland, in the lands of the Grand Duchy, the peasants had little personal freedom in the Middle Ages. However, with time, the magnates and the gentry gradually limited the few liberties of the serfs, at the same time increasing their taxation, often in labour for the local gentry. This made many Ruthenians flee to the scarcely populated lands, "Dzikie Pola" (Wild Fields), the Polish name of the Zaporizhian Sich, where they formed a large part of the Cossacks. Others sought refuge in the lands of other magnates or in Russia.
Also, with time the religious conflicts started to arise. The gentry with time started to adopt Catholicism while the common people by large remained faithful to Eastern Orthodoxy. Initially the Warsaw Compact of 1573 codified the preexisting freedom of worship. However, the rule of an ultra-Catholic King Sigismund III Vasa was marked by numerous attempts to spread the Catholicism, mostly through his support for counterreformation and the Jesuits. Possibly to avoid such conflicts, in 1595 the Orthodox hierarchs of Kiev signed the Union of Brest, breaking their links with the Patriarch of Constantinople and placing themselves under the Pope. Although the union was generally supported by most local Orthodox bishops and the king himself, it was opposed by some prominent nobles and, more importantly, by the nascent Cossack movement. This led to a series of conflicts and rebellions against the local authorities. The first of such happened in 1595, when the Cossack insurgents under Severyn Nalivaiko took the towns of Slutsk and Mogilyov and executed Polish magistrates there. Other such clashes took place in Mogilyov (1606–10), Vitebsk (1623), and Polotsk (1623, 1633). This left the population of the Grand Duchy divided between Greek Catholic and Greek Orthodox parts. At the same time, after the schism in the Orthodox Church (Raskol), some Old Believers migrated west, seeking refuge in the Rzeczpospolita, which allowed them to freely practice their faith.
From 1569, the Polish–Lithuanian Commonwealth suffered a series of Tatar invasions, the goal of which was to loot, pillage and capture slaves into jasyr. The borderland area to the south-east was in a state of semi-permanent warfare until the 18th century. Some researchers estimate that altogether more than 3 million people, predominantly Ukrainians but also Russians, Belarusians and Poles, were captured and enslaved during the time of the Crimean Khanate.
Despite the abovementioned conflicts, the literary tradition of Belarus evolved. Until the 17th century, the Ruthenian language, the predecessor of modern Belarusian, was used in Grand Duchy as a "chancery language", that is the language used for official documents. Afterwards, it was replaced with the Polish language, commonly spoken by the upper classes of Belarusian society. Both Polish and Ruthenian cultures gained a major cultural centre with the foundation of the Academy of Vilna. At the same time the Belarusian lands entered a path of economic growth, with the formation of numerous towns that served as centres of trade on the east-west routes.
However, both economic and cultural growth came to an end in mid-17th century with a series of violent wars against Tsardom of Russia, Sweden, Brandenburg and Transylvania, as well as internal conflicts, known collectively as The Deluge. The misfortunes were started in 1648 by Bohdan Chmielnicki, who started a large-scale Cossack uprising in Ukraine. Although the Cossacks were defeated in 1651 in the battle of Beresteczko, Khmelnytsky sought help from Russian tsar, and by the Treaty of Pereyaslav Russia dominated and partially occupied the eastern lands of the Commonwealth since 1655. The Swedes invaded and occupied the rest in the same year. The wars had shown internal problems of the state, with some people of the Grand Duchy supporting Russia while others (most notably Janusz Radziwiłł) supporting the Swedes. Although the Swedes were finally driven back in 1657 and the Russians were defeated in 1662, most of the country was ruined. It is estimated that the Commonwealth lost a third of its population, with some regions of Belarus losing as much as 50%. This broke the power of the once-powerful Commonwealth and the country gradually became vulnerable to foreign influence.
Subsequent wars in the area (Great Northern War and the War of Polish succession) damaged its economy even further. In addition, Russian armies raided the Commonwealth under the pretext of the returning of fugitive peasants. By mid-18th century their presence in the lands of modern Belarus became almost permanent.
The last attempt to save the Commonwealth's independence was a Polish–Belarusian–Lithuanian national uprising of 1794 led by Tadeusz Kościuszko, however it was eventually quenched.
Eventually by 1795 Poland was partitioned by its neighbors. Thus a new period in Belarusian history started, with all its lands annexed by the Russian Empire, in a continuing endeavor of Russian tsars of "gathering the Rus lands" started after the liberation from the Tatar yoke by Grand Duke Ivan III of Russia.
Russian Empire.
Under Russian administration, the territory of Belarus was divided into the "guberniyas" of Minsk, Vitebsk, Mogilyov, and Hrodno. Belarusians were active in the guerrilla movement against Napoleon's occupation.. With Napoleon's defeat, Belarus again became a part of Imperial Russia and its "guberniyas" constituted part of the Northwestern Krai. The anti-Russian uprisings of the gentry in 1830 and 1863 were subdued by government forces.
Although under Nicholas I and Alexander III the national cultures were repressed due to the policies of de-Polonization and Russification, which included the return to Orthodoxy, the 19th century was signified by the rise of the modern Belarusian nation and self-confidence. A number of authors started publishing in the Belarusian language, including Jan Czeczot, Władysław Syrokomla and Konstanty Kalinowski.
In a Russification drive in the 1840s, Nicholas I forbade the use of the term "Belarusia" and renamed the region the "North-Western Territory". He also prohibited the use of Belarusian language in public schools, campaigned against Belarusian publications and tried to pressure those who had converted to Catholicism under the Poles to reconvert to the Orthodox faith. In 1863, economic and cultural pressure exploded into a revolt, led by Kalinowski. After the failed revolt, the Russian government reintroduced the use of Cyrillic to Belarusian in 1864 and banned the use of the Latin alphabet.
In the second half of the 19th century, the Belarusian economy, like that of the entire Europe, was experiencing significant growth due to the spread of the Industrial Revolution to Eastern Europe, particularly after the emancipation of the serfs in 1861. Peasants sought a better lot in foreign industrial centres, with some 1.5 million people leaving Belarus in the half-century preceding the Russian Revolution of 1917.
20th century.
BNR and LBSSR.
Minsk was captured by German troops on 21 February 1918. World War I was the short period when Belarusian culture started to flourish. German administration allowed schools with Belarusian language, previously banned in Russia; a number of Belarusian schools were created until 1919 when they were banned again by the Polish military administration. At the end of World War I, when Belarus was still occupied by Germans, according to the Treaty of Brest-Litovsk, the short-lived Belarus National Republic was pronounced on March 25, 1918, as part of the German Mitteleuropa plan.
In December 1918, Mitteleuropa was obsolete as the Germans withdrew from the Ober-Ost territory, and for the next few years in the newly created political vacuum the territories of Belarus would witness the struggle of various national and foreign factions. On 3 December 1918 the Germans withdrew from Minsk. On 10 December 1918 Soviet troops occupied Minsk. The Rada (Council) of the Belarus National Republic went into exile, first to Kaunas, then to Berlin and finally to Prague. On January 2, 1919, the Soviet Socialist Republic of Byelorussia was declared. On 17 February 1919 it was disbanded. Part of it was included into RSFSR, and part was joined to the Lithuanian SSR to form the LBSSR, Lithuanian-Byelorussian Soviet Socialist Republic, informally known as "Litbel", whose capital was Vilnius. While Belarus National Republic faced off with Litbel, foreign powers were preparing to reclaim what they saw as their territories: Polish forces were moving from the West, and Russians from the East. When Vilnius was captured by Polish forces on 17 April 1919, the capital of the Soviet puppet state Litbel was moved to Minsk. On 17 July 1919 Lenin dissolved Litbel because of the pressure of Polish forces advancing from the West. Polish troops captured Minsk on 8 August 1919.
Belarusian Soviet Republic and West Belarus.
Some time in 1918 or 1919, Sergiusz Piasecki returned to Belarus, joining Belarusian anti-Soviet units, the "Green Oak" (in Polish, "Zielony Dąb"), led by Ataman Wiaczesław Adamowicz (pseudonym: J. Dziergacz). When on August 8, 1919, the Polish Army captured Minsk, Adamowicz decided to work with them. Thus Belarusian units were created, and Piasecki was transferred to a Warsaw school of infantry cadets. In the summer of 1920, during the Polish-Soviet War, Piasecki fought in the Battle of Radzymin.
The frontiers between Poland, which had established an independent government after World War I, and the former Russian Empire were not recognized by the League of Nations. Poland's Józef Piłsudski, who envisioned the formation of an Intermarum Federation as a Central and East European bloc that would be a bulwark against Germany to the west and Russia to the east, carried out a Kiev Offensive into Ukraine in 1920. This met with a Red Army counter-offensive that drove into Polish territory almost to Warsaw, Minsk itself was re-captured by the Soviet Red Army on 11 July 1920 and a new Byelorussian Soviet Socialist Republic was declared on 31 July 1920. Piłsudski, however, halted the Soviet advance at the Battle of Warsaw and resumed his eastward offensive. Finally the Treaty of Riga, ending the Polish–Soviet War, divided Belarus between Poland and Soviet Russia. Over the next two years, the Belarus National Republic prepared a national uprising, ceasing the preparations only when the League of Nations recognized the Soviet Union's western borders on March 15, 1923.
The Polish part of Belarus was subject to Polonization policies (especially in the 1930s), while the Soviet Belarus was one of the original republics which formed the USSR. For several years, the national culture and language enjoyed a significant boost of revival in the Soviet Belarus. This was however soon ended during the Great Purge, when almost all prominent Belarusian national intelligentsia were executed, many of them buried in Kurapaty. Thousands were deported to Asia. As the result of Polish operation of the NKVD tens of thousands people of many nationalities were killed. Belarusian orthography was Russified in 1933 and use of Belarusian language was discouraged as exhibiting anti-soviet attitude.
In West Belarus, up to 30 000 families of Polish veterans ("osadniks") were settled in the lands formerly belonging to the Russian tsar family and Russian aristocracy. Belarusian representation in Polish parliament was reduced as a result of the 1930 elections. Since the early 1930s, the Polish government introduced a set of policies designed to Polonize all minorities (Belarusians, Ukrainians, Jews, etc.). The usage of Belarusian language was discouraged and the Belarusian schools were facing severe financial problems. In spring of 1939, there already was neither single Belarusian official organisation in Poland nor a single Belarusian school (with only 44 schools teaching Belarusian language left).
Belarus in World War II.
When the Soviet Union invaded Poland on September 17, 1939, following the terms of the Molotov–Ribbentrop Pact's secret protocol, much of what had been eastern Poland was annexed to the BSSR. Similarly to the times of German occupation during World War I, Belarusian language and Soviet culture enjoyed relative prosperity in this short period. Already in October 1940, over 75% of schools used the Belarusian language, also in the regions where no Belarus people lived, e.g. around Łomża, what was Ruthenization. After twenty months of Soviet rule, Germany and its Axis allies invaded the Soviet Union on June 22, 1941. Soviet authorities immediately evacuated about 20% of the population of Belarus and destroyed all the food supplies. The country suffered particularly heavily during the fighting and the German occupation. Minsk was captured by the Germans on 28 June 1941. Following bloody encirclement battles, all of the present-day Belarus territory was occupied by the Germans by the end of August 1941.
During World War II, the Nazis attempted to establish a puppet Belarusian government, Belarusian Central Rada, with the symbolics similar to BNR. In reality, however, the Germans imposed a brutal racist regime, burning down some 9 000 Belarusian villages, deporting some 380,000 people for slave labour, and killing hundreds of thousands of civilians more. Local police took part in many of those crimes. Almost the whole, previously very numerous, Jewish populations of Belarus that did not evacuate were killed. One of the first uprisings of a Jewish ghetto against the Nazis occurred in 1942 in Belarus, in the small town of Lakhva.
Since the early days of the occupation, a powerful and increasingly well-coordinated Belarusian resistance movement emerged. Hiding in the woods and swamps, the partisans inflicted heavy damage to German supply lines and communications, disrupting railway tracks, bridges, telegraph wires, attacking supply depots, fuel dumps and transports and ambushing German soldiers. Not all anti-German partisans were pro-Soviet. In the largest partisan sabotage action of the entire Second World War, the so-called Asipovichy diversion of 30 July 1943 four German trains with supplies and Tiger tanks were destroyed. To fight partisan activity, the Germans had to withdraw considerable forces behind their front line. On 22 June 1944 the huge Soviet offensive Operation Bagration was launched, Minsk was re-captured on 3 July 1944, and all of Belarus was regained by the end of August. Hundred thousand of Poles were expelled after 1944. As part of the Nazis' effort to combat the enormous Belarusian resistance during World War II, special units of local collaborationists were trained by the SS's Otto Skorzeny to infiltrate the Soviet rear. In 1944 thirty Belarusians (known as Čorny Kot ("Black Cat") and personally led by Michał Vituška) were airdropped by the Luftwaffe behind the lines of the Red Army, which had already liberated Belarus during Operation Bagration. They experienced some initial success due to disorganization in the rear of the Red Army, and some other German-trained Belarusian nationalist units also slipped through the Białowieża Forest in 1945. The NKVD, however, had already infiltrated these units. Vituška managed to escape to the West following the war, along with several other Belarusian Central Rada leaders.
In total, Belarus lost a quarter of its pre-war population in World War II including practically all its intellectual elite. About 9 200 villages and 1.2 million houses were destroyed. The major towns of Minsk and Vitsebsk lost over 80% of their buildings and city infrastructure. For the defence against the Germans, and the tenacity during the German occupation, the capital Minsk was awarded the title "Hero City" after the war. The fortress of Brest was awarded the title "Hero-Fortress".
BSSR from 1945 to 1990.
After the end of War in 1945, Belarus became one of the founding members of the United Nations Organisation. Joining Belarus was the Soviet Union itself and another republic Ukraine. In exchange for Belarus and Ukraine joining the UN, the United States had the right to seek two more votes, a right that has never been exercised.
The Belarusian economy was completely devastated by the events of the war. Most of the industry, including whole production plants were removed either to Russia or Germany. Industrial production of Belarus in 1945 amounted for less than 20% of its pre-war size. Most of the factories evacuated to Russia, with several spectacular exceptions, were not returned to Belarus after 1945. During the immediate postwar period, the Soviet Union first rebuilt and then expanded the BSSR's economy, with control always exerted exclusively from Moscow. During this time, Belarus became a major center of manufacturing in the western region of the USSR. Huge industrial objects like the BelAZ, MAZ, and the Minsk Tractor Plant were built in the country. The increase in jobs resulted in a huge immigrant population of Russians in Belarus. Russian became the official language of administration and the peasant class, which traditionally was the base for Belarusian nation, ceased to exist.
On April 26, 1986, the Chernobyl disaster occurred at the Chernobyl nuclear power plant in Ukraine situated close to the border with Belarus. It is regarded as the worst nuclear accident in the history of nuclear power. It produced a plume of radioactive debris that drifted over parts of the western Soviet Union, Eastern Europe, and Scandinavia. Large areas of Belarus, Ukraine and Russia were contaminated, resulting in the evacuation and resettlement of roughly 200,000 people. About 60% of the radioactive fallout landed in Belarus. The effects of the Chernobyl accident in Belarus were dramatic: about 50,000 km² (or about a quarter of the territory of Belarus) formerly populated by 2.2 million people (or a fifth of the Belarusian population) now require permanent radioactive monitoring (after receiving doses over 37 kBq/m² of caesium-137). 135,000 persons were permanently resettled and many more were resettled temporarily. After 10 years since the accident, the occurrences of thyroid cancer among children increased fifteenfold (the sharp rise started in about four years after the accident).
Republic of Belarus.
On 27 July 1990, Belarus declared its national sovereignty, a key step toward independence from the Soviet Union. The BSSR was formally renamed the Republic of Belarus on 25 August 1991. Around that time, Stanislav Shushkevich became the chairman of the Supreme Soviet of Belarus, the top leadership position in Belarus. On December 8, 1991, Shushkevich met with Boris Yeltsin of Russia and Leonid Kravchuk of Ukraine, in Belavezhskaya Pushcha, to formally declare the dissolution of the Soviet Union and the formation of the Commonwealth of Independent States.
In 1994, the first presidential elections were held and Alexander Lukashenko was elected president of Belarus. The 1996 referendum resulted in the amendment of the constitution that took key powers off the parliament. In 2001, he was re-elected as president in elections described as undemocratic by Western observers. At the same time the west began criticising him of authoritarianism. In 2006, Lukashenko was once again re-elected in presidential elections which were again criticised as flawed by most European Union countries. In 2010, Lukashenko was re-elected once again in presidential elections, which were described as flawed by most EU countries and institutions. A peaceful protest against the electoral fraud was attacked by riot police and by armed men dressed in black. After that, up to 700 opposition activists, including 7 presidential candidates, were arrested by KGB.

</doc>
<doc id="42762" url="http://en.wikipedia.org/wiki?curid=42762" title="Java Naming and Directory Interface">
Java Naming and Directory Interface

The Java Naming and Directory Interface (JNDI) is a Java API for a directory service that allows Java software clients to discover and look up data and objects via a name. Like all Java APIs that interface with host systems, JNDI is independent of the underlying implementation. Additionally, it specifies a service provider interface (SPI) that allows directory service implementations to be plugged into the framework. It may make use of a server, a flat file, or a database; the choice is up to the vendor.
Typical uses of JNDI include:
Background.
The Java RMI and Java EE APIs use the JNDI API to look up objects in a network.
The API provides:
The SPI portion allows support for practically any kind of naming or directory service, including:
Sun Microsystems first released the JNDI specification on March 10, 1997. s of 2006[ [update]], the current version is JNDI 1.2.
Basic lookup.
JNDI (Java Naming and Directory Interface) organizes its names into a hierarchy. A name can be any string such as "com.mydomain.ejb.MyBean". A name can also be an object that implements the codice_1 interface, however a string is the most common way to name an object. A name is bound to an object in the directory by storing either the object or a reference to the object in the directory service identified by the name.
The JNDI API defines a context that specifies where to look for an object. The initial context is typically used as a starting point.
In the simplest case, an initial context must be created using the specific implementation and extra parameters required by the implementation. The initial context will be used to look up a name. The initial context is analogous to the root or top of a directory tree for a file system. Below is an example of creating an initial context:
A context is then used to look up previously bound names in that context. For example:
Alternative to above code is as below:
The Context object can also be configured by adding jndi.properties file in classpath containing initial context factory class name and provider URL. The above code will be reduced as shown below:
A context is then used to look up previously bound names in that context. For example:
Searching.
Attributes may be attached to special entries called directories. Directories enable searching for objects by their associated attributes. Directories are a type of context; they restrict the name space much like a directory structure on a file system does.

</doc>
<doc id="42764" url="http://en.wikipedia.org/wiki?curid=42764" title="Hagia Sophia">
Hagia Sophia

Hagia Sophia (from the Greek: Ἁγία Σοφία, "Holy Wisdom"; Latin: "Sancta Sophia" or "Sancta Sapientia"; Turkish: "Ayasofya") is a former Greek Orthodox patriarchal basilica (church), later an imperial mosque, and now a museum (Ayasofya Müzesi) in Istanbul, Turkey. From the date of its construction in 537 until 1453, it served as an Eastern Orthodox cathedral and seat of the Patriarchate of Constantinople, except between 1204 and 1261, when it was converted to a Roman Catholic cathedral under the Latin Empire. The building was a mosque from 29 May 1453 until 1931. It was then secularized and opened as a museum on 1 February 1935.
The church was dedicated to the "Wisdom of God", the Logos, the second person of the Holy Trinity, its patronal feast taking place on 25 December, the commemoration of the birth of the incarnation of the Logos in Christ. Although sometimes referred to as Sancta Sophia (as though it were named after Saint Sophia), "sophia" being the phonetic spelling in Latin of the Greek word for wisdom, its full name in Greek is Ναός τῆς Ἁγίας τοῦ Θεοῦ Σοφίας, "Shrine of the Holy Wisdom of God".
Famous in particular for its massive dome, it is considered the epitome of Byzantine architecture and is said to have "changed the history of architecture". It remained the world's largest cathedral for nearly a thousand years, until Seville Cathedral was completed in 1520.
The current building was originally constructed as a church between 532 and 537 on the orders of the Byzantine Emperor Justinian I and was the third Church of the Holy Wisdom to occupy the site, the previous two having both been destroyed by rioters. It was designed by the Greek scientists Isidore of Miletus and Anthemius of Tralles.
The church contained a large collection of holy relics and featured, among other things, a 15 m silver iconostasis. The focal point of the Eastern Orthodox Church for nearly one thousand years, the building witnessed the excommunication of Patriarch Michael I Cerularius on the part of Pope Leo IX in 1054, an act which is commonly considered the start of the Great Schism.
In 1453, Constantinople was conquered by the Ottoman Turks under Sultan Mehmed II, who ordered this main church of the Orthodox Christianity converted into a mosque. By that point, the church had fallen into a state of disrepair. Nevertheless, the Christian cathedral made a strong impression on the new Ottoman rulers and they decided to convert it into a mosque.<ref name="http://www.livescience.com/27574-hagia-sophia.html">." LiveScience.</ref> The bells, altar, iconostasis, and sacrificial vessels and other relics were removed and the mosaics depicting Jesus, his Mother Mary, Christian saints and angels were also removed or plastered over. Islamic features—such as the mihrab, minbar, and four minarets—were added. It remained a mosque until 1931, when it was closed to the public for four years. It was re-opened in 1935 as a museum by the Republic of Turkey. Hagia Sophia is currently (2014) the second-most visited museum in Turkey, attracting almost 3.3 million visitors annually.
From its initial conversion until the construction of the nearby larger Sultan Ahmed Mosque (Blue Mosque of Istanbul) in 1616, it was the principal mosque of Istanbul. The Hagia Sophia served as inspiration for many other Ottoman mosques, such as the Blue Mosque, the Şehzade Mosque, the Süleymaniye Mosque, the Rüstem Pasha Mosque and the Kılıç Ali Paşa Mosque.
History.
First church.
The first church on the site was known as the Μεγάλη Ἐκκλησία ("Megálē Ekklēsíā", "Great Church"), or in Latin "Magna Ecclesia", because of its larger dimensions in comparison to the contemporary churches in the City. Inaugurated on 15 February 360 (during the reign of Constantius II) by the Arian bishop Eudoxius of Antioch, it was built next to the area where the imperial palace was being developed. The nearby Hagia Eirene ("Holy Peace") church was completed earlier and served as cathedral until the Great Church was completed. Both churches acted together as the principal churches of the Byzantine Empire.
Writing in 440, Socrates of Constantinople claimed that the church was built by Constantius II, who was working on it in 346. A tradition which is not older than the 7th – 8th century, reports that the edifice was built by Constantine the Great. Zonaras reconciles the two opinions, writing that Constantius had repaired the edifice consecrated by Eusebius of Nicomedia, after it had collapsed. Since Eusebius was bishop of Constantinople from 339 to 341, and Constantine died in 337, it seems possible that the first church was erected by the latter. The edifice was built as a traditional Latin colonnaded basilica with galleries and a wooden roof. It was preceded by an atrium. It was claimed to be one of the world's most outstanding monuments at the time.
The Patriarch of Constantinople John Chrysostom came into a conflict with Empress Aelia Eudoxia, wife of the emperor Arcadius, and was sent into exile on 20 June 404. During the subsequent riots, this first church was largely burned down. Nothing remains of the first church today.
Second church.
A second church was ordered by Theodosius II, who inaugurated it on 10 October 415. The basilica with a wooden roof was built by architect Rufinus. A fire started during the tumult of the Nika Revolt and burned the second Hagia Sophia to the ground on 13–14 January 532.
Several marble blocks from the second church survive to the present; among them are reliefs depicting 12 lambs representing the 12 apostles. Originally part of a monumental front entrance, they now reside in an excavation pit adjacent to the museum's entrance after they were discovered in 1935 beneath the western courtyard by A. M. Schneider. Further digging was forsaken for fear of impinging on the integrity of the building.
Third church (current structure).
On 23 February 532, only a few weeks after the destruction of the second basilica, Emperor Justinian I decided to build a third and entirely different basilica, larger and more majestic than its predecessors.
Justinian chose physicist Isidore of Miletus and mathematician Anthemius of Tralles as architects; Anthemius, however, died within the first year of the endeavor. The construction is described in the Byzantine historian Procopius' "On Buildings" ("Peri ktismatōn", Latin: "De aedificiis"). The emperor had material brought from all over the empire – such as Hellenistic columns from the Temple of Artemis at Ephesus, large stones from quarries in porphyry from Egypt, green marble from Thessaly, black stone from the Bosporus region, and yellow stone from Syria. More than ten thousand people were employed. This new church was contemporaneously recognized as a major work of architecture. The theories of Heron of Alexandria may have been utilized to address the challenges presented by building such an expansive dome over so large a space. The emperor, together with the Patriarch Menas, inaugurated the new basilica on 27 December 537 – 5 years and 10 months after construction start - with much pomp. The mosaics inside the church were, however, only completed under the reign of Emperor Justin II (565–578).
Hagia Sophia was the seat of the Orthodox patriarch of Constantinople and a principal setting for Byzantine imperial ceremonies, such as coronations. Like other churches throughout Christendom, the basilica offered sanctuary from persecution to outlaws.
Earthquakes in August 553 and on 14 December 557 caused cracks in the main dome and eastern half-dome. The main dome collapsed completely during a subsequent earthquake on 7 May 558, destroying the ambon, altar, and ciborium. The crash was due mainly to the too high bearing load and to the enormous shearing load of the dome, which was too flat. These caused the deformation of the piers which sustained the dome. The emperor ordered an immediate restoration. He entrusted it to Isidorus the Younger, nephew of Isidore of Miletus, who used lighter materials and elevated the dome by "30 feet" (about 6.25 m) – giving the building its current interior height of 55.6 m. Moreover, Isidorus changed the dome type, erecting a ribbed dome with pendentives, whose diameter lay between 32.7 and 33.5 m. Under Justinian's orders, eight Corinthian columns were disassembled from Baalbek, Lebanon, and shipped to Constantinople around 560. This reconstruction, giving the church its present 6th-century form, was completed in 562. The Byzantine poet Paul the Silentiary composed a long epic poem (still extant), known as "Ekphrasis", for the rededication of the basilica presided over by Patriarch Eutychius on 23 December 562.
In 726, the emperor Leo the Isaurian issued a series of edicts against the veneration of images, ordering the army to destroy all icons – ushering in the period of Byzantine iconoclasm. At that time, all religious pictures and statues were removed from the Hagia Sophia. After a brief reprieve under Empress Irene (797–802), the iconoclasts made a comeback. Emperor Theophilus (829–842) was strongly influenced by Islamic art, which forbids the representation of living beings. He had a two-winged bronze door with his monograms installed at the southern entrance of the church.
The basilica suffered damage, first in a great fire in 859, and again in an earthquake on 8 January 869, that made a half-dome collapse. Emperor Basil I ordered the church repaired.
After the great earthquake of 25 October 989, which collapsed the Western dome arch, Emperor Basil II asked for the Armenian architect Trdat, creator of the great churches of Ani and Argina, to direct the repairs. He erected again and reinforced the fallen dome arch, and rebuilt the west side of the dome with 15 dome ribs. The extent of the damage required six years of repair and reconstruction; the church was re-opened on 13 May 994. At the end of the reconstruction, the church's decorations were renovated, including the additions of paintings of four immense cherubs, a new depiction of Christ on the dome, and on the apse a new depiction of the Virgin Mary holding Jesus between the apostles Peter and Paul. On the great side arches were painted the prophets and the teachers of the church.
In his book "De caerimoniis aulae Byzantinae" ("Book of Ceremonies"), Emperor Constantine VII (913–919) wrote a detailed account of the ceremonies held in the Hagia Sophia by the emperor and the patriarch.
Upon the capture of Constantinople during the Fourth Crusade, the church was ransacked and desecrated by the Latin Christians, as described by the Byzantine historian Niketas Choniates. During the Latin occupation of Constantinople (1204–1261) the church became a Roman Catholic cathedral. Baldwin I of Constantinople was crowned emperor on 16 May 1204 in Hagia Sophia, at a ceremony which closely followed Byzantine practices. Enrico Dandolo, the Doge of Venice who commanded the sack and invasion of the city by the Latin Crusaders in 1204, is buried inside the church. The tomb inscription carrying his name, which has become a part of the floor decoration, was spat upon by many of the angry Byzantines who recaptured Constantinople in 1261. However, restoration led by the brothers Gaspare and Giuseppe Fossati during the period 1847–1849 cast doubt upon the authenticity of the doge's grave; it is more likely a symbolic memorial rather than burial site.
After the recapture in 1261 by the Byzantines, the church was in a dilapidated state. In 1317, emperor Andronicus II ordered four new buttresses (Πυραμὶδας, Greek:"Piramídas") to be built in the eastern and northern parts of the church, financing them with the inheritance of his deceased wife, Irene. New cracks developed in the dome after the earthquake of October 1344, and several parts of the building collapsed on 19 May 1346; consequently, the church was closed until 1354, when repairs were undertaken by architects Astras and Peralta.
Mosque (1453–1935).
Constantinople was taken by the Ottomans on 29 May 1453. In accordance with the custom at the time Sultan Mehmet II allowed his troops three days of unbridled pillage once the city fell, after which he would claim its contents for himself.
Hagia Sophia was not exempted from the pillage, becoming its focal point as the invaders believed it to contain the greatest treasures of the city.
Shortly after the city's defenses collapsed, pillagers made their way to the Hagia Sophia and battered down its doors. Throughout the siege worshipers participated in the Holy Liturgy and Prayer of the Hours at the Hagia Sophia, and the church formed a refuge for many of those who were unable to contribute to the city's defense, such as women, children and elderly. Trapped in the church, congregants and refugees became spoils to be divided amongst the Ottoman invaders. The building was desecrated and looted, and occupants enslaved, violated or slaughtered; while elderly and infirm were killed, women and girls were raped and the remainder chained and sold into slavery. Priests continued to perform Christian rites until stopped by the invaders. When the Sultan and his cohort entered the church, he insisted it should be at once transformed into a mosque. One of the Ulama then climbed the pulpit and recited the Shahada.
As described by several Western visitors (such as the Córdoban nobleman Pero Tafur and the Florentine Cristoforo Buondelmonti), the church was in a dilapidated state, with several of its doors fallen from their hinges; Mehmed II ordered a renovation as well as the conversion. Mehmet attended the first Friday prayer in the mosque on 1 June 1453. Aya Sofya became the first imperial mosque of Istanbul. To the corresponding Waqf were endowed most of the existing houses in the city and the area of the future Topkapı Palace. From 1478, 2,360 shops, 1,300 houses, 4 caravanserais, 30 "boza" shops, and 23 shops of sheep heads and trotters gave their income to the foundation. Through the imperial charters of 1520 (AH 926) and 1547 (AH 954) shops and parts of the Grand Bazaar and other markets were added to the foundation.
Before 1481 a small minaret was erected on the southwest corner of the building, above the stair tower. Later, the subsequent sultan, Bayezid II (1481–1512), built another minaret at the northeast corner. One of these collapsed after the earthquake of 1509, and around the middle of the 16th century they were both replaced by two diagonally opposite minarets built at the east and west corners of the edifice.
In the 16th century the sultan Suleiman the Magnificent (1520–1566) brought back two colossal candlesticks from his conquest of Hungary. They were placed on either side of the mihrab. During the reign of Selim II (1566–1574), the building started showing signs of fatigue and was extensively strengthened with the addition of structural supports to its exterior by the great Ottoman architect Mimar Sinan, who is also considered one of the world's first earthquake engineers. In addition to strengthening the historic Byzantine structure, Sinan built the two additional large minarets at the western end of the building, the original sultan's lodge, and the Türbe (mausoleum) of Selim II to the southeast of the building in 1576-7 / AH 984. In order to do that, one year before parts of the Patriarchate at the south corner of the building were pulled down. Moreover, the golden crescent was mounted on the top of the dome, while a respect zone 35 "arşin" (about 24 m) wide was imposed around the building, pulling down all the houses which in the meantime had nested around it. Later his türbe hosted also 43 tombs of Ottoman princes. In 1594 / AH 1004 "Mimar" (court architect) Davud Ağa built the türbe of Murad III (1574–1595), where the Sultan and his Valide, Safiye Sultan were later buried. The octagonal mausoleum of their son Mehmed III (1595–1603) and his Valide was built next to it in 1608 / 1017 H by royal architect Dalgiç Mehmet Aĝa. His son Mustafa I (1617–1618; 1622–1623) converted the baptistery into his türbe.
Murad III had also two large alabaster Hellenistic urns transported from Pergamon and placed on two sides of the nave.
In 1717, under Sultan Ahmed III (1703–1730), the crumbling plaster of the interior was renovated, contributing indirectly to the preservation of many mosaics, which otherwise would have been destroyed by mosque workers. In fact, it was usual for them to sell mosaics stones – believed to be talismans – to the visitors. Sultan Mahmud I ordered the restoration of the building in 1739 and added a "medrese" (a Koranic school, now the library of the museum), an "Imaret" (soup kitchen for distribution to the poor) and a library, and in 1740 a "Şadirvan" (fountain for ritual ablutions), thus transforming it into a "külliye", i.e. a social complex. At the same time a new sultan's lodge and a new mihrab were built inside.
The most famous restoration of the Aya Sofya was ordered by Sultan Abdülmecid and completed by eight hundred workers between 1847 and 1849, under the supervision of the Swiss-Italian architect brothers Gaspare and Giuseppe Fossati. The brothers consolidated the dome and vaults, straightened the columns, and revised the decoration of the exterior and the interior of the building. The mosaics in the upper gallery were uncovered and cleaned, although many were recovered "for protection against further damage". The old chandeliers were replaced by new pendant ones. New gigantic circular-framed disks or medallions were hung on columns. These were inscribed with the names of Allah, Muhammad, the first four caliphs Abu Bakr, Umar, Uthman and Ali, and the two grandchildren of Muhammad: Hassan and Hussain, by the calligrapher Kazasker Mustafa İzzed Effendi (1801–1877). In 1850 the architect Fossati built a new sultan's lodge or loge in a Neo-Byzantine style connected to the royal pavilion behind the mosque. They also renovated the minbar and mihrab. Outside the main building, the minarets were repaired and altered so that they were of equal height. A timekeeper's building and a new madrasah were built. When the restoration was finished, the mosque was re-opened with ceremonial pomp on 13 July 1849.
Museum (1935–present).
In 1935, the first Turkish President and founder of the Republic of Turkey, Mustafa Kemal Atatürk, transformed the building into a museum. The carpets were removed and the marble floor decorations such as the Omphalion appeared for the first time in centuries, while the white plaster covering many of the mosaics was removed. Nevertheless, the condition of the structure deteriorated, and the World Monuments Fund placed Hagia Sophia on 1996 World Monuments Watch, and again in 1998. The building's copper roof had cracked, causing water to leak down over the fragile frescoes and mosaics. Moisture entered from below as well. Rising ground water had raised the level of humidity within the monument, creating an unstable environment for stone and paint. With the help of financial services company American Express, WMF secured a series of grants from 1997 to 2002 for the restoration of the dome. The first stage of work involved the structural stabilization and repair of the cracked roof, which was undertaken with the participation of the Turkish Ministry of Culture. The second phase, the preservation of the dome's interior, afforded the opportunity to employ and train young Turkish conservators in the care of mosaics. By 2006, the WMF project was complete, though many other areas of Hagia Sophia continue to require significant stability improvement, restoration and conservation. Haghia Sophia is currently (2014) the second most visited museum in Turkey, attracting almost 3.3 million visitors annually.
Although use of the complex as a place of worship (mosque or church) was strictly prohibited, in 2006 the Turkish government allowed the allocation of a small room in the museum complex to be used as a prayer room for Christian and Muslim museum staff, and since 2013 from the minarets of the museum the muezzin sings the call to prayer twice per day, in the afternoon.
In 2007, Greek American politician Chris Spirou launched an international organization "Free Agia Sophia Council" championing the cause of restoring the building to its original function as a Christian church. Since the early 2010s, several campaigns and government high officials, notably Turkey's deputy prime minister Bülent Arınç in November 2013, have been demanding that Hagia Sophia be converted into a mosque again. In an attempt to retaliate against Pope Francis after his acknowledgment of the Armenian Genocide, the Mufti of Ankara, Mefail Hızlı, stated that the conversion of Haghia Sophia into a mosque will be accelerated.
Architecture.
Hagia Sophia is one of the greatest surviving examples of Byzantine architecture. Its interior is decorated with mosaics and marble pillars and coverings of great artistic value. The temple itself was so richly and artistically decorated that Justinian proclaimed, "Solomon, I have outdone thee!" (Νενίκηκά σε Σολομών). Justinian himself had overseen the completion of the greatest cathedral ever built up to that time, and it was to remain the largest cathedral for 1,000 years up until the completion of the cathedral in Seville in Spain.
Justinian's basilica was at once the culminating architectural achievement of late antiquity and the first masterpiece of Byzantine architecture. Its influence, both architecturally and liturgically, was widespread and enduring in the Eastern Orthodox, Roman Catholic, and Muslim worlds alike.
The vast interior has a complex structure. The nave is covered by a central dome which at its maximum is 55.6 m from floor level and rests on an arcade of 40 arched windows. Repairs to its structure have left the dome somewhat elliptical, with the diameter varying between 31.24 and.
At the western entrance side and eastern liturgical side, there are arched openings extended by half domes of identical diameter to the central dome, carried on smaller semi-domed exedras; a hierarchy of dome-headed elements built up to create a vast oblong interior crowned by the central dome, with a clear span of 76.2 m.
Interior surfaces are sheathed with polychrome marbles, green and white with purple porphyry, and gold mosaics.
The exterior, clad in stucco, was tinted yellow and red during restorations in the 19th century at the direction of the Fossati architects.
Narthex and portals.
The Imperial Gate was the main entrance between the exo- and esonarthex. It was reserved only for the emperor. The Byzantine mosaic above the portal depicts Christ and an unnamed Emperor.
A long ramp from the northern part of the outer narthex leads up to the upper gallery.
Upper Gallery.
The upper gallery is laid out in a horseshoe shape that encloses the nave until the apse. Several mosaics are preserved in the upper gallery, an area traditionally reserved for the empress and her court. The best-preserved mosaics are located in the southern part of the gallery.
The upper gallery contains runic graffiti presumed to be from the Varangian Guard.
Dome.
The dome of Hagia Sophia has spurred particular interest for many art historians, architects and engineers because of the innovative way the original architects envisioned it. The cupola is carried on four spherical triangular pendentives, an element which was first fully realized in this building. The pendentives implement the transition from the circular base of the dome to the rectangular base below, restraining the lateral forces of the dome and allow its weight to flow downwards. They were reinforced with buttresses during Byzantine and later during Ottoman times, under the guidance of the architect Sinan. The weight of the dome remained a problem for most of the building's existence. The original cupola collapsed entirely after the quake of 558; in 563 a new dome was built by Isidore the younger, a nephew of Isidore of Miletus. Unlike the original, this included 40 ribs and was slightly taller, in order to lower the lateral forces on the church walls. A larger section of the second dome collapsed as well, in two episodes, so that today only two sections of the present dome, in the north and south side, still date from the 562 reconstruction. Of the whole dome's 40 ribs, the surviving north section contains 8 ribs, while the south section includes 6 ribs.
Although this design stabilizes the dome and the surrounding walls and arches, the actual construction of the walls of Hagia Sophia weakened the overall structure. The bricklayers used more mortar than brick, weakening the walls. The structure would have been more stable if the builders at least let the mortar cure before they began the next layer; however, they did not do this. When the dome was erected, its weight caused the walls to lean outward because of the wet mortar underneath. When Isidore the Younger rebuilt the fallen cupola, he had to first build up the interior of the walls to make them vertical again. Additionally, the architect raised the height of the rebuilt dome by approximately six metres so that the lateral forces would not be as strong and its weight would flow more easily down into the walls. Moreover, he shaped the new cupola like a scalloped shell or the inside of an umbrella, with ribs that extend from the top down to the base. These ribs allow the weight of the dome to flow between the windows, down the pendentives, and ultimately to the foundation.
Hagia Sophia is famous for the light that reflects everywhere in the interior of the nave, giving the dome the appearance of hovering above this. This effect was achieved by inserting forty windows around the base of the original structure. Moreover, the insertion of the windows in the dome structure lowers its weight.
Minarets.
One of the minarets (at southwest) was built from red brick while the other three were built from white limestone and sand stone; of which the slender one at northeast was erected by Sultan Bayezid II while the two larger minarets at west were erected by Sultan Selim II and designed by the famous Ottoman architect Mimar Sinan.
Notable elements and decorations.
Originally, under Justinian's reign, the interior decorations consisted of abstract designs on marble slabs on the walls and floors, as well as mosaics on the curving vaults. Of these mosaics, one can still see the two archangels Gabriel and Michael in the spandrels of the bema. There were already a few figurative decorations, as attested by the eulogy of Paul the Silentiary. The spandrels of the gallery are revetted in "opus sectile", showing patterns and figures of flowers and birds in precisely cut pieces of white marble set against a background of black marble. In later stages figurative mosaics were added, which were destroyed during the iconoclastic controversy (726–843). Present mosaics are from the post-iconoclastic period. The number of treasures, relics and miracle-working, painted icons of the Hagia Sophia grew progressively richer into an amazing collection. Apart from the mosaics, a large number of figurative decorations were added during the second half of the 9th century: an image of Christ in the central dome; Orthodox saints, prophets and Church Fathers in the tympana below; historical figures connected with this church, such as Patriarch Ignatius; some scenes from the gospel in the galleries.
Basil II let paint on each of the four pendentives a giant six-winged Cherub. The Ottomans covered their face with a golden halo, but in 2009 one of them was restored to the original state.
Loge of the Empress.
The Loge of the Empress is located in the centre of the upper enclosure, or gallery, of the Hagia Sophia. From there the empress and the court-ladies would watch the proceedings down below. A round, green stone marks the spot where the throne of the empress stood.
Lustration urns.
Two huge marble lustration (ritual purification) urns were brought from Pergamon during the reign of Sultan Murad III. Stemming from the Hellenistic period, they are carved from single blocks of marble.
Marble Door.
The Marble Door inside the Hagia Sophia is located in the southern upper enclosure, or gallery. It was used by the participants in synods, they entered and left the meeting chamber through this door.
Wishing column.
At the northwest of the building there is a column with a hole in the middle covered by bronze plates. This column goes by different names; the perspiring column, the wishing column, the sweating column or the crying column. The column is said to be damp when touched and have supernatural powers. The legend states that since St. Gregory the Miracle Worker appeared at the column in year 1200, the column is moist. It is believed that touching the moisture cures many illnesses.
Mosaics.
The church was richly decorated with mosaics throughout the centuries. They either depicted the Virgin Mother, Jesus, saints, or emperors and empresses. Other parts were decorated in a purely decorative style with geometric patterns.
The mosaics however for their most part date to after the end of the Byzantine Iconoclasm of 800 AD.
During the Sack of Constantinople in 1204, the Latin Crusaders vandalized valuable items in every important Byzantine structure of the city, including the golden mosaics of the Hagia Sophia. Many of these items were shipped to Venice, whose Doge, Enrico Dandolo, had organized the invasion and sack of Constantinople.
Following the building's conversion into a mosque in 1453, many of its mosaics were covered with plaster, due to Islam's ban on representational imagery. This process was not completed at once, and reports exist from the 17th century in which travellers note that they could still see Christian images in the former church. In 1847–49, the building was restored by two Swiss Italian Fossati brothers, Gaspare and Giuseppe, and Sultan Abdülmecid allowed them to also document any mosaics they might discover during this process. This work did not include repairing the mosaics and after recording the details about an image, the Fossatis painted it over again. The Fossatis restored the mosaics of the two "hexapteryga" (singular Greek: εξαπτέρυγον, pr. hexapterygon, six-winged angel); it is uncertain whether they are seraphim or cherubim) located on the two east pendentives, covering their faces again before the end of the restoration. The other two placed on the west pendentives are copies in paint created by the Fossatis, since they could find no surviving remains of them. As in this case, the architects reproduced in paint damaged decorative mosaic patterns, sometimes redesigning them in the process. The Fossati records are the primary sources about a number of mosaic images now believed to have been completely or partially destroyed in the 1894 Istanbul earthquake. These include a mosaic over a now-unidentified "Door of the Poor", a large image of a jewel-encrusted cross, and a large number of images of angels, saints, patriarchs, and church fathers. Most of the missing images were located in the building's two tympana.
One mosaic they documented is Christ Pantocrator in a circle, which would indicate it to be a ceiling mosaic, possibly even of the main dome which was later covered and painted over with Islamic calligraphy that expounds God as the light of the universe. The drawings of the Hagia Sophia mosaics are today kept in the Cantonal Archive of Ticino.
Imperial Gate mosaic.
The Imperial Gate mosaic is located in the tympanum above that gate, which was used only by the emperors when entering the church. Based on style analysis, it has been dated to the late 9th or early 10th century. The emperor with a nimbus or halo could possibly represent emperor Leo VI the Wise or his son Constantine VII Porphyrogenitus bowing down before Christ Pantocrator, seated on a jeweled throne, giving His blessing and holding in His left hand an open book. The text on the book reads as follows: "Peace be with you. I am the light of the world". (John 20:19; 20:26; 8:12) On each side of Christ's shoulders is a circular medallion: on His left the Archangel Gabriel, holding a staff, on His right His Mother Mary.
Southwestern entrance mosaic.
The southwestern entrance mosaic, situated in the tympanum of the southwestern entrance, dates from the reign of Basil II. It was rediscovered during the restorations of 1849 by Fossati. The Virgin sits on a throne without a back, her feet resting on a pedestal, embellished with precious stones. The Child Christ sits on her lap, giving His blessing and holding a scroll in His left hand. On her left side stands emperor Constantine in ceremonial attire, presenting a model of the city to Mary. The inscription next to him says: "Great emperor Constantine of the Saints". On her right side stands emperor Justinian I, offering a model of the Hagia Sophia. The medallions on both sides of the Virgin's head carry the monograms MP and ΘY, an abbreviation of ""Mētēr" and "Theou"", meaning "Mother of God".
Apse mosaics.
The Virgin and Child mosaic was the first of the post-iconoclastic mosaics. It was inaugurated on 29 March 867 by Patriarch Photius and the emperors Michael III and Basil I. This mosaic is situated in a high location on the half dome of the apse. Mary is sitting on a throne without a back, holding the Child Jesus on her lap. Her feet rest on a pedestal. Both the pedestal and the throne are adorned with precious stones. The portraits of the archangels Gabriel and Michael (largely destroyed) in the bema of the arch also date from the 9th century. The mosaics are set against the original golden background of the 6th century. These mosaics were believed to be a reconstruction of the mosaics of the 6th century that were previously destroyed during the iconoclastic era by the Byzantines of that time, as represented in the inaugural sermon by the patriarch Photios. However, no record of figural decoration of Hagia Sophia exists before this time.
Emperor Alexander mosaic.
The Emperor Alexander mosaic is not easy to find for the first-time visitor, located in the second floor in a dark corner of the ceiling. It depicts Emperor Alexander in full regalia, holding a scroll in his right hand and a globus cruciger in his left. A drawing by Fossati showed that the mosaic survived until 1849, and that Thomas Whittemore, founder of the Byzantine Institute of America who was granted permission to preserve the mosaics, assumed that it had been destroyed in the earthquake of 1894. Eight years after his death, the mosaic was discovered in 1958 largely through the researches of Robert Van Nice. Unlike most of the other mosaics in Hagia Sophia, which had been covered over by ordinary plaster, the Alexander mosaic was simply painted over and reflected the surrounding mosaic patterns and thus was well hidden. It was duly cleaned by the Byzantine Institute's successor to Whittemore, Paul A. Underwood.
Empress Zoe mosaic.
The Empress Zoe mosaic on the eastern wall of the southern gallery date from the 11th century. Christ Pantocrator, clad in the dark blue robe (as is the custom in Byzantine art), is seated in the middle against a golden background, giving His blessing with the right hand and holding the Bible in His left hand. On either side of His head are the monograms "IC" and "XC", meaning "Iēsous Khristos". He is flanked by Constantine IX Monomachus and Empress Zoe, both in ceremonial costumes. He is offering a purse, as symbol of the donation he made to the church, while she is holding a scroll, symbol of the donations she made. The inscription over the head of the emperor says: "Constantine, pious emperor in Christ the God, king of the Romans, Monomachus". The inscription over the head of the empress reads as follows: "Zoë, the very pious Augusta". The previous heads have been scraped off and replaced by the three present ones. Perhaps the earlier mosaic showed her first husband Romanus III Argyrus or her second husband Michael IV. Another theory is that this mosaic were made for an earlier emperor and empress, with their heads changed into the present ones.
Comnenus mosaic.
The Comnenus mosaic, also located on the eastern wall of the southern gallery, dates from 1122. The Virgin Mary is standing in the middle, depicted, as usual in Byzantine art, in a dark blue gown. She holds the Child Christ on her lap. He gives His blessing with His right hand while holding a scroll in His left hand. On her right side stands emperor John II Comnenus, represented in a garb embellished with precious stones. He holds a purse, symbol of an imperial donation to the church. Empress Irene stands on the left side of the Virgin, wearing ceremonial garments and offering a document. Their eldest son Alexius Comnenus is represented on an adjacent pilaster. He is shown as a beardless youth, probably representing his appearance at his coronation aged seventeen. In this panel one can already see a difference with the Empress Zoe mosaic that is one century older. There is a more realistic expression in the portraits instead of an idealized representation. The empress is shown with plaited blond hair, rosy cheeks and grey eyes, revealing her Hungarian descent. The emperor is depicted in a dignified manner.
Deësis mosaic.
The Deësis mosaic (Δέησις, "Entreaty") probably dates from 1261. It was commissioned to mark the end of 57 years of Roman Catholic use and the return to the Orthodox faith. It is the third panel situated in the imperial enclosure of the upper galleries. It is widely considered the finest in Hagia Sophia, because of the softness of the features, the humane expressions and the tones of the mosaic. The style is close to that of the Italian painters of the late 13th or early 14th century, such as Duccio. In this panel the Virgin Mary and John the Baptist ("Ioannes Prodromos"), both shown in three-quarters profile, are imploring the intercession of Christ Pantocrator for humanity on Judgment Day. The bottom part of this mosaic is badly deteriorated. This mosaic is considered as the beginning of the Renaissance in Byzantine pictorial art.
Northern tympanum mosaics.
The northern tympanum mosaics feature various saints. They have been able to survive due to the very high and unreachable location. They depict Saints John Chrysostom and Ignatius the Younger standing, clothed in white robes with crosses, and holding richly jeweled Holy Bibles. The names of each saint is given around the statues in Greek, in order to enable an identification for the visitor. The other mosaics in the other tympana have not survived probably due to the frequent earthquakes as opposed to any deliberate destruction by the Ottoman conquerors.
20th-century restoration.
A large number of mosaics were uncovered in the 1930s by a team from the Byzantine Institute of America led by Thomas Whittemore. The team chose to let a number of simple cross images remain covered by plaster, but uncovered all major mosaics found.
Because of its long history as both a church and a mosque, a particular challenge arises in the restoration process. Christian iconographic mosaics can be uncovered, but often at the expense of important and historic Islamic art. Restorers have attempted to maintain a balance between both Christian and Islamic cultures. In particular, much controversy rests upon whether the Islamic calligraphy on the dome of the cathedral should be removed, in order to permit the underlying Pantocrator mosaic of Christ as Master of the World, to be exhibited (assuming the mosaic still exists).
See also.
Related buildings:

</doc>
<doc id="42765" url="http://en.wikipedia.org/wiki?curid=42765" title="Chaz Bono">
Chaz Bono

Chaz Salvatore Bono (born Chastity Sun Bono; March 4, 1969) is an American advocate, writer and musician. He is the only child of American entertainers Sonny and Cher. 
Bono is a transgender man. In 1995, several years after being outed as lesbian by the tabloid press, he publicly self-identified as such in a cover story in a leading American gay monthly magazine, "The Advocate", eventually going on to discuss the process of coming out to oneself and to others in two books. "Family Outing: A Guide to the Coming Out Process for Gays, Lesbians, and Their Families" (1998) includes his coming out account. The memoir "The End of Innocence" (2003) discusses his outing, music career, and partner Joan's death from non-Hodgkin's lymphoma.
Between 2008 and 2010, Bono underwent female-to-male gender transition. A two-part "Entertainment Tonight" feature in June 2009 explained that his transition had started a year before. In May 2010, he legally changed his gender and name. A documentary on Bono's experience, "Becoming Chaz", was screened at the 2011 Sundance Film Festival and later made its television debut on OWN: Oprah Winfrey Network.
Early life.
Bono was born in Los Angeles, California, the only child of Cher and Sonny Bono of the pop duo Sonny & Cher, stars of a TV variety show on which the young child often appeared. Bono was named "Chastity Sun Bono" after the film "Chastity", which was produced by Sonny and in which Cher (in her first solo role in a feature film) played a bisexual woman.
Bono came out to both parents as lesbian at age 18. In "Family Outing", Bono wrote that, "as a child, I always felt there was something different about me. I'd look at other girls my age and feel perplexed by their obvious interest in the latest fashion, which boy in class was the cutest, and who looked the most like cover girl Christie Brinkley. When I was 13, I finally found a name for exactly how I was different. I realized I was gay."
Ceremony.
Bono began a short music career with the band Ceremony, which released one album, "Hang Out Your Poetry", in 1993. The band featured Bono on vocals, acoustic guitar, and percussion. Other members were Steve March Tormé (backup vocals), Heidi Shink a.k.a. Chance, Pete McRae, Steve Bauman, Louis Ruiz, and Bryn Mathieu. All but one of the band's songs were written or co-written by Bono, Shink, and Mark Hudson. They used no synthesizers or digital effects on the album; Shink noted, "We turned our back on technology. [ ... ] It's reminiscent of the 60s, but more a tip of the hat than emulating it. We took the music we love and rejuvenated it, made it 90s." Critical reception of the album was lukewarm, with Roch Parisien of Allmusic describing "Hang Out Your Poetry" as a mildly psychedelic take on early 1990s pop, "pleasant, accessible, well-produced ear-candy that's ultimately toothless".
The songs "Could've Been Love" and "Ready for Love" were released as singles from the album.
Tracklist.
Sonny and Cher recorded backing vocals (uncredited) for the last song. As well as the 14 songs listed here, a 15th track, "One World", was released as a B-side to the "Could've Been Love" single.
LGBT activism.
In April 1995, Bono came out as a lesbian in an interview with "The Advocate", a national gay and lesbian magazine. The 1998 book "Family Outing" detailed how Bono's coming out "catapulted me into a political role that has transformed my life, providing me with affirmation as a lesbian, as a woman, and as an individual." In the same book, Bono reported that Cher, who was both a gay icon and an ally of LGBT communities, was quite uncomfortable with the news at first and "went ballistic" before coming to terms with it: "By August 1996, one year after I came out publicly, my mother had progressed so far that she agreed to 'come out' herself on the cover of "The Advocate" as the proud mother of a lesbian daughter." Cher has since become an outspoken LGBT rights activist.
Bono's paternal relationship became strained after Sonny became a Republican Congressman from California. The differences in their political views separated them, and the two had not spoken for more than a year at the time of Sonny's fatal skiing accident in January 1998.
Bono worked as a writer at large for "The Advocate". As a social activist, Bono became a spokesperson for the Human Rights Campaign, promoted National Coming Out Day, campaigned for the reelection of Bill Clinton for US President, campaigned against the Defense of Marriage Act, and served as Entertainment Media Director for the Gay and Lesbian Alliance Against Defamation (GLAAD). Bono was a team captain for "Celebrity Fit Club 3" (2006) and was supported by girlfriend Jennifer Elia, who orchestrated exercise and training sessions.
Transition.
In mid-2008, Bono began undergoing a physical and social transition from female to male. This was confirmed in June 2009 by his publicist, who identified Bono's preferred name as "Chaz Bono" and said, "It is Chaz's hope that his choice to transition will open the hearts and minds of the public regarding this issue, just as his coming out did." GLAAD and the Empowering Spirits Foundation were quick to offer praise and support for the announcement. Bono's legal transition was completed on May 8, 2010, when a California court granted his request for a gender and name change. He chose the name "Chaz Salvatore Bono" in honor of his parents. Bono made "Becoming Chaz", a documentary film about his transition that premiered at the 2011 Sundance Film Festival. OWN: Oprah Winfrey Network acquired the rights to the documentary and debuted it on May 10, 2011.
In September 2011, he became a competitor on the 13th season of the US version of "Dancing with the Stars", paired with professional ballroom dancer Lacey Schwimmer. The duo was eliminated October 25, 2011. This was the first time an openly transgender man starred on a major network television show for something unrelated to being transgender.

</doc>
<doc id="42766" url="http://en.wikipedia.org/wiki?curid=42766" title="Climbing wall">
Climbing wall

A climbing wall is an artificially constructed wall with grips for hands and feet, usually used for indoor climbing, but sometimes located outdoors. Some are brick or wooden constructions, but on most modern walls, the material most often used is a thick multiplex board with holes drilled into it. Recently, manufactured steel and aluminum have also been used. The wall may have places to attach belay ropes, but may also be used to practise lead climbing or bouldering.
Each hole contains a specially formed t-nut to allow modular climbing holds to be screwed onto the wall. With manufactured steel or aluminum walls, an engineered industrial fastener is used to secure climbing holds. The face of the multiplex board climbing surface is covered with textured products including concrete and paint or polyurethane loaded with sand. In addition to the textured surface and hand holds, the wall may contain surface structures such as indentions (incuts) and protrusions (bulges), or take the form of an overhang, underhang or crack.
Some grips are formed to mimic the conditions of outdoor rock, including some that are oversized and can have other grips bolted onto them.
History.
The concept of the artificial climbing wall began in the UK. The first wall was created in 1964 by Don Robinson, a lecturer in Physical Education by inserting pieces of rock into a corridor wall. The first commercial wall was built in Sheffield, traditionally England's centre for climbing due to its proximity to the Peak District. The first indoor climbing gym in the U.S. was established by the Vertical World in Seattle, WA in 1987.
Wall types.
The simplest type of wall is of plywood construction, known colloquially in the climbing community as a 'woody', with a combination of either bolt-on holds or screw on holds. Bolt-on holds are fixed to a wall with iron bolts which are inserted through the hold, which will have specific bolt points, and then fixed into pre-allocated screw-threaded holes in the wall. Screw-on holds are, by contrast, usually much smaller, owing to the nature of their fixing. These holds are connected to the wall by screws which may be fastened anywhere on the wall's surface.
Some other types of walls include slabs of granite, concrete sprayed onto a wire mesh, pre-made fiberglass panels, large trees, manufactured steel and aluminum panels, textured fiberglass walls and inflatables.
Routes and grading.
Holds come in different colours, those of the same colour often being used to denote a route, allowing routes of different difficulty levels to be overlaid on one another. Coloured tape placed under climbing holds is another way that is often used to mark different climbing routes. In attempting a given route, a climber is only allowed to use grips of the designated colour as handholds but is usually allowed to use both handholds and footholds of the designated colour and surface structures and textures of the "rockface" as footholds.
The grade (difficulty) of the route is usually a consensus decision between the setter of the route and the first few people who climb the route.
Many indoor climbing walls have people who are assigned to set these different climbing routes. These people are called route setters or course setters.
As indoor climbing walls are often used to check the development of climber's ability, climbs are color-coded.

</doc>
<doc id="42768" url="http://en.wikipedia.org/wiki?curid=42768" title="Old Church Slavonic">
Old Church Slavonic

Old Church Slavonic (, ), also known as Old Church Slavic (; often abbreviated to OCS; self-name словѣ́ньскъ ѩзꙑ́къ, "slověnĭskŭ językŭ"), was the first Slavic literary language. The 9th-century Byzantine Greek missionaries Saints Cyril and Methodius are credited with standardizing the language and using it in translating the Bible and other Ancient Greek ecclesiastical texts as part of the Christianisation of the Slavic peoples. It is thought to have been based primarily on the dialect of the 9th century Byzantine Slavs living in the Province of Thessalonica (now in Greek Macedonia). It played an important role in the history of the Slavic languages and served as a basis and model for later Church Slavonic traditions, and some Eastern Orthodox and Eastern Catholic churches use this later Church Slavonic as a liturgical language to this day. As the oldest attested Slavic language, OCS provides important evidence for the features of Proto-Slavic, the reconstructed common ancestor of all Slavic languages.
History.
The language was standardized for the mission of the two apostles to Great Moravia in 863 (see Glagolitic alphabet for details). For that purpose, Cyril and his brother Methodius started to translate religious literature to Old Church Slavonic, allegedly based on Slavic dialects spoken in the hinterland of their home-town, Thessaloniki, in the today's region of Macedonia.
As part of the preparation for the mission, in 862/863, the Glagolitic alphabet was created and the most important prayers and liturgical books, including the Aprakos Evangeliar (a Gospel Book lectionary containing only feast-day and Sunday readings), the Psalter, and Acts of the Apostles, were translated. (The Gospels were also translated early, but it is unclear whether Sts. Cyril or Methodius had a hand in this). The language and the alphabet were taught at the Great Moravian Academy (Veľkomoravské učilište) and were used for government and religious documents and books between 863 and 885. The texts written during this phase contain characteristics of the Slavic vernaculars in Great Moravia.
In 885, the use of Old Church Slavonic in Great Moravia was prohibited by the Pope in favour of Latin. Students of the two apostles, who were expelled from Great Moravia in 886, brought the Glagolitic alphabet to Bulgaria. There it was taught at two literary schools: the Preslav Literary School and the Ohrid Literary School. The Glagolitic script was originally used at both schools, though the Cyrillic script was developed early on at the Preslav Literary School where it superseded Glagolitic. The texts written during this era exhibit certain linguistic features of the vernaculars of the First Bulgarian Empire (see "Basis and local influences" below). Old Church Slavonic spread to other South-Eastern and Eastern European Slavic territories, most notably to Bosnia, Croatia, Serbia, Bohemia, Lesser Poland, and principalities of the Kievan Rus' while retaining characteristically South Slavic linguistic features. Later texts written in each of those territories then began to take on characteristics of the local Slavic vernaculars and, by the mid-11th century, Old Church Slavonic had diversified into a number of regional varieties. These local varieties are collectively known as the Church Slavonic language.
Apart from the Slavic countries, Old Church Slavonic has been used as a liturgical language by the Romanian Orthodox Church, as well as a literary and official language of the princedoms of Wallachia and Moldavia (see Old Church Slavonic in Romania), before gradually being replaced by Romanian during the 18th to 19th centuries. Church Slavonic maintained a prestigious status, particularly in Russia, for many centuries – among Slavs in the East it had a status analogous to that of the Latin language in western Europe, but had the advantage of being substantially less divergent from the vernacular tongues of average parishioners. Some Orthodox churches, such as the Bulgarian Orthodox Church, Russian Orthodox Church, Serbian Orthodox Church, Ukrainian Orthodox Church and Macedonian Orthodox Church, as well as several Eastern Catholic churches, still use Church Slavonic in their services and chants today.
Script.
Initially Old Church Slavonic was written with the Glagolitic alphabet, but later Glagolitic was replaced by Cyrillic, which was developed in the First Bulgarian Empire by a decree of Boris I of Bulgaria in the 9th century. In Bosnia was preserved the local Bosnian Cyrillic alphabet, while in Croatia a variant of the Glagolitic alphabet was preserved. See Early Cyrillic alphabet for a detailed description of the script and information about the sounds it originally expressed.
Phonology.
For Old Church Slavonic the following segments are reconstructible. The sounds are given in Slavic transliterated form rather than in IPA, as the exact realisation is uncertain and often differed depending on the area that a text originated from.
Phonotactics.
Several notable constraints on the distribution of the phonemes can be identified, mostly resulting from the tendencies occurring within the Common Slavic period, such as "intrasyllabic synharmony" and the "law of open syllables". For consonant and vowel clusters and for sequences of a consonant and a vowel, the following constraints can be ascertained:
Morphophonemic alternations.
As a result of the first and the second Slavic palatalizations, velars alternate with dentals and palatals. In addition, as a result of a process usually termed "iotation" (or "iodization"), velars and dentals alternate with palatals in various inflected forms and in word formation.
In some forms the alternations of /c/ with /č/ and of /dz/ with /ž/ occur, in which the corresponding velar is missing. The dental alternants of velars occur regularly before /ě/ and /i/ in the declension and in the imperative, and somewhat less regularly in various forms after /i/, /ę/, /ь/ and /rь/. The palatal alternants of velars occur before front vowels in all other environments, where dental alternants do not occur, as well as in various places in inflection and word formation described below.
As a result of earlier alternations between short and long vowels in roots in Proto-Indo-European, Proto-Balto-Slavic and Proto-Slavic times, and of the fronting of vowels after palatalized consonants, the following vowel alternations are attested in OCS: /ь/ : /i/;   /ъ/ : /y/ : /u/;   /e/ : /ě/ : /i/;   /o/ : /a/;   /o/ : /e/;   /ě/ : /a/;   /ъ/ : /ь/;   /y/ : /i/;   /ě/ : /i/;   /y/ : /ę/.
Vowel:∅ alternations sometimes occurred as a result of sporadic loss of weak yer, which later occurred in almost all Slavic dialects. The phonetic value of the corresponding vocalized strong jer is dialect-specific.
Grammar.
As an ancient Indo-European language, OCS has a highly inflective morphology. Inflected forms are divided in two groups - nominals and verbs. Nominals are further divided into nouns, adjectives and pronouns. Numerals inflect either as nouns or pronouns, with one through four also showing gender agreement.
Nominals can be declined in three grammatical genders (masculine, feminine, neuter), three numbers (singular, plural, dual) and seven cases: nominative, vocative, accusative, instrumental, dative, genitive, and locative. There are five basic inflectional classes for nouns: "o/jo"-stems, "a/ja"-stems, "i"-stems, "u"-stems and consonant stems. Forms throughout the inflectional paradigm usually exhibit morphophonemic alternations. Fronting of vowels after palatals and "j" yielded dual inflectional class "o" : "jo" and "a" : "ja", whereas palatalizations affected stem as a synchronic process (N sg. "vlьkъ", V sg. "vlьče"; L sg. "vlьcě"). Productive classes are "o/jo-", "a/ja-" and "i"-stems. Sample paradigms are given in the table below.
Adjectives are inflected as "o/jo"-stems (masculine and neuter) and "a/ja"-stems (feminine), in three genders. They could have short (indefinite) or long (definite) variants, the latter being formed by suffixing to the indefinite form the anaphoric third-person pronoun "jь".
Synthetic verbal conjugation is expressed in present, aorist and imperfect tenses, while perfect, pluperfect, future and conditional tenses/moods are made by combining auxiliary verbs with participles or synthetic tense forms. Sample conjugation for the verb "vesti" "to lead" (underlyingly "ved-ti") is given in the table below.
Basis and local influences.
Written evidence of Old Church Slavonic survives in a relatively small body of manuscripts, most of them written in First Bulgarian Empire during the late 10th and the early 11th centuries. The language has a Southern Slavic basis with an admixture of Western Slavic features inherited during the mission of Saints Cyril and Methodius to Great Moravia (863–885). The only well-preserved manuscript of the Moravian recension, the Kiev Folia, is characterised by the replacement of some Southern Slavic phonetic and lexical features with Western Slavic ones. Manuscripts written in the Second Bulgarian Empire (1185-1396) have, on the other hand, few Western Slavic features.
Old Church Slavonic is valuable to historical linguists since it preserves archaic features believed to have once been common to all Slavic languages. Such features include:
Old Church Slavonic is also likely to have preserved an extremely archaic type of accentuation (probably close to the Chakavian dialect of modern Serbo-Croatian), but unfortunately no accent marks appear in the written manuscripts.
The Southern Slavic nature of the language is evident from the following variations:
Old Church Slavonic has some extra features in common with Bulgarian:
Great Moravia.
The language was standardized for the first time by the mission of the two apostles to Great Moravia from 863. The manuscripts of the Moravian recension are therefore the earliest dated of the OCS recensions. The recension takes its name from the Slavic state of Great Moravia which existed in Central Europe during the 9th century on the territory of today's western Slovakia and Czech Republic.
Moravian recension.
In the "Prague fragments" the only Moravian influence is replacing /ʃt/ with /ts/ and /ʒd/ with /z/. This recension is exemplified by the Kiev Folia. Certain other linguistic characteristics include:
First Bulgarian Empire.
Old Church Slavonic developed in the First Bulgarian Empire and was taught in Preslav (Bulgarian capital between 893 and 972), and in Ohrid (Bulgarian capital between 991/997 and 1015). It didn't represent one regional dialect but a generalized form of early eastern South Slavic, which cannot be localized. The existence of two major literary centres in the Empire led in the period from the 9th to the 11th centuries to the emergence of two recensions (otherwise called "redactions"), termed "Bulgarian" and "Macedonian" respectively. Some researchers do not differentiate between manuscripts of the two recensions, preferring to group them together in a "Macedo-Bulgarian" or simply "Bulgarian" recension. Others, as Horace Lunt, have changed their opinion with time. Initially Lunt (1974:5-6) stated that the differences in the initial OCS were neither great, nor consistent to oppose the Macedonian from the Bulgarian recension. However, a decade later Lunt (1985:202) seems to conceive OCS and its "adjustments" in somewhat different terms, that a Macedonian and a Bulgarian variety of OCS existed, illustrating his point with paleographic, phonological and other differences. The development of Old Church Slavonic literacy had the effect of preventing the assimilation of the South Slavs into neighboring cultures, which promoted the formation of a distinct Bulgarian identity.
Bulgarian recension.
The manuscripts of the Bulgarian recension or "Eastern" variant are among the oldest of the Old Church Slavonic language. This recension was centred around the Preslav Literary School. Since the earliest datable Cyrillic inscriptions were found in the area of Preslav, it is this school which is credited with the development of the Cyrillic alphabet which gradually replaced the Glagolic one. A number of prominent Bulgarian writers and scholars worked at the Preslav Literary School, including Naum of Preslav (until 893), Constantine of Preslav, John Exarch, Chernorizets Hrabar, etc. The main linguistic features of this recension are the following:
Macedonian recension.
The manuscripts of the Macedonian recension or "Western" variant are among the oldest of the Old Church Slavonic language. The recension is sometimes named Macedonian because its literary centre, Ohrid, lies in the historical region of Macedonia. At that period, administratively Ohrid formed part of the province of Kutmichevitsa in the First Bulgarian Empire until the Byzantine conquest. The main literary centre of this dialect was the Ohrid Literary School, whose most prominent member and most likely founder, was Saint Clement of Ohrid who was commissioned by Boris I of Bulgaria to teach and instruct the future clergy of the state in the Slavonic language. The language variety that was used in the area started shaping the modern Macedonian dialects. This recension is represented by the Codex Zographensis and Marianus, among others. The main linguistic features of this recension include:
Later recensions.
Later use of the language in a number of medieval Slavic polities resulted in the adjustment of Old Church Slavonic to the local vernacular, though a number of Southern Slavic, Moravian or Bulgarian features also survived. Significant later recensions of Old Church Slavonic (referred to as Church Slavonic) in the present time include: Slovene, Croatian, Serbian and Russian. In all cases, denasalization of the yuses occurred; so that only Old Church Slavonic and modern Polish retained the old Slavonic nasal vowels.
Serbian recension.
The Serbian recension was written mostly in Cyrillic, but also in the Glagolitic alphabet (depending on region); by the 12th century the Serbs used exclusively the Cyrillic alphabet (and Latin script in coastal areas). The 1186 Miroslav Gospels belong to the Serbian recension. They feature the following linguistic characteristics:
Due to the annexation of Bulgaria in 1396 and to the Ottoman conquest of Serbia in 1459, Serbia saw an influx of educated refugee-scribes who re-introduced a more classical form - as in manuscripts of the Bulgarian recension.
Russian recension.
The Russian recension emerged after the 10th century on the basis of the earlier Bulgarian recension, from which it differed slightly. Its main features are:
Middle Bulgarian.
The line between OCS and post-OCS manuscripts is arbitrary, and terminology varies. The common term "Middle Bulgarian" is usually contrasted to "Old Bulgarian" (an alternative name for Old Church Slavonic), and loosely used for manuscripts whose language demonstrates a broad spectrum of regional and temporal dialect features after the 11th century.
Bosnian recension.
The Bosnian recension used the Bosnian Cyrillic alphabet (better known as "Bosančica") and the Glagolitic alphabet.
Croatian recension.
The Croatian recension of Old Church Slavonic used only the Glagolitic alphabet of angular Croatian type. It shows the development of the following characteristics:
Canon.
The core corpus of Old Church Slavonic manuscripts is usually referred to as "canon". Manuscripts must satisfy certain linguistic, chronological and cultural criteria to be incorporated into the canon, i.e. they must not significantly depart from the language and tradition of Constantine and Methodius, usually known as the "Cyrillo-Methodian tradition".
For example, the Freising Fragments, dating from the 10th century, do show some linguistic and cultural traits of Old Church Slavonic, but are usually not included in the canon as some of the phonological features of the writings appear to belong to certain Pannonian Slavic dialect of the period. Similarly, the Ostromir Gospels exhibits dialectal features that classify it as East Slavic, rather than South Slavic, so it is not included in the canon either. On the other hand, the Kiev Missal is included in the canon, even though it manifests some West Slavic features and contains Western liturgy, due to the Bulgarian linguistic layer and connection to the Moravian mission.
Manuscripts are usually classified in two groups, depending on the alphabet used, Cyrillic or Glagolitic. With the exception of the Kiev Missal and Glagolita Clozianus which exhibit West-Slavic and Croatian features respectively, all Glagolitic texts are assumed to be of the Macedonian recension:
All Cyrillic manuscripts are of the Bulgarian recension and date from the 11th century, except for "Zographos Fragments" which is of the Macedonian recension:
Example text.
The Lord's prayer in Old Church Slavonic:
Authors.
The history of Old Church Slavonic writing includes a northern tradition begun by the mission to Great Moravia, including a short mission in the Balaton principality, and a Bulgarian tradition begun by some of the missionaries who relocated to Bulgaria after the expulsion from Great Moravia.
Old Church Slavonic's first writings, translations of Christian liturgical and Biblical texts, were produced by Byzantine missionaries Saint Cyril and Saint Methodius, mostly during their mission to Great Moravia.
The most important authors in Old Church Slavonic after the death of Methodius and the dissolution of the Great Moravian academy were Clement of Ohrid (active also in Great Moravia), Constantine of Preslav, Chernorizetz Hrabar and John Exarch, all of whom worked in medieval Bulgaria at the end of the 9th and the beginning of the 10th century. The Second Book of Enoch was only preserved in Old Church Slavonic, although the original most certainly had been Greek or even Hebrew or Aramaic.
Nomenclature.
The name of the language in Old Church Slavonic texts was simply "Slavic" (словѣ́ньскъ ѩзꙑ́къ, "slověnĭskŭ językŭ"), derived from the word for "Slavs" (словѣ́нє, "slověne"), the self-designation of the compilers of the texts. This name is preserved in the modern names of the Slovak and Slovene languages. The language is sometimes called "Old Slavic", which may be confused with the distinct Proto-Slavic language. The commonly accepted terms in modern English-language Slavic studies are "Old Church Slavonic" and "Old Church Slavic".
The term "Old Bulgarian" (German: "Altbulgarisch") is the only designation used by Bulgarian-language writers. It was used in numerous 19th-century sources, e.g. by August Schleicher, Martin Hattala, Leopold Geitler and August Leskien, who noted similarities between the first literary Slavic works and the modern Bulgarian language. For similar reasons, Russian linguist Aleksandr Vostokov used the term "Slav-Bulgarian". The term is still used by some writers but nowadays normally avoided in favor of "Old Church Slavonic".
The term "Old Macedonian" is occasionally used by Western scholars in a regional context.
The obsolete term "Old Slovenian" was used by early 19th century scholars who conjectured that the language was based on the dialect of Pannonia.
Modern Slavic nomenclature.
Here are some of the names used by speakers of modern Slavic languages:

</doc>
<doc id="42777" url="http://en.wikipedia.org/wiki?curid=42777" title="299">
299

Year 299 (CCXCIX) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Valerius and Valerius (or, less frequently, year 1052 "Ab urbe condita"). The denomination 299 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="42778" url="http://en.wikipedia.org/wiki?curid=42778" title="298">
298

Year 298 (CCXCVIII) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Faustus and Gallus (or, less frequently, year 1051 "Ab urbe condita"). The denomination 298 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="42779" url="http://en.wikipedia.org/wiki?curid=42779" title="297">
297

Year 297 (CCXCVII) was a common year starting on Friday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Valerius and Valerius (or, less frequently, year 1050 "Ab urbe condita"). The denomination 297 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Persia.
</onlyinclude>

</doc>
<doc id="42780" url="http://en.wikipedia.org/wiki?curid=42780" title="Nordic Council">
Nordic Council

The Nordic Council (Danish: "Nordisk Råd"; Faroese: "Norðurlandaráðið"; Finnish: "Pohjoismaiden Neuvosto"; Icelandic: "Norðurlandaráð"; Norwegian: "Nordisk Råd"; Nynorsk: "Nordisk Råd"; Swedish: "Nordiska Rådet") is a geo-political inter-parliamentary forum for co-operation between the Nordic countries that was established after World War II. Its first concrete result was the introduction in 1952 of a common labour market and free movement across borders without passports for the countries' citizens.
In 1971, the Nordic Council of Ministers, an intergovernmental forum, was established to complement the Council.
History.
During World War II, Denmark and Norway were occupied by Germany; Finland fought a costly war with the Soviet Union; while Sweden, though neutral, still felt the war's effects. Following the war, the Nordic countries pursued the idea of a Scandinavian defence union to ensure their mutual defence. However, Finland, due to its Paasikivi-Kekkonen policy of neutrality and FCMA treaty with the USSR, could not participate. They would unify their foreign policy and defence and remain neutral in the event of a conflict and not ally with NATO, which was being planned at the time. The United States, keen on getting access to bases in Scandinavia and believing the Nordic countries incapable of defending themselves, stated it would not ensure military support for Scandinavia if they did not join NATO. The project collapsed as a result with Denmark, Norway and Iceland joining NATO.
Further Nordic co-operation, such as an economic customs union, also failed. This led then-Danish Prime Minister Hans Hedtoft to propose, in 1951, a consultative inter-parliamentary body. This proposal was agreed by Denmark, Iceland, Norway and Sweden in 1952. The Council's first session was held in the Danish Parliament on 13 February 1953 and it elected Hans Hedtoft as its president. When Finnish-Soviet relations thawed following the death of Joseph Stalin, Finland joined the council in 1955.
On 2 July 1954, the Nordic labour market was created and in 1958, building upon a 1952 passport-free travel area, the Nordic Passport Union was created. These two measures helped ensure Nordic citizens' free movement around Scandinavia. A Nordic Convention on Social Security was implemented in 1955. There were also plans for a single market but they were abandoned in 1959 shortly before Denmark, Norway and Sweden joined the European Free Trade Area (EFTA). Finland became an associated member of EFTA in 1961 and Denmark and Norway applied to join the European Economic Community (EEC).
This move towards the EEC led to desire for a formal Nordic treaty; the Helsinki Treaty outlined the workings of the Council and came into force on 24 March 1962. Further advancements on Nordic cooperation were made in the following years: a Nordic School of Public Health, a Nordic Cultural Fund and Nordic House in Reykjavík. Then-Danish Prime Minister Hilmar Baunsgaard proposed full economic cooperation ("Nordek") in 1968. Nordek was agreed in 1970, but Finland then backtracked, stating that its ties with the Soviet Union meant it could not form close economic ties with potential members of the EEC (Denmark and Norway). Nordek was then abandoned.
As a consequence, Denmark and Norway applied to join the EEC and the Nordic Council of Ministers was set up in 1971 to ensure continued Nordic cooperation. In 1970 representatives of the Faroe Islands and Åland were allowed to take part in the Nordic Council as part of the Danish and Finnish delegations. Norway turned down EEC membership in 1972 while Denmark acted as a bridge builder between the EEC and the Nordics. Also in 1973, although Finland did not opt for full membership of the EEC, Finland negotiated a free trade treaty with the EEC that in practice removed customs duties from 1977 on, although there were transition periods up to 1985 for some products. Sweden did not apply due to its non-alliance policy, which was aimed at preserving neutrality. Greenland subsequently left the EEC and has since sought a more active role in circumpolar affairs.
In the 1970s, the Nordic Council founded the Nordic Industrial Fund, Nordtest and the Nordic Investment Bank. The Council's remit was also expanded to include environmental protection and, in order to clean up the pollution in the Baltic Sea and North Atlantic, a joint energy network was established. The Nordic Science Policy Council was set up in 1983 and, in 1984, representatives from Greenland were allowed to join the Danish delegation.
Following the end of the Soviet Union in 1991, the Nordic Council began to cooperate more with the Baltic States and new Baltic Sea organisations. Sweden and Finland joined the European Union (EU), the EEC's successor, in 1995. Norway had also applied, but once again voted against membership. However, Norway and Iceland did join the European Economic Area (EEA) which integrated them economically with the EU. The Nordic Passport Union was also subsumed into the EU's Schengen Area in 1996.
The Nordic Council became more outward looking, to the Arctic, Baltic, Europe and Canada. The Øresund Bridge linking Sweden and Denmark led to a large amount of cross-border travel leading to further efforts to reduce barriers. However the initially envisioned tasks and functions of the Nordic Council have become partially dormant due to the significant overlap with the EU and EEA. Since 2008 Iceland has also sought EU membership.
Structure.
Council.
The Nordic Council consists of 87 representatives, elected from its members' parliaments and reflecting the relative representation of the political parties in those parliaments. It holds its main session in the autumn, while a so-called "theme session" is arranged in the spring. Each of the national delegations has its own secretariat in the national parliament. The autonomous territories – Greenland, the Faroe Islands and Åland – also have Nordic secretariats.
The Nordic Council uses the three Continental-Scandinavian languages (Danish, Norwegian and Swedish) as its official working languages, but also publishes material in Finnish, Icelandic and English. Since 1987, under the Nordic Language Convention, citizens of the Nordic countries have the opportunity to use their native language when interacting with official bodies in other Nordic countries without being liable to any interpretation or translation costs. The Convention covers visits to hospitals, job centres, the police and social security offices. The languages included are Swedish, Danish, Norwegian, Finnish and Icelandic.
The Council does not have any formal power on its own, but each government has to implement any decisions through its country's legislative assembly (parliament). With Denmark, Norway and Iceland being members of NATO and Finland and Sweden being neutral, the Nordic Council has not been involved in any military cooperation.
Council of Ministers.
The original Nordic Council concentrates on inter-parliamentary cooperation. The "Nordic Council of Ministers", founded in 1971, is responsible for inter-governmental cooperation. Prime Ministers have ultimate responsibility but this is usually delegated to the Minister for Nordic Cooperation and the Nordic Committee for Co-operation, which co-ordinates the day-to-day work. The autonomous territories have the same representation as states.
Location.
The Nordic Council and the Council of Ministers have their headquarters in Copenhagen and various installations in each separate country, as well as many offices in neighbouring countries. The headquarters are located at Ved Stranden No. 18, close to Slotsholmen.
Members.
Members of the Council:
The Sámi political structures (Sami Parliament) have long desired formal representation in the Nordic Council's structures, and are increasingly de facto included in activities touching upon their interests. In addition, the Faroe Islands have expressed their wishes for full membership in the Nordic Council instead of the current associate membership.
General Secretary.
The following have been the General Secretaries presiding over the Council:
Future.
Some desire the Nordic Council's promotion of Nordic cooperation to go much further than at present. If the states of Iceland, Sweden, Norway, Denmark and Finland were to merge in such an integration as some desire, it would command a gross domestic product of US$1.60 trillion, making it the twelfth largest economy in the world, larger than that of Australia, Spain, Mexico or South Korea. Gunnar Wetterberg, a Swedish historian and economist, wrote a book entered into the Nordic Council's year book that proposes the creation of a Nordic Federation from the Council in a few decades.
Expansion.
As far back as 1999 Estonia's Minister of Foreign Affairs Toomas Hendrik Ilves, was already making inroads for Baltic States to join with his famous "Estonia as a Nordic Country" speech in which he outlined Estonia's "special case" for membership.

</doc>
<doc id="42781" url="http://en.wikipedia.org/wiki?curid=42781" title="388">
388

Year 388 (CCCLXXXVIII) was a leap year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Augustus without colleague (or, less frequently, year 1141 "Ab urbe condita"). The denomination 388 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42782" url="http://en.wikipedia.org/wiki?curid=42782" title="387">
387

Year 387 (CCCLXXXVII) was a common year starting on Friday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Augustus and Eutropius (or, less frequently, year 1140 "Ab urbe condita"). The denomination 387 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42783" url="http://en.wikipedia.org/wiki?curid=42783" title="386">
386

Year 386 (CCCLXXXVI) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Honorius and Euodius (or, less frequently, year 1139 "Ab urbe condita"). The denomination 386 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42784" url="http://en.wikipedia.org/wiki?curid=42784" title="384">
384

Year 384 (CCCLXXXIV) was a leap year starting on Monday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Ricomer and Clearchus (or, less frequently, year 1137 "Ab urbe condita"). The denomination 384 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42785" url="http://en.wikipedia.org/wiki?curid=42785" title="383">
383

Year 383 (CCCLXXXIII) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Merobaudes and Saturninus (or, less frequently, year 1136 "Ab urbe condita"). The denomination 383 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42786" url="http://en.wikipedia.org/wiki?curid=42786" title="382">
382

Year 382 (CCCLXXXII) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Antonius and Syagrius (or, less frequently, year 1135 "Ab urbe condita"). The denomination 382 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42787" url="http://en.wikipedia.org/wiki?curid=42787" title="History of Estonia">
History of Estonia

The history of Estonia is a part of the history of Europe. Estonia was settled near the end of the last glacial era, beginning from around 8500 BC. Before the Germans invaded in the 13th century proto-Estonians of the Ancient Estonia worshipped the spirits of nature. Starting with the Northern Crusades Estonia became a battleground for centuries where Denmark, Germany, Russia, Sweden and Poland fought their many wars over controlling the important geographical position of the country as a gateway between East and West.
Being conquered by Danes and Germans in 1227, Estonia was ruled initially by Denmark in the north, by the Livonian Order, an autonomous part of the Monastic state of the Teutonic Knights and Baltic German ecclesiastical states of the Holy Roman Empire. From 1418–1562 the whole of Estonia was part of the Livonian Confederation. After the Livonian War, Estonia became part of Sweden from the 16th century to 1710/1721, when it was ceded to the Russian Empire as the result of the Great Northern War. Throughout this period the Baltic German nobility enjoyed autonomy, where the language of administration and education was German.
The Estophile Enlightenment Period 1750–1840 led to the Estonian national awakening in the middle of the 19th century. In the aftermath of World War I and the Russian revolutions, the Estonian Declaration of Independence was issued in February 1918. The Estonian War of Independence ensued on two fronts between the newly proclaimed state and Bolshevist Russia to the east and the Baltic German forces (the Baltische Landeswehr) to the south, resulting in the Tartu Peace Treaty recognising Estonian independence in perpetuity.
In 1940, Estonia was occupied and (according to e.g. the USA, the EU, and the European Court of Human Rights) illegally annexed by the Soviet Union as a result of the Molotov–Ribbentrop Pact. During the war Estonia was occupied by Nazi Germany in 1941, then reoccupied by the Soviet Union in 1944. Estonia regained independence in 1991 after the collapse of the USSR and joined the European Union in 2004.
Ancient Estonia: pre-history.
The Mesolithic Period.
The region has been populated since the end of the Late Pleistocene Ice Age, about 10,000 BC. The earliest traces of human settlement in Estonia are connected with the Kunda culture. The early Mesolithic Pulli settlement is located by the Pärnu River. It has been dated to the beginning of the 9th millennium BC. The Kunda Culture received its name from the "Lammasmäe" settlement site in northern Estonia, which dates from earlier than 8500 BC. Bone and stone artifacts similar to those found at Kunda have been discovered elsewhere in Estonia, as well as in Latvia, northern Lithuania and southern Finland. Among minerals, flint and quartz were used the most for making cutting tools.
The Neolithic Period.
The beginning of the Neolithic Period is marked by the ceramics of the Narva culture, and appear in Estonia at the beginning of the 5th millennium. The oldest finds date from around 4900 BC. The first pottery was made of thick clay mixed with pebbles, shells or plants. The Narva-type ceramics are found throughout almost the entire Estonian coastal region and on the islands. The stone and bone tools of the era have a notable similarity with the artifacts of the Kunda culture.
Around the beginning of 4th millennium Comb Ceramic culture arrived in Estonia. Until the early 1980s the arrival of Finnic peoples, the ancestors of the Estonians, Finns, and Livonians, on the shores of the Baltic sea was associated with the Comb Ceramic Culture. However, such a linking of archaeologically defined cultural entities with linguistic ones cannot be proven and it has been suggested that the increase of settlement finds in the period is more likely to have been associated with an economic boom related to the warming of climate. Some researchers have even argued that a Uralic form of language may have been spoken in Estonia and Finland since the end of the last glaciation.
The burial customs of the comb pottery people included additions of figures of animals, birds, snakes and men carved from bone and amber. Antiquities from comb pottery culture are found from Northern Finland to Eastern Prussia.
The beginning of the Late Neolithic Period about 2200 BC is characterized by the appearance of the Corded Ware culture, pottery with corded decoration and well-polished stone axes (s.c. boat-shape axes). Evidence of agriculture is provided by charred grains of wheat on the wall of a corded-ware vessel found in Iru settlement. Osteological analysis show an attempt was made to domesticate the wild boar.
Specific burial customs were characterized by the dead being laid on their sides with their knees pressed against their breast, one hand under the head. Objects placed into the graves were made of the bones of domesticated animals.
The Bronze Age.
The beginning of the Bronze Age in Estonia is dated to approximately 1800 BC. The development of the borders between the Finnic peoples and the Balts was under way. The first fortified settlements, Asva and Ridala on the island of Saaremaa and Iru in the Northern Estonia began to be built. The development of shipbuilding facilitated the spread of bronze. Changes took place in burial customs, a new type of burial ground spread from Germanic to Estonian areas, stone cist graves and cremation burials became increasingly common aside a small number of boat-shaped stone graves.
About 7th century BC, a big meteorite hit Saaremaa island and created the Kaali craters.
About 325 BC, the Greek explorer Pytheas possibly visited Estonia. The Thule island he described has been identified as Saaremaa by Lennart Meri, though this identification is not widely considered probable, as Saaremaa lies far south of the Arctic Circle.
The Iron Age.
The Pre-Roman Iron Age began in Estonia about 500 BC and lasted until the middle of the 1st century AD. The oldest iron items were imported, although since the 1st century iron was smelted from local marsh and lake ore. Settlement sites were located mostly in places that offered natural protection. Fortresses were built, although used temporarily. The appearance of square Celtic fields surrounded by enclosures in Estonia date from the Pre-Roman Iron Age. The majority of stones with man-made indents, which presumably were connected with magic designed to increase crop fertility, date from this period. A new type of grave, quadrangular burial mounds began to develop. Burial traditions show the clear beginning of social stratification.
The Roman Iron Age in Estonia is roughly dated to between 50 and 450 AD, the era that was affected by the influence of the Roman Empire. In material culture this is reflected by a few Roman coins, some jewellery and artefacts. The abundance of iron artefacts in Southern Estonia speaks of closer mainland ties with southern areas while the islands of western and northern Estonia communicated with their neighbors mainly by sea. By the end of the period three clearly defined tribal dialectical areas: Northern Estonia, Southern Estonia, and Western Estonia including the islands had emerged, the population of each having formed its own understanding of identity.
Early Middle Ages.
The name of Estonia occurs first in a form of Aestii in the 1st century AD by Tacitus; however, it might have indicated Baltic tribes living in the area. In the Northern Sagas (9th century) the term started to be used to indicate the Estonians.
Ptolemy in his "Geography III" in the middle of the 2nd century CE mentions the Osilians among other dwellers on the Baltic shore.
According to the 5th-century Roman historian Cassiodorus the people known to Tacitius as the Aestii were the Estonians. The extent of their territory in early medieval times is disputed but the nature of their religion is not. They were known to the Scandinavians as experts in wind-magic, as were the Lapps (known at the time as Finns) in the North. Cassiodorus mentions Estonia in his book V. Letters 1–2 dating from the 6th century.
The Chudes, as mentioned by a monk Nestor in the earliest Russian chronicles, were the Ests or Esthonians.
In the 1st centuries AD political and administrative subdivisions began to emerge in Estonia. Two larger subdivisions appeared: the parish (kihelkond) and the county (maakond). The parish consisted of several villages. Nearly all parishes had at least one fortress. The defense of the local area was directed by the highest official, the parish elder. The county was composed of several parishes, also headed by an elder. By the 13th century the following major counties had developed in Estonia: Saaremaa (Osilia), Läänemaa (Rotalia or Maritima), Harjumaa (Harria), Rävala (Revalia), Virumaa (Vironia), Järvamaa (Jervia), Sakala (Saccala), and Ugandi (Ugaunia).
Varbola Stronghold was one of the largest circular rampart fortresses and trading centers built in Estonia, Harju County (Latin: "Harria") at the time.
In the 11th century the Scandinavians are frequently chronicled as combating the Vikings from the eastern shores of the Baltic Sea.
With the rise of Christianity, centralized authority in Scandinavia and Germany eventually led to the Baltic crusades.
The east Baltic world was transformed by military conquest: first the Livs, Letts and Estonians, then the Prussians and the Finns underwent defeat, baptism, military occupation and sometimes extermination by groups of Germans, Danes and Swedes.
Estonian Crusade: The Middle Ages.
Estonia remained one of the last corners of medieval Europe to be Christianized. In 1193 Pope Celestine III called for a crusade against pagans in Northern Europe. The Northern Crusades from Northern Germany established the stronghold of Riga (in modern Latvia). With the help of the newly converted local tribes of Livs and Letts, the crusaders initiated raids into part of what is present-day Estonia in 1208. Estonian tribes fiercely resisted the attacks from Riga and occasionally themselves sacked territories controlled by the crusaders. In 1217 the German crusading order the Sword Brethren and their recently converted allies won a major battle in which the Estonian commander Lembitu was killed. The period of the several Northern Crusade battles in Estonia between 1208 and 1227 is also known as the period of the ancient Estonian fight for independence.
Danish Estonia.
Northern Estonia was conquered by Danish crusaders led by king Waldemar II, who arrived in 1219 on the site of the Estonian town of Lindanisse (now Tallinn) at (Latin) "Revelia" (Estonian) "Revala" or "Rävala", the adjacent ancient Estonian county. The Danish Army defeated the Estonians at Battle of Lyndanisse.
The Estonians of Harria started a rebellion in 1343 (St.George's Night Uprising). The province was occupied by the Livonian Order as a result. In 1346, the Danish dominions in Estonia (Harria and Vironia) were sold for 10 000 marks to the Livonian Order.
Swedish coastal settlements.
The first written mention of the Estonian Swedes comes from 1294, in the laws of the town of Haapsalu. Estonian Swedes are one of the earliest known minorities in Estonia. They have also been called "Coastal Swedes" ("Rannarootslased" in Estonian), or according to their settlement area Ruhnu Swedes, Hiiu Swedes etc. They themselves used the expression "aibofolke" ("island people"), and called their homeland "Aiboland".
The ancient areas of Swedish settlement in Estonia were Ruhnu Island, Hiiumaa Island, the west coast and smaller islands (Vormsi, Noarootsi, Sutlepa, Riguldi, Osmussaar), the north-west coast of the Harju District (Nõva, Vihterpalu, Kurkse, the Pakri Peninsula and the Pakri Islands) and Naissaar Island near Tallinn. The towns with a significant percentage of Swedish population have been Haapsalu and Tallinn.
In earlier times Swedes also lived on the coasts of Saaremaa, the southern part of Läänemaa, the eastern part of Harjumaa and the western part of Virumaa.
Terra Mariana.
In 1227 the Sword Brethren conquered the last indigenous stronghold on the Estonian island of Saaremaa. After the conquest, all the remaining local pagans of Estonia were ostensibly Christianized. An ecclesiastical state Terra Mariana was established. The conquerors exercised control through a network of strategically located castles.
The territory was then divided between the Livonian branch of the Teutonic Order, the Bishopric of Dorpat (in Estonian: "Tartu piiskopkond") and the Bishopric of Ösel-Wiek (in Estonian: "Saare-Lääne piiskopkond"). The Northern part of Estonia – more exactly Harjumaa and Virumaa districts (in German: Harrien und Wierland) – was a nominal possession of Denmark until 1346. Tallinn (Reval) was given the Lübeck Rights in 1248 and joined the Hanseatic League at the end of the 13th century. In 1343 the people of northern Estonia and Saaremaa (Oesel) Island started a rebellion (St. George's Night Uprising) against the rule of their German-speaking landlords. The uprising was put down, and four elected Estonian "kings" were killed in Paide during peace negotiations in 1343 and Vesse, the rebel King of Saaremaa, was hanged in 1344.
Despite local rebellions and Muscovian invasions in 1481 and 1558, the local Low German-speaking upper class continued to rule Estonia. By the end of the Middle Ages, these Baltic Germans had established themselves as the governing elite in Estonia, both as traders and the urban middle-class in the cities, and as landowners in the country-side, through a network of manorial estates.
The Reformation.
The Reformation in Europe that began in 1517 with Martin Luther (1483–1546) spread to Estonia in the 1520s. The Reformation in Estonia was inspired and organized by local and Swedish secular and religious authorities - especially after the end of the Livonian War in 1582. Lutheranism spread literacy among the young, and it had transformed religious art. However the peasants were traditionalists and were more comfortable with Catholic traditions; they delayed the adoption of the new religion. After 1600, Swedish Lutheranism began to dominate the building, furnishing, and (modest) decoration of new churches. Church architecture was now designed to encourage congregational understanding of and involvement in the services. Pews and seats were installed for the common people to make listening to the sermon less of a burden, and altars often featured depictions of the Last Supper, but images and statues of the saints had disappeared. The Baltic German elite promoted Lutheranism and language, education, religion and politics were greatly transformed. Church services were now given in the local vernacular, instead of Latin, and the first book was printed in Estonian.
Division of Estonia in the Livonian War.
Ferdinand I, Holy Roman Emperor once again asked for help of Gustav I of Sweden, and The Kingdom of Poland (1385–1569) also began direct negotiations with Gustavus, but nothing resulted because on September 29, 1560, Gustavus I Vasa died. The chances for success of Magnus von Lyffland and his supporters looked particularly good in 1560 and 1570. In the former case he had been recognised as their sovereign by the Bishopric of Ösel-Wiek and the Bishopric of Courland, and as their prospective ruler by the authorities of the Bishopric of Dorpat; the Bishopric of Reval with the Harrien-Wierland gentry were on his side; Livonian Order conditionally recognised his right of ownership of Estonia (Principality of Estonia). Then along with Archbishop Wilhelm von Brandenburg of The Archbishopric of Riga and his coadjutor Christoph von Mecklenburg, Kettler gave to Magnus the portions of The Kingdom of Livonia, which he had taken possession of, but they refused to give him any more land. Once Eric XIV of Sweden became king he took quick actions to get involved in the war. He negotiated a continued peace with Muscovy and spoke to the burghers of Reval city. He offered them goods to submit to him as well as threatening them. By June 6, 1561 they submitted to him contrary to the persuasions of Kettler to the burghers. The King's brother Johan married the Polish princess Catherine Jagiellon. Wanting to obtain his own land in Livonia, he loaned Poland money and then claimed the castles they had pawned as his own instead of using them to pressure Poland. After Johan returned to Finland, Erik XIV forbade him to deal with any foreign countries without his consent. Shortly after that Erik XIV started acting quickly and lost any allies he was about to obtain, either from Magnus or the Archbishop of Riga. Magnus was upset he had been tricked out of his inheritance of Holstein. After Sweden occupied Reval, Frederick II of Denmark made a treaty with Erik XIV of Sweden in August 1561. The brothers were in great disagreement and Frederick II negotiated a treaty with Ivan IV on August 7, 1562 in order to help his brother obtain more land and stall further Swedish advance. Erik XIV did not like this and the Northern Seven Years' War between the Free City of Lübeck, Denmark, Poland, and Sweden broke out. While only losing land and trade, Frederick II and Magnus were not faring well. But in 1568, Erik XIV became insane and his brother Johan III took his place. Johan III ascended to the throne of Sweden and due to his friendship with Poland he began a policy against Muscovy. He would try to obtain more land in Livonia and exercise strength over Denmark. After all parties had been financially drained, Frederick II let his ally, King Sigismund II Augustus of Polish–Lithuanian Commonwealth, know that he was ready for peace. On December 13, 1570, the Treaty of Stettin was concluded. It is, however, more difficult to estimate the scope and magnitude of the support Magnus received in Livonian cities. Compared to the Harrien-Wierland gentry, the Reval city council, and hence probably the majority of citizens, demonstrated a much more reserved attitude towards Denmark and King Magnus of Livonia. Nevertheless, there is no reason to speak about any strong pro-Swedish sentiments among the residents of Reval. The citizens who had fled to The Bishopric of Dorpat or had been deported to Muscovy hailed Magnus as their saviour until 1571. The analysis indicates that during the Livonian War a pro-independence wing emerged among the Livonian gentry and townspeople, forming the so-called "Peace Party". Dismissing hostilities, these forces perceived an agreement with Muscovy as a chance to escape the atrocities of war and avoid the division of Livonia. That is why Magnus, who represented Denmark and later struck a deal with Ivan the Terrible, proved a suitable figurehead for this faction.
The Peace Party, however, had its own armed forces – scattered bands of household troops ("Hofleute") under diverse command, which only united in action in 1565 (Battle of Pärnu, 1565 and Siege of Reval, 1565), in 1570–1571 (Siege of Reval, 1570–1571; 30 weeks), and in 1574–1576 (first on Sweden’s side, then came the sale of Wiek to the Danish Crown, and the loss of the territory to Muscovites). In 1575 after Muscovy attacked Danish claims in Livonia, Frederick II dropped out of the competition as well as the Holy Roman Emperor. After this Johan III held off on his pursuit for more land due to Muscovy obtaining lands that Sweden controlled. He used the next two years of truce to get in a better position. In 1578, he resumed the fight for not only Livonia, but also everywhere due to an understanding he made with Rzeczpospolita. In 1578 Magnus retired to Rzeczpospolita and his brother all but gave up the land in Livonia.
Having rejected peace proposals from its enemies, Ivan the Terrible found himself in a difficult position by 1579, when Crimean Khanate devastated Muscovian territories and burnt down Moscow (see Russo-Crimean Wars), the drought and epidemics have fatally affected the economy, Oprichnina had thoroughly disrupted the government, while The Grand Principality of Lithuania had united with The Kingdom of Poland (1385–1569) and acquired an energetic leader, Stefan Batory, supported by Ottoman Empire (1576). Stefan Batory replied with a series of three offensives against Muscovy, trying to cut The Kingdom of Livonia from Muscovian territories. During his first offensive in 1579 with 22,000 men he retook Polotsk, during the second, in 1580, with 29,000-strong army he took Velikie Luki, and in 1581 with a 100,000-strong army he started the Siege of Pskov. Frederick II had trouble continuing the fight against Muscovy unlike Sweden and Poland. He came to an agreement with John III in 1580 giving him the titles in Livonia. That war would last from 1577 to 1582. Muscovy recognized Polish–Lithuanian control of Ducatus Ultradunensis only in 1582. After Magnus von Lyffland died in 1583, Poland invaded his territories in The Duchy of Courland and Frederick II decided to sell his rights of inheritance. Except for the island of Œsel, Denmark was out of the Baltic by 1585. In 1598 Polish Livonia was divided into:
During the Livonian War in 1561, northern Estonia submitted to Swedish control, while southern Estonia briefly came under the control of the Polish–Lithuanian Commonwealth in the 1580s. In 1625, mainland Estonia came entirely under Swedish rule. Estonia was administratively divided between the provinces of Estonia in the north and Livonia in southern Estonia and northern Latvia, a division which persisted until the early 20th century.
Polish–Lithuanian Commonwealth.
During 1582–83 southern Estonia (Livonia) became part of the Polish–Lithuanian Commonwealth.
Estonia in the Swedish Empire.
Estonia placed itself under Swedish rule in 1561 to receive protection against Russia and Poland as the Livonian Order lost their foothold in the Baltic provinces. Territorially it represented the northern part of present-day Estonia.
Livonia was conquered from the Polish–Lithuanian Commonwealth by 1629 in the Polish–Swedish War. By the Treaty of Oliva between the Commonwealth and Sweden in 1660 following the Northern Wars the Polish–Lithuanian king renounced all claims to the Swedish throne and Livonia was formally ceded to Sweden. Swedish Livonia represents the southern part of present-day Estonia and the northern part of present-day Latvia (Vidzeme region)
In 1631, Gustavus II Adolphus of Sweden forced the nobility to grant the peasantry greater autonomy, and in 1632 established a printing press and University in the city of Tartu.
Estonia in the Russian Empire.
Sweden's defeat by Russia in the Great Northern War resulted in the Capitulation of Estonia and Livonia in 1710, confirmed by the Treaty of Nystad in 1721, and Russian rule was then imposed on what later became modern Estonia. Nonetheless, the legal system, Lutheran church, local and town governments, and education remained mostly German until the late 19th century and partially until 1918.
The Russian era from the 1720s to the First World War was the Golden age of the German elites. They own most of the land and businesses, controlled the serfs, dominated all the cities, and got along quite well with the Russian Imperial authorities. Unrest and rebellion was uncommon. The Germans were Lutherans, and so were the vast majority of the Estonian population, but the Germans had full control of the Lutheran churches. Moravian Protestant missionaries made an impact in the eighteenth century, and translated the complete Bible into Estonian. The Germans complained so the imperial government banned the Moravians from 1743 to 1764. A theological faculty opened at the University of Dorpat (Tartu), with German professors. The local German gentry controlled the local churches, and rarely hired Estonian graduates, but they made their mark as intellectuals and Estonian nationalists. In the 1840s there was a movement of Lutheran peasants into the Russian Orthodox Church. The czar discouraged them when he realized they were challenging the local authorities. The German character of the Lutheran churches alienated many nationalists, who emphasized the secular in their subcultures. For example, choral societies offered a secular alternative to church music.
By 1819, the Baltic provinces were the first in the Russian empire in which serfdom was abolished, the largely autonomous nobility allowing the peasants to own their own land or move to the cities. These moves created the economic foundation for the coming to life of the local national identity and culture as Estonia was caught in a current of national awakening that began sweeping through Europe in the mid-19th century.
Tartu was a multicultural crossroads with strong representation of Russians Germans and Estonians; Orthodox, Lutherans and Jews, scientists and humanists, all were quite active at the city's university. The students seemed uninterested in the Russification programs introduced in the 1890s.
The Estophile enlightenment period (1750–1840).
Educated German immigrants and local Baltic Germans in Estonia, educated at German universities, introduced Enlightenment ideas of rational thinking, ideas that propagated freedom of thinking and brotherhood and equality. The French Revolution provided a powerful motive for the enlightened local upper class to create literature for the peasantry. The freeing of the peasantry from serfdom on the nobles' estates in 1816 in Southern Estonia: Governorate of Livonia (Russian: Лифляндская губерния) and 1819 in Northern Estonia: Governorate of Estonia (Russian: Эстляндская губерния) by Alexander I of Russia gave rise to a debate as to the future fate of the former enslaved peoples. Although Baltic Germans by and large regarded the future of the Estonians as being a fusion with the Baltic Germans, the Estophile educated class admired the ancient culture of the Estonians and their era of freedom before the conquests by Danes and Germans in the 13th century. The Estophile Enlightenment Period formed the transition from religious Estonian literature to newspapers written in Estonian for the mass public.
National awakening.
A cultural movement sprang forth to adopt the use of Estonian as the language of instruction in schools, all-Estonian song festivals were held regularly after 1869, and a national literature in Estonian developed. "Kalevipoeg", Estonia's national epic, was published in 1861 in both Estonian and German.
1889 marked the beginning of the central government-sponsored policy of Russification. The impact of this was that many of the Baltic German legal institutions were either abolished or had to do their work in Russian – a good example of this is the University of Tartu.
As the Russian Revolution of 1905 swept through Estonia, the Estonians called for freedom of the press and assembly, for universal franchise, and for national autonomy. Estonian gains were minimal, but the tense stability that prevailed between 1905 and 1917 allowed Estonians to advance the aspiration of national statehood.
Road to the republic.
Estonia as a unified political entity first emerged after the Russian February Revolution of 1917. With the collapse of the Russian Empire in World War I, Russia's Provisional Government granted national autonomy to a unified Estonia in April. The Governorate of Estonia in the north (corresponding to the historic Danish Estonia) was united with the northern part of the Governorate of Livonia. Elections for a provisional parliament, "Maapäev" was organized, with the Menshevik and Bolshevik factions of the Russian Social Democratic Labour Party obtaining a part of the vote. On November 5, 1917, two days before the October Revolution in Saint Petersburg, Estonian Bolshevik leader Jaan Anvelt violently usurped power from the legally constituted Maapäev in a coup d'état, forcing the Maapäev underground.
In February, after the collapse of the peace talks between Soviet Russia and the German Empire, mainland Estonia was occupied by the Germans. Bolshevik forces retreated to Russia. Between the Russian Red Army's retreat and the arrival of advancing German troops, the Salvation Committee of the Estonian National Council Maapäev issued the Estonian Declaration of Independence in Pärnu on February 23, 1918.
War of Independence.
After the collapse of the short-lived puppet government of the United Baltic Duchy and the withdrawal of German troops in November 1918, an Estonian Provisional Government retook office. A military invasion by the Red Army followed a few days later, however, marking the beginning of the Estonian War of Independence (1918–1920). The Estonian army cleared the entire territory of Estonia of the Red Army by February 1919. On 5–7 April 1919 The Estonian Constituent Assembly was elected. 
Victory.
On February 2, 1920, the Treaty of Tartu was signed by the Republic of Estonia and the Russian SFSR. The terms of the treaty stated that Russia renounced in perpetuity all rights to the territory of Estonia. The first Constitution of Estonia was adopted on June 15, 1920. The Republic of Estonia obtained international recognition and became a member of the League of Nations in 1921.
In nearby Finland similar circumstances resulted in a bloody civil war. Despite repeated threats from fascist movements, Finland became and remained a free democracy under the rule of law. By contrast Estonia, without a civil war, started as a democracy and was turned into a dictatorship in 1934.
Interwar period.
The first period of independence lasted 22 years, beginning in 1918. Estonia underwent a number of economic, social, and political reforms necessary to come to terms with its new status as a sovereign state. Economically and socially, land reform in 1919 was the most important step. Large estate holdings belonging to the Baltic nobility were redistributed among the peasants and especially among volunteers in the Estonian War of Independence. Estonia's principal markets became Scandinavia, the United Kingdom, and western Europe, with some exports to the United States and to the Soviet Union.
The first constitution of the Republic of Estonia, adopted in 1920, established a parliamentary form of government. The parliament ("Riigikogu") consisted of 100 members elected for 3-year terms. Between 1920 and 1934, Estonia had 21 governments.
A mass anticommunist and antiparliamentary Vaps Movement emerged in the 1930s.
In October 1933 a referendum on constitutional reform initiated by the Vaps Movement was approved by 72.7 percent. The league spearheaded replacement of the parliamentary system with a presidential form of government and laid the groundwork for an April 1934 presidential election, which it expected to win. However, the Vaps Movement was thwarted by a pre-emptive coup d'état on March 12, 1934, by Head of State Konstantin Päts, who then established his own authoritarian rule until a new constitution came to force. During the Era of Silence, political parties were banned and the parliament was not in session between 1934 and 1938 as the country was ruled by decree by Konstantin Päts. The Vaps Movement was officially banned and finally disbanded in December 1935. On May 6, 1936, 150 members of the league went on trial and 143 of them were convicted to long-term prison sentences. They were granted an amnesty and freed in 1938, by which time the league had lost most of its popular support.
The interwar period was one of great cultural advancement. Estonian language schools were established, and artistic life of all kinds flourished. One of the more notable cultural acts of the independence period, unique in western Europe at the time of its passage in 1925, was a guarantee of cultural autonomy to minority groups comprising at least 3,000 persons, including Jews (see history of the Jews in Estonia). Historians see the lack of any bloodshed after a nearly "700-year German rule" as indication that it must have been mild by comparison.
Estonia had pursued a policy of neutrality, but it was of no consequence after the Soviet Union and Nazi Germany signed the Molotov–Ribbentrop Pact on August 23, 1939. In the agreement, the two great powers agreed to divide up the countries situated between them (Poland, Lithuania, Latvia, Estonia, and Finland) with Estonia falling in the Soviet "sphere of influence". After the invasion of Poland, the Orzeł incident took place when Polish submarine ORP "Orzeł" looked for shelter in Tallinn but escaped after the Soviet Union attacked Poland on September 17. Estonian's lack of will and/or inability to disarm and intern the crew caused the Soviet Union to accuse Estonia of "helping them escape" and claim that Estonia was not neutral. On September 24, 1939, the Soviet Union threatened Estonia with war unless provided with military bases in the country –- an ultimatum with which the Estonian government complied.
World War II.
Following the conclusion of the Molotov-Ribbentrop Pact and the Soviet invasion of Poland, warships of the Red Navy appeared off Estonian ports on September 24, 1939, and Soviet bombers began a threatening patrol over Tallinn and the nearby countryside. Moscow demanded Estonia assent to an agreement which allowed the USSR to establish military bases and station 25,000 troops on Estonian soil for the duration of the European war. The government of Estonia accepted the ultimatum, signing the corresponding agreement on September 28, 1939.
Soviet occupation (1940).
The Republic of Estonia was occupied by the Soviet Union in June 1940.
On June 12, 1940, the order for a total military blockade of Estonia by the Soviet Baltic Fleet was given.
On June 14, 1940, while the world's attention was focused on the fall of Paris to Nazi Germany a day earlier, the Soviet military blockade of Estonia went into effect, and two Soviet bombers downed Finnish passenger airplane "Kaleva" flying from Tallinn to Helsinki carrying three diplomatic pouches from the U.S. legations in Tallinn, Riga and Helsinki. US Foreign Service employee Henry W. Antheil, Jr. was killed in the crash.
On June 16, 1940, the Soviet Union invaded Estonia. Molotov accused the Baltic states of conspiracy against the Soviet Union and delivered an ultimatum to Estonia for the establishment of a government approved of by the Soviets.
The Estonian government decided, given the overwhelming Soviet force both on the borders and inside the country, not to resist, to avoid bloodshed and open war. Estonia accepted the ultimatum and the statehood of Estonia de facto ceased to exist as the Red Army exited from their military bases in Estonia on June 17. The following day, some 90,000 additional troops entered the country. The military occupation of the Republic of Estonia was rendered "official" by a communist coup d'état supported by the Soviet troops, followed by "parliamentary elections" where all but pro-Communist candidates were outlawed. The "parliament" so elected proclaimed Estonia a Socialist Republic on July 21, 1940 and unanimously requested Estonia to be "accepted" into the Soviet Union. Those who had fallen short of the "political duty" of voting Estonia into the USSR, who had failed to have their passports stamped for so voting, were allowed to be shot in the back of the head by Soviet tribunals. Estonia was formally annexed into the Soviet Union on August 6 and renamed the Estonian Soviet Socialist Republic. Notice, for instance, the position expressed by the European Parliament, which condemned "the fact that the occupation of these formerly independent and neutral States by the Soviet Union occurred in 1940 following the Molotov/Ribbentrop pact, and continues."
The Soviet authorities, having gained control over Estonia, immediately imposed a regime of terror. During the first year of Soviet occupation (1940–1941) over 8,000 people, including most of the country's leading politicians and military officers, were arrested. About 2,200 of the arrested were executed in Estonia, while most of the others were moved to Gulag prison camps in Russia, from where very few were later able to return alive. On June 14, 1941, when mass deportations took place simultaneously in all three Baltic countries, about 10,000 Estonian civilians were deported to Siberia and other remote areas of the Soviet Union, where nearly half of them later perished. Of the 32,100 Estonian men who were forcibly relocated to Russia under the pretext of mobilisation into the Soviet army after the German invasion of the Soviet Union in 1941, nearly 40 percent died within the next year in the so-called "labour battalions" of hunger, cold and overworking. During the first Soviet occupation of 1940–41 about 500 Jews were deported to Siberia.
Estonian graveyards and monuments were destroyed. Among others, the Tallinn Military Cemetery had the majority of gravestones from 1918–1944 destroyed by the Soviet authorities, and this graveyard became reused by the Red Army.
Other cemeteries destroyed by the authorities during the Soviet era in Estonia include Baltic German cemeteries established in 1774 (Kopli cemetery, Mõigu cemetery) and the oldest cemetery in Tallinn, from the 16th century, Kalamaja cemetery.
Many countries including the United States did not recognize the seizure of Estonia by the USSR. Such countries recognized Estonian diplomats and consuls who still functioned in many countries in the name of their former governments. These aging diplomats persisted in this anomalous situation until the ultimate restoration of Baltic independence.
Ernst Jaakson, the longest-serving foreign diplomatic representative to the United States, served as vice-consul from 1934, and as consul general in charge of the Estonian legation in the United States from 1965 until reestablishment of Estonia's independence. On November 25, 1991 he presented credentials as Estonian ambassador to the United States.
German occupation (1941–1944).
After Nazi Germany invaded the Soviet Union on June 22, 1941, and the Wehrmacht reached Estonia in July 1941, most Estonians greeted the Germans with relatively open arms and hoped to restore independence. But it soon became clear that sovereignty was out of the question. Estonia became a part of the German-occupied "Ostland". A Sicherheitspolizei was established for internal security under the leadership of Ain-Ervin Mere. The initial enthusiasm that accompanied the liberation from Soviet occupation quickly waned as a result and the Germans had limited success in recruiting volunteers. The draft was introduced in 1942, resulting in some 3,400 men fleeing to Finland to fight in the Finnish Army rather than join the Germans. Finnish Infantry Regiment 200 AKA (Estonian: "soomepoisid") was formed out of Estonian volunteers in Finland. With the Allied victory over Germany becoming certain in 1944, the only option to save Estonia's independence was to stave off a new Soviet invasion of Estonia until Germany's capitulation.
By January 1944, the front was pushed back by the Soviet Army almost all the way to the former Estonian border. Narva was evacuated. Jüri Uluots, the last legitimate prime minister of the Republic of Estonia (according to the Constitution of the Republic of Estonia) prior to its fall to the Soviet Union in 1940, delivered a radio address that implored all able-bodied men born from 1904 through 1923 to report for military service (before this, Uluots had opposed Estonian mobilization.) The call drew support from all across the country: 38,000 volunteers jammed registration centers. Several thousand Estonians who had joined the Finnish army came back across the Gulf of Finland to join the newly formed Territorial Defense Force, assigned to defend Estonia against the Soviet advance. It was hoped that by engaging in such a war Estonia would be able to attract Western support for the cause of Estonia's independence from the USSR and thus ultimately succeed in achieving independence.
The initial formation of the volunteer SS Estonian legion created in 1942 was eventually expanded to become a full-sized conscript division of the Waffen-SS in 1944, the 20th Waffen Grenadier Division of the SS (1st Estonian). The Estonian units saw action defending the Narva line throughout 1944.
As the Germans started to retreat on 18 September 1944, Jüri Uluots, the last Prime Minister of the Estonian Republic prior to Soviet occupation, assumed the responsibilities of president (as dictated in the Constitution) and appointed a new government while seeking recognition from the Allies. On 22 September 1944, as the last German units pulled out of Tallinn, the city was re-occupied by the Soviet Red Army. The new Estonian government fled to Stockholm, Sweden and operated in exile until 1992, when Heinrich Mark, the prime minister of the Estonian government in exile acting as president, presented his credentials to incoming president Lennart Meri.
The Holocaust in Estonia.
The process of Jewish settlement in Estonia began in the 19th century, when in 1865 Russian Tsar Alexander II granted them the right to enter the region. The creation of the Republic of Estonia in 1918 marked the beginning of a new era for the Jews. Approximately 200 Jews fought in combat for the creation of the Republic of Estonia and 70 of these men were volunteers. From the very first days of her existence as a state, Estonia showed her tolerance towards all the peoples inhabiting her territories. On 12 February 1925, the Estonian government passed a law pertaining to the cultural autonomy of minority peoples. The Jewish community quickly prepared its application for cultural autonomy. Statistics on Jewish citizens were compiled. They totaled 3,045, fulfilling the minimum requirement of 3,000. In June 1926 the Jewish Cultural Council was elected and Jewish cultural autonomy was declared. Jewish cultural autonomy was of great interest to the global Jewish community. The Jewish National Endowment presented the Government of the Republic of Estonia with a certificate of gratitude for this achievement.
There were, at the time of Soviet occupation in 1940, approximately 2000 Estonian Jews. Many Jewish people were deported to Siberia along with other Estonians by the Soviets. It is estimated that 500 Jews suffered this fate. With the invasion of the Baltics, it was the intention of the Nazi government to use the Baltic countries as their main area of mass genocide. Consequently, Jews from countries outside the Baltics were shipped there to be exterminated. Out of the approximately 4,300 Jews in Estonia prior to the war, between 1,500 and 2,000 were entrapped by the Nazis,
and an estimated 10,000 Jews were killed in Estonia after having been deported to camps there from elsewhere in Eastern Europe.
It is known that there have been 7 ethnic Estonians – Ralf Gerrets, Ain-Ervin Mere, Jaan Viik, Juhan Jüriste, Karl Linnas, Aleksander Laak and Ervin Viks – that have faced trials for crimes against humanity since the reestablishment of Estonian independence and the Estonian International Commission for Investigation of Crimes Against Humanity has been established. Markers were put in place for the 60th anniversary of the mass executions that were carried out at the Lagedi, Vaivara and Klooga (Kalevi-Liiva) camps in September 1944.
Fate of other minorities during and after World War II.
The Baltic Germans had voluntarily evacuated to Germany (in accordance with Hitler's order) following the Molotov–Ribbentrop Pact of August 1939.
Almost all the remaining Estonian Swedes fled Aiboland in August 1944, often in their small boats to the Swedish island of Gotland.
The Russian minority grew significantly in numbers during the postwar era.
Soviet reoccupation (1944-1991).
Stalinism.
In World War II Estonia had suffered huge losses. Ports had been destroyed, and 45% of industry and 40% of the railways had become damaged. Estonia's population had decreased by one-fifth, about 200,000 people. Some 10% of the population (over 80,000 people) had fled to the West between 1940 and 1944. More than 30,000 soldiers had been killed in action. In 1944 Russian air raids had destroyed Narva and one-third of the residential area in Tallinn. By the late autumn of 1944, Soviet forces had ushered in a second phase of Soviet rule on the heels of the German troops withdrawing from Estonia, and followed it up by a new wave of arrests and executions of people considered disloyal to the Soviets.
An anti-Soviet guerrilla movement known as the "Metsavennad" ("Forest Brothers") developed in the countryside, reaching its zenith in 1946–48. It is hard to tell how many people were in the ranks of the "Metsavennad"; however, it is estimated that at different times there could have been about 30,000–35,000 people. Probably the last Forest Brother was caught in September 1978, and killed himself during his apprehension.
In March 1949, 20,722 people (2.5% of the population) were deported to Siberia. By the beginning of the 1950s, the occupying regime had suppressed the resistance movement.
After the war the Communist Party of the Estonian Soviet Socialist Republic (ECP) became the pre-eminent organization in the republic. The ethnic Estonian share in the total ECP membership decreased from 90% in 1941 to 48% in 1952.
Khrushchev era.
After Stalin's death, Party membership vastly expanded its social base to include more ethnic Estonians. By the mid-1960s, the percentage of ethnic Estonian membership stabilized near 50%. On the eve of perestroika the ECP claimed about 100,000 members; less than half were ethnic Estonians and they totalled less than 7% of the country's population.
One positive aspect of the post-Stalin era in Estonia was the regranting of permission in the late 1950s for citizens to make contact with foreign countries. Ties were reactivated with Finland, and in the 1960s, a ferry connection was opened from Tallinn to Helsinki and Estonians began watching Finnish television. This electronic "window on the West" afforded Estonians more information on current affairs and more access to Western culture and thought than any other group in the Soviet Union. This heightened media environment was important in preparing Estonians for their vanguard role in extending perestroika during the Gorbachev era.
Capital investments.
In 1955 the TV Centre was built in Tallinn, that began TV broadcasts on June 29 of that year. The Tallinn Song Festival Grounds, the venue for the song festivals, were built in 1960
Health care.
Only after the Khrushchev Thaw period of 1956 did healthcare networks start to stabilise. Due to natural development, science and technology advanced and popular welfare increased. All demographic indicators improved; birth rates increased, mortality decreased. Healthcare became freely available to everybody during the Soviet era. If one could pay bribes to the relevant officials.
Brezhnev era.
In the late 1970s, Estonian society grew increasingly concerned about the threat of cultural Russification to the Estonian language and national identity. By 1981, Russian was taught in the first grade of Estonian-language schools and was also introduced into Estonian pre-school teaching.
Moscow Olympic Games of 1980.
Tallinn was selected as the host of the sailing events which led to controversy since many governments had not "de jure" recognized ESSR as part of the USSR. During the preparations to the Olympics, sports buildings were built in Tallinn, along with other general infrastructure and broadcasting facilities. This wave of investment included Tallinn Airport, Hotell Olümpia, Tallinn TV Tower, Pirita Yachting Centre and Linnahall.
Andropov and Chernenko era.
On November 10, 1982 Leonid Brezhnev died and was succeeded by Yuri Andropov, the former head of the KGB. Andropov introduced limited economic reforms and established an anti-corruption program. On February 9, 1984 Andropov died and was succeeded by Konstantin Chernenko who in turn died on March 10, 1985.
Alcoholism.
Alcoholism became a growing health issue. Up until 1985 and the beginning of glasnost, it was illegal to publish statistical data on alcohol sales. It is estimated that alcoholism peaked in 1982–1984, when consumption reached 11.2 litres of absolute alcohol per person per annum. (In comparison, in Finland during the same period consumption only 6–7 litres per person per annum).
Gorbachev era.
By the beginning of the Gorbachev era, concern over the cultural survival of the Estonian people had reached a critical point. The ECP remained stable in the early perestroika years but waned in the late 1980s. Other political movements, groupings and parties moved to fill the power vacuum. The first and most important was the Estonian Popular Front, established in April 1988 with its own platform, leadership and broad constituency. The Greens and the dissident-led Estonian National Independence Party soon followed.
Restoration of "de facto" independence.
The Estonian Sovereignty Declaration was issued on November 16, 1988. By 1989 the political spectrum had widened, and new parties were formed and re-formed almost daily. The republic's Supreme Soviet transformed into an authentic regional lawmaking body. This relatively conservative legislature passed an early declaration of sovereignty (November 16, 1988); a law on economic independence (May 1989) confirmed by the Supreme Soviet of the Soviet Union that November; a language law making Estonian the official language (January 1989); and local and republic election laws stipulating residency requirements for voting and candidacy (August, November 1989).
Despite the emergence of the Popular Front and the Supreme Soviet as a new lawmaking body, since 1989 the different segments of the indigenous Estonian population had been politically mobilized by different and competing actors. The Popular Front's proposal, to declare the independence of Estonia as a new, so-called "third republic" whose citizens would be all those living there at the moment found less and less support over time.
A grassroots Estonian Citizens' Committees Movement launched in 1989 with the objective of registering all pre-war citizens of the Republic of Estonia and their descendants in order to convene a Congress of Estonia. Their emphasis was on the illegal nature of the Soviet system and that hundreds of thousands of inhabitants of Estonia had not ceased to be citizens of the Estonian Republic which still existed "de jure", recognized by the majority of Western nations. Despite the hostility of the mainstream official press and intimidation by Soviet Estonian authorities, dozens of local citizens' committees were elected by popular initiative all over the country. These quickly organized into a nation-wide structure and by the beginning of 1990, over 900,000 people had registered themselves as citizens of the Republic of Estonia.
The spring of 1990 saw two free elections and two alternative legislatures developed in Estonia. On 24 February 1990, the 464-member Congress of Estonia (including 35 delegates of refugee communities abroad) was elected by the registered citizens of the republic. The Congress of Estonia convened for the first time in Tallinn March 11–12, 1990, passing 14 declarations and resolutions. A 70-member standing committee (Eesti Komitee) was elected with Tunne Kelam as its chairman.
In March 1991 a referendum was held on the issue of independence. This was somewhat controversial, as holding a referendum could be taken as signalling that Estonian independence would be established rather than "re"-established. There was some discussion about whether it was appropriate to allow the Russian immigrant minority to vote, or if this decision should be reserved exclusively for citizens of Estonia. In the end all major political parties backed the referendum, considering it most important to send a strong signal to the world. To further legitimise the vote, all residents of Estonia were allowed to participate. The result vindicated these decisions, as the referendum produced a strong endorsement for independence. Turnout was 82%, and 64% of all possible voters in the country backed independence, with only 17% against.
Although the majority of Estonia's large Russian-speaking diaspora of Soviet-era immigrants did not support full independence, they were divided in their goals for the republic. In March 1990 some 18% of Russian speakers supported the idea of a fully independent Estonia, up from 7% the previous autumn, and by early 1990 only a small minority of ethnic Estonians were opposed to full independence.
In the March 18, 1990 elections for the 105-member Supreme Soviet all residents of Estonia were eligible to participate, including all Soviet-era immigrants from the U.S.S.R. and approximately 50,000 Soviet troops stationed there. The Popular Front coalition, composed of left and centrist parties and led by former Central Planning Committee official Edgar Savisaar, gained a parliamentary majority.
On May 8, 1990, the Supreme Council of the Republic of Estonia (renamed the previous day) changed the name to the Republic of Estonia. Through a strict, non-confrontational policy in pursuing independence, Estonia managed to avoid the violence which Latvia and Lithuania incurred in the bloody January 1991 crackdowns and in the border customs-post guard murders that summer. During the attempted August coup in the U.S.S.R., Estonia was able to maintain constant operation and control of its telecommunications facilities, thereby offering the West a clear view into the latest developments and serving as a conduit for swift Western support and recognition of Estonia's "confirmation" of independence on August 20, 1991. August 20 remains a national holiday in Estonia because of this. Following Europe's lead, the United States formally reestablished diplomatic relations with Estonia on September 2, and the Supreme Soviet of Russia offered recognition on September 6.
Since the debates about whether the future independent Estonia would be established as a new republic or a continuation of the first republic were not yet complete by the time of the August coup, while the members of the Supreme Soviet generally agreed that independence should be declared rapidly, a compromise was hatched between the two main sides: instead of "declaring" independence, which would imply a new start, or explicitly asserting continuity, the declaration would "confirm" Estonia as a state independent of the Soviet Union, and willing to reestablish diplomatic relations of its own accord. The text of the statement was in Estonian and only a few paragraphs in length.
After more than 3 years of negotiations, on August 31, 1994, the armed forces of Russia withdrew from Estonia. Since fully regaining independence Estonia has had 12 governments with 8 prime ministers: Mart Laar, Andres Tarand, Tiit Vähi, Mart Siimann, Siim Kallas, Juhan Parts, and Andrus Ansip. The PMs of the interim government (1990–1992) were Edgar Savisaar and Tiit Vähi.
Since the last Russian troops left in 1994, Estonia has been free to promote economic and political ties with Western Europe. Estonia opened accession negotiations with the European Union in 1998 and joined in 2004, shortly after becoming a member of NATO.
Contemporary Estonian government.
On June 28, 1992, Estonian voters approved the constitutional assembly's draft constitution and implementation act, which established a parliamentary government with a president as chief of state and with a government headed by a prime minister.
The Riigikogu, a unicameral legislative body, is the highest organ of state authority. It initiates and approves legislation sponsored by the prime minister. The prime minister has full responsibility and control over his cabinet.
Meri presidency and Laar premiership (1992–2001).
Parliamentary and presidential elections were held on September 20, 1992. Approximately 68% of the country's 637,000 registered voters cast ballots. Lennart Meri, an outstanding writer and former Minister of Foreign Affairs, won this election and became president. He chose 32-year-old historian and Christian Democratic Party founder Mart Laar as prime minister.
In February 1992, and with amendments in January 1995, the Riigikogu renewed Estonia's 1938 citizenship law, which also provides equal civil protection to resident aliens.
In 1996, Estonia ratified a border agreement with Latvia and completed work with Russia on a technical border agreement. President Meri was re-elected in free and fair indirect elections in August and September in 1996. During parliamentary elections in 1999, the seats in Riigikogu were divided as follows: the Centre Party received 28, the Pro Patria Union 18, the Reform Party 18, the People's Party Moderates (election cartel between Moderates and People's Party) 17, Coalition Party 7, Country People's Party (now People's Union) 7, United People's Party's electoral cartel 6 seats. Pro Patria Union, the Reform Party, and the Moderates formed a government with Mart Laar as prime minister whereas the Centre Party with the Coalition Party, People's Union, United People's Party, and Members of Parliament who were not members of factions formed the opposition in the Riigikogu.
The 1999 Parliamentary election, with a 5% threshold and no electoral cartel allowed, resulted in a disaster for the Coalition Party, which achieved only seven seats together with two of its smaller allies. Estonian Ruralfolk Party, which participated the election on its own list, obtained seven seats as well.
The programme of Mart Laar's government was signed by Pro Patria Union, Reform Party, Moderates and People’s Party. The latter two merged soon after, so Mart Laar’s second government is widely known as "Kolmikliit", or Tripartite coalition. Notwithstanding the different political orientation of the ruling parties, the coalition stayed united until Mart Laar resigned in December 2001, after Reform Party had broken up the same coalition in Tallinn municipality, making opposition leader Edgar Savisaar new Mayor of Tallinn. After resignation of Laar, Reform Party and Estonian Centre Party formed a coalition that lasted until next parliamentary election, 2003.
The Moderates joined with the People's Party on 27 November 1999, forming the People's Party Moderates.
Rüütel presidency and Siim Kallas government (2001–2002).
In fall 2001 Arnold Rüütel became the President of the Republic of Estonia, and in January 2002 Prime Minister Laar stepped down. On January 28, 2002 the new government was formed from a coalition of the centre-right Estonian Reform Party and the more left wing Centre Party, with Siim Kallas from the Reform Party of Estonia as Prime Minister.
In 2003, Estonia joined the NATO defense alliance.
Juhan Parts government (2003).
Following parliamentary elections in 2003, the seats were allocated as follows (the United People's Party failed to meet the 5% threshold):
Voter turnout was higher than expected at 58%.
The results saw the Centre Party win the most votes but they were only 0.8% ahead of the new Res Publica party. As a result both parties won 28 seats, which was a disappointment for the Centre Party who had expected to win the most seats. Altogether the right of centre parties won 60 seats, compared to only 41 for the left wing, and so were expected to form the next government.
Both the Centre and Res Publica parties said that they should get the chance to try and form the next government, while ruling out any deal between themselves. President Rüütel had to decide who he should nominate as Prime Minister and therefore be given the first chance at forming a government. On the 2 April he invited the leader of the Res Publica party, Juhan Parts to form a government and after negotiations a coalition government composed of Res Publica, the Reform Party and the People's Union of Estonia was formed on the 10 April.
On 14 September 2003, following negotiations that began in 1998, the citizens of Estonia were asked in a referendum whether or not they wished to join the European Union. With 64% of the electorate turning out the referendum passed with a 66.83% margin in favor, 33.17% against. Accession to the EU took place on 1 May of the following year.
In February 2004 the People's Party Moderates renamed themselves as Social Democratic Party of Estonia.
On the 8 May 2004, a defection of several Centre Party members to form a new party, the Social Liberal Party, over a row concerning the Centrists' "no" stance to joining the European Union changed the allocation of the seats in Riigikogu. Social-liberals had 8 seats, but a hope to form a new party disappeared by the 10 May 2005, because most members in the social-liberal group joined other parties.
Andrus Ansip government (2004).
On 24 March Prime Minister Juhan Parts announced his resignation following a vote of no confidence in the Riigikogu against Minister of Justice Ken-Marti Vaher, which was held on the 21 March. Result: 54 pro (Social Democrats, Social Liberals, People's Union, Pro Patria Union and Reform Party) without no against or neutral MPs. 32 MPs (Res Publica and Centre Party) did not take part.
On 4 April 2005, President Rüütel nominated Reform party leader Andrus Ansip as Prime Minister designate by and asked him to form a new government, the 8th in 12 years. Ansip formed a government out of a coalition of his Reform Party with the People’s Union and the Centre Party. Approval by the Riigikogu, which by law must decide within 14 days of his nomination, came on 12 April 2005. Ansip was backed by 53 out of 101 members of the Estonian parliament. Forty deputies voted against his candidature.
The general consensus in the Estonian media seems to be that the new Andrus Ansip's cabinet, on the level of competence, is not necessarily an improvement over the old one.
On 18 May 2005, Estonia signed a border treaty with the Russian Federation in Moscow. The treaty was ratified by the Riigikogu on 20 June 2005. However, in the end of June the Russian Ministry of Foreign Affairs informed that it did not intend to become a party to the border treaty and did not consider itself bound by the circumstances concerning the object and the purposes of the treaty because Riigikogu had attached a preambula to the ratification act that referenced earlier documents that mentioned the Soviet occupation and the uninterrupted legal continuity of the Republic of Estonia during the Soviet period. The issue remains unsolved and is in focus of European level discussions.
On 4 April 2006, Fatherland Union and Res Publica decided to form a united right-conservative party. The two parties joining was approved on 4 June by both parties in Pärnu. The joined party name is Isamaa ja Res Publica Liit (Union of Pro Patria and Res Publica).
2007 elections.
The 2007 Parliamentary Elections have shown an improvement in the scores of the Reform Party, gaining 12 seats and reaching 31 MPs; the Centre Party held, while the unified right-conservative Union of Pro Patria and Res Publica lost 16. Socialdemocrats gained 4 seats, while the Greens entered the Parliaments with 7 seats, at the expenses of the agrarian People's Union which lost 6. The new configuration of the Estonian Parliament shows a prevalence of the centre-left parties. The Centre Party, led by the mayor of Tallinn Edgar Savisaar, has been increasingly excluded from collaboration, since his open collaboration with Putin's United Russia party, real estate scandals in Tallinn, and the Bronze Soldier controversy, considered as a deliberate attempt of splitting the Estonian society by provoking the Russian minority. The lack of a concrete possibility for government alternance in Estonia has been quoted as a concern.
Estonia and the European Union.
On 14 September 2003, following negotiations that began in 1998, the citizens of Estonia were asked in a referendum whether or not they wished to join the European Union. With 64% of the electorate turning out the referendum passed with a 66.83% margin in favor, 33.17% against. Accession to the EU took place on 1 May of the following year.
In its first European Parliament elections in 2004, Estonia elected 3 MEPs for the Social Democratic Party (PES), while the governing Res Publica Party and People's Union polled poorly, not being able to gain any of the other 3 MEPs posts.
The voter turnout in Estonia was one of the lowest of all member countries at only 26.8%. A similar trend was visible in most of the new member states that joined the EU in 2004.
The European Parliament election of 2009 in Estonia scored a 43.9% turnout – about 17.1% higher than during the previous election, and slightly above the European average of 42.94%.
Six seats were up for taking in this election: two of them were won by the Estonian Centre Party. Estonian Reform Party, Union of Pro Patria and Res Publica, Social Democratic Party and an independent candidate Indrek Tarand (who gathered the support of 102,460 voters, only 1,046 votes less than the winner of the election) all won one seat each. The success of independent candidates has been attributed both to general disillusionment with major parties and use of closed lists which rendered voters incapable to cast a vote for specific candidates in party lists.
On 1 January 2011 Estonia adopted the Euro. The enlargement of the eurozone, was hailed as a good sign in a period of global financial crisis. However, the government cut down public service salaries; the only opposition, in the absence of organised unions, came from Estonian teachers, whose salary cuts were therefore limited.
Estonian euro coins entered circulation on 1 January 2011. Estonia is the fifth of ten states that joined the EU in 2004, and the first ex-Soviet republic, to join the eurozone. Of the ten new member states, Estonia was the first to unveil its design. It originally planned to adopt the euro on 1 January 2007; however, it did not formally apply when Slovenia did, and officially changed its target date to 1 January 2008, and later, to 1 January 2011. On 12 May 2010 the European Commission announced that Estonia had met all criteria to join the eurozone. On 8 June 2010, the EU finance ministers agreed that Estonia would be able to join the euro on 1 January 2011. On 13 July 2010, Estonia received the final approval from the ECOFIN to adopt the euro as from 1 January 2011. On the same date the exchange rate at which the kroon would be exchanged for the euro (€1 = 15.6466 krooni) was also announced. On 20 July 2010, mass production of Estonian euro coins began in the mint of Finland.
Being a member of the eurozone, NATO and the EU, Estonia is the most integrated in Western European organizations of all Nordic states
Estonia–Russia relations in the late 2000s.
Estonia–Russia relations remain tense. According to the Estonian Internal Security Service, Russian influence operations in Estonia form a complex system of financial, political, economic and espionage activities in the Republic of Estonia for the purposes of influencing Estonia's political and economic decisions in ways considered favourable to Russian Federation and conducted under the sphere-of-influence doctrine known as "near abroad". According to the Centre for Geopolitical Studies, the Russian information campaign, which the centre characterises as a "real mud-throwing" exercise, had provoked a split in Estonian society amongst Russian speakers, inciting some to riot over the relocation of the Bronze Soldier of Tallinn, a cenotaph commemorating the soldiers killed in World War II. Estonia regarded the 2007 cyberattacks on Estonia as an information operation intended to influence the decisions and actions of the Estonian government. While Russia denied any direct involvement in the attacks, hostile rhetoric in the media from the political elite influenced people to attack. Following the 2007 cyber-attacks, the NATO Cooperative Cyber Defence Centre of Excellence (CCDCOE) was established in Tallinn.

</doc>
<doc id="42788" url="http://en.wikipedia.org/wiki?curid=42788" title="380">
380

Year 380 (CCCLXXX) was a leap year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Augustus and Augustus (or, less frequently, year 1133 "Ab urbe condita"). The denomination 380 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42789" url="http://en.wikipedia.org/wiki?curid=42789" title="377">
377

Year 377 (CCCLXXVII) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Augustus and Merobaudes (or, less frequently, year 1130 "Ab urbe condita"). The denomination 377 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Arts and sciences.
</onlyinclude>

</doc>
<doc id="42790" url="http://en.wikipedia.org/wiki?curid=42790" title="375">
375

Year 375 (CCCLXXV) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year after the Consulship of Augustus and Equitius (or, less frequently, year 1128 "Ab urbe condita"). The denomination 375 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42794" url="http://en.wikipedia.org/wiki?curid=42794" title="History of Burundi">
History of Burundi

Burundi is one of the few countries in Africa, along with its closely linked neighbour Rwanda among others, to be a direct territorial continuation of a pre-colonial era African state.
Kingdom of Burundi (1680–1966).
The origins of Burundi are known from a mix of oral history and archaeology. There are two main founding legends for Burundi. Both suggest that the nation was founded by a man named Cambarantama. The legend most promoted today states that he was Rwandan. The other version, more common in pre-colonial Burundi says that Cambarantama came from the southern state of Buha.
The first evidence of the Burundian state is from 16th century where it emerged on the eastern foothills. Over the following centuries it expanded, annexing smaller neighbours and competing with Rwanda. Its greatest growth occurred under Ntare IV Rutaganzwa Rugamba, who ruled the country from about 1796 to 1850 and saw the kingdom double in size.
The Kingdom of Burundi was characterized by a hierarchical political authority and tributary economic exchange. The king, known as the "mwami" headed a princely aristocracy ("ganwa") which owned most of the land and required a tribute, or tax, from local farmers and herders. In the mid-18th century, this Tutsi royalty consolidated authority over land, production, and distribution with the development of the "ubugabire"—a patron-client relationship in which the populace received royal protection in exchange for tribute and land tenure.
European contact (1856).
European explorers and missionaries made brief visits to the area as early as 1856, and they compared the organization of the kingdom of Burundi with that of the old Greek empire. It was not until 1899 that Burundi became a part of German East Africa. Unlike the Rwandan monarchy, which decided to accept the German advances, the Burundian king Mwezi IV Gisabo opposed all European influence, refusing to wear European clothing and resisting the advance of European missionaries or administrators.
German East Africa (1899-1916).
The Germans used armed force and succeeded in doing great damage, but did not destroy the king’s power. Eventually they backed one of the king's sons-in-law Maconco in a revolt against Gisabo. Gisabo was eventually forced to concede and agreed to German suzerainty. The Germans then helped him suppress Maconco's revolt. The smaller kingdoms along the western shore of Lake Victoria were also attached to Burundi.
Even after this the foreign presence was minimal and the kings continued to rule much as before. The Europeans did, however, bring devastating diseases affecting both people and animals. Affecting the entire region, Burundi was especially hard hit. A great famine hit in 1905, with others striking the entire Great Lakes region in 1914, 1923 and 1944. Between 1905 and 1914 half the population of the western plains region died.
Belgian and United Nations governance (1916-1962).
In 1916 Belgian troops conquered the area during the First World War. In 1923, the League of Nations mandated to Belgium the territory of Ruanda-Urundi, encompassing modern-day Rwanda and Burundi, but stripping the western kingdoms and giving them to British administered Tanganyika. The Belgians administered the territory through indirect rule, building on the Tutsi-dominated aristocratic hierarchy.
Following World War II, Ruanda-Urundi became a United Nations Trust Territory under Belgian administrative authority. It wasn't until 10 November 1959 that Belgium committed itself to political reform and legalised the emergence of competing political parties. Two political parties emerged: the Union for National Progress (UPRONA), a multi-ethnic party led by Tutsi Prince Louis Rwagasore and the Christian Democratic Party (PDC) supported by Belgium. On 13 October 1961, Prime Minister Prince Rwagasore was assassinated following an UPRONA victory in Burundi's United Nations supervised legislative elections of 8 September 1961
Independence (1962).
Full independence was achieved on July 1, 1962. In the context of weak democratic institutions at independence, Tutsi King Mwambutsa IV Bangiriceng established a constitutional monarchy comprising equal numbers of Hutus and Tutsis. The 15 January 1965 assassination of the Hutu prime minister Pierre Ngendandumwe set in motion a series of destabilizing Hutu revolts and subsequent governmental repression.
These were in part in reaction to Rwanda's "Social Revolution" of 1959-1961, where Rwandan Tutsi were subject to mass murder by the new government of Hutu Grégoire Kayibanda. In Burundi the Tutsi became committed to ensuring they would not meet the same fate and much of the country's military and police forces became controlled by Tutsis. Unlike Rwanda, which allied itself with the United States in the Cold War, Burundi after independence became affiliated with China.
The monarchy refused to recognize gains by Hutu candidates in the first legislative elections held by Burundi as an independent country on 10 May 1965. In response, a group of Hutu carried out a failed coup attempt against the monarchy on 18 October 1965, which in turn prompted the killing of scores of Hutu politicians and intellectuals. On 8 July 1966, King Mwambutsa IV was deposed by his son, Prince Ntare V, who himself was deposed by his prime minister Capt. Michel Micombero on 28 November 1966. He abolished the monarchy and declared a republic. A de facto military regime emerged and civil unrest continued throughout the late 1960s and early 1970s. Micombero headed a clique of ruling Hima, the Tutsi subgroup located in southern Burundi. Similar to 1965, rumors of an impending Hutu coup in 1969 prompted the arrest and execution of scores of prominent political and military figures.
In June 1971, a group of Banyaruguru, the socially "higher up" subgroup of Tutsi located in the north of the country, were accused of conspiracy by the ruling Hima clique. On 14 January 1972, a military tribunal sentenced four Banyaruguru officers and five civilians to death, and seven to life imprisonment. To the Hima concerns about a Hutu uprising or Banyaruguru-led coup was added the return of Ntare V from exile, a potential rallying point for the Hutu majority.
1972 genocide.
On April 29, there was an outbreak of violence in the south of the country, also the base of the Hima, where bands of roving Hutu committed innumerable atrocities against Tutsi civilians. All civilian and military authorities in the city of Bururi were killed and the insurgents then seized the armories in the towns of Rumonge and Nyanza-Lac. They then attempted to kill every Tutsi they could, as well as some Hutu who refused to participate in the rebellion, before retreating to Vyanda, near Bururi, and proclaiming a "Republic of Martyazo."
A week after the insurgent proclamation of a republic, government troops moved in. Meanwhile, President Micombero declared martial law on May 30 and asked Zairean President Mobutu Sese Seko for assistance. Congolese paratroopers were deployed to secure the airport while the Burundi army moved into the countryside. Africanist René Lemarchand notes, "What followed was not so much a repression as a hideous slaughter of Hutu civilians. The carnage went on unabated through the month of August. By then virtually every educated Hutu element, down to secondary school students, was either dead or in flight."
Because the perpetrators, composed of government troops and the Jeunesses Révolutionnaires Rwagasore (JRR), the youth wing of the Union for National Progress ruling party, targeted primarily civil servants, educated males and university students, solely because of the "Hutuness" and irrespective of if they posed a threat, Lemarchand terms the eradication a "partial genocide." One of the first to be killed was deposed monarch Ntare V, in Gitega.
From late April to September 1972, an estimated 200,000 to 300,000 Hutu were killed. About 300,000 people became refugees, with most fleeing to Tanzania. In an effort to attract sympathy from the United States, the Tutsi-dominated government accused the Hutu rebels of having Communist leanings, although there is no credible evidence that this was actually the case. Lemarhand notes that, while crushing the rebellion was the first priority, the genocide was successful in a number of other objectives: ensuring the long-term stability of the Tutsi state by eliminating Hutu elites and potential elites; turning the army, police and gendarmie into a Tutsi monopoly; denying the potential return of monarchy through the murder of Ntare V; and creating a new legitimacy for the Hima-dominated state as protector of the country, especially for the previously fractious Tutsi-Banyaruguru.
Post-1972 genocide developments.
In 1976, Colonel Jean-Baptiste Bagaza took power in a bloodless coup. Although Bagaza led a Tutsi-dominated military regime, he encouraged land reform, electoral reform, and national reconciliation. In 1981, a new constitution was promulgated. In 1984, Bagaza was elected head of state, as the sole candidate. After his election, Bagaza's human rights record deteriorated as he suppressed religious activities and detained political opposition members.
In 1987, Major Pierre Buyoya overthrew Col. Bagaza in a military coup d'état. He dissolved opposition parties, suspended the 1981 constitution, and instituted his ruling Military Committee for National Salvation (CSMN). During 1988, increasing tensions between the ruling Tutsis and the majority Hutus resulted in violent confrontations between the army, the Hutu opposition, and Tutsi hardliners. During this period, an estimated 150,000 people were killed, with tens of thousands of refugees flowing to neighboring countries. Buyoya formed a commission to investigate the causes of the 1988 unrest and to develop a charter for democratic reform.
In 1991, Buyoya approved a constitution that provided for a president, non-ethnic government, and a parliament. Burundi's first Hutu president, Melchior Ndadaye, of the Hutu-dominated Front for Democracy in Burundi (FRODEBU) Party, was elected in 1993.
1993 Genocide and Civil War (1993-2005).
Ndadaye was assassinated three months later, in October 1993, by Tutsi army extremists. The country’s situation rapidly declined as Hutu peasants began to rise up and massacre Tutsi. In acts of brutal retribution, the Tutsi army proceeded to round up thousands of Hutu and kill them. The Rwandan Genocide in 1994, sparked by the killing of Ndadaye’s successor Cyprien Ntaryamira, further aggravated the conflict in Burundi by sparking additional massacres of Tutsis.
A decade of civil war followed, as the Hutu formed militias in the refugee camps of northern Tanzania. An estimated 300,000 people were killed in clashes and reprisals against the local population, with 550,000 citizens (nine percent of the population) being displaced. After the assassination of Ntaryamira, the Hutu presidency and Tutsi military operated under a power-sharing political system until July 1996, when Tutsi Pierre Buyoya seized power in a military coup. Under international pressure, the warring factions negotiated a peace agreement in Arusha in 2000, which called for ethnically balanced military and government and democratic elections.
Two powerful Hutu rebel groups (the CNDD-FDD and the FNL) refused to sign the peace agreement and fighting continued in the countryside. Finally, the CNDD-FDD agreed to sign a peace deal in November 2003 and joined the transitional government. The last remaining rebel group, the FNL, continued to reject the peace process and committed sporadic acts of violence in 2003 and 2004, finally signing a cease fire agreement in 2006.
Post-war (2005-).
Former President Domitien Ndayizeye and his political supporters were arrested in 2006 and accused of plotting a coup, but later he was acquitted by the Supreme Court. International human rights groups claimed that the current government was framing Domitien Ndayizeye by torturing him into false confessions of a coup plot. Along with these accusations, in December 2006 the International Crisis Group labeled Burundi’s government with a “deteriorating” status in its treatment of human rights. The organization reported that the government had arrested critics, muzzled the press, committed human rights abuses, and tightened its control over the economy, and that "unless it [reversed] this authoritarian course, it risk[ed] triggering violent unrest and losing the gains of peace process."
In February 2007, the U.N. officially shut down its peacekeeping operations in Burundi and turned its attention to rebuilding the nation’s economy, which relies heavily on tea and coffee, but suffered severely during 12 years of civil war. The U.N. had deployed 5,600 peacekeepers since 2004, and several hundred troops remained to work with the African Union in monitoring the ceasefire. The U.N. donated $35 million to Burundi to work on infrastructure, to promote democratic practices, to rebuild the military, and to defend human rights.
SOS Children, an NGO, uses HIV testing and prevention strategies, counseling, de-stigmatization, antiretroviral drugs and condoms to combat AIDS. Sample testing had shown that those who were HIV positive were 20 percent of the urban population and 6% of the rural population. Nevertheless, the death toll due to the syndrome has been devastating: the UN estimated 25,000 deaths in 2001 and Oxfam estimated 45,000 deaths in 2003.
Reaching a stable compromise on post-transition power sharing was difficult. Although a post-transition constitution was approved in September 2004, it was approved over a boycott by the Tutsi parties. In addition, the Arusha Peace Agreement mandated that local and national elections be held before the ending of the transitional period on 31 October 2004, but transitional institutions were extended. On 28 February 2005, however, Burundians popularly approved a post-transitional constitution by national referendum, with elections set to take place throughout the summer of 2005. After local, parliamentary, and other elections in June and July, on 19 August 2005, the good governance minister, Pierre Nkurunziza, became the first post-transitional president.
He was re-elected in 2010 with more than 91% of the votes amidst an opposition boycott and sworn in for his second term on 26 August 2010.
In April, 2015 Nkurunziza announced that he will seek a third term in office. The opposition said that Nkurunziza's bid to extend his term is in defiance of the constitution, as it bars the president from running for a third term. However, Nkurunziza's allies say his first term does not count as he was appointed by parliament and not directly by the people. On April 26 police clashed with demonstrators protesting Nkurunziza’s announcement that he will seek a third term in office. At least six people were killed in the first two days of ongoing protests. The government shut down multiple radio stations and arrested a prominent civil society leader, Pierre-Claver Mbonimpa. UN General Secretary Ban Ki-moon said, in a statement, that he had despatched his special envoy for the region, Said Djinnit, to Burundi for talks with Nkurunziza. African Union commission head Nkosazana Dlamini-Zuma said she welcomed a decision by Burundi's Senate to ask the Constitutional Court to rule whether Nkurunziza could stand for re-election. More than 24,000 people have fled Burundi in April, as tensions mount ahead of presidential elections in June, the UN refugee agency said.
On May 13, 2015, Burundi army General Godefroid Niyombareh, former head of Burundian intelligence, declared a coup via radio while Nkurunziza was abroad attending a summit in Tanzania with other African leaders. Niyombareh had been fired by the President in February. Despite reports that gunshots had been heard and people were celebrating in the streets of the capital, government officials dismissed the threat and claimed to remain in control.
See also.
General:

</doc>
<doc id="42795" url="http://en.wikipedia.org/wiki?curid=42795" title="Anorexia (symptom)">
Anorexia (symptom)

Anorexia is the decreased sensation of appetite. While the term in non-scientific publications is often used interchangeably with anorexia nervosa, many possible causes exist for a decreased appetite, some of which may be harmless, while others indicate a serious clinical condition or pose a significant risk.
For example, anorexia of infection is part of the acute phase response (APR) to infection. The APR can be triggered by lipopolysaccharides and peptidoglycans from bacterial cell walls, bacterial DNA, double-stranded viral RNA, and viral glycoproteins, which can trigger production of a variety of proinflammatory cytokines. These can have an indirect effect on appetite by a number of means, including peripheral afferents from their sites of production in the body, by enhancing production of leptin from fat stores. Inflammatory cytokines can also signal to the central nervous system more directly by specialized transport mechanisms through the blood–brain barrier, via circumventricular organs (which are outside the barrier), or by triggering production of eicosanoids in the endothelial cells of the brain vasculature. Ultimately the control of appetite by this mechanism is thought to be mediated by the same factors normally controlling appetite, such as neurotransmitters (serotonin, dopamine, histamine, norepinephrine, corticotropin releasing factor, neuropeptide Y, and α-melanocyte-stimulating hormone).
Complications.
Sudden Cardiac Death.
Anorexia is a relatively common condition that can lead patients to have dangerous electrolyte imbalances, leading to acquired long QT syndrome which can result in sudden cardiac death. This can develop over a prolonged period of time, and the risk is further heightened when feeding resumes after a period of abstaining from consumption. Care must be taken under such circumstances to avoid potentially fatal complications of refeeding syndrome.

</doc>
<doc id="42798" url="http://en.wikipedia.org/wiki?curid=42798" title="Sophie Marceau">
Sophie Marceau

Sophie Danièle Sylvie Maupu (]; born 17 November 1966) is a French actress, director, screenwriter, and author. As a teenager, Marceau achieved popularity with her debut films "La Boum" (1980) and "La Boum 2" (1982), receiving a César Award for Most Promising Actress. She became a film star in Europe with a string of successful films, including "L'Étudiante" (1988), "Pacific Palisades" (1990), "Fanfan" (1993), and "Revenge of the Musketeers" (1994). Marceau became an international film star with her performances in "Braveheart" (1995), "Firelight" (1997), and the nineteenth James Bond film "The World Is Not Enough" (1999).
Early life.
Sophie Marceau was born 17 November 1966 in Paris, the second child of Simone (née Morisset), a shop assistant, and Benoît Maupu, a truck driver. Her parents divorced when she was nine years old.
Film career.
In February 1980, Marceau and her mother came across a model agency looking for teenagers. Marceau had photos taken at the agency, but did not think anything would come of it. At the same time, Françoise Menidrey, the casting director for Claude Pinoteau's "La Boum" (1980), asked modeling agencies to recommend a new teenager for the project. After viewing the rushes, Alain Poiré, the director of the Gaumont Film Company, signed Marceau to a long-term contract. "La Boum" was a hit movie, not only in France, where 4,378,500 tickets were sold, but also in several other European countries. In 1981, Marceau made her singing debut with French singer François Valéry on record "Dream in Blue", written by Pierre Delanoë.
In 1982, at age 15, Marceau bought back her contract with Gaumont for one million French francs. She borrowed most of the money. After starring in the sequel film "La Boum 2" (1982), Marceau focused on more dramatic roles, including the historical drama "Fort Saganne" in 1984 with Gérard Depardieu and Catherine Deneuve, "Joyeuses Pâques" ("Happy Easter") in 1984, "L'amour braque" and "Police" in 1985, and "Descente aux enfers" ("Descent Into Hell") in 1986. In 1988, she starred in "L'Étudiante" ("The Student") and the historical adventure film "Chouans!". That year, Marceau was named Best Romantic Actress at the International Festival of Romantic Movies for her role in "Chouans!"
In 1989, Marceau starred in "My Nights Are More Beautiful Than Your Days", which was directed by her long-time boyfriend Andrzej Zulawski. In 1990, she starred in "Pacific Palisades" and "La note bleue", her third film directed by her companion. In 1991, she ventured into the theater in "Eurydice", which earned Marceau the Moliere Award for Best Female Newcomer. Throughout the 1990s, Marceau began making less-dramatic films, such as the comedy "Fanfan" in 1993 and "Revenge of the Musketeers" ("La fille de d'Artagnan") in 1994—both popular in Europe and abroad. That year, she returned to the theatre as Eliza Doolittle in "Pygmalion".
Marceau achieved international recognition in 1995 playing the role of Princess Isabelle in Mel Gibson's "Braveheart". That year, she was part of an ensemble of international actors in the French film directed by Michelangelo Antonioni and Wim Wenders, "Beyond the Clouds". In 1997, she continued her string of successful films with William Nicholson's "Firelight", filmed in England, Véra Belmont's "Marquise", filmed in France, and Bernard Rose's "Anna Karenina", filmed in Russia. In 1999, she played Hippolyta in "A Midsummer Night's Dream", and the villainess Bond girl Elektra King in "The World Is Not Enough". In 2000, Marceau teamed up again with her then-boyfriend Andrzej Zulawski to film "Fidelity", playing the role of a talented photographer who takes a job at a scandal-mongering tabloid and becomes romantically involved with an eccentric children's book publisher.
In recent years, Marceau has continued to appear in a wide variety of roles, mainly in French films, playing a widowed nurse in "Nelly" ("À ce soir") in 2004, an undercover police agent in "Anthony Zimmer" in 2005, and the troubled daughter of a murdered film star in "Trivial" in 2007. In 2008, Marceau played a member of the French Resistance movement in "Female Agents", and a struggling single mother in "LOL (Laughing Out Loud)". In 2009, she teamed up with Monica Bellucci in "Don't Look Back" about the mysterious connection between two women who've never met. In 2010, Marceau played a successful business executive forced to confront her unhappy childhood in "With Love... from the Age of Reason" ("L'âge de raison").
In 2012, Marceau played a forty-something career woman who falls in love with a young jazz musician in "Happiness Never Comes Alone". In 2013, she appeared in "Arrêtez-moi" ("Arrest Me") as a woman who shows up at a police station and confesses to the murder of her abusive husband several years earlier.
She was selected to be on the jury for the main competition section of the 2015 Cannes Film Festival.
Author and director.
In 1995, Marceau wrote a semi-autobiographical novel, "Menteuse" (the English translation, "Telling Lies", was published in 2001). Marceau's work was described as "an exploration of female identity".
In 2002, Marceau made her directorial debut in the feature film "Speak to Me of Love", for which she was named Best Director at the Montreal World Film Festival. The film starred Judith Godrèche. It was her second directorial effort, following her nine-minute short film "L'aube à l'envers" in 1995, which also starred Godrèche. In 2007, she directed "Trivial", her second feature film.
Personal life.
From 1985 to 2001, Marceau had a relationship with director Andrzej Żuławski. Their son Vincent was born in July 1995. In 2001, Marceau separated from Żuławski and began a six-year relationship with producer Jim Lemley. They have a daughter, Juliette (born June 2002 in London).
From 2007 to 2014, she had a relationship with Christopher Lambert, with whom she appeared in the films "Trivial" and "Cartagena". On July 11, 2014 the couple announced their amicable separation.

</doc>
<doc id="42799" url="http://en.wikipedia.org/wiki?curid=42799" title="Speech synthesis">
Speech synthesis

Speech Synthesis is the artificial production of human speech. A computer system used for this purpose is called a speech computer or speech synthesizer, and can be implemented in software or hardware products. A text-to-speech (TTS) system converts normal language text into speech; other systems render symbolic linguistic representations like phonetic transcriptions into speech.
Synthesized speech can be created by concatenating pieces of recorded speech that are stored in a database. Systems differ in the size of the stored speech units; a system that stores phones or diphones provides the largest output range, but may lack clarity. For specific usage domains, the storage of entire words or sentences allows for high-quality output. Alternatively, a synthesizer can incorporate a model of the vocal tract and other human voice characteristics to create a completely "synthetic" voice output.
The quality of a speech synthesizer is judged by its similarity to the human voice and by its ability to be understood clearly. An intelligible text-to-speech program allows people with visual impairments or reading disabilities to listen to written works on a home computer. Many computer operating systems have included speech synthesizers since the early 1990s.
A text-to-speech system (or "engine") is composed of two parts: a front-end and a back-end. The front-end has two major tasks. First, it converts raw text containing symbols like numbers and abbreviations into the equivalent of written-out words. This process is often called "text normalization", "pre-processing", or "tokenization". The front-end then assigns phonetic transcriptions to each word, and divides and marks the text into prosodic units, like phrases, clauses, and sentences. The process of assigning phonetic transcriptions to words is called "text-to-phoneme" or "grapheme-to-phoneme" conversion. Phonetic transcriptions and prosody information together make up the symbolic linguistic representation that is output by the front-end. The back-end—often referred to as the "synthesizer"—then converts the symbolic linguistic representation into sound. In certain systems, this part includes the computation of the "target prosody" (pitch contour, phoneme durations), which is then imposed on the output speech.
History.
Long before electronic signal processing was invented, there were those who tried to build machines to create human speech. Some early legends of the existence of "Brazen Heads" involved Pope Silvester II (d. 1003 AD), Albertus Magnus (1198–1280), and Roger Bacon (1214–1294).
In 1779, the Danish scientist Christian Kratzenstein, working at the Russian Academy of Sciences, built models of the human vocal tract that could produce the five long vowel sounds (in notation, they are [aː], [eː], [iː], [oː] and [uː]). This was followed by the bellows-operated "acoustic-mechanical speech machine" by Wolfgang von Kempelen of Pressburg, Hungary, described in a 1791 paper. This machine added models of the tongue and lips, enabling it to produce consonants as well as vowels. In 1837, Charles Wheatstone produced a "speaking machine" based on von Kempelen's design, and in 1857, M. Faber built the "Euphonia". Wheatstone's design was resurrected in 1923 by Paget.
In the 1930s, Bell Labs developed the vocoder, which automatically analyzed speech into its fundamental tone and resonances. From his work on the vocoder, Homer Dudley developed a keyboard-operated voice synthesizer called The Voder (Voice Demonstrator), which he exhibited at the 1939 New York World's Fair.
The Pattern playback was built by Dr. Franklin S. Cooper and his colleagues at Haskins Laboratories in the late 1940s and completed in 1950. There were several different versions of this hardware device but only one currently survives. The machine converts pictures of the acoustic patterns of speech in the form of a spectrogram back into sound. Using this device, Alvin Liberman and colleagues were able to discover acoustic cues for the perception of phonetic segments (consonants and vowels).
Dominant systems in the 1980s and 1990s were the MITalk system, based largely on the work of Dennis Klatt at MIT, and the Bell Labs system; the latter was one of the first multilingual language-independent systems, making extensive use of natural language processing methods.
Early electronic speech synthesizers sounded robotic and were often barely intelligible. The quality of synthesized speech has steadily improved, but output from contemporary speech synthesis systems is still clearly distinguishable from actual human speech.
As the cost-performance ratio causes speech synthesizers to become cheaper and more accessible to the people, more people will benefit from the use of text-to-speech programs.
Electronic devices.
The first computer-based speech synthesis systems were created in the late 1950s. The first general English text-to-speech system was developed by Noriko Umeda "et al." in 1968 at the Electrotechnical Laboratory, Japan. In 1961, physicist John Larry Kelly, Jr and colleague Louis Gerstman used an IBM 704 computer to synthesize speech, an event among the most prominent in the history of Bell Labs. Kelly's voice recorder synthesizer (vocoder) recreated the song "Daisy Bell", with musical accompaniment from Max Mathews. Coincidentally, Arthur C. Clarke was visiting his friend and colleague John Pierce at the Bell Labs Murray Hill facility. Clarke was so impressed by the demonstration that he used it in the climactic scene of his screenplay for his novel "", where the HAL 9000 computer sings the same song as it is being put to sleep by astronaut Dave Bowman. Despite the success of purely electronic speech synthesis, research is still being conducted into mechanical speech synthesizers.
Handheld electronics featuring speech synthesis began emerging in the 1970s. One of the first was the Telesensory Systems Inc. (TSI) "Speech+" portable calculator for the blind in 1976. Other devices were produced primarily for educational purposes, such as Speak & Spell, produced by Texas Instruments in 1978. Fidelity released a speaking version of its electronic chess computer in 1979. The first video game to feature speech synthesis was the 1980 shoot 'em up arcade game, "Stratovox", from Sun Electronics. Another early example was the arcade version of "Berzerk", released that same year. The first multi-player electronic game using voice synthesis was "Milton" from Milton Bradley Company, which produced the device in 1980.
Synthesizer technologies.
The most important qualities of a speech synthesis system are "naturalness" and "intelligibility". Naturalness describes how closely the output sounds like human speech, while intelligibility is the ease with which the output is understood. The ideal speech synthesizer is both natural and intelligible. Speech synthesis systems usually try to maximize both characteristics.
The two primary technologies generating synthetic speech waveforms are "concatenative synthesis" and "formant synthesis". Each technology has strengths and weaknesses, and the intended uses of a synthesis system will typically determine which approach is used.
Concatenation synthesis.
Concatenative synthesis is based on the concatenation (or stringing together) of segments of recorded speech. Generally, concatenative synthesis produces the most natural-sounding synthesized speech. However, differences between natural variations in speech and the nature of the automated techniques for segmenting the waveforms sometimes result in audible glitches in the output. There are three main sub-types of concatenative synthesis.
Unit selection synthesis.
Unit selection synthesis uses large databases of recorded speech. During database creation, each recorded utterance is segmented into some or all of the following: individual phones, diphones, half-phones, syllables, morphemes, words, phrases, and sentences. Typically, the division into segments is done using a specially modified speech recognizer set to a "forced alignment" mode with some manual correction afterward, using visual representations such as the waveform and spectrogram. An index of the units in the speech database is then created based on the segmentation and acoustic parameters like the fundamental frequency (pitch), duration, position in the syllable, and neighboring phones. At run time, the desired target utterance is created by determining the best chain of candidate units from the database (unit selection). This process is typically achieved using a specially weighted decision tree.
Unit selection provides the greatest naturalness, because it applies only a small amount of digital signal processing (DSP) to the recorded speech. DSP often makes recorded speech sound less natural, although some systems use a small amount of signal processing at the point of concatenation to smooth the waveform. The output from the best unit-selection systems is often indistinguishable from real human voices, especially in contexts for which the TTS system has been tuned. However, maximum naturalness typically require unit-selection speech databases to be very large, in some systems ranging into the gigabytes of recorded data, representing dozens of hours of speech. Also, unit selection algorithms have been known to select segments from a place that results in less than ideal synthesis (e.g. minor words become unclear) even when a better choice exists in the database. Recently, researchers have proposed various automated methods to detect unnatural segments in unit-selection speech synthesis systems.
Diphone synthesis.
Diphone synthesis uses a minimal speech database containing all the diphones (sound-to-sound transitions) occurring in a language. The number of diphones depends on the phonotactics of the language: for example, Spanish has about 800 diphones, and German about 2500. In diphone synthesis, only one example of each diphone is contained in the speech database. At runtime, the target prosody of a sentence is superimposed on these minimal units by means of digital signal processing techniques such as linear predictive coding, PSOLA or MBROLA. or more recent techniques such as pitch modification in the source domain using discrete cosine transform Diphone synthesis suffers from the sonic glitches of concatenative synthesis and the robotic-sounding nature of formant synthesis, and has few of the advantages of either approach other than small size. As such, its use in commercial applications is declining, although it continues to be used in research because there are a number of freely available software implementations.
Domain-specific synthesis.
Domain-specific synthesis concatenates prerecorded words and phrases to create complete utterances. It is used in applications where the variety of texts the system will output is limited to a particular domain, like transit schedule announcements or weather reports. The technology is very simple to implement, and has been in commercial use for a long time, in devices like talking clocks and calculators. The level of naturalness of these systems can be very high because the variety of sentence types is limited, and they closely match the prosody and intonation of the original recordings.
Because these systems are limited by the words and phrases in their databases, they are not general-purpose and can only synthesize the combinations of words and phrases with which they have been preprogrammed. The blending of words within naturally spoken language however can still cause problems unless the many variations are taken into account. For example, in non-rhotic dialects of English the "r" in words like "clear" /ˈklɪə/ is usually only pronounced when the following word has a vowel as its first letter (e.g. "clear out" is realized as /ˌklɪəɾˈʌʊt/). Likewise in French, many final consonants become no longer silent if followed by a word that begins with a vowel, an effect called liaison. This alternation cannot be reproduced by a simple word-concatenation system, which would require additional complexity to be context-sensitive.
Formant synthesis.
Formant synthesis does not use human speech samples at runtime. Instead, the synthesized speech output is created using additive synthesis and an acoustic model (physical modelling synthesis). Parameters such as fundamental frequency, voicing, and noise levels are varied over time to create a waveform of artificial speech. This method is sometimes called "rules-based synthesis"; however, many concatenative systems also have rules-based components.
Many systems based on formant synthesis technology generate artificial, robotic-sounding speech that would never be mistaken for human speech. However, maximum naturalness is not always the goal of a speech synthesis system, and formant synthesis systems have advantages over concatenative systems. Formant-synthesized speech can be reliably intelligible, even at very high speeds, avoiding the acoustic glitches that commonly plague concatenative systems. High-speed synthesized speech is used by the visually impaired to quickly navigate computers using a screen reader. Formant synthesizers are usually smaller programs than concatenative systems because they do not have a database of speech samples. They can therefore be used in embedded systems, where memory and microprocessor power are especially limited. Because formant-based systems have complete control of all aspects of the output speech, a wide variety of prosodies and intonations can be output, conveying not just questions and statements, but a variety of emotions and tones of voice.
Examples of non-real-time but highly accurate intonation control in formant synthesis include the work done in the late 1970s for the Texas Instruments toy Speak & Spell, and in the early 1980s Sega arcade machines and in many Atari, Inc. arcade games using the TMS5220 LPC Chips. Creating proper intonation for these projects was painstaking, and the results have yet to be matched by real-time text-to-speech interfaces.
Articulatory synthesis.
Articulatory synthesis refers to computational techniques for synthesizing speech based on models of the human vocal tract and the articulation processes occurring there. The first articulatory synthesizer regularly used for laboratory experiments was developed at Haskins Laboratories in the mid-1970s by Philip Rubin, Tom Baer, and Paul Mermelstein. This synthesizer, known as ASY, was based on vocal tract models developed at Bell Laboratories in the 1960s and 1970s by Paul Mermelstein, Cecil Coker, and colleagues.
Until recently, articulatory synthesis models have not been incorporated into commercial speech synthesis systems. A notable exception is the NeXT-based system originally developed and marketed by Trillium Sound Research, a spin-off company of the University of Calgary, where much of the original research was conducted. Following the demise of the various incarnations of NeXT (started by Steve Jobs in the late 1980s and merged with Apple Computer in 1997), the Trillium software was published under the GNU General Public License, with work continuing as gnuspeech. The system, first marketed in 1994, provides full articulatory-based text-to-speech conversion using a waveguide or transmission-line analog of the human oral and nasal tracts controlled by Carré's "distinctive region model".
HMM-based synthesis.
HMM-based synthesis is a synthesis method based on hidden Markov models, also called Statistical Parametric Synthesis. In this system, the frequency spectrum (vocal tract), fundamental frequency (vocal source), and duration (prosody) of speech are modeled simultaneously by HMMs. Speech waveforms are generated from HMMs themselves based on the maximum likelihood criterion.
Sinewave synthesis.
Sinewave synthesis is a technique for synthesizing speech by replacing the formants (main bands of energy) with pure tone whistles.
Challenges.
Text normalization challenges.
The process of normalizing text is rarely straightforward. Texts are full of heteronyms, numbers, and abbreviations that all require expansion into a phonetic representation. There are many spellings in English which are pronounced differently based on context. For example, "My latest project is to learn how to better project my voice" contains two pronunciations of "project".
Most text-to-speech (TTS) systems do not generate semantic representations of their input texts, as processes for doing so are unreliable, poorly understood, and computationally ineffective. As a result, various heuristic techniques are used to guess the proper way to disambiguate homographs, like examining neighboring words and using statistics about frequency of occurrence.
Recently TTS systems have begun to use HMMs (discussed above) to generate "parts of speech" to aid in disambiguating homographs. This technique is quite successful for many cases such as whether "read" should be pronounced as "red" implying past tense, or as "reed" implying present tense. Typical error rates when using HMMs in this fashion are usually below five percent. These techniques also work well for most European languages, although access to required training corpora is frequently difficult in these languages.
Deciding how to convert numbers is another problem that TTS systems have to address. It is a simple programming challenge to convert a number into words (at least in English), like "1325" becoming "one thousand three hundred twenty-five." However, numbers occur in many different contexts; "1325" may also be read as "one three two five", "thirteen twenty-five" or "thirteen hundred and twenty five". A TTS system can often infer how to expand a number based on surrounding words, numbers, and punctuation, and sometimes the system provides a way to specify the context if it is ambiguous. Roman numerals can also be read differently depending on context. For example "Henry VIII" reads as "Henry the Eighth", while "Chapter VIII" reads as "Chapter Eight".
Similarly, abbreviations can be ambiguous. For example, the abbreviation "in" for "inches" must be differentiated from the word "in", and the address "12 St John St." uses the same abbreviation for both "Saint" and "Street". TTS systems with intelligent front ends can make educated guesses about ambiguous abbreviations, while others provide the same result in all cases, resulting in nonsensical (and sometimes comical) outputs, such as "co-operation" being rendered as "company operation".
Text-to-phoneme challenges.
Speech synthesis systems use two basic approaches to determine the pronunciation of a word based on its spelling, a process which is often called text-to-phoneme or grapheme-to-phoneme conversion (phoneme is the term used by linguists to describe distinctive sounds in a language). The simplest approach to text-to-phoneme conversion is the dictionary-based approach, where a large dictionary containing all the words of a language and their correct pronunciations is stored by the program. Determining the correct pronunciation of each word is a matter of looking up each word in the dictionary and replacing the spelling with the pronunciation specified in the dictionary. The other approach is rule-based, in which pronunciation rules are applied to words to determine their pronunciations based on their spellings. This is similar to the "sounding out", or synthetic phonics, approach to learning reading.
Each approach has advantages and drawbacks. The dictionary-based approach is quick and accurate, but completely fails if it is given a word which is not in its dictionary. As dictionary size grows, so too does the memory space requirements of the synthesis system. On the other hand, the rule-based approach works on any input, but the complexity of the rules grows substantially as the system takes into account irregular spellings or pronunciations. (Consider that the word "of" is very common in English, yet is the only word in which the letter "f" is pronounced [v].) As a result, nearly all speech synthesis systems use a combination of these approaches.
Languages with a phonemic orthography have a very regular writing system, and the prediction of the pronunciation of words based on their spellings is quite successful. Speech synthesis systems for such languages often use the rule-based method extensively, resorting to dictionaries only for those few words, like foreign names and borrowings, whose pronunciations are not obvious from their spellings. On the other hand, speech synthesis systems for languages like English, which have extremely irregular spelling systems, are more likely to rely on dictionaries, and to use rule-based methods only for unusual words, or words that aren't in their dictionaries.
Evaluation challenges.
The consistent evaluation of speech synthesis systems may be difficult because of a lack of universally agreed objective evaluation criteria. Different organizations often use different speech data. The quality of speech synthesis systems also depends to a large degree on the quality of the production technique (which may involve analogue or digital recording) and on the facilities used to replay the speech. Evaluating speech synthesis systems has therefore often been compromised by differences between production techniques and replay facilities.
Since 2005, however, some researchers have started to evaluate speech synthesis systems using a common speech dataset.
Prosodics and emotional content.
A study in the journal "Speech Communication" by Amy Drahota and colleagues at the University of Portsmouth, UK, reported that listeners to voice recordings could determine, at better than chance levels, whether or not the speaker was smiling. It was suggested that identification of the vocal features that signal emotional content may be used to help make synthesized speech sound more natural. One of the related issues is modification of the pitch contour of the sentence, depending upon whether it is an affirmative, interrogative or exclamatory sentence. One of the techniques for pitch modification uses discrete cosine transform in the source domain (linear prediction residual). Such pitch synchronous pitch modification techniques need a priori pitch marking of the synthesis speech database using techniques such as epoch extraction using dynamic plosion index applied on the integrated linear prediction residual of the voiced regions of speech.
Dedicated hardware.
Early Technology (not available anymore)
Current (as of 2013)
Mattel.
The Mattel Intellivision game console, which is a computer that lacks a keyboard, offered the Intellivoice Voice Synthesis module in 1982. It included the SP0256 Narrator speech synthesizer chip on a removable cartridge. The Narrator had 2kB of Read-Only Memory (ROM), and this was utilized to store a database of generic words that could be combined to make phrases in Intellivision games. Since the Orator chip could also accept speech data from external memory, any additional words or phrases needed could be stored inside the cartridge itself. The data consisted of strings of analog-filter coefficients to modify the behavior of the chip's synthetic vocal-tract model, rather than simple digitized samples.
SAM.
Also released in 1982, Software Automatic Mouth was the first commercial all-software voice synthesis program. It was later used as the basis for Macintalk. The program was available for non-Macintosh Apple computers (including the Apple II, and the Lisa), various Atari models and the Commodore 64. The Apple version preferred additional hardware that contained DACs, although it could instead use the computer's one-bit audio output (with the addition of much distortion) if the card was not present. The Atari made use of the embedded POKEY audio chip. Speech playback on the Atari normally disabled interrupt requests and shut down the ANTIC chip during vocal output. The audible output is extremely distorted speech when the screen is on. The Commodore 64 made use of the 64's embedded SID audio chip.
Atari.
Arguably, the first speech system integrated into an operating system was the 1400XL/1450XL personal computers designed by Atari, Inc. using the Votrax SC01 chip in 1983. The 1400XL/1450XL computers used a Finite State Machine to enable World English Spelling text-to-speech synthesis. Unfortunately, the 1400XL/1450XL personal computers never shipped in quantity.
The Atari ST computers were sold with "stspeech.tos" on floppy disk.
Apple.
The first speech system integrated into an operating system that shipped in quantity was Apple Computer's MacInTalk. The software was licensed from 3rd party developers Joseph Katz and Mark Barton (later, SoftVoice, Inc.) and an early version was featured during the 1984 introduction of the Macintosh computer. This January demo, which used speech synthesis based on the Software Automatic Mouth, or SAM software, required 512 kilobytes of RAM memory. As a result, it could not run in the 128 kilobytes of RAM the first Mac actually shipped with. So, the demo was accomplished with a prototype 512k Mac, although those in attendance were not told of this and the synthesis demo created considerable excitement for the Macintosh. In the early 1990s Apple expanded its capabilities offering system wide text-to-speech support. With the introduction of faster PowerPC-based computers they included higher quality voice sampling. Apple also introduced speech recognition into its systems which provided a fluid command set. More recently, Apple has added sample-based voices. Starting as a curiosity, the speech system of Apple Macintosh has evolved into a fully supported program, PlainTalk, for people with vision problems. VoiceOver was for the first time featured in Mac OS X Tiger (10.4). During 10.4 (Tiger) & first releases of 10.5 (Leopard) there was only one standard voice shipping with Mac OS X. Starting with 10.6 (Snow Leopard), the user can choose out of a wide range list of multiple voices. VoiceOver voices feature the taking of realistic-sounding breaths between sentences, as well as improved clarity at high read rates over PlainTalk. Mac OS X also includes say, a command-line based application that converts text to audible speech. The AppleScript Standard Additions includes a say verb that allows a script to use any of the installed voices and to control the pitch, speaking rate and modulation of the spoken text.
The Apple iOS operating system used on the iPhone, iPad and iPod Touch uses VoiceOver speech synthesis for accessibility. Some third party applications also provide speech synthesis to facilitate navigating, reading web pages or translating text.
AmigaOS.
The second operating system to feature advanced speech synthesis capabilities was AmigaOS, introduced in 1985. The voice synthesis was licensed by Commodore International from SoftVoice, Inc., who also developed the original MacinTalk text-to-speech system. It featured a complete system of voice emulation for American English, with both male and female voices and "stress" indicator markers, made possible through the Amiga's audio chipset. The synthesis system was divided into a narrator device, which was responsible for modulating and concatenating phonemes, and a translator library which translated English text to phonemes via a set of rules. AmigaOS also featured a high-level "Speak Handler", which allowed command-line users to redirect text output to speech. Speech synthesis was occasionally used in third-party programs, particularly word processors and educational software. The synthesis software remained largely unchanged from the first AmigaOS release and Commodore eventually removed speech synthesis support from AmigaOS 2.1 onward.
Despite the American English phoneme limitation, an unofficial version with multilingual speech synthesis was developed. This made use of an enhanced version of the translator library which could translate a number of languages, given a set of rules for each language.
Microsoft Windows.
Modern Windows desktop systems can use SAPI 4 and SAPI 5 components to support speech synthesis and speech recognition. SAPI 4.0 was available as an optional add-on for Windows 95 and Windows 98. Windows 2000 added Narrator, a text–to–speech utility for people who have visual impairment. Third-party programs such as CoolSpeech, Textaloud and Ultra Hal can perform various text-to-speech tasks such as reading text aloud from a specified website, email account, text document, the Windows clipboard, the user's keyboard typing, etc. Not all programs can use speech synthesis directly. Some programs can use plug-ins, extensions or add-ons to read text aloud. Third-party programs are available that can read text from the system clipboard.
Microsoft Speech Server is a server-based package for voice synthesis and recognition. It is designed for network use with web applications and call centers.
Text-to-Speech (TTS) refers to the ability of computers to read text aloud. A TTS Engine converts written text to a phonemic representation, then converts the phonemic representation to waveforms that can be output as sound. TTS engines with different languages, dialects and specialized vocabularies are available through third-party publishers.
Android.
Version 1.6 of Android added support for speech synthesis (TTS).
Internet.
Currently, there are a number of applications, plugins and gadgets that can read messages directly from an e-mail client and web pages from a web browser or Google Toolbar such as Text-to-voice which is an add-on to Firefox. Some specialized software can narrate RSS-feeds. On one hand, online RSS-narrators simplify information delivery by allowing users to listen to their favourite news sources and to convert them to podcasts. On the other hand, on-line RSS-readers are available on almost any PC connected to the Internet. Users can download generated audio files to portable devices, e.g. with a help of podcast receiver, and listen to them while walking, jogging or commuting to work.
A growing field in Internet based TTS is web-based assistive technology, e.g. 'Browsealoud' from a UK company and Readspeaker. It can deliver TTS functionality to anyone (for reasons of accessibility, convenience, entertainment or information) with access to a web browser. The non-profit project was created in 2006 to provide a similar web-based TTS interface to the Wikipedia.
Other work is being done in the context of the W3C through the with the involvement of The BBC and Google Inc.
Speech synthesis markup languages.
A number of markup languages have been established for the rendition of text as speech in an XML-compliant format. The most recent is Speech Synthesis Markup Language (SSML), which became a W3C recommendation in 2004. Older speech synthesis markup languages include Java Speech Markup Language (JSML) and SABLE. Although each of these was proposed as a standard, none of them have been widely adopted.
Speech synthesis markup languages are distinguished from dialogue markup languages. VoiceXML, for example, includes tags related to speech recognition, dialogue management and touchtone dialing, in addition to text-to-speech markup.
Applications.
Speech synthesis has long been a vital assistive technology tool and its application in this area is significant and widespread. It allows environmental barriers to be removed for people with a wide range of disabilities. The longest application has been in the use of screen readers for people with visual impairment, but text-to-speech systems are now commonly used by people with dyslexia and other reading difficulties as well as by pre-literate children. They are also frequently employed to aid those with severe speech impairment usually through a dedicated voice output communication aid.
Speech synthesis techniques are also used in entertainment productions such as games and animations. In 2007, Animo Limited announced the development of a software application package based on its speech synthesis software FineSpeech, explicitly geared towards customers in the entertainment industries, able to generate narration and lines of dialogue according to user specifications. The application reached maturity in 2008, when NEC Biglobe announced a web service that allows users to create phrases from the voices of characters.
In recent years, Text to Speech for disability and handicapped communication aids have become widely deployed in Mass Transit. Text to Speech is also finding new applications outside the disability market. For example, speech synthesis, combined with speech recognition, allows for interaction with mobile devices via natural language processing interfaces.
Text-to speech is also used in second language acquisition. Voki, for instance, is an educational tool created by Oddcast that allows users to create their own talking avatar, using different accents. They can be emailed, embedded on websites or shared on social media.
APIs.
Multiple companies offer TTS APIs to their customers to accelerate development of new applications utilizing TTS technology. Companies offering TTS APIs include AT&T, IVONA, Neospeech, Readspeaker and YAKiToMe!. For mobile app development, Android operating system has been offering text to speech API for a long time. Most recently, with iOS7, Apple started offering an API for text to speech.

</doc>
<doc id="42800" url="http://en.wikipedia.org/wiki?curid=42800" title="374">
374

Year 374 (CCCLXXIV) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Augustus and Equitius (or, less frequently, year 1127 "Ab urbe condita"). The denomination 374 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42801" url="http://en.wikipedia.org/wiki?curid=42801" title="Nutella">
Nutella

Nutella (; ]), is the brand name of an Italian sweetened hazelnut chocolate spread. Manufactured by the Italian company Ferrero, it was introduced to the market in 1964.
History.
Pietro Ferrero, who owned a bakery in Alba, Piedmont, an area known for the production of hazelnuts, sold an initial batch of 300 kg of ""Pasta Gianduja" in 1946. This was originally a solid block, but Ferrero started to sell a creamy version in 1951 as "Supercrema"".
In 1963, Ferrero's son Michele Ferrero revamped "Supercrema" with the intention of marketing it throughout Europe. Its composition was modified and it was renamed "Nutella". The first jar of Nutella left the Ferrero factory in Alba on 20 April 1964. The product was an instant success and remains widely popular.
In 2012, French senator Yves Daudigny proposed a tax increase on palm oil from €100 to €400 per metric tonne. At 20 percent, palm oil is one of Nutella's main ingredients and the tax was dubbed "the Nutella tax" in the media.
World Nutella Day is February 5.
On 14 May 2014, Poste italiane issued a 50th anniversary Nutella commemorative stamp. The 70 Euro cent stamp was designed by Istituto Poligrafico e Zecca dello Stato and features a jar of Nutella on a golden background. Ferrero held a Nutella Day on 17 and 18 May to celebrate the anniversary.
Ingredients.
The main ingredients of Nutella are sugar and palm oil, followed by hazelnut, cocoa solids, and skimmed milk. In the United States, Nutella contains soy products. Nutella is marketed as "hazelnut cream" in many countries. Under Italian law, it cannot be labeled as a "chocolate cream," as it does not meet minimum cocoa solids concentration criteria. Ferrero consumes 25 percent of the global supply of hazelnuts.
An older recipe, Gianduja, was a mixture containing approximately 71.5 percent hazelnut paste and 19.5 percent chocolate. It was developed in Piedmont, Italy, after taxes on cocoa beans hindered the manufacture and distribution of conventional chocolate.
Production.
Nutella is produced in various facilities. In the North American market, it is produced at a plant in Brantford, Ontario in Canada.
For Australia and New Zealand, Nutella has been manufactured in Lithgow, New South Wales since the late 1970s.
Two of the four Ferrero plants in Italy produce Nutella, in Alba, Piedmont, and in Sant'Angelo dei Lombardi in Campania. In France, a production facility is located in Villers-Écalles. For Eastern Europe (including Southeast Europe, Poland, Turkey, Czech Republic and Slovakia) and South Africa it is produced in Warsaw and Manisa. For Germany and northern Europe Nutella is produced at the Ferrero plant in Stadtallendorf, which has been in existence since 1956.
Processing.
Nutella is a form of a chocolate spread. Therefore, the production process for this food item is very similar to a generic production of chocolate spread. Nutella is made from sugar, modified palm oil, hazelnuts, cocoa, skimmed milk powder, whey powder, lecithin, and vanillin.
The process of making chocolate spread begins with the extraction of cocoa powder from the cocoa bean. These cocoa beans are harvested from cocoa trees and are left to dry for about ten days before being shipped for processing. Typically cocoa beans contain approximately 50 percent of cocoa butter; therefore, they must be roasted to reduce the cocoa bean into a liquid form. This step is not sufficient for turning cocoa bean into chocolate paste because it solidifies at room temperature, and would not be spreadable. After the initial roast, the liquid paste is sent to presses, which are used to squeeze the butter out of the cocoa bean. The final products are round discs of chocolate made of pure compressed cocoa. The cocoa butter is transferred elsewhere so it can be used in other products.
The second process involves the hazelnuts. Once the hazelnuts have arrived at the processing plant, a quality control is issued to inspect the nuts so they are suitable for processing. A guillotine is used to chop the nuts to inspect the interior. After this process the hazelnuts are cleaned and roasted. A second quality control is issued by a computer-controlled blast of air, which removes the bad nuts from the batch. This ensures that each jar of Nutella is uniform in its look and taste. Approximately 50 hazelnuts can be found in each jar of Nutella, as claimed by the company.
The cocoa powder is then mixed with the hazelnuts along with sugar, vanillin and skim milk in a large tank until it becomes a paste-like spread. Modified palm oil is then added to help retain the solid phase of the Nutella at room temperature, which substitutes for the butter found in the cocoa bean. In addition, whey powder is added to the mix because it acts as a binder for the paste. Whey powder is an additive commonly used in spreads to prevent the coagulation of the product because it stabilizes the fat emulsions. Similarly to whey powder lecithin, which is a form of fatty substance found in animal and plant tissues, is used to emulsify as it promotes homogenized mixing of the different ingredients allowing the paste to become spreadable. It also aids the lipophilic properties of the cocoa powder which, again, keeps the product from separating. Vanillin is added to enhance the sweetness of the chocolate. The finished product is then packaged.
Nutrition.
Nutella contains 70 percent saturated fat and processed sugar by weight. A two-tablespoon (37 gram) serving of Nutella contains 200 calories including 99 calories from 11 grams of fat (3.5g of which are saturated) and 80 calories from 21 grams of sugar. The spread also contains 15 mg of sodium and 2g of protein per serving.
Class action lawsuit.
In the United States, Ferrero was sued in a class action for false advertising leading to consumer inferences that Nutella has nutritional and health benefits from advertising claims that Nutella is 'part of a nutritious breakfast'. Ferrero agreed to pay $3 million (up to $4 per jar for up to five jars in returns by customers) in an April 2012 settlement. The settlement also required Ferrero to make changes to Nutella's labeling and marketing, including television commercials and their website.

</doc>
<doc id="42802" url="http://en.wikipedia.org/wiki?curid=42802" title="Sophie B. Hawkins">
Sophie B. Hawkins

Sophie Ballantine Hawkins (born 1964 or 1965) is an American singer, songwriter, musician and painter. Her highest-charting singles are "Damn I Wish I Was Your Lover," "Right Beside You," and "As I Lay Me Down."
Career.
Hawkins's debut album, "Tongues and Tails", was released in 1992. It achieved both worldwide commercial success and critical acclaim, earning her a Grammy nomination for Best New Artist in 1993. The single "Damn I Wish I Was Your Lover" went to #5 on the "Billboard" Hot 100 singles chart in the USA and was also a Top 20 hit in the UK. As a result of this success, Hawkins was asked to perform "I Want You" during "Bobfest" a concert held on October 16, 1992 in Madison Square Garden honoring Bob Dylan's 30th Anniversary as a musician (later released as "The 30th Anniversary Concert Celebration").
"Whaler", her second album, was released in 1994. Produced by Stephen Lipson, it also contained a US top 10 hit, "As I Lay Me Down", and was certified gold. Three singles from the album made the UK Top 40, including "Right Beside You" which peaked at #13 (and reached number 2 on the Dutch Top 40 singles chart.).
A 1998 documentary by Gigi Gaston, titled "The Cream Will Rise", followed Hawkins during one of her tours and captured her struggle to deal with past troubles with her family, including her mother and brother. Music and riffs by Hawkins were included throughout the film.
Also in 1998, Hawkins's record company at the time, Sony Music, delayed the release of her third album. Its executives were unhappy with the finished product and wanted Hawkins to rework some of the material. In particular, they insisted that Hawkins remove a banjo track from one of the songs. Hawkins refused to accommodate them, citing artistic integrity as her main reason. After a lengthy battle between Hawkins and the company, the album, "Timbre", was eventually released in 1999, though Sony declined to promote it. Hawkins subsequently left the label and founded her own label, Trumpet Swan Productions. In 2001, "Timbre" was re-released on Hawkins's label, now as a 2-disc set that contained new songs, demos, remixes, and videos. Her first independently recorded and released album, "Wilderness", was released in 2004.
In 2012, Hawkins starred as Janis Joplin in the play, "Room 105" which was written and directed by her longtime girlfriend and manager, Gigi Gaston. After another long hiatus she released her fifth album of all new material in 2012, titled "The Crossing".
On April 4, 2013, Sophie appeared on the TV series "Community" as herself, performing "Damn I Wish I Was Your Lover" and "As I Lay Me Down".
Personal life.
Amid rumors that Hawkins had dated Martina Navratilova and Jodie Foster, she said, "I've never met any of the women I'm supposed to have had affairs with."<ref name=sophie/ref>Sutcliffe, Phil (January 1995). "Sophie B Hawkins interview." "Q Magazine," issue 100.</ref> She identifies as omnisexual.
In 1994, Hawkins posed nude for "Interview Magazine." As she explained it to Ed Rampell when he interviewed her for "Q Magazine," she met the photographer Bruce Weber and was asked if she could do a photo shoot with him. She had her own clothes when she showed up to the photo session, but he had a dress he wanted her to try on. She did not think it looked very good on her. It got to the point where she was only wearing a coat and Weber suggested she remove that too. By this stage, she was not even thinking about how she looked as she felt quite comfortable with him. Weber later told her that giving her the unflattering dress was part of his plan to get her naked.
On November 18, 2008, she gave birth to a son, Dashiell Gaston Hawkins. He was named in part for Hawkins's longtime partner and manager, Gigi Gaston, who had directed "The Cream Will Rise," the documentary about Hawkins mentioned above.
Politics.
In August 2007, Hawkins headlined the first Los Angeles Women's Music Festival in support of its dual agenda of supporting animal rescue groups and promoting and supporting female musicians. Hawkins is a vegan and a long-time supporter of animal rights.
In February 2008, Hawkins re-recorded her hit "Damn I Wish I Was Your Lover" as "Damn, We Wish You Were President" in support of presidential candidate Hillary Clinton. Hawkins also wrote in her blog, "Hillary Clinton's achievements come from her heart. She has initiated so much positive change for families, children, victims of crime and the environment in her struggle for the forward movement of America and the working people of this nation."
In May 2010, Hawkins began supporting Waterkeeper Alliance, an organization of on-the-water advocates who patrol and protect more than 100,000 miles of rivers, streams and coastlines in North and South America, Europe, Australia, Asia and Africa. She donated 100% of the proceeds of her single "The Land, the Sea, and the Sky" to the organization.
In February 2011, Hawkins performed at the Big Gay Party event staged by GOProud, an organization of gay conservatives, as part of the year's Conservative Political Action Conference festivities. In an after-show interview, Hawkins gave her views on issues such as gun ownership, the free market, limited government and identity politics.

</doc>
<doc id="42803" url="http://en.wikipedia.org/wiki?curid=42803" title="Video CD">
Video CD

Before the advent of DVD and Blu-ray, the Video CD (abbreviated as VCD, and also known as Compact Disc digital video) became the first format for distributing films on standard 120 mm optical discs. 
The format is a standard digital format for storing video on a compact disc. VCDs are playable in dedicated VCD players, most DVD and Blu-ray Disc players, personal computers, and some video game consoles.
The VCD standard was created in 1993
by Sony, Philips, Matsushita, and JVC and is referred to as the White Book standard.
Brief history.
In 1979, Philips introduced the optical LaserDisc, which was about 30 cm in diameter. This disc could hold an hour of analog video along with digital audio on each side. The Laserdisc provided picture quality nearly double that of VHS tape and audio quality far superior to VHS. By 1986 in the U.S. Laserdiscs were beginning to develop a small but loyal following from people who wanted better quality than VHS tape. However, Laserdiscs were always overshadowed by VHS because of their high price and lack of recording abilities.
Philips later teamed up with Sony to develop a new type of disc, the compact disc or CD. Introduced in 1982 in Japan (1983 in the U.S.), the CD is about 120 mm in diameter, and is single-sided. The format was initially designed to store digitized sound and proved to be a success in the music industry.
A few years later, Philips decided to give CDs the ability to produce video, just like its Laserdisc counterpart. This led to the creation of CD Video (CD-V) in 1987. However, the disc's small size significantly impeded the ability to store analog video; thus only 5 minutes of picture information could fit on the disc's surface (despite the fact that the audio was digital). Therefore CD-V distribution was limited to featuring music videos.
By the early 1990s engineers were able to digitize and compress video signals, greatly improving storage efficiency. Because this new format could hold 83 minutes of audio and video, releasing movies on compact discs finally became a reality. Extra capacity was obtained by sacrificing the error correction (it was believed that minor errors in the datastream would go unnoticed by the viewer). This format was named Video CD or VCD.
VCD enjoyed a brief period of success, with a few major feature films being released in the format (usually as a 2 disc set). However the introduction of the CD-R disc and associated recorders stopped the release of feature films in their tracks because the VCD format had no means of preventing unauthorized (and perfect) copies from being made. However, VCDs are still being released in several countries in Asia, but they recently had means of copy-protection.
The development of more sophisticated, higher capacity optical disc formats yielded the DVD format, released only a few years later with a copy protection mechanism. DVD players use lasers that are of shorter wavelength than those used on CDs, allowing the recorded pits to be smaller, so that more information can be stored. The DVD was so successful that it eventually pushed VHS out of the video market once suitable recorders became widely available. Nevertheless, VCDs made considerable inroads into developing nations, where they are still in use today.
Technical specifications.
Structure.
Video CDs comply with the CD-i Bridge format, and are authored using tracks in CD-ROM XA mode. The first track of a VCD is in CD-ROM XA Mode 2 Form 1, and stores metadata and menu information inside a ISO 9660 filesystem. This track may also contain other non-essential files, and is shown by operating systems when loading the disc. This track can be absent from a VCD, which would still work but would not allow it to be properly displayed in computers.
The rest of the tracks are usually in CD-ROM XA Mode 2 Form 2 and contain video and audio multiplexed in an MPEG program stream (MPEG-PS) container, but CD audio tracks are also allowed. Using Mode 2 Form 2 allows roughly 800 megabytes of VCD data to be stored on one 80 minute CD (versus 700 megabytes when using CD-ROM Mode 1). This is achieved by sacrificing the error correction redundancy present in Mode 1. It was considered that small errors in the video and audio stream pass largely unnoticed. This, combined with the net bitrate of VCD video and audio, means that almost exactly 80 minutes of VCD content can be stored on an 80 minute CD, 74 minutes of VCD content on a 74 minute CD, and so on. This was done in part to ensure compatibility with existing CD drive technology, specifically the earliest "1x" speed CD drives.
Video.
Video specifications
Although many DVD video players support playback of VCDs, VCD video is only compatible with the DVD-Video standard if encoded at 29.97 frames per second or 25 frames per second. 
The 352x240 and 352x288 (or SIF) resolutions were chosen because it is half the horizontal and vertical resolution of NTSC video, and half the horizontal resolution of PAL (the vertical resolution of PAL already being half of the 576 active lines). This is approximately half the resolution of an analog VHS tape which is ~330 horizontal and 480 vertical (NTSC) or 330x576 (PAL).
Audio.
Audio specifications
As with most CD-based formats, VCD audio is incompatible with the DVD-Video standard due to a difference in sampling frequency; DVDs require 48 kHz, whereas VCDs use 44.1 kHz.
Advantages of compression.
By compressing both the video and audio streams, a VCD is able to hold 74 minutes of picture and sound information, nearly the same duration as a standard 74 minute audio CD. The MPEG-1 compression used records mostly the differences between successive video frames, rather than write out each frame individually. Similarly, the audio frequency range is limited to those sounds most clearly heard by the human ear.
Other features.
The VCD standard also features the option of DVD-quality still images/slide shows with audio, at resolutions of 704x480 (NTSC) or 704x576 (PAL/SECAM). Version 2.0 also adds the playback control (PBC), featuring a simple menu like DVD-video.
Internal Control.
An example of the software control chart (taken from "Flower And Snake" disc 1 of 3) including menu commands found in the configuration volume as "CDI_VCD.CFG"
Similar formats.
CD-i Digital Video.
Shortly before the advent of White Book VCD, Philips started releasing movies in the Green Book CD-i format, calling the subformat CD-i Digital Video (CD-i DV). While these used a similar format (MPEG-1), due to minor differences between the standards these discs are not compatible with VCD players. Philips' CD-i players with the Full Motion Video MPEG-1 decoder cartridge would play both formats. Only a few CD-i DV titles were released before the company switched to the current VCD format for publishing movies.
XVCD.
XVCD (eXtended Video CD) is the name generally given to any format that stores MPEG-1 video on a compact disc in CD-ROM XA Mode 2 Form 2, but does not strictly follow the VCD standard in terms of the encoding of the video or audio.
A normal VCD is encoded to MPEG-1 at a constant bit rate (CBR), so all scenes are required to use exactly the same data rate, regardless of complexity. However, video on an XVCD is typically encoded at a variable bit rate (VBR), so complex scenes can use a much higher data rate for a short time, while simpler scenes will use lower data rates. Some XVCDs use lower bitrates in order to fit longer videos onto the disc, while others use higher bitrates to improve quality. MPEG-2 may be used instead of MPEG-1.
To further reduce the data rate without significantly reducing quality, the size of the GOP can be increased, a different MPEG-1 quantization matrix can be used, the maximum data rate can be exceeded, and the bit rate of the MP2 audio can be reduced (or even the use of MP3 audio instead of MP2 audio). These changes can be advantageous for those who want to either maximize video quality, or use fewer discs.
KVCD.
KVCD (K Video Compression Dynamics) is an XVCD variant that requires the use of a proprietary quantization matrix, available for non-commercial use. KVCD is notable because the specification recommends a non-standard resolution of 528x480 or 528x576. KVCDs encoded at this resolution are only playable by computers with CD-ROM drives, and a small number of DVD players.
DVCD.
DVCD or Double VCD is a method to accommodate longer videos on a CD. A non-standard CD is overburned to include up to 100 minutes of video. However, some CD-ROM drives and players have problems reading these CDs, mostly because the groove spacing is outside specifications and the player's laser servo is unable to track it.
DVI.
DVI (Digital Video Interactive) is a compression technique that stored 72 minutes of full-screen video on a CD-ROM. Acquired by Intel in 1988 from RCA's Sarnoff Research Labs, DVI never caught on.
SVCD.
Super Video CD is a format intended to be the successor of VCD, offering better quality of image and sound.
Adoption.
In North America.
The advent of recordable CDs, inexpensive recorders, and compatible DVD players spurred VCD acceptance in the US in the late 1990s and early 2000s. However, DVD burners and DVD-Video recorders were available by that time, and equipment and media costs for making DVD-Video fell rapidly. DVD-Video, with its longer run time and much higher quality, quickly overshadowed VCD in areas that could afford it. In addition many early DVD players could not read recordable (CD-R) media, and this limited the compatibility of home-made VCDs. Almost every modern stand-alone DVD-Video player can play VCDs burned on recordable media.
In Asia.
It is reported the first home VCD player was made in Hefei, China in 1993.The VCD format was very popular throughout Asia
(except Japan and South Korea) in the late 1990s through the 2000s, with 8 million VCD players sold in China in 1997 alone,
and more than half of all Chinese households owning at least one VCD player by 2005. However, popularity has declined over the years, as the number of Hong Kong factories that produced VCDs dropped from 98 in 1999 to 26 in 2012.
This popularity is, in part, because most households did not already own VHS players when VCDs were introduced, the low price of the players, their tolerance of high humidity (a notable problem for VCRs), easy storage and maintenance, and the lower-cost media. Western sources have cited pirated content as a principal incentive for VCD player ownership.
VCDs are often produced and sold in Asian countries and regions, such as Mainland China, Taiwan, Hong Kong, Singapore, Malaysia, Thailand, Burma, Indonesia, Philippines, Vietnam, India, Pakistan and Bangladesh. In many Asian countries, major Hollywood studios (and Asian home video distributors) have licensed companies to officially produce and distribute the VCDs, such as MCA Home Video in Pakistan, Intercontinental Video Ltd. of Hong Kong, Sunny Video in Malaysia, Vision in Indonesia, CVD International and Pacific Marketing and Entertainment Group in Thailand, Excel Home Video in India, Berjaya-HVN and InnoForm Media in both Malaysia and Singapore, Scorpio East Entertainment in Singapore, as well as VIVA Video, Magnavision Home Video, and C-Interactive Digital Entertainment in the Philippines. Legal Video CDs can often be found in established video stores and major book outlets in most Asian countries. They are typically packaged in jewel cases like commercial CDs, though higher-profile films may be released in keep cases. The consumer should always check for the VCD or DVD logo so as to avoid purchasing the wrong format.
In Asia, the use of VCDs as carriers for karaoke music is very common. One channel would feature a mono track with music and singing, another channel a pure instrumental version for karaoke singing. Prior to this, karaoke music was carried on laserdiscs.
Worldwide trends.
VCD's growth has slowed in areas that can afford DVD-Video, which offers most of the same advantages, as well as better picture quality
(higher resolution with fewer digital compression artifacts) due to its larger storage capacity. However, VCD has simultaneously seen significant new growth in emerging economies like India, Indonesia, South America and Africa as a low-cost alternative to DVD. As of 2004, the worldwide popularity of VCD was increasing.
Compared with VHS.
Overall picture quality is intended to be comparable to VHS video.
Poorly compressed VCD video can sometimes be of lower quality than VHS video, for example exhibiting VCD block artifacts (rather than the analog noise seen in VHS sources), but does not deteriorate further with each use. While both formats need fast-forwarding to find certain scenes, rewinding to the beginning upon reaching the end is not required in VCD. The resolution is just slightly below that of common VHS resolution.
Though technically superior to tape-based mediums, VCDs have a few minor flaws. Videos in the format do not come with closed caption (on-screen text to aid viewers with hearing problems). When watching a film that exceeds 74 minutes, which is the maximum video capacity of one disc, a viewer would have to change the disc upon reaching half-way (unless the discs are played on a VCD changer that could hold multiple discs as well as playing them automatically in succession), whereas a single VHS can hold 3½ hours of continuous video (as of 2014, 10 hour VHS tapes are available).
Compared with DVD.
When playing a DVD, the viewer is brought to a main menu which gives them options (watch the feature film, view "deleted scenes", play some special applications, etc.). VCDs are usually straightforward, playing them often goes directly to the video with extras (mostly trailers and commercials) taking place before or after it.
Subtitles are found on many Asian VCDs but cannot be removed, unlike DVDs. The subtitles are embedded on the video during the encoding process. It's not uncommon to find a VCD with subtitles for two languages.
Though the VCD technology can support it, most films carried on VCDs do not contain chapters, requiring the viewer to fast-forward to resume the program after playback has been stopped. This is mostly because VCD technology is able to start playback at a chapter point but there is nothing to signal the player that the chapter has changed during a program. This can be confusing for the user as the player will indicate that it is still playing chapter 1 when it has played through to chapter 2 or later. Pressing the Next button would cause playback from the beginning of chapter 2. However, preview material is sometimes stored in a separate chapter, followed by a single chapter for the film.
VCDs are often bilingual. Because they feature stereo audio, disc players have an option to play only the left or right audio channel. For example, ERA of Hong Kong's release of the animated film "The Iron Giant" features English on the left audio channel and Cantonese on the right; more commonly Hong Kong VCDs will feature Mandarin on one channel and Cantonese on the other. This is similar to selecting a language track on a DVD, except it's limited to 2 languages, due to there being only two audio channels (left and right). The audio track effectively becomes monaural.
VCD's most noticeable disadvantage compared to DVD is image quality, due both to the more aggressive compression necessary to fit video into such a small capacity as well as the compression method used. Additionally, VCDs are available only in stereo, while DVDs are capable of six channels of discrete surround sound. The audio compression of VCDs also suffers from not being able to pull off the Haas effect for matrixed surround sound.
Hardware and software support.
Early devices supporting Video CD playback include the Philips CD-i systems and the Amiga CD-32 (albeit via an optional decoder card).
Video CDs are not popular in the US, Canada and Europe, so its support is limited among mainstream software. Windows Media Player prior to version 9 and QuickTime Player do not support playing VCD directly, though they can play the .DAT files (stored under \MPEGAV for video and audio data) reliably, and plugins were available. Windows Vista added native support of VCD along with DVD-Video and can launch the preferred application upon insertion. The disc format is also supported using Windows Media Player Classic variations and VLC Media Player both support VCDs natively.
Direct access playback support is available within Windows XP MCE, Windows Vista, Windows 7, BSD, Mac OS, Linux, and Darwin, among others, either directly or with updates and compatible software.
Disc playback is also available both natively and as an option on some CD- and DVD-based video game consoles, including PC-FX, Sega Saturn (pictured), Sega Dreamcast, and Sony PlayStation.
Most DVD players are compatible with VCDs, and VCD-only players are available throughout Asia, and online through many shopping sites. Older Blu-ray and HD-DVD players also retained support, as do CBHD players as well. However, most current Blu-ray players and the Sony PlayStation 3 cannot play VCDs; this is because while they have backwards playback compatibility with the DVD standard, the lasers in these player can not read VCD data because the player hardware does not have support for MPEG-1 video and audio.

</doc>
<doc id="42806" url="http://en.wikipedia.org/wiki?curid=42806" title="Cyclone">
Cyclone

In meteorology, a cyclone is an area of closed, circular fluid motion rotating in the same direction as the Earth. This is usually characterized by inward spiraling winds that rotate counterclockwise in the Northern Hemisphere and clockwise in the Southern Hemisphere of the Earth. Most large-scale cyclonic circulations are centered on areas of low atmospheric pressure. The largest low-pressure systems are cold-core polar cyclones and extratropical cyclones which lie on the synoptic scale. According to the National Hurricane Center glossary, warm-core cyclones such as tropical cyclones and subtropical cyclones also lie within the synoptic scale.
Mesocyclones, tornadoes and dust devils lie within the smaller mesoscale. Upper level cyclones can exist without the presence of a surface low, and can pinch off from the base of the Tropical Upper Tropospheric Trough during the summer months in the Northern Hemisphere. Cyclones have also been seen on extraterrestrial planets, such as Mars and Neptune.
Cyclogenesis describes the process of cyclone formation and intensification. Extratropical cyclones form as waves in large regions of enhanced mid-latitude temperature contrasts called baroclinic zones. These zones contract to form weather fronts as the cyclonic circulation closes and intensifies. Later in their life cycle, cyclones occlude as cold core systems. A cyclone's track is guided over the course of its 2 to 6 day life cycle by the steering flow of the cancer or subtropical jet stream.
Weather fronts separate two masses of air of different densities and are associated with the most prominent meteorological phenomena. Air masses separated by a front may differ in temperature or humidity. Strong cold fronts typically feature narrow bands of thunderstorms and severe weather, and may on occasion be preceded by squall lines or dry lines. They form west of the circulation center and generally move from west to east. Warm fronts form east of the cyclone center and are usually preceded by stratiform precipitation and fog. They move poleward ahead of the cyclone path. Occluded fronts form late in the cyclone life cycle near the center of the cyclone and often wrap around the storm center.
Tropical cyclogenesis describes the process of development of tropical cyclones. Tropical cyclones form due to latent heat driven by significant thunderstorm activity, and are warm core. Cyclones can transition between extratropical, subtropical, and tropical phases under the right conditions. Mesocyclones form as warm core cyclones over land, and can lead to tornado formation. Waterspouts can also form from mesocyclones, but more often develop from environments of high instability and low vertical wind shear. In the Atlantic and the northeastern Pacific oceans, a tropical cyclone is generally referred to as a hurricane (from the name of the ancient Central American deity of wind, Huracan), in the Indian and south Pacific oceans it is called a cyclone, and in the northwestern Pacific it is called a typhoon.
Structure.
There are a number of structural characteristics common to all cyclones. A cyclone is a low-pressure area. A cyclone's center (often known in a mature tropical cyclone as the eye), is the area of lowest atmospheric pressure in the region. Near the center, the pressure gradient force (from the pressure in the center of the cyclone compared to the pressure outside the cyclone) and the force from the Coriolis effect must be in an approximate balance, or the cyclone would collapse on itself as a result of the difference in pressure.
Because of the Coriolis effect, the wind flow around a large cyclone is counterclockwise in the Northern Hemisphere and clockwise in the Southern Hemisphere. Cyclonic circulation is sometimes referred to as contra solem. In the Northern Hemisphere, the fastest winds relative to the surface of the Earth therefore occur on the eastern side of a northward-moving cyclone and on the northern side of a westward-moving one; the opposite occurs in the Southern Hemisphere. (The wind flow around an anticyclone, on the other hand, is clockwise in the northern hemisphere, and counterclockwise in the southern hemisphere.)
Formation.
Cyclogenesis is the development or strengthening of cyclonic circulation in the atmosphere (a low-pressure area). Cyclogenesis is an umbrella term for several different processes, all of which result in the development of some sort of cyclone. It can occur at various scales, from the microscale to the synoptic scale.
Extratropical cyclones form as waves along weather fronts before occluding later in their life cycle as cold core cyclones.
Tropical cyclones form due to latent heat driven by significant thunderstorm activity, and are warm core.
 Mesocyclones form as warm core cyclones over land, and can lead to tornado formation. Waterspouts can also form from mesocyclones, but more often develop from environments of high instability and low vertical wind shear. Cyclogenesis is the opposite of cyclolysis, and has an anticyclonic (high-pressure system) equivalent which deals with the formation of high-pressure areas—Anticyclogenesis.
The surface low has a variety of ways of forming. Topography can force a surface low when dense low-level high-pressure system ridges in east of a north-south mountain barrier. Mesoscale convective systems can spawn surface lows which are initially warm core. The disturbance can grow into a wave-like formation along the front and the low will be positioned at the crest. Around the low, flow will become cyclonic, by definition. This rotational flow will push polar air equatorward west of the low via its trailing cold front, and warmer air with push poleward low via the warm front. Usually the cold front will move at a quicker pace than the warm front and “catch up” with it due to the slow erosion of higher density airmass located out ahead of the cyclone and the higher density airmass sweeping in behind the cyclone, usually resulting in a narrowing warm sector. At this point an occluded front forms where the warm air mass is pushed upwards into a trough of warm air aloft, which is also known as a trowal.
Tropical cyclogenesis is the technical term describing the development and strengthening of a tropical cyclone in the atmosphere. The mechanisms through which tropical cyclogenesis occurs are distinctly different from those through which mid-latitude cyclogenesis occurs. Tropical cyclogenesis involves the development of a warm-core cyclone, due to significant convection in a favorable atmospheric environment. There are six main requirements for tropical cyclogenesis: sufficiently warm sea surface temperatures, atmospheric instability, high humidity in the lower to middle levels of the troposphere, enough Coriolis force to develop a low-pressure center, a preexisting low-level focus or disturbance, and low vertical wind shear. An average of 86 tropical cyclones of tropical storm intensity form annually worldwide, with 47 reaching hurricane/typhoon strength, and 20 becoming intense tropical cyclones (at least Category 3 intensity on the Saffir–Simpson Hurricane Scale).
Synoptic scale.
The following types of cyclones are identifiable in synoptic charts.
Surface-based types.
There are three main types surface-based cyclones: Extratropical cyclones, Subtropical cyclones and Tropical cyclones
Extratropical cyclone.
An extratropical cyclone is a synoptic scale low-pressure weather system that does not have tropical characteristics, being connected with fronts and horizontal gradients in temperature and dew point otherwise known as "baroclinic zones".
The descriptor "extratropical" refers to the fact that this type of cyclone generally occurs outside of the tropics, in the middle latitudes of the planet. These systems may also be described as "mid-latitude cyclones" due to their area of formation, or "post-tropical cyclones" where extratropical transition has occurred, and are often described as "depressions" or "lows" by weather forecasters and the general public. These are the everyday phenomena which along with anti-cyclones, drive the weather over much of the Earth.
Although extratropical cyclones are almost always classified as baroclinic since they form along zones of temperature and dewpoint gradient within the westerlies, they can sometimes become barotropic late in their life cycle when the temperature distribution around the cyclone becomes fairly uniform with radius. An extratropical cyclone can transform into a subtropical storm, and from there into a tropical cyclone, if it dwells over warm waters and develops central convection, which warms its core. One intense type of extratropical cyclone that strikes during wintertime is a "nor'easter".
Polar low.
A polar low is a small-scale, short-lived atmospheric low-pressure system (depression) that is found over the ocean areas poleward of the main polar front in both the Northern and Southern Hemispheres. Polar lows are cold-core so they can be considered as a subset of extratropical cyclones. Polar lows were first identified on the meteorological satellite imagery that became available in the 1960s, which revealed many small-scale cloud vortices at high latitudes. The most active polar lows are found over certain ice-free maritime areas in or near the Arctic during the winter, such as the Norwegian Sea, Barents Sea, Labrador Sea and Gulf of Alaska. Polar lows dissipate rapidly when they make landfall. Antarctic systems tend to be weaker than their northern counterparts since the air-sea temperature differences around the continent are generally smaller. However, vigorous polar lows can be found over the Southern Ocean. During winter, when cold-core lows with temperatures in the mid-levels of the troposphere reach -45 C move over open waters, deep convection forms which allows polar low development to become possible. The systems usually have a horizontal length scale of less than 1000 km and exist for no more than a couple of days. They are part of the larger class of mesoscale weather systems. Polar lows can be difficult to detect using conventional weather reports and are a hazard to high-latitude operations, such as shipping and gas and oil platforms. Polar lows have been referred to by many other terms, such as polar mesoscale vortex, Arctic hurricane, Arctic low, and cold air depression. Today the term is usually reserved for the more vigorous systems that have near-surface winds of at least 17 m/s.
Subtropical.
A subtropical cyclone is a weather system that has some characteristics of a tropical cyclone and some characteristics of an extratropical cyclone. They can form between the equator and the 50th parallel. As early as the 1950s, meteorologists were unclear whether they should be characterized as tropical cyclones or extratropical cyclones, and used terms such as quasi-tropical and semi-tropical to describe the cyclone hybrids. By 1972, the National Hurricane Center officially recognized this cyclone category. Subtropical cyclones began to receive names off the official tropical cyclone list in the Atlantic Basin in 2002. They have broad wind patterns with maximum sustained winds located farther from the center than typical tropical cyclones, and exist in areas of weak to moderate temperature gradient.
Since they form from initially extratropical cyclones which have colder temperatures aloft than normally found in the tropics, the sea surface temperatures required for their formation are lower than the tropical cyclone threshold by three degrees Celsius, or five degrees Fahrenheit, lying around 23 degrees Celsius. This means that subtropical cyclones are more likely to form outside the traditional bounds of the hurricane season. Although subtropical storms rarely have hurricane-force winds, they may become tropical in nature as their cores warm.
Tropical.
A tropical cyclone is a storm system characterized by a low-pressure center and numerous thunderstorms that produce strong winds and flooding rain. A tropical cyclone feeds on heat released when moist air rises, resulting in condensation of water vapour contained in the moist air. They are fueled by a different heat mechanism than other cyclonic windstorms such as nor'easters, European windstorms, and polar lows, leading to their classification as "warm core" storm systems.
The term "tropical" refers to both the geographic origin of these systems, which form almost exclusively in tropical regions of the globe, and their formation in Maritime Tropical air masses. The term "cyclone" refers to such storms' cyclonic nature, with counterclockwise rotation in the Northern Hemisphere and clockwise rotation in the Southern Hemisphere. Depending on their location and strength, tropical cyclones are referred to by other names, such as hurricane, typhoon, tropical storm, cyclonic storm, tropical depression, or simply as a cyclone.
While tropical cyclones can produce extremely powerful winds and torrential rain, they are also able to produce high waves and damaging storm surge. They develop over large bodies of warm water, and lose their strength if they move over land. This is the reason coastal regions can receive significant damage from a tropical cyclone, while inland regions are relatively safe from receiving strong winds. Heavy rains, however, can produce significant flooding inland, and storm surges can produce extensive coastal flooding up to 40 km from the coastline. Although their effects on human populations can be devastating, tropical cyclones can also relieve drought conditions. They also carry heat and energy away from the tropics and transport it toward temperate latitudes, which makes them an important part of the global atmospheric circulation mechanism. As a result, tropical cyclones help to maintain equilibrium in the Earth's troposphere.
Many tropical cyclones develop when the atmospheric conditions around a weak disturbance in the atmosphere are favorable. Others form when other types of cyclones acquire tropical characteristics. Tropical systems are then moved by steering winds in the troposphere; if the conditions remain favorable, the tropical disturbance intensifies, and can even develop an eye. On the other end of the spectrum, if the conditions around the system deteriorate or the tropical cyclone makes landfall, the system weakens and eventually dissipates. A tropical cyclone can become extratropical as it moves toward higher latitudes if its energy source changes from heat released by condensation to differences in temperature between air masses; From an operational standpoint, a tropical cyclone is usually not considered to become subtropical during its extratropical transition.
Upper level types.
Polar cyclone.
A polar, sub-polar, or Arctic cyclone (also known as a polar vortex) is a vast area of low pressure which strengthens in the winter and weakens in the summer. A polar cyclone is a low-pressure weather system, usually spanning 1000 km to 2000 km, in which the air circulates in a counterclockwise direction in the northern hemisphere, and a clockwise direction in the southern hemisphere. In the Northern Hemisphere, the polar cyclone has two centers on average. One center lies near Baffin Island and the other over northeast Siberia. In the southern hemisphere, it tends to be located near the edge of the Ross ice shelf near 160 west longitude. When the polar vortex is strong, westerly flow descends to the Earth's surface. When the polar cyclone is weak, significant cold outbreaks occur.
TUTT cell.
Under specific circumstances, upper cold lows can break off from the base of the Tropical Upper Tropospheric Trough (TUTT), which is located mid-ocean in the Northern Hemisphere during the summer months. These upper tropospheric cyclonic vortices, also known as TUTT cells or TUTT lows, usually move slowly from east-northeast to west-southwest, and generally do not extend below 20,000 feet in altitude. A weak inverted surface trough within the trade wind is generally found underneath them, and they may also be associated with broad areas of high-level clouds. Downward development results in an increase of cumulus clouds and the appearance of a surface vortex. In rare cases, they become warm-core, resulting in the vortex becoming a tropical cyclone. Upper cyclones and upper troughs which trail tropical cyclones can cause additional outflow channels and aid in their intensification process. Developing tropical disturbances can help create or deepen upper troughs or upper lows in their wake due to the outflow jet emanating from the developing tropical disturbance/cyclone.
Mesoscale.
The following types of cyclones are not identifiable in synoptic charts.
Mesocyclone.
A mesocyclone is a vortex of air, 2.0 km to 10 km in diameter (the mesoscale of meteorology), within a convective storm.
Air rises and rotates around a vertical axis, usually in the same direction as low-pressure systems in both northern and southern hemisphere. They are most often cyclonic, that is, associated with a localized low-pressure region within a supercell. Such storms can feature strong surface winds and severe hail. Mesocyclones often occur together with updrafts in supercells, where tornadoes may form. About 1700 mesocyclones form annually across the United States, but only half produce tornadoes.
Tornado.
A tornado is a violently rotating column of air that is in contact with both the surface of the earth and a cumulonimbus cloud or, in rare cases, the base of a cumulus cloud. They are often referred to as twisters, which is the name more commonly used in America, or cyclones, although the word cyclone is used in meteorology, in a wider sense, to name any closed low-pressure circulation.
Dust devil.
A dust devil is a strong, well-formed, and relatively long-lived whirlwind, ranging from small (half a metre wide and a few metres tall) to large (more than 10 metres wide and more than 1000 metres tall). The primary vertical motion is upward. Dust devils are usually harmless, but can on rare occasions grow large enough to pose a threat to both people and property.
Waterspout.
A waterspout is a columnar vortex forming over water that is, in its most common form, a non-supercell tornado over water that is connected to a cumuliform cloud. While it is often weaker than most of its land counterparts, stronger versions spawned by mesocyclones do occur.
Steam devil.
A gentle vortex over calm water or wet land made visible by rising water vapour.
Other planets.
Cyclones are not unique to Earth. Cyclonic storms are common on Jovian planets, such as the Small Dark Spot on Neptune. It is about one third the diameter of the Great Dark Spot and received the nickname "Wizard's Eye" because it looks like an eye. This appearance is caused by a white cloud in the middle of the Wizard's Eye. Mars has also exhibited cyclonic storms. Jovian storms like the Great Red Spot are usually mistakenly named as giant hurricanes or cyclonic storms. However, this is inaccurate, as the Great Red Spot is, in fact, the inverse phenomenon, an anticyclone.

</doc>
<doc id="42808" url="http://en.wikipedia.org/wiki?curid=42808" title="Lignite">
Lignite

Lignite, often referred to as brown coal, is a soft brown combustible sedimentary rock that is formed from naturally compressed peat. It is considered the lowest rank of coal due to its relatively low heat content. It is mined in China, Bulgaria, Greece, Germany, Kosovo, Poland, Serbia, Russia, Turkey, the United States, Canada, India, Australia and many other parts of Europe and it is used almost exclusively as a fuel for steam-electric power generation, but is also mined for its germanium content in China. 26.3% of Germany's electricity comes from lignite power plants, while in Greece lignite provides about 50% of its power needs.
Characteristics.
Lignite is brownish-black in color and has a carbon content of around 25-35%, a high inherent moisture content sometimes as high as 66%, and an ash content ranging from 6% to 19% compared with 6% to 12% for bituminous coal.
The energy content of lignite ranges from 10 – 20 MJ/kg (9–17 million BTU per short ton) on a moist, mineral-matter-free basis. The energy content of lignite consumed in the United States averages 15 MJ/kg (13 million BTU/ton), on the as-received basis (i.e., containing both inherent moisture and mineral matter). The energy content of lignite consumed in Victoria, Australia averages 8.4 MJ/kg (6.5 million BTU/ton).
Lignite has a high content of volatile matter which makes it easier to convert into gas and liquid petroleum products than higher ranking coals. Unfortunately its high moisture content and susceptibility to spontaneous combustion can cause problems in transportation and storage. It is now known that efficient processes that remove latent moisture locked within the structure of brown coal will relegate the risk of spontaneous combustion to the same level as black coal, will transform the calorific value of brown coal to a black coal equivalent fuel while significantly reducing the emissions profile of 'densified' brown coal to a level similar to or better than most black coals.
Uses.
Because of its low energy density and typically high moisture content, brown coal is inefficient to transport and is not traded extensively on the world market compared with higher coal grades. It is often burned in power stations near the mines, such as in Australia's Latrobe Valley and Luminant's Monticello plant in Texas. Primarily because of latent high moisture content and low energy density of brown coal, carbon dioxide emissions from traditional brown-coal-fired plants are generally much higher "per megawatt generated" than for comparable black-coal plants, with the world's highest-emitting being Hazelwood Power Station, Victoria. The operation of traditional brown-coal plants, particularly in combination with strip mining, can be politically contentious due to environmental concerns.
Reaction with quaternary amine forms a product called amine-treated lignite (ATL), which is used in drilling mud to reduce fluid loss during drilling.
Geology.
Lignite begins as an accumulation of partially decayed plant material, or peat. Burial by other sediments results in increasing temperature, depending on the local geothermal gradient and tectonic setting, and increasing pressure. This causes compaction of the material and loss of some of the water and volatile matter (primarily methane and carbon dioxide). This process, called coalification, concentrates the carbon content, and thus the heat content, of the material. Deeper burial and the passage of time result in further expulsion of moisture and volatile matter, eventually transforming the material into higher rank coals such as bituminous and anthracite coal.
Lignite deposits are typically younger than higher rank coals, with the majority of them having formed during the Tertiary period.
Resources.
The Latrobe Valley in the state of Victoria, Australia contains estimated reserves of some 65 billion tonnes of brown coal. The deposit is equivalent to 25% of known world reserves. The coal seams are up to 100 metres thick, with multiple coal seams often giving virtually continuous brown coal thickness of up to 230 metres. Seams are covered by very little overburden (10 to 20 metres).
Types.
Lignite can be separated into two types. The first is xyloid lignite or fossil wood and the second form is the compact lignite or perfect lignite.
Although xyloid lignite may sometimes have the tenacity and the appearance of ordinary wood it can be seen that the combustible woody tissue has experienced a great modification. It is reducible to a fine powder by trituration and if submitted to the action of a weak solution of potash it yields a considerable quantity of Humic acid.
Dark black lignite, or jet, is where the term 'jet black' originates.

</doc>
<doc id="42809" url="http://en.wikipedia.org/wiki?curid=42809" title="423">
423

Year 423 (CDXXIII) was a common year starting on Monday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Marinianus and Asclepiodotus (or, less frequently, year 1176 "Ab urbe condita"). The denomination 423 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42810" url="http://en.wikipedia.org/wiki?curid=42810" title="424">
424

Year 424 (CDXXIV) was a leap year starting on Tuesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Castinus and Victor (or, less frequently, year 1177 "Ab urbe condita"). The denomination 424 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
China.
</onlyinclude>

</doc>
<doc id="42811" url="http://en.wikipedia.org/wiki?curid=42811" title="425">
425

Year 425 (CDXXV) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Theodosius and Valentinianus (or, less frequently, year 1178 "Ab urbe condita"). The denomination 425 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42812" url="http://en.wikipedia.org/wiki?curid=42812" title="428">
428

Year 428 (CDXXVIII) was a leap year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Felix and Taurus (or, less frequently, year 1181 "Ab urbe condita"). The denomination 428 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42813" url="http://en.wikipedia.org/wiki?curid=42813" title="202">
202

Year 202 (CCII) was a common year starting on Friday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Severus and Antoninus (or, less frequently, year 955 "Ab urbe condita"). The denomination 202 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42814" url="http://en.wikipedia.org/wiki?curid=42814" title="203">
203

Year 203 (CCIII) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Plautianus and Geta (or, less frequently, year 956 "Ab urbe condita"). The denomination 203 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42821" url="http://en.wikipedia.org/wiki?curid=42821" title="La Paz">
La Paz

Nuestra Señora de La Paz (]; English: Our Lady of Peace; Aymara: "Chuquiago Marka" or "Chuqiyapu"), commonly known as La Paz (; ]), is Bolivia's third most-populous city, the seat of the country's government and the capital of La Paz Department. It is located on the western side of Bolivia at an elevation of roughly 3650 m above sea level.
It is, "de facto", the world's highest administrative capital. While the official capital of Bolivia (and its seat of justice) is Sucre, La Paz has more government departments.
The city sits in a bowl surrounded by the high mountains of the altiplano. As it grew, the city of La Paz climbed the hills, resulting in varying elevations from 3200 to. Overlooking the city is towering triple-peaked Illimani, which is always snow-covered and can be seen from many parts of the city, including from the neighboring city of El Alto. As of the 2008 census, the city had a population of 877,363.
La Paz Metropolitan area, formed by the cities of La Paz, El Alto, and Viacha, make the most populous urban area of Bolivia, with a population of 2.3 million inhabitants and surpassing the metropolitan area of Santa Cruz de la Sierra.
History.
Founded in 1548 by the Spanish conquistadors at the site of the Native American settlement, Laja, the full name of the city was originally "Nuestra Señora de La Paz" (meaning "Our Lady of Peace"). The name commemorated the restoration of peace following the insurrection of Gonzalo Pizarro and fellow conquistadors four years earlier against Blasco Núñez Vela, the first viceroy of Peru. The city was later moved to its present location in the valley of Chuquiago Marka.
Control over the former Inca lands had been entrusted to Pedro de la Gasca by the Spanish king (and Holy Roman Emperor) Emperor Charles V. Gasca commanded Alonso de Mendoza to found a new city commemorating the end of the civil wars in Peru; the city of La Paz was founded on October 20, 1548.
In 1549, Juan Gutierrez Paniagua was commanded to design an urban plan that would designate sites for public areas, plazas, official buildings, and a cathedral. La Plaza de los Españoles, which is known today as the Plaza Murillo, was chosen as the location for government buildings as well as the Metropolitan Cathedral.
Spain controlled La Paz with a firm grip and the Spanish king had the last word in all matters political. In 1781, for a total of six months, a group of Aymara people laid siege to La Paz. Under the leadership of Tupac Katari, they destroyed churches and government property. Thirty years later Indians laid a two-month siege on La Paz – where and when the legend of the Ekeko is set. In 1809 the struggle for independence from the Spanish rule brought uprisings against the royalist forces. It was on July 16, 1809 that Pedro Domingo Murillo famously said that the Bolivian revolution was igniting a lamp that nobody would be able to turn-off. This formally marked the beginning of the Liberation of South America from Spain. In La Paz, simultaneously with the city of Sucre, was made the first revolution against the Spanish Crown the 16 July 1809. This event is known as the Primer Grito Libertario de América.
Pedro Domingo Murillo was hanged at the Plaza de los Españoles that night, but his name would be eternally remembered in the name of the plaza, and he would be remembered as the voice of revolution across South America.
In 1825, after the decisive victory of the republicans at Ayacucho over the Spanish army in the course of the Spanish American wars of independence, the city's full name was changed to "La Paz de Ayacucho" (meaning "The Peace of Ayacucho").
In 1898, La Paz was made the "de facto" seat of the national government, with Sucre remaining the nominal historical as well as judiciary capital. This change reflected the shift of the Bolivian economy away from the largely exhausted silver mines of Potosí to the exploitation of tin near Oruro, and resulting shifts in the distribution of economic and political power among various national elites.
Geography.
Located at (−16.5, −68.1333), La Paz is built in a canyon created by the Choqueyapu River (now mostly built over), which runs northwest to southeast. The city's main thoroughfare, which roughly follows the river, changes names over its length, but the central tree-lined section running through the downtown core is called the Prado.
The geography of La Paz (in particular the altitude) reflects society: the lower areas of the city are the more affluent areas. While many middle-class residents live in high-rise condos near the center, the houses of the truly affluent are located in the lower neighborhoods southwest of the Prado. And looking up from the center, the surrounding hills are plastered with makeshift brick houses of those less economically fortunate.
The satellite city of El Alto, in which the airport is located, is spread over a broad area to the west of the canyon, on the Altiplano. La Paz is renowned for its unique markets, very unusual topography, and traditional culture.
La Paz is located in the valleys of the Andes, and is closer to the Eastern split of the Altiplano region. Therefore, it is closer to the famous mountains such as the Illimani (guardian of La Paz), Huayna Potosi, Mururata, and Illampu. On the Western side of the Altiplano divide, about an hour to the West of the La Paz, is the site of the tallest mountain in Bolivia and 9th tallest mountain in the Andes, the Sajama Volcano. In July 1994, an earthquake rated at 8.2 struck just 200 mi north of La Paz.
Climate.
At 4000 m above sea level, higher parts of La Paz have an unusual subtropical highland climate ("Cwc", according to the Köppen climate classification), with subpolar oceanic characteristics (the average temperature of the warmest month is lower than 10 °C). The whole city has rainy summers and dry winters. Nighttime temperatures range from cool to cold. Light snow flurries can occur in winter, especially at dawn and it usually melts before noon. At these high altitudes despite being located only 16 degrees from the equator, the city's average temperature is similar to that of cities such as Bergen, Norway or Tórshavn, Faroe Islands located as far as 60 and 62 degrees from the equator.
The temperatures in the central La Paz, at 3600 m, and in the "Zona Sur" (Southern Zone), at 3250 m above sea level, are warmer (subtropical highland climate "Cwb", according to the Köppen classification).
Owing to the altitude of the city, temperatures are consistently cool throughout the year, though the diurnal temperature variation is typically large. The city has a relatively dry climate, with rainfall occurring mainly in the slightly warmer months of November to March.
February and March, the two cloudiest months of the year, both in late summer, receive a low daily average of 5 hours of sunshine. Conversely, June and July, the two sunniest months of the year, both in winter, receive an abundant daily average of 9 hours of sunshine.
The seasonally uneven distribution of the year's annual precipitation, often results in destructive mudslides experienced in summer, due to the copious amount of precipitation typically observed throughout the season. The wettest month is January while the driest months are June and July, the city receiving a monthly average of 130 mm and 5 mm of precipitation respectively.
Colonial architecture.
The city of La Paz has a consistently decreasing volume of colonial buildings, mostly centered around the vicinity of the Plaza Murillo. Due to a lack of funds and the inability of property owners to pay for restorations to colonial buildings, many have been torn down, or are in a dilapidated state. As historic buildings are more expensive to keep, land owners find it less of a burden to construct more modern buildings as opposed to keeping the old ones. Although there has been an increasing number of projects and propositions to restore some of the city's colonial buildings, the future of these historic edifices remains uncertain.
Economy.
The economy of La Paz has improved greatly in recent years, mainly as a result of improved political stability. Due to the long period of high inflation and economic struggle faced by Bolivians in the 1980s and early 1990s, a large informal economy developed. Evidence of this is provided by the markets found all around the city. While there are stable markets, almost every street in the downtown area and surrounding neighborhoods has at least one vendor on it. La Paz remains the principal center of manufacturing enterprises that produce finished-product goods for the country, with about two-thirds of Bolivia's manufacturing located nearby. Historically, industry in Bolivia has been dominated by mineral processing and the preparation of agricultural products. However, in the urban centre of La Paz, small plants carry out a large portion of the industry. Food, tobacco products, clothing, various consumer goods, building materials, and agricultural tools are produced. "The tin quotations from London are watched in La Paz with close interest as an index of the country's prosperity; a third of the national revenue and more than half of the total customs in 1925 were derived from tin; in short, that humble but indispensable metal is the hub around which Bolivia's economic life revolves. The tin deposits of Bolivia, second largest in the world, ... invite development."
Sports.
La Paz is the home of some of the biggest football teams in Bolivia.
The city is host to several other teams that play in the first and second divisions such as: 
With the exception of Deportivo Municipal and Unión Maestranza, all the other teams play the majority of their games in the city stadium, the Estadio Hernando Siles, which also hosts the national football team and international games. Always Ready frequently play at the Estadio Rafael Mendoza which belongs to The Strongest, who rarely use the stadium due to its relatively small capacity.
Education.
The city hosts some of the most important universities of the country:
Tourism.
La Paz is an important cultural center of Bolivia. The city hosts several cathedrals belonging to the colonial times, such as the San Francisco Cathedral and the Metropolitan Cathedral, this last one located on Murillo Square, which is also home of the political and administrative power of the country. Hundreds of different museums can be found across the city, the most notable ones on Jaén Street, which street design has been preserved from the Spanish days and is home of 10 different museums.
The home of the Bolivian government is located on Murillo Square and is known as "Palacio Quemado" ("Burnt Palace") as it has been on fire several times. The palace has been restored many times since, but the name has remained untouched.
Transportation.
Air.
La Paz is served by El Alto International Airport (IATA code: LPB), which is situated eight miles (8 mi) south-west of La Paz. At an elevation of 4061 m, it is one of the highest major airports in the world. Airport facilities include a bank, bars, car rentals, restaurants, free wi-fi internet and duty-free shops. The runway has a length of 4000 m. Additionally, it is the second airport in the Western Hemisphere, and the third airport in the world, to successfully pass the International Civil Aviation Organization's (ICAO) Universal Security Audit Program (USAP).
Bus.
La Paz Bus Station, originally a bus and train station, was built by the French architect Gustave Eiffel. It is the main gateway for inter-city buses with several daily departures to all the main Bolivian cities, and routes to Chile and Peru. The city is connected by road with the city of Oruro from where there are routes to Sucre, Potosí and the south of the country. Another highway branches off before Oruro to reach Cochabamba and Santa Cruz. Roads to the west go to Copacabana and Tiwanaku, near Lake Titicaca, and continue to Cuzco, Peru via the border town of Desaguadero. There are also roads north to get to Yungas crossing the Andes Mountains.
Departures to smaller cities and towns within the department use informal stations located in Villa Fátima (departures to Los Yungas, Beni and Pando), Upper San Pedro (for Apolo) and near the General Cemetery (for Copacabana, Lake Titicaca, or via Tiwanaku to Desaguadero on the Peruvian border).
Cable car system.
A system of urban transit aerial cable cars called Mi Teleférico ("My Cable Car") was opened in 2014. Currently three lines are in operation, and six more lines are in the planning stage. The initial three lines were built by the Austrian company Doppelmayr. The first two lines (Red and Yellow) connect La Paz with El Alto.
Water supply.
The water supply of La Paz is threatened by the impact of climate change through the melting of glaciers. The city receives its drinking water from three water systems: El Alto, Achachiucala and Pampahasi. La Paz shares the first and largest of these systems with its sister city El Alto. All three systems are fed by glaciers and rivers in the Cordillera mountain range. 20-28 % of its water is fed by glaciers, the remainder coming from rainfall and snowmelt. The glaciers recede as a result of climate change, initially increasing water availability during the dry season, but ultimately threatening a substantial decrease in dry season run-off when they completely disappear. A small glacier, the Chacaltaya near El Alto, already disappeared in 2008. The El Alto system receives its water from the Tuni Dam and two water channels. These channels divert water that flows from the Zongo Glacier on the slopes of Huayna Potosi and from Condoriri North of El Alto. The 2.9 km long Zongo glacier retreats at a rate of about 18 meters per year. The Tuni and Condoriri glaciers have lost 39% of their area between 1983 and 2006. According to a study by the Stockholm Environment Institute (SEI), the El Alto system is the least resilient against the impact of climate change among the three systems. The study says that reducing water distribution losses is the most effective short-term strategy to deal with water scarcity. New water sources further to the North in the Cordillera include the Khara Kota and Taypicacha, but they are expensive to develop and their water supply is also affected by glacier melt.
International relations.
Twin towns and sister cities.
La Paz is part of the Union of Ibero-American Capital Cities from October 12, 1982 establishing brotherly relations with the following cities:
Additionally, agreement was reached by Twin Cities with:
In June 2008, a twinning agreement was signed with Zaragoza, Spain.
La Paz has been a member of Merco Ciudades, a group of 180 cities within Mercosur, since 1999.
External links.
Listen to this article ()
This audio file was created from a revision of the "La Paz" article dated 2005-04-16, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="42822" url="http://en.wikipedia.org/wiki?curid=42822" title="244">
244

Year 244 (CCXLIV) was a leap year starting on Monday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Armenius and Aemilianus (or, less frequently, year 997 "Ab urbe condita"). The denomination 244 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years. It is also considered a lucky number in several areas of the UAE; specifically Dubai. 
Events.
<onlyinclude>
By topic.
Commerce.
</onlyinclude>

</doc>
<doc id="42825" url="http://en.wikipedia.org/wiki?curid=42825" title="246">
246

Year 246 (CCXLVI) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Praesens and Albinus (or, less frequently, year 999 "Ab urbe condita"). The denomination 246 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="42826" url="http://en.wikipedia.org/wiki?curid=42826" title="247">
247

Year 247 (CCXLVII) was a common year starting on Friday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Philippus and Severus (or, less frequently, year 1000 "Ab urbe condita"). The denomination 247 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="42827" url="http://en.wikipedia.org/wiki?curid=42827" title="248">
248

Year 248 (CCXLVIII) was a leap year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Philippus and Severus (or, less frequently, year 1001 "Ab urbe condita"). The denomination 248 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42828" url="http://en.wikipedia.org/wiki?curid=42828" title="249">
249

Year 249 (CCXLIX) was a common year starting on Monday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Gavius and Aquilinus (or, less frequently, year 1002 "Ab urbe condita"). The denomination 249 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42829" url="http://en.wikipedia.org/wiki?curid=42829" title="243">
243

Year 243 (CCXLIII) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Arrianus and Papus (or, less frequently, year 996 "Ab urbe condita"). The denomination 243 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Empire.
</onlyinclude>

</doc>
<doc id="42831" url="http://en.wikipedia.org/wiki?curid=42831" title="242">
242

Year 242 (CCXLII) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Gratus and Lepidus (or, less frequently, year 995 "Ab urbe condita"). The denomination 242 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42832" url="http://en.wikipedia.org/wiki?curid=42832" title="241">
241

Year 241 (CCXLI) was a common year starting on Friday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Gordianus and Pompeianus (or, less frequently, year 994 "Ab urbe condita"). The denomination 241 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Europe.
</onlyinclude>

</doc>
<doc id="42833" url="http://en.wikipedia.org/wiki?curid=42833" title="240">
240

Year 240 (CCXL) was a leap year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Sabinus and Venustus (or, less frequently, year 993 "Ab urbe condita"). The denomination 240 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42834" url="http://en.wikipedia.org/wiki?curid=42834" title="Inessive case">
Inessive case

Inessive case (abbreviated INE; from Latin "inesse" "to be in or at") is a locative grammatical case. This case carries the basic meaning of "in": for example, "in the house" is "talo·ssa" in Finnish, "maja·s" in Estonian, "etxea·n" in Basque, "nam·e" in Lithuanian, "sāt·ā" in Latgalian and "ház·ban" in Hungarian.
In Finnish the inessive case is typically formed by adding "ssa/ssä". Estonian adds "s" to the genitive stem. In Hungarian, the suffix "ban/ben" is most commonly used for inessive case, although many others, such as -on, -en, -ön and others are also used, especially with cities.
In the Finnish language, the inessive case is considered the first (in Estonian the second) of the six locative cases, which correspond to locational prepositions in English. The remaining five cases are:

</doc>
<doc id="42835" url="http://en.wikipedia.org/wiki?curid=42835" title="Elative case">
Elative case

Elative (abbreviated ELA; from Latin "efferre" "to bring or carry out") is a locative case with the basic meaning "out of".
In Finnish elative is typically formed by adding "sta/stä", in Estonian by adding "st" to the genitive stem. In Hungarian the suffix "ból/ből" is used for elative.
"talosta" - "out of the house, from house" (Finnish "talo" = "house")<br>
"majast" - "out of the house, from house" (Estonian "maja" = "house")<br>
"házból" - "out of house" (Hungarian "ház" = "house")
In some dialects of colloquial Finnish it is common to drop the last vowel and thus the usage of elative resembles that of Estonian, for example "talost".
Other locative cases are:

</doc>
<doc id="42836" url="http://en.wikipedia.org/wiki?curid=42836" title="239">
239

Year 239 (CCXXXIX) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Gordianus and Aviola (or, less frequently, year 992 "Ab urbe condita"). The denomination 239 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42837" url="http://en.wikipedia.org/wiki?curid=42837" title="Illative case">
Illative case

Illative (abbreviated ILL; from Latin "illatus" "brought in") is, in the Finnish language, Estonian language and the Hungarian language, the third of the locative cases with the basic meaning of "into (the inside of)". An example from Hungarian is "a házba" (into the house, with "a ház" meaning "the house"). An example from Estonian is "majasse" and "majja" (into the house), formed from "maja" (a house). An example from Finnish is "taloon" (into the house), formed from "talo" (a house). 
Illative case in the Finnish language.
In Finnish, the case is formed by adding "-hVn", where 'V' represents the last vowel, and then removing the 'h' if a simple long vowel would result. For example, "talo + hVn" becomes "talohon", where the 'h' elides and produces "taloon" with a simple long 'oo'; cf. "maa + hVn" becomes "maahan", without the elision of 'h'. This unusually complex way of adding a suffix can be explained by its reconstructed origin: a voiced palatal fricative. (Modern Finnish has lost palatalization and fricatives other than 'h' or 's'.) In the dialect of Pohjanmaa, the 'h' is not removed; one does say "talohon".
The other locative cases in Finnish, Estonian and Hungarian are:
Illative case in the Lithuanian language.
The illative case, denoting direction of movement, occurs rarely in modern standard Lithuanian, although used in the common spoken language, especially in certain dialects. Its singular form, heard more often than the plural, appears in books, newspapers, etc. Most Lithuanian nouns can take the illative ending, indicating that from the descriptive point of view the illative still can be treated as a case in Lithuanian, although since the beginning of the 20th century it isn't included in the lists of standard Lithuanian cases in most grammars and textbooks and the prepositional construction į+accusative is more frequently used today to denote direction. The illative case was used extensively in older Lithuanian; the first Lithuanian grammar book, by Daniel Klein, that mentions both illative and į+accusative, calls the usage of the illative "more elegant". In later times, it has often appeared in the written texts of the authors who grew up in Dzukija or Eastern Aukštaitija, such as Vincas Krėvė-Mickevičius.
The illative case in Lithuanian has its own endings, which are different for each declension paradigm, although quite regular, compared with some other Lithuanian cases. An ending of the illative always ends with "n" in the singular, and "sna" is the final part of an ending of the illative in the plural.
Certain fixed phrases in the standard language are illatives, such as "patraukti atsakomybėn" ("to arraign"), "dešinėn!" ("turn right").
Examples:

</doc>
<doc id="42838" url="http://en.wikipedia.org/wiki?curid=42838" title="Adessive case">
Adessive case

In Uralic languages, such as Finnish, Estonian and Hungarian, the adessive case (abbreviated ADE; from Latin "adesse" "to be present") is the fourth of the locative cases with the basic meaning of "on". For example, Estonian "laud" (table) and "laual" (on the table), Hungarian "asztal" and "asztalnál" (at the table). It is also used as an instrumental case in Finnish.
In Finnish, the suffix is "-lla/-llä", e.g. "pöytä" (table) and "pöydällä" (on the table). In addition, it can specify "being around the place", as in "koululla" (at the school including the schoolyard), as contrasted with the inessive "koulussa" (in the school, inside the building). 
In Estonian, the ending "-l" is added to the genitive case, e.g. "laud" (table) - "laual" (on the table). Besides the meaning "on", this case is also used to indicate ownership. For example, "mehe"l" on auto" means "the man owns a car".
As the Uralic languages don't possess the verb "to have", it is the subject in the adessive case + "on" (for example, "minulla on", "I have", literally "at me is").
The other locative cases in Finnish, Estonian and Hungarian are:
Finnish.
The Finnish adessive has the word ending -lla or -llä (according to the rules of vowel harmony). It is usually added to nouns and associated adjectives. 
It is used in the following ways.

</doc>
<doc id="42839" url="http://en.wikipedia.org/wiki?curid=42839" title="Allative case">
Allative case

Allative case (abbreviated ALL; from Latin "allāt-", "afferre" "to bring to") is a type of the locative cases used in several languages. The term allative is generally used for the lative case in the majority of languages which do not make finer distinctions.
Finnish language.
In the Finnish language, the allative is the fifth of the locative cases, with the basic meaning of "onto". Its ending is "-lle", for example "pöytä" (table) and "pöydälle" (onto the top of the table). In addition, it is the logical complement of the adessive case for referring to "being around the place". For example, "koululle" means "to the vicinity of the school". With time, the use is the same: "ruokatunti" (lunch break) and "... lähti ruokatunnille" ("... left to the lunch break"). Some actions require the case, e.g. "kävely" - "mennä kävelylle" "a walk - go for a walk".
The other locative cases in Finnish and Estonian are:
Baltic languages.
In the Lithuanian and Latvian languages the allative had been used dialectally as an innovation since the Proto-Indo-European, but it is almost out of use in modern times. Its ending in Lithuanian is "-op" which was shortened from "-opi", whereas its ending in Latvian is "-up". In the modern languages the remains of the allative can be found in certain fixed expressions that have become adverbs, such as Lit. "išėjo Dievop" ("gone to God", i.e. died), "velniop!" ("to hell!"), "nuteisti myriop" ("sentence to death"), "rudeniop" ("towards autumn"), "vakarop" ("towards the evening"), Lat. "mājup" ("towards home"), "kalnup" ("uphill"), "lejup" ("downhill").
Greek.
In Mycenaean Greek, a "-de" ending is used to denote an allative, when it is not being used as an enclitic, e.g. "te-qa-de", *"Tʰēgʷasde", "to Thebes" (Linear B: 𐀳𐀣𐀆). This ending survives into Ancient Greek in words such as "Athḗnaze", from accusative "Athḗnās" + "-de".
Accusative.
The Latin accusative of towns and small islands is used for motion towards, like the allative case.

</doc>
<doc id="42841" url="http://en.wikipedia.org/wiki?curid=42841" title="238">
238

Year 238 (CCXXXVIII) was a common year starting on Monday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Pius and Pontianus (or, less frequently, year 991 "Ab urbe condita"). The denomination 238 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Commerce.
</onlyinclude>

</doc>
<doc id="42842" url="http://en.wikipedia.org/wiki?curid=42842" title="Essive case">
Essive case

The essive or similaris case (abbreviated ESS) is one example of a grammatical case, an inflectional morphological process by which a form is altered or marked in order to indicate its grammatical function. Marking of the essive case on a noun can express it as a definite period of time during which something happens or during which a continuous action was completed. The essive case can also denote a form as a temporary location, state of being, or character in which the subject was at a given time. The latter of these meanings is often referred to as the equivalent of the English phrase “as a ___”.
In the Finnish language, this case is marked by adding "-na/-nä" to the stem of the noun.
 Example: "lapsi" "child" -> "lapsena" "as a child", "when (I) was a child".
 Example: Veljeni on säveltäjänä “My brother is a composer.”
 säveltäjä -> säveltäjänä “state of being a composer (the given time is the present)”
 Example: Sain kirjeen viime maanantaina. “I received the letter last Monday.”
 maanantai “Monday” -> maanantaina “Monday (referring to the time when the action was completed)”
Use of the essive case for specifying times, days, and dates when something happens is also apparent in Finnish 
 Example: "kuudentena joulukuuta" -> "on the 6th of December".
In Finnish, the essive case is technically categorized as an old locative case, or a case which in some way indicates spatial location. However, in the present day language, the case has lost the majority of its spatial meaning. The case instead typically denotes a state that is temporary or inclined to change.
Some fixed expressions do retain the essive in its ancient locative meaning however, e.g. "at home" is "kotona". 
 Example: "Luen lehtiä kotona." "I read newspapers at home."
When marking something that in fact cannot literally change states, the essive case can implicate the presence of alternative states or even two individual, differing ‘worlds’. This can been seen in the following example:
 Example: Ostin helmen aitona. ”I bought the pearl thinking it was genuine [but later found out that it was not].”
The example above illustrates the process by which marking of the essive case can be seen as creating two differing ‘worlds’: one real and one illusionary. The “temporary” component of the meaning encoded by marking of the essive case on the Finnish word for ‘genuine’ (aito) makes a distinction between the perceived state of the subject as genuine at the time of purchase and the actual state of subject as not genuine as it is perceived at present or at the time of the moment of speech.
If the inessive were used e.g. "kodissani" this would distinguish the activity from reading the papers, say, in the garage or in the garden (of the home).
In the Estonian language, this case is marked by adding "-na" to the genitive stem. Marking of this case in Estonian denotes the capacity in which the subject acts. The essive case is used for indicating "states of being", but not of "becoming", which is instead marked by either the translative case, the elative case, or the nominative case.
 Example: "laps" "child" -> "lapse" "of child" -> "lapsena" "as a child", "when (I) was a child".
 Example: Ta töötab insenerina "He works as an engineer."
In the Spanish language, the essive case does serve as a locative case which encodes spatial meaning. The essive case is marked by use of the adposition "en", which translates to English as "on". The Spanish essive case and its relation to two other locative cases, the allative case (encoded by Spanish adposition “a” meaning “to”) and the ablative case (encoded by Spanish adposition “de” meaning “from”), is discussed by Dein Creissels in "Space in Languages: Linguistic Systems and Cognitive Categories". Creissels asserts that Spanish is just one example of a European languages in which these three cases are distinct, as opposed to other European languages which exhibit some conflation between marking of the essive case and of the allative case. Below is an example of the adposition encoding the essive case in Spanish:
 Example: Los niños están en la playa "The children are on the beach"

</doc>
<doc id="42843" url="http://en.wikipedia.org/wiki?curid=42843" title="237">
237

Year 237 (CCXXXVII) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Perpetuus and Felix (or, less frequently, year 990 "Ab urbe condita"). The denomination 237 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42844" url="http://en.wikipedia.org/wiki?curid=42844" title="Translative case">
Translative case

The translative case (abbreviated TRANSL) is a grammatical case that indicates a change in state of a noun, with the general sense of "becoming "X"" or "change to "X"".
In the Finnish language, this is the counterpart of the essive case, with the basic meaning of a change of state. It is also used for expressing "in (a language)", "considering it's a (status)" and "by (a time)". Its ending is "-ksi". Examples:
Examples in Estonian:
Examples in Hungarian. The ending is -vá / -vé after a vowel; assimilating to the final consonsant otherwise:

</doc>
<doc id="42845" url="http://en.wikipedia.org/wiki?curid=42845" title="Instructive case">
Instructive case

In the Finnish language, the instructive case has the basic meaning of "by means of". It is a comparatively rarely used case, though it is found in some commonly used expressions, such as "omin silmin" → "with one's own eyes".
In modern Finnish, many of its instrumental uses are being superseded by the adessive case, as in "minä matkustin junalla" → "I travelled "by train"."
It is also used with Finnish verbal second infinitives to mean "by ...ing", e.g. "lentäen" → "by flying", "by air" ("lentää" = "to fly").
In Turkish, the suffix "-le" is used for this purpose. Ex: Trenle geldim "I came via train".

</doc>
<doc id="42846" url="http://en.wikipedia.org/wiki?curid=42846" title="Abessive case">
Abessive case

In linguistics, abessive (abbreviated ABE or ABESS), caritive and privative (abbreviated PRIV) is the grammatical case expressing the lack or absence of the marked noun. In English, the corresponding function is expressed by the preposition "without" or by the suffix "-less."
The name "abessive" is derived from Latin "abesse" "to be away/absent", and is especially used in reference to Uralic languages. The name "caritive" is derived from Latin "carere" "to lack", and is especially used in reference to Caucasian languages. The name "privative" is derived from Latin "privare" "to deprive".
In Afro-Asiatic Languages.
Somali.
In the Somali language, the abessive case is marked by "-laa" or "-la" and dropping all but the first syllable on certain words. For example:
In Australian languages.
Martuthunira.
In Martuthunira, the privative case is formed with two suffixes, "-wirriwa" and "-wirraa". What determines which suffix is used in a given situation is unclear.
In Uralic languages.
Finnish.
In the Finnish language, the abessive case is marked by "-tta" for back vowels and "-ttä" for front vowels according to vowel harmony. For example:
An equivalent construction exists using the word "ilman" and the partitive:
or, more uncommonly:
The abessive case of nouns is rarely used in writing and even less in speech, although some abessive forms are more common than their equivalent "ilman" forms:
The abessive is, however, commonly used in nominal forms of verbs (formed with the affix "-ma-" / "-mä-"), such as "puhu-ma-tta" "without speaking", "osta-ma-tta" "without buying," "välittä-mä-ttä" "without caring:"
This form can often be replaced by using the negative form of the verb:
It is possible to occasionally hear what is considered wrong usage of the abessive in Finnish, where the abessive and "ilman" forms are combined:
There is debate as to whether this is interference from Estonian.
Estonian.
Estonian also uses the abessive, which is marked by "-ta" in both the singular and the plural:
Unlike in Finnish, the abessive is commonly used in both written and spoken Estonian.
The nominal forms of verbs are marked with the affix "-ma-" and the abessive marker "-ta":
Tallinn has a pair of bars that play on the use of the comitative and abessive, the Nimeta baar (the nameless bar) and the Nimega baar (the bar with a name).
Skolt Sami.
The abessive marker for nouns in Skolt Sámi is "-tää" in both the singular and the plural:
The abessive-like non-finite verb form (converb) is "-ǩâni" or "-kani":
Unlike in Finnish, the abessive is still commonly used in Skolt Sámi.
Inari Sami.
The abessive marker for nouns in Inari Sámi is "-táá." The corresponding non-finite verb form is "-hánnáá," "-hinnáá" or "-hennáá."
Other Sami languages.
The abessive is not used productively in the Western Sámi languages, although it may occur as a cranberry morpheme.
Hungarian.
In Hungarian, the abessive case is marked by "-talan" for back vowels and "-telen" for front vowels according to vowel harmony. Sometimes, with certain roots, the suffix becomes "-tlan" or "-tlen". For example:
There is also the postposition "nélkül," which also means without, but is not meant for physical locations.
In Turkic Languages.
Bashkir.
In Bashkir the suffix is "-һыҙ"/"-һеҙ" ("-hïð"/"-hĭð").
Turkish.
The suffix "-siz" (variations: "-sız", "-suz", "-süz") is used in Turkish.
Ex: "evsiz" ("ev" = house, houseless/homeless), "barksız", "görgüsüz" ("görgü" = good manners, ill-bred), "yurtsuz".
Azerbaijani.
The same suffix is used in the Azerbaijani language.
Chuvash.
In Chuvash the suffix is "-сĂр".
Kyrgyz.
In Kyrgyz the suffix is "-сIз".

</doc>
<doc id="42847" url="http://en.wikipedia.org/wiki?curid=42847" title="Comitative case">
Comitative case

The comitative case (abbreviated COM) is a grammatical case that denotes accompaniment.:17–23 In English, the preposition "with", in the sense of "in company with" or "together with", plays a substantially similar role (other uses of "with", e.g. with the meaning of "using" or "by means of" (I cut bread with a knife), correspond to the instrumental case or related cases).
Core meaning.
Comitative case encodes a relationship of "accompaniment" between two participants in an event, called the "accompanee" and the "companion." In addition, there is a "relator" (which can be of multiple lexical categories, but is most commonly an affix or adposition).:17–18 Use of Comitative case gives prominence to the accompanee.:602 For example:
In this case, "il professore" is the accompanee, "i suoi studenti" is the companion, and "con" is the relator. As the accompanee, "il professore" is the most prominent.
Animacy also plays a major role in most languages that have a Comitative case. One group of languages requires both the accompanee and the companion to be either human or animate. Another group requires both to be in the same category—that is, both human or both animate. A third group requires an animate accompanee and an inanimate companion. The remaining languages have no restrictions based on animacy.:603–604
Comparison to similar cases.
The definition of Comitative case is often conflated or confused with other similar cases, especially Instrumental case and Associative case.
The chief difference between Comitative and Instrumental is this: while Comitative relates an accompanee and a companion, Instrumental relates an agent, an object, and a patient.:593 Enrique Palancar defines the role of Instrumental case as ‘the role played by the object the Agent manipulates to achieve a change of state of the Patient.’ Even though the difference is straightforward, Instrumental and Comitative are expressed the same way in many languages, including English, so it is often difficult to separate them.
Russian is one of many languages which differentiates morphologically between Instrumental and Comitative, so an example from Russian will help illustrate the difference.
In Russian, Comitative is marked by adding a preposition “s” and declining the companion in the Instrumental case. In the Instrumental case, the object is declined but there is no preposition added.
Comitative case is also often confused with Associative case. Before the term Comitative was applied to the accompanee-companion relationship, the relationship was often called Associative case, and some linguists still use the latter term It is important to distinguish between Comitative and Associative, though, because Associative also refers to a specific variety of Comitative used in Hungarian.:605
Expressions of the Comitative semantic relation.
Grammatical case is a category of inflectional morphology, thus the Comitative case is an expression of the Comitative semantic relation through inflectional affixation—that is, through prefixes, suffixes and circumfixes. Although all three major types of affixes are used in at least a few languages, suffixes are the most common expression. Languages which use affixation to express the Comitative semantic relation include Hungarian, which uses suffixes; Totonac, which uses prefixes; and Chukchi, which uses circumfixes.:602
Comitative relations are also commonly expressed by using adpositions—that is, prepositions, postpositions and circumpositions. Examples of languages which use adpositional constructions to express Comitative relations are French, which uses prepositions; Wayãpi, which uses postpositions; and Bambara, which uses circumpositions.:603
Adverbial constructions can also mark Comitative relations, although they act very similarly to adpositions. One language which uses adverbs to mark Comitative case is Latvian.:603
The final way in which Comitative relations can be expressed is by serial-verb constructions. In these languages, the Comitative marker is usually a verb whose basic meaning is “to follow.” A language which marks Comitative relations with serial-verb constructions is Chinese.:603
Examples.
Indo-European languages.
French.
French uses prepositions to express the Comitative semantic relation.
In this case, the preposition “avec” is used to express the Comitative semantic relation. The preposition “avec” is the standard Comitative marker in French; however, French has a special case called Ornative, a variety of Comitative which is used for bodily property or clothes. The French Ornative marker is “à”.:603
Latvian.
In Latvian, both Instrumental and Comitative are expressed with the preposition “ar” :102 However, “ar” is only used when the companion is in accusative and singular, or when it is in dative and plural. Otherwise the coordinating conjunction “un” is used.:21
In the example above, “ar” is used because Rudolf, the companion, is in accusative and singular. Below, “ar” is used in the other location where it is allowable, with a dative plural companion.
Uralic languages.
Estonian.
In Estonian, the Comitative marker is the suffix “-ga”.:90
Finnish.
In Finnish, the comitative case ("komitatiivi") has the suffix "-ne" with adjectives and "-ne-" + a mandatory possessive suffix with the main noun. There is no singular-plural distinction; only the plural of the comitative is used in both singular and plural senses, thus it appears always as "-ine-". For instance, "with their big ships" is "suuri·ne laivo·i·ne·en" (big-COM ship(oblique)-PL-COM-POS 3PL), while "with his/her big ships" is "suuri·ne laivo·i·ne·nsa" ((big-COM ship(oblique)-PL-COM-POS 3SG)). It is rarely used and is mainly a feature of the formal literary language, appearing very rarely in everyday speech.
The regular "with" is expressed with the postposition "kanssa", thus this form is used in most cases, e.g. "suurien laivojensa kanssa" "with their big ships". The two forms may contrast, however, since the comitative always comes with the possessive suffix, and thus can be only used when the agent has possession of some sort over the main noun. For instance, "Ulkoministeri jatkaa kollegoineen neuvotteluja sissien kanssa", "The foreign minister, with [assistance from] his colleagues, continues the negotiations with the guerrillas", has "kollegoineen" "with his colleagues" contrasted with "sissien kanssa" "with the guerrillas", the former "possessed", the latter not.
Sami languages.
As there is many Sami languages there are variations between them. In the largest Sami language, Northern Sami, the comitative case means either communion, fellowship, connection - or instrument, tool. It can be used either as an object or as an adverbial.
It is expressed through the suffix "-in" in Northern Sami, and is the same in both singular and plural.
An example of the object use in Northern Sami is "Dat láve álo riidalit isidi"in"", meaning "She always argues "with" her husband". An example of the adverbial use is "Mun čálán bleahka"in"", meaning "I write "with" ink".
Hungarian.
In Hungarian, Comitative case is marked by the suffix “-stul/-stül,” as shown in the example below.
However, the Comitative case marker cannot be used if the companion has a plural marker. So when the Comitative marker is added to a noun, it obscures whether that noun is singular or plural.
Chukchi.
Chukchi uses a circumfix to express Comitative case.
In the example, the circumfix га-ма is attached to the root мэлгар “gun” to express Comitative.
Drehu.
In Drehu, there are two prepositions which can be used to mark Comitative. Which of the prepositions is used is determined by the classes of the accompanee and companion.
Hausa.
The Comitative marker in Hausa is the preposition “dà.” In Hausa, a prepositional phrase marked for Comitative can be moved to the front of the sentence for emphasis, as shown in the examples below.
In Hausa it is ungrammatical to do the same with coordinating conjunctions. For example, if the companions were “dog and cat,” it would be ungrammatical to move either “dog” or “cat” to the front of the sentence for emphasis, while it is grammatical to do so when there is a Comitative marker rather than a conjunction.

</doc>
<doc id="42848" url="http://en.wikipedia.org/wiki?curid=42848" title="234">
234

Year 234 (CCXXXIV) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Pupienus and Sulla (or, less frequently, year 987 "Ab urbe condita"). The denomination 234 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
China.
</onlyinclude>

</doc>
<doc id="42849" url="http://en.wikipedia.org/wiki?curid=42849" title="Prolative case">
Prolative case

The prolative case (abbreviated PROL), also vialis case (abbreviated VIA), is a grammatical case of a noun or pronoun that has the basic meaning of "by way of".
In the Finnish language, the so-called prolative has a restricted, by some almost fossilized meaning "by (medium of transaction)". That means, it can still be used for other words, but then does not sound 'native' / 'modern'. Some native examples are for example, "postitse" ("by post"), "puhelimitse" ("by phone"), "meritse" ("by sea"), "netitse" ("over the Internet"). The prolative forms are considered by some Finnish grammarians to be adverbs because they do not show agreement on adjectives like the other Finnish cases (also called the "concord test"). This claim is not true, however, because an adjective will agree with the prolative: "Hän hoiti asian pitkitse kirjeitse."
The prolative exists in a similar state in the Estonian language.
The vialis case in Eskimo–Aleut languages has a similar interpretation, used to express movement using a surface or way. For example, "by way of or through the house".
Basque grammars frequently list the "nortzat / nortako" case (suffix "-tzat" or "-tako") as "prolative" ("prolatiboa"). However, the meaning of this case is unrelated to the one just described above for other languages and alternatively has been called "essive / translative", as it means "for [something else], as (being) [something else]"; e.g., "hiltzat eman" "to give up for dead", "lelotzat hartu zuten" "they took him for a fool". The meaning "by way of" of the case labelled prolative in the above languages is expressed in Basque by means of the instrumental (suffix "-[e]z").

</doc>
<doc id="42850" url="http://en.wikipedia.org/wiki?curid=42850" title="233">
233

Year 233 (CCXXXIII) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Claudius and Paternus (or, less frequently, year 986 "Ab urbe condita"). The denomination 233 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Empire.
</onlyinclude>

</doc>
<doc id="42851" url="http://en.wikipedia.org/wiki?curid=42851" title="232">
232

Year 232 (CCXXXII) was a leap year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Lupus and Maximus (or, less frequently, year 985 "Ab urbe condita"). The denomination 232 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42852" url="http://en.wikipedia.org/wiki?curid=42852" title="Radio frequency">
Radio frequency

Radio frequency (RF) is a rate of oscillation in the range of around to , which corresponds to the frequency of radio waves, and the alternating currents which carry radio signals. RF usually refers to electrical rather than mechanical oscillations; however, mechanical RF systems do exist (see mechanical filter and RF MEMS).
Although radio "frequency" is a rate of oscillation, the term "radio frequency" or its abbreviation "RF" are also used as a synonym for radio – i.e., to describe the use of wireless communication, as opposed to communication via electric wires. Examples include:
Special properties of RF current.
Electric currents that oscillate at radio frequencies have special properties not shared by direct current or alternating current of lower frequencies. 
Radio communication.
To receive radio signals an antenna must be used. However, since the antenna will pick up thousands of radio signals at a time, a radio tuner is necessary to "tune into" a particular frequency (or frequency range). This is typically done via a resonator – in its simplest form, a circuit with a capacitor and an inductor form a tuned circuit. The resonator amplifies oscillations within a particular frequency band, while reducing oscillations at other frequencies outside the band. Another method to isolate a particular radio frequency is by oversampling (which gets a wide range of frequencies) and picking out the frequencies of interest, as done in software defined radio.
The distance over which radio communications is useful depends significantly on things other than wavelength, such as transmitter power, receiver quality, type, size, and height of antenna, mode of transmission, noise, and interfering signals. Ground waves, tropospheric scatter and skywaves can all achieve greater ranges than line-of-sight propagation. The study of radio propagation allows estimates of useful range to be made.
In medicine.
Radio frequency (RF) energy, in the form of radiating waves or electrical currents, has been used in medical treatments for over 75 years, generally for minimally invasive surgeries, using radiofrequency ablation and cryoablation, including the treatment of sleep apnea. Magnetic resonance imaging (MRI) uses radio frequency waves to generate images of the human body.
Radio frequencies at non-ablation energy levels are sometimes used as a form of cosmetic treatment that can tighten skin, reduce fat (lipolysis), or promote healing.
RF diathermy is a medical treatment that uses RF induced heat as a form of physical or occupational therapy and in surgical procedures. It is commonly used for muscle relaxation. It is also a method of heating tissue electromagnetically for therapeutic purposes in medicine. Diathermy is used in physical therapy and occupational therapy to deliver moderate heat directly to pathologic lesions in the deeper tissues of the body. Surgically, the extreme heat that can be produced by diathermy may be used to destroy neoplasms, warts, and infected tissues, and to cauterize blood vessels to prevent excessive bleeding. The technique is particularly valuable in neurosurgery and surgery of the eye. Diathermy equipment typically operates in the short-wave radio frequency (range 1–100 MHz) or microwave energy (range 434–915 MHz).
Pulsed electromagnetic field therapy (PEMF) is a medical treatment that purportedly helps to heal bone tissue reported in a recent NASA study. This method usually employs electromagnetic radiation of different frequencies - ranging from static magnetic fields, through extremely low frequencies (ELF) to higher radio frequencies (RF) administered in pulses.
Effects on the human body.
Extremely low frequency RF.
High-power extremely low frequency RF with electric field levels in the low kV/m range are known to induce perceivable currents within the human body that create an annoying tingling sensation. These currents will typically flow to ground through a body contact surface such as the feet, or arc to ground where the body is well insulated.
Microwaves.
Microwave exposure at low-power levels below the Specific absorption rate set by government regulatory bodies are considered harmless non-ionizing radiation and have no effect on the human body. However, levels above the Specific absorption rate set by the U.S. Federal Communications Commission are considered potentially harmful. See, Mobile phone radiation and health
Long-term exposure to high-levels of microwaves, is recognized, from experimental animal studies and epidemiological studies in humans, to cause cataracts. The mechanism is unclear but may include changes in heat sensitive enzymes that normally protect cell proteins in the lens. Another mechanism that has been advanced is direct damage to the lens from pressure waves induced in the aqueous humor. 
High-power exposure to microwave RF is known to create a range of effects from lower to higher power levels, ranging from unpleasant burning sensation on the skin and Microwave auditory effect, to extreme pain at the mid-range, to physical burning and blistering of skin and internals at high power levels. Also, see Microwave burn.
General RF exposure.
Canadian safety code 6, also, recommends electric field limits of 100 kV/m for pulsed EMF to prevent air breakdown and spark
discharges. Additional rational for EMF restrictions is to avoid auditory effect and energy-induced unconsciousness in rats.
Also, See Electromagnetic radiation and health. 
For high-power RF exposure see radiation burn.
For low-power RF exposure see Radiation-induced cancer.
As a weapon.
A heat ray is a RF harassment device that makes use of microwave radio frequencies to create an unpleasant heating effect in the upper layer of the skin. A publicly known heat ray weapon called the Active Denial System was developed by the US military as an experimental weapon to deny the enemy access to an area. Also, see death ray which is a heat ray weapon that delivers electromagnetic energy at levels that injure human tissue. The inventor of the death ray, Harry Grindell Matthews, claims to have lost sight in his left eye while developing his death ray weapon based on a primitive microwave magnetron from the 1920s. (Note that a typical microwave oven induces a tissue damaging cooking effect inside the oven at about 2 kV/m.)
Measurement.
Since radio frequency radiation has both an electric and a magnetic component, it is often convenient to express intensity of radiation field in terms of units specific to each component. The unit "volts per meter" (V/m) is used for the electric component, and the unit "amperes per meter" (A/m) is used for the magnetic component. One can speak of an electromagnetic field, and these units are used to provide information about the levels of electric and magnetic field strength at a measurement location.
Another commonly used unit for characterizing an RF electromagnetic field is "power density". Power density is most accurately used when the point of measurement is far enough away from the RF emitter to be located in what is referred to as the far field zone of the radiation pattern. In closer proximity to the transmitter, i.e., in the "near field" zone, the physical relationships between the electric and magnetic components of the field can be complex, and it is best to use the field strength units discussed above. Power density is measured in terms of power per unit area, for example, milliwatts per square centimeter (mW/cm²). When speaking of frequencies in the microwave range and higher, power density is usually used to express intensity since exposures that might occur would likely be in the far field zone.

</doc>
<doc id="42853" url="http://en.wikipedia.org/wiki?curid=42853" title="Partitive case">
Partitive case

The partitive case (abbreviated PTV or more ambiguously PART) is a grammatical case which denotes "partialness", "without result", or "without specific identity". It is also used in contexts where a subgroup is selected from a larger group, or with numbers.
Finnish.
In the Finnic languages, such as Finnish and Estonian, this case is often used to express unknown identities and irresultative actions. For example, it is found in the following circumstances, with the characteristic ending of "-a" or "-ta":
Where not mentioned, the accusative case would be ungrammatical. For example, the partitive must always be used after singular numerals.
As an example of the irresultative meaning of the partitive, "ammuin karhun" (accusative) means "I shot the bear (dead)", whereas "ammuin karhua" (partitive) means "I shot (at) the bear" without specifying if it was even hit. Notice that Finnish has no native future tense, so that the partitive provides an important reference to the present as opposed to the future. Thus "luen kirjaa" means "I am reading a/the book" whereas "luen kirjan" means I will read a/the book". Thus "luen" can mean "I am reading" or "I will read" depending on the case form of the word that follows. The partitive form "kirjaa" indicates incompleted action and hence the meaning of the verb form is present tense. The accusative form "kirjan" indicates completed action when used with the past tense verb but indicates planned future action when used with a verb in the present tense. Hence "luen kirjan" means "I will read the book".
The case with an unspecified identity is "onko teillä kirjoja", which uses the partitive, because it refers to unspecified books, as contrasted to nominative "onko teillä (ne) kirjat?", which means "do you have (those) books?"
The partitive case comes from the older ablative case. This meaning is preserved e.g. in "kotoa" (from home), "takaa" (from behind), where it means "from".
A Western Finnish dialectal phenomenon seen in some dialects is the assimilation of the final "-a" into a preceding vowel, thus making the chroneme the partitive marker. For example, "suurii" → "suuria" "some big --".
Sámi.
Of the Sámi languages, Inari and Skolt Sámi still have a partitive, although it is slowly disappearing and its function is being taken over by other cases.
Skolt Sámi.
The partitive is used only in the singular and can always be replaced by the genitive. The partitive marker is "-d".
1. It appears after numbers larger than 6:
This can be replaced with "kää´uc čâustõõǥǥ".
2. It is also used with certain postpositions:
This can be replaced with "kuä´đ vuâstta".
3. It can be used with the comparative to express that which is being compared:
This would nowadays more than likely be replaced by "pue´rab ko kå´ll"

</doc>
<doc id="42854" url="http://en.wikipedia.org/wiki?curid=42854" title="Absolutive case">
Absolutive case

The absolutive case (abbreviated ABS) is the unmarked grammatical case of a core argument of a verb (generally other than the nominative) which is used as the citation form of a noun.
In ergative languages.
In ergative–absolutive languages, the absolutive is the case used to mark both the subject of an intransitive verb and the object of a transitive verb, in addition to being used for the citation form of a noun. It contrasts with the marked ergative case, which marks the subject of a transitive verb.
For example, in Basque the noun "mutil" ("boy") takes the bare singular article "-a" both as subject of the intransitive clause "mutila etorri da" ("the boy came") and as object of the transitive clause "Irakasleak mutila ikusi du" ("the teacher has seen the boy"), in which the subject bears the ergative ending "-a-k".
In a very few cases, a marked absolutive has been reported. This includes Nias and Sochiapam Chinantec.
In marked-nominative languages.
In nominative–absolutive languages, also called "marked-nominative" languages, the nominative has a case inflection, while the accusative and citation form do not. The unmarked accusative/citation form may be called absolutive to clarify that the citation form is used for the accusative case role rather than for the nominative, which it is in most nominative–accusative languages.
In tripartite languages.
In tripartite languages, both the agent and object of a transitive clause have case forms, ergative and accusative, whereas the agent of an intransitive clause is the unmarked citation form. This is occasionally called the intransitive case, but "absolutive" is also used and is perhaps more accurate, since it is not limited to core agents of intransitive verbs.
In accusative languages.
In nominative–accusative languages, both core cases may be marked, but not infrequently only the accusative is. In such situations the term 'absolutive' would aptly describe the nominative, but the term is seldom used that way.

</doc>
