<doc id="46023" url="http://en.wikipedia.org/wiki?curid=46023" title="Speaker of the United States House of Representatives">
Speaker of the United States House of Representatives

The Speaker of the House is the presiding officer of the chamber. The office was established in 1789 by Article I, of the United States Constitution, which states in part, "The House of Representatives shall choose their Speaker..." The current Speaker is John Boehner, a Republican who represents Ohio's 8th congressional district. The Constitution does not require that the Speaker be an elected House Representative, though all Speakers have been an elected Member of Congress.
The Speaker is second in the United States presidential line of succession, after the Vice President and ahead of the President "pro tempore" of the U.S. Senate. Unlike some Westminster system parliaments, in which the office of Speaker is considered non-partisan, in the United States, the Speaker of the House is a leadership position and the office-holder actively works to set the majority party's legislative agenda. The Speaker usually does not personally preside over debates, instead delegating the duty to members of the House from the majority party. The Speaker usually does not participate in debate and rarely votes.
Aside from duties relating to heading the House and the majority political party, the Speaker also performs administrative and procedural functions, and represents his or her Congressional district.
Selection.
The House of Representatives elects the Speaker of the House on the first day of every new Congress and in the event of the death or resignation of an incumbent Speaker. The Clerk of the House of Representatives requests nominations: there are normally two, one from each major party (each party having previously met to decide on its nominee). The Clerk then calls the roll of the Representatives-elect, each Representative-elect indicating the surname of the candidate he or she is supporting. Representatives-elect are not restricted to voting for one of the nominated candidates and may vote for any person, even for someone who is not a member (or member-elect) of the House at all. They may also abstain by voting "present".
To be elected as Speaker, a candidate must receive an absolute majority of all votes cast for individuals, i.e. excluding those who abstain. If no candidate wins such a majority, then the roll call is repeated until a Speaker is elected. The last time repeated votes were required was in 1923, when the Speaker was elected on the ninth ballot.
The new Speaker is then sworn in by the Dean of the United States House of Representatives, the chamber's longest-serving member.
In modern practice, the Speaker is chosen by the majority party from among its senior leaders either when a vacancy in the office arrives or when the majority party changes. It is usually obvious within two or three weeks of a House election who the new Speaker will be. Previous Speakers have been minority leaders (when the majority party changes, as they are already the House party leader, and as the minority leader are usually their party's nominee for Speaker), or majority leaders (upon departure of the current Speaker in the majority party), assuming that the party leadership hierarchy is followed. In the past, other candidates have included chairpersons of influential standing committees.
So far, the Democrats have always elevated their minority leader to the Speakership upon reclaiming majority control of the House. However, Republicans have not always followed this leadership succession pattern. In 1919, Republicans bypassed James Robert Mann, R-IL, who had been Minority Leader for eight years, and elected a backbencher representative, Frederick H. Gillett, R-MA, to be Speaker. Mann had "angered many Republicans by objecting to their private bills on the floor" and was also a protégé of autocratic Speaker Joseph Cannon, R-IL (1903–1911), and many members "suspected that he would try to re-centralize power in his hands if elected Speaker." More recently, although Robert H. Michel was Minority Leader in 1994 when the Republicans regained control of the House in the 1994 midterm elections, he had already announced his retirement and had little or no involvement in the campaign. Including the "Contract with America", which was unveiled six weeks before Election Day. Michel opted not to seek re-election because he had been isolated in the caucus by Minority Whip Newt Gingrich and other younger and more aggressive Congressmen.
It is expected that members of the House vote for their party's candidate. If they do not, they usually vote for someone else in their party or vote "present". Those who vote for the other party's candidate often face serious consequences, up to and including the loss of seniority. The last instance where a representative voted for the other party's candidate was in 2000, when Democrat Jim Traficant of Ohio voted for Republican Dennis Hastert. In response, the Democrats stripped him of his seniority and he lost all of his committee posts.
If the Speaker's party loses control of the House in an election, and if the Speaker and Majority Leader both remain in the leadership hierarchy, they would become the Minority Leader and Minority Whip, respectively. As the minority party has one less leadership position after losing the Speaker's chair, there may be a contest for the remaining leadership positions. Most Speakers whose party has lost control of the House have not returned to the party leadership (Tom Foley lost his seat, Dennis Hastert returned to the backbenches and resigned from the House in late 2007). However, Speakers Joseph William Martin, Jr. and Sam Rayburn did seek the post of Minority Leader in the late 1940s and early 1950s. Nancy Pelosi is the most recent example of an outgoing Speaker who was elected Minority Leader, after the Democrats lost control of the House in the 2010 elections.
History.
The first Speaker was Frederick Muhlenberg, who was elected as a Federalist for the first four Congresses.
The position of Speaker started to gain its partisan role and its power in legislative development under Henry Clay (1811–1814, 1815–1820, and 1823–1825). In contrast to many of his predecessors, Clay participated in several debates, and used his influence to procure the passage of measures he supported—for instance, the declaration of the War of 1812, and various laws relating to Clay's "American System". Furthermore, when no candidate received an Electoral College majority in the 1824 presidential election causing the President to be elected by the House, Speaker Clay threw his support to John Quincy Adams instead of Andrew Jackson, thereby ensuring Adams' victory. Following Clay's retirement in 1825, the power of the Speakership once again began to decline, despite Speakership elections becoming increasingly bitter. As the Civil War approached, several sectional factions nominated their own candidates, often making it difficult for any candidate to attain a majority. In 1855 and again in 1859, for example, the contest for Speaker lasted for two months before the House achieved a result. During this time, Speakers tended to have very short tenures. For example, from 1839 to 1863 there were eleven Speakers, only one of whom served for more than one term. To date, James K. Polk is the only Speaker of the House later elected President of the United States.
Towards the end of the 19th century, the office of Speaker began to develop into a very powerful one. At the time, one of the most important sources of the Speaker's power was his position as Chairman of the Committee on Rules, which, after the reorganization of the committee system in 1880, became one of the most powerful standing committees of the House. Furthermore, several Speakers became leading figures in their political parties; examples include Democrats Samuel J. Randall, John Griffin Carlisle, and Charles F. Crisp, and Republicans James G. Blaine, Thomas Brackett Reed, and Joseph Gurney Cannon.
The power of the Speaker was greatly augmented during the tenure of the Republican Thomas Brackett Reed (1889–1891, 1895–1899). "Czar Reed", as he was called by his opponents, sought to end the obstruction of bills by the minority, in particular by countering the tactic known as the "disappearing quorum". By refusing to vote on a motion, the minority could ensure that a quorum would not be achieved, and that the result would be invalid. Reed, however, declared that members who were in the chamber but refused to vote would still count for the purposes of determining a quorum. Through these and other rulings, Reed ensured that the Democrats could not block the Republican agenda.
The Speakership reached its apogee during the term of Republican Joseph Gurney Cannon (1903–1911). Cannon exercised extraordinary control over the legislative process. He determined the agenda of the House, appointed the members of all committees, chose committee chairmen, headed the Rules Committee, and determined which committee heard each bill. He vigorously used his powers to ensure that Republican proposals were passed by the House. In 1910, however, Democrats and several dissatisfied Republicans joined together to strip Cannon of many of his powers, including the ability to name committee members and his chairmanship of the Rules Committee. Fifteen years later, Speaker Nicholas Longworth restored much, but not all, of the lost influence of the position.
One of the most influential Speakers in history was Democrat Sam Rayburn. Rayburn was the longest-serving Speaker in history, holding office from 1940 to 1947, 1949 to 1953, and 1955 to 1961. He helped shape many bills, working quietly in the background with House committees. He also helped ensure the passage of several domestic measures and foreign assistance programs advocated by Presidents Franklin D. Roosevelt and Harry Truman. Rayburn's successor, Democrat John William McCormack (served 1962–1971), was a somewhat less influential speaker, particularly because of dissent from younger members of the Democratic Party. During the mid-1970s, the power of the Speakership once again grew under Democrat Carl Albert. The Committee on Rules ceased to be a semi-independent panel, as it had been since 1910. Instead, it once again became an arm of the party leadership. Moreover, in 1975, the Speaker was granted the authority to appoint a majority of the members of the Rules Committee. Meanwhile, the power of committee chairmen was curtailed, further increasing the relative influence of the Speaker.
Albert's successor, Democrat Tip O'Neill, was a prominent Speaker because of his public opposition to the policies of President Ronald Reagan. O'Neill is the longest continually serving Speaker, from 1977 through 1987. He challenged Reagan on domestic programs and on defense expenditures. Republicans made O'Neill the target of their election campaigns in 1980 and 1982 but Democrats managed to retain their majorities in both years.
The roles of the parties reversed in 1994 when, after spending forty years in the minority, the Republicans regained control of the House with the "Contract with America", an idea spearheaded by Minority Whip Newt Gingrich. Speaker Gingrich would regularly clash with Democratic President Bill Clinton, leading to the United States federal government shutdown of 1995 and 1996, in which Clinton was largely seen to have prevailed. Gingrich's hold on the leadership was weakened significantly by that and several other controversies, and he faced a caucus revolt in 1997. After the Republicans lost House seats in 1998 (although retaining a majority) he did not stand for a third term as Speaker. His successor, Dennis Hastert, had been chosen as a compromise candidate, since the other Republicans in the leadership were more controversial. Hastert played a much less prominent role than other contemporary Speakers, being overshadowed by House Majority Leader Tom DeLay and President George W. Bush. The Republicans came out of the 2000 elections with a further reduced majority but made small gains in 2002 and 2004. The periods of 2001-2002 and 2003-2007 were the first times since 1953-1955 that there was single-party Republican leadership in Washington, interrupted from 2001-2003 as Senator Jim Jeffords of Vermont left the Republican Party to become independent and caucused with Senate Democrats to give them a 51-49 majority.
In the 2006 midterm elections, the Democrats won a majority in the House. Nancy Pelosi became Speaker when the 110th Congress convened on January 4, 2007, making her the first female to hold the office. With the election of Barack Obama as President and Democratic gains in both houses of Congress, Pelosi became the first Speaker since Tom Foley to hold the office during single-party Democratic leadership in Washington. During the 111th Congress, Pelosi was the driving force behind several of Obama's major initiatives that proved controversial, and the Republicans campaigned against the Democrats' legislation by staging a "Fire Pelosi" bus tour and regained control of the House in the 2010 midterm elections. House Minority Leader John Boehner was elected as Speaker.
Notable elections.
Historically, there have been several controversial elections to the Speakership, such as the contest of 1839. In that case, even though the 26th United States Congress convened on December 2, the House could not begin the Speakership election until December 14 because of an election dispute in New Jersey known as the "Broad Seal War". Two rival delegations, one Whig and the other Democrat, had been certified as elected by different branches of the New Jersey government. The problem was compounded by the fact that the result of the dispute would determine whether the Whigs or the Democrats held the majority. Neither party agreed to permit a Speakership election with the opposite party's delegation participating. Finally, it was agreed to exclude both delegations from the election and a Speaker was finally chosen on December 17.
Another, more prolonged fight occurred in 1855 in the 34th United States Congress. The old Whig Party had collapsed but no single party had emerged to replace it. Candidates opposing the Democrats had run under a bewildering variety of labels, including Whig, Republican, American (Know Nothing), and simply "Opposition". By the time Congress actually met in December 1855, most of the northerners were concentrated together as Republicans, while most of the southerners and a few northerners used the American or Know Nothing label. Opponents of the Democrats held a majority in House, with the party makeup of the 234 Representatives being 83 Democrats, 108 Republicans, and 43 Know Nothings (primarily southern oppositionists). The Democratic minority nominated William Alexander Richardson of Illinois as Speaker, but because of sectional distrust, the various oppositionists were unable to agree on a single candidate for Speaker. The Republicans supported Nathaniel Prentiss Banks of Massachusetts, who had been elected as a Know Nothing but was now largely identified with the Republicans. The southern Know Nothings supported first Humphrey Marshall of Kentucky, and then Henry M. Fuller of Pennsylvania. The voting went on for almost two months with no candidate able to secure a majority, until it was finally agreed to elect the Speaker by plurality vote, and Banks was elected. The House found itself in a similar dilemma when the 36th Congress met in December 1859. Although the Republicans held a plurality, the Republican candidate, John Sherman, was unacceptable to southern oppositionists due to his anti-slavery views, and once again the House was unable to elect a Speaker for several months. After Democrats allied with southern oppositionists to nearly elect the North Carolina oppositionist William N. H. Smith, Sherman finally withdrew in favor of compromise candidate William Pennington of New Jersey, a former Whig of unclear partisan loyalties, who was finally elected Speaker at the end of January 1860.
The last Speakership elections in which the House had to vote more than once occurred in the 65th and 72nd United States Congress. In 1917, neither the Republican nor the Democratic candidate could attain a majority because three members of the Progressive Party and other individual members of other parties voted for their own party. The Republicans had a plurality in the House but James "Champ" Clark remained Speaker of the House because of the support of the Progressive Party members. In 1931, both the Republicans and the Democrats had 217 members with the Minnesota Farmer-Labor Party having one member who served as the deciding vote. The Farmer-Labor Party eventually voted for the Democrats' candidate for Speaker, John Nance Garner, who later became Vice President under Franklin Roosevelt.
In 1997, several Republican congressional leaders tried to force Speaker Newt Gingrich to resign. However, Gingrich refused since that would have required a new election for Speaker, which could have led to Democrats along with dissenting Republicans voting for Democrat Dick Gephardt (then Minority Leader) as Speaker. After the 1998 midterm elections where the Republicans lost seats, Gingrich did not stand for re-election. The next two figures in the House Republican leadership hierarchy, Majority Leader Richard Armey and Majority Whip Tom DeLay, chose not to run for the office. The chairman of the House Appropriations Committee, Bob Livingston, declared his bid for the Speakership, which was unopposed, making him Speaker-designate. It was then revealed, by Livingston himself, who had been publicly critical of President Bill Clinton's perjury during his sexual harassment trial, that he had engaged in an extramarital affair. He opted to resign from the House, despite being urged to stay on by House Democratic leader Gephardt. Subsequently, chief deputy whip Dennis Hastert was selected as Speaker. The Republicans retained their majorities in the 2000, 2002, and 2004 elections.
The Democrats won a majority of seats in the 2006 midterm elections. On November 16, 2006, Nancy Pelosi, who was then Minority Leader, was selected as Speaker-designate by House Democrats. When the 110th Congress convened on January 4, 2007, she was elected as the 60th Speaker by a vote of 233-202, becoming the first woman elected Speaker of the House. Pelosi remained Speaker through the 111th Congress. For the 112th Congress, Republican John Boehner was unanimously designated Speaker-designate by House Republicans and was elected the 61st Speaker of the House. As a show of dissent, nineteen Democratic representatives voted for Democrats other than Pelosi, who had been chosen as House Minority Leader and the Democrats' candidate for Speaker.
Partisan role.
The Constitution does not spell out the political role of the Speaker. As the office has developed historically, however, it has taken on a clearly partisan cast, very different from the Speakership of most Westminster-style legislatures, such as the Speaker of the British House of Commons, which is meant to be scrupulously non-partisan. The Speaker in the United States, by tradition, is the head of the majority party in the House of Representatives, outranking the Majority Leader. However, despite having the right to vote, the Speaker usually does not participate in debate and rarely votes.
The Speaker is responsible for ensuring that the House passes legislation supported by the majority party. In pursuing this goal, the Speaker may use his or her power to determine when each bill reaches the floor. They also chair the majority party's steering committee in the House. While the Speaker is the functioning head of the House majority party, the same is not true of the President "pro tempore" of the Senate, whose office is primarily ceremonial and honorary.
When the Speaker and the President belong to the same party, the Speaker tends to play the role in a more ceremonial light, as seen when Dennis Hastert played a very low-key role during the presidency of fellow Republican George W. Bush. Nevertheless, there are times when the Speaker plays a much larger role if the President is a fellow member of their party, and thus, the Speaker is tasked with pushing through the agenda of the majority party, often at the expense of the minority opposition. This can be seen, most of all, in the Speakership of Democratic-Republican Henry Clay, who personally ensured the presidential victory of fellow Democratic-Republican John Quincy Adams. On the other side of the aisle, Democrat Sam Rayburn was a key player in the passing of New Deal legislation under the presidency of fellow Democrat Franklin Delano Roosevelt. Republican Joseph Gurney Cannon (under Theodore Roosevelt) was particularly infamous for his marginalization of the minority Democrats and centralizing of authority to the Speakership. In more recent times, Speaker Nancy Pelosi played a role in continuing the push for health care reform during the presidency of fellow Democrat Barack Obama. The Republicans campaigned against Pelosi and the Democrats' legislation with their "Fire Pelosi" bus tour.
On the other hand, when the Speaker and the President belong to opposite parties, the public role and influence of the Speaker tend to increase. As the highest-ranking member of the opposition party (and in effect a "de facto" Leader of the Opposition), the Speaker is normally the chief public opponent of the President's agenda. In this scenario, the Speaker is known for undercutting the President's agenda by blocking measures by the minority party or rejecting bills by the Senate. One famous instance came in the form of Thomas Brackett Reed (under Grover Cleveland), a Speaker notorious for his successful attempt to force the Democrats to vote on measures where the Republicans had clear majorities, which ensured that Cleveland's Democrats were in no position to challenge the Republicans in the House. Joseph Cannon was particularly unique in that he led the conservative "Old Guard" wing of the Republican Party, while his President - Theodore Roosevelt - was of the more progressive clique, and more than just marginalizing the Democrats, Cannon used his power to punish the dissidents in his party and obstruct the progressive wing of the Republican Party.
More modern examples include Tip O'Neill, who was a vocal opponent of President Ronald Reagan's economic and defense policies; Newt Gingrich, who fought a bitter battle with President Bill Clinton for control of domestic policy; Nancy Pelosi, who argued with President George W. Bush over the Iraq War; and current Speaker John Boehner, who clashes with President Barack Obama over budget issues and health care.
Presiding officer.
As presiding office of the House of Representatives, the Speaker holds a variety of powers over the House and is the highest-ranking legislative official in the US government. The Speaker may delegate his powers to member of the House to act as Speaker "pro tempore" and preside over the House in the Speaker's absence, but it has always been a member of the same party. During important debates, the Speaker "pro tempore" is ordinarily a senior member of the majority party who may be chosen for his or her skill in presiding. At other times, more junior members may be assigned to preside to give them experience with the rules and procedures of the House. The Speaker may also designate a Speaker "pro tempore" for special purposes, such as designating a Representative whose district is near Washington, DC to sign enrolled bills during long recesses.
On the floor of the House, the presiding officer is always addressed as "Mister Speaker" or "Madam Speaker," even if it is a Speaker "pro tempore", and not the Speaker. When the House resolves itself into a Committee of the Whole, the Speaker designates a member to preside over the Committee as the Chairman, who is addressed as "Mister Chairman" or "Madam Chairwoman." To speak, members must seek the presiding officer's recognition. The presiding officer may call on members as they please, and may therefore control the flow of debate. The presiding officer also rules on all points of order but such rulings may be appealed to the whole House. The Speaker is responsible for maintaining decorum in the House and may order the Sergeant-at-Arms to enforce House rules.
The Speaker's powers and duties extend beyond presiding in the chamber. In particular, the Speaker has great influence over the committee process. The Speaker selects nine of the thirteen members of the powerful Committee on Rules, subject to the approval of the entire majority party. The leadership of the minority party chooses the remaining four members. Furthermore, the Speaker appoints all members of select committees and conference committees. Moreover, when a bill is introduced, the Speaker determines which committee will consider it. As a member of the House, the Speaker is entitled to participate in debate and to vote but, by custom, only does so in exceptional circumstances. Ordinarily, the Speaker votes only when his or her vote would be decisive or on matters of great importance, such as constitutional amendments or major legislation.
Other functions.
Because joint sessions and joint meetings of Congress are held in the House chamber, the Speaker presides over all such joint sessions and meetings. However, the Twelfth Amendment and #Redirect require that the President of the Senate preside over joint sessions of Congress assembled to count electoral votes and to certify the results of a presidential election.
The Speaker is also responsible for overseeing the officers of the House: the Clerk, the Sergeant-at-Arms, the Chief Administrative Officer, and the Chaplain. The Speaker can dismiss any of these officers. The Speaker appoints the House Historian and the General Counsel and, jointly with the Majority and Minority Leaders, appoints the House Inspector General.
The Speaker is second in the presidential line of succession, immediately after the Vice President, under the Presidential Succession Act of 1947. The Speaker is followed in the line of succession by the President "pro tempore" of the Senate and by the heads of federal executive departments. Some scholars argue that this provision of the succession statute is unconstitutional.
To date, the implementation of the Presidential Succession Act has never been necessary and no Speaker has ever acted as President. Implementation of the law almost became necessary in 1973 after the resignation of Vice President Spiro Agnew. At the time, many believed that President Richard Nixon would resign because of the Watergate scandal, allowing Speaker Carl Albert to succeed to the Presidency. However, before he resigned, Nixon appointed Gerald Ford as Vice President in accordance with the Twenty-fifth Amendment. Nevertheless, the United States government takes the Speaker's place in the line of succession seriously enough that, for example, since shortly after the terrorist attacks of September 11, 2001, Speakers used military jets to fly back and forth to their districts and for other travel until Speaker Boehner discontinued the practice in 2011. The Speaker of the House is one of the officers to whom declarations of presidential inability or ability to resume the Presidency must be addressed under the Twenty-fifth Amendment.
List of Speakers of the United States House of Representatives.
It includes the congressional district and political affiliation of each speaker as well as the number of their Congress and time they spent in the position.
"* Note: Banks, a former Democrat originally elected as a Know Nothing, had come to be associated with the Republicans by the time the 34th Congress convened. Because the Republicans did not command a majority in Congress, and Banks did not receive any votes from Democrats or southern Know Nothings, Banks, after two months of deadlocked balloting, could only be elected after a motion was passed allowing the election of a speaker by plurality vote."
List of Speakers by time in office.
This list is based on the difference between dates; if counted by number of calendar days all the figures would be one greater. Time after adjournment of one Congress but before the convening of the next Congress is not counted. For example, Nathaniel Macon was Speaker in both the 8th and 9th Congresses, but the eight-month gap between the two Congresses is not counted toward his service.
Sam Rayburn is the only person to have served as Speaker of the House for more than ten years.
Theodore M. Pomeroy served as Speaker of the House for one day after Speaker Schuyler Colfax resigned to become Vice President of the United States; Pomeroy's term as a Member of Congress ended the next day.
Sam Rayburn, Henry Clay, Thomas Brackett Reed, Joseph William Martin, Jr., Frederick Muhlenberg, and John W. Taylor are the only Speakers of the House to have ever served in non-consecutive Congresses (i.e. another Speaker served in between each tenure).
List of living former Speakers.
Since the death of former Speaker Jim Wright on May 6, 2015, there are now three living former Speakers.
Recent election results.
To be elected as Speaker, a candidate must receive an absolute majority of all votes cast for individuals, excluding those who abstain.
Speaker of the United States House of Representatives election, 2007.
Source: Office of the Clerk of the U.S. House of Representatives. January 4, 2007.
Speaker of the United States House of Representatives election, 2009.
Source: Office of the Clerk of the U.S. House of Representatives. January 6, 2009.
Speaker of the United States House of Representatives election, 2011.
Source: Office of the Clerk of the U.S. House of Representatives. January 5, 2011.
Speaker of the United States House of Representatives election, 2013.
Source: Office of the Clerk of the U.S. House of Representatives. January 3, 2013.
Speaker of the United States House of Representatives election, 2015.
Source: Office of the Clerk of the U.S. House of Representatives. January 6, 2015.

</doc>
<doc id="46024" url="http://en.wikipedia.org/wiki?curid=46024" title="Abraham Robinson">
Abraham Robinson

Abraham Robinson (born "Robinsohn"; October 6, 1918 – April 11, 1974) was a mathematician who is most widely known for development of non-standard analysis, a mathematically rigorous system whereby infinitesimal and infinite numbers were reincorporated into modern mathematics.
Biography.
He was born to a Jewish family with strong Zionist beliefs, in Waldenburg, Germany, which is now Wałbrzych, in Poland. In 1933, he emigrated to British Mandate of Palestine, where he earned a first degree from the Hebrew University. Robinson was in France when the Nazis invaded during World War II, and escaped by train and on foot, being alternately questioned by French soldiers suspicious of his German passport and asked by them to share his map, which was more detailed than theirs. While in London, he joined the Free French Air Force and contributed to the war effort by teaching himself aerodynamics and becoming an expert on the airfoils used in the wings of fighter planes.
After the war, Robinson worked in London, Toronto, and Jerusalem, but ended up at University of California, Los Angeles in 1962.
Work in model theory.
He became known for his approach of using the methods of mathematical logic to attack problems in analysis and abstract algebra. He "introduced many of the fundamental notions of model theory". Using these methods, he found a way of using formal logic to show that there are self-consistent nonstandard models of the real number system that include infinite and infinitesimal numbers. Others, such as Wilhelmus Luxemburg, showed that the same results could be achieved using ultrafilters, which made Robinson's work more accessible to mathematicians who lacked training in formal logic. Robinson's book "Non-standard Analysis" was published in 1966. Robinson was strongly interested in the history and philosophy of mathematics, and often remarked that he wanted to get inside the head of Leibniz, the first mathematician to attempt to articulate clearly the concept of infinitesimal numbers.
While at UCLA his colleagues remember him as working hard to accommodate PhD students of all levels of ability by finding them projects of the appropriate difficulty. He was courted by Yale, and after some initial reluctance, he moved there in 1967. He died of pancreatic cancer in 1974.

</doc>
<doc id="46025" url="http://en.wikipedia.org/wiki?curid=46025" title="Omnipotence paradox">
Omnipotence paradox

The omnipotence paradox is a family of semantic paradoxes which address two general issues and three specific issues:
The omnipotence paradox states that: "If a being can perform any action, then it should be able to create a task which this being is unable to perform; hence, this being cannot perform all actions. Yet, on the other hand, if this being cannot create a task that it is unable to perform, then there exists something it cannot do."
One version of the omnipotence paradox is the so-called "paradox of the stone": "Could an omnipotent being create a stone so heavy that even he could not lift it?" If he could lift the rock, then it seems that the being would not have been omnipotent to begin with in that he would have been incapable of creating a heavy enough stone; if he could not lift the stone, then it seems that the being either would never have been omnipotent to begin with or would have ceased to be omnipotent upon his creation of the stone. 
The argument is medieval, dating at least to the 12th century, addressed by Averroës (1126–1198) and later by Thomas Aquinas. Pseudo-Dionysius the Areopagite (before 532) has a predecessor version of the paradox, asking whether it is possible for God to "deny himself". 
Several answers to the paradox have been proposed.
Overview.
A common modern version of the omnipotence paradox is expressed in the question: "Can [an omnipotent being] create a stone so heavy that it cannot lift it?" This question generates a dilemma. The being can either create a stone which it cannot lift, or it cannot create a stone which it cannot lift. If the being "can" create a stone that it cannot lift, then it seems that it can cease to be omnipotent. If the being "cannot" create a stone which it cannot lift, then it seems it is already not omnipotent. But, if the question is inherently required by the concept of omnipotence, then it seems the logic which allows it to be inherently required is a paradox since the particular concept of omnipotence which requires it is a paradox. In short, the act of seeming to find omnipotence to be a contradiction-of-terms is founded on the act of conceiving something against which to construct the contradiction: prior to any ‘act’, omnipotence is conceived as coherent both with itself and with the possibility of knowledge (which raises the question of what is the knowledge that constitutes the identifiability of omnipotence-as-a-paradox?).
But, whether the concept of omnipotence itself is a material paradox, or is simply too obscure to us to preclude being construed by paradoxical thinking, the central issue of the "omnipotence paradox" is whether the concept of the 'logically possible' is different for a world in which omnipotence exists from a world in which omnipotence does not exist. The reason this is the central issue is because our sense of material paradox, and of the logical contradiction of which material paradox is an expression, are functions of the fact that we presuppose that there must be something which exists which is inherently meaningful or logical, that is, which is "concretely" not a compound of other things or other concepts. So, for example, in a world in which exists a materially paradoxical omnipotence, its very paradoxicality seems either to be a material-paradox-of-a-material-paradox, or to be a non-paradox per the proposition that it exists (i.e., if it exists, then nothing has inherent meaning, including itself). Whereas, a world in which exists non-paradoxical omnipotence, its own omnipotence is coextensive with whatever is the concrete basis of our presupposition that something must be inherently meaningful.
The dilemma of omnipotence is similar to another classic paradox, the irresistible force paradox: "What happens when an irresistible force meets an immovable object?" One response to this paradox is that if a force is irresistible, then, by definition, there is no truly immovable object; conversely, if an immovable object were to exist, then no force could be defined as being truly irresistible. Some claim that the only way out of this paradox is if the irresistible force and the immovable object never meet. But, this way out is not possible in the omnipotence case, because the purpose is to ask if the being's own inherent omnipotence makes its own inherent omnipotence impossible. Moreover, an object cannot in principle be immovable, if there is a force which may move it, regardless of whether the force and the object never meet. So, while, prior to any task, it is easy to imagine that omnipotence is in state of coherence with itself, some imaginable tasks are not possible for such a coherent omnipotence to perform without compromising its coherence.
Types of omnipotence.
Peter Geach describes and rejects four levels of omnipotence. He also defines and defends a lesser notion of the "almightiness" of God. 
St Augustine in his City of God writes "God is called omnipotent on account of His doing what He wills" and thus proposes the definition that "Y is omnipotent" means "If Y wishes to do X then Y can and does do X". 
The notion of omnipotence can also be applied to an entity in different ways. An essentially omnipotent being is an entity that is necessarily omnipotent. In contrast, an accidentally omnipotent being is an entity that can be omnipotent for a temporary period of time, and then becomes non-omnipotent. The omnipotence paradox can be applied to each type of being differently.
Some Philosophers, such as René Descartes, argue that God is absolutely omnipotent. In addition, some philosophers have considered the assumption that a being is either omnipotent or non-omnipotent to be a false dilemma, as it neglects the possibility of varying degrees of omnipotence. Some modern approaches to the problem have involved semantic debates over whether language—and therefore philosophy—can meaningfully address the concept of omnipotence itself.
Proposed answers.
A common response from Christian philosophers, such as Norman Geisler or Richard Swinburne is that the paradox assumes a wrong definition of omnipotence. Omnipotence, they say, does not mean that God can do anything "at all" but, rather, that he can do "anything that's possible according to his nature." The distinction is important. God cannot perform logical absurdities; he cannot, for instance, make 1+1=3. Likewise, God cannot make a being greater than himself because he is, by definition, the greatest possible being. God is limited in his actions to his nature. The Bible supports this, they assert, in passages such as Hebrews 6:18 which says it is "impossible for God to lie." This raises the question, similar to the Euthyphro Dilemma, of where this law of logic, which God is bound to obey, comes from. According to these theologians, this law is not a law above God that he assents to but, rather, logic is an eternal part of God's nature, like his omniscience or omnibenevolence. God obeys the laws of logic because God is eternally logical in the same way that God does not perform evil actions because God is eternally good. So, God, by nature logical and unable to violate the laws of logic, cannot make a boulder so heavy he cannot lift it because that would violate the law of non contradiction by creating an immovable object and an unstoppable force.
Another common response is that since God is supposedly omnipotent, the phrase "could not lift" does not make sense and the paradox is meaningless. This may mean that the complexity involved in rightly understanding omnipotence---contra all the logical details involved in misunderstanding it---is a function of the fact that omnipotence, like infinity, is perceived at all by contrasting reference to those complex and variable things "which it is not". But, an alternative meaning is that a non-corporeal God cannot lift anything, but can raise it (a linguistic pedantry) - or to use the beliefs of Hindus (that there is one God, who can be manifest as several different beings) that whilst it is possible for God to do all things, it is not possible for all his incarnations to do them. As such, God could create a stone so heavy that, in one incarnation, he was unable to lift it - but would be able to do something that an incarnation that could lift it could not.
Other responses claim that the question is sophistry, meaning it makes grammatical sense, but has no intelligible meaning. The lifting a rock paradox ("Can God lift a stone larger than he can carry?") uses human characteristics to cover up the main skeletal structure of the question. With these assumptions made, two arguments can stem from it:
The act of killing oneself is not applicable to an omnipotent being, since, despite that such an act does involve some power, it also involves a "lack" of power: the human person who can kill himself is already not indestructible, and, in fact, every agent constituting his environment is more powerful in some ways than himself. In other words, all non-omnipotent agents are concretely "synthetic": constructed as contingencies of other, smaller, agents, meaning that they, unlike an omnipotent agent, logically can exist not only in multiple instantiation (by being constructed out of the more basic agents of which they are made), but are each bound to a differentiated location in space contra transcendent omnipresence.
Thomas Aquinas asserts that the paradox arises from a misunderstanding of omnipotence. He maintains that inherent contradictions and logical impossibilities do not fall under the omnipotence of God. J. L Cowan sees this paradox as a reason to reject the concept of 'absolute' omnipotence, while others, such as René Descartes, argue that God is absolutely omnipotent, despite the problem.
C. S. Lewis argues that when talking about omnipotence, referencing "a rock so heavy that God cannot lift it" is nonsense just as much as referencing "a square circle"; that it is not logically coherent in terms of power to think that omnipotence includes the power to do the logically impossible. So asking "Can God create a rock so heavy that even he cannot lift it?" is just as much nonsense as asking "Can God draw a square circle?" The logical contradiction here being God's simultaneous ability and disability in lifting the rock: the statement "God can lift this rock" must have a truth value of either true or false, it cannot possess both. This is justified by observing that in order for the omnipotent agent to create such a stone, the omnipotent agent must already be more powerful than itself: such a stone is too heavy for the omnipotent agent to lift, but the omnipotent agent already can create such a stone; If an omnipotent agent already is more powerful than itself, then it already is just that powerful. Which means that its power to create a stone that’s too heavy for it to lift is identical to its power to lift that very stone. While this doesn’t quite make complete sense, Lewis wished to stress its implicit point: that even within the attempt to prove that the concept of omnipotence is immediately incoherent, one admits that it is immediately coherent, and that the only difference is that this attempt if forced to admit this despite that the attempt is constituted by a perfectly irrational route to its own unwilling end, with a perfectly irrational set of 'things' included in that end.
In other words, that the 'limit' on what omnipotence 'can' do is not a "limit" on its actual agency, but an "epistemological boundary" without which omnipotence could not be "identified" (paradoxically or otherwise) in the first place. In fact, this process is merely a fancier form of the classic Liar Paradox: If I say, "I am a liar", then how can it be true if I am telling the truth therewith, and, if I am telling the truth therewith, then how can I be a liar? So, to think that omnipotence is an "epistemological" paradox is like failing to recognize that, when taking the statement, 'I am a liar' self-referentially, the statement is reduced to an actual failure to lie. In other words, if one maintains the supposedly 'initial' position that the necessary conception of omnipotence includes the 'power' to compromise both itself and all other identity, and if one concludes from this position that omnipotence is epistemologically incoherent, then one implicitly is asserting that one's own 'initial' position is incoherent. Therefore the question (and therefore the perceived paradox) is meaningless. Nonsense does not suddenly acquire sense and meaning with the addition of the two words, "God can" before it. Lewis additionally said that "unless something is self-evident, nothing can be proved", which implies for the debate on omnipotence that, "as in matter, so in the human understanding of truth: it takes no true insight to destroy a perfectly integrated structure, and the effort to destroy has greater effect than an equal effort to build; so, a man is thought a fool who assumes its integrity, and thought an abomination who argues for it. It is easier to teach a fish to swim in outer space than to convince a room full of ignorant fools why it cannot be done." 
John Christian Uy said that it is just the same as someone with double-bladed sword (accidentally omnipotent), or sword and a shield (essentially omnipotent). Therefore, an accidentally omnipotent deity CAN remove its omnipotence while an essentially omnipotent deity CANNOT do anything that would make it non-omnipotent. Both however, have no limitations so far other than the essential omnipotent being who cannot do anything which will make it non-omnipotent like making someone equal with him, lowering or improving himself (for omnipotence is the highest) etc. It could, however, make someone with a great power, though it cannot be 99% because Omnipotence is infinite, because that created being is not equal with him. Overall, God in the Christian Bible, is essentially omnipotent.
In a 1955 article published in the philosophy journal "Mind", J. L. Mackie attempted to resolve the paradox by distinguishing between first-order omnipotence (unlimited power to act) and second-order omnipotence (unlimited power to determine what powers to act things shall have). An omnipotent being with both first and second-order omnipotence at a particular time might restrict its own power to act and, henceforth, cease to be omnipotent in either sense. There has been considerable philosophical dispute since Mackie, as to the best way to formulate the paradox of omnipotence in formal logic.
Another common response to the omnipotence paradox is to try to define omnipotence to mean something weaker than absolute omnipotence, such as definition 3 or 4 above. The paradox can be resolved by simply stipulating that omnipotence does not require the being to have abilities which are logically impossible, but only to be able to do anything which conforms to the laws of logic. A good example of a modern defender of this line of reasoning is George Mavrodes. Essentially, Mavrodes argues that it is no limitation on a being's omnipotence to say that it cannot make a round square. Such a "task" is termed by him a "pseudo-task" as it is self-contradictory and inherently nonsense. Harry Frankfurt—following from Descartes—has responded to this solution with a proposal of his own: that God can create a stone impossible to lift and also lift said stone
For why should God not be able to perform the task in question? To be sure, it is a task—the task of lifting a stone which He cannot lift—whose description is self-contradictory. But if God is supposed capable of performing one task whose description is self-contradictory—that of creating the problematic stone in the first place—why should He not be supposed capable of performing another—that of lifting the stone? After all, is there any greater trick in performing two logically impossible tasks than there is in performing one?
If a being is accidentally omnipotent, then it can resolve the paradox by creating a stone which it cannot lift and thereby becoming non-omnipotent. Unlike essentially omnipotent entities, it is possible for an accidentally omnipotent being to be non-omnipotent. This raises the question, however, of whether or not the being was ever truly omnipotent, or just capable of great power. On the other hand, the ability to voluntarily give up great power is often thought of as central to the notion of the Christian Incarnation.
If a being is essentially omnipotent, then it can also resolve the paradox (as long as we take omnipotence not to require absolute omnipotence). The omnipotent being is essentially omnipotent, and therefore it is impossible for it to be non-omnipotent. Further, the omnipotent being can do what is logically impossible and have no limitations just like the accidentally omnipotent but the ability to make oneself non-omnipotent. The creation of a stone which the omnipotent being cannot lift would be an impossibility. The omnipotent being cannot create such a stone because its power will be equal to him and thus, remove his omnipotence for there can only be one omnipotent being in existence, but nevertheless retains its omnipotence. This solution works even with definition 2, as long as we also know the being is essentially omnipotent rather than accidentally so. However, it is possible for non-omnipotent beings to compromise their own powers, which presents the paradox that non-omnipotent beings can do something (to themselves) which an essentially omnipotent being cannot do (to itself).
This was essentially the position taken by Augustine of Hippo in his "The City of God":
 For He is called omnipotent on account of His doing what He wills, not on account of His suffering what He wills not; for if that should befall Him, He would by no means be omnipotent. Wherefore, He cannot do some things for the very reason that He is omnipotent.
Thus Augustine argued that God could not do anything or create any situation that would in effect make God not God.
Language and omnipotence.
The philosopher Ludwig Wittgenstein is often interpreted as arguing that language is not up to the task of describing the kind of power an omnipotent being would have. In his "Tractatus Logico-Philosophicus" he stays generally within the realm of logical positivism, until claim 6.4, but at 6.41 and following the succeeding propositions argue that ethics and several other issues are "transcendental" subjects which we cannot examine with language. Wittgenstein also mentions the will, life after death, and God; arguing that "When the answer cannot be put into words, neither can the question be put into words".
Wittgenstein's work makes the omnipotence paradox a problem in semantics, the study of how symbols are given meaning. (The retort "That's only semantics" is a way of saying that a statement only concerns the definitions of words, instead of anything important in the physical world.) According to the "Tractatus," then, even attempting to formulate the omnipotence paradox is futile, since language cannot refer to the entities the paradox considers. The final proposition of the "Tractatus" gives Wittgenstein's dictum for these circumstances: "What we cannot speak of, we must pass over in silence". Wittgenstein's approach to these problems is influential among other 20th century religious thinkers such as D. Z. Phillips.
But in his later years, Wittgenstein wrote works which are often interpreted as conflicting with his positions in the "Tractatus", and indeed the later Wittgenstein is mainly seen as the leading critic of the early Wittgenstein.
Other versions of the paradox.
In the 6th century, Pseudo-Dionysius claims that a version of the omnipotence paradox constituted the dispute between St. Paul and Elymas the Magician mentioned in Acts 13:8, but it is phrased in terms of a debate as to whether or not God can "deny himself" ala 2 Tim 2:13. In the 11th century, St. Anselm argues that there are many things that God cannot do, but that nonetheless he counts as omnipotent.
Thomas Aquinas advanced a version of the omnipotence paradox by asking whether God could create a triangle with internal angles that did not add up to 180 degrees. As Aquinas put it in Summa contra Gentiles: 
 Since the principles of certain sciences, such as logic, geometry and arithmetic are taken only from the formal principles of things, on which the essence of the thing depends, it follows that God could not make things contrary to these principles. For example, that a genus was not predicable of the species, or that lines drawn from the centre to the circumference were not equal, or that a triangle did not have three angles equal to two right angles.
This can be done on a sphere, and not on a flat surface. The later invention of non-Euclidean geometry does not resolve this question; for one might as well ask, "If given the axioms of Riemannian geometry, can an omnipotent being create a triangle whose angles "do not" add up to "more" than 180 degrees?" In either case, the real question is whether or not an omnipotent being would have the ability to evade the consequences which follow logically from a system of axioms that the being created.
A version of the paradox can also be seen in non-theological contexts. A similar problem occurs when accessing legislative or parliamentary sovereignty, which holds a specific legal institution to be omnipotent in legal power, and in particular such an institution's ability to regulate itself.
In a sense, the classic statement of the omnipotence paradox — a rock so heavy that its omnipotent creator cannot lift it — is grounded in Aristotelian science. After all, if one considers the stone's position relative to the sun around which the planet orbits, one could hold that the stone is "constantly" being lifted—strained though that interpretation would be in the present context. Modern physics indicates that the choice of phrasing about lifting stones should relate to acceleration; however, this does not in itself of course invalidate the fundamental concept of the generalized omnipotence paradox. However, one could easily modify the classic statement as follows: "An omnipotent being creates a universe which follows the laws of Aristotelian physics. Within this universe, can the omnipotent being create a stone so heavy that the being cannot lift it?"
Ethan Allen's "Reason" addresses the topics of original sin, theodicy and several others in classic Enlightenment fashion. In Chapter 3, section IV, he notes that "omnipotence itself" could not exempt animal life from mortality, since change and death are defining attributes of such life. He argues, "the one cannot be without the other, any more than there could be a compact number of mountains without valleys, or that I could exist and not exist at the same time, or that God should effect any other contradiction in nature." Labeled by his friends a Deist, Allen accepted the notion of a divine being, though throughout "Reason" he argues that even a divine being must be circumscribed by logic.
In "Principles of Philosophy," Descartes tried refuting the existence of atoms with a variation of this argument, claiming God could not create things so indivisible that he could not divide them.
The paradox also appears in popular culture. In an episode of "The Simpsons", Homer asks Ned Flanders the question "Could Jesus microwave a burrito so hot that He Himself could not eat it?" In one strip of the webcomic Saturday Morning Breakfast Cereal, a child is seen asking a priest "Could God make an argument so circular that even He couldn't believe it?"
In the Marvel Comics Runaways, Victor Mancha, the technorganic android created by Ultron, is shown as unable to process correctly paradoxes: as such, it's known that a small number of well known paradoxes may force his logic in a permanent loop, shutting his functions down until someone steps in to give Victor the proper solution. As such, his peers stop him once by asking "Could God make a sandwich so big that even he couldn't finish it?", and reboot his mind by explaining him a simplified version of the "God as essentially omnipotent" solution ("Yes. God could make a sandwich so big that even he couldn't finish it, and eat it all"). 
In the book "Bart Simpson's Guide to Life" this question is phrased as, "If God can do anything, could he create a hot dog so big that even he couldn't eat it?"
References.
Listen to this article ()
This audio file was created from a revision of the "Omnipotence paradox" article dated 2007-09-04, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="46026" url="http://en.wikipedia.org/wiki?curid=46026" title="Wallachia">
Wallachia

 |style="width:1.0em; padding:0 0 0 0.6em;"| - 
 |style="padding-left:0;text-align:left;"| 1310–52
Wallachia or Walachia is a historical and geographical region of Romania. It is situated north of the Danube and south of the Southern Carpathians. Wallachia is traditionally divided into two sections, Muntenia (Greater Wallachia) and Oltenia (Lesser Wallachia). Wallachia as a whole is sometimes referred to as Muntenia through identification with the larger of the two traditional sections.
Wallachia was founded as a principality in the early 14th century by Basarab I, after a rebellion against Charles I of Hungary, although the first mention of the territory of Wallachia west of the river Olt dates to a charter given to the voivode Seneslau in 1246 by Béla IV of Hungary. In 1417, Wallachia accepted the suzerainty of the Ottoman Empire; this lasted until the 19th century, albeit with brief periods of Russian occupation between 1768 and 1854. In 1859, Wallachia united with Moldavia to form the United Principalities, which adopted the name "Romania" in 1866 and officially became the Kingdom of Romania in 1881. Later, Transylvania joined the Kingdom of Romania in 1918, forming the modern Romanian state.
Etymology.
The name Wallachia, generally not used by Romanians themselves (but present in some contexts as Valahia or Vlahia), is derived from the word "walha" used by Germanic peoples to describe Celts, and later romanized Celts and all Romance-speaking people. In northwest Europe this gave rise to Wales, Cornwall, Wallonia, among others, while in Southeast Europe it evolved into the ethnonym Valach, used to designate Germanic speakers' Romance-speaking neighbours, and subsequently taken over by Slavic-speakers to refer to Romanians, with variants such as "Vlach, Blach, Bloc, Bloh, Boloh" etc.—see also: Vlachs.
In the early Middle Ages, in Slavonic texts, the name of Zemli Ungro-Vlahiskoi (Земли Унгро-Влахискои or "Hungaro-Wallachian Land") was also used as a designation for its location. The term, translated in Romanian as "Ungrovalahia", remained in use up to the modern era in a religious context, referring to the Romanian Orthodox Metropolitan seat of Hungaro-Wallachia, in contrast to Thessalian Wallachia, or Great Wallachia in Macedonia, a medieval state, or Small Wallachia (Mala Vlaška) in Serbia. Official designations of the state were Muntenia (The Land of Mountains) and Țara Românească (Terra Romana, or The Romanian Country).
For long periods after the 14th century, Wallachia was referred to as Vlaško (Влашко) by Bulgarian sources, Vlaška by Serbian sources and Walachei or Walachey by German-speaking (Transylvanian Saxon) sources. The traditional Hungarian name for Wallachia is "Havasalföld", or literally "Snowy Lowlands" (the older form is "Havaselve", which means "Land beyond the snowy mountains", its translation to Latin - Transalpina - was used in the official royal documents of Kingdom of Hungary). In Ottoman Turkish and Turkish, "Eflâk" (which also means "sky" or "skies"), افلاق, a word derived from "Vlach", is used.
Black Wallachia.
Mavrovlachia, Black-Valachia, is another name of Moldavia. Mavrovlachi is another name of the Balkanic Vlachs or Aromanians. Both names could come from a confusion : Kara Iflak, the Turkish name of Vallachia, means ”land of Vallachians” ; but later "kara" ”land” was mistranslated as kara - black. 
Later, the Turks renamed Moldavia and Vallachia as Kara-Iflak (Moldavia) and Ak Iflak (Vallachia) according to the Turkish cardinal points symbolism : north is symbolized by black, and west is symbolized by white.
Ungro-Vlachia was the name of Transylvania, and Kara-Iflak, ”Northern Vlachia” - either Vallachia, north of the Balkan territories inhabited by Vlachs, or Moldavia (north of Vallachia). 
The second explanation is typologically better.
History.
Ancient times.
In the Second Dacian War (105 AD) western Oltenia became part of the Roman province of Dacia, with parts of Wallachia included in the Moesia Inferior province. The Roman limes was initially built along the Olt River (119), before being moved slightly to the east in the 2nd century—during which time it stretched from the Danube up to Rucăr in the Carpathians. The Roman line fell back to the Olt in 245 and, in 271, the Romans pulled out of the region.
The area was subject to Romanization also during the Migration Period, when most of present-day Romania was also invaded by Goths and Sarmatian peoples known as the Mureș-Cerneahov culture, followed by waves of other nomadic peoples. In 328, the Romans built a bridge between Sucidava and Oescus (near Gigen) which indicates that there was a significant trade with the peoples north of the Danube. A short period of Roman rule in the area is attested under Emperor Constantine I, after he attacked the Goths (who had settled north of the Danube) in 332. The period of Goth rule ended when the Huns arrived in the Pannonian Plain and, under Attila, attacked and destroyed some 170 settlements on both sides of the Danube.
Early Middle Ages.
Byzantine influence is evident during the 5th to 6th century, such as the site at Ipoteşti-Cândeşti, but from the second half of the 6th century and in the 7th century Slavic peoples crossed the territory of Wallachia and settled in it, on their way to Byzantium, occupying the southern bank of the Danube. In 593, the Byzantine commander-in-chief Priscus defeated Slavs, Avars and Gepids on future Wallachian territory, and, in 602, Slavs suffered a crucial defeat in the area; Flavius Mauricius Tiberius, who ordered his army to be deployed north of the Danube, encountered his troops' strong opposition.
Wallachia was under the control of the First Bulgarian Empire from its establishment in 681, until approximately the Magyar conquest of Transylvania at the end of the 10th century. With the decline and subsequent fall of the Bulgarian state to Byzantium (in the second half of the 10th century up to 1018), Wallachia came under the control of the Pechenegs (a Turkic people) who extended their rule west through the 10th and 11th century, until defeated around 1091, when the Cumans of southern Russia took control of the lands of Wallachia. Beginning with the 10th century, Byzantine, Bulgarian, Hungarian, and later Western sources mention the existence of small polities, possibly peopled by, among others, Vlachs/Romanians led by "knyazes" and "voivodes".
In 1241, during the Mongol invasion of Europe, Cuman domination was ended—a direct Mongol rule over Wallachia was not attested, but it remains probable. Part of Wallachia was probably briefly disputed by the Hungarian Kingdom and Bulgarians in the following period, but it appears that the severe weakening of Hungarian authority during the Mongol attacks contributed to the establishment of the new and stronger polities attested in Wallachia for the following decades.
Creation.
One of the first written pieces of evidence of local voivodes is in connection with Litovoi (1272), who ruled over land each side of the Carpathians (including Hațeg Country in Transylvania), and refused to pay tribute to the Hungarian King Ladislaus IV of Hungary. His successor was his brother Bărbat (1285–1288). The continuing weakening of the Hungarian state by further Mongol invasions (1285–1319) and the fall of the Árpád dynasty opened the way for the unification of Wallachian polities, and to independence from Hungarian rule.
Wallachia's creation, held by local traditions to have been the work of one "Radu Negru" (Black Radu), is historically connected with Basarab I (1310–1352), who rebelled against Charles I of Hungary and took up rule on either side of the Olt River, establishing his residence in Câmpulung as the first ruler in the House of Basarab. Basarab refused to grant Hungary the lands of Făgăraș, Almaş and the Banat of Severin, defeated Charles in the Battle of Posada (1330), and extended his lands to the east, to comprise lands as far as Kilia (in the Bujak), as the origin of "Bessarabia"); rule over the latter was not preserved by the princes that followed, as Kilia fell to the Nogais ca.1334.
Basarab was succeeded by Nicolae Alexandru, followed by Vladislav I. Vladislav attacked Transylvania after Louis I occupied lands south of the Danube, conceded to recognize him as overlord in 1368, but rebelled again in the same year; his rule also witnessed the first confrontation between Wallachia and the Ottoman Turks (a battle in which Vladislav was allied with Ivan Shishman of Bulgaria). Under Radu I and his successor Dan I, the realms in Transylvania and Severin continued to be disputed with Hungary.
1400–1600.
Mircea the Elder to Radu the Great.
As the entire Balkan Peninsula became an integral part of the emerging Ottoman Empire (a process which concluded with the fall of Constantinople to Sultan Mehmed II in 1453), Wallachia became engaged in frequent confrontations and, in the final years of Mircea the Elder's reign, became an Ottoman tributary state. Mircea (reigned 1386–1418), initially defeated the Ottomans in several battles (including that of Rovine in 1394), driving them away from Dobruja and briefly extending his rule to the Danube Delta, Dobruja and Silistra (ca.1400–1404). He swung between alliances with Sigismund of Hungary and Jagiellon Poland (taking part in the Battle of Nicopolis), and accepted a peace treaty with the Ottomans in 1417, after Mehmed I took control of Turnu Măgurele and Giurgiu. The two ports remained part of the Ottoman state, with brief interruptions, until 1829. In 1418–1420, Mihail I defeated the Ottomans in Severin, only to be killed in battle by the counter-offensive; in 1422, the danger was averted for a short while when Dan II inflicted a defeat on Murad II with the help of Pippo Spano.
The peace signed in 1428 inaugurated a period of internal crisis, as Dan had to defend himself against Radu Prasnaglava, who led the first in a series of boyar coalitions against established princes (in time, these became overtly pro-Ottoman in answer to repression). Victorious in 1431 (the year when the boyar-backed Alexandru I Aldea took the throne), boyars were dealt successive blows by Vlad II Dracul (1436–1442; 1443–1447), who nevertheless attempted to compromise between the Porte and the Holy Roman Empire.
The following decade was marked by the conflict between the rival houses of Dăneşti and Drăculeşti, the influence of John Hunyadi, Regent of the Kingdom of Hungary, and, after the neutral reign of Vladislav II, by the rise of Vlad III Dracula, better known as Vlad the Impaler. Vlad, during whose rule Bucharest was first mentioned as a princely residence, exercised terror on rebellious boyars, cut off all links with the Ottomans, and, in 1462, defeated Mehmed II's offensive during The Night Attack before being forced to retreat to Târgovişte and accepting to pay an increased tribute. His parallel conflicts with his Muslim brother Radu III the Fair and Laiotă Basarab led to the conquest of Wallachia by Radu III who would rule it for 11 years until his death. Radu the Great (1495–1508) reached several compromises with the boyars, ensuring a period of internal stability that contrasted his clash with Bogdan the Blind of Moldavia.
Mihnea cel Rău to Petru Cercel.
The late 15th century saw the ascension of the powerful Craioveşti family, virtually independent rulers of the Oltenian banat, who sought Ottoman support in their rivalry with Mihnea cel Rău (1508–1510) and replaced him with Vlăduţ; after the latter proved to be hostile to the bans, the House of Basarab formally ended with the rise of Neagoe Basarab, a Craioveşti. Neagoe's peaceful rule (1512–1521), noted for its cultural aspects (the building of the Curtea de Argeş Cathedral and Renaissance influences), also saw an increase in influence for the Saxon merchants in Braşov and Sibiu, and Wallachia's alliance with Louis II of Hungary. Under Teodosie, the country was again under a four-month-long Ottoman occupation, a military administration which seemed to be an attempt to create a Wallachian "Pashaluk". This danger rallied all boyars in support of Radu de la Afumaţi (four rules between 1522 and 1529), who lost the battle after an agreement between the Craioveşti and Sultan Süleyman the Magnificent; Prince Radu eventually confirmed Süleyman's position as suzerain, and agreed to pay an even higher tribute.
Ottoman suzerainty remained virtually unchallenged throughout the following 90 years. Radu Paisie, who was deposed by Süleyman in 1545, ceded the port of Brăila to Ottoman administration in the same year; his successor Mircea Ciobanul (1545–1554; 1558–1559), a prince without any claim to noble heritage, was imposed on the throne and consequently agreed to a decrease in autonomy (increasing taxes and carrying out an armed intervention in Transylvania—supporting the pro-Turkish John Zápolya). Conflicts between boyar families became stringent after the rule of Pătraşcu cel Bun, and boyar ascendancy over rulers was obvious under Petru the Younger (1559–1568; a reign dominated by Doamna Chiajna and marked by huge increases in taxes), Mihnea Turcitul, and Petru Cercel.
The Ottoman Empire increasingly relied on Wallachia and Moldavia for the supply and maintenance of its military forces; the local army, however, soon disappeared due to the increased costs and the much more obvious efficiency of mercenary troops.
17th century.
Initially profiting from Ottoman support, Michael the Brave ascended to the throne in 1593, and attacked the troops of Murad III north and south of the Danube in an alliance with Transylvania's Sigismund Báthory and Moldavia's Aron Vodă (see Battle of Călugăreni). He soon placed himself under the suzerainty of Rudolf II, the Holy Roman Emperor, and, in 1599–1600, intervened in Transylvania against Poland's king Sigismund III Vasa, placing the region under his authority; his brief rule also extended to Moldavia later in the following year. For a brief period, Michael the Brave unified all the territories where Romanians lived, rebuilding the mainland of the ancient Kingdom of Dacia. Following Michael's downfall, Wallachia was occupied by the Polish-Moldavian army of Simion Movilă (see Moldavian Magnate Wars), who held the region until 1602, and was subject to Nogai attacks in the same year.
The last stage in the Growth of the Ottoman Empire brought increased pressures on Wallachia: political control was accompanied by Ottoman economical hegemony, the discarding of the capital in Târgovişte in favour of Bucharest (closer to the Ottoman border, and a rapidly growing trade center), the establishment of serfdom under Michael the Brave as a measure to increase manorial revenues, and the decrease in importance of low-ranking boyars (threatened with extinction, they took part in the "seimeni" rebellion of 1655). Furthermore, the growing importance of appointment to high office in front of land ownership brought about an influx of Greek and Levantine families, a process already resented by locals during the rules of Radu Mihnea in the early 17th century. Matei Basarab, a boyar appointee, brought a long period of relative peace (1632–1654), with the noted exception of the 1653 Battle of Finta, fought between Wallachians and the troops of Moldavian prince Vasile Lupu—ending in disaster for the latter, who was replaced with Prince Matei's favourite, Gheorghe Ştefan, on the throne in Iaşi. A close alliance between Gheorghe Ştefan and Matei's successor Constantin Şerban was maintained by Transylvania's George II Rákóczi, but their designs for independence from Ottoman rule were crushed by the troops of Mehmed IV in 1658–1659. The reigns of Gheorghe Ghica and Grigore I Ghica, the sultan's favourites, signified attempts to prevent such incidents; however, they were also the onset of a violent clash between the Băleanu and Cantacuzino boyar families, which was to mark Wallachia's history until the 1680s. The Cantacuzinos, threatened by the alliance between the Băleanus and the Ghicas, backed their own choice of princes (Antonie Vodă din Popeşti and George Ducas) before promoting themselves—with the ascension of Șerban Cantacuzino (1678–1688).
Russo-Turkish Wars and the Phanariotes.
Wallachia became a target for Habsburg incursions during the last stages of the Great Turkish War ca.1690, when the ruler Constantin Brâncoveanu secretly and unsuccessfully negotiated an anti-Ottoman coalition. Brâncoveanu's reign (1688–1714), noted for its late Renaissance cultural achievements (see Brâncovenesc style), also coincided with the rise of Imperial Russia under Tsar Peter the Great—he was approached by the latter during the Russo-Turkish War of 1710–1711, and lost his throne and life sometime after sultan Ahmed III caught news of the negotiations. Despite his denunciation of Brâncoveanu's policies, Ștefan Cantacuzino attached himself to Habsburg projects and opened the country to the armies of Prince Eugene of Savoy; he was himself deposed and executed in 1716.
Immediately following the deposition of Prince Ştefan, the Ottomans renounced the purely nominal elective system (which had by then already witnessed the decrease in importance of the Boyar Divan over the sultan's decision), and princes of the two Danubian Principalities were appointed from the Phanariotes of Constantinople. Inaugurated by Nicholas Mavrocordatos in Moldavia after Dimitrie Cantemir, Phanariote rule was brought to Wallachia in 1715 by the very same ruler. The tense relations between boyars and princes brought a decrease in the number of taxed people (as a privilege gained by the former), a subsequent increase in total taxes, and the enlarged powers of a boyar circle in the Divan.
In parallel, Wallachia became the battleground in a succession of wars between the Ottomans on one side and Russia or the Habsburg Monarchy on the other. Mavrocordatos himself was deposed by a boyar rebellion, and arrested by Habsburg troops during the Austro-Turkish War of 1716–18, as the Ottomans had to concede Oltenia to Charles VI of Austria (the Treaty of Passarowitz). The region, subject to an enlightened absolutist rule that soon disenchanted local boyars, was returned to Wallachia in 1739 (the Treaty of Belgrade, upon the close of the Austro-Russian–Turkish War (1735–39)). Prince Constantine Mavrocordatos, who oversaw the new change in borders, was also responsible for the effective abolition of serfdom in 1746 (which put a stop to the exodus of peasants into Transylvania); during this period, the ban of Oltenia moved his residence from Craiova to Bucharest, signalling, alongside Mavrocordatos' order to merge his personal treasury with that of the country, a move towards centralism.
In 1768, during the Fifth Russo-Turkish War, Wallachia was placed under its first Russian occupation (helped along by the rebellion of Pârvu Cantacuzino). The Treaty of Kucuk Kaynarca (1774) allowed Russia to intervene in favour of Eastern Orthodox Ottoman subjects, curtailing Ottoman pressures—including the decrease in sums owed as tribute—and, in time, relatively increasing internal stability while opening Wallachia to more Russian interventions.
Habsburg troops, under Prince Josias of Coburg, again entered the country during the Russo-Turkish-Austrian War, deposing Nicholas Mavrogenis in 1789. A period of crisis followed the Ottoman recovery: Oltenia was devastated by the expeditions of Osman Pazvantoğlu, a powerful rebellious pasha whose raids even caused prince Constantine Hangerli to lose his life on suspicion of treason (1799), and Alexander Mourousis to renounce his throne (1801). In 1806, the Russo-Turkish War of 1806–1812 was partly instigated by the Porte's deposition of Constantine Ypsilantis in Bucharest—in tune with the Napoleonic Wars, it was instigated by the French Empire, and also showed the impact of the Treaty of Kucuk Kaynarca (with its permissive attitude towards Russian political influence in the Danubian Principalities); the war brought the invasion of Mikhail Andreyevich Miloradovich. After the Peace of Bucharest, the rule of Jean Georges Caradja, although remembered for a major plague epidemic, was notable for its cultural and industrial ventures. During the period, Wallachia increased its strategic importance for most European states interested in supervising Russian expansion; consulates were opened in Bucharest, having an indirect but major impact on Wallachian economy through the protection they extended to "sudiţi" traders (who soon competed successfully against local guilds).
From Wallachia to Romania.
Early 19th century.
The death of prince Alexander Soutzos in 1821, coinciding with the outbreak of the Greek War of Independence, established a boyar regency which attempted to block the arrival of Scarlat Callimachi to his throne in Bucharest. The parallel uprising in Oltenia, carried out by the Pandur leader Tudor Vladimirescu, although aimed at overthrowing the ascendancy of Greeks, compromised with the Greek revolutionaries in the Filiki Eteria and allied itself with the regents, while seeking Russian support (see also: Rise of nationalism under the Ottoman Empire).
On March 21, 1821, Vladimirescu entered Bucharest. For the following weeks, relations between him and his allies worsened, especially after he sought an agreement with the Ottomans; Eteria's leader Alexander Ypsilantis, who had established himself in Moldavia and, after May, in northern Wallachia, viewed the alliance as broken—he had Vladimirescu executed, and faced the Ottoman intervention without Pandur or Russian backing, suffering major defeats in Bucharest and Drăgăşani (before retreating to Austrian custody in Transylvania). These violent events, which had seen the majority of Phanariotes siding with Ypsilantis, made Sultan Mahmud II place the Principalities under its occupation (evicted by a request of several European powers), and sanction the end of Phanariote rules: in Wallachia, the first prince to be considered a local one after 1715 was Grigore IV Ghica. Although the new system was confirmed for the rest of Wallachia's existence as a state, Ghica's rule was abruptly ended by the devastating Russo-Turkish War of 1828–1829.
The 1829 Treaty of Adrianople, without overturning Ottoman suzerainty, placed Wallachia and Moldavia under Russian military rule, awarding them the first common institutions and the semblance of a constitution (see Regulamentul Organic). Wallachia was returned ownership of Brăila, Giurgiu (both of which soon developed into major trading cities on the Danube), and Turnu Măgurele. The treaty also allowed Moldavia and Wallachia to freely trade with countries other than the Ottoman Empire, which signalled substantial economic and urban growth, as well as improving the peasant situation. Many of the provisions had been specified by the 1826 Akkerman Convention between Russia and the Ottomans (it had never been fully implemented in the three-year interval). The duty of overseeing of the Principalities was left to Russian general Pavel Kiselyov; this interval was marked by a series of major changes, including the reestablishment of a Wallachian Army (1831), a tax reform (which nonetheless confirmed tax exemptions for the privileged), as well as major urban works in Bucharest and other cities. In 1834, Wallachia's throne was occupied by Alexandru II Ghica—a move in contradiction with the Adrianople treaty, as he had not been elected by the new Legislative Assembly; removed by the suzerains in 1842, he was replaced with an elected prince, Gheorghe Bibescu.
1840s–1850s.
Opposition to Ghica's arbitrary and highly conservative rule, together with the rise of liberal and radical currents, was first felt with the protests voiced by Ion Câmpineanu (quickly repressed); subsequently, it became increasingly conspiratorial, and centered on those secret societies created by young officers such as Nicolae Bălcescu and Mitică Filipescu.
"Frăţia", a clandestine movement created in 1843, began planning a revolution to overthrow Bibescu and repeal "Regulamentul Organic" in 1848 (inspired by the European rebellions of the same year). Their pan-Wallachian "coup d'état" was initially successful only near Turnu Măgurele, where crowds cheered the "Islaz Proclamation" (June 21); among others, the document called for political freedoms, independence, land reform, and the creation of a national guard. On June 11–12, the movement was successful in deposing Bibescu and establishing a Provisional Government. Although sympathetic to the anti-Russian goals of the revolution, the Ottomans were pressured by Russia into repressing it: Ottoman troops entered Bucharest on September 13. Russian and Turkish troops, present until 1851, brought Barbu Dimitrie Ştirbei to the throne, during which interval most participants in the revolution were sent into exile.
Briefly under renewed Russian occupation during the Crimean War, Wallachia and Moldavia were given a new status with a neutral Austrian administration (1854–1856) and the Treaty of Paris: a tutelage shared by Ottomans and a Congress of Great Powers (Britain, France, the Kingdom of Piedmont-Sardinia, the Austrian Empire, Prussia, and, albeit never again fully, Russia), with a "kaymakam"-led internal administration. The emerging movement for a union of the Danubian Principalities (a demand first voiced in 1848, and a cause cemented by the return of revolutionary exiles) was advocated by the French and their Sardinian allies, supported by Russia and Prussia, but was rejected or suspicioned by all other overseers.
After an intense campaign, a formal union was ultimately granted: nevertheless, elections for the "ad hoc divans" of 1859 profited from a legal ambiguity (the text of the final agreement specified two thrones, but did not prevent any single person from simultaneously taking part in and winning elections in both Bucharest and Iaşi). Alexander John Cuza, who ran for the unionist "Partida Naţională", won the elections in Moldavia on January 5; Wallachia, which was expected by the unionists to carry the same vote, returned a majority of anti-unionists to its "divan".
Those elected changed their allegiance after a mass protest of Bucharest crowds, and Cuza was voted prince of Wallachia on February 5 (January 24 Old Style), consequently confirmed as "Domnitor" of the United Principalities of Moldavia and Wallachia ("of Romania" from 1862). Internationally recognized only for the duration of his reign, the union was irreversible after the ascension of Carol I in 1866 (coinciding with the Austro-Prussian War, it came at a time when Austria, the main opponent of the decision, was not in a position to intervene).
Society.
Slavery.
Slavery (Romanian: "robie") was part of the social order from before the founding of the Principality of Wallachia, until it was abolished in stages during the 1840s and 1850s. Most of the slaves were of Roma (Gypsy) ethnicity. The very first document attesting the presence of Roma people in Wallachia dates back to 1385, and refers to the group as "aţigani" (from the Greek" athinganoi", the origin of the Romanian term "ţigani", which is synonymous with "Gypsy").
The exact origins of slavery are not known. Slavery was a common practice in Europe at the time, and there is some debate over whether the Romani people came to Wallachia as free men or as slaves. In the Byzantine Empire, they were slaves of the state and it seems the situation was the same in Bulgaria and Serbia until their social organization was destroyed by the Ottoman conquest, which would suggest that they came as slaves who had a change of 'ownership'. Historian Nicolae Iorga associated the Roma people's arrival with the 1241 Mongol invasion of Europe and considered their slavery as a vestige of that era, the Romanians taking the Roma from the Mongols as slaves and preserving their status. Other historians consider that they were enslaved while captured during the battles with the Tatars. The practice of enslaving prisoners may also have been taken from the Mongols. While it is possible that some Romani people were slaves or auxiliary troops of the Mongols or Tatars, the bulk of them came from south of the Danube at the end of the 14th century, some time after the foundation of Wallachia. The arrival of the Roma made slavery a widespread practice.
Traditionally, Roma slaves were divided into three categories. The smallest was owned by the "hospodars", and went by the Romanian-language name of "ţigani domneşti" ("Gypsies belonging to the lord"). The two other categories comprised "ţigani mănăstireşti" ("Gypsies belonging to the monasteries"), who were the property of Romanian Orthodox and Greek Orthodox monasteries, and "ţigani boiereşti" ("Gypsies belonging to the boyars"), who were enslaved by the category of landowners.
The abolition of slavery was carried out following a campaign by young revolutionaries who embraced the liberal ideas of the Enlightenment. The earliest law which freed a category of slaves was in March 1843, which transferred the control of the state slaves owned by the prison authority to the local authorities, leading to their sedentarizing and becoming peasants. During the Wallachian Revolution of 1848, the agenda of the Provisional Government included the emancipation ("dezrobire") of the Roma as one of the main social demands. By the 1850s the movement gained support from almost the whole of Romanian society, and the law from February 1856 emancipated all slaves to the status of taxpayers (citizens).
Geography.
With an area of approximately 77000 km2, Wallachia is situated north of the Danube (and of present-day Bulgaria), east of Serbia and south of the Southern Carpathians, and is traditionally divided between Muntenia in the east (as the political center, Muntenia is often understood as being synonymous with Wallachia), and Oltenia (a former banat) in the west. The division line between the two is the Olt River.
Wallachia's traditional border with Moldavia coincided with the Milcov River for most of its length. To the east, over the Danube north-south bend, Wallachia neighbours Dobruja (Northern Dobruja). Over the Carpathians, Wallachia shared a border with Transylvania; Wallachian princes have for long held possession of areas north of the line (Amlaş, Ciceu, Făgăraş, and Haţeg), which are generally not considered part of Wallachia-proper.
The capital city changed over time, from Câmpulung to Curtea de Argeş, then to Târgovişte and, after the late 17th century, to Bucharest.
Population.
Historical population.
Contemporary historians estimate the populatian of Wallachia in the 15th century, at 500,000 people. In 1859, the population of Wallachia was 2,400,921 (1,586,596 in Muntenia and 814,325 in Oltenia).
Current population.
According to the latest 2002 census data, the region has a total population of approximately 8,750,000 inhabitants, distributed among the ethnic groups as follows: Romanians (97%), Roma (2.5%), others (0.5%).
Cities.
The largest cities (as per the 2011 census) in the Wallachia region are:

</doc>
<doc id="46027" url="http://en.wikipedia.org/wiki?curid=46027" title="Gerhard Gentzen">
Gerhard Gentzen

Gerhard Karl Erich Gentzen (November 24, 1909 – August 4, 1945) was a German mathematician and logician. He made major contributions to the foundations of mathematics, proof theory, especially on natural deduction and sequent calculus. He died in 1945 after the Second World War, because he was deprived of food after being arrested in Prague.
Life and career.
Gentzen was a student of Paul Bernays at the University of Göttingen. Bernays was fired as "non-Aryan" in April 1933 and therefore Hermann Weyl formally acted as his supervisor. Gentzen joined the Sturmabteilung in November 1933 although he was by no means compelled to do so. Nevertheless he kept in contact with Bernays until the beginning of the Second World War. In 1935, he corresponded with Abraham Fraenkel in Jerusalem and was implicated by the Nazi teachers' union as one who "keeps contacts to the Chosen People." In 1935 and 1936, Hermann Weyl, head of the Göttingen mathematics department in 1933 until his resignation under Nazi pressure, made strong efforts to bring him to the Institute for Advanced Study in Princeton.
Between November 1935 and 1939 he was an assistant of David Hilbert in Göttingen. Gentzen joined the Nazi Party in 1937. In April 1939 Gentzen swore the oath of loyalty to Adolf Hitler as part of his academic appointment. From 1943 he was a teacher at the University of Prague. Under a contract from the SS Gentzen evidently worked for the V-2 project.
Gentzen was arrested during the citizens uprising against the occupying German forces on May 5, 1945. He, along with the rest of the staff of the German University in Prague was subsequently handed over to Russian forces. Because of his past association with the SA, NSDAP and NSD Dozentenbund, Gentzen was detained in a prison camp, where he died of malnutrition on August 4, 1945.
Work.
Gentzen's main work was on the foundations of mathematics, in proof theory, specifically natural deduction and the sequent calculus. His cut-elimination theorem is the cornerstone of proof-theoretic semantics, and some philosophical remarks in his "Investigations into Logical Deduction", together with Ludwig Wittgenstein's later work, constitute the starting point for inferential role semantics.
One of Gentzen's papers had a second publication in the ideological "Deutsche Mathematik" that was founded by Ludwig Bieberbach who promoted "Aryan" mathematics.
Gentzen proved the consistency of the Peano axioms in a paper published in 1936. In his Habilitationsschrift, finished in 1939, he determined the proof-theoretical strength of Peano arithmetic. This was done by a direct proof of the unprovability of the principle of transfinite induction, used in his 1936 proof of consistency, within Peano arithmetic. The principle can, however, be expressed in arithmetic, so that a direct proof of Gödel's incompleteness theorem followed. Gödel used a coding procedure to construct an unprovable formula of arithmetic. Gentzen's proof was published in 1943 and marked the beginning of ordinal proof theory.

</doc>
<doc id="46029" url="http://en.wikipedia.org/wiki?curid=46029" title="Wisdom">
Wisdom

Wisdom is the ability to think and act using knowledge, experience, understanding, common sense, and insight. Wisdom has been regarded as one of four cardinal virtues; and as a virtue, it is a habit or disposition to perform the action with the highest degree of adequacy under any given circumstance. This implies a possession of knowledge or the seeking thereof to apply it to the given circumstance. This involves an understanding of people, objects, events, situations, and the willingness as well as the ability to apply perception, judgement, and action in keeping with the understanding of what is the optimal course of action. It often requires control of one's emotional reactions (the "passions") so that the universal principle of reason prevails to determine one's action. In short, wisdom is a disposition to find the truth coupled with an optimum judgement as to what actions should be taken to deliver the correct outcome.
Definitions.
Charles Haddon Spurgeon defined wisdom as "the right use of knowledge". Robert I. Sutton and Andrew Hargadon defined the "attitude of wisdom" as "acting with knowledge while doubting what one knows".
Philosophical perspectives.
The ancient Greeks considered wisdom to be an important virtue, personified as the goddesses Metis and Athena. Athena is said to have sprung from the head of Zeus. She was portrayed as strong, fair, merciful, and chaste. To Socrates and Plato, philosophy was literally the love of Wisdom (philo-sophia). This permeates Plato's dialogues, especially "The Republic", in which the leaders of his proposed utopia are to be philosopher kings, rulers who understand the Form of the Good and possess the courage to act accordingly. Aristotle, in his "Metaphysics", defined wisdom as the understanding of causes, i.e. knowing why things are a certain way, which is deeper than merely knowing that things are a certain way.
The ancient Romans also valued wisdom. It was personified in Minerva, or Pallas. She also represents skillful knowledge and the virtues, especially chastity. Her symbol was the owl which is still a popular representation of wisdom, because it can see in darkness. She was said to be born from Jupiter's forehead.
Wisdom is also important within Christianity. Jesus emphasized it. Paul the Apostle, in his first epistle to the Corinthians, argued that there is both secular and divine wisdom, urging Christians to pursue the latter. Prudence, which is intimately related to wisdom, became one of the four cardinal virtues of Catholicism. The Christian philosopher Thomas Aquinas considered wisdom to be the "father" (i.e. the cause, measure, and form) of all virtues.
In the Inuit tradition, developing wisdom was one of the aims of teaching. An Inuit Elder said that a person became wise when they could see what needed to be done and do it successfully without being told what to do.
Educational perspectives.
Public schools in the US have an approach to character education. Eighteenth century philosophers such as Benjamin Franklin, referred to this as training wisdom and virtue. Traditionally, schools share the responsibility to build character and wisdom along with parents and the community.
Nicholas Maxwell, a contemporary philosopher in the United Kingdom, advocates that academia ought to alter its focus from the acquisition of knowledge to seeking and promoting wisdom, which he defines as the capacity to realize what is of value in life, for oneself and others. He teaches that new knowledge and technological know-how increase our power to act which, without wisdom, may cause human suffering and death as well as human benefit. Wisdom is the application of knowledge to attain a positive goal by receiving instruction in governing oneself.
Psychological perspectives.
Psychologists have gathered data on commonly held beliefs or folk theories about wisdom.
These analyses indicate that although "there is an overlap of the implicit theory of wisdom with intelligence, perceptiveness, spirituality and shrewdness, it is evident that wisdom is a distinct term and not a composite of other terms." Many, but not all, studies find that adults' self-ratings of perspective/wisdom do not depend on age. This stands in contrast to the popular notion that wisdom increases with age, supported by a recent study showing that regardless of their education, IQ or gender, older adults possess superior reasoning about societal and interpersonal conflicts. In many cultures the name for third molars, which are the last teeth to grow, is etymologically linked with wisdom, e.g., as in the English "wisdom tooth". In 2009, a study reviewed which brain processes might be related to wisdom.
Researchers in the field of positive psychology have defined wisdom as the coordination of "knowledge and experience" and "its deliberate use to improve well being." With this definition, wisdom can supposedly be measured using the following criteria.
Measurement instruments that use these criteria have acceptable to good internal consistency and low to moderate test-retest reliability ("r" in the range of 0.35 to 0.67).
John Vervaeke has argued for a cognitive science of wisdom and argues that basic relevance realization processes that underlie cognition, when fed back onto themselves and made self-referential lead to the enhanced insight abilities we associated with wisdom.
Dr. B. Legesse et al., a neuropsychiatrist at McLean Hospital/Harvard Medical School, offers "a theoretical definition that takes into account many cultural, religious, and philosophical themes is that wisdom represents a demonstrated superior ability to understand the nature and behavior of things, people, or events." He states "this results in an increased ability to predict behavior or events which then may be used to benefit self or others." He furthermore adds "there is more often a desire to share the accrued benefits with a larger group for the purpose of promoting survival, cohesion, or well-being of that group. The benefits do not result from malicious or antisocial intents or inequitable behavior. Environmental factors, such as family, education, socioeconomic status, culture, and religion, are involved in generating the milieu in which the personal value system develops. Many of these same factors also influence how a given community decides whether wisdom is present or not. This model of wisdom relies on the individual’s ability to generate a mental representation of the self (cognitive, emotional, and physical), the external world, and the dynamic relationship of the self with the external world." Dr. Legesse proposes that "the neural (brain) systems critical to enable these functions are distributed but heavily dependent on those that support memory, learning, understanding other people’s mental states (Theory of Mind), and assigning relative value to information." The neuroanatomy of wisdom he says depends on "the three frontosubcortical neural networks, the limbic system, and the mirror neuron system" which "are of particular importance for supporting these activities." He describes the function of this neural system as working "in concert to weigh and estimate the risks and benefits of various mentally modeled courses of action to generate wisdom." It was proposed that "the neural substrates of empathy may be conceptualized as biasing the information processing network in favor of valuing others, interpersonal communication, cooperation, and community." 
Sapience.
Sapience is often defined as wisdom, or the ability of an organism or entity to act with appropriate judgement, a mental faculty which is a component of intelligence or alternatively may be considered an additional faculty, apart from intelligence, with its own properties. Robert Sternberg has segregated the capacity for judgement from the general qualifiers for intelligence, which is closer to cognizant aptitude than to wisdom. Displaying sound judgement in a complex, dynamic environment is a hallmark of wisdom.
The word "sapience" is derived from the Latin "sapientia", meaning "wisdom." Related to this word is the Latin verb "sapere", meaning "to taste, to be wise, to know"; the present participle of "sapere" forms part of 'Homo sapiens', the Latin binomial nomenclature created by Carolus Linnaeus to describe the human species. Linnaeus had originally given humans the species name of "diurnus", meaning man of the day. But he later decided that the dominating feature of humans was wisdom, hence application of the name "sapiens". His chosen biological name was intended to emphasize man's uniqueness and separation from the rest of the animal kingdom.
In fantasy fiction and science fiction, sapience often describes an essential property that bestows "personhood" onto a non-human. It indicates that a computer, alien, mythical creature or other similar will be treated as a being with capabilities and desires as any human character, often eligible to full civil rights. The words "sentience," "self-awareness," and "consciousness" are used in similar ways in science fiction.
Religious perspectives.
Some religions have specific teachings relating to wisdom.
Ancient Egypt.
Sia represents the personification of wisdom or the god of wisdom in Ancient Egyptian Mythology.
Bahá'í Faith.
Wisdom and the acquiring of it is mentioned frequently in the Bahá'í scriptures. According to the scriptures 
"The essence of wisdom is the fear of God, the dread of His scourge and punishment, and the apprehension of His justice and decree." Wisdom is seen as a light, that casts away darkness, and "its dictates must be observed under all circumstances", other concepts associated with wisdom and being wise are considering "the regard of place and the utterance of discourse according to measure and state" and not believing or accepting what other people say so easily.
One may obtain knowledge and wisdom through God, his Word, and his Divine Manifestation and the source of all learning is the knowledge of God.
Buddhism.
Buddhist scriptures teach that a wise person is endowed with good bodily conduct, good verbal conduct, and good mental conduct.("AN 3:2") A wise person does actions that are unpleasant to do but give good results, and doesn’t do actions that are pleasant to do but give bad results ("AN 4:115"). Wisdom is the antidote to the self-chosen poison of ignorance. The Buddha has much to say on the subject of wisdom including:
To recover the original supreme wisdom of self-nature covered by the self-imposed three dusty poisons (greed, anger, ignorance) Buddha taught to his students the threefold training by turning greed into generosity and discipline, anger into kindness and meditation, ignorance into wisdom. As the Sixth Patriarch of Chán Buddhism, Huineng, said in his Platform Sutra,"Mind without dispute is self-nature discipline, mind without disturbance is self-nature meditation, mind without ignorance is self-nature wisdom."
Christianity.
In Christian theology, "wisdom" (Hebrew: "chokhmah", Greek: "Sophia", Latin: "Sapientia") describes an aspect of God, or the theological concept regarding the wisdom of God.
There is an oppositional element in Christian thought between secular wisdom and Godly wisdom. The apostle Paul states that worldly wisdom thinks the claims of Christ to be foolishness. However, to those who are "on the path to salvation" Christ represents the wisdom of God. () Also, Wisdom is one of the seven gifts of the Holy Spirit according to Anglican, Catholic, and Lutheran belief. gives an alternate list of nine virtues, among which wisdom is one.
Confucianism.
According to Confucius (551–479 BCE), one can learn wisdom by three methods: 
One does not dispense wisdom oneself unless asked by another. This means that a wise man never tells his wisdom unless asked person to person.
According to the "Doctrine of the Mean", Confucius also said: 
"Love of learning is akin to wisdom. To practice with vigor is akin to humanity. To know to be shameful is akin to courage (zhi, ren, yi.. three of Mengzi's sprouts of virtue)."
Compare this with the Confucian classic "Great Learning", which begins with: "The Way of learning to be great consists in manifesting the clear character, loving the people, and abiding in the highest good." One can clearly see the correlation with the Roman virtue prudence, especially if one interprets "clear character" as "clear conscience". (From Chan's Sources of Chinese Philosophy).
Hinduism.
Wisdom in Hinduism is considered a state of mind and soul where a person achieves liberation.
The God of intellect (wisdom) is Ganesha and the goddess of knowledge is Saraswati.
The Sanskrit verse to attain knowledge is<br>
Wisdom in Hinduism is knowing oneself as the truth, basis for the entire Creation, i.e., of "Shristi". In other words, wisdom simply means a person with Self-awareness as the one who witnesses the entire creation in all its facets and forms. Further it means realization that an individual through right conduct and right living over an unspecified period comes to realize their true relationship with the creation and the "Paramatma" who rules it.
Islam.
In Islam, Wisdom is deemed as one of the greatest gifts humankind can enjoy. The Quran states : "He gives wisdom to whom He wills, and whoever has been given wisdom has certainly been given much good. And none will remember except those of understanding."—Qur'an, sura 2 (Al-Baqara), ayat 269
There are a number of verses where the Q'uran specifically talks about the nature of wisdom. In Surah 22 Al-Hajj (The Pilgrimage) it is said, "Have they not travelled in the land, and have they hearts wherewith to feel and ears wherewith to hear? For indeed it is not the eyes that grow blind, but it is the hearts, which are within the bosoms, that grow blind." (verse 46). In another Surah Al-'An`ām (The Cattle) it's said, "Say: "Come, I will rehearse what Allah (God) hath (really) prohibited you from": Join not anything as equal with Him; be good to your parents; kill not your children on a plea of want;― We provide sustenance for you and for them;― come not nigh to shameful deeds, whether open or secret; take not life, which Allah hath made sacred, except by way of justice and law: thus doth He command you, that ye may learn wisdom"—Qur'an, sura 6 (Al-An'am), ayat 151
Judaism.
The word wisdom (חכם) is mentioned 222 times in the Hebrew Bible. It was regarded as one of the highest virtues among the Israelites along with kindness (חסד) and justice (צדק). Both the books of Proverbs and Psalms urge readers to obtain and to increase in wisdom.
In the Hebrew Bible, wisdom is represented by Solomon, who asks God for wisdom in . Much of the Book of Proverbs, a book of wise sayings, is attributed to Solomon. In the fear of YHWH is called the beginning of wisdom. In there is also reference to wisdom personified in female form, "Wisdom calls aloud in the streets, she raises her voice in the marketplaces." In this personified wisdom is described as being present with God before creation began and even taking part in creation itself.
The Talmud teaches that a wise person is a person who can foresee the future. "Nolad" is the Hebrew word for "future," but also the Hebrew word for "birth", so one rabbinic interpretation of the teaching is that a wise person is one who can foresee the consequences of his/her choices (i.e. can "see the future" that he/she "gives birth" to).
Taoism.
In Taoism, wisdom is construed as adherence to the Three Treasures (Taoism): charity, simplicity, and humility.
Others.
In Mesopotamian religion and mythology, Enki, also known as Ea, was the God of wisdom and intelligence. Wisdom was achieved by restoring balance.
In Norse mythology, the god Odin is especially known for his wisdom, often acquired through various hardships and ordeals involving pain and self-sacrifice. In one instance he plucked out an eye and offered it to Mímir, guardian of the well of knowledge and wisdom, in return for a drink from the well.
In another famous account, Odin hanged himself for nine nights from Yggdrasil, the World Tree that unites all the realms of existence, suffering from hunger and thirst and finally wounding himself with a spear until he gained the knowledge of runes for use in casting powerful magic. He was also able to acquire the mead of poetry from the giants, a drink of which could grant the power of a scholar or poet, for the benefit of gods and mortals alike.

</doc>
<doc id="46030" url="http://en.wikipedia.org/wiki?curid=46030" title="Alleghany County">
Alleghany County

Alleghany County (also spelled Allegany County and Allegheny County) is the name of five counties in the United States of America:

</doc>
<doc id="46034" url="http://en.wikipedia.org/wiki?curid=46034" title="Alfred Bester">
Alfred Bester

Alfred Bester (December 18, 1913 – September 30, 1987) was an American science fiction author, TV and radio scriptwriter, magazine editor and scripter for comic strips and comic books. Though successful in all these fields, he is best remembered for his science fiction, including "The Demolished Man", winner of the inaugural Hugo Award in 1953.
Science fiction author Harry Harrison wrote, "Alfred Bester was one of the handful of writers who invented modern science fiction."
Shortly before his death, the Science Fiction Writers of America (SFWA) named Bester its ninth Grand Master, presented posthumously in 1988.
The Science Fiction and Fantasy Hall of Fame inducted him in 2001.
Biography.
Alfred Bester was born in Manhattan, New York City, on December 18, 1913. His father James J. Bester owned a shoe store and was a first-generation American whose parents were both Austrian. Alfred's mother, Belle (née Silverman), was born in Russia and spoke Yiddish as her first language before coming to America as a youth. Alfred was James and Belle's second and final child, and only son. (Their first child, Rita, was born in 1908.) Though his father was of Jewish background, and his mother became a Christian Scientist, Alfred Bester himself was not raised within any religious traditions; he wrote that "his home life was completely liberal and iconoclastic."
Bester attended the University of Pennsylvania where he was a member of the Philomathean Society. He played on the football team in 1935 and, by his own account, was "the most successful member of the fencing team." He went on to Columbia Law School, but tired of it and dropped out.
Bester and Rolly Goulko married in 1936. Rolly Bester had a successful career as a Broadway, radio and television actress before changing careers to become an advertising executive during the 1960s. The Besters remained married for 48 years until her death on January 12, 1984. Bester was very nearly a lifelong New Yorker, although he lived in Europe for a little over a year in the mid-1950s and moved to Pennsylvania with Rolly in the early 1980s. Once settled there, they lived on Geigel Hill Road in Ottsville, Pennsylvania.
Writing career.
Early SF career, comic books, radio (1939–50).
After his university career, 25-year-old Alfred Bester was working in public relations when he turned to writing science fiction. Bester's first published short story was the "The Broken Axiom", which appeared in the April 1939 issue of "Thrilling Wonder Stories" after winning an amateur story competition. Bester recalled, "Two editors on the staff, Mort Weisinger and Jack Schiff, took an interest in me, I suspect mostly because I'd just finished reading and annotating Joyce's "Ulysses" and would preach it enthusiastically without provocation, to their great amusement. ... They thought "Diaz-X" [Bester's original title] might fill the bill if it was whipped into shape." This was the very same contest that Robert A. Heinlein famously chose not to enter, as the prize was only $50 and Heinlein realized he could do better selling "his" 7,000-word unpublished story to "Astounding Science Fiction" for a penny a word, or $70. Years later, Bester interviewed Heinlein for "Publishers Weekly" and the latter told of changing his mind for "Astounding". Bester says that he replied (in jest), "You sonofabitch. I won that "Thrilling Wonder" contest, and you beat me by twenty dollars."
However, as the winner of the contest, Mort Weisinger also "introduced me to the informal luncheon gatherings of the working science fiction authors of the late thirties." He met Henry Kuttner, Edmond Hamilton, Otto Binder, Malcolm Jameson and Manly Wade Wellman there. During 1939 and 1940 Weisinger published three more of Bester's stories in "Thrilling Wonder Stories" and "Startling Stories".
For the next few years, Bester continued to publish short fiction, most notably in John W. Campbell's "Astounding Science Fiction". In 1942, two of his science fiction editors got work at DC Comics, and invited Bester to contribute to various DC titles. Consequently, Bester left the field of short story writing and began working for DC Comics as a writer on "Superman", "Green Lantern" and other titles. He is credited with writing the version of the Green Lantern Oath that begins "In brightest day, In darkest night".
Bester was also the writer for Lee Falk's comic strips "The Phantom" and "Mandrake the Magician" while their creator served in World War II. It is widely speculated how much influence Bester had on these comics. One theory claims that Bester was responsible for giving the Phantom his surname, "Walker".
After four years in the comics industry, in 1946 Bester turned his attention to radio scripts, after wife Rolly (a busy radio actress) told him that the show "Nick Carter, Master Detective" was looking for story submissions. Over the next few years, Bester wrote for "Nick Carter", as well as "The Shadow", "Charlie Chan", "Nero Wolfe" and other shows. He later wrote for "The CBS Radio Mystery Theater".
With the advent of American network television in 1948, Bester also began writing for television, although most of these projects were lesser-known.
In early 1950, after eight years away from the field, Bester resumed writing science fiction short stories. However, after an initial return to "Astounding" with the story "The Devil's Invention" (aka "Oddy and Id"), he stopped writing for the magazine in mid-1950 when editor John Campbell became preoccupied with L. Ron Hubbard and Dianetics, the forerunner to Scientology. Bester then turned to "Galaxy Science Fiction", where he found in H. L. Gold another exceptional editor as well as a good friend.
In New York, he socialized at the Hydra Club, an organization of New York's science fiction writers, including such luminaries as Isaac Asimov, James Blish, Anthony Boucher, Avram Davidson, Judith Merril, and Theodore Sturgeon.
The classic period: 1951–57.
In his first period of writing science fiction (1939–1942), Bester had been establishing a reputation as a short story writer in science fiction circles with stories such as "Adam and No Eve". However, Bester gained his greatest renown for the work he wrote and published in the 1950s, including "The Demolished Man" and "The Stars My Destination" (also known as "Tiger! Tiger!").
"The Demolished Man" (1953).
"The Demolished Man", recipient of the first Hugo Award for best Science Fiction novel, is a police procedural that takes place in a future world in which telepathy is relatively common. Bester creates a harshly capitalistic, hierarchical and competitive social world that exists without deceit: a society where the right person with some skill (or money) and curiosity can access your memories, secrets, fears and past misdeeds more swiftly than even you.
Originally published in three parts in "Galaxy", beginning in January 1952, "The Demolished Man" appeared in book form in 1953. It was dedicated to Gold, who made a number of suggestions during its writing. Originally, Bester wanted the title to be "Demolition!", but Gold talked him out of it.
"Who He?" aka "The Rat Race" (1953).
Bester's 1953 novel "Who He?" concerns a TV variety show writer who wakes up after an alcoholic blackout and discovers that someone is out to destroy his life. A contemporary novel with no science-fiction elements, it did not receive wide attention. It did, however, earn Bester a fair amount of money from the sale of the paperback reprint rights (the book appeared in paperback as "The Rat Race"). He also received a substantial sum of money from a movie studio for the film option to the book. Reportedly, Jackie Gleason was interested in starring as the variety show writer; however no movie was ever made of "Who He?" Still, the payout from the film option was large enough that Alfred and Rolly Bester decided they could afford to travel to Europe for the next few years. They lived mainly in Italy and England during this period.
"The Stars My Destination" (1956).
Bester's next novel was outlined while he was living in England and mostly written when he was living in Rome. "The Stars My Destination" (aka "Tiger, Tiger") had its origins in a newspaper clipping that Bester found about Poon Lim, a shipwrecked World War II sailor on a raft, who had drifted unrescued in the Pacific for a world record 133 days because passing ships thought he was a lure to bring them within torpedo range of a hidden submarine. From that germ grew the story of Gully Foyle, seeking revenge for his abandonment and causing havoc all about him: a science fiction re-telling of Alexandre Dumas' "The Count of Monte Cristo" with teleportation added to the mix. It has been described as an ancestor of cyberpunk.
As had occurred with "The Demolished Man", "The Stars My Destination" was originally serialized in "Galaxy". It ran in four parts (October 1956 through January 1957) and the book was published later in 1957. Though repeatedly voted in polls the "Best Science Fiction Novel of All Time', "The Stars My Destination" would prove to be Bester's last novel for 19 years. A radio adaptation was broadcast on BBC Radio 4 in 1991.
Film adaptations of "The Stars My Destination" have been frequently rumored. Charlie Jane Anders wrote in 2012, "According to [David] Hughes' "Greatest Sci-Fi Movies Never Made", Richard Gere owned the rights to this novel right after his success with "Pretty Woman", and wanted to star in it. Later, "NeverEnding Story" producer Bernd Eichinger had the rights and hired Neal Adams to do concept art. Still later, Paul W.S. Anderson was set to direct it, but wound up doing "Event Horizon" instead. Since then, a number of scripts have been written, but the film's gotten no closer to happening."
Magazine fiction and non-fiction: 1959–62.
While on his European trip, Bester began selling non-fiction pieces about various European locations to the mainstream travel/lifestyle magazine "Holiday". The "Holiday" editors, impressed with his work, invited Bester back to their headquarters in New York and began commissioning him to write travel articles about various far-flung locales, as well as doing interviews with such stars as Sophia Loren, Anthony Quinn, and Sir Edmund Hillary. As a result of steady work with "Holiday", Bester's science fiction output dropped precipitously in the years following the publication of "The Stars My Destination".
Bester published three short stories each in 1958 and 1959, including 1958's "The Men Who Murdered Mohammed" and 1959's "The Pi Man", both of which were nominated for Hugo Awards. However, for a four-year period from October 1959 to October 1963, he published no fiction at all. Instead, he concentrated on his work at "Holiday" (where he was made a senior editor), reviewed books for "The Magazine of Fantasy and Science Fiction" (from 1960 to 1962) and returned to television scripting.
Television: 1959–62.
During the 1950s, Bester contributed a satiric sketch, "I Remember Hiroshima," to "The Paul Winchell Show". His later story "Hobson's Choice" was based on it.
In 1959, Bester adapted his 1954 story "Fondly Fahrenheit" to television as "Murder and the Android". Telecast in color on October 18, 1959, the hour-long drama took place in the year 2359 amid futuristic sets designed by Ted Cooper. This "NBC Sunday Showcase" production, produced by Robert Alan Aurthur with a cast of Kevin McCarthy, Rip Torn, Suzanne Pleshette and Telly Savalas, was reviewed by syndicated radio-television critic John Crosby:
"Murder and the Android" was nominated for a 1960 Hugo Award for Best Dramatic Presentation and was given a repeat on September 5, 1960, the Labor Day weekend in which that Hugo Award was presented (to "The Twilight Zone") at the World Science Fiction Convention in Pittsburgh. Bester returned to "Sunday Showcase" March 5, 1960 with an original teleplay, "Turn the Key Deftly". Telecast in color, that mystery, set in a traveling circus, starred Julie Harris, Maximilian Schell and Francis Lederer.
For "Alcoa Premiere", hosted by Fred Astaire, he wrote "Mr. Lucifer", which aired November 1, 1962 with Astaire in the title role opposite Elizabeth Montgomery.
Senior editor of "Holiday": 1963–71.
After a four-year layoff, Bester published a handful of science-fiction short stories in 1963 and 1964. However, writing science-fiction was at this stage in Bester's life clearly more of a sideline than the focus of his career. As a result, from 1964 until the original version of "Holiday" folded in 1971, Bester published only one science-fiction short story, a 700-word science fiction spoof in the upscale mainstream magazine "Status".
Still, as senior editor of "Holiday", Bester was able to introduce occasional science-fiction elements into the non-fiction magazine. On one occasion, he commissioned and published an article by Arthur C. Clarke describing a tourist flight to the Moon. Bester himself, though, never published any science fiction in "Holiday", which was a mainstream travel/lifestyle magazine marketed to upscale readers during an era when science fiction was largely dismissed as juvenilia.
Later career: 1972–87.
"Holiday" magazine ceased publication in 1971, although it was later revived and reformatted by other hands, without Bester's involvement. For the first time in nearly 15 years, Bester did not have full-time employment.
After a long layoff from writing science fiction, Bester returned to the field in 1972. His 1974 short story "The Four-Hour Fugue" was nominated for a Hugo Award, and Bester received Hugo and Nebula Award nominations for his 1975 novel "The Computer Connection" (titled "The Indian Giver" as a magazine serial and later reprinted as "Extro"). Despite these nominations, Bester's work of this era generally failed to receive the critical or commercial success of his earlier period.
Bester's eyesight began failing in the mid-1970s, making writing increasingly difficult, and another layoff from published writing took place between early 1975 and early 1979. It is alleged during this period that the producer of the 1978 "Superman" movie sent his son off to search for a writer. The name Alfred Bester came up, but Bester wanted to focus the story on Clark Kent as the real hero, while Superman was only "his gun." The producers instead hired Mario Puzo, author of "The Godfather", to write the film.
Carolyn Wendell wrote, "I shall always remember the time I saw Alfie Bester in larger-than-life action, at an academic conference in New York City ten years before he died":
Bester had been invited to share a panel with Charles L. Grant, Isaac Asimov, and Ben Bova. He arrived attired in well-worn high-top sneakers, jeans whose major characteristic was that they looked comfortable, and a sports coat whose better days had been years before. He carried what must have been the world's largest jock bag, crammed with newly-purchased bottles of wine that did not quite fit into the zippered closing. He sat down behind a long table with the other writers and managed to behave conventionally for about half the discussion. Then, apparently able to stay put no longer, he leapt up, walked around to the front of the table to be closer to the audience, and paced back and forth, gesturing and talking. The other three writers (none exactly shrinking violets) tried to interrupt but finally lapsed into what might have been either respectful or overwhelmed silence. It was one of the most extraordinary performances I have ever seen.
Bester published two short stories in 1979 and rang in the 1980s with the publication of two new novels: "Golem100" (1980), and "The Deceivers" (1981). In addition to his failing eyesight, other health issues began to affect him, and Bester produced no new published work after 1981. His wife Rolly died in 1984. In the following years, Bester dated Judith H. McQuown [pronounced "McQueen"].
In 1985, it was announced that Bester would be Guest of Honor at the 1987 Worldcon, to be held in Brighton, England. As the event neared, however, Bester fell and broke his hip. With his worsening overall health, he was plainly too ill to attend. Doris Lessing stepped in as a last-minute replacement.
Bester died less than a month after the convention from complications related to his broken hip. However, shortly before his death he learned that the Science Fiction Writers of America would honor him with their Grand Master Nebula award at their 1988 convention.
Two works by Bester were issued posthumously. The first, "Tender Loving Rage" (1991), was a mainstream (i.e., non-science fiction) novel that was probably written in the late 1950s or early 1960s. The second, "Psychoshop" (1998), was based on an incomplete 92-page story fragment. It was completed by Roger Zelazny and remained unpublished until three years after Zelazny's death. When issued, it was credited as a collaborative work.
Alfred Bester had no children, and according to legend, left everything to his bartender, Joe Suder. That much is, in fact, true. However, the claim that Suder didn't know or remember Bester is legend rather than fact; Bester stopped by Suder's bar every morning on his way to get his mail, and the two men were friends.
Awards.
The Science Fiction Writers of America made Bester its 9th SFWA Grand Master in 1988 (announced before his 1987 death) and the Science Fiction and Fantasy Hall of Fame inducted him in 2001, its sixth class of two deceased and two living writers.
Beside winning the inaugural Hugo Award he was one of the runners-up for several annual literary awards.
Hugo Award:
Hugo nominations:
In the Best Novel categories, "The Computer Connection" was a finalist for both the Hugo and Nebula Awards and third place for the Locus Award.

</doc>
<doc id="46037" url="http://en.wikipedia.org/wiki?curid=46037" title="Porpoise">
Porpoise

Porpoises (; also called mereswine) are small cetaceans of the family Phocoenidae; they are related to whales and dolphins. They are distinct from dolphins, although the word "porpoise" has been used to refer to any small dolphin, especially by sailors and fishermen. The most obvious visible difference between the two groups is that porpoises have shorter beaks and flattened, spade-shaped teeth distinct from the conical teeth of dolphins.
The name derives from Old French "porpeis", for "porc peis" (Medieval Latin: "porcus piscis"), pig-fish. The Modern French term "marsouin" is cognate to Old English "mereswīn".
Porpoises, divided into six species, live in all oceans, and mostly near the shore. Freshwater populations of the finless porpoise also exist. Probably the best known species is the harbour porpoise, which can be found across the Northern Hemisphere. Like all toothed whales, porpoises are predators, using sounds (echolocation in sonar form) to locate prey and to coordinate with others. They hunt fish, squid, and crustaceans.
Taxonomy and evolution.
Porpoises, along with whales and dolphins, are descendants of land-living ungulates (hoofed animals) that first entered the oceans around 50 million years ago (Mya). During the Miocene (23 to 5 Mya), mammals were fairly modern. The cetaceans diversified, and fossil evidence suggests porpoises and dolphins diverged from their last common ancestor around 15 Mya. The oldest fossils are known from the shallow seas around the North Pacific, with animals spreading to the European coasts and Southern Hemisphere only much later, during the Pliocene.
Suborder Odontoceti toothed whales
Recently discovered hybrids between male harbour porpoises and female Dall's porpoises indicate the two species may actually be members of the same genus.
Species of porpoise.
There are seven recognized species of porpoise.
Physical characteristics.
Porpoises tend to be smaller but stouter than dolphins. They have small, rounded heads and blunt jaws instead of beaks. While dolphins have a round, bulbous "melon", porpoises do not. Their teeth are spade-shaped, whereas dolphins have conical teeth. In addition, a porpoise's dorsal fin is generally triangular, rather than curved like that of many dolphins and large whales. Some species have small bumps, known as tubercles, on the leading edge of the dorsal fin. The function of these bumps is unknown.
These animals are the smallest cetaceans, reaching body lengths up to 2.5 m; the smallest species is the vaquita, reaching up to 1.5 m. In terms of weight, the lightest is the finless porpoise at 30 to, and the heaviest is Dall's porpoise at 130 to. Because of their small size, porpoises lose body heat to the water more rapidly than other cetaceans. Their stout shape, which minimizes surface area, may be an adaptation to reduce heat loss. Thick blubber also insulates them from the cold. The small size of porpoises requires them to eat frequently, rather than depending on fat reserves.
Life history.
Porpoises bear young more quickly than dolphins. Female Dall's and harbour porpoises often become pregnant with a single calf each year, and pregnancy lasts for about 11 months. Porpoises have been known to live 8–10 years, although some have lived to be 20.
Behavior.
Porpoises prey on fish, squid, and crustaceans. Although they are capable of dives up to 200 m, they generally hunt in shallow coastal waters. They are found most commonly in small groups of fewer than ten individuals, referred to as pods. Rarely, some species form brief aggregations of several hundred animals. Like all toothed whales, they are capable of echolocation for finding prey and group coordination. Porpoises are fast swimmers—Dall's porpoise is said to be one of the fastest cetaceans, with a speed of 55 km/h (34 mph). Porpoises tend to be less acrobatic and more sexually aggressive than dolphins.
Humans and porpoises.
Accidental entanglement (bycatch) in fishing nets is the main threat to porpoises today. One of the most endangered cetacean species is the vaquita, having a limited distribution in the Gulf of California, a highly industrialized area. In some countries, porpoises are hunted for food or bait meat.
Porpoises are rarely held in captivity in zoos or oceanaria, as they are generally not as capable of adapting to tank life or as easily trained as dolphins.
Traditionally porpoises under English law were a royal fish which belonged to the Crown.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="46038" url="http://en.wikipedia.org/wiki?curid=46038" title="Jeremy Bentham">
Jeremy Bentham

Jeremy Bentham (; 15 February [O.S. 4 February] 1748 – 6 June 1832) was a British philosopher, jurist, and social reformer. He is regarded as the founder of modern utilitarianism.
Bentham became a leading theorist in Anglo-American philosophy of law, and a political radical whose ideas influenced the development of welfarism. He advocated individual and economic freedom, the separation of church and state, freedom of expression, equal rights for women, the right to divorce, and the decriminalising of homosexual acts. He called for the abolition of slavery, the abolition of the death penalty, and the abolition of physical punishment, including that of children. He has also become known in recent years as an early advocate of animal rights. Though strongly in favour of the extension of individual legal rights, he opposed the idea of natural law and natural rights, calling them "nonsense upon stilts".
Bentham's students included his secretary and collaborator James Mill, the latter's son, John Stuart Mill, the legal philosopher John Austin, as well as Robert Owen, one of the founders of utopian socialism. Bentham has been described as the "spiritual founder" of University College London, though he played little direct part in its foundation.
Life.
Bentham was born in Houndsditch, London, into a wealthy family that supported the Tory party. He was reportedly a child prodigy: he was found as a toddler sitting at his father's desk reading a multi-volume history of England, and he began to study Latin at the age of three. He had one surviving sibling, Samuel Bentham, with whom he shared a close bond.
He attended Westminster School and, in 1760, at age 12, was sent by his father to The Queen's College, Oxford, where he completed his Bachelor's degree in 1763 and his Master's degree in 1766. He trained as a lawyer and, though he never practised, was called to the bar in 1769. He became deeply frustrated with the complexity of the English legal code, which he termed the "Demon of Chicane".
When the American colonies published their Declaration of Independence in July 1776, the British government did not issue any official response but instead secretly commissioned London lawyer and pamphleteer John Lind to publish a rebuttal. His 130-page tract was distributed in the colonies and contained an essay titled "Short Review of the Declaration" written by Bentham, a friend of Lind's, which attacked and mocked the Americans' political philosophy.
Among his many proposals for legal and social reform was a design for a prison building he called the Panopticon. He spent some sixteen years of his life developing and refining his ideas for the building, and hoped that the government would adopt the plan for a National Penitentiary, and appoint him as contractor-governor. Although the prison was never built, the concept had an important influence on later generations of thinkers. Twentieth-century French philosopher Michel Foucault argued that the Panopticon was paradigmatic of several 19th-century "disciplinary" institutions.
Bentham became convinced that his plans for the Panopticon had been thwarted by the King and an aristocratic elite acting in their own interests. It was largely because of his brooding sense of injustice that he developed his ideas of "sinister interest" – that is, of the vested interests of the powerful conspiring against a wider public interest – which underpinned many of his broader arguments for reform.
More successful was his cooperation with Patrick Colquhoun in tackling the corruption in the Pool of London. This resulted in the Thames Police Bill of 1798, which was passed in 1800. The bill created the Thames River Police, which was the first preventive police force in the country and was a precedent for Robert Peel's reforms 30 years later.
Bentham was in correspondence with many influential people. Adam Smith, for example, opposed free interest rates before he was made aware of Bentham's arguments on the subject. As a result of his correspondence with Mirabeau and other leaders of the French Revolution, Bentham was declared an honorary citizen of France. He was an outspoken critic of the revolutionary discourse of natural rights and of the violence that arose after the Jacobins took power (1792). Between 1808 and 1810, he held a personal friendship with Latin American Independence Precursor Francisco de Miranda and paid visits to Miranda's Grafton Way house in London.
In 1823, he co-founded the "Westminster Review" with James Mill as a journal for the "Philosophical Radicals" – a group of younger disciples through whom Bentham exerted considerable influence in British public life. One was John Bowring, to whom Bentham became devoted, describing their relationship as "son and father": he appointed Bowring political editor of the "Westminster Review", and eventually his literary executor. Another was Edwin Chadwick, who wrote on hygiene, sanitation and policing and was a major contributor to the Poor Law Amendment Act: Bentham employed Chadwick as a secretary and bequeathed him a large legacy.
An insight into his character is given in Michael St. John Packe's "The Life of John Stuart Mill":
 During his youthful visits to Bowood House, the country seat of his patron Lord Lansdowne, he had passed his time at falling unsuccessfully in love with all the ladies of the house, whom he courted with a clumsy jocularity, while playing chess with them or giving them lessons on the harpsichord. Hopeful to the last, at the age of eighty he wrote again to one of them, recalling to her memory the far-off days when she had "presented him, in ceremony, with the flower in the green lane" [citing Bentham's memoirs]. To the end of his life he could not hear of Bowood without tears swimming in his eyes, and he was forced to exclaim, "Take me forward, I entreat you, to the future – do not let me go back to the past."
A psychobiographical study by Philip Lucas and Anne Sheeran argues that he may have had Asperger's syndrome.
Death and the auto-icon.
Bentham died on 6 June 1832 aged 84 at his residence in Queen Square Place in Westminster, London. He had continued to write up to a month before his death, and had made careful preparations for the dissection of his body after death and its preservation as an auto-icon. As early as 1769, when Bentham was just twenty-one years old, he made a will leaving his body for dissection to a family friend, the physician and chemist George Fordyce, whose daughter, Maria Sophia (1765–1858), married Jeremy's brother Samuel Bentham. A paper written in 1830, instructing Thomas Southwood Smith to create the auto-icon, was attached to his last will, dated 30 May 1832.
On 8 June 1832, two days after his death, invitations were distributed to a select group of friends, and on the following day at 3 p.m., Southwood Smith delivered a lengthy oration over Bentham's remains in the Webb Street School of Anatomy & Medicine in Southwark, London. The printed oration contains a frontispiece with an engraving of Bentham's body partly covered by a sheet.
Afterward, the skeleton and head were preserved and stored in a wooden cabinet called the "Auto-icon", with the skeleton padded out with hay and dressed in Bentham's clothes. Originally kept by his disciple Thomas Southwood Smith, it was acquired by University College London in 1850. It is normally kept on public display at the end of the South Cloisters in the main building of the college; however, for the 100th and 150th anniversaries of the college, and in 2013, it was brought to the meeting of the College Council, where it was listed as "present but not voting".
Bentham had intended the Auto-icon to incorporate his actual head, mummified to resemble its appearance in life. However, Southwood Smith's experimental efforts at mummification, based on practices of the indigenous people of New Zealand and involving placing the head under an air pump over sulphuric acid and simply drawing off the fluids, although technically successful, left the head looking distastefully macabre, with dried and darkened skin stretched tautly over the skull. The Auto-icon was therefore given a wax head, fitted with some of Bentham's own hair. The real head was displayed in the same case as the Auto-icon for many years, but became the target of repeated student pranks. It is now locked away securely.
A 360-degree rotatable, high-resolution is available at the UCL Bentham Project's website.
Work.
Utilitarianism.
Bentham's ambition in life was to create a "Pannomion", a complete utilitarian code of law. He not only proposed many legal and social reforms, but also expounded an underlying moral principle on which they should be based. This philosophy of utilitarianism took for its "fundamental axiom", "it is the greatest happiness of the greatest number that is the measure of right and wrong"". Bentham claimed to have borrowed this concept from the writings of Joseph Priestley, although the closest that Priestley in fact came to expressing it was in the form "the good and happiness of the members, that is the majority of the members of any state, is the great standard by which every thing relating to that state must finally be determined".
The "greatest happiness principle", or the principle of utility, forms the cornerstone of all Bentham's thought. By "happiness", he understood a predominance of "pleasure" over "pain". He wrote in "The Principles of Morals and Legislation":
Nature has placed mankind under the governance of two sovereign masters, pain and pleasure. It is for them alone to point out what we ought to do, as well as to determine what we shall do. On the one hand the standard of right and wrong, on the other the chain of causes and effects, are fastened to their throne. They govern us in all we do, in all we say, in all we think ...
He also suggested a procedure for estimating the moral status of any action, which he called the Hedonistic or felicific calculus. Utilitarianism was revised and expanded by Bentham's student John Stuart Mill. In Mill's hands, "Benthamism" became a major element in the liberal conception of state policy objectives.
In his exposition of the felicific calculus, Bentham proposed a classification of 12 pains and 14 pleasures, by which we might test the "happiness factor" of any action. Nonetheless, it should not be overlooked that Bentham's "hedonistic" theory (a term from J.J.C. Smart), unlike Mill's, is often criticized for lacking a principle of fairness embodied in a conception of justice. In "Bentham and the Common Law Tradition", Gerald J. Postema states: "No moral concept suffers more at Bentham's hand than the concept of justice. There is no sustained, mature analysis of the notion..." Thus, some critics object, it would be acceptable to torture one person if this would produce an amount of happiness in other people outweighing the unhappiness of the tortured individual. However, as P.J. Kelly argued in "Utilitarianism and Distributive Justice: Jeremy Bentham and the Civil Law", Bentham had a theory of justice that prevented such consequences. According to Kelly, for Bentham the law "provides the basic framework of social interaction by delimiting spheres of personal inviolability within which individuals can form and pursue their own conceptions of well-being". It provides security, a precondition for the formation of expectations. As the hedonic calculus shows "expectation utilities" to be much higher than natural ones, it follows that Bentham does not favour the sacrifice of a few to the benefit of the many.
Bentham's "An Introduction to the Principles of Morals and Legislation" focuses on the principle of utility and how this view of morality ties into legislative practices. His principle of utility regards "good" as that which produces the greatest amount of pleasure and the minimum amount of pain and "evil" as that which produces the most pain without the pleasure. This concept of pleasure and pain is defined by Bentham as physical as well as spiritual. Bentham writes about this principle as it manifests itself within the legislation of a society. He lays down a set of criteria for measuring the extent of pain or pleasure that a certain decision will create.
The criteria are divided into the categories of intensity, duration, certainty, proximity, productiveness, purity, and extent. Using these measurements, he reviews the concept of punishment and when it should be used as far as whether a punishment will create more pleasure or more pain for a society. He calls for legislators to determine whether punishment creates an even more evil offence. Instead of suppressing the evil acts, Bentham argues that certain unnecessary laws and punishments could ultimately lead to new and more dangerous vices than those being punished to begin with, and calls upon legislators to measure the pleasures and pains associated with any legislation and to form laws in order to create the greatest good for the greatest number. He argues that the concept of the individual pursuing his or her own happiness cannot be necessarily declared "right", because often these individual pursuits can lead to greater pain and less pleasure for a society as a whole. Therefore, the legislation of a society is vital to maintain the maximum pleasure and the minimum degree of pain for the greatest number of people.
Economics.
Bentham's opinions about monetary economics were completely different from those of David Ricardo; however, they had some similarities to those of Henry Thornton. He focused on monetary expansion as a means of helping to create full employment. He was also aware of the relevance of forced saving, propensity to consume, the saving-investment relationship, and other matters that form the content of modern income and employment analysis. His monetary view was close to the fundamental concepts employed in his model of utilitarian decision making. His work is considered to be an early precursor of modern welfare economics.
Bentham stated that pleasures and pains can be ranked according to their value or "dimension" such as intensity, duration, certainty of a pleasure or a pain. He was concerned with maxima and minima of pleasures and pains; and they set a precedent for the future employment of the maximisation principle in the economics of the consumer, the firm and the search for an optimum in welfare economics.
Law reform.
Bentham was the first person to aggressively advocate for the codification of "all" of the common law into a coherent set of statutes; he was actually the person who coined the verb "to codify" to refer to the process of drafting a legal code. He lobbied hard for the formation of codification commissions in both England and the United States, and went so far as to write to President James Madison in 1811 to volunteer to write a complete legal code for the young country. After he learned more about American law and realized that most of it was state-based, he promptly wrote to the governors of every single state with the same offer.
During his lifetime, Bentham's codification efforts were completely unsuccessful. Even today, they have been completely rejected by almost every common law jurisdiction, including England. However, his writings on the subject laid the foundation for the moderately successful codification work of David Dudley Field II in the United States a generation later.
Animal rights.
Bentham is widely regarded as one of the earliest proponents of animal rights, and has even been hailed as "the first patron saint of animal rights". He argued that the ability to suffer, not the ability to reason, should be the benchmark, or what he called the "insuperable line". If reason alone were the criterion by which we judge who ought to have rights, human infants and adults with certain forms of disability might fall short, too. In 1789, alluding to the limited degree of legal protection afforded to slaves in the French West Indies by the Code Noir, he wrote:
The day has been, I am sad to say in many places it is not yet past, in which the greater part of the species, under the denomination of slaves, have been treated by the law exactly upon the same footing, as, in England for example, the inferior races of animals are still. The day "may" come when the rest of the animal creation may acquire those rights which never could have been witholden from them but by the hand of tyranny. The French have already discovered that the blackness of the skin is no reason a human being should be abandoned without redress to the caprice of a tormentor. It may one day come to be recognised that the number of the legs, the villosity of the skin, or the termination of the "os sacrum" are reasons equally insufficient for abandoning a sensitive being to the same fate. What else is it that should trace the insuperable line? Is it the faculty of reason or perhaps the faculty of discourse? But a full-grown horse or dog, is beyond comparison a more rational, as well as a more conversable animal, than an infant of a day or a week or even a month, old. But suppose the case were otherwise, what would it avail? The question is not, Can they "reason"? nor, Can they "talk"? but, Can they "suffer"?
Earlier in that paragraph, Bentham makes clear that he accepted that animals could be killed for food, or in defence of human life, provided that the animal was not made to suffer unnecessarily. Bentham did not object to medical experiments on animals, providing that the experiments had in mind a particular goal of benefit to humanity, and had a reasonable chance of achieving that goal. He wrote that otherwise he had a "decided and insuperable objection" to causing pain to animals, in part because of the harmful effects such practices might have on human beings. In a letter to the editor of the "Morning Chronicle" in March 1825, he wrote:
I never have seen, nor ever can see, any objection to the putting of dogs and other inferior animals to pain, in the way of medical experiment, when that experiment has a determinate object, beneficial to mankind, accompanied with a fair prospect of the accomplishment of it. But I have a decided and insuperable objection to the putting of them to pain without any such view. To my apprehension, every act by which, without prospect of preponderant good, pain is knowingly and willingly produced in any being whatsoever, is an act of cruelty; and, like other bad habits, the more the correspondent habit is indulged in, the stronger it grows, and the more frequently productive of its bad fruit. I am unable to comprehend how it should be, that to him to whom it is a matter of amusement to see a dog or a horse suffer, it should not be matter of like amusement to see a man suffer; seeing, as I do, how much more morality as well as intelligence, an adult quadruped of those and many other species has in him, than any biped has for some months after he has been brought into existence; nor does it appear to me how it should be, that a person to whom the production of pain, either in the one or in the other instance, is a source of amusement, would scruple to give himself that amusement when he could do so under an assurance of impunity.
Gender and sexuality.
Bentham said that it was the placing of women in a legally inferior position that made him choose, at the age of eleven, the career of a reformist. Bentham spoke for a complete equality between sexes.
The essay argued for the liberalisation of laws prohibiting homosexual sex. The essay remained unpublished during his lifetime for fear of offending public morality. It was published for the first time in 1931. Bentham does not believe homosexual acts to be unnatural, describing them merely as "irregularities of the venereal appetite". The essay chastises the society of the time for making a disproportionate response to what Bentham appears to consider a largely private offence – public displays or forced acts being dealt with rightly by other laws. When the essay was published in the "Journal of Homosexuality" in 1978, the "Abstract" stated that Bentham's essay was the "first known argument for homosexual law reform in England."
Privacy.
For Bentham, transparency had moral value. For example, journalism puts power-holders under moral scrutiny. However, Bentham wanted such transparency to apply to everyone. This he describes by picturing the world as a gymnasium in which each "gesture, every turn of limb or feature, in those whose motions have a visible impact on the general happiness, will be noticed and marked down". He considered both surveillance and transparency to be useful ways of generating understanding and improvements for people's lives.
Bentham and University College London.
Bentham is widely associated with the foundation in 1826 of London University (the institution that, in 1836, became University College London), though he was 78 years old when the University opened and played only an indirect role in its establishment. His direct involvement was limited to his buying a single £100 share in the new University, making him just one of over a thousand shareholders.
Bentham and his ideas can nonetheless be seen as having inspired several of the actual founders of the University. He strongly believed that education should be more widely available, particularly to those who were not wealthy or who did not belong to the established church; in Bentham's time, membership of the Church of England and the capacity to bear considerable expenses were required of students entering the Universities of Oxford and Cambridge. As the University of London was the first in England to admit all, regardless of race, creed or political belief, it was largely consistent with Bentham's vision. There is some evidence that, from the sidelines, he played a "more than passive part" in the planning discussions for the new institution, although it is also apparent that "his interest was greater than his influence". He failed in his efforts to see his disciple John Bowring appointed professor of English or History, but he did oversee the appointment of another pupil, John Austin, as the first professor of Jurisprudence in 1829.
The more direct associations between Bentham and UCL – the College's custody of his Auto-icon (see above) and of the majority of his surviving papers – postdate his death by some years: the papers were donated in 1849, and the Auto-icon in 1850. A large painting by Henry Tonks hanging in UCL's Flaxman Gallery depicts Bentham approving the plans of the new university, but it was executed in 1922 and the scene is entirely imaginary. Since 1959 (when the Bentham Committee was first established) UCL has hosted the Bentham Project, which is progressively publishing a definitive edition of Bentham's writings.
UCL now endeavours to acknowledge Bentham's influence on its foundation, while avoiding any suggestion of direct involvement, by describing him as its "spiritual founder".
Bibliography.
Bentham was an obsessive writer and reviser, but was constitutionally incapable, except on rare occasions, of bringing his work to completion and publication. Most of what appeared in print in his lifetime (see this ) was prepared for publication by others. Several of his works first appeared in French translation, prepared for the press by Étienne Dumont, for example, Theory of Legislation, Volume 2 (Principles of the Penal Code) 1840, Weeks, Jordan, & Company. Boston. Some made their first appearance in English in the 1820s as a result of back-translation from Dumont's 1802 collection (and redaction) of Bentham's writing on civil and penal legislation.
Publications.
Works published in Bentham's lifetime include:
Posthumous publications.
On his death, Bentham left manuscripts amounting to an estimated 30,000,000 words, which are now largely held by UCL's Special Collections (c. 60,000 manuscript folios), and the British Library (c.15,000 folios). 
Bowring (1838–43).
John Bowring, the young radical writer who had been Bentham's intimate friend and disciple, was appointed his literary executor and charged with the task of preparing a collected edition of his works. This appeared in 11 volumes in 1838–1843. Bowring based much of his edition on previously published texts (including those of Dumont) rather than Bentham's own manuscripts, and elected not to publish Bentham's works on religion at all. The edition was described by the "Edinburgh Review" on first publication as "incomplete, incorrect and ill-arranged", and has since been repeatedly criticised both for its omissions and for errors of detail; while Bowring's memoir of Bentham’s life included in volumes 10 and 11 was described by Sir Leslie Stephen as "one of the worst biographies in the language". Nevertheless, Bowring's remained the standard edition of most of Bentham's writings for over a century, and is still only partially superseded: it includes such interesting writings on international relations as Bentham's A Plan for an Universal and Perpetual Peace written 1786–89, which forms part IV of the Principles of International Law.
Stark (1952–54).
In 1952–54, Werner Stark published a three-volume set, "Jeremy Bentham's Economic Writings", in which he attempted to bring together all of Bentham's writings on economic matters, including both published and unpublished material. Although a significant achievement, the work is considered by scholars to be flawed in many points of detail, and a new edition of the economic writings is currently in preparation by the Bentham Project.
Bentham Project (1968–present).
In 1959, the Bentham Committee was established under the auspices of University College London with the aim of producing a definitive edition of Bentham's writings. 
It set up the to undertake the task, and the first volume in "The Collected Works of Jeremy Bentham" was published in 1968. 
To date, 30 volumes have appeared; the complete edition is projected to run to around seventy. The Project is currently digitising the Bentham papers by crowdsourcing their transcription.
Transcribe Bentham is an award-winning crowdsourced manuscript transcription project, run by University College London's Bentham Project, in partnership with UCL's UCL Centre for Digital Humanities, UCL Library Services, UCL Learning and Media Services, the University of London Computer Centre, and the online community. The project was launched in September 2010 and is making freely available, via a specially designed transcription interface, digital images of UCL's vast Bentham Papers collection – which runs to some 60,000 manuscript folios – to engage the public and recruit volunteers to help transcribe the material. Volunteer-produced transcripts will contribute to the Bentham Project's production of the new edition of "The Collected Works of Jeremy Bentham", and will be uploaded to UCL's digital Bentham Papers repository, widening access to the collection for all and ensuring its long-term preservation. Manuscripts can be viewed and transcribed by signing-up for a transcriber account at the Transcription Desk, via the Transcribe Bentham website.
Legacy.
The Faculty of Laws at University College London occupies Bentham House, next to the main UCL campus.
Bentham's name was adopted by the Australian litigation funder IMF Limited to become Bentham IMF Limited on 28 November 2013, in recognition of Bentham being "among the first to support the utility of litigation funding".
Ivan Vazov, national poet and man of letters of Bulgaria (then recently liberated from Ottoman rule, but divided by the Treaty of Berlin) refers to Bentham in his 1881 poem .
References.
</dl>
External links.
</dl>

</doc>
<doc id="46043" url="http://en.wikipedia.org/wiki?curid=46043" title="Pen name">
Pen name

A pen name, nom de plume, or literary double, is a pseudonym adopted by an author. The author's real name may be known to only the publisher, or may come to be common knowledge.
Western literature.
An author may use a pen name if his or her real name is likely to be confused with that of another author or notable individual. For instance Winston Churchill wrote under the name Winston S. Churchill to distinguish his writings from those of the American novelist of the same name, who was at the time much better known. 
Some authors who regularly write in more than one genre use different pen names for each. Romance writer Nora Roberts writes erotic thrillers under the pen name J.D. Robb, and Samuel Langhorne Clemens used the aliases "Mark Twain" and "Sieur Louis de Conte" for different works. Similarly, an author who writes both fiction and non-fiction (such as the mathematician and fantasy writer Charles Dodgson, who wrote as Lewis Carroll) may use a pseudonym for fiction writing. Science fiction author Harry Turtledove has used the name H.N. Turtletaub for a number of historical novels he has written because he and his publisher felt that the presumed lower sales of those novels might hurt book store orders for the novels he writes under his own name. 
Occasionally a pen name is employed to avoid overexposure. Prolific authors for pulp magazines often had two and sometimes three short stories appearing in one issue of a magazine; the editor would create several fictitious author names to hide this from readers. Robert A. Heinlein wrote stories under pseudonyms of Anson MacDonald (a combination of his middle name and his then-wife's maiden name) and Caleb Strong so that more of his works could be published in a single magazine. Stephen King published four novels under the name Richard Bachman because publishers didn't feel the public would buy more than one novel per year from a single author. Eventually, after critics found a large number of style similarities, publishers revealed Bachman's true identity.
Sometimes a pen name is used because an author believes that his name does not suit the genre he is writing in. Western novelist Pearl Gray dropped his first name and changed the spelling of his last name to become Zane Grey, because he believed that his real name did not suit the Western genre. Romance novelist Angela Knight writes under that name instead of her actual name (Julie Woodcock) because of the double entendre of her surname in the context of that genre.
Edward Gorey had dozens of pseudonyms, each one an anagram of his real name.
A pen name may be shared by different writers in order to suggest continuity of authorship. Thus the Bessie Bunter series of English boarding-school stories, initially written by the prolific Charles Hamilton under the name Hilda Richards, was taken on by other authors who continued to use the same pen-name.
C. S. Lewis used two different pseudonyms for different reasons. He published a collection of poems ("Spirits in Bondage") and a narrative poem ("Dymer") under the pen name "Clive Hamilton", to avoid harming his reputation as a don at Oxford University. His book entitled "A Grief Observed", which describes his experience of bereavement, was originally released under the pseudonym "N. W. Clerk"
Essayist and poet Eric Blair adopted the pseudonym George Orwell for most of his books, including "Animal Farm" and "Nineteen Eighty Four". This was done because he felt himself to be insufficiently established in his writing career to publish under his real name.
In some forms of fiction, the pen name adopted is the name of the lead character, to suggest to the reader that the book is a (fictional) autobiography. Daniel Handler used the pseudonym Lemony Snicket to present his "A Series of Unfortunate Events" books as memoirs by an acquaintance of the main characters.
Legendary comic book writer Stan Lee was born Stanley Martin Lieber. He adopted the pen name Stan Lee because he intended to save his real name for more literary products other than comic books. However, Lee's hopes for a novelistic career never materialized; although he became arguably the most important comic book creator in history. He legally changed his name to Stan Lee because he had become better known under his pen name.
Some, however, do this to fit a certain theme. One famous example, Pseudonymous Bosch, used his pen name just to expand the theme of secrecy in The Secret Series.
Female authors.
Some female authors have used pen names to ensure that their works were accepted by publishers and/or the public. Such is the case of Peru's famous Clarinda, whose work was published in the early 17th century. More often, women have adopted masculine pen names. This was common in the 19th century, when women were beginning to make inroads into literature but, it was felt, would not be taken as seriously by readers as female authors. For example, Mary Ann Evans wrote under the pen name George Eliot; and Amandine Aurore Lucile Dupin, Baronne Dudevant, used the pseudonym George Sand. Charlotte, Emily and Anne Brontë published under the names Currer, Ellis, and Acton Bell respectively. French-Savoyard writer and poet Amélie Gex chose to publish as Dian de Jeânna ("John, son of Jane") during the first half of her career. Karen Blixen's very successful "Out of Africa" (1937) was originally published under the pen name Isak Dinesen. Victoria Benedictsson, one of the most famous Swedish authors of the 19th century, wrote under the name Ernst Ahlgren.
More recently, women who write in genres normally written by men sometimes choose to use initials, such as K. A. Applegate, P. N. Elrod, D. C. Fontana, S. E. Hinton, G. A. Riplinger, J. D. Robb, E.B. Brown and J. K. Rowling. Alternatively, they may use a unisex pen name, such as Robin Hobb (the second pen name of novelist Margaret Astrid Lindholm Ogden).
Collective names.
A collective name, also known as a house name, is sometimes used with series fiction published under one pen name even though more than one author may have contributed to the series. In some cases the first books in the series were written by one writer, but subsequent books were written by ghost writers. For instance, many of the later books in The Saint adventure series were not written by Leslie Charteris, the series' originator. Similarly, Nancy Drew mystery books are published as though they were written by Carolyn Keene, The Hardy Boys books are published as the work of Franklin W. Dixon, and The Bobbsey Twins series are credited to Laura Lee Hope, although several authors have been involved in each series.
Collaborative authors may have also their works published under a single pen name. Frederic Dannay and Manfred B. Lee published their mystery novels and stories under the pen name Ellery Queen (as well as publishing the work of ghost-writers under the same name). Cherith Baldry, Kate Cary, and Victoria Holmes wrote the Warriors series under the pseudonym of Erin Hunter to keep their readers from searching all over the library for their books. The writers of "Atlanta Nights", a deliberately bad book intended to embarrass the publishing firm PublishAmerica, used the pen name Travis Tea. Sometimes multiple authors will write related books under the same pseudonym; examples include T. H. Lain in fiction.
Nicolas Bourbaki is the collective pseudonym under which a group of (mainly French) 20th-century mathematicians wrote a series of books presenting an exposition of modern advanced mathematics, beginning in 1935. With the goal of founding all of mathematics on set theory, the group strove for utmost rigour and generality, creating some new terminology and concepts along the way.
In 2007, three Slovenian artists legally changed their names to Janez Janša, the Slovenia’s economic-liberal, conservative prime minister at the time. When publicly asked whether this gesture was of an affirmative or subversive nature, they claimed they did it for "personal reasons". Most of their works, including art exhibitions, theatrical pieces, and publications, have since been signed under this name.
Suba is an example of collective pen name in Tamil literature. Kozma Prutkov is an example of collective pen name in Russian literature.
Concealment of identity.
A pseudonym may be used to protect the writer for exposé books about espionage or crime. Former SAS soldier Andy McNab used a pseudonym for his book about a failed SAS mission titled "Bravo Two Zero". The name "Ibn Warraq" ("son of a papermaker") has been used by dissident Muslim authors. Author Brian O'Nolan used the pen names "Flann O'Brien" and "Myles na gCopaleen" for his novels and journalistic writing from the 1940s to the 1960s because Irish civil servants were not allowed at that time to publish works under their own names. The identity of the enigmatic twentieth century novelist B. Traven has never been conclusively revealed, despite thorough research.
The "Histoire d'O" ("The Story of O"), an erotic novel of sadomasochism and sexual slavery, was written by an editorial secretary with a reputation for near-prudery who used the pseudonym Pauline Réage.
For reasons unknown, but perhaps to escape the notice of English authorities, the author Robert Noonan chose the name Robert Tressell as his pseudonym to write the novel "The Ragged Trousered Philanthropists."
Alice Bradley Sheldon had a multiplicity of reasons for writing under the pen names of James Tiptree, Jr. and Racoona Sheldon: she was a woman writing in the heavily male-dominated genre of science fiction; she was a bisexual woman who may have wanted to avoid the inherent biases of her readers; and she was a career intelligence officer, first in the Army Air Corps and then in the early years of the CIA, for whom concealment was a way of life.
After Stephen King's pen name Richard Bachman got released to the public, King said that Bachman had "died of cancer". After this, King began writing most of his stories under his own name. The event of King's pen name's being released helped conceive another novel, "The Dark Half" (1989).
A multiple-use name or anonymity pseudonym is a pseudonym open for anyone to use and these have been adopted by various groups, often as a protest against the cult of individual creators. In Italy, two anonymous groups of writers have gained some popularity with the collective names of Luther Blissett and Wu Ming.
Eastern cultures.
India.
In Indian Languages, writers put it at the end of their names, like Ramdhari Singh 'Dinkar'. Sometimes they also write under their pen name without their actual name like Firaq Gorakhpuri.
In early Indian literature, we find authors shying away from using any name considering it to be egotistical. Due to this notion, even today it is hard to trace the authorship of many earlier literary works from India. Later, we find that the writers adopted the practice of using the name of their deity of worship or Guru's name as their pen name. In this case, typically the pen name would be included at the end of the prose or poetry.
Composers of Indian classical music used pen names in compositions to assert authorship, including Sadarang, Gunarang (Fayyaz Ahmed Khan), Ada Rang (court musician of Muhammad Shah), and Ramrang (Ramashreya Jha). Other compositions are apocryphally ascribed to composers with their pen names.
Japan.
Japanese poets who write haiku often use a "haigō" (俳号). The famous haiku poet Matsuo Bashō had used two other haigō before he became fond of a banana plant ("bashō") that had been given to him by a disciple and started using it as his pen name at the age of 36.
Similar to a pen name, Japanese artists usually have a "gō" or art-name, which might change a number of times during their career. In some cases, artists adopted different "gō" at different stages of their career, usually to mark significant changes in their life. One of the most extreme examples of this is Hokusai, who in the period 1798 to 1806 alone used no fewer than six. Manga artist Ogure Ito uses the pen name 'Oh! great' because his real name Ogure Ito is roughly how the Japanese pronounce "oh great".
Persian and Urdu poetry.
A "shâ'er" (Farsi for poet) (a poet who writes "she'rs" in Urdu or Persian) almost always has a "takhallus", a pen name, traditionally placed at the end of the name (often marked by a graphical sign placed above it) when referring to the poet by his full name. For example Hafez is a pen-name for "Shams al-Din", and thus the usual way to refer to him would be "Shams al-Din Hafez" or just "Hafez". "Mirza Asadullah Baig Khan" (his official name and title) is referred to as "Mirza Asadullah Khan Ghalib", or just "Mirza Ghalib".
Etymology.
Despite the use of French words in the phrase "nom de plume", the term did not originate in France; its origin is Latin. H. W. Fowler and F. G. Fowler, in "The King's English" state that the term "nom de plume" "evolved" in Britain, where people wanting a "literary" phrase failed to understand the term "nom de guerre", which already existed in French. Since "guerre" means "war" in French, "nom de guerre" did not make sense to the British, who did not understand the French metaphor. The term was later exported to France. This topic is further discussed in French-language expression, although amongst French speakers "pseudonyme" is much more common.

</doc>
<doc id="46044" url="http://en.wikipedia.org/wiki?curid=46044" title="Political economy">
Political economy

Political economy was the original term used for studying production and trade, and their relations with law, custom, and government, as well as with the distribution of national income and wealth. "Political economy" originated in moral philosophy. It was developed in the 18th century as the study of the economies of states, or "polities", hence the term "political" economy.
In the late 19th century, the term "economics" came to replace "political economy", coinciding with the publication of an influential textbook by Alfred Marshall in 1890. Earlier, William Stanley Jevons, a proponent of mathematical methods applied to the subject, advocated "economics" for brevity and with the hope of the term becoming "the recognised name of a science." 
Today, "political economy", where it is not used as a synonym for economics, may refer to very different things, including Marxian analysis, applied public-choice approaches emanating from the Chicago school and the Virginia school, or simply the advice given by economists to the government or public on general economic policy or on specific proposals. A rapidly growing mainstream literature from the 1970s has expanded beyond the model of economic policy in which planners maximize utility of a representative individual toward examining how political forces affect the choice of economic policies, especially as to distributional conflicts and political institutions. It is available as an area of study in certain colleges and universities. 
Etymology.
Originally, "political economy" meant the study of the conditions under which production or consumption within limited parameters was organized in nation-states. In that way, political economy expanded the emphasis of economics, which comes from the Greek "oikos" (meaning "home") and "nomos" (meaning "law" or "order"); thus political economy was meant to express the laws of production of wealth at the state level, just as economics was the ordering of the home. The phrase "économie politique" (translated in English as "political economy") first appeared in France in 1615 with the well-known book by Antoine de Montchrétien, "Traité de l’economie politique". The French physiocrats, along with Adam Smith, John Stuart Mill, David Ricardo, Henry George, or Karl Marx were some of the exponents of political economy. The world's first professorship in political economy was established in 1754 at the University of Naples Federico II, Italy (then capital city of the Kingdom of Naples); the Neapolitan philosopher Antonio Genovesi was the first tenured professor. In 1763, Joseph von Sonnenfels was appointed a Political Economy chair at the University of Vienna, Austria. Thomas Malthus, in 1805, became England's first professor of political economy, at the East India Company College, Haileybury, Hertfordshire. Glasgow University, where Smith had been Professor of Logic and of Moral Philosophy, changed the name of its Department of Political Economy to the Department of Economics (ostensibly to avoid confusing prospective undergraduates), in the academic year 1997–98, leaving the class of 1998 as the last to be graduated with a Master of Arts in Political Economy.
In the United States, political economy first was taught at the College of William and Mary, where in 1784, Smith's "The Wealth of Nations" was a required textbook.
Current approaches.
In its contemporary meaning, "political economy" refers to different, but related, approaches to studying economic and related behaviours, ranging from the combination of economics with other fields to the use of different, fundamental assumptions that challenge earlier economic assumptions:
Related disciplines.
Because political economy is not a unified discipline, there are studies using the term that overlap in subject matter, but have radically different perspectives:

</doc>
<doc id="46046" url="http://en.wikipedia.org/wiki?curid=46046" title="Psychedelia">
Psychedelia

Psychedelia is a name given to the subculture of people who use psychedelic drugs, and a style of psychedelic artwork and psychedelic music derived from the experience of altered consciousness that uses highly distorted and surreal visuals, sound effects and reverberation, and bright colors and full spectrums and animation (including cartoons) to evoke and convey to a viewer or listener the artist's experience while using such drugs. The term "psychedelic" is derived from the Ancient Greek words "psychē" (ψυχή, "soul") and "dēloun" (δηλοῦν, "to make visible, to reveal"), translating to "mind-revealing".
A psychedelic experience is characterized by the striking perception of aspects of one's mind previously unknown, or by the creative exuberance of the mind liberated from its ostensibly ordinary fetters. Psychedelic states are an array of experiences including changes of perception such as hallucinations, synesthesia, altered states of awareness or focused consciousness, variation in thought patterns, trance or hypnotic states, mystical states, and other mind alterations. These processes can lead some people to experience changes in mental operation defining their self-identity (whether in momentary acuity or chronic development) different enough from their previous normal state that it can excite feelings of newly formed understanding such as revelation, enlightenment, confusion, and psychosis.
Psychedelic states may be elicited by various techniques, such as meditation, sensory stimulation or deprivation, and most commonly by the use of psychedelic substances. When these psychoactive substances are used for religious, shamanic, or spiritual purposes, they are termed entheogens.
Etymology.
The term was first coined as a noun in 1957 by psychiatrist Humphry Osmond as an alternative descriptor for hallucinogenic drugs in the context of psychedelic psychotherapy. Seeking a name for the experience induced by LSD, Osmond contacted Aldous Huxley, a personal acquaintance and advocate for the therapeutic use of the substance. Huxley coined the term "phanerothyme," from the Greek terms for "manifest" (φανερός) and "spirit" (θύμος). In a letter to Osmond, he wrote:
To make this mundane world sublime,<br>
Take half a gram of phanerothyme
To which Osmond responded:
To fathom Hell or soar angelic,<br>
Just take a pinch of psychedelic
It was on this term that Osmond eventually settled, because it was "clear, euphonious and uncontaminated by other associations." This mongrel spelling of the word 'psychodelic' was loathed by American ethnobotanist Richard Evans Schultes, but championed by Timothy Leary, who thought it sounded better. Due to the expanded use of the term "psychedelic" in pop culture and a perceived incorrect verbal formulation, Carl A.P. Ruck, Jeremy Bigwood, Danny Staples, Jonathan Ott, and R. Gordon Wasson proposed the term "entheogen" to describe the religious or spiritual experience produced by such substances.
History.
Timothy Leary was a well-known proponent of the use of psychedelics, as was Aldous Huxley. However, both advanced widely different opinions on the broad use of psychedelics by state and civil society. Leary promulgated the idea of such substances as a panacea, while Huxley suggested that only the cultural and intellectual elite should partake of entheogens systematically.
The use of psychedelic drugs became widespread in modern Western culture, particularly in the United States and Britain, in the mid-1960s. The movement is credited to Michael Hollingshead who arrived in America from London in 1965. He was sent to the U.S. by other members of the psychedelic movement to get their ideas exposure. The Summer of Love of 1967 and the resultant popularization of the hippie culture to the mainstream popularized psychedelia in the minds of popular culture, where it remained dominant through the 1970s. Resurgences of the style are common in the modern era.
Modern usage.
The impact of psychedelic drugs on western culture in the 1960s led to semantic drift in the use of the word "psychedelic", and it is now frequently used to describe anything with abstract decoration of multiple bright colours, similar to those seen in drug-induced hallucinations. In objection to this new meaning, and to what some consider pejorative meanings of other synonyms such as "hallucinogen" and "psychotomimetic", the term "entheogen" was proposed and is seeing increasing use. However, many consider the term "entheogen" best reserved for religious and spiritual usage, such as certain Native American churches do with the peyote sacrament, and "psychedelic" left to describe those who are using these drugs for recreation, psychotherapy, physical healing, or creative problem solving. In science, hallucinogen remains the standard term.
In popular culture.
In art.
 
Psychedelic artists use highly distorted visuals, cartoons, and bright colors and full spectrums to evoke a sense of altered consciousness. Many artists in the late 1960s and early 1970s attempted to illustrate the psychedelic experience in paintings, drawings, illustrations, and other forms of graphic design.
The counterculture folk music scene frequently used psychedelic designs on posters during the Summer of Love, leading to a popularization of the style. The work of Robert Crumb and others doing posters for hippie bands, such as Big Brother and the Holding Company, spawned interest in the artwork among their followers. Peter Max's psychedelic poster designs helped popularize brightly colored spectrums widely, especially among college students.
One example of this experimentation is seen in Mati Klarwein's painting "Annunciation", which was used as the cover art for Santana's "Abraxas" (1970). The cover of Pink Floyd's album "A Saucerful of Secrets" (1968) is also of this type.
The Beatles' album cover for The Magical Mystery Tour album has features common in psychedelic art, such as a wide color palette and surreal visuals.
Examples frequently recur in the modern era. The cover of Oasis' album, "Dig Out Your Soul" (2008), has a psychedelic album cover, with a slightly muted color scheme. In the modern era, computer graphics may be used to produce psychedelic effects for artwork.
Psychedelic festivals.
A psychedelic festival is a gathering that promotes psychedelic music and art in an effort to unite participants in a communal psychedelic experience. Psychedelic festivals have been described as "temporary communities reproduced via personal and collective acts of transgression...through the routine expenditure of excess energy, and through self-sacrifice in acts of abandonment involving ecstatic dancing often fuelled by chemical cocktails." These festivals often emphasize the ideals of peace, love, unity, and respect. Notable psychedelic festivals include the biennial Boom Festival in Portugal, as well as Nevada's Burning Man and California's Symbiosis Gathering in the United States.
In music.
The fashion for psychedelic drugs gave its name to the style of psychedelia, a term describing a category of rock music known as psychedelic rock, as well as visual art, fashion, and culture that is associated originally with the high 1960s, hippies, and the Haight-Ashbury neighborhood of San Francisco, California. It often used new recording techniques and effects while drawing on Eastern sources such as the ragas and drones of Indian music.
One of the first uses of the word in the music scene of this time was in the 1964 recording of "Hesitation Blues" by folk group the Holy Modal Rounders. The term was introduced to rock music and popularized by the 13th Floor Elevators 1966 album "The Psychedelic Sounds of the 13th Floor Elevators". Psychedelia truly took off in 1967 with the Summer of Love and, although associated with San Francisco, the style soon spread across the US, and worldwide.
The counterculture of the 1960s had a strong influence on the popular culture of the early 1970s. It later became linked to a style of electronic dance music, or rave music, commonly known as psychedelic trance.

</doc>
<doc id="46047" url="http://en.wikipedia.org/wiki?curid=46047" title="Pope Evaristus">
Pope Evaristus

Pope Evaristus (died 107) is accounted the fifth Bishop of Rome, holding office from 99 to his death 107. He was also known as Aristus.
Little is known about St. Evaristus. According to the "Liber Pontificalis", he came from a family of Hellenic Jewish origin living in Bethlehem. He was elected during the reign of the Roman Emperor Domitian, the time of the second general persecution, and succeeded St. Clement in the See of Rome.
Eusebius, in his "Ecclesiastical History" IV, I, stated that Evaristus died in the 12th year of the reign of the Roman Emperor Trajan, after holding the office of bishop of the Romans for eight years. He is said by the "Liber Pontificalis" to have divided Rome into several "titles," or parishes, assigning a priest to each, and appointed seven deacons for the city.
He is usually accorded the title of martyr; however, there is no confirmation of this in the case of Pope Evaristus, who is listed without that title in the Roman Martyrology, with a feast day on 26 October. It is probable that St. Evaristus was buried near St. Peter's tomb in the Vatican. It is also probable that St. John the apostle died during the beginning of Evaristus' reign.
External links.
Listen to this article ()
This audio file was created from a revision of the "Pope Evaristus" article dated 1 July 2014, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="46049" url="http://en.wikipedia.org/wiki?curid=46049" title="Smoke testing (mechanical)">
Smoke testing (mechanical)

Smoke testing refers to various classes of tests of systems, usually intended to determine whether they are ready for more robust testing. The expression probably was first used in plumbing in referring to tests for the detection of cracks, leaks or breaks in closed systems of pipes.
History of the term.
The plumbing industry started using the smoke test in 1875.
Smoke testing in various industries.
Plumbing.
In plumbing a "smoke test" forces non-toxic, artificially created smoke through waste and drain pipes under a slight pressure to find leaks. Plumes of smoke form where there are defects. This test can be performed when the plumbing is brand new, but more often it is used to find sewer gas leaks that may plague a building or an area. Any sign of smoke escaping can be considered a possible site for sewer gas to escape. Sewer gas typically has a rotten egg smell and can contain methane gas, which is explosive, or hydrogen sulfide gas, which is deadly.
Plumbing "smoke tests" are also used to find places where pipes will spill fluid, and to check sanitary sewer systems for places where ground water and storm runoff can enter. Smoke testing is particularly useful in places such as ventilated sanitary sewer systems, where completely sealing the system is not practical.
When smoke testing a sanitary sewer system it is helpful to partially block off the section of sewer to be tested. This can be done by using a sand bag on the end of a rope. The sand bag is lowered into the manhole and swung into position to partially block lines. Completely blocking the line can cause water to back up and prevent smoke from escaping through defects. Smoke testing may not be done after rain or when ground water is unusually high as this may also prevent detection of defects.
Large downdraft fans, usually powered by gasoline engines, are placed on top of open manholes at either end of the section to be tested. If possible all lines in the manholes except for the line between the manholes are partially blocked. Smoke is created using either a smoke bomb or liquid smoke. Smoke bombs are lit and placed on a grate or in a holder on top of each fan, while liquid smoke is injected into the fan via a heating chamber. The fans create a pressure differential that forces the smoke into the sewer at a pressure just above atmospheric. With properly installed plumbing, the traps will prevent the smoke from entering the house and redirect it out the plumbing vents. Defective plumbing systems or dry traps will allow smoke to enter the inside of the house.
The area around the section being tested is searched for smoke plumes. Plumes coming from plumbing vents or the interface between the fan shroud and manhole rim are normal; however, smoke plumes outside of the manhole rim are not. Plumes are marked, usually with flags, and defects are noted using measurements from stationary landmarks like the corners of houses. The plumes or markers may also be photographed.
Woodwind instrument repair.
In woodwind instrument repair, a smoke test involves plugging one end of an instrument and blowing smoke into the other to test for leaks. Escaping smoke reveals improperly seated pads and faulty joints (i.e. leaks). After this test the instrument is cleaned to remove nicotine and other deposits left by the smoke. Due to tobacco smoke being used, this test may be hazardous to the health of the technician in the long run.
Described in a repair manual written in the 1930s. "smoke testing" is considered obsolete, and is no longer used by reputable technicians. The usual alternative to smoke is to place a bright light inside the instrument then check for light appearing around pads and joints.
Automotive repair.
In the same way that plumbing and woodwind instruments are tested, the vacuum systems of automobiles may be tested in order to locate difficult-to-find vacuum leaks. Artificial smoke is deliberately introduced into the system under slight pressure and any leaks are indicated by the escaping smoke. Smoke can also be used to locate difficult-to-find leaks in the fuel evaporative emissions control (EVAP) system.
Infectious disease control.
In infectious disease control a "smoke test" is done to see whether a room is under negative pressure. A tube containing smoke is held near the bottom of the negative pressure room door, about two inches in front of the door. The smoke tube is held parallel to the door, and a small amount of smoke is then generated by gently squeezing the bulb. Care is taken to release the smoke from the tube slowly to ensure the velocity of the smoke from the tube does not overpower the air velocity. If the room is at negative pressure, the smoke will travel under the door and into the room. If the room is not at negative pressure, the smoke will be blown outward or will stay stationary.

</doc>
<doc id="46052" url="http://en.wikipedia.org/wiki?curid=46052" title="Sidney Poitier">
Sidney Poitier

Sir Sidney Poitier, KBE ( or ; born February 20, 1927), is a Bahamian-American actor, film director, author and diplomat.
In 1964, Poitier became the first African-American to win an Academy Award for Best Actor, for his role in "Lilies of the Field". The significance of this achievement was bolstered in 1967 when he starred in three successful films, all of which dealt with issues involving race: "To Sir, with Love"; "In the Heat of the Night"; and "Guess Who's Coming to Dinner", making him the top box-office star of that year. In 1999, the American Film Institute named Poitier among the Greatest Male Stars of All Time, ranking 22nd on the list of 25.
Poitier has directed a number of films, including "A Piece of the Action", "Uptown Saturday Night", "Let's Do It Again" (with friend Bill Cosby), "Stir Crazy" (starring Richard Pryor and Gene Wilder) and "Ghost Dad" (also with Cosby). In 2002, thirty-eight years after receiving the Best Actor Award, Poitier was chosen by the Academy of Motion Picture Arts and Sciences to receive an Honorary Award, in recognition of his "remarkable accomplishments as an artist and as a human being." From 1997 to 2007, he served as Bahamian ambassador to Japan. On August 12, 2009, Poitier was awarded the Presidential Medal of Freedom, the United States of America's highest civilian honor, by President Barack Obama.
Early life.
Sidney Poitier's parents were Evelyn (née Outten) and Reginald James Poitier, Bahamian farmers who owned a farm on Cat Island and traveled to Miami to sell tomatoes and other produce. Reginald worked as a cab driver in Nassau, Bahamas. Poitier was born in Miami while his parents were visiting. His birth was two months premature and he was not expected to survive, but his parents remained in Miami for three months to nurse him to health. Poitier grew up in the Bahamas (then a British colony) but, because of his birth in the United States, he automatically gained American citizenship. Poitier's uncle has claimed that the "Poitier" ancestors on his father's side had migrated from Haiti and were probably part of the runaway slaves who had established maroon communities throughout the Bahamas, including Cat Island. He mentions that the surname "Poitier" is a French name, and there were no white Poitiers from the Bahamas.
Poitier lived with his family on Cat Island until he was 10, when they moved to Nassau. He was raised a Roman Catholic but, later became an agnostic with views closer to deism.
At the age of 15 he was sent to Miami to live with his brother. At the age of 17, he moved to New York City and held a string of jobs as a dishwasher. A Jewish waiter sat with him every night for several weeks helping him learn to read the newspaper. He then decided to join the United States Army after which he worked as a dishwasher until a successful audition landed him a spot with the American Negro Theatre.
Hollywood.
Acting career.
Poitier joined the American Negro Theater, but was rejected by audiences. Contrary to what was expected of African American actors at the time, Poitier's tone deafness made him unable to sing. Determined to refine his acting skills and rid himself of his noticeable Bahamian accent, he spent the next six months dedicating himself to achieving theatrical success. On his second attempt at the theater, he was noticed and given a leading role in the Broadway production "Lysistrata", for which he received good reviews. By the end of 1949, he had to choose between leading roles on stage and an offer to work for Darryl F. Zanuck in the film "No Way Out" (1950). His performance in "No Way Out", as a doctor treating a Caucasian bigot (played by Richard Widmark), was noticed and led to more roles, each considerably more interesting and more prominent than those most African American actors of the time were offered. Poitier's breakout role was as a member of an incorrigible high school class in "Blackboard Jungle" (1955).
Poitier was the first male actor of African descent to be nominated for a competitive Academy Award (for "The Defiant Ones", 1958). He was also the first actor of African descent to win the Academy Award for Best Actor (for "Lilies of the Field" in 1963). (James Baskett was the first African American male to receive an Oscar, an Honorary Academy Award for his performance as Uncle Remus in the Walt Disney production of "Song of the South" in 1948, while Hattie McDaniel predated them both, winning as Best Supporting Actress for her role in 1939's "Gone with the Wind", making her the first person of African descent to be nominated for and receive an Oscar). His satisfaction at this honor was undermined by his concerns that this award was more of the industry congratulating itself for having him as a token and it would inhibit him from asking for more substantive considerations afterward. Poitier worked relatively little over the following year; he remained the only major actor of African descent and the roles offered were predominately typecast as a soft-spoken appeaser.
He acted in the first production of "A Raisin in the Sun" on Broadway in 1959, and later starred in the film version released in 1961. He also gave memorable performances in "The Bedford Incident" (1965), and "A Patch of Blue" (1965) co-starring Elizabeth Hartman and Shelley Winters. In 1967, he was the most successful draw at the box office, the commercial peak of his career, with three popular films, "Guess Who's Coming to Dinner"; "To Sir, with Love" and "In the Heat of the Night". The last film featured his most successful character, Virgil Tibbs, a Philadelphia, Pennsylvania, detective whose subsequent career was the subject of two sequels: "They Call Me Mister Tibbs!" (1970) and "The Organization" (1971).
Poitier began to be criticized for being typecast as over-idealized African American characters who were not permitted to have any sexuality or personality faults, such as his character in "Guess Who's Coming To Dinner". Poitier was aware of this pattern himself, but was conflicted on the matter: he wanted more varied roles, but also felt obliged to set a good example with his characters to defy previous stereotypes, as he was the only major actor of African descent in the American film industry at the time. For instance, in 1966 he turned down an opportunity to play the lead in an NBC production of "Othello" with that spirit in mind. In 2001, Poitier received an Honorary Academy Award for his overall contribution to American cinema. With the death of Ernest Borgnine in 2012, Poitier became the oldest living man to have won the Academy Award for Best Actor. On March 2, 2014, Poitier appeared with Angelina Jolie at the 86th Academy Awards, to present the Best Director award. He was given a standing ovation, as Jolie thanked him for all his Hollywood contributions, stating "we are in your debt". Poitier gave a small speech telling his peers to "keep up the wonderful work" to emotional applause.
Directorial career.
Poitier directed several films, the most successful being the Richard Pryor-Gene Wilder comedy "Stir Crazy" which for years was the highest grossing film directed by a person of African descent. His feature film directorial debut was the western "Buck and the Preacher" in which Poitier also starred, alongside Harry Belafonte. Poitier replaced original director Joseph Sargent. The trio of Poitier, Cosby, and Belafonte reunited again (with Poitier again directing) in "Uptown Saturday Night". Poitier also directed Cosby in "Let's Do It Again", "A Piece of the Action", and "Ghost Dad". Poitier also directed the first popular dance battle movie "Fast Forward" in 1985.
From 1995 to 2003 he served as a Member of the Board of Directors of The Walt Disney Company.
Recording career.
Poitier recorded an album with the composer Fred Katz called "Poitier Meets Plato", in which Poitier recites passages from Plato's writings.
Diplomatic career.
In April 1997, Poitier was appointed Ambassador of the Bahamas to Japan, a position he held until 2007. From 2002 to 2007, he was concurrently the Ambassador of the Bahamas to UNESCO.
In March 2014, he swore in the new Mayor of Beverly Hills, California, Lili Bosse, in a ceremony at the Wallis Annenberg Center for the Performing Arts.
Personal life.
Poitier was first married to Juanita Hardy from April 29, 1950, until 1965. He has been married to Joanna Shimkus, a Canadian-born former actress of Lithuanian and Irish descent, since January 23, 1976. He has four daughters with his first wife and two with his second: Beverly, Pamela, Sherri, Gina, Anika, and Sydney Tamiia.
In addition to his six daughters, Poitier has six grandchildren and three great-grandchildren. Both his youngest daughters are expecting as of November 2014.
Works about Poitier.
Autobiographies.
Poitier has written three autobiographical books:
Poitier is also the subject of the biography "Sidney Poitier: Man, Actor, Icon" (2004) by historian Aram Goudsouzian.
Poitier wrote the novel "Montaro Caine", released in May 2013.

</doc>
<doc id="46053" url="http://en.wikipedia.org/wiki?curid=46053" title="Human resources">
Human resources

Human resources is the set of individuals who make up the workforce of an organization, business sector, or economy. "Human capital" is sometimes used synonymously with human resources, although human capital typically refers to a more narrow view (i.e., the knowledge the individuals embody and can contribute to an organization). Likewise, other terms sometimes used include "manpower", "talent", "labour", or simply "people".
The professional discipline and business function that oversees an organization's human resources is called human resource management (HRM, or simply HR).
Overview.
Origins.
Pioneering economist John R. Commons used the term "human resource" in his 1893 book "The Distribution of Wealth" but did not further build upon it. The term "human resource" was subsequently in use during the 1910s and 1920s as was the notion that workers could be seen as a kind of capital asset. Among scholars the first use of "human resources" in its modern form was in a 1958 report by economist E. Wight Bakke.
The term in practice.
From the corporate objective, employees have been traditionally viewed as assets to the enterprise, whose value is enhanced by further learning and development, referred to as human resource development. Organizations will engage in a broad range of human resource management practices to capitalize on those assets.
In governing human resources, three major trends are typically considered:
In regard to how individuals respond to the changes in a labour market, the following must be understood:
Concerns about the terminology.
One major concern about considering people as assets or resources is that they will be commoditized and abused. Some analysis suggests that human beings are not "commodities" or "resources", but are creative and social beings in a productive enterprise. The 2000 revision of ISO 9001, in contrast, requires identifying the processes, their sequence and interaction, and to define and communicate responsibilities and authorities. In general, heavily unionised nations such as France and Germany have adopted and encouraged such approaches. Also, in 2001, the International Labour Organization decided to revisit and revise its 1975 Recommendation 150 on Human Resources Development, resulting in its "Labour is not a commodity" principle. One view of these trends is that a strong social consensus on political economy and a good social welfare system facilitate labour mobility and tend to make the entire economy more productive, as labour can develop skills and experience in various ways, and move from one enterprise to another with little controversy or difficulty in adapting.
Another important controversy regards labour mobility and the broader philosophical issue with usage of the phrase "human resources". Governments of developing nations often regard developed nations that encourage immigration or "guest workers" as appropriating human capital that is more rightfully part of the developing nation and required to further its economic growth. Over time, the United Nations have come to more generally support the developing nations' point of view, and have requested significant offsetting "foreign aid" contributions so that a developing nation losing human capital does not lose the capacity to continue to train new people in trades, professions, and the arts.

</doc>
<doc id="46056" url="http://en.wikipedia.org/wiki?curid=46056" title="Pope Telesphorus">
Pope Telesphorus

Pope Telesphorus (died c. 137) was the Bishop of Rome from c. 126 to his death c. 137, during the reigns of Roman Emperors Hadrian and Antoninus Pius. He was of Greek ancestry and born in Terranova da Sibari, Calabria, Italy.
Telesphorus is traditionally reckoned as being the seventh Roman bishop in succession after Saint Peter. The "Liber Pontificalis" mentions that he had been an anchorite (or hermit) monk prior to assuming office. According to the testimony of Irenæus ("Against Heresies" III.3.3), he suffered a "glorious" martyrdom. Although most early popes are called martyrs by sources such as the "Liber Pontificalis", Telesphorus is the first to whom Irenaeus, writing considerably earlier, gives this title.
Eusebius ("Church History" iv.7; iv.14) places the beginning of his pontificate in the twelfth year of the reign of Emperor Hadrian (128–129) and gives the date of his death as being in the first year of the reign of Antoninus Pius (138–139).
In Roman Martyrology, his feast is celebrated on 5 January; the Greek Church celebrates it on 22 February.
The tradition of Christmas Midnight Masses, the celebration of Easter on Sundays, the keeping of a seven-week Lent before Easter and the singing of the Gloria are usually attributed to his pontificate, but some historians doubt that such attributions are accurate.
A fragment of a letter from Irenæus to Pope Victor I during the Easter controversy in the late 2nd century, also preserved by Eusebius, testifies that Telesphorus was one of the Roman bishops who always celebrated Easter on Sunday, rather than on other days of the week according to the calculation of the Jewish Passover. Unlike Victor, however, Telesphorus remained in communion with those communities that did not follow this custom.
The Carmelites venerate Telesphorus as a patron saint of the order since some sources depict him as a hermit living on Mount Carmel.
The town of Saint-Télesphore, in the southwestern part of Canada's Quebec province, is named after him.

</doc>
<doc id="46059" url="http://en.wikipedia.org/wiki?curid=46059" title="Nuclear blackmail">
Nuclear blackmail

Nuclear blackmail is a form of nuclear strategy in which an aggressor uses the threat of use of nuclear weapons to force an adversary to perform some action or make some concessions. It is a type of extortion, related to brinkmanship.
Effectiveness.
Nuclear blackmail is considered most effective when the person making the threat is unhinged and ostensibly willing to commit suicide. (See game theory)
It is generally regarded as ineffective against a rational opponent who has or is an ally of someone who has assured destruction capability. If both states have nuclear weapons, the form of nuclear blackmail becomes a threat of escalation. In this situation if the opponent refuses to respond, then one's choices are either surrender or suicide. During the Cold War, the explicit threat of nuclear warfare to force an opponent to perform an action was rare in that most nations were allies of either the Soviet Union or the United States.
History.
In 1953, Eisenhower threatened the use of nuclear weapons to end the Korean War if the Chinese refused to negotiate. Consequently, China and North Korea signed the armistice.
The United States issued several nuclear threats against the People's Republic of China in the 1950s to force the evacuation of outlying islands and the cessation of attacks against Quemoy and Matsu, part of Republic of China.
Recently declassified documents from the National Archives (UK) indicate that the United Kingdom considered threatening China with nuclear retaliation in 1961 in the case of a military reclamation of Hong Kong by China.
Ali Magoudi claimed that Margaret Thatcher threatened nuclear war against Argentina during the 1982 Falklands War in order to procure codes from France to disable their French-made missiles.
In 1981, the United States Department of Energy said there had been 75 cases of nuclear blackmail against the United States, though only several were serious attempts.
In fiction.
Nuclear blackmail, typically by a supervillain rather than a state, has been frequently employed as a plot device in spy fiction and action films. Since such a scheme appeared in the film "Thunderball", the trope has been particularly associated with the James Bond series and in 24 (TV series). The notion of a supervillain threatening world leaders with a nuclear device has since become a cliché, and has been parodied in Charles K. Feldman's "Casino Royale", "", "The Simpsons" episode "You Only Move Twice", and other espionage spoofs.

</doc>
<doc id="46061" url="http://en.wikipedia.org/wiki?curid=46061" title="Anne Desclos">
Anne Desclos

Anne Cécile Desclos (23 September 1907 – 27 April 1998) was a French journalist and novelist who wrote under the pseudonyms Dominique Aury and Pauline Réage.
Early life.
Born in Rochefort, Charente-Maritime, France to a bilingual family, Desclos began reading in French and English at an early age. After completing her studies at the Sorbonne, she worked as a journalist until 1946 when she joined Gallimard Publishers as the editorial secretary for one of its imprints where she began using the pen name of Dominique Aury.
An avid reader of English and American literature, Desclos either translated or introduced to readers in France such renowned authors as Algernon Charles Swinburne, Evelyn Waugh, Virginia Woolf, T. S. Eliot, F. Scott Fitzgerald and numerous others. She became a critic and was made a member of the jury for several prominent literary awards.
Career.
Desclos' lover and employer Jean Paulhan, a fervent admirer of the Marquis de Sade, had made the remark to her that no woman was capable of writing an erotic novel. To prove him wrong, Desclos wrote a graphic, sadomasochistic novel that was published under the pseudonym Pauline Réage in June 1954. Titled "Histoire d'O" ("Story of O"), it was an enormous, though controversial, commercial success. The book caused much speculation as to the identity of the author. Many doubted that it was a woman, let alone the demure, intellectual, and almost prudish persona displayed in Dominique Aury's writings. Many well-known writers were alternately suspected to be the author, including André Malraux or Henri de Montherlant.
In addition, the book's graphic content sparked so much controversy that the following March the government authorities brought obscenity charges against the publisher and its mysterious author that were thrown out of court in 1959. However, a publicity ban and a restriction on the book's sale to minors was imposed by the judge. Following the lifting of the publicity ban in 1967, the conclusion to "Story of O" was published under the title "Retour à Roissy" using the pseudonym of Pauline Réage. However, according to her recent biography by Angie David, Desclos did not write this second novel. In 1975, she did a long interview about erotic books with author Régine Deforges, published by "Story of O" editor Jean-Jacques Pauvert, yet at the time her authorship was still unknown. An English-language edition of the interview was released in the United States in 1979 by Viking Press.
Eventually, Desclos publicly admitted that she was the author of "The Story of O" in 1994, 40 years after the book was published, in an interview with "The New Yorker".#Redirect She also explained the pseudonym of Pauline Réage: she chose the first name in homage to Pauline Bonaparte and Pauline Roland and she randomly picked up the name of Réage on a topographic map.
Personal life.
Desclos had a long-time relationship with her employer Jean Paulhan, the director of the prestigious "Nouvelle Revue Française", who was 23 years older than her. She was actively bisexual at times in her life. She notoriously had a liaison with historian and novelist Édith Thomas, who may have been an inspiration for the character of Anne-Marie in "Story of O". She had a son from a brief marriage in her early twenties.
Anne Desclos died in 1998 at the age of 90, in Corbeil-Essonnes, Île-de-France.

</doc>
<doc id="46063" url="http://en.wikipedia.org/wiki?curid=46063" title="Case sensitivity">
Case sensitivity

Text sometimes exhibits case sensitivity; that is, words can differ in meaning based on differing use of uppercase and lowercase letters. Words with capital letters do not always have the same meaning when written with lowercase letters.
For example :
The opposite term of "case-sensitive" is "case-insensitive".
Use in computer systems.
In computers, some examples of usually case-sensitive data are:
Some computer languages are case-sensitive for their identifiers (C, C++, Java, C#, Verilog, Ruby and XML). Others are case-insensitive (i.e., not case-sensitive), such as Ada, most BASICs (an exception being BBC BASIC), Fortran, SQL and Pascal. There are also languages, such as Haskell, Prolog and Go, in which the capitalization of an identifier encodes information about its semantics.
A text search operation could be case-sensitive or case-insensitive, depending on the system, application, or context. A case-insensitive search could be more convenient, not requiring the user to provide the precise casing, and returning more results, while a case-sensitive search enables the user to focus more precisely on a narrower result set. For example, Google searches are generally case-insensitive. In Oracle SQL most operations and searches are case-sensitive by default, while in most other DBMS's SQL searches are case-insensitive by default. In most Internet browsers, the "Find in this page" feature has an option enabling the user to choose whether the search would be case-sensitive or not.
Case-insensitive operations are sometimes said to fold case, from the idea of folding the character code table so that upper- and lower-case letters coincide. The alternative smash case is more likely to be used by someone that considers this behaviour a misfeature or in cases wherein one case is actually permanently converted to the other.
In Unix filesystems, filenames are usually case-sensitive. Old Windows filesystems (VFAT, FAT32) are not case-sensitive (there cannot be a readme.txt and a Readme.txt in the same directory) but are case-preserving, i.e. remembering the case of the letters. The original FAT12 filesystem was case-insensitive. Current Windows file systems, like NTFS, are case-sensitive; that is a readme.txt and a Readme.txt can exist in the same directory. However, the API for file access in Windows applications is case-insensitive, which makes filenames case-insensitive from the application's point of view. Therefore, applications only have access to one of the files whose filenames only differ in case, and some commands may fail if the filename provided is ambiguous.
Mac OS is somewhat unusual in that it uses HFS+ in a case insensitive but case preserving mode by default. This causes some issues for developers and power users, because most other environments are case sensitive, but many Mac Installers fail on case sensitive file systems.

</doc>
<doc id="46064" url="http://en.wikipedia.org/wiki?curid=46064" title="Story of O">
Story of O

Story of O (French: Histoire d'O, ]) is an erotic novel published in 1954 by French author Anne Desclos under the pen name Pauline Réage, and published in French by Jean-Jacques Pauvert.
Desclos did not reveal herself as the author for forty years after the initial publication. Desclos claims she wrote the novel as a series of love letters to her lover Jean Paulhan, who had admired the work of the Marquis de Sade. The novel shares with the latter themes such as love, dominance and submission.
Plot.
"Story of O" is a tale of female submission involving a beautiful Parisian fashion photographer named O, who is taught to be constantly available for oral, vaginal, and anal intercourse, offering herself to any male who belongs to the same secret society as her lover. She is regularly stripped, blindfolded, chained and whipped; her anus is widened by increasingly large plugs; her labium is pierced and her buttocks are branded.
The story begins when O's lover, René, brings her to the château of Roissy, where she is trained to serve the members of an elite club. After this initial training, as a demonstration of their bond and his generosity, René hands O to his elder stepbrother Sir Stephen, a more severe master. René wants O to learn to serve someone whom she does not love, and someone who does not love her. Over the course of this training, O falls in love with Sir Stephen and believes him to be in love with her as well. During the summer, Sir Stephen sends O to Samois, an old mansion solely inhabited by women for advanced training and body modifications related to submission. There she agrees to receive permanent marks of Sir Stephen's ownership, in the form of a brand and a steel tag hanging from a labia piercing.
Meanwhile René has encouraged O to seduce Jacqueline, a vain fashion model, and lure her to Roissy. Jacqueline is repulsed when she first sees O's chains and scars, although O herself is proud of her condition as a willing slave. However, Jacqueline's younger half-sister becomes enamored of O, and begs to be taken to Roissy.
At the climax, O is presented as a sexual slave, nude but for an owl-like mask and a leash attached to her piercing, before a large party of guests who treat her solely as an object. Afterward, she is shared by Sir Stephen and an associate of his who is referred to only as "The Commander".
Some early editions included several different variations of an epilogue which note that O was later abandoned by Sir Stephen, though there is debate as to whether Desclos intended it to be included in the finished work; in one such version, O is so distraught by the threat of this abandonment that she insists she would rather die and asks for permission to commit suicide, which is granted.
Publishing history.
In February 1955, "Story of O" won the French literature prize Prix des Deux Magots, although this did not prevent the French authorities from bringing obscenity charges against the publisher. The charges were rejected by the courts, but a publicity ban was imposed for a number of years.
The first English edition was published by Olympia Press in 1965. Eliot Fremont-Smith (of "The New York Times") called its publishing "a significant event".
A sequel, "Retour à Roissy" ("Return to Roissy," but often translated as "Return to the Chateau", "Continuing the Story of O"), was published in 1969 in French, again with Jean-Jacques Pauvert, "éditeur". It was published again in English by Grove Press, Inc., in 1971. It is not known whether this work is by the same author as the original.
Emmanuelle Arsan claimed the "Story of O" inspired her to write her own erotic novel "Emmanuelle".
A critical view of the novel is that it is about, and derives its erotic power from, the ultimate objectification of a woman. The heroine of the novel has the shortest possible name, consisting solely of the letter O. Although this is in fact a shortening of the name Odile, it could also stand for "object" or "orifice", an O being a symbolic representation of any "hole". The novel was strongly criticised by many feminists, who felt it glorified the abuse of women.
The book has been the source of various terms that are used in the BDSM subculture such as Samois, the name of the estate belonging to the character Anne-Marie, who brands O.
When the film of "The Story of O" was released, "L'Express" magazine ran a feature on the novel and film. This resulted in "L'Express" being picketed by feminists from the group Mouvement de libération des femmes, who found the novel and film objectionable. Journalist François Chalais also criticized "Story of O", claiming the novel glorified violence; he described the novel as "bringing the Gestapo into the boudoir".
Hidden identities.
The author used a pen name, then later revealed herself under another pen name, before finally, prior to her death, revealing her true identity. Her lover Jean Paulhan wrote the preface as if the author were unknown to him.
According to an article by Geraldine Bedell, published in "The Observer" on Sunday 24 July 2004, "Pauline Réage, the author, was a pseudonym, and many people thought that the book could only have been written by a man. The writer's true identity was not revealed until 10 years ago, when, in an interview with John de St Jorre, a British journalist and some-time foreign correspondent of "The Observer", an impeccably dressed 86-year-old intellectual called Dominique Aury acknowledged that the fantasies of castles, masks and debauchery were hers."
According to several other sources, however, Dominique Aury was itself a pseudonym of Anne Cécile Desclos, born 23 September 1907 in Rochefort-sur-Mer, France, and deceased 26 April 1998 (at age 90) in Paris, France.
The Grove Press edition (US, 1965) was translated by publisher Richard Seaver (who had lived in France for many years) under the pseudonym Sabine d'Estree.
Jean Paulhan.
Jean Paulhan, who was the author's lover and the person to whom she wrote "Story of O" in the form of love letters, wrote the preface, "Happiness in Slavery". Paulhan admired the Marquis de Sade's writing and told Desclos that a woman could not write in a similar fashion. Desclos interpreted this as a challenge and wrote the book. Paulhan was so impressed that he sent it to a publisher. Interestingly, in the preface, Paulhan goes out of his way to appear as if he does not know who wrote the book. In one part he says, "But from the beginning to end, the story of O is managed rather like some brilliant feat. It reminds you more of a speech than of a mere effusion; of a letter rather than a secret diary. But to whom is the letter addressed? Whom is the speech trying to convince? Whom can we ask? I don't even know who you are. That you are a woman I have little doubt." Paulhan also explains his own belief that the themes in the book depict the true nature of women. At times, the preface (when read with the knowledge of the relationship between Paulhan and the author), seems to be a continuation of the conversation between them.
Discussing the ending, Paulhan states, "I too was surprised by the end. And nothing you can say will convince me that it is the real end. That in reality (so to speak) your heroine convinces Sir Stephen to consent to her death."
One critic has seen Paulhan's essay as consistent with other themes in his work, including Paulhan's interest in erotica, his "mystification" of love and sexual relationships, and a view of women that is arguably sexist.
Adaptations.
Film.
French director Henri-Georges Clouzot wanted to adapt the novel to film for many years, which was eventually done by director Just Jaeckin in 1975 as "Histoire d'O" ("Story of O"), starring Corinne Cléry and Udo Kier. The film met with far less acclaim than the book. It was banned in the United Kingdom by the British Board of Film Censors until February 2000.
In 1975, American director Gerard Damiano, well known for "Deep Throat" (1972) and "The Devil in Miss Jones" (1973) created the movie "The Story of Joanna", highly influenced by the "Story of O", by combining the motifs from one of the book's chapters and from Jean-Paul Sartre's "No Exit".
In 1979, Danish director Lars von Trier made the short movie entitled "Menthe – la bienheureuse", as an homage to "Story of O". His 2005 film "Manderlay" was also inspired by the book, particularly Paulhan's introduction.
Five years later, in 1984, actress Sandra Wey starred as "O" in "".
In 1992, Brazilian miniseries in 10 episodes with Claudia Cepeda was made by director Eric Rochat, who was the producer of the original 1975 movie.
In 2002, another version of "O" was released, called "The Story of O: Untold Pleasures", with Danielle Ciardi playing the title character.
Comics.
In 1975, it was adapted for comics by the Italian artist Guido Crepax. Both the original and Crepax's adaptation were parodied for comics in 2007 by Charles Alverson and John Linton Roberson.
Documentaries.
"Writer of O", a 2004 documentary film by Pola Rapaport, mixed interviews with re-enactments of certain scenes from the book. In the documentary, the real author of "Histoire d'O", Dominique Aury (also a pen name), talks about the book "A Girl in Love". This book was written about how "Story of O" was written.
A documentary was also made for BBC Radio 4 entitled "The Story of O: The Vice Francaise", presented by Rowan Pelling, former editor of the "Erotic Review", which looked at the history of the book and Pauline Réage.
"", a documentary by filmmaker Maya Gallus, featured the final interview with 90-year-old Dominique Aury before she died. In the film, she recounts the extraordinary love story behind "Histoire d'O" and marvels that she has reached such a grand age.
In popular culture.
In the 1971 film Murmur of the Heart the two main characters (Laurent and Clara) share a conversation about the book.
Edward Gorey's 1961 book "The Curious Sofa" "[satirizes] "The Story of O"."
The comic book character Orlando is a blend of several fictional characters with the name Orlando as well as being known during the mid-sixties as O while engaged in sexual games with the descendants of the Silling Castle survivors, according to Alan Moore and Kevin O'Neill's "The League of Extraordinary Gentlemen" series.
On The Dresden Dolls' album "Yes, Virginia...", the piece "Mrs. O" includes reference to the "Story of O".
The band Oneida has a song "Story of O", on their album "Rated O".
In the 2000 video game Deus Ex, within a game level set in Paris, "O" and "René" can be found in Flat 12, having a conversation laced with subtle sexual and BDSM references.
In Jacqueline Carey's novel "Kushiel's Dart", during a grand ball, the main character—a masochist and submissive—dresses as a naked bird, as in the last scene of "O".
Tori Amos's song "Glory of the 80s", on her album "To Venus and Back", mentions having "The "Story of O" in my bucket seat of my wanna-be Mustang".
In the TV series "Frasier" (season 5 episode 3 "Halloween"), Roz Doyle appears as O at a Halloween party.
Warren Zevon's song "Hostage-O", on his album "Life'll Kill Ya", was inspired by the depictions of sadomasochism and psychological dependence portrayed in "O".
In the 2014 movie "That Awkward Moment", characters Jason and Ellie mention the Story of O in one scene.
In Season 2, Episode 3 of Hemlock Grove (TV Series) a secretarial assistant can be seen reading the Story of O.
In Richard Brautigan's 1975 novel "Willard and His Bowling Trophies", a couple engages in sexual sadomasochism after reading "Story of O".
In James W. Hall's 1987 novel, "Under Cover of Daylight", the novel's antagonist reads the "Story of O" and finds it confirms what he 'always thought about women.'

</doc>
<doc id="46065" url="http://en.wikipedia.org/wiki?curid=46065" title="Gábor Szegő">
Gábor Szegő

Gábor Szegő (]) (January 20, 1895 – August 7, 1985) was a Hungarian mathematician. He was one of the foremost analysts of his generation and made fundamental contributions to the theory of Toeplitz matrices and orthogonal polynomials.
Life.
Szegő was born in Kunhegyes, Austria-Hungary (today Hungary), into a Jewish family as the son of Adolf Szegő and Hermina Neuman. He married the chemist Anna Elisabeth Neményi in 1919, with whom he had two children.
In 1912 he started studies in mathematical physics at the University of Budapest, with summer visits to the University of Berlin and the University of Göttingen, where he attended lectures by Frobenius and Hilbert, amongst others. In Budapest he was taught mainly by Fejér, , Kürschák and Bauer and made the acquaintance of his future collaborators George Pólya and Michael Fekete. His studies were interrupted in 1915 by World War I, in which he served in the infantry, artillery and air corps.
In 1918 while stationed in Vienna, he was awarded a doctorate by the University of Vienna for his work on Toeplitz determinants. He received his Privat-Dozent from the University of Berlin in 1921, where he stayed until being appointed as successor to Knopp at the University of Königsberg in 1926. Intolerable working conditions during the Nazi regime resulted in a temporary position at the Washington University in Saint Louis, Missouri in 1936, before his appointment as chairman of the mathematics department at Stanford University in 1938, where he helped build up the department until his retirement in 1966. He died in Palo Alto, California. His doctoral students include Paul Rosenbloom and Joseph Ullman.
Works.
Szegő's most important work was in analysis. He was one of the foremost analysts of his generation and made fundamental contributions to the theory of Toeplitz matrices and orthogonal polynomials. He wrote over 130 papers in several languages. Each of his four books, several written in collaboration with others, has become a classic in its field. The monograph "Orthogonal polynomials", published in 1939, contains much of his research and has had a profound influence in many areas of applied mathematics, including theoretical physics, stochastic processes and numerical analysis.
Tutoring von Neumann.
At the age of 15, the young John von Neumann, recognised as a mathematical prodigy, was sent to study advanced calculus under Szegő. On their first meeting, Szegő was so astounded by von Neumann's mathematical talent and speed that he was brought to tears. Szegő subsequently visited the von Neumann house twice a week to tutor the child prodigy. Some of von Neumann's instant solutions to the problems in calculus posed by Szegő, sketched out with his father's stationery, are now on display at the von Neumann archive at Budapest.
Honours.
Amongst the many honours received during his lifetime were:

</doc>
<doc id="46068" url="http://en.wikipedia.org/wiki?curid=46068" title="Pope Nicholas IV">
Pope Nicholas IV

Pope Nicholas IV (Latin: "Nicholaus IV"; 30 September 1227 – 4 April 1292), born Girolamo Masci, Pope from 22 February 1288 to his death in 1292. He was the first Franciscan to be elected pope.
Biography.
Early life.
Masci was born at Lisciano, near Ascoli Piceno. He was a pious, peace-loving friar with no ambition save for the Church, the crusades and the extirpation of heresy. Originally a Franciscan friar, he had been legate to the Greeks under Pope Gregory X in 1272, to invite their participation in the Second Council of Lyons. He succeeded Bonaventure as Minister General of his religious order in 1274. He was made Cardinal Priest of Santa Pudenziana and Latin Patriarch of Constantinople in 1278 by Pope Nicholas III, and Cardinal Bishop of Palestrina by Pope Martin IV.
Pontificate.
Papal conclave.
After the death of Pope Honorius IV on 3 April, 1287, the conclave held at Rome was for a time hopelessly divided in its selection of a successor. When fever had carried off six of the electors, the others, with the sole exception of Girolamo, left Rome. It was not until the following year that they reassembled and February 1288, unanimously elected him to the papacy. He became the first Franciscan pope and chose the name Nicholas IV in remembrance of Nicholas III.
Actions.
In regard to the question of the Sicilian succession, as feudal suzerain of the kingdom, Nicholas annulled the treaty, concluded in 1288 through the mediation of Edward I of England, which confirmed James II of Aragon in the possession of the island of Sicily. In May 1289 he crowned King Charles II of Naples and Sicily after the latter had expressly recognized papal suzerainty, and in February 1291 concluded a treaty with Kings Alfonso III of Aragon and Philip IV of France looking toward the expulsion of James from Sicily. 
The loss of Acre in 1291 stirred Nicholas IV to renewed enthusiasm for a crusade. He sent missionaries, among them the Franciscan John of Monte Corvino, to labour among the Bulgarians, Ethiopians, Mongols, Tatars and Chinese.
Nicholas IV issued an important constitution on 18 July 1289, which granted to the cardinals one-half of all income accruing to the Holy See and a share in the financial management, thereby paving the way for that independence of the College of Cardinals which, in the following century, was to be of detriment to the papacy.
Death.
Nicholas IV died 4 April 1292 in the palace, which he had built. He was buried in the Basilica di Santa Maria Maggiore. 
"Taxatio".
The 1291–92 "Taxatio" he initiated, which was a detailed valuation for ecclesiastical taxation of English and Welsh parish churches and prebends, remains an important source document for the mediaeval period. An edition was reprinted by the Record Commission in 1802 as "Taxatio Ecclesiastica Angliae et Walliae Auctoritate".

</doc>
<doc id="46075" url="http://en.wikipedia.org/wiki?curid=46075" title="Kazimierz Kuratowski">
Kazimierz Kuratowski

Kazimierz Kuratowski (February 2, 1896 – June 18, 1980) was a Polish mathematician and logician. He was one of the leading representatives of the Warsaw School of Mathematics.
Biography and studies.
Kazimierz Kuratowski was born in Warsaw, then part of the Russian Empire, on February 2, 1896. He was a son of Marek Kuratow, a barrister, and Róża Karzewska. He completed a Warsaw secondary school, which was named after general Paweł Chrzanowski. In 1913, he enrolled in an engineering course at the University of Glasgow in Scotland, in part because he did not wish to study in Russian; instruction in Polish was prohibited. He completed only one year of study when the outbreak of World War I precluded any further enrollment. In 1915, Russian forces withdrew from Warsaw and Warsaw University was reopened with Polish as the language of instruction. Kuratowski restarted his university education there the same year, this time in mathematics. He obtained his Ph.D. in 1921, in newly independent Poland.
Doctoral thesis.
In autumn 1921 Kuratowski was awarded the Ph.D. degree for his groundbreaking work. His thesis statement consisted of two parts. One was devoted to an axiomatic construction of topology via the closure axioms. This first part (republished in a slightly modified form in 1922) has been cited in hundreds of scientific articles.
The second part of Kuratowski's thesis was devoted to continua irreducible between two points. This was the subject of a French doctoral thesis written by Zygmunt Janiszewski. Since Janiszewski was deceased, Kuratowski's supervisor was Wacław Sierpiński. Kuratowski's thesis solved certain problems in set theory raised by a Belgian mathematician, Charles-Jean Étienne Gustave Nicolas, Baron de la Vallée Poussin.
Academic career until World War II.
Two years later, in 1923, Kuratowski was appointed deputy professor of mathematics at Warsaw University. He was then appointed a full professor of mathematics at Lwów Polytechnic in Lwów, in 1927. He was the head of the Mathematics department there until 1933. Kuratowski was also dean of the department twice. In 1929, Kuratowski became a member of the Warsaw Scientific Society
While Kuratowski associated with many of the scholars of the Lwów School of Mathematics, such as Stefan Banach and Stanislaw Ulam, and the circle of mathematicians based around the Scottish Café he kept close connections with Warsaw. Kuratowski left Lwów for Warsaw in 1934, before the famous Scottish Book was begun (in 1935), hence did not contribute any problems to it. He did however, collaborate closely with Banach in solving important problems in measure theory.
In 1934 he was appointed the professor at Warsaw University. A year later Kuratowski was nominated as the head of Mathematics Department there. From 1936 to 1939 he was secretary of the Mathematics Committee in The Council of Science and Applied Sciences.
During and after the war.
During World War II, he gave lectures at the underground university in Warsaw, since higher education for Poles was forbidden under German occupation.
In February 1945, Kuratowski started to lecture at the reopened Warsaw University. In 1945, he became a member of the Polish Academy of Learning, in 1946 he was appointed vice-president of the Mathematics department at Warsaw University, and from 1949 he was chosen to be the vice-president of Warsaw Scientific Society. In 1952 he became a member of the Polish Academy of Sciences, of which he was the vice-president from 1957 to 1968.
After World War II, Kuratowski was actively involved in the rebuilding of scientific life in Poland. He helped to establish the State Mathematical Institute, which was incorporated into the Polish Academy of Sciences in 1952. From 1948 until 1967 Kuratowski was director of the Institute of Mathematics of the Polish Academy of Sciences, and was also a long-time chairman of the Polish and International Mathematics Societies. He was president of the Scientific Council of the State Institute of Mathematics (1968–1980). From 1948 to 1980 he was the head of the topology section. One of his students was Andrzej Mostowski.
Legacy.
Kazimierz Kuratowski was one of a celebrated group of Polish mathematicians who would meet at Lwów's Scottish Café. He was a president of the Polish Mathematical Society (PTM) and a member of the Warsaw Scientific Society (TNW). What is more, he was chief editor in "Fundamenta Mathematicae", a series of publications in "Polish Mathematical Society Annals". Furthermore, Kuratowski worked as an editor in the Polish Academy of Sciences Bulletin. He was also one of the writers of the Mathematical monographs, which were created in cooperation with the Institute of Mathematics of the Polish Academy of Sciences (IMPAN). High quality research monographs of the representatives of Warsaw's and Lwów’s School of Mathematics, which concerned all areas of pure and applied mathematics, were published in these volumes.
Kazimierz Kuratowski was an active member of many scientific societies and foreign scientific academies, including the Royal Society of Edinburgh, Austria, Germany, Hungary, Italy and the Union of Soviet Socialist Republics (USSR).
Research.
Kuratowski’s research mainly focused on abstract topological and metric structures. He implemented the closure axioms (known in mathematical circles as the Kuratowski closure axioms). This was fundamental for the development of topological space theory and irreducible continuum theory between two points. The most valuable results, which were obtained by Kazimierz Kuratowski after the war are those that concern the relationship between topology and analytic functions (theory), and also research in the field of cutting Euclidean spaces. Together with Ulam, who was Kuratowski’s most talented student during the Lwów Period, he introduced the concept of so-called quasi homeomorphism that opened up a new field in topological studies.
Kuratowski’s research in the field of measure theory, including research with Banach, Tarski, was continued by many students. Moreover, with Alfred Tarski and Wacław Sierpiński he provided most of the theory concerning Polish spaces (that are indeed named after these mathematicians and their legacy). Knaster and Kuratowski brought a comprehensive and precise study to connected components theory. It was applied to issues such as cutting-plane, with the paradoxical examples of connected components.
Kuratowski proved the Kuratowski-Zorn lemma (often called just Zorn's lemma) in 1922 (Fundamenta Mathematicae, vol.3). This result has important connections to many basic theorems. Zorn gave its application in 1935 ("Bulletin of the American Mathematical Society", 41). Kuratowski implemented many concepts in set theory and topology. In many cases, Kuratowski established new terminology and symbolism.
His contributions to mathematics include:
Kuratowski’s post-war works were mainly focused on three strands:
Publications.
Among over 170 published works are valuable monographs and books including Topologie (Vol. I, 1933, translated into English and Russian, and Vol. II, 1950) and Introduction to Set Theory and Topology (Vol. I, 1952, translated into English, French, Spanish, and Bulgarian). He authored “A Half Century of Polish Mathematics 1920-1970: Remembrances and Reflections” (1973) and "Notes to his autobiography" (1981). The latter was published posthumously thanks to Kuratowski's daughter Zofia Kuratowska, who prepared his notes for printing. Kazimierz Kuratowski represented Polish mathematics in the International Mathematics Union where he was vice president from 1963 to 1966. What is more, he participated in numerous international congresses and lectured at dozens of universities around the world. He was an honorary causa doctor at the Universities in Glasgow, Prague, Wroclaw, and Paris. He received the highest national awards, as well as a gold medal of the Czechoslovak Academy of Sciences, and the Polish Academy of Science. Kuratowski died on June 18, 1980 in Warsaw.
References.
</dl>

</doc>
<doc id="46079" url="http://en.wikipedia.org/wiki?curid=46079" title="Pope Lucius II">
Pope Lucius II

Pope Lucius II (Latin: "Lucius II"; died 15 February 1145), born Gherardo Caccianemici dal Orso, was Pope from 9 March 1144 to his death in 1145. His pontificate was notable for the unrest in Rome associated with the Commune of Rome and its attempts to wrest control of the city from the papacy.
Early life.
Gherardo Caccianemici dal Orso, the son of Orso Caccianemici was born in Bologna. He was for many years a canon of the Basilica di San Frediano before his elevation by Pope Honorius II to cardinal priest of Santa Croce in Gerusalemme in 1124. During this time there he renovated the basilica, attached a body of regular canons and improved its revenue stream. After he was elevated as pope, he presented to the church a copy of the Gospels bound with plates of gold and adorned with jewels, as well as an altar-cover and two chased silver-gilt ampullae for use at Mass. Honorius also appointed him the librarian of the Diocese of Rome before appointing him papal legate in Germany in 1125. While there, he helped support the candidacy of Holy Roman Emperor Lothair III as well as appointing Saint Norbert of Xanten as the Archbishop of Magdeburg. In 1128, Gherardo was sent to Benevento to govern the city, which had overthrown the previous rector.
In 1130 he was again appointed legate to Germany by Pope Innocent II, where he was instrumental in convincing Lothair III to make two expeditions to Italy for the purpose of protecting Pope Innocent II against the Antipope Anacletus II. He had a further period as legate to Germany in 1135–36. He was one of the principal negotiators with Lothair III in attempting to force the monks of Monte Cassino to submit themselves to the authority of the papacy. In addition, he was sent to Salerno to negotiate the end of the schism involving Anacletus II with King Roger II of Sicily. As a principal supporter of Pope Innocent II, the pope rewarded him for his efforts by appointing him papal chancellor. After the papal election of 1144, Gherardo was elected as Lucius II and consecrated on 12 March 1144. He probably took his name in honor of Pope Lucius I, who was commemorated a few days prior to Gherardo's consecration.
Pontificate.
Relations with England and Portugal.
Lucius was involved in the usual running of church business throughout medieval Christendom. In England, he granted a number of privileges to bishops, monasteries and churches, including exempting the monastery of St. Edmund from all subjection to the secular authorities. He also dispatched a papal legate, Igmarus (or Hincmar), to England, charged to investigate the request of Bernard, Bishop of St David's, to elevate his see to the rank of Metropolitan bishop, and to take the pallium to William, Archbishop of York. Regarding the political situation in England, he took the side of the Empress Matilda over the rights to the English crown.
Early in his reign, Lucius received a request from prominent members of the town of Lucca to become the suzerain of the castle within the town in order to protect it from the war between Lucca and Pisa. Lucius received it on 18 March 1144 and for a payment of ten pounds of gold, agreed to defend it on his behalf. Lucius then returned the castle to them as a fief.
Meanwhile in Portugal, King Afonso I, eager to maintain the newly established independence of Portugal from the Kingdom of León, offered to do homage to Lucius, as he had done to Pope Innocent II, and to make the pope the feudal suzerain of his lands. He offered Lucius his territory and a yearly tribute of four ounces of gold in exchange for the defence and support of the Apostolic See. Although Lucius accepted Afonso’s feudal homage on 1 May 1144, and excused him from appearing in person, he did not acknowledge Afonso as King of Portugal, but instead as "Dux Portugallensis". The royal title would eventually be conferred by Pope Alexander III.
Finally, the city of Corneto, formally belonging to the Papal States, was restored to the papacy during Lucius’ pontificate by a formal deed on 20 November 1144.
Conflict with Roger II of Sicily.
Although Lucius had been the friend of King Roger II of Sicily and godparent to one of his children, the situation between the two would soon deteriorate. The two parties met at Ceprano in June 1144 to clarify the duties of Roger as a vassal of the Holy See. Lucius demanded the return of the principality of Capua, while Roger instead wanted additional territory that formed part of the Papal States in the south. Lucius II, on the advice of his cardinals, was unwilling to accept Roger’s demands and rejected them. Infuriated, Roger returned to Sicily and asked his son Roger III, Duke of Apulia, to invade Campania. Roger did as he was asked, and sent his general Robert of Selby against Lucius, ravaging the country as far north as Ferentino. This forced the Romans to capitulate, and in September 1144, Lucius agreed to Roger’s terms, negotiating a seven-year truce. The Normans in return withdrew back to their conquered territories and promised not to attack Benevento or any other papal territory.
Emergence of the Roman Commune.
This surrender on the part of Lucius II gave an opportunity for members of the Roman Senate to reassert their ancient independence and authority and to erect a revolutionary republic at Rome which sought to deprive the Pope of his temporal power. The principal groups involved in this movement were the merchants and artisans, while the urban nobility kept their neutrality.
The Senate, which practically took all temporal power from the Pope during the pontificate of Innocent II, had been managed with considerable skill and firmness by Lucius at the beginning of his pontificate, convincing many senators to either leave the Capitoline Hill or to lay down their magisterium. Now, encouraged by Lucius II's defeat, the Senate, led by Giordano Pierleoni, brother of the former Antipope Anacletus II, rebelled against Lucius II, driving out the papal prefects and establishing the Commune of Rome. They demanded the pope abandon all governmental duties, and that he would retain only ecclesiastical taxes and voluntary tributes. At first, Lucius asked for Roger II's aid, but Roger, still annoyed that Lucius had not fully recognised his kingship, withheld his assistance. Lucius then turned for help to Conrad, King of the Romans, and on December 1144 wrote to him pleading for military assistance against the Senate and the Patrician Giordano Pierleoni. Lucius was supported by Bernard of Clairvaux, who also wrote to Conrad, asking for him to intervene.
While waiting for Conrad’s reply, Lucius decided to take matters into his own hands. Turning to the Roman aristocracy, in particular the Frangipani family, he gave them the fortress of the Circus Maximus on 31 January 1145, allowing them complete control of the southern portion of the Palatine Hill. The Roman Forum had become a battleground, and the confusion prevented Lucius from travelling to the Aventine Hill to ordain the abbot of San Saba on 20 January 1145.
Finally, Lucius marched against the Senatorial positions on the Capitol with a small army. He was driven back by Giordano, and according to Godfrey of Viterbo, he was seriously injured during this battle (by a thrown stone). He did not recover from his injuries and died on 15 February 1145 at San Gregorio Magno al Celio, where he was under the protection of the neighbouring Frangipani fortress.
Lucius II was buried at St John Lateran in the circular portico behind the apse. His heraldic badge was a shield of argent, with a bear rampant of proper sable.

</doc>
<doc id="46080" url="http://en.wikipedia.org/wiki?curid=46080" title="Halteres">
Halteres

Halteres (; singular halter or haltere) are small knobbed structures modified from a pair of wings in some two-winged insects. They are flapped rapidly and function as gyroscopes, informing the insect about rotation of the body during flight. The word 'halter' comes from Greek ἁλτήρ, "Halteres", a double knobbed device used in Ancient Greece by athletes during training in jumping.
The halteres evolved from insect wings. The ancestral insect species had two pairs of wings (like dragonflies and most other flying insect species have). In the Strepsiptera (small insects parasitic on bees, wasps and cockroaches) the forewings evolved into halteres, while in the Diptera (flies, mosquitoes and gnats) the hindwings changed into halteres.
In Diptera, the formation of the haltere during metamorphosis is dependent on the homeotic gene Ultrabithorax (Ubx). If this gene is experimentally deactivated, the haltere will develop into a fully developed wing. This is an excellent illustration of an important mechanism of evolutionary development: a simple homeotic gene change resulting in a radically different phenotype.
Halteres flap up and down as the wings do and operate as vibrating structure gyroscopes. Every vibrating object tends to maintain its plane of vibration if its support is rotated, a result of Newton's first law. If the body of the insect changes direction in flight or rotates about its axis, the vibrating halteres thus exert a force on the body, which can be viewed as the Coriolis effect in action. The insect detects this force with sensory organs known as campaniform sensilla located at the base of the halteres. The planes of vibration of the two halteres are orthogonal to each other, each forming an angle of about 45 degrees with the axis of the insect; this increases the amount of information gained from the halteres.
Halteres thus act as a balancing and guidance system, helping these insects to perform their fast aerobatics. In addition to providing rapid feedback to the muscles steering the wings, they also play an important role in stabilizing the head during flight.

</doc>
<doc id="46083" url="http://en.wikipedia.org/wiki?curid=46083" title="Halley's Comet">
Halley's Comet

Halley's Comet or Comet Halley ( or ), officially designated 1P/Halley, is a short-period comet visible from Earth every 75–76 years. Halley is the only known short-period comet that is clearly visible to the naked eye from Earth, and the only naked-eye comet that might appear twice in a human lifetime. Halley last appeared in the inner Solar System in 1986 and will next appear in mid-2061.
Halley's returns to the inner Solar System have been observed and recorded by astronomers since at least 240 BC. Clear records of the comet's appearances were made by Chinese, Babylonian, and medieval European chroniclers, but were not recognized as reappearances of the same object at the time. The comet's periodicity was first determined in 1705 by English astronomer Edmond Halley, after whom it is now named.
During its 1986 apparition, Halley's Comet became the first comet to be observed in detail by spacecraft, providing the first observational data on the structure of a comet nucleus and the mechanism of coma and tail formation. These observations supported a number of longstanding hypotheses about comet construction, particularly Fred Whipple's "dirty snowball" model, which correctly predicted that Halley would be composed of a mixture of volatile ices – such as water, carbon dioxide, and ammonia – and dust. The missions also provided data that substantially reformed and reconfigured these ideas; for instance, now it is understood that the surface of Halley is largely composed of dusty, non-volatile materials, and that only a small portion of it is icy.
Pronunciation.
Comet Halley is commonly pronounced , rhyming with "valley", or , rhyming with "daily". Spellings of Edmond Halley's name during his lifetime included "Hailey, Haley, Hayley, Halley, Hawley", and "Hawly", so its contemporary pronunciation is uncertain.
Computation of orbit.
Halley was the first comet to be recognized as periodic. Until the Renaissance, the philosophical consensus on the nature of comets, promoted by Aristotle, was that they were disturbances in Earth's atmosphere. This idea was disproved in 1577 by Tycho Brahe, who used parallax measurements to show that comets must lie beyond the Moon. Many were still unconvinced that comets orbited the Sun, and assumed instead that they must follow straight paths through the Solar System.
In 1687, Sir Isaac Newton published his "Principia", in which he outlined his laws of gravity and motion. His work on comets was decidedly incomplete. Although he had suspected that two comets that had appeared in succession in 1680 and 1681 were the same comet before and after passing behind the Sun (he was later found to be correct; see Newton's Comet), he was unable to completely reconcile comets into his model.
Ultimately, it was Newton's friend, editor and publisher, Edmond Halley, who, in his 1705 "Synopsis of the Astronomy of Comets", used Newton's new laws to calculate the gravitational effects of Jupiter and Saturn on cometary orbits. This calculation enabled him, after examining historical records, to determine that the orbital elements of a second comet that had appeared in 1682 were nearly the same as those of two comets that had appeared in 1531 (observed by Petrus Apianus) and 1607 (observed by Johannes Kepler). Halley thus concluded that all three comets were, in fact, the same object returning every 76 years, a period that has since been amended to every 75–76 years. After a rough estimate of the perturbations the comet would sustain from the gravitational attraction of the planets, he predicted its return for 1758.
Halley's prediction of the comet's return proved to be correct, although it was not seen until 25 December 1758, by Johann Georg Palitzsch, a German farmer and amateur astronomer. It did not pass through its perihelion until 13 March 1759, the attraction of Jupiter and Saturn having caused a retardation of 618 days. This effect was computed prior to its return (with a one-month error to 13 April) by a team of three French mathematicians, Alexis Clairaut, Joseph Lalande, and Nicole-Reine Lepaute. Halley did not live to see the comet return, as he died in 1742. The confirmation of the comet's return was the first time anything other than planets had been shown to orbit the Sun. It was also one of the earliest successful tests of Newtonian physics, and a clear demonstration of its explanatory power. The comet was first named in Halley's honour by French astronomer Nicolas Louis de Lacaille in 1759.
The possibility has been raised that first-century Jewish astronomers already had recognized Halley's Comet as periodic. This theory notes a passage in the Talmud that refers to "a star which appears once in seventy years that makes the captains of the ships err."
Orbit and origin.
Halley's orbital period over the last 3 centuries has been between 75–76 years, although it has varied between 74–79 years since 240 BC. Its orbit around the Sun is highly elliptical, with an orbital eccentricity of 0.967 (with 0 being a perfect circle and 1 being a parabolic trajectory). The perihelion, the point in the comet's orbit when it is nearest the Sun, is just 0.6 AU. This is between the orbits of Mercury and Venus. Its aphelion, or farthest distance from the Sun, is 35 AU (roughly the distance of Pluto). Unusually for an object in the Solar System, Halley's orbit is retrograde; it orbits the Sun in the opposite direction to the planets, or, clockwise from above the Sun's north pole. The orbit is inclined by 18° to the ecliptic, with much of it lying south of the ecliptic. (Because it is retrograde, the true inclination is 162°). Due to the retrograde orbit, it has one of the highest velocities relative to the Earth of any object in the Solar System. The 1910 passage was at a relative velocity of 70.56 km/s (157,838 mph or 254,016 km/h). Because its orbit comes close to Earth's in two places, Halley is the parent body of two meteor showers: the Eta Aquariids in early May, and the Orionids in late October. Observations conducted around the time of Halley's appearance in 1986, however, suggest that the Eta Aquarid meteor shower might not originate from Halley's Comet, although it might be perturbed by it.
Halley is classified as a "periodic" or "short-period comet"; one with an orbit lasting 200 years or less. This contrasts it with long-period comets, whose orbits last for thousands of years. Periodic comets have an average inclination to the ecliptic of only ten degrees, and an orbital period of just 6.5 years, so Halley's orbit is atypical. Most short-period comets (those with orbital periods shorter than 20 years and inclinations of 20–30 degrees or less) are called Jupiter-family comets. Those resembling Halley, with orbital periods of between 20 to 200 years and inclinations extending from zero to more than 90 degrees, are called Halley-type comets. To date, only 54 Halley-type comets have been observed, compared with nearly 400 identified Jupiter-family comets.
The orbits of the Halley-type comets suggest that they were originally long-period comets whose orbits were perturbed by the gravity of the giant planets and directed into the inner Solar System. If Halley was once a long-period comet, it is likely to have originated in the Oort Cloud, a sphere of cometary bodies that has an inner edge of 20,000–50,000 AU. Conversely the Jupiter-family comets are believed to originate in the Kuiper belt, a flat disc of icy debris between 30 AU (Neptune's orbit) and 50 AU from the Sun (in the scattered disc). Another point of origin for the Halley-type comets has been proposed. In 2008, a trans-Neptunian object with a retrograde orbit similar to Halley's was discovered, 2008 KV42, whose orbit takes it from just outside that of Uranus to twice the distance of Pluto. It may be a member of a new population of small Solar System bodies that serves as the source of Halley-type comets.
Halley has probably been in its current orbit for 16,000–200,000 years, although it is not possible to numerically integrate its orbit for more than a few tens of apparitions, and close approaches before 837 AD can only be verified from recorded observations. The non-gravitational effects can be crucial; as Halley approaches the Sun, it expels jets of sublimating gas from its surface, which knock it very slightly off its orbital path. These orbital changes cause delays in its perihelion of four days, average.
In 1989, Boris Chirikov and Vitaly Vecheslavov performed an analysis of 46 apparitions of Halley's Comet taken from historical records and computer simulations. These studies showed that its dynamics were chaotic and unpredictable on long timescales. Halley's projected lifetime could be as long as 10 million years. More recent work suggests that Halley will evaporate, or split in two, within the next few tens of thousands of years, or will be ejected from the Solar System within a few hundred thousand years. Observations by D.W. Hughes suggest that Halley's nucleus has been reduced in mass by 80–90% over the last 2000–3000 revolutions.
Structure and composition.
The "Giotto" and "Vega" missions gave planetary scientists their first view of Halley's surface and structure. Like all comets, as Halley nears the Sun, its volatile compounds (those with low boiling points, such as water, carbon monoxide, carbon dioxide and other ices) begin to sublime from the surface of its nucleus. This causes the comet to develop a coma, or atmosphere, up to 100,000 km across. Evaporation of this dirty ice releases dust particles, which travel with the gas away from the nucleus. Gas molecules in the coma absorb solar light and then re-radiate it at different wavelengths, a phenomenon known as fluorescence, whereas dust particles scatter the solar light. Both processes are responsible for making the coma visible. As a fraction of the gas molecules in the coma are ionized by the solar ultraviolet radiation, pressure from the solar wind, a stream of charged particles emitted by the Sun, pulls the coma's ions out into a long tail, which may extend more than 100 million kilometers into space. Changes in the flow of the solar wind can cause disconnection events, in which the tail completely breaks off from the nucleus.
Despite the vast size of its coma, Halley's nucleus is relatively small: barely 15 kilometers long, 8 kilometers wide and perhaps 8 kilometers thick.[b] Its shape vaguely resembles that of a peanut. Its mass is relatively low (roughly 2.2 × 1014 kg) and its average density is about 0.6 g/cm3, indicating that it is made of a large number of small pieces, held together very loosely, forming a structure known as a rubble pile. Ground-based observations of coma brightness suggested that Halley's rotation period was about 7.4 days. Images taken by the various spacecraft, along with observations of the jets and shell, suggested a period of 52 hours. Given the irregular shape of the nucleus, Halley's rotation is likely to be complex. Although only 25% of Halley's surface was imaged in detail during the flyby missions, the images revealed an extremely varied topography, with hills, mountains, ridges, depressions, and at least one crater.
Halley is the most active of all the periodic comets, with others, such as Comet Encke and Comet Holmes, being one or two orders of magnitude less active. Its day side (the side facing the Sun) is far more active than the night side. Spacecraft observations showed that the gases ejected from the nucleus were 80% water vapor, 17% carbon monoxide and 3–4% carbon dioxide, with traces of hydrocarbons although more-recent sources give a value of 10% for carbon monoxide and also include traces of methane and ammonia. The dust particles were found to be primarily a mixture of carbon–hydrogen–oxygen–nitrogen (CHON) compounds common in the outer Solar System, and silicates, such as are found in terrestrial rocks. The dust particles decreased in size down to the limits of detection (~0.001 µm). The ratio of deuterium to hydrogen in the water released by Halley was initially thought to be similar to that found in Earth's ocean water, suggesting that Halley-type comets may have delivered water to Earth in the distant past. Subsequent observations showed Halley's deuterium ratio to be far higher than that in found in Earth's oceans, making such comets unlikely sources for Earth's water.
"Giotto" provided the first evidence in support of Fred Whipple's "dirty snowball" hypothesis for comet construction; Whipple postulated that comets are icy objects warmed by the Sun as they approach the inner Solar System, causing ices on their surfaces to sublimate (change directly from a solid to a gas), and jets of volatile material to burst outward, creating the coma. "Giotto" showed that this model was broadly correct, though with modifications. Halley's albedo, for instance, is about 4%, meaning that it reflects only 4% of the sunlight hitting it; about what one would expect for coal. Thus, despite appearing brilliant white to observers on Earth, Halley's Comet is in fact pitch black. The surface temperature of evaporating "dirty ice" ranges from 170 K (−103 °C) at higher albedo to 220 K (−53 °C) at low albedo; Vega 1 found Halley's surface temperature to be in the range 300–400 K (30–130 °C). This suggested that only 10% of Halley's surface was active, and that large portions of it were coated in a layer of dark dust that retained heat. Together, these observations suggested that Halley was in fact predominantly composed of non-volatile materials, and thus more closely resembled a "snowy dirtball" than a "dirty snowball".
Apparitions.
Halley's calculations enabled the comet's earlier appearances to be found in the historical record. The following table sets out the astronomical designations for every apparition of Halley's Comet from 240 BC, the earliest documented widespread sighting. For example, "1P/1982 U1, 1986 III, 1982i" indicates that for the perihelion in 1986, Halley was the first period comet known (designated 1P) and this apparition was the first seen in half-month U (the second half of October) in 1982 (giving 1P/1982 U1); it was the third comet past perihelion in 1986 (1986 III); and it was the ninth comet spotted in 1982 (provisional designation 1982i). The perihelion dates of each apparition are shown. The perihelion dates farther from the present are approximate, mainly because of uncertainties in the modelling of non-gravitational effects. Perihelion dates 1607 and later are in the Gregorian calendar, while perihelion dates of 1531 and earlier are in the Julian calendar.
Prior to 1066.
Halley may have been recorded as early as 467 BC, but this is uncertain. A comet was recorded in ancient Greece between 468 and 466 BC; its timing, location, duration, and associated meteor shower all suggest it was Halley. According to Pliny the Elder, that same year a meteorite fell in the town of Aegospotami, in Thrace. He described it as brown in colour and the size of a wagon load. Chinese chroniclers also mention a comet in that year.
The first certain appearance of Halley's Comet in the historical record is a description from 240 BC, in the Chinese chronicle "Records of the Grand Historian" or "Shiji", which describes a comet that appeared in the east and moved north. The only surviving record of the 164 BC apparition is found on two fragmentary Babylonian tablets, now owned by the British Museum.
The apparition of 87 BC was recorded in Babylonian tablets which state that the comet was seen "day beyond day" for a month. This appearance may be recalled in the representation of Tigranes the Great, an Armenian king who is depicted on coins with a crown that features, according to Vahe Gurzadyan and R. Vardanyan, "a star with a curved tail [that] may represent the passage of Halley's Comet in 87 BC." Gurzadyan and Vardanyan argue that "Tigranes could have seen Halley's Comet when it passed closest to the Sun on August 6 in 87 BC" as the comet would have been a "most recordable event"; for ancient Armenians it could have heralded the New Era of the brilliant King of Kings.
The apparition of 12 BC was recorded in the "Book of Han" by Chinese astronomers of the Han Dynasty who tracked it from August through October. It passed within 0.16 AU of Earth. Halley's appearance in 12 BC, only a few years distant from the conventionally assigned date of the birth of Jesus Christ, has led some theologians and astronomers to suggest that it might explain the biblical story of the Star of Bethlehem. There are other explanations for the phenomenon, such as planetary conjunctions, and there are also records of other comets that appeared closer to the date of Jesus' birth.
If, as has been suggested, the reference in the Talmud to "a star which appears once in seventy years that makes the captains of the ships err" (see above) refers to Halley's Comet, it may be a reference to the 66 AD appearance, because this passage is attributed to the Rabbi Yehoshua ben Hananiah. This apparition was the only one to occur during ben Hananiah's lifetime.
The 141 AD apparition was recorded in Chinese chronicles. It was also recorded in the Tamil work Purananuru, in connection with the death of a south Indian king.
The 374 AD and 607 approaches each came within 0.09 AU of Earth. The 684 AD apparition was recorded in Europe in one of the sources used by the compiler of the 1493 Nuremberg Chronicles. Chinese records also report it as the "broom star".
In 837, Halley's Comet may have passed as close as 0.03 AU (3.2 million miles; 5.1 million kilometers) from Earth, by far its closest approach. Its tail may have stretched 60 degrees across the sky. It was recorded by astronomers in China, Japan, Germany and the greater Middle East. In 912, Halley is recorded in the Annals of Ulster, which state "A dark and rainy year. A comet appeared."
1066.
In 1066, the comet was seen in England and thought to be an omen: later that year Harold II of England died at the Battle of Hastings; it was a bad omen for Harold, but a good omen for the man who defeated him, William the Conqueror. The comet is represented on the Bayeux Tapestry as a fiery star, and the surviving accounts describe it as appearing to be four times the size of Venus and shining with a light equal to a quarter of that of the Moon. Halley came within 0.10 AU of Earth at that time.
This appearance of the comet is also noted in the "Anglo-Saxon Chronicle". Eilmer of Malmesbury may have seen Halley previously in 989, as he wrote of it in 1066: "You've come, have you? ... You've come, you source of tears to many mothers, you evil. I hate you! It is long since I saw you; but as I see you now you are much more terrible, for I see you brandishing the downfall of my country. I hate you!"
The Irish "Annals of the Four Masters" recorded the comet as "A star [that] appeared on the seventh of the Calends of May, on Tuesday after Little Easter, than whose light the brilliance or light of The Moon was not greater; and it was visible to all in this manner till the end of four nights afterwards." Chaco Native Americans in New Mexico may have recorded the 1066 apparition in their petroglyphs.
1145–1378.
The 1145 apparition was recorded by the monk Eadwine. The 1986 apparition exhibited a fan tail similar to Eadwine's drawing. Some claim that Genghis Khan was inspired to turn his conquests toward Europe by the 1222 apparition. The 1301 apparition may have been seen by the artist Giotto di Bondone, who represented the Star of Bethlehem as a fire-colored comet in the Nativity section of his Arena Chapel cycle, completed in 1305. No record survives of the 1378 apparition.
1456.
In 1456, the year of Halley's next apparition, the Ottoman Empire invaded the Kingdom of Hungary, culminating in the Siege of Belgrade in July of that year. In a Papal Bull, Pope Calixtus III ordered special prayers be said for the city's protection. In 1470, the humanist scholar Bartolomeo Platina wrote in his "Lives of the Popes" that,
A hairy and fiery star having then made its appearance for several days, the mathematicians declared that there would follow grievous pestilence, dearth and some great calamity. Calixtus, to avert the wrath of God, ordered supplications that if evils were impending for the human race He would turn all upon the Turks, the enemies of the Christian name. He likewise ordered, to move God by continual entreaty, that notice should be given by the bells to call the faithful at midday to aid by their prayers those engaged in battle with the Turk.
Platina's account is not mentioned in official records. In the 18th century, a Frenchman further embellished the story, in anger at the Church, by claiming that the Pope had "excommunicated" the comet, though this story was most likely his own invention.
Halley's apparition of 1456 was also witnessed in Kashmir and depicted in great detail by Śrīvara, a Sanskrit poet and biographer to the Sultans of Kashmir. He read the apparition as a cometary portent of doom foreshadowing the imminent fall of Sultan Zayn al-Abidin (AD 1418/1420-1470).
After witnessing a bright light in the sky (which most historians have identified as Halley's Comet, visible in Ethiopia in 1456), Emperor Zara Yaqob, ruler from 1434 to 1468, founded the city of Debre Berhan (tr. City of Light) and made it his capital for the remainder of his reign.
1531–1835.
Halley's periodic returns have been subject to scientific investigation since the 16th century. The three apparitions from 1531 to 1682 were noted by Edmond Halley, enabling him to predict its 1759 return. Streams of vapour observed during the comet's 1835 apparition prompted astronomer Friedrich Wilhelm Bessel to propose that the jet forces of evaporating material could be great enough to significantly alter a comet's orbit.
1910.
The 1910 approach, which came into naked-eye view around 10 April and came to perihelion on 20 April, was notable for several reasons: it was the first approach of which photographs exist, and the first for which spectroscopic data were obtained. Furthermore, the comet made a relatively close approach of 0.15 AU, making it a spectacular sight. Indeed, on 19 May, Earth actually passed through the tail of the comet. One of the substances discovered in the tail by spectroscopic analysis was the toxic gas cyanogen, which led astronomer Camille Flammarion to claim that, when Earth passed through the tail, the gas "would impregnate the atmosphere and possibly snuff out all life on the planet." His pronouncement led to panicked buying of gas masks and quack "anti-comet pills" and "anti-comet umbrellas" by the public. In reality, as other astronomers were quick to point out, the gas is so diffuse that the world suffered no ill effects from the passage through the tail.
The comet added to the unrest in China on the eve of Xinhai Revolution that would end the last dynasty in 1911. As James Hutson, a missionary in Sichuan Province at the time, recorded,
The people believe that it indicates calamity such as war, fire, pestilence, and a change of dynasty. In some places on certain days the doors were unopened for half a day, no water was carried and many did not even drink water as it was rumoured that pestilential vapour was being poured down upon the earth from the comet."
The comet was also fertile ground for hoaxes. One that reached major newspapers claimed that the Sacred Followers, a supposed Oklahoma religious group, attempted to sacrifice a virgin to ward off the impending disaster, but were stopped by the police.
American satirist and writer Mark Twain was born on 30 November 1835, exactly two weeks after the comet's perihelion. In his autobiography, published in 1909, he said,
I came in with Halley's comet in 1835. It is coming again next year, and I expect to go out with it. It will be the greatest disappointment of my life if I don't go out with Halley's comet. The Almighty has said, no doubt: 'Now here are these two unaccountable freaks; they came in together, they must go out together.'
Twain died on 21 April 1910, the day following the comet's subsequent perihelion. The 1985 fantasy film "The Adventures of Mark Twain" was inspired by the quotation.
Halley's 1910 apparition is distinct from the Great Daylight Comet of 1910, which surpassed Halley in brilliance and was actually visible in broad daylight for a short period, approximately four months before Halley made its appearance.
1986.
Halley's 1986 apparition was the least favorable on record. The comet and Earth were on opposite sides of the Sun in February 1986, creating the worst viewing circumstances for Earth observers for the last 2,000 years. Halley's closest approach was 0.42 AU. Additionally, with increased light pollution from urbanization, many people failed to even see the comet. It was possible to observe it in areas outside of cities with the help of binoculars. Further, the comet appeared brightest when it was almost invisible from the northern hemisphere in March and April. Halley's approach was first detected by astronomers David Jewitt and G. Edward Danielson on 16 October 1982 using the 5.1 m Hale telescope at Mount Palomar and a CCD camera. The first person to visually observe the comet on its 1986 return was amateur astronomer Stephen James O'Meara on 24 January 1985. O'Meara used a home-built 24-inch telescope on top of Mauna Kea to detect the magnitude 19.6 comet. On 8 November 1985, Stephen Edberg (then serving as the Coordinator for Amateur Observations at NASA's Jet Propulsion Laboratory) and Charles Morris were the first to observe Halley's Comet with the naked eye in its 1986 apparition.
The development of space travel gave scientists the opportunity to study the comet at close quarters, and several probes were launched to do so. The Soviet Vega 1 started returning images of Halley on 4 March 1986, and the first ever of its nucleus, and made its flyby on 6 March, followed by Vega 2 making its flyby on 9 March. On 14 March, the Giotto space probe, launched by the European Space Agency, made the closest pass of the comet's nucleus. There were also two Japanese probes, Suisei and Sakigake. The probes were unofficially known as the Halley Armada.
Based on data retrieved by Astron, the largest ultraviolet space telescope of the time, during its Halley's Comet observations in December 1985, a group of Soviet scientists developed a model of the comet's coma. The comet was also observed from space by the International Cometary Explorer. Originally International Sun-Earth Explorer 3, the probe was renamed and freed from its L1 Lagrangian point location in Earth's orbit to intercept comets 21P/Giacobini-Zinner and Halley.
Two Space Shuttle missions – the ill-fated STS-51-L (ended by the "Challenger" disaster) and STS-61-E – were scheduled to observe Halley's Comet from low Earth orbit. STS-51-L carried the Shuttle-Pointed Tool for Astronomy (SPARTAN-203) satellite, also called the Halley's Comet Experiment Deployable (HCED). STS-61-E was a "Columbia" mission scheduled for March 1986, carrying the ASTRO-1 platform to study the comet. Due to the suspension of America's manned space program after the Challenger explosion, the mission was canceled, and ASTRO-1 would not fly until late 1990 on STS-35.
After 1986.
On 12 February 1991, at a distance of 14.4 AU from the Sun, Halley displayed an outburst that lasted for several months, releasing a cloud of dust 300,000 km across. The outburst likely started in December 1990, and then the comet brightened from magnitude 24.3 to magnitude 18.9. Halley was most recently observed in 2003 by three of the Very Large Telescopes at Paranal, Chile, when Halley's magnitude was 28.2. The telescopes observed Halley, at the faintest and farthest any comet has ever been imaged, in order to verify a method for finding very faint trans-Neptunian objects. Astronomers are now able to observe the comet at any point in its orbit.
The next predicted perihelion of Halley's Comet is 28 July 2061, when it is expected to be better positioned for observation than during the 1985–1986 apparition, as it will be on the same side of the Sun as Earth. It is expected to have an apparent magnitude of −0.3, compared with only +2.1 for the 1986 apparition. It has been calculated that on 9 September 2060, Halley will pass within 0.98 AU of Jupiter, and then on 20 August 2061 will pass within 0.0543 AU of Venus. In 2134, Halley is expected to pass within 0.09 AU of Earth. Its apparent magnitude is expected to be −2.0.

</doc>
<doc id="46084" url="http://en.wikipedia.org/wiki?curid=46084" title="MOPE">
MOPE

MOPE may refer to:

</doc>
<doc id="46086" url="http://en.wikipedia.org/wiki?curid=46086" title="Eared seal">
Eared seal

An eared seal or otariid or otary is any member of the marine mammal family Otariidae, one of three groupings of pinnipeds. They comprise 15 extant species in seven genera (another species became extinct in the 1950s) and are commonly known either as sea lions or fur seals, distinct from true seals (phocids) and the walrus (odobenids). Otariids are adapted to a semiaquatic lifestyle, feeding and migrating in the water, but breeding and resting on land or ice. They reside in subpolar, temperate, and equatorial waters throughout the Pacific and Southern Oceans and the southern Indian and Atlantic Oceans. They are conspicuously absent in the north Atlantic.
The words 'otariid' and 'otary' come from the Greek "otarion" meaning "little ear", referring to the small but visible external ear flaps (pinnae), which distinguishes them from the phocids.
Evolution and taxonomy.
Along with the Phocidae and Odobenidae, the two other members of Pinnipedia, Otаriidae are descended from a common ancestor most closely related to modern bears. Debate remains as to whether the phocids diverged from the otariids before or after the walrus.
Otariids arose in the Miocene (15-17 million years ago) in the North Pacific, diversifying rapidly into the Southern Hemisphere, where most species now live. The earliest known fossil osariid is "Eotaria crypta" from southern California, while the genus "Callorhinus" (northern fur seal) has the oldest fossil record of any living otariid, extending to the middle Pliocene. It probably arose from the extinct fur seal genus "Thalassoleon".
Traditionally, otariids had been subdivided into the fur seal (Arctocephalinae) and sea lion (Otariinae) subfamilies, with the major distinction between them being the presence of a thick underfur layer in the former. Under this categorization, the fur seals comprised two genera: "Callorhinus" in the North Pacific with a single representative, the northern fur seal ("C. ursinus"), and eight species in the Southern Hemisphere under the genus "Arctocephalus"; while the sea lions comprise five species under five genera. Recent analyses of the genetic evidence suggests that "Callorhinus ursinus" is in fact more closely related to several sea lion species. Furthermore, many of the Otariinae appear to be more phylogenetically distinct than previously assumed; for example, the Japanese sea lion ("Zalophus japonicus") is now considered a separate species, rather than a subspecies of the California sea lion ("Zalophus californius"). In light of this evidence, the subfamily separation has been removed entirely and the Otariidae family has been organized into seven genera with 16 species and two subspecies.
Nonetheless, because of morphological and behavioral similarities among the "fur seals" and "sea lions", these remain useful categories when discussing differences between groups of species. Compared to sea lions, fur seals are generally smaller, exhibit greater sexual dimorphism, eat smaller prey and go on longer foraging trips; and, of course, there is the contrast between the coarse short sea lion hair and the fur seal's fur.
Anatomy and appearance.
Otariids have proportionately much larger foreflippers and pectoral muscles than phocids, and have the ability to turn their hind limbs forward and walk on all fours, making them far more maneuverable on land. They are generally considered to be less adapted to an aquatic lifestyle, since they breed primarily on land and haul out more frequently than true seals. However, they can attain higher bursts of speed and have greater maneuverability in the water. Their swimming power derives from the use of flippers more so than the sinuous whole-body movements typical of phocids and walruses.
Otariids are further distinguished by a more dog-like head, sharp, well-developed canines, and the aforementioned visible external pinnae. Their postcanine teeth are generally simple and conical in shape. The dental formula for eared seals is: 3.1.4.1-32.1.4.1. Sea lions are covered with coarse guard hairs, while fur seals have a thick underfur, which has historically made them the objects of commercial exploitation.
Male otariids range in size from the 70-kg (150-lb) Galápagos fur seal, smallest of all pinnipeds, to the over 1,000-kg (2,200-lb) Steller sea lion. Mature male otariids weigh two to six times as much as females, with proportionately larger heads, necks, and chests, making them the most sexually dimorphic of all mammals.
Behavior.
All otariids breed on land during well-defined breeding seasons. Except for the Australian sea lion, which has an atypical 17.5 month breeding cycle, they form strictly annual aggregations on beaches or rocky substrates, often on islands. All species are polygynous; i.e. successful males breed with several females. In most species, males arrive at breeding sites first and establish and maintain territories through vocal and visual displays and occasional fighting. Females typically arrive on shore a day or so before giving birth. While considered social animals, no permanent hierarchies or statuses are established on the colonies. The extent to which males control females or territories varies between species. Thus, the northern fur seal and the South American sea lion tend to herd specific harem-associated females, occasionally injuring them, while the Steller sea lion and the New Zealand sea lion control spatial territories, but do not generally interfere with the movement of the females.
Otariids are carnivorous, feeding on fish, squid and krill. Sea lions tend to feed closer to shore in upwelling zones, feeding on larger fish, while the smaller fur seals tend to take longer, offshore foraging trips and can subsist on large numbers of smaller prey items. They are visual feeders. Some females are capable of dives of up to 400 m (1,300 ft).
Species.
Family Otariidae
Although the two subfamilies of otariids, the Otariinae (sea lions) and Arctocephalinae (fur seals), are still widely used, recent molecular studies have demonstrated that they may be invalid. Instead, they suggest three clades within the family; one consisting of the northern sea lions ("Eumetopias" and "Zalophus"), one of the northern fur seal ("Callorhinus") and its extinct relatives, and the third of all the remaining Southern Hemisphere species.

</doc>
<doc id="46088" url="http://en.wikipedia.org/wiki?curid=46088" title="Pardubice">
Pardubice

Pardubice (]; German: "Pardubitz") is a city in the Czech Republic. It is the capital city of the Pardubice Region and lies on the river Elbe, 96 kilometres east of Prague. There is an old Tower and a Castle. Factories include the Synthesia chemical factory (manufacturer of Semtex, a plastic explosive), an oil refinery Paramo, a heavy machinery factory and an electronic equipment plant. The city is well known for its sport events (Great Pardubice Steeplechase, Golden Helmet of Pardubice, Czech Open), ginger bread, rail and air transport.
History.
The oldest extant Document regarding Pardubice comes from 1295.
The area had a monastery beginning in the early 13th century, and the city was founded  1340. In 1491, Pardubice was bought by William II of Pernstein, who continued to expand the town and made significant impact on its prosperity. Until 1918, the town was part of the Austrian monarchy (Austria side after the compromise of 1867), head of the PARDUBITZ district, one of the 94 "Bezirkshauptmannschaften" in Bohemia.
In 1845, the first train arrived to Pardubice. The town was connected to other railway lines so Pardubice could thrive even more. New industrial enterprises started to emerge in the town, namely a distillery, a factory for mill machines of Josef Prokop and sons and Fanta’s Factory. Since 1874, the Great Pardubice Steeplechase ("Velká Pardubická") horse race has taken place every autumn (second Sunday in October). 
On 13 May 1911, Ing. Jan Kašpar made history by flying the first long-haul flight towards Prague. In Pardubice, industrial expansion was on the rise, especially after the First World War. However, during the Second World War the town was damaged by air strikes of the Allies. The Fanto Werke refinery at Pardubice was repeatedly bombed during the Oil Campaign of World War II, and forced labor was provided by a concentration camp. The German population was expelled according to the Benes Decrees in 1945. Tesla electronics manufacturer operated from 1921–1989, and the Foxconn factory was established in June 2000. After 1989 the town continued to develop and flourish; the Chateau and its surroundings of estates were reconstructed. Pardubice has established contacts with foreign towns.
Geography.
Pardubice is situated on the bank of the second longest river in the Czech Republic, the Labe River, where there is a mouth of another river called Chrudimka. Pardubice is located at approximately 15° east longitude and 50° north longitude. The town is located 100 km east of the capital city of Prague, 150 km north-west of Brno. Pardubice is in the area of Labe Lowlands with average elevation of 225m and its area is 78 km2. The area is of lowland character without many hills. One exception is a nearby hill Kunětická hora.
Industry.
Pardubice is called the city of industry. The dominant industries are chemical industry, electrical engineering and mechanical engineering. The chemical industry is mainly represented by a company Paramo and Synthesia, which was founded in Pardubice-Semtín as a stock factory for explosive substances. This field of industry together with the factory went through significant development, especially in 1960s. Synthesia is now one of the leading Czech companies manufacturing cellulose, pigments and dyes, and organic compounds. Synthesia is also a major exporter mainly for the EU countries and is associated with the invention of Semtex plastic explosive. Paramo – formally known as Fanta’s Factory was until 2012 one of the major companies of its kind, but during the year a major shareholder decided to significantly suppress its production and the future of Paramo is still uncertain.
Cultural monuments and sights.
Pardubice is dominated by the Green Gate with remains of the town’s fortifications. The Chateau, which has been reconstructed, is located nearby. The town itself has many historical buildings, for example, Kamenná vila (Stone Villa), Crematorium, Dům U Bílého koníčka (House at the White Horse), Wernerův dům (Werner’s House), Dům U Jonáše (At Jonah’s), the City Hall. Churches are dedicated to the Our Lady of Sorrows, St. John the Baptist, St. Bartholomew.
Sport.
Hockey club HC Pardubice plays in the Czech Extraliga. The club plays home fixtures in the ČEZ Arena.
VCHZ Pardubice football team played in the top national league in the 1968–69 season. s of 2014[ [update]], the highest-ranked team from the city is FK Pardubice, which plays in the second-tier Fotbalová národní liga. Women's team SK DFO Pardubice plays in the Czech First Division (women).
The basketball team is BK JIP Pardubice.
The city is also home to the Golden Helmet of Pardubice (also known as the Czech Golden Helmet), a Motorcycle speedway competition held at the Svítkova Stadion. The Golden Helmet has been run since 1929 is one of the most prestigious individual titles in world speedway outside of the Speedway World Championship or a riders national championship. Winners of the Golden Helmet have included World Champions Ole Olsen, Erik Gundersen, Hans Nielsen and Nicki Pedersen (Denmark), Ove Fundin, Per Jonsson and Tony Rickardsson (Sweden), and Jason Crump and 2014 winner Chris Holder (Australia).
Ole Olsen holds the record for the most Golden Helmet wins with 7 (1970, 1971, 1972, 1975, 1977, 1979 and 1980).
International relations.
Twin towns – Sister cities.
Pardubice is twinned with:

</doc>
<doc id="46089" url="http://en.wikipedia.org/wiki?curid=46089" title="Frans Hals">
Frans Hals

Frans Hals the Elder (; ]; c. 1582 – 26 August 1666) was a Dutch Golden Age portrait painter who lived and worked in Haarlem. He is notable for his loose painterly brushwork, and he helped introduce this lively style of painting into Dutch art. Hals played an important role in the evolution of 17th-century group portraiture.
Biography.
Hals was born in 1582 or 1583, in Antwerp as the son of the cloth merchant Franchois Fransz Hals van Mechelen (c.1542–1610) and his second wife Adriaentje van Geertenryck. Like many, Hals' parents fled during the Fall of Antwerp (1584–1585) from the Spanish Netherlands to Haarlem, where he lived for the remainder of his life. Hals studied under another Flemish émigré, Karel van Mander, whose Mannerist influence, however, is barely noticeable in Hals' work.
In 1610, Hals became a member of the Haarlem Guild of Saint Luke, and he started to earn money as an art restorer for the city council. He worked on their large art collection that Karel van Mander had described in his "Schilderboeck" ("Painter's Book") published in Haarlem in 1604. The most notable of these were the works of Geertgen tot Sint Jans, Jan van Scorel and Jan Mostaert, that hung in the St. Janskerk in Haarlem. The restoration work was paid for by the city of Haarlem, since all Catholic religious art had been confiscated after the satisfactie van Haarlem, which gave Catholics equal rights to Protestants, had been reversed in 1578. However, the entire collection of paintings was not formally possessed by the city council until 1625, after the city fathers had decided which paintings were suitable for the city hall. The remaining art that was considered too "Roman Catholic" was sold to Cornelis Claesz van Wieringen, a fellow guild member, on the condition that he remove it from the city. It was in this cultural context that Hals began his career in portraiture, since the market for religious themes had disappeared.
The earliest known example of Hals' art is the portrait of "Jacobus Zaffius" (1611). His 'breakthrough' came with the life-size group portrait, "The Banquet of the Officers of the St George Militia Company in 1616". His most noted portrait today is the one he made in 1649 of "René Descartes".
Frans Hals married his first wife, Anneke Harmensdochter around 1610, though since Frans was of Catholic birth, their marriage was recorded in the city hall and not in church. Unfortunately, the exact date is unknown, because the older marriage records of the Haarlem city hall before 1688 have not been preserved. Anneke was born 2 January 1590 as the daughter of the bleacher Harmen Dircksz and Pietertje Claesdr Ghijblant, and her maternal grandfather, the linen producer Claes Ghijblant of , bequeathed the couple the grave in the St. Bavochurch where both are buried, though Frans took over 40 years to join his first wife there. Anneke died in 1615 shortly after the birth of their third child and of these three, we know Harmen survived infancy and one had died before Hals' second marriage. As biographer Seymour Slive has pointed out, older stories of Frans Hals abusing his first wife were confused with another Haarlem resident of the same name. Indeed, at the time of these charges, the artist had no wife to mistreat as Anneke had died in May 1615. Similarly, historical accounts of Hals' propensity for drink have been largely based on embellished anecdotes of his early biographers, namely Arnold Houbraken, with no direct evidence existing documenting such. After his first wife died, Hals took on the young daughter of a fishmonger to look after his children, and in 1617, he married Lysbeth Reyniers. They married in Spaarndam, a small village outside the banns of Haarlem, because she was already 8 months pregnant. Frans Hals was a devoted father and they went on to have eight children.
Where Hals contemporaries such as Rembrandt moved their households according to the caprices of their patrons, Hals remained in Haarlem and insisted that his customers come to him. According to the Haarlem archives, a schutterstuk that Hals started in Amsterdam was finished by Pieter Codde because Hals refused to paint in Amsterdam, insisting that the militiamen come to Haarlem to sit for their portraits. For this reason we can be sure that all sitters were either from Haarlem or were visiting Haarlem when they had their portraits made. Although Hals' work was in demand throughout his life, he lived so long that he eventually went out of style as a painter and experienced financial difficulties. In addition to his painting, he continued throughout his life to work as a restorer, art dealer, and art tax expert for the city councilors. His creditors took him to court several times, and to settle his debt with a baker in 1652 he sold his belongings. The inventory of the property seized mentions only three mattresses and bolsters, an armoire, a table and five pictures (these were by himself, his sons, van Mander, and Maarten van Heemskerck). Left destitute, he was given an annuity of 200 florins in 1664 by the municipality.
At a time when the Dutch nation fought for independence during the Eighty Years' War, Hals was a member of the local schutterij, a military guild. According to its 19th-century painting frame, Hals included a self-portrait in his 1639 painting of the St. Joris company. It has not been possible to confirm this. It was not common for ordinary members to be painted as that privilege was reserved for the officers. Frans Hals painted the company three times. Hals was also a member of a local chamber of rhetoric, and in 1644 he became chairman of the Guild of St. Luke.
Frans Hals died in Haarlem in 1666 and was buried in the city's St. Bavo Church. He had been receiving a city pension, which was highly unusual and a sign of the esteem with which he was regarded. After his death, his widow later also applied for aid and was admitted to the local almshouse where she later died.
Artistic career.
Hals is best known for his portraits, mainly of wealthy citizens, like Pieter van den Broecke and Isaac Massa, whom he painted three times. He also painted large group portraits for local civic guards and for the regents of local hospitals. He was a Dutch Golden Age painter who practiced an intimate realism with a radically free approach. His pictures illustrate the various strata of society; banquets or meetings of officers, guildsmen, local councilmen from mayors to clerks, itinerant players and singers, gentlefolk, fishwives and tavern heroes. In his group portraits, such as "The Banquet of the Officers of the St Adrian Militia Company in 1627", Hals captures each character in a different manner. The faces are not idealized and are clearly distinguishable, with their personalities revealed in a variety of poses and facial expressions.
Hals was fond of daylight and silvery sheen, while Rembrandt used golden glow effects based upon artificial contrasts of low light in immeasurable gloom. Both men were painters of touch, but of touch on different keys — Rembrandt was the bass, Hals the treble. Hals seized, with rare intuition, a moment in the life of his subjects. What nature displayed in that moment he reproduced thoroughly in a delicate scale of color, and with mastery over every form of expression. He became so clever that exact tone, light and shade, and modeling were obtained with a few marked and fluid strokes of the brush. He became a popular portrait painter, and painted the wealthy of Haarlem on special occasions. He won many commissions for wedding portraits (the husband is traditionally situated on the left, and the wife situated on the right). His double portrait of the newly married Olycans hang side by side in the Mauritshuis, but many of his wedding portrait pairs have since been split up and are rarely seen together.
Wedding portraits.
The only record of his work in the first decade of his independent activity is an engraving by Jan van de Velde copied from the lost portrait of "The Minister Johannes Bogardus". Early works by Hals, such as "Two singing boys with a lute and a music book" and the aforementioned "Banquet of the Officers of the St George Militia" (1616), show him as a careful draughtsman capable of great finish, yet spirited withal. The flesh he painted is pastose and burnished, less clear than it subsequently became. Later, he became more effective, displayed more freedom of hand, and a greater command of effect.
During this period he painted the full-length portrait of Madame van Beresteyn (Louvre), and a full-length portrait of Willem van Heythuyzen leaning on a sword. Both these pictures are equalled by the other "Banquet of the Officers of the St George Militia" (with different portraits) and the "same militia in 1627" and "Banquet of the Officers of the St Hadrian Militia" of 1633. A similar painting, with the date of 1639, suggests some study of Rembrandt masterpieces, and a similar influence is apparent in a representing the regents of the St. Elisabeth Gasthuis, and in his 1639 portrait of at Amsterdam.
From 1620 till 1640 he painted many double portraits of married couples, on separate panels, the man on the left panel, his wife at his right. Only once did Hals portray a couple on a single canvas: "Couple in a garden: Wedding portrait of Isaac Abrahamsz. Massa and Beatrix van der Laan", (c. 1622, Rijksmuseum Amsterdam).
His style changed throughout his life. Paintings of vivid color were gradually replaced by pieces where one color dominated: black. This was probably due to the sober dress of his Protestant sitters, more than any personal preference. One simple way to observe this change is to look at all of the portraits he painted through the years with his trademark-pose leaning over the back of a chair:
Portrait painter.
Later in his life his brush strokes became looser, fine detail becoming less important than the overall impression. Where his earlier pieces radiated gaiety and liveliness, his later portraits emphasized the stature and dignity of the people portrayed. This austerity is displayed in ' in 1641, and two decades later, "The Regents" and "Regentesses of the Old Men's Almshouse" (c. 1664), which are masterpieces of color, though in substance all but monochromes. His restricted palette is particularly noticeable in his flesh tints, which from year to year became greyer, until finally the shadows were painted in almost absolute black, as in the '.
As this tendency coincides with the period when he was less popular among the wealthy, some historians have suggested that a reason for his predilection for black and white pigment was the low price of these colors as compared with the costly lakes and carmines. Both conclusions are probably correct, however, because unlike his contemporaries, Hals did not travel to his sitters, but let them come to him. This was good for business because he was exceptionally quick and efficient in his own well-fitted studio, but it was bad for business when Haarlem fell on hard times.
As a portrait painter, Hals had scarcely the psychological insight of a Rembrandt or Velázquez, though in a few works, like the "Admiral de Ruyter", the "Jacob Olycan", and the "Albert van der Meer" paintings, he reveals a searching analysis of character which has little in common with the instantaneous expression of his so-called character portraits. In these, he generally sets upon the canvas the fleeting aspect of the various stages of merriment, from the subtle, half ironic smile that quivers round the lips of the curiously misnamed "Laughing Cavalier" to the imbecile grin of the "Malle Babbe". To this group of pictures belong "Baron Gustav Rothschilds Jester", the "Bohemienne" and the "Laughing Fisherboy", whilst the "Portrait of the Artist with his Second Wife", and the somewhat confused group of the ' at the Louvre show a similar tendency. Far less scattered in arrangement than this Beresteyn group, and in every respect one of the most masterly of Hals' achievements is the group called ', which was almost unknown until it appeared at the winter exhibition at the Royal Academy in 1906.
Many of Hals' works have disappeared, but it is not known how many. According to the most authoritative present day catalogue, compiled by Seymour Slive in 1970−1974 (Slive's last great Hals exhibition catalogue followed in 1989), another 222 paintings can be ascribed to Hals. Another authority on Hals, Claus Grimm, believes this number to be lower (145) in his "Frans Hals. Das Gesamtwerk" (1989).
It is not known whether Hals ever painted landscapes, still lifes or narrative pieces, but it is unlikely. His debut for Haarlem society in 1616 with his large group portrait for the St. George militia shows all three disciplines, but if that painting was his signboard for future commissions, it seems he was subsequently only hired for portraits. Many artists in the 17th century in Holland opted to specialise, and Hals also appears to have been a pure portrait specialist.
Painting technique.
Hals was a master of a technique that utilized something previously seen as a flaw in painting, the visible brushstroke. The soft curling lines of Hals' brush are always clear upon the surface: "materially just lying there, flat, while conjuring substance and space in the eye."
Lively and exciting, the technique can appear "ostensibly slapdash" – people often think that Hals 'threw' his works 'in one toss' ("aus einem Guss") onto the canvas. This impression is not correct. True, the odd work was largely put down without underdrawings or underpainting ('alla prima'), but most of the works were created in successive layers, as was customary at that time. Sometimes a drawing was made with chalk or paint on top of a grey or pink undercoat, and was then more or less filled in, in stages. It does seem that Hals usually applied his underpainting very loosely: he was a virtuoso from the beginning. This applies, of course, particularly to his genre works and his somewhat later, mature works. Hals displayed tremendous daring, great courage and virtuosity, and had a great capacity to pull back his hands from the canvas, or panel, at the moment of the most telling statement. He didn't 'paint them to death', as many of his contemporaries did, in their great accuracy and diligence whether requested by their clients or not.
In the 17th century his first biographer, Schrevelius wrote: "An unusual manner of painting, all his own, surpassing almost everyone," on Hals' painting methods. For that matter, schematic painting was not Hals' own idea (the approach already existed in 16th century Italy), and Hals was probably inspired by Flemish contemporaries, Rubens and Van Dyck, in his painting method.
As early as the 17th century, people were struck by the vitality of Frans Hals' portraits. For example, Haarlem resident Theodorus Schrevelius noted that Hals' works reflected 'such power and life' that the painter 'seems to challenge nature with his brush'. Centuries later Vincent van Gogh wrote to his brother Theo: 'What a joy it is to see a Frans Hals, how different it is from the paintings – so many of them – where everything is carefully smoothed out in the same manner.' Hals chose not to give a smooth finish to his painting, as most of his contemporaries did, but mimicked the vitality of his subject by using smears, lines, spots, large patches of color and hardly any details.
It was not until the 19th century that his technique had followers, particularly among the Impressionists. Pieces such as "The Regentesses of the Old Men's Alms House" and the civic guard paintings demonstrate this technique to the fullest.
Influence.
Frans influenced his brother Dirck Hals (born at Haarlem, 1591–1656), who was also a painter. Additionally, five of his sons became painters:
Though most of his sons became portrait painters, some of them took up still life painting or architectural studies and landscapes. Still lifes formerly attributed to his son Frans II have since been re-attributed to other painters, however. Frans Hals painted a young woman reaching into a basket in a still life market scene by Claes van Heussen.
Other contemporary painters who took inspiration from Frans Hals were:
Hals had a large workshop in Haarlem and many students, though 19th century biographers questioned some of his pupils, since their painting styles were so dissimilar to Hals. In his "De Groote Schouburgh" (1718–21), Arnold Houbraken mentions Philips Wouwerman, Adriaen Brouwer, Pieter Gerritsz van Roestraten, Adriaen van Ostade and Dirck van Delen as students. Vincent Laurensz van der Vinne was also a student, according to his diary with notes left by his son Laurens Vincentsz van der Vinne. Roestraten was not only a student (the Haarlem archives contain a notarised document, which supports this fact), but he also became a son-in-law of Hals when he married his daughter Adriaentje. The Haarlem portrait painter, Johannes Verspronck, one of about 10 competing portraitists in Haarlem at the time, possibly studied for some time with Hals.
In terms of style, the closest to Hals' work is the handful of paintings that are ascribed to Judith Leyster, which she often signed. She also 'qualifies' as a possible student, as does her husband, the painter Jan Miense Molenaer.
Two centuries after his death, Hals received a number of 'posthumous' students. Claude Monet, Édouard Manet, Charles-François Daubigny, Max Liebermann, James Abbott McNeill Whistler, Gustave Courbet, and in the Netherlands, Jacobus van Looy and Isaac Israëls are some of the Impressionists and realists who have delved deeply into the work of Hals by making study copies of his work and further building on his techniques and style. Lovis Corinth named Hals as his biggest influence. Many artists travelled to the Frans Hals Museum in Haarlem (since 1913 on the Groot Heiligland, and before that in the Town Hall), where several of his most important works are kept.
Legacy.
Hals' reputation waned after his death and for two centuries he was held in such poor esteem that some of his paintings, which are now among the proudest possessions of public galleries, were sold at auction for a few pounds or even shillings. The portrait of Johannes Acronius realized five shillings at the Enschede sale in 1786. The portrait of the sold in 1800 for 4, 5s.
Starting at the middle of the 1860s his prestige rose again thanks to the efforts of critic Théophile Thoré-Bürger. With his rehabilitation in public esteem came the enormous rise in value, and, at the Secretan sale in 1889, the portrait of Pieter van den Broecke was bid up to 4,420 francs, while in 1908 the National Gallery paid 25,000 pounds for the from the collection of Lord Talbot de Malahide.
Hals' work remains popular today, particularly with young painters who can find many lessons about practical technique from his unconcealed brushstrokes. Hals' works have found their way to countless cities all over the world and into museum collections. From the late 19th century, they were collected everywhere — from Antwerp to Toronto, and from London to New York. Many of his paintings were then sold to American collectors.
A primary collection of his work is displayed in the Frans Hals Museum in Haarlem.
A crater on Mercury is named in his honor.
Hals was pictured on the Netherlands' 10-guilder banknote of 1968.
References and sources.
Parts of this article are excerpts of "The Frans Hals Museum, Haarlem, July 2005" by Antoon Erftemeijer, Frans Hals Museum curator.

</doc>
<doc id="46092" url="http://en.wikipedia.org/wiki?curid=46092" title="Grasse">
Grasse

Grasse (]; Provençal Occitan: "Grassa" in classical norm (and Italian) or "Grasso" in Mistralian norm) is a commune in the Alpes-Maritimes department (of which it is a sub-prefecture), on the French Riviera.
The town is considered the world's capital of perfume. It obtained two "flowers" in the "Concours des villes et villages fleuris" contest and was made "Ville d'Art et d'Histoire" (town of art and history).
Main sights.
Three perfume factories offer daily tours and demonstrations, which draw in many of the region's visitors. In addition to the perfumeries, Grasse's other main attraction is the Cathedral, dedicated to Notre Dame du Puy and founded in the 11th century. In the interior, are three works by Rubens and one by Jean-Honoré Fragonard, a French painter native of the town. 
Other sights include:
Festivals.
There is an annual "Fête du Jasmin" or "La Jasminade", at the beginning of August. The first festival was on Aug.3-4, 1946. Decorated floats drive through the town, with young women in skimpy costumes on board, throwing flowers into the crowd. Garlands of jasmine decorate the town center, and the fire department fills a fire truck with jasmine-infused water to spray on the crowds. There are also fireworks, free parties, folk music groups and street performers. There is also an annual international exhibition of roses ("Expo Rose") held in May each year.
Transport.
The Gare de Grasse railway station offers connections with Cannes, Nice and Ventimiglia.
Notable people.
Grasse was the birthplace of:
Grasse was the death place of:
Perfume.
Grasse has had a prospering perfume industry since the end of the 18th century. Grasse is the centre of the French perfume industry and is known as the world's perfume capital ("la capitale mondiale des parfums"). Many "noses" (or, in French, "Les nez" (plural)/"Le nez" (singular)) are trained or have spent time in Grasse to distinguish over 2,000 kinds of scent. Grasse produces over two-thirds of France's natural aromas (for perfume and for food flavourings). This industry turns over more than 600 million euros a year. Grasse's particular microclimate encouraged the flower farming industry. It is warm and sufficiently inland to be sheltered from the sea air. There is an abundance of water, thanks to its situation in the hills and the 1860 construction of the Siagne canal for irrigation purposes. The town is 350 m above sea level and 20 km from the "Coast" (Côte d'Azur). Jasmine, a key ingredient of many perfumes, was brought to southern France by the Moors in the 16th century. Twenty-seven tonnes of jasmine are now harvested in Grasse annually. There are numerous old 'parfumeries' in Grasse, such as Galimard, Molinard and Fragonard, each with tours and a museum.
The trade in leather and tanning work developed during the twelfth century around the small canal that runs through the city. This activity produced a strong unpleasant odor. At the time of the Renaissance perfume manufacturers began production of gloves, handbags and belt (clothing), to meet the new fashion from Italy with the entourage of Queen Catherine de Medici.
The countryside around the city began to grow fields of flowers, offering new scents from the city. In 1614, the king recognized the new corporation of "glovers perfumers".
In the middle of the eighteenth century, the perfumery was experiencing a very important development. Leading companies dating from this period includes oldest French perfumerie and third oldest parfumerie in Europe Galimard established in 1747. Introduction of new production methods turned perfume making into a real industry that could adapt to new market demands.
In the nineteenth century, the raw materials began to be imported from abroad. During the twentieth century the creation of synthetic products brought the democratization & affordability of perfumes and their spin-offs; (shampoos and deodorants, cream (pharmaceutical) and detergents, food flavoring for cookies, ice cream and dairy products, beverages, convenience foods, confectionery, preserves and syrups).
In 1905, six hundred tons of flowers were harvested while in the 1940s, five thousand tons were produced annually. However, in early 2000, production was less than 30 tons for all flowers combined.
Historical activity.
In the Middle Ages, Grasse specialized in leather tanning. Once tanned, the hides were often exported to Genoa or Pisa, cities that shared a commercial alliance with Grasse. Several centuries of this intense activity witnessed many technological advances within tanning industries. The hides of Grasse acquired a reputation for high quality. But the leather smelled badly, something that did not please the glove wearing nobility. This is when Galimard, a tanner in Grasse came up with the idea of scented leather gloves. He offered a pair of scented gloves to Catherine de Medici who was seduced by the gift. Therefore, the product spread through the Royal Court and high society and this made a worldwide reputation for Grasse. The seventeenth century became the heyday of "Glovers Perfumers'. However, high taxes on leather and competition from Nice brought a decline for the leather industry in Grasse, and production of leather fragrance ceased. The rare scents from the Grasse (lavender, myrtle, jasmine, rose, orange blossom and wild mimosa) did win the title for the Grasse as the perfume capital of the world. Harvesting jasmine was a labor-intensive business only a few decades ago. Flowers had to be hand picked at dawn, when their scent is the most developed and immediately to be treated by cold enfleurage.
Modern industry.
A network of sixty companies employs 3,500 people in the city and surrounding area. Additionally about 10,000 residents of Grasse are indirectly employed by the perfume industry. Almost half of the business tax for the city comes from the perfume sector and that is ahead of tourism and services. The main activity of perfumery in Grasse is in the production of natural raw materials (essential oils, concretes, absolutes, resinoids and molecular distillation) and the production of concentrate, also called the juice. A concentrate is the main product that when diluted in at least 80% alcohol provides a perfume. Also food flavorings, which developed since the 1970s, account for over half of production output today.
This represents almost half of the production of French perfumes and aromas and around 7-8% of total global activity. However, during the 1960s and 1970s large international groups gradually bought up local family factories (Chiris, Givaudan-Roure and Lautier, for example). Soon after their production has often been relocated overseas. Just 30 years ago most companies were focused on the production of raw materials. However an overwhelming majority of the modern fragrances contain synthetic chemicals in part or in whole. Grasse perfume companies have therefore adapted by turning to aromatic synthesis and especially to food flavorings and successfully ended a long stagnation. However, Grasse perfume industry cannot compete against large chemical multinationals, but they benefit greatly from advantages of knowledge of raw materials, facilities, contractors, etc.. In addition, major brands like Chanel have their own plantations of roses and jasmine in the vicinity of Grasse.
Perfumeries.
Three perfumeries, Fragonard, Molinard and Galimard opened their doors to the public and offer free tours that explain the processes of producing a perfume. It is possible to create one's own perfume, eau de perfume or eau de toilette and participate in all stages of manufacture from picking flowers to bottling.
Cultural references.
The town is the setting in the final chapters of the novel "Perfume" by Patrick Süskind. It was also featured in the film based on the novel (2006).
International relations.
Grasse is twinned with:

</doc>
<doc id="46093" url="http://en.wikipedia.org/wiki?curid=46093" title="Bill Hicks">
Bill Hicks

William Melvin "Bill" Hicks (December 16, 1961 – February 26, 1994) was an American comedian, social critic, satirist and musician. His material, encompassing a wide range of social issues including religion, politics, and philosophy, was controversial, and often steeped in dark comedy. He criticized consumerism, superficiality, mediocrity, and banality within the media and popular culture, which he characterized as oppressive tools of the ruling class that keep people "stupid and apathetic".
At the age of 16, while still in high school, he began performing at the Comedy Workshop in Houston, Texas. During the 1980s, he toured the United States extensively and made a number of high-profile television appearances; but it was in the UK that he amassed a significant fan base, filling large venues during his 1991 tour. He also achieved a modicum of recognition as a guitarist and songwriter.
Hicks died of pancreatic cancer on February 26, 1994 in Little Rock, Arkansas, at the age of 32. In subsequent years – in particular after a series of posthumous album releases – his body of work gained a significant measure of acclaim in creative circles, and he developed a substantial cult following. In 2007, he was voted the fourth greatest stand-up comic on Channel 4's list of the 100 Greatest Stand-Ups, and he maintained that ranking on the 2010 list.
Early life.
Hicks was born in Valdosta, Georgia, the son of James Melvin "Jim" Hicks (1923–2006) and Mary (Reese) Hicks and younger sibling of Lynn and Steve. The family lived in Florida, Alabama, and New Jersey before settling in Houston, Texas, when Bill was seven. He was drawn to comedy at an early age, emulating Woody Allen and Richard Pryor, and writing routines with his friend Dwight Slade. At school he began performing comedy – mostly derivations of Woody Allen material – for his classmates. At home, he would write his own one-liners and slide them under the bedroom door of his brother Steve – the only family member Bill respected – for his critical analysis. "Keep it up", Steve told him. "You're really good at this."
Early on, Hicks began to mock his family's Southern Baptist religious beliefs. "We were Yuppie Baptists," he joked to the "Houston Post" in 1987. "We worried about things like, 'If you scratch your neighbor's Subaru, should you leave a note?' " Biographer Cynthia True described a typical argument with his father: The elder Hicks would say, "I believe that the Bible is the literal word of God." And Bill would counter, "No it's not, Dad." "Well, I believe that it is." "Well," Bill replied, "you know, some people believe that they're Napoleon. That's fine. Beliefs are neat. Cherish them, but don't share them like they're the truth."
 Hicks did not, however, reject spiritual ideology itself, and throughout his life, he sought various alternative methods of experiencing it. Kevin Slade, elder brother of Dwight, introduced him to Transcendental Meditation and other forms of spirituality. Over one Thanksgiving weekend he took Hicks and Dwight to a TM retreat, the "Residence Course", in Galveston. Worried about his rebellious behavior, his parents took him to a psychoanalyst at age 17. According to Hicks, after the first group session the analyst took him aside and told him, "You can continue coming if you want to, but it's them, not you."
Career.
Beginnings.
Hicks was associated with the Texas Outlaw Comics group developed at the Comedy Workshop in Houston in the 1980s. Once Hicks gained some underground success in night clubs and universities, he quit drinking. However, Hicks continued to smoke cigarettes.
California and New York.
By January 1986, Hicks was using recreational drugs and his financial resources had dwindled. His career received another upturn in 1987, however, when he appeared on Rodney Dangerfield's "Young Comedians Special". The same year, he moved to New York City, and for the next five years performed about 300 times a year. On the album "Relentless", he jokes that he quit using drugs because "once you've been taken aboard a UFO, it's kind of hard to top that", although in his performances, he continued to extol the virtues of LSD, marijuana, and psychedelic mushrooms.
He eventually fell back to chain-smoking, a theme that would figure heavily in his performances from then on. His nicotine addiction, love of smoking, and occasional attempts to quit became a recurring theme in his act throughout his later years.
In 1988, Hicks signed on with his first professional business manager, Jack Mondrus.
On the track "Modern Bummer" of his 1990 album "Dangerous", Hicks says he quit drinking alcohol in 1988.
In 1989, he released his first video, "Sane Man"; a remastered version with 30 minutes of extra footage was released in 1999.
Early fame.
In 1990, Hicks released his first album, "Dangerous", performed on the HBO special "One Night Stand," and performed at Montreal's "Just for Laughs" festival. He was also part of a group of American stand-up comedians performing in London's West End in November. Hicks was a huge hit in the UK and Ireland and continued touring there throughout 1991. That year, he returned to "Just for Laughs" and filmed his second video, "Relentless."
Hicks made a brief detour into musical recording with the "Marble Head Johnson" album in 1992 collaborating with Houston high school friend Kevin Booth and Austin Texas drummer Pat Brown. During the same year he toured the UK, where he recorded the "Revelations" video for Channel 4. He closed the show with his soon-to become-famous philosophy regarding life, "It's Just a Ride." Also in that tour he recorded the stand-up performance released in its entirety on a double CD titled "Salvation." Hicks was voted "Hot Standup Comic" by "Rolling Stone magazine" in 1993. He moved to Los Angeles in 1992.
Hicks and Tool.
Progressive metal band Tool invited Hicks to open a number of concerts in its 1993 Lollapalooza appearances, where Hicks once asked the audience to look for a contact lens he had lost. Thousands of people complied.
Members of Tool felt that they and Hicks "were resonating similar concepts". Intending to raise awareness about Hicks's material and ideas, Tool dedicated their triple-platinum album "Ænima" (1996) to Hicks. Both the lenticular casing of the "Ænima" album packaging as well as the chorus of the title track "Ænema" make reference to a sketch from Hicks's "Arizona Bay" album, in which he contemplates the idea of Los Angeles falling into the Pacific Ocean. "Ænima"‍ '​s final track, "Third Eye" contains samples from Hicks's "Dangerous" and "Relentless" albums.
An alternate version of the "Ænima" artwork shows a painting of Bill Hicks, calling him "Another Dead Hero," and mentions of Hicks are found both in the liner notes and on the record.
Controversy.
Censorship and aftermath.
In 1984, Hicks was invited to appear on "Late Night with David Letterman" for the first time. He had a joke that he used frequently in comedy clubs about how he caused a serious accident that left a classmate using a wheelchair. NBC had a policy that no handicapped jokes could be aired on the show, making his stand-up routine difficult to perform without mentioning words such as "wheelchair."
On October 1, 1993, Hicks was scheduled to appear on "Late Show with David Letterman," where Letterman had recently moved to CBS. It was his 12th appearance on a Letterman late-night show, but his entire performance was removed from the broadcast; until that point the only occasion where a comedian's entire routine was cut after taping. Hicks' stand-up routine was removed from the show, Hicks said, because Letterman's producers believed the material, which included jokes involving religion and the anti-abortion movement, was unsuitable for broadcast. Producer Robert Morton initially blamed CBS, which denied responsibility; Morton later conceded it was his decision. Although Letterman later expressed regret at the way Hicks had been handled, Hicks did not appear on the show again.
Letterman finally aired the censored routine in its entirety on January 30, 2009. Hicks's mother, Mary, was present in the studio and appeared on-camera as a guest. Letterman took responsibility for the original decision to remove Hicks's set from the 1993 show. "It says more about me as a guy than it says about Bill," he said, after the set aired, "because there was absolutely nothing wrong with that."
Denis Leary.
For many years, Hicks was friends with fellow comedian Denis Leary. But in 1993, Hicks was angered by Leary's album "No Cure for Cancer", which featured lines and subject matter similar to Hicks's routine. According to "American Scream: The Bill Hicks Story" by Cynthia True, upon hearing the album "Bill was furious. All these years, aside from the occasional jibe, he had pretty much shrugged off Leary's lifting. Comedians borrowed, stole stuff, and even bought bits from one another. Milton Berle and Robin Williams were famous for it. This was different. Leary had practically taken line for line huge chunks of Bill's act and "recorded" it." The friendship ended abruptly as a result.
At least three stand-up comedians have gone on the record stating they believe Leary stole Hicks's material as well as his persona and attitude. In an interview, when Hicks was asked why he had quit smoking, he answered, "I just wanted to see if Denis would, too." In another interview, Hicks said, "I have a scoop for you. I stole his [Leary's] act. I camouflaged it with punchlines, and, to really throw people off, I did it before he did." During a 2003 Comedy Central Roast of Denis Leary, comedian Lenny Clarke, a friend of Leary's, said there was a carton of cigarettes backstage from Bill Hicks with the message, "Wish I had gotten these to you sooner." This joke was cut from the final broadcast.
The controversy surrounding plagiarism is also mentioned in "American Scream":
Leary was in Montreal hosting the "Nasty Show" at Club Soda, and Colleen [McGarr?] was coordinating the talent so she stood backstage and overheard Leary doing material incredibly similar to old Hicks riffs, including his perennial Jim Fixx joke: "Keith Richards outlived Jim Fixx, the runner and health nut. The plot thickens." When Leary came offstage, Colleen, more stunned than angry, said, "Hey, you know that's Bill Hicks's material! Do you know that's his material?" Leary stood there, stared at her without saying a word, and briskly left the dressing room.
Material and style.
Hicks's performance style was seen as a play on his audience's emotions. He expressed anger, disgust, and apathy while addressing the audience in a casual and personal manner, which he likened to merely conversing with his friends. He would invite his audiences to challenge authority and the existential nature of "accepted truth." One such message, which he often used in his shows, was delivered in the style of a news report (in order to draw attention to the negative slant news organizations give to any story about drugs):
Today, a young man on acid realized that all matter is merely energy condensed to a slow vibration—that we are all one consciousness experiencing itself subjectively. There is no such thing as death, life is only a dream, and we're the imagination of ourselves. Here's Tom with the weather.
The American philosopher and ethnomycologist Terence McKenna was a frequent source of Hicks' most controversial psychedelic and philosophical counter-cultural material; he infamously acted out an abridged version of McKenna's "Stoned Ape" model of human evolution as a routine during several of his final shows.
Another of Hicks's most quoted lines was delivered during a gig in Chicago in 1989 (later released as the bootleg "I'm Sorry, Folks"). After a heckler repeatedly shouted "Free Bird", Hicks screamed that "Hitler had the right idea; he was just an underachiever!" Hicks followed this remark with a misanthropic tirade calling for unbiased genocide against the whole of humanity.
Much of Hicks's routine involved direct attacks on mainstream society, religion, politics, and consumerism. Asked in a BBC interview why he cannot do a routine that appeals "to everyone", he said that such an act was impossible. He responded by repeating a comment that an audience member once made to him, "We don't come to comedy to think!", to which he replied, "Gee, where do you go to think? I'll meet you there!" When asked whether there was a "half way" point between audience expectations and his own, he said: "but my way "is" half-way between, I mean, this is a night-club, and, you know, these are adults, and what do you expect?"
Hicks often discussed popular conspiracy theories in his performances, most notably the assassination of President John F. Kennedy. He mocked the Warren Report and the official version of Lee Harvey Oswald as a "lone nut assassin." He also questioned the guilt of David Koresh and the Branch Davidian compound during the Waco Siege. Hicks would end some of his shows, especially those being recorded in front of larger audiences as albums, with a mock "assassination" of himself on stage, making gunshot sound effects into the microphone while falling to the ground.
Cancer diagnosis and death.
On June 16, 1993, Hicks was diagnosed with pancreatic cancer that had spread to his liver.
He started receiving weekly chemotherapy, while still touring and also recording his album, "Arizona Bay", with Kevin Booth. He was also working with comedian Fallon Woodland on a pilot episode of a new talk show, titled "Counts of the Netherworld" for Channel 4 at the time of his death. The budget and concept had been approved, and a pilot was filmed. The "Counts of the Netherworld" pilot was shown at the various Tenth Anniversary Tribute Night events around the world on February 26, 2004.
After being diagnosed with cancer, Hicks would often joke that any given performance would be his last. The public, however, was unaware of Hicks's condition. Only a few close friends and family members knew of his disease. Hicks performed the final show of his career at Caroline's in New York on January 6, 1994. He moved back to his parents' house in Little Rock, Arkansas, shortly thereafter. He called his friends to say goodbye before he stopped speaking on February 14. He died of pancreatic cancer on February 26, 1994 in Little Rock at the age of 32. Hicks was buried in the family plot in Magnolia Cemetery, Leakesville, Mississippi.
On February 7, 1994, a verse Hicks had authored, on his perspective, wishes, and thanks of his life, to be released after his death as his "last word", ended with the words: "I left in love, in laughter, and in truth and wherever truth, love and laughter abide, I am there in spirit."
Legacy.
"Arizona Bay" and "Rant in E-Minor" were released posthumously in 1997 on the Voices imprint of the Rykodisc label. "Dangerous" and "Relentless" were also re-released by Rykodisc on the same date.
In a 2005 poll to find The Comedian's Comedian, fellow comedians and comedy insiders voted Hicks No. 13 on their list of "The Top 20 Greatest Comedy Acts Ever". Likewise, in "Comedy Central Presents: 100 Greatest Stand-ups of All Time" (2004), Hicks was ranked at #19. In March 2007, Channel 4 ran a poll, "The Top 100 Stand-Up Comedians of All Time", in which Hicks was voted #6. Channel 4 renewed this list in April 2010, which saw Hicks move up 2 places to #4.
Devotees of Hicks have incorporated his words, image, and attitude into their own creations. By means of audio sampling, fragments of Hicks's rants, diatribes, social criticisms, and philosophies have found their way into many musical works, such as the live version of Super Furry Animals' "Man Don't Give A Fuck". His influence on the band Tool is well-documented as he is sampled at the beginning of their song Third Eye; he "appears" on the Fila Brazillia album "Maim That Tune" (1996) and on SPA's self-titled album "SPA" (1997), which are both dedicated to Hicks; the British band Radiohead's second album "The Bends" (1995) is also dedicated to his memory. Singer/songwriter Tom Waits listed "Rant in E-Minor" as one of his 20 most cherished albums of all time.
Comedians who have cited Hicks as an inspiration include Joe Rogan, Dave Attell, Lewis Black, Patton Oswalt David Cross, Russell Brand, and Ron White.
The British actor Chas Early portrayed Hicks in the one-man stage show "Bill Hicks: Slight Return", which premiered in 2004. The show was co-written by Chas Early and Richard Hurst, and imagined Hicks's view of the world 10 years after his death.
On February 25, 2004, British MP Stephen Pound tabled an early day motion titled "Anniversary of the Death of Bill Hicks" (EDM 678 of the 2003–04 session), the text of which reads:
That this House notes with sadness the 10th anniversary of the death of Bill Hicks, on 26th February 1994, at the age of 33 ["sic"]; recalls his assertion that his words would be a bullet in the heart of consumerism, capitalism and the American Dream; and mourns the passing of one of the few people who may be mentioned as being worth ["sic"] of inclusion with Lenny Bruce in any list of unflinching and painfully honest political philosophers.
Hicks appeared in a flashback scene in writer Garth Ennis's Vertigo comic-book series "Preacher", in the story "Underworld" in issue No. 31 (Nov. 1997).
Film and documentary.
Annex Houston (1986) was a video in an early stand up performance live at Texas.
Sane Man (1989) is the first officially video recorded Bill Hicks show.
Ninja Bachelor Party (1991) is a 1991 low-budget comedy film produced by and starring Bill Hicks, Kevin Booth, and David Johndrow. It is a parody of martial arts movies and was intentionally dubbed improperly.
One Night Stand (1991) is an HBO stand-up series that first aired on February 15, 1989. The half-hour series aired weekly and featured stand-up comedy specials from some of the top performing comedians. The series originally comprised 55 specials over the course of its four years on HBO.
Revelations (1992) is one more live performance that Bill Hicks performed at the Dominion Theatre, London in November 1992.
Relentless (1992) was recorded at the Centaur Theatre during the annual Just for Laughs Comedy Festival in Montreal, Canada. Despite the common title, the CD album was recorded at a separate performance, after the Just for Laughs festival had closed.
A documentary entitled ", based on interviews with his family and friends, premiered on March 12, 2010, at the South by Southwest Film Festival in Austin, Texas.
Russell Crowe announced in 2012 that he will direct a Bill Hicks biopic. Crowe was originally thought to be playing the comedian, but Mark Staufer, the actor's schoolmate and writer on the film, has suggested the part is now open for casting. Production was expected to start in 2013.
Further reading.
</dl>

</doc>
<doc id="46095" url="http://en.wikipedia.org/wiki?curid=46095" title="Russell's paradox">
Russell's paradox

In the foundations of mathematics, Russell's paradox (also known as Russell's antinomy), discovered by Bertrand Russell in 1901, showed that some attempted formalizations of the naive set theory created by Georg Cantor led to a contradiction. The same paradox had been discovered a year before by Ernst Zermelo but he did not publish the idea, which remained known only to Hilbert, Husserl and other members of the University of Göttingen.
According to naive set theory, any definable collection is a set. Let "R" be the set of all sets that are not members of themselves. If "R" is not a member of itself, then its definition dictates that it must contain itself, and if it contains itself, then it contradicts its own definition as the set of all sets that are not members of themselves. This contradiction is Russell's paradox. Symbolically:
In 1908, two ways of avoiding the paradox were proposed, Russell's type theory and the Zermelo set theory, the first constructed axiomatic set theory. Zermelo's axioms went well beyond Frege's axioms of extensionality and unlimited set abstraction, and evolved into the now-canonical Zermelo–Fraenkel set theory (ZFC). The essential difference between Russell's and Zermelo's solution to the paradox is that Zermelo altered the axioms of set theory while preserving the logical language in which they are expressed (the language of ZFC, with the help of Skolem, turned out to be first-order logic) while Russell altered the logical language itself.
Informal presentation.
Let us call a set "abnormal" if it is a member of itself, and "normal" otherwise. For example, take the set of all squares in the plane. That set is not itself a square in the plane, and therefore is not a member of the set of all squares in the plane. So it is "normal". On the other hand, if we take the complementary set that contains all non-(squares in the plane), that set is itself not a square in the plane and so should be one of its own members as it is a non-(square in the plane). It is "abnormal".
Now we consider the set of all normal sets, "R". Determining whether "R" is normal or abnormal is impossible: if "R" were a normal set, it would be contained in the set of normal sets (itself), and therefore be abnormal; and if "R" were abnormal, it would not be contained in the set of all normal sets (itself), and therefore be normal. This leads to the conclusion that "R" is neither normal nor abnormal: Russell's paradox.
Formal presentation.
Define Naive Set Theory (NST) as the theory of predicate logic with a binary predicate formula_2 and the following axiom schema of unrestricted comprehension:
for any formula "P" with only the variable "x" free.
Substitute formula_4 for formula_5. Then by existential instantiation (reusing the symbol "y") and universal instantiation we have
a contradiction. Therefore NST is inconsistent.
Set-theoretic responses.
In 1908, Ernst Zermelo proposed an axiomatization of set theory that avoided the paradoxes of naive set theory by replacing arbitrary set comprehension with weaker existence axioms, such as his axiom of separation ("Aussonderung"). Modifications to this axiomatic theory proposed in the 1920s by Abraham Fraenkel, Thoralf Skolem, and by Zermelo himself resulted in the axiomatic set theory called ZFC. This theory became widely accepted once Zermelo's axiom of choice ceased to be controversial, and ZFC has remained the canonical axiomatic set theory down to the present day.
ZFC does not assume that, for every property, there is a set of all things satisfying that property. Rather, it asserts that given any set "X", any subset of "X" definable using first-order logic exists. The object "R" discussed above cannot be constructed in this fashion, and is therefore not a ZFC set. In some extensions of ZFC, objects like "R" are called proper classes.
ZFC is silent about types, although the cumulative hierarchy has a notion of layers that resemble types. Zermelo himself never accepted Skolem's formulation of ZFC using the language of first-order logic. As José Ferreirós notes, Zermelo insisted instead that "propositional functions (conditions or predicates) used for separating off subsets, as well as the replacement functions, can be "entirely "arbitrary"" [ganz "beliebig"];" the modern interpretation given to this statement is that Zermelo wanted to include higher-order quantification in order to avoid Skolem's paradox. Around 1930, Zermelo also introduced (apparently independently of von Neumann), the axiom of foundation, thus—as Ferreirós observes— "by forbidding 'circular' and 'ungrounded' sets, it [ZFC] incorporated one of the crucial motivations of TT [type theory]—the principle of the types of arguments". This 2nd order ZFC preferred by Zermelo, including axiom of foundation, allowed a rich cumulative hierarchy. Ferreirós writes that "Zermelo's 'layers' are essentially the same as the types in the contemporary versions of simple TT [type theory] offered by Gödel and Tarski. One can describe the cumulative hierarchy into which Zermelo developed his models as the universe of a cumulative TT in which transfinite types are allowed. (Once we have adopted an impredicative standpoint, abandoning the idea that classes are constructed, it is not unnatural to accept transfinite types.) Thus, simple TT and ZFC could now be regarded as systems that 'talk' essentially about the same intended objects. The main difference is that TT relies on a strong higher-order logic, while Zermelo employed second-order logic, and ZFC can also be given a first-order formulation. The first-order 'description' of the cumulative hierarchy is much weaker, as is shown by the existence of denumerable models (Skolem paradox), but it enjoys some important advantages."
In ZFC, given a set "A", it is possible to define a set "B" that consists of exactly the sets in "A" that are not members of themselves. "B" cannot be in "A" by the same reasoning in Russell's Paradox. This variation of Russell's paradox shows that no set contains everything.
Through the work of Zermelo and others, especially John von Neumann, the structure of what some see as the "natural" objects described by ZFC eventually became clear; they are the elements of the von Neumann universe, "V", built up from the empty set by transfinitely iterating the power set operation. It is thus now possible again to reason about sets in a non-axiomatic fashion without running afoul of Russell's paradox, namely by reasoning about the elements of "V". Whether it is "appropriate" to think of sets in this way is a point of contention among the rival points of view on the philosophy of mathematics.
Other resolutions to Russell's paradox, more in the spirit of type theory, include the axiomatic set theories New Foundations and Scott-Potter set theory.
History.
Russell discovered the paradox in May or June 1901. By his own account in his 1919 "Introduction to Mathematical Philosophy", he "attempted to discover some flaw in Cantor's proof that there is no greatest cardinal". In a 1902 letter, he announced the discovery to Gottlob Frege of the paradox in Frege's 1879 "Begriffsschrift" and framed the problem in terms of both logic and set theory, and in particular in terms of Frege's definition of function:
There is just one point where I have encountered a difficulty. You state (p. 17 [p. 23 above]) that a function too, can act as the indeterminate element. This I formerly believed, but now this view seems doubtful to me because of the following contradiction. Let w be the predicate: to be a predicate that cannot be predicated of itself. Can w be predicated of itself? From each answer its opposite follows. Therefore we must conclude that w is not a predicate. Likewise there is no class (as a totality) of those classes which, each taken as a totality, do not belong to themselves. From this I conclude that under certain circumstances a definable collection [Menge] does not form a totality.
Russell would go on to cover it at length in his 1903 "The Principles of Mathematics", where he repeated his first encounter with the paradox:
Before taking leave of fundamental questions, it is necessary to examine more in detail the singular contradiction, already mentioned, with regard to predicates not predicable of themselves. ... I may mention that I was led to it in the endeavour to reconcile Cantor's proof..."
Russell wrote to Frege about the paradox just as Frege was preparing the second volume of his "Grundgesetze der Arithmetik". Frege responded to Russell very quickly; his letter dated 22 June 1902 appeared, with van Heijenoort's commentary in Heijenoort 1967:126–127. Frege then wrote an appendix admitting to the paradox, and proposed a solution that Russell would endorse in his "Principles of Mathematics", but was later considered by some to be unsatisfactory. For his part, Russell had his work at the printers and he added an appendix on the doctrine of types.
Ernst Zermelo in his (1908) "A new proof of the possibility of a well-ordering" (published at the same time he published "the first axiomatic set theory") laid claim to prior discovery of the antinomy in Cantor's naive set theory. He states: "And yet, even the elementary form that Russell9 gave to the set-theoretic antinomies could have persuaded them [J. König, Jourdain, F. Bernstein] that the solution of these difficulties is not to be sought in the surrender of well-ordering but only in a suitable restriction of the notion of set". Footnote 9 is where he stakes his claim:
9"1903", pp. 366–368. I had, however, discovered this antinomy myself, independently of Russell, and had communicated it prior to 1903 to Professor Hilbert among others".
Frege sent a copy of his "Grundgesetze der Arithmetik" to Hilbert; as noted above, Frege's last volume mentioned the paradox that Russel had communicated to Frege. After receiving Frege's last volume, on 7 November 1903, Hilbert wrote a letter to Frege in which he said, referring to Russel's paradox, "I believe Dr. Zermelo discovered it three or four years ago". A written account of Zermelo's actual argument was discovered in the "Nachlass" of Edmund Husserl.
In 1923, Ludwig Wittgenstein proposed to "dispose" of Russell's paradox as follows:
The reason why a function cannot be its own argument is that the sign for a function already contains the prototype of its argument, and it
cannot contain itself. For let us suppose that the function F(fx) could be its own argument: in that case there would be a proposition 'F(F(fx))', in which the outer function F and the inner function F must have different meanings, since the inner one has the form O(f(x)) and the outer one has the form Y(O(fx)). Only the letter 'F' is common to the two functions, but the letter by itself signifies nothing. This immediately becomes clear if instead of 'F(Fu)' we write '(do) : F(Ou) . Ou = Fu'. That disposes of Russell's paradox. ("Tractatus Logico-Philosophicus", 3.333)
Russell and Alfred North Whitehead wrote their three-volume "Principia Mathematica" hoping to achieve what Frege had been unable to do. They sought to banish the paradoxes of naive set theory by employing a theory of types they devised for this purpose. While they succeeded in grounding arithmetic in a fashion, it is not at all evident that they did so by purely logical means. While "Principia Mathematica" avoided the known paradoxes and allows the derivation of a great deal of mathematics, its system gave rise to new problems.
In any event, Kurt Gödel in 1930–31 proved that while the logic of much of "Principia Mathematica", now known as first-order logic, is complete, Peano arithmetic is necessarily incomplete if it is consistent. This is very widely—though not universally—regarded as having shown the logicist program of Frege to be impossible to complete.
In 2001 A Centenary International Conference celebrating the first hundred years of Russell's paradox was held in Munich and its proceedings have been published.
Applied versions.
There are some versions of this paradox that are closer to real-life situations and may be easier to understand for non-logicians. For example, the barber paradox supposes a barber who shaves all men who do not shave themselves and only men who do not shave themselves. When one thinks about whether the barber should shave himself or not, the paradox begins to emerge.
As another example, consider five lists of encyclopedia entries within the same encyclopedia:
If the "List of all lists that do not contain themselves" contains itself, then it does not belong to itself and should be removed. However, if it does not list itself, then it should be added to itself.
While appealing, these layman's versions of the paradox share a drawback: an easy refutation of the barber paradox seems to be that such a barber does not exist, or at least does not shave (a variant of which is that the barber is a woman). The whole point of Russell's paradox is that the answer "such a set does not exist" means the definition of the notion of set within a given theory is unsatisfactory. Note the difference between the statements "such a set does not exist" and "it is an empty set". It is like the difference between saying, "There is no bucket", and saying, "The bucket is empty".
A notable exception to the above may be the Grelling–Nelson paradox, in which words and meaning are the elements of the scenario rather than people and hair-cutting. Though it is easy to refute the barber's paradox by saying that such a barber does not (and "cannot") exist, it is impossible to say something similar about a meaningfully defined word.
One way that the paradox has been dramatised is as follows:
Applications and related topics.
Russell-like paradoxes.
As illustrated above for the Barber paradox, Russell's paradox is not hard to extend. Take:
Form the sentence:
Sometimes the "all" is replaced by "all <V>ers".
An example would be "paint":
or "elect"
Paradoxes that fall in this scheme include:
References.
</dl>

</doc>
<doc id="46096" url="http://en.wikipedia.org/wiki?curid=46096" title="Simpson's paradox">
Simpson's paradox

Simpson's paradox, or the Yule–Simpson effect, is a paradox in probability and statistics, in which a trend that appears in different groups of data disappears or reverses when these groups are combined. It is sometimes given the impersonal title reversal paradox or amalgamation paradox.
This result is often encountered in social-science and medical-science statistics, and is particularly confounding when frequency
data are unduly given causal interpretations. Simpson's paradox disappears when causal relations are brought into consideration. Many statisticians believe that the mainstream public should be informed of the counter-intuitive results in statistics such as Simpson's paradox.
Edward H. Simpson first described this phenomenon in a technical paper in 1951,
but the statisticians Karl Pearson, et al., in 1899,
and Udny Yule, in 1903, had mentioned similar effects earlier.
The name "Simpson's paradox" was introduced by Colin R. Blyth in 1972.
Examples.
Berkeley gender bias case.
One of the best-known real-life examples of Simpson's paradox occurred when the University of California, Berkeley was sued for bias against women who had applied for admission to graduate schools there. The admission figures for the fall of 1973 showed that men applying were more likely than women to be admitted, and the difference was so large that it was unlikely to be due to chance.
But when examining the individual departments, it appeared that no department was significantly biased against women. In fact, most departments had a "small but statistically significant bias in favor of women." The data from the six largest departments are listed below.
The research paper by Bickel et al. concluded that women tended to apply to competitive departments with low rates of admission even among qualified applicants (such as in the English Department), whereas men tended to apply to less-competitive departments with high rates of admission among the qualified applicants (such as in engineering and chemistry). The conditions under which the admissions' frequency data from specific departments constitute a proper defense against charges of
discrimination are formulated in the book "Causality" by Pearl.
Another example, we investigate whether the annual accident rate on the roads is related to average traffic speed. We take 3 fast roads in rural areas (roads A, B, and C), and three slower urban roads (roads D, E, and F). The data is as follows. Road A, average traffic speed 54 mph, 2.8 accidents a year; road B, average speed 56 mph, 3.0 accidents a year, road C, 58 mph, 3.2 accidents a year. For the slower and busier urban roads we have, road D, average speed 26 mph, 5.4 accidents a year, road E, average speed 28 mph, 5.6 accidents a year, and road F, average speed 30 mph, 5.8 accidents a year. Taken as a whole the data shows a negative correlation of speed and accidents; split into urban and rural data we get a positive association of accidents with speed.
Kidney stone treatment.
This is a real-life example from a medical study comparing the success rates of two treatments for kidney stones.
The table below shows the success rates and numbers of treatments for treatments involving both small and large kidney stones, where Treatment A includes all open surgical procedures and Treatment B is percutaneous nephrolithotomy (which involves only a small puncture). The numbers in parentheses indicate the number of success cases over the total size of the group. (For example, 93% equals 81 divided by 87.)
The paradoxical conclusion is that treatment A is more effective when used on small stones, and also when used on large stones, yet treatment B is more effective when considering both sizes at the same time. In this example the "lurking" variable (or confounding variable) of the stone size was not previously known to be important until its effects were included.
Which treatment is considered better is determined by an inequality between two ratios (successes/total). The reversal of the inequality between the ratios, which creates Simpson's paradox, happens because two effects occur together:
Based on these effects, the paradoxical result is seen to arise by suppression of the causal effect of stone size on successful treatment. The paradoxical result can be rephrased more accurately as follows: When the less effective treatment is applied more frequently to easier cases, it can appear to be a more effective treatment.
Low birth weight paradox.
The low birth weight paradox is an apparently paradoxical observation relating to the birth weights and mortality of children born to tobacco smoking mothers. As a usual practice, babies weighing less than a certain amount (which varies between different countries) have been classified as having low birth weight. In a given population, babies with low birth weights have had a significantly higher infant mortality rate than others. Normal birth weight infants of smokers have about the same mortality rate as normal birth weight infants of non-smokers, and low birth weight infants of smokers have a much lower mortality rate than low birth weight infants of non-smokers, but infants of smokers overall have a much higher mortality rate than infants of non-smokers. This is because many more infants of smokers are low birth weight, and low birth weight babies have a much higher mortality rate than normal birth weight babies.
Batting averages.
A common example of Simpson's Paradox involves the batting averages of players in professional baseball. It is possible for one player to hit for a higher batting average than another player during a given year, and to do so again during the next year, but to have a lower batting average when the two years are combined. This phenomenon can occur when there are large differences in the number of at-bats between the years. (The same situation applies to calculating batting averages for the first half of the baseball season, and during the second half, and then combining all of the data for the season's batting average.)
A real-life example is provided by Ken Ross and involves the batting average of two baseball players, Derek Jeter and David Justice, during the years 1995 and 1996:
In both 1995 and 1996, Justice had a higher batting average (in bold type) than Jeter did. However, when the two baseball seasons are combined, Jeter shows a higher batting average than Justice. According to Ross, this phenomenon would be observed about once per year among the possible pairs of interesting baseball players. In this particular case, the Simpson's Paradox can still be observed if the year 1997 is also taken into account:
The Jeter and Justice example of Simpson's paradox was referred to in the "Conspiracy Theory" episode of the television series "Numb3rs", though a chart shown omitted some of the data, and listed the 1996 averages as 1995.
Correlation between variables.
Simpson’s paradox can also arise in correlations, in which two variables appear to have (say) a positive correlation towards one another, when in fact they have a negative correlation, the reversal having been brought about by a “lurking” confounder. Berman et al. give an example from economics, where a dataset suggests overall demand is positively correlated with price (that is, higher prices lead to "more" demand), in contradiction of expectation. Analysis reveals time to be the confounding variable: plotting both price and demand against time reveals the expected negative correlation over various periods, which then reverses to become positive if the influence of time is ignored by simply plotting demand against price.
Description.
Suppose two people, Lisa and Bart, each edit document articles for two weeks. In the first week, Lisa improves 0 of the 3 articles she edited, and Bart improves 1 of the 7 articles he edited. In the second week, Lisa improves 5 of 7 articles she edited, while Bart improves all 3 of the articles he edited.
Both times Bart improved a higher percentage of articles than Lisa, but the actual number of articles each edited (the bottom number of their ratios, also known as the "sample size") were not the same for both of them either week. When the totals for the two weeks are added together, Bart and Lisa's work can be judged from an equal sample size, i.e. the same number of articles edited by each. Looked at in this more accurate manner, Lisa's ratio is higher and, therefore, so is her percentage. Also when the two tests are combined using a weighted average, overall, Lisa has improved a much higher percentage than Bart because the quality modifier had a significantly higher percentage. Therefore, like other paradoxes, it only appears to be a paradox because of incorrect assumptions, incomplete or misguided information, or a lack of understanding a particular concept.
This imagined paradox is caused when the percentage is provided but not the ratio. In this example, if only the 14.2% in the first week for Bart was provided but not the ratio (1:7), it would distort the information and so cause the imagined paradox. Even though Bart's percentage is higher for the first and second week, when two weeks of articles is combined, overall Lisa had improved a greater proportion, 50% of the 10 total articles. Lisa's proportional total of articles improved exceeds Bart's total.
Here are some notations:
On both occasions Bart's edits were more successful than Lisa's. But if we combine the two sets, we see that Lisa and Bart both edited 10 articles, and:
Bart is better for each set but worse overall.
The paradox stems from the intuition that Bart could not possibly be a better editor on each set but worse overall. Pearl proved how this is possible, when "better editor" is taken in the counterfactual sense: "Were Bart to edit all items in a set he would do better than Lisa would, on those same items". Clearly, frequency data cannot support this sense of "better editor," because it does not tell us how Bart would perform on items edited by Lisa, and vice versa. In the back of our mind, though, we assume that the articles were assigned at random to Bart and Lisa, an assumption which (for a large sample) would support the counterfactual interpretation of "better editor." However, under random assignment conditions, the data given in this example are unlikely, which accounts for our surprise when confronting the rate reversal.
The arithmetical basis of the paradox is uncontroversial. If formula_8 and formula_9 we feel that formula_10 "must be greater" than formula_11. However if "different" weights are used to form the overall score for each person then this feeling may be disappointed. Here the first test is weighted formula_12 for Lisa and formula_13 for Bart while the weights are reversed on the second test.
Lisa is a better editor on average, as her overall success rate is higher. But it is possible to have told the story in a way which would make it appear obvious that Bart is more diligent.
Simpson's paradox shows us an extreme example of the importance of including data about possible confounding variables when attempting to calculate causal relations. Precise criteria for selecting a set of "confounding variables,"
(i.e., variables that yield correct causal relationships if included in the analysis),
is given in Pearl using causal graphs.
While Simpson's paradox often refers to the analysis of count tables, as shown in this example, it also occurs with continuous data: for example, if one fits separated regression lines through two sets of data, the two regression lines may show a positive trend, while a regression line fitted through all data together will show a "negative" trend, as shown on the first picture.
Vector interpretation.
Simpson's paradox can also be illustrated using the 2-dimensional vector space. A success rate of formula_16 can be represented by a vector formula_17, with a slope of formula_16. If two rates formula_19 and formula_20 are combined, as in the examples given above, the result can be represented by the sum of the vectors formula_21 and formula_22, which according to the parallelogram rule is the vector formula_23, with slope formula_24.
Simpson's paradox says that even if a vector formula_25 (in blue in the figure) has a smaller slope than another vector formula_26 (in red), and formula_27 has a smaller slope than formula_28, the sum of the two vectors formula_29 (indicated by "+" in the figure) can still have a larger slope than the sum of the two vectors formula_30, as shown in the example.
Implications for decision making.
The practical significance of Simpson's paradox surfaces in decision making situations where it poses the following dilemma: Which data should we consult in choosing an action, the aggregated or the partitioned? In the Kidney Stone example above, it is clear that if one is diagnosed with "Small Stones" or "Large Stones" the data for the respective subpopulation should be consulted and Treatment A would be preferred to Treatment B. But what if a patient is not diagnosed, and the size of the stone is not known; would it be appropriate to consult the aggregated data and administer Treatment B? This would stand contrary to common sense; a treatment that is preferred both under one condition and under its negation should also be preferred when the condition is unknown.
On the other hand, if the partitioned data is to be preferred a priori, what prevents one from partitioning the data into arbitrary sub-categories (say based on eye color or post-treatment pain) artificially constructed to yield wrong choices of treatments? Pearl shows that, indeed, in many cases it is the aggregated, not the partitioned data that gives the correct choice of action. Worse yet, given the same table, one should sometimes follow the partitioned and sometimes the aggregated data, depending on the story behind the data; with each story dictating its own choice. Pearl considers this to be the real paradox behind Simpson's reversal.
As to why and how a story, not data, should dictate choices, the answer is that it is the story which encodes the causal relationships among the variables. Once we extract these relationships and represent them in a graph called a causal Bayesian network we can test algorithmically whether a given partition, representing confounding variables, gives the correct answer. The test, called "back-door," requires that we check whether the nodes corresponding to the confounding variables intercept certain paths in the graph. This reduces Simpson's Paradox to an exercise in graph theory.
Psychology.
Psychological interest in Simpson's paradox seeks to explain why people deem sign reversal to be impossible at first. The question is where people get this strong intuition from, and how it is encoded in the mind. Simpson's paradox demonstrates that this intuition cannot be supported by probability calculus alone, and thus led philosophers to speculate that it is supported by an innate causal logic that guides people in reasoning about actions and their consequences. Savage's sure-thing principle is an example of what such logic may entail. A qualified version of Savage's sure thing principle can indeed be derived from Pearl's "do"-calculus and reads: "An action "A" that increases the probability of an event "B" in each subpopulation "Ci" of "C" must also increase the probability of "B" in the population as a whole, provided that the action does not change the distribution of the subpopulations." This suggests that knowledge about actions and consequences is stored in a form resembling Causal Bayesian Networks.
Probability.
A study by Pavlides and Perlman suggests that in a randomly selected 2 × 2 × 2 table, Simpson's paradox will occur with a probability of approximately 1/60. A study by Kock suggests that the probability that Simpson’s paradox would occur at random in path models with two predictors and one criterion variable is approximately 12.8 percent; slightly higher than 1 occurrence per 8 path models.

</doc>
<doc id="46097" url="http://en.wikipedia.org/wiki?curid=46097" title="Green party">
Green party

A Green party is a formally organized political party based on the principles of green politics, such as social justice and nonviolence. Greens believe that these issues are inherently related to one another as a foundation for world peace. Green party platforms typically embrace social-democratic economic policies and forming coalitions with leftists. Green parties exist in nearly 90 countries around the world; many are members of Global Greens.
Definitions.
There are distinctions between "green" parties and "Green" parties. Any party, faction, or politician may be labeled "green" if it emphasizes environmental causes. Indeed, the term may even be used as a verb: it is not uncommon to hear of "greening" a party or a candidate.
In contrast, formally organized Green parties may follow a coherent ideology that includes not only environmentalism, but often also other concerns such as social justice, consensus decision-making, and nonviolence. Greens believe that these issues are inherently related to one another as a foundation for world peace. The best-known statement of the above Green values is the Four Pillars of the Green Party, adopted by the German Greens in 1979–1980 (but forsaken since). The Global Greens Charter lists six guiding principles which are ecological wisdom, social justice, participatory democracy, nonviolence, sustainability and respect for diversity.
Influence.
The world's first political parties to campaign on a predominantly environmental platform were the United Tasmania Group which contested the April 1972 state election in Tasmania, Australia and the Values Party of New Zealand, which contested the November 1972 New Zealand general election. The name 'Green' derives from the 'Green Bans': an Australian movement of building workers who refused to build on sites of cultural and environmental significance. The first green party in Europe was the Popular Movement for the Environment, founded in 1972 in the Swiss canton of Neuchâtel. The first national green party in Europe was PEOPLE, founded in Britain in February 1973, which eventually turned into the Ecology Party, and then the Green Party. Several other local political groups were founded in beginning of the 70's and Fons Sprangers was probably the first Green mayor in the world, elected in 1970 in Meer, and active till 2006 for the Flemish Greens. The first political party to use the name "Green" seems to have been the Lower Saxon "", founded Sept. 1, 1977.
The first Green Party to achieve national prominence was the German Green Party, famous for their opposition to nuclear power, as well as an expression of anti-centralist and pacifist values traditional to greens. They were founded in 1980 and have been in coalition governments at state level for some years. They were in federal government with the Social Democratic Party of Germany in a so-called Red-Green alliance from 1998 to 2005. In 2001, they reached an agreement to end reliance on nuclear power in Germany, and agreed to remain in coalition and support the government of Chancellor Gerhard Schröder in the 2001 Afghan War. This put them at odds with many Greens worldwide.
In Finland, in 1995, the Finnish Green Party was the first European Green party to be part of a national Cabinet. Other Green parties that have participated in government at national level include the Groen! (formerly Agalev) and Ecolo in Belgium, Les Verts in France and the Green Party in Ireland. In the Netherlands GroenLinks ("GreenLeft") was founded in 1990 from four small left-wing parties and is now a stable faction in the Dutch parliament.
Around the world, individuals have formed many Green parties over the last thirty years. Green parties now exist in most countries with democratic systems: from Canada to Peru; from Norway to South Africa; from Ireland to Mongolia. There is Green representation at national, regional and local levels in many countries around the world.
Most of the Green parties are formed to win elections, and so organize themselves by the presented electoral or political districts. But that does not apply universally: The Green Party of Alaska is organized along bioregional lines to practice bioregional democracy.
Alliances.
Depending on local conditions or issues, platforms and alliances may vary. In line with the goal of democracy, neighboring ecoregions may require different policies or protections.
Green parties are often formed in a given jurisdiction by a coalition of scientific ecologists, community environmentalists, and local (or national) leftist groups or groups concerned with peace or citizens rights.
A Red-Green alliance is an alliance between Green parties and social democratic parties. Such alliances are typically formed for the purpose of elections (mostly in first-past-the-post election systems), or, after elections, for the purpose of forming a government.
Some Greens, such as those in Hawaii, find more effective alliances with more conservative groups (Blue-Green alliance) or indigenous peoples - who seek to prevent disruption of traditional ways of life or to save ecological resources they depend on. Although Greens find much to support in fostering these types of alliances with groups of historically different backgrounds, they also feel strongly about forming diverse communities through encouragement of diversity in social and economic demographics in communities, especially in the United States.
Alliances often highlight strategic differences between participating in parties and advancing the values of the Green movement. For example, Greens became allied with centre-right parties to oust the centre-left ruling Institutional Revolutionary Party of Mexico. Ralph Nader, the 2000 presidential nominee of the U.S. Greens, campaigned with ultra-conservative Pat Buchanan on joint issues such as farm policy and bans on corporate funding of election campaigns, although this "alliance" between Nader and Buchanan was very specifically limited to the purpose of showing that there was broad support for certain specific issues, across the political spectrum.
U.S. Greens grew dramatically throughout 2001. However, stable coalitions (such as that in Germany) tend to be formed between elections with left-wing parties on social issues, and 'the grassroots right' on such issues as irresponsible corporate subsidies and public ethics.
On 13 June 2007, the Irish Green Party, represented by six members of parliament or TDs, agreed to go into a coalition government for the first time in their history, with Fianna Fáil. Fianna Fáil and the Green Party were joined by the Progressive Democrats in a coalition also supported by a number of independent members of parliament (the Progressive Democrats later dissolved as a party, though their members remained in parliament). The Green Party held two Cabinet seats, as well as two junior ministries, until their withdrawal from government in January 2011. They lost all of their six seats in the following month's general election.
In the Czech Republic, the Green Party was part of the governing coalition, together with the conservative Civic Democratic Party (ODS) and the Christian Democrats (KDU–ČSL) from January 2007 until the government collapsed in March 2009. The Finnish and French Green parties are now the only ones actively participating in government.
Green parties.
Europe.
Belgian and German roots.
The first green parties in Europe were founded in the late 1970s, following the rise of environmental awareness and the development of new social movements. Green parties in Belgium first made a breakthrough. Belgium had Green members of parliament elected first in the 1970s, and with seats on the local council, held the balance of power in the city of Liege, so were the first to go into coalition with the ruling party on that council. In 1979 political campaigns and dissident groups feeling underrepresented in west German politics formed a coalition to contest the 1979 elections to the European Parliament. Although they did not win any seats, the groups in this association formally agreed to become a party and won a breakthrough in the German national elections of 1983. They were not the first Green Party in Europe to have members elected nationally but the impression was created that they had been, because they attracted the most media attention. This was partly due to their charismatic leader Petra Kelly, a German who was of interest to the American media because she had an American step-father. Since its foundation in 1980 and merger with Alliance 90 after the German reunification, Kelly's party, now named Alliance '90/The Greens, has become one of Europe's most important Green parties. It played an important role in the formation of national-level Green parties in other countries such as Spain as well. The forerunner of the Green Party in the United Kingdom was the PEOPLE Party, formed in Coventry in 1972. It changed its name to the Ecology Party in 1973 and the Green Party in 1985.
1984–1989: A new political force.
In 1984 Greens agreed a common platform for the European Parliament Elections and the first Green Members of the European Parliament were elected here. Germany, a stronghold of the Green movement, elected seven MEPs; two more came from Belgium and two from the Netherlands. As those nine MEPs did not entitle the Greens to form a parliamentary group on their own, they concluded an alliance with MEPs from Italy, Denmark, and regionalists from Flanders and Ireland to form the GRAEL (Green Alternative European Link) group, also known as the Rainbow Group. Politically they engaged in the fight against environmental pollution, nuclear energy (1986 saw the Chernobyl disaster), the promotion of animal protection and the campaign against the demolition of Brussels by speculation fuelled by the presence of the European institutions.
From the 1990s until today.
After years of co-operation between the national Green parties they formed a pan-European alliance that unites most European Green parties. The Greens are a party within the European parliament with 46 seats, as of June 2009. It has a long-standing alliance with the European Free Alliance (EFA), an alliance of "stateless nations", such as the Welsh nationalist Plaid Cymru and Scottish National Party. Together European Green Party/EFA have 58 seats and they are the fourth largest party in the European Parliament.
While on many issues European Greens practice the same policies, one issue divides European Green parties: the European Union. Some Green parties, like the Dutch GreenLeft, the Swiss Green Party, the Irish Green Party and the German Alliance '90/The Greens, are pro-European, the Green parties in Sweden and England and Wales are moderately eurosceptic.
Some Green parties have been part of governing coalitions. The first one was the Finnish Green League that entered government in 1995. The Italian Federation of the Greens, the French Greens, the German Alliance '90/The Greens and both Belgian Green parties, the French-speaking Ecolo and the Dutch-speaking Agalev were part of government during the late 1990s. Most successful was the Latvian Green Party, who supplied the Prime Minister of Latvia in 2004. The Swedish Green party was a long term supporter of the social-democratic minority government until the election 2006 when the social-democratic party lost. The Irish Green Party were in government from 2007 until January 2011 when the party withdrew their support for the ruling coalition. During their period in office, the Irish Green Party held two Cabinet portfolios including Minister for the Environment, Heritage and Local Government and Minister for Communications, Energy and Natural Resources.
In Scandinavia, left-wing socialist parties have formed the Nordic Green Left Alliance. These parties have the same ideals as European Greens. However, they do not cooperate with the Global Greens or European Greens, but instead form a combined parliamentary group with the Party of the European Left, which unites communists and post-communists. There is one exception, in 2004 the MEP for Danish Socialist People's Party has left the Nordic Green Left parliamentary group and has joined the Green parliamentary group in the European parliament. The Socialist People's Party is currently an observer at the European Green Party and the Global Greens. Outside of Scandinavia, in 2004, Latvia became the first country in the world to have a Green politician become Head of Government, but in 2006 the Green Party received only 16.71 percent of the vote. In the Estonia 2007 parliamentary elections, the Estonian Greens won 7.1 percent of the vote, and a mandate for six seats in the country's parliament, the Riigikogu.
In some countries Greens have found it difficult to win any representation in the national parliament. Three reasons can be found for this. It includes countries with a first past the post electoral system, such as the United Kingdom. However, despite the first past the post system in the United Kingdom, the Green Party of England and Wales won their first seat in the House of Commons when Caroline Lucas won the seat of Brighton Pavilion. The Scottish Green Party has had success in the devolved Scottish Parliament and the Irish Green Party in Northern Ireland has had success in the devolved Northern Ireland Assembly, where the first-past-the-post system is not used. In countries where a party with similar ideals is stronger, such as Norway and Denmark, Green parties tend to perform worse. In some Eastern European countries, like Romania and Poland, Green parties are still in the process of formation and have therefore not gained enough support. The Green Party of Bulgaria is a part of the left-wing Coalition for Bulgaria, currently in opposition. It has no parliamentary representation but it did supply one Deputy Minister in the government of Prime Minister Sergey Stanishev from 2005 to 2009.
The European Green Party has worked to support weak Green parties in European countries. Until recently, they were giving support to Green parties in the Mediterranean countries. These Green parties are now making electoral gains, e.g. in Spain, Greece and Republic of Cyprus, or getting organized to do so, e.g. in Malta. Therefore the EGP is now turning its attention to Eastern Europe – all these countries have Green parties, but in materially poor Eastern Europe the success of Green Parties is very patchy, except for Hungary, where the local Green party, Politics Can Be Different (LMP), has succeeded in getting into the parliament and many city councils.
Asia and Oceania.
Green parties have achieved national or state parliamentary representation in New Zealand, Australia and Vanuatu. In New Zealand the Green Party of New Zealand currently holds 14 seats in the New Zealand House of Representatives after the 2011 general election. The Australian Greens hold 10 seats in the Australian Senate and one seat in the Australian House of Representatives. They also have representation in the upper and lower houses of state parliaments of five states and in the unicameral chamber of one territory. Greens also hold representative positions in local government across New Zealand and Australia (where a number of local government authorities are controlled by Green councilors). The Greens took the seat of Melbourne from the Australian Labor Party in 2010 with candidate Adam Bandt. This is the first time the Greens have won a Lower House seat at a general election (although they have previously won two seats at by-elections).
Proportional representation has strengthened the position of the Australian Greens and the Green Party of New Zealand and enabled them to participate directly in legislatures and policy-making committees. In countries following British-style 'first past the post' electoral rules, Green parties face barriers to gaining federal or provincial/regional/state seats. The Australian Labor Party's practice of allocating a portion of ALP ticket votes to Australian Greens has helped bring AG candidates into parliament.
In the 2008 ACT election in Australia, the Greens won 15.6% of the vote, winning 4 out of 17 seats. Shane Rattenbury was elected the speaker of the assembly, the first time a Green party member had held such a position in any parliament or assembly in Australia. However, they retained only one seat at the 2012 election in the same territory.
The Green Confederation ("Confédération Verte") in Vanuatu won 3 out of 52 seats in the last general election in October 2012. Its most prominent member is Moana Carcasses Kalosil, who became Prime Minister in March 2013. Carcasses, a Green liberal, does not lead a Green government, but a broad coalition government in which he is the only Green minister.
There is a Papua New Guinea Greens Party, but it does not have any members in Parliament. There was briefly a Green Party in Fiji from 2008 to 2013; as Parliament was suspended by the military regime during this time, the party was not able to take part in any election before all parties were deregistered in 2013.
North America.
As of the May 2011 there was only one Federal elected member of the Green Party in Canada, Elizabeth May, and there remains no representation in the United States. Accordingly, in these countries, Green parties focus on electoral reform. In Mexico, however, the "Partido Verde Ecologista" has 17 deputies and four senators in Congress as a result of the 2006 elections.
In Canada, the strongest provincial Green parties are the Green Party of British Columbia and the Green Party of Ontario; although in Ontario they have yet to win a seat in provincial election. As of May 2013, the first ever BC elected green member was voted in, and in 2014 a green MLA was elected to the New Brunswick legislature. Federally, the Green Party of Canada received 3.91% of the popular vote in the 2011 federal election and its support and influence continues to rise. Support waned slightly in the 2011 federal election when it captured 3.91% of the popular vote, down from the 6.8% captured in the 2008 federal election. Although Ms. May was the first elected Member of Parliament, the first seat was gained in the Canadian House of Commons on August 30, 2008 when sitting Independent MP Blair Wilson joined the party. May continues to sit as a Member of Parliament for Saanich-Gulf Islands and has exerted a considerable amount of legislative influence on a number of issues ranging from denying unanimous consent for military intervention in Libya, playing a central role in exposing changes to environmental legislation hidden in the omnibus Budget Bill C-38, to introducing a private member's bill to develop a national strategy on Lyme disease.
In the 2008 Vancouver municipal election, Stuart Mackinnon, a member of the Vancouver Green Party, was elected to the Vancouver Parks Board. Since that time former Green Party of British Columbia leader, and deputy-leader of the federal Green party, Adriane Carr won the Greens' first seat on Vancouver City Council, in 2011 municipal elections. "See also: Provincial parties in Canada & List of Green party leaders in Canada."
In the United States, Greens first ran for public office in 1985. Since then the Green Party of the U.S. has claimed electoral victories at the municipal, county and state levels. The first U.S. Greens to be elected were David Conley and Frank Koehn in Wisconsin 1986. Each was elected to a position on the County Board of Supervisors in Douglas and Bayfield counties respectively. Keiko Bonk was first elected in 1992 in Hawaii County, becoming Official Chairwoman in 1995. The first Green Party mayor was Kelly Weaverling, elected in Cordova, AK in 1991. Approximately 160 Greens hold elected office across the US as of summer 2009. The first U.S. Green elected to a state legislature was Audie Bock in 1999, to the California State Assembly, followed by John Eder to the Maine House of Representatives in 2002 and 2004 and Richard Carroll to the Arkansas House of Representatives in 2008. While in office in 2003 in the New Jersey General Assembly, incumbent Matt Ahearn made a party switch to Green for the remainder of his term. The Green Party has contested five U.S. presidential elections: in 1996 and 2000 with Ralph Nader for President and Winona LaDuke as Vice President, in 2004 with David Cobb for President and Pat LaMarche for Vice President, and in 2008 with Cynthia McKinney for President and Rosa Clemente for Vice President. In 2012 the Green Party nominated Jill Stein for President and Cheri Honkala for Vice President.
Developing world.
Green parties in the developing world are often organized with help from those in other nations. As of 2002 the foundation of Green parties has been the most notable in Africa.
Other than hosting the first Afghanistan peace conference as part of the German government, Green parties in the developed world have made few concrete moves to spread their values using the diplomatic channels. This is usually seen as one of the responsibilities of the Green movement - allowing parties to concentrate on their voters. However, the leader of the Kenyan Green Party, Wangari Maathai, won the Nobel Peace Prize in 2004, enhancing the image of Green parties across the third world.
In the greater Middle East region, a few Green parties have been created, such as the Green Party of Pakistan since 2002 and Green Party of India since 2010, and the Green Party of Saudi Arabia, but many of these Green parties are underground organizations.
Brazil.
Marina Silva's Green Party in the Brazilian presidential election, 2010 won 19.33% of the vote of the first round taking enough votes from Dilma Rousseff of the incumbent PT party to stop her getting the 50% needed to avoid a second round of which she went on to beat Jose Serra of the opposition PSDB party. Despite only winning numerically the vote in only 1 state; the relatively electorally small Federal District which holds the capital of Brazil; the Green Party interestingly came second to Dilma in Amapa, Amazonas, Pernambuco and in Rio de Janeiro in front of Jose Serra. The Green party also tied with Serra in Ceara, both having 16.36% of the vote, although Dilma won the state. Maranhão also had a 1.5% difference in vote between Serra and Marina with the Green Party coming third. In São Paulo, the Green Party numerically had their strongest showing with almost 5 million votes taking 20.77% of the vote. Success in the legislature however only amounted to winning 2 more seats to a total of 15 in the Chamber of Deputies and the loss of their only senate seat.
Colombia.
In the 1990s, the Oxygen Green Party was created under the leadership of Ingrid Betancourt but dissolved after her infamous kidnapping. Later, the Visionaries Party was created by Antanas Mockus whose ideals earned him the Bogotá Mayoral Office twice.
In the 2010 Colombian presidential election a green party has been created under the name Colombian Green Party, with former Bogotá mayor Antanas Mockus being the leader.
Lebanon.
The Green Party of Lebanon was founded in 2008 as a secular party. Its first president was Philippe Skaff, CEO of Grey Advertising. The party debuted with the May 2010 municipal elections. In 2011, the party became the first political party in Lebanon to elect a female leader when Nada Zaarour was elected its president

</doc>
<doc id="46099" url="http://en.wikipedia.org/wiki?curid=46099" title="Barber paradox">
Barber paradox

The barber paradox is a puzzle derived from Russell's paradox. It was used by Bertrand Russell himself as an illustration of the paradox, though he attributes it to an unnamed person who suggested it to him. It shows that an apparently plausible scenario is logically impossible. Specifically, it describes a barber who is defined such that he both shaves himself and does not shave himself.
Paradox.
Suppose there is a town with just one barber, who is male. In this town, every man keeps himself clean-shaven, and he does so by doing exactly one of two things:
Also, "The barber is a man in town who shaves all those, and only those, men in town who do not shave themselves."
From this, asking the question "Who shaves the barber?" results in a paradox because according to the statement above, he can either shave himself, or go to the barber (which happens to be himself). However, neither of these possibilities are valid: they both result in the barber shaving himself, but he cannot do this because he shaves only those men "who do not shave themselves".
Despite its popular name, however, Barber paradox is not really a paradox in the true sense of this word. A man who shaves exactly those men who do not shave themselves simply cannot and does not exist, and there are virtually no reasons to expect the opposite. This is in contrast with the set of all sets that do not contain themselves (from Russell's paradox), whose existence cannot be painlessly dismissed as it follows from the very intuitive and widely relied upon axioms of naive set theory.
History.
This paradox is often attributed to Bertrand Russell (e.g., by Martin Gardner in "Aha!"). It was suggested to him as an alternative form of Russell's paradox, which he had devised to show that set theory as it was used by Georg Cantor and Gottlob Frege contained contradictions. However, Russell denied that the Barber's paradox was an instance of his own:
 That contradiction [Russell's paradox] is extremely interesting. You can modify its form; some forms of modification are valid and some are not. I once had a form suggested to me which was not valid, namely the question whether the barber shaves himself or not. You can define the barber as "one who shaves all those, and those only, who do not shave themselves." The question is, does the barber shave himself? In this form the contradiction is not very difficult to solve. But in our previous form I think it is clear that you can only get around it by observing that the whole question whether a class is or is not a member of itself is nonsense, i.e. that no class either is or is not a member of itself, and that it is not even true to say that, because the whole form of words is just noise without meaning.
 — Bertrand Russell, "The Philosophy of Logical Atomism"
This point is elaborated further under Applied versions of Russell's paradox.
In first-order logic.
formula_1
This sentence is unsatisfiable (a contradiction) because of the universal quantifier formula_2. The universal quantifier y will include every single element in the domain, including our infamous barber x. So when the value x is assigned to y, the sentence can be rewritten to formula_3, which simplifies to formula_4, a contradiction.
In Prolog.
In Prolog, one aspect of the barber paradox can be expressed by a self-referencing clause:
shaves(barber, X) :- male(X), not shaves(X,X).
male(barber).
where negation as failure is assumed. If we apply the stratification test known from Datalog, the predicate shaves is exposed as unstratifiable since it is defined recursively over its negation.
In literature.
In his book "Alice in Puzzleland", the logician Raymond Smullyan had the character Humpty Dumpty explain the apparent paradox to Alice. Smullyan argues that the paradox is akin to the statement "I know a man who is both five feet tall and six feet tall," in effect claiming that the "paradox" is merely a contradiction, not a true paradox at all, as the two axioms above are mutually exclusive.
A paradox is supposed to arise from plausible and apparently consistent statements; Smullyan suggests that the "rule" the barber is supposed to be following is too absurd to seem plausible.
The paradox is also mentioned several times in David Foster Wallace's first novel, "The Broom of the System" as well as "", by James Gleick.
Multiple barbers.
If the paradox is altered so that there may be multiple barbers in the town, then the paradox may or may not be resolved, depending on the exact phrasing of the initial rules.
If the initial rules state that every man in town must keep himself clean-shaven, either by
(but not both at once), then the paradox is solved. Each barber can be shaved by another barber.
However, if the initial rules describe the responsibilities of the barbers rather than the town's residents in general, then the paradox remains. In this version, the rules state that each barber must shave everyone in town who does not shave himself (and no one else). If Barber A asks Barber B to shave his beard, then Barber A counts as "a person who does not shave himself". But because of this classification, Barber A must shave himself, rather than let Barber B do it for him. However, if Barber A is shaving himself, then he must "not" shave himself. Either way, Barber A is stuck. Other barbers face the same problem.
Non-paradoxical variations.
A modified version of the barber paradox is frequently encountered in the form of a brain teaser puzzle or joke. The joke is phrased nearly identically to the standard paradox, but omitting a detail that allows an answer to escape the paradox entirely. For example, the puzzle can be stated as occurring in a small town whose barber claims: I shave "all" and "only" the men in our town who do not shave themselves. This version identifies the sex of the clients, but omits the sex of the barber, so a simple solution is that "the barber is a woman". The barber's claim applies to only "men in our town," so there is no paradox if the barber is a woman (or a gorilla, or a child, or a man from some other town—or anything other than a "man in our town"). Such a variation is not considered to be a paradox at all: the true barber paradox requires the contradiction arising from the situation where the barber's claim applies to himself.
Notice that the paradox still occurs if we claim that the barber is a man in our town with a beard. In this case, the barber does not shave himself (because he has a beard); but then according to his claim (that he shaves all men who do not shave themselves), he must shave himself.
In a similar way, the paradox still occurs if the barber is a man in our town who "cannot" grow a beard. Once again, he does not shave himself (because he has no hair on his face), but that implies that he does shave himself.

</doc>
<doc id="46104" url="http://en.wikipedia.org/wiki?curid=46104" title="25th century BC">
25th century BC

The 25th century BC is a century which lasted from the year 2500 BC to 2401 BC.

</doc>
<doc id="46105" url="http://en.wikipedia.org/wiki?curid=46105" title="Indigo children">
Indigo children

Indigo children, according to a pseudoscientific New Age concept, are children who are believed to possess special, unusual and sometimes supernatural traits or abilities. The idea is based on concepts developed in the 1970s by Nancy Ann Tappe and further developed by Jan Tober and Lee Carroll. The concept of indigo children gained popular interest with the publication of a series of books in the late 1990s and the release of several films in the following decade. A variety of books, conferences and related materials have been created surrounding belief in the idea of indigo children and their nature and abilities. The interpretations of these beliefs range from their being the next stage in human evolution, in some cases possessing paranormal abilities such as telepathy, to the belief that they are more empathetic and creative than their peers.
Although no scientific studies give credibility to the existence of indigo children or their traits, the phenomenon appeals to some parents whose children have been diagnosed with learning disabilities and to parents seeking to believe that their children are special. Critics view this as a way for parents to avoid considering pediatric treatment or a psychiatric diagnosis. The list of traits used to describe the children has also been criticized for being vague enough to be applied to almost anyone, a form of the Forer effect.
Origins.
The term "indigo children" originated with parapsychologist and self-described synesthete and psychic Nancy Ann Tappe, who developed the concept in the 1970s. Tappe published the book "Understanding Your Life Through Color" in 1982 describing the concept, stating that during the mid-1960s she began noticing that many children were being born with "indigo" auras (in other publications Tappe said the color indigo came from the "life colors" of the children which she acquired through her synesthesia). The idea was later popularized by the 1998 book "The Indigo Children: The New Kids Have Arrived", written by husband and wife self-help lecturers Lee Carroll and Jan Tober. In 2002, an international conference on indigo children was held in Hawaii, drawing 600 attendees, with subsequent conferences the following years in Florida and Oregon. Several films have also been produced on the subject, including two English feature films in 2003 and 2005, and a documentary in 2005 (both the latter were directed by James Twyman, a New Age writer).
Sarah W. Whedon suggests in a 2009 article in "Nova Religio" that the social construction of indigo children is a response to an "apparent crisis of American childhood" in the form of increased youth violence and diagnoses of attention deficit disorder and attention deficit hyperactivity disorder. Whedon believes parents label their children as "indigo" to provide an alternative explanation for their children's improper behavior stemming from ADD and ADHD.
Claimed characteristics.
Descriptions of indigo children include that they:
Other alleged traits include:
According to Tober and Carroll, indigo children may function poorly in conventional schools due to their rejection of rigid authority, their being smarter or more spiritually mature than their teachers, and their lack of response to guilt-, fear- or manipulation-based discipline.
According to research psychologist Russell Barkley, the New Age movement has yet to produce empirical evidence of the existence of indigo children, as the traits most commonly attributed to them are closely aligned with the Forer effect—so vague that they could describe nearly anyone. Many critics see the concept of indigo children as made up of extremely general traits, a sham diagnosis that is an alternative to a medical diagnosis, with a complete lack of science or studies to support it. The lack of scientific foundation is acknowledged by some believers, including Doreen Virtue, author of "The Care and Feeding of Indigos", and James Twyman, who produced two films on indigo children and who offers materials and courses related to the phenomenon. Virtue has been criticized for claiming to have a Ph.D., despite this being awarded by California Coast University, a then-unaccredited institution sometimes accused of being a diploma mill.
Indigo as an alternative to diagnosis.
Retired professor of philosophy and skeptic Robert Todd Carroll notes that many of the commentators on the indigo phenomenon are of varying qualifications and expertise, and parents may prefer labeling their child an indigo as an alternative to a diagnosis that implies imperfection, poor parenting, narcissistic parenting, damage or mental illness. This is a belief echoed by academic psychologists. Some mental health experts are concerned that labeling a disruptive child an "indigo" may delay proper diagnosis and treatment that could help the child or look into the parenting style that may be causing the behavior. Others have stated that many of the traits of indigo children could be more prosaically interpreted as simple unruliness and alertness.
Relationship to attention-deficit hyperactivity disorder.
Many children labeled indigo by their parents are diagnosed with attention-deficit hyperactivity disorder (ADHD) and Tober and Carroll's book "The Indigo Children" linked the concept with diagnosis of ADHD. David Cohen points out that labeling a child an indigo is an alternative to a diagnosis that implies imperfection, damage or mental illness, which may appeal to many parents. Cohen has stated, "The view in medicine is that ADHD is a defect. It's a disorder. If you're a parent, the idea of 'gifted' is much more appealing than the idea of a disorder." Linking the concept of indigo children with the distaste for the use of Ritalin to control ADHD, Robert Todd Carroll states "The hype and near-hysteria surrounding the use of Ritalin has contributed to an atmosphere that makes it possible for a book like "Indigo Children" to be taken seriously. Given the choice, who wouldn't rather believe their children are special and chosen for some high mission rather than that they have a brain disorder?" Stephen Hinshaw, a professor of psychology at the University of California, Berkeley, states that concerns regarding the overmedicalization of children are legitimate but even gifted children with ADHD learn better with more structure rather than less, even if the structure initially causes difficulties. Many labeled as indigo children are or have been home schooled. Many children labeled as Indigo Children have the same identifying criteria as those children who have experienced being raised by a Narcissistic parent, and are considered to have been emotionally abused.
A 2011 study suggested parents of children with ADHD who label their children as "indigos" may perceive problematic behaviors emblematic of ADHD to be more positive and experience less frustration and disappointment, though they still experience more negative emotions and conflicts than parents of children without a diagnosis.
Relation to autism.
Crystal children, a concept related to indigo children, has been linked to the autistic spectrum. Proponents recategorize autistic symptoms as telepathic powers, and attempt to "‍[reconceptualize] the autistic traits associated with them as part of a positive identity". Autism researcher Mitzi Waltz states that there may be inherent dangers to these beliefs, leading parents to deny the existence of impairments, avoid proven treatments and spend considerable money on unhelpful interventions. Waltz states that "Parents may also transmit belief systems to the child that are self-aggrandizing, confusing, or potentially frightening".
Commercialization.
The concept of indigo children has been criticized for being less about children and their needs, and more about the profits to be made by self-styled experts in book and video sales as well as lucrative counseling sessions, summer camps, conferences and speaking engagements.
Discussion as a new religious movement.
At the 2014 University of Cambridge Festival of Ideas, anthropologist Beth Singler discussed how the term Indigo Children functioned as a new religious movement, along with Jediism. Singler's work focuses in the Indigo movement as a part of an overall discussion on "wider moral panics around children, parenting, the diagnosis of conditions such as ADHD and autism and conspiracy theories about Big Pharma and vaccinations."

</doc>
<doc id="46106" url="http://en.wikipedia.org/wiki?curid=46106" title="The Skeptic's Dictionary">
The Skeptic's Dictionary

The Skeptic's Dictionary is a collection of cross-referenced skeptical essays by Robert Todd Carroll, published on his website skepdic.com and in a printed book. The skepdic.com site was launched in 1994 and the book was published in 2003 with nearly 400 entries. As of January 2011 the website has over 700 entries. A comprehensive single-volume guides to skeptical information on pseudoscientific, paranormal, and occult topics, the bibliography contains some seven hundred references for more detailed information. According to the back cover of the book, the on-line version receives approximately 500,000 hits per month.
Contents.
"The Skeptic's Dictionary" is, according to its foreword, intended to be a small counterbalance to the voluminous occult and paranormal literature; not to present a balanced view of occult subjects.
The articles in the book are in several categories:
Print versions are available in Dutch, English, Japanese, Korean, and Russian. Numerous entries have been translated for the Internet in several other languages. A newsletter keeps interested parties up to date on new entries and an archived list of previous newsletters is available for online perusal. Norcross et al. state that Carroll has made considerable progress in exposing pseudoscience and quackery.
According to the author,
Carroll defines each of these categories, explaining how and why, in his opinion, his dictionary may be of interest, use, and benefit to each of them. He also defines the term “skepticism” as he uses it and identifies two types of skeptic, the Apollonian, who is “committed to clarity and rationality” and the Dionysian, who is “committed to passion and instinct.” William James, Bertrand Russell, and Friedrich Nietzsche exemplify the Apollonian skeptic, Carroll says, and Charles Sanders Peirce, Tertullian, Søren Kierkegaard, and Blaise Pascal are Dionysian skeptics.
Roy Herbert's review of the paperback version written for the "New Scientist" magazine commented that "it is an amazing assembly, elegantly written and level-headed, with a wry remark here and there", and that "this superb work is likely to be used so often that it is a pity it is a softback book."

</doc>
<doc id="46111" url="http://en.wikipedia.org/wiki?curid=46111" title="Qibla al-Qudsiyya">
Qibla al-Qudsiyya

The Qibla al-Qudsiyya is the name given to a small sect of the Jews of Medina who converted to Islam in 622/623. When the qibla (direction of prayer for Muslims) was changed from Jerusalem (known in Arabic as "al-Quds") to Mecca, these Jews protested and finally declined the change. They remained Muslims, but did not accept any of the verses in the Qur'an written after the date of the split. Little is known of the existence of these Islamic Jews afterwards.
Did they exist?
The Qur'anic verses referring to the change of Qiblah are all within the second chapter, "Al-Baqarah" (meaning the Heifer). One verse reads thus: ""The fools among the people will say: "What hath turned them from the Qibla to which they were used?" Say: To God belong both east and West: He guideth whom He will to a Way that is straight."" (2:142 from Abdullah Y. Ali's translation)
It is conceivable that the sources mentioning the Qibla al-Qudsiyya like the Sirat un-Nabi (1) could in fact be trying to "fill in the gaps" of Muhammad's biography by taking Qur'anic verses and drawing implicit information from them. The verses in the chapter in question clearly indicate a group of people who dislike the change of direction from Jerusalem to Makkah. Interestingly the commentary in the translation of Abdullah Y. Ali - which is the most widespread English translation of the Qur'an - indicate that this group in fact were the Arabs and not Jews: "The Qibla of Jerusalem might itself have seemed strange to the Arabs, and the change from it to the Ka'ba might have seemed strange after they had become used to the other. In reality one direction or another, or east or west, in itself did not matter, as God is in all places, and is independent of Time and Place." (c. 145"ff")
The subsequent verse mentions the unwillingness of the People of the Book (Jews and Christians) to follow the Qibla of Islam: "Even if thou wert to bring to the people of the Book all the Signs (together), they would not follow Thy Qibla; nor art thou going to follow their Qibla; nor indeed will they follow each other's Qibla. If thou after the knowledge hath reached thee, Wert to follow their (vain) desires,-then wert thou Indeed (clearly) in the wrong. " (2:145)
According to the commentary of the same scholar this does not refer to Jewish converts to Islam, rather the main corpus of the Jews, arguing that it refers to the traditions mentioned in the Bible: "His window being open in his chambers towards Jerusalem, he kneeled upon his knees three times a day, and prayed, and gave thanks before his God as he gave aforetime." (The Book of Daniel, 6:10)
Thus rationalizing the criticism that the Qur'an would direct towards the main corpus of Jews (and Christians) for not accepting the concept of Qibla, particularly that of Kaaba. Or in Qur'anic wording: "The people of the Book know this as they know their own sons; but some of them conceal the truth which they themselves know." (2:146)
If this understanding of these verses is correct that would render "the need" for a specific group of Jewish converts which did not accept the new Qiblah of Kaaba (or rather the old, because this had been Qiblah before the Hijra) to "fill in the gap" in story, if this is in fact what has been done.
cf. 
(1) Sirat un-Nabi
Jahiz - Kitab al-Bukhala

</doc>
<doc id="46112" url="http://en.wikipedia.org/wiki?curid=46112" title="Violence">
Violence

Violence is defined by the World Health Organization as "the intentional use of physical force or power, threatened or actual, against oneself, another person, or against a group or community, which either results in or has a high likelihood of resulting in injury, death, psychological harm, maldevelopment, or deprivation", but acknowledges that the inclusion of "the use of power" in its definition expands on the conventional meaning of the word. This definition involves intentionality with the committing of the act itself, irrespective of the outcome it produces. However, generally, anything that is excited in an injurious or damaging way may be described as violent even if not meant to be violence (by a person and against a person).
The most cause of death in interpersonal violence is assault with a firearm (180,000), followed by a sharp object (114,000). Other means contribute to another 110,000 deaths.
Violence in many forms is preventable. There is a strong relationship between levels of violence and modifiable factors such as concentrated poverty, income and gender inequality, the harmful use of alcohol, and the absence of safe, stable, and nurturing relationships between children and parents. Strategies addressing the underlying causes of violence can be effective in preventing violence.
Globally, violence resulted in the death of 1.28 million people in 2013 up from 1.13 million in 1990. 842,000 were due to suicide, 405,000 were due to homicide, and 31,000 were due to war. In Africa, out of every 100,000 people, each year an estimated 60.9 die a violent death. Gunfire kills ten children a day in the United States. Corlin, past president of the American Medical Association said: "The United States leads the world—in the rate at which its children die from firearms." He concluded: "Gun violence is a threat to the public health of our country." For each single death due to violence, there are dozens of hospitalizations, hundreds of emergency department visits, and thousands of doctors' appointments. Furthermore, violence often has lifelong consequences for physical and mental health and social functioning and can slow economic and social development.
Types.
Violence can be divided into three broad categories:
Violent acts can be:
This initial categorization differentiates between violence a person inflicts upon himself or herself, violence inflicted by another individual or by a small group of individuals, and violence inflicted by larger groups such as states, organized political groups, militia groups and terrorist organizations. These three broad categories are each divided further to reflect more specific types of violence.
Self-directed violence.
Self-directed violence is subdivided into suicidal behaviour and self-abuse. The former includes suicidal thoughts, attempted suicides – also called "para suicide" or "deliberate self-injury" in some countries – and completed suicides. Self-abuse, in contrast, includes acts such as self-mutilation.
Collective violence.
Collective violence is subdivided into structural violence and economic violence. Unlike the other two broad categories, the subcategories of collective violence suggest possible motives for violence committed by larger groups of individuals or by states. Collective violence that is committed to advance a particular social agenda includes, for example, crimes of hate committed by organized groups, terrorist acts and mob violence. Political violence includes war and related violent conflicts, state violence and similar acts carried out by larger groups. Economic violence includes attacks by larger groups motivated by economic gain – such as attacks carried out with the purpose of disrupting economic activity, denying access to essential services, or creating economic division and fragmentation. Clearly, acts committed by larger groups can have multiple motives.
This typology, while imperfect and far from being universally accepted, does provide a useful framework for understanding the complex patterns of violence taking place around the world, as well as violence in the everyday lives of individuals, families and communities. It also overcomes many of the limitations of other typologies by capturing the nature of violent acts, the relevance of the setting, the relationship between the perpetrator and the victim, and – in the case of collective violence – possible motivations for the violence. However, in both research and practice, the dividing lines between the different types of violence are not always so clear.
War is a state of prolonged violent large-scale conflict involving two or more groups of people, usually under the auspices of government. It is the most extreme form of collective violence.
War is fought as a means of resolving territorial and other conflicts, as war of aggression to conquer territory or loot resources, in national self-defence or liberation, or to suppress attempts of part of the nation to secede from it. We know also ideological, religious and revolutionary wars.
Since the Industrial Revolution, the lethality of modern warfare has grown. World War I casualties were over 40 million and World War II casualties were over 70 million.
Nevertheless, some hold the actual deaths from war have decreased compared to past centuries. In "War Before Civilization", Lawrence H. Keeley, a professor at the University of Illinois, calculates that 87% of tribal societies were at war more than once per year, and some 65% of them were fighting continuously. The attrition rate of numerous close-quarter clashes, which characterize endemic warfare, produces casualty rates of up to 60%, compared to 1% of the combatants as is typical in modern warfare. "Primitive Warfare" of these small groups or tribes was driven by the basic need for sustenance and violent competition. Their environment dictated the size of their groups for the most part, they would only include as many people as the tribe could provide for. The small group size also made moving much easier if needed, once resources were becoming scarce in the area. Stephen Pinker agrees, writing that "in tribal violence, the clashes are more frequent, the percentage of men in the population who fight is greater, and the rates of death per battle are higher."
Jared Diamond in his books, "Guns, Germs and Steel" and "The Third Chimpanzee" provides sociological and anthropological evidence for the rise of large-scale warfare as a result of advances in technology and city-states. The rise of agriculture provided a significant increase in the number of individuals that a region could sustain over hunter-gatherer societies, allowing for development of specialized classes such as soldiers, or weapons manufacturers. On the other hand, tribal conflicts in hunter-gatherer societies tend to result in wholesale slaughter of the opposition (other than perhaps females of child-bearing years) instead of territorial conquest or slavery, presumably as hunter-gatherer numbers could not sustain empire-building.
Non-physical.
Violence includes those acts that result from a power relationship, including threats and intimidation, neglect or acts of omission, in addition to more obvious violent acts. Violence has a broad range of outcomes – including psychological harm, deprivation and maldevelopment. Violence may not necessarily result in injury or death, but nonetheless poses a substantial burden on individuals, families, communities and health care systems worldwide. Many forms of violence against women, children and the elderly, for instance, can result in physical, psychological and social problems that do not necessarily lead to injury, disability or death. These consequences can be immediate, as well as latent, and can last for years after the initial abuse. Defining outcomes solely in terms of injury or death thus limits the understanding of the full impact of violence.
Interpersonal violence.
Interpersonal violence is divided into two subcategories: Family and intimate partner violence – that is, violence largely between family members and intimate partners, usually, though not exclusively, taking place in the home. Community violence – violence between individuals who are unrelated, and who may or may not know each other, generally taking place outside the home. The former group includes forms of violence such as child abuse, intimate partner violence and abuse of the elderly. The latter includes youth violence, random acts of violence, rape or sexual assault by strangers, and violence in institutional settings such as schools, workplaces, prisons and nursing homes. When interpersonal violence occurs in families, its psychological consequences can affect parents, children, and their relationship in the short- and long-terms.
Child maltreatment.
Child maltreatment is the abuse and neglect that occurs to children under 18 years of age. It includes all types of physical and/or emotional ill-treatment, sexual abuse, neglect, negligence and commercial or other exploitation, which results in actual or potential harm to the child’s health, survival, development or dignity in the context of a relationship of responsibility, trust or power. Exposure to intimate partner violence is also sometimes included as a form of child maltreatment
Child maltreatment is a global problem with serious lifelong consequences, which is, however, complex and difficult to study.
There are no reliable global estimates for the prevalence of child maltreatment. Data for many countries, especially low- and middle-income countries, are lacking. Current estimates vary widely depending on the country and the method of research used. Approximately 20% of women and 5–10% of men report being sexually abused as children, while 25–50% of all children report being physically abused.
Consequences of child maltreatment include impaired lifelong physical and mental health, and social and occupational functioning (e.g. school, job, and relationship difficulties). These can ultimately slow a country's economic and social development. Preventing child maltreatment before it starts is possible and requires a multisectoral approach. Effective prevention programmes support parents and teach positive parenting skills. Ongoing care of children and families can reduce the risk of maltreatment reoccurring and can minimize its consequences.
Youth violence.
Following the World Health Organization, youth are defined as people between the ages of 10 and 29 years. Youth violence refers to violence occurring between youths, and includes acts that range from bullying and physical fighting, through more severe sexual and physical assault to homicide.
Worldwide some 250,000 homicides occur among youth 10–29 years of age each year, which is 41% of the total number of homicides globally each year ("Global Burden of Disease", World Health Organization, 2008). For each young person killed, 20-40 more sustain injuries requiring hospital treatment. Youth violence has a serious, often lifelong, impact on a person's psychological and social functioning. Youth violence greatly increases the costs of health, welfare and criminal justice services; reduces productivity; decreases the value of property; and generally undermines the fabric of society.
Prevention programmes shown to be effective or to have promise in reducing youth violence include life skills and social development programmes designed to help children and adolescents manage anger, resolve conflict, and develop the necessary social skills to solve problems; schools-based anti-bullying prevention programmes; and programmes to reduce access to alcohol, illegal drugs and guns. Also, given significant neighbourhood effects on youth violence, interventions involving relocating families to less poor environments have shown promising results. Similarly, urban renewal projects such as business improvement districts have shown a reduction in youth violence.
Intimate partner violence.
Intimate partner violence refers to behaviour in an intimate relationship that causes physical, sexual or psychological harm, including physical aggression, sexual coercion, psychological abuse and controlling behaviours.
Population-level surveys based on reports from victims provide the most accurate estimates of the prevalence of intimate partner violence and sexual violence in non-conflict settings. A study conducted by WHO in 10 mainly developing countries found that, among women aged 15 to 49 years, between 15% (Japan) and 70% (Ethiopia and Peru) of women reported physical and/or sexual violence by an intimate partner.
Intimate partner and sexual violence have serious short- and long-term physical, mental, sexual and reproductive health problems for victims and for their children, and lead to high social and economic costs. These include both fatal and non-fatal injuries, depression and post-traumatic stress disorder, unintended pregnancies, sexually transmitted infections, including HIV.
Factors associated with the perpetration and experiencing of intimate partner violence are low levels of education, past history of violence as a perpetrator, a victim or a witness of parental violence, harmful use of alcohol, attitudes that are accepting of violence as well as marital discord and dissatisfaction. Factors associated only with perpetration of intimate partner violence are having multiple partners, and antisocial personality disorder.
The primary prevention strategy with the best evidence for effectiveness for intimate partner violence is school-based programming for adolescents to prevent violence within dating relationships. Evidence is emerging for the effectiveness of several other primary prevention strategies – those that: combine microfinance with gender equality training; promote communication and relationship skills within communities; reduce access to, and the harmful use of alcohol; and change cultural gender norms.
Sexual violence.
Sexual violence is any sexual act, attempt to obtain a sexual act, unwanted sexual comments or advances, or acts to traffic, or otherwise directed against a person’s sexuality using coercion, by any person regardless of their relationship to the victim, in any setting. It includes rape, defined as the physically forced or otherwise coerced penetration of the vulva or anus with a penis, other body part or object.
Population-level surveys based on reports from victims estimate that between 0.3–11.5% of women reported experiencing sexual violence. Sexual violence has serious short- and long-term consequences on physical, mental, sexual and reproductive health for victims and for their children as described in the section on intimate partner violence. If perpetrated during childhood, sexual violence can lead to increased smoking, drug and alcohol misuse, and risky sexual behaviours in later life. It is also associated with perpetration of violence and being a victim of violence.
Many of the risk factors for sexual violence are the same as for domestic violence. Risk factors specific to sexual violence perpetration include beliefs in family honour and sexual purity, ideologies of male sexual entitlement and weak legal sanctions for sexual violence.
Few intervention to prevent sexual violence have been demonstrated to be effective. School-based programmes to prevent child sexual abuse by teaching children to recognize and avoid potentially sexually abusive situations are run in many parts of the world and appear promising, but require further research. To achieve lasting change, it is important to enact legislation and develop policies that protect women; address discrimination against women and promote gender equality; and help to move the culture away from violence.
Elder maltreatment.
Elder maltreatment is a single or repeated act, or lack of appropriate action, occurring within any relationship where there is an expectation of trust which causes harm or distress to an older person. This type of violence constitutes a violation of human rights and includes physical, sexual, psychological, emotional; financial and material abuse; abandonment; neglect; and serious loss of dignity and respect
While there is little information regarding the extent of maltreatment in elderly populations, especially in developing countries, it is estimated that 4–6% of elderly people in high-income countries have experienced some form of maltreatment at home However, older people are often afraid to report cases of maltreatment to family, friends, or to the authorities. Data on the extent of the problem in institutions such as hospitals, nursing homes and other long-term care facilities are scarce. Elder maltreatment can lead to serious physical injuries and long-term psychological consequences. Elder maltreatment is predicted to increase as many countries are experiencing rapidly ageing populations.
Many strategies have been implemented to prevent elder maltreatment and to take action against it and mitigate its consequences including public and professional awareness campaigns, screening (of potential victims and abusers), caregiver support interventions (e.g. stress management, respite care), adult protective services and self-help groups. Their effectiveness has, however, not so far been well-established.
Targeted violence.
Several rare but painful episodes of assassination, attempted assassination and shootings in schools and universities in the United States led to a considerable body of research on ascertainable behaviours of persons who have planned or carried out such attacks. These studies (1995-2002) investigated what the authors called "targeted violence," described the "path to violence" of those who planned or carried out attacks, and laid out suggestions for law enforcement and educators. A major point from these research studies is that targeted violence does not just "come out of the blue".
Consequences.
Beyond deaths and injuries, highly prevalent forms of violence (such as child maltreatment and intimate partner violence) have serious lifelong non-injury health consequences. Victims may engage in high-risk behaviours such as alcohol and substance misuse, smoking, and unsafe sex, which in turn can contribute to cardiovascular disorders, cancers, depression, diabetes and HIV/AIDS, resulting in premature death Violence may beget violence. The balances of prevention, mitigation, mediation and exacerbation are complex, and vary with the underpinnings of violence.
Factors.
Violence cannot be attributed to a single factor. Its causes are complex and occur at different levels. To represent this complexity, the ecological, or social ecological model is often used. The following four-level version of the ecological model is often used in the study of violence:
The first level identifies biological and personal factors that influence how individuals behave and increase their likelihood of becoming a victim or perpetrator of violence: demographic characteristics (age, education, income), genetics, brain lesions, personality disorders, substance abuse, and a history of experiencing, witnessing, or engaging in violent behaviour.
The second level focuses on close relationships, such as those with family and friends. In youth violence, for example, having friends who engage in or encourage violence can increase a young person’s risk of being a victim or perpetrator of violence. For intimate partner violence, a consistent marker at this level of the model is marital conflict or discord in the relationship. In elder abuse, important factors are stress due to the nature of the past relationship between the abused person and the care giver.
The third level explores the community context—i.e., schools, workplaces, and neighbourhoods. Risk at this level may be affected by factors such as the existence of a local drug trade, the absence of social networks, and concentrated poverty. All these factors have been shown to be important in several types of violence.
Finally, the fourth level looks at the broad societal factors that help to create a climate in which violence is encouraged or inhibited: the responsiveness of the criminal justice system, social and cultural norms regarding gender roles or parent-child relationships, income inequality, the strength of the social welfare system, the social acceptability of violence, the availability of weapons, the exposure to violence in mass media, and political instability.
Psychology.
The causes of violent behaviour in humans are often a topic of research in psychology. Neurobiologist Jan Volavka emphasizes that, for those purposes, "violent behavior is defined as intentional physically aggressive behavior against another person."
Based on the idea of human nature, scientists do agree violence is inherent in humans. Among prehistoric humans, there is archaeological evidence for both contentions of violence and peacefulness as primary characteristics.
Since violence is a matter of perception as well as a measurable phenomenon, psychologists have found variability in whether people perceive certain physical acts as "violent". For example, in a state where execution is a legalized punishment we do not typically perceive the executioner as "violent", though we may talk, in a more metaphorical way, of the state acting violently. Likewise, understandings of violence are linked to a perceived aggressor-victim relationship: hence psychologists have shown that people may not recognise defensive use of force as violent, even in cases where the amount of force used is significantly greater than in the original aggression.
The "violent male ape" image is often brought up in discussions of human violence. Dale Peterson and Richard Wranghamin "Demonic Males: Apes and the Origins of Human Violence" write that violence is inherent in humans, though not inevitable.
However, William L. Ury, editor of a book called "Must We Fight? From the Battlefield to the Schoolyard—A New Perspective on Violent Conflict and Its Prevention" criticizes the "killer ape" myth in his book which brings together discussions from two Harvard Law School symposiums. The conclusion is that "we also have lots of natural mechanisms for cooperation, to keep conflict in check, to channel aggression, and to overcome conflict. These are just as natural to us as the aggressive tendencies."
James Gilligan writes violence is often pursued as an antidote to shame or humiliation. The use of violence often is a source of pride and a defence of honor, especially among males who often believe violence defines manhood.
In an article entitled "The History of Violence" in "The New Republic", Steven Pinker offers evidence that, on average, the amount and cruelty of violence to humans and animals has decreased over the last few centuries.
Pinker's observation of the decline in interpersonal violence echoes the work of Norbert Elias, who attributes the decline to a "civilizing process", in which the state's monopolization of violence, the maintenance of socioeconomic interdependencies or "figurations", and the maintenance of behavioural codes in culture all contribute to the development of individual sensibilities, which increase the repugnance of individuals towards violent acts.
Some scholars disagree with the argument that all violence is decreasing arguing that not all types of violent behaviour are lower now than in the past. They suggest that research typically focuses on lethal violence, often looks at homicide rates of death due to warfare, but ignore the less obvious forms of violence. However, non-lethal violence, such as assaults or bullying appear to be declining as well
This concept that violence can be normalized, is known as socially sanctioned or structural violence, and is a topic of increasing interest to researchers trying to understand violent behavior. It has been discussed at length by researchers in sociology, medical anthropology, psychology, philosophy, and bioarchaeology.
Evolutionary psychology offers several explanations for human violence in various contexts, such as sexual jealousy in humans, child abuse, and homicide. Goetz (2010) argues that humans are similar to most mammal species and use violence in specific situations. He writes that "Buss and Shackelford (1997a) proposed seven adaptive problems our ancestors recurrently faced that might have been solved by aggression: co-opting the resources of others, defending against attack, inflicting costs on same-sex rivals, negotiating status and hierarchies, deterring rivals from future aggression, deterring mate from infidelity, and reducing resources expended on genetically unrelated children."
Goetz writes that most homicides seem to start from relatively trivial disputes between unrelated men who then escalate to violence and death. He argues that such conflicts occur when there is a status dispute between men of relatively similar status. If there is a great initial status difference, then the lower status individual usually offers no challenge and if challenged the higher status individual usually ignores the lower status individual. At the same an environment of great inequalities between people may cause those at the bottom to use more violence in attempts to gain status.
Media.
Research into the media and violence examines whether links between consuming media violence and subsequent aggressive and violent behaviour exists. Although some scholars had claimed media violence may increase aggression, this view is coming increasingly in doubt both in the scholarly community and was rejected by the US Supreme Court in the Brown v EMA case, as well as in a review of video game violence by the Australian Government (2010) which concluded evidence for harmful effects were inconclusive at best and the rhetoric of some scholars was not matched by good data.
Prevention.
The threat and enforcement of physical punishment has been a tried and tested method of preventing some violence since civilisation began. It is used in various degrees in most countries.
The most significant factor for reducing violence in a society is the guidance and discipline of children as they mature. The effectiveness of physical punishment at this level is much debated, but if it is used, it should be as a last resort and never done in anger. More important preventative measures are showing children love and understanding which is described further in the sections that follow.
Interpersonal violence.
A rigorous review of the literature on the effectiveness of strategies to prevent interpersonal violence identified the seven strategies below as being supported by either strong or emerging evidence for effectiveness. These strategies target risk factors at all four levels of the ecological model.
Child caregiver relationships.
Among the most effective such programmes to prevent child maltreatment and reduce childhood aggression are the Nurse Family Partnership home-visiting programme and the Triple P (Parenting Program). There is also emerging evidence that these programmes reduce convictions and violent acts in adolescence and early adulthood, and probably help decrease intimate partner violence and self-directed violence in later life.
Life skills in youth.
Evidence shows that the life skills acquired in social development programmes can reduce involvement in violence, improve social skills, boost educational achievement and improve job prospects. Life skills refer to social, emotional, and behavioural competencies which help children and adolescents effectively deal with the challenges of everyday life.
Gender equality.
Evaluation studies are beginning to support community interventions that aim to prevent violence against women by promoting gender equality. For instance, evidence suggests that programmes that combine microfinance with gender equity training can reduce intimate partner violence. School-based programmes such as Safe Dates programme in the United States of America and the Youth Relationship Project in Canada have been found to be effective for reducing dating violence.
Cultural norms.
Rules or expectations of behaviour – norms – within a cultural or social group can encourage violence. Interventions that challenge cultural and social norms supportive of violence can prevent acts of violence and have been widely used, but the evidence base for their effectiveness is currently weak. The effectiveness of interventions addressing dating violence and sexual abuse among teenagers and young adults by challenging social and cultural norms related to gender is supported by some evidence
Support programmes.
Interventions to identify victims of interpersonal violence and provide effective care and support are critical for protecting health and breaking cycles of violence from one generation to the next. Examples for which evidence of effectiveness is emerging includes: screening tools to identify victims of intimate partner violence and refer them to appropriate services; psychosocial interventions – such as trauma-focused cognitive behavioural therapy – to reduce mental health problems associated with violence, including post-traumatic stress disorder; and protection orders, which prohibit a perpetrator from contacting the victim, to reduce repeat victimization among victims of intimate partner violence.
Collective violence.
Not surprisingly, scientific evidence about the effectiveness of interventions to prevent collective violence is lacking. However, policies that facilitate reductions in poverty, that make decision-making more accountable, that reduce inequalities between groups, as well as policies that reduce access to biological, chemical, nuclear and other weapons have been recommended. When planning responses to violent conflicts, recommended approaches include assessing at an early stage who is most vulnerable and what their needs are, co-ordination of activities between various players and working towards global, national and local capabilities so as to deliver effective health services during the various stages of an emergency.
Criminal justice.
One of the main functions of law is to regulate violence.
Sociologist Max Weber stated that the state claims the monopoly of the legitimate use of force practised within the confines of a specific territory. Law enforcement is the main means of regulating nonmilitary violence in society. Governments regulate the use of violence through legal systems governing individuals and political authorities, including the police and military. Civil societies authorize some amount of violence, exercised through the police power, to maintain the status quo and enforce laws.
However, German political theorist Hannah Arendt noted: "Violence can be justifiable, but it never will be legitimate ... Its justification loses in plausibility the farther its intended end recedes into the future. No one questions the use of violence in self-defence, because the danger is not only clear but also present, and the end justifying the means is immediate". Arendt made a clear distinction between violence and power. Most political theorists regarded violence as an extreme manifestation of power whereas Arendt regarded the two concepts as opposites.
In the 20th century in acts of democide governments may have killed more than 260 million of their own people through police brutality, execution, massacre, slave labour camps, and sometimes through intentional famine.
Violent acts that are not carried out by the military or police and that are not in self-defence are usually classified as crimes, although not all crimes are violent crimes. Damage to property is classified as violent crime in some jurisdictions but not in all.
The Federal Bureau of Investigation classifies violence resulting in homicide into criminal homicide and justifiable homicide (e.g. self-defence).
The criminal justice approach sees its main task as enforcing laws that proscribe violence and ensuring that "justice is done". The notions of individual blame, responsibility, guilt, and culpability are central to criminal justice's approach to violence and one of the criminal justice system's main tasks is to "do justice", i.e. to ensure that offenders are properly identified, that the degree of their guilt is as accurately ascertained as possible, and that they are punished appropriately. To prevent and respond to violence, the criminal justice approach relies primarily on deterrence, incarceration and the punishment and rehabilitation of perpetrators.
The criminal justice approach, beyond justice and punishment, has traditionally emphasized indicated interventions, aimed at those who have already been involved in violence, either as victims or as perpetrators. One of the main reasons offenders are arrested, prosecuted, and convicted is to prevent further crimes – through deterrence (threatening potential offenders with criminal sanctions if they commit crimes), incapacitation (physically preventing offenders from committing further crimes by locking them up) and through rehabilitation (using time spent under state supervision to develop skills or change one's psychological make-up to reduce the likelihood of future offences).
In recent decades in many countries in the world, the criminal justice system has taken an increasing interest in preventing violence before it occurs. For instance, much of community and problem-oriented policing aims to reduce crime and violence by altering the conditions that foster it - and not to increase the number of arrests. Indeed, some police leaders have gone so far as to say the police should primarily be a crime prevention agency. Juvenile justice systems – an important component of criminal justice systems – are largely based on the belief in rehabilitation and prevention. In the US, the criminal justice system has, for instance, funded school- and community-based initiatives to reduce children's access to guns and teach conflict resolution. In 1974, the US Department of Justice assumed primary responsibility for delinquency prevention programmes and created the Office of Juvenile Justice and Delinquency Prevention, which has supported the "Blueprints for violence prevention" programme at the University of Colorado Boulder.
Public health.
The public health approach is a science-driven, population-based, interdisciplinary, intersectoral approach based on the ecological model which emphasizes primary prevention. Rather than focusing on individuals, the public health approach aims to provide the maximum benefit for the largest number of people, and to extend better care and safety to entire populations. The public health approach is interdisciplinary, drawing upon knowledge from many disciplines including medicine, epidemiology, sociology, psychology, criminology, education and economics. Because all forms of violence are multi-faceted problems, the public health approach emphasizes a multi-sectoral response. It has been proved time and again that cooperative efforts from such diverse sectors as health, education, social welfare, and criminal justice are often necessary to solve what are usually assumed to be purely "criminal" or "medical" problems. The public health approach considers that violence, rather than being the result of any single factor, is the outcome of multiple risk factors and causes, interacting at four levels of a nested hierarchy (individual, close relationship/family, community and wider society) of the Social ecological model.
From a public health perspective, prevention strategies can be classified into three types:
A public health approach emphasizes the primary prevention of violence, i.e. stopping them from occurring in the first place. Until recently, this approach has been relatively neglected in the field, with the majority of resources directed towards secondary or tertiary prevention. Perhaps the most critical element of a public health approach to prevention is the ability to identify underlying causes rather than focusing upon more visible "symptoms". This allows for the development and testing of effective approaches to address the underlying causes and so improve health.
The public health approach is an evidence-based and systematic process involving the following four steps:
In many countries, violence prevention is still a new or emerging field in public health. The public health community has started only recently to realize the contributions it can make to reducing violence and mitigating its consequences. In 1949, Gordon called for injury prevention efforts to be based on the understanding of causes, in a similar way to prevention efforts for communicable and other diseases. In 1962, Gomez, referring to the WHO definition of health, stated that it is obvious that violence does not contribute to "extending life" or to a "complete state of well-being". He defined violence as an issue that public health experts needed to address and stated that it should not be the primary domain of lawyers, military personnel, or politicians.
However, it is only in the last 30 years that public health has begun to address violence, and only in the last fifteen has it done so at the global level. This is a much shorter period of time than public health has been tackling other health problems of comparable magnitude and with similarly severe lifelong consequences.
The global public health response to interpersonal violence began in earnest in the mid-1990s. In 1996, the World Health Assembly adopted Resolution WHA49.25 which declared violence "a leading worldwide public health problem" and requested that the World Health Organization (WHO) initiate public health activities to (1) document and characterize the burden of violence, (2) assess the effectiveness of programmes, with particular attention to women and children and community-based initiatives, and (3) promote activities to tackle the problem at the international and national levels. The World Health Organization's initial response to this resolution was to create the Department of Violence and Injury Prevention and Disability and to publish the World report on violence and health (2002).
The case for the public health sector addressing interpersonal violence rests on four main arguments. First, the significant amount of time health care professionals dedicate to caring for victims and perpetrators of violence has made them familiar with the problem and has led many, particularly in emergency departments, to mobilize to address it. The information, resources, and infrastructures the health care sector has at its disposal are an important asset for research and prevention work. Second, the magnitude of the problem and its potentially severe lifelong consequences and high costs to individuals and wider society call for population-level interventions typical of the public health approach. Third, the criminal justice approach, the other main approach to addressing violence (link to entry above), has traditionally been more geared towards violence that occurs between male youths and adults in the street and other public places – which makes up the bulk of homicides in most countries – than towards violence occurring in private settings such as child maltreatment, intimate partner violence and elder abuse – which makes up the largest share of non-fatal violence. Fourth, evidence is beginning to accumulate that a science-based public health approach is effective at preventing interpersonal violence.
Human rights.
The human rights approach is based on the obligations of states to respect, protect and fulfill human rights and therefore to prevent, eradicate and punish violence. It recognizes violence as a violation of many human rights: the rights to life, liberty, autonomy and security of the person; the rights to equality and non-discrimination; the rights to be free from torture and cruel, inhuman and degrading treatment or punishment; the right to privacy; and the right to the highest attainable standard of health. These human rights are enshrined in international and regional treaties and national constitutions and laws, which stipulate the obligations of states, and include mechanisms to hold states accountable. The Convention on the Elimination of All Forms of Discrimination Against Women, for example, requires that countries party to the Convention take all appropriate steps to end violence against women. The Convention on the Rights of the Child in its Article 19 states that States Parties shall take all appropriate legislative, administrative, social and educational measures to protect the child from all forms of physical or mental violence, injury or abuse, neglect or negligent treatment, maltreatment or exploitation, including sexual abuse, while in the care of parent(s), legal guardian(s) or any other person who has the care of the child.
Geographical context.
Violence, as defined in the dictionary of human geography, "appears whenever power is in jeopardy" and "in and of itself stands emptied of strength and purpose: it is part of a larger matrix of socio-political power struggles". Violence can be broadly divided into three broad categories – direct violence, structural violence and cultural violence. Thus defined and delineated, it is of note, as Hyndman says, that "geography came late to theorizing violence" in comparison to other social sciences. Social and human geography, rooted in the humanist, Marxist, and feminist subfields that emerged following the early positivist approaches and subsequent behavioral turn, have long been concerned with social and spatial justice.
Along with critical geographers and political geographers, it is these groupings of geographers that most often interact with violence. Keeping this idea of social/spatial justice via geography in mind, it is worthwhile to look at geographical approaches to violence in the context of politics.
Derek Gregory and Alan Pred assembled the influential edited collection "Violent Geographies: Fear, Terror, and Political Violence", which demonstrates how place, space, and landscape are foremost factors in the real and imagined practices of organized violence both historically and in the present. Evidently, political violence often gives a part for the state to play. When "modern states not only claim a monopoly of the legitimate means of violence; they also routinely use the threat of violence to enforce the rule of law", the law not only becomes a form of violence but is violence. Philosopher Giorgio Agamben's concepts of state of exception and "homo sacer" are useful to consider within a geography of violence. The state, in the grip of a perceived, potential crisis (whether legitimate or not) takes preventative legal measures, such as a suspension of rights (it is in this climate, as Agamben demonstrates, that the formation of the Social Democratic and Nazi government's lager or concentration camp can occur). However, when this "in limbo" reality is designed to be in place "until further notice…the state of exception thus ceases to be referred to as an external and provisional state of factual danger and comes to be confused with juridical rule itself". For Agamben, the physical space of the camp "is a piece of land placed outside the normal juridical order, but it is nevertheless not simply an external space". At the scale of the body, in the state of exception, a person is so removed from their rights by "juridical procedures and deployments of power" that "no act committed against them could appear any longer as a crime"; in other words, people become only "homo sacer". Guantanamo Bay could also be said to represent the physicality of the state of exception in space, and can just as easily draw man as homo sacer.
In the 1970s, genocides in Cambodia under the Khmer Rouge and Pol Pot resulted in the deaths of over two million Cambodians (which was 25% of the Cambodian population), forming one of the many contemporary examples of state-sponsored violence. About fourteen thousand of these murders occurred at Choeung Ek, which is the best-known of the extermination camps referred to as the Killing Fields. The killings were arbitrary; for example, a person could be killed for wearing glasses, since that was seen as associating them with intellectuals and therefore as making them part of the enemy. People were murdered with impunity because it was no crime; Cambodians were made "homo sacer" in a condition of bare life. The Killing Fields—manifestations of Agamben's concept of camps beyond the normal rule of law—featured the state of exception. As part of Pol Pot's "ideological intent…to create a purely agrarian society or cooperative", he "dismantled the country's existing economic infrastructure and depopulated every urban area". Forced movement, such as this forced movement applied by Pol Pot, is a clear display of structural violence. When "symbols of Cambodian society were equally disrupted, social institutions of every kind…were purged or torn down", cultural violence (defined as when "any aspect of culture such as language, religion, ideology, art, or cosmology is used to legitimize direct or structural violence") is added to the structural violence of forced movement and to the direct violence, such as murder, at the Killing Fields. Vietnam eventually intervened and the genocide officially ended. However, ten million landmines left by opposing guerillas in the 1970s continue to create a violent landscape in Cambodia.
Human geography, though coming late to the theorizing table, has tackled violence through many lenses, including anarchist geography, feminist geography, Marxist geography, political geography, and critical geography. However, Adriana Cavarero notes that, "as violence spreads and assumes unheard-of forms, it becomes difficult to name in contemporary language". Cavarero proposes that, in facing such a truth, it is prudent to reconsider violence as "horrorism"; that is, "as though ideally all the…victims, instead of their killers, ought to determine the name". With geography often adding the forgotten spatial aspect to theories of social science, rather than creating them solely within the discipline, it seems that the self-reflexive contemporary geography of today may have an extremely important place in this current (re)imaging of violence, exemplified by Cavarero.
Epidemiology.
As of 2010, all forms of violence resulted in about 1.34 million deaths up from about 1 million in 1990. Suicide accounts for about 883,000, interpersonal violence for 456,000 and collective violence for 18,000. Deaths due to collective violence have decreased from 64,000 in 1990.
By way of comparison, the 1.5 millions deaths a year due to violence is greater than the number of deaths due to tuberculosis (1.34 million), road traffic injuries (1.21 million), and malaria (830'000), but slightly less than the number of people who die from HIV/AIDS (1.77 million).
For every death due to violence, there are numerous nonfatal injuries. In 2008, over 16 million cases of non-fatal violence-related injuries were severe enough to require medical attention. Beyond deaths and injuries, forms of violence such as child maltreatment, intimate partner violence, and elder maltreatment have been found to be highly prevalent.
Self-directed violence.
In the last 45 years, suicide rates have increased by 60% worldwide. Suicide is among the three leading causes of death among those aged 15–44 years in some countries, and the second leading cause of death in the 10–24 years age group. These figures do not include suicide attempts which are up to 20 times more frequent than completed suicide. Suicide was the 16th leading cause of death worldwide in 2004 and is projected to increase to the 12th in 2030. Although suicide rates have traditionally been highest among the male elderly, rates among young people have been increasing to such an extent that they are now the group at highest risk in a third of countries, in both developed and developing countries.
Interpersonal violence.
Rates and patterns of violent death vary by country and region. In recent years, homicide rates have been highest in developing countries in Sub-Saharan Africa and Latin America and the Caribbean and lowest in East Asia, the western Pacific, and some countries in northern Africa. Studies show a strong, inverse relationship between homicide rates and both economic development and economic equality. Poorer countries, especially those with large gaps between the rich and the poor, tend to have higher rates of homicide than wealthier countries. Homicide rates differ markedly by age and sex. Gender differences are least marked for children. For the 15 to 29 age group, male rates were nearly six times those for female rates; for the remaining age groups, male rates were from two to four times those for females.
Studies in a number of countries show that, for every homicide among young people age 10 to 24, 20 to 40 other young people receive hospital treatment for a violent injury.
Forms of violence such as child maltreatment and intimate partner violence are highly prevalent. Approximately 20% of women and 5–10% of men report being sexually abused as children, while 25–50% of all children report being physically abused. A WHO multi-country study found that between 15–71% of women reported experiencing physical and/or sexual violence by an intimate partner at some point in their lives
Collective violence.
Wars grab headlines, but the individual risk of dying violently in an armed conflict is today relatively low—much lower than the risk of violent death in many countries that are not suffering from an armed conflict. For example, between 1976 and 2008, African Americans were victims of 329,825 homicides. Although there is a widespread perception that war is the most dangerous form of armed violence in the world, the average person living in a conflict-affected country had a risk of dying violently in the conflict of about 2.0 per 100,000 population between 2004 and 2007. This can be compared to the average world homicide rate of 7.6 per 100,000 people. This illustration highlights the value of accounting for all forms of armed violence rather than an exclusive focus on conflict related violence. Certainly, there are huge variations in the risk of dying from armed conflict at the national and subnational level, and the risk of dying violently in a conflict in specific countries remains extremely high. In Iraq, for example, the direct conflict death rate for 2004–07 was 65 per 100,000 people per year and, in Somalia, 24 per 100,000 people. This rate even reached peaks of 91 per 100,000 in Iraq in 2006 and 74 per 100,000 in Somalia in 2007.
History.
Organized, large-scale, militaristic, or regular human-on-human violence was absent for the vast majority of the human timeline, and is first documented to have started only relatively recently in the Holocene, an epoch that began about 11,700 years ago, probably with the advent of higher population densities due to sedentism. Social anthropologist Douglas P. Fry contends that scholars are divided on the origins of this greater degree of violence—in other words, war-like behavior: There are basically two schools of thought on this issue. One holds that warfare... goes back at least to the time of the first thoroughly modern humans and even before then to the primate ancestors of the hominid lineage. The second positions on the origins of warfare sees war as much less common in the cultural and biological evolution of humans. Here, warfare is a latecomer on the cultural horizon, only arising in very specific material circumstances and being quite rare in human history until the development of agriculture in the past 10,000 years.
In academia, the idea of the peaceful pre-history and non-violent tribal societies gained popularity with the post-colonial perspective. The trend, starting in archaeology and spreading to anthropology reached its height in the late half of the 20th century. However, some newer research in archaeology and bioarchaeology may provide evidence that violence within and among groups is not a recent phenomenon. According to the book "The Bioarchaeology of Violence" violence is a behavior that is found throughout human history. 
The book "War Before Civilization", provides an substantial exploration of the various roles that violence played among past societies. The following year an edited volume called "Troubled Times: Violence and Warfare in the Past" dedicated to the identification of violent encounters in the past through the analysis of the scars that these events leave on the body was published.
Fry explores Keeley's argument in depth and counters that such sources erroneously focus on the ethnography of hunters and gatherers in the present, whose culture and values have been infiltrated externally by modern civilization, rather than the actual archaeological record spanning some two million years of human existence. Fry determines that all present ethnographically studied tribal societies, "by the very fact of having been described and published by anthropologists, have been irrevocably impacted by history and modern colonial nation states" and that "many have been affected by state societies for at least 5000 years."
"The Better Angels of Our Nature".
Steven Pinker's 2011 book, "The Better Angels of Our Nature", roused both acclaim and controversy by asserting that modern society is less violent than in periods of the past, whether on the short scale of decades or long scale of centuries or millennia.
Steven Pinker argues that by every possible measure, every type of violence has drastically decreased since ancient and medieval times. A few centuries ago, for example, genocide was a standard practice in all kinds of warfare and was so common that historians did not even bother to mention it. According to Pinker, rape, murder, warfare and animal cruelty have all seen drastic declines in the 20th century. However, Pinker's analyses have met with much criticism; for example, Pinker himself, on his FAQ page, states that he does not include catastrophic ecological violence (including violence against wild or domesticated non-human animals or plants, or against ecosystems) or the violence of economic inequality and of coercive working conditions in his definition; he controversially regards these forms of violence as "metaphorical". Some critics have therefore argued that Pinker suffers from "a reductive vision of what it means to be violent."
Society and culture.
Economic effects.
In countries with high levels of violence, economic growth can be slowed down, personal and collective security eroded, and social development impeded. Families edging out of poverty and investing in schooling their sons and daughters can be ruined through the violent death or severe disability of the main breadwinner. Communities can be caught in poverty traps where pervasive violence and deprivation form a vicious circle that stifles economic growth. For societies, meeting the direct costs of health, criminal justice, and social welfare responses to violence diverts many billions of dollars from more constructive societal spending. The much larger indirect costs of violence due to lost productivity and lost investment in education work together to slow economic development, increase socioeconomic inequality, and erode human and social capital.
Additionally, communities with high level of violence do not provide the level of stability and predictability vital for a prospering business economy. Individuals will be less likely to invest money and effort towards growth in such unstable and violent conditions.
Religion.
Religious and political ideologies have been the cause of interpersonal violence throughout history. Ideologues often falsely accuse others of violence, such as the ancient blood libel against Jews, the medieval accusations of casting witchcraft spells against women, and modern accusations of satanic ritual abuse against day care center owners and others.
Both supporters and opponents of the 21st century War on Terrorism regard it largely as an ideological and religious war.
Vittorio Bufacchi describes two different modern concepts of violence, one the "minimalist conception" of violence as an intentional act of excessive or destructive force, the other the "comprehensive conception" which includes violations of rights, including a long list of human needs.
Anti-capitalists assert that capitalism is violent. They believe private property, trade, interest and profit survive only because police violence defends them and that capitalist economies need war to expand. They may use the term "structural violence" to describe the systematic ways in which a given social structure or institution kills people slowly by preventing them from meeting their basic needs, for example the deaths caused by diseases because of lack of medicine.
Frantz Fanon critiqued the violence of colonialism and wrote about the counter violence of the "colonized victims."
Throughout history, most religions and individuals like Mahatma Gandhi have preached that humans are capable of eliminating individual violence and organizing societies through purely nonviolent means. Gandhi himself once wrote: "A society organized and run on the basis of complete non-violence would be the purest anarchy." Modern political ideologies which espouse similar views include pacifist varieties of voluntarism, mutualism, anarchism and libertarianism.
Terence Fretheim writing about the Old Testament:
For many people, ... only physical violence truly qualifies as violence. But, certainly, violence is more than killing people, unless one includes all those words and actions that kill people slowly. The effect of limitation to a “killing fields” perspective is the widespread neglect of many other forms of violence. We must insist that violence also refers to that which is psychologically destructive, that which demeans, damages, or depersonalizes others. In view of these considerations, violence may be defined as follows: any action, verbal or nonverbal, oral or written, physical or psychical, active or passive, public or private, individual or institutional/societal, human or divine, in whatever degree of intensity, that abuses, violates, injures, or kills. Some of the most pervasive and most dangerous forms of violence are those that are often hidden from view (against women and children, especially); just beneath the surface in many of our homes, churches, and communities is abuse enough to freeze the
blood. Moreover, many forms of systemic violence often slip past our attention because they are so much a part of the infrastructure of life (e.g., racism, sexism, ageism).

</doc>
<doc id="46117" url="http://en.wikipedia.org/wiki?curid=46117" title="Eratosthenes">
Eratosthenes

Eratosthenes of Cyrene (; Greek: Ἐρατοσθένης, ]; c. 276 BC – c. 195/194 BC) was a Greek mathematician, geographer, poet, astronomer, and music theorist. He was a man of learning, becoming the chief librarian at the Library of Alexandria. He invented the discipline of geography, including the terminology used today.
He is best known for being the first person to calculate the circumference of the Earth, which he did by applying a measuring system using stades, or the length of stadia during that time period. His calculation was remarkably accurate. He was also the first to calculate the tilt of the Earth's axis (again with remarkable accuracy). Additionally, he may have accurately calculated the distance from the Earth to the Sun and invented the leap day. He created the first map of the world incorporating parallels and meridians, based on the available geographical knowledge of the era.
Eratosthenes was the founder of scientific chronology; he endeavored to revise the dates of the chief literary and political events from the conquest of Troy. In number theory, he introduced the sieve of Eratosthenes, an efficient method of identifying prime numbers.
He was a figure of influence who declined to specialize in only one field. According to an entry in the Suda (a 10th-century reference), his critics scorned him, calling him "Beta", from the second letter of the Greek alphabet, because he always came in second in all his endeavors. Nonetheless, his devotees nicknamed him "Pentathlos", after the Olympians who were well rounded competitors, for he had proven himself to be knowledgeable in every area of learning. Eratosthenes yearned to understand the complexities of the entire world.
Life.
The son of Aglaos, Eratosthenes was born in 276 BC, in Cyrene. Now part of modern-day Libya, Cyrene had been founded by the Greeks centuries earlier, and became the capital of Pentapolis (North Africa), a country of five cities: Cyrene, Arsinde, Berenice, Ptolemias, and Apollonia, Cyrenaica. Alexander the Great conquered Cyrene in 332 BC, and following his death in 323 BC its rule was given to one of his generals, Ptolemy I Soter, the founder of the Ptolemaic Kingdom. Under Ptolemaic rule the economy prospered, based largely on the export of horses and silphium, a plant used for rich seasoning and medicine. Cyrene became a place of cultivation, where knowledge blossomed. Like any young Greek, Eratosthenes would have studied in the local gymnasium, where he would have learned physical skills and social discourse as well as reading, writing, arithmetic, poetry, and music.
Eratosthenes went to Athens to further his studies. There he was taught Stoicism by its founder, Zeno of Citium, in philosophical lectures on living a virtuous life. He then studied under Ariston of Chios, who led a more cynical school of philosophy. He also studied under the head of the Platonic Academy, who was Arcesilaus of Pitane. His interest in Plato led him to write his very first work at a scholarly level, "Platonikos", inquiring into the mathematical foundation of Plato's philosophies. Eratosthenes was a man of many perspectives and investigated the art of poetry under Callimachus. He had talent as a most imaginative poet. He wrote poems: one in hexameters called "Hermes" illustrating the god's life history; and another, in elegiacs, called "Erigone", describing the suicide of the Athenian maiden Erigone (daughter of Icarius). He wrote "Chronographies", a text that scientifically depicted dates of importance, beginning with the Trojan War. This work was highly esteemed for its accuracy: George Syncellus was later able to preserve from "Chronographies" a list of 38 kings of the Egyptian Thebes. Eratosthenes also wrote "Olympic Victors", a chronology of the winners of the Olympic Games. It is not known when he wrote his works, but they highlighted his abilities.
These works and his great poetic abilities led the pharaoh Ptolemy III Euergetes to seek to place him as a librarian at the Library of Alexandria in the year 245 BC. Eratosthenes, then thirty years old, accepted Ptolemy's invitation and traveled to Alexandria, where he lived for the rest of his life. Within about five years he became Chief Librarian, a position that the poet Apollonius Rhodius had previously held. As head of the library Eratosthenes tutored the children of Ptolemy, including Ptolemy IV Philopator who became the fourth Ptolemaic pharaoh. He expanded the library's holdings: in Alexandria all books had to be surrendered for duplication. It was said that these were copied so accurately that it was impossible to tell if the library had returned the copy or the original.
He sought to maintain the reputation of the Library of Alexandria against competition from the Pergamum. Eratosthenes created a whole section devoted to the examination of Homer, and acquired original works of great tragic dramas of Aeschylus, Sophocles and Euripides.
Eratosthenes made several important contributions to mathematics and science, and was a friend of Archimedes. Around 255 BC, he invented the armillary sphere. In "On the Circular Motions of the Celestial Bodies", Cleomedes credited him with having calculated the Earth's circumference around 240 BC, using knowledge of the angle of elevation of the Sun at noon on the summer solstice in Alexandria and on Elephantine Island near Syene (now Aswan, Egypt).
Eratosthenes believed there was good and bad in every nation and criticized Aristotle for arguing that humanity was divided into Greeks and barbarians, and that the Greeks should keep themselves racially pure. As he aged he contracted ophthalmia, becoming blind around 195 BC. Losing the ability to read and to observe nature plagued and depressed him, leading him to voluntarily starve himself to death. He died in 194 BC at the age of 82 in his beloved Alexandria.
Measurement of the Earth's circumference.
Eratosthenes calculated the circumference of the Earth without leaving Egypt. Eratosthenes knew that at local noon on the summer solstice in the Ancient Egyptian city of Swenet (known in ancient Greek as Syene, and now as Aswan) on the Tropic of Cancer, the Sun would appear at the zenith, directly overhead. He knew this because he had been told that the shadow of someone looking down a deep well in Syene would block the reflection of the Sun at noon off the water at the bottom of the well. Using a gnomon, he measured the Sun's angle of elevation at noon on the solstice in Alexandria, and found it to be 1/50th of a circle (7°12') south of the zenith. He may have used a compass to measure the angle of the shadow cast by the Sun. Assuming that the Earth was spherical (360°), and that Alexandria was due north of Syene, he concluded that the meridian arc distance from Alexandria to Syene must therefore be 1/50th of a circle's circumference, or 7°12'/360°.
His knowledge of the size of Egypt was founded on the work of many generations of surveying trips. Pharaonic bookkeepers gave a distance between Swenet and Alexandria of 5,000 stadia. This distance was corroborated by inquiring about the time that it took to travel from Syene to Alexandria by camel. He rounded the result to a final value of 700 stadia per degree, which implies a circumference of 252,000 stadia. Some claim Erathostenes used the Egyptian stade of 157.5 meters, which would imply a circumference of 39,690 km, an error of 1.6%, but the 185 meter Attic stade is the most commonly accepted value for the length of the stade used by Eratosthenes in his measurements of the Earth, which imply a circumference of 46,620 km, an error of 16.3%. It is unlikely, however, that Eratosthenes got an accurate measurement of the circumference of the Earth, given three errors in the assumptions he made:
If we repeat Eratosthenes' calculation with more accurate data, the result is 40,074 km, which is 66 km different (0.16%) from the currently accepted circumference of the Earth.
Seventeen hundred years after Eratosthenes' death, while Christopher Columbus studied what Eratosthenes had written about the size of the Earth, he chose to believe that the Earth's circumference was much smaller. Had Columbus set sail knowing that Eratosthenes' larger circumference value was more accurate, he would have known that the place where he made landfall was not Asia, but rather a New World.
"Father of geography".
Eratosthenes continued from his knowledge about the Earth, his discoveries of its size and shape, and began to sketch it. In the Library of Alexandria he had access to various travel books, which contained various items of information and representations of the world that needed to be pieced together in some organized format. In his three-volume work "Geography" (Greek: "Geographika"), he described and mapped his entire known world, even dividing the Earth into five climate zones: two freezing zones around the pole, two temperate zones, and a zone encompassing the equator and the tropics. He had invented geography. He created terminology that is still used today. He placed grids of overlapping lines over the surface of the Earth. He used parallels and meridians to link together every place in the world. It was now possible to estimate one's distance from remote locations with this network over the surface of the Earth. In the "Geography" the names of over 400 cities and their locations were shown: this had never been achieved before. Unfortunately, his "Geography" has been lost to history, but fragments of the work can be pieced together from other great historians like Pliny, Polybius, Strabo, and Marcianus.
Other achievements.
Eratosthenes was described by the Suda Lexicon as a Πένταθλος (Pentathlos) which can be translated as "All-Rounder", for he was skilled in a variety of things: He was a true polymath. He was nicknamed Beta, because he was great at many things and tried to get his hands on every bit of information, but never achieved the highest rank in anything, so much so that Strabo accounts Eratosthenes as a mathematician among geographers, and a geographer among mathematicians.
Prime numbers.
Eratosthenes proposed a simple algorithm for finding prime numbers. This algorithm is known in mathematics as the Sieve of Eratosthenes.
In mathematics, the sieve of Eratosthenes (Greek: κόσκινον Ἐρατοσθένους), one of a number of prime number sieves, is a simple, ancient algorithm for finding all prime numbers up to any given limit. It does so by iteratively marking as composite, "i.e.", not prime, the multiples of each prime, starting with the multiples of 2. The multiples of a given prime are generated starting from that prime, as a sequence of numbers with the same difference, equal to that prime, between consecutive numbers. This is the sieve's key distinction from using trial division to sequentially test each candidate number for divisibility by each prime.
Works.
Eratosthenes was one of the most pre-eminent scholarly figures of his time, and produced works covering a vast area of knowledge before and during his time at the Library. He wrote on many topics — geography, mathematics, philosophy, chronology, literary criticism, grammar, poetry, and even old comedies. Unfortunately, there are only fragments left of his works after the Destruction of the Library of Alexandria.
Further reading.
</dl>

</doc>
<doc id="46118" url="http://en.wikipedia.org/wiki?curid=46118" title="Iona Nikitchenko">
Iona Nikitchenko

Major-General Iona Timofeevich Nikitchenko (Russian: Иона Тимофеевич Никитченко) (June 28, 1895, Don Voisko Oblast, Russian Empire – April 22, 1967, Moscow, Russian SFSR) was a judge of the Supreme Court of the Soviet Union.
Iona was born to a peasant family in khutor Tuzlukov (now Rostov Oblast). He studied at his local Agricultural Institute and from 1916 was a Bolshevik. His court experience started in May 1920 when he was appointed as the chairman-deputy of the Military Court of Semirechye Army Group during the Civil War. During the Civil War he participated on the frontlines in the Middle Asia. In 1924 was appointed as the member of the Military Court Collegiate of the Moscow Military District.
Nikitchenko presided over some of the most notorious of Joseph Stalin's show trials during the Great Purges of 1936 to 1938, where he among other things sentenced Kamenev and Zinoviev.
Nuremberg Trial.
Nikitchenko was one of the three main drafters of the London Charter. He was also the Soviet Union's judge at the Nuremberg trials, and was President for the session at Berlin. Nikitchenko's prejudices were evident from the outset. Before the Tribunal convened, Nikitchenko explained the Soviet perspective of the trials:"We are dealing here with the chief war criminals who have already been convicted and whose conviction has been already announced by both the Moscow and Crimea [Yalta] declarations by the heads of the [Allied] governments... The whole idea is to secure quick and just punishment for the crime."
His statements in this respect call to mind the statements of US Supreme Court Chief Justice Harlan Fiske Stone who wrote "Chief US prosecutor Jackson is away conducting his high-grade lynching party in Nuremberg, I don't mind what he does to the Nazis, but I hate to see the pretense that he is running a court and proceeding according to common law. This is a little too sanctimonious a fraud to meet my old-fashioned ideas." Nikitchenko was thus far from alone in viewing the Nuremberg trials as a farcical cloaking in law of the process of putting to death a large number of notorious villains.
Nikitchenko dissented against the acquittals of Hjalmar Schacht, Franz von Papen and Hans Fritzsche, and argued for a death sentence for Rudolf Hess. Nikitchenko said, in the lead-up to the trials, "If... the judge is supposed to be impartial, it would only lead to unnecessary delays." Hess, formerly Hitler's deputy fuhrer, the man charged by Hitler with implementing Nazi Germany's Nuremberg laws, the man who signed the decree establishing the notorious German occupation government of Poland, and since May 1941 in a British Prison, was sentenced to life in prison by the tribunal. In this respect, he was by far the most senior surviving Nazi official to escape a death sentence. Nikitchenko also found the majority judgments incorrect with regard to the Reich Cabinet, the German General Staff and the Oberkommando der Wehrmacht. Having never before written a dissenting opinion - these being unheard of in Soviet jurisprudence - and being unsure of the form of such an opinion, Nikitchenko was assisted in writing his dissents by his fellow judge Norman Birkett.
Nikitchenko feared a compromise on too lenient a level. At the point of final deliberation he reexamined Hess' case and voted for a life sentence so that the opportunity for Hess to get away with a lesser degree of punishment did not occur.

</doc>
<doc id="46120" url="http://en.wikipedia.org/wiki?curid=46120" title="Range encoding">
Range encoding

Range encoding is an entropy coding method defined by G. Nigel N. Martin in a 1979 paper, which effectively rediscovered the FIFO arithmetic code first introduced by Richard Clark Pasco in 1976. Given a stream of symbols and their probabilities, a range coder produces a space efficient stream of bits to represent these symbols and, given the stream and the probabilities, a range decoder reverses the process.
Range coding is very similar to arithmetic encoding, except that encoding is done with digits in any base, instead of with bits, and so it is faster when using larger bases (e.g. a byte) at small cost in compression efficiency. After the expiration of the first (1978) arithmetic coding patent, range encoding appeared to clearly be free of patent encumbrances. This particularly drove interest in the technique in the open source community. Since that time, patents on various well-known arithmetic coding techniques have also expired.
How range encoding works.
Range encoding conceptually encodes all the symbols of the message into one number, unlike Huffman coding which assigns each symbol a bit-pattern and concatenates all the bit-patterns together. Thus range encoding can achieve greater compression ratios than the one-bit-per-symbol lower bound on Huffman encoding and it does not suffer the inefficiencies that Huffman does when dealing with probabilities that are not exact powers of two.
The central concept behind range encoding is this: given a large-enough range of integers, and a probability estimation for the symbols, the initial range can easily be divided into sub-ranges whose sizes are proportional to the probability of the symbol they represent. Each symbol of the message can then be encoded in turn, by reducing the current range down to just that sub-range which corresponds to the next symbol to be encoded. The decoder must have the same probability estimation the encoder used, which can either be sent in advance, derived from already transferred data or be part of the compressor and decompressor.
When all symbols have been encoded, merely identifying the sub-range is enough to communicate the entire message (presuming of course that the decoder is somehow notified when it has extracted the entire message). A single integer is actually sufficient to identify the sub-range, and it may not even be necessary to transmit the entire integer; if there is a sequence of digits such that every integer beginning with that prefix falls within the sub-range, then the prefix alone is all that's needed to identify the sub-range and thus transmit the message.
Example.
Suppose we want to encode the message "AABA<EOM>", where <EOM> is the end-of-message symbol. For this example it is assumed that the decoder knows that we intend to encode exactly five symbols in the base 10 number system (allowing for 105 different combinations of symbols with the range [0, 100000)) using the probability distribution {A: .60; B: .20; <EOM>: .20}. The encoder breaks down the range [0, 100000) into three subranges:
 A: [ 0, 60000)
 B: [ 60000, 80000)
 <EOM>: [ 80000, 100000)
Since our first symbol is an A, it reduces our initial range down to [0, 60000). The second symbol choice leaves us with three sub-ranges of this range, we show them following the already-encoded 'A':
 AA: [ 0, 36000)
 AB: [ 36000, 48000)
 A<EOM>: [ 48000, 60000)
With two symbols encoded, our range is now [0, 36000) and our third symbol leads to the following choices:
 AAA: [ 0, 21600)
 AAB: [ 21600, 28800)
 AA<EOM>: [ 28800, 36000)
This time it is the second of our three choices that represent the message we want to encode, and our range becomes [21600, 28800). It may look harder to determine our sub-ranges in this case, but it is actually not: we can merely subtract the lower bound from the upper bound to determine that there are 7200 numbers in our range; that the first 4320 of them represent 0.60 of the total, the next 1440 represent the next 0.20, and the remaining 1440 represent the remaining 0.20 of the total. Adding back the lower bound gives us our ranges:
 AABA: [21600, 25920)
 AABB: [25920, 27360)
 AAB<EOM>: [27360, 28800)
Finally, with our range narrowed down to [21600, 25920), we have just one more symbol to encode. Using the same technique as before for dividing up the range between the lower and upper bound, we find the three sub-ranges are:
 AABAA: [21600, 24192)
 AABAB: [24192, 25056)
 AABA<EOM>: [25056, 25920)
And since <EOM> is our final symbol, our final range is [25056, 25920). Because all five-digit integers starting with "251" fall within our final range, it is one of the three-digit prefixes we could transmit that would unambiguously convey our original message. (The fact that there are actually eight such prefixes in all implies we still have inefficiencies. They have been introduced by our use of base 10 rather than base 2.)
The central problem may appear to be selecting an initial range large enough that no matter how many symbols we have to encode, we will always have a current range large enough to divide into non-zero sub-ranges. In practice, however, this is not a problem, because instead of starting with a very large range and gradually narrowing it down, the encoder works with a smaller range of numbers at any given time. After some number of digits have been encoded, the leftmost digits will not change. In the example after encoding just three symbols, we already knew that our final result would start with "2". More digits are shifted in on the right as digits on the left are sent off. This is illustrated in the following code:
To finish off we may need to emit a few extra digits. The top digit of codice_1 is probably too small so we need to increment it, but we have to make sure we don't increment it past codice_2. So first we need to make sure codice_3 is large enough.
One problem that can occur with the codice_4 function above is that codice_3 might become very small but codice_1 and codice_2 still have differing first digits. This could result in the interval having insufficient precision to distinguish between all of the symbols in the alphabet. When this happens we need to fudge a little, output the first couple of digits even though we might be off by one, and re-adjust the range to give us as much room as possible. The decoder will be following the same steps so it will know when it needs to do this to keep in sync.
Base 10 was used in this example, but a real implementation would just use binary, with the full range of the native integer data type. Instead of codice_8 and codice_9 you would likely use hexadecimal constants such as codice_10 and codice_11. Instead of emitting a digit at a time you would emit a byte at a time and use a byte-shift operation instead of multiplying by 10.
Decoding uses exactly the same algorithm with the addition of keeping track of the current codice_12 value consisting of the digits read from the compressor. Instead of emitting the top digit of codice_1 you just throw it away, but you also shift out the top digit of codice_12 and shift in a new digit read from the compressor. Use codice_15 below instead of codice_16.
In order to determine which probability intervals to apply, the decoder needs to look at the current value of codice_12 within the interval [low, low+range) and decide which symbol this represents.
For the AABA<EOM> example above, this would return a value in the range 0 to 9. Values 0 through 5 would represent A, 6 and 7 would represent B, and 8 and 9 would represent <EOM>.
Relationship with arithmetic coding.
Arithmetic coding is the same as range encoding, but with the integers taken as being the numerators of fractions. These fractions have an implicit, common denominator, such that all the fractions fall in the range [0,1). Accordingly, the resulting arithmetic code is interpreted as beginning with an implicit "0.". As these are just different interpretations of the same coding methods, and as the resulting arithmetic and range codes are identical, each arithmetic coder is its corresponding range encoder, and vice versa. In other words, arithmetic coding and range encoding are just two, slightly different ways of understanding the same thing.
In practice, though, so-called range "encoders" tend to be implemented pretty much as described in Martin's paper, while arithmetic coders more generally tend not to be called range encoders. An often noted feature of such range encoders is the tendency to perform renormalization a byte at a time, rather than one bit at a time (as is usually the case). In other words, range encoders tend to use bytes as encoding digits, rather than bits. While this does reduce the amount of compression that can be achieved by a very small amount, it is faster than when performing renormalization for each bit.

</doc>
<doc id="46122" url="http://en.wikipedia.org/wiki?curid=46122" title="Karl Brandt">
Karl Brandt

Karl Brandt (January 8, 1904 – June 2, 1948) was a German physician and "Schutzstaffel" (SS) officer during the Third Reich. Trained in surgery, Brandt joined the Nazi Party in 1932 and became Adolf Hitler's escort physician in August 1934. A member of Hitler's inner circle at the Berghof, he was selected by Philipp Bouhler, the head of Hitler's Chancellery, to administer the "Aktion T4" euthanasia program. Brandt was later appointed the Reich Commissioner of Sanitation and Health ("Bevollmächtiger für das Sanitäts und Gesundheitswesen"). Accused of involvement in human experimentation and other war crimes, Brandt was indicted in late-1946 and faced trial before a U.S. military tribunal along with 22 others in "United States of America v. Karl Brandt, et al". He was convicted, sentenced to death, and later hanged on June 2, 1948.
Early life.
Brandt was born in Mulhouse in the then German Alsace-Lorraine territory (now in Haut-Rhin, France) into the family of a Prussian Army officer. He became a medical doctor and surgeon in 1928, specializing in head and spinal injuries. He joined the Nazi Party in January 1932, and first met Hitler in the summer of 1932. He became a member of the SA in 1933 and a member of the SS on July 29, 1934; appointed the officer rank of "Untersturmführer". From the Summer of 1934 forward, he was Hitler's "Escort Physician". Karl Brandt married Anni Rehborn (born 1907), a champion swimmer, on March 17, 1934. They had one son, Karl Adolf Brandt (born October 4, 1935).
Career in the Third Reich.
In the context of the 1933 Nazi law "Gesetz zur Verhütung erbkranken Nachwuchses" (Law for the Prevention of Hereditarily Diseased Offspring), he was one of the medical scientists who performed abortions in great numbers on women deemed genetically disordered, mentally or physically handicapped or racially deficient, or whose unborn fetuses were expected to develop such genetic "defects". These abortions had been legalized, as long as no healthy Aryan fetuses were aborted.
On September 1, 1939, Brandt was appointed by Hitler co-head of the T-4 Euthanasia Program, with Philipp Bouhler. Additional power was afforded Brandt when on July 28, 1942, he was appointed Commissioner of Sanitation and Health ("Bevollmächtiger für das Sanitäts und Gesundheitswesen") by Hitler and was thereafter only bound by the Führer's instructions alone. He received regular promotions in the SS; by April 1944, Brandt was a SS-"Gruppenführer" in the "Allgemeine-SS" and a SS-"Brigadeführer" in the Waffen-SS. On April 16, 1945, he was arrested by the Gestapo for moving his family out of Berlin so they could surrender to American forces. He was condemned to death by a military court and then sent to Kiel. Brandt was released from arrest by order of Karl Dönitz on May 2, 1945. He was later placed under arrest by the British on May 23, 1945.
Brandt's medical ethics.
Brandt's medical ethics, particularly regarding euthanasia, were influenced by Alfred Hoche whose courses he attended. Like many other German doctors of the period, Brandt came to believe that the health of society as a whole should take precedence over that of its individual members. Because society was viewed as an organism that had to be cured, its weakest, most invalid and incurable members were only parts that should be removed. Such hapless creatures should therefore be granted a "merciful death" ("Gnadentod"). In addition to these considerations, Brandt's explanation at his trial for his criminal actions - particularly ordering experimentation on human beings - was that "...Any personal code of ethics must give way to the total character of the war". Historian Horst Freyhofer asserts that, in the absence of at least Brandt's "tacit" approval, it is highly unlikely that the grotesque and cruel medical experiments for which the Nazi doctors are infamous, could have been performed. Brandt and Hitler discussed multiple killing techniques during the initial planning of the euthanasia program, during which Hitler asked Brandt, “which is the most humane way;” Brandt suggested the use of poisonous gas, whereupon the two agreed.
Life in the inner circle.
Karl Brandt and his wife Anni were members of Hitler's inner circle at Berchtesgaden where Hitler maintained his private residence known as the Berghof. This very exclusive group functioned as Hitler's de facto family circle. It included Eva Braun, Albert Speer, his wife Margarete, Dr. Theodor Morell, Martin Bormann, Hitler's photographer Heinrich Hoffmann, Hitler's adjutants and his secretaries. Brandt and Hitler’s chief architect Albert Speer were good friends as the two shared technocratic dispositions about their work. Brandt looked at killing "useless eaters" and the handicapped as a means to an end, namely since it was in the interest of public health. Similarly, Speer viewed the use of concentration camp labor for his defense and building projects in much the same way. As members of this inner circle, the Brandts had a residence near the Berghof and spent extensive time there when Hitler was present. In his memoirs, Speer described the familial but numbing lifestyle of Hitler's intimate companions who were forced to stay up most of the night—night after night—listening to the Nazi leader's repetitive monologues or to an unvarying selection of music. Despite Brandt's personal closeness to Hitler, the dictator was furious when he learned shortly before the end of the war that the doctor had sent Anni and their son toward the American lines in hopes of evading capture by the Russians. Only the intervention of Heinrich Himmler and Albert Speer saved Brandt from execution in the war's closing days.
Trial and execution.
Brandt was tried along with twenty-two others at the Palace of Justice in Nuremberg, Germany. The trial was officially titled "United States of America v. Karl Brandt et al.", but is more commonly referred to as the "Doctors' Trial"; it began on December 9, 1946. He was charged with four counts: 
1) conspiracy to commit war crimes and crimes against humanity as described in counts 2 and 3; 
2) War crimes: performing medical experiments, without the subjects' consent, on prisoners of war and civilians of occupied countries, in the course of which experiments the defendants committed murders, brutalities, cruelties, tortures, atrocities, and other inhuman acts. Also planning and performing the mass murder of prisoners of war and civilians of occupied countries, stigmatized as aged, insane, incurably ill, deformed, and so on, by gas, lethal injections, and diverse other means in nursing homes, hospitals, and asylums during the Euthanasia Program and participating in the mass murder of concentration camp inmates; 
3) Crimes against humanity: committing crimes described under count 2 also on German nationals; 
4) Membership in a criminal organization, the SS. The charges against him included special responsibility for, and participation in, Freezing, Malaria, LOST Gas, Sulfanilamide, Bone, Muscle and Nerve Regeneration and Bone Transplantation, Sea-Water, Epidemic Jaundice, Sterilization, and Typhus Experiments.
After a defense led by Robert Servatius, on August 19, 1947, Brandt was found guilty on counts 2-4 of the indictment. With six others, he was sentenced to death by hanging, and all were executed at Landsberg Prison on June 2, 1948. Nine other defendants received prison terms of between fifteen years and life, while a further seven were found not guilty.
While on the gallows, Brandt remarked: "It is no shame to stand upon the scaffold. This is nothing but political revenge. I have served my Fatherland as others before me...” His speech was cut short when a black hood was placed over his head.
References.
</dl>

</doc>
<doc id="46124" url="http://en.wikipedia.org/wiki?curid=46124" title="ACIS">
ACIS

The 3D ACIS Modeler (ACIS) is a geometric modeling kernel developed by Spatial Corporation (formerly Spatial Technology), part of Dassault Systemes. ACIS is used by many software developers in industries such as computer-aided design (CAD), computer-aided manufacturing (CAM), computer-aided engineering (CAE), architecture, engineering and construction (AEC), coordinate-measuring machine (CMM), 3D animation, and shipbuilding. ACIS provides software developers and manufacturers the underlying 3D modeling functionality.
ACIS features an open, object-oriented C++ architecture that enables robust, 3D modelling capabilities. ACIS is used to construct applications with hybrid modeling features, since it integrates wireframe model, surface, and solid modeling functionality with both manifold and non-manifold topology, and a rich set of geometric operations.
History.
As a geometric kernel, ACIS is a second generation system, coming after the first generation Romulus
There are several versions about what the word ACIS actually stands for, or whether it is an acronym at all. The most popular version is that ACIS stands for "Alan, Charles, Ian's System" (Alan Grayer, Charles Lang and Ian Braid as part of Three-Space Ltd.), or "Alan, Charles, Ian and Spatial" (as the system was later on sold to Spatial Technology, now Spatial Corp). However, when asked, the creators of ACIS would simply suggest that its name was derived from Greek mythology (See also Acis).
In 1985 Charles Lang and Ian Braid (creators of Romulus and Romulus-D) formed Three-Space Ltd. (Cambridge, England) which had been retained by Dick Sowar's Spatial Technology (which had been founded by Sowar in 1986) to develop the ACIS solid modeling kernel for Spatial Technology's Strata CAM software. The first version of ACIS was released in 1989 and was quickly licensed by HP for integration into its ME CAD software.
In late 2000, around the time when Spatial was acquired by Dassault Systemes, the ACIS file format changed slightly and was no longer openly published.
Architecture.
A software component is a functionally specialized unit of software—a collection of software items (functions, classes, etc.) grouped together to serve some distinct purpose. It serves as a constituent part of a whole software system or product. A product is one or more software components that are assembled together and sold as a package. Components can be arranged in different combinations to form different products.
The ACIS product line is designed using software component technology, which allows an application to use only the components it requires. In some cases, more than one component is available (either from Spatial or third party vendors) for a given purpose, so application developers can use the component that best meets their needs. For example, several rendering components are available from Spatial, and developers use the one that works best for their platform or application.
Functionality.
ACIS Modeler.
ACIS core functionality can be subclassified into three categories, namely:
File format.
ACIS saves modeling information to external files which have an open format allowing external applications, even those not based on ACIS, access to the ACIS geometric model. The basic information needed to understand the ACIS file format (focusing on the reading, or restore, operation), includes the structure of the save file format, how data is encapsulated, the types of data written, and subtypes and references.
Save File Types.
ACIS supports two kinds of save files, Standard ACIS Text (SAT), and Standard ACIS Binary (SAB). The two formats store identical information, so the term SAT file is generally used to refer to either when no distinction is needed. 
In the narrow sense, SAT files are ASCII text files that may be viewed with a simple text editor. A SAT file contains carriage returns, white space and other formatting that makes it readable to the human eye. A SAT file has a .sat file extension. 
SAB files cannot be viewed with a simple text editor and are meant for compactness and not for human readability. A SAB file has a .sab file extension. A SAB file uses delimiters between elements and binary tags, without additional formatting.
Structure of the Save File.
A save file contains:
Beginning with ACIS Release 6.3, it is required that the product ID and units be populated for the file header before you can save a SAT file.
Version Numbers and ACIS Releases.
ACIS is currently being developed by Spatial. They maintain the concept of a current version (release) number in ACIS, as well as a save version number. The save version allows one to create a SAT save file that can be read by a previous version of ACIS.
Beginning with ACIS Release 4.0, the SAT save file format does not change with minor releases, only with major releases. This allows applications that are based upon the same major version of ACIS to exchange data without being concerned about the save version. To provide this interoperability in a simple implementation, ACIS save files have contained a symbol that accurately identified the major version number, but not the minor version. This meant that applications created using the same major version of ACIS would produce compatible save files, regardless of their minor versions. This was accomplished by simply not incrementing the internal minor version number between major versions.
Beginning with Release 7.0, ACIS started again providing accurate major, minor, and point version numbers.
To summarize how release numbers and SAT changes are related:
Adoption.
In 2013 the following software uses ACIS as its geometric kernel/engine: AutoCAD,
SpaceClaim
References.
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.

</doc>
<doc id="46126" url="http://en.wikipedia.org/wiki?curid=46126" title="Sulfur mustard">
Sulfur mustard

The sulfur mustards, or sulphur mustards, commonly known as mustard gas, is a class of related cytotoxic and vesicant chemical warfare agents with the ability to form large blisters on the exposed skin and in the lungs. Pure sulfur mustards are colorless, viscous liquids at room temperature. When used in impure form, such as warfare agents, they are usually yellow-brown in color and have an odor resembling mustard plants, garlic, or horseradish, hence the name. Mustard gas was originally assigned the name "LOST," after the scientists "Herren Doktoren" Wilhelm Lommel and Wilhelm Steinkopf, who developed a method for the large-scale production of mustard gas for the Imperial German Army in 1916.
Mustard agents are regulated under the 1993 Chemical Weapons Convention (CWC). Three classes of chemicals are monitored under this Convention, with sulfur and nitrogen mustard grouped in Schedule 1, as substances with no use other than in chemical warfare. Mustard agents could be deployed on the battlefield by means of artillery shells, aerial bombs, rockets, or by spraying from warplanes.
Synthesis.
Sulfur mustard is the organic compound with formula (Cl-CH2CH2)2S. In the Depretz method, sulfur mustard is synthesized by treating sulfur dichloride with ethylene:
In the Levinstein process, disulfur dichloride is used instead:
In the Meyer method, thiodiglycol is produced from chloroethanol and potassium sulfide and chlorinated with phosphorus trichloride:
In the Meyer-Clarke method, concentrated hydrochloric acid (HCl) instead of PCl3 is used as the chlorinating agent:
Thionyl chloride and phosgene, the latter of which (CG) is also a choking agent, have also been used as chlorinating agents.
It is a viscous liquid at normal temperatures. The pure compound has a melting point of 14 °C (57 °F) and decomposes before boiling at 218 °C (424.4 °F).
Mechanism of toxicity.
The compound readily eliminates a chloride ion by intramolecular nucleophilic substitution to form a cyclic sulfonium ion. This very reactive intermediate tends to cause permanent alkylation of the guanine nucleotide in DNA strands, which prevents cellular division and generally leads directly to programmed cell death, or, if cell death is not immediate, the damaged DNA may lead to the development of cancer. Oxidative stress would be another pathology involved in sulfur mustard toxicity. Sulfur mustard is not very soluble in water but is very soluble in fat, contributing to its rapid absorption into the skin.
In the wider sense, compounds with the structural element BCH2CH2X, where "X" is any leaving group and "B" is a Lewis base are known as "mustards." Such compounds can form cyclic "onium" ions (sulfonium, ammoniums, etc.) that are good alkylating agents. Examples are bis(2-chloroethyl)ether, the (2-haloethyl)amines (nitrogen mustards), and sulfur sesquimustard, which has two α-chloroethyl thioether groups (ClH2C-CH2-S-) connected by an ethylene (-CH2CH2-) group. These compounds have a similar ability to alkylate DNA, but their physical properties, e.g. melting point, vary.
Physiological effects.
Mustard gas has extremely powerful vesicant effects on its victims. In addition, it is strongly mutagenic and carcinogenic, due to its alkylating properties. It is also lipophilic. Because people exposed to mustard gas rarely suffer immediate symptoms, and mustard-contaminated areas may appear completely normal, victims can unknowingly receive high dosages. Within 24 hours of exposure to mustard agent, victims experience intense itching and skin irritation, which gradually turns into large blisters filled with yellow fluid wherever the mustard agent contacted the skin. These are chemical burns and are very debilitating. Mustard gas vapour easily penetrates clothing fabrics such as wool or cotton, so it is not only the exposed skin of victims that gets burned. If the victim's eyes were exposed then they become sore, starting with conjunctivitis, after which the eyelids swell, resulting in temporary blindness. In rare cases of extreme ocular exposure to sulfur mustard vapors, corneal ulceration, anterior chamber scarring, and neovascularization have occurred. In these severe and infrequent cases, corneal transplantation has been used as a treatment option. Miosis may also occur, which is probably the result from the cholinomimetic activity of mustard. At very high concentrations, if inhaled, mustard agent causes bleeding and blistering within the respiratory system, damaging mucous membranes and causing pulmonary edema. Depending on the level of contamination, mustard gas burns can vary between first and second degree burns, though they can also be every bit as severe, disfiguring and dangerous as third degree burns. Severe mustard gas burns (i.e. where more than 50% of the victim's skin has been burned) are often fatal, with death occurring after some days or even weeks have passed. Mild or moderate exposure to mustard agent is unlikely to kill, though victims require lengthy periods of medical treatment and convalescence before recovery is complete.
The mutagenic and carcinogenic effects of mustard agent mean that victims who recover from mustard gas burns have an increased risk of developing cancer in later life. In a study of patients 25 years after wartime exposure to chemical weaponry, c-DNA microarray profiling indicated that a total of specific 122 genes were significantly mutated in the lungs and airways of sulfur mustard victims. Those genes all correspond to functions commonly affected by sulfur mustard exposure, including apoptosis, inflammation, and stress responses.
The vesicant property of mustard gas can be neutralised by oxidation or chlorination, using household bleach (sodium hypochlorite), or by nucleophilic attack using e.g. decontamination solution "DS2" (2% NaOH, 70% diethylenetriamine, 28% ethylene glycol monomethyl ether). After initial decontamination of the victim's wounds is complete, medical treatment is similar to that required by any conventional burn. The amount of pain and discomfort suffered by the victim is comparable as well. Mustard gas burns heal slowly, and, as with other types of burn, there is a risk of sepsis caused by pathogens such as "Staphylococcus aureus" and "Pseudomonas aeruginosa". The mechanisms behind sulfur mustard’s effect on endothelial cells are still being studied, but recent studies have shown that high levels of exposure can induce high rates of both necrosis and apoptosis. In vitro tests have shown that at low concentrations of sulfur mustard, where apoptosis is the predominant result of exposure, pretreatment with 50 mM N-acetyl-L-cystein (NAC) was able to decrease the rate of apoptosis. NAC protects actin filaments from reorganization by sulfur mustard, demonstrating that actin filaments play a large role in the severe burns observed in victims.
A British nurse treating soldiers with mustard gas burns during World War I commented:
They cannot be bandaged or touched. We cover them with a tent of propped-up sheets. Gas burns must be agonizing because usually the other cases do not complain, even with the worst wounds, but gas cases are invariably beyond endurance and they cannot help crying out.
Formulations.
In its history, various types and mixtures of sulfur mustard have been employed. These include:
Sulfur mustard agents (class).
The complete list of effective sulfur mustard agents commonly stockpiled is as follows:
History.
Development.
Mustard gas was possibly developed as early as 1822 by César-Mansuète Despretz (1798–1863). Despretz described the reaction of sulfur dichloride and ethylene but never made mention of any irritating properties of the reaction product, which makes the claim doubtful. In 1854, another French chemist, Alfred Riche (1829–1908), repeated this procedure but he did not describe any adverse physiological properties. In 1860, the British scientist Frederick Guthrie synthesized and characterized the mustard gas compound, and he also noted its irritating properties, especially in tasting. In 1860, chemist Albert Niemann, known as a pioneer in cocaine chemistry, repeated the reaction, and recorded blister-forming properties. In 1886, Viktor Meyer published a paper describing a synthesis that produced good yields. He combined 2-chloroethanol with aqueous potassium sulfide, and then treated the resulting thiodiglycol with phosphorus trichloride. The purity of this compound was much higher, and so were the adverse health effects on exposure much more severe. These symptoms presented themselves in his assistant, and in order to rule out the possibility that his assistant was suffering from a mental illness (psychosomatic symptoms), Meyer had this compound tested on laboratory rabbits, most of which died. In 1913, the English chemist Hans Thacher Clarke (known for the Eschweiler-Clarke reaction) replaced the phosphorus trichloride with hydrochloric acid in Meyer's formulation while working with Emil Fischer in Berlin. Clarke was hospitalized for two months for burns after one of his flasks broke. According to Meyer, Fischer's report on this accident to the German Chemical Society sent the German Empire on the road to chemical weapons. The German Empire during World War I relied on the Meyer-Clarke method with the 2-chloroethanol chemical structure already available from the German chemical dye industry of that time.
Use.
Mustard gas was first used effectively in World War I by the German army against British and Canadian soldiers near Ypres, Belgium, in 1917 and later also against the French Second Army. The name Yperite comes from its usage by the German army near the town of Ypres. The Allies did not use mustard gas until November 1917 at Cambrai, France, after the armies had captured a stockpile of German mustard-gas shells. It took the British more than a year to develop their own mustard gas weapon, with production of the chemicals centred on Avonmouth Docks. (The only option available to the British was the Despretz–Niemann–Guthrie process). This was used first in September 1918 during the breaking of the Hindenburg Line.
Mustard gas was dispersed as an aerosol in a mixture with other chemicals, giving it a yellow-brown color and a distinctive odor. Mustard gas has also been dispersed in such munitions as aerial bombs, land mines, mortar rounds, artillery shells, and rockets. Exposure to mustard gas was lethal in about one percent of cases. Its effectiveness was as an incapacitating agent. The early countermeasures against mustard gas were relatively ineffective, since a soldier wearing a gas mask was not protected against absorbing it through his skin and being blistered.
Mustard gas is a persistent weapon that remains on the ground for days and weeks, and it continues to cause ill effects. If mustard gas contaminates a soldier's clothing and equipment, then the other soldiers that he comes into contact with are also poisoned. Towards the end of World War I, mustard gas was used in high concentrations as an area-denial weapon that forced troops to abandon heavily-contaminated areas.
Since World War I, mustard gas has been used in several wars or other conflicts, usually against people who cannot retaliate in kind:
In 1943, during the Second World War, an American shipment of mustard gas exploded aboard a supply ship that was bombed during an air raid in the harbor of Bari, Italy. Eighty-three of the 628 hospitalized victims who had been exposed to the mustard gas died. The deaths and incident were partially classified for many years.
From 1943 to 1944, mustard gas experiments were performed on Australian service volunteers in tropical Queensland, Australia, by British Army and American experimenters, resulting in some severe injuries. One test site, the Brook Islands National Park, was chosen to simulate Pacific islands held by the Imperial Japanese Army.
After WWII stockpiled mustard gas was dumped by the British in the sea near Port Elizabeth, South Africa, resulting in burn cases among trawler crews.
The use of poison gases, including mustard gas, during warfare is known as chemical warfare, and this kind of warfare was prohibited by the Geneva Protocol of 1925, and also by the later Chemical Weapons Convention of 1993. The latter agreement also prohibits the development, production, stockpiling, and sale of such weapons.
Development of the first chemotherapy drug.
As early as 1919 it was known that mustard gas was a suppressor of hematopoiesis. In addition, autopsies performed on 75 soldiers who had died of mustard gas during World War I were done by researchers from the University of Pennsylvania who reported decreased counts of white blood cells. This led the American Office of Scientific Research and Development (OSRD) to finance the biology and chemistry departments at Yale University to conduct research on the use of chemical warfare during World War II. As a part of this effort, the group investigated nitrogen mustard as a therapy for Hodgkin's lymphoma and other types of lymphoma and leukemia, and this compound was tried out on its first human patient in December 1942. The results of this study were not published until 1946, when they were declassified. In a parallel track, after the air raid on Bari in December 1943, the doctors of the U.S. Army noted that white blood cell counts were reduced in their patients. Some years after World War II was over, the incident in Bari and the work of the Yale University group with nitrogen mustard converged, and this prompted a search for other similar chemical compounds. Due to its use in previous studies, the nitrogen mustard called "HN2" became the first cancer chemotherapy drug, mustine, to be used.
Disposal.
Most of the sulfur mustard gas found in Nazi Germany after World War II was dumped into the Baltic Sea. Between 1966 and 2002, fishermen have found about 700 chemical weapons in the region of Bornholm, most of which contain sulfur mustard. One of the more frequently-dumped weapons was the "Sprühbüchse 37" (SprüBü37, Spray Can 37, 1937 being the year of its fielding with the German Army). These weapons contain sulfur mustard mixed with a thickener, which gives it a tar-like viscosity. When the content of the SprüBü37 comes in contact with water, only the sulfur mustard in the outer layers of the lumps of viscous mustard hydrolyzes, leaving behind amber-colored residues that still contain most of the active sulfur mustard. On mechanically breaking these lumps, e.g., with the drag board of a fishing net or by the human hand, the enclosed sulfur mustard is still as active as it had been at the time the weapon was dumped. These lumps, when washed ashore, can be mistaken for amber, which can lead to severe health problems. Artillery shells containing sulfur mustard and other toxic ammunition from World War I (as well as conventional explosives) can still be found in France and Belgium. These were formerly disposed of by explosion undersea, but since the current environmental regulations prohibit this, the French government is building an automated factory to dispose of the accumulation of chemical shells.
In 1972, the U.S. Congress banned the practice of disposing of chemical weapons into the ocean by the United States. 64 million pounds of nerve and mustard agents had already been dumped into the ocean off the United States by the U.S. Army. According to a report created in 1998 by William Brankowitz, a deputy project manager in the U.S. Army Chemical Materials Agency, the army created at least 26 chemical weapons dumping sites in the ocean offshore from at least 11 states on both the East Coast and the West Coast (in Operation CHASE, Operation Geranium, etc.). In addition, due to poor recordkeeping, about one-half of the sites have only their rough locations known.
A significant portion of the stockpile of mustard agent in the United States was stored at the Edgewood Area of Aberdeen Proving Ground in Maryland. Approximately 1,621 tons of mustard agent were stored in one-ton containers on the base under heavy guard. An incineration plant built on the proving ground neutralized the last of this stockpile in February 2005. This stockpile had priority because of the potential for quick reduction of risk to the community. The nearest schools were fitted with overpressurization machinery to protect the students and faculty in the event of a catastrophic explosion and fire at the site. These projects, as well as planning, equipment, and training assistance, were provided to the surrounding community as a part of the Chemical Stockpile Emergency Preparedness Program (CSEPP), a joint program of the Army and the Federal Emergency Management Agency (FEMA). Unexploded shells containing mustard agent and other chemical agents are still present in several test ranges in proximity to schools in the Edgewood area, but the smaller amounts of poison gas (four to 14 pounds) present considerably lower risks. These remnants are being detected and excavated systematically for disposal. The U.S. Army Chemical Materials Agency oversaw disposal of several other chemical weapons stockpiles located across the United States in compliance with international chemical weapons treaties. These include the complete incineration of the chemical weapons stockpiled in Alabama, Arkansas, Indiana, and Oregon. Earlier, this agency had also completed destruction of the chemical weapons stockpile located on Johnston Atoll located south of Hawaii in the Pacific Ocean. The largest mustard gas stockpile, of about 6,196 tons, was stored at the Deseret Chemical Depot in northern Utah. The incineration of this stockpile began in 2006. In May 2011, the last one-ton tank of mustard gas was incinerated at the Deseret Chemical Depot, and the last mustard gas artillery shells at Deseret were incinerated in January 2012.
The storage and incineration of mustard gas and other poison gases was carried out by the U.S. Army Chemical Materials Agency. Disposal projects at the two remaining American chemical weapons sites, will be carried out at their sites near Richmond, Kentucky, and Pueblo, Colorado.
In 2002, an archaeologist at the Presidio Trust archaeology lab in San Francisco was exposed to mustard gas, which had been dug up at the Presidio of San Francisco, a former military base.
In 2008, many empty mustard gas aerial bombs were found in an excavation at the Marrangaroo Army Base just west of Sydney, Australia. In 2009, a mining survey near Chinchilla, Queensland, uncovered 144 105-millimeter howitzer shells, some containing "Mustard H", that had been buried by the U.S. Army during World War II.
In 2010, a clamming boat pulled up some old artillery shells of World War I from the Atlantic Ocean south of Long Island, New York. Multiple fishermen suffered from skin blistering and respiratory irritation severe enough to require their hospitalization.
A large British stockpile of old mustard gas that had been made and stored at M. S. Factory, Valley near Rhydymwyn in Flintshire, Wales, since World War I was destroyed in 1958.
In 2014, a collection of 200 bombs were found on the boundary between the Flemish villages of Passendale and Moorslede. The majority of the bombs were filled with mustard gas. The bombs are a leftover from the German army and were meant to be used in the Battle of Passchendale in World War I. It was the largest collection of chemical weapons ever found in Belgium.
New detection techniques are being developed in order to detect the presence of sulfur mustard and its metabolites. The technology is portable and detects small quantities of the hazardous waste and its oxidized products, which are notorious for harming unsuspecting civilians. The immunochromatographic assay would eliminate the need for expensive, time-consuming lab tests and enable easy-to-read tests to protect civilians from sulfur-mustard dumping sites.
Detection in biological fluids.
Urinary concentrations of the thiodiglycol hydrolysis products of sulfur mustard have been used to confirm a diagnosis of chemical poisoning in hospitalized victims. The presence in urine of 1,1'-sulfonylbismethylthioethane (SBMTE), a conjugation product with glutathione, is considered a more specific marker, since this metabolite is not found in specimens from unexposed persons. Intact sulfur mustard was detected in postmortem fluids and tissues of a man who died one week post-exposure.

</doc>
<doc id="46127" url="http://en.wikipedia.org/wiki?curid=46127" title="Robert Tarjan">
Robert Tarjan

Robert Endre Tarjan (born April 30, 1948) is an American computer scientist and mathematician. He is the discoverer of several graph algorithms, including Tarjan's off-line least common ancestors algorithm, and co-inventor of both splay trees and Fibonacci heaps. Tarjan is currently the James S. McDonnell Distinguished University Professor of Computer Science at Princeton University, and the Chief Scientist at Intertrust Technologies. 
Early life and education.
He was born in Pomona, California. His father was a child psychiatrist specializing in mental retardation, and ran a state hospital. As a child, Tarjan read a lot of science fiction, and wanted to be an astronomer. He became interested in mathematics after reading Martin Gardner's mathematical games column in Scientific American. He became seriously interested in math in the eighth grade, thanks to a "very stimulating" teacher.
While he was in high school, Tarjan got a job, where he worked IBM card punch collators. He first worked with real computers while studying astronomy at the Summer Science Program in 1964.
Tarjan obtained a Bachelor's degree in mathematics from the California Institute of Technology in 1969. At Stanford University, he received his Master's degree in computer science in 1971 and a Ph.D. in computer science (with a minor in mathematics) in 1972. At Stanford, he was supervised by Robert Floyd and Donald Knuth, both highly prominent computer scientists, and his Ph.D. dissertation was "An Efficient Planarity Algorithm". Tarjan selected computer science as his area of interest because he believed that CS was a way of doing mathematics that could have a practical impact.
Computer science career.
Tarjan has been teaching at Princeton University since 1985. He has also held academic positions at Cornell University (1972–73), University of California, Berkeley (1973–1975), Stanford University (1974–1980), and New York University (1981–1985). He has also been a fellow of the NEC Research Institute (1989–1997). In April 2013 he joined Microsoft Research Silicon Valley in addition to the position at Princeton. In October 2014 he rejoined Intertrust Technologies as chief scientist. 
Tarjan has worked at AT&T Bell Labs (1980–1989), Intertrust Technologies (1997–2001, 2014–present), Compaq (2002) and Hewlett Packard (2006–2013).
Algorithms and data structures.
Tarjan is known for his pioneering work on graph theory algorithms and data structures. Some of his well-known algorithms include Tarjan's off-line least common ancestors algorithm, and Tarjan's strongly connected components algorithm, and he was one of five co-authors of the median of medians linear time selection algorithm. The Hopcroft-Tarjan planarity testing algorithm was the first linear-time algorithm for planarity-testing.
Tarjan has also developed important data structures such as the Fibonacci heap (a heap data structure consisting of a forest of trees), and the splay tree (a self-adjusting binary search tree; co-invented by Tarjan and Daniel Sleator). Another significant contribution was the analysis of the disjoint-set data structure; he was the first to prove the optimal runtime involving the inverse Ackermann function.
Awards.
Tarjan received the Turing Award jointly with John Hopcroft in 1986. The citation for the award states that it was:
For fundamental achievements in the design and analysis of algorithms and data structures.
Tarjan was also elected an ACM Fellow in 1994. The citation for this award states:
For seminal advances in the design and analysis of data structures and algorithms.
Some of the other awards for Tarjan include:
Patents.
Tarjan holds at least 18 U.S. patents. These include:

</doc>
<doc id="46128" url="http://en.wikipedia.org/wiki?curid=46128" title="Invasion of Normandy">
Invasion of Normandy

The Invasion of Normandy was the invasion by and establishment of Western Allied forces in Normandy, during Operation Overlord in 1944 during World War II; the largest amphibious invasion to ever take place.
D-Day, the day of the initial assaults, was Tuesday 6 June 1944. Allied land forces that saw combat in Normandy on that day came from Canada, the Free French forces, the United Kingdom, and the United States. In the weeks following the invasion, Polish forces also participated, as well as contingents from Belgium, Czechoslovakia, Greece, and the Netherlands. Most of the above countries also provided air and naval support, as did the Royal Australian Air Force, the Royal New Zealand Air Force, and the Royal Norwegian Navy.
The Normandy invasion began with overnight parachute and glider landings, massive air attacks and naval bombardments. In the early morning, amphibious landings on five beaches codenamed Juno, Gold, Omaha, Utah, and Sword began and during the evening the remaining elements of the parachute divisions landed. Land forces used on D-Day deployed from bases along the south coast of England, the most important of these being Portsmouth.
Planning of the Invasion.
Allied forces rehearsed their D-Day roles for months before the invasion. On 28 April 1944, in south Devon on the English coast, 638 U.S. soldiers and sailors were killed when German torpedo boats surprised one of these landing exercises, Exercise Tiger.
In the months leading up to the invasion, the Allied forces conducted a deception operation, Operation Fortitude, aimed at misleading the Germans with respect to the date and place of the invasion.
There were several leaks prior to or on D-Day. Through the Cicero affair, the Germans obtained documents containing references to Overlord, but these documents lacked all detail. Double Cross agents, such as the Spaniard Joan Pujol (code named Garbo), played an important role in convincing the German High Command that Normandy was at best a diversionary attack. U.S. Major General Henry Miller, chief supply officer of the US 9th Air Force, during a party at Claridge's Hotel in London complained to guests of the supply problems he was having but that after the invasion, which he told them would be before 15 June, supply would be easier. After being told, Eisenhower reduced Miller to lieutenant colonel [Associated Press, June 10, 1944] and sent him back to the U.S. where he retired. Another such leak was General Charles de Gaulle's radio message after D-Day. He, unlike all the other leaders, stated that this invasion was the real invasion. This had the potential to ruin the Allied deceptions Fortitude North and Fortitude South. In contrast, Gen. Eisenhower referred to the landings as the initial invasion.
Only ten days each month were suitable for launching the operation: a day near the full moon was needed both for illumination during the hours of darkness and for the , the former to illuminate navigational landmarks for the crews of aircraft, gliders and landing craft, and the latter to expose defensive obstacles placed by the German forces in the surf on the seaward approaches to the beaches. A full moon occurred on 6 June. Allied Expeditionary Force Supreme Commander Dwight D. Eisenhower had tentatively selected 5 June as the date for the assault. The weather was fine during most of May, but deteriorated in early June. On 4 June, conditions were clearly unsuitable for a landing; wind and high seas would make it impossible to launch landing craft from larger ships at sea, low clouds would prevent aircraft finding their targets. The Allied troop convoys already at sea were forced to take shelter in bays and inlets on the south coast of Britain for the night.
It seemed possible that everything would have to be cancelled and the troops returned to their embarkation camps (which would be almost impossible, as the enormous movement of follow-up formations into them was already proceeding). The next full moon period would be nearly a month away. At a vital meeting on 5 June, Eisenhower's chief meteorologist (Group Captain J.M. Stagg) forecast a brief improvement for 6 June. Commander of all land forces for the invasion General Bernard Montgomery and Eisenhower's Chief of Staff General Walter Bedell Smith wished to proceed with the invasion. Commander of the Allied Air Forces Air Chief Marshal Leigh Mallory was doubtful, but Allied Naval Commander-in-Chief Admiral Bertram Ramsay believed that conditions would be marginally favorable. On the strength of Stagg's forecast, Eisenhower ordered the invasion to proceed. As a result, prevailing overcast skies limited Allied air support, and no serious damage would be done to the beach defences on Omaha and Juno.
The Germans meanwhile took comfort from the existing poor conditions, which were worse over Northern France than over the English Channel itself, and believed no invasion would be possible for several days. Some troops stood down and many senior officers were away for the weekend. Field Marshal Erwin Rommel took a few days' leave to celebrate his wife's birthday, while dozens of division, regimental and battalion commanders were away from their posts conducting war games just prior to the invasion.
Codenames.
The Allies assigned codenames to the various operations involved in the invasion. "Overlord" was the name assigned to the establishment of a large-scale lodgement on the northern portion of the Continent. The first phase, the establishment of a secure foothold, was codenamed "Neptune". According to the D-day museum:
Officers with knowledge of D-Day were not to be sent where there was the slightest danger of being captured. These officers were given the codename of "Bigot", derived from the words "To Gib" (To Gibraltar) that was stamped on the papers of officers who took part in the North African invasion in 1942. On the night of 27 April, during Exercise Tiger, a pre-invasion exercise off the coast of Slapton Sands beach, several American LSTs were attacked by German E boats and among the 638 Americans killed in the attack and a further 308 killed by friendly fire, ten "Bigots" were listed as missing. As the invasion would be cancelled if any were captured or unaccounted for, their fate was given the highest priority and eventually all ten bodies were recovered.
Allied order of battle.
D-Day.
The following major units were landed on D-Day (6 June 1944). A more detailed order of battle for D-Day itself can be found at Normandy landings.
The total number of troops landed on D-Day was around 130,000–156,000 roughly half American and the other from the Commonwealth Realms.
Subsequent days.
The total troops, vehicles and supplies landed over the period of the invasion were:
Naval participants.
The invasion fleet was drawn from eight different navies, comprising 6,939 vessels: 1,213 warships, 4,126 transport vessels (landing ships and landing craft), and 736 ancillary craft and 864 merchant vessels.
The overall commander of the Allied Naval Expeditionary Force, providing close protection and bombardment at the beaches, was Admiral Sir Bertram Ramsay. The Allied Naval Expeditionary Force was divided into two Naval Task Forces: Western (Rear-Admiral Alan G Kirk) and Eastern (Rear-Admiral Sir Philip Vian).
The warships provided cover for the transports against the enemy—whether in the form of surface warships, submarines, or as an aerial attack—and gave support to the landings through shore bombardment. These ships included the Allied Task Force "O".
German order of battle.
The number of military forces at the disposal of Nazi Germany reached its peak during 1944. Tanks on the east front peaked at 5,202 in November 1944, while total aircraft in the Luftwaffe inventory peaked at 5,041 in December 1944. By D-Day 157 German divisions were stationed in the Soviet Union, 6 in Finland, 12 in Norway, 6 in Denmark, 9 in Germany, 21 in the Balkans, 26 in Italy and 59 in France, Belgium and the Netherlands. However, these statistics are somewhat misleading since a significant number of the divisions in the east were depleted; German records indicate that the average personnel complement was at about 50% in the spring of 1944.
A more detailed order of battle for D-Day itself can be found at Normandy landings.
Atlantic Wall.
Standing in the way of the Allies was the English Channel, a crossing which had eluded the Spanish Armada and Napoleon Bonaparte's Navy. Compounding the invasion efforts was the extensive Atlantic Wall, ordered by Hitler in his Directive 51. Believing that any forthcoming landings would be timed for high tide (this caused the landings to be timed for low tide), Hitler had the entire wall fortified with tank top turrets and extensive barbed wire, and laid a million mines to deter landing craft. The sector which was attacked was guarded by four divisions.
Divisional areas.
The following units were deployed in a static defensive mode in the areas of the actual landings:
Adjacent divisional areas.
Other divisions occupied the areas around the landing zones, including:
Armoured reserves.
Rommel's defensive measures were also frustrated by a dispute over armoured doctrine. In addition to his two army groups, von Rundstedt also commanded the headquarters of "Panzer Group West" under General Leo Geyr von Schweppenburg (usually referred to as "von Geyr"). This formation was nominally an administrative HQ for von Rundstedt's armoured and mobile formations, but it was later to be renamed Fifth Panzer Army and brought into the line in Normandy. Von Geyr and Rommel disagreed over the deployment and use of the vital Panzer divisions.
Rommel recognised that the Allies would possess air superiority and would be able to harass his movements from the air. He therefore proposed that the armoured formations be deployed close to the invasion beaches. In his words, it was better to have one Panzer division facing the invaders on the first day, than three Panzer divisions three days later when the Allies would already have established a firm beachhead. Von Geyr argued for the standard doctrine that the Panzer formations should be concentrated in a central position around Paris and Rouen, and deployed "en masse" against the main Allied beachhead when this had been identified.
The argument was eventually brought before Hitler for arbitration. He characteristically imposed an unworkable compromise solution. Only three Panzer divisions were given to Rommel, too few to cover all the threatened sectors. The remainder, nominally under Von Geyr's control, were actually designated as being in "OKW Reserve". Only three of these were deployed close enough to intervene immediately against any invasion of Northern France; the other four were dispersed in southern France and the Netherlands. Hitler reserved to himself the authority to move the divisions in OKW Reserve, or commit them to action. On 6 June many Panzer division commanders were unable to move because Hitler had not given the necessary authorisation, and his staff refused to wake him upon news of the invasion.
Army Group B reserve.
The other two armoured divisions over which Rommel had operational control, the 2nd Panzer Division and 116th Panzer Division, were deployed near the Pas de Calais in accordance with German views about the likely Allied landing sites. Neither was moved from the Pas de Calais for at least fourteen days after the invasion.
OKW reserve.
The other mechanized divisions capable of intervening in Normandy were retained under the direct control of the German Armed Forces HQ (OKW) and were initially denied to Rommel:
Four divisions were deployed to Normandy within seven days of the invasion:
Three other divisions (the 2nd SS Panzer Division Das Reich, which had been refitting at Montauban in Southern France, and the 9th SS Panzer Division Hohenstaufen and 10th SS Panzer Division Frundsberg which had been in transit from the Eastern Front on 6 June), were committed to battle in Normandy around twenty-one days after the first landings.
One more armoured division (the 9th Panzer Division) saw action only after the American breakout from the beachhead. Two other armoured divisions which had been in the west on 6 June (the 11th Panzer Division and 19th Panzer Division) did not see action in Normandy.
Allied establishment in France.
The Allied invasion plans had called for the capture of Saint-Lô, Caen, and Bayeux on the first day, with all the beaches linked except Utah, and Sword (the last linked with paratroopers) and a front line 10 to 16 kilometres (6–10 mi) from the beaches. However, practically none of these objectives had been achieved. It took two months for British and Canadian troops to capture Caen, as they faced 7 heavy Panzer divisions, while their American allies, although advancing more rapidly, faced only 2 of these divisions. Overall the casualties had not been as heavy as some had feared (around 10,000 compared to the 20,000 Churchill had estimated) and the bridgeheads had withstood the expected counterattacks.
Once the beachhead was established, two artificial Mulberry harbours were towed across the English Channel in segments and made operational around D+3 (9 June). One was constructed at Arromanches by British forces, the other at Omaha Beach by American forces. By 19 June, when severe storms interrupted the landing of supplies for several days and destroyed the Omaha harbour, the British had landed 314,547 men, 54,000 vehicles, and 102,000 tons of supplies, while the Americans put ashore 314,504 men, 41,000 vehicles, and 116,000 tons of supplies. Around 9,000 tons of materiel were landed daily at the Arromanches harbour until the end of August 1944, by which time the port of Cherbourg had been secured by the Allies and had begun to return to service.
In addition, with the installation of PLUTO in August 1944 the Allies had fuel piped over directly from England without having to rely on vulnerable tankers.
Assessment of the battle.
The Normandy landings were the first successful opposed landings across the English Channel in over eight centuries. They were costly in terms of men, but the defeat inflicted on the Germans was one of the largest of the war. Strategically, the campaign led to the loss of the German position in most of France and the secure establishment of a new major front. In larger context the Normandy landings helped the Soviets on the Eastern Front, who were facing the bulk of the German forces and, to a certain extent, contributed to the shortening of the conflict there.
Although there was a shortage of artillery ammunition, at no time were the Allies critically short of any necessity. This was a remarkable achievement considering they did not hold a port until Cherbourg fell. By the time of the breakout the Allies also enjoyed a considerable superiority in numbers of troops (approximately 7:2) and armoured vehicles (approximately 4:1) which helped overcome the natural advantages the terrain gave to the German defenders.
Allied intelligence and counterintelligence efforts were successful beyond expectations. The Operation Fortitude deception before the invasion kept German attention focused on the Pas de Calais, and indeed high-quality German forces were kept in this area, away from Normandy, until July. Prior to the invasion, few German reconnaissance flights took place over Britain, and those that did saw only the dummy staging areas. Ultra decrypts of German communications had been helpful as well, exposing German dispositions and revealing their plans such as the Mortain counterattack.
Allied air operations also contributed significantly to the invasion, via close tactical support, interdiction of German lines of communication (preventing timely movement of supplies and reinforcements—particularly the critical Panzer units), and rendering the Luftwaffe ineffective in Normandy. Although the impact upon armoured vehicles was less than expected, air activity intimidated these units and cut their supplies.
Despite initial heavy losses in the assault phase, Allied morale remained high. Casualty rates among all the armies were tremendous, and the Commonwealth forces had to use a recently created category—Double Intense—to be able to describe them.
German leadership.
German commanders at all levels failed to react to the assault phase in a timely manner. Communications problems exacerbated the difficulties caused by Allied air and naval firepower. Local commanders also seemed incapable of the task of fighting an aggressive defense on the beach, as Rommel had envisioned.
The German High Command remained fixated on the Calais area, and von Rundstedt was not permitted to commit the armoured reserve. When it was finally released late in the day, any chance of success was much more difficult. Overall, despite considerable Allied material superiority, the Germans kept the Allies bottled up in a small beachhead for nearly two months, aided immeasurably by terrain factors.
Although there were several known disputes among the Allied commanders, their tactics and strategy were essentially determined by agreement between the main commanders. By contrast, the German leaders were bullied and their decisions interfered with by Hitler, controlling the battle from a distance with little knowledge of local conditions. Field Marshals von Rundstedt and Rommel repeatedly asked Hitler for more discretion but were refused. Von Rundstedt was removed from his command on 29 June after he bluntly told the Chief of Staff at Hitler's Armed Forces HQ (Field Marshal Keitel) to "Make peace, you idiots!" Rommel was severely injured by Allied aircraft on 17 July.
The German commanders also suffered in the quality of the available troops. Sixty thousand of the 850,000 in Rundstedt's command were raised from the many prisoners of war captured on the Eastern Front. These "Ost" units had volunteered to fight against Stalin, but when instead unwisely used to defend France against the Western Allies, ended up being unreliable. Many surrendered or deserted at the first available opportunity.
War memorials and tourism.
The beaches at Normandy are still referred to on maps and signposts by their invasion codenames. There are several vast cemeteries in the area. The American cemetery, in Colleville-sur-Mer, contains row upon row of identical white crosses and Stars of David, immaculately kept, commemorating the American dead. Commonwealth graves, maintained in many locations by the Commonwealth War Graves Commission, uses white headstones engraved with the person's religious or medal (Victoria Cross or George Cross only) symbol and their unit insignia. The Bayeux War Cemetery, with 4,648 burials, is the largest British cemetery of the war. The largest cemetery in Normandy is the La Cambe German war cemetery, with 21,222 burials, which features granite stones almost flush with the ground and groups of low-set crosses. There is also a Polish cemetery.
At the Bayeux Memorial, a monument erected by Britain has a Latin inscription on the memorial reads "Nos a gulielmo victi victoris patriam liberavimus" – freely translated, this reads "We, once conquered by William, have now set free the Conqueror's native land".
Streets near the beaches are still named after the units that fought there, and occasional markers commemorate notable incidents. At significant points, such as Pointe du Hoc and Pegasus Bridge, there are plaques, memorials or small museums. The Mulberry harbour still sits in the sea at Arromanches. In Sainte-Mère-Église, a dummy paratrooper hangs from the church spire. On Juno Beach, the Canadian government has built the Juno Beach Information Centre, commemorating one of the most significant events in Canadian military history.
In England the most significant memorial is the D-Day Museum in Southsea, Hampshire. The Museum was opened in 1984 to commemorate the 40th anniversary of D-Day. Its centrepiece is the Overlord embroidery commissioned by Lord Dulverton of Batsford (1915–92) as a tribute to the sacrifice and heroism of those men and women who took part in Operation Overlord.
On 5 June 1994 a drumhead service was held on Southsea Common adjacent the D-Day Museum. This service was attended by US President Bill Clinton, Queen Elizabeth II and over 100,000 members of the public.
Dramatisations.
The battle of Normandy has been the topic of many films, television shows, songs, computer games and books. Many dramatisations focus on the initial landings, and these are covered at Normandy Landings. Some examples that cover the wider battle include:
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="46129" url="http://en.wikipedia.org/wiki?curid=46129" title="ITV Digital">
ITV Digital

ITV Digital was a British digital terrestrial television broadcaster, which launched a pay-TV service on the world's first digital terrestrial television network. Its main shareholders were Carlton Communications and Granada plc, two franchises of the ITV network. Starting as ONdigital in 1998, the service was re-branded as ITV Digital in July 2001. Low audience figures and an ultimately unaffordable multi-million pound deal with the Football League led to the broadcaster suffering massive losses and force entering administration in March 2002. The service ceased permanently in June 2002, with the terrestrial multiplexes subsequently taken over by Crown Castle and the BBC to create Freeview in October 2002.
History.
Digital terrestrial television (DTT) began in the United Kingdom in 1998. Six multiplexes were set up, with three of them allocated to the existing analogue broadcasters. The other three multiplexes were auctioned off. A consortium of Carlton Television, Granada Television and British Sky Broadcasting won the auction as British Digital Broadcasting (BDB). The brand ONdigital was adopted for launch. BSkyB was forced by the Independent Television Commission (ITC) to withdraw from the consortium on competition grounds; this effectively placed Sky in direct competition with the newly launched service (although BSkyB was still required to provide key channels such as Sky Movies and Sky Sports to the service). With Sky originally part of the consortium, ONdigital would have paid discounted rates to carry Sky's television channels. Instead, with their positioning as a competitor, Sky charged the full market rates for the channels, at an extra cost of around £60million a year to ONdigital. In all ONdigital was given one year from the award of the licence to launch the first DTT service. In addition to launching audio and video services, they also led the specification of an industry-wide advanced interactive engine (based on MHEG-5). This was an open standard that was then used by all broadcasters on DTT.
The new digital broadcaster was launched on 15 November 1998, with a lineup of 18 channels, including many channels developed in-house by Carlton and Granada. On 7 March 2000, Onmail was launched, followed closely on 18 September 2000, by ONdigital text service ONnet, and in the same year a deal with multiplex operator SDN led to the launch of pay-per-view service ONrequest.
From the launch date, however, the service was quickly losing money. Aggressive marketing by BSkyB for their own digital service, Sky Digital, made the ONdigital offer look unattractive. The new digital satellite service provided a dish, Digibox, installation and around 200 channels for £159, a lower price than ONdigital at £199. ONdigital's subscription pricing had been set to compare with the older Sky analogue service of 20 channels.
ONdigital's growth slowed throughout 2000 and by the start of 2001, the number of subscribers did not increase - meanwhile, its competitor Sky Digital was growing. The ONdigital management team hoped to obtain the upper hand by a series of 'free set top box' promotions (initially at retailers such as Currys and Dixons) when ONdigital receiving equipment was purchased at the same time as a television set or similarly priced piece of equipment. These offers eventually became permanent, with the set-top box 'loaned' to the customer at no charge for as long as they continued to subscribe to ONdigital. The offer was matched by Sky. ONdigital's churn rate, a measure of the number of subscribers leaving the service, reached 28% during 2001.
Additional problems for ONdigital were caused by the choice of 64QAM broadcast mode, coupled with far weaker than expected broadcast power, (meaning that the signal was weak in many areas), a complex pricing structure (comprising many menu options), a poor quality subscriber management system (badly adapted from Canal+), a paper magazine TV guide whereas BSkyB had provided an electronic programme guide (EPG), insufficient technical customer services, and much signal piracy. While there was a limited return path provided via an in-built 2400 baud modem, there was no requirement (as with BSkyB) to connect the set-top box's modem to a phone line.
Later problems occurred when ONdigital began to sell 'ONprepaid', a set-top box bundle sold in high street stores and supermarkets at a price that included - in theory - the set-top box on loan and the first year's subscription package. Thousands of these packages were also sold at well below retail price on auction sites such as the then-popular QXL. As the call to activate the viewing card did not require any bank details, many ONdigital boxes which were technically on loan were at unverifiable addresses. This was later changed so a customer could not walk away with a box without ONdigital verifying their address. Many customers did not activate the viewing card at all, although where the viewer's address was known, ONdigital would write informing them that they must activate before a certain deadline.
Additionally, the OnDigital pay-per-view channels had been encrypted using a system - SECA MediaGuard - which had been subsequently cracked. ITV Digital did not update this system, therefore it was very easy for people to produce and sell counterfeit subscription cards which would give access to all the channels.
In 2002, Canal+ accused News Corp of extracting the UserROM code from the MediaGuard cards and leaking it onto the internet.
Canal+ brought a lawsuit against News Corporation alleging that they, with the help of NDS, had been working on breaking the MediaGuard smartcards used by Canal+, ITV Digital and other non-Murdoch-owned TV companies throughout Europe. The action was later partially dropped after News Corporation agreed to buy Canal Plus's struggling Italian operation Telepiu. 
Other legal action by Echostar/NagraStar was being pursued as late as August 2005 accusing NDS of the same wrongdoing. In 2008, NDS was found to have broken piracy laws by hacking EchoStar Communications’ smart card system, however only $1,500 in statutory damages were awarded.
Rebranding.
On 11 July 2001 Carlton and Granada rebranded ONdigital as ITV Digital. They also purchased the TV rights to the Football League and launched the ITV Sport Channel. A re-branding campaign was launched to support the new naming, with customers even being sent ITV Digital stickers to place over the existing ONdigital logos on their remote controls and set top boxes. The software running on the receivers was not changed though, and always displayed 'ON' on nearly every screen. A plan to change the onscreen software was planned along with a change to a stronger encryption system in Autumn 2002, however this never arose due to liquidation. The rebrand was not without controversy as SMG plc (owner of Scottish Television and Grampian Television), UTV and Channel Television all pointed out that the ITV brand did not belong solely to Carlton and Granada. SMG and UTV initially refused to carry the advertising campaign for ITV Digital and did not allow the ITV Sports Channel space on their multiplex, meaning that it was not available at launch in most of Scotland and Northern Ireland. The case was resolved in Scotland, and the Channel Islands and later still in Northern Ireland, allowing the ITV Sport Channel to launch in the non-Carlton and Granada regions (although it was never made available in the Channel Islands, as the islands do not have DTT or Cable and it never appeared on Sky Digital).
Monkey.
ITV Digital also ran an advertising campaign involving the comedian Johnny Vegas as Al and a knitted monkey simply called Monkey (voiced by Ben Miller). A knitted replica of Monkey could be obtained by signing up to ITV Digital. Because the monkey could not be obtained without signing up to the service, a popular market for second-hand monkeys developed. At one time, original ITV Digital Monkeys were fetching several hundred pounds on eBay, and even knitting patterns delivered by email were sold for several pounds. The campaign was created by the advertising agency Mother. In early 2007, Monkey and Al reappeared in an advert for PG Tips tea, which included a reference to ITV Digital's downfall.
Administration and Freeview.
ITV Digital was placed into administration on 27 March 2002, after the League refused to accept a £130m pay cut in its £315m deal with the ITV Sport Channel. Most subscription channels ceased broadcasting on ITV Digital on 1 May 2002. The collapse on 30 June 2002 caused financial difficulties for lower-division football clubs who had budgeted for large incomes from the television contract. The Football League sued ITV Digital's parent companies, Carlton and Granada, claiming that the firms had breached their contract in failing to deliver the guaranteed income. And so, by the end of June 2002, the service ceased. The League lost the case, with the judge ruling that it had "failed to extract sufficient written guarantees". The League then filed a negligence claim against its lawyers for failing to press for a written guarantee at the time of the deal with ITV Digital. This time it was awarded a paltry £4 in damages of the £150m it was seeking.
A consortium made up of the BBC, BSkyB and Crown Castle International was granted ITV Digital's old broadcasting licence, and launched the Freeview service on 30 October 2002, offering 30 free-to-air TV channels and 20 free-to-air radio channels including several interactive channels such as BBCi and Teletext but no subscription or premium services. Those followed on 31 March 2004 when Top Up TV began broadcasting eleven pay TV channels in timeshared broadcast slots.
During 2002, ITV Digital's liquidators started to ask customers to return set top boxes or pay a £39.99 fee. Had this been successful it could have threatened to undermine the fledgling Freeview service, since at the time most digital terrestrial receivers were former ONdigital and ITV Digital units. Carlton and Granada stepped in and paid £2.8m to have the boxes stay with their customers, as at the time the ITV companies received a discount on their licence payments based on the number of digital homes they had converted.
Following the administration in 2002, the three multiplexes that were run by ITV Digital remained blank until a week or so before Freeview's launch.
ITV Digital operated out of Marco Polo House, the south London building home to shopping channel QVC and which had once housed "The Observer" newspaper, and was headquarters of British Satellite Broadcasting. ITV Digital had call centres located in Pembroke Dock, Wales and in Plymouth, England, with other calls outsourced BT's in Cork, Republic of Ireland and Belfast, Northern Ireland.
Set top boxes.
This is a list of ex-ITV and ONdigital set-top boxes. All boxes used similar software, in that a unified interface and design was used between all models. Top Up TV provided a small update in 2004 which upgraded minor technicalities with encryption services.
All these set top boxes (and some ONdigital branded IDTVs) become obsolete after the digital switchover (DSO), as post-DSO broadcasts utilise a newer 8k modulation scheme with which this earlier equipment is not compatible.
iDTVs.
ONdigital and ITVdigital could also be received with an Integrated Digital Television (iDTV) receiver. They used a conditional-access module (CAM) with a smart card, plugged into a DVB Common Interface slot in the back of the set.
Purchasers of iDTVs were given a substantially discounted price on using the ONdigital service, as there was no cost for a set-top box.
Some of the original iDTVs needed firmware upgrades to work with the CAM. For example, Sony sent technicians out to homes to make the necessary updates free of charge.
Carlton/Granada digital television channels.
Carlton and Granada (later ITV Digital Channels Ltd) created a selection of channels which formed some of the core content of channels available via the service, which were:

</doc>
<doc id="46132" url="http://en.wikipedia.org/wiki?curid=46132" title="Rennes">
Rennes

Rennes (]; French: "Rennes", Gallo: "Resnn", Breton: "Roazhon", Latin: "Condate, Civitas Redonum") is a city in the east of Brittany in northwestern France. Rennes is the capital of the region of Brittany, as well as the Ille-et-Vilaine department. The city is the tenth largest in France, with a metropolitan area of about 700,000 inhabitants. With more than 63,000 students in 2013, is also the eighth-largest university campus of France.
In 2012, "l'Express" named Rennes as "the most liveable city in France".
History.
Overall significance.
Rennes is the administrative capital of the French department of Ille-et-Vilaine. Before the French revolution, prior to the integration of the Duchy of Brittany into the Kingdom of France, Rennes was the capital of the duchy, with the other historical capitals of Brittany's Ducal period being Nantes and Vannes. It has a long history due to its location at the confluence of two rivers and its proximity to the bordering regions from which arose various challenges to the borders of Brittany.
Earliest history.
By the second century BC the Gallic tribe known as the Redones had occupied a territory in eastern Brittany roughly equivalent to the modern department of Ille-et-Vilaine and had established their chief township at the confluence of the Ille and Vilaine rivers, the site of the modern city of Rennes. Although the tribe's name - from the Celtic root "red" cognate with "ride" suggesting the Redones were known for their horsemanship - would eventually default to their chief township ultimately yielding the name of the modern city of Rennes, the chief township of the Redones was contemporaneously referred to as "Condate" a Celtic term for confluence which was utilized to designate numerous towns in ancient Gaul.
Early in the 1st century BC, the Redones adopted the Greek and Roman practice of issuing coinage, adapting the widely imitated gold staters of Philip II of Macedon, in the characteristic Celtic coin metal alloy called billion. Without inscriptions, as the Celtic practice was, the Redones coinage features a charioteer whose pony has a human head. Large hoards of their coins were unearthed in the "treasure of Amanlis" found in June 1835 and that of Saint-Jacques-de-la-Lande, discovered in February 1941. The museum at Rennes contains a large representative collection.
In 57 BC the Redones joined the Gaulish coalition against Rome which was suppressed by Crassus. In 56 BC Roman emissaries were held hostage by the Redones causing Julius Caesar to intervene in Armorica suppressing the rebels, and the following year to cross the Channel to discourage further support of the Redones by the Britons. In 52 BC the Redones responded to the call of Vercingetorix to furnish a large contingent of warriors.
Roman era.
It was subsequent to its Roman occupation that the chief township of the Redones became known as Condate Riedonum - alternately Civitas Riedonum - the second element, referring to the Redones tribe who had founded it, ultimately yielding the name of the modern city of Rennes. The oldest known Rennais is Titus Flavius Postuminus, known to us from his steles found in Rennes in 1969. As indicated by his name, he would have been born under the Flavian dynasty, under the reign of Titus, i.e. between 79 and 81 AD. One of the steles tells us, in Latin, that he took charge over all the public affairs in the Civitas Riedonum. He was twice duumvir and flamen for life for Mars Mullo.
During the Roman era, the strategic position of the town contributed to its importance. To the west the principal Roman route, via Osismii, stretched from Condate Riedonum to Vorgium (modern Carhaix).
In the year 275, the threat of barbarians led to the erection of a robust brick wall around Rennes. Threatened by the danger of the peasant marauders designated as "bagaudae" in the final days of the Roman Empire in the 5th century, the Armorican peninsula, including Brittany and therefore Rennes, constituted the last stronghold of the western Roman Empire with the Armorican Romans invincible against Clovis I, who occupied most of Alamans, then the Visigoths.
The Holy See of Rennes had been established by 453, with a church having occupied the site of the current Rennes Cathedral since the start of the sixth century.
One of the earliest bishops of Rennes: Melaine - who would become the city's patron saint - played an important role in the peace treaty between the Franks and the Armoricans in the year 497. He famously declared ""Il faut faire la paix entre chrétiens"" ("Peace must be made between Christians").
Middle Ages.
From the 5th century, Bretons occupied the western part of the Armorican peninsula, which was resultantly known as Brittany (i.e. Little Britain), while the Franks took the rest of Armorica. To contain the expansion and avoid Breton incursions, the Carolingians instituted a Breton March or frontier province, composed of the counties of Rennes, Nantes, and Vannes. These marches were entirely absorbed by the kingdom of Brittany in the 9th century, with Rennes becoming fully Breton in 851. Throughout Brittany's existence as an independent state - first as a kingdom and then as a duchy - Rennes generally was considered to be one of three cities acting as the territory's capital, the others being Nantes and Vannes, with Rennes Cathedral being the coronation site for the dukes of Brittany.
During the Breton War of Succession (1356–57) Rennes was laid siege to by Henry of Grosmont (Duke of Lancaster), cousin of the English king, but Bertrand du Guesclin penetrated the city and commandeered the resistance with ultimate victory. After nearly a year, Lancaster abandoned the English siege in 1357.
In 1491, the French army of Charles VIII, led by General Louis II de la Trémoille, unsuccessfully attacked Rennes. Brittany having already capitulated elsewhere, Rennes alone resisted. The defenders of Rennes were determined to resist to the death, but the Duchess Anne of Brittany chose instead to negotiate. The resulting treaty of Rennes of 15 November 1491 dictated her marriage to Charles VIII and brought Brittany into the French kingdom. Anne zealously guarded Brittany's autonomy and the treaty promised that justice would continue to be dispensed according to practices, usages and customs maintained and observed heretofore. Furthermore, he promised the continuation of the Parlement of Brittany which met in February–April 1493, September 1494 and September 1495.
In 1720, a major fire destroyed all timber framing houses in the northern part of the city. The rebuilding was made of stone, on a grid plan.
Modern era.
In 1857, the Rennes train station was built, which gradually led to the southward sprawl of the town. In 1899, Alfred Dreyfus' second trial in Rennes caused a national sensation.
With several faculties of the University of Brittany having transferred from Nantes to Rennes beginning with the law school in 1730, the full-fledged University of Rennes began operation in 1885 (although it was not so named until 1896 rather being referred to as a "Conseil des facultés").
During World War II, Rennes suffered heavy damage from just three German aircraft which hit an ammunition train parked alongside French and English troop trains and near a refugee train on the yard: 1,000 died. The next day, 18 June 1940, German troops entered the city. Later, Rennes endured heavy bombing by the US and Royal Air Forces in March and May 1943, and again in June 1944, causing hundreds of deaths. Rennes contained a German transit POW camp and a POW hospital which contained many of the paratroopers captured on D-Day. Patton's army freed the capital of Brittany on 4 August, as retreating German troops blew up the bridges behind them, adding further damage. About 50,000 German prisoners were kept in four camps, in a city of only about 100,000 inhabitants at the time.
From 1954 onward, the city developed extensive building plans to accommodate upwards of 220,000 inhabitants, helping it become the second fastest-growing city in France, after Toulouse (1999 census).
Administration.
Rennes is divided into 11 cantons:
Since the 2008 cantonal elections, all eleven cantons are held by Socialists or their allies. The right held Rennes-Nord-Ouest until 2008.
Rennes is divided into 12 quarters:
Mayors.
The current mayor of Rennes is Nathalie Appéré. A member of the Socialist Party, she replaced retiring Socialist incumbent Daniel Delaveau, in office from 2008 to 2014.
Among previous well-known mayors are:
The "mairie" ("City hall") is right in the centre of Rennes.
National representation.
The French Prison Service operates the "Centre pénitentiaire de Rennes", the largest women's prison in France.
Geography.
The ancient centre of the town is built on a hill, with the north side being more elevated than the south side. It is at the confluence of two rivers: the Ille and the Vilaine.
Climate.
Rennes features an oceanic climate with mild winters and warm summers. Precipitation in Rennes is considerably less abundant than in the Western parts of Brittany, reaching only half of the levels of, e.g., the city of Quimper, which makes rainfall in Rennes comparable to the levels of larger parts of Western Germany. Sunshine hours range between 1700 and 1850 annually, which is about the amount of sunshine received by the city of Lausanne.
Population.
In 2012, the inner population of the city was of 209,860, and in the urban area they counted 690,467 inhabitants.
The inhabitants of Rennes are called "Rennais" in French.
Rennes has the second-fastest growing metropolitan area in France after Toulouse and before Montpellier, Bordeaux and Nantes.
Sights.
Rennes is classified as a city of art and history.
Historic centre.
The historic centre is located on the former plan of the ramparts. There is a difference between the northern city centre and the southern city centre due to the 1720 fire, which destroyed most of the timber framed houses in the northern part of the city. The rebuilding was done in stone, on a grid plan. The southern part, the poorest at this time, was not rebuilt.
Due to the presence of the "parlement de Bretagne", many "hôtels particuliers" were built in the northern part, the richest in the 18th century. Most of the monuments historiques can be found there.
Colourful traditional half-timbered houses are situated primarily along the roads of Saint-Sauveur, Saint-Georges, de Saint-Malo, Saint-Guillaume, des Dames, du Chapitre, Vasselot, Saint-Michel, de la Psallette and around the plazas of Champ-Jacquet, des Lices, Saint-Anne and Rallier-du-Baty.
The Parlement de Bretagne and city hall area.
The "Parlement de Bretagne" (Administrative and judicial centre of Brittany, Breton: "Breujoù Breizh") is the most famous 17th century building in Rennes. It was rebuilt after a terrible fire in 1994 that may have been caused by a flare fired by a protester during a demonstration. It houses the Rennes Court of Appeal. The plaza around is built on the classical architecture.
On the west, the Place de la Mairie (City Hall Plaza, Plasenn Ti Kêr) :
On the east, at the end of the "rue saint-Georges" with traditional half-timbered houses : 
On the south-east :
The Place des Lices and Cathedral area.
The place des Lices is lined by hôtels particuliers. with the place Railler-du-Baty, is the location of the weekly big market, the marché des Lices.
Near the Rennes Cathedral (cathédrale Saint-Pierre de Rennes) is the rue du Chapitre :
On this era are the former St. Yves chapel, now the tourism office and a museum about the historical development of Rennes . and the Basilica Saint-Sauveur
Remains of the ramparts.
Built from the 3rd to the 12th centuries, the ramparts were largely destroyed between the beginning of the 16th century and the 1860s.
The Place Saint-Anne area.
Place Saint-Anne (Plasenn Santez-Anna)
South-western, "la rue Saint-Michel" nicknamed "Rue de La Soif" ("Road of Thirst") because there are bars all along this street.
South-eastern, the Champ-Jacquet square, with Renaissance buildings and a statue of mayor Jean Leperdit ripping up a conscription list.
East : the Thabor park area.
Area of Saint-Melaine square
The Jardin botanique du Thabor (formal French garden, orangerie, rose garden, aviary) a botanical garden on 10 hectares of land, built between 1860 and 1867.
The 17th century promenade "la Motte à Madame", and a monumental stairway overlooking the rue de Paris entrance to the Thabor.
South city centre.
The south city centre is a mix of old buildings and 19th and 20th centuries constructions.
South of the Vilaine.
The Fine Arts Museum is situated on Quai Émile Zola, by the Vilaine River.
Les Champs Libres is a building on Esplanade Charles de Gaulle, and was designed by the architect Christian de Portzamparc. It houses the Brittany Museum (Musée de Bretagne), the regional library Bibliothèque de Rennes Métropole with six floors, and the Espace des Sciences science centre with a planetarium.
At Place Honoré Commeurec is Les Halles Centrales, a covered market from 1922, with one part converted into contemporary art gallery.
The Mercure Hotel is located in a restored building on rue du Pré-Botté, which was the prior location of Ouest-Éclair, and then of Ouest-France, a premier daily regional newspaper.
There are large mills at Rue Duhamel, constructed on each side of the south branch of the Vilaine in 1895 and 1902.
Other sights.
To the northwest of Rennes, near rue de Saint-Malo are the locks of the Canal d'Ille-et-Rance of 1843.
There are two halls of the printer, Oberthür, built by Marthenot between 1870 and 1895 on Rue de Paris in the eastern part of the city. Oberthür Park is the second biggest garden in the city.
The 17th century manor of Haute-Chalais, a granite chateau, is situated to the south of the city in Blosne Quarter (Bréquigny).
Economy.
Local industries include car manufacturing and telecommunications. PSA Peugeot Citroën, currently the largest employer of the population of Rennes, opened a manufacturing plant at La Janais in Chartres-de-Bretagne in 1961. Technicolor (ex-Thomson) employs over 1,000, and France Telecom R&D over 1,200.
In few years, Rennes became one of the main centres in high technology industry. The city hosts one of the first Technopoles established in France.
Rennes is the 2nd concentration of ITC firms in France after Paris (with well-known companies like Atos, SFR, Orange France Telecom, Envivio, Technicolor R&D, Canon, Mitsubishi, Alcatel-Lucent, Texas Instruments, NXP, Sopra Group, Thales or Logica), and the 3rd innovation potential in agrofood French industry.
Other large firms located in Rennes include the restaurant conglomerate Groupe Le Duff, owners of Brioche Dorée.
Culture.
Rennes invests heavily in arts and culture and a number of its festivals (such as the music festival "Les Transmusicales", "les Tombées de la Nuit" and "Travelling (a cinematic festival)") are well known throughout France. Rennes is often cited as the French town of rock music.
There are five museums in Rennes:
In addition to this list, there is art facilities, such as "40mcube" exhibition space or the center for contemporary art "La Criée".
Rennes is also well equipped in musical facilities :
There is also miscellaneous cultural places : the dance dedicated place the "Triange", an "Art et Essai" - art house cinemas - movie theater called l'Arvor. Remark that the surrounding citys house many other cultural places.
Rennes was one of the first towns in France to have its own local television channel 'TV Rennes', created in 1987.
In Rennes is the only Institut Franco-Américain in France.
The Parc du Thabor contains a compact but significant botanical garden, the Jardin botanique du Thabor. The University of Rennes 1, with a campus in the city's eastern section, also contains a botanical garden and collections (the Jardin botanique de l'Université de Rennes).
Education.
The Rennes agglomeration has a large student population (around 63,000).
The city has two main universities; "Université de Rennes 1", which offers courses in science, technology, medicine, philosophy, law, management and economics and "Université Rennes 2", which has courses in the arts, literature, languages, communication, human and social sciences, sport. The official website of Université Rennes 2 identifies that facility as "the largest research and higher learning institution in Arts, Literature, Languages, Social Sciences, and Humanities in the West of France."
There are a few "École Supérieures" in Rennes, like the "École Normale Supérieure de Cachan" (which has a branch on the Ker Lann campus, just outside Rennes), the "Institut d'études politiques de Rennes", or the ESC Rennes School of Business.
There is also branches of "École Supérieure d'Électricité" – Supélec and Telecom Bretagne in the east of the city (Cesson-Sévigné), a campus of the "École pour l'informatique et les nouvelles technologies", and the "grande école" Institut National des Sciences Appliquées, which is next to the "École Nationale Supérieure de Chimie de Rennes".
The computer science and applied mathematics research institute, IRISA, is located on the campus of the Université des Sciences, nearby Cesson-Sévigné. The "Délégation Générale pour l'Armement" (defense procurement agency) operates the CELAR research center, dedicated to electronics and computing, in Bruz, a neighboring town.
The city is also home to an American study abroad program for high school students, School Year Abroad, in which students are immersed in French culture through five classes in the language and a nine-month home stay.
The "École Compleméntaire Japonaise de Rennes" (レンヌ補習授業校 "Rennu Hoshū Jugyō Kō"), a part-time Japanese supplementary school, is held in the "Collège Anne de Bretagne" in Rennes.
Breton language.
In Brittany, two regional languages are spoken: the Breton and the Gallo. In and around Rennes, Gallo was traditionally spoken as a local language, but Breton have always been spoken by regional migrants coming from the western part of the region.
Nowadays, the Breton language is taught in one Diwan school, some bilingual public and catholic schools, in evening courses, and in university.
The municipality launched a linguistic plan through Ya d'ar brezhoneg on 24 January 2008.
In 2008, 2.87% of primary-school children were enrolled in bilingual primary schools, and the number of children enrolled in these schools is steadily growing.
Transport.
Rennes has well-developed national road, rail and air links :
International relations.
Twin towns – sister cities.
Rennes is twinned with:
"(These twinned towns are inscribed on the bridge over the central canal of Rennes)"
Within France
Pacts of cooperation
Sponsorship

</doc>
<doc id="46133" url="http://en.wikipedia.org/wiki?curid=46133" title="Cardiomyopathy">
Cardiomyopathy

Cardiomyopathy (literally "heart muscle disease") is the measurable deterioration for any reason of the ability of the myocardium (the heart muscle) to contract, usually leading to heart failure. Common symptoms include dyspnea (breathlessness) and peripheral edema (swelling of the legs). Those with cardiomyopathy are often at risk of dangerous forms of irregular heart rate and sudden cardiac death.
The most common form of cardiomyopathy is dilated cardiomyopathy. Although the term "cardiomyopathy" could theoretically apply to almost any disease affecting the heart, it is usually reserved for "severe myocardial disease leading to heart failure".
Cardiomyopathy and myocarditis, resulted in 443,000 deaths in 2013 up from 294,000 in 1990.
Classification.
The term 'cardiomyopathy' only came into use about 50 years ago. The definition has advanced as knowledge has increased and new diagnostic tests have been introduced. In 2008, the European Society of Cardiology defined it as a myocardial disorder in which the heart muscle was structurally abnormal and functioned abnormally.
Two years earlier, the American Heart Association had pointed out that cardiomyopathies were either confined to the heart or were part of a generalized disorder, both often leading to death or progressive heart failure. Both groups excluded heart disease due to coronary artery disease, hypertension, abnormalities of the heart valves, and heart disease present at birth from the definition.
Earlier, simpler, categories such as intrinsic, (defined as weakness of the heart muscle without an identifiable external cause), and extrinsic, (where the primary pathology arose outside the myocardium itself), became more difficult to sustain.
For example, as more external causes were recognized, the intrinsic category became smaller. Alcoholism, for example, has been identified as a cause of dilated cardiomyopathy, as has drug toxicity, and certain infections (including Hepatitis C). On the other hand, molecular biology and genetics have given rise to the recognition of various genetic causes, increasing the intrinsic category. For example, mutations in the cardiac desmosomal genes as well as in the DES gene may cause arrhythmogenic right ventricular cardiomyopathy (ARVC).
At the same time, a more clinical categorization of cardiomyopathy as 'hypertrophied', 'dilated', or 'restrictive', became difficult to maintain when it became apparent that some of the conditions could fulfill more than one of those three categories at any particular stage of their development.
The current American Heart Association definition divides cardiomyopathies into primary, which affect the heart alone, and secondary, which are the result of illness affecting other parts of the body. These categories are further broken down into subgroups which incorporate new genetic and molecular biology knowledge.
Signs and symptoms.
Symptoms and signs may mimic those of almost any form of heart disease. Chest pain is common. Mild myocarditis or cardiomyopathy is frequently asymptomatic; severe cases are associated with heart failure, arrhythmias, and systemic embolization. Manifestations of the underlying disease (e.g., Chagas' disease) may be prominent. Most patients with biopsy-proven myocarditis report a recent viral prodrome preceding cardiovascular symptoms.
ECG abnormalities are often present, although the changes are frequently nonspecific. A pattern characteristic of left ventricular hypertrophy may be present. Flat or inverted T waves are most common, often with low-voltage QRS complexes. Intraventricular conduction defects and bundle branch block, especially left bundle branch block, are also common. An echocardiogram is useful to detect wall motion abnormalities or a pericardial effusion. Chest radiographs can be normal or can show evidence of congestive heart failure with pulmonary edema or cardiomegaly.
Treatment.
Treatment depends on the type of cardiomyopathy and condition of disease, but may include medication (conservative treatment) or iatrogenic/implanted pacemakers for slow heart rates, defibrillators for those prone to fatal heart rhythms, ventricular assist devices (VADs) for severe heart failure, or ablation for recurring dysrhythmias that cannot be eliminated by medication or mechanical cardioversion. The goal of treatment is often symptom relief, and some patients may eventually require a heart transplant. Treatment of cardiomyopathy (and other heart diseases) using alternative methods such as stem cell therapy is commercially available but is not supported by convincing evidence.

</doc>
<doc id="46134" url="http://en.wikipedia.org/wiki?curid=46134" title="Robert Bylot">
Robert Bylot

Robert Bylot was a 17th-century explorer who made four voyages to the Arctic. He was uneducated and from a working-class background, but was able to rise to rank of Master in the British Royal Navy.
"1610 with Hudson:" Bylot was first mate on Henry Hudson's ship "Discovery", during Hudson's 1610-1611 expedition into what is now known as Hudson Bay. In the spring of 1611, Hudson wanted to continue the expedition, but the crew wanted to return home. There was discontent between the Captain (Hudson) and members of the crew, Bylot was stripped of his rank. Later there was a mutiny in which Hudson, his son and several sailors were set adrift in an open boat. It was due to Bylot's navigational skills that the ship was able to return from the Arctic safely. Upon return to England, Bylot was tried as a mutineer but was pardoned.
"1612 with Button:" Bylot returned to Hudson Bay in 1612 with Sir Thomas Button. They wintered over at the mouth of the Nelson River, and in the spring of 1613 continued north. They were able to reach latitude 65°, then returned to England.
"1615:" In 1615, the Muscovy Company hired Bylot to find the Northwest Passage as captain of the "Discovery". He sailed west from Hudson Strait and was blocked by ice at Frozen Strait.
"1616 with Baffin:" The following year (1616), the Muscovy Company again hired Bylot to continue to search for the Northwest Passage. This time he was accompanied by pilot William Baffin. The Bylot-Baffin voyage resulted in several notable achievements. First was the circumnavigation and mapping of what is now called Baffin Bay. Second was the discovery of Smith Sound, by which the North Pole would eventually be reached. Third was the discovery of Lancaster Sound, through which the Northwest Passage would eventually be found three centuries later. Fourth, and perhaps most significantly, they were able to reach 77° 45' North latitude, a record which held for 236 years.
Bylot and Baffin's work in Baffin Bay was doubted by cartographers back in England. As late as 1812, charts of the area only showed a dotted bulge with the words: "Baffin's Bay according to the relation of W. Baffin in 1616, but not now believed". When the bay was "rediscovered" by Sir John Ross in 1818, the records of the Bylot-Baffin voyage proved extremely accurate. In England, almost total credit for the discovery was given to Baffin, and Bylot was virtually ignored. Historian Farley Mowat has speculated two possible reasons for this: Bylot's lack of education and lower position relative to Baffin in English society, and his involvement in the mutiny during Hudson's expedition.
Bylot Island, one of the more dramatic of the Arctic Islands, was named after him.

</doc>
<doc id="46135" url="http://en.wikipedia.org/wiki?curid=46135" title="George Lakoff">
George Lakoff

George P. Lakoff (, born May 24, 1941) is an American cognitive linguist, best known for his thesis that lives of individuals are significantly influenced by the central metaphors they use to explain complex phenomena.
The metaphor thesis, introduced in his 1980 book "Metaphors We Live By" has found applications in a number of academic disciplines and its application to politics, literature, philosophy and mathematics has led him into territory normally considered basic to political science. In the 1996 book "Moral Politics", Lakoff described conservative voters as being influenced by the "strict father model" as a central metaphor for such a complex phenomenon as the state and liberal/progressive voters as being influenced by the "nurturant parent model" as the folk psychological metaphor for this complex phenomenon. According to him, an individual's experience and attitude towards sociopolitical issues is influenced by being framed in linguistic constructions. In "Metaphor and War: The Metaphor System Used to Justify War in the Gulf", he argues that the American involvement in the Gulf war was either obscured or was put a spin on, by the metaphors which were used by the first Bush administration to justify it. Between 2003 and 2008, Lakoff was involved with a progressive think tank, the now defunct Rockridge Institute. He is a member of the scientific committee of the Fundación IDEAS (IDEAS Foundation), Spain's Socialist Party's think tank.
The more general theory that elaborated his thesis is known as embodied mind. He is a professor of linguistics at the University of California, Berkeley, where he has taught since 1972.
Work.
Reappraisal of metaphor.
Although some of Lakoff's research involves questions traditionally pursued by linguists, such as the conditions under which a certain linguistic construction is grammatically viable, he is most famous for his reappraisal of the role that metaphors play in socio-political lives of humans.
Metaphor has been seen within the Western scientific tradition as purely a linguistic construction. The essential thrust of Lakoff's work has been the argument that metaphors are primarily a conceptual construction, and indeed are central to the development of thought.
He suggested that: 
Non-metaphorical thought is for Lakoff only possible when we talk about purely physical reality. For Lakoff the greater the level of abstraction the more layers of metaphor are required to express it. People do not notice these metaphors for various reasons. One reason is that some metaphors become 'dead' and we no longer recognize their origin. Another reason is that we just don't "see" what is "going on".
For instance, in intellectual debate the underlying metaphor is usually that argument is war (later revised as "argument is struggle"):
For Lakoff, the development of thought has been the process of developing better metaphors. The application of one domain of knowledge to another domain of knowledge offers new perceptions and understandings.
Linguistics wars.
Lakoff began his career as a student and later a teacher of the theory of transformational grammar developed by Massachusetts Institute of Technology professor Noam Chomsky. In the late 1960s, however, he joined with others to promote generative semantics as an alternative to Chomsky's generative syntax. In an interview he stated:
During that period, I was attempting to unify Chomsky's transformational grammar with formal logic. I had helped work out a lot of the early details of Chomsky's theory of grammar. Noam claimed then — and still does, so far as I can tell — that syntax is independent of meaning, context, background knowledge, memory, cognitive processing, communicative intent, and every aspect of the body...In working through the details of his early theory, I found quite a few cases where semantics, context, and other such factors entered into rules governing the syntactic occurrences of phrases and morphemes. I came up with the beginnings of an alternative theory in 1963 and, along with wonderful collaborators like "Haj" Ross and Jim McCawley, developed it through the sixties.
Lakoff's claim that Chomsky asserts independence between syntax and semantics has been rejected by Chomsky, who has given examples from within his work where he talks about the relationship between his semantics and syntax. Chomsky goes further and claims that Lakoff has "virtually no comprehension of the work he is discussing" (the work in question being Chomsky's). His differences with Chomsky contributed to fierce, acrimonious debates among linguists that have come to be known as the "linguistics wars".
Embodied mind.
When Lakoff claims the mind is "embodied", he is arguing that almost all of human cognition, up through the most abstract reasoning, depends on and makes use of such concrete and "low-level" facilities as the sensorimotor system and the emotions. Therefore embodiment is a rejection not only of dualism vis-a-vis mind and matter, but also of claims that human reason can be basically understood without reference to the underlying "implementation details".
Lakoff offers three complementary but distinct sorts of arguments in favor of embodiment. First, using evidence from neuroscience and neural network simulations, he argues that certain concepts, such as color and spatial relation concepts (e.g. "red" or "over"; see also "qualia"), can be almost entirely understood through the examination of how processes of perception or motor control work.
Second, based on cognitive linguistics' analysis of figurative language, he argues that the reasoning we use for such abstract topics as warfare, economics, or morality is somehow rooted in the reasoning we use for such mundane topics as spatial relationships. (See conceptual metaphor.)
Finally, based on research in cognitive psychology and some investigations in the philosophy of language, he argues that very few of the categories used by humans are actually of the black-and-white type amenable to analysis in terms of necessary and sufficient conditions. On the contrary, most categories are supposed to be much more complicated and messy, just like our bodies.
"We are neural beings," Lakoff states, "Our brains take their input from the rest of our bodies. What our bodies are like and how they function in the world thus structures the very concepts we can use to think. We cannot think just anything — only what our embodied brains permit."
Lakoff believes consciousness to be neurally embodied, however he explicitly states that the mechanism is not just neural computation alone. Using the concept of disembodiment, Lakoff supports the physicalist approach to the afterlife. If the soul can not have any of the properties of the body, then Lakoff claims it can not feel, perceive, think, be conscious, or have a personality. If this is true, then Lakoff asks what would be the point of the afterlife? 
Many scientists share the belief that there are problems with falsifiability and foundation ontologies purporting to describe "what exists", to a sufficient degree of rigor to establish a reasonable method of empirical validation. But Lakoff takes this further to explain why hypotheses built with complex metaphors cannot be directly falsified. Instead, they can only be rejected based on interpretations of empirical observations guided by other complex metaphors. This is what he means when he says that falsifiability itself can never be established by any reasonable method that would not rely ultimately on a shared human bias. The bias he's referring to is the set of conceptual metaphors governing how people interpret observations.
Lakoff is, with coauthors Mark Johnson and Rafael E. Núñez, one of the primary proponents of the embodied mind thesis. Lakoff discussed these themes in his 2001 Gifford Lectures at the University of Glasgow, published as "The Nature and Limits of Human Understanding". Others who have written about the embodied mind include philosopher Andy Clark (See his Being There), philosopher and neurobiologists Humberto Maturana and Francisco Varela and his student Evan Thompson (See Varela, Thompson & Rosch's "The Embodied Mind"), roboticists such as Rodney Brooks, Rolf Pfeifer and Tom Ziemke, the physicist David Bohm (see his "Thought As A System"), Ray Gibbs (see his "Embodiment and Cognitive Science"), John Grinder and Richard Bandler in their neuro-linguistic programming, and Julian Jaynes. All of these writers can be traced back to earlier philosophical writings, most notably in the phenomenological tradition, such as Maurice Merleau-Ponty and Heidegger. The basic thesis of "embodied mind" is also traceable to the American contextualist or pragmatist tradition, notably John Dewey in such works as Art As Experience.
Mathematics.
According to Lakoff, even mathematics is subjective to the human species and its cultures: thus "any question of math's being inherent in physical reality is moot, since there is no way to know whether or not it is." By this, he is saying that there is nothing outside of the thought structures we derive from our embodied minds that we can use to "prove" that mathematics is somehow beyond biology. Lakoff and Rafael E. Núñez (2000) argue at length that mathematical and philosophical ideas are best understood in light of the embodied mind. The philosophy of mathematics ought therefore to look to the current scientific understanding of the human body as a foundation ontology, and abandon self-referential attempts to ground the operational components of mathematics in anything other than "meat".
Mathematical reviewers have generally been critical of Lakoff and Núñez, pointing to mathematical errors . Lakoff claims that these errors have been corrected in subsequent printings . Although their book attempts a refutation of some of the most widely accepted viewpoints in philosophy of mathematics and advice for how the field might proceed going forward, they have yet to elicit much of a reaction from philosophers of mathematics themselves. The small community specializing in the psychology of mathematical learning, to which Núñez belongs, is paying attention.
Lakoff has also claimed that we should remain agnostic about whether math is somehow wrapped up with the very nature of the universe. Early in 2001 Lakoff told the American Association for the Advancement of Science (AAAS): "Mathematics may or may not be out there in the world, but there's no way that we scientifically could possibly tell." This is because the structures of scientific knowledge are not "out there" but rather in our brains, based on the details of our anatomy. Therefore, we cannot "tell" that mathematics is "out there" without relying on conceptual metaphors rooted in our biology. This claim bothers those who believe that there really is a way we could "tell". The falsifiability of this claim is perhaps the central problem in the cognitive science of mathematics, a field that attempts to establish a foundation ontology based on the human cognitive and scientific process.
Political significance and involvement.
Lakoff has publicly expressed both ideas about the conceptual structures that he views as central to understanding the political process, and some of his particular political views. He almost always discusses the latter in terms of the former.
"Moral Politics" (1996, revisited in 2002) gives book-length consideration to the conceptual metaphors that Lakoff sees as present in the minds of American "liberals" and "conservatives". The book is a blend of cognitive science and political analysis. Lakoff makes an attempt to keep his personal views confined to the last third of the book, where he explicitly argues for the superiority of the liberal vision.
Lakoff argues that the differences in opinions between liberals and conservatives follow from the fact that they subscribe with different strength to two different central metaphors about the relationship of the state to its citizens. Both, he claims, see governance through metaphors of the family. Conservatives would subscribe more strongly and more often to a model that he calls the "strict father model" and has a family structured around a strong, dominant "father" (government), and assumes that the "children" (citizens) need to be disciplined to be made into responsible "adults" (morality, self-financing). Once the "children" are "adults", though, the "father" should not interfere with their lives: the government should stay out of the business of those in society who have proved their responsibility. In contrast, Lakoff argues that liberals place more support in a model of the family, which he calls the "nurturant parent model", based on "nurturant values", where both "mothers" and "fathers" work to keep the essentially good "children" away from "corrupting influences" (pollution, social injustice, poverty, etc.). Lakoff says that most people have a blend of both metaphors applied at different times, and that political speech works primarily by invoking these metaphors and urging the subscription of one over the other.
Lakoff further argues that one of the reasons liberals have had difficulty since the 1980s is that they have not been as aware of their own guiding metaphors, and have too often accepted conservative terminology framed in a way to promote the strict father metaphor. Lakoff insists that liberals must cease using terms like "partial birth abortion" and "tax relief" because they are manufactured specifically to allow the possibilities of only certain types of opinions. "Tax relief" for example, implies explicitly that taxes are an affliction, something someone would want "relief" from. To use the terms of another metaphoric worldview, Lakoff insists, is to unconsciously support it. Liberals must support linguistic think tanks in the same way that conservatives do if they are going to succeed in appealing to those in the country who share their metaphors.
Between 2003 and 2008, Lakoff was involved with a progressive think tank, the Rockridge Institute, an involvement that follows in part from his recommendations in "Moral Politics". Among his activities with the Institute, which concentrates in part on helping liberal candidates and politicians with re-framing political metaphors, Lakoff has given numerous public lectures and written accounts of his message from "Moral Politics." In 2008, Lakoff joined Fenton Communications, the nation's largest public interest communications firm, as a Senior Consultant.
One of his political works, "Don't Think of an Elephant! Know Your Values and Frame the Debate", self-labeled as "the Essential Guide for Progressives", was published in September 2004 and features a foreword by former Democratic presidential candidate Howard Dean.
Disagreement with Steven Pinker.
In 2006 Steven Pinker wrote an unfavorable review of Lakoff's book "Whose Freedom? The Battle over America's Most Important Idea". Pinker's review was published in "The New Republic". Pinker argued that Lakoff's propositions are unsupported and his prescriptions are a recipe for electoral failure. He wrote that Lakoff was condescending and deplored Lakoff's "shameless caricaturing of beliefs" and his "faith in the power of euphemism". Pinker portrayed Lakoff's arguments as "cognitive relativism, in which mathematics, science, and philosophy are beauty contests between rival frames rather than attempts to characterize the nature of reality". Lakoff wrote a rebuttal to the review stating that his position on many matters is the exact reverse of what Pinker attributes to him. Lakoff explicitly rejected, for example, the cognitive relativism and faith in euphemism described above, arguing in favor of a deeper understanding of rationality that discards the modal logic conceptualization of rationality in favor of the better supported framing conceptualization.

</doc>
<doc id="46136" url="http://en.wikipedia.org/wiki?curid=46136" title="The Football Association">
The Football Association

The Football Association, also known simply as The FA, is the governing body of football in England, and the Crown dependencies of Jersey, Guernsey and the Isle of Man. Formed in 1863, it is the oldest football association in the world and is responsible for overseeing all aspects of the amateur and professional game in its territory.
The FA sanctions all competitive football matches within its remit at national level, and indirectly at local level through the County Football Associations. It runs numerous competitions, the most famous of which is the FA Cup. It is also responsible for appointing the management of the men's, women's and youth national football teams.
The FA is a member of both UEFA and FIFA and holds a permanent seat on the International Football Association Board (IFAB) which is responsible for the laws of the game. As the first football association, it does not use the national name "English" in its title. The FA is based at Wembley Stadium, London. The FA is a member of the British Olympic Association, meaning that the FA has control over the men's and women's Great Britain Olympic football team.
All of England's professional football teams are members of the Football Association. Although it does not run the day-to-day operations of the Premier League, it has veto power over the appointment of the League Chairman and Chief Executive and over any changes to league rules. The Football League, made up of the professional leagues below the Premier League, is self-governing.
History.
For centuries before the first meeting of the Football Association in The Freemasons' Tavern on Great Queen Street, London on 26 October 1863, there were no universally accepted rules for playing football. In each public school the game was formalised according to local conditions; but when the schoolboys reached university, chaos ensued when the players used different rules, so members of the University of Cambridge devised and published a set of Cambridge Rules in 1848 which was widely adopted. Another set of rules, the Sheffield Rules, was used by a number of clubs in the North of England from the 1850s.
Eleven London football clubs and schools representatives met in 26 October 1863 to agree on common rules. The founding clubs present at the first meeting were Barnes, Civil Service, Crusaders, Forest of Leytonstone (later to become Wanderers), N.N. (No Names) Club (Kilburn), the original Crystal Palace, Blackheath, Kensington School, Perceval House (Blackheath), Surbiton and Blackheath Proprietary School; Charterhouse sent their captain, B.F. Hartshorne, but declined the offer to join. Many of these clubs are now defunct or play rugby union.
Central to the creation of the Football Association and modern football was Ebenezer Cobb Morley. He was a founding member of the Football Association in 1863. In 1862, as captain of the Mortlake-based club, he wrote to "Bell's Life" newspaper proposing a governing body for the sport that led to the first meeting at The Freemasons' Tavern that created the FA. He was the FA's first secretary (1863–66) and its second president (1867–74) and drafted the Laws of the Game generally called the "London Rules" at his home in Barnes, London. As a player, he played in the first ever match in 1863.
The first version of the rules for the modern game was drawn up over a series of six meetings held in The Freemasons' Tavern from October till December. At the final meeting, F. M. Campbell, the first FA treasurer and the Blackheath representative, withdrew his club from the FA over the removal of two draft rules at the previous meeting, the first which allowed for the running with the ball in hand and the second, obstructing such a run by hacking (kicking an opponent in the shins), tripping and holding. Other English rugby clubs followed this lead and did not join the FA but instead in 1871 formed the Rugby Football Union. The term "soccer" dates back to this split to refer to football played under the "association" rules.
An inaugural game using the new FA rules was initially scheduled for Battersea Park on 2 January 1864, but enthusiastic members of the FA could not wait for the new year and an experimental game was played at Mortlake on 19 December 1863 between Morley's Barnes team and their neighbours Richmond (who were not members of the FA), ending in a goalless draw. The Richmond side were obviously unimpressed by the new rules in practice because they subsequently helped form the Rugby Football Union in 1871. The Battersea Park game was postponed for a week, and the first exhibition game using FA rules was played there on Saturday 9 January 1864. The members of the opposing teams for this game were chosen by the President of the FA (A. Pember) and the Secretary (E. C. Morley) and included many well-known footballers of the day.
After the first match according to the new FA rules a toast was given "Success to football, irrespective of class or creed".
Charles Alcock (of Harrow School) of the Wanderers was elected to the committee of the FA in 1866, becoming its first full-time secretary and treasurer in 1870. He masterminded the creation of the Football Association Cup—the longest-running association football competition in the world—in 1871. Fifteen participating clubs subscribed to purchase a trophy. The first Cup Final was held at The Oval on 16 March 1872, fought between the Wanderers and the Royal Engineers (RE), watched by 2,000 spectators.
This competition was initially contested by mostly amateur teams but by the end of the 19th century it was dominated by professional teams that were mostly members of the Football League that had been founded in 1888 and expanded during the 1890s.
After many years of wrangling between the London Association and the Sheffield Football Association, the FA Cup brought the acceptance that one undisputed set of laws was required. The two associations had played 16 inter-association matches under differing rules; the Sheffield Rules, the London Rules and Mixed Rules. In April 1877, those laws were set with a number of Sheffield Rules being incorporated.
In 1992, the Football Association took control of the newly created Premier League which consisted of 22 clubs who had broken away from the First Division of the Football League. The Premier League reduced to 20 clubs in 1995 and is one of the richest football leagues in the world.
The Football Association celebrated their 150th year by changing their logo. The new logo has retained the current logo's three lions but it would be in golden colour and also have "The FA" written above and also have "1863 150 years 2013" written below. It also has some writings of the laws of the game penned at the first meeting held at The Freemasons' Tavern.
Crown dependencies.
The Football Associations within the Crown dependencies Jersey (Jersey Football Association), Guernsey (Guernsey Football Association) and the Isle of Man (Isle of Man Football Association) are affiliated to the Football Association despite having a separate identity from that of the United Kingdom and by extension England. They are considered County Football Associations by the Football Association. Matt Le Tissier and Graeme Le Saux have represented The Football Associations' full national representative team and were born in Guernsey and Jersey respectively.
The Guernsey Football Association, Isle of Man Football Association and Jersey Football Association have been affiliated with the Football Association since 1903, 1908 and 1905 respectively.
The British Overseas Territory of Gibraltar's Gibraltar Football Association were affiliated the Football Association from 1911 until they opted to become a fully recognised member of UEFA, a feat achieved after a 14-year legal battle. Joseph Nunez, the Gibraltar FA President claimed they were "unilaterally thrown out" of the FA following an intervention from Geoff Thompson.
A loophole was closed in May 2008 by FIFA which allowed players born in the Channel Islands to choose which nation belonging to the United Kingdom to present at international level. During the 1990s, Trevor Wood (Jersey) and Chris Tardif (Guernsey) represented Northern Ireland.
Relationship with FIFA.
The Football Association first joined FIFA in 1905. The "British Associations" (England, Ireland, Scotland and Wales) opted to leave FIFA after World War I after FIFA chose not to exclude those who were part of the Central Powers from the organisation. The British Associations' stance had changed by 1922 and in 1924 they had rejoined FIFA.
The British Olympic Association had fought against 'broken time' - monetary compensation for athletes' earnings when competing in the Olympic games. At the 1925 Olympic Congress in Prague, the British had made an amendment that concluded governing federations should define amateur status for their sports but only in accordance with the definition of amateurism accepted by the Olympic Congress. In 1928, Switzerland proposed to FIFA that in certain circumstances, 'broken time' payments should be allowed and FIFA accepted. The FA resigned from FIFA in protest against the proposal. As a result of The FA's resignation, England did not participate in the 1930, 1934 or 1938 FIFA World Cup.
At the 1930 Olympic Congress in Berlin, Belgian delegates proposed that for each sport the definition of amateur status be left to its international federation. The BOA argued for a common definition of amateurism and argued that 'broken time' payments were against the Olympic ideal.
The FA rejoined FIFA in 1946 and participated in their first World Cup in 1950. One of the first actions of the Football Association was to request the expulsion of the German and Japanese national football associations for their countries' role in World War II. Germany and Japan were prevented from qualifying for the 1950 FIFA World Cup as a consequence. They were re-acquainted with FIFA in 1950 following a second request from Switzerland who had a previous request rejected in 1948.
Finances.
The FA's main commercial asset is its ownership of the rights to England internationals and the FA Cup. Turnover for the year ending 31 December 2008 was £261.8 million. on which it made an operating profit of £16.6 million and loss before tax of £15.3 million. The loss was attributable to £39.6 million of interest payable and similar charges, principally relating to the cost of constructing the new Wembley Stadium, opened in 2006, which the FA owns via its subsidiary Wembley National Stadium Limited. For the 4 seasons from 2008 to 2012, the FA has secured £425 million from ITV and Setanta for England and FA Cup games domestic television rights, a 42% increase over the previous contract, and £145 million for overseas television rights, up 272% on the £39 million received for the previous four-year period. However during 2008–09 Setanta UK went into administration, which weakened the FA's cashflow position.
The FA's income does not include the turnover of English football clubs, which are independent businesses. As well as running its own operations the FA chooses five charities each year to which it gives considerable financial support.
During the last three years, The FA received £350,000 in fines from players over comments made on Twitter, the most recent fine being a £25,000 to Rio Ferdinand. The highest fine given during the last three years was a £90,000 fine to Ashley Cole in 2012 after calling The FA "a bunch of twats." The FA has been more and more strict on comments made by players on Twitter, as The FA has disciplined 121 players overall in the last three years.
Competitions.
The FA also runs several competitions:
Principals.
The FA has a figurehead President, since 1939, who is always a member of the British Royal Family. The Chairman of the FA has overall responsibility for policy. Traditionally this person rose through the ranks of the FA's committee structure (e.g. by holding posts such the chairmanship of a county football association). In 2008 the politician David Triesman was appointed as the FA's first "independent chairman", that is the first from outside the football hierarchy. The day to day head of the FA was known as the Secretary until 1989, when the job title was changed to Chief Executive.
Board of directors.
None of the FA board of directors has ever played football professionally.
Taken from thefa.com website on 2014-08-06
General Secretary: Alex Horne
Roger Burden (Gloucestershire FA)†

</doc>
<doc id="46137" url="http://en.wikipedia.org/wiki?curid=46137" title="Rafael E. Núñez">
Rafael E. Núñez

Rafael E. Núñez is a professor of cognitive science at the University of California, San Diego and a proponent of embodied cognition. He co-authored "Where Mathematics Comes From" with George Lakoff.

</doc>
<doc id="46138" url="http://en.wikipedia.org/wiki?curid=46138" title="Sicherheitsdienst">
Sicherheitsdienst

Sicherheitsdienst (English: Security Service), full title Sicherheitsdienst des Reichsführers-SS, or SD, was the intelligence agency of the SS and the Nazi Party in Nazi Germany. The organization was the first Nazi Party intelligence organization to be established and was considered a sister organization with the Gestapo, which the SS had infiltrated heavily after 1934. Between 1933 and 1939, the SD was administered as an independent SS office, after which it was transferred to the authority of the Reich Main Security Office ("Reichssicherheitshauptamt", or RSHA), as one of its seven departments/offices. Its first director, Reinhard Heydrich, intended for the SD to bring every single individual within the Third Reich's reach under "continuous supervision."
Following Germany's defeat in World War II, the SD was declared a criminal organisation at the Nuremberg Trials, along with the rest of Reinhard Heydrich's Reich Security Main Office (including the Gestapo) both individually and as branches of the SS in the collective. Heydrich's successor, Ernst Kaltenbrunner, was sentenced to death for war crimes at the Nuremberg Tribunals and hanged in 1946.
History.
The SD was one of the oldest security organizations of the SS and was first formed in 1931 as the "Ic-Dienst", operating out of a single apartment and reporting directly to Heinrich Himmler. Himmler appointed a former naval officer, Reinhard Heydrich, to organise the small agency. The office was renamed "Sicherheitsdienst" (SD) in the summer of 1932. The SD became more powerful after the Nazis took control of Germany and the SS started infiltrating all leading positions of the security apparatus of the Reich. Even before Hitler came to power, the SD was a veritable "watchdog" over the SS and members of the Nazi Party and played a critical role in consolidating political police powers into the hands of Himmler and Heydrich.
Growth of SD and SS power.
Once Hitler was appointed Chancellor by German President, Paul von Hindenburg, he quickly made efforts to manipulate the aging president. On 28 February 1933, Hitler convinced Hindenburg to declare a state of emergency which suspended all civil liberties throughout Germany, due at least in part to the Reichstag Fire the night before, assuring Hindenburg throughout that he was attempting to stabilize the tumultuous political scene in Germany by taking a "defensive measure against Communist acts of violence endangering the state." Wasting no time, Himmler set the SD in motion as they began creating an extensive card index of the Nazi regime's political opponents, arresting labor organizers, socialists, Jewish leaders, journalists, and communists in the process, sending them to their new prison facility near Munich, Dachau. Himmler's SS and SD made their presence felt at once by helping rid the regime of its known political enemies and its perceived ones, as well. As far as Heydrich and Himmler were concerned, the SD left their mission somewhat vaguely defined so as to "remain an instrument for all eventualities." One of those eventualities would soon arise.
For a while, the SS was in ‘competition’ with the "Sturmabteilung" (SA) for influence within the Third Reich. Himmler distrusted the SA and came to deplore the ‘rabble-rousing’ brownshirts (despite once having been a member) and what they considered to be the indecent sexual deviants amid its leadership. At least one pretext to secure additional influence for Himmler's SS and Heydrich's SD in "protecting" Hitler and securing his absolute trust in their intelligence collection abilities involved thwarting a plot from Ernst Roehm's SA using subversive means.
On 20 April 1934 Hermann Göring handed over control of the Gestapo to Himmler. Heydrich, named chief of the Gestapo by Himmler on 22 April 1934, also continued as head of the SD. These events further extended Himmler’s control of the security mechanism of the Reich, which by proxy also strengthened the surveillance power of Heydrich’s SD, as both entities methodically infiltrated every police agency in Germany. Thereafter, the SD was made the sole "Party information service" on 9 June 1934.
Under pressure from the "Reichswehr" (German armed forces) leadership, whose members viewed the enormous armed forces of the SA as an existential threat and with the collusion of Göring, Joseph Goebbels, the Gestapo and SD, Hitler was led to believe that Roehm’s SA posed a serious conspiratorial threat requiring a drastic and immediate solution. For its part, the SD provided fictitious information that there was an assassination plot on Hitler’s life and that an SA putsch to assume power was imminent since they were allegedly amassing weapons. Additionally, reports were coming in to the SD and Gestapo that the vulgarity of the SA's behavior was damaging the party and was even making antisemitism less palatable. In what became known as the Night of the Long Knives, the SS took one of its most decisive steps in eliminating its competition for command of security within the Third Reich and established itself firmly in the Nazi hierarchy, making the SS and its intelligence organ, the SD, responsible only to the Führer. Moreover, the brutal crushing of the SA and its leadership sent a clear message to everyone that opposition to Hitler’s regime could be deadly. In many ways, it struck fear across the Nazi leadership and a tangible concern about the reach and influence of Himmler’s intelligence collection and policing powers since not only had the SA’s chief been murdered, but numerous party functionaries and “opponents” had been eliminated in the action based on an extensive list which Hitler only saw after the event.
The SD and Austria.
During the autumn of 1937, Hitler secured Mussolini’s support to annex Austria (Mussolini was originally apprehensive of the Nazi takeover of Austria) and informed his generals of his intentions to invade both Austria and Czechoslovakia. Getting Mussolini to approve political intrigue against Austria was a major accomplishment as the Italian leader had expressed great concern previously in the wake of an Austrian SS unit’s attempt to stage a coup not more than three weeks after the Roehm affair; an episode that embarrassed the SS, enraged Hitler, and which ended in the assassination of Austrian Chancellor Engelbert Dollfuss. Nonetheless, to facilitate the incorporation of Austria into the greater Reich, the SD and Gestapo went to work arresting people right away using lists compiled by Heydrich. Heydrich’s SD and Austrian SS members received financing from Berlin to harass Austrian Chancellor von Schuschnigg’s government all throughout 1937. One section of the SD that was nothing more than a front for subversive activities against Austria, ironically promoted “German-Austrian peace.”
Throughout the events leading to the "Anschluß" and even after the Nazis marched into Austria, Heydrich - convinced that only his SD could pull off a peaceful union between the two German-speaking nations - organized demonstrations, conducted clandestine operations, ordered terror attacks, distributed propaganda materials, encouraged the intimidation of opponents, and had his SS and SD personnel round-up prominent anti-Nazis, most of whom ended up in Mauthausen concentration camp. Once the "Anschluß" was official, the Austrian police was immediately subordinated to Heydrich’s SD, SS and the Gestapo. Machinations by the SD, the Gestapo, and the SS helped to bring Austria into Hitler's grasp and on 13 March 1938, he signed into law the union with Austria as tears streamed down his face.
“Case Green” and the Sudetenland.
Concomitant to their machinations against Austria, the SD was also afoot in subversive activities throughout Czechoslovakia. Focusing on the Sudetenland with its 3 million ethnic Germans and the disharmony there which the Czech government could not seem to remedy, Hitler set Heydrich’s SD in motion there in what later came to be known as "Case Green". This SD intelligence operation was akin to their earlier efforts in Austria; however, unlike Austria, the Czechs fielded their own Secret Service against which, Heydrich had to contend. Once "Case Green" (which included military invasion to smash Czechoslovakia) began (as early as 1937), Heydrich’s SD spies began covertly gathering (even going so far as having SD agents use their spouses and children in the cover scheme) every conceivable type of intelligence data possible using a myriad of cameras and photographic equipment, focusing their efforts on important strategic locations like government buildings, police stations, postal services, public utilities, logistical routes, and above all, airfields. The SD activities in this regard can only be described as military espionage.
Hitler worked out a sophisticated plan to acquire the Sudetenland, which included manipulating Slovak nationalists to vie for independence and the suppression of this movement by the Czech government. Under directions from Heydrich, SD operative Alfred Naujocks was once again activated to engage in sabotage activities designed to incite a response from the Slovakians and the Czechs, a mission that ultimately failed. In June 1938, a directive from the head SD office indicated that Hitler issued an order at Jueterbog to his generals to prepare for the invasion of Czechoslovakia.
To hasten a presumed heavy response from the French, British, and Czechs, Hitler then upped the stakes and claimed that the Czechs were slaughtering Sudeten Germans, demanding the unconditional and prompt cession of the Sudetenland to Germany in order to secure the safety of endangered ethnic Germans. It was around this time that early plots from select members of the German General Staff to rid themselves of Hitler arose. How much the SD knew about schemes to subvert Hitler remains unknown. 
Eventually a diplomatic showdown pitting Hitler against the governments of Czechoslovakia, Great Britain, and France, whose tepid reaction to Austria precipitated this crisis to some degree, ensued. The Sudetenland Crisis came to an end when Neville Chamberlain and Hitler signed the Munich Agreement on 29 September 1938, effectively ceding the Sudetenland to Nazi Germany. Involvement in international affairs by the SD certainly did not end there and they remained active in foreign operations to such a degree that the head of the Foreign Ministry office, Joachim von Ribbentrop, complained of their meddling, since Hitler would apparently make decisions based on SD reports without consulting him. Following the Sudetenland Crisis, the SD then took part in operations against Poland.
Intrigue against Poland.
Aside from their participation in diminishing the power of the SA and their scheme to kill Ernst Roehm, the SD took part in international intrigue, first by activities in Austria, again in Czechoslovakia, and then by helping provoke the 'reactive' war against Poland. Code-named "Operation Himmler" and part of Hitler's plan to justify an attack upon Poland, the SD's clandestine activity for this mission included faking a Polish attack against 'innocent Germans' at a German radio station in Gleiwitz. Using concentration camp inmates condemned to die, the SD fitted them with Polish Army uniforms Heinz Jost had acquired from Admiral Canaris' "Abwehr". Leading this mission and personally selected by Heydrich was SS veteran Alfred Naujocks, who later reported during a War Criminal proceeding that he brought a Polish-speaking German along so he could broadcast a message in Polish from the German radio station 'under siege' that it was time for an all out confrontation between Germans and Poles. To add documented proof of this attack, the SD operatives placed the fictitious Polish troops (killed by lethal injection, then shot for appearance) around the 'attacked' radio station with the intention of taking members of the press to the site of the incident. Immediately in the wake of the staged incidents on 1 September 1939, Hitler proclaimed from the Reichstag in a famous radio address that German soldiers had been 'returning' fire since 0545 in the morning, setting the Second World War in Europe into motion.
Tasks and general structure.
The SD was tasked with the detection of actual or potential enemies of the Nazi leadership and the neutralization of this opposition as the action against the SA demonstrated. To fulfill this task, the SD created an organization of agents and informants throughout the Reich and later throughout the occupied territories, all part of the development of an extensive SS state and a totalitarian regime without parallel. The organization consisted of a few hundred full-time agents and several thousand informants. Historian George C. Browder writes that SD regiments were comparable to SS regiments, in that:
The SD was mainly the information-gathering agency, and the Gestapo, and to a degree the "Kriminalpolizei" (Kripo), was the executive agency of the political police system. Both the SD and the "Geheime Staatspolizei" (Gestapo) were departments under Heydrich's control which answered to Himmler as both Chief of the German Police and "Reichsfuhrer-SS", but the Kripo kept a level of independence since its structure was longer-established. 
Part and parcel to intelligence operations, the SD carefully tracked foreign opinion and criticism of Nazi policies, censoring when necessary and likewise publishing hostile political cartoons in the SS weekly magazine, "Das Schwarze Korps". An additional task assigned to the SD and the "Gestapo" was keeping tabs on the morale of the German population at large which meant they were charged to "carefully supervise the political health of the German ethnic body" and once any symptoms of "disease and germs" appeared, it was their job to "remove them by every appropriate means." When the Nuremberg Laws were passed in 1935, the SD reported that the measures against the Jews were well received by the German populace.
In 1936, the police were divided into the "Ordnungspolizei" (Orpo or Order Police) and the "Sicherheitspolizei" (SiPo or Security Police). The "Ordnungspolizei" consisted mainly of the "Schutzpolizei" (Urban police), the "Gendarmerie" (Rural police) and the "Gemeindepolizei" (Municipal police). The "Sicherheitspolizei" was composed of the Kripo and the Gestapo. Heydrich became Chief of the SiPo (Security Police) and continued as Chief of the SD. Continuing escalation of antisemitic policies in the spring of 1937 from the SD organization concerned with Jewish affairs, staffed by members like Adolf Eichmann, Herbert Hagen, and Theodor Dannecker, led to an advocation for the complete removal ("Entfernung") of all Jews from Germany with little concern for where they were headed. 
Due to the fact that the Gestapo and SD had parallel duties, Heydrich tried to reduce any confusion or related territorial disputes through a decree on 1 July 1937, clearly defining the SD's area of responsibility as those dealing with "learning ("Wissenschaft"), art, party and state, constitution and administration, foreign lands, Freemasonry and associations" whereas the "Gestapo's jurisdiction was Marxism, treason, and emigrants." Additionally, the SD was responsible for matters related to "churches and sects, pacifism, the Jews, right-wing movements", as well as "the economy, and the Press", but the SD was instructed to "avoid all matters which touched the 'state police executive powers' ["staatspolizeiliche Vollzugsmaßnahmen"] since these belonged to the Gestapo, as did all individual cases."
In 1938, the SD was made the intelligence organization for the State as well as for the Party, supporting the "Gestapo" and working with the General and Interior Administration. As such, the SD came into immediate, fierce competition with the German "Abwehr" (military intelligence), headed by Admiral Wilhelm Canaris. The competition stemmed from Heydrich and Himmler's intention to absorb the "Abwehr" and Admiral Canaris' view of the SD as an amateur upstart. Canaris refused to give up the autonomy that his military intelligence organ was granted. Additional problems also existed, like the racial exemption for members of the "Abwehr" from the Nazi Aryan screening process, and then there was competition for resources which occurred throughout the Third Reich's existence.
On 27 September 1939, the "Sicherheitspolizei" became a part of the RSHA under Heydrich. The operational sections of the SD became (department) "Amt" III and for foreign intelligence, "Amt" VI; the Gestapo became "Amt" IV and the Kripo became "Amt" V. Otto Ohlendorf was named the Chief of "Amt" III, the SD-Inland (within Germany); Heinrich Müller was named the Chief of "Amt" IV, the Gestapo; Arthur Nebe was named the Chief of "Amt" V, the Kripo; and Walter Schellenberg became Chief of "Amt" VI, the SD-Ausland (outside Germany). In 1944, the sections of the "Abwehr" were incorporated into "Amt" VI.
SD relationship to the "Einsatzgruppen".
The SD was the overarching agency under which the "Einsatzgruppen der Sicherheitspolizei und des SD", also known as the "Einsatzgruppen", was subordinated; this was one of the principal reasons for the later war-crimes indictment against the organization by the Allies. The "Einsatzgruppen’s" part in the Holocaust has been well documented. Its mobile killing units were active in the implementation of the Final Solution in the territories overrun by the Nazi war machine. This SD subsidiary worked closely with the Wehrmacht in persecuting Jews, communists, partisans, and other groups, as well. Starting with the invasion of Poland throughout the campaign in the East, the "Einsatzgruppen" ruthlessly killed anyone suspected of being an opponent of the regime, either real or imagined. The men of the "Einsatzgruppen" were recruited from the SD, Gestapo, Kripo, Orpo, and Waffen-SS.
On 31 July 1941 Hermann Göring gave written authorisation to SD Chief Heydrich to ensure the cooperation of administrative leaders of various government departments in the implementation of a "Endlösung der Judenfrage" (Final Solution to the Jewish question) in territories under German control. An SD headquarter's memorandum indicated that the SD was tasked to accompany military invasions so as to assist in control and pacification efforts. The memo explicitly stated:
Correspondingly, SD affiliated units, including the "Einsatzgruppen" followed German troops into Austria, the Sudetenland, Bohemia, Moravia, Poland, Lithuania, as well as Russia. Since their task included cooperating with military leadership and vice versa, suppression of opposition in the occupied territories was a joint venture, so any attempt to feign ignorance on the part of the military leadership loses credibility in light of the orders passed up and down their respective commands. There were territorial disputes and disagreement about how some of these policies were to be implemented.
On 20 January 1942, Heydrich chaired a meeting, now called the Wannsee Conference, to discuss the implementation of the plan. Facilities such as Chelmno, Maydenek, Sobibor, Treblinka, and Auschwitz have their origins in the planning actions undertaken by Heydrich. Heydrich remained chief of the Security Police (SiPo) and the SD (through the RSHA) until his assassination in 1942, after which Ernst Kaltenbrunner was named chief by Himmler on 30 January 1943, and remained there until the end of the war. The SD was declared a criminal organization after the war and its members were tried as war criminals at Nuremberg. Whatever their original purpose, the SD and SS were ultimately created to identify and eradicate internal enemies of the State, as well as to pacify, subjugate, and exploit conquered territories and peoples. 
Organization.
By 1933, the organization was known as the SS "SD-Amt" and, in 1934, became the official security organization of the entire Nazi Party. Consisting at first of paid agents and a few hundred unpaid informants scattered across Germany, the SD was quickly professionalized under Heydrich, who commissioned National Socialist academics and lawyers to ensure that the SS and the SD in particular, operated "within the framework of National Socialist ideology." Heydrich was given the power to select men for the SD from among any of the SS component commands since Himmler considered the organization of the SD so important. In 1939, the SD was divided into two offices, the "Inland-SD" and "Ausland-SD", and placed under the authority of the RSHA.
By 1941, the SD had been organized into the following internal sections:
Inland-SD.
The "Inland-SD" (Office II) was originally headed by SS-Colonel Hermann Behrends until September 1939 and it was within this organization that Adolf Eichmann began working out the details for the Final Solution of the Jewish problem. The "Inland SD" was responsible for intelligence and security within Germany and was divided into the following sub-offices:
After 27 September 1939, (Office II) became officially "Amt" III (department III), the SD-Inland of the RSHA. Otto Ohlendorf was named the Chief of "Amt" III.
Ausland-SD.
The "Ausland-SD" (Office III) was the civilian foreign intelligence agency of the Third Reich and was "nominally commanded by Heydrich, but his chief of staff was SS-Colonel Heinz Jost." Jost ran the department until March 1942. Jost was fired from his position as Chief of "Ausland-SD" which, as of September 1939, had officially become known as "Amt" VI (department VI) of the RSHA. Jost's place was taken by "Brigadeführer" Walter Schellenberg, a deputy of Heydrich. After the July 20 Plot in 1944, the "Ausland-SD" took over the functions of the "Abwehr" (military intelligence). The "Ausland-SD" was divided into the following sections:
Membership.
Given the nature of the intelligence operations assigned to the SD, there were clear delineations between what constituted a full member ("Mitglieder") of the SD and those who were considered "associates" ("Mitarbeiter") with a further subset for clerical support personnel (typists, file clerks, etc.) who were connoted as V-persons ("Vertrauensleute"). All SD personnel, whether simply associates or full members were required to swear an oath of secrecy, had to meet all the requirements for SS membership, were assigned SD code numbers ("Chiffre Nummer") and if they were "above the level of V-person" they had to carry "an SD identification card." The vast majority of early SD members were relatively young, but the officers were typically older by comparison; nevertheless, the average age of an SD member was approximately 2 years older than the average Nazi Party member. Much like the Nazi revolution in general, membership in the SS and the SD appealed more to the impressionable youth. Most SD members were Protestant by faith, had served in the military, and generally had a significant amount of education, representing "an educated elite" in the general sense - with about 14 percent of them earning doctorate degrees. Heydrich viewed the SD as spiritual-elite leaders within the SS and the "cream of the cream of the NSDAP."
According to historian George C. Browder, "SD men represented no pathological or psychically susceptible group. Few were wild or extreme Nazi fanatics. In those respects they were 'ordinary men'. Yet in most other respects, they were an extraordinary mix of men, drawn together by a unique mix of missions." Along with members of the Gestapo, SD personnel were "regarded with a mixture of fear and foreboding," and people wanted as little to do with them as possible. Belonging to the security apparatus of the Third Reich obviously had its advantages but it was also fraught with occupationally related social disadvantages as well, and if post-war descriptions of the SD by historians are any indication, membership therein implied being a part of a "ubiquitous secret society" which was "sinister" and a "messenger of terror" not just for the German population, but within the "ranks of the Nazi Party itself."
Security forces.
The SD and the SiPo were the main sources of officers for the security forces in occupied territories. SD-SiPo led battalions were typically placed under the command of the SS and Police Leaders, reporting directly to the RSHA in Berlin. The SD also maintained a presence at all concentration camps and supplied personnel, on an as-needed basis, to such special action troops as the "Einsatzgruppen". In fact, all members of the "Einsatzgruppen" wore the SD sleeve diamond on their uniforms. The SD-SiPo was also the primary agency, in conjunction with the "Ordnungspolizei", assigned to maintain order and security in the Jewish ghettos established by the Germans on the territory of occupied Eastern Europe. On 7 December 1941, the same day that the American naval station at Pearl Harbor was bombed by the Japanese, the first extermination camp was opened at Chelmno near Lodz by the SD and SiPo commander in occupied Poznań (Posen), then SS-"Standartenführer" Ernst Damzog. Damzog had personally selected the staff for the killing centre and later supervised the daily operation of the camp which was under the command of SS-"Hauptsturmführer "Herbert Lange. Over a span of approximately 15 months, 150,000 people were killed there.
Local offices.
The SD also maintained local offices in Germany's cities and larger towns. The small offices were known as "SD-Unterabschnitte", and the larger offices were referred to as "SD-Abschnitte". All SD offices answered to a local commander known as the "Inspektor des Sicherheitspolizei und SD" who, in turn, was under the dual command of the RSHA and local SS and Police Leaders.
Infiltration.
According to the book "Piercing the Reich", the SD was infiltrated in 1944 by a Russian who was working for the Americans. The agent's parents had fled the Russian Revolution, and he had been raised in Berlin, and then moved to Paris. He was recruited by Albert Jolis of the Office of Strategic Services (OSS) Seventh Army detachment. The mission was codenamed RUPPERT.
Early plots against the Führer.
How extensive the SD’s knowledge was about the early plots to kill Hitler by key members of the military remains a contested subject and a veritable unknown. According to British historian John W. Wheeler-Bennett, “in view of the wholesale destruction of Gestapo archives it is improbable that this knowledge will ever be forthcoming. That the authorities were aware of serious 'defeatism' is certain, but it is doubtful whether they suspected anyone of outright treason.”
References.
</dl>

</doc>
<doc id="46143" url="http://en.wikipedia.org/wiki?curid=46143" title="Planner (programming language)">
Planner (programming language)

Planner (often seen in publications as "PLANNER" although it is not an acronym) is a programming language designed by Carl Hewitt at MIT, and first published in 1969. First, subsets such as Micro-Planner and Pico-Planner were implemented, and then essentially the whole language was implemented as "Popler" by Julian Davies at the University of Edinburgh in the POP-2 programming language. Derivations such as QA4, Conniver, QLISP and Ether (see Scientific Community Metaphor) were important tools in Artificial Intelligence research in the 1970s, which influenced commercial developments such as KEE and ART.
Procedural approach versus logical approach.
The two major paradigms for constructing semantic software systems were procedural and logical. The procedural paradigm was epitomized by 
Lisp [McCarthy "et al." 1962] which featured recursive procedures that operated on list structures.
The logical paradigm was epitomized by uniform proof procedure resolution theorem provers [Robinson 1965]. According to the logical paradigm it was “cheating” to incorporate procedural knowledge [Green 1969].
Procedural embedding of knowledge.
Planner was invented for the purposes of the procedural embedding of knowledge [Hewitt 1971] and was a rejection of the resolution uniform proof procedure paradigm [Robinson 1965], which
Planner was a kind of hybrid between the procedural and logical paradigms because it combined programmability with logical reasoning. Planner featured a procedural interpretation of logical sentences where an implication of the form (P implies Q) can be procedurally interpreted in the following ways using pattern-directed invocation:
In this respect, the development of Planner was influenced by natural deductive logical systems (especially the one by Frederic Fitch [1952]).
Micro-planner implementation.
A subset called Micro-Planner was implemented by Gerry Sussman, Eugene Charniak and Terry Winograd [Sussman, Charniak, and Winograd 1971] and was used in Winograd's natural-language understanding program SHRDLU, Eugene Charniak's story understanding work, Thorne McCarty's work on legal reasoning, and some other projects. This generated a great deal of excitement in the field of AI. It also generated controversy because it proposed an alternative to the logic approach that had been one of the mainstay paradigms for AI.
At SRI International, Jeff Rulifson, Jan Derksen, and Richard Waldinger developed QA4 which built on the constructs in Planner and introduced a context mechanism to provide modularity for expressions in the database. Earl Sacerdoti and Rene Reboh developed QLISP, an extension of QA4 embedded in INTERLISP, providing Planner-like reasoning embedded in a procedural language and developed in its rich programming environment. QLISP was used by Richard Waldinger and Karl Levitt for program verification, by Earl Sacerdoti for planning and execution monitoring, by Jean-Claude Latombe for computer-aided design, by Richard Fikes for deductive retrieval, and by Steven Coles for an early expert system that guided use of an econometric model.
Computers were expensive. They had only a single slow processor and their memories were very small by comparison with today. So Planner adopted some efficiency expedients including the following:
The genesis of Prolog.
Gerry Sussman, Eugene Charniak, Seymour Papert and Terry Winograd visited the University of Edinburgh in 1971, spreading the news about Micro-Planner and SHRDLU and casting doubt on the resolution uniform proof procedure approach that had been the mainstay of the Edinburgh Logicists. At the University of Edinburgh, Bruce Anderson implemented a subset of Micro-Planner called PICO-PLANNER (Anderson 1972) and Julian Davies (1973) implemented essentially all of Planner.
According to Donald MacKenzie, Pat Hayes recalled the impact of a visit from Papert to Edinburgh, which had become the "heart of artificial intelligence's Logicland," according to Papert's MIT colleague, Carl Hewitt. Papert eloquently voiced his critique of the resolution approach dominant at Edinburgh "...and at least one person upped sticks and left because of Papert." [MacKenzie 2001 pg 82.]
The above developments generated tension among the Logicists at Edinburgh. These tensions were exacerbated when the UK Science Research Council commissioned Sir James Lighthill to write a report on the AI research situation in the UK. The resulting report [Lighthill 1973; McCarthy 1973] was highly critical although SHRDLU was favorably mentioned.
Pat Hayes visited Stanford where he learned about Planner. When he returned to Edinburgh, he tried to influence his friend Bob Kowalski to take Planner into account in their joint work on automated theorem proving. "Resolution theorem-proving was demoted from a hot topic to a relic of the misguided past. Bob Kowalski doggedly stuck to his faith in the potential of resolution theorem proving. He carefully studied Planner.” according to Bruynooghe, Pereira, Sickmann, and van Emden [2004]. Kowalski [1988] states "I can recall trying to convince Hewitt that Planner was similar to SL-resolution." But Planner was invented for the purposes of the procedural embedding of knowledge and was a rejection of the resolution uniform proof procedure paradigm. Colmerauer and Roussel recalled their reaction to learning about Planner in the following way:
"While attending an IJCAI convention in September ‘71 with Jean Trudel, we met Robert Kowalski again and heard a lecture by Terry Winograd on natural language processing. The fact that he did not use a unified formalism left us puzzled. It was at this time that we learned of the existence of Carl Hewitt’s programming language, Planner [Hewitt, 1969]. The lack of formalization of this language, our ignorance of Lisp and, above all, the fact that we were absolutely devoted to logic meant that this work had little influence on our later research." [Colmerauer and Roussel 1996]
In the fall of 1972, Philippe Roussel implemented a language called Prolog (an abbreviation for PROgrammation en LOGique - French for "programming in logic"). Prolog programs are generically of the following form (which is a special case of the backward-chaining in Planner):
Prolog duplicated the following aspects of Micro-Planner:
Prolog also duplicated the following capabilities of Micro-Planner which were pragmatically useful for the computers of the era because they saved space and time:
Use of the Unique Name Assumption and Negation as Failure became more questionable when attention turned to Open Systems [Hewitt and de Jong 1983, Hewitt 1985, Hewitt and Inman 1991].
The following capabilities of Micro-Planner were omitted from Prolog:
Prolog did not include negation in part because it raises implementation issues. Consider for example if negation were included in the following Prolog program:
The above program would be unable to prove "not" P even though it follows by the rules of mathematical logic. This is an illustration of the fact that Prolog (like Planner) is intended to be a programming language and so does not (by itself) prove many of the logical consequences that follow from a declarative reading of its programs.
The work on Prolog was valuable in that it was much simpler than Planner. However, as the need arose for greater expressive power in the language, Prolog began to include many of the capabilities of Planner that were left out of the original version of Prolog.
References.
</dl>

</doc>
<doc id="46145" url="http://en.wikipedia.org/wiki?curid=46145" title="Solaris (operating system)">
Solaris (operating system)

Solaris is a Unix operating system originally developed by Sun Microsystems. It superseded their earlier SunOS in 1993. Oracle Solaris, as it is now known, has been owned by Oracle Corporation since Oracle's acquisition of Sun in January 2010.
Solaris is known for its scalability, especially on SPARC systems, and for originating many innovative features such as DTrace, ZFS and Time Slider. Solaris supports SPARC-based and x86-based workstations and servers from Oracle and other vendors, with efforts underway to port to additional platforms. Solaris is registered as compliant with the Single Unix Specification.
Historically, Solaris was developed as proprietary software. In June 2005, Sun Microsystems released most of the codebase under the CDDL license, and founded the OpenSolaris open source project. With OpenSolaris, Sun wanted to build a developer and user community around the software. After the acquisition of Sun Microsystems in January 2010, Oracle decided to discontinue the OpenSolaris distribution and the development model. Just ten days before the internal Oracle memo announcing this decision to employees was "leaked", Garrett D'Amore had announced the illumos project, creating a fork of the Solaris kernel and launching what has since become a thriving alternative to Oracle Solaris.
In August 2010, Oracle discontinued providing public updates to the source code of the Solaris Kernel, effectively turning Solaris 11 into a closed source proprietary operating system. However, through the Oracle Technology Network (OTN), industry partners can still gain access to the in-development Solaris source code. Source code for the open source components of Solaris 11 is available for download from Oracle.
History.
In 1987, AT&T Corporation and Sun announced that they were collaborating on a project to merge the most popular Unix variants on the market at that time: BSD, System V, and Xenix. This became Unix System V Release 4 (SVR4).
On September 4, 1991, Sun announced that it would replace its existing BSD-derived Unix, SunOS 4, with one based on SVR4. This was identified internally as SunOS 5, but a new marketing name was introduced at the same time: Solaris 2. Although SunOS 4.1."x" micro releases were retroactively named Solaris 1 by Sun, the Solaris name is used almost exclusively to refer to the SVR4-derived SunOS 5.0 and later.
The justification for this new "overbrand" was that it encompassed not only SunOS, but also the OpenWindows graphical user interface and Open Network Computing (ONC) functionality. The SunOS minor version is included in the Solaris release number. For example, Solaris 2.4 incorporated SunOS 5.4. After Solaris 2.6, Sun dropped the "2." from the number, so Solaris 7 incorporates SunOS 5.7, and the latest release SunOS 5.11 forms the core of Solaris 11.2.
Supported architectures.
Solaris uses a common code base for the platforms it supports: SPARC and i86pc (which includes both x86 and x86-64).
Solaris has a reputation for being well-suited to symmetric multiprocessing, supporting a large number of CPUs. It has historically been tightly integrated with Sun's SPARC hardware (including support for 64-bit SPARC applications since Solaris 7), with which it is marketed as a combined package. This has led to more reliable systems, but at a cost premium compared to commodity PC hardware. However, it has supported x86 systems since Solaris 2.1 and 64-bit x86 applications since Solaris 10, allowing Sun to capitalize on the availability of commodity 64-bit CPUs based on the x86-64 architecture. Sun has heavily marketed Solaris for use with both its own "x64" workstations and servers based on AMD Opteron and Intel Xeon processors, as well as x86 systems manufactured by companies such as Dell, Hewlett-Packard, and IBM. As of 2009, the following vendors support Solaris for their x86 server systems:
As of July 2010, Dell and HP certify and resell Oracle Solaris, Oracle Enterprise Linux and Oracle VM on their respective x86 platforms,
and IBM stopped direct support for Solaris on x64 kit.
Other platforms.
Solaris 2.5.1 included support for the PowerPC platform (PowerPC Reference Platform), but the port was canceled before the Solaris 2.6 release. In January 2006 a community of developers at Blastwave began work on a PowerPC port which they named "Polaris". In October 2006, an OpenSolaris community project based on the Blastwave efforts and Sun Labs' "Project Pulsar", which re-integrated the relevant parts from Solaris 2.5.1 into OpenSolaris, announced its first official source code release.
A port of Solaris to the Intel Itanium architecture was announced in 1997 but never brought to market.
On November 28, 2007, IBM, Sun, and Sine Nomine Associates demonstrated a preview of OpenSolaris for System z running on an IBM System z mainframe under z/VM, called "Sirius" (in analogy to the Polaris project, and also due to the primary developer's Australian nationality: HMS "Sirius" of 1786 was a ship of the First Fleet to Australia). On October 17, 2008 a prototype release of Sirius was made available and on November 19 the same year, IBM authorized the use of Sirius on System z IFL processors.
Solaris also supports the Linux platform ABI, allowing Solaris to run native Linux binaries on x86 systems. This feature is called "Solaris Containers for Linux Applications" or SCLA, based on the branded zones functionality introduced in Solaris 10 8/07.
Installation and usage options.
Solaris can be installed from various pre-packaged software groups, ranging from a minimalistic "Reduced Network Support" to a complete "Entire Plus OEM". Installation of Solaris is not necessary for an individual to use the system. Additional software, like Apache, MySQL, etc. can be installed as well in a packaged form from "sunfreeware" and OpenCSW. Solaris can be installed from physical media or a network for use on a desktop or server, or be without installing on a desktop or server.
Desktop environments.
Early releases of Solaris used OpenWindows as the standard desktop environment. In Solaris 2.0 to 2.2, OpenWindows supported both NeWS and X applications, and provided backward compatibility for SunView applications from Sun's older desktop environment. NeWS allowed applications to be built in an object oriented way using PostScript, a common printing language released in 1982. The X Window System originated from MIT's Project Athena in 1984 and allowed for the display of an application to be disconnected from the machine where the application was running, separated by a network connection. Sun’s original bundled SunView application suite was ported to X.
Sun later dropped support for legacy SunView applications and NeWS with OpenWindows 3.3, which shipped with Solaris 2.3, and switched to X11R5 with Display Postscript support. The graphical look and feel remained based upon OPEN LOOK. OpenWindows 3.6.2 was the last release under Solaris 8. The OPEN LOOK Window Manager (olwm) with other OPEN LOOK specific applications were dropped in Solaris 9, but support libraries were still bundled, providing long term binary backwards compatibility with existing applications. The OPEN LOOK Virtual Window Manager (olvwm) can still be downloaded for Solaris from and works on releases as recent as Solaris 10.
Sun and other Unix vendors created an industry alliance to standardize Unix desktops. As a member of COSE, the Common Open Software Environment initiative, Sun helped co-develop the Common Desktop Environment. CDE was an initiative to create a standard Unix desktop environment. Each vendor contributed different components: Hewlett-Packard contributed the window manager, IBM provided the file manager, and Sun provided the e-mail and calendar facilities as well as drag-and-drop support (ToolTalk). This new desktop environment was based upon the Motif look and feel and the old OPEN LOOK desktop environment was considered legacy. CDE unified Unix desktops across multiple open system vendors. CDE was available as an unbundled add-on for Solaris 2.4 and 2.5, and was included in Solaris 2.6 through 10. In 2001, Sun issued a preview release of the open-source desktop environment GNOME 1.4, based on the GTK+ toolkit, for Solaris 8. Solaris 9 8/03 introduced GNOME 2.0 as an alternative to CDE. Solaris 10 includes Sun's Java Desktop System (JDS), which is based on GNOME and comes with a large set of applications, including StarOffice, Sun's office suite. Sun describes JDS as a "major component" of Solaris 10. The Java Desktop System is not included in Solaris 11 which instead ships with a stock version of GNOME. Likewise, CDE applications are no longer included in Solaris 11, but many libraries remain for binary backwards compatibility.
The open source desktop environments KDE and Xfce, along with numerous other window managers, also compile and run on recent versions of Solaris.
Sun was investing in a new desktop environment called Project Looking Glass since 2003. The project has been inactive since late 2006.
License.
From 2005–2010, when Solaris was still being developed by Sun Microsystems, Solaris' source code (with a few exceptions) was released under the Common Development and Distribution License (CDDL) via the OpenSolaris project.
When Sun was acquired by Oracle in 2010 the OpenSolaris project was discontinued after the board became unhappy with Oracle's stance on the project. In March 2010, the previously freely available Solaris 10 was placed under a restrictive license that limited the use, modification and redistribution of the operating system. The license allowed the user to download the operating system free of charge, through the Oracle Technology Network, and use it for a 90-day trial period. After that trial period had expired the user would then have to purchase a support contract from Oracle to continue using the operating system.
With the release of Solaris 11 in 2011 the license terms changed again. The new license allows Solaris 10 and Solaris 11 to be downloaded free of charge from the Oracle Technology Network and used without a support contract indefinitely however the license only expressly permits the user to use Solaris as a development platform and expressly forbids commercial and "production" use. Educational use is permitted in some circumstances. From the OTN license:
When Solaris is used without a support contract it can be upgraded to each new "point release" however a support contract is required for access to patches and updates that are released monthly.
Version history.
Notable features of Solaris currently include DTrace, Doors, Service Management Facility, Solaris Containers, Solaris Multiplexed I/O, Solaris Volume Manager, ZFS, and Solaris Trusted Extensions.
Updates to Solaris versions are periodically released, such as Solaris 10 10/09.
In ascending order, the following versions of Solaris have been released:
A more comprehensive summary of some Solaris versions is also available. Solaris releases are also described in the Solaris 2 FAQ.
Development release.
The underlying Solaris codebase has been under continuous development since work began in the late 1980s on what was eventually released as Solaris 2.0. Each version such as Solaris 10 is based on a snapshot of this development codebase, taken near the time of its release, which is then maintained as a derived project. Updates to that project are built and delivered several times a year until the next official release comes out.
The Solaris version under development by Sun since the release of Solaris 10 in 2005 is codenamed "Nevada", and is derived from what is now the OpenSolaris codebase.
In 2003, an addition to the Solaris development process was initiated. Under the program name "Software Express for Solaris" (or just "Solaris Express"), a binary release based on the current development basis was made available for download on a monthly basis, allowing anyone to try out new features and test the quality and stability of the OS as it progressed to the release of the next official Solaris version. A later change to this program introduced a quarterly release model with support available, renamed "Solaris Express Developer Edition" (SXDE).
In 2007, Sun announced "Project Indiana" with several goals, including providing an open source binary distribution of the OpenSolaris project, replacing SXDE. The first release of this distribution was "OpenSolaris 2008.05".
The "Solaris Express Community Edition" (SXCE) was intended specifically for OpenSolaris developers. It was updated every two weeks until it was discontinued in January 2010, with a recommendation that users migrate to the OpenSolaris distribution. Although the download license seen when downloading the image files indicates its use is limited to personal, educational and evaluation purposes, the license acceptance form displayed when the user actually installs from these images lists additional uses including commercial and production environments.
SXCE releases terminated with build 130 and OpenSolaris releases terminated with build 134 a few weeks later. The next release of OpenSolaris based on build 134 was due in March 2010 but it was never fully released, though the packages were made available on the package repository. Instead, Oracle renamed the binary distribution Solaris 11 Express, changed the license terms and released build 151a as 2010.11 in November 2010.

</doc>
<doc id="46149" url="http://en.wikipedia.org/wiki?curid=46149" title="GLONASS">
GLONASS

GLONASS (Russian: ГЛОНАСС, ]; Глобальная навигационная спутниковая система; transliteration "Globalnaya navigatsionnaya sputnikovaya sistema"), or "GLObal NAvigation Satellite System", is a space-based satellite navigation system operated by the Russian Aerospace Defence Forces. It provides an alternative to Global Positioning System (GPS) and is the second alternative navigational system in operation with global coverage and of comparable precision.
Manufacturers of GPS devices say that adding GLONASS made more satellites available to them, meaning positions can be fixed more quickly and accurately, especially in built-up areas where the view to some GPS satellites is obscured by buildings. As of 2015 it is used in most smartphones.
Development of GLONASS began in the Soviet Union in 1976. Beginning on 12 October 1982, numerous rocket launches added satellites to the system until the constellation was completed in 1995. After a decline in capacity during the late 1990s, in 2001, under Vladimir Putin's presidency, the restoration of the system was made a top government priority and funding was substantially increased. GLONASS is the most expensive program of the Russian Federal Space Agency, consuming a third of its budget in 2010.
By 2010, GLONASS had achieved 100% coverage of Russia's territory and in October 2011, the full orbital constellation of 24 satellites was restored, enabling full global coverage. The GLONASS satellites' designs have undergone several upgrades, with the latest version being GLONASS-K.
History.
Inception and design.
The first satellite-based radio navigation system developed in the Soviet Union was Tsiklon, which had the purpose of providing ballistic missile submarines a method for accurate positioning. 31 Tsiklon satellites were launched between 1967 and 1978. The main problem with the system was that, although highly accurate for stationary or slow-moving ships, it required several hours of observation by the receiving station to fix a position, making it unusable for many navigation purposes and for the guidance of the new generation of ballistic missiles. In 1968–1969, a new navigation system, which would support not only the navy, but also the air, land and space forces, was conceived. Formal requirements were completed in 1970; in 1976, the government made a decision to launch development of the "Unified Space Navigation System GLONASS".
The task of designing GLONASS was given to a group of young specialists at NPO PM in the city of Krasnoyarsk-26 (today called Zheleznogorsk). Under the leadership of Vladimir Cheremisin, they developed different proposals, from which the institute's director Grigory Chernyavsky selected the final one. The work was completed in the late 1970s; the system consists of 24 satellites operating at an altitude of 20,000 km in medium circular orbit. It would be able to promptly fix the receiving station's position based on signals from 4 satellites, and also reveal the object's speed and direction. The satellites would be launched 3 at a time on the heavy-lift Proton rocket. Due to the large number of satellites needed for the program, NPO PM delegated the manufacturing of the satellites to PO Polyot in Omsk, which had better production capabilities.
Originally, GLONASS was designed to have an accuracy of 65 m, but in reality it had an accuracy of 20 m in the civilian signal and 10 m in the military signal. The first generation GLONASS satellites were 7.8 m tall, had a width of 7.2 m, measured across their solar panels, and a mass of 1,260 kg.
Achieving full orbital constellation.
In the early 1980s, NPO PM received the first prototype satellites from PO Polyot for ground tests. Many of the produced parts were of low quality and NPO PM engineers had to perform substantial redesigning, leading to a delay. On 12 October 1982, three satellites, designated Kosmos-1413, Kosmos-1414, and Kosmos-1415 were launched aboard a Proton rocket. As only one GLONASS satellite was ready in time for the launch instead of the expected three, it was decided to launch it along with two mock-ups. The American media reported the event as a launch of one satellite and "two secret objects." For a long time, the Americans could not find out the nature of those "objects". The Telegraph Agency of the Soviet Union (TASS) covered the launch, describing GLONASS as a system "created to determine positioning of civil aviation aircraft, navy transport and fishing-boats of the Soviet Union".
From 1982 through April 1991, the Soviet Union successfully launched a total of 43 GLONASS-related satellites plus five test satellites. When the Soviet Union disintegrated in 1991, twelve functional GLONASS satellites in two planes were operational; enough to allow limited usage of the system (to cover the entire territory of the country, 18 satellites would have been necessary.) The Russian Federation took over control of the constellation and continued its development. In 1993, the system, now consisting of 12 satellites, was formally declared operational and in December 1995, the constellation was finally brought to its optimal status of 24 operational satellites. This brought the precision of GLONASS on-par with the American GPS system, which had achieved full operational capability а year earlier.
Economic crisis and fall into disrepair.
Since the first generation satellites operated for 3 years each, to keep the system at full capacity, two launches per year would have been necessary to maintain the full network of 24 satellites. However, in the financially difficult period of 1989–1999, the space program's funding was cut by 80% and Russia consequently found itself unable to afford this launch rate. After the full complement was achieved in December 1995, there were no further launches until December 1999. As a result, the constellation reached its lowest point of just 6 operational satellites in 2001. As a prelude to demilitarisation, responsibility of the program was transferred from the Ministry of Defence to Russia's civilian space agency Roscosmos.
Renewed efforts and modernization.
In the 2000s, under Vladimir Putin's presidency, the Russian economy recovered and state finances improved considerably. Putin himself took special interest in GLONASS and the system's restoration was made one of the government's top priorities. For this purpose, on August 2001, the Federal Targeted Program "Global Navigation System" 2002–2011 (Government Decision No. 587) was launched. The program was given a budget of $420 million and aimed at restoring the full constellation by 2009.
On 10 December 2003, the second generation satellite design, GLONASS-M, was launched for the first time. It had a slightly larger mass than the baseline GLONASS, standing at 1,415 kg, but it had seven years lifetime, four years longer than the lifetime of the original GLONASS satellite, decreasing the required replacement rate. The new satellite also had better accuracy and ability to broadcast two extra civilian signals.
In 2006, Defence Minister Sergey Ivanov ordered one of the signals (with an accuracy of 30 m) to be made available to civilian users. Putin, however, was not satisfied with this, and demanded that the whole system should be made fully available to everyone. Consequently, on 18 May 2007, all restrictions were lifted. The accurate, formerly military-only signal with a precision of 10 m, has since then been freely available to civilian users.
During the middle of the first decade of the 21st century, the Russian economy boomed, resulting in substantial increases in the country's space budget. In 2007, the financing of the GLONASS program was increased considerably; its budget was more than doubled. While in 2006 the GLONASS had received $181 million from the federal budget, in 2007 the amount was increased to $380 million.
In the end, 140.1 billion rubles ($4.7 billion) were spent on the program 2001–2011, making it Roscosmos' largest project and consuming a third of its 2010 budget of 84.5 billion rubles.
For the period of 2012 to 2020 320 billion rubles ($10 billion) were allocated to support the system.
Restoring full capacity.
In June 2008, the system consisted of 16 satellites, 12 of which were fully operational at the time. At this point, Roscosmos aimed at having a full constellation of 24 satellites in orbit by 2010, one year later than previously planned.
In September 2008, Prime Minister Vladimir Putin signed a decree allocating additional 67 billion rubles (2.6 billion USD) to GLONASS from the federal budget.
Promoting commercial use.
Although the GLONASS constellation has reached global coverage, its commercialisation, especially development of the user segment, has been lacking compared to the American GPS system. For example, the first commercial Russian-made GLONASS navigation device for cars, Glospace SGK-70, was introduced in 2007, but it was much bigger and costlier than similar GPS receivers. In late 2010, there were only a handful of GLONASS receivers on the market, and few of them were meant for ordinary consumers. To improve the situation, the Russian government has been actively promoting GLONASS for civilian use.
To improve development of the user segment, on August 11, 2010, Sergei Ivanov announced a plan to introduce a 25% import duty on all GPS-capable devices, including mobile phones, unless they are compatible with GLONASS. The government also planned to force all car manufacturers in Russia to support GLONASS starting from 2011. This would affect all car makers, including foreign brands like Ford and Toyota, which have car assembly facilities in Russia.
GPS and phone baseband chips from major vendors Qualcomm, Exynos and Broadcom all support GLONASS in combination with GPS.
In April 2011, Sweden's Swepos, a national network of satellite reference stations which provides data for real-time positioning with meter accuracy, became the first known foreign company to use GLONASS.
Smartphones and Tablets also saw implementation of GLONASS support in 2011 with devices released that year from Xiaomi Tech Company (Xiaomi Phone 2), Sony Ericsson, Samsung (Galaxy Note Galaxy Note II, Galaxy SII, the Google Nexus 10 in late 2012), Asus, Apple (iPhone 4S and iPad Mini in late 2012) and HTC adding support for the system allowing increased accuracy and lock on speed in difficult conditions. For a more complete list of smartphones see List of smartphones using GLONASS Navigation and for a more complete list of tablets see the text GLONASS in the GPS column in the Comparison of tablet computers.
Finishing the constellation.
Russia's aim of finishing the constellation in 2010 suffered a setback when a December 2010 launch of three GLONASS-M satellites failed. The Proton-M rocket itself performed flawlessly, but the upper stage Blok DM3 (a new version which was to make its maiden flight) was loaded with too much fuel due to a sensor failure. As a result, the upper stage and the three satellites crashed into the Pacific Ocean. Kommersant estimated that the launch failure cost up to $160 million. Russian President Dmitry Medvedev ordered a full audit of the entire program and an investigation into the failure.
Following the mishap, Roscosmos activated two reserve satellites and decided to make the first improved GLONASS-K satellite, to be launched in February 2011, part of the operational constellation instead of mainly for testing as was originally planned. This would bring the total number of satellites to 23, obtaining almost complete worldwide coverage. The GLONASS-K2 was originally scheduled to be launched by 2013, however by 2012 was not expected to be launched until 2015.
In 2010, President Dmitry Medvedev ordered the government to prepare a new federal targeted program for GLONASS, covering the years 2012–2020. The original 2001 program is scheduled to end in 2011. On 22 June 2011, Roscosmos revealed that the agency was looking for a funding of 402 billion rubles ($14.35 billion) for the program. The funds would be spent on maintaining the satellite constellation, on developing and maintaining navigational maps as well as on sponsoring supplemental technologies to make GLONASS more attractive to users.
On 2 October 2011 the 24th satellite of the system, a GLONASS-M, was successfully launched from Plesetsk Cosmodrome and is now in service. This made the GLONASS constellation fully restored, for the first time since 1996.
On 5 November 2011 the Proton-M booster successfully put three GLONASS-M units in final orbit.
On Monday 28 November 2011, a Soyuz rocket, launched from the Plesetsk Cosmodrome Space Centre, placed a single GLONASS-M satellite into orbit into Plane 3.
On 26 April 2013 a single GLONASS-M satellite was delivered to the orbit by Soyuz rocket from Plesetsk Cosmodrome, restoring the constellation to 24 operational satellites, the minimum to provide global coverage.
On 2 July 2013 a Proton-M rocket, carrying 3 GLONASS-M satellites, crashed during takeoff from Baikonur Cosmodrome. It veered off the course just after leaving the pad and plunged into the ground nose first. The rocket employed a DM-03 booster, for the first time since the December 2010 launch, when the vehicle had also failed, resulting in a loss of another 3 satellites.
However, as of 2014, while the system was completed from technical point of view, the operational side was still not closed by the Ministry of Defense and its formal status was still "in development".
System description.
GLONASS is a global satellite navigation system, providing real time position and velocity determination for military and civilian users. The satellites are located in middle circular orbit at 19,100 km altitude with a 64.8 degree inclination and a period of 11 hours and 15 minutes. GLONASS' orbit makes it especially suited for usage in high latitudes (north or south), where getting a GPS signal can be problematic. The constellation operates in three orbital planes, with 8 evenly spaced satellites on each. A fully operational constellation with global coverage consists of 24 satellites, while 18 satellites are necessary for covering the territory of Russia. To get a position fix the receiver must be in the range of at least four satellites.
Signals.
FDMA.
GLONASS satellites transmit two types of signal: open standard-precision signal L1OF/L2OF, and obfuscated high-precision signal L1SF/L2SF.
The signals use similar DSSS encoding and binary phase-shift keying (BPSK) modulation as in GPS signals. All GLONASS satellites transmit the same code as their standard-precision signal; however each transmits on a different frequency using a 15-channel frequency division multiple access (FDMA) technique spanning either side from 1602.0 MHz, known as the L1 band. The center frequency is 1602 MHz + "n" × 0.5625 MHz, where "n" is a satellite's frequency channel number ("n"=−7,−6,−5...0...,6, previously "n"=0...,13). Signals are transmitted in a 38° cone, using right-hand circular polarization, at an EIRP between 25 to 27 dBW (316 to 500 watts). Note that the 24-satellite constellation is accommodated with only 15 channels by using identical frequency channels to support antipodal (opposite side of planet in orbit) satellite pairs, as these satellites will never both be in view of an earth-based user at the same time.
The L2 band signals use the same FDMA as the L1 band signals, but transmit straddling 1246 MHz with the center frequency 1246 MHz + "n"×0.4375 MHz, where "n" spans the same range as for L1. In the original GLONASS design, only obfuscated high-precision signal was broadcast in the L2 band, but starting with GLONASS-M, an additional civil reference signal L2OF is broadcast with an identical standard-precision code to the L1OF signal.
The open standard-precision signal is generated with modulo-2 addition (XOR) of 511 kbit/s pseudo-random ranging code, 50 bit/s navigation message, and an auxiliary 100 Hz meander sequence (Manchester code), all generated using a single time/frequency oscillator. The pseudo-random code is generated with a 9-stage shift register operating with a period of 1 ms.
The navigational message is modulated at 50 bits per second. The superframe of the open signal is 7500 bits long and consists of 5 frames of 30 seconds, taking 150 seconds (2.5 minutes) to transmit the continuous message. Each frame is 1500 bits long and consists of 15 strings of 100 bits (2 seconds for each string), with 85 bits (1.7 seconds) for data and check-sum bits, and 15 bits (0.3 seconds) for time mark. Strings 1-4 provide immediate data for the transmitting satellite, and are repeated every frame; the data include ephemeris, clock and frequency offsets, and satellite status. Strings 5-15 provide non-immediate data (i.e. almanac) for each satellite in the constellation, with frames I-IV each describing 5 satellites, and frame V describing remaining 4 satellites.
The ephemerides are updated every 30 minutes using data from the Ground Control segment; they use Earth Centred Earth Fixed (ECEF) Cartesian coordinates in position and velocity, and include lunisolar acceleration parameters. The almanac uses modified Keplerian parameters and is updated daily.
The more accurate high-precision signal is available for authorized users, such as the Russian Military, yet unlike the US P(Y) code which is modulated by an encrypting W code, the GLONASS restricted-use codes are broadcast in the clear using only 'security through obscurity'. The details of the high-precision signal have not been disclosed. The modulation (and therefore the tracking strategy) of the data bits on the L2SF code has recently changed from unmodulated to 250 bit/s burst at random intervals. The L1SF code is modulated by the navigation data at 50 bit/s without a Manchester meander code.
The high-precision signal is broadcast in phase quadrature with the standard-precision signal, effectively sharing the same carrier wave, but with a ten-times-higher bandwidth than the open signal. The message format of the high-precision signal remains unpublished, although attempts at reverse-engineering indicate that the superframe is composed of 72 frames, each containing 5 strings of 100 bits and taking 10 seconds to transmit, with total length of 36 000 bits or 720 seconds (12 minutes) for the whole navigational message. The additional data are seemingly allocated to critical Luni-Solar acceleration parameters and clock correction terms.
Accuracy.
At peak efficiency, the standard-precision signal offers horizontal positioning accuracy within 5–10 meters, vertical positioning within 15 meters, a velocity vector measuring within 10 cm/s, and timing within 200 ns, all based on measurements from four first-generation satellites simultaneously; newer satellites such as GLONASS-M improve on this.
GLONASS uses a coordinate datum named "PZ-90" (Earth Parameters 1990 – Parametry Zemli 1990), in which the precise location of the North Pole is given as an average of its position from 1900 to 1905. This is in contrast to the GPS's coordinate datum, WGS 84, which uses the location of the North Pole in 1984. As of September 17, 2007 the PZ-90 datum has been updated to version PZ-90.02 which differ from WGS 84 by less than 40 cm in any given direction. Since December 31, 2013, version PZ-90.11 is being broadcast, which is aligned to the International Terrestrial Reference System at epoch 2011.0 at the centimeter level.
CDMA.
Since 2008, new CDMA signals are being researched for use with GLONASS.
According to preliminary statements from GLONASS developers, there will be three open and two restricted CDMA signals. The open signal L3OC is centered at 1202.025 MHz and uses BPSK(10) modulation for both data and pilot channels; the ranging code transmits at 10.23 million chips per second, modulated onto the carrier frequency using QPSK with in-phase data and quadrature pilot. The data is error-coded with 5-bit Barker code and the pilot with 10-bit Neuman-Hoffman code.
Open L1OC and restricted L1SC signals are centered at 1600.995 MHz, and open L2OC and restricted L2SC signals are centered at 1248.06 MHz, overlapping with GLONASS FDMA signals. Open signals L1OC and L2OC use time-division multiplexing to transmit pilot and data signals, with BPSK(1) modulation for data and BOC(1,1) modulation for pilot; wide-band restricted signals L1SC and L2SC use BOC (5, 2.5) modulation for both data and pilot, transmitted in quadrature phase to the open signals; this places peak signal strength away from the center frequency of narrow-band open signals.
Binary phase-shift keying (BPSK) is used by standard GPS and GLONASS signals, however both BPSK and quadrature phase-shift keying (QPSK) can be considered as variations of quadrature amplitude modulation (QAM), specifically QAM-2 and QAM-4. Binary offset carrier (BOC) is the modulation used by Galileo, modernized GPS, and COMPASS.
The navigational message of the L3OC signal is transmitted at 100 bit/s. The navigational frame is 15 seconds (1500 bits) long and includes 5 strings of symbols each taking 3 seconds (300 bits); a frame contains ephemerides for the current satellite and part of the almanac for three satellites. The superframe consists of 8 navigational frames, so it takes 120 seconds (2 minutes) to transmit 12000 bits of almanac for all current 24 satellites; in the future, the superframe will be expanded to 10 frames or 15000 bits (150 seconds or 2.5 minutes) of data to cover full 30 satellites. The system time marker is transmitted with each string; UTC leap second correction is achieved by shortening or lengthening (zero-padding) the final string of the day by one second (100 bits), with shortened strings being discarded by the receiver. The strings have a version tag to facilitate forward compatibility: future upgrades to the message format will not break older equipment, which will continue to work by ignoring new data (as long as old types of strings are still transmitted by the constellation), but up-to-date equipment will be able to use additional information provided by newer satellites.
Glonass-K1 test satellite launched in 2011 introduced L3OC signal. The final Glonass-M satellites launched in 2014–2017 will also include L3OC signal.
Glonass-K2 satellites, to be launched in 2015, will feature a full suite of modernized CDMA signals in the existing L1 and L2 bands, which includes L1SC, L1OC, L2SC, and L2OC, as well as the L3OC signal. Glonass-K2 should gradually replace existing satellites starting from 2017, when Glonass-M launches will cease.
Glonass-KM satellites will be launched by 2025. Additional open signals are being studied for these satellites, based on the same frequencies and formats as GPS signals L5 and L1C and corresponding Galileo/COMPASS signals E1, E5a and E5b. These signals include:
Such an arrangement will allow easier and cheaper implementation of multi-standard GNSS receivers.
With the introduction of CDMA signals, the constellation will be expanded to 30 active satellites by 2025; this may require eventual deprecation of FDMA signals. The new satellites will be deployed into three additional planes, bringing the total to six planes from the current three, aided by System for Differential Correction and Monitoring (SDCM) which is a GNSS augmentation system based on a network of ground-based control stations and communication satellites Luch 5A and Luch 5B. Additional satellites may use Molniya orbit, Tundra orbit, geosynchronous orbit, or inclined orbit to offer increased regional availability, similar to Japanese QZSS system.
Satellites.
The main contractor of the GLONASS program is Joint Stock Company Reshetnev Information Satellite Systems (formerly called NPO-PM). The company, located in Zheleznogorsk, is the designer of all GLONASS satellites, in cooperation with the Institute for Space Device Engineering () and the Russian Institute of Radio Navigation and Time. Serial production of the satellites is accomplished by the company PC Polyot in Omsk.
Over the three decades of development, the satellite designs have gone through numerous improvements, and can be divided into three generations: the original GLONASS (since 1982), GLONASS-M (since 2003) and GLONASS-K (since 2011). Each GLONASS satellite has a GRAU designation 11F654, and each of them also has the military "Cosmos-NNNN" designation.
First generation.
The true first generation of GLONASS (also called Uragan) satellites were all 3-axis stabilized vehicles, generally weighing 1,250 kg and were equipped with a modest propulsion system to permit relocation within the constellation. Over time they were upgraded to Block IIa, IIb, and IIv vehicles, with each block containing evolutionary improvements.
Six Block IIa satellites were launched in 1985–1986 with improved time and frequency standards over the prototypes, and increased frequency stability. These spacecraft also demonstrated a 16-month average operational lifetime. Block IIb spacecraft, with a 2-year design lifetimes, appeared in 1987, of which a total of 12 were launched, but half were lost in launch vehicle accidents. The six spacecraft that made it to orbit worked well, operating for an average of nearly 22 months.
Block IIv was the most prolific of the first generation. Used exclusively from 1988 to 2000, and continued to be included in launches through 2005, a total of 25 satellites were launched. The design life was three years, however numerous spacecraft exceeded this, with one late model lasting 68 months.
Block II satellites were typically launched three at a time from the Baikonur Cosmodrome using Proton-K Blok-DM-2 or Proton-K Briz-M boosters. The only exception was when, on two launches, an Etalon geodetic reflector satellite was substituted for a GLONASS satellite.
Second generation.
The second generation of satellites, known as Glonass-M, were developed beginning in 1990 and first launched in 2003. These satellites possess a substantially increased lifetime of seven years and weigh slightly more at 1,480 kg. They are approximately 2.4 m in diameter and 3.7 m high, with a solar array span of 7.2 m for an electrical power generation capability of 1600 watts at launch. The aft payload structure houses 12 primary antennas for L-band transmissions. Laser corner-cube reflectors are also carried to aid in precise orbit determination and geodetic research. On-board cesium clocks provide the local clock source.
A total of 41 second generation satellites were launched through the end of 2013. As with the previous generation, the second generation spacecraft were launched in triplets using Proton-K Blok-DM-2 or Proton-K Briz-M boosters. Some where launched alone with Soyuz-2-1b/Fregat
Third generation.
GLONASS-K is a substantial improvement of the previous generation: it is the first unpressurised GLONASS satellite with a much reduced mass (750 kg versus 1,450 kg of GLONASS-M). It has an operational lifetime of 10 years, compared to the 7-year lifetime of the second generation GLONASS-M. It will transmit more navigation signals to improve the system's accuracy, including new CDMA signals in the L3 and L5 bands which will use modulation similar to modernized GPS, Galileo and Compass. The new satellite's advanced equipment—made solely from Russian components—will allow the doubling of GLONASS' accuracy. As with the previous satellites, these are 3-axis stabilized, nadir pointing with dual solar arrays. The first GLONASS-K satellite was successfully launched on 26 February 2011.
Due to their weight reduction, GLONASS-K spacecraft can be launched in pairs from the Plesetsk Cosmodrome launch site using the substantially lower cost Soyuz-2.1b boosters or in six-at-once from the Baikonur Cosmodrome using Proton-K Briz-M launch vehicles.
Ground control.
The ground control segment of GLONASS is almost entirely located within former Soviet Union territory, except for a station in Brasilia, Brazil. The Ground Control Center and Time Standards is located in Moscow and the telemetry and tracking stations are in Saint Petersburg, Ternopil, Eniseisk, and Komsomolsk-na-Amure.
Receivers.
Septentrio, Topcon, C-Nav, JAVAD, Magellan Navigation, Novatel, Leica Geosystems, Hemisphere GNSS and Trimble Inc produce GNSS receivers making use of GLONASS. NPO Progress a receiver called "GALS-A1" which combines GPS and GLONASS reception. SkyWave Mobile Communications manufactures an Inmarsat-based satellite communications terminal that uses both GLONASS and GPS. s of 2011[ [update]], some of the latest receivers in the Garmin eTrex line also support GLONASS (along with GPS). Garmin also produce a standalone Bluetooth receiver, the GLOTM for Aviation, which combines GPS, WAAS and GLONASS. Various smartphones from 2011 onwards have integrated GLONASS capability, including devices from Xiaomi Tech Company (Xiaomi Phone 2), Sony Ericsson, ZTE, Huawei, Samsung (Galaxy Note, Galaxy Note II, Galaxy S3, Galaxy S4), Apple (iPhone 4S, iPhone 5, iPhone 5C, iPhone 5S, iPhone 6 and iPhone 6 Plus), iPad Mini (LTE models only), iPad Mini 2 (LTE models only), iPad Mini 3 (LTE models only), iPad (3rd generation and 4th Generation, 4G and LTE models only [respectively]), iPad Air (LTE models only) and iPad Air 2 (LTE models only), HTC, LG, Motorola and Nokia.
Status.
Availability.
As of 1 December 2014, the GLONASS is:
The system requires 18 satellites for continuous navigation services covering the entire territory of the Russian Federation, and 24 satellites to provide services worldwide. The GLONASS system covers 100% of worldwide territory.
On 2 April 2014 the system experienced a technical failure that resulted in practical unavailability of the navigation signal for around 12 hours.
On April 14–15, 2014 nine GLONASS satellites experienced another technical failure due to software problems.
Accuracy.
According to Russian System of Differentional Correction and Monitoring's data, as of 2010[ [update]], precisions of GLONASS navigation definitions (for p=0.95) for latitude and longitude were 4.46—7.38 m with mean number of navigation space vehicles (NSV) equals 7—8 (depending on station). In comparison, the same time precisions of GPS navigation definitions were 2.00—8.76 m with mean number of NSV equals 6—11 (depending on station). Civilian GLONASS used alone is therefore very slightly less accurate than GPS. On high latitudes (north or south), GLONASS' accuracy is better than that of GPS due to the orbital position of the satellites.
Some modern receivers are able to use both GLONASS and GPS satellites together, providing greatly improved coverage in urban canyons and giving a very fast time to fix due to over 50 satellites being available. In indoor, urban canyon or mountainous areas, accuracy can be greatly improved over using GPS alone. For using both navigation systems simultaneously, precisions of GLONASS/GPS navigation definitions were 2.37—4.65 m with mean number of NSV equals 14—19 (depends on station).
In May 2009, Anatoly Perminov the then director of the Russian Federal Space Agency stated that actions were undertaken to expand GLONASS's constellation and to improve the ground segment in order to increase the navigation definition of GLONASS to an accuracy of 2.8 m by 2011. In particular, the latest satellite design, GLONASS-K has the ability to double the system's accuracy once introduced. The system's ground segment is also to undergo improvements. As of early 2012, sixteen positioning ground stations are under construction in Russia and in the Antarctic at the Bellingshausen and Novolazarevskaya bases. New stations will be built around the southern hemisphere from Brazil to Indonesia. Together, these improvements are expected to bring GLONASS' accuracy to 0.6 m or better by 2020.

</doc>
<doc id="46150" url="http://en.wikipedia.org/wiki?curid=46150" title="Lua (programming language)">
Lua (programming language)

Lua ( , from Portuguese: "lua" ] meaning "moon"; explicitly not "LUA") is a lightweight multi-paradigm programming language designed as a scripting language with extensible semantics as a primary goal. Lua is cross-platform since it is written in ANSI C, and has a relatively simple C API.
History.
Lua was created in 1993 by Roberto Ierusalimschy, Luiz Henrique de Figueiredo, and Waldemar Celes, members of the Computer Graphics Technology Group (Tecgraf) at the Pontifical Catholic University of Rio de Janeiro, in Brazil.
From 1977 until 1992, Brazil had a policy of strong trade barriers (called a market reserve) for computer hardware and software. In that atmosphere, Tecgraf's clients could not afford, either politically or financially, to buy customized software from abroad. Those reasons led Tecgraf to implement the basic tools it needed from scratch.
Lua's historical "father and mother" were the data-description/configuration languages "SOL" (Simple Object Language) and "DEL" (data-entry language). They had been independently developed at Tecgraf in 1992–1993 to add some flexibility into two different projects (both were interactive graphical programs for engineering applications at Petrobras company). There was a lack of any flow-control structures in SOL and DEL, and Petrobras felt a growing need to add full programming power to them.
As the language's authors wrote, in "The Evolution of Lua":
In 1993, the only real contender was Tcl, which had been explicitly designed to be embedded into applications. However, Tcl had unfamiliar syntax, did not offer good support for data description, and ran only on Unix platforms. We did not consider LISP or Scheme because of their unfriendly syntax. Python was still in its infancy. In the free, do-it-yourself atmosphere that then reigned in Tecgraf, it was quite natural that we should try to develop our own scripting language ... Because many potential users of the language were not professional programmers, the language should avoid cryptic syntax and semantics. The implementation of the new language should be highly portable, because Tecgraf's clients had a very diverse collection of computer platforms. Finally, since we expected that other Tecgraf products would also need to embed a scripting language, the new language should follow the example of SOL and be provided as a library with a C API.
Lua 1.0 was designed in such a way that its object constructors, being then slightly different from the current light and flexible style, incorporated the data-description syntax of SOL (hence the name Lua – "sol" is Portuguese for sun; "lua" is moon). Lua syntax for control structures was mostly borrowed from Modula (if, while, repeat/until), but also had taken influence from CLU (multiple assignments and multiple returns from function calls, as a simpler alternative to reference parameters or explicit pointers), C++ ("neat idea of allowing a local variable to be declared only where we need it"), SNOBOL and AWK (associative arrays). In an article published in "Dr. Dobb's Journal", Lua's creators also state that LISP and Scheme with their single, ubiquitous data structure mechanism (the list) were a major influence on their decision to develop the table as the primary data structure of Lua.
Lua semantics have been increasingly influenced by Scheme over time, especially with the introduction of anonymous functions and full lexical scoping.
Versions of Lua prior to version 5.0 were released under a license similar to the BSD license. From version 5.0 onwards, Lua has been licensed under the MIT License. Both are permissive free software licences and are almost identical.
Features.
Lua is commonly described as a "multi-paradigm" language, providing a small set of general features that can be extended to fit different problem types, rather than providing a more complex and rigid specification to match a single paradigm. Lua, for instance, does not contain explicit support for inheritance, but allows it to be implemented with metatables. Similarly, Lua allows programmers to implement namespaces, classes, and other related features using its single table implementation; first-class functions allow the employment of many techniques from functional programming; and full lexical scoping allows fine-grained information hiding to enforce the principle of least privilege.
In general, Lua strives to provide flexible meta-features that can be extended as needed, rather than supply a feature-set specific to one programming paradigm. As a result, the base language is light — the full reference interpreter is only about 180 kB compiled — and easily adaptable to a broad range of applications.
Lua is a dynamically typed language intended for use as an extension or scripting language, and is compact enough to fit on a variety of host platforms. It supports only a small number of atomic data structures such as boolean values, numbers (double-precision floating point by default), and strings. Typical data structures such as arrays, sets, lists, and records can be represented using Lua's single native data structure, the table, which is essentially a heterogeneous associative array.
Lua implements a small set of advanced features such as first-class functions, garbage collection, closures, proper tail calls, coercion (automatic conversion between string and number values at run time), coroutines (cooperative multitasking) and dynamic module loading.
By including only a minimum set of data types, Lua attempts to strike a balance between power and size.
Example code.
The classic hello world program can be written as follows:
print('Hello World!')
It can also be written as
io.write('Hello World!\n')
or, the example given on 
io.write("Hello world, from ",_VERSION,"!\n")
Comments use the following syntax, similar to that of Ada, Eiffel, Haskell, SQL and VHDL:
-- A comment in Lua starts with a double-hyphen and runs to the end of the line.
-- Multi-line strings & comments
 are adorned with double square brackets. 
--[=[ Comments like this can have other --comments nested. ]=]
The factorial function is implemented as a function in this example:
function factorial(n)
 local x = 1.
 for i = 2,n do
 x = x * i
 end
 return x
end
Loops.
Lua has four types of loops: the while loop, the repeat loop (similar to a do while loop), the for loop, and the generic for loop.
--condition = true
while condition do
 --statements
end
repeat
 --statements
until condition
for i = first,last,delta do --delta may be negative, allowing the for loop to count down or up
 --statements
 --example: print(i)
end
The generic for loop:
for key, value in pairs(_G) do
 print(key, value)
end
would iterate over the table _G using the standard iterator function pairs, until it returns nil.
Functions.
Lua's treatment of functions as first-class values is shown in the following example, where the print function's behavior is modified:
do
 local oldprint = print
 -- Store current print function as oldprint
 function print(s)
 -- Redefine print function, the usual print function can still be used 
 through oldprint. The new one has only one argument.
 oldprint(s == "foo" and "bar" or s)
 end
end
Any future calls to print will now be routed through the new function, and because of Lua's lexical scoping, the old print function will only be accessible by the new, modified print.
Lua also supports closures, as demonstrated below:
function addto(x)
 -- Return a new function that adds x to the argument
 return function(y)
 --[=[ When we refer to the variable x, which is outside of the current
 scope and whose lifetime would be shorter than that of this anonymous
 function, Lua creates a closure.]=]
 return x + y
 end
end
fourplus = addto(4)
print(fourplus(3)) -- Prints 7
--This can also be achieved by calling the function in the following way:
print(addto(4)(3))
-- This is because we are calling the returned function from `addto(4)' with the argument `3' directly.
 This also helps to reduce data cost and up performance if being called iteratively.
A new closure for the variable x is created every time addto is called, so that each new anonymous function returned will always access its own x parameter. The closure is managed by Lua's garbage collector, just like any other object.
Tables.
Tables are the most important data structures (and, by design, the only built-in composite data type) in Lua, and are the foundation of all user-created types. They are conceptually similar to associative arrays in PHP, dictionaries in Python and Hashes in Ruby or Perl.
A table is a collection of key and data pairs, where the data is referenced by key; in other words, it's a hashed heterogeneous associative array. A key (index) can be any value but nil and NaN. A numeric key of 1 is considered distinct from a string key of "1".
Tables are created using the codice_1 constructor syntax:
a_table = {} -- Creates a new, empty table
Tables are always passed by reference (See Call by sharing):
a_table = {x = 10} -- Creates a new table, with one entry mapping "x" to the number 10.
print(a_table["x"]) -- Prints the value associated with the string key, in this case 10.
b_table = a_table
b_table["x"] = 20 -- The value in the table has been changed to 20.
print(b_table["x"]) -- Prints 20.
print(a_table["x"]) -- Also prints 20, because a_table and b_table both refer to the same table.
As record.
A table is often used as structure (or record) by using strings as keys. Because such use is very common, Lua features a special syntax for accessing such fields.
Example:
point = { x = 10, y = 20 } -- Create new table
print(point["x"]) -- Prints 10
print(point.x) -- Has exactly the same meaning as line above. The easier-to-read
 -- dot notation is just syntactic sugar.
Quoting the Lua 5.1 Reference Manual:
"The syntax var.Name is just syntactic sugar for var['Name'];"
As namespace.
By using a table to store related functions, it can act as a namespace.
Point.new = function(x, y)
end
Point.set_x = function(point, x)
 point.x = x -- point["x"] = x;
end
As array.
By using a numerical key, the table resembles an array data type. Lua arrays are 1-based: the first index is 1 rather than 0 as it is for many other programming languages (though an explicit index of 0 is allowed).
A simple array of strings:
array = { "a", "b", "c", "d" } -- Indices are assigned automatically.
print(array[2]) -- Prints "b". Automatic indexing in Lua starts at 1.
print(#array) -- Prints 4. # is the length operator for tables and strings.
array[0] = "z" -- Zero is a legal index.
print(#array) -- Still prints 4, as Lua arrays are 1-based.
The length of a table t is defined to be any integer index n such that t[n] is not nil and t[n+1] is nil; moreover, if t[1] is nil, n can be zero. For a regular array, with non-nil values from 1 to a given n, its length is exactly that n, the index of its last value. If the array has "holes" (that is, nil values between other non-nil values), then #t can be any of the indices that directly precedes a nil value (that is, it may consider any such nil value as the end of the array).
A two dimensional table:
ExampleTable = 
 {1,2,3,4},
print(ExampleTable[1][3]) -- Prints "3"
print(ExampleTable[2][4]) -- Prints "8"
An array of objects:
function Point(x, y) -- "Point" object constructor
 return { x = x, y = y } -- Creates and returns a new object (table)
end
array = { Point(10, 20), Point(30, 40), Point(50, 60) } -- Creates array of points
 -- array = { { x = 10, y = 20 }, { x = 30, y = 40 }, { x = 50, y = 60 } };
print(array[2].y) -- Prints 40
Using a hash map to emulate an array normally is slower than using an actual array; however, Lua tables are optimized for use as arrays to help avoid this issue.
Metatables.
Extensible semantics is a key feature of Lua, and the metatable concept allows Lua's tables to be customized in powerful ways. The following example demonstrates an "infinite" table. For any formula_1, fibs[n] will give the formula_1th Fibonacci number using dynamic programming and memoization.
fibs = { 1, 1 } -- Initial values for fibs[1] and fibs[2].
setmetatable(fibs, {
 __index = function(values, n) -- __index is a function predefined by Lua, 
 it is called if key "n" does not exist. 
 values[n] = values[n - 1] + values[n - 2] -- Calculate and memorize fibs[n].
 return values[n]
 end
Object-oriented programming.
Although Lua does not have a built-in concept of classes, they can be implemented using two language features: first-class functions and tables. By placing functions and related data into a table, an object is formed. Inheritance (both single and multiple) can be implemented via the metatable mechanism, telling the object to look up nonexistent methods and fields in parent object(s).
There is no such concept as "class" with these techniques; rather, prototypes are used, as in the programming languages Self or JavaScript. New objects are created either with a factory method (that constructs new objects from scratch), or by cloning an existing object.
Lua provides some syntactic sugar to facilitate object orientation. To declare member functions inside a prototype table, one can use function table:func(args), which is equivalent to function table.func(self, args). Calling class methods also makes use of the colon: object:func(args) is equivalent to object.func(object, args).
Creating a basic vector object:
Vector.__index = Vector
function Vector:new(x, y, z) -- The constructor
 return setmetatable({x = x, y = y, z = z}, Vector)
end
function Vector:magnitude() -- Another method
 -- Reference the implicit object using self
 return math.sqrt(self.x^2 + self.y^2 + self.z^2)
end
local vec = Vector:new(0, 1, 0) -- Create a vector
print(vec:magnitude()) -- Call a method (output: 1)
print(vec.x) -- Access a member variable (output: 0)
Internals.
Lua programs are not interpreted directly from the textual Lua file, but are compiled into bytecode which is then run on the Lua virtual machine. The compilation process is typically invisible to the user and is performed during run-time, but it can be done offline in order to increase loading performance or reduce the memory footprint of the host environment by leaving out the compiler.
Like most CPUs, and unlike most virtual machines (which are stack-based), the Lua VM is register-based, and therefore more closely resembles an actual hardware design. The register architecture both avoids excessive copying of values and reduces the total number of instructions per function. The virtual machine of Lua 5 is one of the first register-based pure VMs to have a wide use. Perl's
Parrot and Android's Dalvik are two other well-known register-based VMs.
This example is the bytecode listing of the factorial function defined above (as shown by the luac 5.1 compiler):
 function <factorial.lua:1,7> (9 instructions, 36 bytes at 0x8063c60)
 1 param, 6 slots, 0 upvalues, 6 locals, 2 constants, 0 functions
 1 [2] LOADK 1 -1 ; 1
 2 [3] LOADK 2 -2 ; 2
 3 [3] MOVE 3 0
 4 [3] LOADK 4 -1 ; 1
 5 [3] FORPREP 2 1 ; to 7
 6 [4] MUL 1 1 5
 7 [3] FORLOOP 2 -2 ; to 6
 8 [6] RETURN 1 2
 9 [7] RETURN 0 1
C API.
Lua is intended to be embedded into other applications, and provides a C API for this purpose. The API is divided into two parts: the Lua core and the Lua auxiliary library.
The Lua API's design eliminates the need for manual reference management in C code, unlike Python's API. The API, like the language, is minimalistic. Advanced functionality is provided by the auxiliary library, which consists largely of preprocessor macros which assist with complex table operations.
Stack.
The Lua C API is stack based. Lua provides functions to push and pop most simple C data types (integers, floats, etc.) to and from the stack, as well as functions for manipulating tables through the stack. The Lua stack is somewhat different from a traditional stack; the stack can be indexed directly, for example. Negative indices indicate offsets from the top of the stack. For example, −1 is the top (most recently pushed value), while positive indices indicate offsets from the bottom (oldest value).
Marshalling data between C and Lua functions is also done using the stack. To call a Lua function, arguments are pushed onto the stack, and then the lua_call is used to call the actual function. When writing a C function to be directly called from Lua, the arguments are popped from the stack.
Example.
Here is an example of calling a Lua function from C:
int main(void)
 //create a Lua state
 lua_State *L = luaL_newstate();
 //load and execute a string
 if (luaL_dostring(L, "function foo (x,y) return x+y end")) {
 lua_close(L);
 return -1;
 //push value of global "foo" (the function defined above)
 //to the stack, followed by integers 5 and 3
 lua_getglobal(L, "foo");
 lua_pushinteger(L, 5);
 lua_pushinteger(L, 3);
 lua_call(L, 2, 1); //call a function with two arguments and one return value
 printf("Result: %d\n", lua_tointeger(L, -1)); //print integer value of item at stack top
 lua_close(L); //close Lua state
 return 0;
Running this example gives:
Special tables.
The C API also provides some special tables, located at various "pseudo-indices" in the Lua stack. At LUA_GLOBALSINDEX prior to Lua 5.2 is the globals table, _G from within Lua, which is the main namespace. There is also a registry located at LUA_REGISTRYINDEX where C programs can store Lua values for later retrieval.
Extension and binding.
It is possible to write extension modules using the Lua API. Extension modules are shared objects which can be used to extend the functionality of the interpreter by providing native facilities to Lua scripts. From the Lua side, such a module appears as a namespace table holding its functions and variables. Lua scripts may load extension modules using require, just like modules written in Lua itself.
A growing collection of modules known as "rocks" are available through a package management system called "LuaRocks", in the spirit of CPAN, RubyGems and Python Eggs.
Other modules can be found through the "Lua Addons" directory of the wiki.
Prewritten Lua bindings exist for most popular programming languages, including other scripting languages. For C++, there are a number of template-based approaches and some automatic binding generators.
Applications.
Video games.
In video game development, Lua is widely used as a scripting language by game programmers, perhaps due to its perceived easiness to embed, fast execution, and short learning curve.
In 2003, a poll conducted by GameDev.net showed Lua as a most popular scripting language for game programming. On 12 January 2012, Lua was announced as a winner of the Front Line Award 2011 from the magazine "Game Developer" in the category Programming Tools.
Other.
Other applications using Lua include:

</doc>
<doc id="46151" url="http://en.wikipedia.org/wiki?curid=46151" title="1480s BC">
1480s BC


</doc>
<doc id="46157" url="http://en.wikipedia.org/wiki?curid=46157" title="Garrett County, Maryland">
Garrett County, Maryland

Garrett County is the westernmost county of the U.S. state of Maryland. As of the 2010 census, the population was 30,097, making it the third-least populous county in Maryland. Its county seat is Oakland. The county was named for John Work Garrett (1820–1884), president of the Baltimore and Ohio Railroad. Created from Allegany County, Maryland in 1872, it was the last Maryland county to be formed.
Garrett County has long been part of the media market of Pittsburgh, Pennsylvania. It is considered to be a part of Western Maryland.
The Commonwealth of Pennsylvania is to the north. The Maryland–Pennsylvania boundary was surveyed and marked between April 1765 and October 1767 by astronomer Charles Mason and surveyor Jeremiah Dixon. This boundary is commonly known as the Mason-Dixon Line. The eastern border with Allegany County was defined by the Bauer Report, submitted to Governor Lloyd Lowndes, Jr. on November 9, 1898. The Potomac River and State of West Virginia lie to the south and west.
Garrett County lies in the Allegheny Mountains, which here form the western flank of the Appalachian Mountain Range. Hoye-Crest, a summit along Backbone Mountain, is the highest point in Maryland.
The Eastern Continental Divide runs along portions of Backbone Mountain. The western part of the county, drained by the Youghiogheny River, is the only part of Maryland within the Mississippi River drainage basin. All other parts of the county are in the Chesapeake Bay basin.
Garrett County contains over 76000 acre of parks, lakes, and publicly accessible forestland. Popular activities in the county include camping, hiking, backpacking, rock climbing, alpine and cross county skiing, snowmobiling, hunting, ice fishing, fly fishing, whitewater canoeing, kayaking, rafting, boating, swimming, sailing, horseback riding, and water skiing.
The National Register of Historic Places listings in Garrett County, Maryland has 20 National Register of Historic Places properties and districts, including Casselman Bridge, National Road a National Historic Landmark. Garrett County is part of Maryland's 6th congressional district.
History.
In the early 20th century, the railroad and tourism started to decline. Coal mining and timber production continued at a much slower pace. Today, tourism has made a dramatic rebound in the county with logging and farming making up the greatest part of the economic base. Due to a cold climate and lack of any large city, Garrett County has remained a sparsely populated rural area.
Law and government.
Government.
The County is governed by an elected Board of County Commissioners (the "Board"), whose three members serve four-year terms and must live in the District they represent. The Board is the traditional form of county government in Maryland and may exercise only such powers as are conferred by the General Assembly of Maryland.
The County is administered under a line organizational method, with the County Administrator responsible for the general administration of County Government. The administration of the County is centralized with the County Administrator responsible for overseeing the financial planning, annual budget process, personnel management, and direction and management of operations within the organization.
The county is part of Maryland's 6th congressional district and is the most Republican in the state. The Republican candidate for President has won in each of the last thirteen elections. In 2008, John McCain carried Garrett County by a 40.2% margin over Barack Obama, with Obama carrying Maryland by a 25.5% margin over McCain.
County seal.
On December 15, 1977, the seal of Garrett County went into effect by virtue of Resolution #7. The seal is elliptical, with the name "Garrett County" inscribed above the upper fourth of the ellipse, and "Maryland 1872" inscribed below the lower fourth of the ellipse. The date “1872" depicts the year of the formation of Garrett County. The seal illustrates a large snowflake to depict winter; water to represent sailing; and oaks and conifer to represent the county’s mountains. The colors are peacock blue for the sky and water. The blue and white background is divided by kelly green.
County flag.
The official flag for Garrett County is elliptical. The flag illustrates a large snowflake to depict winter; water to represent sailing; and oaks and conifer to represent the county’s mountains. The colors are peacock blue for the sky and water. The blue and white background is divided by kelly green.
Law enforcement.
The county is policed by the Garrett County Sheriff's Office and the Maryland State Police.
The state parks are police by the Department of Natural Resources Police.
Geography.
According to the U.S. Census Bureau, the county has a total area of 656 sqmi, of which 647 sqmi is land and 8.6 sqmi (1.3%) is water. It is the second-largest county in Maryland by land area.
Garrett County is Maryland's westernmost county, bordered to the north by the Mason-Dixon Line with Pennsylvania, to the south by the Potomac River and West Virginia, to the west by a land border with West Virginia, and to the east by a land border with Allegany County, Maryland. The county's northwesternmost point is approximately 60 mi southeast of Pittsburgh, Pennsylvania, and its southeasternmost point is approximately 160 mi northwest of Baltimore, Maryland.
Garrett County is located entirely within the highland zone of the Appalachian Mountains known variously as the Allegheny Mountains, the Allegheny Plateau, and the Appalachian Plateau. The county's highest elevations are located along four flat-topped ridges and range to a height of 3360 ft at Hoye-Crest along Backbone Mountain, the highest point in the state of Maryland. As is typical in the Allegheny region, broad flats generally lie below the ridge crests at elevations of approximately 500 ft. River valleys are generally narrow and deep, with ravines typically 1,000 to 1800 ft below surrounding peaks.
The county contains over 76000 acre of parks, lakes, and publicly accessible forestland. It is drained by two river systems, the Potomac and the Youghiogheny. The Savage River, a tributary of the Potomac, drains about a third of the county. The Casselman River, a tributary of the Youghiogheny, flows north from the county’s central section into Pennsylvania. The Youghiogheny itself drains the westernmost area of the county and flows north into Pennsylvania, where it empties into the Monongahela River at McKeesport, just south of Pittsburgh.
Geologic points of interest.
The Glades.
The Glades' 601 acre is of great scientific interest because it is an ombrotrophic system (fed solely by rainwater) with peat layers up to 9 ft thick, and is one of the oldest examples of mountain peatland in the Appalachians.
On the western edge of the Savage River State Forest along Maryland Route 495 lies Bittinger, Maryland. Named after Henry Bittinger who first settled in the area, other German settlers moved in and took up the fertile farm land. On the eastern edge of Bittinger is one of the largest glades area of Garrett County. Geographically, this is an area which seems to have been affected by the last great ice sheet of North America. Two miles southeast of Bittinger, there is a large deposit of peat moss.
Loess Dunes.
In the Casselman River valley, 1 mi south of Grantsville, Maryland and beside Maryland Route 495, one can see remains of geological evidence about the last great ice sheet over North America. A series of low mounds can be seen in the fields on the west side of Maryland Route 495 that are "loess" (wind-blown) material. Apparently, these are the only ones still visible in the northern part of Garrett County.
The mounds were formed when a glacier lake existed in the Casselman valley, and the ice around the edges of the frozen lake melted. Wind blew fine grains of earth into the water around the edges where it sank to the bottom, and the mounds were the result of the deposit of this wind-blown material.
Forests, rivers, caves.
See these articles for information on the forests, rivers and caves of Garrett County:
Parks and recreation.
State parks.
Six state parks are in Garrett County. All offer picnic and fishing areas; all but Casselman River State Park have hiking paths. Mountain bike paths, swimming areas, and boat launches and rentals are available at Deep Creek, Herrington Manor, and New Germany state parks. Rental cabins are available at Herrington Manor and New Germany state parks. Big Run, Deep Creek, Herrington Manor and New Germany state parks all offer canoeing, while campsites may be found at Big Run, Deep Creek, New Germany, and Swallow Falls state parks.
County parks.
Garrett County owns four park sites and fifteen recreation facilities. The parks are maintained in cooperation with local associations and civic groups. The recreation areas are attached to public schools and colleges and maintained by the Garrett County Board of Education.
Municipal parks.
The municipal parks of Garrett County provide sport facilities, hiking, bike and walk paths, playgrounds, picnic areas, boat ramps, and fishing.
Demographics.
2010.
Whereas according to the 2010 United States Census Bureau:
2000.
As of the census of 2000, there were 29,846 people, 11,476 households, and 8,354 families residing in the county. The population density was 18/km² (46/sq mi). There were 16,761 housing units at an average density of 10/km² (26/sq mi). The racial makeup of the county was 98.83% White, 0.43% Black or African American, 0.07% Native American, 0.19% Asian, 0.02% Pacific Islander, 0.09% from other races, and 0.37% from two or more races. 0.44% of the population were Hispanic or Latino of any race. 36.1% were of German, 22.9% American, 9.6% English and 8.8% Irish ancestry.
There were 11,476 households out of which 32.60% had children under the age of 18 living with them, 60.70% were married couples living together, 8.40% had a female householder with no husband present, and 27.20% were non-families. 23.50% of all households were made up of individuals and 10.60% had someone living alone who was 65 years of age or older. The average household size was 2.55 and the average family size was 3.00.
In the county the population was spread out with 25.10% under the age of 18, 7.80% from 18 to 24, 27.60% from 25 to 44, 24.60% from 45 to 64, and 14.90% who were 65 years of age or older. The median age was 38 years. For every 100 females there were 97.20 males. For every 100 females age 18 and over, there were 93.80 males.
The median income for a household in the county was $32,238, and the median income for a family was $37,811. Males had a median income of $29,469 versus $20,673 for females. The per capita income for the county was $16,219. 13.30% of the population and 9.80% of families were below the poverty line. Out of the total people living in poverty, 16.60% are under the age of 18 and 13.90% are 65 or older.
Economy.
Garrett County produces natural gas, the only county in the state to do so.
Transportation.
Airport.
Garrett County Airport (2G4) is a general aviation airport surrounded by the mountains of Western Maryland. The airport enhances the region's tourist industry and provides emergency air service evacuation and landing facilities for general aviation.
Media.
Garrett County is part of the Pittsburgh DMA, a regional media market centered in neighboring Pennsylvania.
Events.
Annual events include the Autumn Glory Festival, the Scottish Highland Festival, and the Garrett County Fair.
Communities.
Census-designated places.
The United States Census Bureau recognizes seven census-designated places (CDPs) in Garrett County.
Unincorporated communities.
The following communities are classified as populated places or locales by the Geographic Names Information System.

</doc>
<doc id="46159" url="http://en.wikipedia.org/wiki?curid=46159" title="Omaha, Nebraska">
Omaha, Nebraska

Omaha is the largest city in the state of Nebraska, United States, and is the county seat of Douglas County. It is located in the Midwestern United States on the Missouri River, about 10 mi north of the mouth of the Platte River. Omaha is the anchor of the Omaha-Council Bluffs metropolitan area, which includes Council Bluffs, Iowa, across the Missouri River from Omaha. According to the 2010 census, Omaha's population was 408,958, making it the nation's 42nd-largest city. According to the 2013 Population Estimates, Omaha's population was 434,353. Including its suburbs, Omaha formed the 60th-largest metropolitan area in the United States in 2013 with an estimated population of 895,151 residing in eight counties. The Omaha-Council Bluffs-Fremont, NE-IA Combined Statistical Area is 931,666, according to the U.S. Census Bureau's 2013 estimate. There are nearly 1.3 million residents within a 50-mile (80 km) radius of the city's center, forming the Greater Omaha area.
Omaha's pioneer period began in 1854 when the city was founded by speculators from neighboring Council Bluffs, Iowa. The city was founded along the Missouri River, and a crossing called Lone Tree Ferry earned the city its nickname, the "Gateway to the West." It introduced this new West to the world when in 1898 it played host to the World's Fair, dubbed the Trans-Mississippi Exposition. During the 19th century, Omaha's central location in the United States spurred the city to become an important national transportation hub. Throughout the rest of the 19th century, the transportation and jobbing sectors were important in the city, along with its railroads and breweries. In the 20th century, the Omaha Stockyards, once the world's largest, and its meatpacking plants, gained international prominence.
Today, Omaha is the home to the headquarters of five Fortune 500 companies: packaged-food giant ConAgra Foods; the U.S.'s largest railroad operator, Union Pacific Corporation; insurance and financial firm Mutual of Omaha; one of the world's largest construction companies, Kiewit Corporation; and mega-conglomerate Berkshire Hathaway. Berkshire Hathaway is headed by local investor Warren Buffett, one of the richest people in the world, according to a decade's worth of Forbes Magazine rankings, some of which have ranked him as high as No. 1. Omaha is also the home to five Fortune 1000 headquarters: TD Ameritrade, West Corporation, Valmont Industries, Green Plains Renewable Energy and Werner Enterprises. First National Bank of Omaha is the largest privately held bank in the United States. Headquarters for Leo A Daly, HDR, Inc. and DLR Group, three of the US's largest 10 architecture/engineering firms, are based in Omaha. The Gallup Organization, of Gallup Poll fame, also is based in Omaha, with its riverfront Gallup University. Enron began in Omaha as Northern Natural Gas in 1930 before taking over a smaller Houston company in 1985 to form InterNorth, which was moved permanently to Houston in 1987 by the notorious Kenneth Lay.
The modern economy of Omaha is diverse and built on skilled knowledge jobs. In 2009, "Forbes" identified Omaha as the nation's number one "Best Bang-For-The Buck City" and number one on "America's Fastest-Recovering Cities" list. Tourism in Omaha benefits the city's economy greatly, with the annual College World Series providing important revenue and the city's Henry Doorly Zoo serving as the top attraction in Nebraska. Omaha hosted the U.S. Olympic swim trials in 2008, 2012, and will host the event again in 2016.
Notable modern Omaha inventions include the TV dinner, developed by Omaha's then-Carl Swanson Co.; Raisin Bran, developed by Omaha's Skinner Macaroni Co.; cake mix, developed by Duncan Hines, then a division of Omaha's Nebraska Consolidated Mills, the forerunner to today's ConAgra Foods; Butter Brickle Ice Cream and the Reuben sandwich, conceived by a chef at the then-Blackstone Hotel on 33rd and Farnam Streets; center-pivot irrigation by Omaha's now-Valmont Corporation; the bobby pin and the "pink hair curler," at Omaha's Tip Top; the ski lift, in 1936, by Omaha's Union Pacific Corp; the "Top 40" radio format, pioneered by Todd Storz, scion of Omaha's Storz Brewing Co., and head of Storz Broadcasting, which was the first in the U.S. to use the "Top 40" format at Omaha's KOWH Radio. A character in a Rudyard Kipling essay claimed "dice were invented in Omaha, and the man who invented 'em, he made a colossal fortune." 
History.
Various Native American tribes had lived in the land that became Omaha, including since the 17th century, the Omaha and Ponca, Dhegian-Siouan-language people who had originated in the lower Ohio River valley and migrated west by the early 17th century; Pawnee, Otoe, Missouri, and Ioway. The word "Omaha" (actually "Umoⁿhoⁿ" or "Umaⁿhaⁿ") means "Dwellers on the bluff".
In 1804 the Lewis and Clark Expedition passed by the riverbanks where the city of Omaha would be built. Between July 30 and August 3, 1804, members of the expedition, including Meriwether Lewis and William Clark, met with Oto and Missouria tribal leaders at the Council Bluff at a point about 20 miles (30 km) north of present-day Omaha. Immediately south of that area, Americans built several fur trading outposts in succeeding years, including Fort Lisa in 1812; Fort Atkinson in 1819; Cabanné's Trading Post, built in 1822, and Fontenelle's Post in 1823, in what became Bellevue. There was fierce competition among fur traders until John Jacob Astor created the monopoly of the American Fur Company. The Mormons built a town called Cutler's Park in the area in 1846. While it was temporary, the settlement provided the basis for further development in the future.
Through 26 separate treaties with the United States federal government, Native American tribes in Nebraska gradually ceded the lands currently comprising the state. The treaty and cession involving the Omaha area occurred in 1854 when the Omaha Tribe ceded most of east-central Nebraska. Logan Fontenelle, an interpreter for the Omaha and signatory to the 1854 treaty, played an essential role in those proceedings.
Pioneer Omaha.
Before it was legal to claim land in Indian Country, William D. Brown was operating the Lone Tree Ferry to bring settlers from Council Bluffs, Iowa to the area that became Omaha. Brown is generally credited as having the first vision for a city where Omaha now sits. The passage of the Kansas–Nebraska Act in 1854 was presaged by the staking out of claims around the area to become Omaha by residents from neighboring Council Bluffs. On July 4, 1854, the city was informally established at a picnic on Capital Hill, current site of Omaha Central High School. Soon after, the Omaha Claim Club was formed to provide vigilante justice for claim jumpers and others who infringed on the land of many of the city's founding fathers. Some of this land, which now wraps around Downtown Omaha, was later used to entice Nebraska Territorial legislators to an area called Scriptown. The Territorial capitol was located in Omaha, but when Nebraska became a state in 1867, the capital was relocated to Lincoln, 53 mi south-west of Omaha. The U.S. Supreme Court later ruled against numerous landowners whose violent actions were condemned in "Baker v. Morton".
Many of Omaha's founding figures stayed at the Douglas House or the Cozzens House Hotel. Dodge Street was important early in the city's early commercial history; North 24th Street and South 24th Street developed independently as business districts, as well. Early pioneers were buried in Prospect Hill Cemetery and Cedar Hill Cemetery. Cedar Hill closed in the 1860s and its graves were moved to Prospect Hill, where pioneers were later joined by soldiers from Fort Omaha, African Americans and early European immigrants. There are several other historical cemeteries in Omaha, historical Jewish synagogues and historical Christian churches dating from the pioneer era, as well.
19th century.
The economy of Omaha boomed and busted through its early years. Omaha was a stopping point for settlers and prospectors heading west, either overland or via the Missouri River. The steamboat "Bertrand" sank north of Omaha on its way to the goldfields in 1865. Its massive collection of artifacts is on display at the nearby Desoto National Wildlife Refuge. The jobbing and wholesaling district brought new jobs, followed by the railroads and the stockyards. Groundbreaking for the First Transcontinental Railroad in 1863, provided an essential developmental boom for the city. The Union Pacific Railroad was authorized by the U.S. Congress to begin building westward railways in 1862; in January 1866 it commenced construction out of Omaha.
Equally as important, the Union Stockyards were founded in 1883. Within twenty years of the founding of the Union Stockyards in South Omaha, four of the five major meatpacking companies in the United States were located in Omaha. By the 1950s, half the city's workforce was employed in meatpacking and processing. Meatpacking, jobbing and railroads were responsible for most of the growth in the city from the late 19th century through the early decades of the 20th century.
Immigrants soon created ethnic enclaves throughout the city, including Irish in Sheelytown in South Omaha; Germans in the Near North Side, joined by the European Jews and black migrants from the South; Little Italy and Little Bohemia in South Omaha. Beginning in the late 19th century, Omaha's upper class lived in posh enclaves throughout the city, including the south and north Gold Coast neighborhoods, Bemis Park, Kountze Place, Field Club and throughout Midtown Omaha. They traveled the city's sprawling park system on boulevards designed by renowned landscape architect Horace Cleveland. The Omaha Horse Railway first carried passengers throughout the city, as did the later Omaha Cable Tramway Company and several similar companies. In 1888, the Omaha and Council Bluffs Railway and Bridge Company built the Douglas Street Bridge, the first pedestrian and wagon bridge between Omaha and Council Bluffs. Gambling, drinking and prostitution were widespread in the 19th century, first rampant in the city's Burnt District and later in the Sporting District. Controlled by Omaha's political boss Tom Dennison by 1890, criminal elements enjoyed support from Omaha's "perpetual" mayor, "Cowboy Jim" Dahlman, nicknamed for his eight terms as mayor. Calamities such as the Great Flood of 1881 did not slow down the city's violence. In 1882, the Camp Dump Strike pitted state militia against unionized strikers, drawing national attention to Omaha's labor troubles. The Governor of Nebraska had to call in U.S. Army troops from nearby Fort Omaha to protect strikebreakers for the Burlington Railroad, bringing along Gatling guns and a cannon for defense. When the event ended, one man was dead and several were wounded. In 1891, a mob hanged Joe Coe, an African-American porter after he was accused of raping a white girl. There were several other riots and civil unrest events in Omaha during this period as well.
In 1898, Omaha's leaders, under the guidance of Gurdon Wattles, held the Trans-Mississippi and International Exposition, touted as a celebration of agricultural and industrial growth throughout the Midwest. The Indian Congress, which drew more than 500 American Indians from across the country, was held simultaneously. More than 2 million visitors attended these events, located at Kountze Park and the Omaha Driving Park in the Kountze Place neighborhood.
20th century.
With dramatically increasing population in the 20th century, there was major civil unrest in Omaha, resulting from competition and fierce labor struggles. In 1900, Omaha was the center of a national uproar over the kidnapping of Edward Cudahy, Jr., the son of a local meatpacking magnate.
The city's labor and management clashed in bitter strikes, racial tension escalated as blacks were hired as strikebreakers, and ethnic strife broke out. A major riot by ethnic whites in South Omaha destroyed the city's Greek Town in 1909, completely driving out the Greek population.
The civil rights movement in Omaha has roots that extend back to 1912, when the first chapter of the National Association for the Advancement of Colored People west of the Mississippi River was founded in the city.
The Omaha Easter Sunday Tornado of 1913 destroyed much of the city's African-American community, in addition to much of Midtown Omaha.
Six years later, in 1919, the city was caught up in the Red Summer riots when thousands of ethnic whites marched from South Omaha to the courthouse to lynch a black worker, Willy Brown, a suspect in an alleged rape of a white woman. The mob burned the Douglas County Courthouse to get the prisoner, causing more than $1,000,000 damage. They hung and shot Will Brown, then burned his body. Troops were called in from Fort Omaha to quell the riot, prevent more crowds gathering in South Omaha, and to protect the black community in North Omaha.
The culture of North Omaha thrived throughout the 1920s through 1950s, with several creative figures, including Tillie Olsen, Wallace Thurman, Lloyd Hunter, and Anna Mae Winburn emerging from the vibrant Near North Side.
Musicians created their own world in Omaha, and also joined national bands and groups that toured and appeared in the city.
After the tumultuous Great Depression of the 1930s, Omaha rebounded with the development of Offutt Air Force Base just south of the city. The Glenn L. Martin Company operated a factory there in the 1940s that produced 521 B-29 "Superfortresses", including the "Enola Gay" and "Bockscar" used in the atomic bombing of Japan in World War II.
The construction of Interstates 80, 480 and 680, along with the North Omaha Freeway, spurred development. There was also controversy, particularly in North Omaha, where several neighborhoods were bisected by new routes. Creighton University hosted the DePorres Club, an early civil rights group whose sit-in strategies for integration of public facilities predated the national movement, starting in 1947.
Following the development of the Glenn L. Martin Company bomber manufacturing plant in Bellevue at the beginning of World War II, the relocation of the Strategic Air Command to the Omaha suburb in 1948 provided a major economic boost to the area.
From the 1950s through the 1960s, more than 40 insurance companies were headquartered in Omaha, including Woodmen of the World and Mutual of Omaha. By the late 1960s, the city rivaled, but never surpassed, the United States insurance centers of Hartford, Connecticut, New York City and Boston, Massachusetts.
After surpassing Chicago in meat processing by the late 1950s, Omaha suffered the loss of 10,000 jobs as both the railroad and meatpacking industries restructured. The city struggled for decades to shift its economy as workers suffered. Poverty became more entrenched among families who remained in North Omaha.
In the 1960s, three major race riots along North 24th Street destroyed the Near North Side's economic base, with recovery slow for decades. In 1969, Woodmen Tower was completed and became Omaha's tallest building and first major skyscraper at 478 ft, a sign of renewal.
Since the 1970s, Omaha has continued expanding and growing, mostly to available land to the west. West Omaha has become home to the majority of the city's population. North and South Omaha's populations continue to be centers of new immigrants, with economic and racial diversity. In 1975 a major tornado, along with a major blizzard, caused more than $100 million in damages in 1975 dollars.
Downtown Omaha has since been rejuvenated in numerous ways, starting with the development of Gene Leahy Mall and W. Dale Clark Library in the late 1970s. In the 1980s, Omaha's fruit warehouses were converted into a shopping area called the Old Market.
The demolition of Jobber's Canyon in 1989 led to the creation of the ConAgra Foods campus. Several nearby buildings, including the Nash Block, have been converted into condominiums. The stockyards were taken down; the only surviving building is the Livestock Exchange Building, which was converted to multi-use and listed on the National Register of Historic Places.
A historic preservation movement in Omaha has led to a number of historic structures and districts being designated Omaha Landmarks or listed on the National Register of Historic Places. Much of the push toward preservation came after Omaha gained the notorious designation of having, in 1989, demolished the largest-ever National Register historic district in the United States, a record that still stands as of 2013. The Jobbers Canyon Historic District, along the Missouri River, was felled for a new headquarters campus for ConAgra Foods, a company which threatened to relocate if Omaha did not allow them to raze the city's historic district. The Jobber's Canyon warehouses had before then been allowed to deteriorate and were the scene of several fires set by the homeless population that had come to live in the abandoned buildings. At the time, there were no plans in place for revitalizing the buildings.
In the 1980s and 1990s, Omaha also saw major company headquarters leave the city, including Enron, founded in the city in 1930 and taken to Houston in 1987 by the now-notorious Kenneth Lay. First Data Corporation, a large credit-card processor, also was founded in Omaha in 1969; as of 2009, its headquarters are in Atlanta.
Bozell, once one of the largest advertising agencies in the United States, according to Advertising Age, was founded in 1921 as Bozell and Jacobs and is famous for campaigns including Pork: The Other White Meat and Got Milk? The firm left for New York in 1997 when it was acquired by True North. Inacom, founded in Omaha in 1991, was a technology company that customized computer systems for large businesses, and was on the Fortune 500 list from 1997 until 2000, when it filed for bankruptcy. Northwestern Bell, the Bell System affiliate for Northwestern states, had its headquarters in Omaha from its founding in 1896 until it moved to Denver in 1991 as US West. Level 3 Communications, a large Tier 1 network provider, was founded in Omaha in 1985 as Kiewit Diversified Group, a division of Kiewit Corporation, a Fortune 500 construction and mining company still headquartered in Omaha; Level 3 moved to Denver in 1998. World Com was founded by a merger with Omaha's MFS Communications, started as Metropolitan Fiber Systems in 1993. MFS, backed by Kiewit Corporation CEO Walter Scott and Warren Buffett, purchased UUNET, one of the largest Internet backbones in the world, for $2 billion in 1996. The now-infamous Bernie Ebbers purchased the much larger MFS for $14.3 billion in 1997 under his World Com. He moved headquarters of the merged company from Omaha to Mississippi.
21st century.
Around the start of the 21st century, several new downtown skyscrapers and cultural institutions were built. One First National Center was completed in 2002, surpassing the Woodmen Tower as the tallest building in Omaha as well as in the state at 634 ft. The creation of the city's new North Downtown included the construction of the CenturyLink Center and the Slowdown/Film Streams development at North 14th and Webster Streets. Construction of the new TD Ameritrade Park began in 2009 and was completed in 2011, also in the North Downtown area, near the CenturyLink Center.
New construction has occurred throughout the city since the start of the 21st century. Important retail and office developments have occurred in West Omaha such as the Village Pointe shopping center and several business parks including First National Business Park and parks for Bank of the West and C&A Industries, Inc and Morgan Stanley Smith Barney and several others. Downtown and Midtown Omaha have both seen the development of a significant number of condominiums in recent years. In Midtown Omaha significant mixed-use projects are underway. The site of the former Ak-Sar-Ben arena has been redeveloped into a mixed use development Aksarben Village. In January 2009 Blue Cross Blue Shield of Nebraska announced plans to build a new 10 story, $98 million headquarters, in the Aksarben Village, completed in Spring 2011. Gordmans is also currently building their new corporate headquarters in Aksarben. The other major mixed-use development is Midtown Crossing at Turner Park. Developed by Mutual of Omaha, the development includes several condominium towers and retail businesses built around Omaha's Turner Park.
The Holland Performing Arts Center opened in 2005 near the Gene Leahy Mall and the Union Pacific Center opened in 2004.
There have also been several developments along the Missouri River waterfront in downtown. The Bob Kerrey Pedestrian Bridge was opened to foot and bicycle traffic on September 28, 2008. Started in 2003, RiverFront Place Condos first phase was completed in 2006 and is fully occupied and the second phase was opened in 2011. The development along Omaha's riverfront is attributed with prompting the City of Council Bluffs to move their own riverfront development time line forward.
In the summers of 2008 and 2012, the United States Olympic Team swimming trials were held in Omaha, at the Qwest/Century Link Center. The event was a highlight in the city's sports community, as well as a showcase for redevelopment in the downtown area.
Geography.
Omaha is located at . According to the United States Census Bureau, the city has a total area of 130.58 sqmi, of which 127.09 sqmi is land and 3.49 sqmi is water. Situated in the Midwestern United States on the bank of the Missouri River in eastern Nebraska, much of Omaha is built in the Missouri River Valley. Other significant bodies of water in the Omaha-Council Bluffs metropolitan area include Lake Manawa, Papillion Creek, Carter Lake, Platte River and the Glenn Cunningham Lake. The city's land has been altered considerably with substantial land grading throughout Downtown Omaha and scattered across the city. East Omaha sits on a flood plain west of the Missouri River. The area is the location of Carter Lake, an oxbow lake. The lake was once the site of East Omaha Island and Florence Lake, which dried up in the 1920s.
The Omaha-Council Bluffs metropolitan area consists of eight counties; five in Nebraska and three in Iowa. The metropolitan area now includes Harrison, Pottawattamie, and Mills Counties in Iowa and Washington, Douglas, Sarpy, Cass, and Saunders Counties in Nebraska. This area was formerly referred to only as the Omaha Metropolitan Statistical Area and consisted of only five counties: Pottawattamie in Iowa, and Washington, Douglas, Cass, and Sarpy in Nebraska. The Omaha-Council Bluffs combined statistical area comprises the Omaha-Council Bluffs metropolitan statistical area and the Fremont Micropolitan statistical area; the CSA has a population of 858,720 (2005 Census Bureau estimate). Omaha ranks as the 42nd-largest city in the United States, and is the core city of its 60th-largest metropolitan area. There are currently no consolidated city-counties in the area; the City of Omaha studied the possibility extensively through 2003 and concluded, "The City of Omaha and Douglas County should merge into a municipal county, work to commence immediately, and that functional consolidations begin immediately in as many departments as possible, including but not limited to parks, fleet management, facilities management, local planning, purchasing and personnel."
Geographically, Omaha is considered as being located in the "Heartland" of the United States. Important environmental impacts on the natural habitat in the area include the spread of invasive plant species, restoring prairies and bur oak savanna habitats, and managing the whitetail deer population.
Omaha is home to several hospitals, located mostly along Dodge St (US6). Being the county seat, it is also the location of the county courthouse.
Neighborhoods.
Omaha is generally divided into six geographic areas: Downtown, Midtown, North Omaha, South Omaha, West Omaha, and East Omaha. West Omaha includes the Miracle Hills, Boys Town, Regency, and Gateway areas. There is also a small community in East Omaha. The city has a wide range of historical and new neighborhoods and suburbs that reflect its socioeconomic diversity. Early neighborhood development happened in ethnic enclaves, including Little Italy, Little Bohemia, Little Mexico and Greek Town. According to U.S. Census data, five European ethnic enclaves existed in Omaha in 1880, expanding to nine in 1900.
Around the start of the 20th century. the City of Omaha annexed several surrounding communities, including Florence, Dundee and Benson. At the same time, the city annexed all of South Omaha, including the Dahlman and Burlington Road neighborhoods. From its first annexation in 1857 (of East Omaha) to its recent and controversial annexation of Elkhorn, Omaha has continually had an eye towards growth.
Starting in the 1950s, development of highways and new housing led to movement of middle class to suburbs in West Omaha. Some of the movement was designated as white flight from racial unrest in the 1960s. Newer and poorer migrants lived in older housing close to downtown; those residents who were more established moved west into newer housing. Some suburbs are gated communities or have become edge cities. Recently, Omahans have made strides to revitalize the downtown and Midtown areas with the redevelopment of the Old Market, Turner Park, Gifford Park, and the designation of the Omaha Rail and Commerce Historic District.
Landmark preservation.
Omaha is home to dozens of nationally, regionally and locally significant landmarks. The city has more than a dozen historic districts, including Fort Omaha Historic District, Gold Coast Historic District, Omaha Quartermaster Depot Historic District, Field Club Historic District, Bemis Park Historic District, and the South Omaha Main Street Historic District. Omaha is notorious for its 1989 demolition of 24 buildings in the Jobbers Canyon Historic District, which represents to date the largest loss of buildings on the National Register. The only original building surviving of that complex is the Nash Block.
Omaha has almost one hundred individual properties listed on the National Register of Historic Places, including the Bank of Florence, Holy Family Church, the Christian Specht Building and the Joslyn Castle. There are also three properties designated as National Historic Landmarks.
Locally designated landmarks, including residential, commercial, religious, educational, agricultural and socially significant locations across the city, honor Omaha's cultural legacy and important history. The City of Omaha Landmarks Heritage Preservation Commission is the government body that works with the mayor of Omaha and the Omaha City Council to protect historic places. Important history organizations in the community include the Douglas County Historical Society.
Climate.
Omaha, due to its latitude of 41.26˚ N and location far from moderating bodies of water or mountain ranges, displays a humid continental climate (Köppen "Dfa"), with hot, humid summers and cold, relatively dry winters. July averages 76.7 °F, with moderate, but sometimes-high humidity and relatively frequent thunderstorms, usually rather violent and capable of spawning severe weather or tornadoes; temperatures reach 90 °F on 29 days and 100 °F on 1.7 days annually. The January daily average is 23.5 °F, with lows reaching 0 °F on 11 days annually. The lowest temperature recorded in the city was -32 °F on January 5, 1884, and the highest 114 °F on July 25, 1936. Average yearly precipitation is 30.6 in, falling mostly in the warmer months. What precipitation that does fall in winter usually takes the form of snow, with average seasonal snowfall being 28.7 in.
Based on 30-year averages obtained from NOAA's National Climatic Data Center for the months of December, January and February, Weather Channel ranked Omaha the 5th coldest major U.S. city as of 2014.
Demographics.
2010 census.
As of the census of 2010, there were 408,958 people, 162,627 households, and 96,477 families residing in the city. The population density was 3217.9 PD/sqmi. There were 177,518 housing units at an average density of 1396.8 /sqmi. The racial makeup of the city was 73.1% White, 13.7% African American, 0.8% Native American, 2.4% Asian, 0.1% Pacific Islander, 6.9% from other races, and 3.0% from two or more races. Hispanic or Latino of any race were 13.1% of the population. Non-Hispanic Whites were 68.0% of the population.
There were 162,627 households of which 31.3% had children under the age of 18 living with them, 40.6% were married couples living together, 13.7% had a female householder with no husband present, 4.9% had a male householder with no wife present, and 40.7% were non-families. 32.3% of all households were made up of individuals and 9.3% had someone living alone who was 65 years of age or older. The average household size was 2.45 and the average family size was 3.14.
The median age in the city was 33.5 years. 25.1% of residents were under the age of 18; 11.4% were between the ages of 18 and 24; 27.9% were from 25 to 44; 24.4% were from 45 to 64; and 11.4% were 65 years of age or older. The gender makeup of the city was 49.2% male and 50.8% female.
2000 census.
As of the census of 2000, there were 390,007 people, 156,738 households, and 94,983 families residing within city limits. The population density was 3,370.7 people per square mile (1,301.5/km2). There were 165,731 housing units at an average density of 1,432.4 per square mile (553.1/km2). The racial makeup of the city was 78.4% White, 13.3% African American, 0.7% Native American, 1.7% Asian, 0.1% Pacific Islander, 3.9% from other races, and 1.9% from two or more races. 7.5% of the population were Hispanic or Latino of any race.
The median income for a household in the city was $40,006, and the median income for a family was $50,821. Males had a median income of $34,301 versus $26,652 for females. The per capita income for the city was $21,756. 11.3% of the population and 7.8% of families lived below the poverty line. Out of the total population, 15.6% of those under the age of 18 and 7.4% of those 65 and older were living below the poverty line.
People.
Native Americans were the first residents of the Omaha area. The city of Omaha was established by European Americans from neighboring Council Bluffs who arrived from the Northeast United States a few years earlier. While much of the early population was of Yankee stock, over the next 100 years numerous ethnic groups moved to the city. In 1910, the Census Bureau reported Omaha's population as 96.4% White and 3.6% Black. Irish immigrants in Omaha originally moved to an area in present-day North Omaha called "Gophertown", as they lived in dirt dugouts. That population was followed by Polish immigrants in the Sheelytown neighborhood, and many immigrants were recruited for jobs in South Omaha's stockyards and meatpacking industry. The German community in Omaha was largely responsible for founding its once-thriving beer industry, including the Metz, Krug, and the Storz breweries.
Since its founding, ethnic groups in the city have clustered in enclaves in north, south and downtown Omaha. In its early days, the sometimes lawless nature of a new frontier city included crime, such as illicit gambling and riots.
In the early 20th century, Jewish immigrants set up numerous businesses along the North 24th Street commercial area. It suffered with the loss of industrial jobs in the 1960s and later, the shifting of population west of the city. The commercial area is now the center of the African American community, concentrated in North Omaha. The African-American community has maintained its social and religious base, while it is currently experiencing an economic revitalization.
The Little Italy neighborhood grew south of downtown, as many Italian immigrants came to the city to work in the Union Pacific shops. Scandinavians first came to Omaha as Mormon settlers in the Florence neighborhood. Czechs had a strong political and cultural voice in Omaha, and were involved in a variety of trades and businesses, including banks, wholesale houses, and funeral homes. The Notre Dame Academy and Convent and Czechoslovak Museum are legacies of their residence. Today the legacy of the city's early European immigrant populations is evident in many social and cultural institutions in Downtown and South Omaha.
Mexicans originally immigrated to Omaha to work in the rail yards. Today they compose the majority of South Omaha's Hispanic population and many have taken jobs in meat processing. Other significant early ethnic populations in Omaha included Danes, Poles, and Swedes.
A growing number of African immigrants have made their homes in Omaha in the last twenty years. There are approximately 8,500 Sudanese living in Omaha, comprising the largest population of Sudanese refugees in the United States. Most have immigrated since 1995 because of warfare in their nation. Ten different tribes are represented, including the Nuer, Dinka, Equatorians, Maubans and Nubians. Most Sudanese people in Omaha speak the Nuer language. Other Africans have immigrated to Omaha as well, with one-third from Nigeria, and significant populations from Kenya, Togo, Cameroon and Ghana.
With the expansion of railroad and industrial jobs in meatpacking, Omaha attracted many new immigrants and migrants. As the major city in Nebraska, it has historically been more racially and ethnically diverse than the rest of the state. At times rapid population change, overcrowded housing and job competition have aroused racial and ethnic tensions. Around the start of the 20th century, violence towards new immigrants in Omaha often erupted out of suspicions and fears.
The Greek Town Riot in 1909 flared after increased Greek immigration, Greeks' working as strikebreakers, and the killing of an Irish policeman provoked violence among earlier immigrants such as ethnic Irish. That mob violence forced the Greek immigrant population to flee from the city. By 1910, 53.7% of Omaha’s residents and 64.2% of South Omaha’s residents were foreign born or had at least one parent born outside of America. Six years after the Greek Town Riot, in 1915, a Mexican immigrant named Juan Gonzalez was killed by a mob near Scribner, a town in the Greater Omaha metropolitan area. The event occurred after an Omaha Police Department officer was investigating a criminal operation selling goods stolen from the nearby railroad yards. Racial profiling targeted Gonzalez as the culprit. After escaping the city, he was trapped along the Elkhorn River, where the mob, including several policemen from Omaha, shot him more than twenty times. Afterward it was discovered that Gonzalez was unarmed, and that he had a reliable alibi for the time of the murder. Nobody was ever indicted for his lynching. In the fall of 1919, following Red Summer, postwar social and economic tensions, the earlier hiring of blacks as strikebreakers, and job uncertainty contributed to a mob from South Omaha lynching Willy Brown and the ensuing Omaha Race Riot. Trying to defend Brown, the city's mayor, Edward Parsons Smith, was lynched also, surviving only after a quick rescue.
Similar to other industrial cities in the U.S., Omaha suffered severe job losses in the 1950s, more than 10,000 in total, as both the railroad and meatpacking industries restructured. Stockyards and packing plants were located closer to ranches, and union achievements were lost as wages declined in surviving jobs. Many workers left the area if they could get to other jobs. Poverty deepened in areas of the city whose residents had depended on those jobs, specifically North and South Omaha. At the same time, with reduced revenues, the city had less financial ability to respond to longstanding problems. Despair after the assassination of Dr. Martin Luther King, Jr. in April 1968 contributed to riots in North Omaha, including one at the Logan Fontenelle Housing Project. For some, the Civil Rights Movement in Omaha, Nebraska evolved towards black nationalism, as the Black Panther Party was involved in tensions in the late 1960s. Organizations such as the Black Association for Nationalism Through Unity became popular among the city's African-American youth. This tension culminated in the "cause célèbre" trial of the Rice/Poindexter Case, in which an Omaha Police Department officer was killed by a bomb while answering an emergency call. After 5 years of obscurity, the black population was finally able to vote.
Whites in Omaha have followed the white flight pattern, suburbanizing to West Omaha over time. In the late 1990s and early 2000s, gang violence and incidents between the Omaha Police Department and members of the African-American community aggravated relations between groups in North and South Omaha. More recent Hispanic immigrants, concentrated in South Omaha, have struggled to earn living wages in meatpacking, adapt to a new society, and deal with discrimination.
Economy.
According to "USA Today", Omaha ranks eighth among the nation's 50 largest cities in both per-capita billionaires and Fortune 500 companies. With diversification in several industries, including banking, insurance, telecommunications, architecture/construction, and transportation, Omaha's economy has grown dramatically since the early 1990s. In 2001 "Newsweek" identified Omaha as one of the Top 10 high-tech havens in the nation. Six national fiber optic networks converge in Omaha.
Omaha's most prominent businessman is Warren Buffett, nicknamed the "Oracle of Omaha", who is regularly ranked one of the richest people in the world. Five Omaha-based companies: Berkshire Hathaway, ConAgra Foods, Union Pacific Railroad, Mutual of Omaha, and Kiewit Corporation, are among the "Fortune" 500.
Omaha is the headquarters of several other major corporations, including the Gallup Organization, TD Ameritrade, infoGROUP, Werner Enterprises, First National Bank, Gavilon and First Comp Insurance. Many large technology firms have major operations or operational headquarters in Omaha, including Bank of the West, First Data, PayPal and LinkedIn. The city is also home to three of the 30 largest architecture firms in the United States, including HDR, Inc., DLR Group, Inc., and Leo A Daly. Omaha has the fifth highest percentage of low-income African Americans in the country. In 2013, "Forbes"' named Omaha among its list of the Best Places for Business and Careers.
Top employers.
According to the Greater Omaha Economic Development Partnership, the largest regional employers are:
Tourism.
Tourist attractions in Omaha include history, sports, outdoors and cultural experiences. Its principal tourist attractions are the Henry Doorly Zoo and the College World Series. The Old Market in Downtown Omaha is another major attraction and is important to the city's retail economy. The city has been a tourist destination for many years. Famous early visitors included British author Rudyard Kipling and General George Crook. In 1883 Omaha hosted the first official performance of the Buffalo Bill's Wild West Show for eight thousand attendees. In 1898 the city hosted more than 1,000,000 visitors from across the United States at the Trans-Mississippi and International Exposition, a world's fair that lasted for more than half the year.
Research on leisure and hospitality situates Omaha in the same tier for tourists as the neighboring cities of Des Moines, Iowa, Topeka, Kansas, Kansas City, Missouri, Oklahoma City, Oklahoma, Denver, Colorado, and Sioux Falls, South Dakota. A recent study found that investment of $1 million in cultural tourism generated approximately $83,000 in state and local taxes, and provided support for hundreds of jobs for the metropolitan area, which in turn led to additional tax revenue for government.
Culture.
The city's historical and cultural attractions have been lauded by numerous national newspapers, including the "Boston Globe" and The "New York Times". Omaha is home to the Omaha Community Playhouse, the largest community theater in the United States. The Omaha Symphony Orchestra and its modern Holland Performing Arts Center, the Opera Omaha at the Orpheum theater, the Blue Barn Theatre, and The Rose Theater form the backbone of Omaha's performing arts community. Opened in 1931, the Joslyn Art Museum has significant art collections. Since its inception in 1976, Omaha Children's Museum has been a place where children can challenge themselves, discover how the world works and learn through play. The Bemis Center for Contemporary Arts, one of the nation's premier urban artist colonies, was founded in Omaha in 1981, and the Durham Museum is accredited with the Smithsonian Institution for traveling exhibits. The city is also home to the largest singly funded mural in the nation, , by Meg Saligman. The annual Omaha Blues, Jazz, & Gospel Festival celebrates local music along with the Omaha Black Music Hall of Fame.
In 1955 Omaha's Union Stockyards overtook Chicago's stockyards as the United States' meat packing center. This legacy is reflected in the cuisine of Omaha, with renowned steakhouses such as Gorat's and the recently closed Mister C's, as well as the retail chain Omaha Steaks.
Henry Doorly Zoo.
The Henry Doorly Zoo is widely considered one of the premier zoos in the world. The zoo is home to the world's largest nocturnal exhibit and indoor swamp; the world's largest indoor rainforest, the world's largest indoor desert, and the largest geodesic dome in the world (13 stories tall). The Zoo is Nebraska’s number one paid attendance attraction and has welcomed more than 25 million visitors over the past 40 years.
Old Market.
The Old Market is a major historic district in Downtown Omaha listed on the National Register of Historical Places. Today, its warehouses and other buildings house shops, restaurants, bars, coffee shops, and art galleries. Downtown is also the location of the Omaha Rail and Commerce Historic District, which has several art galleries and restaurants as well. The Omaha Botanical Gardens features 100 acre with a variety of landscaping, and the new Kenefick Park recognizes Union Pacific Railroad's long history in Omaha. North Omaha has several historical cultural attractions including the Dreamland Historical Project, Love’s Jazz and Art Center, and the John Beasley Theater. The annual River City Roundup is celebrated at Fort Omaha, and the neighborhood of Florence celebrates its history during "Florence Days". Native Omaha Days is a biennial event celebrating Near North Side heritage.
Religious institutions reflect the city's heritage. The city's Christian community has several historical churches dating from the founding of the city. There are also all sizes of congregations, including small, medium and megachurches. Omaha hosts the only Church of Jesus Christ of Latter-day Saints temple in Nebraska, along with a significant Jewish community. There are 152 parishes in the Roman Catholic Archdiocese of Omaha, and several Orthodox Christian congregations throughout the city.
Music.
Omaha's rich history in rhythm and blues, and jazz gave rise to a number of influential bands, including Anna Mae Winburn's Cotton Club Boys and Lloyd Hunter's Seranaders. Rock and roll pioneer Wynonie Harris; jazz great Preston Love; drummer Buddy Miles; and Luigi Waites are among the city's homegrown talent. Doug Ingle from the late 1960s band Iron Butterfly was born in Omaha as was indie-folk singer/songwriter Elliott Smith, though both were raised elsewhere.
Today, the diverse culture of Omaha includes a variety of performance venues, museums, and musical heritage, including the historically significant jazz scene in North Omaha and the modern and influential "Omaha Sound".
Contemporary music groups either located in or originally from Omaha include Mannheim Steamroller, Bright Eyes, The Faint, Cursive, Azure Ray, Tilly and the Wall and 311. During the late 1990s, Omaha became nationally known as the birthplace of Saddle Creek Records, and the subsequent "Omaha Sound" was born from their bands' collective style.
Omaha also has a fledgling hip hop scene. Long-time bastion Houston Alexander, a one-time graffiti artist and professional Mixed Martial Arts competitor, is currently a local hip-hop radio show host. Cerone Thompson, known as "Scrybe," has had a number one single on college radio stations across the United States. He has also had several number one hits on the local hip hop station respectively titled, "Lose Control" and "Do What U Do". More recently, in 2009 Eric Scheid, also known as "Titus," released a single called "What Do You Believe" featuring Bizzy Bone from the nationally known hip hop group Bone Thugs-n-Harmony. The single was produced by Omaha producer J Keez. The record was released by Smashmode Publishing and Timeless Keys Music Publishing which are two Omaha-based music publishing companies. South Omaha's OTR Familia, consisting of MOC and Xpreshin aka XP, have worked with Fat Joes Terror Squad on several songs and have participated in summer concerts with Pitbull, Nicky Jam, and Aventura.
A long heritage of ethnic and cultural bands have come from Omaha. The Omaha Black Music Hall of Fame celebrates the city's long history of African-American music and the Strathdon Caledonia Pipe Band carries on a Scottish legacy. Internationally renowned composer Antonín Dvořák wrote his Ninth ("New World") Symphony in 1893 based on his impressions of the region after visiting Omaha's robust Czech community. In the period surrounding World War I Valentin J. Peter encouraged Germans in Omaha to celebrate their rich musical heritage, too. Frederick Metz, Gottlieb Storz and Frederick Krug were influential brewers whose beer gardens kept many German bands active.
Popular culture.
In 1939, the world premiere of the film "Union Pacific" was held in Omaha, Nebraska and the accompanying three-day celebration drew 250,000 people. A special train from Hollywood carried director Cecil B. DeMille and stars Barbara Stanwyck and Joel McCrea. Omaha's Boys Town was made famous by the Spencer Tracy and Mickey Rooney movie "Boys Town". Omaha has been featured in recent years by a handful of relatively big budget motion pictures. The city's most extensive exposure can be accredited to Omaha native Alexander Payne, the Oscar-nominated director who shot parts of "About Schmidt", "Citizen Ruth" and "Election" in the city and suburbs of Papillion and La Vista.
Built in 1962, Omaha's Cinerama was called Indian Hills Theater. Its demolition in 2001 by the Nebraska Methodist Health System was unpopular, with objections from local historical and cultural groups and luminaries from around the world. The Dundee Theatre is the lone surviving single-screen movie theater in Omaha and still shows films. A recent development to the Omaha film scene was the addition of Film Streams's Ruth Sokolof Theater in North Downtown. The two-screen theater is part of the Slowdown facility. It features new American independents, foreign films, documentaries, classics, themed series, and director retrospectives. There are many new theaters opening in Omaha. In addition to the five Douglas Theatres venues in Omaha, two more are opening, including Midtown Crossing Theatres, located on 32nd and Farnam Streets by the Mutual of Omaha Building. Westroads Mall has opened a new multiplex movie theater with 14 screens, operated by Rave Motion Pictures.
Songs about Omaha include "Omaha" by Moby Grape, "Omaha", by the indie rock band Tapes 'n Tapes;, "Omaha" by Counting Crows, "Omaha Celebration" by Pat Metheny, "Omaha" sung by Waylon Jennings, "Greater Omaha" by Desaparecidos and "(Ready Or Not) Omaha Nebraska" by Bowling for Soup.
The 1935 winner of the Triple Crown of Thoroughbred Racing was named Omaha, and after traveling the world the horse eventually retired to a farm south of the city. The horse made promotional appearances at Ak-Sar-Ben during the 1950s and following his death in 1959 was buried at the racetrack's Circle of Champions.
Sports and recreation.
Sports have been important in Omaha for more than a century, and the city currently plays host to three minor-league professional sports teams. It is perhaps more known as the home of the College World Series, to which it has played host since 1950. The Kings, an NBA franchise, called Omaha and Kansas City home from 1972 to 1978. The Kansas City-Omaha Kings split their time between the two cities, playing at Kansas City's Municipal Auditorium and the Omaha Civic Auditorium, before decamping solely to Kansas City until 1985, when the team moved to its current home of Sacramento.
The Omaha Sports Commission is a quasi-governmental nonprofit organization that coordinates much of the professional and amateur athletic activity in the city, including the 2008 and 2012 US Olympic Swimming Team Trials and the building of a new stadium in North Downtown. The University of Nebraska and the Commission co-hosted the 2008 National Collegiate Athletic Association (NCAA) Division One Women's Volleyball Championship in December of that year. Another quasi-governmental board, the Metropolitan Entertainment and Convention Authority (MECA), was created by city voters in 2000, and is responsible for maintaining the CenturyLink Center Omaha. 
Omaha's Johnny Rosenblatt Stadium was home to the Omaha Storm Chasers (at the time known as the Omaha Royals) minor-league baseball team (the AAA affiliate of the Kansas City Royals). From 1950 to 2010, it hosted the annual NCAA College World Series, or CWS, men's baseball tournament in mid-June.
After Rosenblatt Stadium closed, its tenants moved to new venues. On April 16, 2011, the Omaha Storm Chasers played their first game at the new Werner Park in the Omaha suburb of Papillion. The CWS moved to the new downtown stadium TD Ameritrade Park in 2011–present.
Omaha is also home to the Omaha Diamond Spirit, a collegiate summer baseball team that plays in the MINK league.
On April 15, 2010, it was announced that Omaha would be home to a new expansion team in the United Football League to begin play in 2010. The team played its inaugural season at Johnny Rosenblatt Stadium before moving to TD Ameritrade Park for its second and final season.
Named in tribute to Omaha's meatpacking past, the Omaha Beef indoor football team played at the Omaha Civic Auditorium until 2012 when they moved to the new Ralston Arena.
The Creighton University Bluejays compete in a number of NCAA Division I sports. They are a member of the Big East. Baseball is played at TD Ameritrade Park Omaha, soccer is played at Morrison Stadium, and basketball is played at the 18,000 seat CenturyLink Center. The Jays annually rank in the top 15 in attendance each year, averaging more than 16,000 people per game.
Ice hockey is a popular spectator sport in Omaha and there are two Omaha-area teams. The Omaha Lancers, a United States Hockey League team that played at Aksarben until 2004, moved to neighboring city of Council Bluffs at the Mid-America Center, and moved back to Omaha in 2009 to play at the Civic Auditorium The University of Nebraska Omaha Mavericks, is an NCAA Division I team that plays at the CenturyLink Center. Omaha has a thriving running community and many miles of paved running and biking trails throughout the city and surrounding communities. The Omaha Marathon involves a half-marathon and a 10 km race that take place annually in September. Omaha also has a history of curling, including multiple junior national champions.
The city's historic boulevards were originally designed by Horace Cleveland in 1889 to work with the parks to create a seamless flow of trees, grass and flowers throughout the city. Florence Boulevard and Fontenelle Boulevard are among the remnants of this system. Omaha boasts more than 80 mi of trails for pedestrians, bicyclists and hikers. They include the American Discovery Trail, which traverses the entire United States, and the Lewis and Clark National Historic Trail passes through Omaha as it travels 3700 mi westward from Illinois to Oregon. Trails throughout the area are included in comprehensive plans for the city of Omaha, the Omaha metropolitan area, Douglas County, and long-distance coordinated plans between the municipalities of southeast Nebraska.
Government and politics.
Omaha has a strong mayor form of government, along with a city council that is elected from seven districts across the city. The current mayor is Jean Stothert, who was elected in May 2013. The longest serving mayor in Omaha's history was "Cowboy" Jim Dahlman, who served 20 years over eight terms. He was regarded as the "wettest mayor in America" because of the flourishing number of bars in Omaha during his tenure. Dahlman was a close associate of political boss Tom Dennison. During Dahlman's tenure, the city switched from its original strong-mayor form of government to a city commission government. In 1956, the city switched back.
The city clerk is Buster Brown. The City of Omaha administers twelve departments, including finance, police, human rights, libraries and planning. The Omaha City Council is the legislative branch and is made up seven members elected from districts across the city. The council enacts local ordinances and approves the city budget. Government priorities and activities are established in a budget ordinance approved annually. The council takes official action through the passage of ordinances and resolutions. Nebraska’s constitution grants the option of home rule to cities with more than 5,000 residents, meaning they may operate under their own charters. Omaha is one of only three cities in Nebraska to use this option, out of 17 eligible. The City of Omaha is currently considering consolidating with Douglas County government.
Although registered Republicans outnumbered Democrats in the 2nd congressional district, which includes Omaha, Democratic presidential candidate Barack Obama opened three campaign offices in the city with 15 staff members to cover the state in fall 2008. Mike Fahey, the former Democratic mayor of Omaha, said he would do whatever it took to deliver the district's electoral vote to Obama; and the Obama campaign considered the district "in play". Former Nebraska U.S. Senator Bob Kerrey and former Senator Ben Nelson campaigned in the city for Obama, and in November 2008 Obama won the district's electoral vote. This was an exceptional win, because with Nebraska's split electoral vote system Obama became the first Democratic presidential candidate to win an electoral vote in Nebraska since 1964.
In 2011, Nebraska lawmakers moved Offutt Air Force Base and the town of Bellevue — an area with a large minority population — out of the Omaha-based 2nd District and shifted in the Republican-heavy Omaha suburbs in Sarpy County. The move is expected to dilute the city’s urban Democratic vote.
Crime.
Omaha's rate of violent crimes per 100,000 residents has been lower than the average rates of three dozen United States cities of similar size. Unlike Omaha, those cities have experienced an increase in violent crime overall since 2003. Rates for property crime have decreased for both Omaha and its peer cities during the same time period. In 2006, Omaha was ranked for homicides as 46th out of the 72 cities in the United States of more than 250,000 in population.
As a major industrial city into the mid-20th century, Omaha shared in social tensions of larger cities that accompanied rapid growth and many new immigrants and migrants. By the 1950s, Omaha was a center for illegal gambling, while experiencing dramatic job losses and unemployment because of dramatic restructuring of the railroads and the meatpacking industry, as well as other sectors. Persistent poverty resulting from racial discrimination and job losses generated different crimes in the late 20th century, with drug trade and drug abuse becoming associated with violent crime rates, which climbed after 1986 as Los Angeles gangs made affiliates in the city. Gambling in Omaha has been significant throughout the city's history. From its founding in the 1850s through the 1930s, the city was known as a "wide-open" town, meaning that gambling of all sorts was accepted either openly or in closed quarters. By the mid-20th century, Omaha reportedly had more illicit gambling per capita than any other city in the nation. From the 1930s through the 1970s the city's gambling was controlled by an Italian criminal element. Today, gambling in Omaha is limited to keno, lotteries, and parimutuel betting, leaving Omahans to drive across the Missouri River to Council Bluffs, Iowa, where casinos are legal and there are numerous businesses operating currently. Recently a controversial proposal by the Ponca tribe of Nebraska was approved by the National Indian Gaming Commission. It will allow the tribe to build a casino in Carter Lake, Iowa, which sits geographically on the west side of the Missouri River, adjacent to Omaha, where casinos are illegal.
Education.
Education in Omaha is provided by many private and public institutions. Omaha Public Schools is the largest public school district in Nebraska, with more than 47,750 students in more than 75 schools. After a contentious period of uncertainty, in 2007 the Nebraska Legislature approved a plan to create a learning community for Omaha-area school districts with a central administrative board. The Roman Catholic Archdiocese of Omaha maintains numerous private Catholic schools with 21,500 students in 32 elementary schools and nine high schools. at 3869 Webster St. in Midtown Omaha and St. Stephen the Martyr School at 168th and Q street in western Omaha earned national distinction when they received the U.S. Department of Education Blue Ribbon School award. Omaha is also home to Brownell-Talbot School, the only preschool through grade 12, independent college preparatory school in the state of Nebraska.
There are eleven colleges and universities among Omaha's higher education institutions, including the University of Nebraska Omaha. The University of Nebraska Medical Center is located in midtown Omaha and is home to the Eppley Cancer Center, one of 66 designated Cancer Centers by the National Cancer Institute in the United States. The University of Nebraska College of Medicine, also located on the UNMC campus, is ranked 7th in the country by US News and World Report for primary care medical education. Omaha's Creighton University is ranked the top non-doctoral regional university in the Midwestern United States by "U.S. News and World Report". Creighton maintains a 132 acre campus just outside of Downtown Omaha in the new North Downtown district, and the Jesuit institution has an enrollment of around 6,700 in its undergraduate, graduate, medical, and law schools. There are more than 10 other colleges and universities in Omaha in the Omaha metro area.
Media.
The major daily newspaper in Nebraska is the "Omaha World-Herald", which is the largest employee-owned newspaper in the United States. Weeklies in the city include the Midlands Business Journal (weekly business publication), "American Classifieds" (Formerly "Thrifty Nickel"), a weekly classified newspaper, "The Reader (newspaper)", and "Omaha Magazine", as well as "The Omaha Star". Founded in 1938 in North Omaha, the "Star" is Nebraska's only African-American newspaper. The city is the focus of the Omaha designated market area, and is the 76th largest in the United States. Omaha's four television news stations were found not to represent the city's racial composition in a 2007 study. Cox Communications provides cable television services throughout the metropolitan area.
Infrastructure.
In 2008 "Kiplinger's Personal Finance" magazine ranked Omaha the No. 3 best city in the United States to "live, work and play". Omaha's growth has required the constant development of new urban infrastructure that influence, allow and encourage the constant expansion of the city.
Retail natural gas and water public utilities in Omaha are provided by the Metropolitan Utilities District. Nebraska is the only public power state in the nation. All electric utilities are non-profit and customer-owned. Electricity in the city is provided by the Omaha Public Power District. Public housing is governed by the Omaha Housing Authority, and public transportation is provided by Metro Area Transit. CenturyLink and Cox provide local telephone and internet services. The City of Omaha maintains two modern sewage treatment plants.
Portions of the Enron corporation began as Northern Natural Gas Company in Omaha. Northern currently provides three natural gas lines to Omaha. Enron formerly owned UtiliCorp United, Inc., which became Aquila, Inc.. Peoples Natural Gas, a division of Aquila, Inc., currently serves several surrounding communities around the Omaha metropolitan area, including Plattsmouth.
There are several hospitals in Omaha. Research hospitals include the Boys Town National Research Hospital, the University of Nebraska Medical Center and the Creighton University Medical Center. The Boys Town facility is well known for world-class researchers in hearing-related research and high quality treatment. The University of Nebraska Medical Center hosts the Eppley Institute for Research in Cancer and Allied Diseases, a world-renowned cancer treatment facility named in honor of Omahan Eugene Eppley.
Transportation.
Omaha's central role in the history of transportation across America earned it the nickname "Gate City of the West." Despite President Lincoln's decree that Council Bluffs, Iowa, be the starting point for the Union Pacific Railroad, construction began from Omaha on the eastern portion of the first transcontinental railroad. By the middle of the 20th century, Omaha was served by almost every major railroad. Today, the Omaha Rail and Commerce Historic District celebrates this connection, along with the listing of the Burlington Train Station and the Union Station on the National Register of Historic Places. First housed in the former Herndon House, the Union Pacific Railroad's corporate headquarters have been in Omaha since the company began. Their new headquarters, the Union Pacific Center, was opened in Downtown Omaha in 2004. Amtrak, the national passenger rail system, provides service through Omaha. The Greyhound lines terminal is at 1601 Jackson St in downtown Omaha. Megabus has a stop at Crossroads Mall - N 72nd St between Dodge St and Cass St, and provides service to Des Moines, Iowa City, and Chicago. Metro Transit, previously known as Metro Area Transit, is the local bus. 
Omaha's position as a transportation center was finalized with the 1872 opening of the Union Pacific Missouri River Bridge linking the transcontinental railroad to the railroads terminating in Council Bluffs. In 1888, the first road bridge, the Douglas Street Bridge, opened. In the 1890s, the Illinois Central drawbridge opened as the largest bridge of its type in the world. Omaha's Missouri River road bridges are now entering their second generation, including the Works Progress Administration-financed South Omaha Bridge, now called Veteran's Memorial Bridge, which was added to the National Register of Historic Places. In 2006, Omaha and Council Bluffs announced joint plans to build the Missouri River Pedestrian Bridge, which opened in 2008.
Today, the primary mode of transportation in Omaha is by automobile, with I-80, I-480, I-680, I-29, and U.S. Route 75 (JFK Freeway and North Freeway) providing freeway service across the metropolitan area. The expressway along West Dodge Road (U.S. Route 6 and Nebraska Link 28B) and U.S. Route 275 has been upgraded to freeway standards from I-680 to Fremont. City owned Metro Transit formerly as MAT Metro Area Transit provides public bus service to hundreds of locations throughout the Metro.
A 2011 study by Walk Score ranked Omaha 21st most walkable of fifty largest U.S. cities. There is an extensive trail system throughout the city for walkers, runners, bicyclists, and other pedestrian modes of transportation.
Omaha is laid out on a grid plan, with 12 blocks to the mile with a north-to-south house numbering system. Omaha is the location of a historic boulevard system designed by H.W.S. Cleveland who sought to combine the beauty of parks with the pleasure of driving cars. The historic Florence and Fontenelle Boulevards, as well as the modern Sorenson Parkway, are important elements in this system.
Eppley Airfield, Omaha's airport, serves the region with over 4.2 million passengers in 2006. United Airlines, Southwest Airlines, US Airways, Delta Air Lines, American Airlines, and Frontier Airlines serve the airport with direct and connecting service. Eppley is situated in East Omaha, with many users driving through Carter Lake, Iowa and getting a view of Carter Lake before getting there. General aviation airports serving the area are the Millard Municipal Airport, North Omaha Airport and the Council Bluffs Airport. Offutt Air Force Base continues to serve as a military airbase; it is located at the southern edge of Bellevue, which in turn lies immediately south of Omaha.
Sister cities.
Omaha has six sister cities:

</doc>
<doc id="46160" url="http://en.wikipedia.org/wiki?curid=46160" title="Accuracy in Media">
Accuracy in Media

Accuracy In Media (AIM) is an American, politically conservative non-profit news media watchdog founded in 1969 by economist Reed Irvine. AIM describes itself as "a non-profit, grassroots citizens watchdog of the news media that critiques botched and bungled news stories and sets the record straight on important issues that have received slanted coverage."
History.
At its inception, Accuracy In Media was run primarily by Reed Irvine and then-executive secretary Abraham Kalish. The two sent letters to the editors of many newspapers and magazines they identified as skewed, calling out slanted news stories. If the newspaper rejected the letter, AIM bought space and printed the letter in that newspaper. Beginning in 1975, Accuracy In Media began purchasing stock in major media companies, allowing Irvine to attend annual shareholder meetings. He used these opportunities to express AIM's concerns to the various companies' owners. Reed's son, Don, chairs the organization. Don Irvine referred to his father as a "die-hard anti-communist."
In 1972, Accuracy In Media began publishing the" AIM Report", a twice-monthly newsletter originally edited by Reed Irvine. Cliff Kincaid and Roger Aronoff, AIM Senior Editor and AIM Executive Secretary and Media Analyst, respectively, continue to handle the publication, as well as daily online updates. The" AIM Report" often calls on its subscribers to contact newsmakers, reporters and news corporations to end perceived liberal media bias. 
AIM's work.
Human rights.
In 1982, "New York Times" reporter Raymond Bonner broke the story of the El Mozote massacre. This report was strongly criticized by AIM and the Reagan White House, and Bonner was pressured into business reporting, later deciding to resign. Although the report was embarrassing to the Reagan administration, who was heavily aiding the right-wing junta at the time, skeletons unearthed a decade later confirmed the original story's veracity. AIM was critical of journalist Helen Marmor, who in 1983 produced a documentary for NBC concerning the Russian Orthodox Church. AIM contended that "it ignored the repressive religious policies of the Soviet state".
Vincent Foster conspiracy theory.
AIM received a substantial amount of funding from Richard Mellon Scaife who paid Christopher W. Ruddy to investigate allegations that President Bill Clinton was connected to the suicide of Vincent Foster. AIM contended that "Foster was murdered", which is contrary to three independent reports including one by Kenneth Starr. AIM faulted the media for not picking up on the conspiracy. The organization even went to court for documents and recordings linked to the case.
AIM credited much of its reporting on the Foster case to Ruddy. Yet, his work was called a "hoax" and "discredited" by conservatives such as Ann Coulter, it was also disputed by the "American Spectator", which caused Scaife to end his funding of the Arkansas Project with the publisher. As CNN explained on February 28, 1997, "The [Starr] report refutes claims by conservative political organizations that Foster was the victim of a murder plot and coverup", but "despite those findings, right-wing political groups have continued to allege that there was more to the death and that the president and First Lady tried to cover it up".
Ruddy operates a conservative news website, NewsMax, that, as of 2004, continued to assert there was a conspiracy and faulted the media.
United Nations.
AIM has been critical of the United Nations and its coverage by the media. In February 2005, AIM alleged that United Nations correspondents, including Ian Williams, a correspondent for "The Nation" had accepted money from the UN while covering it for their publications. AIM also asserted that the United Nations Correspondents Association may have violated immigration laws by employing the wife of Williams. Williams and The Nation denied wrongdoing. The charges were reiterated by "FrontPage Magazine" and the allegation concerning Williams receiving UN cash was picked up by Brit Hume and the Fox News Channel.
Cliff Kincaid and Fox News Channel.
In November 2005, AIM columnist Cliff Kincaid criticized Fox News for broadcasting a program "The Heat is On", which reported that global warming represents a serious problem (the program was broadcast with a disclaimer). Kincaid argued the piece was one-sided and stated that this "scandal" amounted to a "hostile takeover of Fox News".
On October 20, 2006, Accuracy in Media released a list of 27 questions to pose at the Fox News Executive meeting that was attended by AIM editor Cliff Kincaid. Of these 27 questions, 8 dwell on Rupert Murdoch's relationship with the Clintons and how that may have affected Fox News coverage. Moreover, AIM wrote "News Corporation hired the Glover Park Group, a public relations firm run by friends of Bill and Hillary Clinton, to block changes in the TV ratings system", and asks, "Was this part of News Corporation's move to the left?"
In May 2007, Accuracy in Media raised questions about a conflict of interest in Fox News' co-sponsorship of the May 15 Republican Presidential Candidates debate, pointing out that News Corporation, the parent company of Fox News, is a client of presidential candidate Rudy Giuliani.
Funding.
Only three donors of the remainder are given by name: the Allied Educational Foundation (founded and chaired by George Barasch), Shelby Cullom Davis, and billionaire Richard Mellon Scaife. Scaife gave $2 million to Accuracy in Media between 1977 and 1997.

</doc>
<doc id="46165" url="http://en.wikipedia.org/wiki?curid=46165" title="Charlton Heston">
Charlton Heston

Charlton Heston (born John Charles Carter; October 4, 1923 – April 5, 2008) was an American actor and political activist.
As a Hollywood star he appeared in 100 films over the course of 60 years. He played the leading role in "The Ten Commandments" (1956); "Touch of Evil" (1958) with Orson Welles; "Ben-Hur", for which he won the Academy Award for Best Actor (1959); "El Cid" (1961); and "Planet of the Apes" (1968). He also starred in the films "The Greatest Show on Earth" (1952); "Secret of the Incas" (1954); "The Big Country" (1958); and "The Agony and the Ecstasy" (1965). The starring roles gave the actor a grave, authoritative persona and embodied responsibility, individualism and masculinity; later in his career, he rejected scripts that did not emphasize those virtues. His media image as a spokesman for Judeo-Christian moral values enabled his political voice.
A supporter of Democratic politicians and civil rights in the 1960s, Heston eventually rejected liberalism, becoming a Republican, founding a conservative political action committee and supporting Ronald Reagan. Heston's most famous role in politics came as the five-term president of the National Rifle Association from 1998 to 2003. After being diagnosed with Alzheimer's disease, he retired from both acting and being the NRA president in 2003.
Heston died on April 5, 2008, aged 84, from complications of Alzheimer's disease and pneumonia.
Early years.
Charlton Heston was born John Charles Carter, the son of Lila (née Charlton; 1899–1994) and Russell Whitford Carter (1897–1966), a sawmill operator. Many sources indicate he was born in Evanston, Illinois. Heston's autobiography, however, and some other sources, place his birth in No Man's Land, Illinois, which usually refers to a then-unincorporated area now part of Wilmette, a wealthy northern suburb of Chicago.
Heston said in a 1995 interview that he was not very good at remembering addresses or his early childhood. Heston was partially of Scottish descent, including from the Clan Fraser, but the majority of his ancestry was English.
In his autobiography, Heston refers to his father participating in his family's construction business. When Heston was an infant, his father's work moved the family to St. Helen, Michigan. It was a rural, heavily forested part of the state, and Heston lived an isolated yet idyllic existence spending much time hunting and fishing in the backwoods of the area.
When Heston was 10 years old, his parents divorced. Shortly thereafter, his mother married Chester Heston. The new family moved back to Wilmette. Heston (his new surname) attended New Trier High School. He recalled living there: All kids play pretend games, but I did it more than most. Even when we moved to Chicago, I was more or less a loner. We lived in a North Shore suburb, where I was a skinny hick from the woods, and all the other kids seemed to be rich and know about girls.:xii
Throughout Heston's life he was known by friends as "Chuck" although his wife always called him "Charlie." His stage name Charlton Heston is drawn from his mother's maiden surname (Charlton) and his stepfather's surname (Heston), and was used for his first film, an adaptation of Ibsen's "Peer Gynt".
Career.
Heston frequently recounted that while growing up in northern Michigan in a sparsely populated area, he often wandered in the forest, "acting" out the characters from books he had read. Later, in high school, Heston enrolled in New Trier's drama program, playing in the amateur silent 16 mm film adaptation of "Peer Gynt", from the Ibsen play, by future film activist David Bradley released in 1941.
From the Winnetka Community Theatre (or the Winnetka Dramatist's Guild, as it was then known) in which he was active, he earned a drama scholarship to Northwestern University; among his acting teachers was Alvina Krause. Several years later Heston teamed up with Bradley to produce the first sound version of William Shakespeare's "Julius Caesar", in which Heston played Mark Antony.
World War II service.
In 1944, Heston enlisted in the United States Army Air Forces. He served for two years as a radio operator and aerial gunner aboard a B-25 Mitchell stationed in the Alaskan Aleutian Islands with the 77th Bombardment Squadron of the Eleventh Air Force. He reached the rank of Staff Sergeant.
Heston married Northwestern University student Lydia Marie Clarke in the same year he joined the military. After his rise to fame, Heston narrated for highly classified military and Department of Energy instructional films, particularly relating to nuclear weapons, and "for six years Heston [held] the nation's highest security clearance" or Q clearance." The Q clearance is similar to a DoD or Defense Intelligence Agency (DIA) clearance of Top Secret.
Theater and television.
After the war, Heston and Clarke lived in Hell's Kitchen, New York City, where they worked as artists' models. Seeking a way to make it in theater, Heston and his wife Lydia decided to manage a playhouse in Asheville, North Carolina in 1947, making $100 a week. In 1948, they returned to New York where Heston was offered a supporting role in a Broadway revival of Shakespeare's "Antony and Cleopatra", starring Katharine Cornell. In television, Heston played a number of roles in CBS's "Studio One", one of the most popular anthology dramas of the 1950s. Film producer Hal B. Wallis of "Casablanca" spotted Heston in a 1950 television production of "Wuthering Heights" and offered him a contract. When his wife reminded Heston they had decided to pursue theater and television, he replied, "Well, maybe just for one film to see what it's like." 
Heston turned down the lead opposite Marilyn Monroe in "Let's Make Love" to appear in Benn W. Levy's play "The Tumbler", directed by Sir Laurence Olivier. Called a "harrowingly pretentious verse drama' by Time Magazine, the production went through a troubled out-of-town tryout period in Boston and closed after five performances on Broadway in February 1960. Heston, a great admirer of Olivier the actor, took on the play to work with him as a director. After the play flopped, Heston told columnist Joe Hyams, "I feel I am the only one who came out with a profit... I got out of it precisely what I went in for – a chance to work with Olivier. I learned from him in six weeks things I never would have learned otherwise. I think I've ended up a better actor."
Heston enjoyed acting on stage, believing it revivified him as an actor. He never returned to Broadway, but acted in regional theaters. His most frequent stage roles included the title role in "Macbeth", Sir Thomas More in "A Man for All Seasons", and Mark Antony in "Julius Caesar" and "Antony and Cleopatra".
Heston played More in several regional productions in the 1970s and 1980s, eventually playing it in London's West End. The play was a success and the West End production was taken to Aberdeen, Scotland, for a week where it was staged at His Majesty's Theatre. 
Hollywood.
Heston's first professional movie appearance was, at age 27, the leading role in "Dark City", a 1950 film noir. His breakthrough came when Cecil B. DeMille cast him as a circus manager in "The Greatest Show on Earth", which was named by the Motion Picture Academy as the best picture of 1952. In 1953, Heston was Billy Wilder's first choice to play Sefton in "Stalag 17". However, the role was given to William Holden, who won an Oscar for it. In 1954, he played the lead in "Secret of the Incas", which was shot on location at the archeological site Macchu Picchu and had numerous similarities to "Raiders of the Lost Ark", which was filmed a quarter century later, including a tomb scene with the revelatory shaft of light pointing out a clue on a map, and Heston's roguish antiquities thief's costume and light beard; the latter film's costume designer Deborah Nadoolman Landis noted that it was "almost a shot for shot "Raiders of the Lost Ark"." Heston became an icon for portraying Moses in the hugely successful film 'The Ten Commandments' (1956), selected by director Cecil B. DeMille who reportedly thought Heston bore an uncanny resemblance to Michelangelo's statue of Moses. In 1955, Heston appeared with Jane Wyman in "Lucy Gallant". In 1958, he played a Mexican police officer, Ramon Miguel Vargas, in Orson Welles's widely acclaimed film noir "Touch of Evil". He also starred in William Wyler's "The Big Country" opposite Gregory Peck and Burl Ives. After Marlon Brando, Burt Lancaster, and Rock Hudson turned down the title role in 'Ben-Hur' (1959), Heston accepted the role, winning the Academy Award for Best Actor, one of the unprecedented eleven Oscars the film earned. After Moses and "Ben-Hur", Heston became more identified with Biblical epics than any other actor. He voiced the role of Ben-Hur in a cartoon version of the Lew Wallace in 2003.
Heston played leading roles in a number of fictional and historical epics: "El Cid" (1961), "55 Days at Peking" (1963), as Michelangelo in "The Agony and the Ecstasy" (1965), and "Khartoum" (1966). Heston also played the eponymous role in the western movie "Will Penny" (1968).
From 1965–71, Heston served as president of the Screen Actors Guild. The Guild had been created in 1933 for the benefit of actors, who had different interests than the producers and directors who controlled the Academy of Motion Pictures Arts and Sciences. Heston was more conservative than most actors, and publicly clashed with the outspoken liberal actor Ed Asner.
In 1968, Heston starred in "Planet of the Apes" and in 1970, he had a smaller supporting role in the sequel, "Beneath the Planet of the Apes". Also in 1970, Heston portrayed Mark Antony again in another film version of Shakespeare's "Julius Caesar". His co-stars included Jason Robards as Brutus, Richard Chamberlain as Octavius, Robert Vaughn as Casca, and English actors Richard Johnson as Cassius, John Gielgud as Caesar, and Diana Rigg as Portia.
In 1971, he starred in the science fiction film, "The Omega Man". Although critically panned, the film is now considered a classic of post-apocalyptic horror. In 1972, Heston made his directorial debut and starred as Mark Antony in an adaptation of the William Shakespeare play he had performed earlier in his theater career, "Antony and Cleopatra". Hildegarde Neil was Cleopatra and English actor Eric Porter was Enobarbus. After receiving scathing reviews, the film was never released to theaters, and is rarely seen on television. It was finally released on DVD in March 2011. He subsequently starred in more successful films such as "Soylent Green" (1973) and "Earthquake" (1974).
Beginning with playing Cardinal Richelieu in 1973's "The Three Musketeers", Heston was seen in an increasing number of supporting roles, cameos and live theater. From 1985-87, he starred in his only prime time stint on a television series in the soap, "The Colbys". With his son Fraser, he produced and starred in several TV movies, including remakes of "Treasure Island" and "A Man For All Seasons". In 1992, Heston appeared on the A&E cable network in a short series of videos, "Charlton Heston Presents the Bible", reading passages from the King James Version.
Never taking himself too seriously, he also made a few appearances as "Chuck" in Dame Edna Everage's shows, both on stage and on television. Heston appeared in 1993 in a cameo role in "Wayne's World 2", in a scene where Wayne Campbell (Mike Myers) requests casting a better actor for a small role. After the scene is reshot with Heston, Campbell weeps in awe. That same year, Heston hosted "Saturday Night Live". He had cameos in the films "Hamlet", "Tombstone", and "True Lies". He starred in many theatre productions at the Los Angeles Music Center, where he appeared in "Detective Story and The Caine Mutiny Court Martial," and as Sherlock Holmes in "The Crucifer of Blood" opposite Richard Johnson as Dr. Watson. In 2001, he made a cameo appearance as an elderly, dying chimpanzee in Tim Burton's remake of "Planet of the Apes". Heston's last film role was as the infamous Nazi doctor Josef Mengele in "My Father, Rua Alguem 5555", which had limited release (mainly to festivals) in 2003.
Heston's distinctive voice had also landed him roles as a film narrator, including "Armageddon" and Disney's "Hercules".
Heston played the title role in "Mister Roberts" three times and cited it as one of his favorite roles. In the early 1990s, he tried unsuccessfully to revive and direct the show with Tom Selleck in the title role.
In 1995, Heston starred with Peter Graves, Mickey Rooney and Deborah Winters in the Warren Chaney docudrama "". In 1998, Heston had a cameo role playing himself in the American television series "Friends", in the episode "The One with Joey's Dirty Day".
Political activism.
Heston's political activism had four stages. In the first stage, 1955 to 1961, he endorsed Democratic candidates for President, and signed on to petitions and liberal political causes. From 1961 to 1972, the second stage, he continued to endorse Democratic candidates for President. From 1965 to 1971, he served as the elected president of the Screen Actors Guild, and clashed with his liberal rival Ed Asner. Moving beyond Hollywood, he became nationally visible in 1963 in support of the Civil Rights Act of 1964, and in 1968 used his "cowboy" persona to publicize gun control measures. The third stage began in 1972. Like many neoconservatives of the same era who moved from liberal Democrat to conservative Republican, he rejected the liberalism of George McGovern and supported Richard Nixon in 1972 for President. In the 1980s, he gave strong support to his friend Ronald Reagan during his conservative presidency. In 1995, Heston entered his fourth stage by establishing his own political action fund-raising committee, and jumped into the internal politics of the National Rifle Association. He gave numerous culture wars speeches and interviews upholding the conservative position, blaming media and academia for imposing affirmative action, which he saw as unfair reverse discrimination.
Heston campaigned for Presidential candidate Adlai Stevenson in 1956, although he was unable to campaign for John F. Kennedy in 1960 due to filming on "El Cid" in Spain. Reportedly, when in 1961 a segregated Oklahoma movie theater was showing his movie "El Cid" for the first time, he joined a picket line outside. Heston made no reference to this in his autobiography, but describes traveling to Oklahoma City to picket segregated restaurants, to the chagrin of Allied Artists, the producers of "El Cid". During the March on Washington for Jobs and Freedom held in Washington, D.C. in 1963, he accompanied Martin Luther King Jr. In later speeches, Heston said he helped the civil rights cause "long before Hollywood found it fashionable."
In the 1964 election, he endorsed Lyndon Baines Johnson, who had masterminded the passage of the Civil Rights Act of 1964 through Congress over the vociferous opposition of Southern Democrats. That year, Heston publicly opposed California Proposition 14 that rolled back the state's fair housing law, the Rumford Fair Housing Act. In his 1995 autobiography, "In the Arena", written after he became a conservative Republican, Heston claimed that on a drive back from the set of "The War Lord", he saw a "Barry Goldwater for President" billboard with his campaign slogan "In Your Heart You Know He's Right" and thought to himself, "Son of a bitch, he "is" right."
Heston also later claimed that his "support" for Goldwater was the event that helped turn him against gun control laws.
Following the assassination of Senator Robert F. Kennedy in 1968, Heston and actors Gregory Peck, Kirk Douglas, and James Stewart issued a statement calling for support of President Johnson's Gun Control Act of 1968. The White House had actually solicited Heston's support. He endorsed Hubert Humphrey in the 1968 Presidential election.
Heston opposed the Vietnam War during its course (though he changed his opinion in the years following the war) and in 1969 was approached by the Democratic Party to run for the U.S. Senate against incumbent George Murphy. He agonized over the decision but ultimately determined he could never give up acting. He is reported to have voted for Richard Nixon in 1972, though Nixon is not mentioned in his autobiography.
By the 1980s, Heston supported gun rights and changed his political affiliation from Democrat to Republican. When asked why he changed political alliances, Heston replied "I didn't change. The Democratic party changed." In 1987, he first registered as a Republican. He campaigned for Republicans and Republican Presidents Ronald Reagan, George H. W. Bush and George W. Bush.
Heston resigned in protest from Actors Equity, claiming the union's refusal to allow a white actor to play a Eurasian role in "Miss Saigon" was "obscenely racist". This became a major issue as conservatives battled "reverse discrimination."
Heston charged that CNN's telecasts from Baghdad were "sowing doubts" about the allied effort in the 1990–91 Gulf War."
At a Time Warner stockholders' meeting, Heston castigated the company for releasing an Ice-T album which included a song "Cop Killer" about killing police officers. While filming "The Savage", Heston was initiated by blood into the Miniconjou Lakota Nation, but claimed no natural American Indian heritage. He claimed to be "Native American" to salvage the term from exclusively referring to American Indians.
In 1993, Heston teamed up with John Anthony West and Robert M. Schoch in an Emmy Award winning NBC special, "The Mystery of the Sphinx". The documentary proposed a much earlier date for the construction of the Great Sphinx than originally suggested. Heston, when hosting the documentary, suggested that the main type of weathering evident on the Great Sphinx and surrounding enclosure walls could only have been caused by prolonged and extensive rainfall, and the whole structure was carved out of limestone bedrock by an ancient advanced culture (such as the Heavy Neolithic Qaraoun culture).
In a 1997 speech called "Fighting the Culture War in America", Heston rhetorically deplored a culture war he said was being conducted by a generation of media people, educators, entertainers, and politicians against:...the God fearing, law-abiding, Caucasian, middle-class Protestant – or even worse, evangelical Christian, Midwestern or Southern – or even worse, rural, apparently straight – or even worse, admitted heterosexuals, gun-owning – or even worse, NRA-card-carrying, average working stiff – or even worse, male working stiff – because, not only don’t you count, you are a down-right obstacle to social progress. Your voice deserves a lower decibel level, your opinion is less enlightened, your media access is insignificant; and frankly, mister, you need to wake up, wise up, and learn a little something from your new America; and until you do, would you mind shutting up?
He went on to say:The Constitution was handed down to guide us by a bunch of wise old dead white guys who invented our country! Now some flinch when I say that. Why! It's true-they were white guys! So were most of the guys that died in Lincoln's name opposing slavery in the 1860s. So why should I be ashamed of white guys? Why is "Hispanic Pride" or "Black Pride" a good thing, while "White Pride" conjures shaven heads and white hoods? Why was the Million Man March on Washington celebrated by many as progress, while the Promise Keepers March on Washington was greeted with suspicion and ridicule? I'll tell you why: Cultural warfare!
In an address to students at Harvard Law School entitled "Winning the Cultural War", Heston said, "If Americans believed in political correctness, we'd still be King George's boys – subjects bound to the British crown."
He said to the students:
You are the best and the brightest. You, here in this fertile cradle of American academia, here in the castle of learning on the Charles River. You are the cream. But I submit that you and your counterparts across the land are the most socially conformed and politically silenced generation since Concord Bridge. And as long as you validate that and abide it, you are, by your grandfathers' standards, cowards.
Heston later stated, "Political correctness is tyranny with manners." In a speech to the National Press Club in 1997, Heston said, "Now, I doubt any of you would prefer a rolled up newspaper as a weapon against a dictator or a criminal intruder."
Heston was the president (a largely ceremonial position) and spokesman of the NRA from 1998 until he resigned in 2003. At the 2000 NRA convention, he raised a rifle over his head and declared that a potential Al Gore administration would take away his Second Amendment rights "from my cold, dead hands". In announcing his resignation in 2003, he again raised a rifle over his head, repeating the five famous words of his 2000 speech. Heston was an honorary life member. 
In the 2002 film "Bowling for Columbine", Michael Moore interviewed Heston at Heston's home, asking him about an April 1999 meeting the NRA held in Denver, Colorado shortly after the Columbine high school massacre. Moore criticized Heston for the perceived thoughtlessness in the timing and location of the meeting. When Moore asked Heston for his thoughts on why gun-related homicide is so much higher in the United States than in other countries, Heston said it was because "we have probably more mixed ethnicity". Heston subsequently, on-camera, excused himself and walked away. Moore was later criticized for having conducted the interview in what some viewed as an ambush. The interview was conducted early in 2001, before Heston publicly announced that he was suffering from Alzheimer's, but the film was released afterward, causing some to say that Moore should have cut the interview from the final film.
In April 2003 Heston sent a message of support to US forces in the Iraq war, attacking opponents of the war as "pretend patriots."
Heston opposed abortion and did the introduction to Bernard Nathanson's 1987 pro-life documentary, "Eclipse of Reason", which focuses on late-term abortions. Heston served on the Advisory Board of Accuracy in Media, a conservative media watchdog group founded by Reed Irvine.
Later life and death.
In 1996, Heston had a hip replacement. He was diagnosed with prostate cancer in 1998. Following a course of radiation treatment, the cancer went into remission. In 2000, he publicly disclosed that he had been treated for alcoholism at a Utah clinic in May–June of that year.
On August 9, 2002, Heston publicly announced (via a taped message) he was diagnosed with symptoms consistent with Alzheimer's disease. In January 2003, George Clooney made a controversial joke about the fact that Heston was suffering from Alzheimer's, and Clooney initially refused to apologize. While speaking at a National Board of Review event as he accepted an award on television, Clooney said: "Charlton Heston announced again today that he is suffering from Alzheimer's". When syndicated columnist Liz Smith asked Clooney whether he wasn't "going too far" with his remark, he responded: "I don't care. Charlton Heston is the head of the National Rifle Association; he deserves whatever anyone says about him." Heston himself commented, "It just goes to show that sometimes class does skip a generation", apparently referring to Clooney's aunt, singer and actress Rosemary Clooney, who was largely apolitical. Heston further commented on the Clooney joke: "I don't know the man – never met him, never even spoken to him, but I feel sorry for George Clooney – one day he may get Alzheimer's disease. I served my country in World War II. I survived that – I guess I can survive some bad words from this fellow." Clooney later said, "It was a joke... They got the quote wrong. What I said was 'The head of the NRA announced today ...' (Filmmaker) Michael Moore had just gotten an award ... Heston shows up with guns over his head after a school shooting and then says in the documentary it's because of ethnic diversity that we have problems with violence in America. I think he's going to have to take whatever hits he gets. It was just a joke." Clooney claimed in 2008 that he had subsequently apologized to Heston in a letter, and received a gracious response from Heston's wife.
In July 2003, in his final public appearance, Heston received the Presidential Medal of Freedom at the White House from President George W. Bush. In March 2005, various newspapers reported that family and friends were shocked by the progression of his illness, and that he was sometimes unable to get out of bed.
Heston died on April 5, 2008, at his home in Beverly Hills, California, with Lydia, his wife of 64 years, by his side. He was also survived by their son, Fraser Clarke Heston, and adopted daughter, Holly Ann Heston. The cause of death was not disclosed by the family. A month later media outlets reported his death was due to complications with pneumonia. Heston's family released a statement, reading:
 Charlton Heston was seen by the world as larger than life. He was known for his chiselled jaw, broad shoulders and resonating voice, and, of course, for the roles he played. No one could ask for a fuller life than his. No man could have given more to his family, to his profession and to his country.
 — Family of Charlton Heston
Early tributes came in from leading figures; President George W. Bush called Heston "a man of character and integrity, with a big heart ... [H]e served his country during World War II, marched in the civil rights movement, led a labor union and vigorously defended Americans’ Second Amendment rights." Former First Lady Nancy Reagan said that she was "heartbroken" over Heston's death and released a statement, reading, "I will never forget Chuck as a hero on the big screen in the roles he played, but more importantly I considered him a hero in life for the many times that he stepped up to support Ronnie in whatever he was doing."
Heston's funeral was held a week later on April 12, 2008, in a ceremony which was attended by 250 people including Nancy Reagan and Hollywood stars such as California Governor Arnold Schwarzenegger, Olivia de Havilland, Keith Carradine, Pat Boone, Tom Selleck, Oliver Stone (who had cast Heston in his 1999 movie "Any Given Sunday"), Rob Reiner, and Christian Bale.
The funeral was held at Episcopal Parish of St. Matthew's Church in Pacific Palisades, California, the church where Heston regularly worshipped and attended Sunday services since the early 1980s. He was cremated and his ashes were given to his family.
Legacy.
Richard Corliss wrote in "Time" magazine, "From start to finish, Heston was a grand, ornery anachronism, the sinewy symbol of a time when Hollywood took itself seriously, when heroes came from history books, not comic books. Epics like "Ben-Hur" or "El Cid" simply couldn't be made today, in part because popular culture has changed as much as political fashion. But mainly because there's no one remotely like Charlton Heston to infuse the form with his stature, fire and guts."
In his obituary for the actor, film critic Roger Ebert noted "Heston made at least three movies that almost everybody eventually sees: "Ben-Hur", "The Ten Commandments" and "Planet of the Apes"."
Heston's cinematic legacy was the subject of "Cinematic Atlas: The Triumphs of Charlton Heston", an eleven-film retrospective by the Film Society of the Lincoln Center that was shown at the Walter Reade Theater from August 29 to September 4, 2008.
On April 17, 2010, Heston was inducted into the National Cowboy and Western Heritage Museum's Hall of Great Western Performers.
In his childhood hometown of St. Helen, Michigan, a charter school, Charlton Heston Academy, opened on September 4, 2012. It is housed in the former St. Helen Elementary School. Enrollment on the first day was 220 students in grades Kindergarten through 8th.
Charlton Heston was commemorated on a United States postage stamp issued on April 11, 2014.
Bibliography.
by Heston:

</doc>
<doc id="46169" url="http://en.wikipedia.org/wiki?curid=46169" title="Lucca">
Lucca

Lucca (]) is a city and "comune" in Tuscany, Central Italy, situated on the river Serchio in a fertile plain near the Tyrrhenian Sea. It is the capital city of the province of Lucca. It is famous among other things for its intact Renaissance-era city walls.
History.
Ancient and medieval city.
Lucca was founded by the Etruscans (there are traces of a pre-existing Ligurian settlement) and became a Roman colony in 180 BC. The rectangular grid of its historical centre preserves the Roman street plan, and the Piazza San Michele occupies the site of the ancient forum. Traces of the amphitheatre can still be seen in the Piazza dell'Anfiteatro.
At the Lucca Conference, in 56 BC, Julius Caesar, Pompey, and Crassus reaffirmed their political alliance known as the First Triumvirate.
Frediano, an Irish monk, was bishop of Lucca in the early 6th century. At one point, Lucca was plundered by Odoacer, the first Germanic King of Italy. Lucca was an important city and fortress even in the 6th century, when Narses besieged it for several months in 553. Under the Lombards, it was the seat of a duke who minted his own coins. The Holy Face of Lucca (or Volto Santo), a major relic supposedly carved by Nicodemus, arrived in 742. During the 8th - 10th centuries Lucca was a center of Jewish life, the Jewish community being led by the Kalonymos family (which at some point during this time migrated to Germany to become a major component of proto-Ashkenazic Jewry). Lucca became prosperous through the silk trade that began in the 11th century, and came to rival the silks of Byzantium. During the 10–11th centuries Lucca was the capital of the feudal margraviate of Tuscany, more or less independent but owing nominal allegiance to the Holy Roman Emperor.
First republic.
After the death of Matilda of Tuscany, the city began to constitute itself an independent commune, with a charter in 1160. For almost 500 years, Lucca remained an independent republic. There were many minor provinces in the region between southern Liguria and northern Tuscany dominated by the Malaspina; Tuscany in this time was a part of feudal Europe. Dante’s "Divine Comedy" includes many references to the great feudal families who had huge jurisdictions with administrative and judicial rights. Dante spent some of his exile in Lucca.
In 1273 and again in 1277, Lucca was ruled by a Guelph "capitano del popolo" (captain of the people) named Luchetto Gattilusio. In 1314, internal discord allowed Uguccione della Faggiuola of Pisa to make himself lord of Lucca. The Lucchesi expelled him two years later, and handed over the city to another "condottiere", Castruccio Castracani, under whose rule it became a leading state in central Italy. Lucca rivalled Florence until Castracani's death in 1328. On 22 and 23 September 1325, in the battle of Altopascio, Castracani defeated Florence's Guelphs. For this he was nominated by Louis IV the Bavarian to become duke of Lucca. Castracani's tomb is in the church of San Francesco. His biography is Machiavelli's third famous book on political rule.
In 1408, Lucca hosted the convocation intended to end the schism in the papacy. Occupied by the troops of Louis of Bavaria, the city was sold to a rich Genoese, Gherardino Spinola, then seized by John, king of Bohemia. Pawned to the Rossi of Parma, by them it was ceded to Mastino II della Scala of Verona, sold to the Florentines, surrendered to the Pisans, and then nominally liberated by the emperor Charles IV and governed by his vicar. Lucca managed, at first as a democracy, and after 1628 as an oligarchy, to maintain its independence alongside of Venice and Genoa, and painted the word "Libertas" on its banner until the French Revolution in 1789.
After Napoleonic conquest.
Lucca had been the second largest Italian city state (after Venice) with a republican constitution ("comune") to remain independent over the centuries.
In 1805, Lucca was conquered by Napoleon, who installed his sister Elisa Bonaparte Baciocchi as "Queen of Etruria".
From 1815 to 1847 it was a Bourbon-Parma duchy. The only reigning dukes of Lucca were Maria Luisa of Spain, who was succeeded by her son Charles II, Duke of Parma in 1824. Meanwhile, the Duchy of Parma had been assigned for life to Marie Louise, Duchess of Parma, the second wife of Napoleon. In accordance with the Treaty of Vienna (1815), upon the death of Marie Louise, Duchess of Parma in 1847, Parma reverted to Charles II, Duke of Parma, while Lucca lost independence and was annexed to the Grand Duchy of Tuscany. As part of Tuscany, it became part of the Kingdom of Sardinia in 1860 and finally part of the Italian State in 1861.
Main sights.
The walls around the old town remained intact as the city expanded and modernized, unusual for cities in the region. As the walls lost their military importance, they became a pedestrian promenade which encircled the old town, although they were used for a number of years in the 20th century for racing cars. They are still fully intact today; each of the four principal sides is lined with a different tree species.
The Academy of Sciences (1584) is the most famous of several academies and libraries.
The Casa di Puccini was re-opened to the public on 14 September 2011. At the nearby town of Torre del Lago, there is a Puccini opera festival every year in July/August. Puccini had a house there as well.
There are many richly built medieval basilica-form churches in Lucca with rich arcaded façades and campaniles, a few as old as the 8th century.
Culture.
Lucca is the birthplace of composers Giacomo Puccini ("La Bohème" and "Madama Butterfly)", Nicalao Dorati, Francesco Geminiani, Gioseffo Guami, Luigi Boccherini, and Alfredo Catalani. It is also the birthplace of Bruno Menconi and artist Benedetto Brandimarte.
Lucca, "Piazza Anfiteatro"
Events.
Lucca annually hosts the . The 2006 edition saw Eric Clapton, Placebo, Massive Attack, Roger Waters, Tracy Chapman and Santana play live in the "Piazza Napoleone".
Lucca hosts the annual Lucca Comics and Games festival, Italy's largest festival for comics and related subjects.
Other events include:
Film and television.
Mauro Bolognini's 1958 film "Giovani mariti" with Sylva Koscina is set and was filmed in Lucca.
International relations.
Lucca is twinned with:
 Colmar, France
 Sint-Niklaas, Belgium

</doc>
<doc id="46172" url="http://en.wikipedia.org/wiki?curid=46172" title="Siderno">
Siderno

Siderno (Greek: Σιδέρνο "Siderinò") is a town and "comune" located in Calabria, Italy about 3 kilometres from Locri.
Siderno Marina is the newer town located on the Ionian coast. It is popular with both Italian and foreign tourists and has a bathing beach.
Siderno Superiore is the old town, higher up on the flank of the coastal mountain range. It has historic palaces, old buildings and very narrow streets. It has now become a ghost town because most of the old population has moved to the more modern Siderno which is the new city and offers more job opportunities and services.
History.
The early history of the town is unknown. The old town in the hilly inland was probably founded in the 10th century by some people from Locri, who had fled to the area to defend themselves from Saracen incursions; in the following century it became a hamlet of the county of Grotteria and was home to various feudal lords. Siderno Marina was built along the coast after the 1783 earthquake.
Emigration.
Large-scale emigration abroad as well as to Northern Italy, which began to diminish only in the 1970s, has had a lasting effect on the demographic situation in the region. Emigrants from Siderno emigrated to the United States, Canada and Australia since the end of the 19th century to find employment.
Many moving to Canada settled in Schreiber, Ontario, because of the construction of the Canadian Pacific Railway and many played a major part in its completion. One of them was Cosimo Figliomeni. His good fortune and letters home lured many of his villagers to jobs in Schreiber. Half of Schreiber’s 2,000 residents trace their roots to the Italian city of Siderno.
Economy.
Siderno, also known as the "pearl" of the "Jasmine Riviera" (Italian: "Riviera dei Gelsomini")), is one of the most well-equipped tourist resorts on the Ionian coast of the province of Reggio Calabria, with wide, sandy beaches, clear sea and a magnificently-coloured seabed.
The Siderno area is famous for the production of bergamot orange, a citrus fruit that is used as an essence and fundamental ingredient in cosmetics, for its wound healing properties in the pharmaceutical industry, and for flavouring in the food industry.
Crime.
The town is home to the 'Ndrangheta, a Mafia-type criminal organization based in Calabria. Several powerful criminal clans originate from the town. Siderno was the fiefdom of Antonio Macrì, the undisputed local boss until his demise in January 1975. Several of the criminal clans are sometimes involved in bloody feuds. The town is home to one of the 'Ndrangheta's biggest and most important clans, the Commisso 'ndrina, heavily involved in the global cocaine business and money laundering.
Several clans moved to Canada, in particular the Greater Toronto Area, home to what Canadian law enforcement call the Siderno Group, which has been here since at least the 1950s. "The criminal minds of Siderno are in Canada", according to the Siderno police force. One of them, Antonio Commisso, was arrested in June 2005.
Frazioni.
Donisi (Greek: Διονυσση "Dionyssi"), Vennerello, Mirto (Greek: Μυρτώς "Myrtos"), Campo, Lucis, Zammariti, Pellegrina, Arona, San Filippo, Leone, Grappidaro, Gonia (Greek: Γωνιά "Ghonià"), Pergola, Lamia.
External links.
 

</doc>
<doc id="46173" url="http://en.wikipedia.org/wiki?curid=46173" title="Fish and chips">
Fish and chips

Fish and chips is a hot dish of English origin, consisting of battered fish, commonly Atlantic cod or haddock, and deep-fried chips. It is a common take-away food.
History.
Fish and chips became a stock meal among the working classes in England as a consequence of the rapid development of trawl fishing in the North Sea, and the development of railways which connected the ports to major industrial cities during the second half of the 19th century, which meant that fresh fish could be rapidly transported to the heavily populated areas. Deep-fried fish was first introduced into Britain during the 16th century by Jewish refugees from Portugal and Spain, and is derived from pescado frito. In 1860, the first fish and chip shop was opened in London by Joseph Malin.
Deep-fried chips (slices or pieces of potato) as a dish may have first appeared in Britain in about the same period: the "Oxford English Dictionary" notes as its earliest usage of "chips" in this sense the mention in Dickens' "A Tale of Two Cities" (published in 1859): "Husky chips of potatoes, fried with some reluctant drops of oil".
The modern fish-and-chip shop ("chippy" or "chipper" in modern British slang) originated in the United Kingdom, although outlets selling fried food occurred commonly throughout Europe. Early fish-and-chip shops had only very basic facilities. Usually these consisted principally of a large cauldron of cooking fat, heated by a coal fire. During World War II fish and chips remained one of the few foods in the United Kingdom not subject to rationing.
In the United Kingdom the Fish Labelling Regulations 2003 and in Ireland the European Communities (Labelling of Fishery and Aquaculture Products) Regulations 2003 respectively enact directive 2065/2001/EC, and generally mean that "fish" must be sold with the particular commercial name or species named; so "cod and chips" now appears on menus rather than the more vague "fish and chips". In the United Kingdom the Food Standards Agency guidance excludes caterers from this; but several local Trading Standards authorities and others do say it cannot be sold merely as "fish and chips".
England.
The dish became popular in wider circles in London and South East England in the middle of the 19th century.
(Charles Dickens mentions a "fried fish warehouse" in "Oliver Twist", first published in 1838), while in the north of England a trade in deep-fried chipped potatoes developed. The first chip shop stood on the present site of Oldham's Tommyfield Market. It remains unclear exactly when and where these two trades combined to become the fish-and-chip shop industry we know. A Jewish immigrant, Joseph Malin opened the first recorded combined fish-and-chip shop in London in 1860 or in 1865; a Mr Lees pioneered the concept in the North of England, in Mossley, in 1863.
The concept of a fish restaurant was introduced by Samuel Isaacs (born 1856 in Whitechapel, London; died 1939 in Brighton, Sussex) who ran a thriving wholesale and retail fish business throughout London and the South of England in the latter part of the 19th century. Isaacs' first restaurant opened in London in 1896 serving fish and chips, bread and butter, and tea for nine pence, and its popularity ensured a rapid expansion of the chain.
The restaurants were carpeted, had waited service, tablecloths, flowers, china and cutlery, and made the trappings of upmarket dining affordable to the working classes for the first time. They were located in Tottenham Court Road, St Pancras, The Strand, Hoxton, Shoreditch, Brixton and other London districts, as well as Clacton, Brighton, Ramsgate, Margate and other seaside resorts in southern England. Menus were expanded in the early 20th century to include meat dishes and other variations as their popularity grew to a total of thirty restaurants. Sam Isaacs' trademark was the phrase "This is the Plaice" combined with a picture of the punned-upon fish in question. A glimpse of the old Brighton restaurant at No.1 Marine Parade can be seen in the background of Norman Wisdom's 1955 film "One Good Turn" just as Norman/Pitkin runs onto the seafront; this is now the site of a Harry Ramsden's fish and chips restaurant. A blue plaque at Oldham's Tommyfield Market marks the first chips fried in Britain in 1860, and the origin of the fish and chip shop and fast food industries in Britain.
Scotland.
Dundee City Council claims that "… in the 1870s, that glory of British gastronomy—the chip—was first sold by Belgian immigrant Edward De Gernier in the city's Greenmarket."
In Edinburgh, a combination of Gold Star brown sauce and water or malt vinegar, known as "sauce", or more specifically as "chippy sauce", has great popularity.
Ireland.
In Ireland, the first fish and chips were sold by an Italian immigrant, Giuseppe Cervi, who mistakenly stepped off an America-bound ship at Cobh (then called Queenstown) in County Cork in the 1880s and walked all the way to Dublin. He started by selling fish and chips outside Dublin pubs from a handcart. He then found a permanent spot in Great Brunswick Street (now Pearse Street). His wife Palma would ask customers "Uno di questa, uno di quella?" This phrase (meaning "one of this, one of the other") entered the vernacular in Dublin as "one and one", which is still a way of referring to fish and chips in the city.
India.
The National Federation of fish Friers believes that the first fish and chip shop was set up in 1860. In India, the dish is usually based on Pomfret fish, and uses more chili paste and pepper than would be used in the UK. The dish is more of a niche market delicacy in India than a mass market dish.
United States.
In the United States, the dish is most commonly sold as "fish and chips," except in Wisconsin and other parts of the Upper Midwest, where this meal would be called a fish fry. The name "fish and chips" remains despite the fact that the word "chips" in the US generally refers to what are called "crisps" in the UK. (Americans typically refer to fried, sliced potatoes as fries).
Composition.
Cooking.
Traditional frying uses beef dripping or lard; however, vegetable oils, such as peanut oil (used because of its relatively high smoke point) now[ [update]] predominate. A minority of vendors in the north of England and Scotland and the majority of vendors in Northern Ireland still use dripping or lard, as it imparts a different flavour to the dish, but it has the side effect of making the fried chips unsuitable for vegetarians and for adherents of certain faiths. Lard is used in some living industrial history museums, such as the Black Country Living Museum.
Thickness.
British chips are usually thicker than American-style French fries sold by major multinational fast food chains, resulting in a lower fat content per portion. In their homes or in non-chain restaurants, people in or from the United States may eat a thick type of chip, more similar to the British variant, sometimes referred to as steak fries.
How much cooking fat soaks into the potato depends on the surface area and how long they are cooked. Chips have a smaller surface area per unit weight than French fries, which means absorbing less oil in a given time. On the other hand, chips, being thicker, take longer to cook than fries.
Batter.
UK fish and chip shops traditionally use a simple water and flour batter, adding a little sodium bicarbonate (baking soda) and a little vinegar to create lightness, as they create bubbles in the batter. Other recipes may use beer or milk batter, where these liquids are often substitutes for water. The carbon dioxide in the beer lends a lighter texture to the batter. Beer also results in an orange-brown colour. A simple beer batter might consist of a 2:3 ratio of flour to beer by volume. The type of beer makes the batter taste different: some prefer lager whereas others use stout or bitter.
Choice of fish.
In Britain and Ireland, cod and haddock appear most commonly as the fish used for fish and chips, but vendors also sell many other kinds of fish, especially other white fish, such as pollock or coley, plaice, skate, and ray (particularly popular in Ireland); and huss or rock salmon (a term covering several species of dogfish and similar fish). In Northern Ireland, cod, plaice or whiting appear most commonly in 'fish suppers'—'supper' being Scottish & Northern Irish chip-shop slang for a food item accompanied by chips. Suppliers in Devon and Cornwall regularly offer pollock and coley as cheap alternatives to haddock due to their regular availability in a common catch.
In Australia, reef cod and rock cod (a different variety from that used in the United Kingdom), barramundi an expensive option or flake (a type of shark meat), or snapper—both cheaper options—are commonly used. From the early 21st century, farmed basa imported from Vietnam and hoki have become common in Australian fish and chip shops. Other types of fish are also used based on regional availability.
In New Zealand, snapper was originally the preferred species for battered fillets in the North Island. As catches for this fish declined, it was replaced by hoki, shark (particularly rig) – marketed as lemon fish – and tarakihi. Bluefin gurnard and blue cod predominate in South Island fish and chips.
In the United States, the type of fish used depends on availability in a given region. Some common types are cod, halibut, flounder, tilapia or, in New England, Atlantic cod or haddock. Salmon is growing common on the West Coast, while freshwater catfish is most commonly used in the Southeast.
Accompaniments.
In chip shops in the United Kingdom and Ireland, salt and vinegar is traditionally sprinkled over fish and chips at the time it is served. Suppliers use malt vinegar, onion vinegar (used for pickling onions), or the cheaper non-brewed condiment. In Britain a portion of mushy peas is a popular side dish as are a range of pickles that typically include gherkins, onions and eggs. In table-service restaurants and pubs, the dish is usually served with a slice of lemon for squeezing over the fish and without any sauces or condiments, with salt, vinegar and sauces available at the customer's leisure.
In Ireland, Wales and Northern England, most takeaways serve warm portions of side-sauces such as curry sauce, gravy or mushy peas. The sauces are usually poured over the chips. In some areas, this dish without fish is referred to as 'wet chips'. Other fried products include 'scraps' (also known as 'bits' in Southern England), originally a by-product of fish frying. Still popular in Northern England, they were given as treats to the children of customers. Portions prepared and sold today consist of loose blobs of batter, deep fried to a crunchy golden crisp in the cooking-fat. The very popular potato scallop or potato cake consists of slices of potato dipped in fish batter and deep fried until golden brown. These are often accompanied for dipping by the warm sauces listed above.
Vendors.
In the United Kingdom, Ireland, Australia, Canada, New Zealand and South Africa, fish and chips usually sell through independent restaurants and take-aways known as fish and chip shops. Outlets range from small affairs to chain restaurants. Locally-owned seafood restaurants are also popular in many local markets. Mobile "chip vans" serve to cater for temporary occasions. In Canada, the outlets may be referred to as "chip wagons". In the United Kingdom some shops have amusing names, such as "A Salt and Battery", "The Codfather","The Frying Scotsman", "Oh My Cod", and "Frying Nemo" In countries such as New Zealand and Australia, fish-and-chip vendors are a popular business and source of income among the Asian community, particularly Chinese migrants.
In Ireland, the majority of traditional vendors are migrants or the descendants of migrants from southern Italy. A trade organisation exists to represent this tradition.
Fish and chips is a popular lunch meal eaten by families travelling to seaside resorts for day trips who do not bring their own picnic meals.
Fish-and-chip outlets sell roughly 25% of all the white fish consumed in the United Kingdom, and 10% of all potatoes.
The existence of numerous competitions and awards for "best fish-and-chip shop" testifies to the recognised status of this type of outlet in popular culture.
Fish-and-chip shops traditionally wrapped their product in newspaper, or with an inner layer of white paper (for hygiene) and an outer layer of newspaper or blank newsprint (for insulation and to absorb grease), though the use of newspaper for wrapping has almost ceased on grounds of hygiene. Nowadays[ [update]], establishments usually use food-quality wrapping paper, occasionally printed on the outside to emulate newspaper.
The British National Federation of Fish Friers was founded in 1913. It promotes fish and chips and offers training courses.
A previous world record for the "largest serving of fish and chips" was held by Gadaleto's Seafood Market in New Paltz, New York. This 2004 record was broken by Yorkshire pub "Wensleydale Heifer" in July 2011. An attempt to break this record was made by Doncaster fish and chip shop Scawsby Fisheries in August 2012, which served 33 lb of battered cod alongside 64 lb of chips.
Cultural impact.
The long-standing Roman Catholic tradition of not eating meat on Fridays, especially during Lent, and of substituting fish for other types of meat on that day continues to influence habits even in predominantly Protestant, Anglican, semi-secular and secular societies. Friday night remains a traditional occasion for eating fish-and-chips; and many cafeterias and similar establishments, while varying their menus on other days of the week, habitually offer fish and chips every Friday.
In Australia and New Zealand, the words "fish and chips" are often used to highlight the difference in each country's short-i vowel sound [ɪ]. Australian English has a higher forward sound [i], close to the "y" in "happy" and "city", while New Zealand English has a lower backward sound [ɘ], a slightly higher version of the "a" in "about" and "comma". Hence many people from other dialects hear an Australian say "feesh and cheeps" and a New Zealander say "fush and chups" for fish and chips.
Environment.
In the UK, waste oil from fish and chip shops has become a useful source of biodiesel. German biodiesel company Petrotec have outlined plans to produce biodiesel in the UK from waste oil from the British fish-and-chip industry.

</doc>
<doc id="46174" url="http://en.wikipedia.org/wiki?curid=46174" title="National Institutes of Health">
National Institutes of Health

The National Institutes of Health (NIH) is a biomedical research facility primarily located in Bethesda, Maryland. An agency of the United States Department of Health and Human Services, it is the primary agency of the United States government responsible for biomedical and health-related research. The NIH both conducts its own scientific research through its Intramural Research Program (IRP) and provides major biomedical research funding to non-NIH research facilities through its Extramural Research Program.
With 1,200 principal investigators and more than 4,000 postdoctoral fellows in basic, translational, and clinical research, the IRP is the largest biomedical research institution in the world, while, as of 2003, the extramural arm provided 28% of biomedical research funding spent annually in the US, or about US$26.4 billion.
The NIH comprises 27 separate institutes and centers that conduct research in different disciplines of biomedical science. The IRP is responsible for many scientific accomplishments, including the discovery of fluoride to prevent tooth decay, the use of lithium to manage bipolar disorder, and the creation of vaccines against hepatitis, "Haemophilus influenzae" (HIB) and human papillomavirus.
History.
NIH's roots extend back to a Marine Hospital Service in the late 1790s that provided medical relief to sick and disabled men in the U.S. Navy. By 1870, a network of marine hospitals had developed and was placed under the charge of a medical officer within the Bureau of the Treasury Department. In the late 1870s, Congress allocated funds to investigate the causes of epidemics like cholera and yellow fever, and it created the National Board of Health, making medical research an official government initiative.
In 1887, a laboratory for the study of bacteria, the Hygienic Laboratory, was established at the Marine Hospital in New York. In the early 1900s, Congress began appropriating funds for the Marine Hospital Service. By 1922, this organization changed its name to Public Health Services and established a Special Cancer Investigations laboratory at Harvard Medical School. This marked the beginning of a partnership with universities. In 1930, the Hygienic Laboratory was re-designated as the National Institutes of Health by the Ransdell Act and was given $750,000 to construct two NIH buildings. Over the next few decades, Congress would increase its funding tremendously to the NIH, and various institutes and centers within the NIH were created for specific research programs.
In 1967, the was created to administer grants for research for heart disease, cancer, and strokes. That same year, the NIH director lobbied the White House for increased federal funding in order to increase research and the speed with which health benefits could be brought to the people. An advisory committee was formed to oversee further development of the NIH and its research programs. By 1971, cancer research was in full force and President Nixon signed the National Cancer Act, initiating a National Cancer Program, President's Cancer Panel, National Cancer Advisory Board, and 15 new research, training, and demonstration centers.
The funding of NIH has often been a source of contention in Congress, serving as a proxy for the political currents of the time. This contention was seen most dramatically during the 1980s, when President Reagan repeatedly tried to cut funding for research, only to see Congress partly restore funding. The political contention over NIH funding slowed the nation's response to the AIDS epidemic; while AIDS was reported in newspaper articles from 1981, no funding was provided for research on the disease. In 1984, National Cancer Institute scientists found implications that "variants of a human cancer virus called HTLV-III are the primary cause of acquired immunodeficiency syndrome (AIDS)," a new epidemic that gripped the nation. But it was not until July 1987, as NIH celebrated its 100th anniversary, that President Reagan announced a committee to research the HIV epidemic. 
By the 1990s, the focus of the NIH committee had shifted to DNA research, and the Human Genome Project was launched. In 2009, President Obama reinstated federally funded stem-cell research, revoking the ban imposed by President Bush in 2001.
From logistical restructuring, to funding increases, to research prioritization, to government expansion and political influence, the history of the National Institutes of Health is extensive and full of change. The NIH has grown to encompass nearly 1 percent of the federal government's operating budget. The NIH now controls more than 50 percent of all funding for health research, and 85 percent of all funding for health studies in universities.
Locations and campuses.
Intramural research is primarily conducted at the main campus in Bethesda, Maryland, and the surrounding communities. The National Institute on Aging and the National Institute on Drug Abuse are located in Baltimore, Maryland, and the National Institute of Environmental Health Sciences is located in the Research Triangle region of North Carolina. The National Institute of Allergy and Infectious Diseases maintains its Rocky Mountain Labs in Hamilton, Montana, with an emphasis on BSL3 and BSL4 laboratory work.
Research.
NIH devotes 10% of its funding to research within its own facilities (intramural research). The institution gives 80% of its funding in research grants to extramural (outside) researchers. Of this extramural funding, a certain percentage (2.8% in 2014) must be granted to small businesses under the SBIR/STTR program. The extramural funding consists of about 50,000 grants to more than 325,000 researchers at more than 3000 institutions. In FY 2010[ [update]], NIH spent US$ (not including temporary funding from the American Recovery and Reinvestment Act of 2009) on clinical research, US$ on genetics-related research, US$ on prevention research, US$ on cancer, and US$ on biotechnology.
Public Access Policy.
In 2008, a Congressional mandate called for investigators funded by the NIH to submit an electronic version of their final manuscripts to the National Library of Medicine's research repository, PubMed Central (PMC), no later than 12 months after the official date of publication. The NIH Public Access Policy was the first public access mandate for a U.S. public funding agency.
NIH Interagency Pain Research Coordinating Committee.
February 13, 2012 the National Institutes of Health (NIH) announced a new group of individuals assigned to research pain. This committee is composed of researchers from different organizations and will focus to "coordinate pain research activities across the federal government with the goals of stimulating pain research collaboration… and providing an important avenue for public involvement" ("Members of new," 2012). With a committee such as this research will not be conducted by each individual organization or person but instead a collaborating group which will increase the information available. With this hopefully more pain management will be available including techniques for arthritis sufferers.
NIH Toolbox.
In September 2006, a contract for the NIH Toolbox for the Assessment of Neurological and Behavioral Function () was initiated by the NIH Blueprint for Neuroscience Research (www.neuroscienceblueprint.nih.gov) to develop a set of state-of-the-art measurement tools to enhance collection of data in large cohort studies and to advance the biomedical research enterprise. The NIH Toolbox was officially rolled out to the research community on September 10–11, 2012 at a public conference "Unveiling the NIH Toolbox" held in Bethesda, Maryland and Washington, D.C. Scientists from more than 100 institutions nationwide contributed to the development of the NIH Toolbox. The construction of NIH Toolbox assessments is based, where possible, on Item Response Theory and adapted for testing by computer.
Economic impact.
In 2000, a report from a Joint Economic Committee of Congress outlined the benefits of NIH research. It noted that some econometric studies had given its research, which was funded at $16 billion a year in 2000, a rate of return of 25 to 40 percent per year. It also found that of the 21 drugs with the highest therapeutic impact on society introduced between 1965 and 1992, public funding was "instrumental" for 15.
Funding.
To allocate funds, the NIH must first obtain its budget from Congress. This process begins with institute and center (IC) leaders collaborating with scientists to determine the most important and promising research areas within their fields. IC leaders discuss research areas with NIH management who then develops a budget request for continuing projects, new research proposals, and new initiatives from the Director. NIH submits its budget request to HHS, and HHS considers this request within the parameters of its overall budget. Many adjustments and appeals occur between NIH and HHS before the agency submits NIH's budget request to the Office of Management and Budget (OMB). 
OMB determines what amounts and research areas are approved for incorporation into the President's final budget. The President then sends NIH's budget request to Congress in February for the next fiscal year's allocations. The House and Senate Appropriations Subcommittees deliberate and by fall, Congress usually appropriates funding. This process takes approximately 18 months before the NIH can allocate any actual funds.
NIH employs five broad decision criteria in its funding policy. First, ensure the highest quality of scientific research by employing an arduous peer review process. Second, seize opportunities that have the greatest potential to yield new knowledge and that will lead to better prevention and treatment of disease. Third, maintain a diverse research portfolio in order to capitalize on major discoveries in a variety of fields such as cell biology, genetics, physics, engineering, and computer science. Fourth, address public health needs according to the disease burden (e.g., prevalence and mortality). And fifth, construct and support the scientific infrastructure (e.g., well-equipped laboratories and safe research facilities) necessary to conduct research.
In 2007 the director of the agency stated "responsibilities for identifying ... FCOIs (financial conflict of interest) must remain with grantee institutions" but institutions that administer grants have no interest to identify grantee's conflicts of interest.
The NIH issued dozens of waivers for NIH's advisory committee members up to 2012. Such waivers exempt a conflicted government employee from ethics laws. Since 2005 the US Office of Government Ethics had documented only three times where the NIH consulted with the office as required by law, and none of the waivers in question had to do with a member of an advisory committee.
Advisory committee members advise the Institute on policy and procedures affecting the external research programs and provide a second level of review for all grant and cooperative agreement applications considered by the Institute for funding.
Recent changes.
The NIH funding policy has changed in several significant ways over time. First, the amount of money given to the NIH has increased in hard dollars, although in relation to inflation and the increased expense of doing science, the funding levels are currently down. Most scientists know that in the history of the NIH, it is more difficult now to get R01 grant funding than ever before, let alone have them renewed. The scientific community concerns the low funding to be a crisis in America, and wonder how China can busily recruit our scientists who cannot stay funded given the lowered NIH funding levels without American realizing the great cost to this nation of not funding research. R01 Grants scored even in the top 20% are not being funded nowadays—and that 20% is after fifty percent of the grants are first triaged. So only five percent of grants are getting funded. And grants cannot be submitted more than two times.
However in hard dollars, the funding appears to have gone up. For example, in 1999, Congress increased the NIH's budget by $2.3 billion. (to $17.2 billion in 2000) 
In 2009 Congress again increased the NIH budget to $31 billion in 2010. Second, with the creation of the various ICs, the responsibility to allocate funding to researchers has shifted from the OD and Advisory Committee to the individual ICs. Additionally, Congress increasingly sets apart funding for particular causes. In the 1970s, Congress began to earmark funds specifically for cancer research and in the 1980s there was a significant amount allocated for AIDS/HIV research. Congress has continued to play an active role in allotting funds for specified research. These are some of the most significant changes in NIH funding policy over the last century.
A few of the key issues in evaluating NIH funding policy were previously mentioned in the paper as the five criteria the NIH has established. Three of the criteria are particularly relevant. First, is the NIH ensuring the highest quality of research by fairly implementing their peer review process and allocating extramural research funds? Second, is the NIH maximizing opportunities to produce research that yields new knowledge that will lead to better disease prevention and treatment? Next, are public health needs being addressed according to the public disease burden; in other words, are the most important needs of society being met rather than those of special interest groups? These are the most important issues in evaluating NIH funding policy.
When a government shutdown occurs, the NIH continues to treat people who are already enrolled in clinical trials, but does not start any new clinical trials and does not admit new patients who are not already enrolled in a clinical trial, except for the most critically ill, as determined by the NIH Director.
In 2014, it was announced that the NIH is directing scientists to perform their experiments with both female and male animals, or cells derived from females as well as males if they are studying cell cultures, and that the NIH would take the balance of each study design into consideration when awarding grants. 
 However, the announcement also stated that this rule would probably not apply when studying sex-specific diseases (for example, ovarian or testicular cancer.) 
Grant allocation bias controversy.
In 2011, a paper published in "Science" found that black researchers were 10% less likely to win NIH R01 grants (the oldest and most widely used) than white researchers, after controlling for "educational background, country of origin, training, previous research awards, publication record, and employer characteristics." It also found that black researchers are significantly less likely to resubmit an unapproved grant than white researchers. The study lead and economist Donna Grant said that grant reviewers do not have access to the applicant race, but may infer it from biographies or names. She also speculated that the decreased re-submission rate may be due to lack of mentoring. The study, which was commissioned by the NIH, included in its analysis 83,000 grant applications, made between 2000 and 2006.<ref name="doi10.1038/news.2011.485">Error: Bad DOI specified: 10.1038/news.2011.485</ref> Dr. Otis W. Brawley, chief medical officer at the American Cancer Society and a black man, commented on the cause of the disparity as one unrelated to racism per se, but rather to the reviewers' unconscious tendency to more likely give the benefit of the doubt to someone they are familiar with, in a scientific world where black researchers tend to keep a lower profile than other groups. The study did not reveal similar difficulties for members of other races and ethnic groups (e.g., Hispanics).
Stakeholders.
Many groups are highly invested in NIH funding.
General Public.
One of the goals of the NIH is to "expand the base in medical and associated sciences in order to ensure a continued high return on the public investment in research." Taxpayer dollars funding NIH are from the taxpayers, making them the primary beneficiaries of advances in research. Thus, the general public is a key stakeholder in the decisions resulting from the NIH funding policy. Congress theoretically represents the public interest as the NIH Advisory Committee allocates to the NIH, and the funds to the Director. However, many in the general public do not feel their interests are being accurately represented. As a result, individuals have formed patient advocacy groups to represent their own interests. Patient advocacy groups tend to focus on specific aspects of health care or diseases. Advocates get involved in many different areas such as organizing awareness campaigns, promoting patients' rights, and enhancing health policy initiatives. Most importantly, patient advocacy groups are often involved with advisory panels to ensure that current projects and those projects being considered for funding will directly impact patients' lives, improve delivery of care, and provide support for tertiary care. Advocacy groups strive to promote a health care system that is beneficial for all parties involved. Through congressional representation, NIH Advisory Committee efforts, and patient advocacy groups, the public is able to influence funding allocation as well as the policy itself.
Extramural Researchers and Scientists.
Other important stakeholders of the NIH funding policy are the researchers and scientists themselves. Extramural researchers differ from intramural researchers in that they are not employed by the NIH but must apply for funding. Throughout the history of the NIH, the amount of funding received has increased, but the proportion to each IC remains relatively constant. The individual ICs then decide who will receive the grant money and how much will be allotted. Research funding is important to extramural researchers for multiple reasons. Without the help of an NIH grant (or a similar type of funding), researchers and scientists are unable to pursue their own research interests but are obliged to follow the agenda of the company or university for which they work. This could potentially hinder discoveries in novel research areas.
In 2000, Brian Jacobs and Lars Lefgren researched extensively the impact of NIH grants on basic research and development, and the careers of grant recipients. 
For the period of 1980–2000, they reviewed all postdoctoral research grants and standard research grants for those who received funding and those who did not. Jacobs and Lefgren found that scientists who received postdoctoral research grants were 20 percent more likely to be published within the first five years after receiving the grant. They also found that scientists who received grants were 11 percent more likely to have one publication and 23 percent more likely to have five publications. Due to the 'publish or perish' standard that many researchers face, NIH funding can have a great impact on researchers' careers. 
Receiving a standard research grant also has a significant impact on researchers. Young scientists who receive a first-time grant (R01) usually produce more than one additional publication in the five-year period after they receive the grant. Those who receive an NIH grant will typically receive $252,000 more in NIH funding in the following six to ten years, and a statistically significant relationship exists between scientists receiving NIH grants and their research productivity throughout their careers.
Policy changes on who receives funding also significantly affect researchers. For example, the NIH has recently attempted to approve more first-time NIH R01 applicants, or the research grant applications of young scientists. To encourage the participation of young scientists who potentially think outside the box, the application process has been shortened and made easier. In addition, first-time applicants are being offered more funding for their research grants than those who have received grants in the past. Although this change provides greater opportunities for young scientists, it also places older, more experienced scientists at a funding disadvantage.
Commercial partnerships.
In 2011 and 2012, the Department of Health and Human Services Office of Inspector General published a series of audit reports revealing that throughout the fiscal years 2000–2010, institutes under the aegis of the NIH, did not comply with the time and amount requirements specified in appropriations statutes, in awarding federal contracts to commercial partners, committing the federal government to tens of millions of dollars of expenditure ahead of appropriation of funds from Congress.
Institutes and Centers.
The NIH is composed of 27 separate institutes and centers (ICs) that conduct and coordinate research across different disciplines of biomedical science. These are:
In addition, the National Center for Research Resources operated from April 13, 1962 to December 23, 2011.

</doc>
<doc id="46177" url="http://en.wikipedia.org/wiki?curid=46177" title="Epidemic typhus">
Epidemic typhus

Epidemic typhus (also called "camp fever", "jail fever", "hospital fever", "ship fever", "famine fever", "putrid fever", "petechial fever", "Epidemic louse-borne typhus," and "louse-borne typhus") is a form of typhus so named because the disease often causes epidemics following wars and natural disasters. The causative organism is "Rickettsia prowazekii", transmitted by the human body louse ("Pediculus humanus humanus"). Feeding on a human who carries the bacterium infects the louse. "R. prowazekii" grows in the louse's gut and is excreted in its feces. The disease is then transmitted to an uninfected human who scratches the louse bite (which itches) and rubs the feces into the wound. The incubation period is one to two weeks. "R. prowazekii" can remain viable and virulent in the dried louse feces for many days. Typhus will eventually kill the louse, though the disease will remain viable for many weeks in the dead louse.
Signs and symptoms.
Symptoms include severe headache, a sustained high fever, cough, rash, severe muscle pain, chills, falling blood pressure, stupor, sensitivity to light, delirium and death. A rash begins on the chest about five days after the fever appears, and spreads to the trunk and extremities. A symptom common to all forms of typhus is a fever which may reach 39 °C (102 °F).
Brill-Zinsser disease, first described by Nathan Brill in 1913 at Mount Sinai Hospital in New York City, is a mild form of epidemic typhus which recurs in someone after a long period of latency (similar to the relationship between chickenpox and shingles). This recurrence often occurs in times of relative immunosuppression, which is often in the context of malnutrition and other illnesses. In combination with poor sanitation and hygiene which leads to a greater density of lice, this reactivation is why typhus forms epidemics in times of social chaos and upheaval.
Transmission.
Epidemic typhus is thus found most frequently during times of war and deprivation. For example, typhus killed hundreds of thousands of prisoners in Nazi concentration camps during World War II. The deteriorating quality of hygiene in camps such as Theresienstadt and Bergen-Belsen created conditions where diseases such as typhus flourished. Situations in the twenty-first century with potential for a typhus epidemic would include refugee camps during a major famine or natural disaster. In the periods between outbreaks, when human to human transmission occurs less often, the flying squirrel serves as a zoonotic reservoir for the "Rickettsia prowazekii" bacterium.
Henrique da Rocha Lima in 1916 then proved that the bacterium "Rickettsia prowazekii" was the agent responsible for typhus; he named it after H. T. Ricketts and Stanislaus von Prowazek, two zoologists who had died from typhus while investigating epidemics. Once these crucial facts were recognized, Rudolf Weigl in 1930 was able to fashion a practical and effective vaccine production method by grinding up the insides of infected lice that had been drinking blood. It was, however, very dangerous to produce, and carried a high likelihood of infection to those who were working on it.
A safer mass-production-ready method using egg yolks was developed by Herald R. Cox in 1938. This vaccine was widely available and used extensively by 1943.
Treatment.
The infection is treated with antibiotics. Intravenous fluids and oxygen may be needed to stabilize the patient. The mortality rate is 10% to 60%, but is vastly lower (close to zero) if intracellular antibiotics such as tetracycline are used before 8 days. Chloramphenicol is also used. Infection can also be prevented by vaccination.
History.
The first description of typhus was probably given in 1083 at La Cava abbey near Salerno, Italy. In 1546, Girolamo Fracastoro, a Florentine physician, described typhus in his famous treatise on viruses and contagion, "De Contagione et Contagiosis Morbis".
Before a vaccine was developed during World War II, typhus was a devastating disease for humans and has been responsible for a number of epidemics throughout history. These epidemics tend to follow wars, famine, and other conditions that result in mass casualties.
During the second year of the Peloponnesian War (430 BC), the city-state of Athens in ancient Greece was hit by a devastating epidemic, known as the Plague of Athens, which killed, among others, Pericles and his two elder sons. The plague returned twice more, in 429 BC and in the winter of 427/6 BC. Epidemic typhus is a strong candidate for the cause of this disease outbreak, supported by both medical and scholarly opinions.
Typhus also arrived in Europe with soldiers who had been fighting on Cyprus. The first reliable description of the disease appears during the Spanish siege of Moorish Granada in 1489. These accounts include descriptions of fever and red spots over arms, back and chest, progressing to delirium, gangrenous sores, and the stench of rotting flesh. During the siege, the Spaniards lost 3,000 men to enemy action but an additional 17,000 died of typhus.
Typhus was also common in prisons (and in crowded conditions where lice spread easily), where it was known as "Gaol fever" or "Jail fever". Gaol fever often occurs when prisoners are frequently huddled together in dark, filthy rooms. Imprisonment until the next term of court was often equivalent to a death sentence. It was so infectious that prisoners brought before the court sometimes infected the court itself. Following the Assize held at Oxford in 1577, later deemed the Black Assize, over 300 died from epidemic typhus, including Sir Robert Bell, Lord Chief Baron of the Exchequer. The outbreak that followed, between 1577 to 1579, killed about 10% of the English population. During the Lent Assize Court held at Taunton (1730) typhus caused the death of the Lord Chief Baron, as well as the High Sheriff, the sergeant, and hundreds of others. During a time when there were 241 capital offences, more prisoners died from 'gaol fever' than were put to death by all the public executioners in the realm. In 1759 an English authority estimated that each year a quarter of the prisoners had died from gaol fever. In London, typhus frequently broke out among the ill-kept prisoners of Newgate Gaol and then moved into the general city population.
Epidemics occurred throughout Europe and occurred during the English Civil War, the Thirty Years' War and the Napoleonic Wars. During Napoleon's retreat from Moscow in 1812, more French soldiers died of typhus than were killed by the Russians. A major epidemic occurred in Ireland between 1816–19, and again in the late 1830s, and yet another major typhus epidemic occurred during the Great Irish Famine between 1846 and 1849. The Irish typhus spread to England, where it was sometimes called "Irish fever" and was noted for its virulence. It killed people of all social classes, since lice were endemic and inescapable, but it hit particularly hard in the lower or "unwashed" social strata. In Canada, the typhus epidemic of 1847 killed more than 20,000 people died from 1847 to 1848, mainly Irish immigrants in fever sheds and other forms of quarantine, who had contracted the disease aboard coffin ships.
In America, a typhus epidemic killed the son of Franklin Pierce in Concord, New Hampshire in 1843 and struck in Philadelphia in 1837. Several epidemics occurred in Baltimore, Memphis and Washington DC between 1865 and 1873. Typhus fever was also a significant killer during the US Civil War, although typhoid fever was the more prevalent cause of US Civil War "camp fever." Typhoid is a completely different disease from typhus.
During World War I typhus caused three million deaths in Russia and more in Poland and Romania. Delousing stations were established for troops on the Western front but the disease ravaged the armies of the Eastern front, with over 150,000 dying in Serbia alone. Fatalities were generally between 10 to 40 percent of those infected, and the disease was a major cause of death for those nursing the sick. Between 1918 and 1922 typhus caused at least 3 million deaths out of 20–30 million cases. In Russia after World War I, during a civil war between the White and Red armies, typhus killed three million, largely civilians. 
During World War II typhus struck the German Army as it invaded Russia in 1941. In 1942 and 1943 typhus hit French North Africa, Egypt and Iran particularly hard. Typhus epidemics killed inmates in the Nazi Germany concentration camps; infamous pictures of typhus victims' mass graves can be seen in footage shot at Bergen-Belsen concentration camp. Thousands of prisoners held in appalling conditions in Nazi concentration camps such Theresienstadt and Bergen-Belsen also died of typhus during World War II, including Anne Frank at the age of 15 and her sister Margot. Even larger epidemics in the post-war chaos of Europe were only averted by the widespread use of the newly discovered DDT to kill the lice on millions of refugees and displaced persons.
Following the development of a vaccine during World War II, epidemics have usually occurred in Eastern Europe, the Middle East and parts of Africa, particularly Ethiopia, where its eradication was the focus of major research efforts by Naval Medical Research Unit Five.
Society and culture.
Biological weapon.
Typhus was one of more than a dozen agents that the United States researched as potential biological weapons before President Richard Nixon suspended all non-defensive aspects of the U.S. biological weapons program in 1969.

</doc>
<doc id="46178" url="http://en.wikipedia.org/wiki?curid=46178" title="SQUID">
SQUID

A SQUID (for superconducting quantum interference device) is a very sensitive magnetometer used to measure extremely subtle magnetic fields, based on superconducting loops containing Josephson junctions.
SQUIDs are sensitive enough to measure fields as low as 5 aT (5×10−18 T) within a few days of averaged measurements. Their noise levels are as low as 3 fT·Hz-½. For comparison, a typical refrigerator magnet produces 0.01 teslas (10−2 T), and some processes in animals produce very small magnetic fields between 10−9 T and 10−6 T. Recently invented SERF atomic magnetometers are potentially more sensitive and do not require cryogenic refrigeration but are orders of magnitude larger in size (~1 cm3) and must be operated in a near-zero magnetic field.
History and design.
There are two main types of SQUID: direct current (DC) and radio frequency (RF). RF SQUIDs can work with only one Josephson junction (superconducting tunnel junction), which might make them cheaper to produce, but are less sensitive.
DC SQUID.
The DC SQUID was invented in 1964 by Robert Jaklevic, John J. Lambe, James Mercereau, and Arnold Silver of Ford Research Labs after Brian David Josephson postulated the Josephson effect in 1962, and the first Josephson junction was made by John Rowell and Philip Anderson at Bell Labs in 1963. It has two Josephson junctions in parallel in a superconducting loop. It is based on the DC Josephson effect. In the absence of any external magnetic field, the input current formula_1 splits into the two branches equally. If a small external magnetic field is applied to the superconducting loop, a screening current, formula_2, begins circulating in the loop that generates a magnetic field canceling the applied external flux. The induced current is in the same direction as formula_1 in one of the branches of the superconducting loop, and is opposite to formula_1 in the other branch; the total current becomes formula_5 in one branch and formula_6 in the other. As soon as the current in either branch exceeds the critical current, formula_7, of the Josephson junction, a voltage appears across the junction.
Now suppose the external flux is further increased until it exceeds formula_8, half the magnetic flux quantum. Since the flux enclosed by the superconducting loop must be an integer number of flux quanta, instead of screening the flux the SQUID now energetically prefers to increase it to formula_9. The screening current now flows in the opposite direction. Thus the screening current changes direction every time the flux increases by half integer multiples of formula_9. Thus the critical current oscillates as a function of the applied flux. If the input current is more than formula_7, then the SQUID always operates in the resistive mode. The voltage in this case is thus a function of the applied magnetic field and the period equal to formula_9. Since the current-voltage characteristics of the DC SQUID is hysteretic, a shunt resistance, formula_13 is connected across the junction to eliminate the hysteresis (in the case of copper oxide based high-temperature superconductors the junction's own intrinsic resistance is usually sufficient). The screening current is the applied flux divided by the self-inductance of the ring. Thus formula_14 can be estimated as the function of formula_15 (flux to voltage converter) as follows:
The discussion in this Section assumed perfect flux quantization in the loop. However, this is only true for big loops with a large self-inductance. According to the relations, given above, this implies also small current and voltage variations. In practice the self-inductance "L" of the loop is not so large. The general case can be evaluated by introducing a parameter
with "i"c the critical current of the SQUID. Usually "λ" is of order one.
RF SQUID.
The RF SQUID was invented in 1965 by Robert Jaklevic, John J. Lambe, Arnold Silver, and James Edward Zimmerman at Ford. It is based on the AC Josephson effect and uses only one Josephson junction. It is less sensitive compared to DC SQUID but is cheaper and easier to manufacture in smaller quantities. Most fundamental measurements in biomagnetism, even of extremely small signals, have been made using RF SQUIDS.
The RF SQUID is inductively coupled to a resonant tank circuit. Depending on the external magnetic field, as the SQUID operates in the resistive mode, the effective inductance of the tank circuit changes, thus changing the resonant frequency of the tank circuit. These frequency measurements can be easily taken, and thus the losses which appear as the voltage across the load resistor in the circuit are a periodic function of the applied magnetic flux with a period of "Φ"0. For a precise mathematical description refer to the original paper by Erné et al.
Materials used.
The traditional superconducting materials for SQUIDs are pure niobium or a lead alloy with 10% gold or indium, as pure lead is unstable when its temperature is repeatedly changed. To maintain superconductivity, the entire device needs to operate within a few degrees of absolute zero, cooled with liquid helium.
"High-temperature" SQUID sensors are more recent; they are made of high-temperature superconductors, particularly YBCO, and are cooled by liquid nitrogen which is cheaper and more easily handled than liquid helium. They are less sensitive than conventional "low temperature" SQUIDs but good enough for many applications.
Uses.
The extreme sensitivity of SQUIDs makes them ideal for studies in biology. Magnetoencephalography (MEG), for example, uses measurements from an array of SQUIDs to make inferences about neural activity inside brains. Because SQUIDs can operate at acquisition rates much higher than the highest temporal frequency of interest in the signals emitted by the brain (kHz), MEG achieves good temporal resolution. Another area where SQUIDs are used is magnetogastrography, which is concerned with recording the weak magnetic fields of the stomach. A novel application of SQUIDs is the magnetic marker monitoring method, which is used to trace the path of orally applied drugs. In the clinical environment SQUIDs are used in cardiology for magnetic field imaging (MFI), which detects the magnetic field of the heart for diagnosis and risk stratification.
Probably the most common commercial use of SQUIDs is in magnetic property measurement systems (MPMS). These are turn-key systems, made by several manufacturers, that measure the magnetic properties of a material sample. This is typically done over a temperature range from that of 300 mK to roughly 400 K.
For example, SQUIDs are being used as detectors to perform magnetic resonance imaging (MRI). While high-field MRI uses precession fields of one to several teslas, SQUID-detected MRI uses measurement fields that lie in the microtesla range. In a conventional MRI system, the signal scales as the square of the measurement frequency (and hence precession field): one power of frequency comes from the thermal polarization of the spins at ambient temperature, while the second power of field comes from the fact that the induced voltage in the pickup coil is proportional to the frequency of the precessing magnetization. In the case of untuned SQUID detection of prepolarized spins, however, the NMR signal strength is independent of precession field, allowing MRI signal detection in extremely weak fields, of order the Earth's field. SQUID-detected MRI has advantages over high-field MRI systems, such as the low cost required to build such a system, and its compactness. The principle has been demonstrated by imaging human extremities, and its future application may include tumor screening.
Another application is the scanning SQUID microscope, which uses a SQUID immersed in liquid helium as the probe. The use of SQUIDs in oil prospecting, mineral exploration, earthquake prediction and geothermal energy surveying is becoming more widespread as superconductor technology develops; they are also used as precision movement sensors in a variety of scientific applications, such as the detection of gravitational waves.
A SQUID is the sensor in each of the four gyroscopes employed on Gravity Probe B in order to test the limits of the theory of general relativity.
A modified RF SQUID was used to observe the dynamical Casimir effect for the first time.
Proposed uses.
It has also been suggested that they might be implemented in a quantum computer.
A potential military application exists for use in anti-submarine warfare as a magnetic anomaly detector (MAD) fitted to maritime patrol aircraft.

</doc>
<doc id="46182" url="http://en.wikipedia.org/wiki?curid=46182" title="White noise">
White noise

In signal processing, white noise is a random signal with a constant power spectral density. The term is used, with this or similar meanings, in many scientific and technical disciplines, including physics, acoustic engineering, telecommunications, statistical forecasting, and many more. White noise refers to a statistical model for signals and signal sources, rather than to any specific signal.
The term is also used for a discrete signal whose samples are regarded as a sequence of serially uncorrelated random variables with zero mean and finite variance. Depending on the context, one may also require that the samples be independent and have the same probability distribution (in other words i.i.d is a simplest representative of the white noise). In particular, if each sample has a normal distribution with zero mean, the signal is said to be Gaussian white noise.
The samples of a white noise signal may be sequential in time, or arranged along one or more spatial dimensions. In digital image processing, the pixels of a "white noise image" are typically arranged in a rectangular grid, and are assumed to be independent random variables with uniform probability distribution over some interval. The concept can be defined also for signals spread over more complicated domains, such as a sphere or a torus.
An infinite-bandwidth white noise signal is a purely theoretical construction. The bandwidth of white noise is limited in practice by the mechanism of noise generation, by the transmission medium and by finite observation capabilities. Thus, a random signal is considered "white noise" if it is observed to have a flat spectrum over the range of frequencies that is relevant to the context. For an audio signal, for example, the relevant range is the band of audible sound frequencies, between 20 to 20,000 Hz. Such a signal is heard as a hissing sound, resembling the /sh/ sound in "ash". In music and acoustics, the term "white noise" may be used for any signal that has a similar hissing sound.
White noise draws its name from white light, although light that appears white generally does not have a flat spectral power density over the visible band.
The term white noise is sometimes used in the context of phylogenetically based statistical methods to refer to a lack of phylogenetic pattern in comparative data. It is sometimes used in non technical contexts, in the metaphoric sense of "random talk without meaningful contents".
Statistical properties.
Being uncorrelated in time does not restrict the values a signal can take. Any distribution of values is possible (although it must have zero DC component). Even a binary signal which can only take on the values 1 or -1 will be white if the sequence is statistically uncorrelated. Noise having a continuous distribution, such as a normal distribution, can of course be white.
It is often incorrectly assumed that Gaussian noise (i.e., noise with a Gaussian amplitude distribution — see normal distribution) necessarily refers to white noise, yet neither property implies the other. Gaussianity refers to the probability distribution with respect to the value, in this context the probability of the signal falling within any particular range of amplitudes, while the term 'white' refers to the way the signal power is distributed (i.e., independently) over time or among frequencies.
We can therefore find Gaussian white noise, but also Poisson, Cauchy, etc. white noises. Thus, the two words "Gaussian" and "white" are often both specified in mathematical models of systems. Gaussian white noise is a good approximation of many real-world situations and generates mathematically tractable models. These models are used so frequently that the term additive white Gaussian noise has a standard abbreviation: AWGN.
White noise is the generalized mean-square derivative of the Wiener process or Brownian motion.
A generalization to random elements on infinite dimensional spaces, such as random fields, is the white noise measure.
Practical applications.
Music.
White noise is commonly used in the production of electronic music, usually either directly or as an input for a filter to create other types of noise signal. It is used extensively in audio synthesis, typically to recreate percussive instruments such as cymbals or snare drums which have high noise content in their frequency domain.
Electronics engineering.
White noise is also used to obtain the impulse response of an electrical circuit, in particular of amplifiers and other audio equipment. It is not used for testing loudspeakers as its spectrum contains too great an amount of high frequency content. Pink noise, which differs from white noise in that it has equal energy in each octave, is used for testing transducers such as loudspeakers and microphones.
Acoustics.
To set up the equalization for a concert or other performance in a venue, a short burst of white or pink noise is sent through the PA system and monitored from various points in the venue so that the engineer can tell if the acoustics of the building naturally boost or cut any frequencies. The engineer can then adjust the overall equalization to ensure a balanced mix.
Computing.
White noise is used as the basis of some random number generators. For example, Random.org uses a system of atmospheric antennae to generate random digit patterns from white noise.
Tinnitus treatment.
White noise is a common synthetic noise source used for sound masking by a tinnitus masker. White noise machines and other white noise sources are sold as privacy enhancers and sleep aids and to mask tinnitus. Alternatively, the use of an FM radio tuned to unused frequencies ("static") is a simpler and more cost-effective source of white noise. However, white noise generated from a common commercial radio receiver tuned to an unused frequency is extremely vulnerable to being contaminated with spurious signals, such as adjacent radio stations, harmonics from non-adjacent radio stations, electrical equipment in the vicinity of the receiving antenna causing interference, or even atmospheric events such as solar flares and especially lightning.
Work environment.
The effects of white noise upon cognitive function are mixed. Recently, a small study found that white noise background stimulation improves cognitive functioning among secondary students with attention deficit hyperactivity disorder (ADHD), while decreasing performance of non-ADHD students. Other work indicates it is effective in improving the mood and performance of workers by masking background office noise, but decreases cognitive performance in complex card sorting tasks.
Mathematical definitions.
White noise vector.
A random vector (that is, a partially indeterminate process that produces vectors of real numbers) is said to be a white noise vector or white random vector if its components each have a probability distribution with zero mean and finite variance, and are statistically independent: that is, their joint probability distribution must be the product of the distributions of the individual components.
A necessary (but, in general, not sufficient) condition for statistical independence of two variables is that they be statistically uncorrelated; that is, their covariance is zero. Therefore, the covariance matrix "R" of the components of a white noise vector "w" with "n" elements must be an "n" by "n" diagonal matrix, where each diagonal element "R""ii" is the variance of component "w""i"; and the correlation matrix must be the "n" by "n" identity matrix.
In particular, if in addition to being independent every variable in "w" also has a normal distribution with zero mean and the same variance formula_1, "w" is said to be a Gaussian white noise vector. In that case, the joint distribution of "w" is a multivariate normal distribution; the independence between the variables then implies that the distribution has spherical symmetry in "n"-dimensional space. Therefore, any orthogonal transformation of the vector will result in a Gaussian white random vector. In particular, under most types of discrete Fourier transform, such as FFT and Hartley, the transform "W" of "w" will be a Gaussian white noise vector, too; that is, the "n" Fourier coefficients of "w" will be independent Gaussian variables with zero mean and the same variance formula_1.
The power spectrum "P" of a random vector "w" can be defined as the expected value of the squared modulus of each coefficient of its Fourier transform "W", that is, "P""i" = E(|"W""i"|2). Under that definition, a Gaussian white noise vector will have a perfectly flat power spectrum, with "P""i" = formula_1 for all "i".
If "w" is a white random vector, but not a Gaussian one, its Fourier coefficients "W""i" will not be completely independent of each other; although for large "n" and common probability distributions the dependencies are very subtle, and their pairwise correlations can be assumed to be zero.
Often the weaker condition "statistically uncorrelated" is used in the definition of white noise, instead of "statistically independent". However some of the commonly expected properties of white noise (such as flat power spectrum) may not hold for this weaker version. Under this assumption, the stricter version can be referred to explicitly as independent white noise vector.:p.60 Other authors use strongly white and weakly white instead.
An example of a random vector that is "Gaussian white noise" in the weak but not in the strong sense is "x"=["x"1,"x"2] where "x"1 is a normal random variable with zero mean, and "x"2 is equal to +"x"1 or to −"x"1, with equal probability. These two variables are uncorrelated and individually normally distributed, but they are not jointly normally distributed and are not independent. If "x" is rotated by 45 degrees, its two components will still be uncorrelated, but their distribution will no longer be normal.
In some situations one may relax the definition by allowing each component of a white random vector "w" to have non-zero expected value formula_4. In image processing especially, where samples are typically restricted to positive values, one often takes formula_4 to be one half of the maximum sample value. In that case, the Fourier coefficient "W"0 corresponding to the zero-frequency component (essentially, the average of the "w"_i) will also have a non-zero expected value formula_6; and the power spectrum "P" will be flat only over the non-zero frequencies.
Continuous-time white noise.
In order to define the notion of "white noise" in the theory of continuous-time signals, one must replace the concept of a "random vector" by a continuous-time random signal; that is, a random process that generates a function formula_7 of a real-valued parameter formula_8.
Such a process is said to be white noise in the strongest sense if the value formula_9 for any time formula_8 is a random variable that is statistically independent of its entire history before formula_8. A weaker definition requires independence only between the values formula_12 and formula_13 at every pair of distinct times formula_14 and formula_15. An even weaker definition requires only that such pairs formula_12 and formula_13 be uncorrelated. As in the discrete case, some authors adopt the weaker definition for "white noise", and use the qualifier independent to refer to either of the stronger definitions. Others use weakly white and strongly white to distinguish between them.
However, a precise definition of these concepts is not trivial, because some quantities that are finite sums in the finite discrete case must be replaced by integrals that may not converge. Indeed, the set of all possible instances of a signal formula_7 is no longer a finite-dimensional space formula_19, but an infinite-dimensional function space. Moreover, by any definition a white noise signal formula_7 would have to be essentially discontinuous at every point; therefore even the simplest operations on formula_7, like integration over a finite interval, require advanced mathematical machinery.
Some authors require each value formula_9 to be a real-valued random variable with some finite variance formula_1. Then the covariance formula_24 between the values at two times formula_14 and formula_15 is well-defined: it is zero if the times are distinct, and formula_1 if they are equal. However, by this definition, the integral
over any interval with positive width formula_29 would be zero. This property would render the concept inadequate as a model of physical "white noise" signals.
Therefore, most authors define the signal formula_7 indirectly by specifying non-zero values for the integrals of formula_9 and formula_32 over any interval formula_33, as a function of its width formula_29. In this approach, however, the value of formula_9 at an isolated time cannot be defined as a real-valued random variable. Also the covariance formula_24 becomes infinite when formula_37; and the autocorrelation function formula_38 must be defined as formula_39, where formula_40 is some real constant and formula_41 is Dirac's "function".
In this approach, one usually specifies that the integral formula_42 of formula_9 over an interval formula_44 is a real random variable with normal distribution, zero mean, and variance formula_45; and also that the covariance formula_46 of the integrals formula_42, formula_48 is formula_49, where formula_29 is the width of the intersection formula_51 of the two intervals formula_52. This model is called a Gaussian white noise signal (or process).
Mathematical applications.
Time series analysis and regression.
In statistics and econometrics one often assumes that an observed series of data values is the sum of a series of values generated by a deterministic linear process, depending on certain independent (explanatory) variables, and on a series of random noise values. Then regression analysis is used to infer the parameters of the model process from the observed data, e.g. by ordinary least squares, and to test the null hypothesis that each of the parameters is zero against the alternative hypothesis that it is non-zero. Hypothesis testing typically assumes that the noise values are mutually uncorrelated with zero mean and the same Gaussian probability distribution — in other words, that the noise is white. If there is non-zero correlation between the noise values underlying different observations then the estimated model parameters are still unbiased, but estimates of their uncertainties (such as confidence intervals) will be biased (not accurate on average). This is also true if the noise is heteroskedastic — that is, if it has different variances for different data points.
Alternatively, in the subset of regression analysis known as time series analysis there are often no explanatory variables other than the past values of the variable being modeled (the dependent variable). In this case the noise process is often modeled as a moving average process, in which the current value of the dependent variable depends on current and past values of a sequential white noise process.
Random vector transformations.
These two ideas are crucial in applications such as channel estimation and channel equalization in communications and audio. These concepts are also used in data compression.
In particular, by a suitable linear transformation (a coloring transformation), a white random vector can be used to produce a "non-white" random vector (that is, a list of random variables) whose elements have a prescribed covariance matrix. Conversely, a random vector with known covariance matrix can be transformed into a white random vector by a suitable whitening transformation.
Generation.
White noise may be generated digitally with a digital signal processor, microprocessor, or microcontroller. Generating white noise typically entails feeding an appropriate stream of random numbers to a digital-to-analog converter. The quality of the white noise will depend on the quality of the algorithm used.

</doc>
<doc id="46183" url="http://en.wikipedia.org/wiki?curid=46183" title="Butter">
Butter

Butter is a dairy product made by churning fresh or fermented cream or milk, to separate the butterfat from the buttermilk. It is generally used as a spread and a condiment, as well as in cooking, such as baking, sauce making, and pan frying. Butter consists of butterfat, milk proteins and water.
Most frequently made from cows' milk, butter can also be manufactured from the milk of other mammals, including sheep, goats, buffalo, and yaks. Salt, flavorings and preservatives are sometimes added to butter. Rendering butter produces clarified butter or "ghee", which is almost entirely butterfat.
Butter is a water-in-oil emulsion resulting from an inversion of the cream, an oil-in-water emulsion; the milk proteins are the emulsifiers. Butter remains a solid when refrigerated, but softens to a spreadable consistency at room temperature, and melts to a thin liquid consistency at 32–35 °C (90–95 °F). The density of butter is 911 g/L (56.9 lb/ft3).
It generally has a pale yellow color, but varies from deep yellow to nearly white. Its unmodified color is dependent on the animals' feed and is commonly manipulated with food colorings in the commercial manufacturing process, most commonly annatto or carotene.
Etymology.
The word "butter" derives (via Germanic languages) from the Latin "butyrum", which is the latinisation of the Greek βούτυρον ("bouturon"). This may have been a construction meaning "cow-cheese", from βοῦς ("bous"), "ox, cow" + τυρός ("turos"), "cheese", but perhaps this is a false etymology of a Scythian word. Nevertheless, the earliest attested form of the second stem, "turos" ("cheese"), is the Mycenaean Greek "tu-ro", written in Linear B syllabic script. The root word persists in the name butyric acid, a compound found in rancid butter and dairy products such as Parmesan cheese.
In general use, the term "butter" refers to the spread dairy product when unqualified by other descriptors. The word commonly is used to describe puréed vegetable or seed and nut products such as peanut butter and almond butter. It is often applied to spread fruit products such as apple butter. Fats such as cocoa butter and shea butter that remain solid at room temperature are also known as "butters". In addition to the act of applying butter being called "to butter", non-dairy items that have a dairy butter consistency may use "butter' to call that consistency to mind, including food items such as maple butter and witch's butter and nonfood items such as baby bottom butter, hyena butter, and rock butter.
Production.
Unhomogenized milk and cream contain butterfat in microscopic globules. These globules are surrounded by membranes made of phospholipids (fatty acid emulsifiers) and proteins, which prevent the fat in milk from pooling together into a single mass. Butter is produced by agitating cream, which damages these membranes and allows the milk fats to conjoin, separating from the other parts of the cream. Variations in the production method will create butters with different consistencies, mostly due to the butterfat composition in the finished product. Butter contains fat in three separate forms: free butterfat, butterfat crystals, and undamaged fat globules. In the finished product, different proportions of these forms result in different consistencies within the butter; butters with many crystals are harder than butters dominated by free fats.
Churning produces small butter grains floating in the water-based portion of the cream. This watery liquid is called buttermilk—although the buttermilk most common today is instead a directly fermented skimmed milk. The buttermilk is drained off; sometimes more buttermilk is removed by rinsing the grains with water. Then the grains are "worked": pressed and kneaded together. When prepared manually, this is done using wooden boards called scotch hands. This consolidates the butter into a solid mass and breaks up embedded pockets of buttermilk or water into tiny droplets.
Commercial butter is about 80% butterfat and 15% water; traditionally made butter may have as little as 65% fat and 30% water. Butterfat is a mixture of triglyceride, a triester derived from glycerol and three of any of several fatty acid groups. Butter becomes rancid when these chains break down into smaller components, like butyric acid and diacetyl. The density of butter is 0.911 g/cm3 (0.527 oz/in3), about the same as ice.
Types.
Before modern factory butter making, cream was usually collected from several milkings and was therefore several days old and somewhat fermented by the time it was made into butter. Butter made from a fermented cream is known as cultured butter. During fermentation, the cream naturally sours as bacteria convert milk sugars into lactic acid. The fermentation process produces additional aroma compounds, including diacetyl, which makes for a fuller-flavored and more "buttery" tasting product. Today, cultured butter is usually made from pasteurized cream whose fermentation is produced by the introduction of "Lactococcus" and "Leuconostoc" bacteria.
Another method for producing cultured butter, developed in the early 1970s, is to produce butter from fresh cream and then incorporate bacterial cultures and lactic acid. Using this method, the cultured butter flavor grows as the butter is aged in cold storage. For manufacturers, this method is more efficient, since aging the cream used to make butter takes significantly more space than simply storing the finished butter product. A method to make an artificial simulation of cultured butter is to add lactic acid and flavor compounds directly to the fresh-cream butter; while this more efficient process is claimed to simulate the taste of cultured butter, the product produced is not cultured but is instead flavored.
Dairy products are often pasteurized during production to kill pathogenic bacteria and other microbes. Butter made from pasteurized fresh cream is called sweet cream butter. Production of sweet cream butter first became common in the 19th century, with the development of refrigeration and the mechanical cream separator. Butter made from fresh or cultured unpasteurized cream is called raw cream butter. While butter made from pasteurized cream may keep for several months, raw cream butter has a shelf life of roughly ten days.
Throughout continental Europe, cultured butter is preferred, while sweet cream butter dominates in the United States and the United Kingdom. Therefore, cultured butter is sometimes labeled "European-style" butter in the United States. Commercial raw cream butter is virtually unheard-of in the United States. Raw cream butter is generally only found made at home by consumers who have purchased raw whole milk directly from dairy farmers, skimmed the cream themselves, and made butter with it. It is rare in Europe as well.
Several "spreadable" butters have been developed. These remain softer at colder temperatures and are therefore easier to use directly out of refrigeration. Some methods modify the makeup of the butter's fat through chemical manipulation of the finished product, some manipulate the cattle's feed, and some incorporate vegetable oil into the butter. "Whipped" butter, another product designed to be more spreadable, is aerated by incorporating nitrogen gas—normal air is not used to avoid oxidation and rancidity.
All categories of butter are sold in both salted and unsalted forms. Either granular salt or a strong brine are added to salted butter during processing. In addition to enhanced flavor, the addition of salt acts as a preservative. The amount of butterfat in the finished product is a vital aspect of production. In the United States, products sold as "butter" must contain a at least 80% butterfat. In practice, most American butters contain slightly more than that, averaging around 81% butterfat. European butters generally have a higher ratio—up to 85%.
Clarified butter is butter with almost all of its water and milk solids removed, leaving almost-pure butterfat. Clarified butter is made by heating butter to its melting point and then allowing it to cool; after settling, the remaining components separate by density. At the top, whey proteins form a skin, which is removed. The resulting butterfat is then poured off from the mixture of water and casein proteins that settle to the bottom.
Ghee is clarified butter that has been heated to around 120 °C (250 °F) after the water evaporated, turning the milk solids brown. This process flavors the ghee, and also produces antioxidants that help protect it from rancidity. Because of this, ghee can keep for six to eight months under normal conditions.
<span id="Whey butter" />
Whey butter.
Cream may be separated (usually by a centrifugal separator) from whey instead of milk, as a byproduct of cheese-making. Whey butter may be made from whey cream. Whey cream and butter have a lower fat content and taste more salty, tangy and "cheesy". They are also cheaper than "sweet" cream and butter. The fat content of whey is low, so 1000 pounds of whey will typically give 3 pounds of butter.
European butters.
There are several butters produced in Europe with Protected geographical indications; these include:
History.
The earliest butter would have been from sheep or goat's milk; cattle are not thought to have been domesticated for another thousand years. An ancient method of butter making, still used today in parts of Africa and the Near East, involves a goat skin half filled with milk, and inflated with air before being sealed. The skin is then hung with ropes on a tripod of sticks, and rocked until the movement leads to the formation of butter.
In the Mediterranean climate, unclarified butter spoils quickly— unlike cheese, it is not a practical method of preserving the nutrients of milk. The ancient Greeks and Romans seemed to have considered butter a food fit more for the northern barbarians. A play by the Greek comic poet Anaxandrides refers to Thracians as "boutyrophagoi", "butter-eaters". In his "Natural History", Pliny the Elder calls butter "the most delicate of food among barbarous nations", and goes on to describe its medicinal properties. Later, the physician Galen also described butter as a medicinal agent only.
Historian and linguist Andrew Dalby says most references to butter in ancient Near Eastern texts should more correctly be translated as ghee. Ghee is mentioned in the Periplus of the Erythraean Sea as a typical trade article around the first century CE Arabian Sea, and Roman geographer Strabo describes it as a commodity of Arabia and Sudan. In India, ghee has been a symbol of purity and an offering to the gods—especially Agni, the Hindu god of fire—for more than 3000 years; references to ghee's sacred nature appear numerous times in the "Rigveda", circa 1500–1200 BCE. The tale of the child Krishna stealing butter remains a popular children's story in India today. Since India's prehistory, ghee has been both a staple food and used for ceremonial purposes, such as fueling holy lamps and funeral pyres.
Middle Ages.
In the cooler climates of northern Europe, people could store butter longer before it spoiled. Scandinavia has the oldest tradition in Europe of butter export trade, dating at least to the 12th century. After the fall of Rome and through much of the Middle Ages, butter was a common food across most of Europe—but had a low reputation, and so was consumed principally by peasants. Butter slowly became more accepted by the upper class, notably when the early 16th century Roman Catholic Church allowed its consumption during Lent. Bread and butter became common fare among the middle class, and the English, in particular, gained a reputation for their liberal use of melted butter as a sauce with meat and vegetables.
In antiquity, butter was used for fuel in lamps as a substitute for oil. The "Butter Tower" of Rouen Cathedral was erected in the early 16th century when Archbishop Georges d'Amboise authorized the burning of butter instead of oil, which was scarce at the time, during Lent.
Across northern Europe, butter was sometimes treated in a manner unheard-of today: it was packed into barrels (firkins) and buried in peat bogs, perhaps for years. Such "bog butter" would develop a strong flavor as it aged, but remain edible, in large part because of the unique cool, airless, antiseptic and acidic environment of a peat bog. Firkins of such buried butter are a common archaeological find in Ireland; the National Museum of Ireland – Archaeology has some containing "a grayish cheese-like substance, partially hardened, not much like butter, and quite free from putrefaction." The practice was most common in Ireland in the 11th–14th centuries; it ended entirely before the 19th century.
Industrialization.
Like Ireland, France became well known for its butter, particularly in Normandy and Brittany. By the 1860s, butter had become so in demand in France that Emperor Napoleon III offered prize money for an inexpensive substitute to supplement France's inadequate butter supplies. A French chemist claimed the prize with the invention of margarine in 1869. The first margarine was beef tallow flavored with milk and worked like butter; vegetable margarine followed after the development of hydrogenated oils around 1900.
Until the 19th century, the vast majority of butter was made by hand, on farms. The first butter factories appeared in the United States in the early 1860s, after the successful introduction of cheese factories a decade earlier. In the late 1870s, the centrifugal cream separator was introduced, marketed most successfully by Swedish engineer Carl Gustaf Patrik de Laval. This dramatically sped up the butter-making process by eliminating the slow step of letting cream naturally rise to the top of milk. Initially, whole milk was shipped to the butter factories, and the cream separation took place there. Soon, though, cream-separation technology became small and inexpensive enough to introduce an additional efficiency: the separation was accomplished on the farm, and the cream alone shipped to the factory. By 1900, more than half the butter produced in the United States was factory made; Europe followed suit shortly after.
In 1920, Otto Hunziker authored "The Butter Industry, Prepared for Factory, School and Laboratory", a well-known text in the industry that enjoyed at least three editions (1920, 1927, 1940). As part of the efforts of the American Dairy Science Association, Professor Hunziker and others published articles regarding: causes of tallowiness (an odor defect, distinct from rancidity, a taste defect); mottles (an aesthetic issue related to uneven color); introduced salts; the impact of creamery metals and liquids; and acidity measurement. These and other ADSA publications helped standardize practices internationally.
Butter also provided extra income to farm families. They used wood presses with carved decoration to press butter into pucks or small bricks to sell at nearby markets or general stores. The decoration identified the farm that produced the butter. This practice continued until production was mechanized and butter was produced in less decorative stick form. Today, butter presses remain in use for decorative purposes.
Per capita butter consumption declined in most western nations during the 20th century, in large part because of the rising popularity of margarine, which is less expensive and, until recent years, was perceived as being healthier. In the United States, margarine consumption overtook butter during the 1950s, and it is still the case today that more margarine than butter is eaten in the U.S. and the EU.
Size and shape of butter packaging.
Butter has traditionally been made into small, rectangular blocks by means of a pair of wooden butter paddles.
In the United States, butter is usually produced in 4-ounce sticks, wrapped in waxed or foiled paper and sold four to a one-pound carton. This practice is believed to have originated in 1907, when Swift and Company began packaging butter in this manner for mass distribution.
Due to historical differences in butter printers (machines that cut and package butter), these sticks are commonly produced in two different shapes:
Both sticks contain the same amount of butter, although most butter dishes are designed for Elgin-style butter sticks.
The stick's wrapper is usually marked off as eight tablespoons (120 ml); the actual volume of one stick is approximately nine tablespoons (130 ml).
Outside of the United States, butter is packaged and sold by weight only, not by volume (fluid measure) nor by unit (stick), but the package shape remains approximately the same. The wrapper is usually a foil and waxed-paper laminate (the waxed paper is now a siliconised substitute, but is still referred to in some places as parchment, from the wrapping used in past centuries; and the term 'parchment-wrapped' is still employed where the paper alone is used, without the foil laminate).
In the UK and Ireland, and in some other regions historically accustomed to using British measures, this was traditionally ½lb and 1 lb packs; since metrication, pack sizes have changed to similar metric sizes such as 250g and 500g. In cooking (recipes), butter is specified and measured by weight only (grams or ounces); although melted butter could be measured by fluid measure (centiliters or fluid ounces), this is rare.
In the remainder of the metricated world, butter is packed and sold in 250g and 500g packs (roughly equivalent to the ½lb and 1 lb measures) and measured for cooking in grams or kilograms.
Butter for commercial and industrial use is packaged in plastic buckets, tubs, or drums, in quantities and units suited to the local market.
Worldwide.
In 1997, India produced 1470000 MT of butter, most of which was consumed domestically. Second in production was the United States (522000 MT), followed by France (466000 MT), Germany (442000 MT), and New Zealand (307000 MT). France ranks first in per capita butter consumption with 8 kg per capita per year. In terms of absolute consumption, Germany was second after India, using 578000 MT of butter in 1997, followed by France (528000 MT), Russia (514000 MT), and the United States (505000 MT). New Zealand, Australia, and the Ukraine are among the few nations that export a significant percentage of the butter they produce.
Different varieties are found around the world. "Smen" is a spiced Moroccan clarified butter, buried in the ground and aged for months or years. Yak butter is a speciality in Tibet; "tsampa", barley flour mixed with yak butter, is a staple food. Butter tea is consumed in the Himalayan regions of Tibet, Bhutan, Nepal and India. It consists of tea served with intensely flavored—or "rancid"—yak butter and salt. In African and Asian developing nations, butter is traditionally made from sour milk rather than cream. It can take several hours of churning to produce workable butter grains from fermented milk.
Storage and cooking.
Normal butter softens to a spreadable consistency around 15 °C (60 °F), well above refrigerator temperatures. The "butter compartment" found in many refrigerators may be one of the warmer sections inside, but it still leaves butter quite hard. Until recently, many refrigerators sold in New Zealand featured a "butter conditioner", a compartment kept warmer than the rest of the refrigerator—but still cooler than room temperature—with a small heater. Keeping butter tightly wrapped delays rancidity, which is hastened by exposure to light or air, and also helps prevent it from picking up other odors. Wrapped butter has a shelf life of several months at refrigerator temperatures.
"French butter dishes" or "Acadian butter dishes" have a lid with a long interior lip, which sits in a container holding a small amount of water. Usually the dish holds just enough water to submerge the interior lip when the dish is closed. Butter is packed into the lid. The water acts as a seal to keep the butter fresh, and also keeps the butter from overheating in hot temperatures. This method lets butter sit on a countertop for several days without spoiling.
Once butter is softened, spices, herbs, or other flavoring agents can be mixed into it, producing what is called a "compound butter" or "composite butter" (sometimes also called "composed butter"). Compound butters can be used as spreads, or cooled, sliced, and placed onto hot food to melt into a sauce. Sweetened compound butters can be served with desserts; such hard sauces are often flavored with spirits.
Melted butter plays an important role in the preparation of sauces, most obviously in French cuisine. "Beurre noisette" (hazelnut butter) and "Beurre noir" (black butter) are sauces of melted butter cooked until the milk solids and sugars have turned golden or dark brown; they are often finished with an addition of vinegar or lemon juice. Hollandaise and béarnaise sauces are emulsions of egg yolk and melted butter; they are in essence mayonnaises made with butter instead of oil. Hollandaise and béarnaise sauces are stabilized with the powerful emulsifiers in the egg yolks, but butter itself contains enough emulsifiers—mostly remnants of the fat globule membranes—to form a stable emulsion on its own. "Beurre blanc" (white butter) is made by whisking butter into reduced vinegar or wine, forming an emulsion with the texture of thick cream. "Beurre monté" (prepared butter) is melted but still emulsified butter; it lends its name to the practice of "mounting" a sauce with butter: whisking cold butter into any water-based sauce at the end of cooking, giving the sauce a thicker body and a glossy shine—as well as a buttery taste.
In Poland, the butter lamb ("Baranek wielkanocny") is a traditional addition to the Easter Meal for many Polish Catholics. Butter is shaped into a lamb either by hand or in a lamb-shaped mould. Butter is also used to make edible decorations to garnish other dishes.
Butter is used for sautéing and frying, although its milk solids brown and burn above 150 °C (250 °F)—a rather low temperature for most applications. The smoke point of butterfat is around 200 °C (400 °F), so clarified butter or ghee is better suited to frying. Ghee has always been a common frying medium in India, where many avoid other animal fats for cultural or religious reasons.
Butter fills several roles in baking, where it is used in a similar manner as other solid fats like lard, suet, or shortening, but has a flavor that may better complement sweet baked goods. Many cookie doughs and some cake batters are leavened, at least in part, by creaming butter and sugar together, which introduces air bubbles into the butter. The tiny bubbles locked within the butter expand in the heat of baking and aerate the cookie or cake. Some cookies like shortbread may have no other source of moisture but the water in the butter. Pastries like pie dough incorporate pieces of solid fat into the dough, which become flat layers of fat when the dough is rolled out. During baking, the fat melts away, leaving a flaky texture. Butter, because of its flavor, is a common choice for the fat in such a dough, but it can be more difficult to work with than shortening because of its low melting point. Pastry makers often chill all their ingredients and utensils while working with a butter dough.
Butter also has many non-culinary, traditional uses, specific to certain cultures. For instance, in North America, applying butter to the handle of a door is a common prank on April Fools' Day.
Nutritional information.
As butter is essentially just the milk fat, it contains only traces of lactose, so moderate consumption of butter is not a problem for lactose intolerant people. People with milk allergies may still need to avoid butter, which contains enough of the allergy-causing proteins to cause reactions. Whole milk, butter and cream have high levels of saturated fat.
Butter is a good source of Vitamin A.
Further reading.
</dl>

</doc>
<doc id="46184" url="http://en.wikipedia.org/wiki?curid=46184" title="The Star Beast">
The Star Beast

The Star Beast is a 1954 science fiction novel by Robert A. Heinlein about a high school senior who discovers that his late father's extraterrestrial pet is more than it appears to be. The novel, somewhat abridged, was originally serialised in "The Magazine of Fantasy & Science Fiction" (May, June, July 1954) as "Star Lummox" and then published in hardcover as part of Scribner's series of Heinlein juveniles.
Plot summary.
An ancestor of John Thomas Stuart XI brought the alien, long-lived Lummox home from an interstellar voyage. The articulate, sentient pet he inherited has gradually grown from the size of a collie pup to a ridable behemoth—especially after consuming a used car. The childlike Lummox is perceived to be a neighborhood nuisance and, upon leaving the Stuart property one day, causes substantial property damage across the city of Westville. John's mother wants him to get rid of it, and a court orders it destroyed.
Desperate to save his pet, John Thomas considers selling Lummox to a zoo. He rapidly changes his mind and runs away from home, riding into the nearby wilderness on Lummox's back. His girlfriend Betty Sorenson joins him and suggests bringing the beast back into town and hiding it in a neighbor's greenhouse. However, it isn't easy to conceal such a large creature. Eventually, the court tries to have Lummox destroyed, but is unable to do so, much to Lummox's amusement.
Meanwhile, representatives of an advanced, powerful and previously unknown alien race appear and demand the return of their lost child...or else. A friendly alien diplomat of a third species intimates that the threat is not an empty one. Initially, no one associates Lummox with the newcomers, in part due to the size difference (Lummox was overfed). Lummox is identified as royalty, complicating the already-tense negotiations. It is discovered that, from her viewpoint, the young Lummox has been pursuing her only hobby and principal interest: the raising of John Thomases. She makes it clear that she intends to continue doing so. This gives the chief human negotiator the leverage he needs to establish diplomatic relations with the aliens, who normally do not hold regular relations with other species. At the request of Lummox, the recently married John and Betty accompany her back to her people as members of the human diplomatic mission.
Race.
Heinlein grew up in the era of racial segregation in the United States. This book was very much ahead of its time both in its explicit rejection of racism and in its inclusion of non-white protagonists. It was published in 1954 before the beginning of the US civil rights movement. The mere existence of non-white characters was a remarkable novelty. In this juvenile the "de facto" ruler of Earth is a Mr. Kiku who is from Africa. Heinlein explicitly states his skin is "ebony black", and that Kiku is in an arranged marriage that is happy.
Critical response.
The noted science fiction author and critic Damon Knight wrote:
This is a novel that won't go bad on you. Many of science fiction's triumphs, even from as little as ten years ago, are unreadable today; they were shoddily put together, not meant for re-use. But Heinlein is durable. I've read this story twice, so far – once in the "Fantasy and Science Fiction" serialized version, once in hard covers – and expect to read it again, sooner or later, for pleasure. I don't know any higher praise.
Groff Conklin described the novel as "one of Heinlein's most enchanting tales." P. Schuyler Miller found "The Star Beast" to be "one of the best of 1954."
Editions.
All paperback editions and the Science Fiction Book Club hard cover edition omit page 148 of Chapter VIII, "The Sensible Thing to Do", which was in the Scribner's edition and the magazine serialization. In this chapter, John Thomas rereads the entries in his great-grandfather's diary of how Lummox was found. Of significance on the omitted page is that:
The diary skipped a couple of days; the "Trail Blazer" had made an emergency raise-ship and Assistant Powerman J. T. Stuart had been too busy to write. John Thomas knew why ... the negotiations opened so hopefully with the dominant race had failed ... no one knew why.
The rest of the page summarizes John Thomas' grandfather's family history, discussing the first John Thomas Stuart, who had retired as a sea captain. The history, as reprinted in the paperback and Science Fiction Book Club editions, then resumes with John Thomas Stuart, Junior.

</doc>
<doc id="46185" url="http://en.wikipedia.org/wiki?curid=46185" title="Mineral matter in plants">
Mineral matter in plants

Minerals are required by plants as part of their food, to form their structure. The firmness of straw for example, is due to the presence in it of silica, the principal constituent of sand and flints. Potassa, soda, lime, magnesia, and phosphoric acid are contained in plants, in different proportions. All of these they must obtain from the soil. The alkalies above named appear to be essential to the proper development of the higher vegetable forms. Some plants require them in one mode of combination, and some in another; and thus the soil that is very good for one, may be quite unfit for others. Firs and pines find enough to support them in barren, sandy soil.
The proportion of silicate of potash, (necessary for the firmness of wheat straw), does not vary perceptibly in the soil of grain fields, because what is removed by the reaper, is again replaced by decaying straw. But this is not the case with meadow-land. Hence you would never find a luxuriant crop of grass on sandy and limestone soils which contain little potash, evidently because one of the constituents indispensable to the growth of the plants is wanting. If a meadow be well manured, we remove, with the increased crop of grass, a greater quantity of potash than can, by a repetition of the same manure, be restored to it. So grass-land manured with gypsum soon ceases to feel its agency. But if the meadow be strewed from time to time with wood ashes, or soap-boilers' lye made from wood ashes, then the grass thrives as luxuriantly as before. The ashes are only a means of restoring the necessary potash for the grass stalks. So oats, barley, and rye may be made for once to grow upon a sandy heath, by mixing with the scanty soil the ashes of the heath-plants that grow upon it. Those ashes contain soda and potash, conveyed to the growing furze or gorse by rain-water. The soil of one district consists of sandstone; certain trees find in it a quantity of alkaline earths sufficient for their own sustenance. When felled, and burnt and sprinkled upon the soil, oats will grow and thrive that without such aid would not vegetate. 
The most decisive proof of the absurdity of the indiscriminate use of any strong manure was obtained at Bingen, a town on the Rhine, where the produce and development of vines were highly increased by manuring them with animal matters such as shavings of horn. After some years, the formation of the wood and leaves decreased perceptibly. Such manure had too much hastened the growth of the vines: in two or three years they had exhausted the potash in the formation of their fruit leaves and wood; so that none remained for the future crops, as shavings of horn contain no potash. Cow-dung would have been better, and is known to be better.
References.
"This text was taken from the "Household Cyclopedia" of 1881."

</doc>
<doc id="46187" url="http://en.wikipedia.org/wiki?curid=46187" title="Lobotomy">
Lobotomy

Lobotomy (Greek: λοβός "lobos" "lobe (of brain)"; τομή "tomē" "cut, slice") is a neurosurgical procedure, a form of psychosurgery, also known as a leukotomy or leucotomy (from the Greek λευκός "leukos" "clear, white" and "tome"). It consists of cutting or scraping away most of the connections to and from the prefrontal cortex, the anterior part of the frontal lobes of the brain.
The procedure, controversial from its inception, was a mainstream procedure for more than two decades (prescribed for psychiatric and occasionally other conditions) despite general recognition of frequent and serious side effects. While some patients experienced symptomatic improvement with the operation, this was achieved at the cost of creating other impairments, and this balance between benefits and risks contributed to the controversial nature of the procedure. The originator of the procedure, the Portuguese neurologist António Egas Moniz, shared the Nobel Prize for Physiology or Medicine of 1949 for the "discovery of the therapeutic value of leucotomy in certain psychoses",
The use of the procedure increased dramatically in some countries from the early 1940s and into the 1950s; by 1951, almost 20,000 lobotomies had been performed in the United States. Following the introduction of antipsychotic medications in the mid-1950s, lobotomies underwent a gradual but definite decline.
Context.
In the early 20th century, the number of patients residing in mental hospitals increased significantly while little in the way of effective medical treatment was available. Lobotomy was one of a series of radical and invasive physical therapies developed in Europe at this time that signaled a break with a psychiatric culture of therapeutic nihilism that had prevailed since the late nineteenth-century. The new "heroic" physical therapies devised during this experimental era, including malarial therapy for general paresis of the insane (1917), deep sleep therapy (1920), insulin shock therapy (1933), cardiazol shock therapy (1934), and electroconvulsive therapy (1938), helped to imbue the then therapeutically moribund and demoralised psychiatric profession with a renewed sense of optimism in the curability of insanity and the potency of their craft. The success of the shock therapies, despite the considerable risk they posed to patients, also helped to accommodate psychiatrists to ever more drastic forms of medical intervention, including lobotomy.
The clinician-historian Joel Braslow argues that from malarial therapy onward to lobotomy, physical psychiatric therapies "spiral closer and closer to the interior of the brain" with this organ increasingly taking "center stage as a source of disease and site of cure." For Roy Porter, once the doyen of medical history, the often violent and invasive psychiatric interventions developed during the 1930s and 1940s are indicative of both the well-intentioned desire of psychiatrists to find some medical means of alleviating the suffering of the vast number of patients then in psychiatric hospitals and also the relative lack of social power of those same patients to resist the increasingly radical and even reckless interventions of asylum doctors. Many doctors, patients and family members of the period believed that despite potentially catastrophic consequences, the results of lobotomy were seemingly positive in many instances or, at least they were deemed as such when measured next to the apparent alternative of long-term institutionalisation. Lobotomy has always been controversial, but for a period of the medical mainstream, it was even feted and regarded as a legitimate if desperate remedy for categories of patients who were otherwise regarded as hopeless. Today, lobotomy has become a disparaged procedure, a byword for medical barbarism and an exemplary instance of the medical trampling of patients' rights.
Early psychosurgery.
Prior to the 1930s, individual doctors had infrequently experimented with novel surgical operations on the brains of those deemed insane. Most notably in 1888, the Swiss psychiatrist, Gottlieb Burckhardt, initiated what is commonly considered the first systematic attempt at modern human psychosurgery. He operated on six chronic patients under his care at the Swiss Préfargier Asylum, removing sections of their cerebral cortex. Burckhardt's decision to operate was informed by three pervasive views on the nature of mental illness and its relationship to the brain. First, the belief that mental illness was organic in nature, and reflected an underlying brain pathology; next, that the nervous system was organized according to an associationist model comprising an input or afferent system (a sensory center), a connecting system where information processing took place (an association center), and an output or efferent system (a motor centre); and, finally, a modular conception of the brain whereby discrete mental faculties were connected to specific regions of the brain. Burckhardt's hypothesis was that by deliberately creating lesions in regions of the brain identified as association centres a transformation in behaviour might ensue. According to his model, those mentally ill might experience "excitations abnormal in quality, quantity and intensity" in the sensory regions of the brain and this abnormal stimulation would then be transmitted to the motor regions giving rise to mental pathology. He reasoned, however, that removing material from either of the sensory or motor zones could give rise to "grave functional disturbance". Instead, by targeting the association centres and creating a "ditch" around the motor region of the temporal lobe, he hoped to break their lines of communication and thus alleviate both mental symptoms and the experience of mental distress.
Intending to ameliorate symptoms in those with violent and intractable conditions rather than effect a cure, Burckhardt began operating on patients in December 1888, but both his surgical methods and instruments were crude and the results of the procedure were mixed at best. He operated on six patients in total and, according to his own assessment, two experienced no change, two patients became quieter, one patient experienced epileptic convulsions and died a few days after the operation, and one patient improved. Complications included motor weakness, epilepsy, sensory aphasia and "word deafness". Claiming a success rate of 50 percent, he presented the results at the Berlin Medical Congress and published a report, but the response from his medical peers was hostile and he did no further operations.
In 1912, two physicians based in Saint Petersburg, the leading Russian neurologist Vladimir Bekhterev and his younger Estonian colleague, the neurosurgeon Ludvig Puusepp, published a paper reviewing a range of surgical interventions that had been performed on the mentally ill. While generally treating these endeavours favourably, in their consideration of psychosurgery they reserved unremitting scorn for Burckhardt's surgical experiments of 1888 and opined that it was extraordinary that a trained medical doctor could undertake such an unsound procedure.
"We have quoted this data to show not only how groundless but also how dangerous these operations were. We are unable to explain how their author, holder of a degree in medicine, could bring himself to carry them out ..."
The authors neglected to mention, however, that in 1910 Puusepp himself had performed surgery on the brains of three mentally ill patients, sectioning the cortex between the frontal and parietal lobes. He had abandoned these attempts because of unsatisfactory results and this experience probably inspired the invective that was directed at Burckhardt in the 1912 article. By 1937, Puusepp, despite his earlier criticism of Burckhardt, was increasingly persuaded that psychosurgery could be a valid medical intervention for the mentally disturbed. In the late 1930s he worked closely with the neurosurgical team of the Racconigi Hospital near Turin to establish it as an early and influential centre for the adoption of leucotomy in Italy.
The development of leucotomy.
Leucotomy was first undertaken in 1935 under the direction of the Portuguese neurologist (and inventor of the term "psychosurgery") António Egas Moniz. First developing an interest in psychiatric conditions and their somatic treatment in the early 1930s, Moniz apparently conceived a new opportunity for recognition in the development of a surgical intervention on the brain as a treatment for mental illness.
Frontal lobes.
The source of inspiration for Moniz's decision to hazard psychosurgery has been clouded by contradictory statements made on the subject by Moniz and others both contemporaneously and retrospectively. The traditional narrative addresses the question of why Moniz targeted the frontal lobes by way of reference to the work of the Yale neuroscientist John Fulton and, most dramatically, to a presentation Fulton made with his junior colleague Carlyle Jacobsen at the Second International Congress of Neurology held in London in 1935. Fulton's primary area of research was on the cortical function of primates and he had established America's first primate neurophysiology laboratory at Yale in the early 1930s. At the 1935 Congress, with Moniz in attendance, Fulton and Jacobsen presented two chimpanzees, named Becky and Lucy who had had frontal lobectomies and subsequent changes in behaviour and intellectual function. According to Fulton's account of the congress, they explained that prior to surgery, both animals, and especially Becky, the more emotional of the two, exhibited "frustrational behaviour" – that is, have tantrums that could include rolling on the floor and defecating—if, because of their poor performance in a set experimental tasks, they were not rewarded. Following the surgical removal of their frontal lobes, the behaviour of both primates changed markedly and Becky was pacified to such a degree that Jacobsen apparently stated it was as if she had joined a "happiness cult". During the question and answer section of the paper, Moniz, it is alleged, "startled" Fulton by inquiring if this procedure might be extended to human subjects suffering from mental illness. Fulton stated that he replied that while possible in theory it was surely "too formidable" an intervention for use on humans.
That Moniz began his experiments with leucotomy just three months after the congress has reinforced the apparent cause and effect relationship between the Fulton and Jacobsen's presentation and the Portuguese neurologist's resolve to operate on the frontal lobes. As the author of this account Fulton, who has sometimes been claimed as the father of lobotomy, was later able to record that the technique had its true origination in his laboratory. Endorsing this version of events, in 1949, the Harvard neurologist Stanley Cobb remarked during his presidential address to the American Neurological Association that, "seldom in the history of medicine has a laboratory observation been so quickly and dramatically translated into a therapeutic procedure." Fulton's report, penned ten years after the events described, is, however, without corroboration in the historical record and bears little resemblance to an earlier unpublished account he wrote of the congress. In this previous narrative he mentioned an incidental, private exchange with Moniz, but it is likely that the official version of their public conversation he promulgated is without foundation. In fact, Moniz stated that he had conceived of the operation some time before his journey to London in 1935, having told in confidence his junior colleague, the young neurosurgeon Pedro Almeida Lima, as early as 1933 of his psychosurgical idea. The traditional account exaggerates the importance of Fulton and Jacobsen to Moniz's decision to initiate frontal lobe surgery, and omits the fact that a detailed body of neurological research that emerged at this time suggested to Moniz and other neurologists and neurosurgeons that surgery on this part of the brain might yield significant personality changes in the mentally ill.
As the frontal lobes had been the object of scientific inquiry and speculation since the late 19th century, Fulton's contribution, while it may have functioned as source of intellectual support, is of itself unnecessary and inadequate as an explanation of Moniz's resolution to operate on this section of the brain. Under an evolutionary and hierarchical model of brain development it had been hypothesized that those regions associated with more recent development, such as the mammalian brain and, most especially, the frontal lobes, were responsible for more complex cognitive functions. However, this theoretical formulation found little laboratory support, as 19th century experimentation found no significant change in animal behaviour following surgical removal or electrical stimulation of the frontal lobes. This picture of the so-called "silent lobe" changed in the period after World War I with the production of clinical reports of ex-servicemen who had suffered brain trauma. The refinement of neurosurgical techniques also facilitated increasing attempts to remove brain tumours, treat focal epilepsy in humans and led to more precise experimental neurosurgery in animal studies. Cases were reported where mental symptoms were alleviated following the surgical removal of diseased or damaged brain tissue. The accumulation of medical case studies on behavioural changes following damage to the frontal lobes led to the formulation of the concept of "Witzelsucht", which designated a neurological condition characterised by a certain hilarity and childishness in the afflicted. The picture of frontal lobe function that emerged from these studies was complicated by the observation that neurological deficits attendant on damage to a single lobe might be compensated for if the opposite lobe remained intact. In 1922, the Italian neurologist Leonardo Bianchi published a detailed report on the results of bilateral lobectomies in animals that supported the contention that the frontal lobes were both integral to intellectual function and that their removal led to the disintegration of the subject's personality. This work, while influential, was not without its critics due to deficiencies in experimental design.
The first bilateral lobectomy of a human subject was performed by the American neurosurgeon Walter Dandy in 1930. The neurologist Richard Brickner reported on this case in 1932, relating that the recipient, known as "Patient A", while experiencing a flattening of affect, had suffered no apparent decrease in intellectual function and seemed, at least to the casual observer, perfectly normal. Brickner concluded from this evidence that "the frontal lobes are not 'centers' for the intellect". These clinical results were replicated in a similar operation undertaken in 1934 by the neurosurgeon Roy Glenwood Spurling and reported on by the neuropsychiatrist Spafford Ackerly. By the mid-1930s, interest in the function of the frontal lobes reached a high-water mark. This was reflected in the 1935 neurological congress in London, which hosted as part of its deliberations, "a remarkable symposium ... on the functions of the frontal lobes." The panel was chaired by Henri Claude, a French neuropsychiatrist, who commenced the session by reviewing the state of research on the frontal lobes, and concluded that, "altering the frontal lobes profoundly modifies the personality of subjects". This parallel symposium contained numerous papers by neurologists, neurosurgeons and psychologists; amongst these was one by Brickner, which impressed Moniz greatly, that again detailed the case of "Patient A". Fulton and Jacobsen's paper, presented in another session of the conference on experimental physiology, was notable in linking animal and human studies on the function of the frontal lobes. Thus, at the time of the 1935 Congress, Moniz had available to him an increasing body of research on the role of the frontal lobes that extended well beyond the observations of Fulton and Jacobsen.
Nor was Moniz the only medical practitioner in the 1930s to have contemplated procedures directly targeting the frontal lobes. Although ultimately discounting brain surgery as carrying too much risk, physicians and neurologists such as William Mayo, Thierry de Martel, Richard Brickner, and Leo Davidoff had, prior to 1935, entertained the proposition. Inspired by Julius Wagner-Jauregg's development of malarial therapy for the treatment of general paresis of the insane, the French physician Maurice Ducosté reported in 1932 that he had injected 5 ml of malarial blood directly into the frontal lobes of over 100 paretic patients through holes drilled into the skull. He claimed that the injected paretics showed signs of "uncontestable mental and physical amelioration" and that the results for psychotic patients undergoing the procedure was also "encouraging". The experimental injection of fever inducing malarial blood into the frontal lobes was also replicated during the 1930s in the work of Ettore Mariotti and M. Sciutti in Italy and Ferdière Coulloudon in France. In Switzerland, almost simultaneously with the commencement of Moniz's leucotomy programme, the neurosurgeon François Ody had removed the entire right frontal lobe of a catatonic schizophrenic patient. In Romania, Ody's procedure was adopted by Dimitri Bagdasar and Constantinesco working out of the Central Hospital in Bucharest. Ody, who delayed publishing his own results for several years, later rebuked Moniz for claiming to have cured patients through leucotomy without waiting to determine if there had been a "lasting remission".
Neurological model.
The theoretical underpinnings of Moniz's psychosurgery were largely commensurate with the nineteenth century ones that had informed Burckhardt's decision to excise matter from the brains of his patients. Although in his later writings Moniz referenced both the neuron theory of Ramón y Cajal and the conditioned reflex of Ivan Pavlov, in essence he simply interpreted this new neurological research in terms of the old psychological theory of associationism. He differed significantly from Burckhardt, however in that he did not think there was any organic pathology in the brains of the mentally ill, but rather that their neural pathways were caught in fixed and destructive circuits leading to "predominant, obsessive ideas." As Moniz wrote in 1936:
[The] mental troubles must have ... a relation with the formation of cellulo-connective groupings, which become more or less fixed. The cellular bodies may remain altogether normal, their cylinders will not have any anatomical alterations; but their multiple liaisons, very variable in normal people, may have arrangements more or less fixed, which will have a relation with persistent ideas and deliria in certain morbid psychic states.
For Moniz, "to cure these patients," it was necessary to "destroy the more or less fixed arrangements of cellular connections that exist in the brain, and particularly those which are related to the frontal lobes," thus removing their fixed pathological brain circuits. Moniz believed the brain would functionally adapt to such injury. A significant advantage of this approach was that, unlike the position adopted by Burckhardt, it was unfalsifiable according to the knowledge and technology of the time as the absence of a known correlation between physical brain pathology and mental illness could not disprove his thesis.
The first leucotomies.
The hypotheses underlying the procedure might be called into question; the surgical intervention might be considered very audacious; but such arguments occupy a secondary position because it can be affirmed now that these operations are not prejudicial to either physical or psychic life of the patient, and also that recovery or improvement may be obtained frequently in this way
”
Egas Moniz (1937)
On 12 November 1935 at the Hospital Santa Marta in Lisbon, Moniz initiated the first of a series of operations on the brains of the mentally ill. The initial patients selected for the operation were provided by the medical director of Lisbon's Miguel Bombarda Mental Hospital, José de Matos Sobral Cid. As Moniz lacked training in neurosurgery and his hands were crippled from gout, the procedure was performed under general anaesthetic by Pedro Almeida Lima, who had previously assisted Moniz with his research on cerebral angiography. The intention was to remove some of the long fibres that connected the frontal lobes to other major brain centres. To this end, it was decided that Lima would trephine into the side of the skull and then inject ethanol into the "subcortical white matter of the prefrontal area" so as to destroy the connecting fibres, or association tracts, and create what Moniz termed a "frontal barrier". After the first operation was complete, Moniz considered it a success and, observing that the patient's depression had been relieved, he declared her "cured" although she was never, in fact, discharged from the mental hospital. Moniz and Lima persisted with this method of injecting alcohol into the frontal lobes for the next seven patients but, after having to inject some patients on numerous occasions to elicit what they considered a favourable result, they modified the means by which they would section the frontal lobes. For the ninth patient they introduced a surgical instrument called a leucotome; this was a cannula that was 11 cm in length and 2 cm in diameter. It had a retractable wire loop at one end that, when rotated, produced a 1 cm diameter, circular lesion in the white matter of the frontal lobe. Typically, six lesions were cut into each lobe, but, if they were dissatisfied by the results, Lima might perform several procedures, each producing multiple lesions in the left and right frontal lobes.
By the conclusion of this first run of leucotomies in February 1936, Moniz and Lima had operated on twenty patients with an average period of one week between each procedure; Moniz published his findings with great haste in March of the same year. The patients were aged between 27 and 62 years of age, twelve were female and eight were male. Nine of the patients were diagnosed as suffering from depression, six from schizophrenia, two from panic disorder, and one each from mania, catatonia and manic-depression with the most prominent symptoms being anxiety and agitation. The duration of the illness prior to the procedure varied from as little as four weeks to as much as 22 years, although all but four had been ill for at least one year. Patients were normally operated on the day they arrived at Moniz's clinic and returned within ten days to the Miguel Bombarda Mental Hospital. A perfunctory post-operative follow-up assessment took place anywhere from one to ten weeks following surgery. Complications were observed in each of the leucotomy patients and included: "increased temperature, vomiting, bladder and bowel incontinence, diarrhea, and ocular affections such as ptosis and nystagmus, as well as psychological effects such as apathy, akinesia, lethargy, timing and local disorientation, kleptomania, and abnormal sensations of hunger". Moniz asserted that these effects were transitory and, according to his published assessment, the outcome for these first twenty patients was that 35%, or seven cases, improved significantly, another 35% were somewhat improved and the remaining 30% (six cases) were unchanged. There were no deaths and he did not consider that any patients had deteriorated following leucotomy.
Reception.
Moniz rapidly disseminated his results through articles in the medical press and a monograph in 1936. Initially, however, the medical community appeared hostile to the new procedure. On 26 July 1936, one of his assistants, Diogo Furtado, gave a presentation at the Parisian meeting of the Société Médico-Psychologique on the results of the second cohort of patients leucotomised by Lima. Sobral Cid, who had supplied Moniz with the first set of patients for leucotomy from his own hospital in Lisbon, attended the meeting where he denounced frontal lobe surgery, declaring that the patients who had been returned to his care post-operatively were "diminished" and had suffered a "degradation of personality". He also claimed that the changes Moniz observed in patients were more properly attributed to shock and brain trauma, and he derided the theoretical architecture that Moniz had constructed to support the new procedure as "cerebral mythology." At the same meeting the Parisian psychiatrist, Paul Courbon, stated that he could not endorse a surgical technique that was solely supported by theoretical considerations rather than clinical observations. He also opined that the mutilation of an organ could not improve its function and that such cerebral wounds as were occasioned by leucotomy risked the later development of meningitis, epilepsy and brain abscesses. Nonetheless, Moniz's reported successful surgical treatment of 14 out of 20 patients led to the rapid adoption of the procedure on an experimental basis by individual clinicians in countries such as Brazil, Cuba, Italy, Romania and the United States during the 1930s.
Italian leucotomy.
In the present state of affairs if some are critical about lack of caution in therapy, it is, on the other hand, deplorable and inexcusable to remain apathetic, with folded hands, content with learned lucubrations upon symptomatologic minutiae or upon psychopathic curiosities, or even worse, not even doing that.
”
Amarro Fiamberti
Throughout the remainder of the 1930s the number of leucotomies performed in most countries where the technique was adopted remained quite low. In Britain, which was later a major centre for leucotomy, only six operations had been undertaken prior to 1942. Generally, medical practitioners who attempted the procedure adopted a cautious approach and few patients were leucotomised prior to the 1940s. Italian neuropsychiatrists, who were typically early and enthusiastic adopters of leucotomy, were exceptional in eschewing such a gradualist course.
Leucotomy was first reported in the Italian medical press in 1936 and Moniz published an article in Italian on the technique in the following year. In 1937, he was invited to Italy to demonstrate the procedure and for a two-week period in June of that year he visited medical centres in Trieste, Ferrara, and one close to Turin – the Racconigi Hospital – where he instructed his Italian neuropsychiatric colleagues on leucotomy and also oversaw several operations. Leucotomy was featured at two Italian psychiatric conferences in 1937 and over the next two years a score of medical articles on Moniz's psychosurgery was published by Italian clinicians based in medical institutions located in Racconigi, Trieste, Naples, Genoa, Milan, Pisa, Catania and Rovigo. The major centre for leucotomy in Italy was the Racconigi Hospital, where the experienced neurosurgeon Ludvig Puusepp provided a guiding hand. Under the medical directorship of Emilio Rizzatti, the medical personnel at this hospital had completed at least 200 leucotomies by 1939. Reports from clinicians based at other Italian institutions detailed significantly smaller numbers of leucotomy operations.
Experimental modifications of Moniz's operation were introduced with little delay by Italian medical practitioners. Most notably, in 1937 Amarro Fiamberti, the medical director of a psychiatric institution in Varese, first devised the transorbital procedure whereby the frontal lobes were accessed through the eye sockets. Fiamberti's method was to puncture the thin layer of orbital bone at the top of the socket and then inject alcohol or formalin into the white matter of the frontal lobes through this aperture. Using this method, while sometimes substituting a leucotome for a hypodermic needle, it is estimated that he leucotomised about 100 patients in the period up to the outbreak of World War II. Fiamberti's innovation of Moniz's method would later prove inspirational for Walter Freeman's development of transorbital lobotomy.
American leucotomy.
The first prefrontal leucotomy on American soil was performed at the George Washington University Hospital on 14 September 1936 by the neuropsychiatrist Walter Freeman and his friend and colleague, the neurosurgeon, James W. Watts. Freeman had first encountered Moniz at the London hosted Second International Congress of Neurology in 1935 where he had presented a poster exhibit of the Portuguese neurologist's work on cerebral angiography. Fortuitously occupying a booth next to Moniz, Freeman, delighted by their chance meeting, formed a highly favourable impression of Moniz, later remarking upon his "sheer genius". According to Freeman, if they had not met in person it is highly unlikely that he would have ventured into the domain of frontal lobe psychosurgery. Freeman's interest in psychiatry was the natural outgrowth of his appointment in 1924 as the medical director of the Research Laboratories of the Government Hospital for the Insane in Washington, known colloquially as St Elizabeth's. Ambitious and a prodigious researcher, Freeman, who favoured an organic model of mental illness causation, spent the next several years exhaustively, yet ultimately fruitlessly, investigating a neuropathological basis for insanity. Chancing upon a preliminary communication by Moniz on leucotomy in the spring of 1936, Freeman initiated a correspondence in May of that year. Writing that he had been considering psychiatric brain surgery previously, he informed Moniz that, "having your authority I expect to go ahead". Moniz, in return, promised to send him a copy of his forthcoming monograph on leucotomy and urged him to purchase a leucotome from a French supplier.
Upon receipt of Moniz's monograph, Freeman reviewed it anonymously for the "Archives of Neurology and Psychiatry". Praising the text as one whose "importance can scarcely be overestimated", he summarised Moniz's rationale for the procedure as based on the fact that while no physical abnormality of cerebral cell bodies was observable in the mentally ill, their cellular interconnections may harbour a "fixation of certain patterns of relationship among various groups of cells" and that this resulted in obsessions, delusions and mental morbidity. While recognising that Moniz's thesis was inadequate, for Freeman it had the advantage of circumventing the search for diseased brain tissue in the mentally ill by instead suggesting that the problem was a functional one of the brain's internal wiring where relief might be obtained by severing problematic mental circuits.
In 1937 Freeman and Watts adapted Lima and Moniz's surgical procedure, and created the "Freeman-Watts technique", also known as the "Freeman-Watts standard prefrontal lobotomy," which they styled the "precision method."
Transorbital lobotomy.
The Freeman-Watts prefrontal lobotomy still required drilling holes in the scalp, so surgery had to be performed in an operating room by trained neurosurgeons. Walter Freeman believed this surgery would be unavailable to those he saw as needing it most: patients in state mental hospitals that had no operating rooms, surgeons, or anesthesia and limited budgets. Freeman wanted to simplify the procedure so that it could be carried out by psychiatrists in psychiatric hospitals.
Inspired by the work of Italian psychiatrist Amarro Fiamberti, Freeman at some point conceived of approaching the frontal lobes through the eye sockets instead of through drilled holes in the skull. In 1945 he took an icepick from his own kitchen and began testing the idea on grapefruit and cadavers. This new "transorbital" lobotomy involved lifting the upper eyelid and placing the point of a thin surgical instrument (often called an orbitoclast or leucotome, although quite different from the wire loop leucotome described above) under the eyelid and against the top of the eyesocket. A mallet was used to drive the orbitoclast through the thin layer of bone and into the brain along the plane of the bridge of the nose, around fifteen degrees toward the interhemispherical fissure. The orbitoclast was malleted five centimeters (2 in) into the frontal lobes, and then pivoted forty degrees at the orbit perforation so the tip cut toward the opposite side of the head (toward the nose). The instrument was returned to the neutral position and sent a further two centimeters (4⁄5 in) into the brain, before being pivoted around twenty-eight degrees each side, to cut outwards and again inwards. (In a more radical variation at the end of the last cut described, the butt of the orbitoclast was forced upwards so the tool cut vertically down the side of the cortex of the interhemispherical fissure; the "Deep frontal cut".) All cuts were designed to transect the white fibrous matter connecting the cortical tissue of the prefrontal cortex to the thalamus. The leucotome was then withdrawn and the procedure repeated on the other side.
Freeman performed the first transorbital lobotomy on a live patient in 1946. Its simplicity suggested the possibility of carrying it out in mental hospitals lacking the surgical facilities required for the earlier, more complex procedure (Freeman suggesting that, where conventional anesthesia was unavailable, electroconvulsive therapy be used to render the patient unconscious). In 1947, the Freeman and Watts partnership ended, as the latter was disgusted by Freeman's modification of the lobotomy from a surgical operation into a simple "office" procedure. Between 1940 and 1944, 684 lobotomies were performed in the United States. However, because of the fervent promotion of the technique by Freeman and Watts, those numbers increased sharply towards the end of the decade. In 1949, the peak year for lobotomies in the US, 5,074 procedures were undertaken, and by 1951 over 18,608 individuals had been lobotomized in the US.
Prevalence.
In the United States, approximately 40,000 people were lobotomized. In Great Britain, 17,000 lobotomies were performed, and the three Nordic countries of Finland, Norway, and Sweden had a combined figure of approximately 9,300 lobotomies. Scandinavian hospitals lobotomized 2.5 times as many people per capita as hospitals in the US. Sweden lobotomized at least 4,500 people between 1944 and 1966, mainly women. This figure includes young children. In Norway, there were 2,500 known lobotomies. In Denmark, there were 4,500 known lobotomies, mainly young women, as well as mentally retarded children. In Japan, the majority of lobotomies were performed on children with behavior problems. The Soviet Union banned the practice in 1950 on moral grounds, and Japan and Germany soon followed suit. By the late 1970s, the practice of lobotomy had generally ceased.
Effects.
I fully realize that this operation will have little effect on her mental condition but am willing to have it done in the hope that she will be more comfortable and easier to care for.
”
Comments added to the consent form for a lobotomy operation on "Helaine Strauss", "a patient at an elite private hospital".
The purpose of the operation was to reduce the symptoms of mental disorder, and it was recognized that this was accomplished at the expense of a person's personality and intellect. British psychiatrist Maurice Partridge, who conducted a follow-up study of 300 patients, said that the treatment achieved its effects by "reducing the complexity of psychic life". Following the operation, spontaneity, responsiveness, self-awareness and self-control were reduced. Activity was replaced by inertia, and people were left emotionally blunted and restricted in their intellectual range.
The consequences of the operation have been described as "mixed".
Some patients died as a result of the operation and others later committed suicide. Some were left severely brain damaged. Others were able to leave the hospital, or became more manageable within the hospital. A few people managed to return to responsible work, while at the other extreme people were left with severe and disabling impairments. Most people fell into an intermediate group, left with some improvement of their symptoms but also with emotional and intellectual deficits to which they made a better or worse adjustment. On average, there was a mortality rate of approximately 5 percent during the 1940s.
The lobotomy procedure could have severe negative effects on a patient's personality and ability to function independently. Lobotomy patients often show a marked reduction in initiative and inhibition. They may also exhibit difficulty putting themselves in the position of others because of decreased cognition and detachment from society.
Immediately following surgery patients were often stuporous, confused, and incontinent. Some developed an enormous appetite and gained considerable weight. Seizures were another common complication of surgery. Emphasis was put on the training of patients in the weeks and months following surgery.
Freeman coined the term "surgically induced childhood" and used it constantly to refer the results of lobotomy. The operation left people with an "infantile personality"; a period of maturation would then, according to Freeman, lead to recovery. In an unpublished memoir he described how the "personality of the patient was changed in some way in the hope of rendering him more amenable to the social pressures under which he is supposed to exist." He described one 29-year-old woman as being, following lobotomy, a "smiling, lazy and satisfactory patient with the personality of an oyster" who couldn't remember Freeman's name and endlessly poured coffee from an empty pot. When her parents had difficulty dealing with her behaviour, Freeman advised a system of rewards (ice-cream) and punishment (smacks).
Russian psychiatrist Fedor Kondratev, of the Serbsky Center, said that thousands of the people with schizophrenia to whom the method was applied have completely lost the remnants of their mental health, their fate has been irrevocably broken.
Criticism.
As early as 1944 an author in the "Journal of Nervous and Mental Disease" remarked: "The history of prefrontal lobotomy has been brief and stormy. Its course has been dotted with both violent opposition and with slavish, unquestioning acceptance." Beginning in 1947 Swedish psychiatrist Snorre Wohlfahrt evaluated early trials, reporting that it is "distinctly hazardous to leucotomize schizophrenics" and lobotomy to be "still too imperfect to enable us, with its aid, to venture on a general offensive against chronic cases of mental disorder" and stating that "Psychosurgery has as yet failed to discover its precise indications and contraindications and the methods must unfortunately still be regarded as rather crude and hazardous in many respects." In 1948 Norbert Wiener, the author of "", said: "[P]refrontal lobotomy ... has recently been having a certain vogue, probably not unconnected with the fact that it makes the custodial care of many patients easier. Let me remark in passing that killing them makes their custodial care still easier."
Concerns about lobotomy steadily grew. Soviet psychiatrist Vasily Gilyarovsky criticized lobotomy and the mechanistic brain localization assumption used to carry out lobotomy: "It is assumed that the transection of white substance of the frontal lobes impairs their connection with the thalamus and eliminates the possibility to receive from it stimuli which lead to irritation and on the whole derange mental functions. This explanation is mechanistic and goes back to the narrow localizationism characteristic of psychiatrists of America, from where leucotomy was imported to us." The USSR officially banned the procedure in 1950 on the initiative of Gilyarovsky. Doctors in the Soviet Union concluded that the procedure was "contrary to the principles of humanity" and "'through lobotomy' an insane person is changed into an idiot." By the 1970s, numerous countries had banned the procedure as had several US states.
In 1977 the US Congress, during the presidency of Jimmy Carter, created the National Committee for the Protection of Human Subjects of Biomedical and Behavioral Research to investigate allegations that psychosurgery—including lobotomy techniques—were used to control minorities and restrain individual rights. The committee concluded that some extremely limited and properly performed psychosurgery could have positive effects.
There have been calls in the early 21st century for the Nobel Foundation to rescind the prize it awarded to Moniz for developing the lobotomy, a decision that has been called an astounding error of judgment at the time and one that psychiatry might still need to learn from, but the Foundation declined to take action and has continued to host an article defending the results of the procedure.
Tennessee Williams criticised lobotomy in his play "Suddenly, Last Summer" because it was sometimes inflicted on homosexuals—to render them "morally sane".
Notable cases.
See also: .
Significant literary and cinematic portrayals.
Lobotomies have been featured in several literary and cinematic presentations that both reflected society's attitude towards the procedure and, at times, changed it. Writers and film-makers have played a pivotal role in forming a negative public sentiment towards the procedure.
Sources.
Print Sources
. The Rise and Fall of Frontal Leucotomy . In: Whitelaw, W.A. (ed). ". Calgary: 2004. p. 32–41.
 . "Shadowland ". Berkley Books ; 1982. ISBN 0-425-05481-0.
. La chirugie des aliénes [Surgery of the Insane] . "Archives Internationales de Neurologie". 1912;34:1–17, 69–89.
. . "History of Psychiatry". 1997;8(1):61–81. doi:. PMID 11619209.
. ". New York & Edinburgh: William Wood & Co; E.S Livingstone; 1922.
. ". Cambridge University Press; 1985. ISBN 978-0-521-27717-4.
 . ". 1st ed. TwoDot; 2005. ISBN 978-0-7627-3427-6.
. Effect of therapeutic innovation on perception of disease and the doctor-patient relationship: a history of general paralysis of the insane and malaria fever therapy, 1910–1950 . "American Journal of Psychiatry". May 1995;152(2):660–665. PMID 7726304.
. ". University of California; 1997. ISBN 0-520-20547-2.
. An interpretation of frontal lobe function based upon the study of a case of partial bilateral frontal lobotomy . "Research Publications – Association for Research for Nervous end Mental Disease". 1932;13:259–351.
 . . "History of Psychiatry". 2000;11(4):371–382. doi:.
 . On deciding to have a lobotomy: either lobotomies were justified or decisions under risk should not always seek to maximise expected utility . "Medicine, Health Care and Philosophy". 2014;17(1):143-154. doi:.
. Neuropatients in Historyland . In: Jacyna, Stephen J.; Casper, Stephen T. (eds). ". Rochester NY: University of Rochester Press; 2012. ISBN 978-1-58046-412-3. p. 215–222.
. . "The Guardian". 12 January 2008 [Retrieved 31 March 2010].
. . "Federal Register". 23 May 1977;42(99):26317–26332.
. ". New York: Harcourt, Brace; 1948.
. . "Journal of the History of the Neurosciences". 1999;8(1):60–69. doi:. PMID 11624138.
 . . " American Journal of Roentgenology ". 1992;359(2):364.
. "The Lobotomist: A Maverick Medical Genius and His Tragic Quest to Rid the World of Mental Illness ". Wiley; 2005. ISBN 0-471-23292-0.
 . ". Arcade Publishing; 2001. ISBN 1-55970-592-2.
. Psychosurgery: A Historical Overview . "Neurosurgery". March 2001;48(3):647–57; discussion 657–9. PMID 11270556.
. ". Oxford: Oxford University Press; 2001. ISBN 978-0-19-514694-3.
. Functional neurosurgical intervention: neuroethics in the operating rooms . In: Illes, Judy (ed.). ". Oxford University Press; 2006. ISBN 978-0-19-856721-9.
. "Discovering Biological Psychology ". 2nd ed. Belmont, California: Wadsworth; 2010. ISBN 0-547-17779-8.
 . Psychosurgery: An Evaluation of Two Hundred Cases over Seven Years . " Journal of Mental Science ". 1944;90(379):532–537.
 . ". 2nd ed. American Psychiatric Publishing, Inc.; 1999. ISBN 978-0-88048-964-5.
. Учение Павлова — основа психиатрии [Pavlov's teaching is the basis of psychiatry] . "Медицинский работник [Health Worker]". 14 September 1950;(37). Russian.
. "Избранные труды [Selected Works]". Moscow: Медицина [Medicine]; 1973. Russian. p. 4.
. . "British Medical Journal". 21 September 1996;313(7059):708–709. doi:. PMID 11644825.
. Functional and epilepsy neurosurgery . In: Johnson, Reuben; Green, Alexander (eds). ". Oxford University Press; 2010. ISBN 978-0-19-959125-1.
 . . "Journal of Libertarian Studies ". 1978 [Retrieved 22 January 2008];2(1):29–44. PMID 11614766.
. Egas Moniz (1874–1955) and the "invention" of modern psychosurgery: a historical and ethical reanalysis under special consideration of Portuguese original sources . "Neurosurgical Focus". 2011;30(2):8. doi:. PMID 21284454.
. Some continuities and discontinuities in the pharmacotherapy of nervous conditions before and after chlorpromazine and imipramine . "History of Psychiatry". January 2000;11(44):393–412. doi:.
. Surgery of the mind and mood: A mosaic of issues in time and evolution . " Neurosurgery". 2006;59(4):727.
. Schizophrenia . In: Berrios, German E.; Porter, Roy (eds). "A History of Clinical Psychiatry: The Origin and History of Psychiatric Disorders ". Athlone; 1995. ISBN 0-485-24011-4.
. From theory to practice: the unconventional contribution of Gottlieb Burckhardt to psychosurgery . "Brain and Language". 1993;45(4):572–587. doi:. PMID 8118674.
. Turning the Mind Inside Out . "Saturday Evening Post". 24 May 1941;213(47):18–19, 69–74.
. . "Journal of Dramatic Theory and Criticism". Spring 1998;12(2):35–55.
. ". Moscow: ЗАО Юстицинформ [Closed joint-stock company Justitsinform]; 2010. Russian. The ISBN in the document (978-5-9977-0014-9) is bad; it causes a checksum error.
 . . " Gesnerus ". 2005;62(1/2):77–101.
. Psychosurgery in Italy, 1936–39 . " History of Psychiatry". December 2008;19(4):476–489. doi:.
 . . "Medical History". 2010;54(3):341–64. doi:. PMID 20592884. PMC .
. ". Oxford: Oxford University Press; 1991. ISBN 978-0-19-506284-7.
. On the history of psychosurgery in Russia . "Acta Neurochirugie". 1993;125(1–4):1–4. doi:. PMID 8122532.
. . 6 May 1946.
. Modern psychosurgery before Egas Moniz: a tribute to Gottlieb Burckhardt . "Neurosurgery Focus". 2008;25(1):1–4.
 . "An Odd Kind of Fame: Stories of Phineas Gage ". MIT Press ; 2000. ISBN 0-262-13363-6.
. Deep brain stimulation in psychiatric disorders . In: Fangerau, Heiner; Jörg, Fegert; Mareke, Arends (eds). ". Verlag; 2010. ISBN 978-3-8376-1433-6.
. . "New York Times". 30 January 2004 [Retrieved 17 November 2007].
. . "Brain Research Reviews". 2005;48(3):409–419. doi:. PMID 15914249.
 . Autobiography of L.J. Meduna . "Convulsive Therapy". 1985;1(1):43–57. PMID 11940805.
. ". American Psychiatric Publishing; 1994. ISBN 978-0-89042-275-5. p. 237–239.
. ". Infobase Publishing; January 2007. ISBN 978-0-8160-6405-2.
. "Modern clinical psychiatry ". Philadelphia and London: W.B. Saunders; 1962.
 . . " Journal of the History of the Neurosciences ". 2005;14(4):353–67. doi:. PMID 16338693.
. "Pre-frontal leucotomy: ". Oxford: Blackwell Scientific Publications; 1950.
. ". Cambridge: Cambridge University Press; 2002. ISBN 0-521-52459-8.
 . . "The Independent ". 6 February 2008 [Retrieved 11 July 2010].
. "The Greatest Benefit to Mankind: A Medical History of Humanity from Antiquity to the Present ". Fontana Press; 1999. ISBN 0-00-637454-9.
. Alcune considerazioni sugli interventi chirurgici nelle malattie mentali [Some Considerations about Surgery in Mental Illness] . "G Acad Med Torino". 1937;100:3–16.
. Psychosurgery, Industry and Personal Responsibility, 1940–1965 . "Social History of Medicine". 2009;23(1):116–133. doi:.
. "The lobotomy letters: the making of American psychosurgery ". University of Rochester Press; 2013. ISBN 9781580464499.
. ". ABC-CLIO; 2010. ISBN 978-0-313-34576-0.
. Neurosurgery for psychiatric disorders: from the excision of brain tissue to the chronic electrical stimulation of neural networks . In: Sakas, D.E.; Simpson, B.A. (eds). ". Vol. 2. Springer; 2007. ISBN 978-3-211-33081-4. p. 365–374.
. ". Wiley; 1997. ISBN 0-471-24531-3.
. ". AuthorHouse; 2012. ISBN 1-4685-4962-6.
. ". Van Nostrand Reinhold; 1982. ISBN 978-0-442-20252-1.
. ". Williams & Wilkins; 1963.
. Milestones in the development of neurology and psychiatry in Europe . "Schweizer Archiv fur Neurologie und Psychiatrie". 2010;161(3):85–9.
 . Gottlieb Burckhardt – The Pioneer of Psychosurgery . "Journal of the History of the Neurosciences ". 2001;10(1):79–92. doi:. PMID 11446267.
. ". Guilford Press; 2011. ISBN 978-1-60918-072-0.
. . "American Journal of Psychiatry". 1995;152(4):505–515. PMID 7900928.
. "Coercion as Cure: A Critical History of Psychiatry ". New Brunswick, New Jersey: Transaction; 2007. ISBN 0-7658-0379-8.
. Egas Moniz and the Origins of Psychosurgery: A Review Commemorating the 50th Anniversary of Moniz's Nobel Prize . "Journal of the History of the Neurosciences". April 2000;9(1):22–36. doi:. PMID 11232345.
. . "The Journal of Mind and Behavior ". 1996;17(1):1–20.
. . "History of Psychiatry". 2005 [ 3 December 2007];16(1):107–110. doi:.
. Приказ МЗ СССР 1003 (9 декабря 1950) [Order 1003 (9 December 1950)] . "Невропатология и психиатрия [Neuropathology and Psychiatry]". 1951;20(1):17–18. Russian.
. The Prefrontal Area and Psychosurgery . In: Uylings, H.B.M.; Van Eden, C.G.; De Bruin, J.P.C.; Corner, M.A.; Feenstra, M.G.P. (eds). ". Amsterdam & New York: Elsevier; 1990. (Progress in Brain Research, Volume 85). ISBN 978-0-444-81124-0. p. 539–554.
. History of Psychosurgery . In: Greenblatt, Samuel H. (ed.). ". Contributing editors Dagi, T. Forcht; Epstein, Mel H.. Park Ridge, IL: The American Association of Neurological Surgeons; 1997. ISBN 978-1-879284-17-3. p. 499–516.
. . In: Miller, Bruce L.; Cummings, Jeffrey L. (eds). "The Human Frontal Lobes: Functions and Disorders ". Guilford Press; 2007. ISBN 978-1-59385-329-7. p. 505–517.
. A psychosurgical chapter in the history of cerebral localization: the six cases of Gottlieb Burkhardt . In: Code, Christopher; Wallesch, C.-W.; Joanette Y.; Roch A. (eds). ". Hove: Psychology Press; 1996. ISBN 978-0-86377-395-2. p. 275–304.
. "Cybernetics ". MIT Press; 1948. ISBN 0-262-73009-X.
 . ". Dramatists Play Service; 1998. ISBN 978-0-8222-1094-8.
. ". New Harbinger Publications ; 2008. ISBN 978-1-57224-568-6.
. ". Philadelphia: Mental Health Foundation; 1947.
Online sources
. " [podcast]. NPR; 16 November 2005 [Retrieved 28 November 2009].
. ". Nobelprize.org; 29 October 1998 [Retrieved 24 January 2012].
 . ". The Center for the History of Psychology, University of Akron, Ohio, USA; 1999–2012 [Retrieved 21 March 2009].
. ". Nobelprize.org; 2013 [Retrieved 24 January 2013].
. . "The Guardian ". 5 August 2004 [Retrieved 22 December 2011].
</dl>

</doc>
<doc id="46191" url="http://en.wikipedia.org/wiki?curid=46191" title="Tillage">
Tillage

Tillage is the agricultural preparation of soil by mechanical agitation of various types, such as digging, stirring, and overturning. Examples of human-powered tilling methods using hand tools include shovelling, picking, mattock work, hoeing, and raking. Examples of draft-animal-powered or mechanized work include ploughing (overturning with moldboards or chiseling with chisel shanks), rototilling, rolling with cultipackers or other rollers, harrowing, and cultivating with cultivator shanks (teeth). Small-scale gardening and farming, for household food production or small business production, tends to use the smaller-scale methods above, whereas medium- to large-scale farming tends to use the larger-scale methods. There is a fluid continuum, however. Any type of gardening or farming, but especially larger-scale commercial types, may also use low-till or no-till methods as well.
Tillage is often classified into two types, primary and secondary. There is no strict boundary between them so much as a loose distinction between tillage that is deeper and more thorough (primary) and tillage that is shallower and sometimes more selective of location (secondary). Primary tillage such as ploughing tends to produce a rough surface finish, whereas secondary tillage tends to produce a smoother surface finish, such as that required to make a good seedbed for many crops. Harrowing and rototilling often combine primary and secondary tillage into one operation.
"Tillage" can also mean the land that is tilled. The word "cultivation" has several senses that overlap substantially with those of "tillage". In a general context, both can refer to agriculture. Within agriculture, both can refer to any of the kinds of soil agitation described above. Additionally, "cultivation" or "cultivating" may refer to an even narrower sense of shallow, selective secondary tillage of row crop fields that kills weeds while sparing the crop plants.
Tillage systems.
Reduced tillage.
Reduced tillage leaves between 15 and 30% residue cover on the soil or 500 to 1000 pounds per acre (560 to 1100 kg/ha) of small grain residue during the critical erosion period. This may involve the use of a chisel plow, field cultivators, or other implements. See the general comments below to see how they can affect the amount of residue.
Intensive tillage.
Intensive tillage leaves less than 15% crop residue cover or less than 500 pounds per acre (560 kg/ha) of small grain residue. This type of tillage is often referred to as conventional tillage but as conservational tillage is now more widely used than intensive tillage (in the United States), it is often not appropriate to refer to this type of tillage as conventional. Intensive tillage often involves multiple operations with implements such as a mold board, disk, and/or chisel plow. Then a finisher with a harrow, rolling basket, and cutter can be used to prepare the seed bed. There are many variations.
Conservation tillage.
Conservation tillage leaves at least 30% of crop residue on the soil surface, or at least 1,000 lb/ac (1,100 kg/ha) of small grain residue on the surface during the critical soil erosion period. This slows water movement, which reduces the amount of soil erosion. Conservation tillage also benefits farmers by reducing fuel consumption and soil compaction. By reducing the number of times the farmer travels over the field, farmers realize significant savings in fuel and labor. In most years since 1997, conservation tillage was used in US cropland more than intensive or reduced tillage.
However, conservation tillage delays warming of the soil due to the reduction of dark earth exposure to the warmth of the spring sun, thus delaying the planting of the next year's spring crop of corn.
Zone tillage.
Zone tillage is a form of modified deep tillage in which only narrow strips are tilled, leaving soil in between the rows untilled. This type of tillage agitates the soil to help reduce soil compaction problems and to improve internal soil drainage.
Purpose.
Zone tillage is designed to only disrupt the soil in a narrow strip directly below the crop row. In comparison to no-till, which relies on the previous year’s plant residue to protect the soil and aides in postponement of the warming of the soil and crop growth in Northern climates, zone tillage creates approximately a 5-inch-wide strip that simultaneously breaks up plow pans, assists in warming the soil and helps to prepare a seedbed. When combined with cover crops, zone tillage helps replace lost organic matter, slows the deterioration of the soil, improves soil drainage, increases soil water and nutrient holding capacity, and allows necessary soil organisms to survive.
Usage.
It has been successfully used on farms in the mid-west and west for over 40 years and is currently used on more than 36% of the U.S. farmland. Some specific states where zone tillage is currently in practice are Pennsylvania, Connecticut, Minnesota, Indiana, Wisconsin, and Illinois.
Unfortunately, there aren't consistent yield results in the Northern Cornbelt states however; there is still interest in deep tillage within the agriculture industry. In areas that are not well-drained, deep tillage may be used as an alternative to installing more expensive tile drainage.
Effects of tillage.
Positive.
Plowing:
Definitions.
"Primary tillage" loosens the soil and mixes in fertilizer and/or plant material, resulting in soil with a rough texture.
"Secondary tillage" produces finer soil and sometimes shapes the rows, preparing the seed bed. It also provides weed control throughout the growing season during the maturation of the crop plants, unless such weed control is instead achieved with low-till or no-till methods involving herbicides.
History of tilling.
Tilling was first performed via human labor, sometimes involving slaves. Hoofed animals could also be used to till soil via trampling. The wooden plow was then invented. It could be pulled by mule, ox, elephant, water buffalo, or similar sturdy animal. Horses are generally unsuitable, though breeds such as the scyne could work. The steel plow allowed farming in the American Midwest, where tough prairie grasses and rocks caused trouble. Soon after 1900, the farm tractor was introduced, which eventually made modern large-scale agriculture possible.
Alternatives to tilling.
Modern agricultural science has greatly reduced the use of tillage. Crops can be grown for several years without any tillage through the use of herbicides to control weeds, crop varieties that tolerate packed soil, and equipment that can plant seeds or fumigate the soil without really digging it up. This practice, called no-till farming, reduces costs and environmental change by reducing soil erosion and diesel fuel usage. Researchers are investigating farming in polyculture that would eliminate the need for both tillage and pesticides, such as no-dig gardening.

</doc>
<doc id="46193" url="http://en.wikipedia.org/wiki?curid=46193" title="Threshing machine">
Threshing machine

The thrashing machine, or, in modern spelling, threshing machine (or simply thresher), was first invented by Scottish mechanical engineer Andrew Meikle for use in agriculture. It was devised (c.1786) for the separation of grain from stalks and husks. For thousands of years, grain was separated by hand with flails, and was very laborious and time consuming, taking about one-quarter of agricultural labor by the 18th century. Mechanization of this process took much of the drudgery out of farm labour.
Early social impacts.
The Swing Riots in the UK were partly a result of the threshing machine. Following years of war, high taxes and low wages, farm labourers finally revolted in 1830. These farm labourers had faced unemployment for a number of years due to the widespread introduction of the threshing machine and the policy of enclosing fields. No longer were thousands of men needed to tend the crops, a few would suffice. With fewer jobs, lower wages and no prospects of things improving for these workers the threshing machine was the final straw, the machine was to place them on the brink of starvation. The Swing Rioters smashed threshing machines and threatened farmers who had them.
The riots were dealt with very harshly. Nine of the rioters were hanged and a further 450 were transported to Australia.
Later adoption.
Early threshing machines were hand-fed and horse-powered. They were small by today's standards and were about the size of an upright piano. Later machines were steam-powered, driven by a portable engine or traction engine. Isaiah Jennings, a skilled inventor, created a small thresher that doesn't harm the straw in the process. In 1834, John Avery and Hiram Abial Pitts devised significant improvements to a machine that automatically threshes and separates grain from chaff, freeing farmers from a slow and laborious process. Avery and Pitts were granted United States patent #542 on December 29, 1837.
John Ridley, an Australian inventor, also developed a threshing machine in South Australia in 1843.
The 1881 "Household Cyclopedia" said of Meikle's machine:
Steam-powered machines used belts connected to a traction engine; often both engine and thresher belonged to a contractor who toured the farms of a district. Steam remained a viable commercial option until the early post-WWII years.
Farming process.
Threshing is just one process in getting cereals to the grinding mill and customer.
The wheat needs to be grown, cut, stroked (shocked, bundled), hauled, threshed, de-chaffed, straw baled, and then the grain hauled to a grain elevator. For many years each of these steps was an individual process, requiring teams of workers and many machines. In the steep hill wheat country of Palouse in the Northwest of the United States, steep ground meant moving machinery around was problematic and prone to rolling. To reduce the amount of work on the sidehills, the idea arose of combining the wheat binder and thresher into one machine—a combine harvester, known as a combine harvester. About 1910, horse pulled combines appeared and became a success. Later, gas and diesel engines appeared with other refinements and specifications.
Modern developments.
In Europe and Americas.
Modern day combine harvesters (or simply combines) operate on the same principles and use the same components as the original threshing machines built in the 19th century. Combines also perform the reaping operation at the same time. The name "combine" is derived from the fact that the two steps are combined in a single machine. Also, most modern combines are self-powered, usually by a diesel engine, and self-propelled although tractor powered pull type combines models were offered by John Deere and Case International into the 1990s.
Today, as in the 19th century, the threshing begins with a cylinder and concave. The cylinder has sharp serrated bars, and rotates at high speed (about 500 RPM), so that the bars beat against the grain. The concave is curved to match the curve of the cylinder, and serves to hold the grain as it is beaten. The beating releases the grain from the straw and chaff.
Whilst the majority of the grain falls through the concave, the straw is carried by a set of "walkers" to the rear of the machine, allowing any grain and chaff still in the straw to fall below. Below the straw walkers, a fan blows a stream of air across the grain, removing dust and fines and blowing them away.
The grain, either coming through the concave or the walkers, meets a set of sieves mounted on an assembly called a shoe, which is shaken mechanically. The top sieve has larger openings, and serves to remove large pieces of chaff from the grain. The lower sieve separates clean grain, which falls through, from incompletely threshed pieces. The incompletely threshed grain is returned to the cylinder by means of a system of conveyors, where the process repeats.
Some threshing machines were equipped with a bagger, which invariably held two bags, one being filled, and the other being replaced with an empty. A worker called a "sewer" removed and replaced the bags, and sewed full bags shut with a needle and thread. Other threshing machines would discharge grain from a conveyor, for bagging by hand. Combines are equipped with a grain tank, which accumulates grain for deposit in a truck or wagon.
A large amount of chaff and straw would accumulate around a threshing machine, and several innovations, such as the air chaffer, were developed to deal with this. Combines generally chop and disperse straw as they move through the field, though the chopping is disabled when the straw is to be baled, and chaff collectors are sometimes used to prevent the dispersal of weed seed throughout a field.
The corn sheller was almost identical in design, with slight modifications to deal with the larger kernel size and presence of cobs. Modern-day combines can be adjusted to work with any grain crop, and many unusual seed crops.
Both the older and modern machines require a good deal of skill to operate. The concave clearance, cylinder speed, fan velocity, sieve sizes, and feeding rate must be adjusted for crop conditions.
Another development in Asia.
From the early 20th century, gasoline or diesel-powered threshing machines, designed especially to thresh rice, the most important crop in Asia, have been developed along different lines to the modern combine.
Even after the combine was invented and became popular, a new compact-size thresher called a "harvester", with wheels, still remains in use and at present it is available from a Japanese agricultural manufacturer. The compact-size machine is very convenient to handle in small terrace fields in mountain areas where a large machine, such as combine, is not usable.
People there use this harvester with a modern compact binder.
Preservation.
A number of older threshing machines have survived into preservation. They are often to be seen in operation at live steam festivals and traction engine rallies such as the Great Dorset Steam Fair in England, and the Western Minnesota Steam Threshers Reunion in northwest Minnesota.
Musical references.
Irish songwriter John Duggan immortalized the threshing machine in a song "The Old Thrashing Mill". The song has been recorded by Foster and Allen and Brendan Shine.
On the Alan Lomax collection Songs of Seduction (Rounder Select, 2000), there's a bawdy Irish folk song called "The Thrashing Machine" sung by tinker Annie O'Neil, as recorded in the early 20th Century.
In his film score for "Of Mice and Men" (1939) and consequently in his collection "Music for the Movies" (1942), American composer Aaron Copland titled a section of the score "Threshing Machines," to suit a scene in the Lewis Milestone film where Curley is threatening Slim over giving May a puppy, when many of the itinerant worker men are standing around or working on threshers.
Northampton, MA-based indie rock band Winterpills have a song on their 2005 debut album Winterpills called "Threshing Machine". It is not about agricultural machinery.
In the song Thrasher from the album Rust Never Sleeps, Neil Young compares the modern threshing machine's technique of separating wheat from wheat stalks to the natural forces of time that separate close friends from one another.

</doc>
<doc id="46195" url="http://en.wikipedia.org/wiki?curid=46195" title="Moss, Norway">
Moss, Norway

   is a coastal town and a municipality in Østfold county, Norway. The administrative centre of the municipality is the town of Moss. The city of Moss was established as a municipality on 1 January 1838 (see formannskapsdistrikt). The rural municipality of Jeløy was merged with the city on 1 July 1943.
Its administrative district covers areas east of the town, such as the island of Dillingøy in the lake Vansjø. Parts of the town are located on the peninsula of Jeløy. Moss city has 30,723 inhabitants (2012).
General information.
Name.
The Old Norse form of the name was "Mors". It may be derived from an old root "mer-" which means to "divide" or "split".
The adjacent topography shares similar etymology:
Coat-of-arms.
The coat-of-arms is from modern times. They were granted on 2 April 1954. Moss became a separate city in 1786 and received its first seal in the same year. The seal showed a church under some clouds, the whole thing placed within a circle. Above the circle there were some fasces, the freedom symbol of the late 19th century. A later seal, dating from around 1829, shows the same composition, but now also with six birds flying around the church.
When in the 1930s the city wanted to adopt a coat-of-arms and the birds were chosen as a possible symbol. The original birds probably were doves, symbol of peace. In 1934, the idea of the crow was launched, since the nickname of the inhabitants was 'crows'. The arms were finally granted in 1954 and show a yellow crow on a red background. It was designed by Christian Stenersen.
There is a tale being told in Moss about the Church fire: The city of Moss always had a lot of crows, most likely because of the corn being harvested in the region. The fire disturbed the crows that started to make a lot of noise and the inhabitants rescued the church from total destruction. After this episode the idea of crow as arms was launched.
History.
Archeological finds suggest that there were settlements in the area more than 7,000 years ago and continuously through the Iron Age, Viking Age, through to modern times. During the Viking era, the place was known as "Varna" (forne, vorne, front-protection?) and was the site of a cooperative for battleships held by local warlords on behalf of the king.
The first literary reference to the name Mo(u)ſs(ß) is from Bishop Eystein Aslaksson's Red book (NRA AM fol. 328) from 1396, and by then the town had become a commercial center with craftsmen and mills. By the 16th century, the town's port was significant enough to warrant its own customs official. Liquor distilleries became one of the dominant industries, and it was not until 1607, after the Reformation, that the town got its own church.
By 1700, Moss had become a hub for both ship and land traffic between Copenhagen and Christiania, and in 1704 Moss Jernverk (Moss Ironworks) was established just north of the city center. By 1720 it received its charter as a merchant town, with its own official. This may have had background in an important battle in 1716 that was fought in the town square in Moss in which Norwegian troops commanded by Vincent Budde prevailed over invading Swedish forces, sent by Charles XII to capture Akershus Fortress. In 1767 a local resident built a "pleasure pavilion" near the town, which survives as the Hotel Refsnes Gods.
In 1814, Moss became the site for the signing of the Convention of Moss, which effectively put an end to the Dano-Norwegian kingdom. This set the stage for economic development that has persisted to this day.
On the morning of 14 July 2006, a bolide exploded above the nearby town of Rygge - moments later, several stony meteorites fell over Moss. A number of meteorites were recovered by local residents and visiting meteorite hunters, which after analysis and classification, were found to be a rare type of carbonaceous chondrite.
Norwegian lady statues.
Moss and Virginia Beach, Virginia in the United States are sister cities. On Good Friday, 27 March 1891, the Norwegian bark "Dictator", whose home port was Moss, was lost in the treacherous waters of the Graveyard of the Atlantic. The ship had been en route to England from Pensacola, Florida with a cargo of Georgia Pine lumber. After being caught and disabled in a storm, she was headed for port at Hampton Roads, Virginia to make repairs when she encountered another storm just off Virginia Beach.
Working in the high winds and seas, lifesaving crews from shore were able to save some of the 17 persons aboard. However, the pregnant wife of Captain J.M. Jorgensen, Johanne, and their 4 year-old son Carl were among the 7 persons who drowned.
The ship's wooden female figurehead had washed ashore. It was placed in a vertical position facing the ocean near the boardwalk as a memorial to those who lost their lives in the shipwreck. It was a landmark there for more than 60 years, but gradually became weathered and eroded.
In 1962, Norwegian sculptor Ørnulf Bast was commissioned to create two nine-foot bronze replicas of the original figurehead by the City of Moss. The Norwegian Lady Statues were unveiled on 22 September 1962. One was presented as a gift to Virginia Beach, and an exact duplicate was erected in Moss to unite the two sister cities. Each statue gives the appearance of facing the other across the Atlantic Ocean.
On 13 October 1995, Queen Sonja of Norway visited the Norwegian Lady statue in Virginia Beach, and placed memorial flowers.
Industry.
The town is known for paper mills, as well as metalworks and other factories. Dillingøy is known as a place for alternative non-military civil service. Moss is mentioned since the Renaissance and was the site of the signing of the Convention of Moss in 1814, which solidified the union with Sweden. The headquarters of textile producer Helly Hansen were located in Moss until 2009. The maker of international hotel keycards, Trio Ving, also has their headquarters here.
Transport.
Moss is served by Moss Airport, Rygge, which is located in the neighboring municipality of Rygge. It opened as a civilian airport in 2007 and is served predominantly by low-cost airlines, particularly Ryanair. The railway Østfold Line runs through Moss, stopping at Moss Station, which is the southern terminus of one service of the Oslo Commuter Rail and an intermediate stop for regional trains. Moss connects across the Oslofjord to Horten via the Moss–Horten Ferry. There are also bus-lines to Oslo Airport, Gardermoen, Gothenbourg, Copenhagen, Oslo in addition to local bus lines. Moss port is one of the top 3 busiest container ports in Norway (messured in TEU) .
International relations.
Twin towns — Sister cities.
The following cities are twinned with Moss:

</doc>
<doc id="46196" url="http://en.wikipedia.org/wiki?curid=46196" title="Marl">
Marl

Marl or marlstone is a calcium carbonate or lime-rich mud or mudstone which contains variable amounts of clays and silt. The dominant carbonate mineral in most marls is calcite, but other carbonate minerals such as aragonite, dolomite, and siderite may be present. Marl was originally an old term loosely applied to a variety of materials, most of which occur as loose, earthy deposits consisting chiefly of an intimate mixture of clay and calcium carbonate, formed under freshwater conditions; specifically an earthy substance containing 35–65% clay and 65–35% carbonate. It also describes a habit of coralline red alga. The term is today often used to describe indurated marine deposits and lacustrine (lake) sediments which more accurately should be named 'marlstone'. Marlstone is an indurated rock of about the same composition as marl, more correctly called an earthy or impure argillaceous limestone. It has a blocky subconchoidal fracture, and is less fissile than shale. The term 'marl' is widely used in English-language geology, while the terms "Mergel" and "Seekreide" (German for "lake chalk") are used in European references.
The lower stratigraphic units of the chalk cliffs of Dover consist of a sequence of glauconitic marls followed by rhythmically banded limestone and marl layers. Upper Cretaceous cyclic sequences in Germany and marl–opal-rich Tortonian-Messinian strata in the Sorbas basin related to multiple sea drawdown have been correlated with Milankovitch orbital forcing.
Marl as lacustrine sediment is common in postglacial lake-bed sediments, often found underlying peat bogs. It has been used as a soil conditioner and acid soil neutralizing agent.
References.
Bibliography.
</dl>

</doc>
<doc id="46202" url="http://en.wikipedia.org/wiki?curid=46202" title="Pink noise">
Pink noise

Pink noise or 1⁄"f" noise (sometimes also called flicker noise) is a signal or process with a frequency spectrum such that the power spectral density (energy or power per Hz) is inversely proportional to the frequency of the signal. In pink noise, each octave (halving/doubling in frequency) carries an equal amount of noise power. The name arises from the pink appearance of visible light with this power spectrum.
Within the scientific literature the term pink noise is sometimes used a little more loosely to refer to any noise with a power spectral density of the form
where "f" is frequency and 0 < α < 2, with exponent α usually close to 1. These pink-like noises occur widely in nature and are a source of considerable interest in many fields. The distinction between the noises with α near 1 and those with a broad range of α approximately corresponds to a much more basic distinction. The former (narrow sense) generally come from condensed matter systems in quasi-equilibrium, as discussed below. The latter (broader sense) generally correspond to a wide range of non-equilibrium driven dynamical systems.
The term "flicker noise" is sometimes used to refer to pink noise, although this is more properly applied only to its occurrence in electronic devices due to a direct current. Mandelbrot and Van Ness proposed the name "fractional noise" (sometimes since called "fractal noise") to emphasize that the exponent of the spectrum could take non-integer values and be closely related to fractional Brownian motion, but the term is very rarely used.
Description.
There is equal energy in all octaves (or similar log bundles) of frequency. In terms of power at a constant bandwidth, pink noise falls off at 3 dB per octave. At high enough frequencies pink noise is never dominant. (White noise is equal energy per hertz.)
The human auditory system, which processes frequencies in a roughly logarithmic fashion approximated by the Bark scale, does not perceive different frequencies with equal sensitivity; signals around 1–4 kHz sound loudest for a given intensity. However, humans still differentiate between white noise and pink noise with ease.
Graphic equalizers also divide signals into bands logarithmically and report power by octaves; audio engineers put pink noise through a system to test whether it has a flat frequency response in the spectrum of interest. Systems that do not have a flat response can be equalized by creating an inverse filter using a graphic equalizer. Because pink noise has a tendency to occur in natural physical systems it is often useful in audio production. Pink noise can be processed, filtered, and/or effects can be added to produce desired sounds. Pink noise generators are commercially available.
One parameter of noise, the peak versus average energy contents, or crest factor, is important for testing purposes, such as for audio power amplifier and loudspeaker capabilities because the signal power is a direct function of the crest factor. Various crest factors of pink noise can be used in simulations of various levels of dynamic range compression in music signals. On some digital pink noise generators the crest factor can be specified.
Generalization to more than one dimension.
The spectrum of pink noise is 1⁄"f" only for one-dimensional signals. For two-dimensional signals (e.g., images) the spectrum is reciprocal to "f" 2. In general, in an "n"-dimensional system, the spectrum is reciprocal to "f n". For higher-dimensional signals it is still true (by definition) that each octave carries an equal amount of noise power. The frequency spectrum of two-dimensional signals, for instance, is also two-dimensional, and the area covered by succeeding octaves is four times as large.
Occurrence.
In the past quarter century, pink noise has been discovered in the temporal fluctuations of an extraordinarily diverse number of physical and biological systems (Press, 1978; see articles in Handel & Chung, 1993, and references therein). Examples of its occurrence include fluctuations in tide and river heights, quasar light emissions, heart beat, firings of single neurons, and resistivity in solid state devices. The most accessible introduction to the significance of pink noise is one given by Martin Gardner (1978) in his Scientific American column "Mathematical Games". In this particular column, Gardner asked for the sense in which music imitates nature. Sounds in nature are not musical in that they tend to be either too repetitive (bird song, insect noises) or too chaotic (ocean surf, wind in trees, and so forth). The answer to this question was given in a statistical sense by Voss and Clarke (1975, 1978), who showed that pitch and loudness fluctuations in speech and music are pink noises. So music is like tides not in terms of how tides sound, but in how tide heights vary.
Because pink noise occurs in many physical, biological and economic systems, some researchers describe it as being ubiquitous. In physical systems, it is present in some meteorological data series, the electromagnetic radiation output of some astronomical bodies, and in almost all electronic devices (referred to as flicker noise). In biological systems, it is present in, for example, heart beat rhythms, neural activity, and the statistics of DNA sequences, as a generalized pattern.
In financial systems, it is often referred to as a "long-term memory effect". Also, it describes the statistical structure of many natural images (images from the natural environment). Recently, pink noise has also been successfully applied to the modeling of mental states in psychology, and used to explain stylistic variations in music from different cultures and historic periods. Richard F. Voss and J. Clarke claim that almost all musical melodies, when each successive note is plotted on a scale of pitches, will tend towards a pink noise spectrum. Similarly, a generally pink distribution pattern has been observed in film shot length by researcher James E. Cutting of Cornell University, in the study of 150 popular movies released from 1935 to 2005.
Pink noise has also been found to be endemic in human response. Gilden et al. (1995) found extremely pure examples of this noise in the time series formed upon iterated production of temporal and spatial intervals. Later, Gilden (1997) and Gilden (2001) found that time series formed from reaction time measurement and from iterated two-alternative forced choice also produced pink noises.
Although self-organised criticality has been able to reproduce pink noise in sandpile models, there are no simple mathematical models to create pink noise. It is usually generated by filtering white noise.
There are many theories of the origin of pink noise. Some theories attempt to be universal, while others are applicable to only a certain type of material, such as semiconductors. Universal theories of pink noise remain a matter of current research interest.
A hypothesis (referred to as the Tweedie hypothesis) has been proposed to explain the genesis of pink noise on the basis of a mathematical convergence theorem related to the central limit theorem of statistics. The Tweedie convergence theorem describes the convergence of certain statistical processes towards a family of statistical models known as the Tweedie distributions. These distributions are characterized by a variance to mean power law, that have been variously identified in the ecological literature as Taylor's law and in the physics literature as "fluctuation scaling". When this variance to mean power law is demonstrated by the method of expanding enumerative bins this implies the presence of pink noise, and vice versa. Both of these effects can be shown to be the consequence of mathematical convergence such as how certain kinds of data will converge towards the normal distribution under the central limit theorem. This hypothesis also provides for an alternative paradigm to explain power law manifestations that have been attributed to self-organized criticality.
Electronic devices.
A pioneering researcher in this field was Aldert van der Ziel.
In electronics, white noise will be stronger than pink noise (flicker noise) above some corner frequency. There is no known lower bound to pink noise in electronics. Measurements made down to 10−6 Hz (taking several weeks) have not shown a ceasing of pink-noise behaviour.
A pink noise source is sometimes included on analog synthesizers (although a white noise source is more common), both as a useful audio sound source for further processing, and also as a source of random control voltages for controlling other parts of the synthesizer.
The principal sources of pink noise in electronic devices are almost invariably the slow fluctuations of properties of the condensed-matter materials of the devices. In many cases the specific sources of the fluctuations are known. These include fluctuating configurations of defects in metals, fluctuating occupancies of traps in semiconductors, and fluctuating domain structures in magnetic materials. The explanation for the approximately pink spectral form turns out to be relatively trivial, usually coming from a distribution of kinetic activation energies of the fluctuating processes. Since the frequency range of the typical noise experiment (e.g., 1 Hz — 1 kHz) is low compared with typical microscopic "attempt frequencies" (e.g., 1014 Hz), the exponential factors in the Arrhenius equation for the rates are large. Relatively small spreads in the activation energies appearing in these exponents then result in large spreads of characteristic rates. In the simplest toy case, a flat distribution of activation energies gives exactly a pink spectrum, because formula_2

</doc>
