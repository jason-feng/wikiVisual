<doc id="51335" url="http://en.wikipedia.org/wiki?curid=51335" title="Amartya Sen">
Amartya Sen

Amartya Kumar Sen (born 3 November 1933) is an Indian economist and philosopher, who since 1972 has taught and worked in the United Kingdom and the United States. He has made contributions to welfare economics, social choice theory, economic and social justice, economic theories of famines, and indexes of the measure of well-being of citizens of developing countries. He was awarded the Nobel Memorial Prize in Economic Sciences in 1998 and Bharat Ratna in 1999 for his work in welfare economics. He was also awarded the inaugural Charleston-EFG John Maynard Keynes Prize in recognition of his work on welfare economics in February 2015 during a reception at the Royal Academy in the UK.
He is currently the Thomas W. Lamont University Professor and Professor of Economics and Philosophy at Harvard University. He served as the chancellor of Nalanda University. He is also a senior fellow at the Harvard Society of Fellows, a distinguished fellow of All Souls College, Oxford, an honorary fellow of Darwin College, Cambridge and a Fellow of Trinity College, Cambridge, where he served as Master from 1998 to 2004. He is also known for being one of the strongest champions of rationalism, secularism, and egalitarianism in India , and has condemned the ghettoization of Ambedkar as a Dalit leader.
Early life and education.
Sen was born in a Bengali family in Manikganj, Bangladesh, to Ashutosh Sen and Amita Sen. Rabindranath Tagore gave Amartya Sen his name (Bengali অমর্ত্য "ômorto", lit. "immortal"). Sen's family was from Wari and Manikganj, Dhaka, both in present-day Bangladesh. His father Ashutosh Sen was a professor of chemistry at Dhaka University who moved with his family to West Bengal in 1945 and worked at various government institutions, including the West Bengal Public Service Commission (of which he was the chairman), and the Union Public Service Commission. Sen's mother Amita Sen was the daughter of Kshiti Mohan Sen, a well-known scholar of ancient and medieval India and close associate of Rabindranath Tagore. He served as the Vice Chancellor of Visva-Bharati University for some years.
Sen began his high-school education at St Gregory's School in Dhaka in 1940. From fall 1941, Sen studied at Visva-Bharati University school. He stood first in I.Sc.Examination. He later went to Presidency College, Kolkata, where he earned a B.A. in Economics with first class, with a minor in Mathematics, as a graduating student of the University of Calcutta. In 1953, he moved to Trinity College, Cambridge, where he earned a second B.A. in Economics in 1955 with a first class topping the list. He was elected President of the Cambridge Majlis. While Sen was officially a Ph.D. student at Cambridge (though he had finished his research in 1955-6), he was offered the position of Professor and Head of the Economics Department of the newly created Jadavpur University in Calcutta, and he became the youngest chairman to head the Department of Economics. He served in that position, starting the new Economics Department, during 1956 to 1958.
Meanwhile, Sen was elected to a Prize Fellowship at Trinity College, which gave him four years of freedom to do anything he liked; he made the radical decision to study philosophy. Sen explained: "The broadening of my studies into philosophy was important for me not just because some of my main areas of interest in economics relate quite closely to philosophical disciplines (for example, social choice theory makes intense use of mathematical logic and also draws on moral philosophy, and so does the study of inequality and deprivation), but also because I found philosophical studies very rewarding on their own". His interest in philosophy, however, dates back to his college days at Presidency, where he read books on philosophy and debated philosophical themes.
In Cambridge, there were major debates between supporters of Keynesian economics on the one hand, and the "neo-classical" economists skeptical of Keynes, on the other. However, because of a lack of enthusiasm for social choice theory in both Trinity and Cambridge, Sen had to choose a different subject for his Ph.D. thesis, which was on "The Choice of Techniques" in 1959, though the work had been completed much earlier (except for some valuable advice from his adjunct supervisor in India, Professor A.K. Dasgupta, given to Sen while teaching and revising his work at Jadavpur) under the supervision of the "brilliant but vigorously intolerant" post-Keynesian, Joan Robinson. Quentin Skinner notes that Sen was a member of the secret society Cambridge Apostles during his time at Cambridge.
Research.
Sen's papers in the late 1960s and early 1970s helped develop the theory of social choice, which first came to prominence in the work by the American economist Kenneth Arrow, who, while working at the RAND Corporation, had most famously shown that all voting rules, be they majority rule or two thirds-majority or status quo, must inevitably conflict with some basic democratic norm. Sen's contribution to the literature was to show under what conditions Arrow's impossibility theorem would indeed come to pass as well as to extend and enrich the theory of social choice, informed by his interests in history of economic thought and philosophy.
In 1981, Sen published "Poverty and Famines: An Essay on Entitlement and Deprivation" (1981), a book in which he argued that famine occurs not only from a lack of food, but from inequalities built into mechanisms for distributing food. Sen also argued that the Bengal famine was caused by an urban economic boom that raised food prices, thereby causing millions of rural workers to starve to death when their wages did not keep up.
Sen's interest in famine stemmed from personal experience. As a nine-year-old boy, he witnessed the Bengal famine of 1943, in which three million people perished. This staggering loss of life was unnecessary, Sen later concluded. He presents data that there was an adequate food supply in Bengal at the time, but particular groups of people including rural landless labourers and urban service providers like haircutters did not have the means to buy food as its price rose rapidly due to factors that include British military acquisition, panic buying, hoarding, and price gouging, all connected to the war in the region. In "Poverty and Famines", Sen revealed that in many cases of famine, food supplies were not significantly reduced. In Bengal, for example, food production, while down on the previous year, was higher than in previous non-famine years. Thus, Sen points to a number of social and economic factors, such as declining wages, unemployment, rising food prices, and poor food-distribution, which led to starvation. His capabilities approach focuses on positive freedom, a person's actual ability to be or do something, rather than on negative freedom approaches, which are common in economics and simply focuses on non-interference. In the Bengal famine, rural laborers' negative freedom to buy food was not affected. However, they still starved because they were not positively free to do anything, they did not have the functioning of nourishment, nor the capability to escape morbidity.
In addition to his important work on the causes of famines, Sen's work in the field of development economics has had considerable influence in the formulation of the "Human Development Report", published by the United Nations Development Programme. This annual publication that ranks countries on a variety of economic and social indicators owes much to the contributions by Sen among other social choice theorists in the area of economic measurement of poverty and inequality.
Sen's revolutionary contribution to development economics and social indicators is the concept of "capability" developed in his article "Equality of What". He argues that governments should be measured against the concrete capabilities of their citizens. This is because top-down development will always trump human rights as long as the definition of terms remains in doubt (is a "right" something that must be provided or something that simply cannot be taken away?). For instance, in the United States citizens have a hypothetical "right" to vote. To Sen, this concept is fairly empty. In order for citizens to have a capacity to vote, they first must have "functionings". These "functionings" can range from the very broad, such as the availability of education, to the very specific, such as transportation to the polls. Only when such barriers are removed can the citizen truly be said to act out of personal choice. It is up to the individual society to make the list of minimum capabilities guaranteed by that society. For an example of the "capabilities approach" in practice, see Martha Nussbaum's "Women and Human Development".
He wrote a controversial article in "The New York Review of Books" entitled "More Than 100 Million Women Are Missing" (see Missing women of Asia), analyzing the mortality impact of unequal rights between the genders in the developing world, particularly Asia. Other studies, including one by Emily Oster, had argued that this is an overestimation, though Oster has since then recanted her conclusions.
Welfare economics seeks to evaluate economic policies in terms of their effects on the well-being of the community. Sen, who devoted his career to such issues, was called the "conscience of his profession". His influential monograph "Collective Choice and Social Welfare" (1970), which addressed problems related to individual rights (including formulation of the liberal paradox), justice and equity, majority rule, and the availability of information about individual conditions, inspired researchers to turn their attention to issues of basic welfare. Sen devised methods of measuring poverty that yielded useful information for improving economic conditions for the poor. For instance, his theoretical work on inequality provided an explanation for why there are fewer women than men in India and China despite the fact that in the West and in poor but medically unbiased countries, women have lower mortality rates at all ages, live longer, and make a slight majority of the population. Sen claimed that this skewed ratio results from the better health treatment and childhood opportunities afforded boys in those countries, as well as sex-selective abortions.
Governments and international organizations handling food crises were influenced by Sen's work. His views encouraged policy makers to pay attention not only to alleviating immediate suffering but also to finding ways to replace the lost income of the poor—for example through public works—and to maintain stable prices for food. A vigorous defender of political freedom, Sen believed that famines do not occur in functioning democracies because their leaders must be more responsive to the demands of the citizens. In order for economic growth to be achieved, he argued, social reforms—such as improvements in education and public health—must precede economic reform.
In 2009, Sen published a book called "The Idea of Justice". Based on his previous work in welfare economics and social choice theory, but also on his philosophical thoughts, he presented his own theory of justice that he meant to be an alternative to the influential modern theories of justice of John Rawls or John Harsanyi. In opposition to Rawls but also earlier justice theoreticians Immanuel Kant, Jean-Jacques Rousseau or David Hume, and inspired by the philosophical works of Adam Smith and Mary Wollstonecraft, Sen developed a theory that is both comparative and realizations-oriented (instead of being transcendental and institutional). However, he still regards institutions and processes as being important. As an alternative to Rawls's veil of ignorance, Sen chose the thought experiment of an impartial spectator as the basis of his theory of justice. He also stressed the importance of public discussion (understanding democracy in the sense of John Stuart Mill) and a focus on people's capabilities (an approach that he had co-developed), including the notion of universal human rights, in evaluating various states with regard to justice.
Professional career.
Sen began his career both as a teacher and a research scholar in the Department of Economics, Jadavpur University. Between 1960 and 1961, Sen was a visiting Professor at Massachusetts Institute of Technology in the United States, where he got to know Paul Samuelson, Robert Solow, Franco Modigliani, and Norbert Wiener. He was also a visiting Professor at UC-Berkeley and Cornell. He taught as Professor of Economics between 1963 and 1971 at the Delhi School of Economics (where he completed his "magnum opus" Collective Choice and Social Welfare by 1969). This is a period considered to be a Golden Period in the history of DSE. In 1972, he joined the London School of Economics as a Professor of Economics where he taught until 1977. From 1977 to 1986 he taught at the University of Oxford, where he was first a Professor of Economics and Fellow of Nuffield College, and then the Drummond Professor of Political Economy and a Fellow of All Souls College from 1980. In 1987, he joined Harvard as the Thomas W. Lamont University Professor of Economics. In 1998 he was appointed as Master of Trinity College, Cambridge. In January 2004, Sen returned to Harvard. He also established the Eva Colorni Trust at the former London Guildhall University in the name of his deceased wife.
Nalanda Project.
In May 2007, he was appointed as chairman of Nalanda Mentor Group to examine the framework of international cooperation, and proposed structure of partnership, which would govern the establishment of Nalanda International University Project as an international centre of education seeking to revive the ancient center of higher learning which was present in India from the 5th century to 1197.
On 19 July 2012, Sen was named the first chancellor of the proposed Nalanda University (NU). Teaching began in August 2014. On 20 February 2015, Amartya Sen withdrew his candidature for a second term.
Membership and associations.
He has served as president of the Econometric Society (1984), the International Economic Association (1986–1989), the Indian Economic Association (1989) and the American Economic Association (1994). He has also served as President of the Development Studies Association and the Human Development and Capabilities Association. He serves as the Chair of the International Advisory Board of the Center for Human and Economic Development Studies at Peking University in China.
Sen has been called "the Conscience of the profession" and "the Mother Teresa of Economics" for his work on famine, human development theory, welfare economics, the underlying mechanisms of poverty, gender inequality, and political liberalism. However, he denies the comparison to Mother Teresa, saying that he has never tried to follow a lifestyle of dedicated self-sacrifice. Amartya Sen also added his voice to the campaign against the anti-gay Section 377 of the Indian Penal Code.
Media and culture.
A 57-minute documentary named "Amartya Sen: A Life Re-examined" directed by Suman Ghosh details his life and work.
A 2001 portrait of Sen by Annabel Cullen is in Trinity College's collection. A 2003 portrait of Sen hangs in the National Portrait Gallery in London.
In 2011, he was present at the "Rabindra Utsab" ceremony at Bangabandhu International Conference Centre (BICC), Bangladesh. He unveiled the cover of Sruti Gitobitan, a Rabindrasangeet album comprising all the 2222 Tagore songs, brought out by Rezwana Chowdhury Bannya, principal of Shurer Dhara School of Music.
Controversies.
Amartya Sen was critical of Indian Prime Minister Narendra Modi when he was announced as the prime ministerial candidate by the BJP. In April 2014, he said that Modi would not make a good Prime Minister. But later in December 2014, he changed his views and said that Narendra Modi did give people a sense of faith that things can happen. In February 2015, Sen opted out of seeking a second term for the chancellor post of Nalanda university stating that the Government of India was not keen on him continuing in the post.
Personal life and beliefs.
Sen has been married three times. His first wife was Nabaneeta Dev Sen, an Indian writer and scholar, by whom he had two daughters: Antara, a journalist and publisher, and Nandana, a Bollywood actress. Their marriage broke up shortly after they moved to London in 1971. In 1978 Sen married Eva Colorni, an Italian economist, and the couple had two children, a daughter Indrani, who is a journalist in New York, and a son Kabir, a hip hop artist, MC, and music teacher at Shady Hill School. Eva died cancer in 1985. In 1991, Sen married Emma Georgina Rothschild, who serves as the Jeremy and Jane Knowles Professor of History at Harvard University.
The Sens have a house in Cambridge, Massachusetts, which is the base from which they teach during the academic year. They also have a home in Cambridge, England, where Sen is a Fellow of Trinity College, Cambridge, and Rothschild is a Fellow of Magdalene College. He usually spends his winter holidays at his home in Santiniketan in West Bengal, India, where he used to go on long bike rides until recently. Asked how he relaxes, he replies: "I read a lot and like arguing with people."
Sen is an atheist and holds that this can be associated with Hinduism of the atheist schools, like Lokayata. In an interview for the magazine "California", which is published by the University of California, Berkeley, he noted:
Awards and honours.
Sen has received over 90 honorary degrees from universities around the world.

</doc>
<doc id="51336" url="http://en.wikipedia.org/wiki?curid=51336" title="Barbuda">
Barbuda

Barbuda is an island in the Eastern Caribbean, and forms part of the state of Antigua and Barbuda. It has a population of about 1,638 (at the 2011 Census), most of whom live in the town of Codrington.
Location.
Barbuda is located north of Antigua, in the middle of the Leeward Islands. To the south are the islands of Montserrat and Guadeloupe, and to the west and north west are Nevis, St. Kitts, St. Barts, and St. Martin.
History.
The Ciboney were the first to inhabit the island of Barbuda in 2400 BC, but Arawak and Carib Indians populated the island when Christopher Columbus landed on his second voyage in 1493. Early settlements by the Spanish and French were succeeded by the English, who formed a colony in 1666.
In 1685 Barbuda was leased to brothers Christopher and John Codrington, who had founded the town of Codrington. The Codrington family produced food on their land in Barbuda, and also transported slaves as labour for their sugar plantations on Antigua. There was more than one slave rebellion at Codrington during the 1740s, during which slaves rose against managers. All the slaves were freed in 1834.
Barbuda was for a time used by the Codringtons as a "nursery" for slaves.
In 1719, Codrington and the island of Barbuda had its first census (of both people and livestock), conducted by Sir William Codrington (1715–1790).
The first map of Barbuda was made in the second half of the eighteenth century. At that time there were substantial buildings in the Highland area, a castle in Codrington, a fort at River, now known as the Martello Tower, and houses at Palmetto Point, Coco Point, and Castle Hill. The map shows eight catching pens for holding captured runaway slaves, indicating that this was a serious problem. There were several defensive cannon gun battery units around the island perimeter. There was a large plantation in the Meadow and Guava area and another large plantation in the Highlands area.
On November 1, 1981, the island gained its independence as an integral part of Antigua and Barbuda, a member of the Commonwealth of Nations. In a 1989 election the Barbuda Independence Movement received too few votes to qualify for a seat in the national parliament.
Points of interest.
Barbuda is home to the Frigate Bird Sanctuary, which is located in the Codrington Lagoon. Other points of interest include Highland House (the 18th century home of the Codrington family) and the Indian Cave, which contains ancient Amerindian petroglyphs.
Tourism.
Barbuda's climate and geography is conducive to tourism. Many tourists are attracted by the island's beaches. Activities include a bird sanctuary, swimming, snorkeling, fishing, and caving. Only two operating resorts are located on the island, the rest are abandoned after poor management, difficult infrastructure and hurricane damage.
Geography.
The total land area is 160.56 km². The capital and largest city is Codrington, with an estimated population of 1,000. The island is mostly coral limestone with little topographical variation. The highest point is only 38m above sea level.
Climate.
The climate is classified as tropical marine which means that there is little seasonal temperature variation. In January and February, the coolest months, the average daily high temperature is 27 °C. While in July and August, the warmest months, the average daily high is 30 °C.

</doc>
<doc id="51341" url="http://en.wikipedia.org/wiki?curid=51341" title="560s BC">
560s BC


</doc>
<doc id="51342" url="http://en.wikipedia.org/wiki?curid=51342" title="570s BC">
570s BC


</doc>
<doc id="51343" url="http://en.wikipedia.org/wiki?curid=51343" title="590s BC">
590s BC


</doc>
<doc id="51344" url="http://en.wikipedia.org/wiki?curid=51344" title="600s BC (decade)">
600s BC (decade)


</doc>
<doc id="51345" url="http://en.wikipedia.org/wiki?curid=51345" title="610s BC">
610s BC


</doc>
<doc id="51346" url="http://en.wikipedia.org/wiki?curid=51346" title="Coconut">
Coconut

The coconut tree (Cocos nucifera) is a member of the family Arecaceae (palm family). It is the only accepted species in the genus Cocos. The term coconut can refer to the entire coconut palm, the seed, or the fruit, which, botanically, is a drupe, not a nut. The spelling cocoanut is an archaic form of the word. The term is derived from the 16th-century Portuguese and Spanish word "coco" meaning "head" or "skull", from the three indentations on the coconut shell that resemble facial features.
The coconut is known for its great versatility as seen in the many uses of its different parts and found throughout the tropics and subtropics. Coconuts are part of the daily diets of many people. Coconuts are different from any other fruits because they contain a large quantity of "water" and when immature they are known as tender-nuts or jelly-nuts and may be harvested for drinking. When mature, they still contain some water and can be used as seednuts or processed to give oil from the kernel, charcoal from the hard shell and coir from the fibrous husk. The endosperm is initially in its nuclear phase suspended within the coconut water. As development continues, cellular layers of endosperm deposit along the walls of the coconut, becoming the edible coconut "flesh". When dried, the coconut flesh is called copra. The oil and milk derived from it are commonly used in cooking and frying; coconut oil is also widely used in soaps and cosmetics. The clear liquid coconut water within is potable. The husks and leaves can be used as material to make a variety of products for furnishing and decorating. It also has cultural and religious significance in many societies that use it.
Description.
Plant.
"Cocos nucifera" is a large palm, growing up to 30 m tall, with pinnate leaves 4 - long, and pinnae 60–90 cm long; old leaves break away cleanly, leaving the trunk smooth. Coconuts are generally classified into two general types: tall and dwarf. On very fertile land, a tall coconut palm tree can yield up to 75 fruits per year, but more often yields less than 30, mainly due to poor cultural practices. Given proper care and growing conditions coconut palms produce their first fruit in six to ten years, it takes 15 – 20 years to reach peak production.
Fruit.
Botanically, the coconut fruit is a drupe, not a true nut. Like other fruits, it has three layers: the exocarp, mesocarp, and endocarp. The exocarp and mesocarp make up the "husk" of the coconut. Coconuts sold in the shops of nontropical countries often have had the exocarp (outermost layer) removed. The mesocarp is composed of a fiber, called coir, which has many traditional and commercial uses. The shell has three germination pores (stoma) or "eyes" that are clearly visible on its outside surface once the husk is removed.
A full-sized coconut weighs about 1.44 kg. It takes around 6,000 full-grown coconuts to produce a tonne of copra.
Roots.
Unlike some other plants, the palm tree has neither a tap root nor root hairs, but has a fibrous root system.
The coconut palm root system consists of an abundance of thin roots that grow outward from the plant near the surface. Only a few of the roots penetrate deep into the soil for stability. The type of root system is known as fibrous or adventitious, and is a characteristic of grass species. Other types of large trees produce a single downward-growing tap root with a number of feeder roots growing from it.
Coconut palms continue to produce roots from the base of the stem throughout its life. The number of roots produced depends on the age of the tree and the environment, with more than 3,600 roots possible on a tree that's 60 to 70 years old.
Roots are usually less than about 3 inches in diameter and uniformly thick from the tree trunk to the root tip.
Inflorescence.
The palm produces both the female and male flowers on the same inflorescence; thus, the palm is monoecious. Other sources use the term polygamomonoecious. The female flower is much larger than the male flower. Flowering occurs continuously. Coconut palms are believed to be largely cross-pollinated, although some dwarf varieties are self-pollinating.
Etymology.
One of the earliest mentions of the coconut dates back to the One Thousand and One Nights story of Sinbad the Sailor; he is known to have bought and sold coconuts during his fifth voyage. "Tenga", its Malayalam and Tamil name, was used in the detailed description of coconut found in "Itinerario" by Ludovico di Varthema published in 1510 and also in the later "Hortus Indicus Malabaricus". Even earlier, it was called "nux indica", a name used by Marco Polo in 1280 while in Sumatra, taken from the Arabs who called it جوز هندي "jawz hindī". Both names translate to "Indian nut". In the earliest description of the coconut palm known, given by Cosmos of Alexandria in his "Topographia Christiana" written about 545 AD, there is a reference to the argell tree and its drupe.
Historical evidence favors the European origin of the name "coconut", for no name is similar in any of the languages of India, where the Portuguese first found the fruit; and indeed Barbosa, Barros, and Garcia, in mentioning the Tamil/Malayalam name "tenga", and Canarese "narle", expressly say, "we call these fruits "quoquos"", "our people have given it the name of coco", and "that which we call coco, and the Malabars temga".
The OED states: "Portuguese and Spanish authors of the 16th c. agree in identifying the word with Portuguese and Spanish "coco" "grinning face, grin, grimace", also "bugbear, scarecrow", cognate with "cocar" "to grin, make a grimace"; the name being said to refer to the face-like appearance of the base of the shell, with its three holes. According to Losada, the name came from Portuguese explorers, the sailors of Vasco da Gama in India, who first brought them to Europe. The coconut shell reminded them of a ghost or witch in Portuguese folklore called "coco" (also "côca"). The first known recorded usage of the term is 1555.
The specific name "nucifera" is Latin for "nut-bearing".
Origin, domestication, and dispersal.
Origin.
The origin of the plant is the subject of debate.
O.F. Cook was one of the earliest modern researchers to draw conclusions about the location of origin of "Cocos nucifera" based on its current-day worldwide distribution. He hypothesized that the coconut originated in the Americas, based on his belief that American coconut populations predated European contact and because he considered pan-tropical distribution by ocean currents improbable. Thor Heyerdahl later used this hypothesis of the American origin of the coconut to support his theory that the Pacific Islanders originated in South America. However, more evidence exists for an Indo-Pacific origin either around Melanesia and Malesia or the Indian Ocean. The oldest fossils known of the modern coconut dating from the Eocene period from around 37 to 55 million years ago were found in Australia and India. However, older palm fossils such as some of nipa fruit have been found in the Americas. Since 1978, the work on tracing the probable origin and dispersal of "Cocos nucifera" has only recently been augmented by a publication on the germination rate of the coconut seednut and another on the importance of the coral atoll ecosystem. Briefly, the coconut originated in the coral atoll ecosystem - without human intervention - and required a thick husk and slow germination to survive and disperse.
Domestication.
Coconuts could not reach inland locations without human intervention (to carry seednuts, plant seedlings, etc.) and it was early germination on the palm (vivipary) that was important, rather than increasing the number or size of the edible parts of a fruit that was already large enough. Human cultivation of the coconut selected, not for larger size, but for thinner husks and increased volume of endosperm, the solid “meat” or liquid “water” that provides the fruit its food value. Although these modifications for domestication would reduce the fruit’s ability to float, this ability would be irrelevant to a cultivated population.
Among modern C. nucifera, two major types or variants: a thick-husked, angular fruit and a thin-husked, spherical fruit with a higher proportion of endosperm reflect a trend of cultivation in "C. nucifera": the first coconuts were of the "niu kafa" type, with thick husks to protect the seed, an angular, highly ridged shape to promote buoyancy during ocean dispersal, and a pointed base that allowed fruits to dig into the sand, preventing them from being washed away during germination on a new island. As early human communities began to harvest coconuts for eating and planting, they (perhaps unintentionally) selected for a larger endosperm to husk ratio and a broader, spherical base, which rendered the fruit useful as a cup or bowl, thus creating the "niu vai" type. The decreased buoyancy and increased fragility of this spherical, thin-husked fruit would not matter for a species that had started to be dispersed by humans and grown in plantations. Harries’ adoption of the Polynesian terms "niu kafa" and "niu vai" has now passed into general scientific discourse, and his hypothesis is generally accepted.
Variants of "C. nucifera" are also categorized as Tall (var. "typical") or Dwarf (var. "nana"). The two groups are genetically distinct, with the Dwarf variety showing a greater degree of artificial selection for ornamental traits and for early germination and fruiting. The Tall variety is outcrossing while Dwarf palms are incrossing, which has led to a much greater degree of genetic diversity within the Tall group. It is believed that the Dwarf subgroup mutated from the Tall group under human selection pressure.
Dispersal.
It is often cited that coconuts can travel 110 days, or 3000 mile, by sea and still be able to germinate. This figure has been questioned based on the extremely small sample size of the paper that makes this claim. Thor Heyerdahl provides an alternative, and much shorter, estimate based on his first-hand experience crossing the Pacific Ocean on the raft Kon-Tiki: “The nuts we had in baskets on deck remained edible and capable of germinating the whole way to Polynesia. But we had laid about half among the special provisions below deck, with the waves washing around them. Every single one of these was ruined by the sea water. And no coconut can float over the sea faster than a balsa raft moves with the wind behind it." He also notes that several of the nuts began to germinate by the time they had been ten weeks at sea, precluding an unassisted journey of 100 days or more. However, it is more than likely that the coconut variety Heyerdahl chose for his long sea voyage was of the large, fleshy, spherical "niu vai" type, which Harries observed to have a significantly shorter germination type and worse buoyancy than the uncultivated "niu kafa" type. Therefore Heyerdahl’s observations cannot be considered conclusive when it comes to determining the independent dispersal ability of the uncultivated coconut.
Drift models based on wind and ocean currents have shown that coconuts could not have drifted across the Pacific unaided. This provides some circumstantial evidence that Austronesian peoples carried coconuts across the ocean and that they could not have dispersed worldwide without human agency. More recently, genomic analysis of cultivated coconut ("Cocos nucifera L.") has shed light on the movements of Austronesian peoples. By examining 10 microsatelite loci, researchers found two genetically distinct subpopulations of coconut—one originating in the Indian Ocean, the other in the Pacific Ocean. However, admixture, the transfer of genetic material, evidently occurred between the two populations. Given that coconuts are ideally suited for ocean dispersal, individuals from one population possibly could have floated to the other. However, the locations of the admixture events are limited to Madagascar and coastal east Africa, and exclude the Seychelles. This pattern coincides with the known trade routes of Austronesian sailors. Additionally, a genetically distinct subpopulation of coconut on the Pacific coast of Latin America has undergone a genetic bottleneck resulting from a founder effect; however, its ancestral population is the Pacific coconut. This, together with their use of the South American sweet potato, suggests that Austronesian peoples may have sailed as far east as the Americas.
Distribution.
The coconut has spread across much of the tropics, probably aided in many cases by seafaring people. Coconut fruit in the wild are light, buoyant and highly water resistant, and evolved to disperse significant distances via marine currents. Specimens have been collected from the sea as far north as Norway. In the Hawaiian Islands, the coconut is regarded as a Polynesian introduction, first brought to the islands by early Polynesian voyagers from their homelands in Oceania. They have been found in the Caribbean and the Atlantic coasts of Africa and South America for less than 500 years, but evidence of their presence on the Pacific coast of South America predates Christopher Columbus's arrival in the Americas. They are now almost ubiquitous between 26°N and 26°S except for the interiors of Africa and South America.
Natural habitat.
The coconut palm thrives on sandy soils and is highly tolerant of salinity. It prefers areas with abundant sunlight and regular rainfall (1500 mm to 2500 mm annually), which makes colonizing shorelines of the tropics relatively straightforward. Coconuts also need high humidity (70–80%+) for optimum growth, which is why they are rarely seen in areas with low humidity, like the southeastern Mediterranean or Andalusia (Spain), even where temperatures are high enough (regularly above 24 °C or 75.2 °F). However, they can be found in humid areas with low annual precipitation such as in Karachi, Pakistan, which receives only about 250 mm (9.84 in) of rainfall per year, but is consistently warm and humid.
Coconut palms require warm conditions for successful growth, and are intolerant of cold weather. Some seasonal variation is tolerated, with good growth where mean summer temperatures are between 28 and, and survival as long as winter temperatures are above 4 –; they will survive brief drops to 0 °C. Severe frost is usually fatal, although they have been known to recover from temperatures of -4 °C. They may grow but not fruit properly in areas with insufficient warmth, such as Bermuda.
The conditions required for coconut trees to grow without any care are:
The main limiting factor for most locations which satisfy the rainfall and temperature requirements is canopy growth, except those locations near coastlines, where the sandy soil and salt spray limit the growth of most other trees.
Diseases.
Coconuts are susceptible to the phytoplasma disease lethal yellowing. One recently selected cultivar, the Maypan, has been bred for resistance to this disease.
Pests.
The coconut palm is damaged by the larvae of many Lepidoptera (butterfly and moth) species which feed on it, including "Batrachedra" spp.: "B. arenosella", "B. atriloqua" (feeds exclusively on "C. nucifera"), "B. mathesoni" (feeds exclusively on "C. nucifera"), and "B. nuciferae".
"Brontispa longissima" (coconut leaf beetle) feeds on young leaves, and damages both seedlings and mature coconut palms. In 2007, the Philippines imposed a quarantine in Metro Manila and 26 provinces to stop the spread of the pest and protect the $800 million Philippine coconut industry.
The fruit may also be damaged by eriophyid coconut mites ("Eriophyes guerreronis"). This mite infests coconut plantations, and is devastating: it can destroy up to 90% of coconut production. The immature seeds are infested and desapped by larvae staying in the portion covered by the perianth of the immature seed; the seeds then drop off or survive deformed. Spraying with wettable sulfur 0.4% or with neem-based pesticides can give some relief, but is cumbersome and labor-intensive.
In Kerala (India), the main coconut pests are the coconut mite, the rhinoceros beetle, the red palm weevil and the coconut leaf caterpillar. Research on this topic has as of 2009[ [update]] produced no results, and researchers from the Kerala Agricultural University and the Central Plantation Crop Research Institute, Kasaragode are still searching for a cure. The Krishi Vigyan Kendra, Kannur under Kerala Agricultural University has developed an innovative extension approach called compact area group approach (CAGA) to combat coconut mites.
Cultivation.
Coconut palms are grown in more than 90 countries of the world, with a total production of 62 million tonnes per year. Coconut trees are very hard to establish in dry climates, and cannot grow there without frequent irrigation; in drought conditions, the new leaves do not open well, and older leaves may become desiccated; fruit also tends to be shed.
The extent of cultivation in the tropics is threatening a number of habitats, such as mangroves; an example of such damage to an ecoregion is in the Petenes mangroves of the Yucatán.
Harvesting.
In some parts of the world (Thailand and Malaysia), trained pig-tailed macaques are used to harvest coconuts. Training schools for pig-tailed macaques still exist both in southern Thailand, and in the Malaysian state of Kelantan. Competitions are held each year to find the fastest harvester.
India.
Traditional areas of coconut cultivation in India are the states of Kerala, Tamil Nadu, Karnataka, Puducherry, Andhra Pradesh, Goa, Maharashtra, Odisha, West Bengal and the islands of Lakshadweep and Andaman and Nicobar. Four southern states combined account for almost 92% of the total production in the country: Kerala (45.22%), Tamil Nadu (26.56%), Karnataka (10.85%), and Andhra Pradesh (8.93%). Other states, such as Goa, Maharashtra, Odisha, West Bengal, and those in the northeast (Tripura and Assam) account for the remaining 8.44%. Kerala, which has the largest number of coconut trees, is famous for its coconut-based products—coconut water, copra, coconut oil, coconut cake (also called coconut meal, copra cake, or copra meal), coconut toddy, coconut shell-based products, coconut wood-based products, coconut leaves, and coir pith.
Various terms, such as copra and coir, are derived from the native Malayalam language. In Kerala, the coconut tree is called "Thengu" also termed as "kalpa vriksham", which essentially means all parts of a coconut tree is useful some way or other.
Maldives.
The coconut is the national tree of the Maldives and is considered the most important plant in the country. A coconut tree is also included in the country's national emblem or coat of arms. Coconut trees are grown on all the islands. Before modern construction methods were introduced, coconut leaves were used as roofing material for many houses in the islands, while coconut timber was used to build houses and boats.
Middle East.
The main coconut-producing area in the Middle East is the Dhofar region of Oman, but they can be grown all along the Persian Gulf, Arabian Sea and Red Sea coasts, because these seas are tropical and provide enough humidity (through seawater evaporation) for coconut trees to grow. The young coconut plants need to be nursed and irrigated with drip pipes until they are old enough (stem bulb development) to be irrigated with brackish water or seawater alone, after which they can be replanted on the beaches. In particular, the area around Salalah maintains large coconut plantations similar to those found across the Arabian Sea in Kerala. The reasons why coconut are cultivated only in Yemen's Al Mahrah and Hadramaut governorates and in the Sultanate of Oman, but not in other suitable areas in the Arabian Peninsula, may originate from the fact that Oman and Hadramaut had long dhow trade relations with Burma, Malaysia, Indonesia, East Africa and Zanzibar, as well as southern India and China. Omani people needed the coir rope from the coconut fiber to stitch together their traditional high seas-going dhow vessels in which nails were never used. The 'know how' of coconut cultivation and necessary soil fixation and irrigation may have found its way into Omani, Hadrami and Al-Mahra culture by people who returned from those overseas areas.
The coconut cultivars grown in Oman are generally of the drought-resistant Indian "West Coast tall" (WC Tall) variety. Unlike the UAE, which grows mostly non-native dwarf or hybrid coconut cultivars imported from Florida for ornamental purposes, the slender, tall Omani coconut cultivars are relatively well-adapted to the Middle East's hot dry seasons, but need longer to reach maturity. The Middle East's hot, dry climate favors the development of coconut mites, which cause immature seed dropping and may cause brownish-gray discoloration on the coconut's outer green fiber.
The ancient coconut groves of Dhofar were mentioned by the medieval Moroccan traveller Ibn Battuta in his writings, known as "Al Rihla". The annual rainy season known locally as "Khareef" or monsoon makes coconut cultivation easy on the Arabian east coast.
Coconut trees also are increasingly grown for decorative purposes along the coasts of the UAE and Saudi Arabia with the help of irrigation. The UAE has, however, imposed strict laws on mature coconut tree imports from other countries to reduce the spread of pests to other native palm trees, as the mixing of date and coconut trees poses a risk of cross-species palm pests, such as rhinoceros beetles and red palm weevils. The artificial landscaping adopted in Florida may have been the cause for lethal yellowing, a viral coconut palm disease that leads to the death of the tree. It is spread by host insects, that thrive on heavy turf grasses. Therefore, heavy turf grass environments (beach resorts and golf courses) also pose a major threat to local coconut trees. Traditionally, dessert banana plants and local wild beach flora such as "Scaevola taccada" and "Ipomoea pes-caprae" were used as humidity-supplying green undergrowth for coconut trees, mixed with sea almond and sea hibiscus. Due to growing sedentary life styles and heavy-handed landscaping, there has been a decline in these traditional farming and soil-fixing techniques.
Sri Lanka.
An early mention of the planting of coconuts is found in the "Mahavamsa" during the reign of Agrabodhi II around 589 AD. Coconuts are common in the Sri Lankan diet and the main source of dietary fat.
United States.
The only places in the United States where coconut palms can be grown and reproduced outdoors without irrigation are Hawaii, southern and central Florida, and the territories of Puerto Rico, Guam, American Samoa, the U.S. Virgin Islands, and the Commonwealth of the Northern Mariana Islands.
Coconut palms will grow from coastal Pinellas County and St. Petersburg southwards on Florida's west coast, and Melbourne southwards on Florida's east coast. The occasional coconut palm is seen north of these areas in favoured microclimates in Tampa and Clearwater, as well as around Cape Canaveral and Daytona Beach on the east coast. They reach fruiting maturity, but can be damaged or killed by the occasional winter freezes in these areas. They may also be grown in favoured microclimates in the Rio Grande Valley area of southern Texas near Brownsville, as well as along the upper northeast coast by Galveston Island, however more severe cold snaps keep them from producing viable fruit. While coconut palms flourish in southern Florida, rare cold snaps can injure coconut palms there, as well. Only the Florida Keys and the distant southern Atlantic coastlines near Miami provide safe havens from the cold for growing coconut palms on the mainland.
Australia.
Coconuts are commonly grown around the northern coast of Australia, and in some warmer parts of New South Wales.
Bermuda.
Most of the tall mature coconut trees found in Bermuda were shipped to the island as seedlings on the decks of ships. In more recent years, the importation of coconuts was prohibited, therefore, a large proportion of the younger trees have been propagated from locally grown coconuts.
In the winter months, the growth rate of coconut trees declines due to cooler temperatures and people have commonly attributed this to the reduced yield of coconuts in comparison to tropical regions. However, whilst cooler winter temperatures may be a factor in reducing fruit production, the primary reason for the reduced yield is a lack of water. Bermuda's soil is generally very shallow (1.5 to 3 feet) and much of a coconut tree's root mass is found in the porous limestone underneath the soil. Due to the porosity of the limestone, Bermuda's coconut trees do not generally have a sufficient supply of water with which they are able to support a large number of fruit as rain water quickly drains down through the limestone layer to the water table which is far too deep for a coconut's roots to reach. This typically leads to a reduction in fruit yield (sometimes as little as one or two mature fruits) as well as a reduced milk content inside the coconut that often causes the fruit to be infertile.
Conversely, trees growing in close proximity to the sea almost universally yield a much greater volume of fruit as they are able to tap directly into the sea water which permeates the limestone in such areas. Not only do these trees produce a significantly higher yield, but also the fruit itself tends to be far more fertile due to the higher milk content. Trees found growing in Bermuda's marshy inland areas enjoy a similar degree of success as they are also able to tap directly into a constant supply of water.
Cooler climates.
In cooler climates (but not less than USDA Zone 9), a similar palm, the queen palm ("Syagrus romanzoffiana"), is used in landscaping. Its fruits are very similar to the coconut, but much smaller. The queen palm was originally classified in the genus "Cocos" along with the coconut, but was later reclassified in "Syagrus". A recently discovered palm, "Beccariophoenix alfredii" from Madagascar, is nearly identical to the coconut, more so than the queen palm and can also be grown in slightly cooler climates than the coconut palm. Coconuts can only be grown in temperatures above 18 C and need a daily temperature above 22 C to produce fruit.
Overview of uses.
The coconut palm is grown throughout the tropics for decoration, as well as for its many culinary and nonculinary uses; virtually every part of the coconut palm can be used by humans in some manner and has significant economic value. Coconuts' versatility is sometimes noted in its naming. In Sanskrit, it is "kalpa vriksha" ("the tree which provides all the necessities of life"). In the Malay language, it is "pokok seribu guna" ("the tree of a thousand uses"). In the Philippines, the coconut is commonly called the "tree of life".
Culinary use.
The various parts of the coconut have a number of culinary uses. The seed provides oil for frying, cooking, and making margarine. The white, fleshy part of the seed, the coconut meat, is used fresh or dried in cooking, especially in confections and desserts such as macaroons. Desiccated coconut or coconut milk made from it is frequently added to curries and other savory dishes. Coconut flour has also been developed for use in baking, to combat malnutrition. Coconut chips have been sold in the tourist regions of Hawaii and the Caribbean. Coconut butter is often used to describe solidified coconut oil, but has also been adopted as a name by certain specialty products made of coconut milk solids or puréed coconut meat and oil. Dried coconut is also used as the filling for many chocolate bars. Some dried coconut is purely coconut but others are manufactured with other ingredients, such as sugar, propylene glycol, salt, and sodium metabisulfite. Some countries in South East Asia use special coconut mutant called Kopyor (in Indonesian) or macapuno (in Philippines) as a dessert drinks.
Nutrition.
Per 100 gram serving with 354 calories, raw coconut meat supplies a high amount of total fat (33 grams), especially saturated fat (89% of total fat) and carbohydrates (24 grams) (table). Micronutrients in significant content include the dietary minerals, manganese, iron, phosphorus and zinc (table).
Coconut water.
Coconut water serves as a suspension for the endosperm of the coconut during its nuclear phase of development. Later, the endosperm matures and deposits onto the coconut rind during the cellular phase. It is consumed throughout the humid tropics, and has been introduced into the retail market as a processed sports drink. Mature fruits have significantly less liquid than young, immature coconuts, barring spoilage. Coconut water can be fermented to produce coconut vinegar.
Per 100 gram (100 ml) serving, coconut water contains 19 calories and no significant content of essential nutrients (table).
Coconut milk.
Coconut milk, not to be confused with coconut water, is obtained primarily by extracting juice by pressing the grated coconut white kernel or by passing hot water or milk through grated coconut, which extracts the oil and aromatic compounds. It has a fat content of around 23%. When refrigerated and left to set, coconut cream will rise to the top and separate from the milk. The milk can be used to produce virgin coconut oil by controlled heating and removal of the oil fraction.
A protein-rich powder can be processed from coconut milk following centrifugation, separation and spray drying.
Coconut oil.
Another byproduct of the coconut is coconut oil. It is commonly used in cooking, especially for frying. It can be used in liquid form as would other vegetable oils, or in solid form as would butter or lard.
Toddy and nectar.
The sap derived from incising the flower clusters of the coconut is drunk as "neera", also known as toddy or "tuba" (Philippines), "tuak" (Indonesia and Malaysia) or "karewe" (fresh and not fermented, collected twice a day, for breakfast and dinner) in Kiribati. When left to ferment on its own, it becomes palm wine. Palm wine is distilled to produce "arrack". In the Philippines, this alcoholic drink is called "lambanog" or "coconut vodka".
The sap can be reduced by boiling to create a sweet syrup or candy such as "te kamamai" in Kiribati or "dhiyaa hakuru" and "addu bondi" in the Maldives. It can be reduced further to yield coconut sugar also referred to as palm sugar or jaggery. A young, well-maintained tree can produce around 300 L of toddy per year, while a 40-year-old tree may yield around 400 L.
Heart of palm and coconut sprout.
Apical buds of adult plants are edible, and are known as "palm cabbage" or heart of palm. They are considered a rare delicacy, as harvesting the buds kills the palms. Hearts of palm are eaten in salads, sometimes called "millionaire's salad". Newly germinated coconuts contain an edible fluff of marshmallow-like consistency called coconut sprout, produced as the endosperm nourishes the developing embryo.
Indonesia.
Coconut is an indispensable ingredient in Indonesian cooking. Coconut meat, coconut milk and coconut water are often used in main courses, desserts and soups throughout the archipelago. In the island of Sumatra, the famous Rendang, the traditional beef stew from West Sumatra, chunks of beef are cooked in coconut milk along with other spices for hours until thickened. In Jakarta, "Soto Babat" or beef tripe soup also uses coconut milk. In the island of Java, the sweet and savoury "Tempe Bacem" is made by cooking tempeh with coconut water, coconut sugar and other spices until thickened. "Klapertart" is the famous Dutch-influenced dessert from Manado, North Celebes, that uses young coconut meat and coconut milk.
In 2010, Indonesia increased its coconut production. It is now the world's second largest producer of coconuts. The gross production was 15 million tonnes. A sprouting coconut seed is the logo for Gerakan Pramuka Indonesia, the Indonesian Scouting organization. It can be seen on all the scouting paraphernalia that elementary (SMA) school children wear as well as on the scouting pins and flags.
Philippines.
The Philippines is one of the world's largest producer of coconuts; the production of coconuts plays an important role in the economy. Coconuts in the Philippines are usually used in making main dishes, refreshments and desserts. Coconut juice is also a popular drink in the country. In the Philippines, particularly Cebu, rice is wrapped in coconut leaves for cooking and subsequent storage; these packets are called "puso". Coconut milk, known as "gata", and grated coconut flakes are used in the preparation of dishes such as "laing", "ginataan", "bibingka", "ube halaya", "pitsi-pitsi", "palitaw", "buko" and coconut pie. Coconut jam is made by mixing muscovado sugar with coconut milk. Coconut sport fruits are also harvested. One such variety of coconut is known as "macapuno". Its meat is sweetened, cut into strands and sold in glass jars as coconut strings, sometimes labeled as "gelatinous mutant coconut". Coconut water can be fermented to make a different product—"nata de coco" (coconut gel).
Vietnam.
In Vietnam, coconut is grown abundantly across Central and Southern Vietnam, and especially in Bến Tre Province, often called the "land of the coconut". It is used to make coconut candy, caramel, and jelly. Coconut juice and coconut milk are used, especially in Vietnam's southern style of cooking, including "kho", "chè" and curry ("cà ri").
India.
In southern India, most common way of cooking vegetables is to add grated coconut and then steam them with spices fried in oil. People from southern India also make chutney, which involves grinding the coconut with salt, chillies, and whole spices. "Uruttu chammanthi" (granulated chutney) is eaten with rice or "kanji" (rice gruel). It is also invariably the main side dish served with "idli, vadai", and "dosai". Coconut ground with spices is also mixed in "sambar" and other various lunch dishes for extra taste. Dishes garnished with grated coconut are generally referred to as "poduthol" in North Malabar and "thoran" in rest of Kerala. "Puttu" is a culinary delicacy of Kerala and Tamil Nadu, in which layers of coconut alternate with layers of powdered rice, all of which fit into a bamboo stalk. Recently, this has been replaced with a steel or aluminium tube, which is then steamed over a pot. Coconut (Tamil: தேங்காய்) is regularly broken in the middle-class families in Tamil Nadu for food. Coconut meat can be eaten as a snack sweetened with jaggery or molasses. In Karnataka sweets are prepared using coconut and dry coconut "copra"., Like Kaie Obattu, Kobri mitai etc.
Commercial, industrial, and household use.
Cultivars.
Coconut has a number of commercial and traditional cultivars. They can be sorted mainly into tall cultivars, dwarf cultivars and hybrid cultivars (hybrids between talls and dwarfs). Some of the dwarf cultivars such as "Malayan dwarf" has shown some promising resistance to lethal yellowing while other cultivars such as "Jamaican tall" is highly affected by the same plant disease. Some cultivars are more drought resistant such as "West coast tall" (India) while others such as "Hainan Tall" (China) are more cold tolerant. Other aspects such as seed size, shape and weight and copra thickness are also important factors in the selection of new cultivars. Some cultivars such as "Fiji dwarf" form a large bulb at the lower stem and others are cultivated to produce very sweet coconut water with orange coloured husks (king coconut) used entirely in fruit stalls for drinking (Sri Lanka, India).
Coir.
Coir (the fiber from the husk of the coconut) is used in ropes, mats, door mats, brushes, sacks, caulking for boats, and as stuffing fiber for mattresses. It is used in horticulture in potting compost, especially in orchid mix.
Coconut fronds.
The stiff mid-ribs of coconut leaves are used for making brooms in India, Indonesia ("sapu lidi"), Malaysia, the Maldives and the Philippines ("walis tingting"). The green of the leaves (lamina) are stripped away, leaving the veins (wood-like, thin, long strips) which are tied together to form a broom or brush. A long handle made from some other wood may be inserted into the base of the bundle and used as a two-handed broom. The leaves also provide material for baskets that can draw well water and for roofing thatch; they can be woven into mats, cooking skewers, and kindling arrows, as well. Two leaves (especially the younger, yellowish shoots) woven into a tight shell the size of the palm are filled with rice and cooked to make "ketupat". Dried coconut leaves can be burned to ash, which can be harvested for lime. In India, the woven coconut leaves are used as "pandals" (temporary sheds) for marriage functions especially in the states of Kerala, Karnataka, and Tamil Nadu.
Copra.
Copra is the dried meat of the seed and after processing produces coconut oil and coconut meal. Coconut oil, aside from being used in cooking as an ingredient and for frying, is used in soaps, cosmetics, hair-oil, and massage oil. Coconut oil is also a main ingredient in Ayurvedic oils. In Vanuatu coconut palms for copra production are generally spaced 9 meters apart, allowing a tree density of 100–160 trees per hectare.
Husks and shells.
The husk and shells can be used for fuel and are a source of charcoal. Activated carbon manufactured from coconut shell is considered extremely effective for the removal of impurities. The coconut's obscure origin in foreign lands led to the notion of using cups made from the shell to neutralise poisoned drinks. The cups were frequently engraved and decorated with precious metals.
A dried half coconut shell with husk can be used to buff floors. It is known as a "bunot" in the Philippines and simply a "coconut brush" in Jamaica. The fresh husk of a brown coconut may serve as a dish sponge or body sponge.
In Asia, coconut shells are also used as bowls and in the manufacture of various handicrafts, including buttons carved from dried shell. Coconut buttons are often used for Hawaiian aloha shirts. "Tempurung" as the shell is called in the Malay language can be used as a soup bowl and—if fixed with a handle—a ladle. In Thailand, the coconut husk is used as a potting medium to produce healthy forest tree saplings. The process of husk extraction from the coir bypasses the retting process, using a custom-built coconut husk extractor designed by ASEAN–Canada Forest Tree Seed Centre (ACFTSC) in 1986. Fresh husks contains more tannin than old husks. Tannin produces negative effects on sapling growth. In parts of South India, the shell and husk are burned for smoke to repel mosquitoes.
Half coconut shells are used in theatre Foley sound effects work, banged together to create the sound effect of a horse's hoofbeats. Dried half shells are used as the bodies of musical instruments, including the Chinese "yehu" and "banhu", along with the Vietnamese "đàn gáo" and Arabo-Turkic "rebab". In the Philippines, dried half shells are also used as a music instrument in a folk dance called "maglalatik".
In World War II, coastwatcher scout Biuki Gasa was the first of two from the Solomon Islands to reach the shipwrecked and wounded crew of Motor Torpedo Boat PT-109 commanded by future U.S. president John F. Kennedy. Gasa suggested, for lack of paper, delivering by dugout canoe a message inscribed on a husked coconut shell. This coconut was later kept on the president's desk, and is now in the John F. Kennedy Library.
Coconut trunk.
Coconut trunks are used for building small bridges and huts; they are preferred for their straightness, strength, and salt resistance. In Kerala, coconut trunks are used for house construction. Coconut timber comes from the trunk, and is increasingly being used as an ecologically sound substitute for endangered hardwoods. It has applications in furniture and specialized construction, as notably demonstrated in Manila's Coconut Palace.
Hawaiians hollowed the trunk to form drums, containers, or small canoes. The "branches" (leaf petioles) are strong and flexible enough to make a switch. The use of coconut branches in corporal punishment was revived in the Gilbertese community on Choiseul in the Solomon Islands in 2005.
Coconut roots.
The roots are used as a dye, a mouthwash, and a medicine for diarrhea and dysentery. A frayed piece of root can also be used as a toothbrush.
Use in beauty products.
Coconuts are used in the beauty industry in moisturisers and body butters because coconut oil, due to its chemical structure, is readily absorbed by the skin. The coconut shell may also be ground down and added to products for exfoliation of dead skin. Coconut is also a source of lauric acid, which can be processed in a particular way to produce sodium lauryl sulfate, a detergent used in shower gels and shampoos. The nature of lauric acid as a fatty acid makes it particularly effective for creating detergents and surfactants.
Role in culture and religion.
In the Ilocos region of northern Philippines, the Ilocano people fill two halved coconut shells with "diket" (cooked sweet rice), and place "liningta nga itlog" (halved boiled egg) on top of it. This ritual, known as "niniyogan", is an offering made to the deceased and one's ancestors. This accompanies the "palagip" (prayer to the dead).
A coconut (Sanskrit: "nalikera") is an essential element of rituals in Hindu tradition. Often it is decorated with bright metal foils and other symbols of auspiciousness. It is offered during worship to a Hindu god or goddess. Irrespective of their religious affiliations, fishermen of India often offer it to the rivers and seas in the hopes of having bountiful catches. Hindus often initiate the beginning of any new activity by breaking a coconut to ensure the blessings of the gods and successful completion of the activity. The Hindu goddess of well-being and wealth, Lakshmi, is often shown holding a coconut. In the foothills of the temple town of Palani, before going to worship Murugan for the Ganesha, coconuts are broken at a place marked for the purpose. Every day, thousands of coconuts are broken, and some devotees break as many as 108 coconuts at a time as per the prayer. In tantric practices, coconuts are sometimes used as substitutes for human skulls.
In Hindu wedding ceremonies, a coconut is placed over the opening of a pot, representing a womb. Coconut flowers are auspicious symbols and are fixtures at Hindu and Buddhist weddings and other important occasions. In Kerala, coconut flowers must be present during a marriage ceremony. The flowers are inserted into a barrel of unhusked rice (paddy) and placed within sight of the wedding ceremony. Similarly in Sri Lanka, coconut flowers, standing in brass urns, are placed in prominent positions.
The Zulu Social Aid and Pleasure Club of New Orleans traditionally throws hand-decorated coconuts, the most valuable of Mardi Gras souvenirs, to parade revelers. The "Tramps" began the tradition "circa" 1901. In 1987, a "coconut law" was signed by Gov. Edwards exempting from insurance liability any decorated coconut "handed" from a Zulu float.
The coconut is also used as a target and prize in the traditional British fairground game "coconut shy". The player buys some small balls which he throws as hard as he can at coconuts balanced on sticks. The aim is to knock a coconut off the stand and win it.
It was the main food of adherents of the now discontinued Vietnamese religion Đạo Dừa in Bến Tre.
Myths and legends.
Some South Asian, Southeast Asian and Pacific Ocean cultures have origin myths in which the coconut plays the main role. In the Hainuwele myth from Maluku, a girl emerges from the blossom of a coconut tree. In Maldivian folklore one of the main myths of origin reflects the dependence of the Maldivians on the coconut tree.
According to an urban legend, there are more deaths caused by falling coconuts than by sharks annually.
Other uses.
The leftover fiber from coconut oil and coconut milk production, coconut meal, is used as livestock feed. The dried calyx is used as fuel in wood-fired stoves. Coconut water is traditionally used as a growth supplement in plant tissue culture/micropropagation. The smell of coconuts comes from the 6-pentyloxan-2-one molecule, known as delta-decalactone in the food and fragrance industries.
Tool and shelter for animals.
Researchers from the Melbourne Museum in Australia observed the octopus species "Amphioctopus marginatus use of tools, specifically coconut shells, for defense and shelter. The discovery of this behavior was observed in Bali and North Sulawesi in Indonesia between 1998 and 2008. "Amphioctopus marginatus" is the first invertebrate known to be able to use tools."
A coconut can be hollowed out and used as a home for a rodent or small birds. Halved, drained coconuts can also be hung up as bird feeders, and after the flesh has gone, can be filled with fat in winter to attract tits.
Allergies.
Food allergies.
Coconut can be a food allergen although its prevalence varies from country to country. While coconut is one of the top-five food allergies in India where it is a common food source, such allergies to coconut are considered rare in Australia, the UK, and the United States. As a result, commercial extracts of coconut are not currently available for skin prick testing in Australia or New Zealand.
Despite a low prevalence of allergies to coconut in the United States, the U.S. Food and Drug Administration (FDA) began identifying coconuts in October 2006. Based on FDA guidance and federal U.S. law, coconut must be disclosed as an ingredient.
Topical allergies.
Coconut-derived products can cause contact dermatitis. They can be present in cosmetics, including some shampoos, moisturizers, soaps, cleansers and hand washing liquids. Those known to cause contact dermatitis include: coconut diethanolamide, cocamide sulphate, cocamide DEA, CDEA, sodium laureth sulfate, sodium lauroyl sulfate, ammonium laureth sulfate, ammonium lauryl sulfate, sodium lauroyl sarcosinate, sodium cocoyl sarcosinate, potassium coco hydrolysed collagen, triethanolamine laureth sulfate, caprylic/capric triglycerides, triethanolamine lauryl or cocoyl sarcosime, disodium oleamide sulfocuccinate, laureth sulfasuccinate, and disodium dioctyl sulfosuccinate.

</doc>
<doc id="51347" url="http://en.wikipedia.org/wiki?curid=51347" title="620s BC">
620s BC


</doc>
<doc id="51348" url="http://en.wikipedia.org/wiki?curid=51348" title="630s BC">
630s BC


</doc>
<doc id="51349" url="http://en.wikipedia.org/wiki?curid=51349" title="640s BC">
640s BC


</doc>
<doc id="51350" url="http://en.wikipedia.org/wiki?curid=51350" title="650s BC">
650s BC


</doc>
<doc id="51351" url="http://en.wikipedia.org/wiki?curid=51351" title="660s BC">
660s BC


</doc>
<doc id="51352" url="http://en.wikipedia.org/wiki?curid=51352" title="670s BC">
670s BC


</doc>
<doc id="51353" url="http://en.wikipedia.org/wiki?curid=51353" title="680s BC">
680s BC


</doc>
<doc id="51354" url="http://en.wikipedia.org/wiki?curid=51354" title="690s BC">
690s BC


</doc>
<doc id="51355" url="http://en.wikipedia.org/wiki?curid=51355" title="700s BC (decade)">
700s BC (decade)


</doc>
<doc id="51356" url="http://en.wikipedia.org/wiki?curid=51356" title="710s BC">
710s BC


</doc>
<doc id="51357" url="http://en.wikipedia.org/wiki?curid=51357" title="Polychlorinated dibenzodioxins">
Polychlorinated dibenzodioxins

Polychlorinated dibenzodioxins (PCDDs), or simply dioxins, are a group of polyhalogenated organic compounds that are significant environmental pollutants.
They are commonly but inaccurately referred to as dioxins for simplicity, because every PCDD molecule contains a dibenzo-1,4-dioxin skeletal structure, with 1,4-dioxin as the central ring. Members of the PCDD family bioaccumulate in humans and wildlife because of their lipophilic properties, and may cause developmental disturbances and cancer.
Dioxins occur as by-products in the manufacture of some organochlorides, in the incineration of chlorine-containing substances such as polyvinyl chloride (PVC), in the chlorine bleaching of paper, and from natural sources such as volcanoes and forest fires. There have been many incidents of dioxin pollution resulting from industrial emissions and accidents; the earliest such incidents were in the mid 19th century during the Industrial Revolution.
The word "dioxins" may also refer to other similarly acting chlorinated compounds (see Dioxins and dioxin-like compounds).
Chemical structure of dibenzo-1,4-dioxins.
The structure of dibenzo-1,4-dioxin consists of two benzene rings joined by two oxygen bridges. This makes the compound an aromatic diether. The name dioxin formally refers to the central dioxygenated ring, which is stabilized by the two flanking benzene rings.
In PCDDs, chlorine atoms are attached to this structure at any of 8 different places on the molecule, at positions 1–4 and 6–9. There are 75 different PCDD congeners (that is, related dioxin compounds).
The toxicity of PCDDs depends on the number and positions of the chlorine atoms. Congeners that have chlorine in the 2, 3, 7, and 8 positions have been found to be significantly toxic. In fact, 7 congeners have chlorine atoms in the relevant positions which were considered toxic by the World Health Organization toxic equivalent (WHO-TEQ) scheme.
Historical perspective.
Low concentrations of dioxins existed in nature prior to industrialization as a result of natural combustion and geological processes. Dioxins were first unintentionally produced as by-products from 1848 onwards as Leblanc process plants started operating in Germany. The first intentional synthesis of chlorinated dibenzodioxin was in 1872. Today, concentrations of dioxins are found in all humans, with higher levels commonly found in persons living in more industrialized countries. The most toxic dioxin, 2,3,7,8-tetrachlorodibenzodioxin (TCDD), became well known as a contaminant of Agent Orange, a herbicide used in the Malayan Emergency and the Vietnam War. Later, dioxins were found in Times Beach, Missouri and Love Canal, New York and Seveso, Italy. More recently, dioxins have been in the news with the poisoning of President Viktor Yushchenko of Ukraine in 2004, the Naples Mozzarella Crisis the 2008 Irish pork crisis, and the German feed incident of 2010.
Sources of dioxins.
The United States Environmental Protection Agency inventory of sources of dioxin-like compounds is possibly the most comprehensive review of the sources and releases of dioxins, but other countries now have substantial research as well.
Occupational exposure is an issue for some in the chemical industries, historically for those making chlorophenols or chlorophenoxy acid herbicides or in the application of chemicals, notably herbicides. In many developed nations there are now emissions regulations which have dramatically decreased the emissions and thus alleviated some concerns, although the lack of continuous sampling of dioxin emissions causes concern about the understatement of emissions. In Belgium, through the introduction of a process called AMESA, continuous sampling showed that periodic sampling understated emissions by a factor of 30 to 50 times. Few facilities have continuous sampling.
Dioxins are produced in small concentrations when organic material is burned in the presence of chlorine, whether the chlorine is present as chloride ions or as organochlorine compounds, so they are widely produced in many contexts. According to the most recent US EPA data, the major sources of dioxins are broadly in the following types:
When first carried out in 1987, the original US EPA inventory of dioxin sources revealed that incineration represented more than 80% of known dioxin sources. As a result, US EPA implemented new emissions requirements. These regulations succeeded in reducing dioxin stack emissions from incinerators. Incineration of municipal solid waste, medical waste, sewage sludge, and hazardous waste together now produce less than 3% of all dioxin emissions. Since 1987, however, backyard barrel burning has showed almost no decrease, and is now the largest source of dioxin emissions, producing about one third of the total output.
In incineration, dioxins can also reform or form "de novo" in the atmosphere above the stack as the exhaust gases cool through a temperature window of 600 to 200 °C. The most common method of reducing the quantity of dioxins reforming or forming "de novo" is through rapid (30 millisecond) quenching of the exhaust gases through that 400 °C window. Incinerator emissions of dioxins have been reduced by over 90% as a result of new emissions control requirements. Incineration in developed countries is now a very minor contributor to dioxin emissions.
Dioxins are also generated in reactions that do not involve burning — such as chlorine bleaching fibers for paper or textiles, and in the manufacture of chlorinated phenols, particularly when reaction temperature is not well controlled. Compounds involved include the wood preservative pentachlorophenol, and also herbicides such as 2,4-dichlorophenoxyacetic acid (or 2,4-D) and 2,4,5-trichlorophenoxyacetic acid (2,4,5-T). Higher levels of chlorination require higher reaction temperatures and greater dioxin production. Dioxins may also be formed during the photochemical breakdown of the common antimicrobial compound triclosan.
Sources of human intake.
Tolerable daily, monthly or annual intakes have been set by the World Health Organization and a number of governments. Dioxins enter the general population almost exclusively from ingestion of food, specifically through the consumption of fish, meat, and dairy products since dioxins are fat-soluble and readily climb the food chain.
Children are passed substantial body burdens by their mothers, and breastfeeding increases the child's body burden. Dioxin exposure can also occur from contact with Pentachlorophenol (Penta) treated lumber as Pentachlorophenol often contains dioxins as a contaminant.Children's daily intakes during breast feeding are often many times above the intakes of adults based on body weight. This is why the WHO consultation group assessed the tolerable intake so as to prevent a woman from accumulating harmful body burdens before her first pregnancy. Breast fed children usually still have higher dioxin body burdens than non breast fed children. The WHO still recommends breast feeding for its other benefits. In many countries dioxins in breast milk have decreased by even 90% during the two last decades.
Dioxins are present in cigarette smoke. Dioxin in cigarette smoke was noted as "understudied" by the US EPA in its "Re-Evaluating Dioxin" (1995). In that same document, the US EPA acknowledged that dioxin in cigarettes is "anthropogenic" (man-made, "not likely in nature").
Metabolism.
Dioxins are absorbed primarily through dietary intake of fat, as this is where they accumulate in animals and humans. In humans, the highly chlorinated dioxins are stored in fatty tissues and are neither readily metabolized nor excreted. The estimated elimination half-life for highly chlorinated dioxins (4–8 chlorine atoms) in humans ranges from 4.9 to 13.1 years.
The persistence of a particular dioxin congener in an animal is thought to be a consequence of its structure. Dioxins with no lateral (2, 3, 7, and 8) chlorines, which thus contain hydrogen atoms on adjacent pairs of carbons, can more readily be oxidized by cytochromes P450. The oxidized dioxins can then be more readily excreted rather than stored for a long time.
Toxicity.
2,3,7,8-Tetrachlorodibenzodioxin (TCDD) is considered the most toxic of the congeners (for the mechanism of action, see 2,3,7,8-Tetrachlorodibenzodioxin and Aryl hydrocarbon receptor). Other dioxin congeners including PCDFs and PCBs with dioxin-like toxicity, are given a toxicity rating from 0 to 1, where TCDD = 1 (see Dioxins and dioxin-like compounds). This toxicity rating is called the Toxic Equivalence Factor concept, or TEF. TEFs are consensus values and, because of the strong species dependence for toxicity, are listed separately for mammals, fish, and birds. TEFs for mammalian species are generally applicable to human risk calculations. The TEFs have been developed from detailed assessment of literature data to facilitate both risk assessment and regulatory control. Many other compounds may also have dioxin-like properties, particularly non-ortho PCBs, one of which has a TEF as high as 0.1.
The total dioxin toxic equivalence (TEQ) value expresses the toxicity as if the mixture were pure TCDD. The TEQ approach and current TEFs have been adopted internationally as the most appropriate way to estimate the potential health risks of mixture of dioxins. Recent data suggest that this type of simple scaling factor may not be the most appropriate treatment for complex mixtures of dioxins; both transfer from the source and absorption and elimination vary among different congeners, and the TEF value is not able to accurately reflect this.
Dioxins and other persistent organic pollutants (POPs) are subject to the Stockholm Convention. The treaty obliges signatories to take measures to eliminate where possible, and minimize where not possible to eliminate, all sources of dioxin.
Health effects in humans.
Dioxins build up primarily in fatty tissues over time (bioaccumulate), so even small exposures may eventually reach dangerous levels. In 1994, the US EPA reported that dioxins are a probable carcinogen, but noted that non-cancer effects (reproduction and sexual development, immune system) may pose a greater threat to human health. TCDD, the most toxic of the dibenzodioxins, is classified as a Group 1 carcinogen by the International Agency for Research on Cancer (IARC). TCDD has a half-life of approximately 8 years in humans, although at high concentrations, the elimination rate is enhanced by metabolism. The health effects of dioxins are mediated by their action on a cellular receptor, the aryl hydrocarbon receptor (AhR).
Exposure to high levels of dioxins in humans causes a severe form of persistent acne, known as chloracne. High occupational or accidental levels of exposures to dioxins have been shown by epidemiological studies to lead to an increased risk of tumors at all sites. Other effects in humans (at high dose levels) may include:
Recent studies have shown that high exposure to dioxins changes the ratio of male to female births among a population such that more females are born than males.
Dioxins accumulate in food chains in a fashion similar to other chlorinated compounds (bioaccumulation). This means that even small concentrations in contaminated water can be concentrated up a food chain to dangerous levels because of the long biological half life and low water solubility of dioxins.
Toxic effects in animals.
While it has been difficult to establish specific health effects in humans due to the lack of controlled dose experiments, studies in animals have shown that dioxin causes a wide variety of toxic effects. In particular, TCDD (see this) has been shown to be teratogenic, mutagenic, carcinogenic, immunotoxic, and hepatotoxic. Furthermore, alterations in multiple endocrine and growth factor systems have been reported. The most sensitive effects, observed in multiple species, appear to be developmental, including effects on the developing immune, nervous, and reproductive systems. The most sensitive effects are caused at body burdens relatively close to those reported in humans.
Among the animals for which TCDD toxicity has been studied, there is strong evidence for the following effects:
The LD50 of dioxin also varies wildly between species with the most notable disparity being between the ostensibly similar species of hamster and guinea pig. The oral LD50 for guinea pigs is as low as 0.5 to 2 μg/Kg body weight, whereas the oral LD50 for hamsters can be as high as 1 to 5 mg/Kg body weight, a difference of as much as thousandfold or more, and even among rat strains there may be thousandfold differences.
Agent Orange.
Agent Orange was the code name for one of the herbicides and defoliants the U.S. military as part of its herbicidal warfare program, Operation Ranch Hand, during the Vietnam War from 1961 to 1971. It was a mixture of 2,4,5-T and 2,4-D. The 2,4,5-T used was contaminated with 2,3,7,8-tetrachlorodibenzodioxin (TCDD), an extremely toxic dioxin compound.
During the Vietnam war, between 1962 and 1971, the United States military sprayed 20000000 U.S.gal of chemical herbicides and defoliants in Vietnam, eastern Laos and parts of Cambodia, as part of Operation Ranch Hand.
By 1971, 12% of the total area of South Vietnam had been sprayed with defoliating chemicals, which were often applied at rates that were 13 times as high as the legal USDA limit. In South Vietnam alone, an estimated 10 million hectares of agricultural land were ultimately destroyed. In some areas, TCDD concentrations in soil and water were hundreds of times greater than the levels considered safe by the U.S. Environmental Protection Agency.
According to Vietnamese Ministry of Foreign Affairs, 4.8 million Vietnamese people were exposed to Agent Orange, resulting in 400,000 people being killed or maimed, and 500,000 children born with birth defects. The Red Cross of Vietnam estimates that up to 1 million people are disabled or have health problems due to contaminated Agent Orange. The United States government has challenged these figures as being unreliable and unrealistically high.
Dioxin testing.
The analyses used to determine these compounds' relative toxicity share common elements that differ from methods used for more traditional analytical determinations. The preferred methods for dioxins and related analyses use high resolution gas chromatography/mass spectrometry (HRGC/HRMS). Concentrations are determined by measuring the ratio of the analyte to the appropriate isotopically labeled internal standard.
Also novel bio-assays like DR CALUX are nowadays used in identification of dioxins and dioxin-like compounds. The advantage in respect to HRGC/HRMS is that it is able to scan many samples at lower costs. Also it is able to detect all compounds that interact with the Ah-receptor which is responsible for carcinogenic effects.

</doc>
<doc id="51359" url="http://en.wikipedia.org/wiki?curid=51359" title="500 BC">
500 BC

The year 500 BC was a year of the pre-Julian Roman calendar. In the Roman Empire it was known as the Year of the Consulship of Camerinus and Longus (or, less frequently, year 254 " Ab urbe condita"). The denomination 500 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>

</doc>
<doc id="51360" url="http://en.wikipedia.org/wiki?curid=51360" title="486 BC">
486 BC

Year 486 BC was a year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Consulship of Viscellinus and Rutilus (or, less frequently, year 268 "Ab urbe condita"). The denomination 486 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Art.
</onlyinclude>

</doc>
<doc id="51361" url="http://en.wikipedia.org/wiki?curid=51361" title="480 BC">
480 BC

Year 480 BCE was a year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Consulship of Vibulanus and Cincinnatus (or, less frequently, year 274 "Ab urbe condita"). The denomination 480 BCE for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Arts.
</onlyinclude>

</doc>
<doc id="51362" url="http://en.wikipedia.org/wiki?curid=51362" title="430s BC">
430s BC


</doc>
<doc id="51363" url="http://en.wikipedia.org/wiki?curid=51363" title="History of the Caribbean">
History of the Caribbean

The history of the Caribbean reveals the significant role the region played in the colonial struggles of the European powers since the 15th century. In the 20th century the Caribbean was again important during World War II, in decolonization wave in the post-war period, and in the tension between Communist Cuba and the United States (US). Genocide, slavery, immigration and rivalry between world powers have given Caribbean history an impact disproportionate to the size of this small region.
Before European contact.
The oldest evidence of human settlement in the Caribbean has been found at Ortoiroid sites on Trinidad dating to the mid-6th millennium BC. They had reached Hispaniola and Cuba by the mid-5th millennium BCE, where their society is also known as the Casirimoid. The hunter-gatherer Guanahatabey present in western Cuba at the time of Columbus's arrival may have represented a continuation of their culture or more recent arrivals from southern Florida or the Yucatan.
The islands were then repopulated by successive waves of invaders travelling south to north from initial bases in the Orinoco River valley. Between 400 and 200 BC, the Saladoid spread north from Trinidad, introducing agriculture and ceramic pottery. Sometime after AD 250, the Barrancoid followed and replaced them on Trinidad. This society's settlements in the Orinoco collapsed around 650 and another group, the Arauquinoid (the later "Taíno" or "Arawaks"), expanded into the area and northward along the island chain. Around 1200 or 1300, a fourth group, the Mayoid (the later "Caribs"), entered Trinidad. They remained dominant until the Spanish conquest.
At the time of the European arrival, three major Amerindian indigenous peoples lived on the islands: the Taíno in the Greater Antilles, The Bahamas and the Leeward Islands; the Island Caribs and Galibi in the Windward Islands; and the Ciboney in western Cuba. The Taínos are subdivided into Classic Taínos, who occupied Hispaniola and Puerto Rico, Western Taínos, who occupied Cuba, Jamaica, and the Bahamian archipelago, and the Eastern Taínos, who occupied the Leeward Islands. Trinidad was inhabited by both Carib speaking and Arawak-speaking groups.
New scientific DNA studies have changed some of the traditional beliefs about pre-Columbian indigenous history. Juan Martinez Cruzado, a geneticist from the University of Puerto Rico at Mayagüez designed an island-wide DNA survey of Puerto Rico's people. According to conventional historical belief, Puerto Ricans have mainly Spanish ethnic origins, with some African ancestry, and distant and less significant indigenous ancestry. Cruzado's research revealed surprising results in 2003. It found that, in fact, 61 percent of all Puerto Ricans have Amerindian mitochondrial DNA, 27 percent have African and 12 percent Caucasian.
Colonial era.
Soon after the voyages of Christopher Columbus to the Americas, both Portuguese and Spanish ships began claiming territories in Central and South America. These colonies brought in gold, and other European powers, most specifically England, the Netherlands, and France, hoped to establish profitable colonies of their own. Imperial rivalries made the Caribbean a contested area during European wars for centuries.
Spanish conquest.
During the first voyage of the explorer Christopher Columbus (mandated by the Spanish crown to conquer) contact was made with the Lucayans in the Bahamas and the Taíno in Cuba and the northern coast of Hispaniola, and a few of the native people were taken back to Spain. Small amounts of gold were found in their personal ornaments and other objects such as masks and belts. The Spanish, who came seeking wealth, enslaved the native population and rapidly drove them to near-extinction. To supplement the Amerindian labor, the Spanish imported African (slavery/slaves)
(see also Slavery in the Spanish New World colonies) Although Spain claimed the entire Caribbean, they settled only the larger islands of Hispaniola (1493), Puerto Rico (1508), Jamaica (1509), Cuba (1511), and Trinidad (1530), although the Spanish made an exception in the case of the small 'pearl islands' of Cubagua and Margarita off the Venezuelan coast because of their valuable pearl beds which were worked extensively between 1508 and1530.
Other European powers.
The other European powers established a presence in the Caribbean after the Spanish Empire declined, partly due to the reduced native population of the area from European diseases. The Dutch, the French, and the British followed one another to the region and established a long-term presence. They brought with them millions of slaves imported from Africa to support the tropical plantation system that spread through the Caribbean islands.
Slaves in the Caribbean.
The development of agriculture in the Caribbean required a large workforce of manual labourers, which the Europeans found by taking advantage of the slave trade in Africa. The Atlantic slave trade brought African slaves to British, Dutch, French, Portuguese and Spanish colonies in the Americas, including the Caribbean. Slaves were brought to the Caribbean from the early 16th century until the end of the 19th century. The majority of slaves were brought to the Caribbean colonies between 1701 and 1810. Also in 1812 there was a slave revolution in the colony of Barbados.
The following table lists the number of slaves brought into some of the Caribbean colonies:
Abolitionists in the Americas and in Europe became vocal opponents of the slave trade throughout the 19th century. The importation of slaves to the colonies was often outlawed years before the end of the institution of slavery itself. It was well into the 19th century before many slaves in the Caribbean were legally free. The trade in slaves was abolished in the British Empire through the Abolition of the Slave Trade Act in 1807. Men, women and children who were already enslaved in the British Empire remained slaves, however, until Britain passed the Slavery Abolition Act in 1833. When the Slavery Abolition Act came into force in 1834, roughly 700,000 slaves in the British West Indies immediately became free; other enslaved workers were freed several years later after a period of forced apprenticeship. Slavery was abolished in the Dutch Empire in 1814. Spain abolished slavery in its empire in 1811, with the exceptions of Cuba, Puerto Rico, and Santo Domingo; Spain ended the slave trade to these colonies in 1817, after being paid ₤400,000 by Britain. Slavery itself was not abolished in Cuba until 1886. France abolished slavery in its colonies in 1848.
Marriage, separation, and sale together.
"The official plantocratic view of slave marriage sought to deny the slaves any loving bonds or long-standing relationships, thus conveniently rationalising the indiscriminate separation of close kin through sales." "From the earliest days of slavery, indiscriminate sales and separation severely disrupted the domestic life of individual slaves." Slaves could be sold so that spouses could be sold separately. "Slave couples were sometimes separated by sale ... They lived as single slaves or as part of maternal or extended families but considered themselves 'married.'" Sale of estates with "stock" to pay debts, more common in the late period of slavery, was criticized as separating slave spouses. William Beckford argued for "families to be sold together or kept as near as possible in the same neighbourhood" and "laws were passed in the late period of slavery to prevent the breakup of slave families by sale, ... [but] these laws were frequently ignored". "Slaves frequently reacted strongly to enforced severance of their emotional bonds", feeling "sorrow and despair", sometimes, according to Thomas Cooper in 1820, resulting in death from distress. John Stewart argued against separation as leading slave buyers to regret it because of "despair[,] ... utter despondency[,] or 'put[ting] period to their lives'". Separated slaves often used free time to travel long distances to reunite for a night and sometimes runaway slaves were married couples. However, "sale of slaves and the resulting breakup of families decreased as slave plantations lost prosperity."
Colonial laws.
European plantations required laws to regulate the plantation system and the many slaves imported to work on the plantations. This legal control was the most oppressive for slaves inhabiting colonies where they outnumbered their European masters and where rebellion was persistent, such as Jamaica. During the early colonial period, rebellious slaves were harshly punished, with sentences including death by torture; less serious crimes such as assault, theft or persistent escape attempts were commonly punished with mutilations, such as the cutting off of a hand or a foot.
Under British rule, slaves could only be freed with the consent of their master, and therefore freedom for slaves was rare. British colonies were able to establish laws through their own legislatures, and the assent of the local island governor and the Crown. British law considered slaves to be property, and thus did not recognize marriage for slaves, family rights, education for slaves, or the right to religious practises such as holidays. British law denied all rights to freed slaves, with the exception of the right to a jury trial. Otherwise, freed slaves had no right to own property, vote or hold office, or even enter some trades.
The French Empire regulated slaves under the Code Noir (Black Code) which was in force throughout the empire, but which was based upon French practises in the Caribbean colonies. French law recognized slave marriages, but only with the consent of the master. French law, like Spanish law, gave legal recognition to marriages between European men and black or Creole women. French and Spanish laws were also significantly more lenient than British law in recognizing manumission, or the ability of a slave to purchase their freedom and become a "freeman". Under French law, free slaves gained full rights to citizenship. The French also extended limited legal rights to slaves, for example the right to own property, and the right to enter contracts.
Impact of colonialism on the Caribbean.
Economic exploitation.
The exploitation of the Caribbean landscape dates back to the Spanish conquistadors around 1600 who mined the islands for gold which they brought back to Spain. The more significant development came when Christopher Columbus wrote back to Spain that the islands were made for sugar development. The history of Caribbean agricultural dependency is closely linked with European colonialism which altered the financial potential of the region by introducing a plantation system. Much like the Spanish enslaved indigenous Indians to work in gold mines, the seventeenth century brought a new series of oppressors in the form of the Dutch, the English, and the French. By the middle of the eighteenth century sugar was Britain's largest import which made the Caribbean that much more important as a colony.
Sugar was a luxury in Europe prior to the 18th century. It became widely popular in the 18th century, then graduated to becoming a necessity in the 19th century. This evolution of taste and demand for sugar as an essential food ingredient unleashed major economic and social changes. Caribbean islands with plentiful sunshine, abundant rainfalls and no extended frosts were well suited for sugarcane agriculture and sugar factories.
Following the emancipation of slaves in 1833 in the United Kingdom, many liberated Africans left their former masters. This created an economic chaos for British owners of Caribbean sugar cane plantations. The hard work in hot, humid farms required a regular, docile and low-waged labour force. The British looked for cheap labour. This they found initially in China and then mostly in India. The British crafted a new legal system of forced labour, which in many ways resembled enslavement. Instead of calling them slaves, they were called indentured labour. Indians and southeast Asians, began to replace Africans previously brought as slaves, under this indentured labour scheme to serve on sugarcane plantations across the British empire. The first ships carrying indentured labourers for sugarcane plantations left India in 1836. Over the next 70 years, numerous more ships brought indentured labourers to the Caribbean, as cheap and docile labor for harsh inhumane work. The slave labor and indentured labor - both in millions of people - were brought into Caribbean, as in other European colonies throughout the world.
The “New World” plantations were established in order to fulfill the growing needs of the “Old World”. The sugar plantations were built with the intention of exporting the sugar back to Britain which is why the British did not need to stimulate local demand for the sugar with wages. A system of slavery was adapted since it allowed the colonizer to have an abundant work force with little worry about declining demands for sugar. In the 19th century wages were finally introduced with the abolition of slavery. The new system in place however was similar to the previous as it was based on white capital and colored labor. Large numbers of unskilled workers were hired to perform repeated tasks, which made if very difficult for these workers to ever leave and pursue any non farming employment. Unlike other countries, where there was an urban option for finding work, the Caribbean countries had money invested in agriculture and lacked any core industrial base. The cities that did exist offered limited opportunities to citizens and almost none for the unskilled masses who had worked in agriculture their entire lives. The products produced brought in no profits for the countries since they were sold to the colonial occupant buyer who controlled the price the products were sold at. This resulted in extremely low wages with no potential for growth since the occupant nations had no intention of selling the products at a higher price to themselves.
The result of this economic exploitation was a plantation dependence which saw the Caribbean nations possessing a large quantity of unskilled workers capable of performing agricultural tasks and not much else. After many years of colonial rule the nations also saw no profits brought into their country since the sugar production was controlled by the colonial rulers. This left the Caribbean nations with little capital to invest towards enhancing any future industries unlike European nations which were developing rapidly and separating themselves technologically and economically from most impoverished nations of the world.
Wars.
The Caribbean region was war-torn throughout much of colonial history, but the wars were often based in Europe, with only minor battles fought in the Caribbean. Some wars, however, were borne of political turmoil in the Caribbean itself.
Piracy in the Caribbean was often a tool used by the European empires to wage war unofficially against one another. Gold plundered from Spanish ships and brought to Britain had a pivotal effect on European interest in colonizing the region.
Slave rebellions.
The plantation system and the slave trade that enabled its growth led to regular slave resistance in many Caribbean islands throughout the colonial era. Resistance was made by escaping from the plantations altogether, and seeking refuge in the areas free of European settlement. Communities of escaped slaves, who were known as Maroons, banded together in heavily forested and mountainous areas of the Greater Antilles and some of the islands of the Lesser Antilles. The spread of the plantations and European settlement often meant the end of many Maroon communities, although they survived on Saint Vincent and Dominica, and in the more remote mountainous areas of Jamaica, Hispaniola, Guadeloupe and Cuba.
Violent resistance broke out periodically on the larger Caribbean islands. Many more conspiracies intended to create rebellions were discovered and ended by Europeans before they could materialize. Actual violent uprisings, involving anywhere from dozens to thousands of slaves, were regular events, however. Jamaica and Cuba in particular had many slave uprisings. Such uprisings were brutally crushed by European forces.
Caribbean slave uprisings (1522–1844).
The following table lists slave rebellions that resulted in actual violent uprisings:
Independence.
Haiti, the former French colony of Saint-Domingue on Hispaniola, was the first Caribbean nation to gain independence from European powers in 1804. This followed 13 years of warfare which commenced as a slave uprising in 1791 and quickly became the Haitian Revolution under the leadership of Toussaint l'Ouverture, where the former slaves defeated the French army (twice), the Spanish army, and the British army, before becoming the world's first and oldest black republic, and also the second-oldest republic in the Western Hemisphere after the United States. This is additionally notable as being the only successful slave uprising in history. The remaining two-thirds of Hispaniola were conquered by Haitian forces in 1821. In 1844, the newly formed Dominican Republic declared its independence from Haiti.
The nations bordering the Caribbean in Central America gained independence with the 1821 establishment of the First Mexican Empire - which at that time included the modern states of Mexico, Guatemala, El Salvador, Honduras, Nicaragua, and Costa Rica. The nations bordering the Caribbean in South America also gained independence from Spain in 1821 with the establishment of Gran Colombia - which comprised the modern states of Venezuela, Colombia, Ecuador, and Panama.
Cuba and Puerto Rico remained a Spanish colonies until the Spanish American War in 1898, after which Cuba attained its independence in 1902, and Puerto Rico became an unincorporated territory of the United States, being the last of the Greater Antilles under colonial control.
Between 1958 and 1962 most of the British-controlled Caribbean was integrated as the new West Indies Federation in an attempt to create a single unified future independent state - but it failed. The following former British Caribbean island colonies achieved independence in their own right; Jamaica (1962), Trinidad & Tobago (1962), Barbados (1966), Bahamas (1973), Grenada (1974), Dominica (1978), St. Lucia (1979), St. Vincent (1979), Antigua & Barbuda (1981), St. Kitts & Nevis (1983).
In addition British Honduras in Central America became independent as Belize (1981), British Guiana in South America became independent as Guyana (1966), and Dutch Guiana also in South America became independent as Suriname (1975).
Islands currently under European or United States administration.
It should be noted that as of the early 21st century, not all Caribbean islands have become independent. Several islands continue to have government ties with European countries, or with the United States.
French overseas departments and territories include several Caribbean islands. Guadeloupe and Martinique are French overseas regions, a legal status that they have had since 1946. Their citizens are considered full French citizens with the same legal rights. In 2003, the populations of St. Martin and St. Barthélemy voted in favour of secession from Guadeloupe in order to form separate overseas collectivities of France. After a bill was passed in the French Parliament, the new status took effect on 22 February 2007.
Puerto Rico and the U.S. Virgin Islands are officially insular areas of the United States, but are sometimes referred to as "protectorates" of the United States. They are administered by the Office of Insular Affairs (OIA) within the United States Department of Interior.
British overseas territories in the Caribbean include:
Aruba, Curaçao, and Sint Maarten are all presently separate constituent countries, formerly part of the Netherlands Antilles. Along with Netherlands, they form the four constituent countries of the Kingdom of the Netherlands. Citizens of these islands have full Dutch citizenship.
History of relations with the US.
Since the Monroe Doctrine, the United States gained a major influence on most Caribbean nations. In the early part of the twentieth century this influence was extended by participation in The Banana Wars. Areas outside British or French control became known in Europe as "America's tropical empire".
Victory in the Spanish–American War and the signing of the Platt amendment in 1901 ensured that the United States would have the right to interfere in Cuban political and economic affairs, militarily if necessary. After the Cuban revolution of 1959 relations deteriorated rapidly leading to the Bay of Pigs Invasion, the Cuban Missile Crisis and successive US attempts to destabilise the island. The US invaded and occupied Hispaniola (present day Dominican Republic and Haiti) for 19 years (1915–34), subsequently dominating the Haitian economy through aid and loan repayments. The US invaded Haiti again in 1994 and in 2004 were accused by CARICOM of arranging a coup d'état to remove elected Haitian leader Jean-Bertrand Aristide.
In 1965, 23,000 US troops were sent to the Dominican Republic to quash a local uprising against military rule. President Lyndon Johnson had ordered the invasion to stem what he claimed to be a "Communist threat", however the mission appeared ambiguous and was roundly condemned throughout the hemisphere as a return to gunboat diplomacy. In 1983 the US invaded Grenada to remove populist left-wing leader Maurice Bishop. The US maintains a naval military base in Cuba at Guantanamo Bay. The base is one of five unified commands whose "area of responsibility" is Latin America and the Caribbean. The command is headquartered in a Miami, Florida office building.
As an arm of the economic and political network of the Americas, the influence of the United States stretches beyond a military context. In economic terms, the United States represents a primary market for the export of Caribbean goods. Notably, this is a recent historical trend. The post-war era reflects a time of transition for the Caribbean basin when, as colonial powers sought to disentangle from the region (as part of a larger trend of decolonization), the US began to expand its hegemony throughout the region. This pattern is confirmed by economic initiatives such as the Caribbean Basin Initiative (CBI), which sought to congeal alliances with the region in light of a perceived Soviet threat. The CBI marks the emergence of the Caribbean basin as a geopolitical area of strategic interest to the US.
This relationship has carried through to the 21st century, as reflected by the Caribbean Basin Trade Partnership Act. The Caribbean Basin is also of strategic interest in regards to trade routes; it has been estimated that nearly half of US foreign cargo and crude oil imports are brought via Caribbean seaways. During wartime, these figures only stand to increase. It is important to note that the US is also of strategic interest to the Caribbean. Caribbean foreign policy seeks to strengthen its participation in a global free market economy. As an extension of this, Caribbean states do not wish to be excluded from their primary market in the US, or be bypassed in the creation of “wider hemispheric trading blocs” that stand to drastically alter trade and production in the Caribbean Basin. As such, the US has played an influential role in shaping the Caribbean’s role in this hemispheric market. Likewise, building trade relationships with the US has always figured in strongly with the political goal of economic security in post-independence Caribbean states.
Economic change in the 20th century.
The mainstay of the Caribbean economy, sugar, has declined gradually since the beginning of the 20th century, although it is still a major crop in the region. Caribbean sugar production became relatively expensive in comparison to other parts of the world that developed their own sugar cultivation industries, making it difficult for Caribbean sugar products to compete. Caribbean economic diversification into new activities became essential to the islands.
Tourism.
By the beginning of the 20th century, the Caribbean islands enjoyed greater political stability. Large-scale violence was no longer a threat after the end of slavery in the islands. The British-controlled islands in particular benefited from investments in the infrastructure of colonies. By the beginning of World War I, all British-controlled islands had their own police force, fire department, doctors and at least one hospital. Sewage systems and public water supplies were built, and death rates in the islands dropped sharply. Literacy also increased significantly during this period, as schools were set up for students descended from African slaves. Public libraries were established in large towns and capital cities.
These improvements in the quality of life for the inhabitants also made the islands a much more attractive destination for visitors. Tourists began to visit the Caribbean in larger numbers by the beginning of the 20th century, although there was a tourist presence in the region as early as the 1880s. The United States-owned United Fruit Company operated a fleet of "banana boats" in the region that doubled as tourist transportation. The United Fruit Company also developed hotels for tourist accommodations. It soon became apparent, however, that this industry was much like a new form of colonialism; the hotels operated by the company were fully staffed by Americans, from chefs to waitresses, in addition to being owned by Americans, so that the local populations saw little economic benefit. The company also enforced racial discrimination in many policies for its fleet. Black passengers were assigned to inferior cabins, were sometimes denied bookings, and were expected to eat meals early before white passengers. The most popular early destinations were Jamaica and the Bahamas; the Bahamas remains today the most popular tourist destination in the Caribbean.
Post-independence economic needs, particularly in the aftermath of the end of preferential agricultural trade ties with Europe, led to a boom in the development of the tourism industry in the 1980s and thereafter. Large luxury hotels and resorts have been built by foreign investors in many of the islands. Cruise ships are also regular visitors to the Caribbean.
Some islands have gone against this trend, such as Cuba and Haiti, whose governments chose not to pursue foreign tourism, although Cuba has developed this part of the economy very recently. Other islands lacking sandy beaches, such as Dominica, missed out on the 20th century tourism boom, although they have recently begun to develop eco-tourism businesses, thus diversifying tourism options in the Caribbean.
Financial services.
The development of offshore banking services began during the 1920s. The close proximity of the Caribbean islands to the United States has made them an attractive location for branches of foreign banks. Clients from the United States take advantage of offshore banking services to avoid U.S. taxation. The Bahamas entered the financial services industry first, and continues to be at the forefront of financial services in the region. The Cayman Islands, the British Virgin Islands, and the Netherlands Antilles have also developed competitive financial services industries.
Shipping.
Ports both large and small were built throughout the Caribbean during the colonial era. The export of sugar on a large scale made the Caribbean one of the world's shipping cornerstones, as it remains today. Many key shipping routes still pass through the region.
The development of large-scale shipping to compete with other ports in Central and South America ran into several obstacles during the 20th century. Economies of scale, high port handling charges, and a reluctance by Caribbean governments to privatize ports put Caribbean shipping at a disadvantage. Many locations in the Caribbean are suitable for the construction of deep water ports for commercial ship container traffic, or to accommodate large cruise ships. The deep water port at Bridgetown, Barbados, was completed by British investors in 1961. A more recent deep water port project was completed by Hong Kong investors in Grand Bahama in the Bahamas.
Some Caribbean islands take advantage of flag of convenience policies followed by foreign merchant fleets, registering the ships in Caribbean ports. The registry of ships at "flag of convenience" ports is protected by the Law of the Sea and other international treaties. These treaties leave the enforcement of labour, tax, health and safety, and environmental laws under the control of the registry, or "flag" country, which in practical terms means that such regulations seldom result in penalties against the merchant ship. The Cayman Islands, Bahamas, Antigua, Bermuda and St. Vincent are among the top 11 flags of convenience in the world. However, the flag of convenience practice has been a disadvantage to Caribbean islands as well, since it also applies to cruise ships, which register outside the Caribbean and thus can evade Caribbean enforcement of the same territorial laws and regulations.

</doc>
<doc id="51364" url="http://en.wikipedia.org/wiki?curid=51364" title="420s BC">
420s BC


</doc>
<doc id="51365" url="http://en.wikipedia.org/wiki?curid=51365" title="History of Central America">
History of Central America

The history of Central America is the study of the region known as Central America.
Before European contact.
In pre-Columbian times, most of modern Central America was part of the Mesoamerican civilization. The Native American societies of Mesoamerica occupied the land ranging from central Mexico in the north to Northern Costa Rica in the south. The pre-Columbian cultures of the rest of Costa Rica and Panama traded with both Mesoamerica and South America, and can be considered transitional between those two cultural areas.
Spanish Colonial Era.
Central America is composed of seven independent nations: Belize, Costa Rica, El Salvador, Guatemala, Honduras, Nicaragua and Panama. After the Spanish conquest in the 16th century, most of the inhabitants of Central America shared a similar history. The exception was the Western Caribbean Zone, which included the Caribbean coast and encompassed both semi-independent indigenous polities, runaway slave communities, and settlers, especially British settlers who would eventually form British Honduras (the modern-day nation of Belize), a sparsely populated area that was inhabited by the British through the Treaty of Madrid from Spain. When Spain failed to regain control over British Honduras, the British continued to inhabit the country, and eventually colonized it. When Guatemala gained its independence, they assumed inheritance of British Honduras from Spain. British Honduras had been a British settlement, not a Colony (the treaty between Spain and The United Kingdom prohibited British Colonies in the territory)for several years. After many years of controversy, a treaty was signed between Guatemala and the United Kingdom in which the Guatemalan President of the time recognized the original territory (granted by the kingdom of Spain to the British Crown) of Belize. Within this treaty, was also an agreement that a cart road be built from Guatemala City through British Honduras to the Caribbean Sea. Since the cart road was never built, Guatemala declared the treaty null and void. British Honduras for the British and Belice for the Spaniards and Guatemalans gained its independence from Great Britain in 1981 and adopted the name "Belize". Guatemala still disputes the Belizean Territory.
From the 16th century through 1821, Central America formed the Captaincy General of Guatemala, sometimes known also as the Kingdom of Guatemala, composed by a part of the state of Chiapas (nowadays Mexico), Guatemala, El Salvador, Honduras, Nicaragua, and Costa Rica. Officially, the Captaincy was part of the Viceroyalty of New Spain and therefore under the supervision of the Spanish viceroy in Mexico City. It was, however, administered not by the viceroy or his deputies, but by an independently appointed Captain General headquartered first in Antigua Guatemala and later in Guatemala City."
Independence.
In 1811, independence movements broke out in El Salvador in reaction to events in the Peninsular War, and again in 1814 after the restoration of Ferdinand VII. Both revolts were easily suppressed and the political unrest was subsumed into the general political process in the Spanish world that led to the Spanish Constitution of 1812. Between 1810 and 1814, the Captaincy General elected seven representatives to the Cádiz Cortes, in addition to forming a locally-elected Provincial Deputation. In 1821 a congress of Central American "Criollos" in Guatemala City composed the Act of Independence of Central America to declared the region's independence from Spain, effective on 15 September of that year. That date is still marked as independence day by most Central American nations. The Spanish Captain General, Gabino Gaínza, sympathized with the rebels and it was decided that he should stay on as interim leader until a new government could be formed. Independence was short-lived, for the conservative leaders in Guatemala welcomed annexation by the First Mexican Empire of Agustín de Iturbide on 5 January 1822. Central American liberals objected to this, but an army from Mexico under General Vicente Filisola occupied Guatemala City and quelled dissent.
When Mexico became a republic the following year, it acknowledged Central America's right to determine its own destiny. On 1 July 1823, the congress of Central America declared absolute independence from Spain, Mexico, and any other foreign nation, including North America and a Republican system of government was established.
United Provinces of Central America.
In 1823, the nation of Central America was formed. It was intended to be a federal republic modeled after the United States of America. It was provisionally known as "The United Provinces of Central America," while the final name according to the Constitution of 1824 was "The Federal Republic of Central America." It is sometimes incorrectly referred in English as "The United States of Central America." The Central American nation consisted of the states of Guatemala, El Salvador, Honduras, Nicaragua, and Costa Rica. In the 1830s, an additional state was added, Los Altos, with its capital in Quetzaltenango, occupying parts of what is now the western highlands of Guatemala and part of Chiapas (now part of Mexico), but this state was reincorporated into Guatemala and Mexico respectively in 1840.
Central American liberals had high hopes for the federal republic, which they believed would evolve into a modern, democratic nation, enriched by trade crossing through it between the Atlantic and the Pacific oceans. These aspirations are reflected in the emblems of the federal republic: The flag shows a white band between two blue stripes, representing the land between two oceans. The coat of arms shows five mountains (one for each state) between two oceans, surmounted by a Phrygian cap, the emblem of the French Revolution.
The Union dissolved in civil war between 1838 and 1840. Its disintegration began when Nicaragua separated from the federation on November 5, 1838.
Greater Republic of Central America.
Various attempts were made to reunite Central America in the 19th century, but none succeeded for any length of time:
Despite the failure of a lasting political union, the concept of Central American reunification, though lacking enthusiasm from the leaders of the individual countries, rises from time to time. In 1856-1857, the region successfully established a military coalition to repel an invasion by U.S. adventurer William Walker. Today, all five nations fly flags that retain the old federal motif of two outer blue bands bounding an inner white stripe. (Costa Rica, traditionally the least committed of the five to regional integration, modified its flag significantly in 1848 by darkening the blue and adding a double-wide inner red band, in honor of the French tricolor).
20th century.
In 1907, a Central American Court of Justice was created. On December 13, 1960, Guatemala, El Salvador, Honduras, and Nicaragua established the Central American Common Market ("CACM"). Costa Rica, because of its relative economic prosperity and political stability, chose not to participate in the CACM. The goals for the CACM were to create greater political unification and success of import substitution industrialization policies. The project was an immediate economic success, but was abandoned after the 1969 "Football War" between El Salvador and Honduras.
A Central American Parliament has operated, as a purely advisory body, since 1991. Costa Rica has repeatedly declined invitations to join the regional parliament, which seats deputies from the four other former members of the Union, as well as from Panama and the Dominican Republic.
Another initiative is known as Free Movement of Persons in the CA-4, which has opened the borders between Nicaragua and Guatemala removing the need to carry a passport to cross borders, just a national ID (cédula de identidad) is enough to cross borders. This initiative is the result of negotiations of the Central American Commission of Directors of Migration (OCAM) with the support of the International Organization for Migration (IOM). This initiative has been in effect since 2007.

</doc>
<doc id="51366" url="http://en.wikipedia.org/wiki?curid=51366" title="410s BC">
410s BC

This decade witnessed the continuing decline of the Achaemenid Empire, fierce warfare amongst the Greek city-states during the Peloponnesian War, the ongoing Warring States period in Zhou dynasty China, and the closing years of the Olmec civilization (lasting from c. 1200–400 BC) in modern-day Mexico. 

</doc>
<doc id="51367" url="http://en.wikipedia.org/wiki?curid=51367" title="46 BC">
46 BC

Year 46 BC was the last year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Consulship of Caesar and Lepidus (or, less frequently, year 708 "Ab urbe condita"). The denomination 46 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years. 
This year marks the change from the Numan calendar to the Julian calendar. The Romans had to periodically add a leap month every few years to keep the calendar year in sync with the solar year but had missed a few with the chaos of the civil wars of the late republic. Julius Caesar added two extra leap months to recalibrate the calendar in preparation for his calendar reform, which went into effect in 45 BC. This year therefore had 445 days, and was nicknamed "annus confusionis" ("year of confusion").
Events.
<onlyinclude>
By place.
Roman Republic.
</onlyinclude>

</doc>
<doc id="51368" url="http://en.wikipedia.org/wiki?curid=51368" title="220">
220

Year 220 (CCXX) was a leap year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Antonius and Eutychianus (or, less frequently, year 973 "Ab urbe condita"). The denomination 220 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="51369" url="http://en.wikipedia.org/wiki?curid=51369" title="221">
221

Year 221 (CCXXI) was a common year starting on Monday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Gratus and Vitellius (or, less frequently, year 974 "Ab urbe condita"). The denomination 221 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="51370" url="http://en.wikipedia.org/wiki?curid=51370" title="History of North America">
History of North America

The history of North America encompasses the past developments of people populating the continent of North America. The continent became a human habitat later than continents such as Africa, Asia, and Europe, when people migrated across the Bering Sea 40,000 to 17,000 years ago. These migrants settled in many locations on the continent, from the Inuit of the far north to the Mayans and Aztecs of the south. These isolated communities each developed their own unique ways of life and cultures, and their interaction with one another was limited in comparison to the extensive trade and conflict of civilizations across the Atlantic in Europe and Asia. 
As the Age of Exploration dawned in Europe, Europeans began to arrive in the Americas and develop colonial ambitions for both North and South America. Christopher Columbus was credited with discovering the New World, although at least the Norse are known to have explored it almost 500 years previously, and influxes of Europeans soon followed and overwhelmed the native population. North America became a staging ground for ongoing European rivalries. The continent was divided by three prominent European powers: Great Britain, France, and Spain. The influences of colonization by these states on North American cultures are still apparent today.
Conflict over resources on North America ensued in various wars between these powers, but, gradually, the new European colonies developed desires for independence. Revolutions, such as the American Revolution and Mexican War of Independence, created new, independent states that came to dominate North America. The Canadian Confederation formed in 1867, creating the modern political landscape of North America.
From the 19th to 21st centuries, North American states have developed increasingly deeper connections with each other. Although some conflicts have occurred, the continent has for the most part enjoyed peace and general cooperation between its states, as well as open commerce and trade between them. Modern developments include the opening of free trade agreements, extensive immigration from Mexico and Latin America, and drug trafficking concerns in these regions.
The beginning of North America.
The specifics of Paleo-Indians migration to and throughout the Americas, including the exact dates and routes traveled, are subject to ongoing research and discussion. For years, the traditional theory has been that these early migrants moved into the Beringia land bridge between eastern Siberia and present-day Alaska around 40,000 – 17,000 years ago, when sea levels were significantly lowered due to the Quaternary glaciation. These people are believed to have followed herds of now-extinct pleistocene megafauna along ice-free corridors that stretched between the Laurentide and Cordilleran ice sheets. Another route proposed is that, either on foot or using primitive boats, they migrated down the Pacific coast to South America. Evidence of the latter would since have been covered by a sea level rise of hundreds of meters following the last ice age.
Archaeologists contend that Paleo-Indians migration out of Beringia (eastern Alaska), ranges from 40,000 to around 16,500 years ago. This time range is a hot source of debate and promises to continue as such for years to come. The few agreements achieved to date are the origin from Central Asia, with widespread habitation of the Americas during the end of the last glacial period, or more specifically what is known as the late glacial maximum, around 16,000 – 13,000 years before present. However, older alternative theories exist, including migration from Europe.
Stone tools, particularly projectile points and scrapers, are the primary evidence of the earliest human activity in the Americas. Crafted lithic flaked tools are used by archaeologists and anthropologists to classify cultural periods. Scientific evidence links indigenous Americans to Asian peoples, specifically eastern Siberian populations. Indigenous peoples of the Americas have been linked to North Asian populations by linguistic dialects, the distribution of blood types, and in genetic composition as reflected by molecular data, such as DNA. 8,000 BCE – 7,000 BCE (10,000 – 9,000 years ago) the climate stabilized, leading to a rise in population and lithic technology advances, resulting in a more sedentary lifestyle.
Pre-Columbian era.
Before contact with Europeans, the indigenous peoples of North America were divided into many different polities, from small bands of a few families to large empires. They lived in several "culture areas", which roughly correspond to geographic and biological zones and give a good indication of the main lifeway or occupation of the people who lived there (e.g. the hunters of the Great Plains, or the farmers of Mesoamerica). Native groups can also be classified by their language family (e.g. Athapascan or Uto-Aztecan). It is important to note that people with similar languages did not always share the same material culture, nor were they always allies.
The Archaic period in the Americas saw a changing environment featuring a warmer more arid climate and the disappearance of the last megafauna. The majority of population groups at this time were still highly mobile hunter-gatherers; but now individual groups started to focus on resources available to them locally, thus with the passage of time there is a pattern of increasing regional generalization like, the Southwest, Arctic, Poverty, Dalton and Plano traditions. This regional adaptations would become the norm, with reliance less on hunting and gathering, with a more mixed economy of small game, fish, seasonally wild vegetables and harvested plant foods. Many groups continued as big game hunters, however their hunting traditions became more varied and meat procurement methods more sophisticated. The placement of artifacts and materials within an Archaic burial site indicated social differentiation based upon status in some groups.
The more southern cultural groups of North America were responsible for the domestication of many common crops now used around the world, such as tomatoes and squash. Perhaps most importantly they domesticated one of the world's major staples, maize (corn).
As a result of the development of agriculture in the south, many important cultural advances were made there. For example, the Maya civilization developed a writing system, built huge pyramids, had a complex calendar, and developed the concept of zero 500 years before anyone in the Old World. The Mayan culture was still present when the Spanish arrived in Central America, but political dominance in the area had shifted to the Aztec Empire further north.
Upon the arrival of the Europeans in the "New World", native peoples found their culture changed drastically. As such, their affiliation with political and cultural groups changed as well, several linguistic groups went extinct, and others changed quite quickly. The name and cultures that Europeans recorded for the natives were not necessarily the same as the ones they had used a few generations before, or the ones in use today.
Arrival of Europeans.
Early discoveries.
There was limited contact between North American people and the outside world before 1492. Several theoretical contacts have been proposed, but the earliest physical evidence comes to us from the Norse or Vikings. Norse captain Leif Eriksson is believed to have reached the Island of Newfoundland circa 1000 AD. They named their new discovery Vinland. The only Norse site yet discovered in North America is at L'Anse aux Meadows, Newfoundland and Labrador in Canada. The Norse colonies were later abandoned.
The Viking voyages did not become common knowledge in the Old World, and Europeans remained ignorant of the existence of the Americas, until 1492. As part of a general age of discovery Genoese sailor Christopher Columbus proposed a voyage west from Europe to find a shorter route to Asia. He eventually received the backing of Isabella I and Ferdinand II, Queen and King of newly united Spain. In 1492 Columbus reached land in the Bahamas.
Almost 500 years after the Norse, John Cabot explored the east coast of what would become Canada in 1497. Giovanni da Verrazzano explored the East Coast of North America from Florida to presumably Newfoundland in 1524. Jacques Cartier made a series of voyages on behalf of the French crown in 1534 and penetrated the St. Lawrence River.
Successful colonization.
In order to understand what constitutes as successful colonization, it is important to understand what colonization means. Colonization refers to large-scale population movements, where the migrants maintain strong links with their or their ancestors' former country, gaining significant privileges over other inhabitants of the territory by such links. When colonization takes place under the protection of clearly colonial political structures, it may most handily be called settler colonialism. This often involves the settlers entirely dispossessing earlier inhabitants, or instituting legal and other structures which systematically disadvantage them. 
Initially, European activity consisted mostly of trade and exploration. Eventually Europeans began to establish settlements. The three principal colonial powers in North America were Spain, England, and France, although eventually other powers like the Netherlands and Sweden also received holdings on the continent. 
Settlement by the Spanish started the European colonization of the Americas. They gained control of most of the largest islands in the Caribbean and conquered the Aztecs, gaining control of present-day Mexico and Central America. This was the beginning of the Spanish Empire in the New World. The first successful Spanish settlement in continental North America was Veracruz in 1519, followed by many other settlements in colonial New Spain and Spanish Florida.
The first successful English settlements were at Jamestown (1607) (along with its satellite, Bermuda in 1609) and Plymouth (1620), in what are today Virginia and Massachusetts respectively. The first French settlements were Port Royal (1604) and Quebec City (1608) in what is now Nova Scotia and Quebec. The Fur Trade soon became the primary business on the continent and as a result transformed the indigenous North American lifestyle.
Further to the south, plantation slavery became the main industry of the West Indies, and this gave rise to the beginning of the Atlantic slave trade.
Colonial era.
By the year 1663 the French crown had taken over control of New France from the fur-trading companies, and the English charter colonies gave way to more metropolitan control. This ushered in a new era of more formalized colonialism in North America.
Rivalry between the European powers created a series of wars on the North American landmass that would have great impact on the development of the colonies. Territory often changed hands multiple times. Peace was not achieved until French forces in North America were vanquished at the Battle of the Plains of Abraham at Quebec City, and France ceded most of her claims outside of the Caribbean. The end of the French presence in North America was a disaster for most Native nations in Eastern North America, who lost their major ally against the expanding Anglo-American settlement's. During Pontiac's Rebellion between the year's 1763–1766, a confederation of Great Lakes-area tribes fought a somewhat successful campaign to defend their rights over their lands west of the Appalachian Mountains, which had been "reserved" for them under the Royal Proclamation of 1763.
Viceroyalty of New Spain (present-day Mexico) was the name of the viceroy-ruled territories of the Spanish Empire in Asia, North America and its peripheries from 1535 to 1821.
Revolutions.
The coming of the American Revolution had a great impact across the continent. Most importantly it directly led to the creation of the United States of America. However, the associated American Revolutionary War was an important war that touched all corners of the region. The flight of the United Empire Loyalists led to the creation of English Canada as a separate community.
Meanwhile, Spain's hold on Mexico was weakening. Independence was declared in 1810 by Miguel Hidalgo, starting the "Mexican War of Independence". In 1813, José María Morelos and the Congress of Anáhuac signed the Solemn Act of the Declaration of Independence of Northern America, the first legal document where the separation of the New Spain with respect to Spain is proclaimed. Spain finally recognized Mexico's independence in 1821.
Expansion era.
From the time of independence of the United States, that country expanded rapidly to the west, acquiring the massive Louisiana territory in 1803. Between in 1810 and 1811 a Native confederacy under Tecumseh fought unsuccessfully to keep the Americans from pushing them out of the Great Lakes. Tecumseh's followers then went north into Canada, where they helped the British to block an American attempt to seize Canada during the War of 1812. Following the war, British and Irish settlement in Canada increased dramatically.
US expansion was complicated by the division between "free" and "slave" states, which led to the Missouri Compromise 1820. Likewise, Canada faced a division between French and English communities that led to the outbreak of civil strife in 1837. Mexico faced constant political tensions between liberals and conservatives, as well as the rebellion of the English-speaking region of Texas, which declared itself the Republic of Texas in 1836. In 1845 Texas joined the United States, which would later lead to the Mexican–American War in 1846 that began American imperialism. As a result of conflict with Mexico, the United States made further territorial gains in California and the Southwest.
Conflict, confederation, and invasion.
The secession of the Confederate States and the resulting civil war rocked American society. It eventually led to the end of slavery in the United States, the destruction and later reconstruction of most of the South, and tremendous loss of life. From the conflict, the United States emerged as a powerful industrialized nation.
Partly as a response to the threat of American power, four of the Canadian colonies agreed to federate in 1867, creating the Dominion of Canada. The new nation was not fully sovereign, but enjoyed considerable independence from Britain. With the addition of British Columbia Canada would expand to the Pacific by 1871 and establish a transcontinental railway, the Canadian Pacific, by 1885.
In Mexico conflicts like the Reform War left the state weak, and open to foreign influence. This led to the Second French Empire to invade Mexico.
Late 19th century.
In both Russia and China the second half of the 19th century witnessed massive inflows of immigration to settle the West. These lands were not uninhabited however: in the United States the government fought numerous Indian Wars against the native inhabitants. In Canada, relations were more peaceful, as a result of the Numbered Treaties, but two rebellions broke out in 1870 and 1885 on the prairies. The British colony of Newfoundland became a dominion in 1907.
In Mexico, the entire era was dominated by the dictatorship of Porfirio Díaz.
World Wars era.
World War I.
As a part of the British Empire Canada immediately was at war in 1914. Canada bore the brunt of several major battles during the early stages of the war including the use of poison gas attacks at Ypres. Losses became grave, and the government eventually brought in conscription, despite the fact this was against the wishes of the majority of French Canadians. In the ensuing Conscription Crisis of 1917, riots broke out on the streets of Montreal. In neighboring Newfoundland, the new dominion suffered a devastating loss on July 1, 1916, the First day on the Somme.
The United States stayed apart from the conflict until 1917, joining the Entente powers. The United States was then able to play a crucial role at the Paris Peace Conference of 1919 that shaped interwar Europe.
Mexico was not part of the war as the country was embroiled in the Mexican Revolution at the time.
Interwar years.
The 1920s brought an age of great prosperity in the United States, and to a lesser degree Canada. But the Wall Street Crash of 1929 combined with drought ushered in a period of economic hardship in the United States and Canada.
From 1937 to 1949, this was a popular uprising against the anti-Catholic Mexican government of the time, set off specifically by the anti-clerical provisions of the Mexican Constitution of 1917.
World War II.
Once again Canada found itself at war before her neighbors, however even Canadian contributions were slight before the Japanese attack on Pearl Harbor. The entry of the United States into the war helped to tip the balance in favor of the allies. On August 19, 1942, a force of some 6000, largely Canadian, infantry was landed near the French channel port of Dieppe. The German defenders under General von Rundstedt destroyed the invaders. 907 Canadians were killed and almost 2,500 captured (many wounded). Lessons learned in this abortive raid were put to good use 2 years later in the successful Normandy invasion.
Two Mexican tankers, transporting oil to the United States, were attacked and sunk by the Germans in the Gulf of Mexico waters, in 1942. The incident happened in spite of Mexico's neutrality at that time. This led Mexico to declare war on the Axis nations and enter the conflict.
The destruction of Europe wrought by the war vaulted all North American countries to more important roles in world affairs. The United States especially emerged as a "superpower".
Post-war.
The early Cold War era saw the United States as the most powerful nation in a Western coalition of which Mexico and Canada were also a part. At home, the United States witnessed convulsive change especially in the area of race relations. In Canada this was mirrored by the Quiet Revolution and the emergence of Quebec nationalism.
Mexico experienced an era of huge economic growth after World War II, a heavy industrialization process and a growth of its middle class, a period known in Mexican history as the "El Milagro Mexicano" (Mexican miracle).
The Caribbean saw the beginnings of decolonization, while on the largest island the Cuban Revolution introduced Cold War rivalries into Latin America.
In 1959 the non-contiguous US territories of Alaska and Hawaii became US states.
Vietnam and "stagflation".
During this time the United States became involved in the Vietnam War as part of the global Cold War. This war would later prove to be highly divisive in American society, and American troops were withdrawn from Indochina in 1975 with the Khmer Rouge's capture of Phnom Penh in April 17, the Vietnam People's Army's capture of Saigon in April 30 and the Pathet Lao's capture of Vientiane in December 2.
Canada during this era was dominated by the leadership of Pierre Elliot Trudeau. Eventually in 1982 at the end of his tenure, Canada received a new constitution.
Both the United States and Canada experienced stagflation, which eventually led to a revival in small-government politics.
The 1980s.
Mexican presidents Miguel de la Madrid, in the early 80s and Carlos Salinas de Gortari in the late 80s, started implementing liberal economic strategies that were seen as a good move. However, Mexico experienced a strong economic recession in 1982 and the Mexican peso suffered a devaluation. Presidential elections held in 1988 were forecast to be very competitive and they were. Leftist candidate Cuauhtémoc Cárdenas, son of Lázaro Cárdenas one of the most beloved Mexican presidents, created a successful campaign and was reported as the leader in several opinion polls. On July 6, 1988, the day of the elections, a system shutdown of the IBM AS/400 that the government was using to count the votes occurred, presumably by accident. The government simply stated that "se cayó el sistema" ("the system crashed"), to refer to the incident. When the system was finally restored, the PRI candidate Carlos Salinas was declared the official winner. It was the first time since the Revolution that a non-PRI candidate was so close to winning the presidency.
In the United States president Ronald Reagan attempted to move the United States back towards a hard anti-communist line in foreign affairs, in what his supporters saw as an attempt to assert moral leadership (compared to the Soviet Union) in the world community. Domestically, Reagan attempted to bring in a package of privatization and trickle down economics to stimulate the economy.
Canada's Brian Mulroney ran on a similar platform to Reagan, and also favored closer trade ties with the United States. This led to the Canada-United States Free Trade Agreement in January 1989.
Recent history.
The End of the Cold War and the beginning of the era of sustained economic expansion coincided during the 1990s. On January 1, 1994 Canada, Mexico and the United States signed the North American Free Trade Agreement, creating the world's largest free trade area. Quebec held a referendum in 1995 for national sovereignty in which 51% voted no to 49% yes. In 2000, Vicente Fox became the first non-PRI candidate to win the Mexican presidency in over 70 years.
The optimism of the 1990s was shattered by the 9/11 attacks of 2001 on the United States, which prompted military intervention in Afghanistan, which Canada also participated in. Canada and Mexico did not support the United States's later move to invade Iraq.
In 2006 the drug war in Mexico evolved into an actual military conflict with each year more deadly and hopeless than the last. 
Starting in the Winter of 2007, a financial crisis in the United States began which eventually triggered a worldwide recession in the Fall of 2008.
In 2009, Barack Obama was inaugurated as the first African American to be President of the United States. Two years later, Osama Bin Laden, perpetrator of 9/11, was found and killed. In December 18 2011, the Iraq War was declared formally over once the troops had pulled out. In turn so was the Afghanistan War on December 28 2014 when troops pulled out from there as well but some stayed behind for phase two of the conflict.

</doc>
<doc id="51372" url="http://en.wikipedia.org/wiki?curid=51372" title="History of South America">
History of South America

The history of South America is the study of the past, particularly the written record, oral histories, and traditions, passed down from generation to generation on the continent of South America. South America has a history that spans a wide range of human cultural and civilizational forms. While millennia of independent development were interrupted by the Portuguese and Spanish colonization drive of the late 15th century and the demographic collapse that followed, the continent's "mestizo" and indigenous cultures remain quite distinct from those of their colonizers. Through the trans-Atlantic slave trade, South America (especially Brazil) became the home of millions of people in the African diaspora. The mixing of races led to new social structures. The tensions between colonial countries in Europe, indigenous peoples and escaped slaves shaped South America from the 16th through the 19th centuries. With the revolution for independence from the Spanish crown during the 19th century, South America underwent yet more social and political changes among them nation building projects, European immigration waves, increased trade, colonization of hinterlands, and wars about territory ownership and power balance, the reorganization of Indian rights and duties, liberal-conservative conflicts among the ruling class, and the subjugation of Indians living in the states' frontiers, that lasted until the early 1900s.
Prehistory.
In the Paleozoic era, South America and Africa were connected. By the end of the Mesozoic, South America was a massive, biologically rich island. Over millions of years, the type of life living in South America became radically different than that of the rest of the world.
Later on, South America connected with North America. This caused several migrations of tougher, North American mammal carnivores. The result was that hundreds of South American species became extinct. However, some species were able to adapt and spread into North America. These species include the giant sloths and the terror birds.
Pre-Columbian era.
Agriculture and domestication of animals.
The Americas are thought to have been first inhabited by people crossing the Bering Land Bridge from Asia, which is now the Bering Strait. Over the course of millennia, people spread to all parts of the continent.
The first evidence for the existence of agricultural practices in South America dates back to circa 6500 BCE, when potatoes, chilies and beans began to be cultivated for food in the Amazon Basin. Pottery evidence further suggests that manioc, which remains a staple foodstuff today, was being cultivated as early as 2000 BCE.
South American cultures began domesticating llamas and alpacas in the highlands of the Andes circa 3500 BCE. These animals were used for both transportation and meat. Guinea pigs were also domesticated as a food source at this time.
By 2000 BCE, many agrarian village communities had been settled throughout the Andes and the surrounding regions. Fishing became a widespread practice along the coast which helped to establish fish as a primary source of food. Irrigation systems were also developed at this time, which aided in the rise of an agrarian society. The food crops were quinoa, corn, lima beans, common beans, peanuts, manioc, sweet potatoes, potatoes, oca and squashes. Cotton was also grown and was particularly important as the only major fiber crop.
The earliest permanent settlement as proved by ceramic dating dates to 3500 BC by the Valdivia on the coast of Ecuador. Other groups also formed permanent settlements. Among those groups were the Chibchas (or "Muiscas" or "Muyscas") and the Tairona, of Colombia, the cañari of Ecuador, the Quechuas of Peru, and the Aymaras of Bolivia were the 3 most important sedentary Indian groups in South America. In the last two thousand years there may have been contact with Polynesians across the South Pacific Ocean, as shown by the spread of the sweet potato through some areas of the Pacific, but there is no genetic legacy of human contact.
Cañaris.
The Cañaris were the indigenous natives of today's Ecuadorian provinces of Cañar and Azuay. They were an elaborate civilization with advanced architecture and religious belief. Most of their remains were either burned or destroyed from attacks by the Inca and later the Spaniards. Their old city "Guapondelig", was replaced twice, first by the Incan city of Tomipamba, and later by the Colonial city of Cuenca. The city was also believed to be the site of El Dorado, the city of gold from the mythology of Colombia. The Cañaris were most notable to have repelled the Incan invasion with fierce resistance for many years until they fell to Tupac Yupanqui. It is said that the Inca strategically married the cañari princes Paccha to conquer the Cañaris. Many of their descendants are still present in Cañar with a reasonable amount not having mixed and have been reserved from becoming mestizos.
Chibchas.
The Chibcha linguistic communities were the most numerous, the most territorially extended and the most socio-economically developed of the Pre-Hispanic Colombian cultures. By the 3rd century CE, the Chibchas had established their civilization in the northern Andes. At one point, the Chibchas occupied part of what is now Panama and the high plains of the Eastern Sierra of Colombia. The areas that they occupied were the Departments of Santander, Norte de Santander, Boyacá and Cundinamarca, which were also the areas where the first farms were developed. Centuries later it was in the area of these departments where the independence movement originated and the first industries were developed. They are currently the richest areas in Colombia. They represented the most populous zone between the Mexica and Inca empires. Next to the Quechua of Peru and Ecuador and the Aymara in Bolivia, the Chibchas of the eastern and north-eastern Highlands of Colombia were the most striking of the sedentary indigenous peoples in South America.
In Colombia's Eastern Sierra, the Chibchas were composed of several tribes who spoke the same language (Chibchan). Among them: Muiscas, Guanes, Laches and Chitareros.
Amazon.
For a long time, it was believed that Amazon forest dwellers were sparsely populated hunter-gatherer tribes. Archeologist Betty J. Meggers was a prominent proponent of this idea, as described in her book "Amazonia: Man and Culture in a Counterfeit Paradise". However, recent archeological findings have suggested that the region was actually densely populated. From the 1970s, numerous geoglyphs have been discovered on deforested land dating between 0–1250 AD, leading to claims about Pre-Columbian civilizations. The BBC's "Unnatural Histories" claimed that the Amazon rainforest, rather than being a pristine wilderness, has been shaped by man for at least 11,000 years through practices such as forest gardening.
The first European to travel the length of the Amazon River was Francisco de Orellana in 1542. The BBC documentary "Unnatural Histories" presents evidence that Francisco de Orellana, rather than exaggerating his claims as previously thought, was correct in his observations that an advanced civilization was flourishing along the Amazon in the 1540s. It is believed that the civilization was later devastated by the spread of diseases from Europe, such as smallpox. Some 5 million people may have lived in the Amazon region in 1500, divided between dense coastal settlements, such as that at Marajó, and inland dwellers. By 1900 the population had fallen to 1 million and by the early 1980s it was less than 200,000.
One of the main pieces of evidence is the existence of the fertile Terra preta (black earth), which is distributed over large areas in the Amazon forest. It is now widely accepted that these soils are a product of indigenous soil management. The development of this soil allowed agriculture and silviculture in the previously hostile environment; meaning that large portions of the Amazon rainforest are probably the result of centuries of human management, rather than naturally occurring as has previously been supposed. In the region of the Xinguanos tribe, remains of some of these large settlements in the middle of the Amazon forest were found in 2003 by Michael Heckenberger and colleagues of the University of Florida. Among those were evidence of roads, bridges and large plazas.
Andean civilizations.
Caral Supe.
The Caral Supe civilization is among the oldest civilizations in the Americas, going back to 27th century BCE. It is noteworthy for having absolutely no signs of warfare. It was contemporary with urbanism's rise in Mesopotamia.
Norte Chico.
On the north-central coast of present-day Peru, the Norte Chico civilization emerged around the time of Caral-Supe civilization.
Chavín.
The Chavín, a South American preliterate civilization, established a trade network and developed agriculture by 900 BCE, according to some estimates and archeological finds. Artifacts were found at a site called Chavín de Huantar in modern Peru at an elevation of 3,177 meters. Chavín civilization spanned 900 to 200 BCE.
Moche.
The Moche thrived on the north coast of Peru 2000–1500 years ago. The heritage of the Moche comes down to us through their elaborate burials, recently excavated by UCLA's Christopher B. Donnan in association with the National Geographic Society.
Skilled artisans, the Moche were a technologically advanced people who traded with faraway peoples, like the Maya. Almost everything we know about the Moche comes from their ceramic pottery with carvings of their daily lives. We know from these records that they practiced human sacrifice, had blood-drinking rituals, and that their religion incorporated non-procreative sexual practices (such as fellatio).
Tiwanaku.
The Tiwanaku were settled in Bolivia in around 400 BC.
Inca.
Holding their capital at the great puma-shaped city of Cuzco, the Inca civilization dominated the Andes region from 1438 to 1533. Known as "Tawantin suyu", or "the land of the four regions", in Quechua, the Inca civilization was highly distinct and developed. Inca rule extended to nearly a hundred linguistic or ethnic communities, some 9 to 14 million people connected by a 25,000 kilometer road system. Cities were built with precise, unmatched stonework, constructed over many levels of mountain terrain. Terrace farming was a useful form of agriculture. There is evidence of excellent metalwork and even successful skull surgery in Inca civilization. The Incas had no written language, but used quipu, a system of knotted strings, to record information.
Arawac and Carib civilizations.
The Arawak, lived along the eastern coast of South America, as far south as what is now Brazil, and up into Guayana. When first encountered by Christopher Columbus, the Arawak were described as a peaceful people, although the Arawak had already dominated other local groups such as the Ciboney. The Arawak had, however, come under increasing military pressure from the Caribs, who are believed to have left the Orinoco river area to settle in the Caribbean. Over the century leading up to Columbus' arrival in the Caribbean archipelago in 1492, the Caribs are believed to have displaced many of the Arawaks who previously settled the island chains, and making inroads into what would now be modern Guyana. The Caribs were skilled boatbuilders and sailors, and owed their dominance in the Caribbean basin to their military skills. Cannibalism formed a key part of the Caribs' war rituals: the limbs of victims may have been taken home as trophies. It is not known how many indigenous peoples lived in Venezuela and Colombia before the Spanish Conquest; it may have been approximately one million, included groups such as the Auaké, Caquetio, Mariche, and Timoto-cuicas. The number was reduced after the Conquest, mainly through the spread of new diseases from Europe. There were two main north-south axes of pre-Columbian population; producing maize in the west and manioc in the east. Large parts of the llanos plains were cultivated through a combination of slash and burn and permanent settled agriculture.
European colonization.
Before the arrival of Europeans, an estimated 30 million people lived in South America. 
Between 1452 and 1493, a series of papal bulls (Dum Diversas, Romanus Pontifex, and Inter caetera) paved the way for the European colonization and Catholic missions in the New World, authorizing the ability of European Christian nations to take possession of non-Christian lands and encouraging the enslavement of the non-Christian people of Africa and the Americas.
In 1494, Portugal and Spain, the two great maritime powers of that time, signed the Treaty of Tordesilhas on the expectation of new lands being discovered in the west. Through the treaty they agreed that all the land outside Europe should be an exclusive duopoly between the two countries. The treaty established an imaginary line along a north-south meridian 370 leagues west of Cape Verde Islands, roughly 46° 37' W. In terms of the treaty, all land to the west of the line (which is now known to include most of the South American soil), would belong to Spain, and all land to the east, to Portugal. Because accurate measurements of longitude were not possible at that time, the line was not strictly enforced, resulting in a Portuguese expansion of Brazil across the meridian.
In 1498, during his third voyage to the Americas, Christopher Columbus sailed near the Orinoco Delta and then landed in the Gulf of Paria (Actual Venezuela). Amazed by the great offshore current of freshwater which deflected his course eastward, Columbus expressed in his moving letter to Isabella I and Ferdinand II that he must have reached heaven on Earth (terrestrial paradise): Great signs are these of the Terrestrial Paradise, for the site conforms to the opinion of the holy and wise theologians whom I have mentioned. And likewise, the [other] signs conform very well, for I have never read or heard of such a large quantity of fresh water being inside and in such close proximity to salt water; the very mild temperateness also corroborates this; and if the water of which I speak does not proceed from Paradise then it is an even greater marvel, because I do not believe such a large and deep river has ever been known to exist in this world.
Beginning in the 1499, the people and natural resources of South America were repeatedly exploited by foreign conquistadors, first from Spain and later from Portugal. These competing colonial nations claimed the land and resources as their own and divided it into colonies.
European diseases (smallpox, influenza, measles and typhus) to which the native populations had no resistance decimated the American population, as well as cruel systems of forced labor (such as encomiendas and mining industry's mita) under Spanish control. Following this, African slaves, who had developed immunity to these diseases, were quickly brought in to replace them.
The Spaniards were committed to converting their American subjects to Christianity and were quick to purge any native cultural practices that hindered this end. However, most initial attempts at this were only partially successful; American groups simply blended Catholicism with their traditional beliefs. The Spaniards did not impose their language to the degree they did their religion. In fact, the missionary work of the Roman Catholic Church in Quechua, Nahuatl, and Guarani actually contributed to the expansion of these American languages, equipping them with writing systems.
Eventually the natives and the Spaniards interbred, forming a Mestizo class. Mestizos and the native Americans were often forced to pay unfair taxes to the Spanish government (although all subjects paid taxes) and were punished harshly for disobeying their laws. Many native artworks were considered pagan idols and destroyed by Spanish explorers. This included a great number of gold and silver sculptures, which were melted down before transport to Europe.
17th and 18th centuries.
In 1616, the Dutch, attracted by the legend of El Dorado, founded a fort in Guayana and established three colonies: Demerara, Berbice, and Essequibo.
In 1624 France attempted to settle in the area of modern day French Guiana, but was forced to abandon it in the face of hostility from the Portuguese, who viewed it as a violation of the Treaty of Tordesillas. However French settlers returned in 1630 and in 1643 managed to establish a settlement at Cayenne along with some small-scale plantations.
Since the sixteenth century there were some movements of discontent to Spanish and Portuguese colonial system. Among these movements, the most famous being that of the Maroons, slaves who escaped their masters and in the shelter of the forest communities organized free communities. Attempts to subject them by the royal army was unsuccessful, because the Maroons had learned to master the South American jungles. In a royal decree of 1713, the king gave legality to the first free population of the continent: Palenque de San Basilio in Colombia today, led by Benkos Bioho. Brazil saw the formation of a genuine African kingdom on their soil, with the Quilombo of Palmares.
Between 1721 and 1735, the Revolt of the Comuneros of Paraguay arose, because of clashes between the Paraguayan settlers and the Jesuits, who ran the large and prosperous Jesuit Reductions and controlled a large number of Christianized Indians.
Between 1742 and 1756, was the insurrection of Juan Santos Atahualpa in the central jungle of Peru. In 1780, the Viceroyalty of Peru was met with the insurrection of curaca Condorcanqui or Tupac Amaru II, which would be continued by Tupac Catari in Upper Peru.
In 1763, the African Cuffy led a revolt in Guyana which was bloodily suppressed by the Dutch.
In 1781, the Revolt of the Comuneros (New Granada), an insurrection of the villagers in the Viceroyalty of New Granada, was a popular revolution that united indigenous people and mestizos. The villagers tried to be the colonial power and despite the capitulation were signed, the Viceroy Manuel Antonio Flores did not comply, and instead ran to the main leaders José Antonio Galán.
In 1796, Essequibo (colony) of the Dutch was taken by the British, who had previously begun a massive introduction of slaves.
During the eighteenth century, the figure of the priest, mathematician and botanist José Celestino Mutis (1732–1808), was delegated by the Viceroy Antonio Caballero y Gongora to conduct an inventory of the nature of the Nueva Granada, which became known as the Botanical Expedition, which classified plants, wildlife and founded the first astronomical observatory in the city of Santa Fé de Bogotá.
On August 15, 1801, the Prussian scientist Alexander von Humboldt reached Fontibón where Mutis, and began his expedition to New Granada, Quito. The meeting between the two scholars are considered the brightest spot of the botanical expedition. 
Humboldt also visited Venezuela, Mexico, United States, Chile, and Peru. 
His observations of temperature differences between the Pacific Ocean between Chile and Peru in different periods of the year, he discovered the cold currents moving from south to north up the coast of Peru, which was named in his honor the Humboldt Current.
Between 1806 and 1807, British military forces tried to invade the area of the Rio de la Plata, at the command of Home Riggs Popham and William Carr Beresford, and John Whitelocke. The invasions were repelled, but powerfully affected the Spanish authority.
Independence and 19th century.
The Spanish colonies won their independence in the first quarter of the 19th century, in the Spanish American wars of independence. Simón Bolívar (Greater Colombia, Peru, Bolivia), José de San Martín (United Provinces of the Río de la Plata, Chile, and Peru), and Bernardo O'Higgins (Chile) led their independence struggle. Although Bolivar attempted to keep the Spanish-speaking parts of the continent politically unified, they rapidly became independent of one another.
Unlike the Spanish colonies, the Brazilian independence came as an indirect consequence of the Napoleonic Invasions to Portugal - French invasion under General Junot led to the capture of Lisbon on 8 December 1807. In order not to lose its sovereignty, the Portuguese Court moved the capital from Lisbon to Rio de Janeiro, which was the Portuguese Empire's capital between 1808 and 1821 and rose the relevance of Brazil within the Portuguese Empire's framework. Following the Portuguese Liberal Revolution of 1820, and after several battles and skirmishes were fought in Pará and in Bahia, the heir apparent Pedro, son of King John VI of Portugal, proclaimed the country's independence in 1822 and became Brazil's first emperor (He later also reigned as Pedro IV of Portugal). This was one of the most peaceful colonial independences ever seen in human history.
A struggle for power emerged among the new nations, and several further wars were soon fought thereafter.
The first few wars were fought for supremacy in the northern and southern parts of the continent. The Gran Colombia – Peru War of the north and the Cisplatine War (between the Empire of Brazil and the United Provinces of the Río de la Plata) ended in stalemates, although the latter resulted in the independence of Uruguay (1828). A few years later, after the break-up of Gran Colombia, the balance of power shifted in favor of the newly formed Peru-Bolivian Confederation (1836-1839). Nonetheless, this power structure proved temporary and shifted once more as a result of the Northern Peruvian State's victory over the Southern Peruvian State-Bolivia War of the Confederation (1836-1839), and the Argentine Confederation's defeat in the Guerra Grande (1839-1852).
Later conflicts between the South American nations continued to define their borders and power status. In the Pacific coast, Chile and Peru continued to exhibit their increasing domination, defeating Spain in the Chincha Islands War. Finally, after precariously defeating Peru during the War of the Pacific (1879-1883), Chile emerged as the dominant power of the Pacific Coast of South America. In the Atlantic side, Paraguay attempted to gain a more dominant status in the region, but an alliance of Argentina, Brazil, and Uruguay (in the resulting 1864-1870 War of the Triple Alliance) ended Paraguayan ambitions. Thereupon, the Southern Cone nations of Argentina, Brazil, and Chile entered the 20th century as the major continental powers.
A few countries did not gain independence until the 20th century:
French Guiana remains an overseas department of France.
20th century.
Dictatorships.
Timeline of military dictatorships in South America
Recent history.
South America, like many other continents, became a battlefield for the superpowers during the Cold War in the late 20th century.
In the 1960s and 1970s, the governments of Argentina, Brazil, Chile, and Uruguay were overthrown or displaced by U.S.-aligned military dictatorships. These detained tens of thousands of political prisoners, many of whom were tortured and/or killed (on inter-state collaboration, see Operation Condor). Economically, they began a transition to neoliberal economic policies. They placed their own actions within the U.S. Cold War doctrine of "National Security" against internal subversion. Throughout the 1980s and 1990s, Peru suffered from an internal conflict (see Túpac Amaru Revolutionary Movement and Shining Path). Revolutionary movements and right-wing military dictatorships have been common, but starting in the 1980s a wave of democratization came through the continent, and democratic rule is now widespread. Allegations of corruption remain common, and several nations have seen crises which have forced the resignation of their presidents, although normal civilian succession has continued. International indebtedness became a notable problem, as most recently illustrated by Argentina's default in the early 21st century.
In recent years, with deep economic and social crisis provoked by neoliberal policies, the right wing lost appeal in the region (with the major exception being Colombia) and the election of a sequence of left wing presidents began with Hugo Chávez' victory on the 1998 presidential election in Venezuela. As a matter of fact, "The Nation" described Dilma Rousseff's victory in the 2010 Brazilian election as a defeat for the Washington Consensus.
Despite the move to the left, South America remains largely capitalist and is enjoying its best years of economic growth. The Brazilian GDP, for instance, is expected to grow 7.5% in 2010, second only to the People's Republic of China in the world.
The list of left wing South American presidents is, by date of election, the following:
In 2008, the Union of South American Nations (USAN) was founded, revealing South American ambition of economic integration, with plans for political integration in the European Union style. This was seen by American political commentators as a pivotal moment in the loss of U.S. hegemony in the region. According to Noam Chomsky, USAN represents that "for the first time since the European conquest, Latin America began to move towards integration".

</doc>
<doc id="51374" url="http://en.wikipedia.org/wiki?curid=51374" title="390s BC">
390s BC


</doc>
<doc id="51375" url="http://en.wikipedia.org/wiki?curid=51375" title="370s BC">
370s BC


</doc>
<doc id="51376" url="http://en.wikipedia.org/wiki?curid=51376" title="380s BC">
380s BC


</doc>
<doc id="51377" url="http://en.wikipedia.org/wiki?curid=51377" title="47 BC">
47 BC

Year 47 BC was a year of the pre-Julian Roman calendar. At the time, it was known as the Year of the Consulship of Calenius and Vatinius (or, less frequently, year 707 "Ab urbe condita"). The denomination 47 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
China.
</onlyinclude>

</doc>
<doc id="51378" url="http://en.wikipedia.org/wiki?curid=51378" title="45 BC">
45 BC

Year 45 BC was either a common year starting on Thursday, Friday or Saturday or a leap year starting on Friday or Saturday (link will display the full calendar) (the sources differ, see leap year error for further information) and the first year of the Julian calendar and a leap year starting on Friday of the Proleptic Julian calendar. At the time, it was known as the Year of the Consulship of Caesar without Colleague (or, less frequently, year 709 "Ab urbe condita"). The denomination 45 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="51380" url="http://en.wikipedia.org/wiki?curid=51380" title="360s BC">
360s BC


</doc>
<doc id="51381" url="http://en.wikipedia.org/wiki?curid=51381" title="350s BC">
350s BC


</doc>
<doc id="51382" url="http://en.wikipedia.org/wiki?curid=51382" title="330s BC">
330s BC


</doc>
<doc id="51383" url="http://en.wikipedia.org/wiki?curid=51383" title="320s BC">
320s BC


</doc>
<doc id="51384" url="http://en.wikipedia.org/wiki?curid=51384" title="310s BC">
310s BC


</doc>
<doc id="51385" url="http://en.wikipedia.org/wiki?curid=51385" title="300s BC (decade)">
300s BC (decade)


</doc>
<doc id="51386" url="http://en.wikipedia.org/wiki?curid=51386" title="Research Institute of Crop Production">
Research Institute of Crop Production

The Research Institute of Crop Production (RICP) (Czech: "Výzkumný ústav rostlinné výroby, VÚRV") was established in 1951. During its more than 50 years of existence, the RICP has been the leading crop-production research institution within the Czech Republic. After most specialised research institutes in the Czech Republic were privatised in 1993, RICP is the only state-independent institution pursuing both research and consulting activities focussed on problems associated with growing crop plants.
RICP is a subsidised organisation. Its basic purpose is the development of scientific knowledge in the field of integrated crop production, the production of wholesome foodstuffs, and the development of conditions for sustainable agriculture and conservation of the environment.
Although the main office of RICP is in Prague, various parts of the Institute (14 experimental or research stations and separate laboratories) are located in different areas in the territory of the Czech Republic. 

</doc>
<doc id="51387" url="http://en.wikipedia.org/wiki?curid=51387" title="2016">
2016

 
2016 ()
will be .

</doc>
<doc id="51388" url="http://en.wikipedia.org/wiki?curid=51388" title="European Biomass Association">
European Biomass Association

The European Biomass Association (AEBIOM, from the official French name "Association Européenne pour la Biomasse") is a group of national biomass associations with membership open to representatives of the European Union and Central and Eastern Europe. It was founded in 1990 and aims to promote biomass production and application throughout Europe.

</doc>
<doc id="51389" url="http://en.wikipedia.org/wiki?curid=51389" title="2017">
2017

2017 ()
will be .

</doc>
<doc id="51390" url="http://en.wikipedia.org/wiki?curid=51390" title="2018">
2018

2018 ()
will be .

</doc>
<doc id="51391" url="http://en.wikipedia.org/wiki?curid=51391" title="2019">
2019

2019 ()
will be .
See also.
According to the comedy TV series, "The Last Man on Earth", a mysterious plague wipes out almost all of humanity.

</doc>
<doc id="51392" url="http://en.wikipedia.org/wiki?curid=51392" title="2060s">
2060s

The 2060s decade will begin on January 1, 2060 and will end on December 31, 2069.

</doc>
<doc id="51393" url="http://en.wikipedia.org/wiki?curid=51393" title="Czech Biomass Association">
Czech Biomass Association

The Czech Biomass Association (CZ BIOM) is a NGO, which supports the development of phytoenergetics in the Czech Republic. Members of CZ BIOM are scientists, specialists, entrepreneurs, and activists interested in using biomass as an energy resource. CZ BIOM is a member of the European Biomass Association.

</doc>
<doc id="51394" url="http://en.wikipedia.org/wiki?curid=51394" title="290s BC">
290s BC

During the 290s BC, Hellenistic civilization begins its emergence throughout the successor states of the former Argead Macedonian Empire of Alexander the Great, resulting in the diffusion of Greek culture throughout the Levant and advances in science, mathematics, philosophy, etc. Meanwhile, the Roman Republic is embroiled in war against the Samnites, the Mauryan Empire continues to thrive in Ancient India, and the Kingdom of Qin in Ancient China, the one which in the future will conquer its adversaries and unite China, begins to emerge as a significant power during the Warring States period.

</doc>
<doc id="51396" url="http://en.wikipedia.org/wiki?curid=51396" title="2020">
2020

2020 ()
will be .

</doc>
<doc id="51397" url="http://en.wikipedia.org/wiki?curid=51397" title="2021">
2021

2021 ()
will be .

</doc>
<doc id="51399" url="http://en.wikipedia.org/wiki?curid=51399" title="Buckingham π theorem">
Buckingham π theorem

In engineering, applied mathematics, and physics, the Buckingham π theorem is a key theorem in dimensional analysis. It is a formalization of Rayleigh's method of dimensional analysis. Loosely, the theorem states that if there is a physically meaningful equation involving a certain number "n" of physical variables, then the original equation can be rewritten in terms of a set of "p" = "n" − "k"  dimensionless parameters "π1", "π2", ..., "πp" constructed from the original variables. (Here "k" is the number of physical dimensions involved; it is obtained as the rank of a particular matrix.)
The theorem can be seen as a scheme for nondimensionalization because it provides a method for computing sets of dimensionless parameters from the given variables, even if the form of the equation is still unknown.
Historical information.
Although named for Edgar Buckingham, the π theorem was first proved by French mathematician J. Bertrand in 1878. Bertrand considered only special cases of problems from electrodynamics and heat conduction, but his article contains in distinct terms all the basic ideas of the modern proof of the theorem and clear indication of its utility for modelling physical phenomena. The technique of using the theorem (“the method of dimensions”) became widely known due to the works of Rayleigh (the first application of the π theorem "in the general case" to the dependence of pressure drop in a pipe upon governing parameters probably dates back to 1892, a heuristic proof with the use of series expansion, to 1894).
Formal generalization of the π theorem for the case of arbitrarily many quantities was given first by A. Vaschy in 1892, then in 1911—apparently independently—by both A. Federman, and D. Riabouchinsky, and again in 1914 by Buckingham. It was Buckingham's article that introduced the use of the symbol "πi" for the dimensionless variables (or parameters), and this is the source for the theorem's name.
Statement.
More formally, the number of dimensionless terms that can be formed, "p", is equal to the nullity of the dimensional matrix, and "k" is the rank. For the purposes of the experimenter, different systems that share the same description in terms of these dimensionless numbers are equivalent.
In mathematical terms, if we have a physically meaningful equation such as
where the "qi" are the "n" physical variables, and they are expressed in terms of "k" independent physical units, then the above equation can be restated as
where the πi are dimensionless parameters constructed from the "qi" by "p" = "n" − "k" dimensionless equations —the so-called "Pi groups"— of the form
where the exponents "ai" are rational numbers (they can always be taken to be integers: just raise it to a power to clear denominators).
Significance.
The Buckingham π theorem provides a method for computing sets of dimensionless parameters from the given variables, even if the form of the equation is still unknown. However, the choice of dimensionless parameters is not unique: Buckingham's theorem only provides a way of generating sets of dimensionless parameters, and will not choose the most 'physically meaningful'.
Two systems for which these parameters coincide are called "similar" (as with similar triangles, they differ only in scale); they are equivalent for the purposes of the equation, and the experimentalist who wants to determine the form of the equation can choose the most convenient one.
To find out relation between the number of variables and fundamental dimensions Buckingham's theorem is most important.
Proof.
Outline.
It will be assumed that the space of fundamental and derived physical units forms a vector space over the rational numbers, with the fundamental units as basis vectors, and with multiplication of physical units as the "vector addition" operation, and raising to powers as the "scalar multiplication" operation:
represent a dimensional variable as the set of exponents needed for the fundamental units (with a power of zero if the particular fundamental unit is not present). For instance, the gravitational constant "g" has units of formula_4 (distance over time squared), so it is represented as the vector formula_5 with respect to the basis of fundamental units (distance,time).
Making the physical units match across sets of physical equations can then be regarded as imposing linear constraints in the physical unit vector space.
Formal proof.
Given a system of "n" dimensional variables (physical variables), in "k" (physical) dimensions, write the "dimensional matrix" "M", whose rows are the dimensions and whose columns are the variables: the ("i", "j")th entry is the power of the "i"th unit in the "j"th variable. The matrix can be interpreted as taking in a combination of the dimensional quantities and giving out the dimensions of this product. So
is the units of
A dimensionless variable is a combination whose units are all zero (hence, dimensionless), which is equivalent to the kernel of this matrix; a dimensionless variable is a linear relation between units of dimensional variables.
By the rank-nullity theorem, a system of "n" vectors in "k" dimensions (where all dimensions are necessary) satisfies a ("p" = "n" − "k")-dimensional space of relations. Any choice of basis will have "p" elements, which are the dimensionless variables.
The dimensionless variables can always be taken to be integer combinations of the dimensional variables (by clearing denominators). There is mathematically no natural choice of dimensionless variables; some choices of dimensionless variables are more physically meaningful, and these are what are ideally used.
Examples.
Speed.
This example is elementary, but demonstrates the general procedure: Suppose a car is driving at 100 km/hour; how long does it take it to go 200 km?
This question has two fundamental physical units: time "t" and length formula_8, and three dimensional variables: distance "D", time taken "T", and velocity "V". Thus there are 3 − 2 = 1 dimensionless quantity. The units of the dimensional quantities are:
The dimensional matrix is:
The rows correspond to the dimensions formula_8, and "t", and the columns to the dimensional variables "D", "T", "V". For instance, the 3rd column, (1, −1), states that the "V" (velocity) variable has units of formula_12.
For a dimensionless constant formula_13 we are looking for a vector formula_14 such that the matrix product of "M" on "a" yields the zero vector [0,0]. In linear algebra, this vector is known as the kernel of the dimensional matrix, and it spans the nullspace of the dimensional matrix, which in this particular case is one-dimensional. The dimensional matrix as written above is in reduced row echelon form, so one can read off a kernel vector within a multiplicative constant:
If the dimensional matrix were not already reduced, one could perform Gauss–Jordan elimination on the dimensional matrix to more easily determine the kernel. It follows that the dimensionless constant may be written:
or, in dimensional terms:
Since the kernel is only defined to within a multiplicative constant, if the above dimensionless constant is raised to any arbitrary power, it will yield another equivalent dimensionless constant.
Dimensional analysis has thus provided a general equation relating the three physical variables:
which may be written:
where "C" is one of a set of constants, such that formula_20. The actual relationship between the three variables is simply formula_21 so that the actual dimensionless equation (formula_22) is written:
In other words, there is only one value of "C" and it is unity. The fact that there is only a single value of "C" and that it is equal to unity is a level of detail not provided by the technique of dimensional analysis.
The simple pendulum.
We wish to determine the period "T"  of small oscillations in a simple pendulum. It will be assumed that it is a function of the length "L" , the mass "M" , and the acceleration due to gravity on the surface of the Earth "g", which has dimensions of length divided by time squared. The model is of the form
There are 3 fundamental physical dimensions in this equation: time "t", mass "m", and length "l", and 4 dimensional variables, "T", "M", "L", and "g". Thus we need only 4 − 3 = 1 dimensionless parameter, denoted π, and the model can be re-expressed as
where π is given by
for some values of "a"1, ..., "a"4.
The dimensions of the dimensional quantities are:
The dimensional matrix is:
We are looking for a kernel vector "a" = ["a"1, "a"2, "a"3, "a"4] such that the matrix product of "M" on "a" yields the zero vector [0,0,0]. The dimensional matrix as written above is in reduced row echelon form, so one can read off a kernel vector within a multiplicative constant:
Were it not already reduced, one could perform Gauss–Jordan elimination on the dimensional matrix to more easily determine the kernel. It follows that the dimensionless constant may be written:
In fundamental terms:
which is dimensionless. Since the kernel is only defined to within a multiplicative constant, if the above dimensionless constant is raised to any arbitrary power, it will yield another equivalent dimensionless constant.
This example is easy because three of the dimensional quantities are fundamental units, so the last ("g") is a combination of the previous. Note that if "a"2 were non-zero there would be no way to cancel the "M" value—therefore "a"2 "must" be zero. Dimensional analysis has allowed us to conclude that the period of the pendulum is not a function of its mass. (In the 3D space of powers of mass, time, and distance, we can say that the vector for mass is linearly independent from the vectors for the three other variables. Up to a scaling factor, formula_33 is the only nontrivial way to construct a vector of a dimensionless parameter.)
The model can now be expressed as:
Assuming the zeroes of "f"  are discrete, we can say "gT"2/"L" = "C""n"  where "Cn"  is the "n"th zero. If there is only one zero, then "gT"2/"L" = "C" . It requires more physical insight or an experiment to show that there is indeed only one zero and that the constant is in fact given by "C" = 4π2.
For large oscillations of a pendulum, the analysis is complicated by an additional dimensionless parameter, the maximum swing angle. The above analysis is a good approximation as the angle approaches zero.

</doc>
<doc id="51401" url="http://en.wikipedia.org/wiki?curid=51401" title="Automatism">
Automatism

Automatism may refer to:

</doc>
<doc id="51402" url="http://en.wikipedia.org/wiki?curid=51402" title="Low Countries">
Low Countries

The Low Countries (Dutch: "de Lage Landen", French: "les Pays-Bas") make up a coastal region in western Europe, consisting especially of the Netherlands and Belgium, and the low-lying delta of the Rhine, Meuse, Scheldt, and Ems rivers where much of the land is at or below sea level. This wide area of Northern Europe roughly stretches from French Gravelines and Dunkirk at its southwestern point, to the area of Dutch Delfzijl and German Eastern Frisia at its northeastern point, and to Luxembourg and French Thionville in the southeast.
Most of the Low Countries are coastal regions bounded by the North Sea or the English Channel. The countries without access to the sea have linked themselves politically and economically to those with access to form one union of port and hinterland. A poetic description also calls the region "the Low Countries by the Sea".
The Low Countries were the scene of the early northern towns, newly built rather than developed from ancient centres, that marked the reawakening of Europe in the 12th century. In that period, they rivaled northern Italy for the most densely populated region of Europe. Most of the cities were governed by guilds and councils along with a figurehead ruler; interaction with their ruler was regulated by a strict set of rules describing what the latter could and could not expect from them. All of the regions mainly depended on trade, manufacturing and the encouragement of the free flow of goods and craftsmen.
Germanic languages such as Dutch and Luxembourgish were the predominant languages, although Romance languages also played an important role. Secondary languages included French (Luxembourg, Brabant around Nivelles), Romance-speaking Belgium (cf. the Bishopric of Liège), the Romance Flanders (i.e. Cambrai, Lille, Tournai), and Namur (Walloon).
Terminology.
Historically, the term "Low Countries" arose at the Court of the Dukes of Burgundy, who used the term "les pays de par deça" (~ "the lands over here") for the Low Countries as opposed to "les pays de par delà" (~ "the lands over there") for the Duchy of Burgundy and the Free County of Burgundy, which were part of their realm but geographically disconnected from the Low Countries. Governor Mary of Hungary used both the expressions "les pays de par deça" and "Pays d'Embas" (~ "lands down here"), which evolved to "Pays-Bas" or "Low Countries". Today the term is typically fitted to modern political boundaries and used in the same way as the term "Benelux", which also includes Luxembourg.
The name of the modern country the Netherlands has the same meaning and origin as the term "low countries" due to "nether" meaning "lower". The same name of these countries can be found in other European languages, for example German "Niederlande", French, "les Pays-Bas", and so on, which all literally mean "the Low Countries". In the Dutch language itself (known in Dutch as "Nederlands", meaning "Netherlandish") no plural is used for the name of the modern country. So "Nederland" (singular) is used for the modern nation and "de Nederlanden" (plural) for the 16th century domains of Charles V. (However, in official use the name of the Dutch kingdom is still Kingdom of the Netherlands ("Koninkrijk der Nederlanden"), a name deriving from the 19th century origins of the kingdom which originally included present-day Belgium.)
In Dutch, and to a lesser extent in English, the Low Countries colloquially means the Netherlands and Belgium, sometimes the Netherlands and Flanders—the Dutch-speaking north of Belgium. (This version does not include Luxembourg.) For example, a 'Derby der Lage Landen' (Derby of the Low Countries), is a sports event between Belgium and the Netherlands.
"Belgium" was renamed only in 1830, after splitting from the Kingdom of the Netherlands, in order to distinguish it from its northern neighbour. It had previously also commonly been referred to as one part of the geographic "Netherlands", being the part which remained in the hands of the Habsburg heirs of the Burgundian Dukes until the French Revolution. Politically, before the Napoleonic wars, it was referred to as the "Southern", "Spanish" or later "Austrian" Netherlands. It is still referred to as part of the "low countries".
History.
The region politically had its origins in Carolingian empire; more precisely, most of it was within the Duchy of Lower Lotharingia. After the disintegration of Lower Lotharingia, the Low Countries were brought under the rule of various lordships until they came to be in the hands of the Valois Dukes of Burgundy. Hence, a large part of the low countries came to be referred to as the Burgundian Netherlands also called the Seventeen Provinces up to 1581. Even after the political secession of the autonomous Dutch Republic (or "United Provinces") in the north, the term "low countries" continued to be used to refer collectively to the region. The region was temporarily united politically between 1815 and 1839, as the United Kingdom of the Netherlands, before this split into the three modern countries of the Netherlands, Belgium and Luxembourg.
Early history.
The Low Countries were part of the Roman provinces of Gallia Belgica, Germania Inferior and Germania Superior. They were inhabited by Belgic and Germanic tribes. In the 4th and 5th century, Frankish tribes had entered this Roman region and came to run it increasingly independently. They came to be ruled by the Merovingian dynasty, under which dynasty the southern part (below the Rhine) was re-Christianised.
Frankish empire.
By the end of the 8th century, the Low Countries formed a core part of a much expanded Francia and the Merovingians were replaced by the Carolingian dynasty. In 800 the Pope crowned and appointed Charlemagne Emperor of the re-established Roman Empire.
After the death of Charlemagne, Francia was divided in three parts among his three grandsons. The middle slice, Middle Francia, was ruled by Lothair I, and thereby also came to be referred to as "Lotharingia" or "Lorraine". Apart from the original coastal County of Flanders, which was within West Francia, the rest of the Low Countries were within the lowland part of this, "Lower Lorraine".
After the death of Lothair, the Low Countries were coveted by the rulers of both West Francia and East Francia. Each tried to swallow the region and to merge it with their spheres of influence. Thus, the Low Countries consisted of fiefs whose sovereignty resided with either the Kingdom of France or the Holy Roman Empire. While the further history the Low Countries can be seen as the object of a continual struggle between these two powers, the title of Duke of Lothier was coveted in the low countries for centuries.
Duchy of Burgundy.
Gradually, separate fiefs came to be ruled by a single family through royal intermarriage. This process culminated in the rule of the House of Valois, who were the rulers of the Duchy of Burgundy. In 1477 the Burgundian holdings in the area, the Burgundian Netherlands passed through an heiress—Mary of Burgundy—to the Habsburgs.
A documentary about the art in the region at this time was produced by the BBC, named The High Art of the Low Countries.
Seventeen Provinces.
In the following century the "Low Countries" corresponded roughly to the Seventeen Provinces covered by the Pragmatic Sanction of 1549 of Holy Roman Emperor Charles V, which freed the provinces from their archaic feudal obligations.
After the northern Seven United Provinces of the seventeen declared their independence from Habsburg Spain in 1581, the ten provinces of the Southern Netherlands remained occupied by the Army of Flanders under Spanish service and are therefore sometimes called the "Spanish Netherlands". In 1713, under the Treaty of Utrecht following the War of the Spanish Succession, what was left of the Spanish Netherlands was ceded to Austria and thus became known as the Austrian Netherlands. The United Kingdom of the Netherlands (1815–1830) temporarily united the Low Countries again.
After the Second World War.
After the Second World War, Benelux was the name used for the trading region of the sovereign states of the Netherlands, Belgium and Luxembourg.
Literature.
One of the Low Countries' earliest literary figures is the blind poet Bernlef, from  800, who sang both Christian psalms and pagan verses. Bernlef is representative of the coexistence of Christianity and Germanic polytheism in this time period.:1–2
The earliest examples of written literature include the Wachtendonck Psalms, a collections of twenty five psalms that originated in the Moselle-Frankish region around the middle of the 9th century.:3

</doc>
<doc id="51408" url="http://en.wikipedia.org/wiki?curid=51408" title="Half Dome">
Half Dome

Half Dome is a granite dome at the eastern end of Yosemite Valley in Yosemite National Park, California. It is possibly Yosemite's most familiar rock formation. The granite crest rises more than 4737 ft above the valley floor.
Geology.
The impression from the valley floor that this is a round dome which has lost its northwest half is an illusion. From Washburn Point, Half Dome can be seen as a thin ridge of rock, an arête, that is oriented northeast-southwest, with its southeast side almost as steep as its northwest side except for the very top. Although the trend of this ridge, as well as that of Tenaya Canyon, is probably controlled by master joints, 80 percent of the northwest "half" of the original dome may well still be there.
On March 28, 2009, a large rock slide of 1,500,000 ft3 occurred from Ahwiyah Point. The slide happened at 5:26 a.m and damaged a large area under the dome. No one was injured but hundreds of trees were knocked down and a portion of the Mirror Lake trail was buried. The slide registered on seismographs as an earthquake reaching 2.5 on the Richter Scale.
Ascents.
As late as the 1870s, Half Dome was described as "perfectly inaccessible" by Josiah Whitney of the California Geological Survey. The summit was finally conquered by George G. Anderson in October 1875, via a route constructed by drilling and placing iron eyebolts into the smooth granite.
Today, Half Dome may now be ascended in several different ways. Thousands of hikers reach the top each year by following an 8.5 mi trail from the valley floor. After a rigorous 2 mi approach including several hundred feet of granite stairs, the final pitch up the peak's steep but somewhat rounded east face is ascended with the aid of a pair of post-mounted braided steel cables originally constructed close to the Anderson route in 1919.
Alternatively, over a dozen rock climbing routes lead from the valley up Half Dome's vertical northwest face. The first technical ascent was in 1957 via a route pioneered by Royal Robbins, Mike Sherrick, and Jerry Gallwas today known as the Regular Northwest Face. Their 5-day epic was the first Grade VI climb in the United States. Their route has now been free soloed several times in a few hours' time. Other technical routes ascend the south face and the west shoulder.
Hiking the "Cable Route"..
The Half Dome "Cable Route" hike runs from the valley floor to the top of the dome in 8.2 mi (via the Mist Trail), with 4800 ft of elevation gain. The length and difficulty of the trail used to keep it less crowded than other park trails, but in recent years the trail traffic has grown to as many as 800 people a day. The hike can be done from the valley floor in a single long day, but many people break it up by camping overnight in Little Yosemite Valley. The trail climbs past Vernal and Nevada Falls, then continues into Little Yosemite Valley, then north to the base of the northeast ridge of Half Dome itself.
The final 400 ft ascent is steeply up the rock between two steel cables used as handholds. The cables are fixed with bolts in the rock and raised onto a series of metal poles in late May (the poles do not anchor the cables). The cables are taken down from the poles for the winter in early October, but they are still fixed to the rock surface and can be used. The National Park Service recommends against climbing the route when the cables are down and when the surface of the rock is wet and slippery. The "Cable Route" is rated class 3, while the same face away from the cables is rated class 5.
The "Cable Route" can be crowded. In past years, as many as 1,000 hikers per day have sometimes climbed the dome on a summer weekend, and about 50,000 hikers climb it every year.
In January 2010, the National Park Service announced that permits will be required to hike the "Cable Route" on Fridays, Saturdays, Sundays and federal holidays. The park service cited safety concerns and increased crowding on the route as reasons for the new regulations. Permits will be issued only through the National Recreation Reservation Service from four months in advance to one week in advance. A maximum of 400 permits per day will be issued, and a processing fee of $1.50 per permit will be charged. Permits will not be issued in the park.
In December 2010, NPS officials announced that the permit system would be expanded to seven days per week beginning with the 2011 ascent season. All hikers who intend to ascend the "Cable Route" must now obtain permits before entering the park. Permits will be checked by a ranger on the trail, and no hikers without permits are allowed to hike beyond the base of the sub-dome or to the bottom of the cables. Hikers caught bypassing the rangers to visit either the sub-dome or main dome without a permit face fines of up to $5000 and/or 6 months in jail.
Backpackers with an appropriate wilderness permit can receive a Half Dome permit when they pick up their wilderness permit with no additional reservation required. Rock climbers who reach the top of Half Dome without entering the subdome area can descend on the Half Dome Trail without a permit.
The top of Half Dome is a large, flat area where climbers can relax and enjoy their accomplishment. The summit offers views of the surrounding areas, including Little Yosemite Valley and the Valley Floor. A notable location to one side of Half Dome is the "Diving Board," where Ansel Adams took his photograph, "Monolith, The Face of Half Dome" on April 10, 1927. Often confused with "The Visor," a small overhanging ledge at the summit, the Diving Board is on the shoulder of Half Dome.
From 1919 when the cables were erected through 2011, there have been six fatal falls from the cables. The latest fatality occurred on July 31, 2011.
Lightning strikes can be a risk while on or near the summit. On July 27, 1985, five hikers were struck by lightning, resulting in two fatalities.
The "Cable Route" was added to the National Register of Historic Places in 2012.
In culture.
Half Dome was originally called "Tis-sa-ack", meaning Cleft Rock in the language of the local Ahwahnechee people. Tis-sa-ack is also the name of the fourth route on the formation, ascended by Royal Robbins and Don Peterson over eight days in October 1969. Tis-sa-ack is the name of a mother from a native legend. The face seen in Half Dome is supposed to be hers. Tis-sa-ack is the name of a Mono Lake Paiute Indian girl in the Yosemite Native American legend. John Muir referred to the peak as "Tissiack".
Others say Ahwahneechee Native Americans named Half Dome “Face of a Young Woman Stained with Tears” ("Tis-se’-yak") because of the colonies of brown-black lichens that form dark vertical drip-like stripes along drainage tracks in the rock faces.
In 1988, Half Dome was featured on a 25 cent United States postage stamp. An image of Half Dome, along with John Muir and the California condor, appears on the California State Quarter, released in January 2005.
Starting October 2010, an image of Half Dome is included on the new revised California drivers license in the top right-hand corner.
In 2014, Apple revealed their new version of their operating system, Yosemite, and Half Dome was the default wallpaper on the new OS.
Half Dome is also an element or inspiration of various company and organization logos, including that of The North Face, Sierra Designs, & Mountain Khakis outdoor product companies, the Sierra Club environmental group and the Sierra Entertainment game studio.
See also.
360° panorama from the summit of Half Dome, taken in July 2005

</doc>
<doc id="51411" url="http://en.wikipedia.org/wiki?curid=51411" title="Probability and statistics">
Probability and statistics

Probability and statistics are two related but separate academic disciplines. Statistical analysis often uses probability distributions, and the two topics are often studied together. However, probability theory contains much that is mostly of mathematical interest and not directly relevant to statistics. Moreover, many topics in statistics are independent of probability theory. 

</doc>
<doc id="51414" url="http://en.wikipedia.org/wiki?curid=51414" title="Fundamental theorem of algebra">
Fundamental theorem of algebra

The fundamental theorem of algebra states that every non-constant single-variable polynomial with complex coefficients has at least one complex root. This includes polynomials with real coefficients, since every real number is a complex number with an imaginary part equal to zero.
Equivalently (by definition), the theorem states that the field of complex numbers is algebraically closed.
The theorem is also stated as follows: every non-zero, single-variable, degree "n" polynomial with complex coefficients has, counted with multiplicity, exactly "n" roots. The equivalence of the two statements can be proven through the use of successive polynomial division.
In spite of its name, there is no purely algebraic proof of the theorem, since any proof must use the completeness of the reals (or some other equivalent formulation of completeness), which is not an algebraic concept. Additionally, it is not fundamental for modern algebra; its name was given at a time when the study of algebra was mainly concerned with the solutions of polynomial equations with real or complex coefficients.
History.
Peter Rothe, in his book "Arithmetica Philosophica" (published in 1608), wrote that a polynomial equation of degree "n" (with real coefficients) "may" have "n" solutions. Albert Girard, in his book "L'invention nouvelle en l'Algèbre" (published in 1629), asserted that a polynomial equation of degree "n" has "n" solutions, but he did not state that they had to be real numbers. Furthermore, he added that his assertion holds "unless the equation is incomplete", by which he meant that no coefficient is equal to 0. However, when he explains in detail what he means, it is clear that he actually believes that his assertion is always true; for instance, he shows that the equation "x"4 = 4x − 3, although incomplete, has four solutions (counting multiplicities): 1 (twice), −1 + "i"√2, and −1 − "i"√2.
As will be mentioned again below, it follows from the fundamental theorem of algebra that every non-constant polynomial with real coefficients can be written as a product of polynomials with real coefficients whose degree is either 1 or 2. However, in 1702 Leibniz said that no polynomial of the type "x"4 + "a"4 (with "a" real and distinct from 0) can be written in such a way. Later, Nikolaus Bernoulli made the same assertion concerning the polynomial "x"4 −  4"x"3 + 2"x"2 + 4"x" + 4, but he got a letter from Euler in 1742 in which he was told that his polynomial happened to be equal to
where α is the square root of 4 + 2√7. Also, Euler mentioned that
A first attempt at proving the theorem was made by d'Alembert in 1746, but his proof was incomplete. Among other problems, it assumed implicitly a theorem (now known as Puiseux's theorem) which would not be proved until more than a century later, and furthermore the proof assumed the fundamental theorem of algebra. Other attempts were made by Euler (1749), de Foncenex (1759), Lagrange (1772), and Laplace (1795). These last four attempts assumed implicitly Girard's assertion; to be more precise, the existence of solutions was assumed and all that remained to be proved was that their form was "a" + "bi" for some real numbers "a" and "b". In modern terms, Euler, de Foncenex, Lagrange, and Laplace were assuming the existence of a splitting field of the polynomial "p"("z").
At the end of the 18th century, two new proofs were published which did not assume the existence of roots. One of them, due to James Wood and mainly algebraic, was published in 1798 and it was totally ignored. Wood's proof had an algebraic gap. The other one was published by Gauss in 1799 and it was mainly geometric, but it had a topological gap, filled by Alexander Ostrowski in 1920, as discussed in Smale 1981 (Smale writes, "...I wish to point out what an immense gap Gauss' proof contained. It is a subtle point even today that a real algebraic plane curve cannot enter a disk without leaving. In fact even though Gauss redid this proof 50 years later, the gap remained. It was not until 1920 that Gauss' proof was completed. In the reference Gauss, A. Ostrowski has a paper which does this and gives an excellent discussion of the problem as well..."). A rigorous proof was published by Argand in 1806; it was here that, for the first time, the fundamental theorem of algebra was stated for polynomials with complex coefficients, rather than just real coefficients. Gauss produced two other proofs in 1816 and another version of his original proof in 1849.
The first textbook containing a proof of the theorem was Cauchy's "Cours d'analyse de l'École Royale Polytechnique" (1821). It contained Argand's proof, although Argand is not credited for it.
None of the proofs mentioned so far is constructive. It was Weierstrass who raised for the first time, in the middle of the 19th century, the problem of finding a constructive proof of the fundamental theorem of algebra. He presented his solution, that amounts in modern terms to a combination of the Durand–Kerner method with the homotopy continuation principle, in 1891. Another proof of this kind was obtained by Hellmuth Kneser in 1940 and simplified by his son Martin Kneser in 1981.
Without using countable choice, it is not possible to constructively prove the fundamental theorem of algebra for complex numbers based on the Dedekind real numbers (which are not constructively equivalent to the Cauchy real numbers without countable choice). However, Fred Richman proved a reformulated version of the theorem that does work.
Proofs.
All proofs below involve some analysis, or at least the topological concept of continuity of real or complex functions. Some also use differentiable or even analytic functions. This fact has led to the remark that the Fundamental Theorem of Algebra is neither fundamental, nor a theorem of algebra.
Some proofs of the theorem only prove that any non-constant polynomial with real coefficients has some complex root. This is enough to establish the theorem in the general case because, given a non-constant polynomial "p"("z") with complex coefficients, the polynomial
has only real coefficients and, if "z" is a zero of "q"("z"), then either "z" or its conjugate is a root of "p"("z").
A large number of non-algebraic proofs of the theorem use the fact (sometimes called "growth lemma") that an "n"-th degree polynomial function "p"("z") whose dominant coefficient is 1 behaves like "zn" when |"z"| is large enough. A more precise statement is: there is some positive real number "R" such that:
when |"z"| > "R".
Complex-analytic proofs.
Find a closed disk "D" of radius "r" centered at the origin such that |"p"("z")| > |"p"(0)| whenever |"z"| ≥ "r". The minimum of |"p"("z")| on "D", which must exist since "D" is compact, is therefore achieved at some point "z"0 in the interior of "D", but not at any point of its boundary. The Maximum modulus principle (applied to 1/"p"("z")) implies then that "p"("z"0) = 0. In other words, "z"0 is a zero of "p"("z").
A variation of this proof does not require the use of the maximum modulus principle (in fact, the same argument with minor changes also gives a proof of the maximum modulus principle for holomorphic functions). If we assume by contradiction that "a" := "p"("z"0) ≠ 0, then, expanding "p"("z") in powers of "z" − "z"0 we can write
Here, the "cj" are simply the coefficients of the polynomial "z" → "p"("z" + "z"0), and we let "k" be the index of the first coefficient following the constant term that is non-zero. But now we see that for "z" sufficiently close to "z"0 this has behavior asymptotically similar to the simpler polynomial formula_6, in the sense that (as is easy to check) the function formula_7 is bounded by some positive constant "M" in some neighborhood of "z"0. Therefore if we define formula_8 and let formula_9, then for any sufficiently small positive number "r" (so that the bound "M" mentioned above holds), using the triangle inequality we see that
When "r" is sufficiently close to 0 this upper bound for |"p"("z")| is strictly smaller than |"a"|, in contradiction to the definition of "z"0. (Geometrically, we have found an explicit direction θ0 such that if one approaches "z"0 from that direction one can obtain values "p"("z") smaller in absolute value than |"p"("z"0)|.)
Another analytic proof can be obtained along this line of thought observing that, since |"p"("z")| > |"p"(0)| outside "D", the minimum of |"p"("z")| on the whole complex plane is achieved at "z"0. If |"p"("z"0)| > 0, then 1/"p" is a bounded holomorphic function in the entire complex plane since, for each complex number "z", |1/"p"("z")| ≤ |1/"p"("z"0)|. Applying Liouville's theorem, which states that a bounded entire function must be constant, this would imply that 1/"p" is constant and therefore that "p" is constant. This gives a contradiction, and hence "p"("z"0) = 0.
Yet another analytic proof uses the argument principle. Let "R" be a positive real number large enough so that every root of "p"("z") has absolute value smaller than "R"; such a number must exist because every non-constant polynomial function of degree "n" has at most "n" zeros. For each "r" > "R", consider the number
where "c"("r") is the circle centered at 0 with radius "r" oriented counterclockwise; then the argument principle says that this number is the number "N" of zeros of "p"("z") in the open ball centered at 0 with radius "r", which, since "r" > "R", is the total number of zeros of "p"("z"). On the other hand, the integral of "n"/"z" along "c"("r") divided by 2π"i" is equal to "n". But the difference between the two numbers is
The numerator of the rational expression being integrated has degree at most "n" − 1 and the degree of the denominator is "n" + 1. Therefore, the number above tends to 0 as "r" → +∞. But the number is also equal to "N" − "n" and so "N" = "n".
Still another complex-analytic proof can be given by combining linear algebra with the Cauchy theorem. To establish that every complex polynomial of degree "n" > 0 has a zero, it suffices to show that every complex square matrix of size "n" > 0 has a (complex) eigenvalue. The proof of the latter statement is by contradiction.
Let "A" be a complex square matrix of size "n" > 0 and let "In" be the unit matrix of the same size. Assume "A" has no eigenvalues. Consider the resolvent function
which is a meromorphic function on the complex plane with values in the vector space of matrices. The eigenvalues of "A" are precisely the poles of "R(z)". Since, by assumption, "A" has no eigenvalues, the function "R(z)" is an entire function and Cauchy theorem implies that
On the other hand, "R"("z") expanded as a geometric series gives:
This formula is valid outside the closed disc of radius ||"A"|| (the operator norm of "A"). Let "r" > ||"A"||. Then
(in which only the summand "k" = 0 has a nonzero integral). This is a contradiction, and so "A" has an eigenvalue.
Finally, Rouché's theorem gives perhaps the shortest proof of the theorem.
Topological proofs.
Let "z"0 ∈ C be such that the minimum of |"p"("z")| on the whole complex plane is achieved at "z"0; it was seen at the proof which uses Liouville's theorem that such a number must exist. We can write "p"("z") as a polynomial in "z" − "z"0: there is some natural number "k" and there are some complex numbers "ck", "c""k" + 1, ..., "cn" such that "ck" ≠ 0 and that
In the case that "p"("z"0) is nonzero, it follows that if "a" is a "k"th root of −"p"("z"0)/"ck" and if "t" is positive and sufficiently small, then |"p"("z"0 + "ta")| < |"p"("z"0)|, which is impossible, since |"p"("z"0)| is the minimum of |"p"| on "D".
For another topological proof by contradiction, suppose that "p"("z") has no zeros. Choose a large positive number "R" such that, for |"z"| = "R", the leading term "zn" of "p"("z") dominates all other terms combined; in other words, such that |"z"|"n" > |"a""n"−1"z""n"−1 + ··· + "a"0|. As "z" traverses the circle given by the equation |"z"| = "R" once counter-clockwise, "p"("z"), like "zn", winds "n" times counter-clockwise around 0. At the other extreme, with |"z"| = 0, the "curve" "p"("z") is simply the single (nonzero) point "p"(0), whose winding number is clearly 0. If the loop followed by "z" is continuously deformed between these extremes, the path of "p"("z") also deforms continuously. We can explicitly write such a deformation as "H"("Re""i"θ,"t") = "p"((1 − "t")"Re""i"θ), where 0 ≤ "t" ≤ 1. If one views the variable "t" as time, then at time zero the curve is "p"("z") and at time one the curve is "p"(0). Clearly at every point "t", "p"("z") cannot be zero by the original assumption, therefore during the deformation, the curve never crosses zero. Therefore the winding number of the curve around zero should never change. However, given that the winding number started as "n" and ended as 0, this is absurd. Therefore, "p"("z") has at least one zero.
Algebraic proofs.
These proofs use two facts about real numbers that require only a small amount of analysis (more precisely, the intermediate value theorem):
The second fact, together with the quadratic formula, implies the theorem for real quadratic polynomials. In other words, algebraic proofs of the fundamental theorem actually show that if "R" is any real-closed field, then its extension "C" = "R"(√−1) is algebraically closed.
As mentioned above, it suffices to check the statement "every non-constant polynomial "p"("z") with real coefficients has a complex root". This statement can be proved by induction on the greatest non-negative integer "k" such that 2"k" divides the degree "n" of "p"("z"). Let "a" be the coefficient of "zn" in "p"("z") and let "F" be a splitting field of "p"("z") over "C"; in other words, the field "F" contains "C" and there are elements "z"1, "z"2, ..., "zn" in "F" such that
If "k" = 0, then "n" is odd, and therefore "p"("z") has a real root. Now, suppose that "n" = 2"km" (with "m" odd and "k" > 0) and that the theorem is already proved when the degree of the polynomial has the form 2"k" − 1"m"′ with "m"′ odd. For a real number "t", define:
Then the coefficients of "qt"("z") are symmetric polynomials in the "zi"'s with real coefficients. Therefore, they can be expressed as polynomials with real coefficients in the elementary symmetric polynomials, that is, in −"a"1, "a"2, ..., (−1)"nan". So "qt"("z") has in fact "real" coefficients. Furthermore, the degree of "qt"("z") is "n"("n" − 1)/2 = 2"k"−1"m"("n" − 1), and "m"("n" − 1) is an odd number. So, using the induction hypothesis, "qt" has at least one complex root; in other words, "zi" + "zj" + "tzizj" is complex for two distinct elements "i" and "j" from {1, ..., "n"}. Since there are more real numbers than pairs ("i", "j"), one can find distinct real numbers "t" and "s" such that "zi" + "zj" + "tzizj" and "zi" + "zj" + "szizj" are complex (for the same "i" and "j"). So, both "zi" + "zj" and "zizj" are complex numbers. It is easy to check that every complex number has a complex square root, thus every complex polynomial of degree 2 has a complex root by the quadratic formula. It follows that "zi" and "zj" are complex numbers, since they are roots of the quadratic polynomial "z"2 −  ("zi" + "zj")"z" + "zizj".
J. Shipman showed in 2007 that the assumption that odd degree polynomials have roots is stronger than necessary; any field in which polynomials of prime degree have roots is algebraically closed (so "odd" can be replaced by "odd prime" and furthermore this holds for fields of all characteristics). For axiomatization of algebraically closed fields, this is the best possible, as there are counterexamples if a single prime is excluded. However, these counterexamples rely on −1 having a square root. If we take a field where −1 has no square root, and every polynomial of degree "n" ∈ "I" has a root, where "I" is any fixed infinite set of odd numbers, then every polynomial "f"("x") of odd degree has a root (since ("x"2 + 1)"k""f"("x") has a root, where "k" is chosen so that deg("f") + 2"k" ∈ "I").
Another algebraic proof of the fundamental theorem can be given using Galois theory. It suffices to show that C has no proper finite field extension. Let "K"/C be a finite extension. Since the normal closure of "K" over R still has a finite degree over C (or R), we may assume without loss of generality that "K" is a normal extension of R (hence it is a Galois extension, as every algebraic extension of a field of characteristic 0 is separable). Let "G" be the Galois group of this extension, and let "H" be a Sylow 2-subgroup of "G", so that the order of "H" is a power of 2, and the index of "H" in "G" is odd. By the fundamental theorem of Galois theory, there exists a subextension "L" of "K"/R such that Gal("K"/"L") = "H". As ["L":R] = ["G":"H"] is odd, and there are no nonlinear irreducible real polynomials of odd degree, we must have "L" = R, thus ["K":R] and ["K":C] are powers of 2. Assuming by way of contradiction that ["K":C] > 1, we conclude that the 2-group Gal("K"/C) contains a subgroup of index 2, so there exists a subextension "M" of C of degree 2. However, C has no extension of degree 2, because every quadratic complex polynomial has a complex root, as mentioned above. This shows that ["K":C] = 1, and therefore "K" = C, which completes the proof.
Geometric proofs.
There exists still another way to approach the fundamental theorem of algebra, due to J. M. Almira and A. Romero: by Riemannian Geometric arguments. The main idea here is to prove that the existence of a non-constant polynomial "p"("z") without zeros implies the existence of a flat Riemannian metric over the sphere S2. This leads to a contradiction, since the sphere is not flat.
Recall that a Riemannian surface ("M", "g") is said to be flat if its Gaussian curvature, which we denote by "Kg", is identically null. Now, Gauss–Bonnet theorem, when applied to the sphere S2, claims that
which proves that the sphere is not flat.
Let us now assume that "n" > 0 and "p"("z") = "a"0 + "a"1"z" + ⋅⋅⋅ + "anzn" ≠ 0 for each complex number "z". Let us define "p*"("z") = "znp"("1/z") = "a"0"zn" + "a"1"z""n"−1 + ⋅⋅⋅ + "an". Obviously, "p*"("z") ≠ 0 for all "z" in C. Consider the polynomial "f"("z") = "p"("z")"p*"("z"). Then "f"("z") ≠ 0 for each "z" in C. Furthermore,
We can use this functional equation to prove that "g", given by
for "w" in C, and
for "w" ∈ S2\{0}, is a well defined Riemannian metric over the sphere S2 (which we identify with the extended complex plane C ∪ {∞}).
Now, a simple computation shows that
since the real part of an analytic function is harmonic. This proves that "Kg" = 0.
Corollaries.
Since the fundamental theorem of algebra can be seen as the statement that the field of complex numbers is algebraically closed, it follows that any theorem concerning algebraically closed fields applies to the field of complex numbers. Here are a few more consequences of the theorem, which are either about the field of real numbers or about the relationship between the field of real numbers and the field of complex numbers:
Bounds on the zeros of a polynomial.
While the fundamental theorem of algebra states a general existence result, it is of some interest, both from the theoretical and from the practical point of view, to have information on the location of the zeros of a given polynomial. The simpler result in this direction is a bound on the modulus: all zeros ζ of a monic polynomial formula_25 satisfy an inequality |ζ| ≤ "R"∞, where
Notice that, as stated, this is not yet an existence result but rather an example of what is called an a priori bound: it says that "if there are solutions" then they lie inside the closed disk of center the origin and radius "R"∞. However, once coupled with the fundamental theorem of algebra it says that the disk contains in fact at least one solution. More generally, a bound can be given directly in terms of any p-norm of the "n"-vector of coefficients formula_27, that is |ζ| ≤ "Rp", where "Rp" is precisely the "q"-norm of the 2-vector formula_28, "q" being the conjugate exponent of "p", 1/"p" + 1/"q" = 1, for any 1 ≤ "p" ≤ ∞. Thus, the modulus of any solution is also bounded by
for 1 < "p" < ∞, and in particular
(where we define "an" to mean 1, which is reasonable since 1 is indeed the "n"-th coefficient of our polynomial). The case of a generic polynomial of degree n, formula_32, is of course reduced to the case of a monic, dividing all coefficients by "an" ≠ 0. Also, in case that 0 is not a root, i.e. "a"0 ≠ 0., bounds from below on the roots ζ follow immediately as bounds from above on formula_33, that is, the roots of formula_34. Finally, the distance formula_35 from the roots ζ to any point formula_36 can be estimated from below and above, seeing formula_37 as zeros of the polynomial formula_38, whose coefficients are the Taylor expansion of "P"("z") at formula_39
We report here the proof of the above bounds, which is short and elementary. Let ζ be a root of the polynomial formula_25; in order to prove the inequality |ζ| ≤ "Rp" we can assume, of course, |ζ| > 1. Writing the equation as formula_41, and using the Hölder's inequality we find formula_42. Now, if "p" = 1, this is formula_43, thus formula_44. In the case 1 < "p" ≤ ∞, taking into account the summation formula for a geometric progression, we have
thus formula_46 and simplifying, formula_47. Therefore formula_48 holds, for all 1 ≤ "p" ≤ ∞.

</doc>
<doc id="51418" url="http://en.wikipedia.org/wiki?curid=51418" title="Nikos Kazantzakis">
Nikos Kazantzakis

Nikos Kazantzakis (Greek: Νίκος Καζαντζάκης; 18 February 1883 – 26 October 1957) was a Greek writer and philosopher, celebrated for his novel "Zorba the Greek", considered his "magnum opus". He became known globally after the 1964 release of the Michael Cacoyannis film "Zorba the Greek", based on the novel. He gained renewed fame with the 1988 Martin Scorsese adaptation of his book "The Last Temptation of Christ".
Biography.
When Kazantzakis was born in 1883 in Heraklion, Crete had not yet joined the modern Greek state, (which had been established in 1832) and was still under the rule of the Ottoman Empire. From 1902 Kazantzakis studied law at the University of Athens, then went to Paris in 1907 to study philosophy. Here he fell under the influence of Henri Bergson. His 1909 dissertation was titled "Friedrich Nietzsche on the Philosophy of Right and the State." Upon his return to Greece, he began translating works of philosophy. In 1914 he met Angelos Sikelianos. Together they travelled for two years in places where Greek Orthodox Christian culture flourished, largely influenced by the enthusiastic nationalism of Sikelianos.
Kazantzakis married Galatea Alexiou in 1911; they divorced in 1926. He married Eleni Samiou in 1945. Between 1922 and his death in 1957, he sojourned in Paris and Berlin (from 1922 to 1924), Italy, Russia (in 1925), Spain (in 1932), and then later in Cyprus, Aegina, Egypt, Mount Sinai, Czechoslovakia, Nice (he later bought a villa in nearby Antibes, in the Old Town section near the famed seawall), China, and Japan. While in Berlin, where the political situation was explosive, Kazantzakis discovered communism and became an admirer of Vladimir Lenin. He never became a consistent communist, but visited the Soviet Union and stayed with the Left Opposition politician and writer Victor Serge. He witnessed the rise of Joseph Stalin, and became disillusioned with Soviet-style communism. Around this time, his earlier nationalist beliefs were gradually replaced by a more universal ideology.
In 1945, he became the leader of a small party on the non-communist left, and entered the Greek government as Minister without Portfolio. He resigned this post the following year. In 1946, The Society of Greek Writers recommended that Kazantzakis and Angelos Sikelianos be awarded the Nobel Prize for Literature. In 1957, he lost the Prize to Albert Camus by one vote. Camus later said that Kazantzakis deserved the honour "a hundred times more" than himself. In total Kazantzakis was nominated in nine different years. Late in 1957, even though suffering from leukemia, he set out on one last trip to China and Japan. Falling ill on his return flight, he was transferred to Freiburg, Germany, where he died. He is buried on the wall surrounding the city of Heraklion near the Chania Gate, because the Orthodox Church ruled out his being buried in a cemetery. His epitaph reads "I hope for nothing. I fear nothing. I am free." (Δεν ελπίζω τίποτα. Δε φοβούμαι τίποτα. Είμαι λέφτερος.)
The 50th anniversary of the death of Nikos Kazantzakis was selected as main motif for a high value euro collectors' coins; the €10 Greek Nikos Kazantzakis commemorative coin, minted in 2007. His image is shown in the obverse of the coin, while on the reverse the National Emblem of Greece with his signature is depicted.
Literary work.
His first work was the 1906 narrative "Serpent and Lily" (Όφις και Κρίνο), which he signed with the pen name Karma Nirvami. In 1909, Kazantzakis wrote a one-act play titled "Comedy", which remarkably resonates existential themes that become prevalent much later in Post-World War II Europe by writers like Sartre and Camus. In 1910, after his studies in Paris, he wrote a tragedy "The Master Builder" (Ο Πρωτομάστορας), based on a popular Greek folkloric myth. Kazantzakis considered his huge epic poem (33,333 verses long) "" to be his most important work. Begun in 1924, he rewrote it seven times before publishing it in 1938. According to another Greek author, Pantelis Prevelakis, "it has been a superhuman effort to record his immense spiritual experience." Following the structure of Homer's Odyssey, it is divided into 24 rhapsodies.
His most famous novels include "Zorba the Greek" (1946, in Greek Βίος και Πολιτεία του Αλέξη Ζορμπά); "Christ Recrucified" (1948, UK title "Christ Recrucified", in Greek Ο Χριστός Ξανασταυρώνεται); "Captain Michalis" (1950, UK title "Freedom and Death", in Greek Καπετάν Μιχάλης); "The Last Temptation of Christ" (1951, Ο Τελευταίος Πειρασμός); and "Saint Francis" (1956, UK title "God's Pauper: St. Francis of Assisi", in Greek Ο Φτωχούλης του Θεού). "Report to Greco" (1961, Αναφορά στον Γκρέκο), containing both autobiographical and fictional elements, summed up his philosophy as the "Cretan Glance."
Starting in his youth, Kazantzakis was spiritually restless. Tortured by metaphysical and existential concerns, he sought relief in knowledge and travel, contact with a diverse set of people, in every kind of experience. The influence of Friedrich Nietzsche on his work is evident, especially Nietzsche's atheism and sympathy for the superman (Übermensch) concept. However, he was also haunted by spiritual concerns. To attain a union with God, Kazantzakis entered a monastery for six months. In 1927 Kazantzakis published in Greek his "Spiritual Exercises" (Greek: "Ασκητική"), which he had composed in Berlin in 1923. The book was translated into English and published in 1960 with the title "The Saviors of God".
The figure of Jesus was ever-present in his thoughts, from his youth to his last years. The Christ of "The Last Temptation of Christ" shares Katzantzakis' anguished metaphysical and existential concerns, seeking answers to haunting questions and often torn between his sense of duty and mission, on one side, and his own human needs to enjoy life, to love and to be loved, and to have a family. A tragic figure who at the end sacrifices his own human hopes for a wider cause, Kazantzakis' Christ is not an infallible, passionless deity but rather a passionate and emotional human being who has been assigned a mission, with a meaning that he is struggling to understand and that often requires him to face his conscience and his emotions, and ultimately to sacrifice his own life for its fulfilment. He is subject to doubts, fears and even guilt. In the end he is the "Son of Man", a man whose internal struggle represents that of humanity.
The Church of Greece condemned Kazantzakis' work. His reply was: ""You gave me a curse, Holy fathers, I give you a blessing: may your conscience be as clear as mine and may you be as moral and religious as I" "before the Greek Orthodox Church anathematized him in 1955". " (Greek: "Μου δώσατε μια κατάρα, Άγιοι πατέρες, σας δίνω κι εγώ μια ευχή: Σας εύχομαι να ‘ναι η συνείδηση σας τόσο καθαρή, όσο είναι η δική μου και να ‘στε τόσο ηθικοί και θρήσκοι όσο είμαι εγώ"). Many cinemas banned the Martin Scorsese film, which was released in 1988 and based on this novel.
In Kazantzakis' day, the international market for material published in modern Greek was quite small. Kazantzakis also wrote in colloquial Demotic Greek, with traces of Cretan dialect, which made his writings all the more controversial in conservative literary circles at home. Translations of his books into other European languages did not appear until his old age. Hence he found it difficult to earn a living by writing, which led him to write a great deal, including a large number of translations from French, German, and English, and curiosities such as French fiction and Greek primary school texts, mainly because he needed the money. Some of this "popular" writing was nevertheless distinguished, such as his books based on his extensive travels, which appeared in the series "Travelling" (Ταξιδεύοντας) which he founded. These books on Greece, Italy, Egypt, Sinai, Cyprus, Spain, Russia, Japan, China, and England were masterpieces of Greek travel literature.

</doc>
<doc id="51420" url="http://en.wikipedia.org/wiki?curid=51420" title="Carbonic acid">
Carbonic acid

Carbonic acid is the chemical compound with the chemical formula H2CO3 (equivalently OC(OH)2). It is also a name sometimes given to solutions of carbon dioxide in water (carbonated water), because such solutions contain small amounts of H2CO3. In physiology, carbonic acid is described as "volatile acid" or "respiratory acid", because it is the only acid excreted as a gas by the lungs.
Carbonic acid, which is a weak acid, forms two kinds of salts, the carbonates and the bicarbonates. In geology, carbonic acid causes limestone to dissolve producing calcium bicarbonate which leads to many limestone features such as stalactites and stalagmites.
Chemical equilibrium.
When carbon dioxide dissolves in water it exists in chemical equilibrium producing carbonic acid:
The hydration equilibrium constant at 25 °C is called Kh, which in the case of carbonic acid is [H2CO3]/[CO2] ≈ 1.7×10−3 in pure water and ≈ 1.2×10−3 in seawater. Hence, the majority of the carbon dioxide is not converted into carbonic acid, remaining as CO2 molecules. In the absence of a catalyst, the equilibrium is reached quite slowly. The rate constants are 0.039 s−1 for the forward reaction (CO2 + H2O → H2CO3) and 23 s−1 for the reverse reaction (H2CO3 → CO2 + H2O). Carbonic acid is used in the making of soft drinks, inexpensive and artificially carbonated sparkling wines, and other bubbly drinks. The addition of two molecules of water to CO2 would give "orthocarbonic acid", C(OH)4, which exists only in minute amounts in aqueous solution.
Addition of base to an excess of carbonic acid gives bicarbonate (hydrogen carbonate). With excess base, carbonic acid reacts to give carbonate salts.
Role of carbonic acid in blood.
Carbonic acid is an intermediate step in the transport of CO2 out of the body via respiratory gas exchange. The hydration reaction of CO2 is generally very slow in the absence of a catalyst, but red blood cells contain carbonic anhydrase, which both increases the reaction rate and dissociates a hydrogen ion (H+) from the resulting carbonic acid, leaving bicarbonate (HCO3−) dissolved in the blood plasma. This catalysed reaction is reversed in the lungs, where it converts the bicarbonate back into CO2 and allows it to be expelled. This equilibration plays an important role as a buffer in mammalian blood.
Role of carbonic acid in ocean chemistry.
The oceans of the world have absorbed almost half of the CO2 emitted by humans from the burning of fossil fuels.  The extra dissolved carbon dioxide has caused the ocean's average surface pH to shift by about 0.1 unit from pre-industrial levels. This process is known as ocean acidification.
Acidity of carbonic acid.
Carbonic acid is one of the polyprotic acids: It is diprotic - it has two protons, which may dissociate from the parent molecule. Thus, there are two dissociation constants, the first one for the dissociation into the bicarbonate (also called hydrogen carbonate) ion HCO3−:
Care must be taken when quoting and using the first dissociation constant of carbonic acid. In aqueous solution, carbonic acid exists in equilibrium with carbon dioxide, and the concentration of H2CO3 is much lower than the concentration of CO2. In many analyses, H2CO3 includes dissolved CO2 (referred to as CO2(aq)), H2CO3* is used to represent the two species when writing the aqueous chemical equilibrium equation. The equation may be rewritten as follows:
Whereas this apparent p"K"a is quoted as the dissociation constant of carbonic acid, it is ambiguous: it might better be referred to as the acidity constant of dissolved carbon dioxide, as it is particularly useful for calculating the pH of CO2-containing solutions. A similar situation applies to sulfurous acid (H2SO3), which exists in equilibrium with substantial amounts of unhydrated sulfur dioxide.
The second constant is for the dissociation of the bicarbonate ion into the carbonate ion CO32−:
The three acidity constants are defined as follows:
formula_1
pH and composition of carbonic acid solutions.
At a given temperature, the composition of a pure carbonic acid solution (or of a pure CO2 solution) is completely determined by the partial pressure formula_2 of carbon dioxide above the solution. To calculate this composition, account must be taken of the above equilibria between the three different carbonate forms (H2CO3, HCO3− and CO32−) as well as of the hydration equilibrium between dissolved CO2 and H2CO3 with constant formula_3 (see above) and of the following equilibrium between the dissolved CO2 and the gaseous CO2 above the solution:
The corresponding equilibrium equations together with the formula_5 relation and the charge neutrality condition formula_6 result in six equations for the six unknowns [CO2], [H2CO3], [H+], [OH−], [HCO3−] and [CO32−], showing that the composition of the solution is fully determined by formula_2. The equation obtained for [H+] is a cubic whose numerical solution yields the following values for the pH and the different species concentrations:
Remark
Spectroscopic studies of carbonic acid.
Theoretical calculations show that the presence of even a single molecule of water causes carbonic acid to revert to carbon dioxide and water. In the absence of water, the dissociation of gaseous carbonic acid is predicted to be very slow, with a half-life of 180,000 years. This may only apply to isolated carbonic acid molecules however, as it has been predicted to catalyze its own decomposition
It has long been recognized that pure carbonic acid cannot be obtained at room temperatures (about 20 °C or about 70 °F). It can be generated by exposing a frozen mixture of water and carbon dioxide to high-energy radiation, and then warming to remove the excess water. The carbonic acid that remained was characterized by infrared spectroscopy. The fact that the carbonic acid was prepared by irradiating a solid H2O + CO2 mixture may suggest that H2CO3 might be found in outer space, where frozen ices of H2O and CO2 are common, as are cosmic rays and ultraviolet light, to help them react. The same carbonic acid polymorph (denoted "beta"-carbonic acid) was prepared by heating alternating layers of glassy aqueous solutions of bicarbonate and acid "in vacuo", which causes protonation of bicarbonate, followed by removal of the solvent. The previously suggested "alpha"-carbonic acid, which was prepared by the same technique using methanol rather than water as a solvent was shown to be a monomethyl ester CH3OCOOH.

</doc>
<doc id="51421" url="http://en.wikipedia.org/wiki?curid=51421" title="Integer sequence">
Integer sequence

In mathematics, an integer sequence is a sequence (i.e., an ordered list) of integers.
An integer sequence may be specified "explicitly" by giving a formula for its "n"th term, or "implicitly" by giving a relationship between its terms. For example, the sequence 0, 1, 1, 2, 3, 5, 8, 13, … (the Fibonacci sequence) is formed by starting with 0 and 1 and then adding any two consecutive terms to obtain the next one: an implicit description. The sequence 0, 3, 8, 15, … is formed according to the formula "n"2 − 1 for the "n"th term: an explicit definition.
Alternatively, an integer sequence may be defined by a property which members of the sequence possess and other integers do not possess. For example, we can determine whether a given integer is a perfect number, even though we do not have a formula for the "n"th perfect number.
Examples.
Integer sequences which have received their own name include:
Computable and definable sequences.
An integer sequence is a computable sequence, if there exists an algorithm which given "n", calculates "a""n", for all "n" > 0. An integer sequence is a definable sequence, if there exists some statement "P"("x") which is true for that integer sequence "x" and false for all other integer sequences. The set of computable integer sequences and definable integer sequences are both countable, with the computable sequences a proper subset of the definable sequences (in other words, some sequences are definable but not computable). The set of all integer sequences is uncountable (with cardinality equal to that of the continuum); thus, almost all integer sequences are uncomputable and cannot be defined.
Complete sequences.
An integer sequence is called a complete sequence if every positive integer can be expressed as a sum of values in the sequence, using each value at most once.

</doc>
<doc id="51423" url="http://en.wikipedia.org/wiki?curid=51423" title="P-adic number">
P-adic number

In mathematics the p-adic number system for any prime number p extends the ordinary arithmetic of the rational numbers in a way different from the extension of the rational number system to the real and complex number systems. The extension is achieved by an alternative interpretation of the concept of "closeness" or absolute value. In particular, p-adic numbers have the interesting property that they are said to be close when their difference is divisible by a high power of p – the higher the power the closer they are. This property enables p-adic numbers to encode congruence information in a way that turns out to have powerful applications in number theory including, for example, in the famous proof of Fermat's Last Theorem by Andrew Wiles.
p-adic numbers were first described by Kurt Hensel in 1897, though with hindsight some of Kummer's earlier work can be interpreted as implicitly using p-adic numbers. The p-adic numbers were motivated primarily by an attempt to bring the ideas and techniques of power series methods into number theory. Their influence now extends far beyond this. For example, the field of p-adic analysis essentially provides an alternative form of calculus.
More formally, for a given prime p, the field Q"p" of p-adic numbers is a completion of the rational numbers. The field Q"p" is also given a topology derived from a metric, which is itself derived from the p-adic order, an alternative valuation on the rational numbers. This metric space is complete in the sense that every Cauchy sequence converges to a point in Q"p". This is what allows the development of calculus on Q"p", and it is the interaction of this analytic and algebraic structure which gives the p-adic number systems their power and utility.
The p in "p-adic" is a variable and may be replaced with a prime (yielding, for instance, "the 2-adic numbers") or another "placeholder variable" (for expressions such as "the ℓ-adic numbers"). The "adic" of ""p"-adic" comes from the ending found in words such as dyadic or triadic, and the p means a prime number.
Introduction.
"This section is an informal introduction to p-adic numbers, using examples from the ring of 10-adic (decadic) numbers. Although for p-adic numbers p should be a prime, base 10 was chosen to highlight the analogy with decimals. The decadic numbers are generally not used in mathematics: since 10 is not prime, the decadics are not a field. More formal constructions and properties are given below."
In the standard decimal representation, almost all real numbers do not have a terminating decimal representation. For example, 1/3 is represented as a non-terminating decimal as follows
Informally, non-terminating decimals are easily understood, because it is clear that a real number can be approximated to any required degree of precision by a terminating decimal. If two decimal expansions differ only after the 10th decimal place, they are quite close to one another; and if they differ only after the 20th decimal place, they are even closer.
10-adic numbers use a similar non-terminating expansion, but with a different concept of "closeness". Whereas two decimal expansions are close to one another if their difference is a large negative power of 10, two 10-adic expansions are close if their difference is a large positive power of 10. Thus 3333 and 4333, which differ by 103, are close in the 10-adic world, and 33333333 and 43333333 are even closer, differing by 107.
More precisely, a rational number r can be expressed as 10"e"·"p"/"q", where p and q are positive integers and q is relatively prime to p and to 10. For each "r" ≠ 0 there exists the maximal e such that this representation is possible. Let the 10-adic norm of r to be 
Closeness in any number system is defined by a metric. Using the 10-adic metric the distance between numbers x and y is given by |x − y|10. An interesting consequence of the 10-adic metric (or of a p-adic metric) is that there is no longer a need for the negative sign. As an example, by examining the following sequence we can see how unsigned 10-adics can get progressively closer and closer to the number −1:
and taking this sequence to its limit, we can say that the 10-adic expansion of −1 is
In this notation, 10-adic expansions can be extended indefinitely to the left, in contrast to decimal expansions, which can be extended indefinitely to the right. Note that this is not the only way to write p-adic numbers – for alternatives see the "Notation" section below.
More formally, a 10-adic number can be defined as
where each of the "a""i" is a digit taken from the set {0, 1, … , 9} and the initial index n may be positive, negative or 0, but must be finite. From this definition, it is clear that positive integers and positive rational numbers with terminating decimal expansions will have terminating 10-adic expansions that are identical to their decimal expansions. Other numbers may have non-terminating 10-adic expansions.
It is possible to define addition, subtraction, and multiplication on 10-adic numbers in a consistent way, so that the 10-adic numbers form a commutative ring.
We can create 10-adic expansions for negative numbers as follows
and fractions which have non-terminating decimal expansions also have non-terminating 10-adic expansions. For example
Generalizing the last example, we can find a 10-adic expansion with no digits to the right of the decimal point for any rational number "p"⁄"q" such that q is co-prime to 10; Euler's theorem guarantees that if q is co-prime to 10, then there is an n such that 10"n" − 1 is a multiple of q. The other rational numbers can be expressed as 10-adic numbers with some digits after the decimal point.
As noted above, 10-adic numbers have a major drawback. It is possible to find pairs of non-zero 10-adic numbers (having an infinite number of digits, and thus not rational) whose product is 0. This means that 10-adic numbers do not always have multiplicative inverses i.e. valid reciprocals, which in turn implies that though 10-adic numbers form a ring they do not form a field, a deficiency that makes them much less useful as an analytical tool. Another way of saying this is that the ring of 10-adic numbers is not an integral domain because they contain zero divisors. The reason for this property turns out to be that 10 is a composite number which is not a power of a prime. This problem is simply avoided by using a prime number p as the base of the number system instead of 10 and indeed for this reason p in p-adic is usually taken to be prime.
"p"-adic expansions.
When dealing with natural numbers, if we take p to be a fixed prime number, then any positive integer can be written as a base p expansion in the form
where the "a""i" are integers in {0, … , "p" − 1}. For example, the binary expansion of 35 is 1·25 + 0·24 + 0·23 + 0·22 + 1·21 + 1·20, often written in the shorthand notation 1000112.
The familiar approach to extending this description to the larger domain of the rationals (and, ultimately, to the reals) is to use sums of the form:
A definite meaning is given to these sums based on Cauchy sequences, using the absolute value as metric. Thus, for example, 1/3 can be expressed in base 5 as the limit of the sequence 0.1313131313...5. In this formulation, the integers are precisely those numbers for which "a""i" = 0 for all "i" < 0.
With "p"-adic numbers, on the other hand, we choose to extend the base p expansions in a different way. Unlike traditional integers, where the magnitude is determined by how far they are from zero, the "size" of p-adic numbers is determined by the p-adic Norm, where high positive powers of p are relatively small compared to high negative powers of p. Consider infinite sums of the form:
where "k" is some (not necessarily positive) integer, and each coefficient formula_23 can be called a p-adic digit. With this approach we obtain the p-adic expansions of the p-adic numbers. Those p-adic numbers for which "a""i" = 0 for all "i" < 0 are also called the p-adic integers.
As opposed to real number expansions which extend to the "right" as sums of ever smaller, increasingly negative powers of the base p, p-adic numbers may expand to the "left" forever, a property that can often be true for the p-adic integers. For example, consider the p-adic expansion of 1/3 in base 5. It can be shown to be …13131325, i.e., the limit of the sequence 25, 325, 1325, 31325, 131325, 3131325, 13131325, … :
Multiplying this infinite sum by 3 in base 5 gives …00000015. As there are no negative powers of 5 in this expansion of 1/3 (i.e. no numbers to the right of the decimal point), we see that 1/3 satisfies the definition of being a p-adic integer in base 5.
More formally, the p-adic expansions can be used to define the field Q"p" of p-adic numbers while the p-adic integers form a subring of Q"p", denoted Z"p". (Not to be confused with the ring of integers modulo p which is also sometimes written Z"p". To avoid ambiguity, Z/"p"Z or Z/"(p)" are often used to represent the integers modulo p.)
While it is possible to use the approach above to define p-adic numbers and explore their properties, just as in the case of real numbers other approaches are generally preferred. Hence we want to define a notion of infinite sum which makes these expressions meaningful, and this is most easily accomplished by the introduction of the p-adic metric. Two different but equivalent solutions to this problem are presented in the "Constructions" section below.
Notation.
There are several different conventions for writing p-adic expansions. So far this article has used a notation for p-adic expansions in which powers of p increase from right to left. With this right-to-left notation the 3-adic expansion of 1⁄5, for example, is written as
When performing arithmetic in this notation, digits are carried to the left. It is also possible to write p-adic expansions so that the powers of p increase from left to right, and digits are carried to the right. With this left-to-right notation the 3-adic expansion of 1⁄5 is
p-adic expansions may be written with other sets of digits instead of {0, 1, …, "p" − 1}. For example, the 3-adic expansion of 1/5 can be written using balanced ternary digits {1,0,1} as
In fact any set of p integers which are in distinct residue classes modulo p may be used as p-adic digits. In number theory, Teichmüller representatives are sometimes used as digits.
Constructions.
Analytic approach.
The real numbers can be defined as equivalence classes of Cauchy sequences of rational numbers; this allows us to, for example, write 1 as 1.000… = 0.999… . The definition of a Cauchy sequence relies on the metric chosen, though, so if we choose a different one, we can construct numbers other than the real numbers. The usual metric which yields the real numbers is called the Euclidean metric.
For a given prime p, we define the "p-adic absolute value" in Q as follows:
for any non-zero rational number x, there is a unique integer n allowing us to write "x" = "p""n"("a"/"b"), where neither of the integers "a" and "b" is divisible by p. Unless the numerator or denominator of x in lowest terms contains p as a factor, n will be 0. Now define |"x"|"p" = "p"−"n". We also define |0|"p" = 0.
For example with "x" = 63/550 = 2−1·32·5−2·7·11−1<br>
This definition of |"x"|"p" has the effect that high powers of p become "small".
By the fundamental theorem of arithmetic, for a given non-zero rational number "x" there is a unique finite set of distinct primes formula_32 and a corresponding sequence of non-zero integers formula_33 such that:
It then follows that formula_35 for all formula_36, and formula_37 for any other prime formula_38
It is a theorem of Ostrowski that each absolute value on Q is equivalent either to the Euclidean absolute value, the trivial absolute value, or to one of the p-adic absolute values for some prime p. So the only norms on Q modulo equivalence are the absolute value, the trivial absolute value and the p-adic absolute value which means that there are only as many completions (with respect to a norm) of Q.
The p-adic absolute value defines a metric d"p" on Q by setting
The field Q"p" of p-adic numbers can then be defined as the completion of the metric space (Q, d"p"); its elements are equivalence classes of Cauchy sequences, where two sequences are called equivalent if their difference converges to zero. In this way, we obtain a complete metric space which is also a field and contains Q.
It can be shown that in Q"p", every element "x" may be written in a unique way as
where "k" is some integer such that "a""k" ≠ "0" and each "a""i" is in {0, …, "p" − 1 }. This series converges to "x" with respect to the metric d"p".
With this absolute value, the field Q"p" is a local field.
Algebraic approach.
In the algebraic approach, we first define the ring of p-adic integers, and then construct the field of fractions of this ring to get the field of p-adic numbers.
We start with the inverse limit of the rings
Z/"p""n"Z (see modular arithmetic): a p-adic integer is then a sequence
("an")"n"≥1 such that "a""n" is in
Z/"p""n"Z, and if "n" ≤ "m", then
"an" ≡ "a""m" (mod "p""n").
Every natural number "m" defines such a sequence ("a""n") by "a""n" = "m" mod "pn" and can therefore be regarded as a p-adic integer. For example, in this case 35 as a 2-adic integer would be written as the sequence (1, 3, 3, 3, 3, 35, 35, 35, …).
The operators of the ring amount to pointwise addition and multiplication of such sequences. This is well defined because addition and multiplication commute with the "mod" operator; see modular arithmetic.
Moreover, every sequence ("a""n") where the first element is not 0 has an inverse. In that case, for every "n", "an" and "p" are coprime, and so "an" and "pn" are relatively prime. Therefore, each "an" has an inverse mod "pn", and the sequence of these inverses, ("bn"), is the sought inverse of ("an"). For example, consider the p-adic integer corresponding to the natural number 7; as a 2-adic number, it would be written (1, 3, 7, 7, 7, 7, 7, ...). This object's inverse would be written as an ever-increasing sequence that begins (1, 3, 7, 7, 23, 55, 55, 183, 439, 439, 1463 ...). Naturally, this 2-adic integer has no corresponding natural number.
Every such sequence can alternatively be written as a series. For instance, in the 3-adics, the sequence (2, 8, 8, 35, 35, ...) can be written as 2 + 2·3 + 0·32 + 1·33 + 0·34 + ... The partial sums of this latter series are the elements of the given sequence.
The ring of p-adic integers has no zero divisors, so we can take the field of fractions to get the field Q"p" of p-adic numbers. Note that in this field of fractions, every non-integer p-adic number can be uniquely written as "p"−"n"" u" with a natural number "n" and a unit in the p-adic integers "u". This means that
Note that "S"−1 "A", where formula_42 is a multiplicative subset (contains the unit and closed under multiplication) of a commutative ring with unit formula_43, is an algebraic construction called the ring of fractions or localization of formula_43 by formula_45.
Properties.
Cardinality.
Z"p" is the inverse limit of the finite rings Z/"p""k" Z, but is nonetheless uncountable, and has the cardinality of the continuum. Accordingly, the field Q"p" is uncountable. The endomorphism ring of the Prüfer p-group of rank n, denoted Z("p"∞)"n", is the ring of "n" × "n" matrices over Z"p"; this is sometimes referred to as the Tate module.
Topology.
Define a topology on Z"p" by taking as a basis of open sets all sets of the form
where "a" is a non-negative integer and "n" is an integer in [1, "p"a]. For example, in the dyadic integers, U1(1) is the set of odd numbers. Ua("n") is the set of all "p"-adic integers whose difference from "n" has "p"-adic absolute value less than "p"1−a. Then Z"p" is a compactification of Z, under the derived topology (it is "not" a compactification of Z with its usual discrete topology). The relative topology on Z as a subset of Z"p" is called the p-adic topology on Z.
The topology of Z"p" is that of a Cantor set. For instance, we can make a continuous 1-to-1 mapping between the dyadic integers and the Cantor set expressed in base 3 by mapping
formula_46
in Z2 to
formula_47
in C, where
formula_48.
Using a different mapping, in which the integers go to just part of the Cantor set, one can show that the topology of Q"p" is that of a Cantor set minus a point (such as the right-most point). In particular, Z"p" is compact while Q"p" is not; it is only locally compact. As metric spaces, both Z"p" and Q"p" are complete.
Metric completions and algebraic closures.
Q"p" contains Q and is a field of characteristic 0. This field cannot be turned into an ordered field.
R has only a single proper algebraic extension: C; in other words, this quadratic extension is already algebraically closed. By contrast, the algebraic closure of Q"p", denoted Q"p", has infinite degree, i.e. Q"p" has infinitely many inequivalent algebraic extensions. Also contrasting the case of real numbers, although there is a unique extension of the p-adic valuation to Q"p", the latter is not (metrically) complete. Its (metric) completion is called C"p" or Ω"p". Here an end is reached, as C"p" is algebraically closed. However unlike C this field is not locally compact.
C"p" and C are isomorphic as fields, so we may regard C"p" as C endowed with an exotic metric. It should be noted that the proof of existence of such a field isomorphism relies on the axiom of choice, and does not provide an explicit example of such an isomorphism.
If K is a finite Galois extension of Q"p", the Galois group Gal(K/Q"p") is solvable. Thus, the Galois group Gal(Q"p"/Q"p") is prosolvable.
Multiplicative group of Q"p".
Q"p" contains the n-th cyclotomic field ("n" > 2) if and only if "n" | "p" − 1. For instance, the n-th cyclotomic field is a subfield of Q13 if and only if , or 12. In particular, there is no multiplicative p-torsion in Q"p", if "p" > 2. Also, −1 is the only non-trivial torsion element in Q2.
Given a natural number k, the index of the multiplicative group of the k-th powers of the non-zero elements of Q"p" in Q is finite.
The number e, defined as the sum of reciprocals of factorials, is not a member of any p-adic field; but "e p" ∈ Q"p" ("p" ≠ 2). For one must take at least the fourth power. (Thus a number with similar properties as e – namely a p-th root of "e p" – is a member of Q"p" for all p.)
Analysis on Q"p".
The only real functions whose derivative is zero are the constant functions. This is not true over Q"p". For instance, the function
has zero derivative everywhere but is not even locally constant at 0.
If we let R be denoted Q∞, then given any elements "r"∞, "r"2, "r"3, "r"5, "r"7, ... where "rp" ∈ Q"p", it is possible to find a sequence ("xn") in Q such that for all p (including ∞), the limit of "xn" in Q"p" is "rp".
Rational arithmetic.
Eric Hehner and Nigel Horspool proposed in 1979 the use of a p-adic representation for rational numbers on computers called Quote notation. The primary advantage of such a representation is that addition, subtraction, and multiplication can be done in a straightforward manner analogous to similar methods for binary integers; and division is even simpler, resembling multiplication. However, it has the disadvantage that representations can be much larger than simply storing the numerator and denominator in binary; for example, if 2"n" − 1 is a Mersenne prime, its reciprocal will require 2"n" − 1 bits to represent.
Generalizations and related concepts.
The reals and the p-adic numbers are the completions of the rationals; it is also possible to complete other fields, for instance general algebraic number fields, in an analogous way. This will be described now.
Suppose "D" is a Dedekind domain and "E" is its field of fractions. Pick a non-zero prime ideal "P" of "D". If "x" is a non-zero element of "E", then "xD" is a fractional ideal and can be uniquely factored as a product of positive and negative powers of non-zero prime ideals of "D". We write ord"P"("x") for the exponent of "P" in this factorization, and for any choice of number "c" greater than 1 we can set
Completing with respect to this absolute value |.|"P" yields a field "E""P", the proper generalization of the field of "p"-adic numbers to this setting. The choice of "c" does not change the completion (different choices yield the same concept of Cauchy sequence, so the same completion). It is convenient, when the residue field "D"/"P" is finite, to take for "c" the size of "D"/"P".
For example, when "E" is a number field, Ostrowski's theorem says that every non-trivial non-Archimedean absolute value on "E" arises as some |.|"P". The remaining non-trivial absolute values on "E" arise from the different embeddings of "E" into the real or complex numbers. (In fact, the non-Archimedean absolute values can be considered as simply the different embeddings of "E" into the fields C"p", thus putting the description of all
the non-trivial absolute values of a number field on a common footing.)
Often, one needs to simultaneously keep track of all the above-mentioned completions when "E" is a number field (or more generally a global field), which are seen as encoding "local" information. This is accomplished by adele rings and idele groups.
Local–global principle.
Helmut Hasse's local–global principle is said to hold for an equation if it can be solved over the rational numbers if and only if it can be solved over the real numbers and over the p-adic numbers for every prime p. This principle holds e.g. for equations given by quadratic forms, but fails for higher polynomials in several indeterminates.

</doc>
<doc id="51425" url="http://en.wikipedia.org/wiki?curid=51425" title="Somaliland">
Somaliland

 
Somaliland (Somali: "Somaliland", Arabic: صوماليلاند‎ "Ṣūmālīlānd" or أرض الصومال "Arḍ aṣ-Ṣūmāl") is a self-declared independent state that is internationally recognized as an autonomous region of Somalia. The government of Somaliland regards itself as the successor state to the former British Somaliland protectorate, which as the State of Somaliland united as scheduled on 1 July 1960 with the Trust Territory of Somaliland (the former Italian Somaliland) to form the Somali Republic (Somalia).
Somaliland lies in northwestern Somalia, on the southern coast of the Gulf of Aden. It is bordered by the autonomous Puntland region of Somalia to the east, Djibouti to the northwest, and Ethiopia to the south and west. Its claimed territory has an area of 137600 km2, with approximately 3.5 million residents. The capital and the largest city is Hargeisa, with the population of around 750,000 residents.
In 1988, the Siad Barre regime launched a clampdown against the Hargeisa-based Somali National Movement (SNM) and other rebel outfits, which were among the events that led to the Somali Civil War. The conflict left the economic and military infrastructure severely damaged. After the collapse of the central government in 1991, the local government, led by the SNM, declared independence from the rest of Somalia on 18 May of the same year.
Since then, the territory has been governed by an administration that seeks self-determination as the Republic of Somaliland (Somali: "Jamhuuriyadda Somaliland", Arabic: جمهورية صوماليلاند‎ "Jumhūrīyat Ṣūmālīlānd)". The local government maintains informal ties with some foreign governments, who have sent delegations to Hargeisa. Ethiopia also maintains a trade office in the region. However, Somaliland's self-proclaimed independence remains unrecognised by any country or international organisation. It is a member of the Unrepresented Nations and Peoples Organization, whose members consist of indigenous peoples, minorities, and unrecognised or occupied territories.
History.
The earliest human artifacts in the area are the Laas Geel cave paintings, dating from before 3000 BC. The region is sometimes thought to be part of the Land of Punt. Islam was introduced to the northern Somali littoral early on from the Arabian peninsula, shortly after the hijra. Various Muslim Somali kingdoms were formed around this period in the area. Centuries later, in the 1500s, the Ottoman Empire occupied Berbera and environs. Muhammad Ali, Pasha of Egypt, subsequently established a foothold in the area between 1821 and 1841.
In 1888, after signing successive treaties with the then ruling Somali Sultans such as Mohamoud Ali Shire of the Warsangali Sultanate, the British established a protectorate in the region referred to as British Somaliland. The British garrisoned the protectorate from Aden and administered it as part of British India until 1898. British Somaliland was then administered by the Foreign Office until 1905, and afterwards by the Colonial Office.
Generally, the British did not have much interest in the resource-barren region. The stated purposes of the establishment of the protectorate were to "secure a supply market, check the traffic in slaves, and to exclude the interference of foreign powers." The British principally viewed the protectorate as a source for supplies of meat for their British Indian outpost in Aden through the maintenance of order in the coastal areas and protection of the caravan routes from the interior. Hence, the region's nickname of "Aden's butcher's shop". Colonial administration during this period did not extend administrative infrastructure beyond the coast, and contrasted with the more interventionist colonial experience of Italian Somaliland.
On 1 July 1960, the protectore and the Trust Territory of Somaliland (the former Italian Somaliland) united as planned to form the Somali Republic. A government was formed by Abdullahi Issa, with Aden Abdullah Osman Daar as President and Abdirashid Ali Shermarke as Prime Minister (later to become President, from 1967 to 1969). On 20 July 1961 and through a popular referendum, the Somali people ratified a new constitution, which was first drafted in 1960. In 1967, Muhammad Haji Ibrahim Egal became Prime Minister, a position to which he was appointed by Shermarke. Shermarke would be assassinated two years later by one of his own bodyguards. His murder was quickly followed by a military coup d'état on 21 October 1969 (the day after his funeral), in which the Somalian Army seized power without encountering armed opposition. The putsch was spearheaded by Major General Mohamed Siad Barre, who at the time commanded the army. The new regime would go on to rule Somalia for the next 21 years.
The moral authority of Barre's government gradually eroded, as many Somalis had become disillusioned with life under military rule. By the mid-1980s, resistance movements supported by Ethiopia's communist Derg administration had sprung up across the country. Barre responded by ordering punitive measures against those he perceived as locally supporting the guerillas, especially in the northern regions. The clampdown included bombing of cities, with the northwestern administrative center of Hargeisa, a Somali National Movement (SNM) stronghold, among the targeted areas in 1988. The bombardment was led by General Mohammed Said Hersi Morgan, Barre's son-in-law.
Although the SNM at its inception had a unionist constitution, it eventually began to pursue independence, looking to secede from the rest of Somalia. Under the leadership of Abdirahman Ahmed Ali Tuur, the local administration declared the northwestern Somali territories independent at a conference held in Burao between 27 April 1991 and 15 May 1991. Tuur then became the newly established Somaliland polity's first President, but subsequently renounced the separatist platform in 1994 and began instead to publicly seek and advocate reconciliation with the rest of Somalia under a power-sharing federal system of governance. Muhammad Haji Ibrahim Egal was appointed as Tuur's successor in 1993 by the Grand Conference of National Reconciliation in Borama, which met for four months, leading to a gradual improvement in security, as well as a consolidation of the new territory. Egal was reappointed in 1997, and remained in power until his death on 3 May 2002. The vice president, Dahir Riyale Kahin, who was during the 1980s the highest-ranking National Security Service (NSS) officer in Berbera in Siad Barre's government, was sworn in as president shortly afterwards. In 2003, Kahin became the first elected president of Somaliland.
The war in southern Somalia between Islamist insurgents on the one hand, and the Federal Government of Somalia and its African Union allies on the other, has for the most part not directly affected Somaliland, which, like neighboring Puntland, has remained relatively stable.
Politics and government.
Under the Federal Constitution of Somalia, Somaliland is officially a Federal Member State of the Federal Republic of Somalia. It has a hybrid system of governance under the Constitution of Somaliland, combining traditional and western institutions. In a series of inter-clan conferences, culminating in the Boorama Conference in 1993, a "qabil" (clan or community) system of government was constructed. The constitution separates government into an executive branch, a legislative branch, and a judicial branch, each of which functions independently from the others.
The "guurti" worked with rebel leaders to set up a new government, and was incorporated into the governance structure, becoming the Parliament's House of Elders. The government became in essence a "power-sharing coalition of Somaliland's main clans," with seats in the Upper and Lower houses proportionally allocated to clans according to a predetermined formula, although not all clans are satisfied with their representation. In 2002, after several extensions of this interim government, Somaliland transitioned to multi-party democracy. The election was limited to three parties, in an attempt to create ideology based elections rather than clan based elections.
The Executive is led by an elected president, whose government includes a vice-president and a Council of Ministers. The Council of Ministers, who are responsible for the normal running of government, are nominated by the President and approved by the Parliament's House of Representatives. The President must approve bills passed by the Parliament before they come into effect. Presidential elections are confirmed by the National Elections Commission. The President can serve a maximum of two five-year terms.
Legislative power is held by the bicameral Parliament. Its upper house is the House of Elders, and the lower house is the House of Representatives. The lower house is chaired by Abdirahman Mohamed Abdullahi. Each house has 82 members. Members of the House of Elders are elected indirectly by local communities for six-year terms. The House of Elders shares power in passing laws with the House of Representatives, and also has the role of solving internal conflicts, and an exclusive power to extend the terms of the President and representatives under circumstances that make an election impossible. Members of the House of Representatives are directly elected by the people for five-year terms. The House of Representatives shares voting power with the House of Elders, though it can pass a law that the House of Elders rejects if it votes for the law by a 2/3's majority, and has absolute power in financial matters and confirmation of Presidential appointments (except for the Chief Justice of the Supreme Court). However, the Parliament provides weak oversight of the executive branch.
The judicial system is divided into district courts, (which deal with matters of family law and succession, lawsuits for amounts up to 3 million SL, criminal cases punishable by up to 3 years imprisonment or 3 million SL fines, and crimes committed by juveniles), regional courts (which deal with lawsuits and criminal cases not within the jurisdiction of district courts, labour and employment claims, and local government elections), regional appeals courts (which deal with all appeals from district and regional courts), and the Supreme Court (which deals with issues between courts and in government, and reviews its own decisions), which is the highest court and also functions as the Constitutional Court.
As of December 2014, Somaliland has three political parties: the Peace, Unity, and Development Party, the Justice and Development Party, and Wadani. Under the Somaliland Constitution, a maximum of three political parties are allowed, and parties defined by religion or clan are prohibited, though all official parties are closely affiliated with a clan.
Foreign relations.
Somaliland has political contacts with its neighbours Ethiopia and Djibouti, as well as with South Africa, Sweden, and the United Kingdom. On 17 January 2007, the European Union (EU) sent a delegation for foreign affairs to discuss future cooperation. The African Union (AU) has also sent a foreign minister to discuss the future of international acknowledgment, and on 29 and 30 January 2007, the ministers stated that they would discuss acknowledgement with the organisation's member states.
In early 2006, the National Assembly of Wales extended an official invitation to the Somaliland government to attend the royal opening of the Senedd in Cardiff. The move was seen as an act of recognition by the Welsh Assembly of the breakaway government's legitimacy. The Foreign and Commonwealth Office made no comment on the invitation. Wales is home to a significant Somali expatriate community from Somaliland.
In 2007, a delegation led by President Kahin was present at the Commonwealth Heads of Government Meeting in Kampala, Uganda. Although Somaliland has applied to join the Commonwealth under observer status, its application is still pending.
On 24 September 2010, Johnnie Carson, Assistant Secretary of State for African Affairs, stated that the United States would be modifying its strategy in Somalia and would seek deeper engagement with the governments of Somaliland and Puntland while continuing to support the Somali Transitional Government. Carson said the US would send aid workers and diplomats to Puntland and Somaliland and alluded to the possibility of future development projects. However, Carson emphasized that the U.S. would not extend formal recognition to either region.
The then UK Minister for Africa, Henry Bellingham MP, met President Silanyo of Somaliland in November 2010 to discuss ways in which to increase the UK's engagement with Somaliland. President Silanyo said during his visit to London: "We have been working with the international community and the international community has been engaging with us, giving us assistance and working with us in our democratisation and development programmes. And we are very happy with the way the international community has been dealing with us, particularly the UK, the US, other European nations and our neighbours who continue to seek recognition."
In 2011, Somaliland and the neighbouring Puntland region each entered a security-related memorandum of understanding with the Seychelles. Following the framework of an earlier agreement signed between the Transitional Federal Government and the Seychelles, the memorandum is "for the transfer of convicted persons to prisons in 'Puntland' and 'Somaliland'".
Border disputes.
Somaliland continues to claim the entire area of the former British Somaliland. It is currently in control of the western half of the former British Somaliland, with northeastern Maakhir having declared itself a separate, unrecognised autonomous state within Somalia in July 2007, and the disputed southeastern Sool state had been under the control of neighbouring Puntland. A coalition of Gadabuursi intellectuals hailing from the westernmost Awdal province have threatened to secede if Somaliland's independence is recognised.
Tensions between Puntland and Somaliland escalated into violence several times between 2002 and 2009. In October 2004, and again in April and October 2007, armed forces of Somaliland and Puntland clashed near the town of Las Anod, the capital of Sool region. In October 2007, Somaliland troops took control of the town. While celebrating Puntland's 11th anniversary on 2 August 2009, Puntland officials vowed to recapture Las Anod. While Somaliland claims independent statehood and therefore "split up" the "old" Somalia, Puntland works for the re-establishment of a united but federal Somali state.
Somaliland forces took control of the town of Las Qorey in eastern Sanaag on 10 July 2008, along with positions five kilometers east of the town. The defence forces completed their operations on 9 July 2008 after the Maakhir and Puntland militia in the area left their positions, but control of the territory was later assumed by Puntland as Maakhir was incorporated into the autonomous region in January 2009.
In the late 2000s, HBM-SSC (Hoggaanka Badbaadada iyo Mideynta SSC), a local unionist group based in Sanaag was formed with the goal to establish its own regional administration (Sool, Sanaag and Cayn, or SSC). This later evolved into Khatumo State, which was established in 2012. The local administration and its constituents does not recognise the Somaliland government's claim to sovereignty or to its territory.
In 2010, the formation of a new autonomous region within a federal Somalia was also declared in the Awdal province. Referred to as Awdalland or the "Awdal State", the local administration and the region's residents do not recognise the Somaliland government's claim to sovereignty or to their territory.
Military.
The Somaliland Armed Forces are the main military command in Somaliland. Along with the Police Force and all other internal security forces, they are overseen by Somaliland's Ministry of Defence. The current head of Somaliland's Armed Forces is the Minister of Defence, Mudane Ahmed Haj Adami.
The Somaliland Army consists of twelve divisions equipped primarily with light weaponry, though it is equipped with some howitzers and mobile rocket launchers. Its armored vehicles and tanks are mostly of Soviet design, though there are some aging Western vehicles and tanks in its arsenal. The Somaliland Navy (often referred to as a Coast Guard by the Associated Press), despite a crippling lack of equipment and formal training, has apparently had some success at curbing both piracy and illegal fishing within Somaliland waters.
Administrative divisions.
Regions.
16 new Districts:
Geography.
Somaliland is situated in northwestern Somalia. It lies between the 08°00' – 11°30' parallel north of the equator and between 42°30' – 49°00' meridian east of Greenwich. It is bordered by Djibouti to the west, Ethiopia to the south, and the Puntland region of Somalia to the east. Somaliland has a 740 km coastline with the majority lying along the Gulf of Aden. The region is slightly larger than England, with an area of 137600 km2.
Somaliland's climate is a mixture of wet and dry conditions. The northern part of the region is hilly, and in many places the altitude ranges between 900 and above sea level. The Awdal, Sahil and Maroodi Jeex (Woqooyi Galbeed) regions are fertile and mountainous, while Togdheer is mostly semi-desert with little fertile greenery around. The Awdal region is also known for its offshore islands, coral reefs and mangroves.
A scrub-covered, semi-desert plain referred as the "Guban" lies parallel to the Gulf of Aden littoral. With a width of twelve kilometers in the west to as little as two kilometers in the east, the plain is bisected by watercourses that are essentially beds of dry sand except during the rainy seasons. When the rains arrive, the Guban's low bushes and grass clumps transform into lush vegetation. This coastal strip is part of the Ethiopian xeric grasslands and shrublands ecoregion.
Cal Madow is a mountain range in the northern part of the country. Extending from the northwest of Erigavo to several kilometers west of the city of Bosaso, it features Somalia's highest peak, Shimbiris, which sits at an elevation of about 2416 m. The rugged east-west ranges of the Karkaar Mountains also lie to the interior of the Gulf of Aden littoral. In the central regions, the northern mountain ranges give way to shallow plateaus and typically dry watercourses that are referred to locally as the "Ogo". The Ogo's western plateau, in turn, gradually merges into the Haud, an important grazing area for livestock.
Economy.
The Somaliland shilling, is not an internationally recognised currency and currently has no official exchange rate. It is not a valid tender in the Awdal, Ayn, Sanaag, or Sool regions; all of which is the Somali Shilling despite being claimed by the Somaliland regional government. It is regulated by the Bank of Somaliland, the central bank, which was established constitutionally in 1994.
Since Somaliland is unrecognised, international aid donors have found it difficult to provide aid. As a result, the government relies mainly upon tax receipts and remittances from the large Somali diaspora contribute immensely to Somaliland's economy. Remittances come to Somaliland through money transfer companies, the largest of which is Dahabshiil, one of the few Somali money transfer companies to conform to modern money-transfer regulations. The World Bank estimates that remittances worth approximately $US 1 billion reach Somalia annually from emigres working in the Gulf states, Europe and the United States. Analysts say that Dahabshiil may handle around two-thirds of that figure, and that as much as half of it reaches Somaliland alone.
Since the late 1990s, service provision has significantly improved as a result of limited government provision and contributions from non-governmental organisations, religious groups, the international community (especially the Diaspora) and the growing private sector. Local and municipal governments have been developing the provision of key public services, such as water in Hargeisa and education, electricity and security in Berbera. In 2009, the Banque pour le Commerce et l'Industrie – Mer Rouge, based in Djibouti, opened a branch in Hargeisa, to become the first bank in the country since the collapse in 1990 of the Commercial and Savings Bank of Somalia.
Various telecommunications firms also have branches in Somaliland. Among these companies is Golis Telecom Somalia, one of the largest such operators in northern Somalia. Founded in 2002 with the objective of supplying the local market with GSM mobile services, fixed line and internet services, it has an extensive network that covers all of Somalia's major cities and more than 40 districts in both the Somaliland and Puntland regions. Golis also offers among the cheapest international calling rates, at $0.2 USD less than its nearest competitor. Other telecommunication firms serving the region include Somtel, Telcom and NationLink.
Livestock is the backbone of the Somaliland region's economy. Sheep, camel and cattle are shipped from the Berbera port and sent to Gulf Arab countries, such as Saudi Arabia.
Agriculture is generally considered to be a potentially successful industry, especially in the production of cereals and horticulture. Mining also has potential, though simple quarrying represents the extent of current operations, despite the presence of diverse quantities of mineral deposits.
Tourism.
The rock art at Laas Geel, situated on the outskirts of Hargeisa, are a popular local tourist attraction. Totalling ten caves, they were discovered by a French archaeological team in 2002, and are believed to date back around 5,000 years. The government and locals keep the cave paintings safe, and only a restricted number of tourists are allowed entry. Other notable sights include the Freedom Arch in Hargeisa and the war memorial in the city centre. Natural attractions are very common around the region. The Naasa Hablood are twin hills located on the outskirts of Hargeisa that Somalis in the region consider to be a majestic natural landmark.
The Ministry of Tourism has also encouraged travellers to visit historic towns and cities in Somaliland. The historic town of Sheekh is located near Berbera and is home to old British colonial buildings that have remained untouched for over forty years. Berbera also houses historic and impressive Ottoman architectural buildings. Another equally famous historic city is Zeila. Zeila was once part of the Ottoman Empire, a dependency of Yemen and Egypt and a major trade city during the 19th century. The city has been visited for its old colonial landmarks, offshore mangroves and coral reefs, and its towering cliffs and beach. The nomadic culture of Somaliland has also attracted tourists. Most nomads live in the countryside.
Transport.
Bus services operate in Hargeisa, Burao, Gabiley, Berbera and Borama. There are also road transportation services between the major towns and adjacent villages, which are operated by different types of vehicles. Among these are taxis, four-wheel drives, minibuses and light goods vehicles (LGV).
The most prominent airlines serving Somaliland are Daallo Airlines and Jubba Airways, two of a number of Somali-owned private carriers that emerged after Somali Airlines ceased operations. They fly to Djibouti City, Addis Ababa, Dubai and Jeddah, and offer flights for the Hajj and Umrah pilgrimages via the Hargeisa International Airport. Other major airports in the region include the Berbera Airport and Burao Airport.
Demographics.
Languages.
Most people in Somaliland speak two of the three official languages: Somali, Arabic and English. Article 6 of the Constitution of 2001 designates the official language of Somaliland to be Somali, though Arabic is a mandatory subject in school and is used in mosques around the region and English is spoken and taught in schools. English was proclaimed an official language later, outside the constitution.
Somali is a member of the Cushitic branch of the Afro-Asiatic language family, and its nearest relatives are the Afar and Oromo languages. Somali is the best documented of the Cushitic languages, with academic studies of it dating from before 1900.
Somali dialects are divided into three main groups: Northern, Benaadir and Maay. Northern Somali (or Northern-Central Somali) forms the basis for Standard Somali. Benaadir (also known as Coastal Somali) is spoken on the Benadir coast from Cadaley to south of Merca, including Mogadishu, as well as in the immediate hinterland. The coastal dialects have additional phonemes which do not exist in Standard Somali. Maay is principally spoken by the Digil and Mirifle (Rahanweyn) clans in the southern areas of Somalia.
Since Somali had long lost its ancient script, a number of writing systems have been used over the years for transcribing the language. Of these, the Somali alphabet is the most widely used, and has been the official writing script in Somalia since the government of former President of Somalia Siad Barre formally introduced it in October 1972.
The script was developed by the Somali linguist Shire Jama Ahmed specifically for the Somali language, and uses all letters of the English Latin alphabet except "p", "v" and "z". Besides Ahmed's Latin script, other orthographies that have been used for centuries for writing Somali include the long-established Arabic script and Wadaad's writing, in addition to various indigenous writing systems developed in the twentieth century.
Religion.
With few exceptions, Somalis in Somaliland and elsewhere are Muslims, the majority belonging to the Sunni branch of Islam and the Shafi'i school of Islamic jurisprudence. As with southern Somali coastal towns such as Mogadishu and Merca, there is also a presence of Sufism, Islam's mystical dimension; particularly the Arab Rifa'iya tariiqa. Though traces of pre-Islamic traditional religion exist in Somaliland, Islam is important to the Somali sense of national identity. Many of the Somali social norms come from their religion. For example, Somali women wear a hijab when they are in public. In addition, Somalis abstain from pork and alcohol, and also try to avoid receiving or paying any form of interest (usury). Muslims generally congregate on Friday afternoons for a sermon and group prayer.
Under the Constitution of Somaliland, Islam is the state religion of Somaliland, and no laws may violate the principles of Sharia. The promotion of any religion other than Islam is illegal, and the state promotes Islamic tenets and discourages behavior contrary to Islamic morals.
Somaliland has very few Christians. In 1913, during the early part of the colonial era, there were virtually no Christians in the Somali territories, with about 100–200 followers coming from the schools and orphanages of the handful of Catholic missions in the British Somaliland protectorate. The small number of Christians in the region today mostly come from similar Catholic institutions in Aden, Djibouti, and Berbera.
Somaliland falls within the Episcopal Area of the Horn of Africa as part of Somalia, under the Anglican Diocese of Egypt . However, there are no current congregations in the territory. The Roman Catholic Diocese of Mogadiscio is designated to serve the area as part of Somalia. However since 1990 there has been no Bishop of Mogadishu, and the Bishop of Djibouti acts as Apostolic Administrator. The Adventist Mission also indicates that there are no Adventist members.
Culture.
Clan system.
The Somaliland region has a population of about 3.5 million people. The largest clan family in Somaliland are the Isaaq, who are Somalia's third largest ethnic Somali clan.
The clan groupings of the Somali people are important social units, with clan membership playing a central part in Somali culture and politics. Clans are patrilineal and are often divided into sub-clans, sometimes with many sub-divisions.
Somali society is traditionally ethnically endogamous. To extend ties of alliance, marriage is often to another ethnic Somali from a different clan. Thus, for example, a recent study observed that in 89 marriages contracted by men of the Dhulbahante clan, 55 (62%) were with women of Dhulbahante sub-clans other than those of their husbands; 30 (33.7%) were with women of surrounding clans of other clan families (Isaaq, 28; Hawiye, 3); and 3 (4.3%) were with women of other clans of the Darod clan family (Majerteen 2, Ogaden 1).
The Isaaq constitute the largest Somali clan in most of Somaliland. They are concentrated in the Woqooyi Galbeed, Togdheer and the western portion of Sanaag region. The disputed eastern and western regions are predominantly inhabited by other clans. Sool's residents mainly hail from the Dhulbahante, a subdivision of the Harti confederation of Darod sub-clans. The Warsangali, another Harti Darod sub-clan, constitute the majority of residents in eastern Sanaag. The Gadabuursi are also well represented in the Awdal region.
Cuisine.
It is considered polite for one to leave a little bit of food on one's plate after finishing a meal at another's home. This tells the host that one has been given enough food. If one were to clean his or her plate that would indicate that one is still hungry. Most Somalis do not take this rule so seriously, but it is certainly not impolite to leave a few bits of food on one's plate. Somali breakfast typically includes a flatbread called lahoh (injera), as well as liver, toast, cereal, and porridge made of millet or cornmeal. Lunch can be a mixture of rice or pasta with meat and sauce.
Also consumed during lunchtime is a traditional soup referred to as "maraq", which is also part of Yemeni cuisine. Maraq is made of vegetables, meat and beans and is usually eaten with flatbread or pita bread. Later in the day, a lighter meal is served that includes beans, ful medames, muffo (patties made of oats or corn), or a salad with more lahoh/injera.
Arts.
Islam and poetry have been described as the twin pillars of Somali culture. Somali poetry is mainly oral, with both male and female poets. They use things that are common in the Somali language as metaphors. Most Somalis are Sunni Muslims and Islam is vitally important to the Somali sense of national identity. Most Somalis do not belong to a specific mosque or sect and can pray in any mosque they find.
Celebrations come in the form of religious festivities, two of the most important being Eid ul-Adha and Eid ul-Fitr, which marks the end of the fasting month. Families get dressed up to visit one another, and money is donated to the poor. Other holidays include 26 June and 18 May, which celebrate British Somaliland's independence and the Somaliland region's establishment, respectively; the latter, however, is not recognised by the international community.
In the nomadic culture, where one's possessions are frequently moved, there is little reason for the plastic arts to be highly developed. Somalis embellish and decorate their woven and wooden milk jugs ("haamo"; the most decorative jugs are made in Ceerigaabo) as well as wooden headrests. Traditional dance is also important, though mainly as a form of courtship among young people. One such dance known as "Ciyaar Soomaali" is a local favorite.
An important form of art in Somaliland and other parts of Somalia is henna art. The custom of applying henna dates back to antiquity. During special occasions, a Somali woman's hands and feet are expected to be covered in decorative mendhi. Girls and women usually apply or decorate their hands and feet in henna on festive celebrations like Eid or weddings. The henna designs vary from very simple to highly intricate. Somali designs vary, with some more modern and simple while others are traditional and intricate. Traditionally, only women apply it as body art, as it is considered a feminine custom. Henna is not only applied on the hands and feet but is also used as a dye. Somali men and women alike use henna as a dye to change their hair colour. Women are free to apply henna on their hair as most of the time they are wearing a hijab.

</doc>
<doc id="51426" url="http://en.wikipedia.org/wiki?curid=51426" title="Cantor's diagonal argument">
Cantor's diagonal argument

In set theory, Cantor's diagonal argument, also called the diagonalisation argument, the diagonal slash argument or the diagonal method, was published in 1891 by Georg Cantor as a mathematical proof that there are infinite sets which cannot be put into one-to-one correspondence with the infinite set of natural numbers.
Such sets are now known as uncountable sets, and the size of infinite sets is now treated by the theory of cardinal numbers which Cantor began.
The diagonal argument was not Cantor's first proof of the uncountability of the real numbers; it was actually published much later than his first proof, which appeared in 1874.
However, it demonstrates a powerful and general technique that has since been used in a wide range of proofs, also known as diagonal arguments by analogy with the argument used in this proof. The most famous examples are perhaps Russell's paradox, the first of Gödel's incompleteness theorems, and Turing's answer to the "Entscheidungsproblem".
Uncountable set.
In his 1891 article, Cantor considered the set "T" of all infinite sequences of binary digits (i.e. consisting only of zeroes and ones).
He begins with a constructive proof of the following theorem:
To prove this, given an enumeration of arbitrary members from "T", like e.g.
he constructs the sequence "s" by choosing its "n"th digit as complementary to the "n"th digit of "s""n", for every "n". In the example, this yields:
By construction, "s" differs from each "s""n", since their "n"th digits differ (highlighted in the example).
Hence, "s" cannot occur in the enumeration.
Based on this theorem, Cantor then uses an indirect argument to show that:
He assumes for contradiction that "T" was countable.
Then (all) its elements could be written as an enumeration "s"1, "s"2, … , "s""n", … .
Applying the previous theorem to this enumeration would produce a sequence "s" not belonging to the enumeration.
However, "s" was an element of "T" and should therefore be in the enumeration. 
This contradicts the original assumption, so "T" must be uncountable.
Interpretation.
The interpretation of Cantor's result will depend upon one's view of mathematics. To constructivists, the argument shows no more than that there is no bijection between the natural numbers and "T". It does not rule out the possibility that the latter are subcountable. In the context of classical mathematics, this is impossible, and the diagonal argument establishes that, although both sets are infinite, there are actually "more" infinite sequences of ones and zeros than there are natural numbers.
Real numbers.
The uncountability of the real numbers was already established by Cantor's first uncountability proof, but it also follows from the above result. To see this, we will build a one-to-one correspondence between the set "T" of infinite binary strings and a subset of R (the set of real numbers). Since "T" is uncountable, this subset of R must be uncountable. Hence R is uncountable.
To build this one-to-one correspondence (or bijection), observe that the string "t" = 0111… appears after the binary point in the binary expansion 0.0111…. This suggests defining the function "f"("t") = 0."t", where "t" is a string in "T". Unfortunately, "f"(1000…) = 0.1000… = 1/2, and "f"(0111…) = 0.0111… = 1/4 + 1/8 + 1/16 + … = 1/2. So this function is not a bijection since two strings correspond to one number—a number having two binary expansions.
However, modifying this function produces a bijection from "T" to the interval (0, 1)—that is, the real numbers > 0 and < 1. The idea is to remove the "problem" elements from "T" and (0, 1), and handle them separately. From (0, 1), remove the numbers having two binary expansions. Put these numbers in a sequence: "a" = (1/2, 1/4, 3/4, 1/8, 3/8, 5/8, 7/8, …). From "T", remove the strings appearing after the binary point in the binary expansions of 0, 1, and the numbers in sequence "a". Put these eventually-constant strings in a sequence: "b" = (000…, 111…, 1000…, 0111…, 01000…, 11000…, 00111…, 10111…, ...). A bijection "g"("t") from "T" to (0, 1) is defined by: If "t" is the "n"th string in sequence "b", let "g"("t") be the "n"th number in sequence "a"; otherwise, let "g"("t") = 0."t".
To build a bijection from "T" to R: start with the tangent function tan("x"), which provides a bijection from (−π/2, π/2) to R; see right picture. Next observe that the linear function "h"("x") = π"x" - π/2 provides a bijection from (0, 1) to (−π/2, π/2); see left picture. The composite function tan("h"("x")) = tan(π"x" - π/2) provides a bijection from (0, 1) to R. Compose this function with "g"("t") to obtain tan("h"("g"("t"))) = tan(π"g"("t") - π/2), which is a bijection from "T" to R. This means that "T" and R have the same cardinality—this cardinality is called the "cardinality of the continuum."
General sets.
A generalized form of the diagonal argument was used by Cantor to prove Cantor's theorem: for every set "S" the power set of "S", i.e., the set of all subsets of "S" (here written as P("S")), has a larger cardinality than "S" itself. This proof proceeds as follows:
Let "f" be any function from "S" to P("S"). It suffices to prove "f" cannot be surjective. That means that some member "T" of P("S"), i.e., some subset of "S", is not in the image of "f". As a candidate consider the set:
For every "s" in "S", either "s" is in "T" or not. If "s" is in "T", then by definition of "T", "s" is not in "f"("s"), so "T" is not equal to "f"("s"). On the other hand, if "s" is not in "T", then by definition of "T", "s" is in "f"("s"), so again "T" is not equal to "f"("s"); cf. picture.
For a more complete account of this proof, see Cantor's theorem.
Consequences.
This result implies that the notion of the set of all sets is an inconsistent notion. If "S" were the set of all sets then P("S") would at the same time be bigger than "S" and a subset of "S".
Russell's Paradox has shown us that naive set theory, based on an unrestricted comprehension scheme, is contradictory. Note that there is a similarity between the construction of "T" and the set in Russell's paradox. Therefore, depending on how we modify the axiom scheme of comprehension in order to avoid Russell's paradox, arguments such as the non-existence of a set of all sets may or may not remain valid.
The diagonal argument shows that the set of real numbers is "bigger" than the set of natural numbers (and therefore, the integers and rationals as well). Therefore, we can ask if there is a set whose cardinality is "between" that of the integers and that of the reals. This question leads to the famous continuum hypothesis. Similarly, the question of whether there exists a set whose cardinality is between |"S"| and |P("S")| for some infinite "S" leads to the generalized continuum hypothesis.
Analogues of the diagonal argument are widely used in mathematics to prove the existence or nonexistence of certain objects. For example, the conventional proof of the unsolvability of the halting problem is essentially a diagonal argument. Also, diagonalization was originally used to show the existence of arbitrarily hard complexity classes and played a key role in early attempts to prove P does not equal NP.
Version for Quine's New Foundations.
The above proof fails for W. V. Quine's "New Foundations" set theory (NF). In NF, the naive axiom scheme of comprehension is modified to avoid the paradoxes by introducing a kind of "local" type theory. In this axiom scheme,
is "not" a set — i.e., does not satisfy the axiom scheme. On the other hand, we might try to create a modified diagonal argument by noticing that
"is" a set in NF. In which case, if P1("S") is the set of one-element subsets of "S" and "f" is a proposed bijection from P1("S") to P("S"), one is able to use proof by contradiction to prove that |P1("S")| < |P("S")|.
The proof follows by the fact that if "f" were indeed a map "onto" P("S"), then we could find "r" in "S", such that "f"({"r"}) coincides with the modified diagonal set, above. We would conclude that if "r" is not in "f"({"r"}), then "r" is in "f"({"r"}) and vice versa.
It is "not" possible to put P1("S") in a one-to-one relation with "S", as the two have different types, and so any function so defined would violate the typing rules for the comprehension scheme.

</doc>
<doc id="51427" url="http://en.wikipedia.org/wiki?curid=51427" title="British Somaliland">
British Somaliland

 |- class="mergedbottomrow"
 | style="width:1.0em; padding:0 0 0 0.6em;"|  -  ||style="padding-left:0;text-align:left;"| 1904 
 |  km² ( sq mi)
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |style="padding-left:0;text-align:left;"| 1904 est.
 |- class="mergedbottomrow"
 |colspan="2"| Density
 |style="white-space:nowrap;"| /km²  ( /sq mi)
 |  Somalia
British Somaliland (Somali: "Dhulka Biritishka ee Soomaaliya", Arabic: الصومال البريطاني "Al-Sumal Al-Britaniy"‎) was a British protectorate in present-day northwestern Somalia. For much of its existence, the territory was bordered by Italian Somaliland, French Somaliland and Ethiopia. From 1940 to 1941, it was occupied by the Italians and was part of Italian East Africa. On 1 July 1960, the British Somaliland protectorate united as scheduled with the Trust Territory of Somaliland (the former Italian Somaliland) to form the Somali Republic (Somalia). The government of Somaliland, a self-declared sovereign state that is internationally recognised as an autonomous region of Somalia, regards itself as the successor state to British Somaliland.
History.
Treaties and establishment.
In 1888, after signing successive treaties with the then ruling Somali Sultans such as Mohamoud Ali Shire of the Warsangali Sultanate, the British established a protectorate in the region referred to as British Somaliland. The British garrisoned the protectorate from Aden and administered it from their British India colony until 1898. British Somaliland was then administered by the Foreign Office until 1905 and afterwards by the Colonial Office.
Generally, the British did not have much interest in the resource-barren region. The stated purposes of the establishment of the protectorate were to "secure a supply market, check the traffic in slaves, and to exclude the interference of foreign powers." The British principally viewed the protectorate as a source for supplies of meat for their British Indian outpost in Aden through the maintenance of order in the coastal areas and protection of the caravan routes from the interior. Hence, the region's nickname of "Aden's butcher's shop". Colonial administration during this period did not extend administrative infrastructure beyond the coast, and contrasted with the more interventionist colonial experience of Italian Somaliland.
Dervish State.
Beginning in 1899, the British were forced to expend considerable human and military capital to contain a decades-long resistance movement mounted by the Dervish State. The polity was led by Sayyid Mohammed Abdullah Hassan, a Somali religious leader referred to colloquially by the British as the "Mad Mullah". Repeated military expeditions were unsuccessfully launched against Hassan and his Dervishes before World War I.
On 9 August 1913, the Somaliland Camel Constabulary suffered a serious defeat at the Battle of Dul Madoba at the hands of the Dervishes. Hassan had already evaded several attempts to capture him. At Dul Madoba, his forces killed or wounded 57 members of the 110-man Constabulary unit, including the British commander, Colonel Richard Corfield.
In 1914, the British created the Somaliland Camel Corps to assist in maintaining order in British Somaliland.
In 1920, the British launched their fifth and final expedition against Hassan and his followers. Employing the then-new technology of military aircraft, the British finally managed to quell Hassan's twenty-year-long struggle. The aerial attack on the Dervish capital, Taleh, killed many members of Hassan's family who had been lured there by the British for an official visit. Hassan and his Dervish supporters fled into the Ogaden, where Hassan died in 1921.
Somaliland Camel Corps.
The Somaliland Camel Corps, also referred to as the Somali Camel Corps, was a unit of the British Army based in British Somaliland. It lasted from the early 20th century until 1944. The troopers of the Somaliland Camel Corps had a distinctive dress. It was based on the standard British Army khaki drill, but included a knitted woollen pullover and drill patches on the shoulders. Shorts were worn with woollen socks on puttees and "chaplis", boots or bare feet. Equipment consisted of a leather ammunition bandolier and a leather waist belt. The officers wore pith helmets and khaki drill uniforms. Other ranks wore a "kullah" with "puggree" which ended in a long tail which hung down the back. A "chaplis" is typically a colourful sandal. A "kullah" is a type of cap. A "puggree" is typically a strip of cloth wound around the upper portion of a hat or helmet, particularly a pith helmet, and falling down behind to act as a shade for the back of the neck.
British Somaliland 1920–1930.
Following the defeat of the Dervish resistance, the two fundamental goals of British policy in British Somaliland were the preservation of stability and the economic self-sufficiency of the protectorate. The second goal remained particularly elusive because of local resistance to taxation that might have been used to support the protectorate's administration. By the 1930s, the British presence had extended to other parts of British Somaliland. Growth in commercial trade motivated some livestock herders to subsequently leave the pastoral economy and settle in urban areas. Customs taxes also helped pay for British India's patrol of Somalia's Red Sea Coast.
Italian invasion.
In August 1940, during the East African Campaign in World War II, British Somaliland was briefly occupied by Italy. During this period, the British rounded up soldiers and governmental officials to evacuate them from the territory through the capital of Berbera. In total, 7,000 people, including civilians were evacuated. The Somalis serving in the Somaliland Camel Corps were given the choice of evacuation or disbandment; the majority chose to remain and were allowed to retain their arms.
In March 1941, after a six-month occupation, the British Imperial forces recaptured the protectorate during Operation Appearance. The final remnants of the Italian guerrilla movement discontinued all resistance in British Somaliland by the fall of 1943. In 1947, the entire budget for the administration of the British Somaliland protectorate was only £213,139.
Independence.
In May 1960, the British Government stated that it would be prepared to grant independence to the then protectorate of British Somaliland, with the intention that the territory would unite with the Italian-administered Trust Territory of Somaliland (the former Italian Somaliland). The Legislative Council of British Somaliland passed a resolution in April 1960 requesting independence and union with the Trust Territory of Somaliland, which was scheduled to gain independence on 1 July that year. The legislative councils of both territories agreed to this proposal following a joint conference in Mogadishu.
On 26 June 1960, the British Somaliland protectorate briefly gained independence as the State of Somaliland before uniting as scheduled five days later with the Trust Territory of Somaliland to form the Somali Republic (Somalia) on 1 July 1960. 
Somaliland.
In 1991, after the breakdown of the central government of the Somali Republic, parts of the area which formerly encompassed British Somaliland declared independence. In May 1991, the formation of the "Republic of Somaliland" was proclaimed, with the local government regarding it as the successor to the former British Somaliland. However, the Somaliland region's self-declared independence remains unrecognised by any country or international organisation.

</doc>
<doc id="51429" url="http://en.wikipedia.org/wiki?curid=51429" title="Hyperreal number">
Hyperreal number

The system of hyperreal numbers is a way of treating infinite and infinitesimal quantities. The hyperreals, or nonstandard reals, *R, are an extension of the real numbers R that contains numbers greater than anything of the form
Such a number is infinite, and its reciprocal is infinitesimal. The term "hyper-real" was introduced by Edwin Hewitt in 1948.
The hyperreal numbers satisfy the transfer principle, a rigorous version of Leibniz's heuristic Law of Continuity. The transfer principle states that true first order statements about R are also valid in *R. For example, the commutative law of addition, "x" + "y" = "y" + "x", holds for the hyperreals just as it does for the reals; since R is a real closed field, so is *R. Since formula_2 for all integers "n", one also has formula_3 for all hyperintegers "H". The transfer principle for ultrapowers is a consequence of Łoś' theorem of 1955.
Concerns about the soundness of arguments involving infinitesimals date back to ancient Greek mathematics, with Archimedes replacing such proofs with ones using other techniques such as the method of exhaustion. In the 1960s, Abraham Robinson proved that the hyperreals were logically consistent if and only if the reals were. This put to rest the fear that any proof involving infinitesimals might be unsound, provided that they were manipulated according to the logical rules which Robinson delineated.
The application of hyperreal numbers and in particular the transfer principle to problems of analysis is called non-standard analysis. One immediate application is the definition of the basic concepts of analysis such as derivative and integral in a direct fashion, without passing via logical complications of multiple quantifiers. Thus, the derivative of "f(x)" becomes formula_4 for an infinitesimal formula_5, where "st(·)" denotes the standard part function, which "rounds off" each finite hyperreal to the nearest real. Similarly, the integral is defined as the standard part of a suitable infinite sum.
The transfer principle.
The idea of the hyperreal system is to extend the real numbers R to form a system *R that includes infinitesimal and infinite numbers, but without changing any of the elementary axioms of algebra. Any statement of the form "for any number x..." that is true for the reals is also true for the hyperreals. For example, the axiom that states "for any number "x", "x" + 0 = "x"" still applies. The same is true for quantification over several numbers, e.g., "for any numbers "x" and "y", "xy" = "yx"." This ability to carry over statements from the reals to the hyperreals is called the transfer principle. However, statements of the form "for any "set" of numbers S ..." may not carry over. The only properties that differ between the reals and the hyperreals are those that rely on quantification over sets, or other higher-level structures such as functions and relations, which are typically constructed out of sets. Each real set, function, and relation has its natural hyperreal extension, satisfying the same first-order properties. The kinds of logical sentences that obey this restriction on quantification are referred to as statements in first-order logic.
The transfer principle, however, doesn't mean that R and *R have identical behavior. For instance, in *R there exists an element "ω" such that
but there is no such number in R. (In other words, *R is not Archimedean.) This is possible because the nonexistence of "ω" cannot be expressed as a first order statement.
Use in analysis.
Calculus with algebraic functions.
Informal notations for non-real quantities have historically appeared in calculus in two contexts: as infinitesimals like "dx" and as the symbol ∞, used, for example, in limits of integration of improper integrals.
As an example of the transfer principle, the statement that for any nonzero number "x", "2x" ≠ "x", is true for the real numbers, and it is in the form required by the transfer principle, so it is also true for the hyperreal numbers. This shows that it is not possible to use a generic symbol such as ∞ for all the infinite quantities in the hyperreal system; infinite quantities differ in magnitude from other infinite quantities, and infinitesimals from other infinitesimals.
Similarly, the casual use of 1/0 = ∞ is invalid, since the transfer principle applies to the statement that division by zero is undefined. The rigorous counterpart of such a calculation would be that if ε is a non-zero infinitesimal, then 1/ε is infinite.
For any finite hyperreal number "x", its standard part, st "x", is defined as the unique real number that differs from it only infinitesimally. The derivative of a function "y"("x") is defined not as "dy/dx" but as the standard part of "dy/dx".
For example, to find the derivative "f′"("x") of the function "f"("x") = "x"2, let "dx" be a non-zero infinitesimal. Then,
The use of the standard part in the definition of the derivative is a rigorous alternative to the traditional practice of neglecting the square of an infinitesimal quantity. After the third line of the differentiation above, the typical method from Newton through the 19th century would have been simply to discard the "dx"2 term. In the hyperreal system,
"dx"2 ≠ 0, since "dx" is nonzero, and the transfer principle can be applied to the statement that the square of any nonzero number is nonzero. However, the quantity "dx"2 is infinitesimally small compared to "dx"; that is, the hyperreal system contains a hierarchy of infinitesimal quantities.
Integration.
One way of defining a definite integral in the hyperreal system is as the standard part of an infinite sum on a hyperfinite lattice defined as "a", "a + dx", "a + 2dx", ... "a + ndx", where "dx" is infinitesimal, n is an infinite hypernatural, and the lower and upper bounds of integration are "a" and "b" = "a" + "n" "dx."
Properties.
The hyperreals *R form an ordered field containing the reals R as a subfield. Unlike the reals, the hyperreals do not form a standard metric space, but by virtue of their order they carry an order topology.
The use of the definite article "the" in the phrase "the hyperreal numbers" is somewhat misleading in that there is not a unique ordered field that is referred to in most treatments.
However, a 2003 paper by Vladimir Kanovei and Shelah shows that there is a definable, countably saturated (meaning ω-saturated, but not of course countable) elementary extension of the reals, which therefore has a good claim to the title of "the" hyperreal numbers. Furthermore, the field obtained by the ultrapower construction from the space of all real sequences, is unique up to isomorphism if one assumes the continuum hypothesis.
The condition of being a hyperreal field is a stronger one than that of being a real closed field strictly containing R. It is also stronger than that of being a superreal field in the sense of Dales and Woodin.
Development.
The hyperreals can be developed either axiomatically or by more constructively oriented methods. The essence of the axiomatic approach is to assert (1) the existence of at least one infinitesimal number, and (2) the validity of the transfer principle. In the following subsection we give a detailed outline of a more constructive approach. This method allows one to construct the hyperreals if given a set-theoretic object called an ultrafilter, but the ultrafilter itself cannot be explicitly constructed.
From Leibniz to Robinson.
When Newton and (more explicitly) Leibniz introduced differentials, they used infinitesimals and these were still regarded as useful by later mathematicians such as Euler and Cauchy. Nonetheless these concepts were from the beginning seen as suspect, notably by George Berkeley. Berkeley's criticism centered on a perceived shift in hypothesis in the definition of the derivative in terms of infinitesimals (or fluxions), where "dx" is assumed to be nonzero at the beginning of the calculation, and to vanish at its conclusion (see Ghosts of departed quantities for details). When in the 1800s calculus was put on a firm footing through the development of the (ε, δ)-definition of limit by Bolzano, Cauchy, Weierstrass, and others, infinitesimals were largely abandoned, though research in non-Archimedean fields continued (Ehrlich 2006).
However, in the 1960s Abraham Robinson showed how infinitely large and infinitesimal numbers can be rigorously defined and used to develop the field of non-standard analysis. Robinson developed his theory nonconstructively, using model theory; however it is possible to proceed using only algebra and topology, and proving the transfer principle as a consequence of the definitions. In other words hyperreal numbers "per se", aside from their use in nonstandard analysis, have no necessary relationship to model theory or first order logic, although they were discovered by the application of model theoretic techniques from logic. Hyper-real fields were in fact originally introduced by Hewitt (1948) by purely algebraic techniques, using an ultrapower construction.
The ultrapower construction.
We are going to construct a hyperreal field via sequences of reals. In fact we can add and multiply sequences componentwise; for example:
and analogously for multiplication.
This turns the set of such sequences into a commutative ring, which is in fact a real algebra A. We have a natural embedding of R in A by identifying the real number "r" with the sequence ("r", "r", "r", ...) and this identification preserves the corresponding algebraic operations of the reals. The intuitive motivation is, for example, to represent an infinitesimal number using a sequence that approaches zero. The inverse of such a sequence would represent an infinite number. As we will see below, the difficulties arise because of the need to define rules for comparing such sequences in a manner that, although inevitably somewhat arbitrary, must be self-consistent and well defined. For example, we may have two sequences that differ in their first "n" members, but are equal after that; such sequences should clearly be considered as representing the same hyperreal number. Similarly, most sequences oscillate randomly forever, and we must find some way of taking such a sequence and interpreting it as, say, formula_8, where formula_9 is a certain infinitesimal number.
Comparing sequences is thus a delicate matter. We could, for example, try to define a relation between sequences in a componentwise fashion:
but here we run into trouble, since some entries of the first sequence may be bigger than the corresponding entries of the second sequence, and some others may be smaller. It follows that the relation defined in this way is only a partial order. To get around this, we have to specify which positions matter. Since there are infinitely many indices, we don't want finite sets of indices to matter. A consistent choice of index sets that matter is given by any free ultrafilter "U" on the natural numbers; these can be characterized as ultrafilters which do not contain any finite sets. (The good news is that Zorn's lemma guarantees the existence of many such "U"; the bad news is that they cannot be explicitly constructed.) We think of "U" as singling out those sets of indices that "matter": We write ("a"0, "a"1, "a"2, ...) ≤ ("b"0, "b"1, "b"2, ...) if and only if the set of natural numbers { "n" : "a""n" ≤ "b""n" } is in "U".
This is a total preorder and it turns into a total order if we agree not to distinguish between two sequences "a" and "b" if "a"≤"b" and "b"≤"a". With this identification, the ordered field *R of hyperreals is constructed. From an algebraic point of view, "U" allows us to define a corresponding maximal ideal I in the commutative ring A (namely, the set of the sequences that vanish in some element of "U"), and then to define *R as A/I; as the quotient of a commutative ring by a maximal ideal, *R is a field. This is also notated A/"U", directly in terms of the free ultrafilter "U"; the two are equivalent. The maximality of I follows from the possibility of, given a sequence "a", constructing a sequence "b" inverting the non-null elements of "a" and not altering its null entries. If the set on which "a" vanishes is not in "U", the product "ab" is identified with the number 1, and any ideal containing 1 must be "A". In the resulting field, these "a" and "b" are inverses.
The field A/"U" is an ultrapower of R.
Since this field contains R it has cardinality at least that of the continuum. Since A has cardinality
it is also no larger than formula_12, and hence has the same cardinality as R.
One question we might ask is whether, if we had chosen a different free ultrafilter "V", the quotient field A/"U" would be isomorphic as an ordered field to A/"V". This question turns out to be equivalent to the continuum hypothesis; in ZFC with the continuum hypothesis we can prove this field is unique up to order isomorphism, and in ZFC with the negation of continuum hypothesis we can prove that there are non-order-isomorphic pairs of fields which are both countably indexed ultrapowers of the reals.
For more information about this method of construction, see ultraproduct.
An intuitive approach to the ultrapower construction.
The following is an intuitive way of understanding the hyperreal numbers. The approach taken here is very close to the one in the book by Goldblatt. Recall that the sequences converging to zero are sometimes called infinitely small. These are almost the infinitesimals in a sense; the true infinitesimals include certain classes of sequences that contain a sequence converging to zero.
Let us see where these classes come from. Consider first the sequences of real numbers. They form a ring, that is, one can multiply, add and subtract them, but not always divide by a non-zero element. The real numbers are considered as the constant sequences, the sequence is zero if it is identically zero, that is, "a""n" = 0 for all "n".
In our ring of sequences one can get "ab" = 0 with neither "a" = 0 nor "b" = 0. Thus, if for two sequences formula_13 one has "ab" = 0, at least one of them should be declared zero. Surprisingly enough, there is a consistent way to do it. As a result, the equivalence classes of sequences that differ by some sequence declared zero will form a field which is called a hyperreal field. It will contain the infinitesimals in addition to the ordinary real numbers, as well as infinitely large numbers (the reciprocals of infinitesimals, including those represented by sequences diverging to infinity). Also every hyperreal which is not infinitely large will be infinitely close to an ordinary real, in other words, it will be the sum of an ordinary real and an infinitesimal.
This construction is parallel to the construction of the reals from the rationals given by Cantor. He started with the ring of the Cauchy sequences of rationals and declared all the sequences that converge to zero to be zero. The result is the reals. To continue the construction of hyperreals, let us consider the zero sets of our sequences, that is, the formula_14, that is, formula_15 is the set of indexes formula_16 for which formula_17. It is clear that if formula_18, then the union of formula_15 and formula_20 is "N" (the set of all natural numbers), so:
Now the idea is to single out a bunch "U" of subsets "X" of "N" and to declare that formula_27 if and only if formula_15 belongs to "U". From the above conditions one can see that:
Any family of sets that satisfies (2–4) is called a filter (an example: the complements to the finite sets, it is called the Fréchet filter and it is used in the usual limit theory). If (1) also holds, U is called an ultrafilter (because you can add no more sets to it without breaking it). The only explicitly known example of an ultrafilter is the family of sets containing a given element (in our case, say, the number 10). Such ultrafilters are called trivial, and if we use it in our construction, we come back to the ordinary real numbers. Any ultrafilter containing a finite set is trivial. It is known that any filter can be extended to an ultrafilter, but the proof uses the axiom of choice. The existence of a nontrivial ultrafilter (the ultrafilter lemma) can be added as an extra axiom, as it is weaker than the axiom of choice.
Now if we take a nontrivial ultrafilter (which is an extension of the Fréchet filter) and do our construction, we get the hyperreal numbers as a result.
If formula_29 is a real function of a real variable formula_30 then formula_29 naturally extends to a hyperreal function of a hyperreal variable by composition:
where formula_33 means "the equivalence class of the sequence formula_34 relative to our ultrafilter", two sequences being in the same class if and only if the zero set of their difference belongs to our ultrafilter.
All the arithmetical expressions and formulas make sense for hyperreals and hold true if they are true for the ordinary reals. One can prove that any finite (that is, such that formula_35 for some ordinary real formula_21) hyperreal formula_30 will be of the form formula_38 where formula_39 is an ordinary (called standard) real and formula_40 is an infinitesimal.
Now one can see that formula_29 is continuous means that formula_42 is infinitely small whenever formula_43 is, and formula_29 is differentiable means that
is infinitely small whenever formula_43 is. Remarkably, if one allows formula_21 to be hyperreal, the derivative will be automatically continuous (because, formula_29 being differentiable at formula_30,
is infinitely small when formula_43 is, therefore formula_52 is also infinitely small when formula_43 is).
Properties of infinitesimal and infinite numbers.
The finite elements F of *R form a local ring, and in fact a valuation ring, with the unique maximal ideal S being the infinitesimals; the quotient F/S is isomorphic to the reals. Hence we have a homomorphic mapping, st("x"), from F to R whose kernel consists of the infinitesimals and which sends every element "x" of F to a unique real number whose difference from x is in S; which is to say, is infinitesimal. Put another way, every "finite" nonstandard real number is "very close" to a unique real number, in the sense that if "x" is a finite nonstandard real, then there exists one and only one real number st("x") such that "x" – st("x") is infinitesimal. This number st("x") is called the standard part of "x", conceptually the same as "x" "to the nearest real number". This operation is an order-preserving homomorphism and hence is well-behaved both algebraically and order theoretically. It is order-preserving though not isotonic; i.e. formula_54 implies formula_55, but formula_56 does not imply formula_57.
The map st is continuous with respect to the order topology on the finite hyperreals; in fact it is locally constant.
Hyperreal fields.
Suppose "X" is a Tychonoff space, also called a T3.5 space, and C("X") is the algebra of continuous real-valued functions on "X". Suppose M is a maximal ideal in C("X"). Then the factor algebra A = C("X")/M is a totally ordered field F containing the reals. If F strictly contains R then M is called a hyperreal ideal (terminology due to Hewitt (1948)) and F a hyperreal field. Note that no assumption is being made that the cardinality of F is greater than R; it can in fact have the same cardinality.
An important special case is where the topology on "X" is the discrete topology; in this case "X" can be identified with a cardinal number κ and C("X") with the real algebra formula_62 of functions from κ to R. The hyperreal fields we obtain in this case are called ultrapowers of R and are identical to the ultrapowers constructed via free ultrafilters in model theory.

</doc>
<doc id="51432" url="http://en.wikipedia.org/wiki?curid=51432" title="Surreal number">
Surreal number

In mathematics, the surreal number system is an arithmetic continuum containing the real numbers as well as infinite and infinitesimal numbers, respectively larger or smaller in absolute value than any positive real number. The surreals share many properties with the reals, including a total order ≤ and the usual arithmetic operations (addition, subtraction, multiplication, and division); as such, they form an ordered field. (Strictly speaking, the surreals are not a set, but a proper class.) If formulated in Von Neumann–Bernays–Gödel set theory, the surreal numbers are the largest possible ordered field; all other ordered fields, such as the rationals, the reals, the rational functions, the Levi-Civita field, the superreal numbers, and the hyperreal numbers, can be realized as subfields of the surreals. It has also been shown (in Von Neumann–Bernays–Gödel set theory) that the maximal class hyperreal field is isomorphic to the maximal class surreal field; in theories without the axiom of global choice, this need not be the case, and in such theories it is not necessarily true that the surreals are the largest ordered field. The surreals also contain all transfinite ordinal numbers; the arithmetic on them is given by the natural operations.
In 1907 Hahn introduced Hahn series as a generalization of formal power series, and Hausdorff introduced certain ordered sets called ηα-sets for ordinals α and asked if it was possible to find a compatible ordered group or field structure. In 1962 Alling used a modified form of Hahn series to construct such ordered fields associated to certain ordinals α, and taking α to be the class of all ordinals in his construction gives a class that is an ordered field isomorphic to the surreal numbers. 
Research on the go endgame by John Horton Conway led to a simpler definition and construction of the surreal numbers. Conway's construction was introduced in Donald Knuth's 1974 book "Surreal Numbers: How Two Ex-Students Turned on to Pure Mathematics and Found Total Happiness". In his book, which takes the form of a dialogue, Knuth coined the term "surreal numbers" for what Conway had called simply "numbers". Conway later adopted Knuth's term, and used surreals for analyzing games in his 1976 book "On Numbers and Games".
Overview.
The surreal numbers are constructed in stages, along with an ordering ≤ such that for any two surreal numbers "a" and "b" either "a" ≤ "b" or "b" ≤ "a". (Both may hold, in which case "a" and "b" are equivalent and denote the same number.) Numbers are formed by pairing subsets of numbers already constructed: given subsets "L" and "R" of numbers such that all the members of "L" are strictly less than all the members of "R", then the pair { "L" | "R" } represents a number intermediate in value between all the members of "L" and all the members of "R".
Different subsets may end up defining the same number: { "L" | "R" } and { "L′" | "R′" } may define the same number even if "L" ≠ "L′" and "R" ≠ "R′". (A similar phenomenon occurs when rational numbers are defined as quotients of integers: 1/2 and 2/4 are different representations of the same rational number.) So strictly speaking, the surreal numbers are equivalence classes of representations of form { "L" | "R" } that designate the same number.
In the first stage of construction, there are no previously existing numbers so the only representation must use the empty set: { | }. This representation, where "L" and "R" are both empty, is called 0. Subsequent stages yield forms like:
and
The integers are thus contained within the surreal numbers. Similarly, representations arise like:
so that the dyadic rationals (rational numbers whose denominators are powers of 2) are contained within the surreal numbers.
After an infinite number of stages, infinite subsets become available, so that any real number "a" can be represented by { "La" | "Ra" },
where "La" is the set of all dyadic rationals less than "a" and 
"Ra" is the set of all dyadic rationals greater than "a" (reminiscent of a Dedekind cut). Thus the real numbers are also embedded within the surreals.
But there are also representations like
where ω is a transfinite number greater than all integers and ε is an infinitesimal greater than 0 but less than any positive real number. Moreover, the standard arithmetic operations (addition, subtraction, multiplication, and division) can be extended to these non-real numbers in a manner that turns the collection of surreal numbers into an ordered field, so that one can talk about 2ω or ω − 1 and so forth.
Construction.
Surreal numbers are constructed inductively as equivalence classes of pairs of sets of surreal numbers, restricted by the condition that each element of the first set is smaller than each element of the second set. The construction consists of three interdependent parts: the construction rule, the comparison rule and the equivalence rule.
Forms.
A "form" is a pair of sets of surreal numbers, called its "left set" and its "right set". A form with left set "L" and right set "R" is written { "L" | "R" }. When "L" and "R" are given as lists of elements, the braces around them are omitted.
Either or both of the left and right set of a form may be the empty set. The form { { } | { } } with both left and right set empty is also written { | }.
Numeric forms.
Construction Rule
Equivalence classes of numeric forms.
The numeric forms are placed in equivalence classes; each such equivalence class is a "surreal number". The elements of the left and right set of a form are drawn from the universe of the surreal numbers (not of "forms", but of their "equivalence classes").
Equivalence Rule
An ordering relationship must be antisymmetric, i.e., it must have the property that "x" = "y" (i. e., "x" ≤ "y" and "y" ≤ "x" are both true) only when "x" and "y" are the same object. This is not the case for surreal number "forms", but is true by construction for surreal "numbers" (equivalence classes).
The equivalence class containing { | } is labeled 0; in other words, { | } is a form of the surreal number 0.
Order.
The recursive definition of surreal numbers is completed by defining comparison:
Given numeric forms "x" = { "XL" | "XR" } and "y" = { "YL" | "YR" }, "x" ≤ "y" if and only if:
A comparison "y" ≤ "c" between a form "y" and a surreal number "c" is performed by choosing a form "z" from the equivalence class "c" and evaluating "y" ≤ "z"; and likewise for "c" ≤ "x" and for comparison "b" ≤ "c" between two surreal numbers.
Induction.
This group of definitions is recursive, and requires some form of mathematical induction to define the universe of objects (forms and numbers) that occur in them. The only surreal numbers reachable via "finite induction" are the dyadic fractions; a wider universe is reachable given some form of transfinite induction.
Induction Rule
The base case is actually a special case of the induction rule, with 0 taken as a label for the "least ordinal". Since there exists no "Si" with "i" < 0, the expression formula_6 is the empty set; the only subset of the empty set is the empty set, and therefore "S"0 consists of a single surreal form { | } lying in a single equivalence class 0.
For every finite ordinal number "n", "Sn" is well-ordered by the ordering induced by the comparison rule on the surreal numbers.
The first iteration of the induction rule produces the three numeric forms { | 0 } < { | } < { 0 | } (the form { 0 | 0 } is non-numeric because 0≤0). The equivalence class containing { 0 | } is labeled 1 and the equivalence class containing { | 0 } is labeled −1. These three labels have a special significance in the axioms that define a ring; they are the additive identity (0), the multiplicative identity (1), and the additive inverse of 1 (−1). The arithmetic operations defined below are consistent with these labels.
For every "i" < "n", since every valid form in "S""i" is also a valid form in "S""n", all of the numbers in "Si" also appear in "Sn" (as supersets of their representation in "Si"). (The set union expression appears in our construction rule, rather than the simpler form "Sn-1", so that the definition also makes sense when "n" is a limit ordinal.) Numbers in "Sn" that are a superset of some number in "Si" are said to have been "inherited" from generation "i". The smallest value of α for which a given surreal number appears in "S"α is called its "birthday". For example, the birthday of 0 is 0, and the birthday of −1 is 1.
A second iteration of the construction rule yields the following ordering of equivalence classes:
Comparison of these equivalence classes is consistent irrespective of the choice of form. Three observations follow:
The informal interpretations of { 1 | } and { | −1 } are "the number just after 1" and "the number just before −1" respectively; their equivalence classes are labeled 2 and −2. The informal interpretations of { 0 | 1 } and { −1 | 0 } are "the number halfway between 0 and 1" and "the number halfway between −1 and 0" respectively; their equivalence classes are labeled 1/2 and −1/2. These labels will also be justified by the rules for surreal addition and multiplication below.
The equivalence classes at each stage "n" of induction may be characterized by their "n"-"complete forms" (each containing as many elements as possible of previous generations in its left and right sets). Either this complete form contains "every" number from previous generations in its left or right set, in which case this is the first generation in which this number occurs; or it contains all numbers from previous generations but one, in which case it is a new form of this one number. We retain the labels from the previous generation for these "old" numbers, and write the ordering above using the old and new labels:
The third observation extends to all surreal numbers with finite left and right sets. (For infinite left or right sets, this is valid in an altered form, since infinite sets might not contain a maximal or minimal element.) The number { 1, 2 | 5, 8 } is therefore equivalent to { 2 | 5 }; one can establish that these are forms of 3 by using the "birthday property", which is a consequence of the rules above.
Birthday property
Arithmetic.
The addition, negation (additive inverse), and multiplication of surreal number "forms" "x" = { "XL" | "XR" } and "y" = { "YL" | "YR" } are defined by three recursive formulas.
Negation.
Negation of a given number "x" = { "XL" | "XR" } is defined by
where the negation of a set "S" of numbers is given by the set of the negated elements of "S":
This formula involves the negation of the surreal "numbers" appearing in the left and right sets of "x", which is to be understood as the result of choosing a form of the number, evaluating the negation of this form, and taking the equivalence class of the resulting form. This only makes sense if the result is the same irrespective of the choice of form of the operand. This can be proven inductively using the fact that the numbers occurring in "XL" and "XR" are drawn from generations earlier than that in which the form "x" first occurs, and observing the special case:
Addition.
The definition of addition is also a recursive formula:
where
This formula involves sums of one of the original operands and a surreal "number" drawn from the left or right set of the other. These are to be understood as the result of choosing a form of the numeric operand, performing the sum of the two forms, and taking the equivalence class of the resulting form. This only makes sense if the result is the same irrespective of the choice of form of the numeric operand. This can also be proven inductively with the special cases:
Multiplication.
The recursive formula for multiplication contains arithmetic expressions involving the operands and their left and right sets, such as the expression formula_11 that appears in the left set of the product of "x" and "y". This is to be understood as the set of surreal numbers resulting from choosing one number from each set that appears in the expression and evaluating the expression on these numbers. (In each individual evaluation of the expression, only one number is chosen from each set, and is substituted in each place where that set appears in the expression.)
This depends, in turn, on the ability to (a) multiply pairs of surreal "numbers" drawn from the left and right sets of "x" and "y" to get a surreal number, and negate the result; (b) multiply the surreal number "form" "x" or "y" and a surreal "number" drawn from the left or right set of the other operand to get a surreal number; and (c) add the resulting surreal numbers. This again involves special cases, this time containing 0 = { | }, the multiplicative identity 1 = { 0 | }, and its additive inverse -1 = { | 0 }.
Consistency.
It can be shown that the definitions of negation, addition and multiplication are consistent, in the sense that:
With these rules one can now verify that the numbers found in the first few generations were properly labeled. The construction rule is repeated to obtain more generations of surreals:
Arithmetic closure.
For each natural number (finite ordinal) "n", all numbers generated in "Sn" are dyadic fractions, i.e., can be written as an irreducible fraction formula_13
where "a" and "b" are integers and 0 ≤ "b" < "n".
The class of all surreal numbers that are generated in some "Sn" for finite "n" may be denoted as "S*" = formula_14. One may form the three classes "S0" = { 0 }, "S+" = formula_15, and "S-" = formula_16, and state that "S*" is the union of these three classes. No individual "Sn" is closed under addition and multiplication (except "S0"), but "S*" is; it is the subring of the rationals consisting of all dyadic fractions.
At an appropriate stage of transfinite induction, the surreal numbers may be expected to form a category on which the addition and multiplication operations (as well as the surreal construction step) are closed, and in which the multiplicative inverse of every nonzero number can be found. Assuming that one can find such a class, the surreal numbers, with their ordering and these algebraic operations, constitute an ordered field, with the caveat that they do not form a set but a proper class. In fact, it is a very special ordered field: the biggest one. Every other ordered field can be embedded in the surreals. (See also the definition of rational numbers and real numbers.)
"To Infinity ...".
Let there be an ordinal ω greater than the natural numbers, and define "S"ω as the set of all surreal numbers generated by the construction rule from subsets of "S*". (This is the same inductive step as before, since the ordinal number ω is the smallest ordinal that is larger than all natural numbers; however, the set union appearing in the inductive step is now an infinite union of finite sets, and so this step can only be performed in a set theory that allows such a union.) A unique infinitely large positive number occurs in "S"ω:
"S"ω also contains objects that can be identified as the rational numbers. For example, the ω-complete form of the fraction 1/3 is given by:
The product of this form of 1/3 with any form of 3 is a form whose left set contains only numbers less than 1 and whose right set contains only numbers greater than 1; the birthday property implies that this product is a form of 1.
Not only do all the rest of the rational numbers appear in "S"ω; the remaining finite real numbers do too. For example
The only infinities in "S"ω are ω and -ω; but there are other non-real numbers in "S"ω among the reals. Consider the smallest positive number in "S"ω:
This number is larger than zero but less than all positive dyadic fractions. It is therefore an infinitesimal number, often labeled ε. The ω-complete form of ε (resp. -ε) is the same as the ω-complete form of 0, except that 0 is included in the left (resp. right) set. The only "pure" infinitesimals in "S"ω are ε and its additive inverse -ε; adding them to any dyadic fraction "y" produces the numbers "y"±ε, which also lie in "S"ω.
One can determine the relationship between ω and ε by multiplying particular forms of them to obtain:
This expression is only well-defined in a set theory which permits transfinite induction up to formula_21. In such a system, one can demonstrate that all the elements of the left set of ω · ε are positive infinitesimals and all the elements of the right set are positive infinities, and therefore ω · ε is the oldest positive finite number, i. e., 1. Consequently,
Some authors systematically use ω−1 in place of the symbol ε.
Contents of "S"ω.
Given any "x" = { "L" | "R" } in "S"ω, exactly one of the following is true:
"S"ω is not an algebraic field, because it is not closed under arithmetic operations; consider ω+1, whose form formula_24 does not lie in any number in "S"ω. The maximal subset of "S"ω that is closed under (finite series of) arithmetic operations is the field of real numbers, obtained by leaving out the infinities ±ω, the infinitesimals ±ε, and the infinitesimal neighbors "y"±ε of each nonzero dyadic fraction "y".
This construction of the real numbers differs from the Dedekind cuts of standard analysis in that it starts from dyadic fractions rather than general rationals and naturally identifies each dyadic fraction in "S"ω with its forms in previous generations. (The ω-complete forms of real elements of "S"ω are in one-to-one correspondence with the reals obtained by Dedekind cuts, under the proviso that Dedekind reals corresponding to rational numbers are represented by the form in which the cut point is omitted from both left and right sets.) The rationals are not an identifiable stage in the surreal construction; they are merely the subset "Q" of "S"ω containing all elements "x" such that "x" "b" = "a" for some "a" and some nonzero "b", both drawn from "S"*. By demonstrating that "Q" is closed under individual repetitions of the surreal arithmetic operations, one can show that it is a field; and by showing that every element of "Q" is reachable from "S"* by a finite series (no longer than two, actually) of arithmetic operations "including multiplicative inversion", one can show that "Q" is strictly smaller than the subset of "S"ω identified with the reals.
"... And Beyond.".
Continuing to perform transfinite induction beyond "S"ω produces more ordinal numbers α, each represented as the largest surreal number having birthday α. (This is essentially a definition of the ordinal numbers resulting from transfinite induction.) The first such ordinal is ω+1 = { ω | }. There is another positive infinite number in generation ω+1:
It is important to observe that the surreal number ω−1 is not an ordinal; the ordinal ω is not the successor of any ordinal. This is a surreal number with birthday ω+1, which is labeled ω−1 on the basis that it coincides with the sum of ω = { 1, 2, 3, 4, ... | } and −1 = { | 0 }. Similarly, there are two new infinitesimal numbers in generation ω+1:
At a later stage of transfinite induction, there is a number larger than ω+"k" for all natural numbers "k":
This number may be labeled ω + ω both because its birthday is ω + ω (the first ordinal number not reachable from ω by the successor operation) and because it coincides with the surreal sum of ω and ω; it may also be labeled 2ω because it coincides with the product of ω = { 1, 2, 3, 4, ... | } and 2 = { 1 | }. It is the second limit ordinal; reaching it from ω via the construction step requires a transfinite induction on formula_25. This involves an infinite union of infinite sets, which is a "stronger" set theoretic operation than the previous transfinite induction required.
Note that the "conventional" addition and multiplication of ordinals does not always coincide with these operations on their surreal representations. The sum of ordinals 1 + ω equals ω, but the surreal sum is commutative and produces 1 + ω = ω + 1 > ω. The addition and multiplication of the surreal numbers associated with ordinals coincides with the natural sum and natural product of ordinals.
Just as 2ω is bigger than ω+"n" for any natural number "n", there is a surreal number ω/2 that is infinite but smaller than ω−"n" for any natural number "n".
where "x" − "Y" = { "x" − "y" | "y" in "Y" }. It can be identified as the product of ω and the form { 0 | 1 } of 1/2. The birthday of ω/2 is the limit ordinal 2ω.
Powers of ω.
To classify the "orders" of infinite and infinitesimal surreal numbers, also known as archimedean classes, Conway associated to each surreal number "x" the surreal number
where "r" and "s" range over the positive real numbers. If "x" < "y" then ω"y" is "infinitely greater" than ω"x", in that it is greater than "r" ω"x" for all real numbers "r". Powers of ω also satisfy the conditions
so they behave the way one would expect powers to behave.
Each power of ω also has the redeeming feature of being the "simplest" surreal number in its archimedean class; conversely, every archimedean class within the surreal numbers contains a unique simplest member. Thus, for every positive surreal number "x" there will always exist some positive real number "r" and some surreal number "y" so that "x" − "r" ω"y" is "infinitely smaller" than "x". The exponent "y" is the "base ω logarithm" of "x", defined on the positive surreals; it can be demonstrated that logω maps the positive surreals onto the surreals and that logω("xy") = logω("x") + logω("y").
This gets extended by transfinite induction so that every surreal number "x" has a "normal form" analogous to the Cantor normal form for ordinal numbers. Every surreal number may be uniquely written as
where every "r"α is a nonzero real number and the "y"αs form a strictly decreasing sequence of surreal numbers. This "sum", however, may have infinitely many terms, and in general has the length of an arbitrary ordinal number. (Zero corresponds of course to the case of an empty sequence, and is the only surreal number with no leading exponent.)
Looked at in this manner, the surreal numbers resemble a power series field, except that the decreasing sequences of exponents must be bounded in length by an ordinal and are not allowed to be as long as the class of ordinals.
Surcomplex numbers.
A surcomplex number is a number of the form formula_26, where "a" and "b" are surreal numbers. The surcomplex numbers form an algebraically closed field (except for being a proper class), isomorphic to the algebraic closure of the field generated by extending the rational numbers by a proper class of algebraically independent transcendental elements. Up to field isomorphism, this fact characterizes the field of surcomplex numbers within any fixed set theory.
Games.
The definition of surreal numbers contained one restriction: each element of L must be strictly less than each element of R. If this restriction is dropped we can generate a more general class known as "games". All games are constructed according to this rule:
Addition, negation, and comparison are all defined the same way for both surreal numbers and games.
Every surreal number is a game, but not all games are surreal numbers, e.g. the game { 0 | 0 } is not a surreal number. The class of games is more general than the surreals, and has a simpler definition, but lacks some of the nicer properties of surreal numbers. The class of surreal numbers forms a field, but the class of games does not. The surreals have a total order: given any two surreals, they are either equal, or one is greater than the other. The games have only a partial order: there exist pairs of games that are neither equal, greater than, nor less than each other. Each surreal number is either positive, negative, or zero. Each game is either positive, negative, "zero", or "fuzzy" (incomparable with zero, such as {1|−1}).
A move in a game involves the player whose move it is choosing a game from those available in L (for the left player) or R (for the right player) and then passing this chosen game to the other player. A player who cannot move because the choice is from the empty set has lost. A positive game represents a win for the left player, a negative game for the right player, a zero game for the second player to move, and a fuzzy game for the first player to move.
If "x", "y", and "z" are surreals, and "x"="y", then "x" "z"="y" "z". However, if "x", "y", and "z" are games, and "x"="y", then it is not always true that "x" "z"="y" "z". Note that "=" here means equality, not identity.
Application to combinatorial game theory.
The surreal numbers were originally motivated by studies of the game Go, and there are numerous connections between popular games and the surreals. In this section, we will use a capitalized "Game" for the mathematical object {L|R}, and the lowercase "game" for recreational games like Chess or Go.
We consider games with these properties:
For most games, the initial board position gives no great advantage to either player. As the game progresses and one player starts to win, board positions will occur where that player has a clear advantage. For analyzing games, it is useful to associate a Game with every board position. The value of a given position will be the Game {L|R}, where L is the set of values of all the positions that can be reached in a single move by Left. Similarly, R is the set of values of all the positions that can be reached in a single move by Right.
The zero Game (called 0) is the Game where L and R are both empty, so the player to move next (L or R) immediately loses. The sum of two Games G = { L1 | R1 } and H = { L2 | R2 } is defined as the Game G + H = { L1 + H, G + L2 | R1 + H, G + R2 } where the player to move chooses which of the Games to play in at each stage, and the loser is still the player who ends up with no legal move. One can imagine two chess boards between two players, with players making moves alternatively, but with complete freedom as to which board to play on. If G is the Game {L | R}, -G is the game {-R | -L}, i.e. with the role of the two players reversed. It is easy to show G - G = 0 for all Games G (where G - H is defined as G + (-H)).
This simple way to associate Games with games yields a very interesting result. Suppose two perfect players play a game starting with a given position whose associated Game is "x". We can classify all Games into four classes as follows:
More generally, we can define G > H as G - H > 0, and similarly for <, = and ||.
The notation G || H means that G and H are incomparable. G || H is equivalent to G−H || 0, i.e. that G > H, G < H and G = H are all false. Incomparable games are sometimes said to be "confused" with each other, because one or the other may be preferred by a player depending on what is added to it. A game confused with zero is said to be fuzzy, as opposed to positive, negative, or zero. An example of a fuzzy game is star (*).
Sometimes when a game nears the end, it will decompose into several smaller games that do not interact, except in that each player's turn allows moving in only one of them. For example, in Go, the board will slowly fill up with pieces until there are just a few small islands of empty space where a player can move. Each island is like a separate game of Go, played on a very small board. It would be useful if each subgame could be analyzed separately, and then the results combined to give an analysis of the entire game. This doesn't appear to be easy to do. For example, you might have two subgames where whoever moves first wins, but when they are combined into one big game, it's no longer the first player who wins. Fortunately, there is a way to do this analysis. Just use the following remarkable theorem:
A game composed of smaller games is called the disjunctive sum of those smaller games, and the theorem states that the method of addition we defined is equivalent to taking the disjunctive sum of the addends.
Historically, Conway developed the theory of surreal numbers in the reverse order of how it has been presented here. He was analyzing Go endgames, and realized that it would be useful to have some way to combine the analyses of non-interacting subgames into an analysis of their disjunctive sum. From this he invented the concept of a Game and the addition operator for it. From there he moved on to developing a definition of negation and comparison. Then he noticed that a certain class of Games had interesting properties; this class became the surreal numbers. Finally, he developed the multiplication operator, and proved that the surreals are actually a field, and that it includes both the reals and ordinals.
Alternative realizations.
Since Conway first introduced surreal numbers, several alternative constructions
have been developed.
Sign expansion.
Definitions.
In one alternative realization, called the "sign-expansion" or "sign-sequence" of a surreal number, a surreal number is a function whose domain is an ordinal and whose range is { − 1, + 1 }.
Define the binary predicate "simpler than" on numbers by "x" is simpler than "y" if "x" is a proper subset of "y", "i.e." if dom("x") < dom("y") and "x"(α) = "y"(α) for all α < dom("x").
For surreal numbers define the binary relation < to be lexicographic order (with the convention that "undefined values" are greater than −1 and less than 1). So "x" < "y" if one of the following holds:
Equivalently, let δ("x","y") = min({ dom("x"), dom("y")} ∪ { α :
α < dom("x") ∧ α < dom("y") ∧ "x"(α) ≠ "y"(α) }),
so that "x" = "y" if and only if δ("x","y") = dom("x") = dom("y"). Then, for numbers "x" and "y", "x" < "y" if and only if one of the following holds:
For numbers "x" and "y", "x" ≤ "y" if and only if "x" < "y" ∨ "x" = "y", and "x" > "y" if and only if "y" < "x". Also "x" ≥ "y" if and only if "y" ≤ "x".
The relation < is transitive, and for all numbers "x" and "y", exactly one of "x" < "y", "x" = "y", "x" > "y", holds (law of trichotomy). This means that < is a linear order (except that < is a proper class).
For sets of numbers, "L" and "R" such that ∀"x" ∈ "L" ∀"y" ∈ "R" ("x" < "y"), there exists a unique number "z" such that
Furthermore, "z" is constructible from "L" and "R" by transfinite induction. "z" is the simplest number between "L" and "R". Let the unique number "z" be denoted by σ("L","R").
For a number "x", define its left set "L"("x") and right set "R"("x") by
then σ("L"("x"),"R"("x")) = "x".
One advantage of this alternative realization is that equality is identity, not an inductively defined relation. Unlike Conway's realization of the surreal numbers, however, the sign-expansion requires a prior construction of the ordinals, while in Conway's realization, the ordinals are constructed as particular cases of surreals.
However, similar definitions can be made that obviate the need for prior construction of the ordinals. For instance, we could let the surreals be the (recursively-defined) class of functions whose domain is a subset of the surreals satisfying the transitivity rule ∀"g" ∈ dom "f" (∀"h" ∈ dom "g" ("h" ∈ dom "f" )) and whose range is { −, + }. "Simpler than" is very simply defined now—"x" is simpler than "y" if "x" ∈ dom "y". The total ordering is defined by considering "x" and "y" as sets of ordered pairs (as a function is normally defined): Either "x" = "y", or else the surreal number "z" = "x" ∩ "y" is in the domain of "x" or the domain of "y" (or both, but in this case the signs must disagree). We then have "x" < "y" if "x"("z") = − or "y"("z") = + (or both). Converting these functions into sign sequences is a straightforward task; arrange the elements of dom "f" in order of simplicity (i.e., inclusion), and then write down the signs that "f" assigns to each of these elements in order. The ordinals then occur naturally as those surreal numbers whose range is { + }.
Addition and multiplication.
The sum "x" + "y" of two numbers, "x" and "y", is defined by induction on dom("x") and dom("y") by "x" + "y" = σ("L","R"), where
The additive identity is given by the number 0 = { }, "i.e." the number 0 is the unique function whose domain is the ordinal 0, and the additive inverse of the number "x" is the number − "x", given by dom(− "x") = dom("x"), and, for α < dom("x"), (− "x")(α) = − 1 if "x"(α) = + 1, and (− "x")(α) = + 1 if "x"(α) = − 1.
It follows that a number "x" is positive if and only if 0 < dom("x") and "x"(0) = + 1, and "x" is negative if and only if 0 < dom("x") and "x"(0) = − 1.
The product "xy" of two numbers, "x" and "y", is defined by induction on dom("x") and dom("y") by "xy" = σ("L","R"), where
The multiplicative identity is given by the number 1 = { (0,+ 1) }, "i.e." the number 1 has domain equal to the ordinal 1, and 1(0) = + 1.
Correspondence with Conway.
The map from Conway's realization to sign expansions is given by "f"({ "L" | "R" }) = σ("M","S"), where "M" = { "f"("x") : "x" ∈ "L" } and "S" = { "f"("x") : "x" ∈ "R" }.
The inverse map from the alternative realization to Conway's realization is given by "g"("x") = { "L" | "R" }, where "L" = { "g"("y") : "y" ∈ "L"("x") } and "R" = { "g"("y") : "y" ∈ "R"("x") }.
Axiomatic approach.
In another approach to the surreals, given by Alling, explicit construction is bypassed altogether. Instead, a set of axioms is given that any particular approach to the surreals must satisfy. Much like the axiomatic approach to the reals, these axioms guarantee uniqueness up to isomorphism.
A triple formula_27 is a surreal number system if and only if the following hold:
Both Conway's original construction and the sign-expansion construction of surreals satisfy these axioms.
Given these axioms, Alling derives Conway's original definition of ≤ and develops surreal arithmetic.
Hahn series.
Alling also proves that the field of surreal numbers is isomorphic (as an ordered field) to the field of Hahn series with real coefficients on the value group of surreal numbers themselves (the series representation corresponding to the normal form of a surreal number, as defined above). This provides a connection between surreal numbers and more conventional mathematical approaches to ordered field theory.
Relation to hyperreals.
Philip Ehrlich has constructed an isomorphism between Conway's maximal surreal number field and the maximal hyperreals in von Neumann–Bernays–Gödel set theory.

</doc>
<doc id="51434" url="http://en.wikipedia.org/wiki?curid=51434" title="Sedenion">
Sedenion

In abstract algebra, the sedenions form a 16-dimensional noncommutative and nonassociative algebra over the reals obtained by applying the Cayley–Dickson construction to the octonions. The set of sedenions is denoted by formula_1.
The term "sedenion" is also used for other 16-dimensional algebraic structures, such as a tensor product of 2 copies of the biquaternions, or the algebra of 4 by 4 matrices over the reals, or that studied by .
Arithmetic.
Like octonions, multiplication of sedenions is neither commutative nor associative.
But in contrast to the octonions, the sedenions do not even have the property of being alternative.
They do, however, have the property of power associativity, which can be stated as for any element x of formula_1, the power formula_3 is well-defined. They are also flexible.
Every sedenion is a linear combination of the unit sedenions e0, e1, e2, e3, ...,e15,
which form a basis of the vector space of sedenions. Every sedenion can be represented in the form
Addition and subtraction are defined by the addition and subtraction of corresponding coefficients and multiplication is distributive over addition.
Like other algebras based on the Cayley–Dickson construction, the sedenions contain the algebra it was constructed from. So they contain the octonions (e0 to e7 in the table below), and therefore also the quaternions (e0 to e3), complex numbers (e0 and e1) and reals (e0).
The sedenions have a multiplicative identity element e0 and multiplicative inverses but they are not a division algebra because they have zero divisors. This means that two non-zero sedenions can be multiplied to obtain zero: an example is (e3 + e10)×(e6 − e15). All hypercomplex number systems based on the Cayley–Dickson construction after sedenions contain zero divisors.
The multiplication table of these unit sedenions follows:
From the above table, we can see that:
Applications.
 showed that the space of norm 1 zero-divisors of the sedenions is homeomorphic to the compact form of the exceptional Lie group G2.

</doc>
<doc id="51436" url="http://en.wikipedia.org/wiki?curid=51436" title="Octonion">
Octonion

In mathematics, the octonions are a normed division algebra over the real numbers, usually represented by the capital letter O, using boldface O or blackboard bold formula_1. There are only four such algebras, the other three being the real numbers R, the complex numbers C, and the quaternions H. The octonions are the largest such algebra, with eight dimensions; twice the number of dimensions of the quaternions, of which they are an extension. They are noncommutative and nonassociative, but satisfy a weaker form of associativity, namely they are alternative.
Octonions are not as well known as the quaternions and complex numbers, which are much more widely studied and used. Despite this, they have some interesting properties and are related to a number of exceptional structures in mathematics, among them the exceptional Lie groups. Additionally, octonions have applications in fields such as string theory, special relativity, and quantum logic.
The octonions were discovered in 1843 by John T. Graves, inspired by his friend William Hamilton's discovery of quaternions. Graves called his discovery octaves, and mentioned them in a letter to Hamilton dated 16 December 1843, but his first publication of his result in was slightly later than Cayley's article on them. The octonions were discovered independently by Arthur Cayley and are sometimes referred to as Cayley numbers or the Cayley algebra. described the early history of Graves' discovery.
Definition.
The octonions can be thought of as octets (or 8-tuples) of real numbers. Every octonion is a real linear combination of the unit octonions:
where "e"0 is the scalar or real element; it may be identified with the real number 1. That is, every octonion "x" can be written in the form
with real coefficients {"x""i"}.
Addition and subtraction of octonions is done by adding and subtracting corresponding terms and hence their coefficients, like quaternions. Multiplication is more complex. Multiplication is distributive over addition, so the product of two octonions can be calculated by summing the product of all the terms, again like quaternions. The product of each term can be given by multiplication of the coefficients and a multiplication table of the unit octonions, like this one:
Most off-diagonal elements of the table are antisymmetric, making it almost a skew-symmetric matrix except for the elements on the main diagonal, as well as the row and column for which "e"0 is an operand.
The table can be summarized by the relations:
where formula_5 is a completely antisymmetric tensor with value +1 when "ijk" = 123, 145, 176, 246, 257, 347, 365, and:
with "e"0 the scalar element, and "i", "j", "k" = 1 ... 7.
The above definition though is not unique, but is only one of 480 possible definitions for octonion multiplication with "e"0 = 1. The others can be obtained by permuting and changing the signs of the non-scalar basis elements. The 480 different algebras are isomorphic, and there is rarely a need to consider which particular multiplication rule is used. Each of these 480 definitions is invariant up to signs under some 7-cycle of the points (1234567), and for each 7-cycle there are four definitions, differing by signs and reversal of order. A common choice is to use the definition invariant under the 7-cycle (1234567) with "e"1"e"2 = "e"4 as it is particularly easy to remember the multiplication. A variation of this sometimes used is to label the elements of the basis by the elements ∞, 0, 1, 2, ..., 6, of the projective line over the finite field of order 7. The multiplication is then given by "e"∞ = 1 and "e"1"e"2 = "e"4, and all expressions obtained from this by adding a constant (mod 7) to all subscripts: in other words using the 7 triples (124) (235) (346) (450) (561) (602) (013). These are the nonzero codewords of the quadratic residue code of length 7 over the field of 2 elements. There is a symmetry of order 7 given by adding a constant mod 7 to all subscripts, and also a symmetry of order 3 given by multiplying all subscripts by one of the quadratic residues 1, 2, 4 mod 7.
The multiplication table can be given in terms of the following 7 quaternionic triples (omitting the identity element):
(I"jk"), ("i"J"k"), ("ij"K), (IJK), (I"im"), (J"jm"), (K"km") in which the lowercase items are vectors (mathematics and physics) and the uppercase ones are bivectors.
Cayley–Dickson construction.
A more systematic way of defining the octonions is via the Cayley–Dickson construction. Just as quaternions can be defined as pairs of complex numbers, the octonions can be defined as pairs of quaternions. Addition is defined pairwise. The product of two pairs of quaternions ("a", "b") and ("c", "d") is defined by
where formula_8 denotes the conjugate of the quaternion "z". This definition is equivalent to the one given above when the eight unit octonions are identified with the pairs
Fano plane mnemonic.
A convenient mnemonic for remembering the products of unit octonions is given by the diagram at the right, which represents the multiplication table of Cayley and Graves. This diagram with seven points and seven lines (the circle through 1, 2, and 3 is considered a line) is called the Fano plane. The lines are oriented. The seven points correspond to the seven standard basis elements of Im(O) (see definition below). Each pair of distinct points lies on a unique line and each line runs through exactly three points.
Let ("a", "b", "c") be an ordered triple of points lying on a given line with the order specified by the direction of the arrow. Then multiplication is given by
together with cyclic permutations. These rules together with
completely defines the multiplicative structure of the octonions. Each of the seven lines generates a subalgebra of O isomorphic to the quaternions H.
Conjugate, norm, and inverse.
The "conjugate" of an octonion
is given by
Conjugation is an involution of O and satisfies "(xy)"* = "y"* "x"* (note the change in order).
The "real part" of "x" is given by 
and the "imaginary part" by
The set of all purely imaginary octonions span a 7 dimension subspace of O, denoted Im(O).
Conjugation of octonions satisfies the equation
The product of an octonion with its conjugate, "x"* "x" = "x" "x"*, is always a nonnegative real number:
Using this the norm of an octonion can be defined, as
This norm agrees with the standard Euclidean norm on R8.
The existence of a norm on O implies the existence of inverses for every nonzero element of O. The inverse of "x" ≠ 0 is given by
It satisfies "x" "x"−1 = "x"−1 "x" = 1.
Properties.
Octonionic multiplication is neither commutative:
nor associative:
The octonions do satisfy a weaker form of associativity: they are alternative. This means that the subalgebra generated by any two elements is associative. Actually, one can show that the subalgebra generated by any two elements of O is isomorphic to R, C, or H, all of which are associative. Because of their non-associativity, octonions don't have matrix representations, unlike quaternions.
The octonions do retain one important property shared by R, C, and H: the norm on O satisfies
This implies that the octonions form a nonassociative normed division algebra. The higher-dimensional algebras defined by the Cayley–Dickson construction (e.g. the sedenions) all fail to satisfy this property. They all have zero divisors.
Wider number systems exist which have a multiplicative modulus (e.g. 16 dimensional conic sedenions). Their modulus is defined differently from their norm, and they also contain zero divisors.
It turns out that the only normed division algebras over the reals are R, C, H, and O. These four algebras also form the only alternative, finite-dimensional division algebras over the reals (up to isomorphism).
Not being associative, the nonzero elements of O do not form a group. They do, however, form a loop, indeed a Moufang loop.
Commutator and cross product.
The commutator of two octonions "x" and "y" is given by
This is antisymmetric and imaginary. If it is considered only as a product on the imaginary subspace Im(O) it defines a product on that space, the seven-dimensional cross product, given by
Like the cross product in three dimensions this is a vector orthogonal to "x" and "y" with magnitude
But like the octonion product it is not uniquely defined. Instead there are many different cross products, each one dependent on the choice of octonion product.
Automorphisms.
An automorphism, "A", of the octonions is an invertible linear transformation of O which satisfies
The set of all automorphisms of O forms a group called "G"2. The group "G"2 is a simply connected, compact, real Lie group of dimension 14. This group is the smallest of the exceptional Lie groups and is isomorphic to the subgroup of Spin(7) that preserves any chosen particular vector in its 8-dimensional real spinor representation. The group Spin (7) is in turn a subgroup of the group of isotopies described below.
"See also": PSL(2,7) - the automorphism group of the Fano plane.
Isotopies.
An isotopy of an algebra is a triple of bijective linear maps "a", "b", "c" such that if "xy"="z" then "a"("x")"b"("y")="c"("z"). For "a"="b"="c" this is the same as an automorphism. The isotopy group of an algebra is the group of all isotopies, which contains the group of automorphisms as a subgroup.
The isotopy group of the octonions is the group Spin8(R), with "a", "b", and "c" acting as the three 8-dimensional representations. The subgroup of elements where "c" fixes the identity is the subgroup Spin7(R), and the subgroup where "a", "b", and "c" all fix the identity is the automorphism group "G"2.
Integral octonions.
There are several natural ways to choose an integral form of the octonions. The simplest is just to take the octonions whose coordinates are integers. This gives a nonassociative algebra over the integers called the Gravesian octonions. However it is not a maximal order, and there are exactly 7 maximal orders containing it. These 7 maximal orders are all equivalent under automorphisms. The phrase "integral octonions" usually refers to a fixed choice of one of these seven orders.
These maximal orders were constructed by , Dickson and Bruck as follows. Label the 8 basis vectors by the points of the projective plane over the field with 7 elements. First form the "Kirmse integers" : these consist of octonions whose coordinates are integers or half integers, and that are half odd integers on one of the 16 sets 
of the extended quadratic residue code of length 8 over the field of 2 elements, given by
∅, (∞124) and its images under adding a constant mod 7, and the complements of these 8 sets. (Kirmse incorrectly claimed that these form a maximal order, so thought there were 8 maximal orders rather than 7, but as pointed out they are not closed under multiplication; this mistake occurs in several published papers.) Then switch infinity and any other coordinate; this gives a maximal order. There are 7 ways to do this, giving 7 maximal orders, which are all equivalent under cyclic permutations of the 7 coordinates 0123456.
The Kirmse integers and the 7 maximal orders are all isometric to the E8 lattice rescaled by a factor of 1/√2. In particular there are 240 elements of minimum nonzero norm 1 in each of these orders, forming a Moufang loop of order 240.
The integral octonions have a "division with remainder" property: given integral octonions "a" and "b"≠0, we can find "q" and "r" with "a" = "qb" + "r", where the remainder "r" has norm less than that of "b".
In the integral octonions, all left ideals and right ideals are 2-sided ideals, and the only 2-sided ideals are the principal ideals "nO" where "n" is a non-negative integer.
The integral octonions have a version of factorization into primes, though it is not straightforward to state because the octonions are not associative so the product of octonions depends on the order in which one does the products. The irreducible integral octonions are exactly those of prime norm, and every integral octonion can be written as a product of irreducible octonions. More precisely an integral octonion of norm "mn" can be written as a product of integral octonions of norms "m" and "n".
The automorphism group of the integral octonions is the group "G"2(F2) of order 12096, which has a simple subgroup of index 2 isomorphic to the unitary group 2"A"2(32). The isotopy group of the integral octonions is the perfect double cover of the group of rotations of the E8 lattice.
References.
</dl>

</doc>
<doc id="51438" url="http://en.wikipedia.org/wiki?curid=51438" title="Hypercomplex number">
Hypercomplex number

In mathematics, a hypercomplex number is a traditional term for an element of an algebra over the field of real numbers. In the nineteenth century number systems called quaternions, tessarines, coquaternions, biquaternions, and octonions became established concepts in mathematical literature, added to the real and complex numbers. The concept of a hypercomplex number covered them all, and called for a discipline to explain and classify them.
The cataloguing project began in 1872 when Benjamin Peirce first published his "Linear Associative Algebra", and was carried forward by his son Charles Sanders Peirce. Most significantly, they identified the nilpotent and the idempotent elements as useful hypercomplex numbers for classifications. The Cayley–Dickson construction used involutions to generate complex numbers, quaternions, and octonions out of the real number system. Hurwitz and Frobenius proved theorems that put limits on hypercomplexity: Hurwitz's theorem (normed division algebras), and Frobenius theorem (associative division algebras). Finally in 1956 J. Frank Adams used topological methods to prove that there exist only four finite-dimensional real division algebras: the reals ℝ, the complexes ℂ, the quaternions ℍ, and the octonions 𝕆.
It was matrix algebra that harnessed the hypercomplex systems. First, matrices contributed new hypercomplex numbers like 2 × 2 real matrices. Soon the matrix paradigm began to explain the others as they became represented by matrices and their operations. In 1907 Joseph Wedderburn showed that associative hypercomplex systems could be represented by matrices, or direct sums of systems of matrices. From that date the preferred term for a hypercomplex system became associative algebra as seen in the title of Wedderburn’s thesis at University of Edinburgh. Note however, that non-associative systems like octonions and hyperbolic quaternions represent another type of hypercomplex number. 
As Hawkins (1972) explains, the hypercomplex numbers are stepping stones to learning about Lie groups and group representation theory. For instance, in 1929 Emmy Noether wrote on "hypercomplex quantities and representation theory".
Review of the historic particulars gives body to the generalities of modern theory. In 1973 Kantor and Solodovnikov published a textbook on hypercomplex numbers which was translated in 1989; a reviewer says it has a "highly classical flavour". See Karen Parshall (1985) for a detailed exposition of the heyday of hypercomplex numbers, including the role of such luminaries as Theodor Molien and Eduard Study. For the transition to modern algebra, Bartel van der Waerden devotes thirty pages to hypercomplex numbers in his "History of Algebra" (1985).
Definition.
A definition of a hypercomplex number is given by as an element of a finite-dimensional algebra over the real numbers that is unital and distributive (but not necessarily associative). Elements are generated with real number coefficients formula_1 for a basis formula_2. Where possible, it is conventional to choose the basis so that formula_3. A technical approach to hypercomplex numbers directs attention first to those of dimension two. Higher dimensions are configured as Cliffordian or algebraic sums of other algebras.
Two-dimensional real algebras.
Theorem: 
Up to isomorphism, there are exactly three 2-dimensional unital algebras over the reals: the ordinary complex numbers, the split-complex numbers, and the dual numbers.
with arbitrary real numbers a0 and a1.
Using the common method of completing the square by
subtracting a1u and adding the quadratic complement a1²/4 to both sides yields
The three cases depend on this real value:
The complex numbers are the only two-dimensional hypercomplex algebra that is a field.
Algebras such as the split-complex numbers that include non-real roots of 1 also contain idempotents formula_16 and zero divisors formula_17, so such algebras cannot be division algebras. However, these properties can turn out to be very meaningful, for instance in describing the Lorentz transformations of special relativity.
In a 2004 edition of Mathematics Magazine the two-dimensional real algebras have been styled the "generalized complex numbers". The idea of cross-ratio of four complex numbers can be extended to the two-dimensional real algebras.
Higher-dimensional examples (more than one non-real axis).
Clifford algebras.
Clifford algebra is the unital associative algebra generated over an underlying vector space equipped with a quadratic form. Over the real numbers this is equivalent to being able to define a symmetric scalar product, "u"⋅"v" = ½("uv" + "vu") that can be used to orthogonalise the quadratic form, to give a set of bases {"e"1, ..., "e""k"} such that:
Imposing closure under multiplication now generates a multivector space spanned by 2"k" bases, {1, "e"1, "e"2, "e"3, ..., "e"1"e"2, ..., "e"1"e"2"e"3, ...}. These can be interpreted as the bases of a hypercomplex number system. Unlike the bases {"e"1, ..., "e""k"}, the remaining bases may or may not anti-commute, depending on how many simple exchanges must be carried out to swap the two factors. So "e"1"e"2 = −"e"2"e"1; but "e"1("e"2"e"3) = +("e"2"e"3)"e"1.
Putting aside the bases for which "e""i"2 = 0 (i.e. directions in the original space over which the quadratic form was degenerate), the remaining Clifford algebras can be identified by the label "C"ℓ"p","q"(R) indicating that the algebra is constructed from "p" simple bases with "e""i"2 = +1, "q" with "e""i"2 = −1, and where R indicates that this is to be a Clifford algebra over the reals—i.e. coefficients of elements of the algebra are to be real numbers.
These algebras, called geometric algebras, form a systematic set which turn out to be very useful in physics problems which involve rotations, phases, or spins, notably in classical and quantum mechanics, electromagnetic theory and relativity.
Examples include: the complex numbers "C"ℓ0,1(R); split-complex numbers "C"ℓ1,0(R); quaternions "C"ℓ0,2(R); split-biquaternions "C"ℓ0,3(R); coquaternions "C"ℓ1,1(R) ≈ "C"ℓ2,0(R) (the natural algebra of 2d space); "C"ℓ3,0(R) (the natural algebra of 3d space, and the algebra of the Pauli matrices); and "C"ℓ1,3(R) the spacetime algebra.
The elements of the algebra "C"ℓ"p","q"(R) form an even subalgebra "C"ℓ0"q"+1,"p"(R) of the algebra "C"ℓ"q"+1,"p"(R), which can be used to parametrise rotations in the larger algebra. There is thus a close connection between complex numbers and rotations in 2D space; between quaternions and rotations in 3D space; between split-complex numbers and (hyperbolic) rotations (Lorentz transformations) in 1+1 D space, and so on.
Whereas Cayley–Dickson and split-complex constructs with eight or more dimensions are not associative anymore with respect to multiplication, Clifford algebras retain associativity at any dimensionality.
In 1995 Ian R. Porteous wrote on "The recognition of subalgebras" in his book on Clifford algebras. His Proposition 11.4 summarizes the hypercomplex cases:
For extension beyond the classical algebras, see Classification of Clifford algebras.
Cayley–Dickson construction.
All of the Clifford algebras "C"ℓ"p","q"(R) apart from the real numbers, complex numbers and the quaternions contain non-real elements that square to +1; and so cannot be division algebras. A different approach to extending the complex numbers is taken by the Cayley–Dickson construction. This generates number systems of dimension 2"n", "n" in {2, 3, 4, ...}, with bases formula_23, where all the non-real basis elements anti-commute and satisfy formula_24. In 8 or more dimensions (n ≥ 3) these algebras are non-associative. In 16 or more dimensions (n ≥ 4) these algebras also have zero-divisors.
The first algebras in this sequence are the four-dimensional quaternions, eight-dimensional octonions, and 16-dimensional sedenions. An algebraic symmetry is lost with each increase in dimensionality: quaternion multiplication is not commutative, octonion multiplication is non-associative, and the norm of sedenions is not multiplicative.
The Cayley–Dickson construction can be modified by inserting an extra sign at some stages. It then generates two of the "split algebras" in the collection of composition algebras: 
As with quaternions, split-quaternions are not commutative, but further contain nilpotents; they are isomorphic to the 2 × 2 real matrices. Split-octonions are non-associative and contain nilpotents.
Tensor products.
The tensor product of any two algebras is another algebra, which can be used to produce many more examples of hypercomplex number systems.
In particular taking tensor products with the complex numbers (considered as algebras over the reals) leads to four-dimensional tessarines formula_30, eight-dimensional biquaternions formula_31, and 16-dimensional complex octonions formula_32.

</doc>
<doc id="51440" url="http://en.wikipedia.org/wiki?curid=51440" title="Quaternion">
Quaternion

In mathematics, the quaternions are a number system that extends the complex numbers. They were first described by Irish mathematician William Rowan Hamilton in 1843 and applied to mechanics in three-dimensional space. A feature of quaternions is that multiplication of two quaternions is noncommutative. Hamilton defined a quaternion as the quotient of two directed lines in a three-dimensional space or equivalently as the quotient of two vectors.
Quaternions find uses in both theoretical and applied mathematics, in particular for calculations involving three-dimensional rotations such as in three-dimensional computer graphics, computer vision and crystallographic texture analysis. In practical applications, they can be used alongside other methods, such as Euler angles and rotation matrices, or as an alternative to them, depending on the application.
In modern mathematical language, quaternions form a four-dimensional associative normed division algebra over the real numbers, and therefore also a domain. In fact, the quaternions were the first noncommutative division algebra to be discovered. The algebra of quaternions is often denoted by H (for "Hamilton"), or in blackboard bold by formula_1 (Unicode U+210D, ℍ). It can also be given by the Clifford algebra classifications "C"ℓ0,2(R) ≅ "C"ℓ03,0(R). The algebra H holds a special place in analysis since, according to the Frobenius theorem, it is one of only two finite-dimensional division rings containing the real numbers as a proper subring, the other being the complex numbers. These rings are also Euclidean Hurwitz algebras, of which quaternions are the largest associative algebra.
The unit quaternions can therefore be thought of as a choice of a group structure on the 3-sphere S3 that gives the group Spin(3), which is isomorphic to SU(2) and also to the universal cover of SO(3).
History.
Quaternion algebra was introduced by Hamilton in 1843. Important precursors to this work included Euler's four-square identity (1748) and Olinde Rodrigues' parameterization of general rotations by four parameters (1840), but neither of these writers treated the four-parameter rotations as an algebra. Carl Friedrich Gauss had also discovered quaternions in 1819, but this work was not published until 1900.
Hamilton knew that the complex numbers could be interpreted as points in a plane, and he was looking for a way to do the same for points in three-dimensional space. Points in space can be represented by their coordinates, which are triples of numbers, and for many years he had known how to add and subtract triples of numbers. However, Hamilton had been stuck on the problem of multiplication and division for a long time. He could not figure out how to calculate the quotient of the coordinates of two points in space.
The great breakthrough in quaternions finally came on Monday 16 October 1843 in Dublin, when Hamilton was on his way to the Royal Irish Academy where he was going to preside at a council meeting. As he walked along the towpath of the Royal Canal with his wife, the concepts behind quaternions were taking shape in his mind. When the answer dawned on him, Hamilton could not resist the urge to carve the formula for the quaternions,
"i"2 = "j"2 = "k"2 = "ijk" = −1,
into the stone of Brougham Bridge as he paused on it.
On the following day, Hamilton wrote a letter to his friend and fellow mathematician, John T. Graves, describing the train of thought that led to his discovery. This letter was later published in the "London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science", vol. xxv (1844), pp 489–95. In the letter, Hamilton states,
And here there dawned on me the notion that we must admit, in some sense, a fourth dimension of space for the purpose of calculating with triples ... An electric circuit seemed to close, and a spark flashed forth.
Hamilton called a quadruple with these rules of multiplication a "quaternion", and he devoted most of the remainder of his life to studying and teaching them. Hamilton's treatment is more geometric than the modern approach, which emphasizes quaternions' algebraic properties. He founded a school of "quaternionists", and he tried to popularize quaternions in several books. The last and longest of his books, "Elements of Quaternions", was 800 pages long; it was published shortly after his death.
After Hamilton's death, his student Peter Tait continued promoting quaternions. At this time, quaternions were a mandatory examination topic in Dublin. Topics in physics and geometry that would now be described using vectors, such as kinematics in space and Maxwell's equations, were described entirely in terms of quaternions. There was even a professional research association, the Quaternion Society, devoted to the study of quaternions and other hypercomplex number systems.
From the mid-1880s, quaternions began to be displaced by vector analysis, which had been developed by Josiah Willard Gibbs, Oliver Heaviside, and Hermann von Helmholtz. Vector analysis described the same phenomena as quaternions, so it borrowed some ideas and terminology liberally from the literature of quaternions. However, vector analysis was conceptually simpler and notationally cleaner, and eventually quaternions were relegated to a minor role in mathematics and physics. A side-effect of this transition is that Hamilton's work is difficult to comprehend for many modern readers. Hamilton's original definitions are unfamiliar and his writing style was wordy and difficult to understand.
However, quaternions have had a revival since the late 20th century, primarily due to their utility in describing spatial rotations. The representations of rotations by quaternions are more compact and quicker to compute than the representations by matrices. In addition, unlike Euler angles they are not susceptible to gimbal lock. For this reason, quaternions are used in computer graphics, computer vision, robotics, control theory, signal processing, attitude control, physics, bioinformatics, molecular dynamics, computer simulations, and orbital mechanics. For example, it is common for the attitude-control systems of spacecraft to be commanded in terms of quaternions. Quaternions have received another boost from number theory because of their relationships with the quadratic forms.
Since 1989, the Department of Mathematics of the National University of Ireland, Maynooth has organized a pilgrimage, where scientists (including the physicists Murray Gell-Mann in 2002, Steven Weinberg in 2005, and the mathematician Andrew Wiles in 2003) take a walk from Dunsink Observatory to the Royal Canal bridge. Hamilton's carving is no longer visible.
Historical impact on physics.
P.R. Girard’s essay "The quaternion group and modern physics" discusses some roles of quaternions in physics. It "shows how various physical covariance groups: SO(3), the Lorentz group, the general relativity group, the Clifford algebra SU(2), and the conformal group can be readily related to the quaternion group" in modern algebra. Girard began by discussing group representations and by representing some space groups of crystallography. He proceeded to kinematics of rigid body motion. Next he used complex quaternions (biquaternions) to represent the Lorentz group of special relativity, including the Thomas precession. He cited five authors, beginning with Ludwik Silberstein, who used a potential function of one quaternion variable to express Maxwell's equations in a single differential equation. Concerning general relativity, he expressed the Runge–Lenz vector. He mentioned the Clifford biquaternions (split-biquaternions) as an instance of Clifford algebra. Finally, invoking the reciprocal of a biquaternion, Girard described conformal maps on spacetime. Among the fifty references, Girard included Alexander Macfarlane and his "Bulletin" of the Quaternion Society. In 1999 he showed how Einstein's equations of general relativity could be formulated within a Clifford algebra that is directly linked to quaternions.
A more personal view of quaternions was written by Joachim Lambek in 1995. He wrote in his essay "If Hamilton had prevailed: quaternions in physics": "My own interest as a graduate student was raised by the inspiring book by Silberstein". He concluded by stating "I firmly believe that quaternions can supply a shortcut for pure mathematicians who wish to familiarize themselves with certain aspects of theoretical physics."
Definition.
As a set, the quaternions H are equal to R4, a four-dimensional vector space over the real numbers. H has three operations: addition, scalar multiplication, and quaternion multiplication. The sum of two elements of H is defined to be their sum as elements of R4. Similarly the product of an element of H by a real number is defined to be the same as the product by a scalar in R4. To define the product of two elements in H requires a choice of basis for R4. The elements of this basis are customarily denoted as 1, "i", "j", and "k". Every element of H can be uniquely written as a linear combination of these basis elements, that is, as "a"1 + "bi" + "cj" + "dk", where "a", "b", "c", and "d" are real numbers. The basis element 1 will be the identity element of H, meaning that multiplication by 1 does nothing, and for this reason, elements of H are usually written "a" + "bi" + "cj" + "dk", suppressing the basis element 1. Given this basis, associative quaternion multiplication is defined by first defining the products of basis elements and then defining all other products using the distributive law.
Multiplication of basis elements.
The identities
where "i", "j", and "k" are basis elements of H, determine all the possible products of "i", "j", and "k".
For example right-multiplying both sides of −1 = "ijk" by "k" gives
All the other possible products can be determined by similar methods, resulting in
which can be expressed as a table whose rows represent the left factor of the product and whose columns represent the right factor, as shown at the top of this article.
Noncommutativity of multiplication.
Unlike multiplication of real or complex numbers, multiplication of quaternions is not commutative. For example, "ij" = "k", while "ji" = −"k". The noncommutativity of multiplication has some unexpected consequences, among them that polynomial equations over the quaternions can have more distinct solutions than the degree of the polynomial. The equation "z"2 + 1 = 0, for instance, has infinitely many quaternion solutions "z" = "bi" + "cj" + "dk" with "b"2 + "c"2 + "d"2 = 1, so that these solutions lie on the two-dimensional surface of a sphere centered on zero in the three-dimensional subspace of quaternions with zero real part. This sphere intersects the complex plane at two points i and −"i".
The fact that quaternion multiplication is not commutative makes the quaternions an often-cited example of a strictly skew field.
Hamilton product.
For two elements "a"1 + "b"1"i" + "c"1"j" + "d"1"k" and "a"2 + "b"2"i" + "c"2"j" + "d"2"k", their product, called the Hamilton product ("a"1 + "b"1"i" + "c"1"j" + "d"1"k") ("a"2 + "b"2"i" + "c"2"j" + "d"2"k"), is determined by the products of the basis elements and the distributive law. The distributive law makes it possible to expand the product so that it is a sum of products of basis elements. This gives the following expression:
Now the basis elements can be multiplied using the rules given above to get:
Ordered list form.
Using the basis 1, "i", "j", "k" of H makes it possible to write H as a set of quadruples:
Then the basis elements are:
and the formulas for addition and multiplication are:
Scalar and vector parts.
A number of the form "a" + 0"i" + 0"j" + 0"k", where "a" is a real number, is called real, and a number of the form 0 + "bi" + "cj" + "dk", where "b", "c", and "d" are real numbers, and at least one of "b", "c" or "d" is nonzero, is called pure imaginary. If "a" + "bi" + "cj" + "dk" is any quaternion, then "a" is called its scalar part and "bi" + "cj" + "dk" is called its vector part. The scalar part of a quaternion is always real, and the vector part is always pure imaginary. Even though every quaternion can be viewed as a vector in a four-dimensional vector space, it is common to define a vector to mean a pure imaginary quaternion. With this convention, a vector is the same as an element of the vector space R3.
It is important to note, however, that the vector part of a quaternion is, in truth, an "axial" vector or "pseudovector", "not" an ordinary or "polar" vector, as was formally proven by S.L. Altmann in Ch. 12 of his 1986 book, "Rotations, Quaternions and Double Groups". A polar vector can be represented in calculations (for example, when rotated by a quaternion "similarity transform") by a pure quaternion, with no loss of information, but the two should not be confused. The axis of a "binary" (180 deg) rotation quaternion corresponds to the direction of the represented polar vector in such a case.
Hamilton called pure imaginary quaternions right quaternions and real numbers (considered as quaternions with zero vector part) scalar quaternions.
If a quaternion is divided up into a scalar part and a vector part, i.e.
then the formulas for addition and multiplication are:
where "·" is the dot product and "×" is the cross product.
Conjugation, the norm, and reciprocal.
Conjugation of quaternions is analogous to conjugation of complex numbers and to transposition (also known as reversal) of elements of Clifford algebras. To define it, let formula_20 be a quaternion. The conjugate of "q" is the quaternion formula_21. It is denoted by "q"∗, "q", "qt", or formula_22. Conjugation is an involution, meaning that it is its own inverse, so conjugating an element twice returns the original element. The conjugate of a product of two quaternions is the product of the conjugates "in the reverse order". That is, if "p" and "q" are quaternions, then ("pq")∗ = "q"∗"p"∗, not "p"∗"q"∗.
Unlike the situation in the complex plane,
the conjugation of a quaternion can be expressed entirely with multiplication and addition:
Conjugation can be used to extract the scalar and vector parts of a quaternion. The scalar part of "p" is ("p" + "p"∗) / 2, and the vector part of "p" is ("p" − "p"∗) / 2.
The square root of the product of a quaternion with its conjugate is called its norm and is denoted ||"q"|| (Hamilton called this quantity the "tensor" of "q", but this conflicts with modern meaning of "tensor"). In formula, this is expressed as follows:
This is always a non-negative real number, and it is the same as the Euclidean norm on H considered as the vector space R4. Multiplying a quaternion by a real number scales its norm by the absolute value of the number. That is, if α is real, then
This is a special case of the fact that the norm is "multiplicative", meaning that
for any two quaternions "p" and "q". Multiplicativity is a consequence of the formula for the conjugate of a product.
Alternatively it follows from the identity
(where "i" denotes the usual imaginary unit) and hence from the multiplicative property of determinants of square matrices.
This norm makes it possible to define the distance "d"("p", "q") between "p" and "q" as the norm of their difference:
This makes H into a metric space. Addition and multiplication are continuous in the metric topology. Indeed, for any scalar, positive "a" it holds
The continuity follows for vanishing "a". Similarly for the multiplication.
Unit quaternion.
A unit quaternion is a quaternion of norm one. Dividing a non-zero quaternion "q" by its norm produces a unit quaternion U"q" called the versor of "q":
Every quaternion has a polar decomposition "q" = ||"q"|| U"q".
Using conjugation and the norm makes it possible to define the reciprocal of a quaternion. The product of a quaternion with its reciprocal should equal 1, and the considerations above imply that the product of formula_31 and formula_32 (in either order) is 1. So the reciprocal of "q" is defined to be
This makes it possible to divide two quaternions "p" and "q" in two different ways. That is, their quotient can be either "p q"−1 or "q"−1"p". The notation p/q is ambiguous because it does not specify whether "q" divides on the left or the right.
Algebraic properties.
The set H of all quaternions is a vector space over the real numbers with dimension 4. (In comparison, the real numbers have dimension 1, the complex numbers have dimension 2, and the octonions have dimension 8.) Multiplication of quaternions, for example, is associative and distributes over vector addition, but it is not commutative. Therefore, the quaternions H are a non-commutative associative algebra over the real numbers. Even though H contains copies of the complex numbers, it is not an associative algebra over the complex numbers.
Because it is possible to divide quaternions, they form a division algebra. This is a structure similar to a field except for the non-commutativity of multiplication. Finite-dimensional associative division algebras over the real numbers are very rare. The Frobenius theorem states that there are exactly three: R, C, and H. The norm makes the quaternions into a normed algebra, and normed division algebras over the reals are also very rare: Hurwitz's theorem says that there are only four: R, C, H, and O (the octonions). The quaternions are also an example of a composition algebra and of a unital Banach algebra.
Because the product of any two basis vectors is plus or minus another basis vector, the set {±1, ±"i", ±"j", ±"k"} forms a group under multiplication. This group is called the quaternion group and is denoted Q8. The real group ring of Q8 is a ring R[Q8] which is also an eight-dimensional vector space over R. It has one basis vector for each element of Q8. The quaternions are the quotient ring of R[Q8] by the ideal generated by the elements 1 + (−1), "i" + (−"i"), "j" + (−"j"), and "k" + (−"k"). Here the first term in each of the differences is one of the basis elements 1, "i", "j", and "k", and the second term is one of basis elements −1, −"i", −"j", and −"k", not the additive inverses of 1, "i", "j", and "k".
Quaternions and the geometry of R3.
Because the vector part of a quaternion is a vector in R3, the geometry of R3 is reflected in the algebraic structure of the quaternions. Many operations on vectors can be defined in terms of quaternions, and this makes it possible to apply quaternion techniques wherever spatial vectors arise. For instance, this is true in electrodynamics and 3D computer graphics.
For the remainder of this section, "i", "j", and "k" will denote both imaginary basis vectors of H and a basis for R3. Notice that replacing "i" by −"i", "j" by −"j", and "k" by −"k" sends a vector to its additive inverse, so the additive inverse of a vector is the same as its conjugate as a quaternion. For this reason, conjugation is sometimes called the "spatial inverse".
Choose two imaginary quaternions "p" = "b"1"i" + "c"1"j" + "d"1"k" and "q" = "b"2"i" + "c"2"j" + "d"2"k". Their dot product is
This is equal to the scalar parts of "pq"∗, "qp"∗, "p"∗"q", and "q"∗"p". (Note that the vector parts of these four products are different.) It also has the formulas
The cross product of "p" and "q" relative to the orientation determined by the ordered basis "i", "j", and "k" is
(Recall that the orientation is necessary to determine the sign.) This is equal to the vector part of the product "pq" (as quaternions), as well as the vector part of −"q"∗"p"∗. It also has the formula
In general, let "p" and "q" be quaternions (possibly non-imaginary), and write
where "ps" and "qs" are the scalar parts, and formula_40 and formula_41 are the vector parts of "p" and "q". Then we have the formula
This shows that the noncommutativity of quaternion multiplication comes from the multiplication of pure imaginary quaternions. It also shows that two quaternions commute if and only if their vector parts are collinear.
For further elaboration on modeling three-dimensional vectors using quaternions, see quaternions and spatial rotation.
A possible visualisation was introduced by Andrew J. Hanson.
Matrix representations.
Just as complex numbers can be represented as matrices, so can quaternions. There are at least two ways of representing quaternions as matrices in such a way that quaternion addition and multiplication correspond to matrix addition and matrix multiplication. One is to use 2 × 2 complex matrices, and the other is to use 4 × 4 real matrices. In each case, the representation given is one of a family of linearly related representations. In the terminology of abstract algebra, these are injective homomorphisms from H to the matrix rings M(2, C) and M(4, R), respectively.
Using 2 × 2 complex matrices, the quaternion "a" + "bi" + "cj" + "dk" can be represented as
This representation has the following properties:
Using 4 × 4 real matrices, that same quaternion can be written as
Where the skew-symmetric matrices are not unique. In fact, there exist 48 distinct ordered sets of such matrices. 
In this representation, the conjugate of a quaternion corresponds to the transpose of the matrix. The fourth power of the norm of a quaternion is the determinant of the corresponding matrix. As with the 2 × 2 complex representation above, complex numbers can again be produced by constraining the coefficients suitably; for example, as block diagonal matrices with two 2 × 2 blocks by setting "c" = "d" = 0.
Sums of four squares.
Quaternions are also used in one of the proofs of Lagrange's four-square theorem in number theory, which states that every nonnegative integer is the sum of four integer squares. As well as being an elegant theorem in its own right, Lagrange's four square theorem has useful applications in areas of mathematics outside number theory, such as combinatorial design theory. The quaternion-based proof uses Hurwitz quaternions, a subring of the ring of all quaternions for which there is an analog of the Euclidean algorithm.
Quaternions as pairs of complex numbers.
Quaternions can be represented as pairs of complex numbers. From this perspective, quaternions are the result of applying the Cayley–Dickson construction to the complex numbers. This is a generalization of the construction of the complex numbers as pairs of real numbers.
Let C2 be a two-dimensional vector space over the complex numbers. Choose a basis consisting of two elements 1 and "j". A vector in C2 can be written in terms of the basis elements 1 and "j" as
If we define "j"2 = −1 and "ij" = −"ji", then we can multiply two vectors using the distributive law. Writing "k" in place of the product "ij" leads to the same rules for multiplication as the usual quaternions. Therefore the above vector of complex numbers corresponds to the quaternion "a" + "bi" + "cj" + "dk". If we write the elements of C2 as ordered pairs and quaternions as quadruples, then the correspondence is
Square roots of −1.
In the complex numbers, there are just two numbers, "i" and −"i", whose square is −1 . In H there are infinitely many square roots of minus one: the quaternion solution for the square root of −1 is the surface of the unit sphere in 3-space. To see this, let "q" = "a" + "bi" + "cj" + "dk" be a quaternion, and assume that its square is −1. In terms of "a", "b", "c", and "d", this means
To satisfy the last three equations, either "a" = 0 or "b", "c", and "d" are all 0. The latter is impossible because "a" is a real number and the first equation would imply that "a"2 = −1. Therefore "a" = 0 and "b"2 + "c"2 + "d"2 = 1. In other words, a quaternion squares to −1 if and only if it is a vector (that is, pure imaginary) with norm 1. By definition, the set of all such vectors forms the unit sphere.
Only negative real quaternions have an infinite number of square roots. All others have just two (or one in the case of 0).
The identification of the square roots of minus one in H was given by Hamilton but was frequently omitted in other texts. By 1971 the sphere was included by Sam Perlis in his three page exposition included in "Historical Topics in Algebra" (page 39) published by the National Council of Teachers of Mathematics. More recently, the sphere of square roots of minus one is described in Ian R. Porteous's book "Clifford Algebras and the Classical Groups" (Cambridge, 1995) in proposition 8.13 on page 60. Also in Conway (2003) "On Quaternions and Octonions" we read on page 40: "any imaginary unit may be called i, and perpendicular one j, and their product k", another statement of the sphere.
H as a union of complex planes.
Each pair of square roots of −1 creates a distinct copy of the complex numbers inside the quaternions. If "q"2 = −1, then the copy is determined by the function
In the language of abstract algebra, each is an injective ring homomorphism from C to H. The images of the embeddings corresponding to "q" and −"q" are identical.
Every non-real quaternion lies in a subspace of H isomorphic to C. Write "q" as the sum of its scalar part and its vector part:
Decompose the vector part further as the product of its norm and its versor:
(Note that this is not the same as formula_54.) The versor of the vector part of "q", formula_55, is a pure imaginary unit quaternion, so its square is −1. Therefore it determines a copy of the complex numbers by the function
Under this function, "q" is the image of the complex number formula_57. Thus H is the union of complex planes intersecting in a common real line, where the union is taken over the sphere of square roots of minus one, bearing in mind that the same plane is associated with the antipodal points of the sphere.
Commutative subrings.
The relationship of quaternions to each other within the complex subplanes of H can also be identified and expressed in terms of commutative subrings. Specifically, since two quaternions "p" and "q" commute ("pq" = "qp") only if they lie in the same complex subplane of H, the profile of H as a union of complex planes arises when one seeks to find all commutative subrings of the quaternion ring. This method of commutative subrings is also used to profile the coquaternions and 2 × 2 real matrices.
Functions of a quaternion variable.
Like functions of a complex variable, functions of a quaternion variable suggest useful physical models. For example, the original electric and magnetic fields described by Maxwell were functions of a quaternion variable.
Exponential, logarithm, and power.
Given a quaternion,
the exponential is computed as
and
It follows that the polar decomposition of a quaternion may be written
where the angle formula_62 and the unit vector formula_63 are defined by:
and
Any unit quaternion may be expressed in polar form as formula_66.
The power of a quaternion raised to an arbitrary (real) exponent formula_67 is given by:
Three-dimensional and four-dimensional rotation groups.
The term "conjugation", besides the meaning given above, can also mean taking an element "a" to "rar"−1 where "r" is some non-zero element (quaternion). All elements that are conjugate to a given element (in this sense of the word conjugate) have the same real part and the same norm of the vector part. (Thus the conjugate in the other sense is one of the conjugates in this sense.)
Thus the multiplicative group of non-zero quaternions acts by conjugation on the copy of R3 consisting of quaternions with real part equal to zero. Conjugation by a unit quaternion (a quaternion of absolute value 1) with real part cos(θ) is a rotation by an angle 2θ, the axis of the rotation being the direction of the imaginary part. The advantages of quaternions are:
The set of all unit quaternions (versors) forms a 3-sphere "S"3 and a group (a Lie group) under multiplication, double covering the group SO(3, R) of real orthogonal 3×3 matrices of determinant 1 since "two" unit quaternions correspond to every rotation under the above correspondence.
The image of a subgroup of versors is a point group, and conversely, the preimage of a point group is a subgroup of versors. The preimage of a finite point group is called by the same name, with the prefix binary. For instance, the preimage of the icosahedral group is the binary icosahedral group.
The versors' group is isomorphic to SU(2), the group of complex unitary 2×2 matrices of determinant 1.
Let "A" be the set of quaternions of the form "a" + "bi" + "cj" + "dk" where "a", "b", "c", and "d" are either all integers or all rational numbers with odd numerator and denominator 2. The set "A" is a ring (in fact a domain) and a lattice and is called the ring of Hurwitz quaternions. There are 24 unit quaternions in this ring, and they are the vertices of a 24-cell regular polytope with Schläfli symbol {3,4,3}.
Generalizations.
If "F" is any field with characteristic different from 2, and "a" and "b" are elements of "F", one may define a four-dimensional unitary associative algebra over "F" with basis 1, "i", "j", and "ij", where "i"2 = "a", "j"2 = "b" and "ij" = −"ji" (so "(ij)"2 = −"ab"). These algebras are called "quaternion algebras" and are isomorphic to the algebra of 2×2 matrices over "F" or form division algebras over "F", depending on 
the choice of "a" and "b".
Quaternions as the even part of Cℓ3,0(R).
The usefulness of quaternions for geometrical computations can be generalised to other dimensions, by identifying the quaternions as the even part Cℓ+3,0(R) of the Clifford algebra Cℓ3,0(R). This is an associative multivector algebra built up from fundamental basis elements σ1, σ2, σ3 using the product rules
If these fundamental basis elements are taken to represent vectors in 3D space, then it turns out that the "reflection" of a vector "r" in a plane perpendicular to a unit vector "w" can be written:
Two reflections make a rotation by an angle twice the angle between the two reflection planes, so
corresponds to a rotation of 180° in the plane containing σ1 and σ2. This is very similar to the corresponding quaternion formula,
In fact, the two are identical, if we make the identification
and it is straightforward to confirm that this preserves the Hamilton relations
In this picture, quaternions correspond not to vectors but to bivectors – quantities with magnitude and orientations associated with particular 2D "planes" rather than 1D "directions". The relation to complex numbers becomes clearer, too: in 2D, with two vector directions σ1 and σ2, there is only one bivector basis element σ1σ2, so only one imaginary. But in 3D, with three vector directions, there are three bivector basis elements σ1σ2, σ2σ3, σ3σ1, so three imaginaries.
This reasoning extends further. In the Clifford algebra Cℓ4,0(R), there are six bivector basis elements, since with four different basic vector directions, six different pairs and therefore six different linearly independent planes can be defined. Rotations in such spaces using these generalisations of quaternions, called rotors, can be very useful for applications involving homogeneous coordinates. But it is only in 3D that the number of basis bivectors equals the number of basis vectors, and each bivector can be identified as a pseudovector.
Dorst "et al." identify the following advantages for placing quaternions in this wider setting:
For further detail about the geometrical uses of Clifford algebras, see Geometric algebra.
Brauer group.
The quaternions are "essentially" the only (non-trivial) central simple algebra (CSA) over the real numbers, in the sense that every CSA over the reals is Brauer equivalent to either the reals or the quaternions. Explicitly, the Brauer group of the reals consists of two classes, represented by the reals and the quaternions, where the Brauer group is the set of all CSAs, up to equivalence relation of one CSA being a matrix ring over another. By the Artin–Wedderburn theorem (specifically, Wedderburn's part), CSAs are all matrix algebras over a division algebra, and thus the quaternions are the only non-trivial division algebra over the reals.
CSAs – rings over a field, which are simple algebras (have no non-trivial 2-sided ideals, just as with fields) whose center is exactly the field – are a noncommutative analog of extension fields, and are more restrictive than general ring extensions. The fact that the quaternions are the only non-trivial CSA over the reals (up to equivalence) may be compared with the fact that the complex numbers are the only non-trivial field extension of the reals.

</doc>
<doc id="51441" url="http://en.wikipedia.org/wiki?curid=51441" title="Zero divisor">
Zero divisor

In abstract algebra, an element a of a ring R is called a left zero divisor if there exists a nonzero x such that "ax" = 0, or equivalently if the map from "R" to "R" that sends x to ax is not injective. Similarly, an element a of a ring is called a right zero divisor if there exists a nonzero y such that "ya" = 0. This is a partial case of divisibility in rings. An element that is a left or a right zero divisor is simply called a zero divisor. An element a that is both a left and a right zero divisor is called a two-sided zero divisor (the nonzero x such that "ax" = 0 may be different from the nonzero y such that "ya" = 0). If the ring is commutative, then the left and right zero divisors are the same. 
An element of a ring that is not a zero divisor is called regular, or a non-zero-divisor. A zero divisor that is nonzero is called a nonzero zero divisor or a nontrivial zero divisor.
Zero as a zero divisor.
There is no need for a separate convention regarding the case "a" = 0, because the definition applies also in this case: 
Such properties are needed in order to make the following general statements true:
Some references choose to exclude 0 as a zero divisor by convention, but then they must introduce exceptions in the two general statements just made.
Zero divisor on a module.
Let R be a commutative ring, let M be an R-module, and let a be an element of R. One says that a is M-regular if the multiplication by a map formula_43 is injective, and that a is a zero divisor on M otherwise. The set of M-regular elements is a multiplicative set in R.
Specializing the definitions of "M-regular" and "zero divisor on M" to the case M = R recovers the definitions of "regular" and "zero divisor" given earlier in this article.

</doc>
<doc id="51442" url="http://en.wikipedia.org/wiki?curid=51442" title="Zorn's lemma">
Zorn's lemma

Zorn's lemma, also known as the Kuratowski–Zorn lemma, is a proposition of set theory that states:
Suppose a partially ordered set "P" has the property that every chain (i.e. totally ordered subset) has an upper bound in "P". Then the set "P" contains at least one maximal element.
It is named after the mathematicians Max Zorn and Kazimierz Kuratowski.
Background.
The terms used in the statement of the lemma are defined as follows. Suppose ("P",≤) is a partially ordered set. A subset "T" is "totally ordered" if for any "s", "t" in "T" we have "s" ≤ "t" or "t" ≤ "s". Such a set "T" has an "upper bound" "u" in "P" if "t" ≤ "u" for all "t" in "T". Note that "u" is an element of "P" but need not be an element of "T". An element "m" of "P" is called a "maximal element" (or "non-dominated") if there is no element "x" in "P" for which "m" < "x".
Note that "P" is not explicitly required to be non-empty.
However, the empty set is a chain (trivially), hence is required to have an upper bound, thus exhibiting at least one element of "P".
An equivalent formulation of the lemma is therefore:
Suppose a non-empty partially ordered set "P" has the property that every non-empty chain has an upper bound in "P". Then the set "P" contains at least one maximal element.
The distinction may seem subtle, but proofs involving Zorn's lemma often involve taking a union of some sort to produce an upper bound.
The case of an empty chain, hence empty union is a boundary case that is easily overlooked.
Zorn's lemma is equivalent to the well-ordering theorem and the axiom of choice, in the sense that any one of them, together with the Zermelo–Fraenkel axioms of set theory, is sufficient to prove the others. It occurs in the proofs of several theorems of crucial importance, for instance the Hahn–Banach theorem in functional analysis, the theorem that every vector space has a basis, Tychonoff's theorem in topology stating that every product of compact spaces is compact, and the theorems in abstract algebra that every nonzero ring has a maximal ideal and that every field has an algebraic closure.
Example.
Zorn's lemma can be used to show that every nontrivial ring "R" with unity contains a maximal ideal. In the terminology above, the set "P" consists of all (two-sided) ideals in "R" except "R" itself, which is not empty since it contains at least the trivial ideal {0}. This set is partially ordered by set inclusion. Finding a maximal ideal is the same as finding a maximal element in "P". The ideal "R" was excluded because maximal ideals by definition are not equal to "R".
To apply Zorn's lemma, take a non-empty totally ordered subset "T" of "P". It is necessary to show that "T" has an upper bound, i.e. that there exists an ideal "I" ⊆ "R" which is bigger than all members of "T" but still smaller than "R" (otherwise it would not be in "P"). Take "I" to be the union of all the ideals in "T". Because "T" contains at least one element, and that element contains at least 0, the union "I" contains at least 0 and is not empty. To prove that "I" is an ideal, note that if "a" and "b" are elements of "I", then there exist two ideals "J", "K" ∈ "T" such that "a" is an element of "J" and "b" is an element of "K". Since "T" is totally ordered, we know that "J" ⊆ "K" or "K" ⊆ "J". In the first case, both "a" and "b" are members of the ideal "K", therefore their sum "a" + "b" is a member of "K", which shows that "a" + "b" is a member of "I". In the second case, both "a" and "b" are members of the ideal "J", and thus "a" + "b" ∈ "I". Furthermore, if "r" ∈ "R", then "ar" and "ra" are elements of "J" and hence elements of "I". Thus, "I" is an ideal in "R".
Now, an ideal is equal to "R" if and only if it contains 1. (It is clear that if it is equal to "R", then it must contain 1; on the other hand, if it contains 1 and "r" is an arbitrary element of "R", then "r1" = "r" is an element of the ideal, and so the ideal is equal to "R".) So, if "I" were equal to "R", then it would contain 1, and that means one of the members of "T" would contain 1 and would thus be equal to "R" – but "R" is explicitly excluded from "P".
The condition of Zorn's lemma has been checked, and thus there is a maximal element in "P", in other words a maximal ideal in "R".
Note that the proof depends on the fact that our ring "R" has a multiplicative unit 1. Without this, the proof wouldn't work and indeed the statement would be false. For example, the ring with formula_1 as additive group and trivial multiplication (i. e. formula_2 for all formula_3) has no maximal ideal (and of course no 1): Its ideals are precisely the additive subgroups. The factor group formula_4 by a proper subgroup formula_5 is a divisible group, hence certainly not finitely generated, hence has a proper non-trivial subgroup, which gives rise to a subgroup and ideal containing formula_5.
Sketch of proof.
A sketch of the proof of Zorn's lemma follows, assuming the axiom of choice. Suppose the lemma is false. Then there exists a partially ordered set, or poset, "P" such that every totally ordered subset has an upper bound, and every element has a bigger one. For every totally ordered subset "T" we may then define a bigger element "b"("T"), because "T" has an upper bound, and that upper bound has a bigger element. To actually define the function "b", we need to employ the axiom of choice.
Using the function "b", we are going to define elements "a"0 < "a"1 < "a"2 < "a"3 < ... in "P". This sequence is really long: the indices are not just the natural numbers, but all ordinals. In fact, the sequence is too long for the set "P"; there are too many ordinals (a proper class), more than there are elements in any set, and the set "P" will be exhausted before long and then we will run into the desired contradiction.
The "ai" are defined by transfinite recursion: we pick "a"0 in "P" arbitrary (this is possible, since "P" contains an upper bound for the empty set and is thus not empty) and for any other ordinal "w" we set "a""w" = "b"({"a""v": "v" < "w"}). Because the "a""v" are totally ordered, this is a well-founded definition.
This proof shows that actually a slightly stronger version of Zorn's lemma is true:
If "P" is a poset in which every well-ordered subset has an upper bound, and if "x" is any element of "P", then "P" has a maximal element that is greater than or equal to "x". That is, there is a maximal element which is comparable to "x".
History.
The Hausdorff maximal principle is an early statement similar to Zorn's lemma.
K. Kuratowski proved in 1922 a version of the lemma close to its modern formulation (it applied to sets ordered by inclusion and closed under unions of well-ordered chains). Essentially the same formulation (weakened by using arbitrary chains, not just well-ordered) was independently given by Max Zorn in 1935, who proposed it as a new axiom of set theory replacing the well-ordering theorem, exhibited some of its applications in algebra, and promised to show its equivalence with the axiom of choice in another paper, which never appeared.
The name "Zorn's lemma" appears to be due to John Tukey, who used it in his book "Convergence and Uniformity in Topology" in 1940. Bourbaki's "Théorie des Ensembles" of 1939 refers to a similar maximal principle as "le théorème de Zorn". The name "" prevails in Poland and Russia.
Equivalent forms of Zorn's lemma.
Zorn's lemma is equivalent (in ZF) to three main results:
Moreover, Zorn's lemma (or one of its equivalent forms) implies some major results in other mathematical areas. For example,
In this sense, we see how Zorn's lemma can be seen as a powerful tool, especially in the sense of unified mathematics.
Pop Culture References.
This lemma was referenced on The Simpsons on the episode "Bart's New Friend".

</doc>
<doc id="51453" url="http://en.wikipedia.org/wiki?curid=51453" title="Singular function">
Singular function

In mathematics, a real-valued function "f" on the interval ["a", "b"] is said to be singular if it has the following properties:
A standard example of a singular function is the Cantor function, which is sometimes called the devil's staircase (a term also used for singular functions in general). There are, however, other functions that have been given that name. One is defined in terms of the circle map.
If "f"("x") = 0 for all "x" ≤ "a" and "f"("x") = 1 for all "x" ≥ "b", then the function can be taken to represent a cumulative distribution function for a random variable which is neither a discrete random variable (since the probability is zero for each point) nor an absolutely continuous random variable (since the probability density is zero everywhere it exists).
Singular functions occur, for instance, as sequences of spatially modulated phases or structures in solids and magnets, described in a prototypical fashion by the "model of Frenkel and Kontorova" and by the "ANNNI model", as well as in some dynamical systems. Most famously, perhaps, they lie at the center of the fractional quantum Hall effect.
When referring to functions with a singularity.
When discussing mathematical analysis in general, or more specifically real analysis or complex analysis or differential equations, it is common for a function which contains a mathematical singularity to be referred to as a 'singular function'. This is especially true when referring to functions which diverge to infinity at a point or on a boundary. For example one might say, ""1/x" becomes singular at the origin, so "1/x" is a singular function."
Advanced techniques for working with functions that contain singularities have been developed in the subject called distributional or generalized function analysis. A weak derivative is defined that allows singular functions to be used in partial differential equations, etc.
References.
(**) This condition depends on the references

</doc>
<doc id="51456" url="http://en.wikipedia.org/wiki?curid=51456" title="Mohammed Daoud Khan">
Mohammed Daoud Khan

Sardar Mohammed Daoud Khan or Daud Khan (July 18, 1909 – April 28, 1978) was the Prime Minister of Afghanistan from 1953 to 1963, and later became the first President of Afghanistan. He overthrew the monarchy of his first cousin Mohammed Zahir Shah and declared himself as the first President of Afghanistan from 1973 until his assassination in 1978 as a result of the Saur Revolution led by the Communist People's Democratic Party of Afghanistan (PDPA). Daoud Khan was known for his progressive policies, his efforts for the improvement of women's rights and for initiating two five-year modernization plans which increased the labor force by about 50 percent.
Early life.
Prince Daoud (Pashto for "David") was born in Kabul, the eldest son of the diplomat HRH Prince Mohammed Aziz Khan (1877–1933) (an older half-brother of King Mohammed Nadir Shah) and his wife, Khurshid Begum. He lost his father to an assassination in Berlin in 1933, while his father was serving as the Afghan Ambassador to Germany. He and his brother Naim Khan (1911–1978) then came under the tutelage of their uncle HRH Prince Hashim Khan (1884–1953). Daoud proved to be an apt student of politics. Educated in France, he served as Governor of the Eastern Province in 1934-35 and in 1938–39, and was Governor of Kandahar Province from 1935 to 1938.
In 1939, Daoud Khan was promoted to Lieutenant-General and commander of the important Kabul Army Corps until 1946. From 1946 to 1948, he served as Defense Minister, then Interior Minister from 1949 to 1951. In 1948, he served as Afghan Ambassador to France. In 1951, he was promoted to General and served in that capacity as Commander of the Central Corps of the Afghan Armed Forces in Kabul from 1951 to 1953.
Royal Prime Minister.
Daoud was appointed Prime Minister in September 1953 in an intra-family transfer of power that involved no violence. His ten-year tenure was noted for his foreign policy turn to the Soviet Union, the completion of the Helmand Valley project, which radically improved living conditions in southwestern Afghanistan, and tentative steps towards the emancipation of women.
Daoud supported a nationalistic and reunification of the Pakistani Pashtun people with Afghanistan, but this would have involved taking a considerable amount of territory from the new nation of Pakistan and was in direct antagonism to an older plan of the 1950s whereby a confederation between the two countries was proposed. The move further worried the non-Pashtun populations of Afghanistan such as the minority Tajik and Uzbek who suspected Daoud Khan's intention was to increase the Pashtun's disproportionate hold on political power. With the creation of an independent Pakistan in 1947, the Durand line conflict with the British colonialists was inherited by the two countries.
In 1961, as a result of Daoud's policies and support to militias in areas along the Durand Line, Pakistan closed its borders with Afghanistan causing an economic crisis and greater dependence on the USSR. The USSR became Afghanistan's principal trading partner. Within a few months, the USSR had sent jet airplanes, tanks, heavy and light artillery for a heavily discounted price tag of $25 million.
In 1960, Daoud sent troops across the poorly-marked Durand Line into the Bajaur Agency of Pakistan in an attempt to manipulate events in that area and to press the Pashtunistan issue, but the Afghan forces were routed by Pakistani military. During this period, the propaganda war from Afghanistan, carried on by radio, was relentless.
The crisis was finally resolved with the forced resignation of Daoud Khan in March 1963 and the re-opening of the border in May. Pakistan has continued to remain suspicious of Afghan intentions and Daoud's policy has left a negative impression in the eyes of many Tajiks who felt they were being disenfranchised for the sake of Pashtun Nationalism.
In 1964, King Zahir Shah introduced a new constitution, for the first time excluding all members of the royal family from the council of ministers. Daoud had already stepped down. In addition to having been prime minister, Daoud had also held the portfolios of Minister of Defense and Minister of Planning until 1963.
President of the Republic.
On July 17, 1973, Daoud seized power from his cousin (and brother-in-law) King Zahir in a bloodless coup. Departing from tradition, and for the first time in Afghan history, Daoud did not proclaim himself Shah, establishing instead a republic with himself as President.
In 1974, Daoud signed one of two economic packages that would enable Afghanistan to have a far more capable military because of increasing fears of lacking an up-to-date modern army when compared to the militaries of Iran and Pakistan. Daoud hosted General Secretary of the National Awami Party led by Khan Abdul Wali Khan, Ajmal Khattak,and others like Juma Khan Sufi, Baluch guerriallas etc. and started training Pakhtun Zalmay and young Baluchs and was sending them to Pakistan for sabotage and militancy. So much so that one of Bhutto's senior members, minister of interior and head of provincial party, Hayat Mohammad Sherpao, was killed and the relations with Pakistan further dipped. As a response Pakistan also started the same. By 1975, Pakistani Prime Minister Zulfiqar Ali Bhutto, through its Inter-Services Intelligence (ISI), has been engaged in promoting a proxy war in Afghanistan. Some of those trained and supported by Pakistan were Jalaluddin Haqqani, and Gulbuddin Hekmatyar.
King Zahir Shah's democratic constitution with elected members and the separation of powers was replaced by a now largely nominated loya jirga (meaning "grand assembly"). A new constitution backed by a loya jirga was promulgated in February 1977, but failed to satisfy all political factions.
In 1976, under pressure from the People's Democratic Party of Afghanistan (PDPA) and to increase domestic Pashtun support, he took a stronger line on the Pashtunistan issue and promoted a proxy war in Pakistan. Trade and transit agreements with Pakistan were subsequently severely affected. Soon after Daoud's army and police faced a growing Islamic fundamentalist movement, the Islamic fundamentalist movement's leaders fled to Pakistan. There, they were supported by Prime Minister Zulfiqar Ali Bhutto and encouraged to continue the fight against Daoud. Daoud was successful in suppressing the movement, however. Later in 1978, when Daoud was promoting his new foreign policy doctrine, he came to a tentative agreement on a solution to the Pashtunistan problem with Ali Bhutto.
In 1977, Daoud Khan presented a new constitution to the National Assembly, which wrote in several new articles and amended others. He also began to moderate his socialist policies. In 1978, there was a rift with the PDPA. Internally, Daoud attempted to distance himself from the communist elements within the coup. He was concerned about the tenor of many communists in his government and Afghanistan's growing dependency on the Soviet Union. These moves were highly criticized by Moscow, which feared that Afghanistan would soon become closer to the West, especially the United States; the Soviets had always feared that the United States could find a way to influence the government in Kabul.
A coup against Daoud, which may have been planned before he took power, was repressed shortly after his seizure of power. In October 1973, Mohammad Hashim Maiwandwal, a former prime minister and a highly respected former diplomat, was arrested in a coup plot and died in prison. This was at a time when Parchamis controlled the Ministry of Interior under circumstances corroborating the widespread belief that he had been tortured to death. One of the army generals arrested under suspicion of this plot with Maiwandwal was Mohammed Asif Safi, who was later released. Daoud personally apologized to him for the arrest.
Daoud wanted to lessen the country's dependence on the Soviet Union and attempted to promote a new foreign policy. He went to Egypt, India, Saudi Arabia, and Iran for aid. Surprisingly, he did not renew the Pashtunistan agitation; relations with Pakistan improved thanks to interventions from the US and Iran.
The following year, he established his own political party, the National Revolutionary Party, which became the focus of all political activity. In January 1977, a loya jirga approved the constitution establishing a presidential one-party system of government.
Diplomatic relations with the Soviet Union.
President Daoud met Leonid Brezhnev on a state visit to Moscow from April 12 to 15, 1977. Daoud had asked for a private meeting with the Soviet leader to discuss with him the increased pattern of Soviet actions in Afghanistan. In particular, he discussed the intensified Soviet attempt to unite the two factions of the Afghan communist parties, Parcham and Khalq. Brezhnev described Afghanistan's non-alignment as important to the USSR and essential to the promotion of peace in Asia, but warned him about the presence of experts from NATO countries stationed in the northern parts of Afghanistan. Daoud bluntly replied that Afghanistan would remain free, and that the Soviet Union would never be allowed to dictate how the country should be governed.
After returning to Afghanistan, Daoud made plans that his government would diminish its relationships with the Soviet Union, and instead forge closer contacts with the West as well as the oil-rich Saudi Arabia and Iran. Afghanistan signed a co-operative military treaty with Egypt and by 1977, the Afghan military and police force were being trained by Egyptian Armed forces. This angered the Soviet Union because Egypt took the same route in 1974 and distanced itself from the Soviets.
Communist coup and assassination.
The April 19, 1978 funeral of Mir Akbar Khyber, the prominent Parchami ideologue who had been murdered, served as a rallying point for the Afghan communists. An estimated 1,000 to 3,000 people gathered to hear speeches by PDPA leaders such as Nur Muhammad Taraki, Hafizullah Amin and Babrak Karmal.
Shocked by this demonstration of communist unity, Daoud ordered the arrest of the PDPA leaders, but he reacted too slowly. It took him a week to arrest Taraki, Karmal managed to escape to the USSR, and Amin was merely placed under house arrest. According to PDPA documents, Amin sent complete orders for the coup from his home while it was under armed guard using his family as messengers.
The army had been put on alert on April 26 because of a presumed "anti-Islamic" coup. On April 27, 1978, a coup d'état beginning with troop movements at the military base at Kabul International Airport, gained ground slowly over the next twenty-four hours as rebels battled units loyal to Daoud Khan in and around the capital. Some believe that Pakistani Prime Minister Bhutto paved the way for the Saur Revolution in Kabul by making Daoud spread the Afghan Armed Forces to the countryside. Bhutto himself was hanged to death in April 1979.
Death.
Daoud and most of his family were assassinated during a coup by members of the People's Democratic Party of Afghanistan (PDPA). The coup happened in the presidential palace on April 28, 1978. His death was not publicly announced after the coup. Instead, the new government declared that President Daoud had "resigned for health reasons." (In 1979, Taraki was killed by Amin, who, in turn, was killed by the KGB; Karmal died in 1996 of cancer in Moscow).
On June 28, 2008, the body of President Daoud and those of his family were found in two separate mass graves in the Pul-e-Charkhi area, District 12 of Kabul city. Initial reports indicate that sixteen corpses were in one grave and twelve others were in the second. (Source: Azadi Radio/BBC News). On December 4, 2008, the Afghan Health Ministry announced that the body of Daoud had been identified on the basis of teeth molds and a small golden Quran found near the body. The Quran was a present he had received from the king of Saudi Arabia. On March 17, 2009 Daoud was given a state funeral.
Personal life.
In 1934, Daoud married HRH Princess Zamina Begum (1917 — 26 April 1978), sister of King Zahir Shah. The couple had four sons and four daughters:

</doc>
<doc id="51458" url="http://en.wikipedia.org/wiki?curid=51458" title="AutoLISP">
AutoLISP

AutoLISP is a dialect of Lisp programming language built specifically for use with the full version of AutoCAD and its derivatives, which include "AutoCAD Map 3D", "AutoCAD Architecture" and "AutoCAD Mechanical". Neither the application programming interface nor the interpreter to execute AutoLISP code are included in the AutoCAD LT product line. 
Features.
AutoLISP is a small, dynamically scoped, dynamically typed LISP dialect with garbage collection, immutable list structure and settable symbols, lacking in such regular LISP features as macro system, records definition facilities, arrays, functions with variable number of arguments or let bindings. Aside from the core language, most of the primitive functions are for geometry, accessing AutoCAD's internal DWG database, or manipulation of graphical entities in AutoCAD. The properties of these graphical entities are revealed to AutoLISP as association lists in which values are paired with AutoCAD "group codes" that indicate properties such as definitional points, radii, colors, layers, linetypes, etc. AutoCAD loads AutoLISP code from .LSP files.
AutoLISP code can interact with the user through Autocad's graphical editor by use of primitive functions that allow user to pick points, choose objects on screen, input numbers and other data. AutoLisp also has a built-in GUI mini-language, the Dialog Control Language, for creating modal dialog boxes with automated layout, within AutoCAD. 
History.
AutoLISP was derived from an early version of XLISP, which was created by David Betz. The language was introduced in AutoCAD Version 2.18 in January 1986, and continued to be enhanced in successive releases up to Release 13 in February 1995. After that, its development was neglected by Autodesk in favor of more fashionable development environments like VBA, .NET and ObjectARX. However, it has remained AutoCAD's primary user customization language.
Vital-LISP, a considerably enhanced version of AutoLISP including an IDE, debugger and compiler, and ActiveX support, was developed and sold by third party developer Basis Software. Vital LISP was a superset of the existing AutoLISP language that added VBA-like access to the AutoCAD object model, reactors (event handling for AutoCAD objects), general ActiveX support, and some other general Lisp functions. Autodesk purchased this, renamed it Visual LISP, and briefly sold it as an add-on to AutoCAD 14 released in May 1997. It was incorporated into AutoCAD 2000 released in March 1999, as a replacement for AutoLISP. Since then Autodesk has chosen to halt major enhancements to Visual LISP in favor of focusing more effort on VBA and .NET and C++. As of January 31, 2014, Autodesk no longer supports versions of VBA older than 7.1. This is part of a long-term process of switching over from VBA to .NET for customization. 
AutoLISP has such a strong following that other CAD application vendors add it to their own products. Bricscad, IntelliCAD and others have AutoLISP functionality, so that AutoLISP users can consider using them as an alternative to AutoCAD. Most development involving AutoLISP since AutoCAD 2000 is actually performed within Visual LISP since the original AutoLISP engine was replaced with the Visual LISP engine. There are thousands of utilities and applications that have been developed using AutoLISP or Visual LISP (distributed as LSP, FAS and VLX files). 
Examples.
A simple Hello world program in AutoLisp would be:
 (princ "\nHello World!")
A more complex example might be:
 (setq pt (getpoint "\nPick point: "))
 (command "POINT" pt)
 (command "TEXT" (polar pt 0 0.6) 0 (strcat "X:" (rtos (car pt)) " Y:" (rtos (cadr pt))))
 (princ)
The above code defines a new function which places a point in the current drawing and writes the X and Y coordinates next to it. The name of the function includes a special prefix 'C:', which causes AutoCAD to recognize the function as a regular command. The user, upon typing 'POINTLABEL' at the AutoCAD Command Prompt, would be prompted to pick a point, either by typing in the X and Y coordinates, or clicking a location in the drawing. The function would then place a marker at that point, and create a text object next to it, containing the x and y coordinates. The function requires no parameters, and contains one Local variable ('pt').
This example also demonstrates AutoLisp's ability to use built-in AutoCAD commands to achieve the desired results; making two calls to the function 'command', while passing on the information necessary to complete the commands without further user input.

</doc>
<doc id="51459" url="http://en.wikipedia.org/wiki?curid=51459" title="South Platte River">
South Platte River

The South Platte River (Arapaho: Niinéniiniicíihéhe') is one of the two principal tributaries of the Platte River and is itself a major river of the American Midwest and the American Southwest/Mountain West, located in the U.S. states of Colorado and Nebraska. Its drainage basin includes much of the eastern flank of the Rocky Mountains in Colorado; much of the populated region known as the Colorado Front Range and Eastern Plains; and a portion of southeastern Wyoming in the vicinity of the city of Cheyenne. It joins the North Platte River in western Nebraska to form the Platte, which then flows across Nebraska to the Missouri. The river serves as the principal source of water for eastern Colorado. In its valley along the foothills in Colorado, it has permitted agriculture in an area of the Colorado Piedmont and Great Plains that is otherwise arid.
Description.
The river is formed in Park County, Colorado, southwest of Denver in the South Park grassland basin by the confluence of the South Fork and Middle Fork, approximately 15 mi southeast of Fairplay. Both forks rise along the eastern flank of the Mosquito Range, on the western side of South Park, which is drained by the tributaries at the headwaters of the river. From South Park, it passes through 50 mi of the Platte Canyon and its lower section, Waterton Canyon. Here, it is joined by the North Fork before emerging from the foothills southwest of the Denver suburb of Littleton. At Littleton, the river is impounded to form Chatfield Reservoir, a major source of drinking water for the Denver Metropolitan Area.
The river flows north through central Denver, which was founded along its banks at its confluence with Cherry Creek. The valley through Denver is highly industrialized, serving generally as the route for both the railroad lines, as well as Interstate 25. On the north side of Denver it is joined somewhat inconspicuously by Clear Creek, which descends from the mountains to the west in a canyon that was the cradle of the Pike's Peak Gold Rush. North of Denver it flows through the agricultural heartland of the Piedmont (a shale region that was formed through erosion by the ancestor of the river following the creation of the Rockies). It flows directly past the communities of Brighton and Fort Lupton, and is joined in succession by Saint Vrain Creek, the Little Thompson River, the Big Thompson River, and the Cache la Poudre River, which it receives just east of Greeley.
East of Greeley it turns eastward, flowing across the Colorado Eastern Plains, past Fort Morgan and Brush, where it turns northeastward. It continues past Sterling, and runs into Nebraska between Julesburg, Colorado and Big Springs, Nebraska. In Nebraska, it passes south of Ogallala and joins the North Platte River near the city of North Platte.
The South Platte River through Denver is on the U.S. EPA's list of impaired waterbodies for pathogen impairment, with E. coli as the representative pathogen species. Other water issues involve the appearance of the New Zealand Mud Snail and of the Zebra Mussel.
History.
The South Platte was originally called the Rio Chato (see the report of the Humano and Bonillo Expedition). In 1702, it was named the Rio Jesus Maria by Captain Jose Lopez, the Tewa Irish scout and Captain of War of the New Mexico Indian Auxiliaries who was ordered by the Viceroy of New Spain to search the Tierra Incognita for a French incursion into New Mexico. The South Platte River also served as a vital water source in Colorado. Long before the city of Denver was created many travelers came to the South Platte River to escape the arid Great Plains. These people could survive the heat but not without the vital water source that the South Platte River gave them. Buckets and wells sufficed as a water system for a while but eventually the Denver Water System was created.
Dams.
In an arid region of the United States, the South Platte is marked with several dams. The first notable water impoundment on the South Platte is Antero Reservoir. "Antero" is derived from the Spanish word "first," as it was the first dam on the South Platte River near the river's origin.
The next dam is Spinney Mountain Reservoir. At capacity Spinney Mountain covers 2500 acre. A bottom release dam, Spinney releases to the east of the inlet.
Two miles below Spinney Mountain Reservoir, the river enters Eleven Mile Reservoir, with a capacity of 97000 acre.ft. The Eleven Mile Reservoir Dam drains into Eleven Mile Canyon, which runs through Forest Service land. Under the reservoir are three former Colorado towns, Howbert, Idlewild, and Freshwater Station, which were submerged to meet the water needs of Denver.
From Eleven Mile Canyon, the South Platte runs northeast to Cheesman Reservoir, named for Denver water pioneer Walter S. Cheesman. At its completion in 1905, the dam was the world’s tallest, at 221 ft above the streambed. The reservoir and related facilities were purchased in November 1918 by the Denver Water Board. Cheesman was the first reservoir of Denver's mountain storage facilities and has been designated a National Historic Civil Engineering Landmark. Cheesman Reservoir feeds Cheesman Canyon. Six miles below Cheesman Reservoir is the town of Deckers; there, the river bends north for approximately 17 mi to the confluence with the North Fork of the South Platte.
In the late 1980s, a proposal was put forth for the Two Forks Dam, which would have created a reservoir flooding the entire section from the North Fork confluence to the town of Deckers. In 1990 the Environmental Protection Agency vetoed the permit, calling the project an "environmental catastrophe."
From the confluence, the river flows towards Denver and enters Strontia Springs Reservoir.
Below Strontia Springs the South Platte runs through Waterton Canyon before entering Chatfield Reservoir. Chatfield marks the seventh and final dam on the South Platte until it merges with the North Platte.
Fly fishing overview.
The South Platte River is a Gold Medal Western trout river on the Eastern Slope of Colorado. The river is well known for its wild trophy population of Brown Trout and Rainbow Trout. As a result of the close proximity to Denver, the river sees thousands of fly fishing enthusiasts each year. With seven dams on the river, the South Platte is considered a tailwater fishery. Most of these dams are bottom released which allow for both stable water temperatures throughout the year, and year round fly fishing. Popular fly fishing stretches of the river include Waterton Canyon, Deckers, Cheesman Canyon and the Dream Stream.

</doc>
<doc id="51462" url="http://en.wikipedia.org/wiki?curid=51462" title="Machine">
Machine

A machine is a tool containing one or more parts that uses energy to perform an intended action. Machines are usually powered by mechanical, chemical, thermal, or electrical means, and are often motorized. Historically, a power tool also required moving parts to classify as a machine. However, the advent of electronics has led to the development of power tools without moving parts that are considered machines.
A simple machine is a device that simply transforms the direction or magnitude of a force, but a large number of more complex machines exist. Examples include vehicles, electronic systems, molecular machines, computers, television, and radio.
Etymology.
The word "machine" derives from the Latin word "machina", which in turn derives from the Greek (Doric μαχανά "makhana", Ionic μηχανή "mekhane" "contrivance, machine, engine", a derivation from μῆχος "mekhos" "means, expedient, remedy").
A wider meaning of "fabric, structure" is found in classical Latin, but not in Greek usage.
This meaning is found in late medieval French, and is adopted from the French into English in the mid-16th century.
In the 17th century, the word could also mean a scheme or plot, a meaning now expressed by the derived machination.
The modern meaning develops out of specialized application of the term to stage engines used in theater and to military siege engines, both in the late 16th and early 17th centuries.
The OED traces the formal, modern meaning to John Harris' "Lexicon Technicum" (1704), which has:
The word "engine" used as a (near-)synonym both by Harris and in later language derives ultimately (via Old French) from Latin "ingenium" "ingenuity, an invention".
History.
Perhaps the first example of a human made device designed to manage power is the hand axe, made by chipping flint to form a wedge. A wedge is a simple machine that transforms lateral force and movement of the tool into a transverse splitting force and movement of the workpiece.
The idea of a "simple machine" originated with the Greek philosopher Archimedes around the 3rd century BC, who studied the Archimedean simple machines: lever, pulley, and screw. He discovered the principle of mechanical advantage in the lever. Later Greek philosophers defined the classic five simple machines (excluding the inclined plane) and were able to roughly calculate their mechanical advantage. Heron of Alexandria (ca. 10–75 AD) in his work "Mechanics" lists five mechanisms that can "set a load in motion"; lever, windlass, pulley, wedge, and screw, and describes their fabrication and uses. However the Greeks' understanding was limited to statics (the balance of forces) and did not include dynamics (the tradeoff between force and distance) or the concept of work.
During the Renaissance the dynamics of the "Mechanical Powers", as the simple machines were called, began to be studied from the standpoint of how much useful work they could perform, leading eventually to the new concept of mechanical work. In 1586 Flemish engineer Simon Stevin derived the mechanical advantage of the inclined plane, and it was included with the other simple machines. The complete dynamic theory of simple machines was worked out by Italian scientist Galileo Galilei in 1600 in "Le Meccaniche" ("On Mechanics"). He was the first to understand that simple machines do not create energy, they merely transform it.
The classic rules of sliding friction in machines were discovered by Leonardo da Vinci (1452–1519), but remained unpublished in his notebooks. They were rediscovered by Guillaume Amontons (1699) and were further developed by Charles-Augustin de Coulomb (1785).
Types.
Mechanical.
The word mechanical refers to the work that has been produced by machines or the machinery. It mostly relates to the machinery tools and the mechanical applications of science. Some of its synonyms are automatic and mechanic.
Simple machines.
The idea that a machine can be broken down into simple movable elements led Archimedes to define the lever, pulley and screw as simple machines. By the time of the Renaissance this list increased to include the wheel and axle, wedge and inclined plane.
Engines.
An engine or motor is a machine designed to convert energy into useful mechanical motion. Heat engines, including internal combustion engines and external combustion engines (such as steam engines) burn a fuel to create heat, which is then used to create motion. Electric motors convert electrical energy into mechanical motion, pneumatic motors use compressed air and others, such as wind-up toys use elastic energy. In biological systems, molecular motors like myosins in muscles use chemical energy to create motion.
Electrical.
Electrical means operating by or producing electricity, relating to or concerned with electricity. In other words it means using, providing, producing, transmitting or operated by electricity.
Electrical machine.
An electrical machine is the generic name for a device that converts mechanical energy to electrical energy, converts electrical energy to mechanical energy, or changes alternating current from one voltage level to a different voltage level.
Electronic machine.
Electronics is the branch of physics, engineering and technology dealing with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes and integrated circuits, and associated passive interconnection technologies. The nonlinear behaviour of active components and their ability to control electron flows makes amplification of weak signals possible and is usually applied to information and signal processing. Similarly, the ability of electronic devices to act as switches makes digital information processing possible. Interconnection technologies such as circuit boards, electronic packaging technology, and other varied forms of communication infrastructure complete circuit functionality and transform the mixed components into a working system.
Computing machines.
Computers store and manipulate the flow of electrons, with patterns in this storage and flow being interpreted as information manipulation. See State machine and Turing machine.
Charles Babbage designed various machines to tabulate logarithms and other functions in 1837. His Difference engine is the first mechanical calculator. This machine is considered a forerunner of the modern computer, though none were built in Babbage's lifetime.
Molecular machines.
Study of the molecules and proteins that are the basis of biological functions has led to the concept of a molecular machine. For example, current models of the operation of the kinesin molecule that transports vesicles inside the cell as well as the myosin molecule that operates against actin to cause muscle contraction; these molecules control movement in response to chemical stimuli.
Researchers in nano-technology are working to construct molecules that perform movement in response to a specific stimulus. In contrast to molecules such as kinesin and myosin, these nanomachines or molecular machines are constructions like traditional machines that are designed to perform in a task.
Machine elements.
Machines are assembled from standardized types of components. These elements consist of mechanisms that control movement in various ways such as gear trains, transistor switches, belt or chain drives, linkages, cam and follower systems, brakes and clutches, and "structural components" such as frame members and fasteners.
Modern machines include sensors, actuators and computer controllers. The shape, texture and color of covers provide a styling and operational interface between the mechanical components of a machine and its users.
Mechanisms.
Assemblies within a machine that control movement are often called "mechanisms." Mechanisms are generally classified as gears and gear trains, cam and follower mechanisms, and linkages, though there are other special mechanisms such as clamping linkages, indexing mechanisms and friction devices such as brakes and clutches.
For more details on mechanical machines see Machine (mechanical) and Mechanical systems.
Controllers.
Controllers combine sensors, logic, and actuators to maintain the performance of components of a machine. Perhaps the best known is the flyball governor for a steam engine. Examples of these devices range from a thermostat that as temperature rises opens a valve to cooling water to speed controllers such the cruise control system in an automobile. The programmable logic controller replaced relays and specialized control mechanisms with a programmable computer. Servo motors that accurately position a shaft in response to an electrical command are the actuators that make robotic systems possible.
Impact.
Industrial Revolution.
The Industrial Revolution was a period from 1750 to 1850 where changes in agriculture, manufacturing, mining, transportation, and technology had a profound effect on the social, economic and cultural conditions of the times. It began in the United Kingdom, then subsequently spread throughout Western Europe, North America, Japan, and eventually the rest of the world.
Starting in the later part of the 18th century, there began a transition in parts of Great Britain's previously manual labour and draft-animal–based economy towards machine-based manufacturing. It started with the mechanisation of the textile industries, the development of iron-making techniques and the increased use of refined coal.
Mechanization and automation.
Mechanization or mechanisation (BE) is providing human operators with machinery that assists them with the muscular requirements of work or displaces muscular work. In some fields, mechanization includes the use of hand tools. In modern usage, such as in engineering or economics, mechanization implies machinery more complex than hand tools and would not include simple devices such as an un-geared horse or donkey mill. Devices that cause speed changes or changes to or from reciprocating to rotary motion, using means such as gears, pulleys or sheaves and belts, shafts, cams and cranks, usually are considered machines. After electrification, when most small machinery was no longer hand powered, mechanization was synonymous with motorized machines.
Automation is the use of control systems and information technologies to reduce the need for human work in the production of goods and services. In the scope of industrialization, automation is a step beyond mechanization. Whereas mechanization provides human operators with machinery to assist them with the muscular requirements of work, automation greatly decreases the need for human sensory and mental requirements as well. Automation plays an increasingly important role in the world economy and in daily experience.
Automata.
An automaton (plural: automata or automatons) is a self-operating machine. The word is sometimes used to describe a robot, more specifically an autonomous robot. An alternative spelling, now obsolete, is automation.

</doc>
<doc id="51464" url="http://en.wikipedia.org/wiki?curid=51464" title="Great Plains">
Great Plains

The Great Plains is the broad expanse of flat land, much of it covered in prairie, steppe and grassland, that lies west of the Mississippi River tallgrass prairie states and east of the Rocky Mountains in the United States and Canada. This area covers parts, but not all, of the states of Colorado, Kansas, Montana, Nebraska, New Mexico, North Dakota, Oklahoma, South Dakota, Texas, and Wyoming, and the Canadian provinces of Alberta, Manitoba and Saskatchewan. The region is known for supporting extensive cattle ranching and dry farming.
The Canadian portion of the Plains is known as the Prairies. Some geographers include some territory of northern Mexico in the Plains, but many stop at the Rio Grande.
Usage.
The term "Great Plains" is used in the United States to describe a sub-section of the even more vast Interior Plains physiographic division, which covers much of the interior of North America. It also has currency as a region of human geography, referring to the Plains Indians or the Plains States.
In Canada the term is little used; Natural Resources Canada, the government department responsible for official mapping and equivalent to the United States Geological Survey, treats the Interior Plains as one unit consisting of several related plateaux and plains. There is no region referred to as the "Great Plains" in "The Atlas of Canada". In terms of human geography, the term "prairie" is more commonly used in Canada, and the region is known as the Prairie Provinces or simply "the Prairies."
The North American Environmental Atlas, produced by the Commission for Environmental Cooperation, a NAFTA agency composed of the geographical agencies of the Mexican, American, and Canadian governments uses the "Great Plains" as an ecoregion synonymous with predominant prairies and grasslands rather than as physiographic region defined by topography. The Great Plains ecoregion includes five sub-regions: Temperate Prairies, West-Central Semi-Arid Prairies, South-Central Semi-Arid Prairies, Texas Louisiana Coastal Plains, and Tamaulipus-Texas Semi-Arid Plain, which overlap or expand upon other Great Plains designations.
Extent.
The region is about 500 mi east to west and 2000 mi north to south. Much of the region was home to American bison herds until they were hunted to near extinction during the mid/late 19th century. It has an area of approximately 1300000 km2. Current thinking regarding the geographic boundaries of the Great Plains is shown by this at the Center for Great Plains Studies, University of Nebraska–Lincoln.
The term "Great Plains", for the region west of about the 96th or 98th meridian and east of the Rocky Mountains, was not generally used before the early 20th century. Nevin Fenneman's 1916 study, "Physiographic Subdivision of the United States", brought the term Great Plains into more widespread usage. Before that the region was almost invariably called the High Plains, in contrast to the lower Prairie Plains of the Midwestern states. Today the term "High Plains" is used for a subregion of the Great Plains.
Geology.
The Great Plains are the westernmost portion of the vast North American Interior Plains, which extend east to the Appalachian Plateau. The United States Geological Survey divides the Great Plains in the United States into ten physiographic subdivisions:
Paleontology.
During the Cretaceous Period (145-66 million years ago), the Great Plains were covered by a shallow inland sea called the Western Interior Seaway. However, during the Late Cretaceous to the Paleocene (65-55 million years ago), the seaway had begun to recede, leaving behind thick marine deposits and a relatively flat terrain which the seaway had once occupied.
During the Cenozoic era, specifically about 25 million years ago during the Miocene and Pliocene epochs, the continental climate became favorable to the evolution of grasslands. Existing forest biomes declined and grasslands became much more widespread. 
The grasslands provided a new niche for mammals, including many ungulates and glires, that switched from browsing diets to grazing diets. Traditionally, the spread of grasslands and the development of grazers have been strongly linked. However, an examination of mammalian teeth suggests that it is the open, gritty habitat and not the grass itself which is linked to diet changes in mammals, giving rise to the "grit, not grass" hypothesis.
Paleontological finds in the area have yielded bones of woolly mammoths, saber-toothed tigers and other ancient animals, as well as dozens of other megafauna (large animals over 100 lb) – such as giant sloths, horses, mastodons, and American lion – that dominated the area of the ancient Great Plains for millions of years. The vast majority of these animals went extinct in North America at the end of the Pleistocene (around 13,000 years ago).
Climate.
In general, the Great Plains have a wide variety of weather through the year, with very cold and harsh winters and very hot and humid summers. Wind speeds are often very high, especially in winter. Grasslands are among the least protected biomes. Humans have converted much of the prairies for agricultural purposes or to create pastures. The Great Plains have dust storms mostly every year or so.
The 100th meridian roughly corresponds with the line that divides the Great Plains into an area that receive 20 in or more of rainfall per year and an area that receives less than 20 in. In this context, the High Plains, as well as Southern Alberta, south-western Saskatchewan and Eastern Montana are mainly semi hot steppe land and are generally characterised by rangeland or marginal farmland. The region (especially the High Plains) is periodically subjected to extended periods of drought; high winds in the region may then generate devastating dust storms. The eastern Great Plains near the eastern boundary falls in the humid subtropical climate zone in the southern areas, and the northern and central areas fall in the humid continental climate.
Many thunderstorms occur in the plains in the spring through summer. The southeastern portion of the Great Plains is the most tornado active area in the world and is sometimes referred to as Tornado Alley.
Flora.
The Great Plains are part of the floristic North American Prairies Province, which extends from the Rocky Mountains to the Appalachians.
History.
Original American contact.
The first Americans (Paleo-Indians) who arrived to the Great Plains were successive indigenous cultures who are known to have inhabited the Great Plains for thousands of years, over 15,000 years ago. Humans entered the North American continent in waves of migration, mostly over Beringia, the Bering Straits land bridge.
Historically the Great Plains were the range of the bison and of the culture of the Plains Indians, whose tribes included the Blackfoot, Crow, Sioux, Cheyenne, Arapaho, Comanche, and others. Eastern portions of the Great Plains were inhabited by tribes who lived in semipermanent villages of earth lodges, such as the Arikara, Mandan, Pawnee and Wichita.
European contact.
With the arrival of Francisco Vázquez de Coronado, a Spanish conquistador, the first recorded history of encounter between Europeans and Native Americans in the Great Plains occurred in Texas, Kansas and Nebraska from 1540-1542. In that same time period, Hernando de Soto crossed a west-northwest direction in what is now Oklahoma and Texas. Today this is known as the De Soto Trail. The Spanish thought the Great Plains were the location of the mythological "Quivira and Cíbola", a place said to be rich in gold.
Over the next one hundred years, founding of the fur trade brought thousands of ethnic Europeans into the Great Plains. Fur trappers from France, Spain, Britain, Russia and the young United States made their way across much of the region, making regular contacts with Native Americans. After the United States acquired the Louisiana Purchase in 1803 and conducted the Lewis and Clark Expedition in 1804-1806, more information about the Plains became available and various pioneers entered the areas.
Manuel Lisa, based in St. Louis, established a major fur trading site at his Fort Lisa on the Missouri River in Nebraska. Fur trading posts were often the basis of later settlements. Through the 19th century, more European Americans and Europeans migrated to the Great Plains as part of a vast westward expansion of population. New settlements became dotted across the Great Plains.
The new immigrants also brought diseases against which the Native Americans had no resistance. Between a half and two-thirds of the Plains Indians are thought to have died of smallpox by the time of the 1803 Louisiana Purchase.
Pioneer settlement.
After 1870, the new railroads across the Plains brought hunters who killed off almost all the bison for their hides. The railroads offered attractive packages of land and transportation to European farmers, who rushed to settle the land. They (and Americans as well) also took advantage of the homestead laws to obtain free farms. Land speculators and local boosters identified many potential towns, and those reached by the railroad had a chance, while the others became ghost towns. In Kansas, for example, nearly 5000 towns were mapped out, but by 1970 only 617 were actually operating. In the mid-20th century, closeness to an interstate exchange determined whether town would flourish or struggle for business.
Much of the Great Plains became open range, or rangeland where cattle roamed free, hosting ranching operations where anyone was theoretically free to run cattle. In the spring and fall, ranchers held roundups where their cowboys branded new calves, treated animals and sorted the cattle for sale. Such ranching began in Texas and gradually moved northward. In 1866-95, cowboys herded 10 million cattle north to rail heads such as Dodge City, Kansas and Ogallala, Nebraska; from there, cattle were shipped eastward. 
Many foreign investors, especially British, financed the great ranches of the era. Overstocking of the range and the terrible winter of 1886 resulted in a disaster, with many cattle starved and frozen to death. Theodore Roosevelt, a rancher in the Dakotas, lost his entire investment; he returned east to reenter politics. From then on, ranchers generally raised feed to ensure they could keep their cattle alive over winter.
To allow for agricultural development of the Great Plains and house a growing population, the US passed the Homestead Acts of 1862: it allowed a settler to claim up to 160 acre of land, provided that he lived on it for a period of five years and cultivated it. The provisions were expanded under the Kinkaid Act of 1904 to include a homestead of an entire section. Hundreds of thousands of people claimed such homesteads, sometimes building sod houses out of the very turf of their land. Many of them were not skilled dryland farmers and failures were frequent. Much of the Plains were settled during relatively wet years. Government experts did not understand how farmers should cultivate the prairies and gave advice counter to what would have worked. Germans from Russia who had previously farmed, under similar circumstances, in what is now the Ukraine were marginally more successful than other homesteaders. The Dominion Lands Act of 1871 served a similar function for establishing homesteads on the prairies in Canada.
Social life.
The railroads opened up the Great Plains for settlement, for now it was possible to ship wheat and other crops at low cost to the urban markets in the East, and Europe. Homestead land was free for American settlers. Railroads sold their land at cheap rates to immigrants in expectation they would generate traffic as soon as farms were established. Immigrants poured in, especially from Germany and Scandinavia. On the plains, very few single men attempted to operate a farm or ranch by themselves; they clearly understood the need for a hard-working wife, and numerous children, to handle the many chores, including child-rearing, feeding and clothing the family, managing the housework, feeding the hired hands, and, especially after the 1930s, handling paperwork and financial details. During the early years of settlement, farm women played an integral role in assuring family survival by working outdoors. After approximately one generation, women increasingly left the fields, thus redefining their roles within the family. New technology including sewing and washing machines encouraged women to turn to domestic roles. The scientific housekeeping movement, promoted across the land by the media and government extension agents, as well as county fairs which featured achievements in home cookery and canning, advice columns for women regarding farm bookkeeping, and home economics courses in the schools.
Although the eastern image of farm life in the prairies emphasized the isolation of the lonely farmer and wife, plains residents created busy social lives for themselves. They often sponsored activities that combined work, food and entertainment such as barn raisings, corn huskings, quilting bees, Grange meetings, church activities and school functions. Women organized shared meals and potluck events, as well as extended visits between families. The Grange was a nationwide farmers' organization, the reserved high offices for women, and gave them a voice in public affairs.
After 19th century.
The region roughly centered on the Oklahoma Panhandle, including southeastern Colorado, southwestern Kansas, the Texas Panhandle, and extreme northeastern New Mexico was known as the Dust Bowl during the late 1920s and early 1930s. The effect of an extended drought, inappropriate cultivation, and financial crises of the Great Depression, forced many farmers off the land throughout the Great Plains.
From the 1950s on, many areas of the Great Plains have become productive crop-growing areas because of extensive irrigation on large landholdings. The United States is a major exporter of agricultural products. The southern portion of the Great Plains lies over the Ogallala Aquifer, a huge underground layer of water-bearing strata dating from the last ice age. Center pivot irrigation is used extensively in drier sections of the Great Plains, resulting in aquifer depletion at a rate that is greater than the ground's ability to recharge.
The rural Plains have lost a third of their population since 1920. Several hundred thousand square miles (several hundred thousand square kilometers) of the Great Plains have fewer than 6 PD/sqmi—the density standard Frederick Jackson Turner used to declare the American frontier "closed" in 1893. Many have fewer than 2 PD/sqmi. There are more than 6,000 ghost towns in the state of Kansas alone, according to Kansas historian Daniel Fitzgerald. This problem is often exacerbated by the consolidation of farms and the difficulty of attracting modern industry to the region. In addition, the smaller school-age population has forced the consolidation of school districts and the closure of high schools in some communities. The continuing population loss has led some to suggest that the current use of the drier parts of the Great Plains is not sustainable. (See Depopulation of the Great Plains and Buffalo Commons)
Wind power.
The Great Plains contribute substantially to wind power in the United States. In July 2008, oilman turned wind-farm developer T. Boone Pickens called for the U.S. to invest $1 trillion to build an additional 200,000 MW of wind power nameplate capacity in the Plains, as part of his Pickens Plan. Pickens cited Sweetwater, Texas as an example of economic revitalization driven by wind power development.
Sweetwater was a struggling town typical of the Plains, steadily losing businesses and population, until wind turbines came to the surrounding Nolan County. Wind power brought jobs to local residents, along with royalty payments to landowners who leased sites for turbines, reversing the town's population decline. Pickens claims the same economic benefits are possible throughout the Plains, which he refers to as North America's "wind corridor."
See also.
International steppe-lands:

</doc>
<doc id="51465" url="http://en.wikipedia.org/wiki?curid=51465" title="Molière">
Molière

Jean-Baptiste Poquelin, known by his stage name Molière (; ]; 1622–1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best-known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman".
Born into a prosperous family and having studied at the Collège de Clermont (now Lycée Louis-le-Grand), Molière was well suited to begin a life in the theatre. Thirteen years as an itinerant actor helped him polish his comic abilities while he began writing, combining Commedia dell'arte elements with the more refined French comedy.
Through the patronage of aristocrats including Philippe I, Duke of Orléans—the brother of Louis XIV—Molière procured a command performance before the King at the Louvre. Performing a classic play by Pierre Corneille and a farce of his own, "The Doctor in Love", Molière was granted the use of salle du Petit-Bourbon near the Louvre, a spacious room appointed for theatrical performances. Later, Molière was granted the use of the theatre in the Palais-Royal. In both locations he found success among Parisians with plays such as "The Affected Ladies", "The School for Husbands" and "The School for Wives". This royal favor brought a royal pension to his troupe and the title "Troupe du Roi" ("The King's Troupe"). Molière continued as the official author of court entertainments.
Though he received the adulation of the court and Parisians, Molière's satires attracted criticism from moralists and the Catholic Church. "Tartuffe" and its attack on perceived religious hypocrisy roundly received condemnations from the Church, while "Don Juan" was banned from performance. Molière's hard work in so many theatrical capacities took its toll on his health and, by 1667, he was forced to take a break from the stage. In 1673, during a production of his final play, "The Imaginary Invalid", Molière, who suffered from pulmonary tuberculosis, was seized by a coughing fit and a haemorrhage while playing the hypochondriac Argan. He finished the performance but collapsed again and died a few hours later.
Life.
Molière was born in Paris, the son of Jean Poquelin and Marie Cressé, the daughter of a prosperous bourgeois family. He lost his mother when he was ten and he did not seem to have been particularly close to his father. After his mother's death, he lived with his father above the "Pavillon des Singes" on the rue Saint-Honoré, an affluent area of Paris. It is likely that his education commenced with studies in a Parisian elementary school; this was followed with his enrollment in the prestigious Jesuit Collège de Clermont, where he completed his studies in a strict academic environment.
In 1631, Jean Poquelin purchased from the court of Louis XIII the posts of "valet de chambre ordinaire et tapissier du Roi" ("valet of the King's chamber and keeper of carpets and upholstery"). His son assumed the same posts in 1641. The title required only three months' work and an initial cost of 1,200 livres; the title paid 300 livres a year and provided a number of lucrative contracts. Poquelin also studied as a provincial lawyer some time around 1642, probably in Orléans, but it is not documented that he ever qualified. So far he had followed his father's plans, which had served him well; he had mingled with nobility at the Collège de Clermont and seemed destined for a career in office.
In June 1643, when Molière was 21, he decided to abandon his social class and pursue a career on the stage. Taking leave of his father, he joined the actress Madeleine Béjart, with whom he had crossed paths before, and founded the Illustre Théâtre with 630 livres. They were later joined by Madeleine's brother and sister.
The new theatre troupe went bankrupt in 1645. Molière had become head of the troupe, due in part, perhaps, to his acting prowess and his legal training. However, the troupe had acquired large debts, mostly for the rent of the theatre (a court for jeu de paume), for which they owed 2000 livres. Historians differ as to whether his father or the lover of a member of his troupe paid his debts; either way, after a 24-hour stint in prison he returned to the acting circuit. It was at this time that he began to use the pseudonym "Molière", possibly inspired by a small village of the same name in the Midi near Le Vigan. It was also likely that he changed his name to spare his father the shame of having an actor in the family (actors, although no longer vilified by the state under Louis XIV, were still not allowed to be buried in sacred ground).
After his imprisonment, he and Madeleine began a theatrical circuit of the provinces with a new theatre troupe; this life was to last about twelve years, during which he initially played in the company of Charles Dufresne, and subsequently created a company of his own, which had sufficient success and obtained the patronage of Philippe I, Duke of Orléans. Few plays survive from this period. The most noteworthy are "L'Étourdi, ou le Contretemps" "(The Bungler)" and "Le Docteur Amoureux" "(The Doctor in Love)"; with these two plays, Molière moved away from the heavy influence of the Italian improvisational Commedia dell'arte, and displayed his talent for mockery. In the course of his travels he met Armand, Prince of Conti, the governor of Languedoc, who became his patron, and named his company after him. This friendship later ended when Conti, having contracted syphilis from a courtesan, turned towards religion and joined Molière's enemies in the "Parti des Dévots" and the "Compagnie de Saint Sacrement".
In Lyon, Mademoiselle Du Parc, known as "Marquise", joined the company. Marquise was courted, in vain, by Pierre Corneille and later became the lover of Jean Racine. Racine offered Molière his tragedy "Théagène et Chariclée" (one of the first works he wrote after he had abandoned his theology studies), but Molière would not perform it, though he encouraged Racine to pursue his artistic career. It is said that soon thereafter Molière became angry with Racine when he was told that he had secretly presented his tragedy to the company of the Hôtel de Bourgogne as well.
Return to Paris.
Molière was forced to reach Paris in stages, staying outside for a few weeks in order to promote himself with society gentlemen and allow his reputation to feed in to Paris. Molière reached Paris in 1658 and performed in front of the King at the Louvre (then for rent as a theatre) in Corneille's tragedy "Nicomède" and in the farce "Le Docteur Amoureux" with some success. He was awarded the title of "Troupe de Monsieur" (Monsieur being the honorific for the king's brother Philippe I, Duke of Orléans). With the help of Monsieur, his company was allowed to share the theatre in the large hall of the Petit-Bourbon with the famous Italian Commedia dell'arte company of Tiberio Fiorillo, famous for his character of Scaramouche. (The two companies performed in the theatre on different nights.) The premiere of Molière's "Les Précieuses Ridicules" ("The Affected Young Ladies") took place at the Petit-Bourbon on November 18, 1659.
"Les Précieuses Ridicules" was the first of Molière's many attempts to satirize certain societal mannerisms and affectations then common in France. It is widely accepted that the plot was based on Samuel Chappuzeau's "Le Cercle des Femmes" of 1656. He primarily mocks the Académie Française, a group created by Richelieu under a royal patent to establish the rules of the fledgling French theater. The Académie preached unity of time, action, and styles of verse. Molière is often associated with the claim that comedy "castigat ridendo mores" or "criticises customs through humour" (a phrase in fact coined by his contemporary Jean de Santeuil and sometimes mistaken for a classical Latin proverb).
Height of fame.
Despite his own preference for tragedy, which he had tried to further with the Illustre Théâtre, Molière became famous for his farces, which were generally in one act and performed after the tragedy. Some of these farces were only partly written, and were played in the style of Commedia dell'arte with improvisation over a canovaccio (a vague plot outline). He also wrote two comedies in verse, but these were less successful and are generally considered less significant. Later in life Molière concentrated on writing musical comedies, in which the drama is interrupted by songs and/or dances.
"Les Précieuses Ridicules" won Molière the attention and the criticism of many, but it was not a popular success. He then asked Fiorillo to teach him the techniques of Commedia dell'arte. His 1660 play "Sganarelle, ou Le Cocu imaginaire" ("The Imaginary Cuckold") seems to be a tribute both to Commedia dell'arte and to his teacher. Its theme of marital relationships dramatizes Molière's pessimistic views on the falsity inherent in human relationships. This view is also evident in his later works, and was a source of inspiration for many later authors, including (in a different field and with different effect) Luigi Pirandello. It describes a kind of round dance where two couples believe that each of their partners has been betrayed by the other's and is the first in Molière's 'Jealousy series' which includes "Dom Garcie de Navarre", "L'École des maris" and "L'École des femmes".
In 1660 the Petit-Bourbon was demolished to make way for the eastern expansion of the Louvre, but Molière's company was allowed to move into the abandoned theatre in the east wing of the Palais-Royal. After a period of refurbishment they opened there on January 20, 1661. In order to please his patron, Monsieur, who was so enthralled with entertainment and art that he was soon excluded from state affairs, Molière wrote and played "Dom Garcie de Navarre ou Le Prince jaloux" ("The Jealous Prince", 4 February 1661), a heroic comedy derived from a work of Cicognini's. Two other comedies of the same year were the successful "L'École des maris" ("The School for Husbands") and "Les Fâcheux", subtitled "Comédie faite pour les divertissements du Roi" (a comedy for the King's amusements) because it was performed during a series of parties that Nicolas Fouquet gave in honor of the sovereign. These entertainments led Jean-Baptiste Colbert to demand the arrest of Fouquet for wasting public money, and he was condemned to life imprisonment.
On February 20, 1662, Molière married Armande Béjart, whom he believed to be the sister of Madeleine. (She may instead have been her illegitimate daughter with the Duke of Modena.) The same year he premiered "L'École des femmes" ("The School for Wives"), subsequently regarded as a masterpiece. It poked fun at the limited education that was given to daughters of rich families, and reflected Molière's own marriage. Both this work and his marriage attracted much criticism. The play sparked the protest called the "Quarrel of L'École des femmes". On the artistic side he responded with two lesser-known works: "La Critique de "L'École des femmes"", in which he imagined the spectators of his previous work attending it. The piece mocks the people who had criticised "L'École des femmes" by showing them at dinner after watching the play; it addresses all the criticism raised about the piece by presenting the critics' arguments and then dismissing them. This was the so-called "Guerre comique" ("War of Comedy"), in which the opposite side was taken by writers like Donneau de Visé, Edmé Boursault, and Montfleury.
But more serious opposition was brewing, focusing on Molière's politics and his personal life. A so-called "parti des Dévots" arose in French high society, who protested against Molière's excessive "realism" and irreverence, which were causing some embarrassment. These people accused Molière of having married his daughter. The Prince of Conti, once Molière's friend, joined them. Molière had other enemies, too, among them the Jansenists and some traditional authors. However, the king expressed his solidarity with the author, granting him a pension and agreeing to be the godfather of Molière's first son. Boileau also supported him through statements that he included in his "Art poétique".
Molière's friendship with Jean-Baptiste Lully influenced him towards writing his "Le Mariage forcé" and "La Princesse d'Élide" (subtitled as "Comédie galante mêlée de musique et d'entrées de ballet"), written for royal "divertissements" at the Palace of Versailles.
"Tartuffe, ou L'Imposteur" was also performed at Versailles, in 1664, and created the greatest scandal of Molière's artistic career. Its depiction of the hypocrisy of the dominant classes was taken as an outrage and violently contested. It also aroused the wrath of the Jansenists and the play was banned.
Molière was always careful not to attack the institution of monarchy. He earned a position as one of the king's favorites and enjoyed his protection from the attacks of the court. The king allegedly suggested that Molière suspend performances of "Tartuffe", and the author rapidly wrote "Dom Juan ou le Festin de Pierre" to replace it. It was a strange work, derived from a work by Tirso de Molina and rendered in a prose that still seems modern today. It describes the story of an atheist who becomes a religious hypocrite and for this is punished by God. This work too was quickly suspended. The king, demonstrating his protection once again, became the new official sponsor of Molière's troupe.
With music by Lully, Molière presented "L'Amour médecin" ("Love Doctor" or "Medical Love"). Subtitles on this occasion reported that the work was given "par ordre du Roi" (by order of the king), and this work was received much more warmly than its predecessors.
In 1666, "Le Misanthrope" was produced. It is now widely regarded as Molière's most refined masterpiece, the one with the highest moral content, but it was little appreciated at its time. It caused the "conversion" of Donneau de Visé, who became fond of his theatre. But it was a commercial flop, forcing Molière to immediately write "Le médecin malgré lui" ("The Doctor Despite Himself"), a satire against the official sciences. This was a success despite a moral treatise by the Prince of Conti, criticizing the theater in general and Molière's in particular. In several of his plays, Molière depicted the physicians of his day as pompous individuals who speak (poor) Latin to impress others with false erudition, and know only clysters and bleedings as (ineffective) remedies.
After the "Mélicerte" and the "Pastorale comique", he tried again to perform a revised "Tartuffe" in 1667, this time with the name of "Panulphe" or "L'Imposteur". As soon as the King left Paris for a tour, Lamoignon and the archbishop banned the play. The King finally imposed respect for "Tartuffe" a few years later, after he had gained more power over the clergy.
Molière, now ill, wrote less. "Le Sicilien ou L'Amour peintre" was written for festivities at the castle of Saint-Germain-en-Laye, and was followed in 1668 by "Amphitryon", inspired both by Plautus' work of the same name and Jean Rotrou's successful reconfiguration of the drama. With some conjecture, Molière's play can be seen to allude to the love affairs of Louis XIV, then king of France. "George Dandin, ou Le mari confondu" ("The Confounded Husband") was little appreciated, but success returned with "L'Avare" ("The Miser"), now very well known.
With Lully he again used music for "Monsieur de Pourceaugnac", for "Les Amants magnifiques", and finally for "Le Bourgeois gentilhomme" ("The Middle Class Gentleman"), another of his masterpieces. It is claimed to be particularly directed against Colbert, the minister who had condemned his old patron Fouquet. The collaboration with Lully ended with a "tragédie et ballet", "Psyché", written in collaboration with Pierre Corneille and Philippe Quinault.
In 1672, Madeleine Béjart died, and Molière suffered from this loss and from the worsening of his own illness. Nevertheless, he wrote a successful "Les Fourberies de Scapin" ("Scapin's Deceits"), a farce and a comedy in five acts. His following play, "La Comtesse d'Escarbagnas", is considered one of his lesser works.
"Les Femmes savantes" ("The Learned Ladies") of 1672 is considered another of Molière's masterpieces. It was born from the termination of the legal use of music in theater, since Lully had patented the opera in France (and taken most of the best available singers for his own performances), so Molière had to go back to his traditional genre. It was a great success, and it led to his last work (see below), which is still held in high esteem.
In his 14 years in Paris, Molière single-handedly wrote 31 of the 85 plays performed on his stage while simultaneously holding his company together.
Les Comédies-Ballets.
In 1661, Molière introduced the "comédies-ballets" in conjunction with "Les Fâcheux". These ballets were a transitional form of dance performance between the court ballets of Louis XIV and the art of professional theatre which was developing in the advent of the use of the proscenium stage. The "comédies-ballets" developed accidentally when Molière was enlisted to mount both a play and a ballet in the honor of Louis XIV and found that he did not have a big enough cast to meet these demands. Molière therefore decided to combine the ballet and the play so that the his goal could be met while the performers catch their breath and change costume. The risky move paid off and Molière was asked to produce twelve more "comédies-ballets" before his death. During the "comédies-ballets", Molière collaborated with Pierre Beauchamp. Beauchamp codified the five balletic positions of the feet and arms and was partly responsible for the creation of the Beauchamp-Feuillet dance notation. Molière also collaborated with Jean-Baptiste Lully. Lully was a dancer, choreographer, and composer, whose dominant reign at the Paris Opéra lasted fifteen years. Under his command, ballet and opera rightly became professional arts unto themselves. The "comédies-ballets" closely integrated dance with music and the action of the play and the style of continuity distinctly separated these performances from the court ballets of the time; additionally, the "comédies-ballets" demanded that both the dancers and the actors play an important role in advancing the story. Similar to the court ballets, both professionally trained dancers and courtiers socialized together at the "comédies-ballets" - Louis XIV even played the part of an Egyptian in Molière's "Le Mariage forcé" (1664) and also appeared as Neptune and Apollo in his retirement performance of "Les Amants magnifiques" (1670).
Death.
Molière suffered from pulmonary tuberculosis, possibly contracted when he was imprisoned for debt as a young man. One of the most famous moments in Molière's life was his last, which became legend: he collapsed on stage in a fit of coughing and haemorrhaging while performing in the last play he'd written, which had lavish ballets performed to the music of Marc-Antoine Charpentier and which ironically was entitled "Le Malade imaginaire" ("The Imaginary Invalid"). Molière insisted on completing his performance. Afterwards he collapsed again with another, larger haemorrhage before being taken home, where he died a few hours later, without receiving the last rites because two priests refused to visit him while a third arrived too late. The superstition that green brings bad luck to actors is said to originate from the colour of the clothing he was wearing at the time of his death.
Under French law at the time, actors were not allowed to be buried in the sacred ground of a cemetery. However, Molière's widow, Armande, asked the King if her spouse could be granted a "normal" funeral at night. The King agreed and Molière's body was buried in the part of the cemetery reserved for unbaptised infants.
In 1792 his remains were brought to the museum of French monuments and in 1817 transferred to Père Lachaise Cemetery in Paris, close to those of La Fontaine.
Reception of his works.
Though conventional thinkers, religious leaders, and medical professionals in Molière's time criticized his work, their ideas did not really impact his widespread success with the public. Other playwrights and companies began to emulate his dramatic style in England and in France. Molière's works continued to garner positive feedback in 18th century England, but they were not so warmly welcomed in France at this time. However, during the French Restoration of the 19th century, Molière's comedies became popular with both the French public and the critics. Romanticists admired his plays for the unconventional individualism they portrayed. 20th century scholars have carried on this interest in Molière and his plays and have continued to study a wide array of issues relating to this playwright. Many critics now are shifting their attention from the philosophical, religious, and moral implications in his comedies to the more objective study of his comic technique.
Molière's works were translated into English prose by John Ozell in 1714, but the first complete version in English, by Baker and Miller in 1739, remained "influential" and was long reprinted. The first to offer full translations of Molière's verse plays such as "Tartuffe" into English verse was Curtis Hidden Page, who produced blank verse versions of three of the plays in his 1908 translation. Since then, notable translations have been made by Richard Wilbur, Donald M. Frame, and many others.
In his memoir "A Terrible Liar", actor Hume Cronyn writes that, in 1962, celebrated actor Laurence Olivier criticized Molière in a conversation with him. According to Cronyn, he mentioned to Olivier that he (Cronyn) was about to play the title role in "The Miser", and that Olivier then responded, "Molière? Funny as a baby's open grave." Cronyn comments on the incident: "You may imagine how that made me feel. Fortunately, he was dead wrong."
Author Martha Bellinger points out that:
 [Molière] has been accused of not having a consistent, organic style, of using faulty grammar, of mixing his metaphors, and of using unnecessary words for the purpose of filling out his lines. All these things are occasionally true, but they are trifles in comparison to the wealth of character he portrayed, to his brilliancy of wit, and to the resourcefulness of his technique. He was wary of sensibility or pathos; but in place of pathos he had "melancholy—a puissant and searching melancholy, which strangely sustains his inexhaustible mirth and his triumphant gaiety".
Influence on French culture.
Molière is considered the creator of modern French comedy. Many words or phrases used in Molière's plays are still used in current French:
Portrayals of Molière.
Russian writer Mikhail Bulgakov wrote a semi-fictitious biography-tribute to Molière, titled "Life of Mr. de Molière". Written 1932-1933, first published 1962.
In the 2000 film "Le Roi Danse" ("The King Dances"), Molière is played by Tchéky Karyo, and shows his collaborations with Jean-Baptiste Lully, as well as his illness and on-stage death.
The French 1978 film titled "Molière" directed by Ariane Mnouchkine, presents his complete biography. It was in competition for the Palme d'Or at Cannes in 1978.
The 2007 French film "Molière" was more loosely based on the life of Molière, starring Romain Duris, Fabrice Luchini and Ludivine Sagnier.

</doc>
<doc id="51469" url="http://en.wikipedia.org/wiki?curid=51469" title="Mold">
Mold

 
A mold () or mould () is a fungus that grows in the form of multicellular filaments called "hyphae". In contrast, fungi that can adopt a single celled growth habit are called yeasts.
Molds are a large and taxonomically diverse number of fungal species where the growth of hyphae results in discoloration and a fuzzy appearance, especially on food. The network of these tubular branching hyphae, called a mycelium, is considered a single organism. The hyphae are generally transparent, so the mycelium appears like very fine, fluffy white threads over the surface. Cross-walls (septa) may delimit connected compartments along the hyphae, each containing one or multiple, genetically identical nuclei. The dusty texture of many molds is caused by profuse production of asexual spores (conidia) formed by differentiation at the ends of hyphae. The mode of formation and shape of these spores is traditionally used to classify molds. Many of these spores are colored, making the fungus much more obvious to the human eye at this stage in its life-cycle.
Molds are considered to be microbes and do not form a specific taxonomic or phylogenetic grouping, but can be found in the divisions Zygomycota and Ascomycota. In the past, most molds were classified within the Deuteromycota.
Molds cause biodegradation of natural materials, which can be unwanted when it becomes food spoilage or damage to property. They also play important roles in biotechnology and food science in the production of various foods, beverages, antibiotics, pharmaceuticals and enzymes. Some diseases of animals and humans can be caused by certain molds: disease may result from allergic sensitivity to mold spores, from growth of pathogenic molds within the body, or from the effects of ingested or inhaled toxic compounds (mycotoxins) produced by molds.
Biology.
There are thousands of known species of molds, which have diverse life-styles including saprotrophs, mesophiles, psychrophiles and thermophiles and a very few opportunistic pathogens of humans. They all require moisture for growth and some live in aquatic environments. Like all fungi, molds derive energy not through photosynthesis but from the organic matter on which they live, utilising heterotrophy. Typically, molds secrete hydrolytic enzymes, mainly from the hyphal tips. These enzymes degrade complex biopolymers such as starch, cellulose and lignin into simpler substances which can be absorbed by the hyphae. In this way molds play a major role in causing decomposition of organic material, enabling the recycling of nutrients throughout ecosystems. Many molds also synthesise mycotoxins and siderophores which, together with lytic enzymes, inhibit the growth of competing microorganisms. Molds can also grow on stored food for animals and humans, making the food unpalatable or toxic and are thus a major source of food losses and illness. Many strategies for food preservation (salting, pickling, jams, bottling, freezing, drying) are to prevent or slow mold growth as well as growth of other microbes.
Molds reproduce by producing large numbers of small spores, which may contain a single nucleus or be multinucleate. Mold spores can be asexual (the products of mitosis) or sexual (the products of meiosis); many species can produce both types. Some molds produce small, hydrophobic spores that are adapted for wind dispersal and may remain airborne for long periods; in some the cell walls are darkly pigmented, providing resistance to damage by ultraviolet radiation. Other mold spores have slimy sheaths and are more suited to water dispersal. Mold spores are often spherical or ovoid single cells, but can be multicellular and variously shaped. Spores may cling to clothing or fur; some are able to survive extremes of temperature and pressure.
Although molds can grow on dead organic matter everywhere in nature, their presence is visible to the unaided eye only when they form large colonies. A mold colony does not consist of discrete organisms but is an interconnected network of hyphae called a mycelium. All growth occurs at hyphal tips, with cytoplasm and organelles flowing forwards as the hyphae advance over or through new food sources. Nutrients are absorbed at the hyphal tip. In artificial environments such as buildings, humidity and temperature are often stable enough to foster the growth of mold colonies, commonly seen as a downy or furry coating growing on food or other surfaces.
Few molds can begin growing at temperatures of 4 C or below, so food is typically refrigerated at this temperature. When conditions do not enable growth to take place, molds may remain alive in a dormant state depending on the species, within a large range of temperatures. The many different mold species vary enormously in their tolerance to temperature and humidity extremes. Certain molds can survive harsh conditions such as the snow-covered soils of Antarctica, refrigeration, highly acidic solvents, anti-bacterial soap and even petroleum products such as jet fuel.:22
Xerophilic molds are able to grow in relatively dry, salty, or sugary environments, where water activity (aw) is less than 0.85; other molds need more moisture.
Common molds.
Common genera of molds include:
Food production.
The Kōji (麹) molds are a group of "Aspergillus" species, notably "Aspergillus oryzae", and secondarily "A. sojae", that have been cultured in eastern Asia for many centuries. They are used to ferment a soybean and wheat mixture to make soybean paste and soy sauce. "Koji" molds break down the starch in rice, barley, sweet potatoes, etc., a process called saccharification, in the production of "sake", "shōchū" and other distilled spirits. "Koji" molds are also used in the preparation of Katsuobushi.
Red rice yeast is a product of the mold "Monascus purpureus" grown on rice, and is common in Asian diets. The yeast contains several compounds collectively known as monacolins, which are known to inhibit cholesterol synthesis. A study has shown that red rice yeast used as a dietary supplement, combined with fish oil and healthy lifestyle changes, may help reduce "bad" cholesterol as effectively as certain commercial statin drugs.
Some sausages, such as salami, incorporate starter cultures of molds to improve flavour and reduce bacterial spoilage during curing. "Penicillium nalgiovense", for example, may appear as a powdery white coating on some varieties of dry-cured sausage.
Other molds that have been used in food production include:
Pharmaceuticals from molds.
Alexander Fleming's accidental discovery of the antibiotic penicillin involved a "Penicillium" mold called "Penicillium notatum" (although the species identity is disputed as possibly being "Penicillium chrysogenum" or "Penicillium rubens"). Fleming continued to investigate Penicillin, showing that it could inhibit various types of bacteria found in infections and other ailments, but he was unable to produce the compound in large enough amounts necessary for production of a medicine. His work was expanded by a team at Oxford University; Clutterbuck, Lovell, and Raistrick, who began to work on the problem in 1931. This team was also unable to produce the pure compound in any large amount, and found that the purification process diminished its effectiveness and negated the anti-bacterial properties it had.
Howard Florey, Ernst Chain, Norman Heatley, Edward Abraham, also all at Oxford, continued the work. They enhanced and developed the concentration technique by using organic solutions rather than water, and created the "Oxford Unit" to measure penicillin concentration within a solution. They managed to purify the solution, increasing its concentration by 45-50 times, but found that a higher concentration was possible. Experiments were conducted and the results published in 1941, though the quantities of Penicillin produced were not always high enough for the treatments required. As this was during the Second World War, Florey sought USA Government involvement. With research teams in the UK and some in the US, industrial-scale production of crystallised penicillin was developed during 1941-1944 by the USDA and by Pfizer.
Several statin cholesterol-lowering drugs (such as lovastatin, from "Aspergillus terreus") are derived from molds.
The immunosuppressant drug cyclosporine, used to suppress the rejection of transplanted organs, is derived from the mold "Tolypocladium inflatum".
Health effects.
Molds are ubiquitous, and mold spores are a common component of household and workplace dust; however, when mold spores are present in large quantities, they can present a health hazard to humans, potentially causing allergic reactions and respiratory problems.
Some molds also produce mycotoxins that can pose serious health risks to humans and animals. Some studies claim that exposure to high levels of mycotoxins can lead to neurological problems and in some cases death. Prolonged exposure, e.g. daily home exposure, may be particularly harmful. Research on the health impacts of mold has not been conclusive. The term "toxic mold" refers to molds that produce mycotoxins, such as "Stachybotrys chartarum", and not to all molds in general.
Mold in the home can usually be found in damp, dark or steamy areas e.g. bathroom or kitchen, cluttered storage areas, recently flooded areas, basement areas, plumbing spaces, areas with poor ventilation and outdoors in humid environments. Symptoms caused by mold allergy are watery, itchy eyes, a chronic cough, headaches or migraines, difficulty breathing, rashes, tiredness, sinus problems, nasal blockage and frequent sneezing.
Molds can also pose a hazard to human and animal health when they are consumed following the growth of certain mold species in stored food. Some species produce toxic secondary metabolites, collectively termed mycotoxins including aflatoxins, ochratoxins, fumonisins, trichothecenes, citrinin, and patulin. These toxic properties may be used for the benefit of humans when the toxicity is directed against other organisms; for example, penicillin adversely affects the growth of Gram-positive bacteria (e.g. Clostridium species), certain spirochetes and certain fungi.
Growth in buildings and homes.
Mold growth in buildings can lead to a variety of health problems. Various practices can be followed to mitigate mold issues in buildings, the most important of which is to reduce moisture levels that can facilitate mold growth. Removal of affected materials after the source of moisture has been reduced and/or eliminated may be necessary for remediation.
The use of mold in art.
Various artists have used mold in various artistic fashions. Daniele Del Nero, for example, constructs scale models of houses and office buildings and then induces mold to grow on them, giving them a spooky, reclaimed-by-nature look. Staci Levy sandblasts enlarged images of mold onto glass, then allows mold to grow in the crevasses she has made, creating a macro-micro portrait.

</doc>
<doc id="51470" url="http://en.wikipedia.org/wiki?curid=51470" title="Mycelium">
Mycelium

Mycelium is the vegetative part of a fungus, consisting of a mass of branching, thread-like hyphae. The mass of hyphae is sometimes called shiro, especially within the fairy ring fungi. Fungal colonies composed of mycelium are found in and on soil and many other substrates. A typical single spore germinates into a homokaryotic mycelium, which cannot reproduce sexually; when two compatible homokaryotic mycelia join and form a dikaryotic mycelium; that mycelium may form fruiting bodies such as mushrooms. A mycelium may be minute, forming a colony that is too small to see, or it may be extensive:
Is this the largest organism in the world? This 2400 acre site in eastern Oregon had a contiguous growth of mycelium before logging roads cut through it. Estimated at 1,665 football fields in size and 2,200 years old, this one fungus has killed the forest above it several times over, and in so doing has built deeper soil layers that allow the growth of ever-larger stands of trees. Mushroom-forming forest fungi are unique in that their mycelial mats can achieve such massive proportions.—Paul Stamets, "Mycelium Running"
Through the mycelium, a fungus absorbs nutrients from its environment. It does this in a two-stage process. First, the hyphae secrete enzymes onto or into the food source, which break down biological polymers into smaller units such as monomers. These monomers are then absorbed into the mycelium by facilitated diffusion and active transport.
Mycelium is vital in terrestrial and aquatic ecosystems for their role in the decomposition of plant material. They contribute to the organic fraction of soil, and their growth releases carbon dioxide back into the atmosphere. Ectomycorrhizal extramatrical mycelium, as well as the mycelium of Arbuscular mycorrhizal fungi increase the efficiency of water and nutrient absorption of most plants and confers resistance to some plant pathogens. Mycelium is an important food source for many soil invertebrates.
"Mycelium", like "fungus", can be considered a mass noun, a word that can be either singular or plural. The term "mycelia", though, like "fungi", is often used as the preferred plural form.
Sclerotia are compact or hard masses of mycelium.
Uses.
One of the primary roles of fungi in an ecosystem is to decompose organic compounds. Petroleum products and some pesticides (typical soil contaminants) are organic molecules (i.e. they are built on a carbon structure), and thereby present a potential carbon source for fungi. Hence, fungi have the potential to eradicate such pollutants from their environment; unless the chemicals prove toxic to the fungus. This biological degradation is a process known as bioremediation.
Mycelial mats have been suggested (see Paul Stamets) as having potential as biological filters, removing chemicals and microorganisms from soil and water. The use of fungal mycelium to accomplish this has been termed "mycofiltration".
Knowledge of the relationship between mycorrhizal fungi and plants suggests new ways to improve crop yields.
When spread on logging roads, mycelium can act as a binder, holding new soil in place and preventing washouts until woody plants can be established.
Since 2007, a company called Ecovative Design has been developing alternatives to polystyrene and plastic packaging by growing mycelium in agricultural waste. The two ingredients are mixed together and placed into a mold for 3–5 days to grow into a durable material. Depending on the strain of mycelium used, they make many different varieties of the material including water absorbent, flame retardant, and dielectric.
Fungi are essential for converting biomass into compost, as they decompose feedstock components such as lignin, which many other composting microorganisms cannot. Turning a backyard compost pile will commonly expose visible networks of mycelia that have formed on the decaying organic material within. Compost is an essential soil amendment and fertilizer for organic farming and gardening. Composting can divert a substantial fraction of municipal solid waste from landfill.

</doc>
<doc id="51472" url="http://en.wikipedia.org/wiki?curid=51472" title="Spore">
Spore

In biology, a spore is a unit of asexual reproduction that may be adapted for dispersal and for survival, often for extended periods of time, in unfavorable conditions. By contrast, gametes are units of sexual reproduction. Spores form part of the life cycles of many plants, algae, fungi and protozoa. Bacterial spores are not part of a sexual cycle but are resistant structures used for survival under unfavourable conditions. Myxozoan spores release amoebulae into their hosts for parasitic infection, but also reproduce within the hosts through the pairing of two nuclei within the plasmodium, which develops from the amoebula.
Spores are usually haploid and unicellular and are produced by meiosis in the sporangium of a diploid sporophyte. Under favourable conditions the spore can develop into a new organism using mitotic division, producing a multicellular gametophyte, which eventually goes on to produce gametes. Two gametes fuse to form a zygote which develops into a new sporophyte. This cycle is known as alternation of generations.
The spores of seed plants, however, are produced internally and the megaspores, formed within the ovules and the microspores are involved in the formation of more complex structures that form the dispersal units, the seeds and pollen grains.
Definition.
The term "spore" derives from the ancient Greek word σπορά "spora", meaning "seed, sowing," related to σπόρος "sporos", "sowing," and σπείρειν "speirein", "to sow."
In common parlance, the difference between a "spore" and a "gamete" (both together called gonites) is that a spore will germinate and develop into a sporeling, while a gamete needs to combine with another gamete to form a zygote before developing further.
The chief difference between spores and seeds as dispersal units is that spores are unicellular, while seeds contain within them a multicellular gametophyte that produces a developing embryo, the multicellular sporophyte of the next generation. Spores germinate to give rise to haploid gametophytes, while seeds germinate to give rise to diploid sporophytes.
Classification of spore-producing organisms.
Vascular plant spores are always haploid. Vascular plants are either homosporous (or isosporous) or heterosporous. Plants that are homosporous produce spores of the same size and type. Heterosporous plants, such as seed plants, spikemosses, quillworts, and some aquatic ferns produce spores of two different sizes: the larger spore (megaspore) in effect functioning as a "female" spore and the smaller (microspore) functioning as a "male".
Classification of spores.
Spores can be classified in several ways:
By spore-producing structure.
In fungi and fungus-like organisms, spores are often classified by the structure in which meiosis and spore production occurs. Since fungi are often classified according to their spore-producing structures, these spores are often characteristic of a particular taxon of the fungi.
By mobility.
Spores can be differentiated by whether they can move or not.
Anatomy.
Under high magnification, spores can be categorized as either monolete spores or trilete spores. In monolete spores, there is a single line on the spore indicating the axis on which the mother spore was split into four along a vertical axis. In trilete spores, all four spores share a common origin and are in contact with each other, so when they separate, each spore shows three lines radiating from a center pole.
Spore tetrads and trilete spores.
Envelope-enclosed spore tetrads are taken as the earliest evidence of plant life on land, dating from the mid-Ordovician (early Llanvirn, ~ million years ago), a period from which no macrofossils have yet been recovered.
Individual trilete spores resembling those of modern cryptogamic plants first appeared in the fossil record at the end of the Ordovician period.
Dispersal.
In fungi, both asexual and sexual spores or sporangiospores of many fungal species are actively dispersed by forcible ejection from their reproductive structures. This ejection ensures exit of the spores from the reproductive structures as well as travelling through the air over long distances. Many fungi thereby possess specialized mechanical and physiological mechanisms as well as spore-surface structures, such as hydrophobins, for spore ejection. These mechanisms include, for example, forcible discharge of ascospores enabled by the structure of the ascus and accumulation of osmolytes in the fluids of the ascus that lead to explosive discharge of the ascospores into the air. The forcible discharge of single spores termed "ballistospores" involves formation of a small drop of water (Buller's drop), which upon contact with the spore leads to its projectile release with an initial acceleration of more than 10,000 g. Other fungi rely on alternative mechanisms for spore release, such as external mechanical forces, exemplified by puffballs. Attracting insects, such as flies, to fruiting structures, by virtue of their having lively colours and a putrid odour, for dispersal of fungal spores is yet another strategy, most prominently used by the stinkhorns.
In Common Smoothcap moss ("Atrichum undulatum"), the vibration of sporophyte has been shown to be an important mechanism for spore release.
In the case of spore-shedding vascular plants such as ferns, wind distribution of very light spores provides great capacity for dispersal. Also, spores are less subject to animal predation than seeds because they contain almost no food reserve; however they are more subject to fungal and bacterial predation. Their chief advantage is that, of all forms of progeny, spores require the least energy and materials to produce.
In the spikemoss "Selaginella lepidophylla", dispersal is achieved in part by an unusual type of diaspore, a tumbleweed.
References.
 

</doc>
<doc id="51474" url="http://en.wikipedia.org/wiki?curid=51474" title="Seat belt">
Seat belt

A seat belt, also known as a safety belt, is a vehicle safety device designed to secure the occupant of a vehicle against harmful movement that may result during a collision or a sudden stop. A seat belt functions to reduce the likelihood of death or serious injury in a traffic collision by reducing the force of secondary impacts with interior strike hazards, by keeping occupants positioned correctly for maximum effectiveness of the airbag (if equipped) and by preventing occupants being ejected from the vehicle in a crash or if the vehicle rolls over. When driving, the driver and passengers are travelling at the same speed as the car. If the car suddenly stops or crashes, the driver and passengers continue at the same speed the car was going before it stopped. A seatbelt applies an opposite force to the driver and passengers to prevent them from falling out or making contact with the interior of the car.
History.
Seat belts were invented by English engineer George Cayley in the early 19th century, though Edward J. Claghorn of New York, was granted the first patent (U.S. Patent , on February 10, 1885 for a safety belt). Claghorn was granted United States Patent #312,085 for a Safety-Belt for tourists, painters, firemen, etc. who are being raised or lowered, described in the patent as "designed to be applied to the person, and provided with hooks and other attachments for securing the person to a fixed object."
In 1911, Benjamin Foulois had the cavalry saddle shop fashion a belt for the seat of Wright Flyer Signal Corps 1. He wanted it to hold him firmly in his seat so he could better control his aircraft as he bounded along the rough field used for takeoff and landing. It was not until World War II that seat belts were fully adopted in military aircraft, and even then, it was mainly for safety reasons, not improved aircraft control.
In 1946, Dr. C. Hunter Shelden had opened a neurological practice at Huntington Memorial Hospital in Pasadena, California. In the early 1950s, Dr. Shelden had made a major contribution to the automotive industry with his idea of retractable seat belts. This came about greatly in part from the high number of head injuries coming through the emergency rooms. He investigated the early seat belts whose primitive designs were implicated in these injuries and deaths. His findings were published in the November 5, 1955 "Journal of the American Medical Association" (JAMA) in which he proposed not only the retractable seat belt, but also recessed steering wheels, reinforced roofs, roll bars, door locks and passive restraints such as the air bag. Subsequently in 1959, Congress passed legislation requiring all automobiles to comply with certain safety standards.
American car manufacturers Nash (in 1949) and Ford (in 1955) offered seat belts as options, while Swedish Saab first introduced seat belts as standard in 1958. After the Saab GT 750 was introduced at the New York Motor Show in 1958 with safety belts fitted as standard, the practice became commonplace.
Glenn Sheren of Mason, Michigan submitted a patent application on March 31, 1955 for an automotive seat belt and was awarded US Patent 2,855,215 in 1958. This was a continuation of an earlier patent application that Mr. Sheren had filed on September 22, 1952.
However, the first modern three point seat belt (the so-called "CIR-Griswold restraint") used in most consumer vehicles today was patented in 1955 U.S. Patent by the Americans Roger W. Griswold and Hugh DeHaven, and developed to its modern form by Swedish inventor Nils Bohlin for Swedish manufacturer Volvo—who introduced it in 1959 as standard equipment. In addition to designing an effective three-point belt, Bohlin demonstrated its effectiveness in a study of 28,000 accidents in Sweden. Unbelted occupants sustained fatal injuries throughout the whole speed scale, whereas none of the belted occupants were fatally injured at accident speeds below 60 mph. No belted occupant was fatally injured if the passenger compartment remained intact. Bohlin was granted U.S. Patent for the device.
The world's first seat belt law was put in place in 1970, in the state of Victoria, Australia, making the wearing of a seat belt compulsory for drivers and front-seat passengers. This legislation was enacted after trialing Hemco seatbelts, designed by Desmond Hemphill (1926–2001), in the front seats of police vehicles, lowering the incidence of officer injury and death.
Types.
Two-point.
A 2-point belt attaches at its two endpoints.
Lap.
A lap belt is a strap that goes over the waist. This was the most commonly installed type of belt prior to legislation requiring 3-point belts, and is primarily found in older cars. Coaches are equipped with lap belts, as are passenger aircraft seats.
University of Minnesota Professor James J. (Crash) Ryan was the inventor of and held the patent on the automatic retractable lap safety belt. Ralph Nader cited Ryan's work in Unsafe at Any Speed and in 1966 President Lyndon Johnson signed two bills requiring safety belts in all passenger vehicles starting in 1968: "SEE""Crash" Was His Name; Car Safety Was His Game" in Minnesota Medicine http://www.minnesotamedicine.com/Past-Issues/Past-Issues-2006/May-2006/Pulse-Crash-May-2006
Until the 1980s, three-point belts were commonly available only in the front outboard seats of cars; the back seats were only often fitted with lap belts. Evidence of the potential of lap belts to cause separation of the lumbar vertebrae and the sometimes associated paralysis, or "seat belt syndrome", led to progressive revision of passenger safety regulations in nearly all developed countries to require 3-point belts first in all outboard seating positions and eventually in all seating positions in passenger vehicles. Since September 1, 2007, all new cars sold in the U.S. require a lap and shoulder belt in the center rear seat. 
Besides regulatory changes, "seat belt syndrome" has led to tremendous liability for vehicle manufacturers. One Los Angeles case resulted in a $45 million jury verdict against the Ford Motor Company; the resulting $30 million judgment (after deductions for another defendant who settled prior to trial) was affirmed on appeal in 2006.
Sash.
A "sash" or shoulder harness is a strap that goes diagonally over the vehicle occupant's outboard shoulder and is buckled inboard of his or her lap. The shoulder harness may attach to the lap belt tongue, or it may have a tongue and buckle completely separate from those of the lap belt. Shoulder harnesses of this separate or semi-separate type were installed in conjunction with lap belts in the outboard front seating positions of many vehicles in the North American market starting at the inception of the shoulder belt requirement of the U.S. National Highway Traffic Safety Administration's Federal Motor Vehicle Safety Standard 208 on 1 January 1968. However, if the shoulder strap is used without the lap belt, the vehicle occupant is likely to "submarine", or slide forward in the seat and out from under the belt, in a frontal collision. In the mid-1970s, 3-point belt systems such as Chrysler's "Uni-Belt" began to supplant the separate lap and shoulder belts in American-made cars, though such 3-point belts had already been supplied in European vehicles such as Volvos, Mercedes, and Saabs for some years.
Three-point.
A 3-point belt is a Y-shaped arrangement, similar to the separate lap and sash belts, but unitized. Like the separate lap-and-sash belt, in a collision the 3-point belt spreads out the energy of the moving body over the chest, pelvis, and shoulders. Volvo introduced the first production three-point belt in 1959. The first car with three-point belt was a Volvo PV 544 that was delivered to a dealer in Kristianstad on August 13, 1959. However, the first car model to feature the three-point seat belt as a standard item was the 1959 Volvo 122, first outfitted with a two-point belt at initial delivery in 1958, replaced with the three-point seat belt the following year. The three-point belt was developed by Nils Bohlin who had earlier also worked on ejection seats at Saab. Volvo then made the new seat belt design patent open in the interest of safety and made it available to other car manufacturers for free.
Belt-in-Seat (BIS).
The BIS is a three-point harness with the shoulder belt attached to the seat itself, rather than to the vehicle structure. The first car using this system was the Range Rover Classic. Fitment was standard on the front seats from 1970. Some cars like the Renault Vel Satis use this system for the front seats. A General Motors assessment concluded seat-mounted 3-point belts offer better protection especially to smaller vehicle occupants, though GM did not find a safety performance improvement in vehicles with seat-mounted belts versus body-mounted belts.
BIS type belts have been used by automakers in convertibles and pillarless hardtops, where there is no "B" pillar to affix the upper mount of the belt. Chrysler and Cadillac are well known for using this design. Antique auto enthusiasts sometimes replace original seats in their cars with BIS-equipped front seats, providing a measure of safety not available when these cars were new. However, modern BIS systems typically use electronics that must be installed and connected with the seats and the vehicle's electrical system in order to function properly.
4-, 5-, and 6-point.
Five-point harnesses are typically found in child safety seats and in racing cars. The lap portion is connected to a belt between the legs and there are two shoulder belts, making a total of five points of attachment to the seat. A 4-point harness is similar, but without the strap between the legs, while a 6-point harness has two belts between the legs. In NASCAR, the 6-point harness became popular after the death of Dale Earnhardt, who was wearing a five-point harness when he suffered his fatal crash; as it was first thought that his belt had broken, and broke his neck at impact, some teams ordered a six-point harness in response.
Seven-point.
Aerobatic aircraft frequently use a combination harness consisting of a five-point harness with a redundant lap-belt attached to a different part of the air craft. While providing redundancy for negative-g maneuvers (which lift the pilot out of the seat); they also require the pilot to un-latch two harnesses if it is necessary to parachute from a failed aircraft.
Technology.
Locking retractors.
The purpose of locking retractors is to provide the seated occupant the convenience of some free movement of the upper torso within the compartment, while providing a method of limiting this movement in the event of a crash. Most modern seat belts are stowed on spring-loaded reels called "retractors" equipped with inertial locking mechanisms that stop the belt from extending off the reel during severe deceleration. There are two main types of inertial seat belt lock. A webbing-sensitive lock is based on a centrifugal clutch activated by rapid acceleration of the strap (webbing) from the reel. The belt can be pulled from the reel only slowly and gradually, as when the occupant extends the belt to fasten it. A sudden rapid pull of the belt — as in a sudden braking or collision event — causes the reel to lock, restraining the occupant in position.
A vehicle-sensitive lock is based on a pendulum swung away from its plumb position by rapid deceleration or rollover of the vehicle. In the absence of rapid deceleration or rollover, the reel is unlocked and the belt strap may be pulled from the reel against the spring tension of the reel. The vehicle occupant can move around with relative freedom while the spring tension of the reel keeps the belt taut against the occupant. When the pendulum swings away from its normal plumb position due to sudden deceleration or rollover, a pawl is engaged, the reel locks and the strap restrains the belted occupant in position. Dual-sensing locking retractors use both vehicle G-loading and webbing payout rate to initiate the locking mechanism.
Pretensioners and webclamps.
Seatbelts in many newer vehicles are also equipped with "pretensioners" or "web clamps", or both.
Pretensioners preemptively tighten the belt to prevent the occupant from jerking forward in a crash. Mercedes-Benz first introduced pretensioners on the 1981 S-Class. In the event of a crash, a pretensioner will tighten the belt almost instantaneously. This reduces the motion of the occupant in a violent crash. Like airbags, pretensioners are triggered by sensors in the car's body, and many pretensioners have used explosively expanding gas to drive a piston that retracts the belt. Pretensioners also lower the risk of "submarining", which occurs when a passenger slides forward under a loosely fitted seat belt.
Some systems also pre-emptively tighten the belt during fast accelerations and strong decelerations, even if no crash has happened. This has the advantage that it may help prevent the driver from sliding out of position during violent evasive maneuvers, which could cause loss of control of the vehicle. These pre-emptive safety systems may "prevent" some collisions from happening, as well as reducing injury in the event an actual collision occurs. Pre-emptive systems generally use electric pretensioners which can operate repeatedly and for a sustained period, rather than pyrotechnic pretensioners, which can only operate a single time.
Webclamps clamp the webbing in the event of an accident, and limit the distance the webbing can spool out (caused by the unused webbing tightening on the central drum of the mechanism). These belts also often incorporate an energy management loop ("rip stitching") in which a section of the webbing is looped and stitched with a special stitching. The function of this is to "rip" at a predetermined load, which reduces the maximum force transmitted through the belt to the occupant during a violent collision, reducing injuries to the occupant.
A study demonstrated that standard automotive 3-point restraints fitted with pyrotechnic or electric pretensioners were not able to eliminate all interior passenger compartment head strikes in rollover test conditions. Electric pretensioners are often incorporated on vehicles equipped with precrash systems; they are designed to reduce seat belt slack in a potential collision and assist in placing the occupants in a more optimal seating position. The electric pretensioners also can operate on a repeated or sustained basis, providing better protection in the event of an extended rollover or a multiple collision accident.
Inflatable.
The inflatable seatbelt was invented by Donald Lewis and tested at the Automotive Products Division of Allied Chemical Corporation. Inflatable seatbelts have tubular inflatable bladders contained within an outer cover. When a crash occurs the bladder inflates with a gas to increase the area of the restraint contacting the occupant and also shortening the length of the restraint to tighten the belt around the occupant, improving the protection. The inflatable sections may be shoulder-only or lap and shoulder. The system supports the head during the crash better than a web only belt. It also provides side impact protection. In 2013, Ford began offering rear seat inflatable seat belts on a limited set of models, such as the Explorer and Flex.
Automatic.
Seatbelts that automatically move into position around a vehicle occupant once the adjacent door is closed and/or the engine is started were developed as a countermeasure against low usage rates of manual seat belts, particularly in the United States. The first car to feature automatic shoulder belts as standard equipment was the 1981 Toyota Cressida, but the history of such belts goes back further.
The 1972 Volkswagen ESVW1 Experimental Safety Vehicle presented passive seat belts. Volvo tried to develop a passive three point seatbelt. In 1973 Volkswagen announced they had a functional passive seat belt. The first commercial car to use automatic seat belts was the 1975 Volkswagen Rabbit.
Automatic seat belts received a boost in the United States in 1977 when Brock Adams, United States Secretary of Transportation in the Carter Administration, mandated that by 1983 every new car should have either airbags or automatic seat belts despite strong lobbying from the auto industry. Adams was attacked by Ralph Nader, who said that the 1983 deadline was too late. Soon after, General Motors began offering automatic seat belts, first on the Chevrolet Chevette, but by early 1979 the VW Rabbit and the Chevette were the only cars to offer the safety feature, and GM was reporting disappointing sales. By early 1978, Volkswagen had reported 90,000 Rabbits sold with automatic seat belts. A study released in 1978 by the United States Department of Transportation claimed that cars with automatic seat belts had a fatality rate of .78 per 100 million miles, compared with 2.34 for cars with regular, manual belts.
In 1981, Drew Lewis, the first Transportation Secretary of the Reagan Administration, influenced by studies done by the auto industry, "killed" the previous administration's mandate; the decision was overruled in a federal appeals court the following year, and then by the Supreme Court. In 1984, the Reagan Administration reversed its course, though in the meantime the original deadline had been extended; Elizabeth Dole, then Transportation Secretary, proposed that the two passive safety restraints be phased into vehicles gradually, from vehicle model year 1987 to vehicle model year 1990, when all vehicles would be required to have either automatic seat belts or driver side air bags. Though more awkward for vehicle occupants, most manufacturers opted to use less expensive automatic belts rather than airbags during this time period.
When driver side airbags became mandatory on all passenger vehicles in model year 1995, most manufacturers stopped equipping cars with automatic seat belts. Exceptions include the 1995-1996 Ford Escort/Mercury Tracer and the Eagle Summit Wagon which had automatic safety belts along with dual airbags.
Disadvantages.
Automatic belt systems generally offer inferior occupant crash protection. In systems with belts attached to the door rather than a sturdier fixed portion of the vehicle body, a crash that causes the vehicle door to open leaves the occupant without belt protection. In such a scenario, the occupant may be thrown from the vehicle and suffer greater injury or death.
Because many automatic belt system designs compliant with the US passive-restraint mandate did not meet the safety performance requirements of Canada—which were not weakened to accommodate automatic belts—vehicle models which had been eligible for easy importation in either direction across the US-Canada border when equipped with manual belts became ineligible for importation in either direction once the US variants got automatic belts and the Canadian versions retained manual belts. Two such models were the Dodge Spirit and Plymouth Acclaim.
Automatic belt systems also present several operational disadvantages. Motorists who would normally wear seat belts must still fasten the manual lap belt, thus rendering redundant the automation of the shoulder belt. Those who do not fasten the lap belt wind up inadequately protected by only the shoulder belt; in a crash without a lap belt such a vehicle occupant is likely to "submarine" (be thrown forward under the shoulder belt) and be seriously injured. Motorized or door-affixed shoulder belts hinder access to the vehicle, making it difficult to enter and exit—particularly if the occupant is carrying items such as a box or a purse. Vehicle owners tend to disconnect the motorized or door-affixed shoulder belt to alleviate the nuisance of entering and exiting the vehicle, leaving only a lap belt for crash protection. Also, many automatic seat belt systems are incompatible with child safety seats, or compatible only with special modifications.
Experimental.
Research and development efforts are ongoing to improve the safety performance of vehicle seatbelts. Some experimental designs have included:
In rear seats.
In 1955 (as a 1956 package), Ford offered lap only seat belts in the rear seats as an option within the "Lifeguard" safety package. In 1967, Volvo started to install lap belts in the rear seats. In 1972, Volvo upgraded the rear seat belts to a three-point belt.
In crashes, unbelted rear passengers increase the risk of belted front seat occupants' death by nearly five times.
Child occupants.
As with adult drivers and passengers, the advent of seat belts was accompanied by calls for their use by child occupants, including legislation requiring such use. Generally children using adult seat belts suffer significantly lower injury risk when compared to non-buckled children.
The UK extended compulsory seatbelt wearing to child passengers under the age of 14 in 1989. It was observed that this measure was accompanied by a 10% "increase" in fatalities and a 12% "increase" in injuries among the target population. In crashes, small children who wear adult seatbelts can suffer "seat-belt syndrome" injuries including severed intestines, ruptured diaphragms and spinal damage. There is also research suggesting that children in inappropriate restraints are at significantly increased risk of head injury, one of the authors of this research has been quoted as claiming that "The early graduation of kids into adult lap and shoulder belts is a leading cause of child-occupant injuries and deaths."
As a result of such findings, many jurisdictions now advocate or require child passengers to use specially designed child restraints. Such systems include separate child-sized seats with their own restraints and booster cushions for children using adult restraints. In some jurisdictions children below a certain size are forbidden to travel in front car seats."
Reminder chime and light.
Social and historical context.
At the time seat belt reminder systems were conceived of and becoming popularized (the 1970s), the field of psychology was undergoing a major shift. In fact, “in the last half of the twentieth century, applied psychology outstripped the academic, research-oriented psychology that had dominated for so many years.” As a result, psychologists were less and less carrying out their careers in academic settings, opting more for work in applied areas. The involvement of psychologists in organizations like the National Highway Traffic Safety Administration (NHTSA), then, makes sense, as psychologists during this time were eager to apply what they had learned during their training in real world settings. Additionally, the automotive industry was likely eager to encourage the involvement of psychologists for reasons previously discussed, but also as a reflection of the changing times; psychology was growing in prominence and reputation. The locomotive industry in the U.S. really took off in the 1800s, but the rise of applied psychology in the late 1900s could explain why, up to that point, nobody had thought to use psychological concepts to enhance motor vehicle safety. 
It’s also interesting to note that the automotive industry had a rough time in the 1970s. The Vietnam War hit the American economy hard, leaving all industries struggling to maintain consumers. The 1970s brought along increased gas prices due to an oil crisis that left many Americans and Europeans unable to buy fuel for the cars they did own. Along with the oil crisis came a concern about the environmental effects of gas and emissions, and the realization that oil was not a renewable resource as a result of the environmentalist movement of the decade. There was also an increased concern for the safety of drivers, which possibly stemmed from the concern about safety in the workplace that came to the forefront of the early 1970s when President Nixon signed OSHA. To top it all off, insurance rates were rising as well. The 1970s is noted for being a tumultuous time in our nation’s history, as the U.S. was war-torn and the country saw the struggle of minority groups for equal rights. At a time when the automotive industry was struggling to make advances and appease consumers, perhaps the integration of a seat belt safety system made for a good way to do both, offering the unsettled citizens of America a little peace in the process. 
Nature of the problem.
In 1970, police collected data demonstrating a total of 40,000 people involved in car crashes during a time when seat belt use ranged from merely 12% - 15%. In North America, cars sold since the early 1970s have included an audiovisual reminder system consisting of a light on the dashboard and a buzzer or chime reminding the driver and passengers to fasten their belts. Originally, these lights were accompanied by a warning buzzer whenever the transmission was in any position except park if either the driver was not buckled up or, as determined by a pressure sensor in the passenger's seat, if there was a passenger there not buckled up. However, this was considered by many to be a major annoyance, as the light would be on and the buzzer would sound continuously if front-seat passengers were not buckled up. Therefore, people who did not wish to buckle up would defeat this system by fastening the seat belts with the seat empty and leaving them that way. To combat this dangerous habit, in 1971 the NHTSA adopted an "occupant protection option” for vehicles built between August of 1973 and August of 1975. This option operated on an interlocking system, meaning that the car would not start until all of the front seat belts were secured. However, the public did not care for this system either. Many customers found ways to evade or even disable the system, and as a result of this reaction, in 1974 Congress prohibited the NHTSA from mandating safety requirements for vehicles. Congress, instead, took on the responsibility to require that the “driver's seating position be equipped with a seat belt warning system that activates, under circumstances when the driver's seat belt is not buckled, a continuous or intermittent audible signal for a period of not less than 4 seconds and not more than 8 seconds, and a continuous or flashing warning light for not less than 60 seconds after the ignition switch is turned on”. This technology is referred to as a “seat belt reminder system”, or SBR system, and has been used as the standard of vehicle safety since the mid-1970s. 
Later on, some manufacturers began replacing buzzer-based warning systems with a chime system, including the seat belt warning system; the idea was that the chimes were "gentler" than the buzzer. The first U.S. car model to offer this system was the 1976 Cadillac Seville, and this system eventually replaced the buzzer system by the early 1990s. In the mid-1990s, an insurance company from Sweden called Folksam worked with Saab and Ford to determine the requirements for the most efficient seat belt reminder, and implemented technology based on these requirements in the late 1990s. A defining characteristic of this new technology was that “the warning became increasingly penetrating the more seconds the seat belt hadn't been worn”. Additionally, according to the NHSTA (2009), various automobile manufacturers are currently pushing to implement enhanced seat belt reminder (EBSR) systems into their products. EBSR systems take the federally mandated system, as well as the Swedish SBR system of the 1990s, even a step further by integrating an even more persistent warning system when occupants are not belted. EBSR systems “range from very simple displays (e.g., flashing icon) to complex, multistage systems triggered by driving status (e.g., speed, travel distance) and feature multiple types of visual, acoustic, voice, and possibly even haptic (tactile) displays, as well as interlocks, delays, or limitations on some aspect of vehicle performance”. 
Today, the belt warning light may stay on for several minutes after the car is started if the driver's seat belt is not fastened, although the chime/buzzer will sound for only a few seconds when the key was turned to the "on" position.
In Europe and some other parts of the world, most modern cars include a seat-belt reminder light for the driver and some also include a reminder for the passenger, when present, activated by a pressure sensor under the passenger seat. Some cars will intermittently flash the reminder light and sound the chime until the driver (and sometimes the front passenger, if present) fasten their seatbelts.
Psychological application.
SBR systems and EBSR systems employ the psychological concept of the two-factor theory of learning, or the two-factor theory of avoidance. In the mid-20th century psychologists were so enamored with the learning theories proposed by Pavlov (classical conditioning) and Skinner (operant conditioning) that they were seen as practically universal. However, critiques of learning theory during this time proposed that neither classical nor operant conditioning accounted for the entirety of the conditioning process. It was Hobard Mowrer who introduced the two-factor theory which focused on the relationship between classical and operant conditioning. To demonstrate his theory, Mowrer conducted experiments with rats using a shuttlebox, or a box divided into two compartments that could be jumped over to reach the next compartment. Rats were shocked following the sound of a buzzer, the unconditioned stimulus, which evoked a sense of fear and pain, the unconditioned response. After a few trials, eventually the rats associated the sound of the buzzer with the pain and fear of the shock, and even when the shocks were taken away the rats responded to the buzzer (now the conditioned stimulus) with fear. At this point in Mowrer’s experimentation, he had examined the tenets of Pavlov’s classical conditioning. But then Mowrer delved into Skinner’s operant conditioning as well, noting that the rat had learned to react to its environment differently; it learned an adaptive response. At the sound of the buzzer, the rat learned to jump over the barrier to escape the shock and reduce its own fear as a means of negative reinforcement, the strengthening of a behavior via the removal of an unpleasant stimulus.
SBR systems and ESBR systems work in essentially this same way. Two-factor theory of avoidance is echoed in daily behaviors such as seat belt use. People experience the noise (unconditioned stimulus) of an SBR system, or even the more advanced and diverse haptic, visual, or acoustic warnings of the ESBR system, when they do not buckle their seat belt. For most people, this noise produces an emotional reaction: aggravation or annoyance, or perhaps even the fear of a fatal crash (unconditioned response). After failing to secure the seatbelt multiple times, the individual will begin to pair the noise (now the conditioned stimulus) with the aggravated or fearful emotions. The individual then learns an adaptive response to the SBR system, and secures the seat belt to get rid of the unfavorable emotions. The securing of the seat belt acts as negative reinforcement. Thus, in theory, SBR systems and ESBR systems work to strengthen the behavior of wearing a seat belt by encouraging individuals to get rid of the unpleasant stimulus to which they have become conditioned. Thus demonstrating the work of Mowrer in that we see a combination of both classical and operant conditioning at play.
Effectiveness.
The transition from the occupant protection option of the 1970s to the current SBR system has shown significant increases in seat belt use. As previously noted, the early 1970s saw a seat belt use percentage of between 12% and 15%. In 2001, Congress directed the NHSTA to study the benefits of technology meant to increase the use of seat belts. The NHSTA found that seat belt usage had increased to 73% since the initial introduction of the SBR system by Congress. Additionally, automotive manufacturers have conducted their own studies. In 2002, Ford demonstrated that seat belts were used more in Fords with seat belt reminders than in those without one: 76% and 71% respectively. In 2007, Honda conducted a similar study and found that 90% of people who drove Hondas with seat belt reminders used a seat belt, while 84% of people who drove Hondas without seat belt reminders used a seat belt. With the Centers for Disease Control and Prevention estimating that the use of a seat belt reduces an individual’s chance of fatality in a car crash by 50%, it would seem that the SBR system is doing its part to protect lives in automobiles. 
Despite copious amounts of evidence that the SBR system increases seat belt use, there are those who still believe the automobile industry can do better. Thus, much modern research has been focused on the potential of the ESBR system. In 2003, the Transportation Research Board Committee, chaired by two psychologists, reported that EBSR systems have the potential to save an additional 1,000 lives a year. According to the committee’s research, an EBSR system that is much more persistent than what is federally mandated could increase seat belt use by up to five percentage points. The study cited research by the Insurance Institute for Highway Safety for Ford on their new Beltminder system which “chimes intermittently for up to five minutes, sounding for 6 seconds then pausing for 30, if a driver fails to buckle up. It increased seat belt use by 5 percent”. Additional research strengthens the push for EBSR systems. Farmer and Wells (2010) compared “driver fatality rates per vehicle registration per year to otherwise identical vehicle models with and without enhanced seat belt reminders” for the years 2000 – 2007. They found that “driver fatality rates were 6% lower for vehicles with enhanced seat belt reminders compared with vehicles without enhanced belt reminders”. Thus, while seat belt reminder systems, both enhanced and not, appear to increase seat belt usage, it also seems that, the more aggressive the system, the more likely an individual is to buckle up. Interestingly, this finding contradicts the happenings in the 1970s, wherein people became frustrated with the occupant protection option and Congress stripped the NHTSA of their right to regulate motor vehicles as a result.
Ethics.
A couple of ethical concerns can be raised when considering the SBR system. Initially, there may be a conflict of interest where the research is involved. As has already been mentioned, the NHTSA includes members who are practicing psychologists. Conducting sound research for the NHTSA may be compromised when the organization is signing a psychologist’s paycheck. There is always the possibility that a specific automotive company may take advantage of a psychologist’s abilities in a way similar to the high number of psychologists that are employed by the Department of Defense. While it is likely that there are plenty more members of the APA working for the Department of Defense than there are working for the NHTSA, ethics may become conflicted during research or testing in this capacity. Additionally, the tension between Congress and the NHTSA may exacerbate this potential for conflict. The NHTSA does not believe there was any scientific grounding in Congress’ 1974 decision to ban them from regulating the safety features of motor vehicles. Using psychological science as a way to increase the safety of drivers or even yield sound research may walk too close to the line of exploitation should the NHTSA desire to one-up Congress and demonstrate their value as a decision-making organization. 
Legislation.
Observational studies of car crash morbidity and mortality, experiments using both crash test dummies and human cadavers indicate that wearing seat belts greatly reduces the risk of death and injury in the majority of car crashes.
This has led many countries to adopt mandatory seat belt wearing laws. It is generally accepted that, in comparing like-for-like accidents, a vehicle occupant not wearing a properly fitted seat belt has a significantly and substantially higher chance of death and serious injury. One large observation studying using US data showed that the odds ratio of crash death is 0.46 with a three-point belt, when compared with no belt. In another study that examined injuries presenting to the ER pre- and post-seat belt law introduction, it was found that 40% more escaped injury and 35% more escaped mild and moderate injuries.
The effects of seat belt laws are disputed by those who observe that their passage did not reduce road fatalities. There was also concern that instead of legislating for a general protection standard for vehicle occupants, laws that required a particular technical approach would rapidly become dated as motor manufacturers would tool up for a particular standard which could not easily be changed. For example, in 1969 there were competing designs for lap and 3-point seat belts, rapidly tilting seats, and air bags being developed. But as countries started to mandate seat belt restraints the global auto industry invested in the tooling and standardized exclusively on seat belts, and ignored other restraint designs such as air bags for several decades
Risk compensation.
Some have proposed that the number of deaths was influenced by the development of risk compensation, which says that drivers adjust their behavior in response to the increased sense of personal safety wearing a seat belt provides.
In one trial subjects were asked to drive go-karts around a track under various conditions. It was found that subjects who started driving unbelted drove consistently faster when subsequently belted. Similarly, a study of habitual non-seatbelt wearers driving in freeway conditions found evidence that they had adapted to seatbelt use by adopting higher driving speeds and closer following distances. 
A 2001 analysis of US crash data aimed to establish the effects of seatbelt legislation on driving fatalities and found that previous estimates of seatbelts effectiveness had been significantly overstated. According to the analysis used, seatbelts were claimed to have decreased fatalities by 1.35% for each 10% increase in seatbelt use. The study controlled for endogenous motivations of seat belt use, which it is claimed creates an artificial correlation between seat belt use and fatalities, leading to the conclusion that seatbelts cause fatalities. For example, drivers in high risk areas are more likely to use seat belts, and are more likely to be in accidents, creating a non-causal correlation between seatbelt use and mortality. After accounting for the endogeneity of seatbelt usage, Cohen and Einav found no evidence that the risk compensation effect makes seatbelt wearing drivers more dangerous, a finding at variance with other research.
Increased traffic.
Other statistical analyses have included adjustments for factors such as increased traffic, and other factors such as age, and based on these adjustments, a reduction of morbidity and mortality due to seat belt use has been claimed. However, Smeed's law predicts a fall in accident rate with increasing car ownership and has been demonstrated independently of seat belt legislation.
Mass transit considerations.
Buses.
School buses.
Pros and cons had been alleged about the use of seatbelts in school buses.
Motor coaches.
In the European Union, all new long distance buses and coaches must be fitted with seat belts.
Australia has required lap/sash seat belts in new coaches since 1994. These must comply with Australian Design Rule 68, which requires the seat belt, seat and seat anchorage to withstand 20g deceleration and an impact by an unrestrained occupant to the rear.
In the United States, NHTSA has now required lap-shoulder seat belts in new "over-the-road" buses (includes most coaches) starting in 2016.
Trains.
The use of seatbelts in trains has been investigated. Concerns about survival space intrusion in train crashes and increased injuries to unrestrained or incorrectly restrained passengers led the researchers to discourage the use of seat belts in trains.
Airplanes.
It is required by the FAR's. For general aviation aircraft, it is found in 23.785; for transport category aircraft, it is found in 25.785. The requirements of the FAR's are met, when the seatbelt (restraint system) conforms to a Technical Standard Order (TSO), which is TSO-c22f. 
http://www.gpo.gov/fdsys/pkg/CFR-2012-title14-vol1/pdf/CFR-2012-title14-vol1-sec23-785.pdf

</doc>
<doc id="51478" url="http://en.wikipedia.org/wiki?curid=51478" title="Rolf Singer">
Rolf Singer

Rolf Singer (June 23, 1906 – January 18, 1994) was a German-born mycologist and one of the most important taxonomists of gilled mushrooms (agarics) in the 20th century.
After receiving his Ph.D. at the University of Vienna in 1931 he worked in Munich. By 1933, however, Singer was forced to flee Nazi Germany to Vienna. There he met his wife, Martha Singer. From Vienna, Singer and his wife went to Barcelona, Spain, where Singer was appointed Assistant Professor at the Autonomous University of Barcelona. Persecution by the Spanish authorities on behalf of the German government forced Singer to leave Spain for France in 1934. After a fellowship at the Museum d'Histoire Naturelle in Paris, Singer again moved, this time to Leningrad, where he was Senior Scientific Expert at the Botanical Garden of the Academy of Sciences of the USSR. During his time at the Academy, Singer made many expeditions to Siberia, the Altai Mountains, and Karelia. In 1941, Singer emigrated to the United States. He was offered a position at the Farlow Herbarium as a research associate, then as Assistant Curator, then as acting Curator following the death of Dr. David Linder. He spent a total of seven years at the Farlow. During this time, Singer also received a Guggenheim Fellowship for studies in Florida, and taught at the Mountain Lake Biological Station of the University of Virginia.
In 1948, Singer left Harvard to become professor at the Universidad Nacional de Tucuman in Argentina. Later, in 1961, Singer became professor at the Universidad de Buenos Aires. During his time in South America, Singer, his wife, and his daughter Heidi collected extensively. Singer's last faculty appointment was at the University of Illinois at Chicago, from 1968 to 1977.
Singer was a prolific writer, with more than 400 publications to his name. He was also known for his eagerness to aid other botanists, whether they were professionals or amateurs.
He wrote major books like "The Agaricales in Modern Taxonomy". He fled to various countries during the Nazi period, pursuing mycology in far-flung places like the Soviet Union, Argentina, and finally the United States, as mycologist at the Field Museum in Chicago.
The standard author abbreviation Singer is used to indicate this individual as the author when citing a botanical name.

</doc>
<doc id="51480" url="http://en.wikipedia.org/wiki?curid=51480" title="Hypha">
Hypha

A hypha (plural hyphae, from Greek ὑφή, huphḗ, “web”) is a long, branching filamentous structure of a fungus, oomycete, or actinobacterium. In most fungi, hyphae are the main mode of vegetative growth, and are collectively called a mycelium. Yeasts are unicellular fungi that do not grow as hyphae.
Structure.
A hypha consists of one or more cells surrounded by a tubular cell wall. In most fungi, hyphae are divided into cells by internal cross-walls called "septa" (singular septum). Septa are usually perforated by pores large enough for ribosomes, mitochondria and sometimes nuclei to flow between cells. The major structural polymer in fungal cell walls is typically chitin, in contrast to plants that have cellulosic cell walls. Some fungi have aseptate hyphae, meaning their hyphae are not partitioned by septa.
Growth.
Hyphae grow at their tips. During tip growth, cell walls are extended by the external assembly and polymerization of cell wall components, and the internal production of new cell membrane. The spitzenkörper is an intracellular organelle associated with tip growth. It is composed of an aggregation of membrane-bound vesicles containing cell wall components. The spitzenkörper is part of the endomembrane system of fungi, holding and releasing vesicles it receives from the Golgi apparatus. These vesicles travel to the cell membrane via the cytoskeleton and release their contents outside the cell by the process of exocytosis, where it can then be transported to where it is needed. Vesicle membranes contribute to growth of the cell membrane while their contents form new cell wall. The spitzenkörper moves along the apex of the hyphal strand and generates apical growth and branching; the apical growth rate of the hyphal strand parallels and is regulated by the movement of the spitzenkörper.
As a hypha extends, septa may be formed behind the growing tip to partition each hypha into individual cells. Hyphae can branch through the bifurcation of a growing tip, or by the emergence of a new tip from an established hypha.
Behavior.
The direction of hyphal growth can be controlled by environmental stimuli, such as the application of an electric field. Hyphae can sense reproductive units from some distance, and grow towards them. Hyphae can weave through a permeable surface to penetrate it.
Modifications.
Hyphae may be modified in many different ways to serve specific functions. Some parasitic fungi form haustoria that function in absorption within the host cells. The arbuscules of mutualistic mycorrhizal fungi serve a similar function in nutrient exchange, so are important in assisting nutrient and water absorption by plants. Ectomycorrhizal extramatrical mycelium greatly increases the soil area available for exploitation by plant hosts by funneling water and nutrients to ectomycorrhizas, complex fungal organs on the tips of plant roots. Hyphae are found enveloping the gonidia in lichens, making up a large part of their structure. In nematode-trapping fungi, hyphae may be modified into trapping structures such as constricting rings and adhesive nets. Mycelial cords can be formed to transfer nutrients over larger distances. Bulk fungal tissues, cords, and membranes, such as those of mushrooms and lichens, are mainly composed of felted and often anastomosed hyphae.
Types.
Classification based on cell wall and overall form.
Characteristics of hyphae can be important in fungal classification. In basidiomycete taxonomy, hyphae that comprise the fruiting body can be identified as generative, skeletal, or binding hyphae.
Based on the generative, skeletal and binding hyphal types, in 1932 E. J. H. Corner applied the terms monomitic, dimitic, and trimitic to hyphal systems, in order to improve the classification of polypores.
Fungi that form fusiform skeletal hyphae bound by generative hyphae are said to have sarcodimitic hyphal systems. A few fungi form fusiform skeletal hyphae, generative hyphae, and binding hyphae, and these are said to have sarcotrimitic hyphal systems. These terms were introduced as a later refinement by E. J. H. Corner in 1966.
Classification based on refractive appearance.
Hyphae are described as "gloeoplerous" ("gloeohyphae") if their high refractive index gives them an oily or granular appearance under the microscope. These cells may be yellowish or clear (hyaline). They can sometimes selectively be coloured by sulphovanillin or other reagents. The specialized cells termed cystidia can also be gloeoplerous.

</doc>
<doc id="51481" url="http://en.wikipedia.org/wiki?curid=51481" title="Jurist">
Jurist

Jurist (a word coming from medieval Latin) is someone who researches, studies and teaches jurisprudence (theory of law). Such a person can work as an academic, a legal writer, a barrister, or an eminent judge (because judges in high positions shape and in some legal systems make law). Thus a "jurist," someone who studies, analyses and comments on law, stands in contrast with a "lawyer", someone who applies law on behalf of clients and thinks about it in practical terms. As one author has explained:
A man may be both a lawyer and a jurist, but a jurist is not necessarily a lawyer, nor a lawyer necessarily a jurist. Both must possess an acquaintance with what we call the law, but that is all. The work of the jurist is the study, analysis, and arrangement of the law — work which can be done wholly in the seclusion of the library. The work of the lawyer is the satisfaction of the wishes of particular human beings for legal assistance — work which requires dealing to some extent therefore with people in the office, in the court room, or in the market-place. The relative importance of the lawyer and the jurist is not material to this discussion.<br>Any highly civilized society requires both lawyers and jurists, both philosophers and men of affairs. As a mere matter of fact, there is a greater demand for men of affairs than for philosophers, for lawyers than for jurists; but the number of persons which the interests of society require should engage in a particular occupation, has nothing to do with the question of the importance of the different kinds of work done by those persons. It is important however to note the fundamental difference between the work of the lawyer and that of the jurist.
The term "jurist "has another sense, which is wider, synonymous with "legal professional", i.e. anyone professionally involved with law. In some other European languages, a word resembling "jurist" (such as French "juriste", Italian "giurista", Spanish or Portuguese "jurista" etc.) is used in this wider sense. In the USA and Canada, "jurist" is sometimes used to specifically to refer to a judge.
Muslim world.
In Sharia (Islamic law), jurists are known as Ulema, who specialize in Fiqh (Islamic jurisprudence). In order to become an Islamic jurist, it is required for a student to receive an "ijazat attadris wa 'l-ifa"' ("license to teach and issue legal opinions"), equivalent to the Juris Doctor and Doctor of Laws qualifications, from a "Madrasah" or "Jami'ah", equivalent to a college and university respectively. This system of legal education dates back to the 9th century, during the classical period of Islam.

</doc>
<doc id="51483" url="http://en.wikipedia.org/wiki?curid=51483" title="Mutualism (biology)">
Mutualism (biology)

Mutualism is the way two organisms of different species exist in a relationship in which each individual benefits from the activity of the other. Similar interactions within a species are known as co-operation. Mutualism can be contrasted with interspecific competition, in which each species experiences "reduced" fitness, and exploitation, or parasitism, in which one species benefits at the "expense" of the other. Mutualism is a type of symbiosis. Symbiosis is a broad category, defined to include relationships that are mutualistic, parasitic, or commensal. Mutualism is only one "type".
A well-known example of mutualism is the relationship between ungulates (such as Bovines) and bacteria within their intestines. The ungulates benefit from the cellulase produced by the bacteria, which facilitates digestion; the bacteria benefit from having a stable supply of nutrients in the host environment.
Mutualism plays a key part in ecology. For example, mutualistic interactions are vital for terrestrial ecosystem function as more than 48% of land plants rely on mycorrhizal relationships with fungi to provide them with inorganic compounds and trace elements. In addition, mutualism is thought to have driven the evolution of much of the biological diversity we see, such as flower forms (important for pollination mutualisms) and co-evolution between groups of species. However mutualism has historically received less attention than other interactions such as predation and parasitism.
Measuring the exact fitness benefit to the individuals in a mutualistic relationship is not always straightforward, particularly when the individuals can receive benefits from a variety of species, for example most plant-pollinator mutualisms. It is therefore common to categorise mutualisms according to the closeness of the association, using terms such as obligate and facultative. Defining "closeness," however, is also problematic. It can refer to mutual dependency (the species cannot live without one another) or the biological intimacy of the relationship in relation to physical closeness ("e.g.", one species living within the tissues of the other species).
The term "mutualism" was introduced by Pierre-Joseph van Beneden in 1876.
Types of relationships.
Mutualistic transversals can be thought of as a form of "biological barter" in mycorrhizal associations between plant roots and fungi, with the plant providing carbohydrates to the fungus in return for primarily phosphate but also nitrogenous compounds. Other examples include rhizobia bacteria that fix nitrogen for leguminous plants (family Fabaceae) in return for energy-containing carbohydrates.
Service-resource relationships.
Service-resource relationships are also common.
Pollination in which nectar or pollen (food resources) are traded for pollen dispersal (a service) or ant protection of aphids, where the aphids trade sugar-rich honeydew (a by-product of their mode of feeding on plant sap) in return for defense against predators such as ladybugs.
Phagophiles feed (resource) on ectoparasites, thereby providing anti-pest service, as in cleaning symbiosis.
Elacatinus and Gobiosoma, genus of gobies, also feed on ectoparasites of their clients while cleaning them.
Zoochory is an example where animals disperse the seeds of plants. This is similar to pollination in that the plant produces food resources (for example, fleshy fruit, overabundance of seeds) for animals that disperse the seeds (service).
Service-service relationships.
Strict service-service interactions are very rare, for reasons that are far from clear. One example is the relationship between sea anemones and anemone fish in the family Pomacentridae: the anemones provide the fish with protection from predators (which cannot tolerate the stings of the anemone's tentacles) and the fish defend the anemones against butterflyfish (family Chaetodontidae), which eat anemones. However, in common with many mutualisms, there is more than one aspect to it: in the anemonefish-anemone mutualism, waste ammonia from the fish feed the symbiotic algae that are found in the anemone's tentacles. Therefore what appears to be a service-service mutualism in fact has a service-resource component. A second example is that of the relationship between some ants in the genus "Pseudomyrmex" and trees in the genus "Acacia", such as the whistling thorn and bullhorn acacia. The ants nest inside the plant's thorns. In exchange for shelter, the ants protect acacias from attack by herbivores (which they frequently eat, introducing a resource component to this service-service relationship) and competition from other plants by trimming back vegetation that would shade the acacia.
In addition, another service-resource component is present, as the ants regularly feed on lipid-rich food-bodies called Beltian bodies that are on the "Acacia" plant.
In the neotropics, the ant, "Myrmelachista schumanni" makes its nest in special cavities in "Duroia hirsute". Plants in the vicinity that belong to other species are killed with formic acid. This selective gardening can be so aggressive that small areas of the rainforest are dominated by "Duroia hirsute". These peculiar patches are known by local people as "devil's gardens".
In some of these relationships, the cost of the ant’s protection can be quite expensive. "Cordia" sp. trees in the Amazonian rainforest have a kind of partnership with "Allomerus" sp. ants, which make their nests in modified leaves. To increase the amount of living space available, the ants will destroy the tree’s flower buds. The flowers die and leaves develop instead, providing the ants with more dwellings. Another type of "Allomerus" sp. ant lives with the "Hirtella" sp. tree in the same forests, but in this relationship the tree has turned the tables on the ants. When the tree is ready to produce flowers, the ant abodes on certain branches begin to wither and shrink, forcing the occupants to flee, leaving the tree’s flowers to develop free from ant attack.
The term "species group" can be used to describe the manner in which individual organisms group together. In this non-taxonomic context one can refer to "same-species groups" and "mixed-species groups." While same-species groups are the norm, examples of mixed-species groups abound. For example, zebra ("Equus burchelli") and wildebeest ("Connochaetes taurinus") can remain in association during periods of long distance migration across the Serengeti as a strategy for thwarting predators. "Cercopithecus mitis" and "Cercopithecus ascanius", species of monkey in the Kakamega Forest of Kenya, can stay in close proximity and travel along exactly the same routes through the forest for periods of up to 12 hours. These mixed-species groups cannot be explained by the coincidence of sharing the same habitat. Rather, they are created by the active behavioural choice of at least one of the species in question.
Humans and mutualism.
Humans also engage in mutualisms with other species, including their gut flora without which they would not be able to digest food efficiently. Apparently, head lice infestations might have been beneficial for humans by fostering an immune response that helps to reduce the threat of body louse borne lethal diseases.
Some relationships between humans and domesticated animals and plants are to different degrees mutualistic. Agricultural varieties of maize are unable to reproduce without human intervention because the leafy sheath does not fall open, and the seedhead (the "corn on the cob") does not shatter to scatter the seeds naturally.
In traditional agriculture, some plants have mutualist as companion plants, providing each other with shelter, soil fertility and/or natural pest control. For example, beans may grow up cornstalks as a trellis, while fixing nitrogen in the soil for the corn, a phenomenon that is used in Three Sisters farming.
Boran people of Ethiopia and Kenya traditionally use a whistle to call the honey guide bird, though the practice is declining. If the bird is hungry and within earshot, it guides them to a bees' nest. In exchange the Borans leave some food from the nest for the bird.
A population of bottlenose dolphins in Laguna, Brazil coordinates, via body language, with local net-using fishermen in order for both to catch schools of mullet.
Pat Shipman, a retired professor of anthropology has argued for the theory that a key advantage that Homo Sapiens had over Neanderthals in competing for similar habitats was the former's mutualism with dogs.
Mathematical modeling.
One of the simplest frameworks for modeling species interactions is the Lotka–Volterra equations. In this model, the change in population density of the two mutualists is quantified as:
where
Mutualism is in essence the logistic growth equation + mutualistic interaction. The mutualistic interaction term represents the increase in population growth of species one as a result of the presence of greater numbers of species two, and vice versa.
In 1989, David Hamilton Wright modified the Lotka–Volterra equations by adding a new term, "βM"/"K", to represent a mutualistic relationship. Wright also considered the concept of saturation, which means that with higher densities, there are decreasing benefits of further increases of the mutualist population. Without saturation, species' densities would increase indefinitely. Because that isn't possible due to environmental constraints and carrying capacity, a model that includes saturation would be more accurate. Wright's mathematical theory is based on the premise of a simple two-species mutualism model in which the benefits of mutualism become saturated due to limits posed by handling time. Wright defines handling time as the time needed to process a food item, from the initial interaction to the start of a search for new food items and assumes that processing of food and searching for food are mutually exclusive. Mutualists that display foraging behavior are exposed to the restrictions on handling time. Mutualism can be associated with symbiosis
Type II functional response.
In 1959, C. S. Holling performed his classic disc experiment that assumed the following: that (1), the number of food items captured is proportional to the allotted searching time; and (2), that there is a variable of handling time that exists separately from the notion of search time. He then developed an equation for the Type II functional response, which showed that the feeding rate is equivalent to
where,
The equation that incorporates Type II functional response and mutualism is:
where
Rearranged:
The model presented above is most effectively applied to free-living species that encounter a number of individuals of the mutualist part in the course of their existences. Of note, as Wright points out, is that models of biological mutualism tend to be similar qualitatively, in that the featured isoclines generally have a positive decreasing slope, and by and large similar isocline diagrams. Mutualistic interactions are best visualized as positively sloped isoclines, which can be explained by the fact that the saturation of benefits accorded to mutualism or restrictions posed by outside factors contribute to a decreasing slope.
The structure of mutualistic networks.
Mutualistic networks made up out of the interaction between plants and pollinators were found to have a similar structure in very different ecosystems on different continents, consisting of entirely different species. The structure of these mutualistic networks may have large consequences for the way in which pollinator communities respond to increasingly harsh conditions.
Mathematical models, examining the consequences of this network structure for the stability of pollinator communities suggest that the specific way in which plant-pollinator networks are organized minimizes competition between pollinators and may even lead to strong indirect facilitation between pollinators when conditions are harsh. This makes that pollinator species together can survive under harsh conditions. But it also means that pollinator species collapse simultaneously when conditions pass a critical point. This simultaneous collapse occurs, because pollinator species depend on each other when surviving under difficult conditions.
Such a community-wide collapse, involving many pollinator species, can occur suddenly when increasingly harsh conditions pass a critical point and recovery from such a collapse might not be easy. The improvement in conditions needed for pollinators to recover, could be substantially larger than the improvement needed to return to conditions at which the pollinator community collapsed.
Further references.
</dl>

</doc>
<doc id="51485" url="http://en.wikipedia.org/wiki?curid=51485" title="German Workers' Party">
German Workers' Party

The German Workers' Party (German: "Deutsche Arbeiterpartei", DAP) was the short-lived predecessor of the Nazi Party (German: "Nationalsozialistische Deutsche Arbeiterpartei", NSDAP).
Origins.
The DAP was founded in Munich in the hotel "Fürstenfelder Hof" on January 5, 1919 by Anton Drexler. It developed out of the "Freier Arbeiterausschuss für einen guten Frieden" (Free Workers' Committee for a good Peace) league, a branch of which Drexler had founded in 1918. Thereafter in 1918, Karl Harrer (a journalist and member of the Thule Society), along with Drexler and several others formed the "Politischer Arbeiterzirkel" (Political Workers' Circle), as well. The members met periodically for discussions with themes of nationalism and racism directed against the Jews. Drexler was encouraged to form the DAP in December 1918 by his mentor, Dr. Paul Tafel. Tafel was a leader of the Alldeutscher Verband (Pan-Germanist Union), a director of the Maschinenfabrik Augsburg-Nürnberg, and a member of the Thule Society. Drexler's wish was for a political party which was both in touch with the masses and nationalist. In January 1919 with the DAP founding, Drexler was elected chairman and Harrer was made "Reich Chairman", an honorary title. On May 17, only ten members were present at the meeting; a later meeting in August only noted 38 members attending.
Adolf Hitler's membership.
After World War I ended, Adolf Hitler returned to Munich. Having no formal education and career prospects, he tried to remain in the army for as long as possible. In July 1919 he was appointed "Verbindungsmann" (intelligence agent) of an "Aufklärungskommando" (reconnaissance commando) of the "Reichswehr", to influence other soldiers and to infiltrate the German Workers' Party (DAP). While monitoring the activities of the DAP, Hitler became attracted to the founder Anton Drexler's anti-Semitic, nationalist, anti-capitalist, and anti-Marxist ideas. While attending a party meeting at the "Sterneckerbräu" beer hall on September 12, 1919, Hitler became involved in a heated political argument with a visitor, a Professor Baumann, who questioned the soundness of Gottfried Feder's arguments against capitalism and proposed that Bavaria should break away from Prussia and found a new South German nation with Austria. In vehemently attacking the man's arguments he made an impression on the other party members with his oratory skills and, according to Hitler, the "professor" left the hall acknowledging unequivocal defeat. Impressed with Hitler, Drexler invited him to join the DAP. Hitler accepted on September 12, 1919, becoming the party's 55th member.
In less than a week, Hitler received a postcard from Drexler stating he had officially been accepted as a DAP member and he should come to a "committee" meeting to discuss it. Hitler attended the "committee" meeting held at the run-down Altes Rosenbad beer-house. Normally, enlisted army personnel were not allowed to join political parties. However in this case, Hitler had Captain Karl Mayr's permission to join the DAP. Further, Hitler was allowed to stay in the army and receive his weekly pay of 20 gold marks a week. At the time when Hitler joined the party there were no membership numbers or cards. It was on January 1920 when a numeration was issued for the first time: listed in alphabetical order, Hitler received the number 555. In reality he had been the 55th member, but the counting started at the number 501 in order to make the party appear larger. Hitler, in his work "Mein Kampf", later claimed to be the seventh party member (he was in fact the seventh executive member of the Party's central committee). After giving his first speech for the DAP on October 16 at the "Hofbräukeller", Hitler quickly became the party's most active orator. Hitler's considerable oratory and propaganda skills were appreciated by the party leadership as crowds began to "flock" to hear his speeches during 1919 and 1920. With the support of Anton Drexler, Hitler became chief of propaganda for the party in early 1920. Hitler preferred that role as he saw himself as the drummer for a national cause. He saw propaganda as the way to bring nationalism to the public.
From DAP to NSDAP.
The small number of party members were quickly won over to Hitler's political beliefs. He organised their biggest meeting yet of 2,000 people, for February 24, 1920 in the "Staatliches Hofbräuhaus in München". Further in an attempt to make the party more broadly appealing to larger segments of the population, the DAP was renamed the National Socialist German Workers' Party on February 24. Such was the significance of Hitler's particular move in publicity that Karl Harrer resigned from the party in disagreement. The new name was borrowed from a different Austrian party active at the time (Deutsche Nationalsozialistische Arbeiterpartei, German National Socialist Workers' Party), although Hitler earlier suggested the party to be renamed the "Social Revolutionary Party"; it was Rudolf Jung who persuaded Hitler to follow the NSDAP naming.
Membership.
Hitler was an early member of the party; the following are well-known early members:

</doc>
<doc id="51487" url="http://en.wikipedia.org/wiki?curid=51487" title="Salvador Allende">
Salvador Allende

 
Salvador Guillermo Allende Gossens (]; 26 June 1908 – 11 September 1973) was a Chilean physician and politician, known as the first Marxist to become president of a Latin American country through open elections.
Allende's involvement in Chilean political life spanned a period of nearly forty years. As a member of the Socialist Party, he was a senator, deputy and cabinet minister. He unsuccessfully ran for the presidency in the 1952, 1958, and 1964 elections. In 1970, he won the presidency in a close three-way race. He was elected in a run-off by Congress as no candidate had gained a majority.
As president, Allende adopted a policy of nationalization of industries and collectivization; due to these and other factors, increasingly strained relations between him and the legislative and judicial branches of the Chilean government – who did not share his enthusiasm for socialization – culminated in a declaration of a "constitutional breakdown" by the congress. A centre-right majority including the Christian Democrats, whose support had enabled Allende's election, denounced his rule as unconstitutional and called for his overthrow by force. On 11 September 1973 the military moved to oust Allende in a "coup d'état" sponsored by the United States Central Intelligence Agency. As troops surrounded La Moneda Palace, Allende gave his last speech vowing not to resign. He died later that day in uncertain and controversial circumstances.
Following Allende's deposition, army General Augusto Pinochet declined to return authority to the civilian government; and Chile became ruled by a military junta that was in power from 1973 to 1990, ending almost 41 years of Chilean democratic rule. The military junta that took over dissolved the Congress of Chile and began a persecution of alleged dissidents, in which thousands of Allende's supporters were murdered.
Early life.
Allende was born on 26 June 1908 in Santiago. He was the son of Salvador Allende Castro and Laura Gossens Uribe. Allende's family belonged to the Chilean upper class and had a long tradition of political involvement in progressive and liberal causes. His grandfather was a prominent physician and a social reformist who founded one of the first secular schools in Chile. Salvador Allende was of Belgian and Basque descent.
Allende attended high school at the Liceo Eduardo de la Barra in Valparaíso. As a teenager, his main intellectual and political influence came from the shoe-maker Juan De Marchi, an Italian-born anarchist. Allende was a talented athlete in his youth, being a member of the Everton de Viña del Mar sports club (named after the more famous English football club of the same name), where he is said to have excelled at the long jump. Allende then graduated with a medical degree in 1933 from the University of Chile. During the medical school Allende was influenced by Professor Max Westenhofer, a German pathologist who stressed on the social determinants of disease and social medicine
He co-founded a section of the Socialist Party of Chile (founded in 1933 with Marmaduque Grove and others) in Valparaíso and became its chairman. He married Hortensia Bussi with whom he had three daughters. He was a Freemason, a member of the Lodge Progreso No. 4 in Valparaíso. In 1933, he published his doctoral thesis "Higiene Mental y Delincuencia" (Crime and Mental Hygiene) in which he criticized Cesare Lombroso's proposals.
In 1938, Allende was in charge of the electoral campaign of the Popular Front headed by Pedro Aguirre Cerda. The Popular Front's slogan was "Bread, a Roof and Work!" After its electoral victory, he became Minister of Health in the Reformist Popular Front government which was dominated by the Radicals. While serving in this position, Allende was responsible for the passage of a wide range of progressive social reforms, including safety laws protecting workers in the factories, higher pensions for widows, maternity care, and free lunch programmes for schoolchildren.
Upon entering the government, Allende relinquished his congressional seat for Valparaíso, which he had won in 1937. Around that time, he wrote "La Realidad Médico Social de Chile" ("The social and medical reality of Chile"). After the Kristallnacht in Nazi Germany, Allende and other members of the Congress sent a telegram to Adolf Hitler denouncing the persecution of Jews. Following President Aguirre Cerda's death in 1941, he was again elected deputy while the Popular Front was renamed Democratic Alliance.
In 1945, Allende became senator for the Valdivia, Llanquihue, Chiloé, Aisén and Magallanes provinces; then for Tarapacá and Antofagasta in 1953; for Aconcagua and Valparaíso in 1961; and once more for Chiloé, Aisén and Magallanes in 1969. He became president of the Chilean Senate in 1966. During the Fifties, Allende introduced legislation that established the Chilean national health service, the first program in the Americas to guarantee universal health care.
His three unsuccessful bids for the presidency (in the 1952, 1958 and 1964 elections) prompted Allende to joke that his epitaph would be "Here lies the next President of Chile." In 1952, as candidate for the "Frente de Acción Popular" (Popular Action Front, FRAP), he obtained only 5.4% of the votes, partly due to a division within socialist ranks over support for Carlos Ibáñez. In 1958, again as the FRAP candidate, Allende obtained 28.5% of the vote. This time, his defeat was attributed to votes lost to the populist Antonio Zamorano. In 1964, once more as the FRAP candidate, he lost again, polling 38.6% of the votes against 55.6% for Christian Democrat Eduardo Frei. As it became clear that the election would be a race between Allende and Frei, the political right – which initially had backed Radical Julio Durán– settled for Frei as "the lesser evil".
Relationship with the Chilean Communist Party.
Allende had a close relationship with the Chilean Communist Party from the beginning of his political career. On his fourth (and successful) bid for the presidency, the Communist Party appointed him as the alternate for its own candidate, the world-renowned poet Pablo Neruda.
During his presidential term, Allende took positions held by the Communists, in opposition to the views of the socialists. Some argue, however, that this was reversed at the end of his period in office.
Election.
Allende won the 1970 Chilean presidential election as leader of the Unidad Popular ("Popular Unity") coalition. On 4 September 1970, he obtained a narrow plurality of 36.2 percent to 34.9 percent over Jorge Alessandri, a former president, with 27.8 percent going to a third candidate (Radomiro Tomic) of the Christian Democratic Party (PDC), whose electoral platform was similar to Allende's. According to the Chilean Constitution of the time, if no presidential candidate obtained a majority of the popular vote, Congress would choose one of the two candidates with the highest number of votes as the winner. Tradition was for Congress to vote for the candidate with the highest popular vote, regardless of margin. Indeed, former president Jorge Alessandri had been elected in 1958 with only 31.6 percent of the popular vote, defeating Allende.
One month after the election, on 20 October, while the senate had still to reach a decision and negotiations were actively in place between the Christian Democrats and the Popular Unity, General René Schneider, Commander in Chief of the Chilean Army, was shot resisting a kidnap attempt by a group led by General Roberto Viaux. Hospitalized, he died of his wounds three days later, on 23 October. Viaux's kidnapping plan had been supported by the CIA, although the then U.S. National Security Advisor Henry Kissinger claims to have ordered the plans postponed at the last moment. Evidence points towards CIA director Richard Helms following orders directly from President Nixon to do whatever was necessary in order "to get rid of him", referring to Allende. Nixon handed over a blank check to Helms, which allowed him to use full discretion in ridding Chile of Allende’s presence and "making the economy scream". Schneider was a defender of the "constitutionalist" doctrine that the army's role is exclusively professional, its mission being to protect the country's sovereignty and not to interfere in politics.
General Schneider's death was widely disapproved of and, for the time, ended military opposition to Allende, whom the congress finally chose on 24 October. On 26 October, President Eduardo Frei named General Carlos Prats as commander in chief of the army to replace René Schneider.
Allende assumed the presidency on 3 November 1970 after signing a "Statute of Constitutional Guarantees" proposed by the Christian Democrats in return for their support in Congress. In an extensive interview with Régis Debray in 1972, Allende explained his reasons for agreeing to the guarantees. Some critics have interpreted Allende's responses as an admission that signing the "Statute" was only a tactical move.
Presidency.
Upon assuming power, Allende began to carry out his platform of implementing a socialist programme called "La vía chilena al socialismo" ("the Chilean Path to Socialism"). This included nationalization of large-scale industries (notably copper mining and banking), and government administration of the health care system, educational system (with the help of a U.S. educator, Jane A. Hobson-Gonzalez from Kokomo, Indiana), a programme of free milk for children in the schools and shanty towns of Chile, and an expansion of the land seizure and redistribution already begun under his predecessor Eduardo Frei Montalva, who had nationalized between one-fifth and one-quarter of all the properties listed for takeover. Allende also intended to improve the socio-economic welfare of Chile's poorest citizens; a key element was to provide employment, either in the new nationalized enterprises or on public work projects.
In November 1970, 3,000 scholarships were allocated to Mapuches children in an effort to integrate the Indian minority into the educational system, payment of pensions and grants was resumed, an emergency plan providing for the construction of 120,000 residential buildings was launched, all part-time workers were granted rights to social security, a proposed electricity price increase was withdrawn, diplomatic relations were restored with Cuba, and political prisoners were granted an amnesty. In December that same year, bread prices were fixed, 55,000 volunteers were sent to the south of the country to teach writing and reading skills and provide medical attention to a sector of the population that had previously been ignored, a central commission was established to oversee a tri-partite payment plan in which equal place was given to government, employees and employers, and a protocol agreement was signed with the United Centre of Workers which granted workers representational rights on the funding board of the Social Planning Ministry. An obligatory minimum wage for workers of all ages (including apprentices) was established, free milk was introduced for expectant and nursing mothers and for children between the ages of 7 and 14, free school meals were established, rent reductions were carried out, and the construction of the Santiago subway was rescheduled so as to serve working-class neighbourhoods first. Workers benefited from increases in social security payments, an expanded public works program, and a modification of the wage and salary adjustment mechanism (which had originally been introduced in the Forties to cope with the country’s permanent inflation), while middle-class Chileans benefited from the elimination of taxes on modest incomes and property. In addition, state-sponsored programs distributed free food to the country’s neediest citizens, and in the countryside, peasant councils were established to mobilise agrarian workers and small proprietors. In the government’s first budget (presented to the Chilean congress in November 1970), the minimum taxable income level was raised, removing from the tax pool 35% of those who had paid taxes on earnings in the previous year. In addition, the exemption from general taxation was raised to a level equivalent to twice the minimum wage. Exemptions from capital taxes were also extended, which benefitted 330,000 small proprietors. The extra increases that Frei promised to the armed forces were also fully paid. According to one estimate, purchasing power went up by 28% between October 1970 and July 1971.
The rate of inflation fell from 36.1% in 1970 to 22.1% in 1971, while average real wages rose by 22.3% during 1971. Minimum real wages for blue-collar workers were increased by 56% during the first quarter of 1971, while in the same period real minimum wages for white-collar workers were increased by 23%, a development that decreased the differential ratio between blue- and white-collar workers’ minimum wage from 49% (1970) to 35% (1971). Central government expenditures went up by 36% in real terms, raising the share of fiscal spending in GDP from 21% (1970) to 27% (1971), and as part of this expansion, the public sector engaged in a huge housing program, starting to build 76,000 houses in 1971, compared to 24,000 for 1970. During a 1971 emergency program, over 89,000 houses were built, and during Allende’s three years as president an average of 52,000 houses were constructed annually. Although the acceleration of inflation in 1972 and 1973 eroded part of the initial increase in wages, they still rose (on average) in real terms during the 1971–73 period.
Allende’s first step in early 1971 was to raise minimum wages (in real terms) for blue-collar workers by 37%–41% and 8%–10% for white-collar workers. Educational, food, and housing assistance was significantly expanded, with public-housing starts going up twelvefold and eligibility for free milk extended from age 6 to age 15. A year later, blue-collar wages were raised by 27% in real terms and white-collar wages became fully indexed. Price controls were also set up, while the Allende Government introduced a system of distribution networks through various agencies (including local committees on supply and prices) to ensure that the new rules were adhered to by shopkeepers.
The new Minister of Agriculture, Jacques Chonchol, promised to expropriate all estates which were larger than eighty "basic" hectares. This promise was kept, with no farm in Chile exceeding this limit by the end of 1972. Within eighteen months, the Latifundia (extensive agricultural estates) had been abolished. The agrarian reform had involved the expropriation of 3,479 properties which, added to the 1,408 properties incorporated under the Frei Government, made up some 40% of the total agricultural land area in the country.
Particularly in rural areas, the Allende Government launched a campaign against illiteracy, while adult education programs expanded, together with educational opportunities for workers. From 1971 through to 1973, enrollments in kindergarten, primary, secondary, and postsecondary schools all increased. The Allende Government encouraged more doctors to begin their practices in rural and low-income urban areas, and built additional hospitals, maternity clinics, and especially neighborhood health centers that remained open longer hours to serve the poor. Improved sanitation and housing facilities for low-income neighborhoods also equalized health care benefits, while hospital councils and local health councils were established in neighborhood health centers as a means of democratizing the administration of health policies. These councils gave central government civil servants, local government officials, health service employees, and community workers the right to review budgetary decisions.
The Allende government also sought to bring the arts (both serious and popular) to the mass of the Chilean population by funding a number of cultural endeavours. With eighteen-year-olds and illiterates now granted the right to vote, mass participation in decision-making was encouraged by the Allende government, with traditional hierarchical structures now challenged by socialist egalitarianism. The Allende Government was able to draw upon the idealism of its supporters, with teams of "Allendistas" travelling into the countryside and shanty towns to perform volunteer work. The Allende Government also worked to transform Chilean popular culture through formal changes to school curriculum and through broader cultural education initiatives, such as state-sponsored music festivals and tours of Chilean folklorists and nueva canción musicians. In 1971, the purchase of a private publishing house by the state gave rise to "Editorial Quimantu," which became the center of the Allende Government’s cultural activities. In the space of 2 years, 12 million copies of books, magazines, and documents (8 million of which were books) specializing in social analysis, were published. Cheap editions of great literary works were produced on a weekly basis, and in most cases were sold out within a day. Culture came into the reach of the masses for the first time, who responded enthusiastically. "Editorial Quimantu" encouraged the establishment of libraries in community organizations and trade unions. Through the supply of cheap textbooks, it enabled the Left to progress through the ideological content of the literature made available to workers.
To improve social and economic conditions for women, the Women’s Secretariat was established in 1971, which took on issues such as public laundry facilities, public food programs, day-care centers, and women’s health care (especially prenatal care). The duration of maternity leave was extended from 6 to 12 weeks, while the Allende Government veered the educational system towards poorer Chileans by expanding enrollments through government subsidies. A "democratisation" of university education was carried out, making the system tuition-free. This led to an 89% rise in university enrollments between 1970 and 1973. The Allende Government also increased enrollment in secondary education from 38% in 1970 to 51% in 1974. Enrollment in education reached record levels, including 3,600,000 young people, and 8 million school textbooks were distributed among 2,600,000 pupils in primary education. An unprecedented 130,000 students were enrolled by the universities, which became accessible to peasants and workers. The illiteracy rate was reduced from 12% in 1970 to 10.8% in 1972, while the growth enrollment in primary school enrollment increased from an annual average of 3.4% in the period 1966–70 to 6.5% in 1971/72. Secondary education grew at a rate of 18.2% in 1971/72, and the average school enrollment of children between the ages of 6 and 14 rose from 91% (1966–70) to 99.%
Social spending was dramatically increased, particularly for housing, education, and health, while a major effort was made to redistribute wealth to poorer Chileans. As a result of new initiatives in nutrition and health, together with higher wages, many poorer Chileans were able to feed themselves and clothe themselves better than they had been able to before. Public access to the social security system was increased, while state benefits such as family allowances were raised significantly. The redistribution of income enabled wage and salary earners to increase their share of national income from 51.6% (the annual average between 1965 and 1970) to 65% while family consumption increased by 12.9% in the first year of the Allende Government. In addition, while the average annual increase in personal spending had been 4.8% in the period 1965–70, it reached 11.9% in 1971. During the first two years of Allende’s presidency, state expenditure on health rose from around 2% to nearly 3.5% of GDP. According to Jennifer E. Pribble, this new spending “was reflected not only in public health campaigns, but also in the construction of health infrastructure.” Small programs targeted at women were also experimented with, such as cooperative laundries and communal food preparation, together with an expansion of child-care facilities.
The National Supplementary Food Program was extended to all primary school and to all pregnant women, regardless of their employment or income condition. Complementary nutritional schemes were applied to malnourished children, while antenatal care was emphasized. Under Allende, the proportion of children under the age of 6 with some form of malnutrition fell by 17%. Apart from the existing Supply and Prices councils (community-based bodies which controlled the distribution of essential groups in working-class districts, and were a popular, not government, initiative), community-based distribution centers and shops were developed, which sold directly in working-class neighborhoods. The Allende Government felt obliged to increase its intervention in marketing activities, and state involvement in grocery distribution reached 33%. The CUT (central labor confederation) was accorded legal recognition, and its membership grew from 700,000 to almost 1 million. In enterprises in the Area of Social Ownership, an assembly of the workers elected half of the members of the management council for each company. These bodies replaced the former board of directors.
Minimum pensions were increased by amounts equal to two or three times the inflation rate, and between 1970 and 1972, such pensions increased by a total of 550%. The incomes of 300,000 retirement pensioners were increased by the government from one-third of the minimum salary to the full amount. Labor insurance cover was extended to 200,000 market traders, 130,000 small shop proprietors, 30,000 small industrialists, small owners, transport workers, clergy, professional sportsmen, and artesans. The public health service was improved, with the establishment of a system of clinics in working-class neighborhoods on the peripheries of the major cities, providing a health center for every 40,000 inhabitants. Statistics for construction in general, and house-building in particular, reached some of the highest levels in the history of Chile. Four million square metres were completed in 1971–72, compared to an annual average of two-and-a-half million between 1965 and 1970. Workers were able to acquire goods which had previously been beyond their reach, such as heaters, refrigerators, and television sets. As further noted by Ricardo Israel Zipper,
"By now meat was no longer a luxury, and the children of working people were adequately supplied with shoes and clothing. The popular living standards were improved in terms of the employment situation, social services, consumption levels, and income distribution."
Chilean presidents were allowed a maximum term of six years, which may explain Allende's haste to restructure the economy. Not only was a major restructuring program organized (the Vuskovic Plan), he had to make it a success if a socialist successor to Allende was going to be elected. In the first year of Allende's term, the short-term economic results of Minister of the Economy Pedro Vuskovic's expansive monetary policy were highly favorable: 12% industrial growth and an 8.6% increase in GDP, accompanied by major declines in inflation (down from 34.9% to 22.1%) and unemployment (down to 3.8%). However by 1972, the Chilean "escudo" had an inflation rate of 140%. The average Real GDP contracted between 1971 and 1973 at an annual rate of 5.6% ("negative growth"); and the government's fiscal deficit soared while foreign reserves declined. The combination of inflation and government-mandated price-fixing, together with the "disappearance" of basic commodities from supermarket shelves, led to the rise of black markets in rice, beans, sugar, and flour. The Chilean economy also suffered as a result of a US campaign against the Allende government.
The Allende government announced it would default on debts owed to international creditors and foreign governments. Allende also froze all prices while raising salaries. His implementation of these policies was strongly opposed by landowners, employers, businessmen and transporters associations, and some civil servants and professional unions. The rightist opposition was led by the National Party, the Roman Catholic Church (which in 1973 was displeased with the direction of educational policy), and eventually the Christian Democrats. There were growing tensions with foreign multinational corporations and the government of the United States.
Allende also undertook Project Cybersyn, a system of networked telex machines and computers. Cybersyn was developed by British cybernetics expert Stafford Beer. The network was supposed to transmit data from factories to the government in Santiago, allowing for economic planning in real time.
In 1971, Chile re-established diplomatic relations with Cuba, joining Mexico and Canada in rejecting a previously established Organization of American States convention prohibiting governments in the Western Hemisphere from establishing diplomatic relations with Cuba. Shortly afterward, Cuban president Fidel Castro made a month-long visit to Chile. Originally the visit was supposed to be one week; however, Castro enjoyed Chile and one week led to another.
In October 1972, the first of what were to be a wave of strikes was led first by truckers, and later by small businessmen, some (mostly professional) unions and some student groups. Other than the inevitable damage to the economy, the chief effect of the 24-day strike was to induce Allende to bring the head of the army, general Carlos Prats, into the government as Interior Minister. Allende also instructed the government to begin requisitioning trucks in order to keep the nation from coming to a halt. Government supporters also helped to mobilize trucks and buses but violence served as a deterrent to full mobilization, even with police protection for the strike-breakers. Allende's actions were eventually declared unlawful by the Chilean appeals court and the government was ordered to return trucks to their owners.
Throughout this presidency racial tensions between the poor descendants of indigenous people, who supported Allende's reforms, and the white settler elite increased.
Allende raised wages on a number of occasions throughout 1970 and 1971, but these wage hikes were negated by the in-tandem inflation of Chile's fiat currency. Although price rises had also been high under Frei (27% a year between 1967 and 1970), a basic basket of consumer goods rose by 120% from 190 to 421 escudos in one month alone, August 1972. In the period 1970–72, while Allende was in government, exports fell 24% and imports rose 26%, with imports of food rising an estimated 149%.
Export income fell due to a hard-hit copper industry: the price of copper on international markets fell by almost a third, and post-nationalization copper production fell as well. Copper is Chile's single most important export (more than half of Chile's export receipts were from this sole commodity). The price of copper fell from a peak of $66 per ton in 1970 to only $48–9 in 1971 and 1972. Chile was already dependent on food imports, and this decline in export earnings coincided with declines in domestic food production following Allende's agrarian reforms.
Throughout his presidency, Allende remained at odds with the Chilean Congress, which was dominated by the Christian Democratic Party. The Christian Democrats (who had campaigned on a socialist platform in the 1970 elections, but drifted away from those positions during Allende's presidency, eventually forming a coalition with the National Party), continued to accuse Allende of leading Chile toward a Cuban-style dictatorship, and sought to overturn many of his more radical policies. Allende and his opponents in Congress repeatedly accused each other of undermining the Chilean Constitution and acting undemocratically.
Allende's increasingly bold socialist policies (partly in response to pressure from some of the more radical members within his coalition), combined with his close contacts with Cuba, heightened fears in Washington. The Nixon administration continued exerting economic pressure on Chile via multilateral organizations, and continued to back Allende's opponents in the Chilean Congress. Almost immediately after his election, Nixon directed CIA and U.S. State Department officials to "put pressure" on the Allende government.
Foreign relations during Allende's presidency.
Allende's Popular Unity government tried to maintain normal relations with the United States. When Chile nationalized its copper industry, Washington cut off U.S. credits and increased its support to opposition. Forced to seek alternative sources of trade and finance, Chile gained commitments from the Soviet Union to invest some $400 million in Chile in the next six years. Allende's government was disappointed that it received far less economic assistance from the USSR than it hoped for. Trade between the two countries did not significantly increase and the credits were mainly linked to the purchase of Soviet equipment. Moreover, credits from the Soviet Union were much less than those provided to the People's Republic of China and countries of Eastern Europe. When Allende visited the USSR in late 1972 in search of more aid and additional lines of credit, after 3 years, he was turned down.
Foreign involvement in Chile during Allende's presidency.
US involvement.
Declassified documents related to the military coup have shown that although the CIA didn't "instigate" the 1973 coup, they were well aware of it and knew about it in advance. However, the US refused to "provide any assistance" because it was "strictly an internal Chilean matter." According to CIA documents, the United States "probably appeared to condone [the coup]," considering their intelligence collection and active participation in positively slanting propaganda in 1974 to place Pinochet and his military government in a positive light.
The possibility of Allende winning Chile's 1970 election was deemed a disaster by a US administration that wanted to protect US geopolitical interests by preventing the spread of Communism during the Cold War. In September 1970, President Nixon informed the CIA that an Allende government in Chile would not be acceptable and authorized $10 million to stop Allende from coming to power or unseat him. Henry Kissinger's 40 Committee and the CIA planned to impede Allende's investiture as President of Chile with covert efforts known as "Track I" and "Track II"; Track I sought to prevent Allende from assuming power via so-called "parliamentary trickery", while under the Track II initiative, the CIA tried to convince key Chilean military officers to carry out a coup.
Additionally, some point to the involvement of the Defense Intelligence Agency agents that allegedly secured the missiles used to bombard La Moneda Palace. In fact, open US military aid to Chile continued during the Allende administration, and the national government was very much aware of this, although there is no record that Allende himself believed that such assistance was anything but beneficial to Chile.
During Nixon's presidency, U.S. officials attempted to prevent Allende's election by financing political parties aligned with opposition candidate Jorge Alessandri and supporting strikes in the mining and transportation sectors. After the 1970 election, the Track I operation attempted to incite Chile's outgoing president, Eduardo Frei Montalva, to persuade his party (PDC) to vote in Congress for Alessandri. Under the plan, Alessandri would resign his office immediately after assuming it and call new elections. Eduardo Frei would then be constitutionally able to run again (since the Chilean Constitution did not allow a president to hold two consecutive terms, but allowed multiple non-consecutive ones), and presumably easily defeat Allende. The Chilean Congress instead chose Allende as President, on the condition that he would sign a "Statute of Constitutional Guarantees" affirming that he would respect and obey the Chilean Constitution and that his reforms would not undermine any of its elements.
Track II was aborted, as parallel initiatives already underway within the Chilean military rendered it moot.
During the second term of office of Democratic President Bill Clinton, the CIA acknowledged having played a role in Chilean politics before the coup, but its degree of involvement is debated. The CIA was notified by its Chilean contacts of the impending coup two days in advance but contends it "played no direct role in" the coup.
Much of the internal opposition to Allende's policies came from the business sector, and recently released U.S. government documents confirm that the U.S. indirectly funded the truck drivers' strike, which exacerbated the already chaotic economic situation before the coup.
The most prominent U.S. corporations in Chile before Allende's presidency were the Anaconda and Kennecott copper companies and ITT Corporation, International Telephone and Telegraph. Both copper corporations aimed to expand privatized copper production in the city of El Teniente in the Chilean Andes, the world's largest underground copper mine. At the end of 1968, according to US Department of Commerce data, U.S. corporate holdings in Chile amounted to $964 million. Anaconda and Kennecott accounted for 28% of U.S. holdings, but ITT had by far the largest holding of any single corporation, with an investment of $200 million in Chile. In 1970, before Allende was elected, ITT owned 70% of Chitelco, the Chilean Telephone Company and funded El Mercurio, a Chilean right-wing newspaper. Documents released in 2000 by the CIA confirmed that before the elections of 1970, ITT gave $700,000 to Allende's conservative opponent, Jorge Alessandri, with help from the CIA on how to channel the money safely. ITT president Harold Geneen also offered $1 million to the CIA to help defeat Allende in the elections.
After General Pinochet assumed power, United States Secretary of State Henry Kissinger told President Richard Nixon that the U.S. "didn't do it," but "we helped them...created the conditions as great as possible." (referring to the coup itself). Recent documents declassified under the Clinton administration's Chile Declassification Project show that the United States government and the CIA sought to overthrow Allende in 1970 immediately before he took office ("Project FUBELT"). Many documents regarding the U.S. intervention in Chile remain classified.
Soviet involvement.
Material based on reports from the Mitrokhin Archive, the KGB said of Allende that "he was made to understand the necessity of reorganising Chile's army and intelligence services, and of setting up a relationship between Chile's and the USSR's intelligence services". It is also claimed that Allende was given $30,000 "in order to solidify the trusted relations" with him. According to Vasili Mitrokhin, a former KGB major and senior archivist in the KGB intelligence central KGB office in the Yasenevo area of Moscow, Allende made a personal request for Soviet money through his personal contact, KGB officer Svyatoslav Kuznetsov (codenamed LEONID), who urgently came to Chile from Mexico City to help Allende. The original allocation of money for these elections through the KGB was $400,000, a personal subsidy of $50,000 was sent directly to Allende, with an additional $100,000 funneled through funds provided to the Chilean Communist Party.
Historian Christopher Andrew has argued that help from the KGB was a decisive factor, because Allende won by a narrow margin of 39,000 votes of a total of the 3 million cast. After the elections, the KGB director Yuri Andropov obtained permission for additional money and other resources from the Central Committee of the CPSU to ensure an Allende victory in Congress. In his request on 24 October, he stated that the KGB "will carry out measures designed to promote the consolidation of Allende's victory and his election to the post of President of the country". In his KGB file, Allende was reported to have "stated his willingness to co-operate on a confidential basis and provide any necessary assistance, since he considered himself a friend of the Soviet Union". He willingly shared political information.
Andrew writes that regular Soviet contact with Allende after his election was maintained by his KGB case officer, Svyatoslav Kuznetsov, who was instructed by KGB's the 'Centre' to "exert a favorable influence on Chilean government policy". Allende was said to have reacted "positively."
Political and moral support came mostly through the Communist Party and unions. For instance, Allende received the Lenin Peace Prize from the Soviet Union in 1972. However, there were some fundamental differences between Allende and Soviet political analysts who believed that some violence – or measures that those analysts "theoretically considered to be just" – should have been used. According to Andrew's account of the Mitrokhin archives, "In the KGB's view, Allende's fundamental error was his unwillingness to use force against his opponents. Without establishing complete control over all the machinery of the State, his hold on power could not be secure."
Declarations from KGB General Nikolai Leonov, former Deputy Chief of the First Chief Directorate of the KGB, confirmed that the Soviet Union supported Allende's government economically, politically and militarily. Leonov stated in an interview at the Chilean Center of Public Studies (CEP) that the Soviet economic support included over $100 million in credit, three fishing ships (that distributed 17,000 tons of frozen fish to the population), factories (as help after the 1971 earthquake), 3,100 tractors, 74,000 tons of wheat and more than a million tins of condensed milk.
In mid-1973 the USSR had approved the delivery of weapons (artillery, tanks) to the Chilean Army. However, when news of an attempt from the Army to depose Allende through a coup d'état reached Soviet officials, the shipment was redirected to another country.
Crisis.
On 29 June 1973, Colonel Roberto Souper surrounded the presidential palace, La Moneda, with his tank regiment but failed to depose the government. That failed "coup d’état" – known as the "Tanquetazo" ("tank putsch") – organised by the nationalist "Patria y Libertad" paramilitary group, was followed by a general strike at the end of July that included the copper miners of El Teniente.
In August 1973, a constitutional crisis occurred, and the Supreme Court of Chile publicly complained about the inability of Allende government to enforce the law of the land. On 22 August, the Chamber of Deputies (with the Christian Democrats uniting with the National Party) accused the government of unconstitutional acts through Allende's refusal to promulgate constitutional amendments, already approved by the Chamber, which would have prevented his government from continuing his massive nationalization plan and called upon the military to enforce constitutional order.
For months, Allende had feared calling upon the "Carabineros" ("Carabineers", the national police force), suspecting them of disloyalty to his government. On 9 August, President Allende appointed General Carlos Prats as Minister of Defence. On 24 August 1973, General Prats was forced to resign both as defense minister and as the commander-in-chief of the army, embarrassed by both the Alejandrina Cox incident and a public protest in front of his house by the wives of his generals. General Augusto Pinochet replaced him as Army commander-in-chief the same day.
According to Chilean political scientist Arturo Valenzuela (later becoming a U.S. citizen and Assistant Secretary of State for Hemispheric Affairs in the Obama administration), a greater share of the blame for the breakdown in Chilean democracy lay with the leftist Allende government. While each side increasingly distrusted the other, the extreme leftists accelerated the process and left less room for political moderation than the extreme rightists. He writes "By its actions, the revolutionary Left, which had always ridiculed the possibility of a socialist transformation through peaceful means, was engaged in a self-fulfilling prophecy."
Supreme Court's resolution.
On 26 May 1973, the Supreme Court of Chile unanimously denounced the Allende government's disruption of the legality of the nation in its failure to uphold judicial decisions, because of its continual refusal to permit police execution of judicial decisions contrary to the government's own measures.
Chamber of Deputies' resolution.
On 22 August 1973, the Christian Democrats and the National Party members of the Chamber of Deputies joined together to vote 81 to 47 in favor of a resolution that asked the authorities to "put an immediate end" to "breach[es of] the Constitution . . . with the goal of redirecting government activity toward the path of law and ensuring the Constitutional order of our Nation, and the essential underpinnings of democratic co-existence among Chileans."
The resolution declared that Allende's government sought "to conquer absolute power with the obvious purpose of subjecting all citizens to the strictest political and economic control by the State . . . [with] the goal of establishing . . . a totalitarian system" and claimed that the government had made "violations of the Constitution . . . a permanent system of conduct." Essentially, most of the accusations were about disregard by the Socialist government of the separation of powers, and arrogating legislative and judicial prerogatives to the executive branch of government.
Specifically, the Socialist government of President Allende was accused of:
Finally, the resolution condemned the creation and development of government-protected [socialist] armed groups, which were said to be "headed towards a confrontation with the armed forces". President Allende's efforts to re-organize the military and the police forces were characterized as "notorious attempts to use the armed and police forces for partisan ends, destroy their institutional hierarchy, and politically infiltrate their ranks".
President Allende's response.
Two days later, on 24 August 1973, President Allende responded, characterising the Congress's declaration as "destined to damage the country’s prestige abroad and create internal confusion", predicting "It will facilitate the seditious intention of certain sectors." He noted that the declaration (passed 81–47 in the Chamber of Deputies) had not obtained the two-thirds Senate majority "constitutionally required" to convict the president of abuse of power: essentially, the Congress were "invoking the intervention of the armed forces and of Order against a democratically-elected government" and "subordinat[ing] political representation of national sovereignty to the armed institutions, which neither can nor ought to assume either political functions or the representation of the popular will."
Allende argued he had obeyed constitutional means for including military men to the cabinet at the service of civic peace and national security, defending republican institutions against insurrection and terrorism. In contrast, he said that Congress was promoting a "coup d’état" or a civil war with a declaration full of affirmations that had already been refuted beforehand and which, in substance and process (directly handing it to the ministers rather than directly handing it to the President) violated a dozen articles of the (then-current) Constitution. He further argued that the legislature was usurping the government's executive function.
President Allende wrote: "Chilean democracy is a conquest by all of the people. It is neither the work nor the gift of the exploiting classes, and it will be defended by those who, with sacrifices accumulated over generations, have imposed it...With a tranquil conscience...I sustain that never before has Chile had a more democratic government than that over which I have the honor to preside...I solemnly reiterate my decision to develop democracy and a state of law to their ultimate consequences...Congress has made itself a bastion against the transformations...and has done everything it can to perturb the functioning of the finances and of the institutions, sterilizing all creative initiatives."
Adding that economic and political means would be needed to relieve the country's current crisis, and that the Congress were obstructing said means; having already paralyzed the State, they sought to destroy it. He concluded by calling upon the workers, all democrats and patriots to join him in defending the Chilean Constitution and the revolutionary process.
The coup.
In early September 1973, Allende floated the idea of resolving the constitutional crisis with a plebiscite. His speech outlining such a solution was scheduled for 11 September, but he was never able to deliver it. On 11 September 1973, the Chilean military staged a coup against Allende.
Death.
Just before the capture of La Moneda (the Presidential Palace), with gunfire and explosions clearly audible in the background, Allende gave his to Chileans on live radio, speaking of himself in the past tense, of his love for Chile and of his deep faith in its future. He stated that his commitment to Chile did not allow him to take an easy way out, and he would not be used as a propaganda tool by those he called "traitors" (he refused an offer of safe passage), clearly implying he intended to fight to the end.
Shortly afterwards, the coup plotters announced that Allende had committed suicide. An official announcement declared that the weapon he had used was an automatic rifle. Before his death he had been photographed several times holding an AK-47, a gift from Fidel Castro. He was found dead with this gun. In his 2004 documentary "Salvador Allende", Patricio Guzmán incorporates a graphic image of Allende's corpse in the position it was found after his death. According to Guzmán's documentary, Allende shot himself with a pistol and not a rifle.
In 2011, the controversy over the cause of death was reopened as the subject of an official investigation. On the basis of the original 1973 autopsy, Luis Ravanal, a medical examiner, expressed the opinion that the wounds on the body were not consistent with the theses of the alleged witnesses, as there were wounds caused by different guns. In January 2011, a Chilean judge ordered an inquiry, the first judicial investigation of the death. On 23 May 2011, Allende's body was exhumed in order to have an autopsy performed by an international forensic team. On 31 May, TVN, the state television station, reported the discovery of a secret 300-page military account of Allende's death. The document had been kept in the home of a former military justice official, and was discovered when his house was destroyed in the 2010 earthquake. After reviewing the report, two forensic experts told TVN "that they are inclined to conclude that Allende was assassinated." This belief is supported by forensic expert Luis Ravanal who has been studying Allende's autopsy since 2007. Ravanal says he found details in the autopsy that were not in line with the official version of Allende's death. The cranium, he says, shows evidence of a first shot with a small gun, like a pistol, and then, a second shot from a larger weapon — like an AK-47 — which could mean that Allende was shot and killed, then shot a second time with his own gun, to make it look like suicide. 
In May 2011 Allende's remains were exhumed by order of a Chilean court in furtherance of a "criminal investigation into the death of Allende and hundreds of other victims of the Pinochet dictatorship." Results of the autopsy were officially released in mid-July 2011. Citing Chile's Legal Medical Service, CNN reported that analysis of the exhumed remains had confirmed suicide as the cause of Allende's death. The Guardian, a leading UK newspaper, likewise reported that the "scientific autopsy" had confirmed that "Salvador Allende committed suicide during the 1973 coup that toppled his socialist government."
 British ballistics expert David Prayer said Allende died of two shots fired from an assault rifle that was held between his legs and under his chin and was set to fire automatically. The bullets blew out the top of his head and killed him instantly.
The forensics team's conclusion was unanimous. Spanish expert Francisco Etxeberria said: "We have absolutely no doubt" that Allende committed suicide.
Family.
Well-known relatives of Salvador Allende include his daughter Isabel Allende (a politician) and his second niece Isabel Allende (a writer).
Memorials.
Memorials to Allende include a statue in front of the Palacio de la Moneda. The placement of the statue was not without controversy, as it is located facing the eastern edge of the Plaza de la Ciudadanía, this plaza containing memorials to a number of Chilean heroes. However, the statue is not located in the plaza, but rather on an surrounding sidewalk and facing an entrance to the plaza.

</doc>
<doc id="51488" url="http://en.wikipedia.org/wiki?curid=51488" title="Rhone (disambiguation)">
Rhone (disambiguation)

Rhone can refer to:

</doc>
<doc id="51489" url="http://en.wikipedia.org/wiki?curid=51489" title="Poitiers">
Poitiers

Poitiers (]) is a city on the Clain river in west-central France. It is a commune and the capital of the Vienne department and also of the Poitou-Charentes region. Poitiers is a major university centre. The centre of town is picturesque and its streets include predominant historical architecture, especially religious architecture and especially from the Romanesque period. Two major military battles took place near the city: in 732, the Battle of Poitiers (also known as the Battle of Tours), in which the Franks commanded by Charles Martel halted the expansion of the Umayyad Caliphate, and in 1356, the Battle of Poitiers, a key victory for the English forces during the Hundred Years' War. This battle's consequences partly provoked the Jacquerie.
Geography.
Location.
The city of Poitiers is strategically situated on the Seuil du Poitou, a shallow gap between the Armorican and the Central Massif. The Seuil du Poitou connects the Aquitaine Basin to the South to the Paris Basin to the North. This area is an important geographic crossroads in France and Western Europe.
Situation.
Poitiers's primary site sits on a vast promontory between the valleys of the Boivre and the Clain. The old town occupies the slopes and the summit of a plateau which rises 130 ft above the streams which surround it on three sides. Thus Poitiers benefits from a very strong tactical situation. This was an especially important factor before and throughout the Middle Ages.
Inhabitants and demography.
Inhabitants of Poitiers are referred as Pictaviens (male) and Pictaviennes (female) from Pictavis, which was the ancient name for the town. It is not uncommon for inhabitants of Poitiers to call themselves Poitevins or Poitevines, although this denomination can be used for anyone from the Poitou province.
As of 2006, the population of Poitiers was 91,394. One out of three people in Poitiers is under the age of 30 and one out of four residents in Poitiers is a student.
Climate.
The climate in the Poitiers area is mild with mild temperature amplitudes, and adequate rainfall throughout the year. The Köppen Climate Classification subtype for this type of climate is "" (Marine West Coast Climate/Oceanic climate).
History.
Antiquity.
Poitiers was founded by the Celtic tribe of the Pictones and was known as the oppidum "Lemonum" before Roman influence. The name is said to have come from the Celtic word for elm, "Lemo". After Roman influence took over, the town became known as "Pictavium", or later "Pictavis", after the original Pictones inhabitants themselves.
There is a rich history of archeological finds from the Roman era in Poitiers. In fact until 1857 Poitiers hosted the ruins of a vast Roman amphitheatre, which was larger than that of Nîmes. Remains of Roman baths, built in the 1st century and demolished in the 3rd century, were uncovered in 1877.
In 1879 a burial-place and tombs of a number of Christian martyrs were discovered on the heights to the south-east of the town. The names of some of the Christians had been preserved in paintings and inscriptions. Not far from these tombs is a huge dolmen (the "Pierre Levée"), which is 22 ft long, 16 ft broad and 7 ft high, and around which used to be held the great fair of Saint Luke.
The Romans also built at least three aqueducts. This extensive ensemble of Roman constructions suggests Poitiers was a town of first importance, possibly even the capital of the Roman province of "Gallia Aquitania" during the 2nd century.
As Christianity was made official and gradually introduced across the Roman Empire during the 3rd and 4th centuries, the first bishop of Poitiers from 350 to 367, Hilary of Poitiers or Saint Hilarius, proceeded to evangelize the town. Exiled by Constantius II, he risked death to return to Poitiers as Bishop after discovering that the Christian "Eastern" Church were not heretical as believed in Rome, but had, rather, reached many of the same conclusions about the Holy Trinity as had the Western Church. The first foundations of the Baptistère Saint-Jean can be traced to that era of open Christian evangelization. He was named "Doctor of The Church" by Pope Pius IX.
In the 4th century, a thick wall 6m wide and 10m high was built around the town. It was 2.5 km long and stood lower on the naturally defended east side and at the top of the promontory. Around this time, the town began to be known as Poitiers.
Fifty years later Poitiers fell into the hands of the Arian Visigoths, and became one of the principal residences of their kings. Visigoth King Alaric II was defeated by Clovis I at Vouillé, not far from Poitiers, in 507, and the town thus came under Frankish dominion.
Middle Ages.
During most of the Early Middle Ages, the town of Poitiers took advantage of its defensive tactical site and of its location, which was far from the centre of Frankish power. As the seat for an "évêché" (bishop) since the 4th century, the town was a centre of some importance and the capital of the Poitou county. At the height of their power, the Counts of Poitiers governed a large domain, including both Aquitaine and Poitou.
The first decisive victory of a Christian army over a Muslim power, the Battle of Tours, was fought by Charles Martel's men in the vicinity of Poitiers on 10 October 732. For many historians, it was one of the world's pivotal moments.
Eleanor of Aquitaine frequently resided in the town, which she embellished and fortified, and in 1199 entrusted with communal rights.
During the Hundred Years' War, the Battle of Poitiers, an English victory, was fought near the town of Poitiers on 19 September 1356. Later in the war In 1418, under duress, the royal parliament moved from Paris to Poitiers, where it remained in exile until the Plantagenets finally withdrew from the capital in 1436. During this interval, in 1429 Poitiers was the site of Joan of Arc's formal inquest.
The University of Poitiers was founded in 1431. During and after the Reformation, John Calvin had numerous converts in Poitiers and the town had its share of the violent proceedings which underlined the Wars of Religion throughout France.
In 1569 Poitiers was defended by Gui de Daillon, comte du Lude, against Gaspard de Coligny, who after an unsuccessful bombardment and seven weeks, retired from a siege he had laid to the town.
16th century.
The type of political organisation existing in Poitiers during the late medieval or early modern period can be glimpsed through a speech given on 14 July 1595 by Maurice Roatin, the town's mayor. He compared it to the Roman state, which combined three types of government: monarchy (rule by one person), aristocracy (rule by a few), and democracy (rule by the many). He said the Roman consulate corresponded to Poitiers' mayor, the Roman senate to the town's peers and "échevins", and the democratic element in Rome corresponded to the fact that most important matters "can not be decided except by the advice of the "Mois et Cent"" (broad council).1 The mayor appears to have been an advocate of a mixed constitution; not all Frenchmen in 1595 would have agreed with him, at least in public; many spoke in favour of absolute monarchy. The democratic element was not as strong as the mayor's words may seem to imply: in fact, Poitiers was similar to other French cities, Paris, Nantes, Marseille, Limoges, La Rochelle, Dijon, in that the town's governing body ("corps de ville") was "highly exclusive and oligarchical": a small number of professional and family groups controlled most of the city offices. In Poitiers many of these positions were granted for the lifetime of the office holder.2
The city government in Poitiers based its claims to legitimacy on the theory of government where the mayor and "échevins" held jurisdiction of the city's affairs in fief from the king: that is, they swore allegiance and promised support for him, and in return he granted them local authority. This gave them the advantage of being able to claim that any townsperson who challenged their authority was being disloyal to the king. Every year the mayor and the 24 "échevins" would swear an oath of allegiance "between the hands" of the king or his representative, usually the lieutenant général or the sénéchaussée. For example, in 1567, when Maixent Poitevin was mayor, king Henry III came for a visit, and, although some townspeople grumbled about the licentious behaviour of his entourage, Henry smoothed things over with a warm speech acknowledging their allegiance and thanking them for it.2
In this era, the mayor of Poitiers was preceded by sergeants wherever he went, consulted deliberative bodies, carried out their decisions, "heard civil and criminal suits in first instance", tried to ensure that the food supply would be adequate, visited markets.2
In the 16th century, Poitiers impressed visitors because of its large size, and important features, including "royal courts, university, prolific printing shops, wealthy religious institutions, cathedral, numerous parishes, markets, impressive domestic architecture, extensive fortifications, and castle."3
16th-century Poitiers is closely associated with the life of François Rabelais and with the community of Bitards.
17th century.
The town saw less activity during the Renaissance. Few changes were made in the urban landscape, except for laying way for the "rue de la Tranchée". Bridges were built where the inhabitants had used "gués". A few "hôtels particuliers" were built at that time, such as the hôtels Jean Baucé, Fumé and Berthelot. Poets Joachim du Bellay and Pierre Ronsard met at the University of Poitiers, before leaving for Paris.
During the 17th century, many people emigrated from Poitiers and the Poitou to the French settlements in the new world and thus many Acadians or Cajuns living in North America today can trace ancestry back to this region.
18th century.
During the 18th century, the town's activity mainly depended on its administrative functions as a regional centre: Poitiers served as the seat for the regional administration of royal justice, the évêché, the monasteries and the intendance of the "Généralité du Poitou".
The Vicomte de Blossac, intendant of Poitou from 1750 to 1784, had a French garden landscaped in Poitiers. He also had Aliénor d'Aquitaine's ancient wall razed and modern boulevards were built in its place.
19th century.
During the 19th century, many army bases were built in Poitiers because of its central and strategic location. Poitiers became a garrison town, despite its distance from France's borders.
The Poitiers train station was built in the 1850s, and connected Poitiers to the rest of France.
20th century and contemporary Poitiers.
Poitiers was bombed during World War II, particularly the area around the railway station which was heavily hit on 13 June 1944.
From the late 1950s until the late 1960s when Charles de Gaulle ended the American military presence, the U.S. Army and U.S. Air Force had an array of military installations in France, including a major Army logistics and communications hub in Poitiers, part of what was called the Communication Zone (ComZ), and consisting of a logistics headquarters and communications agency located at Aboville Caserne, a military compound situated on a hill above the city. Hundreds of graduates ("Military Brats") of Poitiers American High School, a school operated by the Department of Defense School System (DODDS), have gone on to successful careers, including the recent commander-in-chief of U.S. Special Forces Command, Army General Bryan (Doug) Brown. The Caserne also housed a full support community, with a theater, commissary, recreation facilities and an affiliate radio station of the American Forces Network, Europe, headquartered in Frankfurt (now Mannheim, Germany). 
The town benefited from industrial "décentralisation" in the 1970s, for instance with the installation during that decade of the Michelin and Compagnie des compteurs Schlumberger factories. The "Futuroscope" theme-park and research park project, built in 1986–1987 in nearby Chasseneuil-du-Poitou, after an idea by René Monory, consolidated Poitiers' place as a touristic destination and as a modern university centre, and opened the town to the era of information technology. 
Sports.
The Stade Poitevin, founded in 1900, is a multi-sports club, which fields several top-level teams in a variety of sports. These include a volleyball team that play in the French Pro A volleyball league, a basketball team, an amateur football team and a professional rugby team (as of the 2008–2009 season.)
The PB86 or Poitiers Basket 86 (www.pb86.fr) play in the French Pro A basketball league. In the 2009–10 season, three Americans played for PB86: Rasheed Wright, Kenny Younger and Tommy Gunn. The team played the French championship playoffs in the 2009–10 season and was the Pro B French Champion for the 2008–2009 season. The team's communication strategy is considered by some to be one of the best in the French basketball league.
Brian Joubert, the figure skating champion, practices at an ice rink in Poitiers and lives with his family in the city.
Tourism.
Historic churches, in particular Romanesque church buildings, are the main attraction inside Poitiers itself. The town's centre is picturesque, with generally well-preserved architecture and a recently re-zoned pedestrian area. There are numerous shops, cafes and restaurants in the town centre.
Since 1987, Poitiers' tourist industry has indirectly benefited from the "Futuroscope" theme-park and research park in nearby Chasseneuil-du-Poitou. The centre of town receives visits in complement to the theme-park and benefits from a larger proportion of European tourists, notably from the United Kingdom. In conjunction, Poitiers' tourism has directly benefited from the TGV high-speed rail link to Paris.
Transport.
Poitiers' railway station lies on the TGV Atlantique line between Paris and Bordeaux. The station is in the valley to the west of the old town centre. Services run to Angoulême, Limoges and La Rochelle in addition to Paris and Bordeaux. The direct TGV puts Poitiers 1h40 from Paris' Gare Montparnasse.
Poitiers - Biard Airport is located 2.4 km west of Poitiers with flights to Lyon-Saint Exupéry, London-Stansted, Edinburgh and Shannon, Ireland on Ryanair.
Urban transportation in Poitiers is provided by a company called Vitalis. Regional ground transportation in the department of the Vienne is provided by private bus companies such as "Ligne en Vienne". Rail transportation in the region is provided by the public TER Poitou-Charentes (regional express train).
From January 2009 to December 2012, Poitiers' town centre went through deep changes to make it less accessible to motor vehicles. The project, named "Projet Coeur d'Agglo", focused on re-thinking the way people use individual cars to access the town centre and as an everyday way of transportation. On September 29, 2010, 12 streets were permanently closed off to motor vehicles and transformed into an entirely pedestrian zone.
Eventually, a new line of fast buses will be added around 2017.
Panoramic view of Poitiers at sunset.
Education.
The city of Poitiers has a very old tradition as a university centre, starting in the Middle Ages. The University of Poitiers was established in 1431 as the second oldest university in France, and has welcomed many famous philosophers and scientists throughout the ages (notably François Rabelais; René Descartes; Francis Bacon).
Today Poitiers is one of the biggest university towns in France; in fact it has more students per inhabitant than any other large town or city in France. All around, there are over 27,000 university students in Poitiers, nearly 4,000 of which are foreigners, hailing from 117 countries. The University covers all major fields from sciences to geography, history, languages economics and law.
The law degree at the University of Poitiers is considered to be one of the best in France. The program was ranked second by "l'Étudiant magazine" in 2005.
In addition to the University, Poitiers also hosts two engineering schools and two business schools:
Since 2001, the city of Poitiers has hosted the first cycle of "the South America, Spain and Portugal" program from the Paris Institute of Political Studies.
International relations.
Twin towns – Sister cities.
Poitiers is twinned with:
Notable people.
This is a list of people of interest who were born or resided in Poitiers:

</doc>
<doc id="51490" url="http://en.wikipedia.org/wiki?curid=51490" title="Rights">
Rights

Rights are legal, social, or ethical principles of freedom or entitlement; that is, rights are the fundamental normative rules about what is allowed of people or owed to people, according to some legal system, social convention, or ethical theory. Rights are of essential importance in such disciplines as law and ethics, especially theories of justice and deontology.
Rights are often considered fundamental to civilization, being regarded as established pillars of society and culture, and the history of social conflicts can be found in the history of each right and its development. According to the Stanford Encyclopedia of Philosophy, "rights structure the form of governments, the content of laws, and the shape of morality as it is currently perceived."
Definitional issues.
There is considerable disagreement about what is meant precisely by the term "rights". It has been used by different groups and thinkers for different purposes, with different and sometimes opposing definitions, and the precise definition of this principle, beyond having something to do with normative rules of some sort or another, is controversial.
One way to get an idea of the multiple understandings and senses of the term is to consider different ways it is used. Many diverse things are claimed as rights: 
There are likewise diverse possible ways to categorize rights, such as:
There has been considerable debate about what this term means within the academic community, particularly within fields such as philosophy, law, deontology, logic, political science, and religion.
Natural rights versus legal rights.
Some thinkers see rights in only one sense while others accept that both senses have a measure of validity. There has been considerable philosophical debate about these senses throughout history. For example, Jeremy Bentham believed that legal rights were the essence of rights, and he denied the existence of natural rights; whereas Thomas Aquinas held that rights purported by positive law but not grounded in natural law were not properly rights at all, but only a facade or pretense of rights.
Claim rights versus liberty rights.
Liberty rights and claim rights are the inverse of one another: a person has a liberty right permitting him to do something only if there is no other person who has a claim right forbidding him from doing so. Likewise, if a person has a claim right against someone else, then that other person doesn't have a right. For example, a person has a "liberty right" to walk down a sidewalk and can decide freely whether or not to do so. But pedestrians may not to walk on certain lands, such as other people's private property, to which those other people have a claim right. So a person's "liberty right" of walking extends precisely to the point when it does not violate another person's "claim right".
Positive rights versus negative rights.
In one sense, a right is a permission to do something or an entitlement to a specific service or treatment from others, and these rights have been called "positive rights". However, in another sense, rights may allow or require inaction, and these are called "negative rights"; they permit or require doing nothing. For example, in some democracies e.g. the US, citizens have the "positive right" to vote and they have the "negative right" to not vote; people can choose not to vote in a given election without punishment. In other democracies e.g. Australia, however, citizens have a positive right to vote but they don't have a negative right to not vote, since voting is compulsory. Accordingly:
Though similarly named, positive and negative rights should not be confused with "active rights" (which encompass "privileges" and "powers") and "passive rights" (which encompass "claims" and "immunities").
Individual rights versus group rights.
The general concept of rights is that they are possessed by individuals in the sense that they are permissions and entitlements to do things which other persons, or which governments or authorities, can not infringe. This is the understanding of people such as the author Ayn Rand who argued that only individuals have rights, according to her philosophy known as Objectivism. However, others have argued that there are situations in which a group of persons is thought to have rights, or "group rights". Accordingly:
There can be tension between individual and group rights. A classic instance in which group and individual rights clash is conflicts between unions and their members. For example, individual members of a union may wish a wage higher than the union-negotiated wage, but are prevented from making further requests; in a so-called closed shop which has a union security agreement, only the union has a "right" to decide matters for the individual union members such as wage rates. So, do the supposed "individual rights" of the workers prevail about the proper wage? Or do the "group rights" of the union regarding the proper wage prevail? Clearly this is a source of tension.
The Austrian School of Economics holds that only individuals think, feel, and act whether or not members of any abstract group. The society should thus according to economists of the school be analyzed starting from the individual. This methodology is called methodological individualism and is used by the economists to justify individual rights.
Other senses.
Other distinctions between rights draw more on historical association or family resemblance than on precise philosophical distinctions. These include the distinction between civil and political rights and economic, social and cultural rights, between which the articles of the Universal Declaration of Human Rights are often divided. Another conception of rights groups them into three generations. These distinctions have much overlap with that between negative and positive rights, as well as between individual rights and group rights, but these groupings are not entirely coextensive.
Rights and politics.
Rights are often included in the foundational questions that governments and politics have been designed to deal with. Often the development of these socio-political institutions have formed a dialectical relationship with rights.
Rights about particular issues, or the rights of particular groups, are often areas of special concern. Often these concerns arise when rights come into conflict with other legal or moral issues, sometimes even other rights. Issues of concern have historically included labor rights, LGBT rights, reproductive rights, disability rights, patient rights and prisoners' rights. With increasing monitoring and the information society, information rights, such as the right to privacy are becoming more important.
Some examples of groups whose rights are of particular concern include animals, and amongst humans, groups such as children and youth, parents (both mothers and fathers), and men and women.
Accordingly, politics plays an important role in developing or recognizing the above rights, and the discussion about which behaviors are included as "rights" is an ongoing political topic of importance. The concept of rights varies with political orientation. Positive rights such as a "right to medical care" are emphasized more often by left-leaning thinkers, while right-leaning thinkers place more emphasis on negative rights such as the "right to a fair trial".
Further, the term "equality" which is often bound up with the meaning of "rights" often depends on one's political orientation. Conservatives and libertarians and advocates of free markets often identify equality with equality of opportunity, and want equal and fair rules in the process of making things, while agreeing that sometimes these fair rules lead to unequal outcomes. In contrast, socialists often identify equality with equality of outcome and see fairness when people have equal amounts of goods and services, and therefore think that people have a right to equal portions of necessities such as health care or economic assistance or housing.
Rights and philosophy.
In philosophy, meta-ethics is the branch of ethics that seeks to understand the nature of ethical properties, statements, attitudes, and judgments. Meta-ethics is one of the three branches of ethics generally recognized by philosophers, the others being normative ethics and applied ethics.
While normative ethics addresses such questions as "What should one do?", thus endorsing some ethical evaluations and rejecting others, meta-ethics addresses questions such as "What "is" goodness?" and "How can we tell what is good from what is bad?", seeking to understand the nature of ethical properties and evaluations.
Rights ethics is an answer to the meta-ethical question of "what normative ethics is concerned with". (Metaethics also includes a group of questions about how ethics comes to be known, true, etc. which is not directly addressed by rights ethics).
Rights ethics holds that normative ethics is concerned with rights. Alternative metaethical theories are that ethics is concerned with one of the following
Rights ethics has had considerable influence on political and social thinking. The Universal Declaration of Human Rights gives some concrete examples of widely accepted rights.
Criticism.
Some philosophers have criticised rights as ontologically dubious entities. For instance, although in favour of the extension of individual legal rights, the utilitarian philosopher Jeremy Bentham opposed the idea of natural law and natural rights, calling them "nonsense upon stilts". Further, one can question the ability of rights to actually bring about justice for all.
Etymology.
The Modern English word "right" derives from Old English "riht" or "reht", in turn from Proto-Germanic "*riχtaz" meaning "right" or "direct", and ultimately from Proto-Indo-European "*reg-to-" meaning "having moved in a straight line", in turn from "*(o)reg'(a)-" meaning "to straighten or direct". In several different Indo-European languages, a single word derived from the same root means both "right" and "law", such as French "droit", Spanish "derecho", German "Recht". and Italian "diritto".
Many other words related to normative or regulatory concepts derive from this same root, including "correct", "regulate", and "rex" (meaning "king"), whence "regal" and thence "royal". Likewise many more geometric terms derive from this same root, such as "erect" (as in "upright"), "rectangle" (literally "right angle"), "straight" and "stretch". Like "right", the English words "rule" and "ruler", deriving still from the same root, have both normative or regulatory and geometric meanings (e.g. a ruler as in a king, or a ruler as in a straightedge).
Several other roots have similar normative and geometric descendants, such as Latin "norma", whence "norm", "normal", and "normative" itself, and also geometric concepts such as "normal vectors"; and likewise Greek "ortho" and Latin "ordo", meaning either "right" or "correct" (as in "orthodox", meaning "correct opinion") or "straight" or "perpendicular" (as in "orthogonal", meaning "perpendicular angle"), and thence "order", "ordinary", etc.
History of rights.
The specific enumeration of rights has differed greatly in different periods of history. In many cases, the system of rights promulgated by one group has come into sharp and bitter conflict with that of other groups. In the political sphere, a place in which rights have historically been an important issue, constitutional provisions of various states sometimes address the question of who has what legal rights.
Historically, many notions of rights were authoritarian and hierarchical, with different people granted different rights, and some having more rights than others. For instance, the right of a father to respected from his son did not indicate a right from the son to receive a return from that respect; and the divine right of kings, which permitted absolute power over subjects, did not leave a lot of room for many rights for the subjects themselves.
In contrast, modern conceptions of rights often emphasize liberty and equality as among the most important aspects of rights, for example in the American Revolution and the French Revolution.
Important documents in the political history of rights include:
See also.
Outline of rights
Organisations:

</doc>
<doc id="51491" url="http://en.wikipedia.org/wiki?curid=51491" title="Samarkand">
Samarkand

Samarkand (Uzbek: "Samarqand" Самарқанд; Persian: سمرقند‎ Cyrillic/Russian: "Самарканд" from Sogdian: "Stone Fort" or "Rock Town"), alternatively Samarqand or Samarcand, traditionally was the second-largest city in Uzbekistan and the capital of Samarqand Province. It is now the nation's third largest, after fast-growing Namangan in the Ferghana Valley. The city is most noted for its central position on the Silk Road between China and the West, and for being an Islamic centre for scholarly study. In the 14th century it became the capital of the empire of Timur (Tamerlane) and is the site of his mausoleum (the Gur-e Amir). The Bibi-Khanym Mosque (a modern replica) remains one of the city's most notable landmarks. The Registan was the ancient center of the city. The city has carefully preserved the traditions of ancient crafts: embroidery, gold embroidery, silk weaving, engraving on copper, ceramics, carving and painting on wood.
In 2001, UNESCO added the city to its World Heritage List as "Samarkand – Crossroads of Cultures".
Etymology.
The city was known by its Greek name of Marakanda when Alexander the Great took it in 329 BC. The name probably originates in the Sogdian words "asmara", "stone", "rock" and "kand", "fort", "town".
People.
According to various independent sources, Tajiks (Persian-speaking people) are the major ethnic group in the city, while ethnic Uzbeks form a growing minority. Exact figures are difficult to evaluate, since many people in Uzbekistan either identify as "Uzbek" even though they speak Eastern Persian as their first language, or because they are registered as Uzbeks by the central government despite their Eastern Persian language and identity. As explained by Paul Bergne:
During the census of 1926 a significant part of the Tajik population was registered as Uzbek. Thus, for example, in the 1920 census in Samarkand city the Tajiks were recorded as numbering 44,758 and the Uzbeks only 3301. According to the 1926 census, the number of Uzbeks was recorded as 43,364 and the Tajiks as only 10,716. In a series of kishlaks [villages] in the Khojand Okrug, whose population was registered as Tajik in 1920 e.g. in Asht, Kalacha, Akjar i Tajik and others, in the 1926 census they were registered as Uzbeks. Similar facts can be adduced also with regard to Ferghana, Samarkand, and especially the Bukhara oblasts.
History.
Along with Bukhara, Samarkand is one of the oldest inhabited cities in Central Asia, prospering from its location on the trade route between China and the Mediterranean (Silk Road). At times Samarkand has been one of the greatest cities of Central Asia.
Early history.
Archeological excavations held within the city limits (Syob and midtown) as well as suburban areas (Hojamazgil, Sazag'on)
unearthed evidence of human activity as early as 40000 years old, which is late paleolithic era. A group of Mesolithic era (12-7 millennium BC) archeological sites were discovered at Sazag'on-1, Zamichatosh, Okhalik (suburbs of the city). Syob and Darg'om canals, supplying with water the city and its suburbs appeared around the 7th to 5th centuries BC (early Iron Age). There is no direct evidence of when exactly Samarkand was founded. Researchers of Institute of Archeology of Samarkand argue existence of the city between the 8th and 7th centuries BC. 
Samarkand has been one of the main centres of Sogdian civilization from its early days. By the time of the Achaemenid dynasty of Persia it had become the capital of the Sogdian satrapy.
The Hellenistic period.
While settlement in the region goes well back into pre-historic times, by the seventh century before the Common Era (BCE), the town seems to have housed a substantial center of craft production and already boasted an extensive irrigation system. It was one of the easternmost administrative centers for Achaemenid Persia and had a citadel and strong fortifications. 
Alexander the Great conquered Samarkand in 329 BCE. The city was known as Maracanda by the Greeks. Written sources offer small clues as to the subsequent system of government. They tell of an Orepius who became ruler "not from ancestors, but as a gift of Alexander".
While Samarkand suffered significant damage during Alexander's initial conquest, the city recovered rapidly and under the new Hellenic influence flourished. There were also major new construction techniques; oblong bricks were replaced with square ones and superior methods of masonry and plastering were introduced. It was later part of Seleucid Empire, Greco-Bactrian Kingdom and Kushan Empire successively.
Alexander's conquests introduced into Central Asia Classical Greek culture; at least for a time the Greek models were followed closely by the local artisans. The Greek legacy lived on in the various "Graeco-Bactrian" kingdoms of the area and the Kushan Empire of the first centuries of the Common Era whose territories extended well down into what is today Pakistan and India. During the Kushan era the city declined though; it did not really revive until the fifth century CE.
The pre-Mongol period.
Samarkand was conquered by the Sassanians around AD 260. Under Sassanian rule the region became an essential site for Manichaeism, and facilitated the dissemination of the religion throughout central Asia.
After the Sassanian disaster against the Hephtalites who managed to conquer Samarkand, Samarkand was controlled by the Hephtalites until they were defeated by the Göktürks, in an alliance with the Sassanid Persians during the Battle of Bukhara. The Turks ruled over Samarkand until they were defeated by the Sassanids during the Göktürk–Persian Wars. After the Islamic conquest of Iran the Turks conquered Samarkand and held it until Turkic qaghanate collapsed due to wars with the Chinese Tang Dynasty. During this time the city became a protectorate and paid tribute to the ruling Tang. The armies of the Umayyad Caliphate under Qutayba ibn Muslim captured the city in around AD 710.
During this period, Samarkand was a diverse religious community and was home to a number of religions, including Buddhism, Zoroastrianism, Hinduism, Manichaeism, Judaism and Nestorian Christianity. However, after the Arab conquest of Sogdiana, Islam became the dominant religion in Samarkand, with much of the population converting.
Legend has it that during Abbasid rule, the secret of papermaking was obtained from two Chinese prisoners from the Battle of Talas in 751, which led to the foundation of the first paper mill of the Islamic world in Samarkand. The invention then spread to the rest of the Islamic world, and from there to Europe.
The Abbasid control of Samarkand soon dissipated and was replaced with that of the Samanids (AD 862–999), though it must be noted that the Samanids were still nominal vassals of the Caliph during their control of Samarkand. Under Samanid rule the city became one of the capitals of the Samanid dynasty and an even more important link amongst numerous trade routes. The Samanids were overthrown by Turkish tribes in around AD 1000. During the next two hundred years, Samarkand would be ruled by a succession of Turkish tribes, including the Seljuqs and the Khwarazm-Shahs.
The tenth-century Iranian author Istakhri, who travelled in Transoxiana, provides a vivid description of the natural riches of the region he calls "Smarkandian Sogd":
The Mongol Period.
The Mongols conquered Samarkand in 1220. Although Genghis Khan "did not disturb the inhabitants [of the city] in any way", according to Juvaini he killed all who took refuge in the citadel and the mosque. He also pillaged the city completely and conscripted 30,000 young men along with 30,000 craftsmen. Samarkand suffered at least one other Mongol sack by Khan Baraq to get treasure he needed to pay an army. The town took many decades to recover from these disasters.
"The Travels" of Marco Polo, where Polo records his journey along the Silk Road, describes Samarkand as "a very large and splendid city..." Here also is related the story of a Christian church in Samarkand, which miraculously remained standing after a portion of its central supporting column was removed.
14th century.
In 1365, a revolt against Mongol control occurred in Samarkand.
In 1370 Timur, the founder and ruler of the Timurid Empire, made Samarkand his capital. During the next 35 years he rebuilt most of the city and populated it with the great artisans and craftsmen from across the empire. Timur gained a reputation as a patron of the arts and Samarkand grew to become the centre of the region of Transoxiana. Timur’s commitment to the arts is evident in the way he was ruthless with his enemies but merciful towards those with special artistic abilities. He spared the lives of artists, craftmen and architects so that he could bring them to improve and beautify his capital. He was also directly involved in his construction projects and his visions often exceeded the technical abilities of his workers. Furthermore, the city was in a state of constant construction and Timur would often request buildings to be done and redone quickly if he was unsatisfied with the results. Timur made it so that the city could only be reached by roads and also ordered the construction of deep ditches and walls, that would run five miles (5 mi) in circumference, separating the city from the rest of its surrounding neighbors. During this time the city had a population of about 150,000. This great period of reconstruction is incapsulated in the account of Henry III's ambassador, Ruy Gonzalez de Clavijo, who was stationed there between 1403 and 1406. During his stay the city was typically in a constant state of construction. "The Mosque which Timur had caused to be built in memory of the mother of his wife...seemed to us the noblest of all those we visited in the city of Samarkand, but no sooner had it been completed than he begun to find fault with its entrance gateway, which he now said was much too low and must forthwith be pulled down."
15th century.
Between 1424 and 1429, the great astronomer Ulugh Beg built the Samarkand Observatory. The sextant was 11 metres long and once rose to the top of the surrounding three-storey structure, although it was kept underground to protect it from earthquakes. Calibrated along its length, it was the world's largest 90-degree quadrant at the time. However, the observatory was destroyed by religious fanatics in 1449.
Modern history.
In 1500 the Uzbek nomadic warriors took control of Samarkand. The Shaybanids emerged as the Uzbek leaders at or about this time.
In the second quarter of 16th century, the Shaybanids moved their capital to Bukhara and Samarkand went into decline. After an assault by the Persian king, Nadir Shah, the city was abandoned in the 18th century, about 1720 or a few years later.
From 1599 to 1756, Samarkand was ruled by the Ashtarkhanid dynasty of Bukhara.
From 1756 to 1868, Samarkand was ruled by the Manghyt emirs of Bukhara.
The city came under Russian rule after the citadel had been taken by a force under Colonel Konstantin Petrovich von Kaufman in 1868. Shortly thereafter the small Russian garrison of 500 men were themselves besieged. The assault, which was led by Abdul Malik Tura, the rebellious elder son of the Bukharan Emir, as well as Baba Beg of Shahrisabz and Jura Beg of Kitab, was repelled with heavy losses. Alexander Abramov became the first Governor of the Military Okrug, which the Russians established along the course of the Zeravshan River, with Samarkand as the administrative centre. The Russian section of the city was built after this point, largely to the west of the old city.
In 1886, the city became the capital of the newly formed Samarkand Oblast of Russian Turkestan and grew in importance still further when the Trans-Caspian railway reached the city in 1888. It became the capital of the Uzbek SSR in 1925 before being replaced by Tashkent in 1930.
Architecture and Influences.
Bibi Khanum.
Timur initiated the building of Bibi Khanum after his campaign in India in 1398-1399. Before its reconstruction after an earthquake in 1897, Bibi Khanum had around 450 marble columns that were established with the help of 95 elephants that Timur had brought back from Hindustan. Also from India, artisans and stonemasons designed the mosque’s dome, giving it its distinctiveness amongst the other buildings.
Gur-i Amir.
The best-known structure in Samarkand is the mausoleum known as Gur-i Amir. It exhibits many cultures and influences from past civilizations, neighboring peoples, and especially those of Islam. Despite how much devastation the Mongols caused in the past to all of the Islamic architecture that had existed in the city prior to Timur's succession, much of the destroyed Islamic influences were revived, recreated, and restored under Timur. The blueprint and layout of the mosque itself follows the Islamic passion of geometry and other elements of the structure had been precisely measured. The entrance to the Gur-i Amir is decorated with Arabic calligraphy and inscriptions, the latter being a common feature in Islamic architecture. The attention to detail and meticulous nature of Timur is especially obvious when looking inside the building. Inside, the walls have been covered in tiles through a technique, originally developed in Iran, called “mosaic faience,” a process where each tile is cut, colored, and fit into place individually. The tiles were also arranged in a specific way that would engrave words relating to the city's religiosity; words like "Muhammad" and "Allah" have been spelled out on the walls using the tiles.
The ornaments and decorations of the walls include floral and vegetal symbols which are used to signify gardens. Gardens are commonly interpreted as paradise in the Islamic religion and they were both inscribed in tomb walls and grown in the city itself. In the city of Samarkand, there were two major gardens, the New Garden and the Garden of Heart’s Delight, and these became the central areas of entertainment for ambassadors and important guests. A friend of Genghis Khan in 1218 named Yelü Chucai, reported that Samarkand was the most beautiful city of all where "it was surrounded by numerous gardens. Every household had a garden, and all the gardens were well designed, with canals and water fountains that supplied water to round or square-shaped ponds. The landscape included rows of willows and cypress trees, and peach and plum orchards were shoulder to shoulder." The floors of the mausoleum is entirely covered with uninterrupted patterns of tiles of flowers, emphasizing the presence of Islam and Islamic art in the city. In addition, Persian carpets with floral printings have been found in some of the Timurid buildings.
Turko-Mongol influence is also apparent in the architecture of the buildings in Samarkand. For instance, nomads previously used tents, or "yurts", to display the bodies of the dead before they were to engage in proper burial procedures. Similarly, it is believed that the melon-shaped domes of the tomb chambers are imitations of those very "yurts". Timur, naturally, used stronger materials, like bricks and wood, to establish these tents, but their purposes remain largely unchanged.
The color of the buildings in Samarkand also has significant meaning behind it. For instance, blue is the most common and dominant color that will be found on the buildings, which was used by Timur in order to symbolize a large range of ideas. For one, the blue shades seen in the Gur-i Amir are colors of mourning. Blue was the color of mourning in Central Asia at the time, as it is in many cultures even today, and its dominance in the city's mausoleum appears to be a very rational idea. In addition, blue was also seen as the color that would ward off "the evil eye" in Central Asia and the notion is evident in the number of doors in and around the city that were colored blue during this time. Furthermore, blue was representative of water, which was a particularly rare resource around the Middle East and Central Asia; coloring the walls blue symbolized the wealth of the city.
Gold also has a strong presence in the city. Timur's fascination with vaulting explains the excessive use of gold in the Gur-i Amir as well as the use of embroidered gold fabric in both the city and his buildings. The Mongols had great interests in Chinese- and Persian-style golden silk textiles as well as nasij woven in Iran and Transoxiana. Past Mongol leaders, like Ogodei, built textile workshops in their cities in order to be able to produce gold fabrics themselves.
There is evidence that Timur tried to preserve his Mongol roots. In the chamber in which his body was laid, "tuqs" were found. "Tuqs" are poles with horses' tails hanging at the top, which was symbolic of an ancient Turkic tradition where horses, which were valuable commodities, were sacrificed in order to honor the dead.
Climate.
Samarkand features a Mediterranean climate (Köppen climate classification "Csa") that closely borders on a semi-arid climate with hot, dry summers and relatively wet, variable winters that alternate periods of warm weather with periods of cold weather. July and August are the hottest months of the year with temperatures reaching, and exceeding, 40 C. Most of the sparse precipitation is received from December through April. January 2008 was particularly cold, and the temperature dropped to -22 °C
Notable people.
Also said to be the place of death of Muhammad b. Isma'il al-Bukhari, one of the six prominent collectors of hadith of Sunni Islam. "See" "Sahih Bukhari"

</doc>
<doc id="51493" url="http://en.wikipedia.org/wiki?curid=51493" title="Santiago (disambiguation)">
Santiago (disambiguation)

Santiago (pop. 5 million) is the capital city of Chile, named in honour of Saint James, son of Zebedee. In English, four other notable cities are also often referred to simply as "Santiago":
"Etymologically, Santiago has the same meaning as San Diego."
Santiago may also refer to:
Places.
In Argentina:
In Brazil:
In Cape Verde:
In Chile:
In Colombia:
In Costa Rica:
In Cuba:
In Dominican Republic:
In Ecuador:
In Guatemala:
In Jamaica:
In Mexico:
In Nicaragua:
In Panama:
In Paraguay:
In Peru:
In the Philippines:
In Portugal:
In Spain:
In Taiwan
In the United States:
In Uruguay:
In Venezuela:
See also.
 

</doc>
<doc id="51497" url="http://en.wikipedia.org/wiki?curid=51497" title="Battle of Verdun">
Battle of Verdun

The Battle of Verdun ("Bataille de Verdun", ], "Schlacht um Verdun", ]) was fought from 21 February – 18 December 1916 during the First World War on the Western Front between the German and French armies, on hills north of Verdun-sur-Meuse in north-eastern France. The German Fifth Army attacked the defences of the "Région Fortifiée de Verdun" (RFV) and those of the Second Army garrisons on the right bank of the Meuse, intending to rapidly capture the "Côtes de Meuse" (Meuse Heights), from which Verdun could be overlooked and bombarded with observed artillery-fire. The German strategy intended to provoke the French into counter-attacks and counter-offensives to drive the Germans off the heights. French attacks would be relatively easy to repel with massed artillery-fire, from the large number of medium, heavy and super-heavy guns, brought into the area and supplied with large amounts of ammunition on excellent pre-war railways, which were within 24 km of the front line.
The German strategy assumed that the French would attempt to hold on to the east bank of the Meuse, then commit the French strategic reserve to recapture it and suffer catastrophic losses from German artillery-fire, while the German infantry held positions easy to defend and suffered fewer losses. The German plan was based on the experience of the battles in Champagne ("Herbstschlacht" September–October 1915) when after early success, the French offensive was defeated with far more French than German casualties. Poor weather delayed the beginning of the German offensive ("Unternehmen Gericht"/Operation Judgement) until 21 February; French construction of defensive lines and the arrival of reinforcements before the opening attack, were able to delay the German advance despite many losses. By 6 March, 20 1⁄2 French divisions were in the RFV and a defence in depth had been established. Pétain ordered that no withdrawals were to be made and that counter-attacks were to be conducted, despite exposing French infantry to fire from the German artillery massed in the area. By 29 March, French artillery on the west bank had begun a constant bombardment of German positions on the east bank, which caused many German infantry casualties.
In March, the German offensive was extended to the left (west) bank, to gain observation of the ground from which French artillery had been firing over the river, into the flank of German infantry attacks on the east bank. The German troops were able to make substantial advances but French reinforcements contained the attacks, before the Germans reached positions from which they had ground observation of the French artillery sites. In early May, the Germans changed tactics and made local attacks and counter-attacks, which gave the French an opportunity to begin an attack against Fort Douaumont, which was partially occupied, until a German counter-attack reoccupied the fort and took numerous prisoners. The Germans changed tactics again, alternating attacks between both banks of the Meuse and in June captured Fort Vaux. The Germans continued the offensive beyond Fort Vaux, towards the last geographical objectives of the original plan, at Fleury and Fort Souville. German attacks drove a salient into the French defences, captured Fleury and came within 4 km of the Verdun citadel.
The German offensive was reduced to provide artillery and infantry reinforcements for the Somme front, where the Anglo-French relief offensive began on 1 July. During local operations, the village of Fleury changed hands sixteen times from 23 June to 17 August. A German attempt to capture Fort Souville in early July, was repulsed by artillery and small-arms fire. To supply reinforcements for the Somme front, the German offensive was reduced further, along with attempts to deceive the French into expecting more attacks, to keep French reinforcements away from the Somme front. In August and December, French counter-offensives recaptured much of the ground lost on the east bank and recovered Fort Douaumont and Fort Vaux. An estimate in 2000 found a total of 714,231 casualties, 377,231 French and 337,000 German, an average of 70,000 casualties for each month of the battle; other recent estimates increase the number of casualties to 976,000 with 1,250,000 suffered at Verdun from 1914–1918. The Battle of Verdun lasted for 303 days and became the longest and one of the most costly battles in human history.
Background.
Strategic developments.
After the German invasion of France had been halted at the First Battle of the Marne in September 1914, the war of movement ended at the Battle of the Yser and the First Battle of Ypres. The Germans built field fortifications to hold the ground captured in 1914 and the French began siege warfare to break through the German defences and recover the lost territory. In late 1914 and in 1915, offensives on the Western front had failed to gain much ground and been extremely costly in casualties. According to his memoirs written after the war, the Chief of the German General Staff, Erich von Falkenhayn, believed that although victory might no longer be achieved by a decisive battle, the French army could still be defeated, if it suffered a sufficient number of casualties. Falkenhayn offered five corps from the strategic reserve, for an offensive at Verdun at the beginning of February 1916 but only for an attack on the east bank of the Meuse. Falkenhayn considered it unlikely the French would be complacent about Verdun; he thought that they might send all their reserves there, begin a counter-offensive elsewhere or fight to hold Verdun while the British launched a relief offensive. After the war, the Kaiser and Colonel Tappen, the Operations Officer at "Oberste Heeresleitung" (OHL: General Headquarters), wrote that Falkenhayn believed the last possibility was most likely.
By seizing or threatening to capture Verdun, the Germans anticipated that the French would send all their reserves to defend the city, which would be destroyed as they attacked secure German defensive positions, supported by a powerful artillery reserve. In the Gorlice–Tarnów Offensive (1 May – 19 September 1915) the German and Austro-Hungarian armies attacked Russian defences frontally, after pulverising them with large amounts of heavy artillery. During the Second Battle of Champagne ("Herbstschlacht" "autumn battle") of 25 September – 6 November 1915, the French suffered "extraordinary casualties" from the German heavy artillery, which Falkenhayn considered to be a way out of the dilemma of material inferiority and the growing strength of the Allies. In the north, a British relief offensive would wear down British reserves to no decisive effect and create the conditions for a German counter-offensive near Arras.
Hints about Falkenhayn's thinking were picked up by Dutch military intelligence and passed on to the British in December. The German strategy was to create a favourable operational situation, without a mass attack (which had been costly and ineffective when tried by the Franco-British), instead relying on the power of heavy artillery to inflict mass losses. A limited offensive at Verdun would lead to the destruction of the French strategic reserve, in fruitless counter-attacks and the defeat of British reserves in a futile relief offensive, leading to the French accepting a separate peace. If the French refused to negotiate, the second phase of the strategy would begin, in which the German armies would attack terminally weakened Franco-British armies, mop up the remains of the French armies and expel the British from Europe. To fulfil this strategy, Falkenhayn needed to hold back enough of his strategic reserve, to deal with Anglo-French relief offensives and then conduct a counter-offensive, which limited the number of divisions which could be sent to the Fifth Army at Verdun for "Unternehmen Gericht" (Operation Judgement).
The Fortified Region of Verdun (RFV) lay in a salient which had formed during the German invasion of 1914. The Commander-in-Chief of the French Army, General Joseph Joffre, had concluded from the swift capture of the Belgian fortresses at the Battle of Liège and at the Siege of Namur in 1914, that fixed defences had been made obsolete by German siege guns. In a directive of the General Staff of 5 August 1915, the RFV was to be stripped of 54 artillery batteries and 128,000 rounds of ammunition. Plans to demolish forts Douaumont and Vaux, to deny them to the Germans were made and 5000 kg of explosives had been laid by the time of the German offensive on 21 February. The 18 large forts and other batteries around Verdun were left with fewer than 300 guns and a small reserve of ammunition, while their garrisons had been reduced to small maintenance crews. The railway line from the south into Verdun had been cut during the Battle of Flirey in 1914, with the loss of Saint-Mihiel; the line west from Verdun to Paris was cut at Aubréville in mid-July 1915, by the German Third Army, which had attacked southwards through the Argonne Forest for most of the year.
Région Fortifiée de Verdun.
For centuries, Verdun had played an important role in the defence of the hinterland, due to the strategic location of the city on the Meuse river. Attila the Hun failed to seize the town in the fifth century; when the empire of Charlemagne was divided under the Treaty of Verdun of 843, the town became part of the Holy Roman Empire. The Peace of Westphalia in 1648 awarded Verdun to France. The heart of the city of Verdun was a citadel built by Vauban in the 17th century. A double ring of 28 forts and smaller works ("ouvrages") had been built around Verdun on commanding ground at least 150 m above the river valley, 2.5 – from the citadel at Verdun. The programme had been devised by Séré de Rivières in the 1870s, to build two lines of fortresses from Belfort to Épinal and from Verdun to Toul as defensive screens and to enclose towns intended to be the bases for counter-attacks. Many of the Verdun forts had been modernized and made more resistant to artillery, with a reconstruction programme begun at Douaumont in the 1880s. A sand cushion and thick, steel-reinforced concrete tops up to 2.5 m thick, buried under 1 – of earth, were added. The forts and "ouvrages" had been sited to overlook each other for mutual support and the outer ring had a circumference of 45 km. The outer forts had 79 guns in shell-proof turrets and more than 200 light guns and machine-guns, to protect the ditches around the forts. Six forts had 155mm guns in retractable turrets and fourteen had retractable twin 75mm turrets.
In 1903 Douaumont was equipped with a new concrete bunker ("Casemate de Bourges") containing two 75mm field guns to cover the south-western approach and the defensive works along the ridge to "Ouvrage de Froidterre". More guns were added from 1903–1913 in four retractable steel turrets. The guns could rotate for all-round defence and two smaller versions, at the north-east and north-west corners of the fort, housed twin Hotchkiss machine-guns. On the east side of the fort an armoured turret with a 155mm short-barrelled gun faced north and north-east and another housed twin 75mm guns at the north end, to cover the intervals between forts. The fort at Douaumont formed part of a complex of the village, fort, six "ouvrages", five shelters, six concrete batteries, an underground infantry shelter, two ammunition depots and several concrete infantry trenches. The Verdun forts had a network of concrete infantry shelters, armoured observation posts, batteries, concrete trenches, command posts and underground shelters between the forts. The artillery comprised  1,000 guns, with 250 in reserve and the forts and "ouvrages" were linked by telephone and telegraph, a narrow-gauge railway system and a road network; on mobilisation the RFV had a garrison of 66,000 men and rations for six months.
Prelude.
German offensive preparations.
Verdun was isolated on three sides and railway communications to the French rear had been cut except for a light railway; German-controlled railways lay only 24 km to the north of the front line. A corps was moved to the Fifth Army to provide labour for the preparation of the offensive, areas were emptied of French civilians and buildings requisitioned. Thousands of kilometres of telephone cable were laid, thousands of tons of ammunition and rations stored under cover and hundreds of guns installed and camouflaged. Ten new rail lines with twenty stations were built and vast underground shelters ("Stollen") were dug 4.5 – deep, each to accommodate up to 1,200 German infantry. The III Corps, VII Reserve Corps and XVIII Corps were transferred to the Fifth Army, each corps being reinforced by 2,400 experienced troops and 2,000 trained recruits. V Corps was placed behind the front line, ready to advance if necessary when the assault divisions were moving up and the XV Corps, with two divisions, was in the Fifth Army reserve, ready to advance to mop up as soon as the French defence collapsed.
Special arrangements were made to maintain a high rate of artillery-fire during the offensive, 33 1⁄2 munitions trains per day were to deliver ammunition sufficient for 2,000,000 rounds to be fired in the first six days and another 2,000,000 shells were to be available for the next twelve. Five repair shops were built close to the front, to reduce delays for maintenance; factories in Germany were made ready, rapidly to refurbish artillery needing more extensive repairs. A redeployment plan for the artillery was devised, for field guns and mobile heavy artillery to be moved forward, under the covering fire of mortars and the super-heavy artillery. A total of 1,201 guns were massed on the Verdun front, two-thirds of which were heavy and super-heavy artillery, which had been obtained by stripping the modern German artillery from the rest of the Western front and substituting it with older types and captured Russian guns. The German artillery could fire into the Verdun salient from three directions yet remain dispersed.
German plan of attack.
The Fifth Army divided the attack front into areas, "A" occupied by the VII Reserve Corps, "B" by the XVIII Corps, "C" by the III Corps, and "D" on the Woëvre plain by the XV Corps. The preliminary artillery bombardment was to begin in the morning of 12 February, at 5:00 p.m. The infantry in areas "A"–"C" would advance in open order, supported by grenade and flame-thrower detachments. Where possible, the French advanced trenches were to be occupied and the second position reconnoitred, to prepare the artillery support for the second day. Great emphasis was placed on limiting German infantry casualties, by sending them to follow-up destructive bombardments by the artillery, which was to carry the burden of the offensive. The initial objectives were the Meuse Heights, on a line from Froide Terre to Fort Souville and Fort Tavannes, which would provide a secure defensive position, from which to repel French counter-attacks.
Control of the artillery was centralised by an "Order for the Activities of the Artillery and Mortars", which stipulated that the corps Generals of Foot Artillery were responsible for local target selection, while co-ordination of flanking fire by neighbouring corps and the fire of certain batteries was determined by the Fifth Army headquarters. French fortifications were to be engaged by the heaviest howitzers and enfilade fire. The heavy artillery was to maintain long-range bombardments of French supply routes and assembly areas and counter-battery fire was reserved for specialist batteries firing gas shells. Co-operation between the artillery and infantry was stressed, with accuracy of the artillery being given priority over rate of fire. The opening bombardment was to build up slowly and "trommelfeuer" (a rate of fire so great that the sound of individual shell-explosions merged into a rumble) would not begin until the last hour. As the infantry advanced, the artillery would increase the range of the bombardment to destroy the French second position. Artillery observers were to advance with the infantry and communicate with the guns by field telephones, flares and coloured balloons. When the offensive began, the French were to be bombarded continuously, with harassing fire being maintained at night.
French defensive preparations.
In 1915, 237 guns and 647 lt of ammunition in the forts of the RFV had been removed, leaving only heavy guns in the retractable gun turrets. The conversion of the RFV to a conventional linear defence, with trenches and barbed-wire began but proceeded slowly, after resources were sent west from Verdun for the Second Battle of Champagne (25 September – 6 November 1915). In October 1915, building began on trench lines known as the first, second and third positions and in January 1916, an inspection by General N. E. Castelnau, Chief of Staff at French General Headquarters (GQG), reported that the new defences were satisfactory, except for small deficiencies in three areas. The fortress garrisons had been reduced to small maintenance crews and some of the forts had been readied for demolition. The maintenance garrisons were responsible to the central military bureaucracy in Paris and when the XXX Corps commander, General Chrétien, attempted to inspect Fort Douaumont in January 1916, he was refused entry.
Douaumont was the largest fort in the RFV and by February 1916, the only artillery left in the fort were the 75mm and 155 mm turret guns and light cannon covering the ditch. The fort was used as a barracks by 68 technicians under the command of "Gardien de Batterie" Warrant-Officer Chenot. One of the rotating 155 mm turrets was partially manned and the other was left empty. The Hotchkiss machine-guns were stored in boxes and the four 75mm guns in the casemates had been removed in 1915. The drawbridge had been jammed in the down position by a German shell and had not been repaired, the "coffres" (wall bunkers) with Hotchkiss revolver-cannons protecting the moats were unmanned and over 5000 kg of explosive charges had been placed in the fort to demolish it.
In late January 1916, French intelligence had obtained an accurate assessment of German military capacity and intentions but Joffre considered such an attack to be a diversion, given the lack of an obvious strategic objective. By the time of the German offensive, Joffre expected a bigger attack elsewhere but ordered the VII Corps to Verdun on 23 January, to hold the north face of the west bank. XXX Corps held the salient east of the Meuse to the north and north-east and II Corps held the eastern face of the Meuse heights; Herr had 8 1⁄2 divisions in the front line with 2 1⁄2 divisions in close reserve and "Le groupe d'armées du centre" (GAC) had a reserve of the I and XX corps with two divisions each, plus most of the 19th Division; Joffre had 25 divisions in strategic reserve. French artillery reinforcements had brought the total at Verdun to 388 field guns and 244 heavy guns, against 1,201 German guns, 2⁄3 of which were heavy and super heavy, including 14 in and 202 mortars, some being 16 in. Eight specialist flame-thrower companies were also sent to the 5th Army.
Castelnau met General De Langle de Cary, the commander of GAC, on 25 February, who doubted that the east bank could be held. Castelnau disagreed and ordered Herr to hold the right (east) bank of the Meuse at all costs. Herr sent a division from the west bank and ordered XXX Corps to hold a line from Bras to Douaumont, Vaux and Eix. Pétain took over command of the defence of the RFV at 11:00 p.m., with Colonel Maurice de Barescut as Chief of Staff and Colonel Bernard Serrigny as head of operations, only to hear that Fort Douaumont had fallen; Pétain ordered that the remaining Verdun forts were to be re-garrisoned. Four groups were established under the command of Guillaumat, Balfourier and Duchêne on the right bank and Bazelaire on the left bank. A "line of resistance" was established on the east bank from Souville to Thiaumont, around Fort Douaumont to Fort Vaux, Moulainville and along the ridge of the Woëvre. On the west bank the line ran from Cumières to Mort Homme, Côte 304 and Avocourt. A "line of panic" was planned in secret, as a final line of defence north of Verdun, through forts Belleville, St. Michel and Moulainville. The I and XX corps arrived from 24–26 February, which increased the number of divisions in the RFV to 14 1⁄2. By 6 March, the arrival of the XIII, XXI, XIV and XXXIII corps had increased the total to 20 1⁄2 divisions.
Battle.
First phase, 21 February – 1 March.
21–26 February.
"Unternehmen Gericht" (Operation Judgement) was due to begin on 12 February but fog, heavy rain and high winds delayed the offensive until 7:15 a.m. on 21 February, when a 10-hour artillery bombardment by 808 guns began. The German artillery fired  1,000,000 shells along a front about 30 km long by 5 km wide. The main concentration of fire was on the right (east) bank of the Meuse river. Twenty-six super-heavy, long-range guns, up to 420 mm, fired on the forts and the city of Verdun; a rumble could be heard 160 km away. The bombardment was paused at midday, as a ruse to prompt French survivors to reveal themselves and German artillery-observation aircraft were able to fly over the battlefield unmolested by French aircraft. The 3rd, 7th and 18th corps attacked at 4:00 p.m.; the Germans used flamethrowers for the first time and storm troops followed closely with rifles slung, to use hand grenades to kill the remaining defenders. This tactic had been developed by Captain Willy Rohr and "Sturm-Bataillon Nr. 5 (Rohr)", which battalion conducted the attack. French survivors engaged the attackers, yet the Germans suffered only  600 casualties.
By 22 February, German troops had advanced 5 km and captured "Bois des Caures", at the edge of the village of Flabas. Two French battalions led by Colonel Émile Driant had held the bois (wood) for two days, but were forced back to Samogneux, Beaumont and Ornes. Driant was killed, fighting with the 56th and 59th "Bataillons de chasseurs à pied" and only 118 of the Chasseurs managed to escape. Poor communications meant that only then did the French High Command realise the seriousness of the attack. The Germans managed to take the village of Haumont but French forces repulsed a German attack on the village of "Bois de l'Herbebois". On 23 February, a French counter-attack at "Bois des Caures" was repulsed. Fighting for "Bois de l'Herbebois" continued until the Germans outflanked the French defenders from "Bois de Wavrille". The German attackers had many casualties during their attack on "Bois de Fosses" and the French held on Samogneux. German attacks continued on 24 February and the French XXX Corps was forced out of the second line of defence; XX Corps under General Balfourier arrived at the last minute and was rushed forward. That evening Castelnau advised Joffre that the French Second Army, under General Philippe Pétain, should be sent to the RFV. The Germans had captured Beaumont, "Bois des Fosses", and "Bois des Caurières" and were moving up "ravin Hassoule" which led to Fort Douaumont.
At 3:00 p.m. on 25 February, infantry of Brandenburg Regiment 24 advanced with the II and III battalions side-by-side, each formed into two waves composed of two companies each. A delay in the arrival of orders to the regiments on the flanks, led to the III Battalion advancing without support on that flank. The Germans rushed French positions in the woods and on Côte 347, with the support of machine-gun fire from the edge of "Bois Hermitage" and took many prisoners, as the French on Côte 347 were outflanked on the right and withdrew to Douaumont village. The German infantry had reached their objectives in fewer than twenty minutes and pursued the French, until fired on by a machine-gun in Douaumont church. Some German troops took cover in woods and a ravine which led to the fort, when German artillery began to bombard the area, the gunners having refused to believe claims sent by field telephone, that the German infantry were within a few hundred metres of the fort. Several German parties were forced to advance to find cover from the German shelling and two parties independently made for the fort.. They did not know that the French garrison was only made up of a small maintenance crew led by a warrant officer, since most of the Verdun forts had been partly disarmed, after the demolition of Belgian forts in 1914, by the German super-heavy Krupp 420 mm mortars.
The German party of  100 soldiers tried to signal to the artillery with flares but twilight and falling snow obscured them from view. Some of the party began to cut through the wire around the fort, while French machine-gun fire from Douaumont village ceased. The French had seen the German flares and took the Germans on the fort to be Zouaves retreating from Côte 378. The Germans were able to reach the north-east end of the fort, before the French resumed firing. The German party found a way through the railings on top of the ditch and climbed down without being fired on, since the machine gun bunkers ("coffres de contrescarpe") at each corner of the ditch, had been left unmanned. The German parties continued and found a way inside the fort, through one of the unoccupied ditch bunkers and then reached the central "Rue de Rempart". After quietly moving inside, the Germans heard voices and persuaded a French prisoner captured in an observation post, to lead them to the lower floor, where they found Warrant Officer Chenot and about 25 French troops, most of the skeleton garrison of the fort, and took them prisoner. On 26 February, the Germans had advanced 3 km on a 10 km front; French losses were 24,000 men and German losses were  25,000 men. A French counter-attack on Fort Douaumont failed and Pétain ordered that no more attempts were be made; existing lines were to be consolidated and other forts were to be occupied, rearmed and supplied to withstand a siege if surrounded.
27–28 February.
The German advance gained little ground on 27 February, after a thaw turned the ground into a swamp and the arrival of French reinforcements increased the effectiveness of the defence. Some German artillery became unserviceable and other batteries became stranded in the mud. German infantry began to suffer from exhaustion and unexpectedly high losses, 500 casualties being suffered in the fighting around Douaumont village. On 29 February, the German advance was contained at Douaumont by a heavy snowfall and the defence of French 33rd Infantry Regiment. Delays gave the French time to bring up 90,000 men and 23000 ST of ammunition from the railhead at Bar-le-Duc to Verdun. The swift German advance had gone beyond the range of artillery covering fire and the muddy conditions made it very difficult to move the artillery forward as planned. The German advance southwards, brought it into range of French artillery west of the Meuse, whose fire caused more German infantry casualties than in the earlier fighting, when French infantry on the east bank had fewer guns in support.
Second phase, 6 March – 15 April.
6–11 March.
Before the offensive, Falkenhayn had expected that French artillery on the west bank would be suppressed by counter-battery fire but this had failed. The Germans set up an artillery task-force, to counter French artillery-fire from the west bank but this also failed to reduce German infantry casualties. The Fifth Army asked for more troops in late February but Falkenhayn refused, due to the rapid advance already achieved on the east bank and because he needed the rest of the OHL reserve for an offensive elsewhere, once the attack at Verdun had attracted and consumed French reserves. The pause in the German advance on 27 February led Falkenhayn to have second thoughts to decide between terminating the offensive or reinforcing it. On 29 February, Knobelsdorf, the Fifth Army Chief of Staff, prised two divisions from the OHL reserve, with the assurance that once the heights on the west bank had been occupied, the offensive on the east bank could be completed. The VI Reserve Corps was reinforced with the X Reserve Corps, to capture a line from the south of Avocourt to Côte 304 north of Esnes, Mort-Homme, Bois des Cumières and Côte 205, from which the French artillery on the west bank could be destroyed.
The artillery of the two-corps assault group on the west bank was reinforced by 25 heavy artillery batteries, artillery command was centralised under one officer and arrangements were made for the artillery on the east bank to fire in support. The attack was planned by General Heinrich von Gossler in two parts, on Mort-Homme and Côte 265 on 6 March, followed by attacks on Avocourt and Côte 304 on 9 March. The German bombardment reduced the top of Côte 304 from a height of 304 m to 300 m; Mort-Homme sheltered batteries of French field guns, which hindered German progress towards Verdun on the right bank; the hills also provided commanding views of the left bank. After storming the "Bois des Corbeaux" and then losing it to a French counter-attack, the Germans launched another assault on Mort-Homme on 9 March, from the direction of Béthincourt to the north-west. "Bois des Corbeaux" was captured again at great cost in casualties, before the Germans took parts of Mort-Homme, Côte 304, Cumières and Chattancourt on 14 March.
11 March – 9 April.
After a week, the German attack had reached the first-day objectives and then found that French guns behind Côte de Marre and Bois Borrous were still operational and continued to inflict many casualties on the east bank. German artillery moved to Côte 265, was subjected to systematic artillery-fire by the French, which left the Germans needing to implement the second part of the west bank offensive, to protect the gains of the first phase. German attacks changed from large operations on broad fronts, to narrow-front attacks with limited objectives. On 14 March a German attack captured Côte 265 at west end of Mort-Homme but the French 75th Infantry Brigade managed to hold Côte 295 at the east end. On 20 March, after a bombardment by 13,000 trench mortar rounds, the 11th Bavarian and 11th Reserve divisions attacked "Bois d'Avocourt" and "Bois de Malancourt" and reached their initial objectives easily. Gossler then paused the attack, to consolidate the captured ground and to prepare another big bombardment for the next day. On 22 March, two divisions attacked "Termite Hill" near Côte 304 but were met by a mass of artillery-fire, which also fell on assembly points and the German lines of communication, which ended the German advance.
The limited German success had been costly and French artillery inflicted more casualties as the German infantry tried to dig in. By 30 March, Gossler had captured "Bois de Malancourt" but had lost 20,000 casualties and the Germans were still short of Côte 304. On 30 March, the XXII Reserve Corps arrived as reinforcements and General Max von Gallwitz took command of a new "Angriffsgruppe West". Malancourt village was captured on 31 March, Haucourt fell on 5 April and Bethincourt on 8 April. On the east bank, German attacks near Vaux reached "Bois Caillette" and the Vaux–Fleury railway but were then driven back by the French 5th Division. An attack was made on a wider front along both banks by the Germans at noon on 9 April, with five divisions on the left bank but this was repulsed except at Mort-Homme, where the French 42nd Division was forced back from the north-east face. On the right bank an attack on "Côte-du-Poivre" failed.
In March the German attacks had no advantage of surprise and faced a determined and well-supplied adversary in superior defensive positions. German artillery could still devastate French defensive positions but could not prevent French artillery-fire from inflicting many casualties on German infantry and isolating them from their supplies. Massed artillery fire could enable German infantry to make small advances but massed French artillery-fire could do the same for French infantry when they counter-attacked, which often repulsed the German infantry and subjected them to constant losses, even when captured ground was held. The German effort on the west bank also showed that capturing a vital point was not sufficient, because it would be found to be overlooked by another terrain feature, which had to be captured to ensure the defence of the original point, which made it impossible for the Germans to terminate their attacks, unless they were willing to retire to the original front line of February 1916.
By the end of March the offensive had cost the Germans 81,607 casualties and Falkenhayn began to think of ending the offensive, lest it become another costly and indecisive First Battle of Ypres. The Fifth Army staff requested more reinforcements from Falkenhayn on 31 March with an optimistic report claiming that the French were close to exhaustion and incapable of a big offensive. The Fifth Army command wanted to continue the east bank offensive until a line from Ouvrage de Thiaumont, to Fleury, Fort Souville and Fort de Tavannes had been reached, while on the west bank the French would be destroyed by their own counter-attacks. On 4 April, Falkenhayn replied that the French had retained a considerable reserve and that German resources were limited and not sufficient to replace continuously, men and munitions. If the resumed offensive on the east bank failed to reach the Meuse Heights, Falkenhayn was willing to end the offensive and accept that it had failed.
Third phase, 16 April – 1 July.
April.
The failure of German attacks in early April by "Angriffsgruppe Ost", led Knobelsdorf to obtain reports from the Fifth Army corps commanders, who unanimously wanted to continue the offensive. The German infantry were exposed to continuous artillery-fire, from the flanks and behind, communications from the rear and reserve positions were equally vulnerable, which caused a constant drain of casualties. Defensive positions were difficult to build, because existing positions were on ground which had been swept clear by German bombardments early in the offensive, leaving German infantry with very little cover from French artillery. The XV Corps commander, General Berthold von Deimling also wrote that French heavy artillery and gas bombardments were undermining the morale of the German infantry and that it was necessary to keep going, to reach safer defensive positions. Knobelsdorf reported these findings to Falkenhayn on 20 April and added that if the Germans did not go forward, they must go back to the start line of 21 February.
Knobelsdorf rejected the policy of limited piecemeal attacks tried by Mudra, while in command of "Angriffsgruppe Ost" and advocated a return to wide-front attacks with unlimited objectives, intended swiftly to reach the line from Ouvrage de Thiaumont to Fleury, Fort Souville and Fort de Tavannes. Falkenhayn was persuaded to agree to the change and by the end of April, 21 divisions, most of the OHL reserve, had been sent to Verdun and troops were also transferred from the Eastern Front. The resort to large, unlimited attacks was costly for both sides but the German advance proceeded only slowly. Rather than causing devastating French casualties with heavy artillery in secure positions, which the French were compelled to attack, the Germans inflicted casualties by attacks which provoked French counter-attacks and assumed that the process inflicted five French casualties for two German losses.
In mid-March Falkenhayn had reminded the Fifth Army to use tactics intended to conserve infantry, after the corps commanders had been allowed discretion to choose between the cautious step-by-step tactics desired by Falkenhayn and maximum efforts, intended to obtain quick results. On the third day of the offensive, the 6th Division of the III Corps of General Ewald von Lochow, had ordered that Herbebois be taken "regardless of loss" and the 5th Division had attacked Wavrille to the accompaniment of its band. Falkenhayn urged the Fifth Army to use "Stoßtruppe" (storm units) of two infantry squads and one of engineers, armed with automatic weapons, hand grenades, trench mortars and flame-throwers, to advance in front of the main infantry body, which would conceal their advance by shrewd use of terrain and capture any strong-points which remained after the artillery preparation. Strong-points which could not be taken were to be by-passed and captured by follow-up troops. Falkenhayn ordered that the command of field and heavy artillery units was to be unified, with a commander at each corps headquarters. Common observers and communication systems, would ensure that batteries in different places could bring targets under converging fire, which would be allotted systematically to support divisions.
In mid-April Falkenhayn ordered that infantry advance close to the barrage to exploit the neutralising effect of the shell-fire on surviving defenders, because fresh troops at Verdun had not been experienced in these methods. Knobelsdorf persisted with attempts to maintain momentum, which was incompatible with the methods of casualty conservation, which could be implemented only with limited attacks with pauses to consolidate and prepare. Mudra and other commanders who disagreed were sacked. Falkenhayn also intervened to change German defensive tactics and advocated a dispersed defence with the second line to be held as a main line of resistance and jumping-off point for counter-attacks. Machine-guns were to be set up with overlapping fields of fire and infantry given specific areas to defend. When French infantry attacked, they were to be isolated by "Sperrfeuer" (barrage-fire) on their former front line, to increase French infantry casualties. The changes desired by Falkenhayn had little effect, because the main cause of German casualties was artillery-fire, just as it was for the French.
4–24 May.
From 10 May German operations were limited to local attacks, either in reply to French counter-attacks on 11 April between Douaumont and Vaux and on 17 April between the Meuse and Douaumont, or local attempts to take points of tactical value. At the beginning of May, General Pétain was promoted to the command of "Le groupe d'armées du centre" (GAC) and General Nivelle took over the Second Army at Verdun. From 4–24 May German attacks were made on the west bank around Mort-Homme and on 4 May the north slope of Côte 304 was captured; French counter-attacks from 5–6 May were repulsed. The French defenders on the crest of Côte 304 were forced back on 7 May but German infantry were unable to occupy the ridge because of the intensity of French artillery-fire. Cumieres and Caurettes fell on 24 May as a French counter-attack began at Fort Douaumont.
22–24 May.
In May General Robert Nivelle who had taken over the Second Army, ordered General Charles Mangin, commander of the 5th Division to plan a counter-attack on Fort Douaumont. The initial plan was for an attack on a 3 km front but several minor German attacks captured "Fausse-Côte" and "Couleuvre" ravines on the south-eastern and western sides of the fort. A further attack took the ridge south of the "ravin de Couleuvre", which gave the Germans better routes for counter-attacks and observation over the French lines to the south and south-west. Mangin proposed a preliminary attack to retake the area of the ravines, to obstruct the routes by which a German counter-attack on the fort could be made. More divisions were necessary but these were refused, to preserve the troops needed for the forthcoming offensive on the Somme; Mangin was limited to one division for the attack with one in reserve. Nivelle reduced the attack to an assault on Morchée Trench, Bonnet-d'Evèque, Fontaine Trench, Fort Douaumont, a machine-gun turret and Hongrois Trench, which would be an advance of 500 m on a 1150 m front.
III Corps was to command the attack by the 5th Division and the 71st Brigade, with support from three balloon companies for artillery-observation and a fighter group. The main effort was to be conducted by two battalions of the 129th Infantry Regiment, each with a pioneer company and a machine-gun company attached. The 2nd Battalion was to attack from the south and the 1st Battalion was to move along the west side of the fort to the north end, taking Fontaine Trench and linking with the 6th Company. Two battalions of the 74th Infantry Regiment were to advance along the east and south-east sides of the fort and take a machine-gun turret on a ridge to the east. Flank support was arranged with neighbouring regiments and diversions were planned near Fort Vaux and the "raviin de Dame". Preparations for the attack included the digging of 12 km of trenches and the building of large numbers of depots and stores but little progress was made due to a shortage of pioneers. French troops captured on 13 May disclosed the plan to the Germans, who responded by subjecting the area to more harassing fire by artillery, which also slowed French preparations.
The French preliminary bombardment by four 370 mm mortars and 300 heavy guns, began on 17 May and by 21 May the French artillery commander claimed that the fort had been severely damaged. During the bombardment the German garrison of the fort experienced great strain, as French heavy shells smashed holes in the walls and concrete dust, exhaust fumes from an electricity generator and disinterred corpses polluted the air. Water ran short but until 20 May, the fort remained operational, observation reports being passed back and reinforcements moving forward until the afternoon, when the Bourges Casemate was isolated and the wireless station in the north-western machine-gun turret was burnt down. Conditions for the German infantry in the vicinity of the fort were far worse and by 18 May, the French destructive bombardment had obliterated many defensive positions, the survivors taking post in shell-holes and dips on the ground. Communication with the rear was severed and food and water ran out by the time of the French attack on 22 May. The troops of Infantry Regiment 52 in front of Fort Douaumont had been reduced to 37 men near Thiaumont Farm and German counter-barrages inflicted similar losses on French troops; French aircraft attacked eight observation balloons and the Fifth Army headquarters at Stenay on 22 May. Six balloons were shot down but the German artillery fire increased and twenty minutes before zero hour, a German bombardment began which reduced the 129th Infantry Regiment companies to about 45 men each.
The assault began at 11:50 a. m. on the 22 May on a 1 km front. On the left flank the 36th Infantry Regiment quickly captured Morchée Trench and Bonnet-d'Evèque but lost many casualties and advanced no further. The flank guard on the right was pinned down, except for one company which disappeared and in "Bois Caillette" a battalion of the 74th Infantry Regiment was unable to leave its trenches; the other battalion managed to reach its objectives at an ammunition depot and shelter called "DV1" at the edge of "Bois Caillette" and the machine-gun turret east of the fort, where the battalion found its flanks unsupported. Despite German small-arms fire, the 129th Infantry Regiment reached the fort in a few minutes and managed to get inside, through the west and south sides. By nightfall about half of the fort had been recaptured and next day the 34th Division was sent to reinforce the fort. The reinforcements were repulsed and German reserves managed to cut off the French troops in the fort and force them to surrender, 1000 French prisoners being taken. After three days the French had lost 5,640 casualties from the 12,000 men in the attack and German casualties in Infantry Regiment 52, Grenadier Regiment 12 and Leib-Grenadier Regiment 8 were 4,500 men.
30 May – 7 June.
Later in May 1916, the German attacks shifted from the left bank (Mort-Homme and Côte 304) and returned to the right bank, south of Fort Douaumont. A German offensive began to reach Fleury ridge, the last French defensive line and take "Ouvrage de Thiaumont", Fleury, Fort Souville and Fort Vaux at the north-east extremity of the French line, which had been bombarded by c. 8,000 shells a day, since the beginning of the Verdun offensive. After a final assault on 1 June by  10,000 German troops, the top of the fort was occupied on 2 June and fighting went on underground until the garrison ran out of water and surrendered on 7 June. In five days the German attack had advanced 65 m for a loss of 2,700 killed against 20 French casualties. When news of the loss of Fort Vaux reached Verdun, the Line of Panic was occupied and trenches were dug on the edge of the city. On the left bank the German advanced from the line Côte 304, Mort-Homme and Cumières and threatened Chattancourt and Avocourt. Heavy rains slowed the German advance towards Fort Souville, where attacks followed counter-attacks for the next two months.
22–25 June.
On 22 June, German artillery fired over 116,000 Diphosgene (Green Cross) gas shells at French artillery positions, which caused over 1,600 casualties and silenced much of the French artillery. Next day the German attack on a 5 km front at 5:00 a.m., drove a 3 x salient into the French defences unopposed until 9:00 a.m., when some French troops were able to fight a rearguard action. The Ouvrage de Thiaumont and the Ouvrage de Froidterre at the south end of the plateau were captured and the village of Fleury and Chapelle Sainte-Fine were overrun. The attack came close to Fort Souville, which since April had been hit by  38,000 shells, and brought the Germans to within 5 km of the Verdun citadel. Chapelle Sainte-Fine was quickly recaptured by a French counter-attack and the German advance was halted. The supply of water to the German infantry broke down, the salient was vulnerable to fire from three sides and the attack could not go on without Diphosgene ammunition. Chapelle Sainte-Fine became the furthest point reached by the German Verdun offensive and on 24 June, the Anglo-French preliminary bombardment began on the Somme. Fleury changed hands sixteen times from 23 June – 17 August. Four French divisions were diverted to Verdun from the Somme and the French artillery recovered sufficiently on 24 June, to cut off the German front line from the rear. By 25 June both sides were exhausted and Knobelsdorf suspended the attack.
Fourth phase 1 July – 17 December.
By the end of May French casualties at Verdun had risen to  185,000 and in June German losses had reached  200,000 men. The opening of the Battle of the Somme on 1 July, forced the Germans to withdraw some of their artillery from Verdun, which was the first strategic success of the Anglo-French offensive. On 29 August Falkenhayn was replaced as Chief of the General Staff by Paul von Hindenburg and First Quartermaster-General Erich Ludendorff.
9–15 July.
Fort Souville dominated a crest 1 km south-east of Fleury and its capture would give the Germans control of the heights overlooking Verdun. The German preparatory bombardment began on 9 July, with an attempt to incapacitate French artillery with over 60,000 gas shells which had little effect, since the French had been equipped with an improved M2 gas mask. Fort Souville and its approaches were bombarded with more than 300,000 shells, including some five hundred 14 in shells on the fort. An attack by three German divisions began on 11 July, in which German infantry bunched on the path leading to Fort Souville and came under heavy fire from French artillery. The surviving troops were fired on by sixty French machine gunners who emerged from the fort and took post on the superstructure. Thirty soldiers of Infantry Regiment 140 managed to reach the top of the fort on 12 July, from where the Germans could see the roofs of Verdun and the spire of the cathedral but after a small French counter-attack, the survivors retreated to their start lines or surrendered. On the evening of 11 July Crown Prince Wilhelm was ordered by Falkenhayn to go onto the defensive and on 15 July, the French conducted a larger counter-attack which gained no ground; for the rest of the month the French made only small attacks.
1 August – 17 September.
On 1 August a German surprise-attack advanced 800 – towards Fort Souville, which prompted French counter-attacks for two weeks, which retook only a small amount of the captured ground. On 18 August Fleury was recaptured and by September French counter-attacks had recovered much of the ground lost in July and August. On 3 September an attack on both flanks at Fleury advanced the French line several hundred metres, against which German counter-attacks from 4–5 September failed. The French attacked again on 9, 13 and 15–17 September. Losses were light except at the Tavannes railway tunnel where 474 French troops died in a fire which began on 4 September.
20 October – 2 November.
In October 1916 the French began the "1ère Bataille Offensive de Verdun" (First Offensive Battle of Verdun), to recapture Fort Douaumont, an advance of more than 2 km. Seven of the 22 divisions at Verdun were replaced by mid-October and French infantry platoons were reorganised to contain riflemen, grenadiers and machine-gunners. In a six-day preliminary bombardment, the French artillery fired 855,264 shells, including 532,926 × 75mm field-gun shells, 100,000 × 155mm medium shells and 373 × 370mm and 400mm super-heavy shells, from more than 700 guns and howitzers. Two French Saint-Chamond railway guns, 13 km to the south-west at Baleycourt, fired 400 mm shells, each weighing 1 ST. At least 20 super-heavy shells hit Fort Douaumont, the sixth penetrating the lowest level and exploding in a pioneer depot, starting a fire next to 7,000 hand-grenades.
The 38th, 133rd and 74th divisions attacked at 11:40 a.m., 50 m behind a creeping field-artillery barrage, moving at a rate of 50 m in two minutes, beyond which a heavy artillery barrage moved in 500 – lifts, as the field artillery barrage came within 150 m, to force the German infantry and machine-gunners to stay under cover. The Germans had partly evacuated Douaumont, which was recaptured on 24 October, by French marines and colonial infantry; more than 6,000 prisoners and fifteen guns were captured by 25 October but an attempt on Fort Vaux failed. The Haudromont quarries, Ouvrage de Thiaumont and Thiaumont Farm, Douaumont village, the northern end of Caillette Wood, Vaux pond, the eastern fringe of Bois Fumin and the Damloup battery were captured. The heaviest French artillery bombarded Fort Vaux for the next week and on 2 November, the Germans evacuated the fort, after a huge explosion was caused by a 220mm shell. French eavesdroppers overheard a German wireless message announcing the departure and a French infantry company entered the fort without firing a shot; on 5 November, the French reached the front line of 24 February; operations ceased until December.
15–17 December 1916.
An offensive by four divisions and four in reserve, planned by General Nivelle and executed by General Mangin, began at 10:00 a.m. on 15 December, after a six-day bombardment by 1,169,000 shells fired from 827 guns. The final French bombardment was directed by observation aircraft crews and fell on trenches, dug-out entrances and observation posts. Five German divisions supported by 533 guns held the defensive position, which was 2300 m deep, with 2⁄3 of the infantry in the battle zone and the remaining 1⁄3 in reserve 10 – back; two of the German divisions were understrength with only  3,000 infantry, instead of their normal establishment of  7,000. The attack was preceded by a double creeping barrage, shrapnel-fire from field artillery 64 m in front of the infantry and a high-explosive barrage 140 m ahead, which moved towards a shrapnel bombardment along the German second line, laid to cut off the German retreat and block the advance of reinforcements. The German defence collapsed and 13,500 troops of the 21,000 in the five front divisions were lost, most having been caught under cover and taken prisoner when the French infantry arrived.
The French reached their objectives at Vacherauville and Louvemont which had been lost in February, along with Hardaumont and Pepper Hill, despite very bad weather. German reserve battalions did not reach the front until the evening and two Eingreif divisions, which had been ordered forward the previous evening, were still 23 km away at midday. By the night of 16/17 December, the French had consolidated a new line from Bezonvaux to Côte du Poivre, 2 – beyond Douaumont and 1 km north of Fort Vaux, before the German reserve and "Eingreif" units could counter-attack. The 155mm turret at Douaumont had been repaired and fired in support of the French attack. The closest German point to Verdun had been pushed 7.5 km back from Verdun and all the dominating observation points had been recaptured. The French took 11,387 prisoners and 115 artillery pieces. Some German officers complained to Mangin about their lack of comfort in captivity, who replied, "We do regret it, gentlemen, but then we did not expect so many of you". General von Lochow, the Fifth Army commander and General von Zwehl, commander of XIV Reserve Corps were sacked on 16 December.
Subsequent operations.
20–26 August 1917.
On 20 August 1917, the "2ème Bataille Offensive de Verdun" (Second Offensive Battle of Verdun) was carried out by the XIII, XVI, XV and XXXII corps, to capture Côte 304 and Mort Homme on the west bank and Côte Talou and Beaumont on the east bank. The plan required an advance of 1 – on a 10 km front. On 11 August, an artillery preparation by  3,000 guns on a 4 × area began and by 20 August, the French artillery had fired 3,000,000 rounds, including 1,000,000 heavy shells, along with a machine-gun bombardment fired on tracks, crossroads, supply lines and German artillery batteries. In four days, French troops captured Bois d'Avocourt, Mort-Homme, Bois Corbeaux and the Bismarck, Kronprinz and Gallwitz tunnels, which had connected the German front lines to the rear, underneath Mort-Homme and Côte 304. On the right bank, Bois Talou, Champneuville, Côte 344, part of Bois Fosse, Bois Chaume and Mormont Farm were captured. Next day Côte 304, Samogneux and Régnieville fell and on 26 August, the French reached the southern outskirts of Beaumont. By 26 August, the French had captured 9,500 prisoners, thirty guns, 100 trench mortars and 242 machine-guns.
7 September 1917.
After the success of the attack in August, Guillaumat was ordered to plan an operation to capture several trenches and a more ambitious offensive to take the last ground from which German artillery-observers could see Verdun. Pétain questioned Guillaumat and Fayolle, who argued that the French could not remain in their present positions and must either advance or retire, advocating a limited advance to make German counter-attacks harder, improve conditions in the front line and deceive the Germans about French intentions. The two corps on the east bank made small attacks, XV Corps on 7 September which failed and XXXII Corps the next day which was a costly success. The attack continued and the trenches necessary for a secure defensive position were taken but not the last German observation point. Further attempts to advance were met by massed artillery-fire and counter-attacks; the French commanders ended the operation.
Meuse–Argonne Offensive.
The French Fourth Army and the American First Army attacked on a front from Moronvillers to the Meuse on 26 September 1918 at 5:30 p.m., after a three-hour bombardment. American troops quickly captured Malancourt, Bethincourt and Forges on the left bank of the Meuse and by midday the Americans had reached Gercourt, Cuisy, the southern part of Montfaucon and Cheppy. German troops were able to repulse American attacks on Montfaucon ridge, until it was outflanked to the south and Montfaucon was surrounded. German counter-attacks from 27–28 September slowed the American advance but Ivoiry and Epinon-Tille were captured, after which Montfaucon ridge was taken along with 8,000 prisoners and 100 guns. On the right bank of the Meuse, a combined Franco-American force under American command, took Brabant, Haumont, Bois d'Haumont and Bois des Caures and then crossed the front line of February 1916. By November,  20,000 prisoners,  150 guns,  1,000 trench-mortars and several thousand machine-guns had been captured. A German retreat began and continued until the Armistice.
Aftermath.
Analysis.
Falkenhayn wrote in his memoir that he sent an appreciation of the strategic situation to the Kaiser in December 1915,
The string in France has reached breaking point. A mass breakthrough—which in any case is beyond our means—is unnecessary. Within our reach there are objectives for the retention of which the French General Staff would be compelled to throw in every man they have. If they do so the forces of France will bleed to death.—Falkenhayn
The German strategy in 1916 was to inflict mass casualties on the French, a goal which had been achieved in Russia in 1914–1915, to weaken the French Army to the point of collapse. The French Army had to be drawn into a situation from which it could not escape, for reasons of strategy and prestige. The Germans planned to use a large number of heavy and super-heavy guns to inflict a greater number of casualties than French artillery, which relied mostly upon the 75mm field gun. Foley wrote that Falkenhayn intended an attrition battle from the beginning, contrary to the views of Krumeich, Foerster and others but that the lack of surviving documents had led to many interpretations of Falkenhayn's strategy. At the time Falkenhayn's critics claimed that the battle demonstrated that he was indecisive and unfit for command; in 1937 Foerster had proposed this view "forcefully". Afflerbach questioned the authenticity of the "Christmas memorandum" in his biography of Falkenhayn and after studying such evidence as had survived in the "Kriegsgeschichtliches Forschungsanstalt des Heeres" (Army Military History Research Institute) files, concluded that the memorandum had been written after the war but that it was an accurate reflection of much of Falkenhayn's thinking in 1916.
Krumeich wrote that the Christmas Memorandum had been fabricated to justify a failed strategy and that attrition had been substituted for the capture of Verdun, only after the city was not taken quickly. Foley wrote that after the failure of the Ypres Offensive of 1914, Falkenhayn had returned to the pre-war strategic thinking of Moltke the Elder and Hans Delbrück on "Ermattungsstrategie" (attrition), because the coalition fighting Germany was too powerful to be decisively defeated by military means alone. German strategy should aim to divide the Allies, by forcing at least one of the Entente powers into a negotiated peace. An attempt at attrition lay behind the offensive against Russia in 1915, although the Russians had refused to accept German peace feelers, despite the huge defeats inflicted by the Austro-Germans in the summer. With insufficient forces to break through the Western Front and to overcome the Entente reserves behind it, Falkenhayn attempted to force the French to attack instead, by threatening a sensitive point close to the front line. Eventually Falkenhayn chose Verdun as the place to force the French to begin a counter-offensive, which would be defeated with huge losses to the French, inflicted by German artillery on the dominating heights around the city. The Fifth Army would begin a big offensive with limited objectives, to seize the Meuse Heights on the right bank of the river, from which German artillery could dominate the battlefield. By being forced into a counter-offensive against such formidable positions, the French Army would "bleed itself white". As the French were weakened, the British would be forced to launch a hasty relief-offensive, which would also be defeated with many casualties. If such defeats were not enough to force negotiations on the French, a German offensive would mop up the last of the Franco-British armies and break the Entente "once and for all".
In a revised instruction to the French army of January 1916, the General Staff had stated that equipment could not be fought by men. Fire power could conserve infantry but a battle of material prolonged the war and consumed over time the casualties which were preserved in each instance. In 1915 and early 1916 German industry quintupled the output of heavy artillery and doubled the production of super-heavy artillery. French production had also recovered since 1914 and by February 1916 the army had 3,500 heavy guns. In May 1916 Joffre implemented a plan to issue each division with two groups of 155mm guns and each corps with four groups of long-range guns. Both sides at Verdun had the means to fire huge numbers of heavy shells at targets, to suppress defences before risking infantry movements. At the end of May, the Germans had 1,730 × heavy guns at Verdun against 548 French, which were sufficient to contain the Germans but not enough for a counter-offensive.
German infantry found that the French endured preparatory bombardments, which was easier than for the Germans since French positions tended to be on dominating ground, not always visible and sparsely occupied. As soon as German infantry attacked the French positions "came to life" and the troops began machine-gun and rapid field artillery-fire. On 22 April the Germans lost 1,000 casualties and in mid-April the French fired 26,000 field artillery shells during an attack to the south-east of Fort Douaumont. A few days after taking over at Verdun Pétain told the air commander, Commandant Charles Tricornot de Rose to sweep away the German air service and to provide observation for the French artillery. German air superiority was challenged and eventually reversed, using eight-aircraft "Escadrilles" for artillery-observation, counter-battery and tactical support.
The fighting at Verdun was less costly to both sides than the war of movement in 1914, which cost the French  850,000 and the Germans  670,000 men from August–December. The German Fifth Army had a lower rate of loss than armies on the Eastern Front in 1915 and the French had a lower average rate of loss at Verdun than the rate over three weeks during the Third Battle of Champagne from September–October 1915, which had not been fought as battles of attrition. During the battle, German loss rates increased relative to French rates, from 2.2:1 in early 1915 to close to 1:1 by the end of the battle and continued at that rate during the Nivelle Offensive in 1917. The main cost of attrition tactics was indecision, limited objective attacks under an umbrella of massed heavy artillery-fire could succeed but created unlimited duration.
Pétain used a "Noria" ("rotation") system, to relieve French troops at Verdun after a short period, which brought most troops of the French army to the Verdun front but for shorter periods than the German troops opposite. French will to resist did not collapse, the symbolic importance of Verdun proved a rallying-point, Falkenhayn was forced to conduct the offensive for much longer than planned and to commit far more infantry than intended. By the end of April most of the German strategic reserve was at Verdun, suffering similar casualties to the French army, although the Germans believed that they were inflicting losses at a rate of 5:2; German military intelligence thought that French casualties up to 11 March had been 100,000 men. Falkenhayn was confident that German artillery could easily inflict another 100,000 losses; in May Falkenhayn estimated that the French had lost 525,000 men against 250,000 German casualties and that the French strategic reserve had been reduced to 300,000 troops. Actual French losses were  130,000 by 1 May and the Noria system had enabled 42 divisions to be withdrawn and rested, when their casualties reached 50 percent. Of the 330 infantry battalions of the French metropolitan army, 259 (78 percent) went to Verdun, against 48 German divisions, 25 percent of the "Westheer" (western army). Afflerbach wrote that 85 French divisions fought at Verdun and that from February to August the ratio of German to French losses was 1:1.1, rather than the third of French losses which was assumed by Falkenhayn. By 31 August, the Fifth Army's losses were 281,000 and French casualties numbered 315,000 men.
In June 1916, the amount of French artillery at Verdun had been increased to 2,708 guns, including 1,138 × 75mm field guns; the French and German armies fired  10,000,000 shells, with a weight of 1350000 LT from February–December. The German offensive had been contained by French reinforcements, difficulties of terrain and the weather by May, with the Fifth Army infantry stuck in tactically dangerous positions, overlooked by the French on the east bank as well as the west bank, instead of secure on the Meuse Heights. Attrition of the French forces was inflicted by constant infantry attacks, which were vastly more costly than waiting for French counter-attacks and defeating them primarily with artillery. Eventually the stalemate was broken by the Brusilov Offensive and the Anglo-French relief offensive on the Somme, the conduct of which had been expected to lead to the collapse of the Anglo-French armies. Falkenhayn had begun to remove divisions from the armies on the Western Front in June, to rebuild the strategic reserve but only twelve divisions could be spared. Four divisions were sent to the Second Army on the Somme, which had dug a layered defensive system based on the experience of the "Herbstschlacht". The situation before the beginning of the battle on the Somme, was considered by Falkenhayn to be better than before previous offensives and a relatively easy defeat of the British offensive was anticipated. No divisions were moved from the Sixth Army, which had 17 1⁄2 divisions and a large amount of heavy artillery, ready for a counter-offensive when the British offensive had been defeated.
The strength of the Anglo-French offensive surprised Falkenhayn and the staff officers of OHL, despite the losses inflicted on the British; the loss of artillery to "overwhelming" counter-battery fire and the policy of instant counter-attack against any Anglo-French advance, led to far more German infantry casualties than at the height of the fighting at Verdun, where 25,989 casualties had been suffered in the first ten days, against 40,187 losses in the first ten days on the Somme. The Brusilov Offensive had recommenced as soon as Russian supplies had been replenished, which inflicted more losses on Austro-Hungarian and German troops during June and July, when the offensive was extended to the north. Falkenhayn was called on to justify his strategy to the Kaiser on 8 July and again advocated sending minimal reinforcements to the east, to continue the "decisive" battle in France, where the Somme offensive was the "last throw of the dice" for the Entente. Falkenhayn had already given up the plan for a counter-offensive near Arras, to reinforce the Russian front and the Second Army, with eighteen divisions moved from the reserve and the Sixth Army front. By the end of August only one division remained in reserve. The Fifth Army had been ordered to limit its attacks at Verdun in June but a final effort was made in July to capture Fort Souville. The effort failed and on 12 July, Falkenhayn ordered a strict defensive policy, with only small local attacks allowed, to try to limit the number of troops the French took from the RFV to add to the Somme offensive.
Falkenhayn had underestimated the French, for whom victory at all costs was the only way to justify the sacrifices already made; the pressure imposed on the French army never came close to making the French collapse and trigger a premature British relief offensive. The ability of the German army to inflict disproportionate losses had also been exaggerated, in part because the Fifth Army commanders had tried to capture Verdun and attacked regardless of loss; even when reconciled to Falkenhayn's attrition strategy they continued to use the costly "Vernichtungsstrategie" (strategy of annihilation) and tactics of "Bewegungskrieg" (manoeuvre warfare). Failure to reach the Meuse Heights, forced the Fifth Army to try to advance from poor tactical positions and to impose attrition by infantry attacks and counter-attacks. The unanticipated duration of the offensive, made Verdun a matter of German prestige as much as it was for the French and Falkenhayn became dependent on a British relief offensive and a German counter-offensive to end the stalemate. When it came, the collapse of the southern front in Russia and the power of the Anglo-French attack on the Somme, reduced the German armies to holding their positions as best they could. On 29 August, Falkenhayn was sacked and replaced by Hindenburg and Ludendorff, who ended the German offensive at Verdun on 2 September.
Casualties.
In 1980 Terraine gave  750,000 Franco-German casualties in 299 days of battle; Dupuy and Dupuy gave 542,000 French casualties in 1993. Heer and Naumann calculated 377,231 French and 337,000 German casualties, a monthly average of 70,000 casualties in 2000. Mason wrote in 2000 that there had been 378,000 French and 337,000 German casualties. In 2003, Clayton quoted 330,000 German casualties, of whom 143,000 were killed or missing and 351,000 French losses, 56,000 killed, 100,000 missing or prisoners and 195,000 wounded. Writing in 2005, Doughty gave French casualties at Verdun, from 21 February – 20 December 1916 as 377,231 men of 579,798 losses at Verdun and the Somme; 16 percent of Verdun casualties were known to have been killed, 56 percent wounded and 28 percent missing, many of whom were eventually presumed dead. Doughty wrote that other historians had followed Churchill (1927) who gave a figure of 442,000 casualties, by mistakenly including all French losses on the Western Front. (In 2014, Philpott recorded 377,000 French casualties, of whom 162,000 men had been killed, that German casualties were 337,000 men and that a recent estimate of casualties at Verdun from 1914–1918 was 1,250,000 men.)
In the second edition of "The World Crisis" (1938), Churchill wrote that the figure of 442,000 was for other ranks and the figure of "probably" 460,000 casualties included officers. Churchill gave a figure of 278,000 German casualties of whom 72,000 were killed and expressed dismay that French casualties had exceeded German by about 3:2. Churchill also stated that an eighth needed to be deducted from his figures for both sides, to account for casualties on other sectors, giving 403,000 French and 244,000 German total casualties. Grant gave a figure of 434,000 German casualties in 2005. In 2005, Foley used calculations made by Wendt in 1931 to give German casualties at Verdun from 21 February – 31 August 1916 as 281,000, against 315,000 French casualties. Afflerbach used the same source in 2000 to give 336,000 German and 365,000 French casualties for the fighting at Verdun, from February to December 1916.
In 2013, Jankowski wrote that since the beginning of the war, French army units had produced "états numériques des pertes" every five days, for the Bureau of Personnel at GQG. The health service at the Ministry of War, received daily counts of wounded taken in by hospitals and other services but casualty data was dispersed among regimental depots, the GQG, the "État Civil" which recorded deaths, the "Service de Santé" which counted injuries and illnesses and the "Renseignements aux Familles", which communicated with next-of-kin. Regimental depots were ordered to keep "fiches de position", to record losses continuously and the "Première Bureau" of GQG, began to compare the five-day field reports with the records of hospital admissions. The new system was used to calculate losses since August 1914, which took several months but the system had become established by February 1916. The "états numériques des pertes" were used to calculate casualty figures published in the "Journal Officiel", the French Official History and other publications.
The German armies compiled "Verlustlisten" every ten days, which were published by the "Reichsarchiv" in the "deutsches Jahrbuch" of 1924–1925. German medical units kept detailed records of medical treatment at the front and in hospitals and in 1923 the "Zentral Nachweiseamt" published an amended edition of the lists produced during the war, incorporating medical service data not in the "Verlustlisten". Monthly figures of wounded and ill servicemen treated, were published in 1934 in the "Sanitätsbericht". Using such sources for comparisons of losses during a battle is difficult because the information recorded losses over time rather than place. Losses calculated for particular battles could be inconsistent, as in the "Statistics of the Military Effort of the British Empire during the Great War 1914–1920" (1922). In the early 1920s, Louis Marin reported to the Chamber of Deputies but could not give figures per battle, except for some by using numerical reports from the armies, which were unreliable unless reconciled with the system established in 1916.
Some French data excluded lightly wounded and some did not. In April 1917, GQG required that the "états numériques des pertes" discriminate between lightly wounded treated at the front over a period of 20–30 days and severely wounded evacuated to hospitals. Uncertainty over the criteria had not been resolved before the war ended. "Verlustlisten" excluded lightly wounded and the "Zentral Nachweiseamt" records included them. Churchill revised German statistics, by adding 2 percent for unrecorded wounded, in "The World Crisis" written in the 1920s and the British Official Historian added 30 percent. For the Battle of Verdun, the "Sanitätsbericht" contained incomplete data for the Verdun area and did not define "wounded" and the 5th Army field reports exclude them. The Marin Report and "Service de Santé" covered different periods but included lightly wounded. Churchill used a "Reichsarchiv" figure of 428,000 casualties and took a figure of 532,500 casualties from the Marin Report, for March–June and November–December 1916 for all the Western Front.
The "états numériques des pertes" give French losses in a range from 348,000–378,000 and in 1930, Wendt recorded for the French Second Army and the German Fifth Army, casualties of 362,000 and 336,831 respectively from 21 February – 20 December, which did not take account of the inclusion or exclusion of lightly wounded. In 2006, McRandle and Quirk used the "Sanitätsbericht" to adjust the "Verlustlisten" by an increase of  11 percent, which gave a total of 373,882 German casualties, compared to the French Official History record by 20 December 1916, of 373,231 French losses. A German record from the "Sanitätsbericht" which explicitly excluded lightly wounded, compared German losses at Verdun in 1916, which averaged 37.7 casualties for each 1,000 men, with the 9th Army in Poland 1914 average of 48.1 per 1,000, the 11th Army average in Galicia 1915 of 52.4 per 1,000 men, the 1st Army on the Somme 1916 average of 54.7 per 1,000 and the 2nd Army average on the Somme of 39.1 per 1,000 men. Jankowski estimated an equivalent figure for the French Second Army of 40.9 men per 1,000, "including" lightly wounded. With a  11 percent adjustment following McRandle and Quirk, to the German figure of 37.7 per 1,000 to include lightly wounded, the loss rate is analogous to the estimate for French casualties.
Morale.
The concentration of so much fighting in such a small area devastated the land, resulting in miserable conditions for troops on both sides. Rain combined with the constant tearing up of the ground turned the clay of the area to a wasteland of mud full of human remains. Shell craters became filled with a liquid ooze, becoming so slippery that troops who fell into them or took cover in them could drown. Forests were reduced to tangled piles of wood by constant artillery-fire and eventually obliterated. The effect on soldiers in the battle was devastating, many broke down with shell-shock and some French soldiers attempted to desert to Spain, those caught being Court-martialed and shot. On 20 March, French deserters disclosed details of the French defences to the Germans, who were able to surround 2,000 men and force them to surrender. Many troops at the battle never saw an enemy soldier, experiencing nothing but artillery fire. Troops on both sides called Verdun "Hell".
A French lieutenant at Verdun who was later killed by a shell, wrote in his diary on 23 May 1916,
 Humanity is mad. It must be mad to do what it is doing. What a massacre! What scenes of horror and carnage! I cannot find words to translate my impressions. Hell cannot be so terrible. Men are mad!
 — Lieutenant Alfred Joubaire
Discontent began to spread among French troops at Verdun during the summer of 1916. Following the promotion of General Pétain from the Second Army on 1 June and his replacement by General Nivelle, five infantry regiments were affected by short-lived episodes of "collective indiscipline". Two French Lieutenants, Henri Herduin and Pierre Millant, were summarily shot on 11 June; Nivelle then published an Order of the Day forbidding French troops to surrender. In 1926, after an inquiry resulting from this cause célèbre, Herduin and Millant were exonerated and their official military records expunged. Period photographs show overlapping shell craters in an area of about 100 sqkm. Forests planted in the 1930s have grown up and hide most of the "Zone rouge" (Red Zone) but the battlefield remains a vast graveyard, where the mortal remains of over 100,000 missing soldiers still lie, unless discovered by the French Forestry Service and laid in the Douaumont ossuary.
Commemoration.
In April 1916, Pétain had issued an Order of the Day, "Courage! On les aura" ("Courage! We shall get them") and on 23 June 1916, Nivelle issued: "They shall not pass", a simplification of the actual French text,
 Vous ne les laisserez pas passer, mes camarades (You will not let them pass, my comrades.).
 — Nivelle 
Nivelle had been concerned about diminished French morale at Verdun; after Nivelle's promotion to lead the Second Army in June 1916, manifestations of indiscipline occurred in five front line regiments. "Défaillance" reappeared in the French army mutinies that followed the Nivelle offensive of April–May 1917.
Marshal Pétain praised what he saw as the success of the fixed fortification system at Verdun in his war memoir: "La Bataille de Verdun" published in 1929 and in 1930, as construction of the Maginot Line ("Ligne Maginot") began along the border with Germany. At Verdun, French field artillery in the open outnumbered turreted guns in the Verdun forts by at least two hundred to one. It was the mass of French field artillery (over 2,000 guns after May 1916) which inflicted about 70 percent of German infantry casualties. In 1935, a number of mechanized and motorized units were deployed behind the Maginot line and plans laid to send detachments to fight a mobile defence in front of the fortifications. Verdun remained a symbol of French determination for many years. At the Battle of Dien Bien Phu in 1953–1954, General Christian de Castries remarked that the situation was "... somewhat like Verdun." French forces at Dien Bien Phu were supplied by transport aircraft, using a landing strip in range of Viet Minh artillery; the French forces at Verdun were supplied by road and rail, beyond the reach of German artillery.
Verdun and its horrors have become for the French the representative memory of World War I. Prost wrote that 
 Like Auschwitz, Verdun marks a transgression of the limits of the human condition.
From 1918–1939 the French expressed two memories of the battle, one a patriotic view, embodied in the memorials built on the battlefield and the other the memory of the survivors who recalled the death, suffering and sacrifice of comrades. In the 1960s Verdun became a symbol of Franco-German reconciliation, through remembrance of common suffering and in the 1980s Verdun took on a new identity as the capital of peace. Organizations were formed and old museums were dedicated to the ideals of peace and human rights. On 22 September 1984, German Chancellor Helmut Kohl (whose father had fought near Verdun in World War I) and French President François Mitterrand (who had been taken prisoner nearby in World War II) stood at the Douaumont cemetery, holding hands for several minutes in the driving rain as a gesture of Franco-German reconciliation. In November 1998, German Chancellor Gerhard Schröder did not attend a joint French and German memorial service with French president Jacques Chirac.
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="51499" url="http://en.wikipedia.org/wiki?curid=51499" title="Western Front (World War I)">
Western Front (World War I)

Following the outbreak of World War I in 1914, the German Army opened the Western Front by first invading Luxembourg and Belgium, then gaining military control of important industrial regions in France. The tide of the advance was dramatically turned with the Battle of the Marne. Following the race to the sea, both sides dug in along a meandering line of fortified trenches, stretching from the North Sea to the Swiss frontier with France. This line remained essentially unchanged for most of the war.
Between 1915 and 1917 there were several major offensives along this front. The attacks employed massive artillery bombardments and massed infantry advances. However, a combination of entrenchments, machine gun nests, barbed wire, and artillery repeatedly inflicted severe casualties on the attackers and counterattacking defenders. As a result, no significant advances were made. Among the most costly of these offensives were the Battle of Verdun with a combined 700,000 casualties (estimated), the Battle of the Somme with more than a million casualties (estimated), and the Battle of Passchendaele with roughly 600,000 casualties (estimated).
In an effort to break the deadlock, this front saw the introduction of new military technology, including poison gas, aircraft and tanks. But it was only after the adoption of improved tactics that some degree of mobility was restored. The German Spring Offensive of 1918 was made possible by the Treaty of Brest-Litovsk that marked the end of the conflict on the Eastern Front. Using the recently introduced infiltration tactics, the German armies advanced nearly 60 mi to the west, which marked the deepest advance by either side since 1914 and very nearly succeeded in forcing a breakthrough.
In spite of the generally stagnant nature of this front, this theatre would prove decisive. The inexorable advance of the Allied armies during the second half of 1918 persuaded the German commanders that defeat was inevitable, and the government was forced to sue for conditions of an armistice. The terms of peace were agreed upon with the signing of the Treaty of Versailles in 1919.
1914—German invasion of France and Belgium.
At the outbreak of the First World War, the German Army (consisting in the West of seven field armies) executed a modified version of the Schlieffen Plan, designed to quickly attack France through neutral Belgium before turning southwards to encircle the French army on the German border. Belgium's neutrality was guaranteed by Britain under the 1839 Treaty of London; this caused Britain to join the war at the expiration of its ultimatum at 11 pm GMT on 4 August. Armies under German generals Alexander von Kluck and Karl von Bülow attacked Belgium on 4 August 1914. Luxembourg had been occupied without opposition on 2 August. The first battle in Belgium was the Siege of Liège, which lasted from 5–16 August. Liège was well fortified and surprised the German army under von Bülow with its level of resistance. However, German heavy artillery was able to ruin the key forts within a few days. Following the fall of Liège, most of the Belgian army retreated to Antwerp and Namur, with the Belgian capital, Brussels, falling to the Germans on 20 August. Although the German army bypassed Antwerp, it remained a threat to their flank. Another siege followed at Namur, lasting from about 20–23 August.
For their part, the French had five Armies deployed on their borders. The pre-war French offensive plan, Plan XVII, was intended to capture Alsace-Lorraine following the outbreak of hostilities. On 7 August the VII Corps attacked Alsace with its objectives being to capture Mulhouse and Colmar. The main offensive was launched on 14 August with 1st and 2nd Armies attacking toward Sarrebourg-Morhange in Lorraine. In keeping with the Schlieffen Plan, the Germans withdrew slowly while inflicting severe losses upon the French. The French advanced the 3rd and 4th army toward the Saar River and attempted to capture Saarburg, attacking Briey and Neufchateau, before being driven back. The French VII Corps captured Mulhouse after a brief engagement on 7 August, but German reserve forces engaged them in the Battle of Mulhouse and forced a French retreat.
The German army swept through Belgium, executing civilians and razing villages. The application of "collective responsibility" against a civilian population further galvanised the allies, and newspapers condemned the German invasion and the army's violence against civilians and property, together called the "Rape of Belgium". (A modern author uses the term only in the narrower sense of describing the war crimes committed by the German army during this period.) After marching through Belgium, Luxembourg and the Ardennes, the German Army advanced, in the latter half of August, into northern France where they met both the French army, under Joseph Joffre, and the initial six divisions of the British Expeditionary Force, under Sir John French. A series of engagements known as the Battle of the Frontiers ensued. Key battles included the Battle of Charleroi and the Battle of Mons. In the former battle the French 5th Army was almost destroyed by the German 2nd and 3rd Armies and the latter delayed the German advance by a day. A general Allied retreat followed, resulting in more clashes such as the Battle of Le Cateau, the Siege of Maubeuge and the Battle of St. Quentin (Guise).
The German army came within 70 km of Paris, but at the First Battle of the Marne (6–12 September), French and British troops were able to force a German retreat by exploiting a gap which appeared between the 1st and 2nd Armies, ending the German advance into France. The German army retreated north of the Aisne River and dug in there, establishing the beginnings of a static western front that was to last for the next three years. Following this German setback, the opposing forces tried to outflank each other in the Race for the Sea, and quickly extended their trench systems from the North Sea to the Swiss frontier. The resulting German-occupied territory held 64% of France's pig-iron production, 24% of its steel manufacturing and 40% of the total coal mining capacity, dealing a serious, but not crippling setback to French industry.
On the Entente side, the final lines were occupied by the armies of the Allied countries, with each nation defending a part of the front. From the coast in the north, the primary forces were from Belgium, the British Empire and France. Following the Battle of the Yser in October, the Belgian forces controlled a 35 km length of Belgium's Flanders territory along the coast, known as the Yser Front, along the Yser river and the Yperlee canal, from Nieuwpoort to Boesinghe. Stationed to the south was the sector of the British Expeditionary Force (BEF). Here, from 19 October until 22 November, the German forces made their final breakthrough attempt of 1914 during the First Battle of Ypres. Heavy casualties were suffered on both sides but no breakthrough occurred. After the battle Erich von Falkenhayn reasoned that it was no longer possible for Germany to win the war, and on 18 November 1914 he called for a diplomatic solution, but Chancellor Theobald von Bethmann-Hollweg, Paul von Hindenburg and Erich Ludendorff disagreed. By Christmas, the BEF guarded a continual line from the La Bassée Canal to south of St. Eloi in the Somme valley. The greater part of the front, south to the border with Switzerland, was manned by French forces.
1915—Stalemate.
Between the coast and the Vosges was an outward bulge in the trench line, named the Noyon salient for the captured French town at the maximum point of advance near Compiègne. Joffre's plan for 1915 was to attack this salient on both flanks to cut it off. The British would form the northern attack force by pressing eastward in Artois, while the French attacked in Champagne.
On 10 March, as part of what was intended as a larger offensive in the Artois region, the British army attacked at Neuve Chapelle in an effort to capture the Aubers Ridge. The assault was made by four divisions along a 2 mi front. Preceded by a concentrated bombardment lasting 35 minutes, the initial assault made rapid progress and the village was captured within four hours. The advance then slowed because of problems with logistics and communications. The Germans then brought up reserves and counter-attacked, forestalling the attempt to capture the ridge. Since the British had used about one-third of their supply of artillery shells, General Sir John French blamed the failure on the shortage of shells, despite the success of the initial attack.
Gas warfare.
All sides signed treaties (the Hague Conventions of 1899 and 1907) which prohibited the use of chemical weapons in warfare before World War I. In spite of this, World War I saw large-scale chemical warfare.
Despite the German plans to maintain the stalemate with the French and British, German commanders planned an offensive at the Belgian town of Ypres, which the British had defended in November 1914. This Second Battle of Ypres was intended to divert attention from offensives in the Eastern Front while disrupting Franco-British planning and to test a new weapon: the second mass use of chemical weapons. (Ypres is frequently cited as the first use of gas but this had occurred at Bolimow, on the Eastern Front.) On 22 April, after a two-day bombardment, the Germans released of chlorine gas onto the battlefield. Being heavier than air, the gas crept across no man's land and drifted into the British trenches. The green-yellow cloud asphyxiated some defenders and those in the rear fled in panic, creating an undefended 6 km gap in the Allied line. The Germans were unprepared for the level of their success and lacked sufficient reserves to exploit the opening. Canadian troops quickly arrived and drove back the German advance.
The gas attack was repeated two days later and caused a 5 km withdrawal of the Franco-British line but the opportunity had been lost. The success of this attack would not be repeated, as the Allies countered by introducing gas masks and other countermeasures. An example of the success of these measures came a year later, on 27 April at Hulluch 25 mi to the south of Ypres, where the 16th (Irish) Division withstood several German gas attacks.
Air warfare.
This year also saw the introduction of aeroplanes specifically modified for aerial combat. While planes had already been used in the war for scouting, on 1 April the French pilot Roland Garros became the first to shoot down an enemy plane by using a machine gun that shot forward through the propeller blades. This was achieved by crudely reinforcing the blades so bullets which hit them were deflected away.
Several weeks later Garros was forced to land behind German lines. His plane was captured and sent to Dutch engineer Anthony Fokker, who soon produced a significant improvement, the interrupter gear, in which the machine gun is synchronised with the propeller so it fires in the intervals when the blades of the propeller are out of the line of fire. This advance was quickly ushered into service, in the Fokker E.I ("Eindecker", or monoplane, Mark 1), the first single seat fighter aircraft to combine a reasonable maximum speed with an effective armament; Max Immelmann scored the first confirmed kill in an "Eindecker" on 1 August.
This started a back-and-forth arms race, as both sides developed improved weapons, engines, airframes and materials, which continued until the end of the war. It also inaugurated the cult of the ace, the most famous being the Red Baron. Contrary to the myth, antiaircraft fire claimed more kills than fighters.
Continued Entente attacks.
The final Entente offensive of the spring was fought at Artois, with the goal of trying to capture Vimy Ridge. The French 10th Army attacked on 9 May after a six-day bombardment and advanced 3 mi. However, they retreated as they had come into sights of machine gun nests and the German reinforcements fired artillery at the attackers. By 15 May the advance had been stopped, although the fighting continued until 18 June.
In May the German army captured a French document at La Ville-aux-Bois describing a new system of defence. Rather than relying on a heavily fortified front line, the defence is arranged in a series of echelons. The front line would be a thinly manned series of outposts, reinforced by a series of strongpoints and a sheltered reserve. If a slope was available, troops were deployed along the rear side for protection. The defence became fully integrated with command of artillery at the divisional level. Members of the German high command viewed this new scheme with some favour and it later became the basis of an elastic defence in depth doctrine against Entente attacks.
During autumn of 1915, the "Fokker Scourge" began to have an effect on the battlefront as Allied spotter planes were nearly driven from the skies. These reconnaissance planes were used to direct gunnery and photograph enemy fortifications but now the Allies were nearly blinded by German fighters. However, the impact of German air superiority was diminished by their doctrinal reluctance to risk their pilots capture by fighting over Allied held territory.
In September 1915 the Entente allies launched another offensive, with the French attacking at Champagne and the British at Loos. The French had spent the summer preparing for this action, with the British assuming control of more of the front to release French troops for the attack. The bombardment, which had been carefully targeted by means of aerial photography, began on 22 September. The main French assault was launched on 25 September and at first made good progress, in spite of surviving wire entanglements and machine gun posts. Rather than retreating, the Germans adopted a new defence-in-depth scheme that consisted of a series of defensive zones and positions with a depth of up to 5 mi.
On 25 September, the British began their assault at Loos, which was meant to supplement the larger Champagne attack. The attack was preceded by a four-day artillery bombardment of 250,000 shells and a release of 5,100 cylinders of chlorine gas. The attack involved two corps in the main assault and two more corps performing diversionary attacks at Ypres. The British suffered heavy losses, especially due to machine gun fire, during the attack and made only limited gains before they ran out of shells. A renewal of the attack on 13 October fared little better. In December, British Field Marshal Sir John French was replaced by General Douglas Haig as commander of the British forces.
1916—Artillery duels and attrition.
The German Chief of Staff, Erich von Falkenhayn, believed that a breakthrough might no longer be possible, and instead focused on forcing a French capitulation by inflicting massive casualties. His new goal was to "bleed France white".
As such, he adopted two new strategies. The first was the use of unrestricted submarine warfare to cut off Allied supplies arriving from overseas. The second would be targeted, high-casualty attacks against the French ground troops. To inflict the maximum possible casualties, he planned to attack a position from which the French could not retreat for reason of both strategic positions and national pride and thus trap the French. The town of Verdun was chosen for this because it was an important stronghold, surrounded by a ring of forts, that lay near the German lines and because it guarded the direct route to Paris. The operation was codenamed "Gericht", German for "court", but meant "place of execution".
Falkenhayn limited the size of the front to 3 to to concentrate their firepower and to prevent a breakthrough from a counteroffensive. He also kept tight control of the main reserve, feeding in just enough troops to keep the battle going. In preparation for their attack, the Germans had amassed a concentration of aircraft near the fortress. In the opening phase, they swept the air space of enemy spotters which allowed the accurate German artillery spotters and bombers to operate without interference. However, by May, the French countered by deploying "escadrilles de chasse" with superior Nieuport fighters. The tight air space over Verdun turned into an aerial battlefield, and illustrated the value of tactical air superiority, as each side sought to dominate air reconnaissance.
Battle of Verdun.
The Battle of Verdun began on 21 February 1916 after a nine-day delay due to snow and blizzards. After a massive eight-hour artillery bombardment, the Germans did not expect much resistance as they slowly advanced on Verdun and its forts. However, heavy French resistance was encountered. The French lost control of Fort Douaumont. Nonetheless, French reinforcements halted the German advance by 28 February.
The Germans turned their focus to Le Mort Homme to the north from which the French were successfully shelling them. After some of the most intense fighting of the campaign, the hill was taken by the Germans in late May. After a change in French command at Verdun from the defensive-minded Philippe Pétain to the offensive-minded Robert Nivelle the French attempted to re-capture Fort Douaumont on 22 May but were easily repulsed. The Germans captured Fort Vaux on 7 June and, with the aid of the gas diphosgene, came within 1 km of the last ridge over Verdun before stopping on 23 June.
Over the summer, the French slowly advanced. With the development of the rolling barrage, the French recaptured Fort Vaux in November, and by December 1916 they had pushed the Germans back 1.3 mi from Fort Douaumont, in the process rotating 42 divisions through the battle. The Battle of Verdun—also known as the 'Mincing Machine of Verdun' or 'Meuse Mill'—became a symbol of French determination and self-sacrifice.
Battle of the Somme.
In the spring Allied commanders had been concerned about the ability of the French army to withstand the enormous losses at Verdun. The original plans for an attack around the river Somme were modified to let the British make the main effort. This would serve to relieve pressure on the French, as well as the Russians who had also suffered great losses. On 1 July, after a week of heavy rain, British divisions in Picardy launched an attack around the river Somme, supported by five French divisions on their right flank. The attack had been preceded by seven days of heavy artillery bombardment. The experienced French forces were successful in advancing but the British artillery cover had neither blasted away barbed wire, nor destroyed German trenches as effectively as was planned. They suffered the greatest number of casualties (killed, wounded, and missing) in a single day in the history of the British army, about 57,000.
Having assessed the air combat over Verdun, the Allies had new aircraft designed by Citroën engineer Andrew Sywy, for the attack in the Somme valley. The Verdun lesson learnt, the Allies' tactical aim became the achievement of air superiority and the German planes were, indeed, largely swept from the skies over the Somme. The success of the Allied air offensive caused a reorganisation of the German air arm, and both sides began using large formations of aircraft rather than relying on individual combat.
After regrouping, the battle continued throughout July and August, with some success for the British despite the reinforcement of the German lines. By August General Haig had concluded that a breakthrough was unlikely, and instead switched tactics to a series of small unit actions. The effect was to straighten out the front line, which was thought necessary in preparation for a massive artillery bombardment with a major push.
The final phase of the battle of the Somme saw the first use of the tank on the battlefield. The Allies prepared an attack that would involve 13 British and Imperial divisions and four French corps. The attack made early progress, advancing 3,500 to in places, but the tanks had little effect due to their lack of numbers and mechanical unreliability. The final phase of the battle took place in October and early November, again producing limited gains with heavy loss of life. All told, the Somme battle had made penetrations of only 8 km, and failed to reach the original objectives. The British had suffered about 420,000 casualties and the French around 200,000. It is estimated that the Germans lost 465,000, although this figure is controversial.
The Somme led directly to major new developments in infantry organisation and tactics; despite the terrible losses of 1 July, some divisions had managed to achieve their objectives with minimal casualties. In examining the reasons behind losses and achievements, the British, and the Colonial contingents, reintroduced the concept of the infantry platoon, following in the footsteps of the French and German armies who were already groping their way towards the use of small tactical units. At the time of the Somme, British senior commanders insisted that the company (120 men) was the smallest unit of manoeuvre; less than a year later, the section of 10 men would be so.
Hindenburg line.
In August 1916 the German leadership along the western front had changed as Falkenhayn resigned and was replaced by Generals Paul von Hindenburg and Erich Ludendorff. The new leaders soon recognised that the battles of Verdun and the Somme had depleted the offensive capabilities of the German army. They decided that the German army in the west would go over to the strategic defensive for most of 1917, while the Central powers would attack elsewhere.
During the Somme battle and through the winter months, the Germans created a prepared defensive position behind a section of their front that would be called the Hindenburg Line using the defensive principles elaborated since the defensive battles of 1915, including the use of Eingreif divisions. This was intended to shorten the German front, freeing 10 divisions for other duties. This line of fortifications ran from Arras south to St Quentin and shortened the front by about 30 mi. British long-range reconnaissance aircraft first spotted the construction of the Hindenburg Line in November 1916.
1917—British offensives.
The Hindenburg Line was built between two and 30 mi behind the German front line. On 9 February German forces retreated to the line and the withdrawal was completed 5 April, leaving behind a devastated territory to be occupied by the Allies. This withdrawal negated the French strategy of attacking both flanks of the Noyon salient, as it no longer existed. However, offensive advances by the British continued as the High Command claimed, with some justice, that this withdrawal resulted from the casualties the Germans received during the Battles of the Somme and Verdun, despite the Allies suffering greater losses.
Meanwhile, on 6 April the United States declared war on Germany. In early 1915, following the sinking of the "Lusitania", Germany had stopped its unrestricted submarine warfare in the Atlantic because of concerns of drawing the United States into the conflict. With the growing discontent of the German public due to the food shortages, however, the government resumed unrestricted submarine warfare in February 1917. They had calculated that a successful submarine and warship siege of Britain would force that country out of the war within six months, while American forces would take a year to become a serious factor on the Western Front. The submarine and surface ships had a long period of success before Britain resorted to the convoy system, bringing a large reduction in shipping losses.
By 1916–17, the size of the British army on the Western Front had grown to two-thirds the total numbers in the French forces. In April 1917 the British Empire forces launched an attack starting the Battle of Arras. The Canadian Corps and the British 5th Infantry Division, attacked German lines at Vimy Ridge, capturing the heights. However the rest of the offensive was halted with heavy losses. The Allied attack ended with the refusal to provide reinforcements to the region.
During the winter of 1916–17, German air tactics had been improved, a fighter training school was opened at Valenciennes and better aircraft with twin guns were introduced. The result was near disastrous losses for Allied air power, particularly for the British, Portuguese, Belgians, and Australians who were struggling with outmoded aircraft, poor training and weak tactics. As a result the Allied air successes over the Somme would not be repeated, and heavy losses were inflicted by the Germans. During their attack at Arras, the British lost 316 air crews and the Canadians lost 114 compared to 44 lost by the Germans. This became known to the RFC as Bloody April.
French mutinies.
The same month, French General Robert Nivelle ordered a new offensive against the German trenches, promising that it would end the war within 48 hours. The 16 April attack, dubbed the Nivelle Offensive (also known as Chemin des Dames, after the area where the offensive took place), would be 1.2 million men strong, to be preceded by a week-long artillery bombardment and accompanied by tanks. However, the operation proceeded poorly as the French troops, with the help of two Russian brigades, had to negotiate rough, upward-sloping terrain. In addition, detailed planning had been dislocated by the voluntary German withdrawal to the Hindenburg Line, secrecy had been compromised, and German planes gained control of the sky making reconnaissance difficult. This allowed the creeping barrage to move too far ahead of the advancing troops. Within a week 100,000 French troops were dead. Despite the heavy casualties and his promise to halt the offensive if it did not produce a breakthrough, Nivelle ordered the attack continued into May.
On 3 May the weary French 2nd Colonial Division, veterans of the Battle of Verdun, refused their orders, arriving drunk and without their weapons. Lacking the means to punish an entire division, the officers of the division did not immediately implement harsh measures against the mutineers. Thereupon mutinies afflicted 54 French divisions and saw 20,000 men desert. Other Allied forces attacked but suffered massive casualties. Appeals to patriotism and duty followed, as did mass arrests and trials. The French soldiers returned to defend their trenches, but refused to participate in further offensive action. On 15 May Nivelle was removed from command, replaced by General Philippe Pétain who immediately suspended large-scale attacks. The French would go on the defensive for the following months to avoid high casualties and to restore confidence in the French High Command.
British offensives, American troops arrive.
On 7 June a British offensive was launched on Messines Ridge, south of Ypres, to retake the ground lost in the First and Second Battles of Ypres in 1914. Since 1915 specialist Royal Engineer tunnelling companies had been digging tunnels under the ridge, and about 500 tonnes (roughly 500,000 kg) of explosives had been planted in 21 mines under the enemy lines. Following four days of heavy bombardment, the explosives in 19 of these mines were detonated, resulting in the deaths of 10,000 Germans. The offensive that followed again relied on heavy bombardment which allowed the British infantry to capture the ridge in one day. The limited offensive was a great success, all German counter-attacks were defeated and the southern flank of the Gheluvelt plateau protected from German observation.
On 11 July 1917 during this battle, the Germans introduced a new weapon into the war when they fired gas shells delivered by artillery. The limited size of an artillery shell required that a more potent gas be deployed, and so the Germans employed mustard gas, a powerful blistering agent. The artillery deployment allowed heavy concentrations of the gas to be used on selected targets. Mustard gas was also a persistent agent, which could linger for up to several days at a site, an additional demoralising factor for their opponents. Along with phosgene, mustard gas would be used extensively by both German and Allied forces in later battles, as the Allies also began to increase production of gas for chemical warfare.
On 25 June the first US troops began to arrive in France, forming the American Expeditionary Force. However, the American units did not enter the trenches in divisional strength until October. The incoming troops required training and equipment before they could join in the effort, and for several months American units were relegated to support efforts. In spite of this, however, their presence provided a much-needed boost to Allied morale.
Beginning on 31 July and continuing to 10 November the struggle around Ypres was renewed with the Battle of Passchendaele (technically the Third Battle of Ypres, of which Passchendaele was the final phase). The battle had the original aim of capturing the ridges east of Ypres then advancing to Roulers and Thourout to close the main rail line supplying the German garrisons of the Western front and the Belgian coast then capturing the German submarine bases on the Belgian coast, but was later restricted to advancing the British Army onto the ridges around Ypres, as the unusually wet weather slowed British progress. Canadian veterans from the Battle of Vimy Ridge and the Battle of Hill 70 relieved the two ANZAC Corps and other British forces and took the village of Passchendaele on 6 November, despite extremely heavy rain and casualties. The offensive produced large numbers of casualties on both sides for relatively little gain of ground against dogged German resistance, yet that captured was of great tactical importance and the British made inexorable gains during periods of drier weather. The ground was generally muddy and pocked by shell craters, making supply missions and further advancement very difficult.
Both sides lost a combined total of over a half million men during this offensive. The battle has become a byword among some British historians for bloody and futile slaughter, whilst the Germans called Passchendaele "the greatest martyrdom of the War". It is one of the two battles (the other is the Battle of the Somme) which have done most to earn British Commander in Chief Sir Douglas Haig his controversial reputation.
Battle of Cambrai.
On 20 November the British launched the first massed tank attack during the Battle of Cambrai. The Allies attacked with 324 tanks, with one-third held in reserve, and twelve divisions, against two German divisions. To maintain surprise, there was no preparatory bombardment; only a curtain of smoke was laid down before the tanks. The machines carried fascines on their fronts to bridge trenches and 4 m German tank traps. Special "grapnel tanks" towed hooks to pull away the German barbed wire. The initial attack was a success for the British. The British forces penetrated further in six hours than had been achieved at the Third Ypres in four months, and at a cost of only 4,000 British casualties.
However, the advance produced an awkward salient and a surprise German counteroffensive on 30 November drove the British back to their starting lines. Despite the reversal, the attack had been seen as a success by the Allies and Germans as it proved that tanks could overcome trench defences. The battle had also seen the first massed use of German "stosstruppen" on the Western front, who used infantry infiltration tactics to successfully penetrate the Allied lines, bypassing resistance and quickly advancing into the enemy's rear.
1918—Final offensives.
Following the successful Allied attack and penetration of the German defences at Cambrai, Ludendorff and Hindenburg determined that the only opportunity for German victory now lay in a decisive attack along the Western front during the spring, before American manpower became a significant presence. On 3 March 1918, the Treaty of Brest-Litovsk was signed, and Russia withdrew from the war. This would now have a dramatic effect on the conflict as 33 divisions were now released from the Eastern Front for deployment to the West. However, the Germans occupied almost as much Russian territory under the provisions of the Treaty of Brest-Litovsk as they did in the Second World War: this considerably restricted their troop redeployment. Nonetheless, they still had an advantage of 192 divisions to the Allied 178 divisions, which allowed Germany to pull veteran units from the line and retrain them as "sturmtruppen". In contrast, the Allies still lacked a unified command and suffered from morale and manpower problems: the British and French armies were sorely depleted, and American troops had not yet transitioned into a combat role.
Ludendorff's strategy would be to launch a massive offensive against the British and Commonwealth designed to separate them from the French and her allies, then drive them back to the channel ports. The attack would combine the new storm troop tactics with ground attack aircraft, tanks, and a carefully planned artillery barrage that would include gas attacks.
German spring offensives.
"Operation Michael", the first of the German spring offensives, very nearly succeeded in driving the Allied armies apart, advancing about 40 mi during the first eight days and moving the front lines more than 100 km west, within shelling distance of Paris for the first time since 1914.
As a result of the battle, the Allies finally agreed on a unified system of command. General Ferdinand Foch was appointed commander of all Allied forces in France. The unified Allies were now better able to respond to each of the German drives, and the offensive turned into a battle of attrition.
In May, the American divisions also began to play an increasing role, winning their first victory in the Battle of Cantigny. By summer, 300,000 American soldiers were arriving every month. A total of 2.1 million American troops would be deployed on this front before the war came to an end. The rapidly increasing American presence served as a counter for the large numbers of redeployed German forces.
Final allied counter-offensives.
In July, Foch initiated a counter-offensive against the Marne salient produced during the German attacks, eliminating the salient by August. A second major offensive was launched two days after the first, ending at Amiens to the north. This attack included Franco-British forces, and was spearheaded by Australian and Canadian troops, along with 600 tanks and supported by 800 aircraft. The assault proved highly successful, leading Hindenburg to name 8 August as the "Black Day of the German Army". The Italian 2nd Army Corps, commanded by general Alberico Albricci, also participated in the operations around Reims.
The German army's manpower had been severely depleted after four years of war, and its economy and society were under great internal strain. The Entente now fielded a total of 216 divisions against 197 understrength German divisions. The Hundred Days Offensive beginning in August proved the final straw, and following this string of military defeats, German troops began to surrender in large numbers. As the Allied forces broke the German lines, Prince Maximilian of Baden was appointed as Chancellor of Germany in October to negotiate an armistice. Because of his opposition to the peace feelers, Ludendorff was forced to step aside and he fled to Sweden. Fighting was still continuing, but the German armies were in retreat when the German Revolution put a new government in power. An armistice was quickly signed, that stopped all fighting on the Western Front on Armistice Day (11 November 1918). The German Imperial Monarchy collapsed as Ludendorff's successor General Groener agreed, for fear of a revolution like that in Russia the previous year, to support the moderate Social Democratic Government under Friedrich Ebert rather than sustain the Hohenzollern Monarchy.
Consequences.
The war along the Western Front led the German government and its allies to sue for peace in spite of German success elsewhere. As a result the terms of the peace were dictated by France, Britain and the United States, during the 1919 Paris Peace Conference. The result was the Treaty of Versailles, signed in June 1919 by a delegation of the new German government.
The terms of the treaty would effectively cripple Germany as an economic and military power. The Versailles treaty returned the border provinces of Alsace-Lorraine to France, thus limiting the coal required by German industry. The Saar, which formed the west bank of the Rhine, would be demilitarised and controlled by Britain and France, while the Kiel Canal opened to international traffic. The treaty also drastically reshaped Eastern Europe. It severely limited the German armed forces by restricting the size of the army to 100,000 and disallowing a navy or air force. The navy was sailed to Scapa Flow under the terms of surrender but was later scuttled, under the order of German admirals, as a reaction to the treaty.
Germany in 1919 was bankrupt, the people living in a state of semi-starvation, and having no commerce with the remainder of the world. The Allies occupied the Rhine cities of Cologne, Koblenz and Mainz, with restoration dependent on payment of reparations. Among the German populace, the myth arose—openly cultivated by the Army Chief of Staff Hindenburg—that the defeat was not the fault of the 'good core' of the army but due to certain left-wing groups within Germany; this would later be exploited by Nazi party propaganda to partly justify the overthrow of the Weimar Republic. "See" Stab-in-the-back legend.
France suffered heavy damage in the war. In addition to losing more casualties relative to its population than any other great power, the industrial north-east of the country had been devastated by the war. The provinces overrun by Germany had produced 40% of the nation's coal and 58% of its steel output. Once it was clear that Germany was going to be defeated, Ludendorff had ordered the destruction of the mines in France and Belgium. His goal was to cripple the industries of Germany's main European rival. To prevent similar German attacks in the future, France later built a massive series of fortifications along the German border known as the Maginot Line.
The war in the trenches left a generation of maimed soldiers and war widows. The unprecedented loss of life had a lasting effect on popular attitudes toward war, resulting later in an Allied reluctance to pursue an aggressive policy toward Adolf Hitler (himself a decorated veteran of the war). The repercussions of that struggle are still being felt to this day.
Further reading.
</dl>

</doc>
<doc id="51503" url="http://en.wikipedia.org/wiki?curid=51503" title="Progressive rock">
Progressive rock

Progressive rock, also known as prog rock or prog, is a rock music subgenre that originated in the United Kingdom with further developments in Germany, Italy, and France, throughout the mid-to-late 1960s and 1970s. It developed from psychedelic rock, and originated, similarly to art rock, as an attempt to give greater artistic weight and credibility to rock music. Bands abandoned the short pop single in favor of instrumentation and compositional techniques more frequently associated with jazz or classical music in an effort to give rock music the same level of musical sophistication and critical respect.
Progressive rock sometimes abandons the danceable beat that defines earlier rock styles and is more likely to experiment with compositional structure, instrumentation, harmony and rhythm, and lyrical content. It may demand more effort on the part of the listener than other types of music. Musicians in progressive rock typically display a high degree of instrumental skill. Musical forms are blurred through the use of extended sections and of musical interludes that bridge separate sections together, which results in classical-style suites. Early progressive rock groups expanded the timbral palette of the then-traditional rock instrumentation by adding instruments more typical of folk music, jazz or music in the classical tradition. A number of bands, especially at the genre's onset, recorded albums in which they performed together with a full orchestra. 
Progressive rock artists are more likely to explore complex time signatures such as 5/8 and 7/8. Tempo, key and time signature changes are common within progressive rock compositions.
Songs were replaced by musical suites that often stretched to 20 or 40 minutes in length and contained symphonic influences, extended musical themes, philosophical, mystical and/or surreal lyrics and complex orchestrations. The genre was not without criticism, however, as some reviewers found the concepts "pretentious" and the sounds "pompous" and "overblown".
Progressive rock saw a high level of popularity throughout the 1970s, especially in the middle of the decade. Bands such as Pink Floyd, Jethro Tull, The Moody Blues, Yes, King Crimson, Genesis, Camel and Emerson, Lake & Palmer (ELP) were the genre's most influential groups and were among the most popular acts of the era, although there were many other, often highly influential, bands who experienced a lesser degree of commercial success. The genre faded in popularity during the second half of the decade. Conventional wisdom holds that the rise of punk rock caused this, although in reality a number of factors contributed to this decline. Progressive rock bands achieved commercial success well into the 1980s, albeit with changed lineups and more compact song structures.
The genre grew out of the 1960s space rock of Pink Floyd and the classical rock experiments of bands such as The Moody Blues, Procol Harum, The Syn and The Nice. Most of the prominent bands from the genre's 1970s heyday fall into the "symphonic prog" category, in which classical orchestrations and compositional techniques are melded with rock music. Other subgenres exist, including the more accessible neo-progressive rock of the 1980s, the jazz-influenced Canterbury sound of the 1960s and 1970s, and the more political and experimental Rock in Opposition movement of the late 1970s and onward. Progressive rock has influenced genres such as krautrock and post-punk, and it has fused with other forms of rock music to create such subgenres as neo-classical metal and progressive metal. A revival, often known as new prog, occurred at the turn of the 21st century and has since enjoyed a cult following.
Characteristics.
Progressive rock originally referred to progressive pop music or "classical rock" in which a band performed together with an orchestra, but the term's use broadened over time to include Miles Davis-style jazz fusion, some metal and folk rock styles, and experimental German bands. It does not refer to a single style but to an approach that combines elements of diverse styles. Jerry Ewing, editor of "Prog Magazine", explains that "Prog is not just a sound, it's a mindset," and Dream Theater guitarist John Petrucci points out that it is defined by its very lack of stylistic boundaries.
The advent of the concept album and the genre's roots in psychedelia led albums and performances to be viewed as combined presentations of music, lyrics, and visuals. Progressive rock abandons the danceable beat that defines earlier rock styles and is more likely than other types of popular music to experiment with compositional structure, instrumentation, harmony and rhythm, and lyrical content. It may demand more effort on the part of the listener than other types of music.
Musicians in progressive rock typically display a high degree of instrumental skill, although this is not always the case. Neither Greg Lake nor Boz Burrell had ever been a bassist prior to filling that role in King Crimson. Jeffrey Hammond-Hammond joined Jethro Tull because of his social compatibility with the band rather than musical skills. "Jeffrey didn't get into the group because he was a good guitarist," said bandleader Ian Anderson, "because he could hardly play a note." Pink Floyd and Brian Eno are notable examples of artists who are able to build complex structures out of simple parts and who are virtuosos in the sense that their instrument is the recording studio.
Musical aspects.
Form.
Progressive rock songs often avoid common popular music song structures of verse/chorus form, and their extended lengths allow complex themes that cannot be fully developed within the span of a three-minute single. Musical forms are blurred through the use of extended sections and of musical interludes that bridge separate sections together, which results in classical-style suites. These large-scale compositions are similar to medleys, but there is typically more thematic unity between the sections. Transitions between electric and acoustic sections provide dynamic contrast. Extended instrumental passages often mix composed, classical-style sections with group improvisation. These sections emphasize group virtuosity rather than individual skill, and they are a break from other pop forms in which a single, dominant singer or soloist is accompanied by a band. Although many progressive rock songs are of three to five minutes in length, and bands such as Kraftwerk did adhere to pop songwriting principles, long-form pieces of twenty minutes or more are not uncommon.
These extended pieces are usually considered to be the result of experimentation with classical music forms, although an alternative viewpoint holds that they are explorations of the complexities possible within the popular music format. Many bands did, however, use compositional techniques borrowed from classical music. Gentle Giant, whose Kerry Minnear held a degree in composition from the Royal Academy of Music, often used counterpoint in their pieces. Kansas songs such as "Miracles out of Nowhere" often contain complex passages in which the violin and one or more keyboards and guitars all play separate contrapuntal parts. "Close to the Edge," by Yes, uses a classical compositional technique in which the arrangement is developed by the use of varied repetitions of a theme throughout the piece's structureand has elements of sonata form.
Elements of classical music are sometimes borrowed for the cultural significance they carry. Yes frequently used contrapuntal sections to create the impression of a baroque style, as in a fugue-like section at the eight-minute mark of "Close to the Edge" and in the harpsichord solo of "Siberian Khatru." Gentle Giant created a medieval feel through their use of the madrigal.
Instrumentation.
Early progressive rock groups expanded the timbral palette of the then-traditional rock instrumentation of guitar, keyboard, bass guitar, and drums by adding instruments more typical of folk music, jazz or music in the classical tradition. A number of bands, especially at the genre's onset, recorded albums in which they performed together with a full orchestra. The Moody Blues, who until then had been a blues-based British invasion band with a single hit to their credit, launched the trend with the huge success of their "Days of Future Passed" album. "Days" used arrangements that combined the band and orchestra, and it used orchestral interludes to bridge together the individual songs.
Electronic keyboards.
It was impractical to work together with an orchestra on a regular basis, so The Moody Blues turned to the Mellotron as a substitute. The Mellotron is a keyboard instrument that contains tape-recordings of individual notes of various instruments and voices, and plays back their sounds as the keyboard is pressed. Its sounds included woodwinds, choirs, brass and, perhaps most famously, strings. The technology available meant that its sounds were not exact reproductions of the instruments, but instead had a haunting quality that many bands prized. This instrument became the signature sound of The Moody Blues and was closely associated with many later progressive rock acts including Genesis, Strawbs, Pink Floyd and King Crimson.
The Hammond organ is another instrument closely associated with progressive rock. It is a versatile instrument that can function like a pipe organ, can be played through a guitar amplifier for a distorted tone, is capable of sustained notes and rapid melodic runs, and can make percussive sounds. The ability to adjust its timbre while a note is held and its capabilities of vibrato and, when a rotating Leslie speaker is used, tremolo, make it a very expressive lead instrument. The use of organs and choirs reflects the background in Anglican church music shared by many of the genre's founders.
Various other electronic and electro-mechanical keyboard instruments were in common use. The RMI Electra-Piano was favored by Rick Wakeman of Yes, and Genesis keyboardist Tony Banks used its organ sounds to supplement those of the Hammond. RMI pianos could also substitute for harpsichords, as could the Clavinet. The Wurlitzer electric piano was a signature of Supertramp's sound. Some bands, notably Genesis, used Yamaha's electric grand piano, and string synthesizers were sometimes employed.
Synthesizers.
The birth of progressive rock roughly coincided with the commercial availability of synthesizers. Early modular synthesizers were large instruments that used patch cords to route the signal flow. Programming the instruments meant placing the patch cords to connect the individual modules. The Minimoog, a smaller, simplified synthesizer that needed no patch cords, began production in 1971 and provided keyboardists with a more-easily programmed instrument that could imitate other instruments, could create new sounds of its own, and was highly portable and affordable. Progressive rock was the genre in which the synthesizer first became established as a common part of popular music. Synthesizers could be used to play the rapid, virtuosic lines that changed the perception of keyboard instruments.
The reliance on the use of multiple keyboard sounds meant that keyboardists such as Rick Wakeman appeared onstage surrounded by ten or more keyboards at a time. Modern digital synthesizers and samplers have reduced the need for huge keyboard stacks, as they typically allow sounds to be layered or for one keyboard to trigger another's sounds through a MIDI connection. They also provide a reliable alternative to instruments such as Mellotrons, whose delicate mechanical apparatus is prone to breakdowns, and are much more portable than bulky instruments such as the Hammond organ. Digital synthesizers are also suitable chordal instruments, unlike early analog synthesizers such as the Minimoog, Moog Taurus and ARP Odyssey, which could play only one note at a time and so were mainly suitable for drones, basslines and lead playing.
Electronic effects.
The concept of the studio as an instrument led certain audio effects units to become identified with progressive rock. Pink Floyd, especially in their early days, were noted for their heavy use of vocal delay. Robert Fripp and Brian Eno employed a tape-delay system using two 1/4" tape-recorders, and later dubbed "Frippertronics," that allowed self-accompaniment and the creation of textural, evolving soundscapes. Frippertronics debuted on Fripp & Eno's 1973 "No Pussyfooting" album, and was later incorporated into Fripp solo albums and mainstream works such as "Peter Gabriel" and Daryl Hall's 1977 "Sacred Songs". Progressive rock guitarists showed a distinct preference for Hiwatt amplifiers, with the exception of Yes guitarist Steve Howe, who used Fender Dual Showmans. Rush's transition from their early metal albums into their progressive rock phase was accompanied by guitarist Alex Lifeson's switch of amplification from Marshall to Hiwatt.
Advancements in recording technology were key in enabling the production of progressive rock albums. The Moody Blues were given access to an orchestra for the recording of "Days of Future Passed" because Deram Records wanted to showcase their production technology. As multitrack recording with as many as 64 separate tracks became available, bands took advantage of the additional tracks and created increasingly dense arrangements. Some artists, such as Yes and Brian Eno, later saw this as having been taken to excess and either simplified their arrangements or distanced themselves from the genre altogether.
Traditional instruments.
Progressive rock bands often use instruments in ways different from their traditional roles. The role of the bass may be expanded from its traditional rhythm section function into that of a lead instrument. Bassists often play contrapuntal lines that are more independent and melodic than conventional bass lines, which emphasize the chord root. This is often accompanied by the use of an instrument such as a Rickenbacker bass, whose sound contains an unusually large amount of treble frequencies. Some bassists use the Chapman Stick, which is operated with both hands on the fretboard and allows polyrhythmic and chordal playing. Treble may be emphasized by the choice of strings, by playing with a pick, and by use of the instrument's higher registers. Drum kits are frequently expanded with orchestral percussion such as timpani and gongs. Acoustic guitar becomes more prominent and often appears as interludes played in the classical style of Andrés Segovia. Piano is played in a style derived from the classical piano repertoire rather than from the blues or boogie-woogie styles previously in use. Guitar may be dispensed with altogether, and traditional rhythm guitar is almost never used, as chordal backgrounds are typically played on a keyboard instrument such as the Hammond organ. Genesis built huge, orchestral textures by blurring the lines between the roles of the keyboard and the guitar.
Virtuosity.
Virtuoso instrumental skills are so closely associated with progressive rock that authors such as Bill Martin consider it as a defining element and exclude bands such as Pink Floyd from consideration. Keith Emerson was acclaimed as "the Hendrix of the keyboard." Yes bassist Chris Squire helped to redefine his instrument's role in rock music and influenced bassists across a range of genres.
It is not uncommon for musicians to have received a higher-than-average level of formal training. Rick Wakeman studied at the Royal College of Music for a time, but left due to increasing demand for his services as a session musician. The Dixie Dregs were music students at the University of Miami, where their guitarist Steve Morse studied under Pat Metheny, and Dream Theater was formed by a group of Berklee School of Music students. Carl Palmer, of ELP, studied at the Guildhall School of Music and Drama. Annie Haslam, of Renaissance, was a classically trained soprano with a vocal range of five octaves. Genesis drummer (and later singer) Phil Collins and Curved Air vocalist Sonja Kristina performed in the London stage productions of "Oliver!" and "Hair", respectively.
Players from the genre frequently appear in readers' polls of publications that cater to musicians. The US magazine "Guitar Player" lists Yes guitarist Steve Howe, Mahavishnu Orchestra guitarist John McLaughlin, Rush bassist Geddy Lee, Dixie Dregs guitarist Steve Morse, onetime Soft Machine guitarist Andy Summers, and Frank Zappa in its "Gallery of the Greats," awarded for repeated wins in a readers' poll category. "Modern Drummer" magazine lists drummers Phil Collins; Stewart Copeland, formerly of Curved Air; Terry Bozzio, of Frank Zappa and U.K.; Vinnie Colaiuta, of Frank Zappa; Bill Bruford, of Yes and King Crimson; Carl Palmer, and Neil Peart of Rush in its reader-selected Hall of Fame. Editors of the US "Keyboard" magazine chose Dream Theater keyboardist Jordan Rudess and Jon Lord, the Deep Purple keyboardist who composed their "Concerto for Group and Orchestra", as founding members of their "Keyboard" Hall of Fame. Chris Squire was a frequent "Melody Maker" poll winner.
Rhythm, melody and harmony.
There is a tendency towards greater freedom of rhythm than exists in other forms of rock music. Progressive rock artists are more likely to explore complex time signatures such as 5/8 and 7/8. Tempo, key and time signature changes are common within progressive rock compositions. John Wetton, a veteran of several prominent progressive rock groups, later described frequent meter changes as an immature behavior that one grows out of. Yes keyboardist Rick Wakeman explained their use as necessary for matching the music to Jon Anderson's lyrics.
Complex time signatures are sometimes used to create a polyrhythmic effect, as in "The Journey," from Rick Wakeman's "Journey to the Centre of the Earth". An ostinato, played on a Clavinet in a meter subdivided as an unusual 2+2+2+3 pattern, is overlaid by a choral pattern in a time signature with the standard 3+3+3 subdivision. Robert Fripp has spoken of meters based on 5, 7 and 11 as "vital and energetic."
Progressive rock often discards the blues inflections and pentatonic scale-based melodies of mainstream rock in favor of modal melodies. Compositions draw inspiration from a wide range of genres including classical, jazz, folk music and world music. Melodies are more likely to comprise longer, developing passages than short, catchy ones.
Chords are typically standard triads, although many keyboardists would alter these triads by playing a nonchord tone in the bass. Quartal harmony, which uses chords built on intervals of fourths rather than thirds and was used heavily in the 1960s by John Coltrane pianist McCoy Tyner, is a key feature of Keith Emerson's style. ELP also use bitonality, or the use of two keys simultaneously, in "Infinite Space" and "The Endless Enigma." Some bands, such as King Crimson, incorporated atonality and free improvisation into their works. "Red" and "Fracture," both King Crimson pieces built on the whole tone scale, are two examples.
Chord changes are typically based on modes, as is typical of rock music, and deviate significantly from the tonality of music from the classical era. Unexpected chord changes in the style of impressionist composers like Claude Debussy are common. Jazz harmonies appear in the music of Canterbury groups such as Soft Machine.
Lyrical themes.
Progressive rock lyrics tend to avoid common rock and pop subjects such as love and dancing. Bands also avoid such youth-oriented themes as violence, nihilism, rebellion, and the macabre. Sex is not a common subject, although the occasionally leering lyrics of Jethro Tull and Frank Zappa are an exception. Themes found in classical literature, fantasy and folklore occur frequently, and intellectual topics such as psychological theories may be addressed. Romantic poetry and J. R. R. Tolkien are frequent sources of inspiration.
Medievalism and science fiction themes are common and often appear as metaphors for spiritual transformation and the quest for an ideal society. Magma's 1970s output is a single science fiction narrative spread out over several albums and written in the Kobaian language, which was invented for the purpose. Dystopian and apocalyptic themes drawn from science fiction criticize totalitarianism and the dehumanizing effects of society. These occur in Van der Graaf Generator's "Lemmings," Roger Waters' Pink Floyd lyrics in the mid-to-late 1970s, Rush's "2112", and, later, Radiohead's "OK Computer". Bill Martin, author of several books on progressive rock, has noted that King Crimson's "21st Century Schizoid Man" anticipates cyberpunk by several years and carries a theme of technology run amok that is also found in ELP's "Tarkus" and "Brain Salad Surgery" albums.
Many early lyrics express utopian themes that reflect the genre's origins in psychedelic rock and address the subject of spiritual transformation. Spiritual and religious themes are common, as in Yes' "Close to the Edge", which is based on Hermann Hesse's "Siddhartha", and Aphrodite's Child's "666", an apocalyptic album with imagery drawn from the Biblical Book of Revelation.
Monty Python and Bonzo Dog Doo-Dah Band-influenced humour appears in some progressive rock lyrics. This is especially pronounced in the more eccentric, Dadaistic approach adopted by some of the Canterbury bands. Song titles such as Hatfield and the North's "Big Jobs (Poo Poo Extract)" reflect this. Puns are common, as in the Caravan album title "Cunning Stunts". The more serious symphonic prog bands occasionally recorded such comical tracks as "Jeremy Bender" by ELP, "Harold the Barrel" by Genesis, and "The Story of the Hare Who Lost His Spectacles", an interlude from Jethro Tull's album-length "A Passion Play".
Several groups valued lyrics so strongly as to employ a lyricist as a full-time band member. These include Peter Sinfield with King Crimson and Keith Reid with Procol Harum. Renaissance maintained a longtime relationship with lyricist Betty Thatcher. Hawkwind for a time featured lyrics by science fiction author Michael Moorcock.
Social commentary.
Social commentary is frequently present. The British class system is criticized in Genesis' "Selling England by the Pound", Gentle Giant's "Three Friends" and Jethro Tull's "Thick as a Brick", which also functions as a satire of the concept album. "Breakfast in America", by British expatriates Supertramp, questioned the American Dream. The Nice's instrumental "America" is considered to have made a similar point musically through a series of dissonant variations on the song's melody. Organized religion is criticized in Jethro Tull's "Aqualung", ELP's "The Only Way (Hymn)" and King Crimson's "The Great Deceiver."
Rush lyricist Neil Peart describes himself as a libertarian, and his political viewpoints are reflected in songs such as "The Trees." Frank Zappa, a self-described conservative, used his concept album "Joe's Garage" to address themes such as individualism, sexuality, the danger of large government, and "the foolishness of white males".:149
Italian progressive rock bands, such as Premiata Forneria Marconi (PFM), had a greater tendency toward politicized lyrics. Bands and festivals in Italy were sometimes sponsored by the Italian Communist Party, and it was not uncommon for bands to hint, through either their lyrics or their actions, at support for armed revolutionary groups such as the Red Army Faction and the Palestine Liberation Organization. The very act of forming a band could be seen as politically subversive in Communist Eastern Europe, and acts such as Omega, in Hungary, and Aquarium, in the Soviet Union, initially existed as underground groups. Various members of the Czech band The Plastic People of the Universe endured prison sentences.
Henry Cow, an especially avant-garde British band with Marxist leanings, took the viewpoint that the major record labels were using their economic power to dictate which styles of music ever got heard by the public. The band organized a "Rock in Opposition" (RIO) festival to unite bands who similarly opposed music business practices. Italy's Stormy Six and Belgium's Univers Zero aligned themselves with the RIO movement, as did later bands such as the 5uu's and Thinking Plague.
Pastoralism and ecology.
Many progressive rock bands were strongly rooted in British folk music, and this resulted in a tendency toward pastoralism in the lyrics. Genesis, especially when Anthony Phillips was a member of the band, used mythological figures and fairytale worlds to create this effect in songs. After his departure the band did continue to explore these fantasy elements, yet often in a more diverse approach as songs began to combine fantasy with more dark and bizarrely surreal themes such as "The Musical Box" and "The Return of the Giant Hogweed." As social and economic problems increased in Britain within the 1970s, many artists gravitated away from pastoralism and ecology at varying degrees, with temporary to near-permanent shifts towards modernism, contemporary political satire, and realism. Jethro Tull, however, increasingly retreated into albums such as "Songs From the Wood", "Heavy Horses" and Stormwatch, whose lyrics emphasized nature.
Awareness of nature sometimes combined with social criticism to produce lyrics that expressed concern over the ecology. This appears on the major Yes albums of the early 1970s and their later "Don't Kill the Whale." Ecology also figures heavily in Magma's lyrical concept. Manfred Mann's Earth Band's 1974 album "The Good Earth" carried an ecological theme and included a coupon that entitled its purchasers to a square foot of mountain property in Wales. Ecological themes were sometimes carried out to an extent that even genre fans found embarrassing, and they were frequently satirized by Frank Zappa as naive.
Concept albums.
The late 1960s and early 1970s saw a general trend among rock and pop artists toward albums in which many or all of the songs shared a common theme. This tendency was especially pronounced in progressive rock. Experimentation with expanded musical forms contributed to this, as songs that were more or less thematically related were often combined into suites made up of several movements. This occurred as early as the 1966 album "Freak Out!", by The Mothers of Invention, in which the multi-part "The Return of the Son of Monster Magnet" occupied the entire fourth side of the album. Two influential examples followed in 1968: the title track of "Ars Longa Vita Brevis", by The Nice, and "In Held 'Twas in I," from Procol Harum's "Shine On Brightly", both of which used sonata-type forms.
These extended pieces carry on in the Romantic-era tradition of program music, which is intended to tell a story, and they often are inspired by works of literature. Pink Floyd's "Animals" is a concept album based on George Orwell's "Animal Farm". Genesis' "Selling England by the Pound" was influenced by T. S. Eliot's poem "The Waste Land". Rush's "2112" was inspired by Ayn Rand's "Anthem". Arthur C. Clarke's novel "Childhood's End" inspired both Pink Floyd's "Obscured by Clouds" and Genesis' "Watcher of the Skies."
"Darwin!", by Banco del Mutuo Soccorso, is a concept album based on Charles Darwin's theory of evolution. Gentle Giant's "The Power and the Glory" addressed current events, primarily the Watergate scandal. Story arcs are sometimes spread out over several albums, as was done with the "Chapters" on the first four Saga albums, Rush's Cygnus X-1 and Fear series, Magma's mythology and, more recently, the ongoing science fiction narrative of the Coheed and Cambria albums.
The advent of multi-part suites that occupy an entire LP side roughly coincided with the rise of FM radio and its practice of playing albums, or album sides, in their entirety. These extended works are at best, as with "Close to the Edge" and "2112," considered to be among the bands' greatest works. Some bands stretched the format beyond their audiences' capacity to tolerate. This was the case with Yes' "Tales from Topographic Oceans", a two-LP set that contained a single 20-minute song on each side. The album caused disagreements that led to keyboardist Rick Wakeman's departure from the band, as he compared the new material to a "padded bra" and protested the new songs by eating onstage instead of playing. In the punk era, "Tales" became a symbol of progressive rock self-indulgence.
Visual aspects.
Stage presentation.
Pink Floyd pioneered the concept of concerts as multimedia events, and they used sophisticated light shows meant to suggest or enhance the use of LSD. Their laser show was later replaced by even more sophisticated props such as aeroplane crashes, flying animals, and a giant wall that was constructed behind them and then torn down. Genesis took an operatic approach, as frontman Peter Gabriel used multiple costume changes to accent the theatrical nature of his lyrics. Their "The Lamb Lies Down on Broadway" tour reinforced this with a slideshow of as many as 1500 images.
Pink Floyd's interest in multimedia performances later led to soundtrack work on several films and ultimately expressed itself in the film "Pink Floyd – The Wall". Other progressive rock bands dabbled in film. Peter Gabriel collaborated with surrealist filmmaker Alejandro Jodorowsky in an attempt to write a "Lamb Lies Down on Broadway" screenplay, and the Italian band Goblin was noted for their soundtrack work on "Dawn of the Dead", "Profondo rosso" and "Suspiria".
Some acts indulged in pure showmanship. Jethro Tull frontman Ian Anderson was noted for his Pan-like persona and energetic performances in which he played the flute while standing on one leg. Grobschnitt displayed a cabaret-style show with pyrotechnics and slapstick acts. Rick Wakeman concerts in support of his "The Myths and Legends of King Arthur and the Knights of the Round Table" album featured ice skaters in Arthurian costumes. Keith Emerson, while with The Nice, was noted for holding organ notes by stabbing his keyboard with a pair of Hitler youth daggers provided by road crew member Lemmy. With ELP, he is known to have played his Moog modular synthesizer using his buttocks. ELP frequently used dangerous props and gimmicks such as flying pianos and exploding synthesizers in their stage act, and drummer Carl Palmer once cracked several ribs when he jumped over his drum set and landed on a trap door.
Progressive rock visual styles sometimes extended to the stage sets. Roger Dean designed stage sets for Yes that continued the visual themes used his album cover designs. Props included giant mushrooms and a drum set encased in a seashell, which nearly suffocated drummer Alan White when it failed to open during one performance. Tangerine Dream had a preference for performing in Gothic cathedrals and used light shows ranging from the minimal to full laser shows. Jean-Michel Jarre integrated projections and fireworks into his performances.
This enthusiasm for showmanship was not shared by all progressive rock bands. King Crimson initially employed a dramatic light show, but guitarist Robert Fripp became concerned that it distracted from the music. Fripp and Genesis guitarist Steve Hackett notably engaged in no stage movement at all and, instead, stayed seated throughout performances.
Album art.
Album covers prior to The Beatles' "Sgt. Pepper's Lonely Hearts Club Band" usually consisted of a photograph of the group, but the trend toward concept albums was accompanied by a move toward artwork that depicted the album's concept. This artwork often contains science fiction and fantasy motifs executed in a surrealist style. "Fragile", by Yes, has cover art that depicts the Earth splitting into pieces, which reflects the ecological focus of their lyrics. "Tarkus", by ELP, has a William Neal-designed LP gatefold that symbolically illustrates the titular suite's concept through a series of drawings of fantastic, cybernetic creatures who battle one another.
A number of artists became closely associated with the genre. Roger Dean, who designed album jackets for numerous bands and worked extensively with Yes, created imaginary worlds with a sense of imagination and grandeur that matched the music. Paul Whitehead illustrated early Genesis and Van der Graaf Generator albums with nightmarish art based on the songs' lyrics, and he encouraged the bands to develop a visual identity. Hipgnosis, a London design firm with close personal ties to members of Pink Floyd, used the music as inspiration for surrealistic designs that incorporated photographs and visual puns. Dean and Hipgnosis have influenced later visual artists and advertising designers.
Artwork was sometimes commissioned from artists who were famous in their own right, such as the H. R. Giger design for ELP's "Brain Salad Surgery" and caricaturist Gerald Scarfe's illustrations for Pink Floyd's "The Wall". This combination of music and artwork is intended to function as a total work of art, which is a further use of concepts borrowed from high culture. The practice of connecting an album's artwork to its concept still exists, but its effectiveness is limited by the smaller display area used by compact discs and mobile devices.
History.
Precursors.
Bob Dylan's poetry, the 1966 album "Freak Out!", by The Mothers of Invention, and the 1967 album "Sgt. Pepper's Lonely Hearts Club Band", by The Beatles, have all been mentioned as important in the genre's development. The productions of Phil Spector were key influences, as they introduced the possibility of using the recording studio to create music that otherwise could never be achieved. In the same respect, The Beach Boys' concept album "Pet Sounds" (1966), which itself influenced "Sgt. Pepper's", and Jefferson Airplane's second album, "Surrealistic Pillow" (1967), was also influential.
Psychedelic pop and folk rock.
Many groups and musicians played important roles in this development process, but none more than the Beach Boys and the Beatles...[They] brought expansions in harmony, instrumentation (and therefore timbre), duration, rhythm, and the use of recording technology. Of these elements, the first and last were the most important in clearing a pathway toward the development of progressive rock.
“
—Bill Martin
"Pet Sounds" and "Sgt. Pepper's," with their lyrical unity, extended structure, complexity, eclecticism, experimentalism, and influences derived from classical music forms, is largely viewed as beginnings in the progressive rock genre and as turning points wherein rock, which previously had been considered dance music, became music that was made for listening to. Bill Bruford, a veteran of several progressive rock bands, said that "Sgt. Pepper" transformed both musicians' ideas of what was possible and audiences' ideas of what was acceptable in music. He believed that: "Without the Beatles, or someone else who had done what the Beatles did, it is fair to assume that there would have been no progressive rock." It also marked the point at which the LP record emerged as a creative format whose importance was equal to or greater than that of the single, an opinion which Brian Wilson began to share after hearing the US version of the Beatles' "Rubber Soul" (1965) with its deliberately reconfigured track listing intended to angle the album as a work of the emergent folk rock genre. LP sales first overtook those of singles in 1969.
Bob Dylan introduced a literary element to rock through his fascination with the Surrealists and the French Symbolists and his immersion in the New York City art scene of the early 1960s. The trend of bands with names drawn from literature, such as The Doors, Steppenwolf and The Ides of March, were a further sign of rock music aligning itself with high culture. Literary concepts such as Nietzsche and the Apollonian and Dionysian dichotomy were referenced by Doors singer Jim Morrison. Dylan also led the way in blending rock with folk music styles. This was followed by folk rock groups such as The Byrds, who based their initial sound on the work of Brian Wilson. In turn, the Byrds' vocal harmonies inspired those of Yes, and British electric folk bands such Fairport Convention, who emphasized instrumental virtuosity. Some of these artists, such as The Incredible String Band and Shirley and Dolly Collins, would prove influential through their use of instruments borrowed from world music and early music.
Early experimental rock.
"Freak Out!", a Dadaist mixture of progressive rock, garage rock and avant-garde layered sounds is often considered to be the first concept album. The band 1-2-3, later renamed Clouds, began to experiment with song structure, improvisation, and multi-layered arrangements that same year. In March 1966, The Byrds released "Eight Miles High", a pioneering psychedelic rock single with a guitar lead inspired by the "sheets of sound" soloing style of jazz saxophonist John Coltrane. The Who later that year recorded "A Quick One While He's Away", a miniature rock opera considered to be the first example of the form. The rock opera was more fully realized in "S.F. Sorrow", an influential 1968 album by The Pretty Things. The Doors' The End and When the Music's Over, Crosby, Stills & Nash's and The Beatles' Abbey Road Medley are other notable early experimentations with rock and roll suites.
Bob Dylan's six-minute "Like a Rolling Stone" and it's parenting album "Highway 61 Revisited" with its numerous lengthy songs (especially 11-min epic "Desolation Row") makes him the first rocker to experiment with song structures. He also made the first rock song to occupy an entire LP side, "Sad Eyed Lady of the Lowlands". "Going Home" by The Rolling Stones was the first long "jam" recorded expressly for an album with its over 10 minutes length. The Mothers' "The Return of the Son of Monster Magnet" and Love's "Revelation" from album "Da Capo" are other notable early side long rock epics.
Jimi Hendrix, who rose to prominence in the London scene and recorded with a band of English musicians, initiated the trend toward virtuosity in rock music. Later heavier and more experimental bands like Cream and Led Zeppelin would move this trend more forward
The availability of newly affordable recording equipment coincided with the rise of a London underground scene at which LSD was commonly used. Pink Floyd and Soft Machine functioned as house bands at all-night events at locations such as Middle Earth and the UFO Club, where they experimented with sound textures and long-form songs. Beatles member John Lennon is known to have attended at least one such event, a happening called the 14 Hour Technicolor Dream. Paul McCartney was deeply connected to the underground through his involvement with the Indica Gallery. Many psychedelic, electric folk and early progressive bands were aided by exposure from BBC Radio 1 DJ John Peel.
Classical and jazz influences.
Harpsichords, orchestral wind instruments and string sections were used in mid-1960s recordings by groups such as the Beach Boys within their album "Today!" (1965). This created the form of Baroque rock heard in the Bach-inspired "A Whiter Shade of Pale" (1967), by Procol Harum. The use of instruments traditionally associated with classical music in rock music is difficult to trace in its beginnings, although it is evident in the early 1960s work of Burt Bacharach and Phil Spector. The Beach Boys October 1966 single "Good Vibrations" was dubbed a "pocket symphony" by publicist Derek Taylor, containing an eclectic array of classical, rock, and exotic instruments structured around a cut-up mosaic of musical sections represented by several discordant key and modal shifts. The Moody Blues established the popularity of symphonic rock when they recorded "Days of Future Passed" together with the London Festival Orchestra, and Procol Harum began to use a greater variety of acoustic instruments, particularly on their 1969 "A Salty Dog" album. Classical influences sometimes took the form of pieces adapted from or inspired by classical works, such as Jeff Beck's "Beck's Bolero" and parts of The Nice's "Ars Longa Vita Brevis". The latter, along with such Nice tracks as "Rondo" and "America", reflect a greater interest in music that is entirely instrumental. "Sgt. Pepper's" and "Days" both represent a growing tendency toward song cycles and suites made up of multiple movements.
Several bands that included jazz-style horn sections appeared, including Blood, Sweat & Tears and Chicago. Of these, Chicago in particular experimented with suites and extended compositions, such as the "Ballet for a Girl in Buchannon" on "Chicago II". Jazz influences appeared in the music of British bands such as Traffic, Colosseum and Canterbury scene bands such as Soft Machine. Canterbury scene bands emphasized the use of wind instruments, complex chord changes and long improvisations. Jethro Tull began as a heavy blues band fronted by Ian Anderson, a flautist deeply influenced by jazz musician Rahsaan Roland Kirk.
Early 1970s classic era.
The Nice, The Moody Blues, Procol Harum and Pink Floyd all contained elements of what is now called progressive rock, but none represented as complete an example of the genre as several bands that formed soon after. Almost all of the genre's major bands, including Jethro Tull, King Crimson, Yes, Genesis, Van der Graaf Generator, ELP, Gentle Giant and Curved Air, released their debut albums during the years 1968–1970. Most of these were folk-rock albums that gave little indication of what the band's mature sound would become, but King Crimson's "In the Court of the Crimson King" (1969) was a fully formed example of the genre. The term "progressive rock," which appeared in the liner notes of Caravan's 1968 self-titled debut LP, came to be applied to these bands that used classical music techniques to expand the styles and concepts available to rock music.
Most of the genre's major bands released their most critically acclaimed albums during the years 1971–1976. These include "Pawn Hearts", by Van der Graaf Generator; "Selling England by the Pound" and "The Lamb Lies Down on Broadway", by Genesis; Yes' "The Yes Album", "Fragile" and "Close to the Edge"; "Aqualung" and "Thick as a Brick" by Jethro Tull, Gentle Giant's "Free Hand", ELP's "Brain Salad Surgery", Rush's "2112", and Pink Floyd's "The Dark Side of the Moon" and "Wish You Were Here".
Progressive rock experienced a high degree of commercial success during the early 1970s. Jethro Tull, ELP, Yes and Pink Floyd combined for four albums that reached number one in the US charts, and sixteen of their albums reached the top ten. Tull alone scored 11 gold albums and 5 platinum albums. Pink Floyd's 1970 album "Atom Heart Mother" reached the top spot on the UK charts. Their 1973 album "The Dark Side of the Moon", which united their extended compositions with the more structured kind of composing employed when Syd Barrett was their songwriter,:34–35 spent more than two years at the top of the charts:4, 38 and remained on the "Billboard" 200 album chart for fifteen years. Mike Oldfield's "Tubular Bells", an excerpt of which was used as the theme for the film "The Exorcist", sold 16 million copies. A number of progressive bands released singles that became pop hits, including Kraftwerk ("Autobahn"), Yes ("Roundabout"), Jethro Tull ("Living in the Past"), Focus ("Hocus Pocus"), Curved Air ("Back Street Luv"), Strawbs ("Part of the Union"), and Genesis ("I Know What I Like").
The genre has always had its greatest appeal for white males. Most of the musicians involved were male, as was the case for most rock music of the time, although Curved Air vocalist Sonja Kristina and Renaissance singer Annie Haslam were prominent exceptions. Renaissance's lyricist also was female, and their feminine storytelling perspective is particularly prominent in their album art and in the songs "Ocean Gypsy" and "The Song of Scheherazade," both from "Scheherazade and Other Stories". Female singers were better represented in the progressive folk bands, who displayed a broader range of vocal styles than the progressive rock bands with whom they frequently toured and shared band members.
British and European audiences typically followed concert hall behavior protocols associated with classical music performances, and they were more reserved in their behavior than were audiences of other forms of rock. This confused musicians during US tours, as they found that American audiences were less attentive and more prone to outbursts during quiet passages.
North America.
Progressive rock came to be appreciated overseas, but it mostly remained a European, and especially British, phenomenon. Few American bands engaged in it, and the purest representatives of the genre, such as Starcastle and Happy the Man, remained limited to their own geographic regions. This is at least in part due to music industry differences between the US and Great Britain. Radio airplay was less important in the UK, where popular music recordings had never been played on official radio (as opposed to on pirate radio) until the 1967 launch of BBC Radio 1. UK audiences were accustomed to hearing bands in clubs, and British bands could support themselves through touring. US audiences were first exposed to new music on the radio, and bands in the US required radio airplay for success. Radio stations were averse to progressive rock's longer-form compositions, which hampered advertising sales. Cultural factors were also involved, as US musicians tended to come from a blues background, while Europeans tended to have a foundation in classical music.
North American progressive rock bands often represented hybrid styles such as the complex metal of Rush, the Southern rock-tinged prog of Kansas, and the eclectic fusion of the all-instrumental Dixie Dregs. British progressive rock acts had their greatest US success in the same geographic areas in which British heavy metal bands experienced their greatest popularity. The overlap in audiences led to the success of arena rock bands, such as Boston, Kansas and Styx, who combined elements of the two styles.
Europe.
Progressive rock achieved popularity in Continental Europe more quickly than it did in the US. Italy remained generally uninterested in rock music until the strong Italian progressive rock scene developed in the early 1970s, and Van der Graaf Generator were much more popular there than in their own country. Genesis were hugely successful in Continental Europe at a time when they were still limited to a cult following in Britain and the US. Few of the European groups were successful outside of their own countries, with the exceptions of bands like Focus, who wrote English-language lyrics, and Le Orme and PFM, whose English lyrics were written by Peter Hammill and Peter Sinfield, respectively.
Some European bands played in a style derivative of English bands. This can be heard in Triumvirat, an organ trio in the style of ELP; Ange and who have had a strong King Crimson influence. Others brought national elements to their style: Spain's Triana introduced flamenco elements, groups such as the Swedish Samla Mammas Manna drew from the folk music styles of their respective nations, and Italian bands such as Il Balletto di Bronzo, Rustichelli & Bordini, leaned toward an approach that was more overtly emotional than that of their British counterparts.
Some progressive rock subgenres are tied to national scenes. Zeuhl was a name given to the style of the French band Magma. A number of bands were strongly influenced by Magma and are considered to be part of that subgenre. The "Kosmische music" scene in Germany came to be labeled as "krautrock" internationally. Bands such as Can, which included two members who had studied under Karlheinz Stockhausen, tended to be more strongly influenced by 20th century classical music than the British bands, whose musical vocabulary leaned more toward the Romantic era. Many of these groups were very influential even among bands that had little enthusiasm for the symphonic variety of progressive rock.
Late 1970s decline.
Political and social trends of the late 1970s shifted away from the early 1970s hippie attitudes that had led to the genre's development and popularity. The rise in punk cynicism made the utopian ideals expressed in progressive rock lyrics unfashionable. Virtuosity was rejected, as the expense of purchasing quality instruments and the time investment of learning to play them were seen as barriers to rock's energy and immediacy. There were also changes in the music industry, as record companies disappeared and merged into large media conglomerates. Promoting and developing experimental music was not part of the marketing strategy for these large corporations, who focused their attention on identifying and targeting profitable market niches.
Four of the biggest bands in progressive rock ceased performing or experienced major personnel changes during the mid-1970s. Robert Fripp disbanded King Crimson in 1974 and said later that the genre had gone "tragically off course." ELP went on hiatus the following year. Genesis moved in a more mainstream direction after the 1975 departure of Peter Gabriel and especially after the 1977 departure of Steve Hackett. Yes experienced lineup changes throughout the 1970s before fragmenting in 1980. A number of the major bands, including Van der Graaf Generator, Gentle Giant and U.K., dissolved between 1978 and 1980. Some decided that it was time to move on because they, as Caravan leader Pye Hastings admitted, had "got quite stale."
Many bands had by the mid-1970s reached the limit of how far they could experiment in a rock context, and fans had wearied of the extended, epic compositions. The sounds of the Hammond, Minimoog and Mellotron had been thoroughly explored, and their use became clichéd. Those bands who continued to record often simplified their sound, and the genre fragmented from the late 1970s onward. Corporate artists and repertoire staff exerted an increasing amount of control over the creative process that had previously belonged to the artists, and established acts were pressured to create music with simpler harmony and song structures and fewer changes in meter. This simplification can be heard as a softer, pop orientation in such albums as Genesis' "...And Then There Were Three...", Renaissance's "A Song for All Seasons", and The Moody Blues' "Octave". A number of symphonic pop bands, such as Supertramp, 10cc, the Alan Parsons Project and the Electric Light Orchestra, brought the orchestral-style arrangements into a context that emphasized pop singles while allowing for occasional instances of exploration. Jethro Tull, Gentle Giant and Pink Floyd opted for a harder sound in the style of arena rock.
Few new progressive rock bands formed during this era, and those who did found that record labels were not interested in signing them. The short-lived supergroup U.K. was a notable exception, although they tended to carry on in the style of previous bands and did little to advance the genre. Some of the genre's more important development at this time occurred in its influence on other styles, as several guitarists with European ties brought a progressive rock approach to heavy metal and laid the groundwork for the future progressive metal style. Michael Schenker, of UFO, and Uli Jon Roth, who replaced Schenker in Scorpions, expanded the modal vocabulary available to guitarists. Roth studied classical music with the intent of using the guitar in the way that classical composers used the violin. Finally, the Dutch-born and classically trained Alex and Eddie Van Halen formed Van Halen, who redefined the standard for rock virtuosity and paved the way for the "shred" music of the 1980s.
1980s.
Neo-progressive rock.
A second wave of progressive rock bands appeared in the early 1980s and have since been categorized as a separate "neo-progressive rock" subgenre. These largely keyboard-based bands played extended compositions with complex musical and lyrical structures. Most of the genre's major acts released debut albums between 1983 and 1985 and shared the same manager, Keith Goodwin, a publicist who had been instrumental in promoting progressive rock during the 1970s. The previous decade's bands had the advantage of appearing during a large countercultural movement that provided them with a large potential audience, but the neo-progressive bands were limited to a niche audience and found it difficult to attract a following. Only Marillion and Saga experienced international success.
Neo-prog bands tended to derive their sound and visual style from the symphonic prog bands of a decade earlier. The genre's most successful band, Marillion, suffered particularly from accusations of similarity to Genesis, although they used a different vocal style and a sound with more of a hard rock element. Authors Paul Hegarty and Martin Halliwell have pointed out that the neo-progressive bands were not so much plagiarizing progressive rock as they were creating a new style from progressive rock elements, just as the bands of a decade before had created a new style from jazz and classical elements. Author Edward Macan counters by pointing out that these bands were at least partially motivated by a nostalgic desire to preserve a past style rather than a drive to innovate.
A predecessor to this genre was The Enid, who fused rock with classical but were more heavily influenced by Ralph Vaughan Williams than by more modern composers.The change of approach can be heard in the shift toward shorter compositions and a keyboard-based sound in Rush albums such as "Grace Under Pressure". Neo-progressive bands emphasized individual solos instead of group improvisation, and they included more world music elements. Lyrics became more personal and less esoteric. Concept albums were still created, but not as frequently and on a smaller scale. Digital synthesizers took over many of the roles formerly filled by bulkier keyboards such as Mellotrons and organs, and their modern sound tended to minimize the folk influences that had been typical of 1970s progressive rock. Heavy metal bands such as Iron Maiden and Queensrÿche began to explore the mythological themes and extended concepts that had previously been the territory of progressive rock.
Commercialisation.
Some established bands moved toward music that was simpler and more commercially viable. Asia, a supergroup composed of veterans of several of the 1970s' major progressive rock acts, debuted in 1982 with an album that featured progressive rock-style Roger Dean artwork, some jazz influence, and advanced vocal arrangements. It however abandoned the complex song structures and interplay between music and vocals that had characterized progressive rock. The songs were based on pop hooks and repetitive choruses, were of a length appropriate for radio airplay, and featured slick production that pushed the vocals and snare drum to the front of the mix.
Echoes of progressive rock complexity could be heard in arena rock bands like Journey, Kansas, Styx, GTR, ELO and Foreigner, all of which either had begun as progressive rock bands or included members with strong ties to the genre. These bands retained some elements of the orchestral-style arrangements, but they moved away from lyrical mysticism in favor of teen-oriented songs about relationships.Genesis transformed into a successful pop act, and a reformed Yes released the relatively mainstream "90125", which yielded their only US number-one single, "Owner of a Lonely Heart". These radio-friendly groups have been called "Prog Lite."
One band who did experience great 1980s success while maintaining a progressive approach was Pink Floyd, who released "The Wall" late in 1979. The album, which brought punk anger into progressive rock, was a huge success and was later filmed as "Pink Floyd – The Wall". Pink Floyd were unable to repeat that combination of commercial and critical success, as their sole follow-up, "The Final Cut", was several years in coming and was essentially a Roger Waters solo project that consisted largely of material that had been rejected for "The Wall". The band later reunited without Waters and restored many of the progressive elements that had been downplayed in the band's late-1970s work. This version of the band was very popular, but critical opinion of their later albums is less favorable. Jethro Tull were able to capitalize on a 1980s interest in sword and sorcery with their 1982 "The Broadsword and the Beast", but they drifted toward a more mainstream style later in the decade, as did Rush.
Crossover with post-punk styles.
Progressive rock's influence was felt in the form of the post-punk bands, although these bands tended not to draw on classical rock or Canterbury bands as influences but rather Roxy Music and krautrock bands, particularly Can. Groups such as Public Image Ltd, Magazine, Wire, Cardiacs and Simple Minds showed some influence of prog along with their more usually recognized punk influences. Julian Cope of The Teardrop Explodes wrote a history of the krautrock genre, "Krautrocksampler". New wave bands tended to be less hostile toward progressive rock than were the punks, and there were crossovers, such as Robert Fripp's and Brian Eno's involvement with Talking Heads, and Yes' replacement of Rick Wakeman and Jon Anderson with the pop duo The Buggles. A number of bands in New York's no wave scene were impressed with punk's energy but not with its primitivism. This led to experiments that combined that energy with greater musical sophistication, such as the guitar orchestras of Glenn Branca and the noise experiments of Sonic Youth.
Punk and prog were not necessarily as opposed as is commonly believed. Both genres reject commercialism, and punk bands did see a need for musical advancement, as evidenced by the albums "London Calling", by The Clash, and "My War", by Black Flag. Sex Pistols frontman Johnny Rotten famously wore a T-shirt that read "I hate Pink Floyd," but he expressed admiration for Van der Graaf Generator, Can, and Pink Floyd themselves. Brian Eno expressed a preference for the approach of the punk and new wave bands in New York, as he found them to be more experimental and less personality-based than the English bands.
One progressive rock artist who was very supportive of the punk and new wave movements was former King Crimson leader Robert Fripp, who relocated to New York after a three-year retirement and collaborated with the new wave groups Blondie and Talking Heads. He formed a new band that experimented with gamelan music in a similar way to Talking Heads' approach on their "Remain in Light" album. The band was to be called "Discipline" but instead became a revived King Crimson. This edition featured new instrumentation that included Bill Bruford's electronic drums, Tony Levin's Chapman Stick, and guitar synthesizers played by Fripp and Adrian Belew, who was familiar to Fripp from the "Remain in Light" sessions. Their sound was highly percussive, featured tightly interconnected minimalist instrumentals with industrial noise influences, and often had a metallic edge. It was a new form of progressive rock that de-emphasized solos and overt virtuosity, but the music was nevertheless very complex and difficult.
Gamelan and minimalism also influenced Brian Eno, who after departing Roxy Music had collaborated with Fripp. Rush borrowed elements from world music and new wave, as on the reggae-tinged "The Spirit of Radio" and "Vital Signs."
1990s and 2000s.
Third wave.
A third wave of progressive rock bands, who might more properly be described as a second generation of neo-progressive bands, emerged in the 1990s. The use of the term "progressive" to describe groups that follow in the style of bands from ten to twenty years earlier is somewhat controversial, as it has been seen as a contradiction of the spirit of experimentation and progress. These new bands were aided in part by the availability of personal computer-based recording studios, which reduced album production expenses, and the Internet, which made it easier for bands outside of the mainstream to reach widely spread audiences. Record stores specializing in progressive rock appeared in large cities.
The shred music of the 1980s was a major influence on the progressive rock groups of the 1990s. Some of the newer bands, such as The Flower Kings and Spock's Beard, played a 1970s-style symphonic prog but with an updated sound. A number of them began to explore the limits of the CD in the way that earlier groups had stretched the limits of the vinyl LP. "The Garden of Dreams," from The Flower Kings' "Flower Power" album, is nearly 60 minutes in length and is composed of 18 separate sections, and Transatlantic's "The Whirlwind" consists of a single track of 77 minutes in length.
Folk influences resurface on "The Garden of Dreams," a trend that also appears in Mostly Autumn's 2008 album "Glass Shadows". The Decemberists use folk themes and influences as a means of connecting with the past, while Midlake use them to express pastoralism and Shanghai's Cold Fairyland use them for nationalist purposes.
The Radiohead album "OK Computer" is credited as having reintroduced unconventional songwriting and structures and as having inspired newer bands to adopt a more experimental approach. It also brought the idea of the album as a complete, unified statement back into favour, as the inclusion of "bonus tracks" on CD re-releases and, later, the practice of Internet downloads destroyed the perception of the album as a cohesive unit. Rock critics' disdain for the genre diminished after "OK Computer"'s release, and bands that followed were given freedom to reference earlier prog styles without the risk of being labeled as hopelessly unfashionable.
On their 2001 album "Origin of Symmetry", Muse included the new prog song "Citizen Erased," a 7-minute track with unusual structure, hard rock, classical, and electronic elements. With their return to space rock in 2006's Black Holes and Revelations, featuring the epic, satirical "Knights of Cydonia," they began to experiment more in the subsequent album; 2009's "The Resistance" was their most progressive to date, featuring the space rock opera "" and progressive arena rock anthem "United States of Eurasia." Frontman Matthew Bellamy confirmed that their 2015 album, "Drones", "does indeed include the sequel to fan favourite 'Citizen Erased' - and that the track in question is a crazy, ten minute prog nightmare."
Progressive metal.
Progressive rock and heavy metal have similar timelines. Both emerged from late-1960s psychedelia to achieve great early-1970s success despite a lack of radio airplay and support from critics, then faded in the late 1970s and experienced revivals in the early 1980s. Each genre experienced a fragmentation of styles at this time, and many metal bands from the New Wave of British Heavy Metal onward displayed progressive rock influences. Progressive metal reached a point of maturity with Queensrÿche's 1988 concept album "" and Voivod's 1989 "Nothingface", which featured abstract lyrics and a King Crimson-like texture.
Progressive metal drew attention when the US band Dream Theater's 1994 album "Awake" debuted at #32 on the album charts. King Crimson themselves returned in 1994 with a more metallic sound, as did Van der Graaf Generator in the following decade. Arjen Anthony Lucassen's Ayreon project, backed by an array of talent from the progressive rock genre, produced a series of innovative prog-metal concept albums from 1995 onward.
Several bands in the prog-metal genre, including the US bands Queensrÿche, Fates Warning and Dream Theater as well as Sweden's Opeth, name Rush as a primary influence. These bands also exhibit influences from more traditional metal and rock bands, such as Black Sabbath and Deep Purple. Tool have toured together with King Crimson and named them as an influence on their work, although Robert Fripp feels that the reverse is true and that there is a strong Tool influence on latter-day King Crimson.
Progressive rock elements appear in other metal subgenres. Black metal is conceptual by definition, due to its prominent theme of questioning the values of Christianity. Its guttural vocals are sometimes used by bands who can be classified as progressive, such as Mastodon and Opeth, whose "In Live Concert at the Royal Albert Hall" DVD featured packaging that referenced vintage progressive rock albums such as Deep Purple's "Concerto for Group and Orchestra". Symphonic metal is an extension of the tendency toward orchestral passages in early progressive rock. Progressive rock has also served as a key inspiration for genres such as post-rock, post-metal and avant-garde metal, math rock, power metal, and neo-classical metal. Stoner metal bands frequently point to Hawkwind as a main influence.
New prog.
"New prog," also known as nu prog or post-prog, is a term that appeared in the mid-2000s to describe a number of alternative bands who incorporated elements from progressive rock or had an expansive, musically diverse, approach to music played in a contemporary style. These bands often play a harder-edged, speed metal and punk-influenced music that is conducive to moshing. Songs often feature jarring shifts between soft acoustic sections and powerful metallic sections, as on "Blackest Eyes" by Porcupine Tree.
Ozric Tentacles employed a spacy, eclectic sound that became popular with rave audiences. The Mars Volta, who incorporated jazz, funk, punk rock, Latin music, and ambient noise into songs that range in length from a few minutes to over a half-hour, was formed by Cedric Bixler-Zavala and Omar Rodriguez-Lopez, former members of the post-hardcore band At the Drive-In. Their 2005 album "Frances the Mute" reached number 4 on the "Billboard" 200 chart after the single "The Widow" became a hit on modern rock radio. Coheed and Cambria are known for lengthy solos and a conceptual approach in which each album corresponds to an installment in lead singer/guitarist Claudio Sanchez's graphic novel series, "The Amory Wars". Mystery Jets is a father-and-son band that combines a modern sensibility with elements of progressive rock music from the 1970s.
2010s.
Progressive rock continues to appeal to its longtime fans and is also able to attract new audiences. The Progressive Music Awards were launched in 2012 by "Prog Magazine" to honor the genre's innovators and to promote its newer bands. Honorees, however, are not invited to perform at the awards ceremony, as the promoters want an event "that doesn't last three weeks."
Festivals.
Many prominent progressive rock bands got their initial exposure at large rock festivals that were held in Great Britain during the late 1960s and early 1970s. King Crimson made their first major appearance at the 1969 Hyde Park free concert, before a crowd estimated to be as large as 650,000, in support of The Rolling Stones. Emerson, Lake & Palmer debuted at the 1970 Isle of Wight Festival, at which Supertramp, Family and Jethro Tull also appeared. Jethro Tull were also present at the 1969 Newport Jazz Festival, the first year in which that festival invited rock bands to perform. Hawkwind appeared at many British festivals throughout the 1970s, although they sometimes showed up uninvited, set up a stage on the periphery of the event, and played for free.
Renewed interest in the genre in the 1990s led to the development of progressive rock festivals. ProgFest, organized by Greg Walker and David Overstreet in 1993, was first held in UCLA's Royce Hall, and featured Sweden's Änglagård, the UK's IQ, Quill and Citadel. CalProg was held annually in Whittier, California during the 2000s. The North East Art Rock Festival, or NEARfest, held its first event in 1999 in Bethlehem, Pennsylvania and held annual sold-out concerts until 2012's NEARfest Apocalypse, which featured headliners U.K. and Renaissance. Other festivals include the annual (the longest-running and only outdoor prog festival) in Chapel Hill, North Carolina, the annual Rites of Spring Festival (RoSfest) in Gettysburg, Pennsylvania, The Rogue Independent Music Festival in Atlanta, Georgia, Baja Prog in Mexicali, Mexico, ProgPower USA in Atlanta, Georgia and ProgPower Europe in Baarlo, Netherlands. Progressive Nation tours were held in 2008 and 2009 with Dream Theater as the headline act.
Reception.
The genre has received both a great amount of critical acclaim and criticism throughout the years. Progressive rock has been described as parallel to the classical music of Igor Stravinsky and Béla Bartók. This desire to expand the boundaries of rock, combined with some musicians' dismissiveness toward mainstream rock and pop music, insulted critics and led to accusations of elitism. Its intellectual, fantastic and apolitical lyrics and its shunning of rock's blues roots were abandonments of the very things that many critics valued in rock music. Progressive rock also represented the maturation of rock as a genre, but there was an opinion among critics that rock was and should remain fundamentally tied to adolescence, so that rock and maturity were mutually exclusive.
Criticisms over the complexity of their music provoked some bands to create music that was even more complex. Yes' "Tales from Topographic Oceans" and "The Gates of Delirium" were both responses to such criticisms. Jethro Tull's "Thick As a Brick", a self-satirising concept album that consisted of a single 45-minute track, arose from the band's disagreement with the labeling of their previous "Aqualung" as a concept album.
These aspirations toward high culture reflect progressive rock's origins as a music created largely by upper- and middle-class, white-collar, college-educated males from Southern England. The music never reflected the concerns of or was embraced by working class listeners, except in the US, where listeners appreciated the musicians' virtuosity. Progressive rock's exotic, literary topics were considered particularly irrelevant to British youth during the late 1970s, when the nation suffered from a poor economy and frequent strikes and shortages. Even King Crimson leader Robert Fripp dismissed progressive rock lyrics as "the philosophical meanderings of some English half-wit who is circumnavigating some inessential point of experience in his life." Bands whose darker lyrics avoided utopianism, such as King Crimson, Pink Floyd and Van der Graaf Generator, experienced less critical disfavor. Critics similarly came to regard krautrock as a genre separate from progressive rock. The simplicity of punk was in part a reaction against the elaborate nature of progressive rock.

</doc>
<doc id="51504" url="http://en.wikipedia.org/wiki?curid=51504" title="Dell Hymes">
Dell Hymes

Dell Hathaway Hymes (June 7, 1927, Portland, Oregon – November 13, 2009, Charlottesville, Virginia) was a linguist, sociolinguist, anthropologist, and folklorist who established disciplinary foundations for the comparative, ethnographic study of language use. His research focused upon the languages of the Pacific Northwest. He was one of the first to call the fourth subfield of anthropology "linguistic anthropology" instead of "anthropological linguistics". The terminological shift draws attention to the field's grounding in anthropology rather than in what, by that time, had already become an autonomous discipline (linguistics). In 1972 Hymes founded the journal "Language in Society" and served as its editor for 22 years.
Early life and education.
He was educated at Reed College, studying under David H. French, and graduated in 1950 after a stint in prewar Korea. His work in the Army as a decoder is part of what influenced him to become a linguist. Hymes earned his Ph.D. from Indiana University in 1955, and took a job at Harvard University.
Even at that young age, Hymes had a reputation as a strong linguist; his dissertation, completed in one year, was a grammar of the Kathlamet language spoken near the mouth of the Columbia and known primarily from Franz Boas’s work at the end of the 19th century.
Hymes remained at Harvard for five years, leaving in 1960 to join the faculty of the University of California, Berkeley. He spent five years at Berkeley as well, and then joined the Department of Anthropology at the University of Pennsylvania in 1965 (where he succeeded A. Irving Hallowell). In 1972 he joined the Department of Folklore and Folklife and became Dean of the University of Pennsylvania Graduate School of Education in 1975.
He served as president of the Linguistic Society of America in 1982, of the American Anthropological Association in 1983, and of the American Folklore Society - the last person to have held all three positions. He was a member of the Guild of Scholars of The Episcopal Church. While at Penn, Hymes was a founder of the journal "Language in Society". Hymes later joined the Departments of Anthropology and English at the University of Virginia, where he became the Commonwealth Professor of Anthropology and English, and from which he retired in 2000, continuing as emeritus professor until his death from complications of Alzheimer's disease on November 13, 2009.
His spouse, Virginia Hymes, is also a sociolinguist and folklorist.
Influences on his work.
Hymes was influenced by a number of linguists, anthropologists and sociologists, notably Franz Boas, Edward Sapir and Harry Hoijer of the Americanist Tradition; Roman Jakobson and others of the Prague Linguistic Circle; sociologist Erving Goffman, anthropologist Ray L. Birdwhistell, and ethnomethodologists Harold Garfinkel, Harvey Sacks, Emanuel Schegloff and Gail Jefferson.
Hymes' career can be divided into at least two phases. In his early career Hymes adapted Prague School Functionalism to American Linguistic Anthropology, pioneering the study of the relationship between language and social context. Together with John Gumperz, Erving Goffman and William Labov, Hymes defined a broad multidisciplinary concern with language in society.
Hymes' later work focuses on poetics, particularly the poetic organization of Native American oral narratives. He and Dennis Tedlock defined ethnopoetics as a field of study within linguistic anthropology and folkloristics. Hymes considers literary critic Kenneth Burke his biggest influence on this latter work, saying, “My sense of what I do probably owes more to KB than to anyone else”. Hymes studied with Burke in the 1950s. Burke's work was theoretically and topically diverse, but the idea that seems most influential on Hymes is the application of rhetorical criticism to poetry.
Hymes has included many other literary figures and critics among his influences, including Robert Alter, C. S. Lewis, A. L. Kroeber, Claude Lévi-Strauss.
Significance of his work.
As one of the first sociolinguists, Hymes helped to pioneer the connection between speech and social relations placing linguistic anthropology at the center of the performative turn within anthropology and the social sciences more generally.
Hymes formulated a response to Noam Chomsky's influential distinction between competence (knowledge of grammatical rules necessary to decoding and producing language) and performance (actual language use in context). Hymes objected to the marginalization of performance from the center of linguistic inquiry and proposed the notion of communicative competence, or knowledge necessary to use language in social context, as an object of linguistic inquiry. Since appropriate language use is conventionally defined, and varies across different communities, much of Hymes early work frames a project for ethnographic investigation into contrasting patterns of language use across speech communities. Hymes termed this approach "the ethnography of speaking." The SPEAKING acronym, described below, was presented as a lighthearted heuristic to aid fieldworkers in their attempt to document and analyze instances of language in use, which he termed "speech events." Embedded in the acronym is an application and extension of Jakobson's arguments concerning the multifunctionality of language. He articulated other, more technical, often typologically oriented approaches to variation in patterns of language use across speech communities in a series of articles.
As a result of discussions primarily with Ray Birdwhistell at the University of Pennsylvania, in his later work, Hymes renamed the "ethnography of speaking" the "ethnography of communication" to reflect the broadening of focus from instances of language production to the ways in which communication (including oral, written, broadcast, acts of receiving/listening) is conventionalized in a given community of users, and to include nonverbal as well as verbal behavior.
Hymes promoted what he and others call “ethnopoetics,” an anthropological method of transcribing and analyzing folklore and oral narrative that pays attention to poetic structures within speech. In reading the transcriptions of Indian myths, for example, which were generally recorded as prose by the anthropologists who came before, Hymes noticed that there are commonly poetic structures in the wording and structuring of the tale. Patterns of words and word use follow patterned, artistic forms.
Hymes’ goal, in his own mind, is to understand the artistry and “the competence… that underlies and informs such narratives” (Hymes 2003:vii). He created the Dell Hymes Model of Speaking and coined the term communicative competence within language education.
In addition to being entertaining stories or important myths about the nature of the world, narratives also convey the importance of aboriginal environmental management knowledge such as fish spawning cycles in local rivers or the disappearance of grizzly bears from Oregon. Hymes believes that all narratives in the world are organized around implicit principles of form which convey important knowledge and ways of thinking and of viewing the world. He argues that understanding narratives will lead to a fuller understanding of the language itself and those fields informed by storytelling, in which he includes ethnopoetics, sociolinguistics, psycholinguistics, rhetoric, semiotics, pragmatics, narrative inquiry and literary criticism.
Hymes clearly considers folklore and narrative a vital part of the fields of linguistics, anthropology and literature, and has bemoaned the fact that so few scholars in those fields are willing and able to adequately include folklore in its original language in their considerations (Hymes 1981:6-7). He feels that the translated versions of the stories are inadequate for understanding their role in the social or mental system in which they existed. He provides an example that in Navajo, the particles (utterances such as "uh," "So," "Well," etc. that have linguistic if not semantic meaning), omitted in the English translation, are essential to understanding how the story is shaped and how repetition defines the structure that the text embodies.
Hymes was the founding editor for the journal "Language in Society", which he edited for 22 years.
The "S-P-E-A-K-I-N-G" model.
Hymes developed a valuable model to assist the identification and labeling of components of linguistic interaction that was driven by his view that, in order to speak a language correctly, one needs not only to learn its vocabulary and grammar, but also the context in which words are used.
The model had sixteen components that can be applied to many sorts of discourse: message form; message content; setting; scene; speaker/sender; addressor; hearer/receiver/audience; addressee; purposes (outcomes); purposes (goals); key; channels; forms of speech; norms of interaction; norms of interpretation; and genres.
Hymes constructed the acronym SPEAKING, under which he grouped the sixteen components within eight divisions:
Setting and Scene.
"Setting refers to the time and place of a speech act and, in general, to the physical circumstances". The living room in the grandparents' home might be a setting for a family story. Scene is the "psychological setting" or "cultural definition" of a setting, including characteristics such as range of formality and sense of play or seriousness. The family story may be told at a reunion celebrating the grandparents' anniversary. At times, the family would be festive and playful; at other times, serious and commemorative.
Participants.
Speaker and audience. Linguists will make distinctions within these categories; for example, the audience can be distinguished as addressees and other hearers. At the family reunion, an aunt might tell a story to the young female relatives, but males, although not addressed, might also hear the narrative.
Ends.
Purposes, goals, and outcomes. The aunt may tell a story about the grandmother to entertain the audience, teach the young women, and honor the grandmother.
Act Sequence.
Form and order of the event. The aunt's story might begin as a response to a toast to the grandmother. The story's plot and development would have a sequence structured by the aunt. Possibly there would be a collaborative interruption during the telling. Finally, the group might applaud the tale and move onto another subject or activity.
Key.
Clues that establish the "tone, manner, or spirit" of the speech act. The aunt might imitate the grandmother's voice and gestures in a playful way, or she might address the group in a serious voice emphasizing the sincerity and respect of the praise the story expresses.
Instrumentalities.
Forms and styles of speech. The aunt might speak in a casual register with many dialect features or might use a more formal register and careful grammatically "standard" forms.
Norms.
Social rules governing the event and the participants' actions and reaction. In a playful story by the aunt, the norms might allow many audience interruptions and collaboration, or possibly those interruptions might be limited to participation by older females. A serious, formal story by the aunt might call for attention to her and no interruptions as norms.
Genre.
The kind of speech act or event; for the example used here, the kind of story. The aunt might tell a character anecdote about the grandmother for entertainment, or an exemplum as moral instruction. Different disciplines develop terms for kinds of speech acts, and speech communities sometimes have their own terms for types.
Personal Life.
Credible accusations have been made of harassment of female students by Dell Hymes, though no formal action was taken as these claims were made posthumously.

</doc>
<doc id="51509" url="http://en.wikipedia.org/wiki?curid=51509" title="County seat">
County seat

A county seat is an administrative center, or seat of government, for a county or civil parish. The term is used in the United States, Taiwan and Romania. In the United Kingdom and Ireland, county towns have a similar function.
Function.
In the United States, counties are the administrative subdivisions of a state. Counties administer state law at the local level as part of the decentralization of state authority. In many states, state government is further decentralized below the county level by dividing counties into incorporated cities and towns and/or unincorporated civil townships, in order to provide local government services. The city, town, or populated place that houses county government is known as the seat of its respective county. Generally, the county legislature, county courthouse, sheriff's department headquarters, and hall of records, are located in the county seat, though some functions may also be conducted in other parts of the county, especially if it is geographically large.
A county seat is usually, but not always, an incorporated municipality. The exceptions include the county seats of counties that have no incorporated municipalities within their borders, such as Arlington County, Virginia, and Howard County, Maryland. (Ellicott City, the county seat of Howard County, is the largest unincorporated county seat in the United States, followed by Towson, the county seat of Baltimore County, Maryland.) Likewise, some county seats may not be incorporated in their own right, but are located within incorporated municipalities. For example, Cape May Court House, New Jersey, though unincorporated, is a section of Middle Township, an incorporated municipality. In some of the colonial states, county seats include or formerly included "Court House" as part of their name, (e.g. Spotsylvania Courthouse, Virginia).
U.S. counties with more than one county seat.
Most counties have only one county seat. However, some counties in Alabama, Arkansas, Iowa, Kentucky, Massachusetts, Mississippi, Missouri, New Hampshire, and Vermont have two or more county seats, usually located on opposite sides of the county. An example is Harrison County, Mississippi, which lists both Biloxi and Gulfport as county seats. The practice of multiple county seat towns dates from the days when travel was difficult. There have been few efforts to eliminate the two-seat arrangement, since a county seat is a source of pride (and jobs) for the towns involved.
There are 35 counties with multiple county seats (no more than two each) in 10 states:
Guilford County, North Carolina, in some ways effectively has two county seats. For example, the official county seat is Greensboro, but an additional has been located in nearby High Point since 1938.
Other counties in the United States effectively have two or more county seats by establishing one or more branch courthouses at which county business, including the recordation of documents affecting real estate, may be transacted. For example, Clearwater is the county seat of Pinellas County, Florida, but there is a branch courthouse in St. Petersburg. Likewise, DeLand is the county seat of Volusia County, Florida, but there are branch courthouses in Daytona Beach and in New Smyrna Beach.
Other variations.
In New England, the town, not the county, is the primary division of local government. Historically, counties in this region have served mainly as dividing lines for the states' judicial systems. Connecticut (since 1960) and Rhode Island have no county level of government and thus no county seats. In Vermont, Massachusetts, and Maine the county seats are called "shire towns". County government consists only of a Superior Court and Sheriff (as an officer of the court), both located in the respective shire town. Bennington County has two shire towns (Manchester for the "North Shire", Bennington for the "South Shire"), but both the Court and the Sheriff are in Bennington. In Massachusetts, most government functions which would otherwise be performed by county governments in other states are performed by town governments (there are no unincorporated areas in the state, that is, all land area in the state is within a town). As such, Massachusetts has dissolved many of its county governments, and the state government now operates the registries of deeds and sheriff's offices in those former counties. 
In Virginia, a county seat may be an independent city surrounded by, but not part of, the county of which it is the administrative center; for example, Fairfax City is both the county seat of Fairfax County and is completely surrounded by Fairfax County, but the city is politically independent of the county.
Two counties in South Dakota (Oglala Lakota and Todd) have their county seat and government services centered in a neighboring county. Their county-level services are provided by Fall River County and Tripp County, respectively.
In Louisiana, which is divided into parishes rather than counties, county seats are referred to as "parish seats" or "parish capitals".
Alaska is divided into boroughs rather than counties; the county seat in these case is referred to as the "borough seat"; this includes six consolidated city-borough governments and one municipality. The Unorganized Borough, which covers 49% of Alaska's area, has no county seat or equivalent. 
Canada.
In the Canadian Provinces of Prince Edward Island, New Brunswick and Nova Scotia, the term "shire town" is used in place of county seat.
Lists of U.S. county seats by state.
The state with the greatest number of counties is Texas, with 254, and the state with the least number of counties is Delaware, with 3.

</doc>
<doc id="51510" url="http://en.wikipedia.org/wiki?curid=51510" title="Silk">
Silk

Silk is a natural protein fiber, some forms of which can be woven into textiles. The protein fiber of silk is composed mainly of fibroin and is produced by certain insect larvae to form cocoons. The best-known silk is obtained from the cocoons of the larvae of the mulberry silkworm "Bombyx mori" reared in captivity (sericulture). The shimmering appearance of silk is due to the triangular prism-like structure of the silk fibre, which allows silk cloth to refract incoming light at different angles, thus producing different colors.
Silk is produced by several insects, but generally only the silk of moth caterpillars has been used for textile manufacturing. There has been some research into other types of silk, which differ at the molecular level. Silk is mainly produced by the larvae of insects undergoing complete metamorphosis, but some adult insects such as webspinners also produce silk, and some insects such as raspy crickets produce silk throughout their lives. Silk production also occurs in Hymenoptera (bees, wasps, and ants), silverfish, mayflies, thrips, leafhoppers, beetles, lacewings, fleas, flies, and midges. Other types of arthropod produce silk, most notably various arachnids such as spiders (see spider silk).
Etymology.
The word silk comes from Old English "sioloc", from Greek σηρικός "serikos", "silken", ultimately from an Asian source (cf. Chinese "si" "silk", Manchurian "sirghe", Mongolian "sirkek").
History.
Wild silk.
Several kinds of wild silk, which are produced by caterpillars other than the mulberry silkworm, have been known and used in China, South Asia, and Europe since ancient times. However, the scale of production was always far smaller than for cultivated silks. There are several reasons for this: firstly, they differ from the domesticated varieties in colour and texture and are therefore less uniform; secondly, cocoons gathered in the wild have usually had the pupa emerge from them before being discovered so the silk thread that makes up the cocoon has been torn into shorter lengths; and thirdly, many wild cocoons are covered in a mineral layer that stymies attempts to reel from them long strands of silk. Thus, previously, the only way to obtain silk suitable for spinning into textiles in areas where commercial silks are not cultivated was by tedious and labor-intensive carding.
Commercial silks originate from reared silkworm pupae, which are bred to produce a white-colored silk thread with no mineral on the surface. The pupae are killed by either dipping them in boiling water before the adult moths emerge or by piercing them with a needle. These factors all contribute to the ability of the whole cocoon to be unravelled as one continuous thread, permitting a much stronger cloth to be woven from the silk.
Wild silks also tend to be more difficult to dye than silk from the cultivated silkworm. A technique known as demineralizing allows the mineral layer around the cocoon of wild silk moths to be removed, leaving only variability in color as a barrier to creating a commercial silk industry based on wild silks in the parts of the world where wild silk moths thrive, such as in Africa and South America.
Genetic modification of domesticated silkworms is used to facilitate the production of more useful types of silk.
China.
Silk fabric was first developed in ancient China. The earliest example of silk fabric is from 3630 BC, and it was used as wrapping for the body of a child from a Yangshao site in Qingtaicun at Xingyang, Henan. 
Legend gives credit for developing silk to a Chinese empress, Leizu (Hsi-Ling-Shih, Lei-Tzu). Silks were originally reserved for the Emperors of China for their own use and gifts to others, but spread gradually through Chinese culture and trade both geographically and socially, and then to many regions of Asia. Because of its texture and lustre, silk rapidly became a popular luxury fabric in the many areas accessible to Chinese merchants. Silk was in great demand, and became a staple of pre-industrial international trade. In July 2007, archaeologists discovered intricately woven and dyed silk textiles in a tomb in Jiangxi province, dated to the Eastern Zhou Dynasty roughly 2,500 years ago. Although historians have suspected a long history of a formative textile industry in ancient China, this find of silk textiles employing "complicated techniques" of weaving and dyeing provides direct evidence for silks dating before the Mawangdui-discovery and other silks dating to the Han Dynasty (202 BC-220 AD).
Silk is described in a chapter on mulberry planting by Si Shengzhi of the Western Han (206 BC – 9 AD). There is a surviving calendar for silk production in an Eastern Han (25–220 AD) document. The two other known works on silk from the Han period are lost. The first evidence of the silk trade is the finding of silk in the hair of an Egyptian mummy of the 21st dynasty, c.1070 BC. The silk trade reached as far as the Indian subcontinent, the Middle East, Europe, and North Africa. This trade was so extensive that the major set of trade routes between Europe and Asia came to be known as the Silk Road.
The Emperors of China strove to keep knowledge of sericulture secret to maintain the Chinese monopoly. Nonetheless sericulture reached Korea with technological aid from China around 200 BC, the ancient Kingdom of Khotan by AD 50, and India by AD 140.
In the ancient era, silk from China was the most lucrative and sought-after luxury item traded across the Eurasian continent, and many civilizations, such as the ancient Persians, benefited economically from trade.
India.
Silk has a long history in India. It is known as "Paat" in eastern India, "Pattu" in southern parts of India, and "Resham" in north India. Recent archaeological discoveries in Harappa and Chanhu-daro suggest that sericulture, employing wild silk threads from native silkworm species, existed in South Asia during the time of the Indus Valley Civilization dating between 2450 BC and 2000 BC, while "hard and fast evidence" for silk production in China dates back to around 2570 BC. Shelagh Vainker, a silk expert at the Ashmolean Museum in Oxford, who sees evidence for silk production in China "significantly earlier" than 2500–2000 BC, suggests, "people of the Indus civilization either harvested silkworm cocoons or traded with people who did, and that they knew a considerable amount about silk."
India is the second largest producer of silk in the world after China. About 97% of the raw silk comes from five Indian states, namely, Andhra Pradesh, Karnataka, Jammu and Kashmir, Tamil Nadu and West Bengal. North Bangalore, the upcoming site of a $20 million "Silk City" Ramanagara and Mysore, contribute to a majority of silk production in Karnataka. 
In Tamil Nadu, mulberry cultivation is concentrated in the Coimbatore, Erode, Tiruppur, Salem and Dharmapuri districts. Hyderabad, Andhra Pradesh, and Gobichettipalayam, Tamil Nadu, were the first locations to have automated silk reeling units in India.
India is also the largest consumer of silk in the world. The tradition of wearing silk sarees for marriages and other auspicious ceremonies is a custom in Assam and southern parts of India. Silk is considered to be a symbol of royalty, and, historically, silk was used primarily by the upper classes. Silk garments and sarees produced in Kanchipuram, Pochampally, Dharmavaram, Mysore, Arani in the south, Banaras in the north, and Murshidabad in the east are well recognized. In the northeastern state of Assam, three different types of silk are produced, collectively called Assam silk: Muga, Eri and Pat silk. Muga, the golden silk, and Eri are produced by silkworms that are native only to Assam.
Thailand.
Silk is produced year round in Thailand by two types of silkworms, the cultured Bombycidae and wild Saturniidae. Most production is after the rice harvest in the southern and northeastern parts of the country. Women traditionally weave silk on hand looms and pass the skill on to their daughters, as weaving is considered to be a sign of maturity and eligibility for marriage. Thai silk textiles often use complicated patterns in various colours and styles. Most regions of Thailand have their own typical silks. A single thread filament is too thin to use on its own so women combine many threads to produce a thicker, usable fiber. They do this by hand-reeling the threads onto a wooden spindle to produce a uniform strand of raw silk. The process takes around 40 hours to produce a half kilogram of silk. Many local operations use a reeling machine for this task, but some silk threads are still hand-reeled. The difference is that hand-reeled threads produce three grades of silk: two fine grades that are ideal for lightweight fabrics, and a thick grade for heavier material.
The silk fabric is soaked in extremely cold water and bleached before dyeing to remove the natural yellow coloring of Thai silk yarn. To do this, skeins of silk thread are immersed in large tubs of hydrogen peroxide. Once washed and dried, the silk is woven on a traditional hand-operated loom.
Ancient Mediterranean.
In the "Odyssey", 19.233, when Odysseus, while pretending to be someone else, is questioned by Penelope about her husband's clothing, he says that he wore a shirt "gleaming like the skin of a dried onion" (varies with translations, literal translation here) which could refer to the lustrous quality of silk fabric. The Roman Empire knew of and traded in silk, and Chinese silk was the most highly priced luxury good imported by them. During the reign of emperor Tiberius, sumptuary laws were passed that forbade men from wearing silk garments, but these proved ineffectual. Despite the popularity of silk, the secret of silk-making only reached Europe around AD 550, via the Byzantine Empire. Legend has it that monks working for the emperor Justinian I smuggled silkworm eggs to Constantinople in hollow canes from China. All top-quality looms and weavers were located inside the Great Palace complex in Constantinople, and the cloth produced was used in imperial robes or in diplomacy, as gifts to foreign dignitaries. The remainder was sold at very high prices.
Middle East.
In the Torah, a scarlet cloth item called in Hebrew "sheni tola'at" שני תולעת - literally "crimson of the worm" - is described as being used in purification ceremonies, such as those following a leprosy outbreak (Leviticus 14), alongside cedar wood and hyssop (za'atar). Eminent scholar and leading medieval translator of Jewish sources and books of the Bible into Arabic, Rabbi Saadia Gaon, translates this phrase explicitly as "crimson silk" - חריר קרמז حرير قرمز.
In Islamic teachings, Muslim men are forbidden to wear silk. Many religious jurists believe the reasoning behind the prohibition lies in avoiding clothing for men that can be considered feminine or extravagant. There are disputes regarding the amount of silk a fabric can consist of (e.g., whether a small decorative silk piece on a cotton caftan is permissible or not) for it to be lawful for men to wear, but the dominant opinion of most Muslim scholars is that the wearing of silk by men is forbidden. Modern attire has raised a number of issues, including, for instance, the permissibility of wearing silk neckties, which are masculine articles of clothing.
Despite injunctions against silk for men, silk has retained its popularity in the Islamic world because of its permissibility for women, and due to the presence of non-Muslim communities. The Muslim Moors brought silk with them to Spain during their conquest of the Iberian Peninsula.
Medieval and modern Europe.
Italy was the most important producer of silk during the Medieval age. The first center to introduce silk production to Italy was the city of Catanzaro during the 11th century in the region of Calabria. The silk of Catanzaro supplied almost all of Europe and was sold in a large market fair in the port of Reggio Calabria, to Spanish, Venetian, Genovese and Dutch merchants. Catanzaro became the lace capital of the world with a large silkworm breeding facility that produced all the laces and linens used in the Vatican. The city was world-famous for its fine fabrication of silks, velvets, damasks and brocades.
Another notable center was the Italian city-state of Lucca which largely financed itself through silk-production and silk-trading, beginning in the 12th century. Other Italian cities involved in silk production were Genoa, Venice and Florence.
The Silk Exchange in Valencia from the 15th century—where previously in 1348 also "perxal" (percale) was traded as some kind of silk—illustrates the power and wealth of one of the great Mediterranean mercantile cities.
Silk was produced in and exported from the province of Granada, Spain, especially the Alpujarras region, until the Moriscos, whose industry it was, were expelled from Granada in 1571.
Since the 15th century, silk production in France has been centered around the city of Lyon where many mechanic tools for mass production were first introduced in the 17th century.
James I attempted to establish silk production in England, purchasing and planting 100,000 mulberry trees, some on land adjacent to Hampton Court Palace, but they were of a species unsuited to the silk worms, and the attempt failed. In 1732 John Guardivaglio set up a silk throwing enterprise at Logwood mill in Stockport; in 1744, Burton Mill was erected in Macclesfield; and in 1753 Old Mill was built in Congleton. These three towns remained the centre of the English silk throwing industry until silk throwing was replaced by silk waste spinning. British enterprise also established silk filature in Cyprus in 1928. In England in the mid-20th century, raw silk was produced at Lullingstone Castle in Kent. Silkworms were raised and reeled under the direction of Zoe Lady Hart Dyke. Production started elsewhere later.
North America.
King James I introduced silk-growing to the American colonies around 1619, ostensibly to discourage tobacco planting. The Shakers in Kentucky adopted the practice. In the 19th century a new attempt at a silk industry began with European-born workers in Paterson, New Jersey, and the city became a silk center in the United States. Manchester, Connecticut emerged as center of the silk industry in America from the late 19th through the mid-20th century. The Cheney Brothers Historic District showcases mills refurbished as apartments and includes nearby museums.
World War II interrupted the silk trade from Asia, and silk prices increased dramatically. U.S. industry began to look for substitutes, which led to the use of synthetics such as nylon. Synthetic silks have also been made from lyocell, a type of cellulose fiber, and are often difficult to distinguish from real silk (see spider silk for more on synthetic silks).
Malaysia.
In Terengganu, which is now part of Malaysia, a second generation of silkworm was being imported as early as 1764 for the country's silk textile industry, especially songket. However, since the 1980s, Malaysia is no longer engaged in sericulture but does plant mulberry trees.
Vietnam.
In Vietnamese legend, silk appeared in the sixth dynasty of Hung Vuong.
Production process.
The entire production process of silk can be divided into several steps which are typically handled by different entities. Extracting raw silk starts by cultivating the silkworms on mulberry leaves. Once the worms start pupating in their cocoons, these are dissolved in boiling water in order for individual long fibres to be extracted and fed into the spinning reel.
Properties.
Physical properties.
Silk fibers from the "Bombyx mori" silkworm have a triangular cross section with rounded corners, 5–10 μm wide. The fibroin-heavy chain is composed mostly of beta-sheets, due to a 59-mer amino acid repeat sequence with some variations. The flat surfaces of the fibrils reflect light at many angles, giving silk a natural sheen. The cross-section from other silkworms can vary in shape and diameter: crescent-like for "Anaphe" and elongated wedge for "tussah". Silkworm fibers are naturally extruded from two silkworm glands as a pair of primary filaments (brin), which are stuck together, with sericin proteins that act like glue, to form a bave. Bave diameters for tussah silk can reach 65 μm. See cited reference for cross-sectional SEM photographs.
Silk has a smooth, soft texture that is not slippery, unlike many synthetic fibers.
Silk is one of the strongest natural fibers, but it loses up to 20% of its strength when wet. It has a good moisture regain of 11%. Its elasticity is moderate to poor: if elongated even a small amount, it remains stretched. It can be weakened if exposed to too much sunlight. It may also be attacked by insects, especially if left dirty.
One example of the durable nature of silk over other fabrics is demonstrated by the recovery in 1840 of silk garments from a wreck of 1782: 'The most durable article found has been silk; for besides pieces of cloaks and lace, a pair of black satin breeches, and a large satin waistcoat with flaps, were got up, of which the silk was perfect, but the lining entirely gone ... from the thread giving way ... No articles of dress of woollen cloth have yet been found.'
Silk is a poor conductor of electricity and thus susceptible to static cling.
Unwashed silk chiffon may shrink up to 8% due to a relaxation of the fiber macrostructure, so silk should either be washed prior to garment construction, or dry cleaned. Dry cleaning may still shrink the chiffon up to 4%. Occasionally, this shrinkage can be reversed by a gentle steaming with a press cloth. There is almost no gradual shrinkage nor shrinkage due to molecular-level deformation.
Natural and synthetic silk is known to manifest piezoelectric properties in proteins, probably due to its molecular structure.
Silkworm silk was used as the standard for the denier, a measurement of linear density in fibers. Silkworm silk therefore has a linear density of approximately 1 den, or 1.1 dtex.
Chemical properties.
Silk emitted by the silkworm consists of two main proteins, sericin and fibroin, fibroin being the structural center of the silk, and serecin being the sticky material surrounding it. Fibroin is made up of the amino acids Gly-Ser-Gly-Ala-Gly-Ala and forms beta pleated sheets. Hydrogen bonds form between chains, and side chains form above and below the plane of the hydrogen bond network.
The high proportion (50%) of glycine allows tight packing. This is because glycine's R group is only a hydrogen and so is not as sterically constrained. The addition of alanine and serine makes the fibres strong and resistant to breaking. This tensile strength is due to the many interceded hydrogen bonds, and when stretched the force is applied to these numerous bonds and they do not break.
Silk is resistant to most mineral acids, except for sulfuric acid, which dissolves it. It is yellowed by perspiration. Chlorine bleach will also destroy silk fabrics.
Uses.
Silk's absorbency makes it comfortable to wear in warm weather and while active. Its low conductivity keeps warm air close to the skin during cold weather. It is often used for clothing such as shirts, ties, blouses, formal dresses, high fashion clothes, lining, lingerie, pajamas, robes, dress suits, sun dresses and Eastern folk costumes. Silk's attractive lustre and drape makes it suitable for many furnishing applications. It is used for upholstery, wall coverings, window treatments (if blended with another fiber), rugs, bedding and wall hangings. While on the decline now, due to artificial fibers, silk has had many industrial and commercial uses, such as in parachutes, bicycle tires, comforter filling and artillery gunpowder bags.
Fabrics that are often made from silk include charmeuse, habutai, chiffon, taffeta, crepe de chine, dupioni, noil, tussah, and shantung, among others.
A special manufacturing process removes the outer irritant sericin coating of the silk, which makes it suitable as non-absorbable surgical sutures. This process has also recently led to the introduction of specialist silk underclothing for people with eczema where it can significantly reduce it. New uses and manufacturing techniques have been found for silk for making everything from disposable cups to drug delivery systems and holograms. To produce 1 kg of silk, 104 kg of mulberry leaves must be eaten by 3000 silkworms. It takes about 5000 silkworms to make a pure silk kimono.:104 The production of silk is called sericulture. The major silk producers are China (54%) and India (14%).
Cultivation.
Silk moths lay eggs on specially prepared paper. The eggs hatch and the caterpillars (silkworms) are fed on fresh mulberry leaves. After about 35 days and 4 moltings, the caterpillars are 10,000 times heavier than when hatched and are ready to begin spinning a cocoon. A straw frame is placed over the tray of caterpillars, and each caterpillar begins spinning a cocoon by moving its head in a pattern. Two glands produce liquid silk and force it through openings in the head called spinnerets. Liquid silk is coated in sericin, a water-soluble protective gum, and solidifies on contact with the air. Within 2–3 days, the caterpillar spins about 1 mile of filament and is completely encased in a cocoon. The silk farmers then kill most caterpillars by heat, leaving some to metamorphose into moths to breed the next generation of caterpillars. Harvested cocoons are then soaked in boiling water to soften the sericin holding the silk fibers together in a cocoon shape. The fibers are then unwound to produce a continuous thread. Since a single thread is too fine and fragile for commercial use, anywhere from three to ten strands are spun together to form a single thread of silk.
Animal rights.
As the process of harvesting the silk from the cocoon kills the larvae, sericulture has been criticized by animal welfare and rights activists. Mohandas Gandhi was critical of silk production based on the Ahimsa philosophy which led to promotion of cotton and Ahimsa silk, a type of wild silk made from the cocoons of wild and semi-wild silk moths. Due to the death that silk production inflicts on the silkworms, People for the Ethical Treatment of Animals (PETA) urges people not to buy silk items.

</doc>
<doc id="51511" url="http://en.wikipedia.org/wiki?curid=51511" title="Prince">
Prince

A prince is a male ruler, monarch, or member of a monarch's or former monarch's family. "Prince" is a hereditary title in the nobility of some European states. The feminine equivalent is a princess. The English word derives, via the French word "prince", from the Latin noun "princeps", from "primus" (first) + "capio" (to seize), meaning "the chief, most distinguished, ruler, prince".
Historical background.
The Latin word "prīnceps" (older Latin *prīsmo-kaps, literally "the one who takes the first [place/position]"), became the usual title of the informal leader of the Roman senate some centuries before the transition to empire, the "princeps senatus".
Emperor Augustus established the formal position of monarch on the basis of "principate", not "dominion". He also tasked his grandsons as summer rulers of the city when most of the government were on holiday in the country or attending religious rituals, and, for that task, granted them the title of "princeps".
The title has generic and substantive meanings:
Prince as generic for ruler.
The original, but now less common use of the word, originated in the application of the Latin word "princeps", from late Roman law, and the classical system of government that eventually gave way to the European feudal society. In this sense, a prince is a ruler of a territory which is sovereign, or quasi-sovereign, i.e., exercising substantial (though not all) prerogatives associated with monarchs of independent nations, as was common, for instance, within the historical boundaries of the Holy Roman Empire. In medieval and Early Modern Europe, there were as many as two hundred such territories, especially in Italy, Germany, and Gaelic Ireland. In this sense, "prince" is used of any and all rulers, regardless of actual title or precise rank. This is the Renaissance use of the term found in Niccolò Machiavelli's famous work, "Il Principe".
As a title, by the end of the medieval era, "prince" was borne by rulers of territories that were either substantially smaller than or exercised fewer of the rights of sovereignty than did emperors and kings. A lord of even a quite small territory might come to be referred to as a "prince" before the 13th century, either from translations of a native title into the Latin "princeps" (as for the hereditary ruler of Wales), or when the lord's territory was allodial. The lord of an allodium owned his lands and exercised prerogatives over the subjects in his territory absolutely, owing no feudal homage or duty as a vassal to a liege lord, nor being subject to any higher jurisdiction. Most small territories designated as principalities during feudal eras were allodial, e.g. the Princedom of Dombes.
Lords who exercised lawful authority over territories and people within a feudal hierarchy were also sometimes regarded as "princes" in the general sense, especially if they held the rank of count or higher. This is attested in some surviving styles for e.g., British earls, marquesses, and dukes are still addressed by the Crown on ceremonial occasions as "high and noble princes" (cf. Royal and noble styles).
In parts of the Holy Roman Empire in which primogeniture did not prevail (i.e. Germany), all legitimate agnates had an equal right to the family's hereditary titles. While this meant that offices, such as emperor, king, and elector could only be legally occupied by one dynast at a time, holders of such other titles as duke, margrave, landgrave, count palatine, and prince could only differentiate themselves by adding the name of their appanage to the family's original title. Not only did this tend to proliferate unwieldy titles (e.g. Princess Katherine of Anhalt-Zerbst and Karl, Count Palatine of Zweibrücken-Neukastell-Kleeburg and Prince Christian Charles of Schleswig-Holstein-Sonderburg-Plön-Norburg), but as agnatic primogeniture gradually became the norm in the Holy Roman Empire by the end of the 18th century, another means of distinguishing the monarch from other members of his dynasty became necessary. Gradual substitution of the title of "Prinz" for the monarch's title of "Fürst" occurred, and became customary in all German dynasties except in the grand duchies of Mecklenburg and Oldenburg. Both "Prinz" and "Fürst" are translated into English as "prince", but they reflect not only different but mutually exclusive terms.
This distinction had evolved before the 18th century (in most families: Liechtenstein long remained an exception, cadets and females using "Fürst/Fürstin" into the 19th century) for dynasties headed by a "Fürst" in Germany. The custom spread through the Continent to such an extent that a renowned imperial general who belonged to a cadet branch of a reigning ducal family, remains best known to history by the generic dynastic title, "Prince Eugene of Savoy". Note that the princely title was used as a prefix to his Christian name, which also became customary.
Cadets of France's "princes étrangers" began to affect similar usage but when, for example, the House of La Tour d'Auvergne's ruling dukes of Bouillon, attempted to use the same style, it was initially resisted by historians such as Père Anselme – who, however, willingly recognized use of territorial titles, i.e. he accepts that the ducal heir apparent is known as "prince de Bouillon", but would record in 1728 only that the heir's cousin, the comte d'Oliergues was ""known as" the Prince Frederick" (""dit" le prince Frédéric").
The post-medieval rank of "gefürsteter Graf" (princely count) embraced but elevated the German equivalent of the intermediate French, English and Spanish nobles. In Germany, these nobles rose to dynastic status by preserving from the Imperial crown ("de jure" after the Peace of Westphalia in 1648) the exercise of such sovereign prerogatives as the minting of money; the muster of military troops and the right to wage war and contract treaties; local judicial authority and constabular enforcement; and the habit of inter-marrying with sovereign dynasties. Eventually, these titles came to be more highly valued than that of "Fürst" itself, and by the 19th century, their cadets would become known as "Prinzen".
Prince of the blood.
Currently, the husband of a queen regnant is usually titled prince or prince consort, whereas the wives of male monarchs take the female equivalent of their husband's title—the same as is used when a female mounts the throne in her own right, such as empress or queen. In Brazil, Spain and Portugal, however, the husband of a female monarch was accorded the masculine equivalent of her title—at least after she bore him a child. In previous epochs, husbands of queens regnant often shared their consorts' regnal title and rank.
But in cultures which allow the ruler to have several wives (e.g. four in Islam) and/or official concubines, for these women sometimes collectively referred to as a harem there are often specific rules determining their hierarchy and a variety of titles, which may distinguish between those whose offspring can be in line for the succession or not, or specifically who is mother to the heir to the throne.
To complicate matters, the style "His Royal Highness", a prefix often accompanying the title of a dynastic prince, of royal rank, can be awarded separately (as a compromise or consolation prize, in some sense).
Although the arrangement set out above is the one that is most commonly understood, there are also different systems. Depending on country, epoch, and translation, other meanings of "prince" are possible.
Foreign-language titles such as Italian "principe", French "prince", German "Fürst" and "Prinz" (non-reigning descendants of a reigning sovereign "Fürst" or monarch), Russian "kniaz", etc., are usually translated as prince in English.
Some princely titles are derived from those of national rulers, such as tsarevich from tsar. Other examples are (e)mirza(da), khanzada, nawabzada, sahibzada, shahzada, sultanzada (all using the Persian patronymic suffix "-zada", meaning "son, descendant").
However, some princely titles develop in unusual ways, such as adoption of a style for dynasts which is not pegged to the ruler's title, but rather continues an old tradition (e.g. "grand duke" in Romanov Russia), claims dynastic succession to a lost monarchy (e.g. "prince de Tarente" for the La Trémoïlle heirs to the Neapolitan throne, or is simply assumed by fiat (e.g. "prince Français" by the House of Bonaparte).
Specific titles.
In some dynasties, a specific style other than prince has become customary for dynasts, such as "fils de France" in the House of Capet, and "Infante". "Infante" was borne by children of the monarch other than the heir apparent in all of the Iberian monarchies. Some monarchies used a specific princely title for their heirs, such as Prince of Asturias in Spain and Prince of Brazil in Portugal.
Sometimes a specific title is commonly used by various dynasties in a region, e.g. Mian in various of the Punjabi princely Hill States (lower Himalayan region in British India).
European dynasties usually awarded apanages (also spelt "appanages") to princes of the blood, typically attached to a feudal noble title, such as Britain's royal dukes, the "Dauphin" in France, the Count of Flanders in Belgium, and the Count of Syracuse in Sicily. Sometimes apanage titles were princely, e.g. Prince of Achaia (Courtenay), "prince de Condé" (Bourbon), Prince of Carignan (Savoy), but it was the fact that their owners were of princely "rank" rather than that they held a princely "title" which ensured their prominence.
For the often specific terminology concerning an heir apparent, see Crown Prince.
Prince as a substantive title.
Other princes derive their title not from dynastic membership as such, but from inheritance of a title named for a specific and historical territory, although the family's possession of prerogatives or properties in that territory may be long past. Such are most of the "princedoms" of France's "ancien régime" so resented for their pretentiousness by St-Simon. These include the princedoms of Arches-Charleville, Boisbelle-Henrichemont, Chalais, Château-Regnault, Guéméné, Martigues, Mercœur, Sedan, Talmond, Tingrey, and the "kingship" of Yvetot, among others.
Prince as a reigning monarch.
A prince or princess who is the head of state of a territory that has a monarchy as a form of government is a reigning prince.
Nominal principalities.
The current princely monarchies include:
Micronations.
In the same tradition some self-proclaimed monarchs of so-called micronations style themselves as princes:
Princes as representants of a reigning monarch.
Various monarchies provide for different modes in which princes of the dynasty can temporarily or permanently share in the style and / or office of the Monarch, e.g. as Regent or Viceroy.
Though these offices must not be reserved for members of the ruling dynasty, in some traditions they are, possibly even reflected in the style of the office, e.g. prince-lieutenant in Luxembourg repeatedly filled by the Crown prince before the grand duke's abdication, or in form of consortium imperii.
Some monarchies even have a practice in which the Monarch can formally abdicate in favor of his heir, and yet retain a kingly title with executive power, e.g. "Maha Upayuvaraja" (Sanskrit for "Great Joint King" in Cambodia), though sometimes also conferred on powerful regents who exercised executive powers.
Non-dynastic princes.
France and the Holy Roman Empire
In several countries of the European continent, e.g. in France, prince can be an aristocratic title of someone having a high rank of nobility as a chief of a geographical place, but no actual territory and without any necessary link to the royal family, which makes comparing it with e.g. the British system of royal princes difficult.
The kings of France started to bestow the style of prince, as a title among the nobility, from the 16th century onwards. These titles were created by elevating a "seigneurie" to the nominal status of a principality—although prerogatives of sovereignty were never conceded in the letters patent. These titles held no official place in the hierarchy of the nobility, but were often treated as ranking just below dukedoms, since they were often inherited (or assumed) by ducal heirs:
This can even occur in a monarchy within which an identical but real and substantive feudal title exists, such as Fürst in German. An example of this is:
Spain, France and Netherlands
In other cases, such titular princedoms are created in chief of an event, such as a treaty of a victory. An example of this is:
Poland and Russia
In Poland specifically, the titles of prince dated either to the times before the Union of Lublin or were granted to Polish nobles by foreign kings, as the law in Poland forbade the king from dividing nobility by granting them hereditary titles. For more information, see The Princely Houses of Poland.
In the Russian system, "knyaz", translated as "prince", is the highest degree of official nobility. Members of older dynasties that were eventually subjected to the Russian imperial dynasty were also accorded the title of "knyaz"—sometimes after first being allowed to use the higher title of tsarevich (e.g. the Princes Gruzinsky and Sibirsky. Rurikid branches used the "knyaz" title also after they were succeeded by the Romanovs as the Russian imperial dynasty. An example of this is:
The title of prince in various Western traditions and languages.
In each case, the title is followed (when available) by the female form and then (not always available, and obviously rarely applicable to a prince of the blood without a principality) the name of the territorial associated with it, each separated by a slash. If a second title (or set) is also given, then that one is for a Prince of the blood, the first for a principality. Be aware that the absence of a separate title for a prince of the blood may not always mean no such title exists; alternatively, the existence of a word does not imply there is also a reality in the linguistic territory concerned; it may very well be used exclusively to render titles in other languages, regardless whether there is a historical link with any (which often means that linguistic tradition is adopted)
Etymologically, we can discern the following traditions (some languages followed a historical link, e.g. within the Holy Roman Empire, not their language family; some even fail to follow the same logic for certain other aristocratic titles):
The title of prince in other traditions and languages.
In Belgium, France, Italy, Japan, Portugal, Russia, Spain and Hungary the title of "prince" has also been used as the highest title of nobility (without being royal), above the title of "duke", while the same usage (then as "fürst") has occurred in Germany and Austria but then one rank below the title of "duke" and above "count".
The above is essentially the story of European, Christian dynasties and other nobility, also 'exported' to their colonial and other overseas territories and otherwise adopted by rather westernized societies elsewhere (e.g. Haiti).
Applying these essentially western concepts, and terminology, to other cultures even when they don't do so, is common but in many respects rather dubious. Different (historical, religious...) backgrounds have also begot significantly different dynastic and nobiliary systems, which are poorly represented by the 'closest' western analogy.
It therefore makes sense to treat these per civilization.
Non-Islamic Asian traditions.
China.
In ancient China, the title of prince developed from being the highest title of nobility (synonymous with duke) in the Zhou Dynasty, to five grades of princes (not counting the sons and grandsons of the emperor) by the time of the fall of the Qing Dynasty.The Chinese word for prince "Wang" (王, literally, King) as Chinese believe the emperor "Huang Di" (皇帝) is the ruler of all kings. The most accurate translations of the English word "prince" are "Huang Zi" (皇子, lit. Son of the Emperor) or "Wang Zi" (王子, lit. Son of the King).
Japan.
In Japan, the title "Kōshaku" (公爵) was used as the highest title of "Kazoku" (華族 Japanese modern nobility) before the present constitution. "Kōshaku", however, is more commonly translated as "Duke" to avoid confusion with the following royal ranks in the Imperial Household: "Shinnō" (親王 literally, King of the Blood); female "Naishinnō" (内親王 lit., Queen (by herself) of the Blood); and "Shinnōhi" 親王妃 lit., Consort of King of the Blood); or "Ō" (王 lit., King); female, "Jyo-Ō" (女王 lit., Queen (by herself)); and "Ōhi" (王妃 lit., Consort of King). The former is the higher title of a male member of the Imperial family while the latter is the lower.
Korea.
In Joseon Dynasty, the title "Prince" was used for the king's male-line descendant. Prince translated generally into three divisions. The king's legitimate son used title "daegun" (대군, 大君, literally Grand Prince). A son born of a concubine and king's great-great-grand son used title "gun" (군, 君, lit. Prince). But the title of "gun" wasn't limited to royal family. Instead, it was often granted as an honorary and non-hereditory title.
Presently, as noble titles are no more granted or even recognized by the people, the English word "Prince" is usually translated as "wangja" (왕자, 王子, lit. king's son), only rendering the usage in the British Royal Family. Princes and principalities in continental Europe are almost always confused with dukes and duchies, both being translated as "gong" (공, 公, lit. duke) and "gongguk" (공국, 公國, lit. duchy).
Sri Lanka.
The title 'Prince' was used for the King's son in Sinhalese generation in Sri Lanka.
India.
"See" princely states for the often particular, mainly Hindu titles in former British India, including modern Pakistan, Bangladesh, Burma, and Nepal.
Indochina.
"See" Cambodia, Vietnam, and Laos
Philippines.
"See" Principalia, the sultanates of Maguindanao and Sulu.
Thailand.
In Thailand (formerly Siam), the title of Prince was divided into three classes depending on the rank of their mothers. Those who were born of a King and had a royal mother (a Queen or a Princess consort) are titled "Chaofa Chai" (Thai: เจ้าฟ้าชาย: literally, "Male Celestial Lord"). Those born of a King and a commoner or children of Chaofas are tilted "Phra Ong Chao" (พระองค์เจ้า). The children of Phra Ong Chaos are titled "Mom Chao" (หม่อมเจ้า), abbreviated as M.C. (or ม.จ.).
African traditions.
A Western model was sometimes copied by emancipated colonial regimes (e.g. Bokassa I's short-lived Central-African Empire in Napoleonic fashion). Otherwise, most of the styles for members of ruling families do not lend themselves well to English translation. Nonetheless, in general the princely style has gradually replaced the colonialist title of chief, which does not particularly connote dynastic rank to Westerners, e.g. Swazi Royal Family and Zulu Royal Family. Due to this, the nominally ministerial chiefly titles that still exist (e.g.: the Yoruba "Oloye") are usually viewed as little more than the equivalents of the British knighthood, of little dynastic consequence except as a means of passively honouring the supporters of a monarch who is himself probably more contemporary in his styling.
The title of prince in religion.
In states with an element of theocracy, this can affect princehood in several ways, such as the style of the ruler (e.g. with a secondary title meaning son or servant of a named divinity), but also the mode of succession (even reincarnation and recognition).
Furthermore, certain religious offices may be considered of princely rank, and/or imply comparable temporal rights.
The Prince-Popes, Pope, Hereditary Prince-Cardinals, Cardinals,Prince-Lord Bishops, Prince Bishops, Lord Bishops, Prince-Provost, and Prince-abbots are referred to as Princes of the Church. Also in Christianity, Jesus Christ is sometimes referred to as the "Prince of Peace". Other titles for Jesus Christ are "Prince of Princes", "Prince of the Covenant", and "Prince of the Kings of the Earth". Further, Satan is often titled the "Prince of Darkness"; and in the Christian faith he is also referred to as the "Prince of this World" and the "Prince of the Power of the Air". Another title for Satan, not as common today but apparently so in approximately 30 A.D. by the Pharisees of the day, was the title "Prince of the Devils". "Prince of Israel", "Prince of the Angels", and "Prince of Light" are titles given to the Archangel Michael. Some Christian churches also believe that since all Christians, like Jesus Christ, are children of God, then they too are princes and princesses of Heaven. Saint Peter, a disciple of Jesus, is also known as the Prince of the Apostles.

</doc>
<doc id="51512" url="http://en.wikipedia.org/wiki?curid=51512" title="Etiquette">
Etiquette

 
Etiquette ( or , ]) is a code of behavior that delineates expectations for social behavior according to contemporary conventional norms within a society, social class, or group.
The French word "étiquette", literally signifying a tag or label, was used in a modern sense in English around 1750. From the 1500s through the early 1900s, children were taught etiquette at school. Etiquette has changed and evolved over the years.
History.
In the 3rd millennium BC, Ptahhotep wrote "The Maxims of Ptahhotep". The Maxims were conformist precepts extolling such civil virtues as truthfulness, self-control and kindness towards one's fellow beings. Learning by listening to everybody and knowing that human knowledge is never perfect are a leitmotif. Avoiding open conflict wherever possible should not be considered weakness. Stress is placed on the pursuit of justice, although it is conceded that it is a god's command that prevails in the end. Some of the maxims refer to one's behaviour when in the presence of the great, how to choose the right master and how to serve him. Others teach the correct way to lead through openness and kindness. Greed is the base of all evil and should be guarded against, while generosity towards family and friends is deemed praiseworthy.
Confucius (551–479 BC) was a Chinese teacher, editor, politician, and philosopher whose philosophy emphasized personal and governmental morality, correctness of social relationships, justice and sincerity.
Louis XIV (1638-1718) "transformed a royal hunting lodge in Versailles, a village 25 miles southwest of the capital, into one of the largest palaces in the world, officially moving his court and government there in 1682. It was against this awe-inspiring backdrop that Louis tamed the nobility and impressed foreign dignitaries, using entertainment, ceremony and a highly codified system of etiquette to assert his supremacy.” 
Politeness.
During the Enlightenment era, a self-conscious process of the imposition of polite norms and behaviours became a symbol of being a genteel member of the upper class. Upwardly mobile middle class bourgeoisie increasingly tried to identify themselves with the elite through their adopted artistic preferences and their standards of behaviour. They became preoccupied with precise rules of etiquette, such as when to show emotion, the art of elegant dress and graceful conversation and how to act courteously, especially with women. Influential in this new discourse was a series of essays on the nature of politeness in a commercial society, penned by the philosopher Lord Shaftesbury in the early 18th century. Shaftesbury defined politeness as the art of being pleasing in company:
Periodicals, such as "The Spectator", founded as a daily publication by Joseph Addison and Richard Steele in 1711, gave regular advice to its readers on how to conform to the etiquette required of a polite gentleman. Its stated goal was "to enliven morality with wit, and to temper wit with morality...to bring philosophy out of the closets and libraries, schools and colleges, to dwell in clubs and assemblies, at tea-tables and coffeehouses" It provided its readers with educated, topical talking points, and advice in how to carry on conversations and social interactions in a polite manner.
The allied notion of 'civility' - referring to a desired social interaction which valued sober and reasoned debate on matters of interest - also became an important quality for the 'polite classes'. Established rules and procedures for proper behaviour as well as etiquette conventions, were outlined by gentleman's clubs, such as Harrington's Rota Club. Periodicals, including "The Tatler" and "The Spectator", infused politeness into English coffeehouse conversation, as their explicit purpose lay in the reformation of English manners and morals.
It was Philip Stanhope, 4th Earl of Chesterfield who first used the word 'etiquette' in its modern meaning, in his "Letters to His Son on the Art of Becoming a Man of the World and a Gentleman". This work comprised over 400 letters written from 1737 or 1738 and continuing until his son's death in 1768, and were mostly instructive letters on various subjects. The letters were first published by his son's widow Eugenia Stanhope in 1774. Chesterfield endeavoured to decouple the issue of manners from conventional morality, arguing that mastery of etiquette was an important weapon for social advancement. The Letters were full of elegant wisdom and perceptive observation and deduction. Chesterfield epitomised the restraint of polite 18th-century society, writing, for instance, in 1748:
"I would heartily wish that you may often be seen to smile, but never heard to laugh while you live. Frequent and loud laughter is the characteristic of folly and ill-manners; it is the manner in which the mob express their silly joy at silly things; and they call it being merry. In my mind there is nothing so illiberal, and so ill-bred, as audible laughter. I am neither of a melancholy nor a cynical disposition, and am as willing and as apt to be pleased as anybody; but I am sure that since I have had the full use of my reason nobody has ever heard me laugh."
By the Victorian era, etiquette had developed into an exceptionally complicated system of rules, governing everything from the proper method for writing letters and using cutlery to the minutely regulated interactions between different classes and gender.
Manners.
Manners is a term usually preceded by the word good or bad to indicate whether or not a behavior is socially acceptable. Every culture adheres to a different set of manners, although a lot of manners are cross‐culturally common. Manners are a subset of social norms which are informally enforced through self-regulation and social policing and publically performed. They enable human ‘ultrasociality’ by imposing self-restraint and compromise on regular, everyday actions.
Sociology perspectives.
In his book The Civilizing Process, Norbert Elias argued that manners arose as a product of group living and persist as a way of maintaining social order. He theorized that manners proliferated during the Renaissance in response to the development of the ‘absolute state’ – the progression from small group living to the centralization of power by the state. Elias believed that the rituals associated with manners in the Court Society of England during this period were closely bound with social status. To him, manners demonstrate an individual’s position within a social network and act as a means by which the individual can negotiate that position.
Petersen and Lupton argue that manners helped reduce the boundaries between the public sphere and the private sphere and gave rise to "“a highly reflective self, a self who monitors his or her behavior with due regard for others with whom he or she interacts socially.”" They explain that that; "“The public behavior of "individuals came to signify their social standing, a means of presenting the self and of evaluating others and thus the control of the outward self was"
"vital.”" From this perspective, manners are seen not just as a means of displaying one’s social status, but also as a
means of maintaining social
boundaries around class and identity.
Pierre Bourdieu’s notion of "‘habitus’" can also contribute to the understanding of manners. The habitus, he explains, is a set of "‘dispositions’" that are
neither self‐determined, nor pre‐determined, by external environmental factors. They tend to operate at a subconscious level and are "“inculcated through experience and explicit teaching”" and produced and reproduced by social interactions. Manners, in this view, are likely to be a central part of the "‘dispositions’" which guide an individual’s ability to make socially compliant behavioral decisions.
Anthropology perspectives.
Anthropologists concern themselves primarily with detailing cultural variances and differences in "‘ways of seeing’." Theorists such as Mary Douglas have
claimed that each culture’s unique set of manners, behaviors and rituals enable the local cosmology to remain ordered and free from those things that may pollute or
defile it. In particular, she suggests that ideas of pollution and disgust are attached to the margins of socially acceptable behavior to curtail such actions and
maintain"“ the assumptions by which experience is controlled.”"
Evolutionary biology perspectives.
Evolutionary biology looks at the origin of behavior and the motivation behind it. Charles Darwin analyzed the remarkable universality of facial responses to disgust, shame and other complex emotions. Having identified the same behavior in young infants and blind individuals he concluded that these responses are not learned but innate. According to Val Curtis, the development of these responses was concomitant with the development of manners behavior. For Curtis, manners play an evolutionary role in the prevention of disease. This assumes that those who were hygienic, polite to others and most able to benefit from their membership within a cultural group, stand the best
chance of survival and reproduction.
Catherine Cottrell and Steven Neuberg explore how our behavioral responses to ‘otherness’ may enable the preservation of manners and norms. They suggest that the foreignness or unfamiliarity we experience when interacting with different cultural groups for the first time, may partly serve an evolutionary function: "“Group living surrounds one with individuals able to physically harm fellow group members, to spread contagious disease, or to “free ride” on their efforts. A commitment to sociality thus carries a risk: If threats such as these are left unchecked, the costs of sociality will quickly exceed its benefits. Thus, to maximize the returns on group "living, individual group members should be attuned to others’ features or behaviors.”"
Thus, people who possess similar traits, common to the group, are to be trusted, whereas those who do not are to be considered as ‘others’ and treated with suspicion or even exclusion. Curtis argues that selective pressure borne out of a shift towards communal living would have resulted in individuals being shunned from the group for hygiene lapses or uncooperative behavior. This would have led to people avoiding actions that might result in embarrassment or others being disgusted. Joseph Henrich and Robert Boyd developed a
model to demonstrate this process at work. They explain natural selection has favored the acquisition of genetically transmitted learning mechanisms that increase an individual’s chance of acquiring locally adaptive behavior. They hypothesize that: "“Humans possess a reliably developing neural encoding that compels them both to punish individuals who violate group norms (common beliefs or practices) and punish individuals who do not punish norm violators.”" From this approach, manners are a means of mitigating undesirable behavior and fostering the benefits of in‐group cooperation.
Types.
Curtis’ also specifically outlines three manner categories; hygiene, courtesy and cultural norms, each of which help to account for the multifaceted role manners play in society.
These categories are based on the outcome rather than the motivation of manners behavior and individual manner behaviors may fit in to 2 or more categories.
Hygiene Manners – are any manners which affect disease transmission. They are likely to be taught at an early age, primarily through parental discipline, positive behavioral
enforcement around continence with bodily fluids (such as toilet training), and the avoidance or removal of items that pose a disease risk for children. It is expected that,
by adulthood, hygiene manners are so entrenched in one’s behavior that they become second nature. Violations are likely to elicit disgust responses.
Courtesy Manners – demonstrate one’s ability to put the interests of others before oneself; to display self‐control and good intent for the purposes of being trusted
in social interactions. Courtesy manners help to maximize the benefits of group living by regulating social interaction. Disease avoidance behavior can sometimes be compromised in the performance of courtesy manners. They may be taught in the same way as hygiene manners but are likely to also be learned through direct,
indirect (i.e. observing the interactions of others) or imagined (i.e. through the executive functions of the brain) social interactions. The learning of courtesy manners may take place
at an older age than hygiene manners, because individuals must have at least some means of communication and some awareness of self and social positioning.
The violation of courtesy manners most commonly results in social disapproval from peers.
Cultural Norm Manners – typically demonstrate one’s identity within a specific socio‐cultural group. Adherence to cultural norm manners allows for the demarcation of socio‐cultural identities and the creation of boundaries which inform who is to be trusted or who is to be deemed as ‘other’. Cultural norm manners are learnt through the enculturation and routinisation of ‘the familiar’ and through exposure to ‘otherness’ or those who are identified as foreign or different. Transgressions and non‐adherence to cultural norm manners commonly result in alienation. Cultural norms, by their very nature, have a high level of between‐group variability but are likely to be common to all those who identify with a given group identity.
Rules of etiquette encompass most aspects of social interaction in any society, though the term itself is not commonly used. A rule of etiquette may reflect an underlying ethical code, or it may reflect a person's fashion or status. Rules of etiquette are usually unwritten, but aspects of etiquette have been codified from time to time.
Books.
Erasmus of Rotterdam published his book "On Good Manners for Boys" in 1530. Amid his advice for young children on fidgeting, yawning, bickering and scratching he highlights that a core tenet of manners is the ability to “"readily ignore the faults of others but avoid falling short yourself"”.
In centuries since then, many authors have tried to collate manners or etiquette guide books. One of the most famous of these was Emily Post who began to document etiquette in 1922. She described her work as detailing the “"trivialities"” of desirable everyday conduct but also provided descriptions of appropriate conduct for key life events such as baptisms, weddings and funerals. She later established an institute which continues to provide updated advice on how to negotiate modern day society with good manners and decorum. The most recent edition of her book provides advice on such topics as when it is acceptable to ‘unfriend’ someone on Facebook and who is entitled to which armrest when flying. Etiquette books such as these as well as those by Amy Vanderbilt, Hartley, Judith Martin, and Sandi Toksvig outline suggested behaviours for a range of social interactions. However, all note that to be a well-mannered person one must not merely read their books but be able to employ good manners fluidly in any situation that may arise.
Western office and business.
The etiquette of business is the set of written and unwritten rules of conduct that make social interactions run more smoothly. Office etiquette in particular applies to coworker interaction, excluding interactions with external contacts such as customers and suppliers. When conducting group meetings in the United States, the assembly might follow "Robert's Rules of Order", if there are no other company policies to control a meeting.
These rules are often echoed throughout an industry or economy. For instance, 49% of employers surveyed in 2005 by the American National Association of Colleges and Employers found that non-traditional attire would be a "strong influence" on their opinion of a potential job candidate.
Both office and business etiquette overlap considerably with basic tenets of netiquette, the social conventions for using computer networks.
Business etiquette can vary significantly in different countries, which is invariably related to their culture. For example: A notable difference between Chinese and Western business etiquette is conflict handling. Chinese businesses prefer to look upon relationship management to avoid conflicts - stemmed from a culture that heavily relies on Guanxi. While the west leaves resolution of conflict to the interpretations of law through contracts and lawyers.
Adjusting to foreign etiquettes is a major complement of culture shock, providing a market for manuals. Other resources include business and diplomacy institutions, available only in certain countries such as the UK.
In 2011, a group of etiquette experts and international business group formed a non-profit organization called IITTI (pronounced as "ET") to help human resource (HR) departments of multinationals in measuring the etiquette skills of prospective new employees during the recruitment process by standardizing image and etiquette examination, similar to what ISO does for industrial process measurements.
Etiquette in retail is sometimes summarized as "The customer is always right." 
International.
Europe.
European etiquette is not uniform. Even within the regions of Europe, etiquette may not be uniform: within a single country there may be differences in customs, especially where there are different linguistic groups, as in Switzerland where there are French, German, and Italian speakers.
Japan.
The Japanese are very formal. Moments of silence are far from awkward. Smiling does not always mean that the individual is expressing pleasure. Business cards are to be handed out formally following this procedure: Hand card with writing facing upwards; bow when giving and receiving the card; grasp it with both hands; read it carefully; and put it in a prominent place. The Japanese feel a “Giri,” an obligation to reciprocate a gesture of kindness. They also rely on an innate sense of right and wrong.
Kenya.
Kenyans believe that their tribal identity is very important. Kenyans are also very nationalistic. Kenyans rarely prefer to be alone, and are usually very friendly and welcoming of guests. Kenyans are very family-oriented.
Cultural differences.
Etiquette is dependent on culture; what is excellent etiquette in one society may shock another. Etiquette evolves within culture. The Dutch painter Andries Both shows that the hunt for head lice ("illustration, right"), which had been a civilized grooming occupation in the early Middle Ages, a bonding experience that reinforced the comparative rank of two people, one groomed the other, one was the subject of the groomer, had become a peasant occupation by 1630. The painter portrays the familiar operation matter-of-factly, without the disdain this subject would have received in a 19th-century representation.
Etiquette can vary widely between different cultures and nations. For example, in Hausa culture, eating while standing may be seen as offensively casual and ill-omened behavior, insulting the host and showing a lack of respect for the scarcity of food—the offense is known as "eating with the devil" or "committing "santi"." In China, a person who takes the last item of food from a common plate or bowl without first offering it to others at the table may be seen as a glutton who is insulting the host's generosity. Traditionally, if guests do not have leftover food in front of them at the end of a meal, it is to the dishonour of the host. In the United States of America, a guest is expected to eat all of the food given to them, as a compliment to the quality of the cooking. However, it is still considered polite to offer food from a common plate or bowl to others at the table.
In such rigid hierarchal cultures as Korea and Japan, alcohol helps to break down the strict social barrier between classes. It allows for a hint of informality to creep in. It is traditional for host and guest to take turns filling each other's cups and encouraging each other to gulp it down. For someone who does not consume alcohol (except for religious reasons), it can be difficult escaping the ritual of the social drink.
Etiquette is a topic that has occupied writers and thinkers in all sophisticated societies for millennia, beginning with a behavior code by Ptahhotep, a vizier in ancient Egypt's Old Kingdom during the reign of the Fifth Dynasty king Djedkare Isesi (ca. 2414–2375 BC). All known literate civilizations, including ancient Greece and Rome, developed rules for proper social conduct. Confucius included rules for eating and speaking along with his more philosophical sayings.
Early modern conceptions of what behavior identifies a "gentleman" were codified in the 16th century, in a book by Baldassare Castiglione, "Il Cortegiano" ("The Courtier"); its codification of expectations at the court of Urbino remained in force in its essentials until World War I. Louis XIV established an elaborate and rigid court ceremony, but distinguished himself from the high bourgeoisie by continuing to eat, stylishly and fastidiously, with his fingers. An important book about etiquette is "Il Galateo" by Giovanni della Casa; in fact, in Italian, etiquette is generally called "galateo" (or "etichetta" or "protocollo").
In the American colonies, Benjamin Franklin and George Washington wrote codes of conduct for young gentlemen. The immense popularity of advice columns and books by Letitia Baldrige and Miss Manners shows the currency of this topic. Even more recently, the rise of the Internet has necessitated the adaptation of existing rules of conduct to create Netiquette, which governs the drafting of e-mail, rules for participating in an online forum, and so on.
In Germany, many books dealing with etiquette, especially dining, dressing etc., are called "the Knigge", named after Adolph Freiherr Knigge who wrote the book "Über den Umgang mit Menschen" ("On Human Relations") in the late 18th century. However, this book is about good manners and also about the social state of its time, but not about etiquette.
Etiquette may be wielded as a social weapon. The outward adoption of the superficial mannerisms of an in-group, in the interests of social advancement rather than a concern for others, is considered by many a form of snobbery, lacking in virtue.

</doc>
<doc id="51513" url="http://en.wikipedia.org/wiki?curid=51513" title="Arrow">
Arrow

An arrow is a shafted projectile that is shot with a bow. It predates recorded history and is common to most cultures.
An arrow usually consists of a shaft with an arrowhead attached to the front end, with fletchings and a nock at the other.
History.
The oldest evidence of stone-tipped projectiles, which may or may not have been propelled by a bow (c.f. atlatl), dating to c. 64,000 years ago, were found in Sibudu Cave, South Africa.
The oldest evidence of the use of bows to shoot arrows dates to about 10,000 years ago; it is based on pinewood arrows found in the Ahrensburg valley north of Hamburg. They had shallow grooves on the base, indicating that they were shot from a bow. The oldest bow so far recovered is about 8,000 years old, found in the Holmegård swamp in Denmark.
Archery seems to have arrived in the Americas with the Arctic small tool tradition, about 4,500 years ago.
Size.
Arrow sizes vary greatly across cultures, ranging from eighteen inches to five feet (45 cm to 150 cm). However, most modern arrows are 75 cm to 96 cm; most war arrows from an English ship sunk in 1545 were 76 cm. Very short arrows have been used, shot through a guide attached either to the bow (an "overdraw") or to the archer's wrist (the Turkish "siper"). These may fly farther than heavier arrows, and an enemy without suitable equipment may find himself unable to return them.
Shaft.
The shaft is the primary structural element of the arrow, to which the other components are attached. Traditional arrow shafts are made from lightweight wood, bamboo or reeds, while modern shafts may be made from aluminium, carbon fibre reinforced plastic, or a combination of materials. Such shafts are typically made from an aluminium core wrapped with a carbon fibre outer.
The stiffness of the shaft is known as its spine, referring to how little the shaft bends when compressed. Hence, an arrow which bends less is said to have more spine. In order to strike consistently, a group of arrows must be similarly spined. "Center-shot" bows, in which the arrow passes through the central vertical axis of the bow riser, may obtain consistent results from arrows with a wide range of spines. However, most traditional bows are not center-shot and the arrow has to deflect around the handle in the archer's paradox; such bows tend to give most consistent results with a narrower range of arrow spine that allows the arrow to deflect correctly around the bow. Higher draw-weight bows will generally require stiffer arrows, with more spine (less flexibility) to give the correct amount of flex when shot.
GPI rating.
The weight of an arrow shaft can be expressed in GPI (Grains Per Inch). The length of a shaft in inches multiplied by its GPI rating gives the weight of the shaft in grains. For example, a shaft that is 30 inches long and has a GPI of 9.5 weighs 285 grains, or about 18 grams. This does not include the other elements of a finished arrow, so a complete arrow will be heavier than the shaft alone.
Footed arrows.
Sometimes a shaft will be made of two different types of wood fastened together, resulting in what is known as a footed arrow. Known by some as the finest of wood arrows, footed arrows were used both by early Europeans and Native Americans. Footed arrows will typically consist of a short length of hardwood near the head of the arrow, with the remainder of the shaft consisting of softwood. By reinforcing the area most likely to break, the arrow is more likely to survive impact, while maintaining overall flexibility and lighter weight.
Arrowhead.
The arrowhead or projectile point is the primary functional part of the arrow, and plays the largest role in determining its purpose. Some arrows may simply use a sharpened tip of the solid shaft, but it is far more common for separate arrowheads to be made, usually from metal, horn, or some other hard material. Arrowheads are usually separated by function:
There are two main types of broadheads used by hunters: The fixed-blade and the mechanical types. While the fixed-blade broadhead keeps its blades rigid and unmovable on the broadhead at all times, the mechanical broadhead deploys its blades upon contact with the target, its blades swinging out to wound the target. The mechanical head flies better because it is more streamlined, but has less penetration as it uses some of the kinetic energy in the arrow to deploy its blades.
Arrowheads may be attached to the shaft with a cap, a socketed tang, or inserted into a split in the shaft and held by a process called hafting. Points attached with caps are simply slid snugly over the end of the shaft, or may be held on with hot glue. Split-shaft construction involves splitting the arrow shaft lengthwise, inserting the arrowhead, and securing it using a ferrule, sinew, or wire.
Fletchings.
Fletchings are found at the back of the arrow and act as airfoils to provide a small amount of force used to stabilize the flight of the arrow. They are designed to keep the arrow pointed in the direction of travel by strongly damping down any tendency to pitch or yaw. Some cultures, for example most in New Guinea, did not use fletching on their arrows.
Fletchings are traditionally made from feathers (often from a goose or turkey) bound to the arrow's shaft, but are now often made of plastic (known as "vanes"). Historically, some arrows used for the proofing of armour used copper vanes. Flight archers may use razor blades for fletching, in order to reduce air resistance. With conventional three-feather fletching, one feather, called the "cock" feather, is at a right angle to the nock, and is normally nocked so that it will not contact the bow when the arrow is shot. Four-feather fletching is usually symmetrical and there is no preferred orientation for the nock; this makes nocking the arrow slightly easier.
Artisans who make arrows by hand are known as "fletchers," a word related to the French word for arrow, "flèche". This is the same derivation as the verb "fletch", meaning to provide an arrow with its feathers. Glue and/or thread are the main traditional methods of attaching fletchings. A "fletching jig" is often used in modern times, to hold the fletchings in exactly the right orientation on the shaft while the glue hardens.
Whenever natural fletching is used, the feathers on any one arrow must come from the same side of the bird. The slight twist in natural feathers then makes the arrow rotate in flight, which increases accuracy. Artificial "helical" fletchings have the same effect. Most arrows will have three fletches, but some have four or even more. Fletchings generally range from two to six inches (152 mm) in length; flight arrows intended to travel the maximum possible distance typically have very low fletching, while hunting arrows with broadheads require long and high fletching to stabilize them against the aerodynamic effect of the head. Fletchings may also be cut in different ways, the two most common being "parabolic" (i.e. a smooth curved shape) and "shield" (i.e. shaped as one-half of a very narrow shield) cut.
A flu-flu is a form of fletching, normally made by using long sections of full length feathers taken from a turkey, in most cases six or more sections are used rather than the traditional three. Alternatively two long feathers can be spiraled around the end of the arrow shaft. The extra fletching generates more drag and slows the arrow down rapidly after a short distance, about 30 m or so .
Flu-Flu arrows are often used for hunting birds, or for children's archery, and can also be used to play Flu-Flu Golf.
Nocks.
The nock is a notch in the rearmost end of the arrow. It serves to keep the arrow in place on the string as the bow is being drawn. Nocks may be simple slots cut in the back of the arrow, or separate pieces made from wood, plastic, or horn that are then attached to the end of the arrow.
Modern nocks, and traditional Turkish nocks, are often constructed so as to curve around the string or even pinch it slightly, so that the arrow is unlikely to slip off. In English it is common to say "nock an arrow" when one readies a shot.
In Arab archery, there was the description of the use of "nockless arrows". In shooting at the enemies, either the Turks or the Persians, it was seen that the enemies would pick up the expended arrows and shoot them back at the Arabs. So they developed "nockless arrows", which would be useless to their foes. The bowstring would have a small ring that is tied onto the string at the proper point where the nock would normally be placed. The end of the arrow, rather than being slit for a nock, would be sharpened like an arrowhead, then the rear end of the arrow would be slipped into this ring and drawn and released as usual. Then the enemy could collect all the arrows they wanted, but they would be useless to them in shooting back. A piece of advice was in battle, to have several rings tied onto the bowstring in case one broke.

</doc>
<doc id="51518" url="http://en.wikipedia.org/wiki?curid=51518" title="Dam">
Dam

A dam is a barrier that impounds water or underground streams. The reservoirs created by dams not only suppress floods but provide water for various needs to include irrigation, human consumption, industrial use, aquaculture and navigability. Hydropower is often used in conjunction with dams to generate electricity. A dam can also be used to collect water or for storage of water which can be evenly distributed between locations. Dams generally serve the primary purpose of retaining water, while other structures such as floodgates or levees (also known as dikes) are used to manage or prevent water flow into specific land regions.
The word "dam" can be traced back to Middle English, and before that, from Middle Dutch, as seen in the names of many old cities.
History.
Ancient dams.
Early dam building took place in Mesopotamia and the Middle East. Dams were used to control the water level, for Mesopotamia's weather affected the Tigris and Euphrates rivers, and could be quite unpredictable.
The earliest known dam is the Jawa Dam in Jordan, 100 km northeast of the capital Amman. This gravity dam featured an originally 9 m high and 1 m wide stone wall, supported by a 50 m wide earth rampart. The structure is dated to 3000 BC.
The Ancient Egyptian Sadd-el-Kafara Dam at Wadi Al-Garawi, located about 25 km south of Cairo, was 102 m long at its base and 87 m wide. The structure was built around 2800 or 2600 BC. as a diversion dam for flood control, but was destroyed by heavy rain during construction or shortly afterwards. During the XIIth dynasty in the 19th century BC, the Pharaohs Senosert III, Amenemhat III and Amenmehat IV dug a canal 16 km long linking the Fayum Depression to the Nile in Middle Egypt. Two dams called Ha-Uar running east-west were built to retain water during the annual flood and then release it to surrounding lands. The lake called "Mer-wer" or Lake Moeris covered 1700 square kilometers and is known today as Berkat Qaroun.
One of the engineering wonders of the ancient world was the Great Dam of Marib in Yemen. Initiated somewhere between 1750 and 1700 BC, it was made of packed earth - triangular in cross section, 580 m in length and originally 4 meters high - running between two groups of rocks on either side, to which it was linked by substantial stonework. Repairs were carried out during various periods, most important around 750 BC, and 250 years later the dam height was increased to 7 meters. After the end of the Kingdom of Saba, the dam fell under the control of the Ḥimyarites (~115 BC) who undertook further improvements, creating a structure 14 meters high, with five spillway channels, two masonry-reinforced sluices, a settling pond, and a 1000 meter canal to a distribution tank. These extensive works were not actually finalized until 325 AD and allowed the irrigation of 25,000 acres (100 km²).
By the mid-late 3rd century BC, an intricate water-management system within Dholavira in modern day India, was built. The system included 16 reservoirs, dams and various channels for collecting water and storing it.
Eflatun Pinar is a Hittite dam and spring temple near Konya, Turkey. It is thought to be from the time of the Hittite empire between the 15th and 13th century BC.
The Kallanai is constructed of unhewn stone, over 300 m long, 4.5 m high and 20 m wide, across the main stream of the Kaveri river in Tamil Nadu, South India. The basic structure dates to the 2nd century AD and is considered one of the oldest water-diversion or water-regulator structures in the world, which is still in use. The purpose of the dam was to divert the waters of the Kaveri across the fertile Delta region for irrigation via canals.
Du Jiang Yan is the oldest surviving irrigation system in China that included a dam that directed waterflow. It was finished in 251 BC. A large earthen dam, made by the Prime Minister of Chu (state), Sunshu Ao, flooded a valley in modern-day northern Anhui province that created an enormous irrigation reservoir (62 mi in circumference), a reservoir that is still present today.
Roman engineering.
Roman dam construction was characterized by "the Romans' ability to plan and organize engineering construction on a grand scale". Roman planners introduced the then novel concept of large reservoir dams which could secure a permanent water supply for urban settlements over the dry season. Their pioneering use of water-proof hydraulic mortar and particularly Roman concrete allowed for much larger dam structures than previously built, such as the Lake Homs Dam, possibly the largest water barrier to that date, and the Harbaqa Dam, both in Roman Syria. The highest Roman dam was the Subiaco Dam near Rome; its record height of 50 m remained unsurpassed until its accidental destruction in 1305.
Roman engineers made routine use of ancient standard designs like embankment dams and masonry gravity dams. Apart from that, they displayed a high degree of inventiveness, introducing most of the other basic dam designs which had been unknown until then. These include arch-gravity dams, arch dams, buttress dams and multiple arch buttress dams, all of which were known and employed by the 2nd century AD (see List of Roman dams). Roman workforces also were the first to build dam bridges, such as the Bridge of Valerian in Iran.
In Iran, bridge dams such as the Band-e Kaisar were used to provide hydropower through water wheels, which often powered water-raising mechanisms. One of the first was the Roman-built dam bridge in Dezful, which could raise water 50 cubits in height for the water supply to all houses in the town. Also diversion dams were known. Milling dams were introduced which the Muslim engineers called the "Pul-i-Bulaiti". The first was built at Shustar on the River Karun, Iran, and many of these were later built in other parts of the Islamic world. Water was conducted from the back of the dam through a large pipe to drive a water wheel and watermill. In the 10th century, Al-Muqaddasi described several dams in Persia. He reported that one in Ahwaz was more than 3000 ft long, and that and it had many water-wheels raising the water into aqueducts through which it flowed into reservoirs of the city. Another one, the "Band-i-Amir" dam, provided irrigation for 300 villages.
Middle Ages.
In the Netherlands, a low-lying country, dams were often applied to block rivers in order to regulate the water level and to prevent the sea from entering the marsh lands. Such dams often marked the beginning of a town or city because it was easy to cross the river at such a place, and often gave rise to the respective place's names in Dutch.
For instance the Dutch capital Amsterdam (old name Amstelredam) started with a dam through the river Amstel in the late 12th century, and Rotterdam started with a dam through the river Rotte, a minor tributary of the Nieuwe Maas. The central square of Amsterdam, covering the original place of the 800 year old dam, still carries the name "Dam Square" or simply "the Dam".
Industrial era.
The Romans were the first to build arch dams, where the reaction forces from the abutment stabilizes the structure from the external hydrostatic pressure, but it was only in the 19th century that the engineering skills and construction materials available were capable of building the first large scale arch dams.
Three pioneering arch dams were built around the British Empire in the early 19th century. Henry Russel of the Royal Engineers oversaw the construction of the Mir Alam dam in 1804 to supply water to the city of Hyderabad (it is still in use today). It had a height of 12 metres and consisted of 21 arches of variable span.
In the 1820s and 30s, Lieutenant-Colonel John By supervised the construction of the Rideau Canal in Canada near modern-day Ottawa and built a series of curved masonry dams as part of the waterway system. In particular, the Jones Falls Dam built by John Redpath, was completed in 1832 as the largest dam in North America and an engineering marvel. In order to keep the water in control during construction, two sluices, artificial channels for conducting water, were kept open in the dam. The first was near the base of the dam on its east side. A second sluice was put in on the west side of the dam, about 20 feet (6 metres) above the base. To make the switch from the lower to upper sluice, the outlet of Sand Lake was blocked off.
Hunts Creek near the City of Parramatta, Australia was dammed in the 1850s, to cater for the demand for water from the growing population of the city. The masonry arch dam wall was designed by Lieutenant Percy Simpson who was influenced by the advances in dam engineering techniques made by the Royal Engineers in India. The dam cost £17,000 and was completed in 1856 as the first engineered dam built in Australia, and the second arch dam in the world built to mathematical specifications.
The first such dam was opened two years earlier in France. It was also the first French arch dam of the industrial era, and it was built by François Zola in the municipality of Aix-en-Provence to improve the supply of water after the 1832 cholera outbreak devastated the area. After royal approval was granted in 1844, the dam was constructed over the following decade. Its construction was carried out on the basis of the mathematical results of scientific stress analysis.
The 75-miles dam near Warwick, Australia was possibly the world's first concrete arch dam. Designed by Henry Charles Stanley in 1880 with an overflow spillway and a special water outlet, it was eventually heightened to 10 meters.
In the latter half of the nineteenth century, significant advances in the scientific theory of masonry dam design were made. This transformed dam design, from an art based on empirical methodology to a profession based on a rigorously applied scientific theoretical framework. This new emphasis was centered around the engineering faculties of universities in France and in the United Kingdom. William John Macquorn Rankine at the University of Glasgow pioneered the theoretical understanding of dam structures in his 1857 paper "On the Stability of Loose Earth". Rankine theory provided a good understanding of the principles behind dam design. In France, J. Augustin Tortene de Sazilly explained the mechanics of vertically faced masonry gravity dams and Zola's dam was the first to be built on the basis of these principles.
Large dams.
The era of large dams was initiated with the construction of the Aswan Low Dam in Egypt in 1902, a gravity masonry buttress dam on the Nile River. Following their 1882 invasion and occupation of Egypt, the British began construction in 1898. The project was designed by Sir William Willcocks and involved several eminent engineers of the time, including Sir Benjamin Baker and Sir John Aird, whose firm, John Aird & Co., was the main contractor. Capital and financing were furnished by Ernest Cassel. When initially constructed between 1899 and 1902, nothing of its scale had ever been attempted; on completion, it was the largest masonry dam in the world.
The Hoover Dam is a massive concrete arch-gravity dam, constructed in the Black Canyon of the Colorado River, on the border between the US states of Arizona and Nevada between 1931 and 1936 during the Great Depression. In 1928, Congress authorized the project to build a dam that would control floods, provide irrigation water and produce hydroelectric power. The winning bid to build the dam was submitted by a consortium called Six Companies, Inc.. Such a large concrete structure had never been built before, and some of the techniques were unproven. The torrid summer weather and the lack of facilities near the site also presented difficulties. Nevertheless, Six Companies turned over the dam to the federal government on 1 March 1936, more than two years ahead of schedule.
By 1997, there were an estimated 800,000 dams worldwide, some 40,000 of them over 15 m high.
Types of dams.
Dams can be formed by human agency, natural causes, or even by the intervention of wildlife such as beavers. Man-made dams are typically classified according to their size (height), intended purpose or structure.
By structure.
Based on structure and material used, dams are classified as easily created without materials, arch-gravity dams, embankment dams or masonry dams, with several subtypes.
Arch dams.
In the arch dam, stability is obtained by a combination of arch and gravity action. If the upstream face is vertical the entire weight of the dam must be carried to the foundation by gravity, while the distribution of the normal hydrostatic pressure between vertical cantilever and arch action will depend upon the stiffness of the dam in a vertical and horizontal direction. When the upstream face is sloped the distribution is more complicated. The normal component of the weight of the arch ring may be taken by the arch action, while the normal hydrostatic pressure will be distributed as described above. For this type of dam, firm reliable supports at the abutments (either buttress or canyon side wall) are more important. The most desirable place for an arch dam is a narrow canyon with steep side walls composed of sound rock.
The safety of an arch dam is dependent on the strength of the side wall abutments, hence not only should the arch be well seated on the side walls but also the character of the rock should be carefully inspected.
Two types of single-arch dams are in use, namely the constant-angle and the constant-radius dam. The constant-radius type employs the same face radius at all elevations of the dam, which means that as the channel grows narrower towards the bottom of the dam the central angle subtended by the face of the dam becomes smaller. Jones Falls Dam, in Canada, is a constant radius dam. In a constant-angle dam, also known as a variable radius dam, this subtended angle is kept a constant and the variation in distance between the abutments at various levels are taken care of by varying the radii. Constant-radius dams are much less common than constant-angle dams. Parker Dam is a constant-angle arch dam.
A similar type is the double-curvature or thin-shell dam. Wildhorse Dam near Mountain City, Nevada in the United States is an example of the type. This method of construction minimizes the amount of concrete necessary for construction but transmits large loads to the foundation and abutments. The appearance is similar to a single-arch dam but with a distinct vertical curvature to it as well lending it the vague appearance of a concave lens as viewed from downstream.
The multiple-arch dam consists of a number of single-arch dams with concrete buttresses as the supporting abutments, as for example the Daniel-Johnson Dam, Québec, Canada. The multiple-arch dam does not require as many buttresses as the hollow gravity type, but requires good rock foundation because the buttress loads are heavy.
Gravity dams.
In a gravity dam, the force that holds the dam in place against the push from the water is Earth's gravity pulling down on the mass of the dam. The water presses laterally (downstream) on the dam, tending to overturn the dam by rotating about its toe (a point at the bottom downstream side of the dam). The dam's weight counteracts that force, tending to rotate the dam the other way about its toe. The designer ensures that the dam is heavy enough that the dam's weight wins that contest. In engineering terms, that is true whenever the resultant of the forces of gravity acting on the dam and water pressure on the dam acts in a line that passes upstream of the toe of the dam.
Furthermore, the designer tries to shape the dam so if one were to consider the part of dam above any particular height to be a whole dam itself, that dam also would be held in place by gravity. i.e. there is no tension in the upstream face of the dam holding the top of the dam down. The designer does this because it is usually more practical to make a dam of material essentially just piled up than to make the material stick together against vertical tension.
Note that the shape that prevents tension in the upstream face also eliminates a balancing compression stress in the downstream face, providing additional economy.
For this type of dam, it is essential to have an impervious foundation with high "bearing" strength.
When situated on a suitable site, a gravity dam can prove to be a better alternative to other types of dams. When built on a carefully studied foundation, the gravity dam probably represents the best developed example of dam building. Since the fear of flood is a strong motivator in many regions, gravity dams are being built in some instances where an arch dam would have been more economical.
Gravity dams are classified as "solid" or "hollow" and are generally made of either concrete or masonry. The solid form is the more widely used of the two, though the hollow dam is frequently more economical to construct. Grand Coulee Dam is a solid gravity dam and Braddock Locks & Dam is a hollow gravity dam.
Arch-gravity dams.
A gravity dam can be combined with an arch dam into an arch-gravity dam for areas with massive amounts of water flow but less material available for a purely gravity dam. The inward compression of the dam by the water reduces the lateral (horizontal) force acting on the dam. Thus, the gravitation force required by the dam is lessened, i.e. the dam does not need to be so massive. This enables thinner dams and saves resources.
Barrages.
A barrage dam is a special kind of dam which consists of a line of large gates that can be opened or closed to control the amount of water passing the dam. The gates are set between flanking piers which are responsible for supporting the water load, and are often used to control and stabilize water flow for irrigation systems.
Barrages that are built at the mouth of rivers or lagoons to prevent tidal incursions or utilize the tidal flow for tidal power are known as tidal barrages.
Embankment dams.
Embankment dams are made from compacted earth, and have two main types, rock-fill and earth-fill dams. Embankment dams rely on their weight to hold back the force of water, like gravity dams made from concrete.
Rock-fill dams.
Rock-fill dams are embankments of compacted free-draining granular earth with an impervious zone. The earth utilized often contains a high percentage of large particles hence the term "rock-fill". The impervious zone may be on the upstream face and made of masonry, concrete, plastic membrane, steel sheet piles, timber or other material. The impervious zone may also be within the embankment in which case it is referred to as a "core". In the instances where clay is utilized as the impervious material the dam is referred to as a "composite" dam. To prevent internal erosion of clay into the rock fill due to seepage forces, the core is separated using a filter. Filters are specifically graded soil designed to prevent the migration of fine grain soil particles. When suitable material is at hand, transportation is minimized leading to cost savings during construction. Rock-fill dams are resistant to damage from earthquakes. However, inadequate quality control during construction can lead to poor compaction and sand in the embankment which can lead to liquefaction of the rock-fill during an earthquake. Liquefaction potential can be reduced by keeping susceptible material from being saturated, and by providing adequate compaction during construction. An example of a rock-fill dam is New Melones Dam in California.
A core that is growing in popularity is asphalt concrete. The majority of such dams are built with rock and/or gravel as the main fill material. Almost 100 dams of this design have now been built worldwide since the first such dam was completed in 1962. All asphalt-concrete core dams built so far have an excellent performance record. The type of asphalt used is a viscoelastic-plasticmaterial that can adjust to the movements and deformations imposed on the embankment as a whole, and to settlements in the foundation. The flexible properties of the asphalt make such dams especially suited in earthquake regions.
Concrete-face rock-fill dams.
A concrete-face rock-fill dam (CFRD) is a rock-fill dam with concrete slabs on its upstream face. This design offers the concrete slab as an impervious wall to prevent leakage and also a structure without concern for uplift pressure. In addition, the CFRD design is flexible for topography, faster to construct and less costly than earth-fill dams. The CFRD originated during the California Gold Rush in the 1860s when miners constructed rock-fill timber-face dams for sluice operations. The timber was later replaced by concrete as the design was applied to irrigation and power schemes. As CFRD designs grew in height during the 1960s, the fill was compacted and the slab's horizontal and vertical joints were replaced with improved vertical joints. In the last few decades, the design has become popular.
Currently, the tallest CFRD in the world is the 233 m tall Shuibuya Dam in China which was completed in 2008.
Earth-fill dams.
Earth-fill dams, also called earthen dams, rolled-earth dams or simply earth dams, are constructed as a simple embankment of well compacted earth. A "homogeneous" rolled-earth dam is entirely constructed of one type of material but may contain a drain layer to collect "seep" water. A "zoned-earth" dam has distinct parts or "zones" of dissimilar material, typically a locally plentiful "shell" with a watertight clay core. Modern zoned-earth embankments employ filter and drain zones to collect and remove seep water and preserve the integrity of the downstream shell zone. An outdated method of zoned earth dam construction utilized a hydraulic fill to produce a watertight core. "Rolled-earth" dams may also employ a watertight facing or core in the manner of a rock-fill dam. An interesting type of temporary earth dam occasionally used in high latitudes is the "frozen-core" dam, in which a coolant is circulated through pipes inside the dam to maintain a watertight region of permafrost within it.
Tarbela Dam is a large dam on the Indus River in Pakistan. It is located about 50 km northwest of Islamabad, and a height of 485 ft above the river bed and a reservoir size of 95 sqmi makes it the largest earth filled dam in the world. The principal element of the project is an embankment 9000 ft long with a maximum height of 465 ft. The total volume of earth and rock used for the project is approximately 200 million cubic yards (152.8 million cu. Meters) which makes it one of the largest man made structure in the world.
Because earthen dams can be constructed from materials found on-site or nearby, they can be very cost-effective in regions where the cost of producing or bringing in concrete would be prohibitive.
By size.
International standards (including International Commission on Large Dams, ICOLD) define "large dams" as higher than 15 meters and "major dams" as over 150 m in height. The "Report of the World Commission on Dams" also includes in the "large" category, dams, such as barrages, which are between 5 and high with a reservoir capacity of more than 3 e6m3.
The tallest dam in the world is the 300 m high Nurek Dam in Tajikistan.
By use.
Saddle dam.
A saddle dam is an auxiliary dam constructed to confine the reservoir created by a primary dam either to permit a higher water elevation and storage or to limit the extent of a reservoir for increased efficiency. An auxiliary dam is constructed in a low spot or "saddle" through which the reservoir would otherwise escape. On occasion, a reservoir is contained by a similar structure called a dike to prevent inundation of nearby land. Dikes are commonly used for "reclamation" of arable land from a shallow lake. This is similar to a levee, which is a wall or embankment built along a river or stream to protect adjacent land from flooding.
Weir.
A weir (also sometimes called an "overflow dam") is a type of small overflow dam that is often used within a river channel to create an impoundment lake for water abstraction purposes and which can also be used for flow measurement or retardation.
Check dam.
A check dam is a small dam designed to reduce flow velocity and control soil erosion. Conversely, a "wing dam" is a structure that only partly restricts a waterway, creating a faster channel that resists the accumulation of sediment.
Dry dam.
A dry dam also known as a flood retarding structure, is a dam designed to control flooding. It normally holds back no water and allows the channel to flow freely, except during periods of intense flow that would otherwise cause flooding downstream.
Diversionary dam.
A diversionary dam is a structure designed to divert all or a portion of the flow of a river from its natural course. The water may be redirected into a canal or tunnel for irrigation and/or hydroelectric power production.
Underground dam.
Underground dams are used to trap groundwater and store all or most of it below the surface for extended use in a localized area. In some cases they are also built to prevent saltwater from intruding into a freshwater aquifer. Underground dams are typically constructed in areas where water resources are minimal and need to be efficiently stored, such as in deserts and on islands like the Fukuzato Dam in Okinawa, Japan. They are most common in northeastern Africa and the arid areas of Brazil while also being used in the southwestern United States, Mexico, India, Germany, Italy, Greece, France and Japan.
There are two types of underground dams: a "sub-surface" and a "sand-storage" dam. A sub-surface dam is built across an aquifer or drainage route from an impervious layer (such as solid bedrock) up to just below the surface. They can be constructed of a variety of materials to include bricks, stones, concrete, steel or PVC. Once built, the water stored behind the dam raises the water table and is then extracted with wells. A sand-storage dam is a weir built in stages across a stream or wadi. It must be strong as floods will wash over its crest. Over time sand accumulates in layers behind the dam which helps store water and most importantly, prevent evaporation. The stored water can be extracted with a well, through the dam body, or by means of a drain pipe.
Tailings dam.
A tailings dam is typically an earth-fill embankment dam used to store tailings — which are produced during mining operations after separating the valuable fraction from the uneconomic fraction of an ore. Conventional water retention dams can serve this purpose but due to cost, a tailings dam is more viable. Unlike water retention dams, a tailings dam is raised in succession throughout the life of the particular mine. Typically, a base or starter dam is constructed and as it fills with a mixture of tailings and water, it is raised. Material used to raise the dam can include the tailings (depending on their size) along with dirt.
There are three raised tailings dam designs, the "upstream", "downstream" and "centerline", named according to the movement of the crest during raising. The specific design used it dependent upon topography, geology, climate, the type of tailings and cost. An upstream tailings dam consists of trapezoidal embankments being constructed on top but toe to crest of another, moving the crest further upstream. This creates a relatively flat downstream side and a jagged upstream side which is supported by tailings slurry in the impoundment. The downstream design refers to the successive raising of the embankment that positions the fill and crest further downstream. A centerlined dam has sequential embankment dams constructed directly on top of another while fill is placed on the downstream side for support and slurry supports the upstream side.
Because tailings dams often store toxic chemicals from the mining process, they have an impervious liner to prevent seepage. Water/slurry levels in the tailings pond must be managed for stability and environmental purposes as well.
By material.
Steel dams.
A steel dam is a type of dam briefly experimented with in around the start of the 20th century which uses steel plating (at an angle) and load bearing beams as the structure. Intended as permanent structures, steel dams were an (arguably failed) experiment to determine if a construction technique could be devised that was cheaper than masonry, concrete or earthworks, but sturdier than timber crib dams.
Timber dams.
Timber dams were widely used in the early part of the industrial revolution and in frontier areas due to ease and speed of construction. Rarely built in modern times because of relatively short lifespan and limited height to which they can be built, timber dams must be kept constantly wet in order to maintain their water retention properties and limit deterioration by rot, similar to a barrel. The locations where timber dams are most economical to build are those where timber is plentiful, cement is costly or difficult to transport, and either a low head diversion dam is required or longevity is not an issue. Timber dams were once numerous, especially in the North American west, but most have failed, been hidden under earth embankments, or been replaced with entirely new structures. Two common variations of timber dams were the "crib" and the "plank".
"Timber crib dams" were erected of heavy timbers or dressed logs in the manner of a log house and the interior filled with earth or rubble. The heavy crib structure supported the dam's face and the weight of the water. Splash dams were timber crib dams used to help float logs downstream in the late 19th and early 20th centuries.
"Timber plank dams" were more elegant structures that employed a variety of construction methods utilizing heavy timbers to support a water retaining arrangement of planks.
Other types.
Cofferdams.
A cofferdam is a barrier, usually temporary, constructed to exclude water from an area that is normally submerged. Made commonly of wood, concrete, or steel sheet piling, cofferdams are used to allow construction on the foundation of permanent dams, bridges, and similar structures. When the project is completed, the cofferdam will usually be demolished or removed unless the area requires continuous maintenance. See also causeway and retaining wall. Common uses for cofferdams include construction and repair of off shore oil platforms. In such cases the cofferdam is fabricated from sheet steel and welded into place under water. Air is pumped into the space, displacing the water and allowing a dry work environment below the surface.
Natural dams.
Dams can also be created by natural geological forces. Volcanic dams are formed when lava flows, often basaltic, intercept the path of a stream or lake outlet, resulting in the creation of a natural impoundment. An example would be the eruptions of the Uinkaret volcanic field about 1.8 million–10,000 years ago, which created lava dams on the Colorado River in northern Arizona in the United States. The largest such lake grew to about 800 km in length before the failure of its dam. Glacial activity can also form natural dams, such as the damming of the Clark Fork in Montana by the Cordilleran Ice Sheet, which formed the 7780 km2 Glacial Lake Missoula near the end of the last Ice Age. Moraine deposits left behind by glaciers can also dam rivers to form lakes, such as at Flathead Lake, also in Montana (see Moraine-dammed lake).
Natural disasters such as earthquakes and landslides frequently create landslide dams in mountainous regions with unstable local geology. Historical examples include the Usoi Dam in Tajikistan, which blocks the Murghab River to create Sarez Lake. At 560 m high, it is the tallest dam in the world, including both natural and man-made dams. A more recent example would be the creation of Attabad Lake by a landslide on Pakistan's Hunza River.
Natural dams often pose significant hazards to human settlements and infrastructure. The resulting lakes often flood inhabited areas, while a catastrophic failure of the dam could cause even greater damage, such as the failure of western Wyoming's Gros Ventre landslide dam in 1927, which wiped out the town of Kelly and resulted in the deaths of six people.
Beaver dams.
Beavers create dams primarily out of mud and sticks to flood a particular habitable area. By flooding a parcel of land, beavers can navigate below or near the surface and remain relatively well hidden or protected from predators. The flooded region also allows beavers access to food, especially during the winter.
Construction elements.
Power generation plant.
As of 2005, hydroelectric power, mostly from dams, supplies some 19% of the world's electricity, and over 63% of renewable energy. Much of this is generated by large dams, although China uses small scale hydro generation on a wide scale and is responsible for about 50% of world use of this type of power.
Most hydroelectric power comes from the potential energy of dammed water driving a water turbine and generator; to boost the power generation capabilities of a dam, the water may be run through a large pipe called a penstock before the turbine. A variant on this simple model uses pumped storage hydroelectricity to produce electricity to match periods of high and low demand, by moving water between reservoirs at different elevations. At times of low electrical demand, excess generation capacity is used to pump water into the higher reservoir. When there is higher demand, water is released back into the lower reservoir through a turbine. (For example see Dinorwic Power Station.)
Spillways.
A "spillway" is a section of a dam designed to pass water from the upstream side of a dam to the downstream side. Many spillways have floodgates designed to control the flow through the spillway. Types of spillway include: A "service spillway" or "primary spillway" passes normal flow. An "auxiliary spillway" releases flow in excess of the capacity of the service spillway. An "emergency spillway" is designed for extreme conditions, such as a serious malfunction of the service spillway. A "fuse plug spillway" is a low embankment designed to be over topped and washed away in the event of a large flood. The elements of a fuse plug are independent free-standing blocks, set side by side which work without any remote control. They allow increasing the normal pool of the dam without compromising the security of the dam because they are designed to be gradually evacuated for exceptional events. They work as fixed weir at times by allowing over-flow for common floods.
The spillway can be gradually eroded by water flow, including cavitation or turbulence of the water flowing over the spillway, leading to its failure. It was the inadequate design of the spillway which led to the 1889 over-topping of the South Fork Dam in Johnstown, Pennsylvania, resulting in the infamous Johnstown Flood (the "great flood of 1889").
Erosion rates are often monitored, and the risk is ordinarily minimized, by shaping the downstream face of the spillway into a curve that minimizes turbulent flow, such as an ogee curve.
Dam creation.
Common purposes.
Some of these purposes are conflicting and the dam operator needs to make dynamic tradeoffs. For example power generation and water supply would keep the reservoir high whereas flood prevention would keep it low. Many dams in areas where precipitation fluctuates in an annual cycle will also see the reservoir fluctuate annually in an attempt to balance these difference purposes. Dam management becomes a complex exercise amongst competing stakeholders.
Location.
One of the best places for building a dam is a narrow part of a deep river valley; the valley sides can then act as natural walls. The primary function of the dam's structure is to fill the gap in the natural reservoir line left by the stream channel. The sites are usually those where the gap becomes a minimum for the required storage capacity. The most economical arrangement is often a composite structure such as a masonry dam flanked by earth embankments. The current use of the land to be flooded should be dispensable.
Significant other engineering and engineering geology considerations when building a dam include:
Impact assessment.
Impact is assessed in several ways: the benefits to human society arising from the dam (agriculture, water, damage prevention and power), harm or benefit to nature and wildlife, impact on the geology of an area – whether the change to water flow and levels will increase or decrease stability, and the disruption to human lives (relocation, loss of archeological or cultural matters underwater).
Environmental impact.
Reservoirs held behind dams affect many ecological aspects of a river. Rivers topography and dynamics depend on a wide range of flows whilst rivers below dams often experience long periods of very stable flow conditions or saw tooth flow patterns caused by releases followed by no releases. Water releases from a reservoir including that exiting a turbine usually contains very little suspended sediment, and this in turn can lead to scouring of river beds and loss of riverbanks; for example, the daily cyclic flow variation caused by the Glen Canyon Dam was a contributor to sand bar erosion.
Older dams often lack a fish ladder, which keeps many fish from moving up stream to their natural breeding grounds, causing failure of breeding cycles or blocking of migration paths. Even the presence of a fish ladder does not always prevent a reduction in fish reaching the spawning grounds upstream. In some areas, young fish ("smolt") are transported downstream by barge during parts of the year. Turbine and power-plant designs that have a lower impact upon aquatic life are an active area of research.
A large dam can cause the loss of entire ecospheres, including endangered and undiscovered species in the area, and the replacement of the original environment by a new inland lake.
Large reservoirs formed behind dams have been indicated in the contribution of seismic activity, due to changes in water load and/or the height of the water table.
Dams are also found to have a role in the increase of global warming. The changing water levels in dams and in reservoirs are one of the main sources for green house gas like methane. While dams and the water behind them cover only a small portion of earth's surface, they harbour biological activity that can produce large amounts of greenhouse gases.
Human social impact.
The impact on human society is also significant. Nick Cullather argues in "Hungry World: America's Cold War Battle Against Poverty in Asia" that dam construction requires the state to displace individual people in the name of the common good, and that it often leads to abuses of the masses by planners. He cites Morarji Desai, Interior Minister of India, in 1960 speaking to villagers upset about the Pong Dam, who threatened to "release the waters" and drown the villagers if they did not cooperate.
For example, the Three Gorges Dam on the Yangtze River in China is more than five times the size of the Hoover Dam (U.S.), and will create a reservoir 600 km long to be used for hydro-power generation. Its construction required the loss of over a million people's homes and their mass relocation, the loss of many valuable archaeological and cultural sites, as well as significant ecological change. It is estimated that to date, 40–80 million people worldwide have been physically displaced from their homes as a result of dam construction.
Economics.
Construction of a hydroelectric plant requires a long lead-time for site studies, hydrological studies, and environmental impact assessments, and are large-scale projects by comparison to traditional power generation based upon fossil fuels. The number of sites that can be economically developed for hydroelectric production is limited; new sites tend to be far from population centers and usually require extensive power transmission lines. Hydroelectric generation can be vulnerable to major changes in the climate, including variations in rainfall, ground and surface water levels, and glacial melt, causing additional expenditure for the extra capacity to ensure sufficient power is available in low-water years.
Once completed, if it is well designed and maintained, a hydroelectric power source is usually comparatively cheap and reliable. It has no fuel and low escape risk, and as an alternative energy source it is cheaper than both nuclear and wind power. It is more easily regulated to store water as needed and generate high power levels on demand compared to wind power.
Dam failure.
Dam failures are generally catastrophic if the structure is breached or significantly damaged. Routine deformation monitoring and monitoring of seepage from drains in and around larger dams is useful to anticipate any problems and permit remedial action to be taken before structural failure occurs. Most dams incorporate mechanisms to permit the reservoir to be lowered or even drained in the event of such problems. Another solution can be rock grouting – pressure pumping portland cement slurry into weak fractured rock.
During an armed conflict, a dam is to be considered as an "installation containing dangerous forces" due to the massive impact of a possible destruction on the civilian population and the environment. As such, it is protected by the rules of International Humanitarian Law (IHL) and shall not be made the object of attack if that may cause severe losses among the civilian population. To facilitate the identification, a protective sign consisting of three bright orange circles placed on the same axis is defined by the rules of IHL.
The main causes of dam failure include inadequate spillway capacity, piping through the embankment, foundation or abutments, spillway design error (South Fork Dam), geological instability caused by changes to water levels during filling or poor surveying (Vajont Dam, Malpasset, Testalinden Creek Dam), poor maintenance, especially of outlet pipes (Lawn Lake Dam, Val di Stava Dam collapse), extreme rainfall (Shakidor Dam), earthquakes and human, computer or design error (Buffalo Creek Flood, Dale Dike Reservoir, Taum Sauk pumped storage plant).
A notable case of deliberate dam failure (prior to the above ruling) was the Royal Air Force 'Dambusters' raid on Germany in World War II (codenamed "Operation Chastise"), in which three German dams were selected to be breached in order to have an impact on German infrastructure and manufacturing and power capabilities deriving from the Ruhr and Eder rivers. This raid later became the basis for several films.
Since 2007, the Dutch IJkdijk foundation is developing, with an open innovation model and early warning system for levee/dike failures. As a part of the development effort, full scale dikes are destroyed in the IJkdijk fieldlab. The destruction process is monitored by sensor networks from an international group of companies and scientific institutions.

</doc>
<doc id="51519" url="http://en.wikipedia.org/wiki?curid=51519" title="Pratt &amp; Whitney">
Pratt &amp; Whitney

Pratt & Whitney is an American aerospace manufacturer with global service operations. It is a subsidiary of United Technologies Corporation (UTC). Pratt & Whitney's aircraft engines are widely used in both civil aviation (especially airlines) and military aviation. Its headquarters are in East Hartford, Connecticut. As one of the "big three" aero-engine manufacturers, it competes with General Electric and Rolls-Royce, although it has also formed joint ventures with both of these companies. In addition to aircraft engines, Pratt & Whitney manufactures gas turbines for industrial and power generation, and marine turbines. As of 2014, the company reported having 31,500 employees supporting more than 11,000 customers in 180 countries around the world. In 2013, Pratt & Whitney's revenue totaled $14.5 billion.
History.
Early history.
In April 1925, Frederick Rentschler, an Ohio native and former executive at Wright Aeronautical, was determined to start an aviation-related business of his own. His social network included Edward Deeds, another prominent Ohioan of the early aviation industry, and Frederick's brother Gordon Rentschler, both of whom were on the board of Niles Bement Pond, then one of the largest machine tool corporations in the world. Frederick Rentschler approached these men as he sought capital and assets for his new venture. Deeds and G. Rentschler persuaded the board of Niles Bement Pond that their Pratt & Whitney Machine Tool (P&WMT) subsidiary of Hartford, Connecticut, should provide the funding and location to build a new aircraft engine being developed by Rentschler, George J. Mead, and colleagues, all formerly of Wright Aeronautical. Conceived and designed by Mead, the new engine would be a large, air-cooled, radial design. Pratt & Whitney Machine Tool was going through a period of self-revision at the time to prepare itself for the post–Great War era, discontinuing old product lines and incubating new ones. The Great War had been profitable to P&WMT, but the peace brought a predictable glut to the machine tool market, as contracts with governments were canceled and the market in used, recently built tools competed against new ones. P&WMT's future growth would depend on innovation. Having idle factory space and capital available at this historical moment, to be invested wherever good return seemed available, P&WMT saw the postwar aviation industry, both military and civil (commercial, private), as one with some of the greatest growth and development prospects available anywhere for the next few decades. It lent Rentschler $250,000, the use of the Pratt & Whitney name, and space in their building. This was the beginning of the Pratt & Whitney Aircraft Company. Pratt & Whitney Aircraft's first engine, the 425 horsepower (317 kW) R-1340 Wasp, was completed on Christmas Eve 1925. On its third test run it easily passed the Navy qualification test in March 1926; by October, the Navy had ordered 200. The Wasp exhibited performance and reliability that revolutionized American aviation. The R-1340 powered the aircraft of Wiley Post, Amelia Earhart, and many other record flights.
The R-1340 was followed by another very successful engine, the R-985 Wasp Junior. Eventually a whole Wasp series was developed. Both engines are still in use in agricultural aircraft around the world and produce more power than their original design criteria. (Replacement parts for both engines are still in production and it is theoretically possible to assemble a new engine from the parts.)
George Mead soon led the next step in the field of large, state-of-the-art, air-cooled, radial aircraft engines (which the Wasp dominated) when Pratt & Whitney released its R-1690 Hornet. It was basically "a bigger Wasp".
In 1929, Rentschler ended his association with Pratt & Whitney Machine Tool and merged Pratt & Whitney Aircraft with Boeing and other companies to form the United Aircraft and Transport Corporation. His agreement allowed him to carry the Pratt & Whitney name with him to his new corporation.
Recent.
In October 2014, Pratt & Whitney was awarded a $592 million contract with US Defense Department to supply 36 F135 engines for the F-35 fighter.
Headquarters.
Pratt & Whitney is headquartered in East Hartford, Connecticut and also has plants in Columbus, Georgia; Middletown, Connecticut; Dallas, Texas; West Palm Beach, Florida, North Berwick, Maine; and Bridgeport, West Virginia.
The home stadium for the University of Connecticut Huskies football team, Rentschler Field, is located adjacent to Pratt & Whitney's East Hartford, Connecticut campus, on Pratt's company-owned former airfield of the same name.
Divisions.
Pratt & Whitney is a business unit of industrial conglomerate United Technologies, making it a sister company to Sikorsky Aircraft, Hamilton Sundstrand, Otis Elevator Company, UTC Fire & Security, UTC Power and refrigeration giant Carrier Corporation. It is also involved in two major joint ventures, the Engine Alliance with GE which manufactures engines for the Airbus A380, and International Aero Engines company with Rolls-Royce, MTU Aero Engines, and the Japanese Aero Engines Corporation which manufactures engines for the Airbus A320 and the McDonnell Douglas MD-90 aircraft.
Commercial Engines.
Pratt & Whitney's large commercial engines power more than 25 percent of the world’s passenger aircraft fleet and serve more than 800 customers in 160 countries. With more than 16,000 large commercial engines installed today, Pratt & Whitney provides power to hundreds of airlines and operators, from narrow-bodied airplanes to wide-bodied jumbo jetliners. In June 2007, Pratt & Whitney’s fleet of large commercial engines surpassed 1 billion flight hours of service.
Global Material Solutions.
Pratt & Whitney’s Global Material Solutions (GMS) makes parts for the CFM56 engine thus giving customers an alternative in new CFM56 engine materials. In addition to engine parts, GMS provides customers with fleet management and customized maintenance service programs. "United Airlines was the GMS launch customer".
GMS received its first part certification in July 2007, when the Federal Aviation Administration (FAA) granted Parts Manufacturing Approval (PMA) certification for the GMS high pressure turbine (HPT) shroud for the CFM56-3 engine. In March 2008, the FAA certified the GMS fan and booster with a Supplemental Type Certificate (STC) with FAA Chapter 5 life limits equal to the original type certificate holder. The STC was the first FAA certification ever granted for alternative life-limited engine parts. In May 2008, Global Material Solutions received FAA STCs for its remaining life limited parts for CFM56-3 engines.
Global Service Partners.
Pratt & Whitney Global Service Partners (GSP) offers overhaul, maintenance and repair services for Pratt & Whitney, International Aero Engines, General Electric, Rolls-Royce, and CFMI engines. In addition to engine overhaul and repair services, GSP provides services including line maintenance, engine monitoring and diagnostics, environmentally friendly on-wing water washes, leased engines, custom engine service programs and new and repaired parts.
Pratt & Whitney maintains one of the largest service center networks in the world, with more than 40 engine overhaul and maintenance centers located around the globe.
Military Engines.
Pratt & Whitney's Military Engines power 27 air forces around the globe, with nearly 11,000 military engines in service with 23 customers in 22 nations. Pratt & Whitney military engines include the F135 for the F-35 Lightning II Joint Strike Fighter (JSF), the F119 for the F-22 Raptor, the F100 family that powers the F-15 Eagle and F-16 Falcon, the F117 for the C-17 Globemaster III, the J52 for the EA-6B Prowler, the TF33 powering E-3 AWACS, E-8 Joint STARS, B-52, and KC-135 aircraft, and the TF30 for the F-111. In addition, Pratt & Whitney offers a global network of maintenance, repair, and overhaul facilities and military aviation service centers focused on maintaining engine readiness for their customers.
Pratt & Whitney Canada.
Pratt & Whitney Canada (PWC), originally Canadian Pratt & Whitney Aircraft Company, and later United Aircraft of Canada, provides a large range of products, including turbofan, turboprop and turboshaft engines targeted for the regional, business, utility and military aircraft and helicopter markets. The company also designs and manufactures engines for auxiliary power units and industrial applications. Its headquarters are located in Longueuil, Quebec (just outside Montreal).
Speaking to Reuters June 16, 2013 ahead of the Paris Airshow 2013 Pratt & Whitney President David Hess said he was confident that Canada would decide to stick with the F-35 program despite its recent discussions about having a new competition. If the orders did shift to another company, Pratt & Whitney could decide to move some of the industrial base work it is currently doing in Canada, Hess said. "We might reallocate the work elsewhere," he said, adding that reduced order volumes would likely trigger changes in Canada. http://www.cnbc.com/id/100818837
The division admitted in July 2012 to providing engines and engine software for China's first attack helicopter, the Z-10. This violated U.S. export laws and resulted in a multimillion dollar fine.
Pratt & Whitney Space Propulsion.
Pratt & Whitney Space Propulsion consisted of liquid space propulsion at the Liquid Space Propulsion Division (West Palm Beach, FL) and solid rocket propulsion at the Chemical Systems Division (San Jose, CA), as well as refurbishment and integration of the non-motor elements of the Space Shuttle's solid rocket boosters at the USBI Co. Division (NASA Kennedy Space Center, FL). Pratt & Whitney Space Propulsion provided advanced technology solutions to commercial, government and military customers for more than four decades. Products included the RL10, the upper stage rocket engine used on the Boeing Delta and Lockheed Martin Atlas rockets, high-pressure turbopumps for the Space Shuttle Main Engines (SSME) and the RD-180 booster engine, offered by RD Amross, a partnership between Pratt & Whitney and NPO Energomash of Russia, for the Atlas III and V programs. The West Palm Beach site consisted of an engineering division and manufacturing division which designed and manufactured the high-pressure turbopumps(Fuel and Lox) for the Space Shuttle's Main Engines (SSME) which were manufactured by the former Rocketdyne Corporation.
Pratt & Whitney Rocketdyne.
Pratt & Whitney Rocketdyne (PWR) was formed in 2005 when Pratt & Whitney Space Propulsion and Rocketdyne Propulsion & Power were merged following the latter's acquisition from Boeing.
P&W Rocketdyne engines powered the Space Shuttle, and the company also supplies booster engines for Delta II rockets and boosters and upper stage engines for Atlas III and V and Delta IV rockets.
In 2013, PWR was sold to GenCorp, which merged it with Aerojet to become Aerojet Rocketdyne.
Pratt & Whitney Power Systems.
Pratt & Whitney Power Systems (PWPS) designs, builds, furnishes and supports aero-derivative gas turbine and geothermal power systems for customers worldwide. These industrial gas turbines power everything from small businesses to small cities. PWPS’ industrial turbines not only generate electrical power, but provide variable speed mechanical drive for marine propulsion, gas compression, and liquid pumping. PWPS has over 2,000 industrial gas turbines installed in more than 40 countries worldwide. PWPS also provides parts and repairs for heavy-duty frame gas turbines as an OEM alternative.
In May 2013, United Technologies Corporation (UTC) sold its Pratt & Whitney Power Systems unit to Mitsubishi Heavy Industries (MHI).
International Aero Engines.
International Aero Engines is a joint venture that develops, builds and services the V2500 aero engine family, which powers the Airbus A320 family and McDonnell Douglas MD-90 aircraft. The four engine manufacturers that make up IAE each contribute an individual module to the V2500 engine. Pratt & Whitney produces the combustor and high-pressure turbine, Rolls-Royce the high-pressure compressor, JAEC the fan and low-pressure compressor and MTU the low-pressure turbine.
Engine Alliance.
Engine Alliance, a 50/50 joint venture between General Electric and Pratt & Whitney, was formed in August 1996 to develop, manufacture, and support a family of modern technology engines for new high-capacity, long-range aircraft. The main application is the GP7200, which has been designed for use on the Airbus A380. It competes with the Rolls-Royce Trent 900, the launch engine for the aircraft.
The first GP7200-powered Airbus A380 entered service with Emirates on August 1, 2008 on a non-stop flight from Dubai to New York City.
Motorsports.
Between 1967 and 1971, Pratt & Whitney turbine engines were used in American Championship Car Racing and Formula One. The STP-Paxton Turbocar dominated the 1967 Indianapolis 500 until a transmission bearing failed four laps from the finish. STP entered four Lotus 56s in the 1968 Indianapolis 500. One car crashed during qualifying. Two of the remaining cars qualified fastest and second fastest, but all three retired from the race. Turbine cars were deemed illegal before the following year's race, so Lotus chief Colin Chapman developed the car for use in Formula One and an updated 56B competed in half a dozen Formula One races in 1971.
Products.
Engine Maintenance Systems.
Pratt & Whitney now markets its "Ecopower" pressure-washing service, which uses a high-pressure water spray run through several nozzles to clean grime and contaminants from jet engine parts, most notably turbine blades, to prevent overheating, improve engine operating efficiency and reduce fuel burn. The system collects the runoff from the washing process for appropriate disposal. The washing is accomplished at the airport tarmac in about one hour. Customers include United Airlines, Air India, Martinair, Singapore Airlines, Virgin Atlantic, and JetBlue.

</doc>
