<doc id="47357" url="http://en.wikipedia.org/wiki?curid=47357" title="Hake">
Hake

The term hake refers to fish in either of:
Hake.
Hake may be found within the same taxonomic order (Gadiformes) as cod and haddock. It is a medium to large fish averaging between 1 to 8 pounds in weight, with specimens as large as 60 lbs. The fish can grow up to 1 metre in length with a lifespan as long as 14 years. Hake may be found in the Atlantic Ocean and Pacific Ocean in waters between 200–350 meters (650-1150 feet) deep. The fish stay in deep sea water during the day and come to shallower depths during the night. An undiscerning predator, hake feed on their prey found near or on the bottom of the sea. Male and female hake are very similar in appearance.
After spawning, the hake eggs float on the surface of the sea where the larvae develop. After a certain period of time, the baby hake then migrate to the bottom of the sea, preferring depths of less than 200 m.
A total of 12 hake species are known in the family Merlucciidae:
Not all hake species are viewed as commercially important, but the deep-water and shallow-water hakes are known to grow rapidly and make up the majority of harvested species.
Commercial use.
The highest demand for hake has been in Europe. Hake has been primarily divided into three principal levels—fresh, frozen, and frozen fillet. Fresh hake is mainly supplied by European production and imports. Frozen hake and frozen hake fillet are effectively supplied by imports and European processing companies.
Spain has the highest consumption of hake in Europe, with a yearly consumption of 6 kg/person. This works out to around half all hake eaten in Europe. Though Spanish consumption of hake and other fish declined in the last decade, hake still accounts for about one third of total fish consumption there. Other countries that eat a lot of hake include Portugal, France, and Italy.
In Spain, fresh hake are mostly purchased by restaurants through retailers. Nonetheless, processed hake products are distributed by hake wholesalers. Fishmongers, public markets and hypermarkets sell hake in various forms: frozen fillet, fillet skin-on, fillet skin-off, etc.
In France, fish are generally purchased in supermarkets. Due to insufficient European hake, French wholesalers purchase fresh hake from external countries such as Argentina and Namibia, and then export them to Spain. Fresh hake is mostly exported to Spain.
In Italy, hotels, restaurants, supermarkets, and institutional purchasers purchase much seafood. However, retailers and wholesalers purchase most frozen hake fillets to sell in markets.
Commercially salable forms.
Hake is sold as frozen, fillets or steaks, fresh, smoked, or salted. When buying hake fillet or other hake product, consumers should look for hake with white flesh that is free of signs of dryness, grayness or browning. It should have a seawater fresh smell.
Fisheries.
The main catching method of deep-water hake is primarily trawling, and shallow-water hake is mostly caught by inshore trawl and longlining. Hake are mostly found in the Southwest Atlantic (Argentina and Uruguay), Southeast Pacific (Chile and Peru), Southeast Atlantic (Namibia and South Africa), Southwest Pacific (New Zealand), and Mediterranean and Black Sea (Italy, Portugal, Spain, Greece and France).
Due to over-fishing, Argentinean hake catches have declined drastically. About 80% of adult hake has apparently disappeared from the Argentinean Sea. Argentinean hake is not expected to disappear, but the stock may be so low that it is no longer economic for commercial fishing. In addition, this adversely affects employment, because many people lose their jobs in the fishing industries. On the other hand, Argentinean hake prices are rising due to scarcity. This has reduced exports, which ultimately affects the economy.
In Chile, seafood exports, especially Chilean hake, have decreased dramatically. Hake export has decreased by almost 19 percent. The main cause of this decline is the earthquake and tsunami that hit Chile in February 2010. These disasters destroyed most processing plants, especially manufacturing companies that produce fishmeal and frozen fillets.
European hake catches are well below historical levels because of hake depletion in the Mediterranean and Black Sea. However, different factors might have caused this decline, including a too-high Total Annual Catch, unsustainable fishing, ecological problems, juvenile catches, or non-registered catches.
According to the Worldwide Fund, the only Hake species not currently over-fished is Cape hake, found in Namibia. Namibia is the only country that has increased its hake quota, from 130,000 tonnes in 2009 to 145,000 tonnes in 2010. Furthermore, the Local Ministry of Fisheries adheres to very strict rules regarding the catch of hake. For example, the closed seasons for hake lasts approximately two months, September and October, depending on the level of stock. This rule has been applied to ensure the regrowth of the hake population. Supplemental restrictions include forbidding trawling for Hake in less than 200 m depth (to avoid damaging non-target species habitat)and minimizing by-catch.
Human introduction to non-native areas.
Frank Forrester's Fishermens' Guide mentions a hake that was transplanted from the coast of Ireland to Cape Cod, United States. It is uncertain which species this was, but the reference is given below:"This is an Irish salt water fish, similar in appearance to the tom cod. In Galway bay, and other sea inlets of Ireland, the hake is exceedingly abundant, and is taken in great numbers. It is also found in England and France. Since the Irish immigration to America, the hake has followed in the wake of their masters, as it is now found in New York bay, in the waters around Boston, and off Cape Cod. Here it is called the stock fish, and the Bostonians call them poor Johns. It is a singular fact that until within a few years this fish was never seen in America. It does not grow as large here as in Europe, though here they are from ten to eighteen inches [250 to 460 mm] in length. The general color of this fish is a reddish brown, with some golden tints—the sides being of a pink silvery luster."

</doc>
<doc id="47358" url="http://en.wikipedia.org/wiki?curid=47358" title="Sea bass">
Sea bass

Sea bass may refer to:
Fish.
Many fish species of various families, including:
Family Serranidae.
Family Serranidae
Other.
The term bass also refers to a number of freshwater sport fish.

</doc>
<doc id="47359" url="http://en.wikipedia.org/wiki?curid=47359" title="White bass">
White bass

The white bass or sand bass ("Morone chrysops") is a freshwater fish of the temperate bass family Moronidae. It is the state fish of Oklahoma.
Range.
White bass are distributed widely across the United States, particularly in the Mideast. They are very abundant in Pennsylvania and the area around Lake Erie. Some native ranges of the white bass are the Arkansas River, Lake Erie near Cleveland, Ohio, and Lake Poinsettia in South Dakota; they are abundant in the Winnebago lakes system of Wisconsin; and they are also very abundant in Oklahoma. White bass have also been found in rivers that flow to the Mississippi. Native to many northern habitats, they have been introduced in many different waters around the United States, particularly in southern locations. They were also successfully introduced to Manitoba starting in the 1960s, where they have gained importance as a sport fish.
Description.
The species' main color is silver-white to pale green. Its back is dark, with white sides and belly, and with narrow dark stripes running lengthwise on its sides. It has large, rough scales and two dorsal fins. The more anterior dorsal fin is much harder and appears to have spines on them. Although these are not true spines, this type of fin is called a spinous ray. The more posterior of the two dorsal fins is much softer, and is thus called a soft-ray. Because the vertebrae do not extend into the tail, the white bass has what is called a homocercal tail. The body is deep and compressed laterally. Most grow to a length between 10 and 12 inches, though they can reach 17 inches or more. Because the dorsal and ventral portions of the its tail angle inward toward a point to create a clear angle, the tail is said to be notched.
The record size for white bass caught on fishing tackle is six pounds and 13 oz (3.09 kg) shared by fish caught in 1989 in Orange Lake, Orange, Virginia, and in 2010 in Amite River, Louisiana.
Diet.
White bass are carnivores. They have four main taxa in their diet: calanoid copepods, cyclopoid copepods, daphnia, and leptodora. They are visual feeders. When not frightened, they will bite readily at live bait such as worms and minnows. Only the largest fish will feed on other fish, and as the summer season progresses, there is an overall trend towards eating fewer fish. Fish that are able to accumulate lipids over the summer are better able to survive cold winters. When looking at midwestern white bass, particularly in South Dakota, diet overlap occurs between the bass and the walleye. As seasons progress through the summer and fall, the amount of diet overlap decreases as a result of both fish increasing in length.
Habitat.
White bass inhabit large reservoirs and rivers. When mating in the spring, they are more often found in shallow rivers, creeks, and streams. White bass are found in high densities in the upstream segment of rivers. This portion of the river becomes the most degraded, as a number of different kinds of fish live in this segment, as well.
Reproduction.
The spawning season for the white bass is mid-March to late May. The optimal water temperatures are 12 to 20°C (54-68°F). They are known to find their home spawning ground even if it is moved to a different part of the same lake. They often spawn in moving water in a tributary stream, but they will spawn in windswept lake shores. They spawn during daylight. Females release 242,000 to 933,000 eggs which stick to the surface of objects. Eggs are laid in clear, relatively shallow water on plants, submerged logs, gravel, or rocks. The parents move to deeper water and do not care for the young fish. The young fish live in shallow water for a while until they move to deeper water.
When trying to find a female with whom to mate, males will bump against a female's abdominal area. The female will then rise closer to the surface and begin spinning and releasing eggs. Several males that have stayed in the area will be able to fertilize the eggs the female releases.

</doc>
<doc id="47360" url="http://en.wikipedia.org/wiki?curid=47360" title="White perch">
White perch

The white perch ("Morone americana") is not a true perch but is, rather, a fish of the temperate bass family, Moronidae, notable as a food and game fish in eastern North America.
The name "white perch" is sometimes applied to the white crappie.
Generally silvery-white in color, hence the name, depending upon habitat and size specimens have begun to develop a darker shade near the dorsal fin and along the top of the fish. This sometimes earns them the nickname "black-back". White perch have been reported up to 49.5 cm (19.5 in.) in length and weighing 2.2 kg (4.8 lbs.).
Although favoring brackish waters, it is also found in fresh water and coastal areas from the St. Lawrence River and Lake Ontario south to the Pee Dee River in South Carolina, and as far east as Nova Scotia.
They are also found in the lower Great Lakes and Chesapeake Bay. The raw meat is of a somewhat pinkish hue, but when cooked, it is white and flaky. At times, a parasite known as "Lironeca ovalis" is located in the gills. They are only known to reduce the growing rate of white perch.
Diet.
White perch are known to eat the eggs of many species native to the Great Lakes, such as walleye and other true perches. At times, fish eggs are 100% of their diet. They prefer to eat small minnows like mud minnows and fathead minnows. In the Chesapeake Bay, white perch commonly prey upon grass shrimp, razor clams, and bloodworms which are all common to the region.
Reproduction.
White perch are a prolific species. The female can deposit over 150,000 eggs in a spawning session, lasting just over a week. Several males will often attend a spawning female, and each may fertilize a portion of her eggs. The young hatch within one to six days of fertilization.
The white perch is currently recovering from a loss of population in the Hudson River.
Aquatic nuisance species.
Some states consider the white perch to be a nuisance species due to its ability to destroy fisheries. They have been associated with the declines in both walleye and white bass populations because they feed heavily on baitfish used by those species and outcompete them for food and space. Many states have enacted laws forbidding possession of live white perch. Additionally, these states recommend not releasing captured white perch back into the water to help control its spread.

</doc>
<doc id="47361" url="http://en.wikipedia.org/wiki?curid=47361" title="Dow Jones Industrial Average">
Dow Jones Industrial Average

The Dow Jones Industrial Average , also called the Industrial Average, the Dow Jones, the Dow Jones Industrial, the Dow 30, or simply the Dow, is a stock market index, and one of several indices created by "Wall Street Journal" editor and Dow Jones & Company co-founder Charles Dow. The industrial average was first calculated on May 26, 1896. Currently owned by S&P Dow Jones Indices, which is majority owned by McGraw-Hill Financial, it is the most notable of the Dow Averages, of which the first (non-industrial) was first published on February 16, 1885. The averages are named after Dow and one of his business associates, statistician Edward Jones. It is an index that shows how 30 large publicly owned companies based in the United States have traded during a standard trading session in the stock market. It is the second oldest U.S. market index after the Dow Jones Transportation Average, which was also created by Dow.
The "Industrial" portion of the name is largely historical, as many of the modern 30 components have little or nothing to do with traditional heavy industry. The average is price-weighted, and to compensate for the effects of stock splits and other adjustments, it is currently a scaled average. The value of the Dow is not the actual average of the prices of its component stocks, but rather the sum of the component prices divided by a divisor, which changes whenever one of the component stocks has a stock split or stock dividend, so as to generate a consistent value for the index. Since the divisor is currently less than one, the value of the index is larger than the sum of the component prices. Although the Dow is compiled to gauge the performance of the industrial sector within the American economy, the index's performance continues to be influenced by not only corporate and economic reports, but also by domestic and foreign political events such as war and terrorism, as well as by natural disasters that could potentially lead to economic harm.
Components.
Beginning on March 18, 2015, after the close, the Dow Jones Industrial Average consists of the following 30 major American companies:
Former components.
The components of the DJIA have changed 53 times in its 128-year history. General Electric has had the longest continuous presence on the index, with its latest addition being in 1907. More recent changes to the index include the following:
History.
Precursor.
In 1884, Charles Dow composed his first stock average, which contained nine railroads and two industrial companies that appeared in the "Customer's Afternoon Letter", a daily two-page financial news bulletin which was the precursor to "The Wall Street Journal". On January 2, 1886, the number of stocks represented in what we now call the "Dow Jones Transportation Average" dropped from 14 to 12, as the Central Pacific Railroad and Central Railroad of New Jersey were dropped from that index. Though comprising the same number of stocks, this index contained only one of the original twelve industrials that would eventually form Dow's most famous index.
Initial components.
Dow calculated his first average purely of industrial stocks on May 26, 1896, creating what is now known as the "Dow Jones Industrial Average". Of the original 12 industrials, only General Electric currently remains part of that index. The other 11 were:
Early years.
When it was first published in the mid-1880s, the index stood at a level of 62.76. It reached a peak of 78.38 during the summer of 1890, but ended up hitting its all-time low of 28.48 in the summer of 1896 during the depths of what later became known as the Panic of 1896. Many of the biggest percentage price moves in the Dow occurred early in its history, as the nascent industrial economy matured. A brief war in 1898 between the United States and the Spanish Empire might have only had a minor impact in the Dow's direction.
The 1900s (decade) would see the Dow halt its momentum as it worked its way through a pair of cataclysmic financial crises; the Panic of 1901 and the Panic of 1907. The Dow would remain stuck in a trading range between 53 and 103 points until late 1914. The negativity surrounding the 1906 San Francisco earthquake did little to improve the economic climate. International disturbances such as the Russo-Japanese War were few and far between and seemed to have little if any influence on the Dow. The average finished the decade at 99.05 points.
At the start of the 1910s, the decade would begin with the Panic of 1910–1911 stifling economic growth for a lengthy period of time. History would later take its course on July 30, 1914; as the average stood at a level of 71.42 when a decision was made to close down the New York Stock Exchange, and suspend trading for a span of four and a half months. Some historians believe the exchange closed because of a concern that markets would plunge as a result of panic over the onset of World War I. An alternative explanation is that the Secretary of the Treasury, William Gibbs McAdoo, closed the exchange because he wanted to conserve the U.S. gold stock in order to launch the Federal Reserve System later that year, with enough gold to keep the United States at par with the gold standard. When the markets reopened on December 12, 1914, the index closed at 74.56, a gain of 4.4 percent. This is frequently reported as a large drop, due to using a later redefinition. Reports from the time say that the day was positive. Following World War I, the United States would experience another downturn in economic activity in what became known as the post-World War I recession. The Dow's performance would remain virtually unchanged from the closing value of the previous decade, adding only 8.26%, from 99.05 points at the beginning of 1910, to a level of 107.23 points at the end of 1919.
During the 1920s, specifically in 1928, the components of the Dow were increased to 30 stocks near the economic height of that decade, which was nicknamed the Roaring Twenties. The prosperous nature of the economic climate, muted the negative influence of an early 1920s recession plus certain international conflicts such as the Polish-Soviet war, the Irish Civil War, the Turkish War of Independence and the initial phase of the Chinese Civil War. The Crash of 1929 in October and the ensuing Great Depression over the next several years returned the average to its starting point, almost 90% below its peak. By July 8, 1932, following its intra-day low of 40.56, the Dow would end up closing the session at 41.22. The high of 381.17 on September 3, 1929, would not be surpassed until 1954, in inflation-adjusted numbers. However, the bottom of the 1929 Crash came just 2½ months later on November 13, 1929, when intra-day it was at the 195.35 level, closing slightly higher at 198.69. For the decade, the Dow would end off with a healthy 131.7% gain, from 107.23 points at the beginning of 1920, to a level of 248.48 points at the end of 1929, just before the bulk of the Crash.
Marked by global instability and the Great Depression, the 1930s contended with several consequential European and Asian outbreaks of war, leading up to catastrophic World War II in 1939. Other conflicts during the decade which affected the stock market included the 1936–1939 Spanish Civil War, the 1935–1936 Second Italo-Abyssinian War, the Soviet-Japanese Border War of 1939 and the Second Sino-Japanese War from 1937. On top of that, the United States dealt with a painful recession in 1937 and 1938 which temporarily brought economic recovery to a halt. The largest one-day percentage gain in the index, 15.34%, happened on March 15, 1933, in the depths of the 1930s bear market when the Dow gained 8.26 points to close at 62.10. However, as a whole throughout the Great Depression, the Dow posted some of its worst performances, for a negative return during most of the 1930s for new and old stock market investors. For the decade, the Dow Jones average was down from 248.48 points at the beginning of 1930, to a stable level of 150.24 points at the end of 1939, a loss of about 40%.
Post-war years.
Post-war reconstruction during the 1940s, along with renewed optimism of peace and prosperity, brought about a 39% surge in the Dow from around the 148 level to 206. The strength in the Dow occurred despite a brief recession in 1949 and other global conflicts which started a short time later including the latter stages of the Chinese Civil War, the Greek Civil War, the Indo-Pakistani War of 1947 and the 1948 Arab-Israeli War.
During the 1950s, the Korean War, the Algerian War, the Cold War and other political tensions such as the Cuban Revolution, as well as widespread political and economic changes in Africa during the initial stages of European Decolonization, did not stop the Dow's bullish climb higher. Additionally, the U.S. would also make its way through two grinding recessions; one in 1953 and another in 1958. A 200% increase in the average from a level of 206 to 616 ensued over the course of that decade.
The Dow's bullish behavior began to stall during the 1960s as the U.S. became entangled with foreign political issues. U.S. military excursions included the Bay of Pigs Invasion involving Cuba, the Vietnam War, the Portuguese Colonial War, the Colombian civil war which the U.S. assisted with short-lived counter-guerrilla campaigns, as well as domestic issues such as the Civil Rights Movement and several influential political assassinations. For the decade though, and despite a mild recession between 1960 and 1961, the average still managed a respectable 30% gain from the 616 level to 800.
The 1970s marked a time of economic uncertainty and troubled relations between the U.S. and certain Middle-Eastern countries.
To begin with, the decade started off with the ongoing Recession of 1969–70. Following that, the 1970s Energy Crisis ensued which included the 1973–75 recession, the 1973 Oil Crisis as well as the 1979 energy crisis beginning as a prelude to a disastrous economic climate injected with stagflation; the combination between high unemployment and high inflation. However, on November 14, 1972, the average closed above the 1,000 mark (1,003.16) for the first time, during a brief relief rally in the midst of a lengthy bear market. Between January 1973 and December 1974, the average lost 48% of its value in what became known as the 1973–1974 Stock Market Crash; with the situation being exacerbated by the events surrounding the Yom Kippur War. The index closed at 577.60, on December 4, 1974. During 1976, the index went above 1000 several times, and it closed the year at 1,004.75. Although the Vietnam War ended in 1975, new tensions arose towards Iran surrounding the Iranian Revolution in 1979. Other notable disturbances such as the Lebanese Civil War, the Ethiopian Civil War, the Indo-Pakistani War of 1971 and the Angolan Civil War which the U.S. and Soviet Union considered critical to the global balance of power, seemed to have had little influence towards the financial markets. Performance-wise for the decade, gains remained virtually flat, rising less than 5% from about the 800 level to 838.
The 1980s decade started with the early 1980s recession. In early 1981, it broke above 1000 several times, but then went down. The index closed at 776.92, on August 12, 1982, which is below the close of 1965, and is usually considered the start of a secular bull. By October, it went above 1000 again, and closed the year above, permanently. The largest one-day percentage drop occurred on Black Monday; October 19, 1987, when the average fell 22.61%. There were no clear reasons given to explain the crash, but program trading may have been a major contributing factor. On October 13, 1989, the Dow stumbled into another downfall, the 1989 Mini-Crash which initiated the collapse of the junk bond market as the Dow registered a loss of almost 7%.
For the decade, the Dow made a 228% increase from the 838 level to 2,753; despite the market crashes, Silver Thursday, an early 1980s recession, the 1980s oil glut, the Japanese asset price bubble and other political distractions such as the Soviet war in Afghanistan, the Falklands War, the Iran-Iraq War, the Second Sudanese Civil War and the First Intifada in the Middle East. The index had only two negative years, which were in 1981 and 1984.
Dot-com boom.
The 1990s brought on rapid advances in technology along with the introduction of the dot-com era. To start off, the markets contended with the 1990 oil price shock compounded with the effects of the Early 1990s recession and a brief European situation surrounding Black Wednesday. Certain influential foreign conflicts such as the 1991 Soviet coup d'état attempt which took place as part of the initial stages of the Dissolution of the USSR and the Fall of Communism; the First and Second Chechen Wars, the Persian Gulf War and the Yugoslav Wars failed to dampen economic enthusiasm surrounding the ongoing Information Age and the "irrational exuberance" (a phrase coined by Alan Greenspan) of the Internet Boom. Even the occurrences of the Rwandan Genocide and the Second Congo War, termed as "Africa's World War" that involved 8 separate African nations which together between the two killed over 5 million people; didn't seem to have any noticeable negative financial impact on the Dow either. Between late 1992 and early 1993, the Dow staggered through the 3,000 level making only modest gains as the Biotechnology sector suffered through the downfall of the Biotech Bubble; as many biotech companies saw their share prices rapidly rise to record levels and then subsequently fall to new all-time lows.
On November 21, 1995, the DJIA closed above the 5,000 level (5,023.55) for the first time. Over the following two years, the Dow would rapidly tower above the 6,000 level during the month of October in 1996, and the 7,000 level in February 1997. On its march higher into record territory, the Dow easily made its way through the 8,000 level in July 1997. However, later in that year during October, the events surrounding the Asian Financial Crisis plunged the Dow into a 554-point loss to a close of 7,161.15; a retrenchment of 7.18% in what became known as the 1997 Mini-Crash. Although internationally there was negativity surrounding the 1998 Russian financial crisis along with the subsequent fallout from the 1998 collapse of the derivatives Long-Term Capital Management hedge fund involving bad bets placed on the movement of the Russian ruble, the Dow would go on to surpass the 9,000 level during the month of April in 1998, making its sentimental push towards the symbolic 10,000 level. On March 29, 1999, the average closed above the 10,000 mark (10,006.78) after flirting with it for two weeks. This prompted a celebration on the trading floor, complete with party hats. The scene at the exchange made front page headlines on many U.S. newspapers such as "The New York Times". On May 3, 1999, the Dow achieved its first close above the 11,000 mark (11,014.70). Total gains for the decade exceeded 315%; from the 2,753 level to 11,497.
The Dow averaged a 5.3% return compounded annually for the 20th century, a record Warren Buffett called "a wonderful century"; when he calculated that to achieve that return again, the index would need to close at about 2,000,000 by December 2099. Even during the height of the dot-com era, authors James K. Glassman and Kevin A. Hassett went so far as to publish a book entitled "Dow 36,000: The New Strategy for Profiting From the Coming Rise in the Stock Market". Their theory was to imply that stocks were still cheap and it was not too late to benefit from rising prices during the Internet boom.
Characterized by fear on the part of newer investors, the uncertainty of the 2000s (decade) brought on a significant bear market. There was indecision on whether the cyclical bull market represented a prolonged temporary bounce or a new long-term trend. Ultimately, there was widespread resignation and disappointment as the lows were revisited, and in some cases, surpassed near the end of the decade.
Post internet-bubble era.
The third largest one-day point drop in DJIA history, and largest at the time, occurred on September 17, 2001, the first day of trading after the September 11, 2001 attacks, when the Dow fell 684.81 points, or 7.1%. However, the Dow had been in a downward trend for virtually all of 2001 prior to September 11, losing well over 1000 points between January 2 and September 10, and had lost 187.51 points on September 6, followed by losing 235.4 points on September 7. By the end of that week, the Dow had fallen 1,369.70 points, or 14.3%. However, the Dow began an upward trend shortly after the attacks, and quickly regained all lost ground to close to above the 10,000 level for the year.
During 2002, the average remained subdued without making substantial gains due to the stock market downturn of 2002 as well as the lingering effects of the dot-com bubble. In 2003, the Dow held steady within the 7,000 to 9,000-point level range by the early 2000s recession, the Afghan War and the Iraq War. But by December of that year, the Dow remarkably returned to the 10,000 mark. In October 2006, four years after its bear market low, the DJIA set fresh record theoretical, intra-day, daily close, weekly, and monthly highs for the first time in almost seven years, closing above the 12,000 level for the first time on the 19th anniversary of Black Monday (1987). On February 27, 2007, the Dow Jones Industrial Average fell 3.3% (415.30 points), its biggest point drop since 2001. The initial drop was caused by a global sell-off after Chinese stocks experienced a mini-crash, yet by April 25, the Dow passed the 13,000 level in trading and closed above that milestone for the first time. On July 19, 2007, the average passed the 14,000 level, completing the fastest 1,000-point advance for the index since 1999. One week later, a 450-point intra-day loss, owing to turbulence in the U.S. sub-prime mortgage market and the soaring value of the yuan, initiated another correction falling below the 13,000 mark, about 10% from its highs.
On October 9, 2007, the Dow Jones Industrial Average closed at a record high of 14,164.53. Two days later on October 11, the Dow traded at an intra-day level high of 14,198.10, a mark which would not be matched until March 2013. In what would normally take many years to accomplish; numerous reasons were cited for the Dow's extremely rapid rise from the 11,000 level in early 2006, to the 14,000 level in late 2007. They included future possible takeovers and mergers, healthy earnings reports particularly in the tech sector, and moderate inflationary numbers; fueling speculation the Federal Reserve would not raise interest rates.
On September 15, 2008, a wider financial crisis became evident when Lehman Brothers filed for Chapter 11 bankruptcy along with the economic effect of record high oil prices which reached almost $150 per barrel two months earlier. When opening that morning, it immediately lost 300 points and overall the DJIA lost more than 500 points for only the sixth time in history, returning to its mid-July lows below the 11,000 level. A series of "bailout" packages, including the Emergency Economic Stabilization Act of 2008, proposed and implemented by the Federal Reserve and U.S. Treasury, as well as FDIC-sponsored bank mergers, did not prevent further losses. After nearly six months of extreme volatility during which the Dow experienced its largest one day point loss, largest daily point gain, and largest intra-day range (more than 1,000 points), the index closed at a new twelve-year low of 6,547.05 on March 9, 2009 (after an intra-day low of 6,469.95 during the March 6 session), its lowest close since April 1997, and had lost 20% of its value in only six weeks.
Bull market of 2009–present.
Towards the latter half of 2009, the average rallied towards the 10,000 level amid optimism that the Late-2000s (decade) Recession, the United States Housing Bubble and the Global Financial Crisis of 2008–2009, were easing and possibly coming to an end. For the decade, the Dow saw a rather substantial pullback for a negative return from the 11,497 level to 10,428, a loss of a little over 9%.
During the early part of the 2010s, aided somewhat by the loose monetary policy practiced by the Federal Reserve, the Dow made a notable rally attempt, though with significant volatility due to growing global concerns such as the 2010 European sovereign debt crisis, the Dubai debt crisis, and the United States debt ceiling crisis. On May 6, 2010, the index lost around 400 points over the day, then just after 2:30 pm EDT, it lost about 600 points in just a few minutes, and gained the last amount back about as quickly. The intra-day change at the lowest point was 998.50 points, the largest intra-day point decline ever, representing an intra-day loss of 9.2%. The event, during which the Dow bottomed out at 9,869 before recovering to end with a 3.2% daily loss at 10,520.32, became known as the 2010 Flash Crash. The index closed the half-year at 9,774.02 for a loss of 7.7%.
On May 3, 2013, the Dow surpassed the 15,000 mark for the first time, while later on November 18, it closed above the 16,000 level. Following a strong jobs report on July 3, 2014, the Dow traded above the 17,000 mark for the first time. On December 23, 2014, the DJIA traded above the 18,000 boundary for the first time, after data showed the U.S. economy posted its strongest growth in more than a decade. The index closed 2014 at 17,823.07 for a gain of 71% for the five years.
Investing.
Investing in the DJIA is made widely accessible in equities through exchange-traded funds (ETFs) as well as in derivatives through option contracts and futures contracts.
Exchange-traded fund.
The index is tracked by an exchange-traded fund, the SPDR Dow Jones Industrial Average (NYSE Arca: [ DIA]), commonly called "diamonds". This fund is part of the SPDR family of ETFs from State Street Global Advisors. This fund was introduced in 1998, and it was previously called DIAMONDS Trust, Series 1.
Due to the advent of pre-market trading, the "diamonds" provide a very accurate opening value for the average. As an example, if the ETF opens the trading session with a 76¢ loss; then that would strongly indicate roughly a 76-point loss for the Dow within the first few seconds or so, even before all of its components open for trade. Likewise, if the ETF starts the trading session higher by $1.12, then that would signal an approximate gain for the Dow of 112 points at the open, even if some components begin trading at 9:31 am or 9:33 am due to a delay.
Leveraged and short funds.
Another asset management firm, ProFunds, issue other related DJIA ETFs through such as the "Inverse Performance" (NYSE Arca: [ DOG]) for a bearish strategy on the average. That is, when the Dow trades in negative territory, the ETF trades higher; thus, making it not needed to sell short if one has a bearish goal in mind.
ProFunds also issues the "2x" (NYSE Arca: [ DDM]), which attempts to match the daily performance of the DJIA by 200% and the "Inverse 2x" (NYSE Arca: [ DXD]), which attempts to match the inverse daily performance by 200%. In the case of 2x performance, the ETF increases the buying power by leveraging money without using margin. Currently, there are also "3x" performance ETFs issued by ProShares that exist too; which attempt to replicate (300% leverage) against the Dow. For "3x" performance, the symbol is (NYSE Arca: [ UDOW]), and for "Inverse 3x" performance, it is (NYSE Arca: [ SDOW]).
Futures contracts.
In the derivatives market, the CME Group through its subsidiaries the Chicago Mercantile Exchange (CME) and the Chicago Board of Trade (CBOT), issues Futures Contracts; including the , the and the which track the average and trade on their exchange floors respectively. Trading is typically carried out in an Open Outcry auction, or over an electronic network such as CME's Globex platform.
Options contracts.
The Chicago Board Options Exchange (CBOE) issues Options Contracts on the Dow through the root symbol in combination with long term expiration options called . There are also options on the various ETFs; , 
,
, and 
Calculation.
To calculate the DJIA, the sum of the prices of all 30 stocks is divided by a divisor, the Dow Divisor. The divisor is adjusted in case of stock splits, spinoffs or similar structural changes, to ensure that such events do not in themselves alter the numerical value of the DJIA. Early on, the initial divisor was composed of the original number of component companies; which made the DJIA at first, a simple arithmetic average. The present divisor, after many adjustments, is less than one (meaning the index is larger than the sum of the prices of the components).
That is:
where p are the prices of the component stocks and d is the "Dow Divisor".
Events like stock splits or changes in the list of the companies composing the index alter the sum of the component prices. In these cases, in order to avoid discontinuity in the index, the Dow Divisor is updated so that the quotations right before and after the event coincide:
The Dow Divisor was 0.15571590501117 on September 27, 2013. Presently, every $1 change in price in a particular stock within the average, equates to a 6.42 (1/0.15571590501117) point movement.
Assessment.
Market performance.
With the current inclusion of only 30 stocks, critics like Ric Edelman argue that the DJIA is not a very accurate representation of overall market performance. Still, it is the most cited and most widely recognized of the stock market indices. Additionally, the DJIA is criticized for being a price-weighted average, which gives higher-priced stocks more influence over the average than their lower-priced counterparts, but takes no account of the relative industry size or market capitalization of the components. For example, a $1 increase in a lower-priced stock can be negated by a $1 decrease in a much higher-priced stock, even though the lower-priced stock experienced a larger percentage change. In addition, a $1 move in the smallest component of the DJIA has the same effect as a $1 move in the largest component of the average. For example, during September–October 2008, former component AIG's reverse split-adjusted stock price collapsed from $22.76 on September 8 to $1.35 on October 27; contributing to a roughly 3,000-point drop in the index. 
As of March 2015, Goldman Sachs and 3M are among the highest priced stocks in the average and therefore have the greatest influence on it. Alternately, Cisco Systems and General Electric are among the lowest priced stocks in the average and have the least amount of sway in the price movement. Many critics of the DJIA recommend the float-adjusted market-value weighted S&P 500 or the Wilshire 5000, the latter of which includes all U.S. equity securities, as better indicators of the U.S. stock market.
Correlation among components.
A study between the correlation of components of the Dow Jones Industrial Average compared with the movement of the index, finds that the correlation is higher in a time period where the average recedes and goes down. The correlation is lowest in a time when the average is flat or rises a modest amount.

</doc>
<doc id="47363" url="http://en.wikipedia.org/wiki?curid=47363" title="Council of Ephesus">
Council of Ephesus

The Council of Ephesus was a council of Christian bishops convened in Ephesus (near present-day Selçuk in Turkey) in AD 431 by the Roman Emperor Theodosius II. This third ecumenical council, an effort to attain consensus in the church through an assembly representing all of Christendom, confirmed the original Nicene Creed
, and condemned the teachings of Nestorius, Patriarch of Constantinople that the Virgin Mary may be called the "Christotokos", "Birth Giver of Christ" but not the "Theotokos", "Birth Giver of God". It met in June and July 431 at the Church of Mary in Ephesus in Anatolia.
Background.
Nestorius' doctrine, Nestorianism, which emphasized the distinction between Christ's human and divine natures and argued that Mary should be called "Christokos" (Christ-bearer) but not "Theotokos" (God-bearer), had brought him into conflict with other church leaders, most notably Cyril, Patriarch of Alexandria. Nestorius himself had requested that the Emperor convene council, hoping to prove his orthodoxy, but in the end his teachings were condemned by the council as heresy. The council declared Mary as "Theotokos" (God-bearer).
Nestorius' dispute with Cyril had led the latter to seek validation from Pope Celestine I, who authorized Cyril to request that Nestorius recant his position or face excommunication. Nestorius pleaded with the Eastern Roman Emperor Theodosius II to call a council in which all grievances could be aired, hoping that he would be vindicated and Cyril condemned.
Approximately 250 bishops were present. The proceedings were conducted in a heated atmosphere of confrontation and recriminations and created severe tensions between Cyril and Theodosius II. Nestorius was decisively outplayed by Cyril and removed from his see, and his teachings were officially anathematized. This precipitated the Nestorian Schism, by which churches supportive of Nestorius, especially in Persia, were severed from the rest of Christendom and became known as Nestorian Christianity, the Persian Church, or the Church of the East, whose present-day representatives are the Assyrian Church of the East, the Chaldean Syrian Church, the Ancient Church of the East, and the Chaldean Catholic Church. Nestorius himself retired to a monastery, always asserting his orthodoxy.
History.
Political context.
McGuckin cites the "innate rivalry" between Alexandria and Constantinople as an important factor in the controversy between Cyril of Alexandria and Nestorius. However, he emphasizes that, as much as political competition contributed to an "overall climate of dissent", the controversy cannot be reduced merely to the level of "personality clashes" or "political antagonisms". According to McGuckin, Cyril viewed the "elevated intellectual argument about christology" as ultimately one and the same as the "validity and security of the simple Christian life".
Even within Constantinople, some supported the Roman-Alexandrian and others supported the Nestorian factions. For example, Pulcheria supported the Roman-Alexandrian popes while the emperor and his wife supported Nestorius.
Theological context.
Contention over Nestorius' teachings, which he developed during his studies at the School of Antioch, largely revolved around his rejection of the long-used title "Theotokos" ("Birth-giver of God") for the Virgin Mary. Shortly after his arrival in Constantinople, Nestorius became involved in the disputes of two theological factions, which differed in their Christology.
McGuckin ascribes Nestorius' importance to his being the representative of the Antiochene tradition and characterizes him as a "consistent, if none too clear, exponent of the longstanding Antiochene dogmatic tradition." Nestorius was greatly surprised that what he had always taught in Antioch without any controversy whatsoever should prove to be so objectionable to the Christians of Constantinople.
Nestorius emphasized the dual natures of Christ, trying to find a middle ground between those that emphasized the fact that in Christ God had been born as a man, insisted on calling the Virgin Mary "Theotokos" (Greek: Θεοτόκος, "God-bearer"), and those that rejected that title because God as an eternal being could not have been born. Nestorius suggested the title "Christotokos" ("Χριστοτόκος", "Christ-bearer"), but this proposal did not gain acceptance on either side.
Nestorius tried to answer a question considered unsolved: "How can Jesus Christ, being part man, not be partially a sinner as well, since man is by definition a sinner since the Fall". To solve that he taught that Mary, the mother of Jesus gave birth to the incarnate Christ, not the divine Logos who existed before Mary and indeed before time itself. The Logos occupied the part of the human soul (the part of man that was stained by the Fall). But wouldn't the absence of a human soul make Jesus less human? Nestorius rejected this proposition, answering that, because the human soul was based on the archetype of the Logos, only to become polluted by the Fall, Jesus was "more" human for having the Logos and not "less". Consequently, Nestorius argued that the Virgin Mary should be called "Christotokos", Greek for "Birth Giver of Christ", and not "Theotokos", Greek for "Birth Giver of God".
Nestorius believed that no union between the human and divine was possible. If such a union of human and divine occurred, Nestorius believed that Christ could not truly be con-substantial with God and con-substantial with us because he would grow, mature, suffer and die (which Nestorius argued God cannot do) and also would possess the power of God that would separate him from being equal to humans.
According to McGuckin, several mid-twentieth-century accounts have tended to "romanticise" Nestorius; in opposition to this view, he asserts that Nestorius was no less dogmatic, uncompromising than Cyril and that he was fully just as prepared to use his political and canonical powers as Cyril or any of the other hierarchs of the period.
Nestorius's opponents charged him with detaching Christ's divinity and humanity into two persons existing in one body, thereby denying the reality of the Incarnation. Eusebius, a layman who later became the bishop of the neighbouring Dorylaeum was the first to accuse Nestorius of heresy but his most forceful opponent however was Patriarch Cyril of Alexandria. Cyril argued that Nestorianism split Jesus in half and denied that he was both human and divine.
Cyril appealed to Pope Celestine I, charging Nestorius with heresy. The Pope agreed and gave Cyril his authority to serve a notice to Nestorius to recant his views within ten days or else be excommunicated. Before acting on the Pope's commission, Cyril convened a synod of Egyptian bishops which condemned Nestorius as well. Cyril then sent four suffragan bishops to deliver both the Pope's commission as well as the synodal letter of the Egyptian bishops. Cyril sent a letter to Nestorius known as "The Third Epistle of Saint Cyril to Nestorius." This epistle drew heavily on the established Patristic Constitutions and contained the most famous article of Alexandrian Orthodoxy: "The Twelve Anathemas of Saint Cyril." In these anathemas, Cyril excommunicated anyone who followed the teachings of Nestorius. For example, "Anyone who dares to deny the Holy Virgin the title "Theotokos" is Anathema!" Nestorius however, still would not repent.
McGuckin points out that other representatives of the Antiochene tradition such as John of Antioch, Theodoret and Andrew of Samosata were able to recognize "the point of the argument for Christ's integrity" and concede the "ill-advised nature of Nestorius' immoveability." Concerned at the potential for a negative result at a council, they urged Nestorius to yield and accept the use of the title "Theotokos" when referring to the Virgin Mary.
For example, John of Antioch wrote to Nestorius urging him to submit to the Pope's judgment and cease stirring up controversy over a word that he disliked (Theotokos) but which could be interpreted as having an orthodox meaning especially in light of the fact that many saints and doctors of the church had sanctioned the word by using it themselves. John wrote to Nestorius, "Don't lose your head. Ten days! It will not take you twenty-four hours to give the needed answer... Ask advice of men you can trust. Ask them to tell you the facts, not just what they think will please you... You have the whole of the East against you, as well as Egypt." Despite this advice from his colleagues, Nestorius persisted in maintaining the rightness of his position.
Convocation.
On 19 November, Nestorius, anticipating the ultimatum which was about to be delivered, convinced Emperor Theodosius II to summon a general council through which Nestorius hoped to convict Cyril of heresy and thereby vindicate his own teachings. Theodosius issued a Sacra calling for the metropolitan bishops to assemble in the city of Ephesus, which was a special seat for the veneration of Mary, where the "theotokos" formula was popular. Each bishop was to bring only his more eminent suffragans. The date set by the Emperor for the opening of the council was Pentecost (7 June) 431.
McGuckin notes that the vagueness of the Sacra resulted in wide variations of interpretation by different bishops. In particular, the vastness of John of Antioch's ecclesiastical territory required a lengthy period to notify and gather his delegates. Because the overland trip from Antioch to Ephesus was long and arduous, John composed his delegation of his metropolitan bishops who were restricted to bring no more than two suffragans each. By doing so, he minimized the number who would have to travel to Ephesus.
Neither of the emperors attended the council. Theodosius appointed Count Candidian as the head of the imperial palace guard to represent him, to supervise the proceedings of the Council, and to keep good order in the city of Ephesus. Despite Nestorius' agenda of prosecuting Cyril, Theodosius intended for the council to focus strictly on the christological controversy. He thus gave Candidianus strict directions to remain neutral and not to interfere in the theological proceedings. It is generally assumed that Candidian initially maintained his neutrality as instructed by the emperor and only gradually became more biased towards Nestorius. McGuckin, however, suggests that Candidian may have favored Nestorius from the start.
Assembly.
Celestine sent Arcadius and Projectus, to represent himself and his Roman council; in addition, he sent the Roman priest, Philip, as his personal representative. Cyril Patriarch of Alexandria was president of the council. Celestine had directed the papal legates not to take part in the discussions, but to give judgment on them.
Bishops arrived in Ephesus over a period of several weeks. While waiting for the other bishops to arrive, they engaged in informal discussions characterized as tending to "exasperate rather than heal their differences". The metropolitan of Ephesus, Memnon, was already present with his 52 bishops. Nestorius and his 16 bishops were the first to arrive shortly after Easter. As archbishop of the imperial city of Constantinople, he traveled with a detachment of troops who were under the command of Count Candidian. McGuckin notes that the troops were not there to serve as Nestorius' bodyguard but to support Candidian in his role as the emperor's representative. However, McGuckin theorizes that Candidian's progressive abandonment of neutrality in favor of Nestorius may have created the perception that Candidian's troops were, in fact, there to support Nestorius. Candidian ordered all monks and lay strangers to leave the city; he further instructed the bishops not to leave on any pretext until the council was concluded. Several sources comment that the purpose of this injunction was to prevent bishops from leaving the council to appeal to the emperor directly.
According to McGuckin, Memnon, as bishop of Ephesus, commanded the "fervent and unquestioned loyalty" of the local populace and thus could count on the support of local factions to counterbalance the military might of Candidian's troops. In view of the verdict of Rome against Nestorius, Memnon refused to have communion with Nestorius, closing the churches of Ephesus to him.
Cyril brought with him 50 bishops, arriving only a few days before Pentecost. There were very few bishops representing the West, the papal representatives did not arrive until July. The Palestinian delegation of 16 bishops and Metropolitan Flavian of Philippi arrived 5 days after the date that had been set for opening the council, and aligned themselves with Cyril.
At this point, Cyril announced his intention to open the council; however, Candidian enjoined him from doing so on the grounds that the Roman and Antiochean delegations had not arrived yet. Cyril initially acceded to Candidian's injunction knowing that he could not legally convene a council without the official reading of the Emperor's Sacra.
A number of bishops, who were undecided between Nestorius and Cyril, did not want to give Cyril, as one party in the dispute, the right to chair the meeting and decide the agenda; however, they began to take Cyril's side for various reasons.
Various circumstances including a detour necessitated by flooding as well as sickness and death of some of the delegates seriously delayed John of Antioch and his bishops. It was rumored that John might be delaying his arrival in order to avoid participating in a council which was likely to condemn Nestorius as a heretic.
First session - June 22.
Two weeks after the date set for the council, John and the bulk of his Syrian group (42 members) had not yet appeared. At this point, Cyril formally opened the council on Monday 22 June by enthroning the Gospels in the centre of the church, as a symbol of Christ's presence among the assembled bishops.
Despite three separate summons, Nestorius refused to acknowledge Cyril's authority to stand in judgment of him and considered the opening of the council before the arrival of the Antiochene contingent as a "flagrant injustice". The 68 bishops who opposed the opening the council entered the church in protest, arriving with Count Candidian who declared that the assembly was illegal and must disperse. He urged Cyril to wait four more days for the Syrian delegation to arrive. However, since even the bishops opposed to opening of the council were now present, Cyril maneuvered Candidian by means of a ruse to read out the text of the Emperor's decree of convocation, which the assembly then acclaimed as recognition of its own legality.
Arrival of the Antiochene delegation.
When John of Antioch and his Syrian bishops finally reached Ephesus five days after the council, they met with Candidian who informed them that Cyril had begun a council without them and had ratified Celestine's conviction of Nestorius as a heretic. Angered at having undertaken such a long and arduous journey only to have been pre-empted by actions taken by Cyril's council, John and the Syrian bishops held their own Council with Candidian presiding. This council condemned Cyril for espousing the Arian, Apollinarian and Eunomian heresies and condemned Memnon for inciting violence. The bishops at this council deposed both Cyril and Memnon. Initially, the emperor concurred with the actions of John's council but eventually withdrew his concurrence.
Second Session - July 10.
The second session was held in Memnon's episcopal residence. Philip, as papal legate, opened the proceedings by commenting that the present question regarding Nestorius had already been decided by Pope Celestine as evidenced by his letter, which had been read to the assembled bishops in the first session. He indicated that he had a second letter from Celestine which was read to the bishops now in attendance. The letter contained a general exhortation to the council, and concluded by saying that the legates had instructions to carry out what the pope had decided on the question and expressed Celestine's confidence that the council would agree. The bishops indicated their approval by acclaiming Celestine and Cyril. Projectus indicated that the papal letter enjoined the council to put into effect the sentence pronounced by Celestine. Firmus, the Exarch of Caesarea in Cappadocia, responded that the pope's sentence had already been carried out in the first session. The session closed with the reading of the pope's letter to the emperor.
Third Session - July 11.
Having read the Acts of the first session, the papal legates indicated that all that was required was that the council's condemnation of Nestorius be formally read in their presence. When this had been done, the three legates each confirmed the council's actions, signing the Acts of all three sessions. The council sent a letter to Theodosius indicating that the condemnation of Nestorius had been agreed upon not only by the bishops of the East meeting in Ephesus but also of the bishops of the West who had convened at a synod in Rome convened by Celestine. The bishops asked Theodosius to allow them to go home since so many of them suffered from their presence at Ephesus.
Fourth Session - July 16.
At the fourth session, Cyril and Memnon presented a formal protest against John of Antioch for convening a separate conciliabulum. The council issued a summons for him to appear before them, but he would not even receive the envoys who were sent to serve him the summons.
Fifth Session - July 17.
Next day the fifth session was held in the same church. John had set up a placard in the city accusing the synod of the Apollinarian heresy. He was again cited, and this was counted as the third canonical summons. He paid no attention. In consequence the council suspended and excommunicated him, together with thirty-four bishops of his party, but refrained from deposing them. Some of John's party had already deserted him, and he had gained only a few. In the letters to the emperor and the pope which were then dispatched, the synod described itself as now consisting of 210 bishops. The long letter to Celestine gave a full account of the council, and mentioned that the pope's decrees against the Pelagians had been read and confirmed.
Sixth Session.
At this session, the bishops approved Canon 7 which condemned any departure from the creed established by the First Council of Nicaea, in particular an exposition by the priest Charisius. According to a report from Cyril to Celestine, Juvenal of Jerusalem tried and failed to create for himself a patriarchate from the territory of the Antiochene patriarchate in which his see lay. He ultimately succeeded in this goal twenty years later at the Council of Chalcedon.
Seventh Session - July 31.
At this session, the council approved the claim of the bishops of Cyprus that their see had been anciently and rightly exempt from the jurisdiction of Antioch. The council also passed five canons condemning Nestorius and Caelestius and their followers as heretics and a sixth one decreeing deposition from clerical office or excommunication for those who did not accept the Council's decrees.
Canons and declarations.
The Council denounced Nestorius' teaching as erroneous and decreed that Jesus was one person, not two separate people: complete God and complete man, with a rational soul and body. The Virgin Mary was to be called Theotokos a Greek word that means "God-bearer" (the one who gave birth to God).
The Council declared it "unlawful for any man to bring forward, or to write, or to compose a different (ἑτέραν) Faith as a rival to that established by the holy Fathers assembled with the Holy Ghost in Nicæa". It quoted the Nicene Creed as adopted by the First Council of Nicaea in 325, not as added to and modified by the First Council of Constantinople in 381.
In addition to its condemnation of Nestorianism, the council also condemned Pelagianism. were passed:
Confirmation of the Council's Acts.
The bishops at Cyril's council outnumbered those at John of Antioch's council by nearly four to one. In addition, they had the agreement of the papal legates and the support of the population of Ephesus who supported their bishop, Memnon.
However, Count Candidian and his troops supported Nestorius as did Count Irenaeus. The emperor had always been a firm supporter of Nestorius, but had been somewhat shaken by the reports of the council. Cyril's group was unable to communicate with the emperor because of interference from supporters of Nestorius both at Constantinople and at Ephesus. Ultimately, a messenger disguised as a beggar was able to carry a letter to Constantinople by hiding it in a hollow cane.
Although Emperor Theodosius had long been a staunch supporter of Nestorius, his loyalty seems to have been shaken by the reports from Cyril's council and caused him to arrive at the extraordinary decision to ratify the depositions decreed by both councils. Thus, he declared that Cyril, Memnon, and John were all deposed. Memnon and Cyril were kept in close confinement. But in spite of all the efforts of the Antiochene party, the representatives of the envoys whom the council was eventually allowed to send, with the legate Philip, to the Court, persuaded the emperor to accept Cyril's council as the true one. Seeing the writing on the wall and anticipating his fate, Nestorius requested permission to retire to his former monastery. The synod was dissolved in the beginning of October, and Cyril arrived amid much joy at Alexandria on 30 October. Pope Celestine had died on July 27 but his successor, Sixtus III, gave papal confirmation to the council's actions.
Aftermath.
The events created a major schism between the followers of the different versions of the council, which was only mended by difficult negotiations. The factions that supported John of Antioch acquiesced in the condemnation of Nestorius and, after additional clarifications, accepted the decisions of Cyril's council. However, the rift would open again during the debates leading up to the Council of Chalcedon.
Persia had long been home to a Christian community that had been persecuted by the Zoroastrian majority, which had accused it of Roman leanings. In 424, the Persian Church declared itself independent of the Byzantine and all other churches, in order to ward off allegations of foreign allegiance. Following the Nestorian Schism, the Persian Church increasingly aligned itself with the Nestorians, a measure encouraged by the Zoroastrian ruling class. The Persian Church became increasingly Nestorian in doctrine over the next decades, furthering the divide between Christianity in Persia and in the Roman Empire. In 486 the Metropolitan of Nisibis, Barsauma, publicly accepted Nestorius' mentor, Theodore of Mopsuestia, as a spiritual authority. In 489 when the School of Edessa in Mesopotamia was closed by Byzantine Emperor Zeno for its Nestorian teachings, the school relocated to its original home of Nisibis, becoming again the School of Nisibis, leading to a wave of Nestorian immigration into Persia. The Persian patriarch Mar Babai I (497–502) reiterated and expanded upon the church's esteem for Theodore, solidifying the church's adoption of Nestorianism.
Conciliation.
In 1994, the Common Christological Declaration between the Catholic Church and the Assyrian Church of the East marked the resolution of a dispute between those two churches that had existed since the Council of Ephesus. They expressed their common understanding of doctrine concerning the divinity and humanity of Christ, and recognized the legitimacy and rightness of their respective descriptions of Mary as, on the Assyrian side, "the Mother of Christ our God and Saviour", and, on the Catholic side, as "the Mother of God" and also as "the Mother of Christ".

</doc>
<doc id="47367" url="http://en.wikipedia.org/wiki?curid=47367" title="Æthelred of Wessex">
Æthelred of Wessex

King Æthelred I (Old English: Æþelræd, sometimes rendered as Ethelred, "noble counsel") (c. 847 – 871) was King of Wessex from 865 to 871. He was the fourth son of King Æthelwulf of Wessex. He succeeded his brother, Æthelberht (Ethelbert), as King of Wessex and Kent in 865.
Early life.
In 853 his younger brother Alfred went to Rome, and according to contemporary references in the "Liber Vitae" of San Salvatore, Brescia, Æthelred accompanied him. He first witnessed his father's charters as an Ætheling in 854, and kept this title until he succeeded to the throne in 865. He may have acted as an underking as early as 862, and in 862 and 863 he issued charters as King of the West Saxons. This must have been as deputy or in the absence of his elder brother, King Æthelberht, as there is no record of conflict between them and he continued to witness his brother's charters as a king's son in 864.
Reign.
In the same year as Æthelred's succession as king, a great Viking army arrived in England, and within five years they had destroyed two of the principal English kingdoms, Northumbria and East Anglia. In 868 Æthelred's brother-in-law, Burgred king of Mercia, appealed to him for help against the Vikings. Æthelred and his brother, the future Alfred the Great, led a West Saxon army to Nottingham, but there was no decisive battle, and Burgred bought off the Vikings. In 874 the Vikings defeated Burgred and drove him into exile.
In 870 the Vikings turned their attention to Wessex, and on 4 January 871 at the Battle of Reading, Æthelred suffered a heavy defeat. Although he was able to re-form his army in time to win a victory at the Battle of Ashdown, he suffered further defeats on 22 January at Basing, and 22 March at Meretun.
In about 867, Æthelred effectively established a common currency between Wessex and Mercia by adopting the Mercian type of lunette penny, and coins minted exclusively at London and Canterbury then circulated in the two kingdoms.
Æthelred died shortly after Easter (15 April) 871, and is buried at Wimborne Minster in Dorset. He was succeeded by his younger brother, Alfred the Great.
Family.
His wife was probably called Wulfthryth. A charter of 868 refers to "Wulfthryth regina" (queen). It was rare in ninth century Wessex for the king's wife to be given the title queen, and it is only definitely known to have been given to Æthelwulf's second wife, Judith of Flanders. Historians Barbara Yorke and Pauline Stafford, and the Prosopography of Anglo-Saxon England, treat the charter as showing that Wulfthryth was Æthelred's queen. She may have been the daughter or sister of Ealdorman Wulfhere of Wiltshire, who forfeited his lands charged with deserting King Alfred for the Danes in about 878. However, Sean Miller in his Oxford Online DNB article on Æthelred does not mention her. Simon Keynes and Michael Lapidge in the notes to their 1983 edition of Asser's "Life of King Alfred the Great" refer to a "mysterious 'Wulfthryth regina'", but Keynes stated in 1994 that she was "presumably the wife of King Æthelred". 
He had two known sons, Æthelhelm and Æthelwold. Æthelwold disputed the throne with Edward the Elder after Alfred's death in 899. Æthelred's descendants include the tenth-century historian, Æthelweard, and Æthelnoth, an eleventh-century Archbishop of Canterbury.

</doc>
<doc id="47369" url="http://en.wikipedia.org/wiki?curid=47369" title="Fionn mac Cumhaill">
Fionn mac Cumhaill

Fionn mac Cumhaill ( ; ]; Old Irish: "Find mac Cumail" or "Umaill"), sometimes transcribed in English as Finn MacCool or Finn MacCoul, was a mythical hunter-warrior of Irish mythology, occurring also in the mythologies of Scotland and the Isle of Man. The stories of Fionn and his followers the Fianna, form the Fenian Cycle ("an Fhiannaíocht"), much of it narrated in the voice of Fionn's son, the poet Oisín.
"Fionn" means "blond", "fair", "white", or "bright". The hero's childhood name was Deimne (; ]), literally "sureness" or "certainty", also a name that means a young male deer; several legends tell how he gained the name Fionn when his hair turned prematurely white. The name Fionn is related to the Welsh name Gwyn, as in the mythological figure Gwyn ap Nudd, and to the continental Celtic Vindos, a form of the god Belenus.
The 19th-century Irish revolutionary organisation known as the Fenian Brotherhood took its name from these legends. The Scottish name Fingal comes from a retelling of the legends in epic form by the 18th-century poet James Macpherson.
Legend.
Birth.
Most of Fionn's early adventures are recounted in the narrative "The Boyhood Deeds of Fionn". He was the son of Cumhall, leader of the Fianna, and Muirne, daughter of the druid Tadg mac Nuadat who lived on the hill of Almu in County Kildare. Cumhall abducted Muirne after her father refused him her hand, so Tadg appealed to the High King, Conn of the Hundred Battles, who outlawed Cumhall. The Battle of Cnucha was fought between Conn and Cumhall, and Cumhall was killed by Goll mac Morna, who took over leadership of the Fianna. Fionn Mac Cumhaill was said to be originally from Ballyfin, in Laois. The direct translation of Ballyfin from Irish to English is 'town of Fionn'.
Muirne was already pregnant; her father rejected her and ordered his people to burn her, but Conn would not allow it and put her under the protection of Fiacal mac Conchinn, whose wife, Bodhmall the druid, was Cumhall's sister. In Fiacal's house Muirne gave birth to a son, whom she called Deimne.
Boyhood.
Muirne left the boy in the care of Bodhmall and a fighting woman, Liath Luachra, and they brought him up in secret in the forest of Sliabh Bladma, teaching him the arts of war and hunting. As he grew older he entered the service – incognito – of a number of local kings, but each one, when he recognised Fionn as Cumhal's son, he told him to leave, fearing they would be unable to protect him from his enemies.
The young Fionn met the leprechaun-like druid and poet Finn Eces, or Finnegas, near the river Boyne and studied under him. Finnegas had spent seven years trying to catch the Salmon of Knowledge, which lived in a pool on the Boyne and became all-knowing through its diets of hazelnuts from a holy tree: whoever ate the salmon would gain all the knowledge in the world. Eventually Finn Eces caught it, and told the boy to cook it for him. While cooking it Deimne burned his thumb, and instinctively put his thumb in his mouth. This imbued him with the salmon's wisdom, and when Finn Eces saw that he had gained wisdom, he gave young Fionn the whole salmon to eat. Fionn then knew how to gain revenge against Goll, and in subsequent stories was able to call on the knowledge of the salmon by putting his thumb to the tooth that had first tasted the salmon. The story of Fionn and the salmon of knowledge and the Welsh tale of Gwion Bach are similar.
Adulthood.
Every year for 23 years at Samhain, a fire-breathing man of the Sidhe, Aillen, would lull the men of Tara to sleep with his music before burning the palace to the ground, and the Fianna, led by Goll mac Morna, were powerless to prevent it. Fionn arrived at Tara, armed with his father's crane-skin bag of magical weapons. He kept himself awake by touching the point of his magically red-hot spear to his forehead. The pain kept Fionn awake, allowing him to pursue and kill Aillen with the same spear. After that his heritage was recognised and he was given command of the Fianna: Goll willingly stepped aside, and became a loyal follower of Fionn, although in some stories their alliance is uneasy. Fionn demanded compensation for his father's death from Tadg, threatening war or single combat against him if he refused. Tadg offered him his home, the hill of Allen, as compensation, which Fionn accepted.
Love life.
Fionn met his most famous wife, Sadhbh, when he was out hunting. She had been turned into a deer by a druid, Fear Doirich, whom she had refused to marry. Fionn's hounds, Bran and Sceólang, born of a human enchanted into the form of a hound, recognised her as human, and Fionn brought her home. She transformed back into a woman the moment she set foot on Fionn's land, as this was the one place she could regain her true form. She and Fionn married and she was soon pregnant. When Fionn was away defending his country, Fear Doirich (literally meaning Dark Man) returned and turned her back into a deer, whereupon she vanished. Fionn spent years searching for her, but to no avail. Bran and Sceólang, again hunting, found her son, Oisín, in the form of a fawn; he transformed into a child, and went on to be one of the greatest of the Fianna.
In "The Pursuit of Diarmuid and Gráinne" the High King Cormac mac Airt promises the ageing Fionn his daughter Gráinne, but at the wedding feast Gráinne falls for one of the Fianna, Diarmuid Ua Duibhne, noted for his beauty. She forces him to run away with her and Fionn pursues them. The lovers are helped by the Fianna, and by Diarmuid's foster-father, the god Aengus. Eventually Fionn makes his peace with the couple. Years later, however, Fionn invites Diarmuid on a boar hunt, and Diarmuid is gored. Water drunk from Fionn's hands has the power of healing, but each time Fionn gathers water he lets it run through his fingers before he gets back to Diarmuid. His grandson Oscar shames Fionn, but when he finally returns with water it is too late; Diarmuid has died.
Death.
According to the most popular account of Fionn's death, he is not dead at all, rather, he sleeps in a cave, surrounded by the Fianna. One day they will awake and defend Ireland in the hour of her greatest need. In one account, it is said they will arise when the Dord Fiann, the hunting horn of the Fianna, is sounded three times, and they will be as strong and as well as they ever were. Another tradition states that he is buried in the crypt of Lund Cathedral in Sweden.
Fionn as a giant.
Many geographical features in Ireland are attributed to Fionn. Legend has it he built the Giant's Causeway as stepping-stones to Scotland, so as not to get his feet wet; he also once scooped up part of Ireland to fling it at a rival, but it missed and landed in the Irish Sea — the clump became the Isle of Man and the pebble became Rockall, the void became Lough Neagh. Fingal's Cave in Scotland is also named after him, and shares the feature of hexagonal basalt columns with the nearby Giant's Causeway in Northern Ireland.
In both Irish and Manx popular folklore, Fionn mac Cumhail (known as "Finn McCool" or "Finn MacCooill" respectively) is portrayed as a magical, benevolent giant. The most famous story attached to this version of Fionn tells of how one day, while making a pathway in the sea towards Scotland – The Giant's Causeway – Fionn is told that the giant Benandonner (or, in the Manx version, a buggane) is coming to fight him. Knowing he cannot withstand Benandonner due to his size, Fionn asks his wife Oona to help him. She dresses her husband as a baby, and he hides in a cradle; then she makes a batch of griddle-cakes, hiding griddle-irons in some. When Benandonner arrives, Oona tells him Fionn is out but will be back shortly. As Benandonner waits, he tries to intimidate Oona with his immense power, breaking rocks with his little finger. Oona then offers Benandonner a griddle-cake, but when he bites into the iron he chips his teeth. Oona scolds him for being weak (saying her husband eats such cakes easily), and feeds one without an iron to the 'baby', who eats it without trouble.
In the Irish version, Benandonner is so awed by the power of the baby's teeth and the size of the baby that, at Oona's prompting, he puts his fingers in Fionn's mouth to feel how sharp his teeth are. Fionn bites Benandonner's little finger, and scared of the prospect of meeting his father considering the baby's size, Benandonner runs back towards Scotland across the Causeway smashing the causeway so Fionn couldn't follow him.
The Manx Gaelic version contains a further tale of how Fionn and the buggane battle at Kirk Christ Rushen. Fionn's feet carve out the channels between the Calf of Man and Kitterland and between Kitterland and the Isle of Man, while the buggane's feet make an opening for the port at Port Erin. The buggane injures Fionn, who flees over the sea (where the buggane cannot follow), but the buggane tears out one of his own teeth and strikes Fionn as he runs away. The tooth falls into the sea, becoming the Chicken Rock, and Fionn curses the tooth, explaining why it is a hazard to sailors.
In Newfoundland, and some parts of Nova Scotia, "Fingal's Rising" is spoken of in a distinct nationalistic sense. Made popular in songs and bars alike, to speak of "Fingle," as his name is pronounced in English versus "Fion MaCool" in Newfoundland Irish, is sometimes used as a stand-in for Newfoundland or its culture.
Modern literature.
In 1761 James Macpherson announced the discovery of an epic written by Ossian (Oisín) in the Scottish Gaelic language on the subject of "Fingal" ("Fionnghall" meaning "white stranger"). It is suggested that Macpherson rendered the name as Fingal through a misapprehension of the name which in old Gaelic would appear as Fionn). In December 1761 he published "Fingal, an Ancient Epic Poem in Six Books, together with Several Other Poems composed by Ossian, the Son of Fingal, translated from the Gaelic Language". His cycle of poems was wildly popular and had widespread influence on such writers as Goethe and the young Walter Scott, but there was controversy from the outset about Macpherson's claims to have translated the works from ancient sources. The authenticity of the poems is now generally doubted, though they may have been based on fragments of Gaelic legend, and to some extent the controversy has overshadowed their considerable literary merit and influence on Romanticism.
"Glencoe: The Story of the Massacre" by John Prebble (Secker & Warburg, 1966), has an account of a legendary battle between Fionn mac Cumhaill, who supposedly lived for a time in Glencoe (in Scotland), and a Viking host in forty longships which sailed up the narrows by Ballachulish into Loch Leven. The Norsemen were defeated by the Feinn of the valley of Glencoe, and their chief Earragan was slain by Goll MacMorna.
Fionn mac Cumhaill features heavily in modern Irish literature. Most notably he makes several appearances in James Joyce's "Finnegans Wake", and some have posited that the title, taken from the street ballad "Finnegan's Wake", may also be a blend of "Finn again is awake," referring to his eventual awakening to defend Ireland.
Fionn also appears as a character in Flann O'Brien's comic novel, "At Swim-Two-Birds", in passages that parody the style of Irish myths. Morgan Llywelyn's book "Finn Mac Cool" tells of Fionn's rise to leader of the Fianna and the love stories that ensue in his life. That character is celebrated in "The Legend of Finn MacCumhail", a song by the Boston-based band Dropkick Murphys featured on their album "Sing Loud Sing Proud!":
Contemporary Scottish poet Marie Marshall has written a semi-serious ballad in parody of 19th-century neo-medievalism "How Finn McCool became Lord of Tara". It tells the story of Fionn saving the house of Tara from the spell of Allan-of-the-Harp, an elf-king with a hatred of human prosperity. A sample passage runs:
Irish Fairy Tales by James Stephens is a retelling of a few of the Fiannaíocht, realistic, funny and entertaining. 
Plays/Shows.
In the 1999 Irish dance show Dancing on Dangerous Ground, conceived and choreographed by former Riverdance leads, Jean Butler and Colin Dunne, Tony Kemp portrayed Fionn in a modernised version of "The Pursuit of Diarmuid and Gráinne". In this, Diarmuid, played by Colin Dunne, dies at the hands of the Fianna after he and Gráinne, played by Jean Butler, run away together into the forests of Ireland, immediately after Fionn and Gráinne's wedding. When she sees Diarmuid's body, Gráinne dies of a broken heart. 
In 2010, Washington DC's Dizzie Miss Lizzie's Roadside Revue debuted their rock musical "Finn McCool" at the Capitol Fringe Festival. The show retells the legend of Fionn mac Cumhaill through punk-inspired rock, and was performed at the Woolly Mammoth Theater in March 2011.

</doc>
<doc id="47372" url="http://en.wikipedia.org/wiki?curid=47372" title="Charles the Bald">
Charles the Bald

Charles the Bald (13 June 823 – 6 October 877) was the King of West Francia (843–77), King of Italy (875–77) and Holy Roman Emperor (875–77, as Charles II). After a series of civil wars that began during the reign of his father, Louis the Pious, Charles succeeded by the Treaty of Verdun (843) in acquiring the western third of the Carolingian Empire. He was a grandson of Charlemagne and the youngest son of Louis the Pious by his second wife, Judith.
Struggle against his brothers.
He was born on 13 June 823 in Frankfurt, when his elder brothers were already adults and had been assigned their own "regna", or subkingdoms, by their father. The attempts made by Louis the Pious to assign Charles a subkingdom, first Alemannia and then the country between the Meuse and the Pyrenees (in 832, after the rising of Pepin I of Aquitaine) were unsuccessful. The numerous reconciliations with the rebellious Lothair and Pepin, as well as their brother Louis the German, King of Bavaria, made Charles's share in Aquitaine and Italy only temporary, but his father did not give up and made Charles the heir of the entire land which was once Gaul and would eventually be France. At a diet in Aachen in 837, Louis the Pious bade the nobles do homage to Charles as his heir. Pepin of Aquitaine died in 838, whereupon Charles at last received that kingdom, which angered Pepin's heirs and the Aquitainian nobles.
The death of the emperor in 840 led to the outbreak of war between his sons. Charles allied himself with his brother Louis the German to resist the pretensions of the new emperor Lothair I, and the two allies defeated Lothair at the Battle of Fontenoy-en-Puisaye on 25 June 841. In the following year, the two brothers confirmed their alliance by the celebrated Oaths of Strasbourg. The war was brought to an end by the Treaty of Verdun in August 843. The settlement gave Charles the Bald the kingdom of the West Franks, which he had been up until then governing and which practically corresponded with what is now France, as far as the Meuse, the Saône, and the Rhône, with the addition of the Spanish March as far as the Ebro. Louis received the eastern part of the Carolingian Empire, known then as East Francia and later as Germany. Lothair retained the imperial title and the Kingdom of Italy. He also received the central regions from Flanders through the Rhineland and Burgundy as king of Middle Francia.
Reign in the West.
The first years of Charles's reign, up to the death of Lothair I in 855, were comparatively peaceful. During these years the three brothers continued the system of "confraternal government", meeting repeatedly with one another, at Koblenz (848), at Meerssen (851), and at Attigny (854). In 858, Louis the German, invited by disaffected nobles eager to oust Charles, invaded the West Frankish kingdom. Charles was so unpopular that he was unable to summon an army, and he fled to Burgundy. He was saved only by the support of the bishops, who refused to crown Louis the German king, and by the fidelity of the Welfs, who were related to his mother, Judith. In 860, he in his turn tried to seize the kingdom of his nephew, Charles of Provence, but was repulsed. On the death of his nephew Lothair II in 869, Charles tried to seize Lothair's dominions, but by the Treaty of Mersen (870) was compelled to share them with Louis the German.
Besides these family disputes, Charles had to struggle against repeated rebellions in Aquitaine and against the Bretons. Led by their chiefs Nomenoë and Erispoë, who defeated the king at the Battle of Ballon (845) and the Battle of Jengland (851), the Bretons were successful in obtaining a "de facto" independence. Charles also fought against the Vikings, who devastated the country of the north, the valleys of the Seine and Loire, and even up to the borders of Aquitaine. Several times Charles was forced to purchase their retreat at a heavy price. Charles led various expeditions against the invaders and, by the Edict of Pistres of 864, made the army more mobile by providing for a cavalry element, the predecessor of the French chivalry so famous during the next 600 years. By the same edict, he ordered fortified bridges to be put up at all rivers to block the Viking incursions. Two of these bridges at Paris saved the city during its siege of 885–886.
Reign as emperor.
In 875, after the death of the Emperor Louis II (son of his half-brother Lothair), Charles the Bald, supported by Pope John VIII, traveled to Italy, receiving the royal crown at Pavia and the imperial insignia in Rome on 29 December. Louis the German, also a candidate for the succession of Louis II, revenged himself by invading and devastating Charles' dominions, and Charles had to return hastily to West Francia. After the death of Louis the German (28 August 876), Charles in his turn attempted to seize Louis's kingdom, but was decisively beaten at Andernach on 8 October 876.
In the meantime, John VIII, menaced by the Saracens, was urging Charles to come to his defence in Italy. Charles again crossed the Alps, but this expedition was received with little enthusiasm by the nobles, and even by his regent in Lombardy, Boso, and they refused to join his army. At the same time Carloman, son of Louis the German, entered northern Italy. Charles, ill and in great distress, started on his way back to Gaul, but died while crossing the pass of Mont Cenis at Brides-les-Bains, on 6 October 877.
According to the Annals of St-Bertin, Charles was hastily buried at the abbey of Nantua, Burgundy because the bearers were unable to withstand the stench of his decaying body. He was to have been buried in the Basilique Saint-Denis and may have been transferred there later. It was recorded that there was a memorial brass there that was melted down at the Revolution.
Charles was succeeded by his son, Louis. Charles was a prince of education and letters, a friend of the church, and conscious of the support he could find in the episcopate against his unruly nobles, for he chose his councillors from among the higher clergy, as in the case of Guenelon of Sens, who betrayed him, and of Hincmar of Reims.
Baldness.
It has been suggested that Charles' nickname was used ironically and not descriptively; i.e. that he was not in fact bald, but rather that he was extremely hairy. In support of this idea is the fact that none of his enemies commented on what would be an easy target. However, none of the voluble members of his court comments on his being hairy; and the "Genealogy of Frankish Kings", a text from Fontanelle dating from possibly as early as 869, and a text without a trace of irony, names him as "Karolus Caluus" ("Charles the Bald"). Certainly, by the end of the 10th century, Richier of Reims and Adhemar of Chabannes refer to him in all seriousness as "Charles the Bald".
An alternative or additional interpretation is based on Charles' initial lack of a "regnum". "Bald" would in this case be a tongue-in-cheek reference to his landlessness, at an age where his brothers already had been sub-kings for some years.
Marriages and children.
Charles married Ermentrude, daughter of Odo I, Count of Orléans, in 842. She died in 869. In 870, Charles married Richilde of Provence, who was descended from a noble family of Lorraine.
With Ermentrude:
With Richilde:

</doc>
<doc id="47373" url="http://en.wikipedia.org/wiki?curid=47373" title="Alberobello">
Alberobello

Alberobello is a small town and "comune" of the Metropolitan City of Bari, Apulia, southern Italy. It has about 11,000 inhabitants and is famous for its unique prehistoric trulli buildings. The Trulli of Alberobello have been designated as a UNESCO World Heritage site since 1996.
International relations.
Twin towns – Sister cities.
Alberobello is twinned with:
External links.
 Media related to at Wikimedia Commons
 

</doc>
<doc id="47374" url="http://en.wikipedia.org/wiki?curid=47374" title="2012">
2012

2012 ()
will be .
2012 was designated as:

</doc>
<doc id="47375" url="http://en.wikipedia.org/wiki?curid=47375" title="Chess strategy">
Chess strategy

Chess strategy is the aspect of chess playing concerned with evaluation of chess positions and setting of goals and long-term plans for future play. While evaluating a position strategically, a player must take into account such factors as the relative value of the pieces on the board, pawn structure, king safety, position of pieces, and control of key squares and groups of squares (e.g. diagonals, open files, individual squares). Chess strategy is distinguished from chess tactics, which is the aspect of role playing concerned with the move-by-move setting up of threats and defenses. Some authors distinguish static strategic imbalances (e.g. having more valuable pieces or better pawn structure), which tend to persist for many moves, from dynamic imbalances (such as one player having an advantage in piece development), which are temporary. This distinction affects the immediacy with which a sought-after plan should take effect. Until players reach the skill level of "master", chess tactics tend to ultimately decide the outcomes of games more often than strategy does. Many chess coaches thus emphasize the study of tactics as the most efficient way to improve one's results in serious chess play.
The most basic way to evaluate one's position is to count the total value of pieces on both sides. The point values used for this purpose are based on experience. Usually pawns are considered to be worth one point, knights and bishops three points each, rooks five points, and queens nine points. The fighting value of the king in the endgame is approximately four points. These basic values are modified by other factors such as the "position of the pieces" (e.g. advanced pawns are usually more valuable than those on their starting squares), "coordination between pieces" (e.g. a bishop pair usually coordinates better than a bishop plus a knight), and the "type of position" (knights are generally better in closed positions with many pawns, while bishops are more powerful in open positions).
Another important factor in the evaluation of chess positions is the pawn structure or pawn skeleton. Since pawns are the most immobile and least valuable of the chess pieces, the pawn structure is relatively static and largely determines the strategic nature of the position. Weaknesses in the pawn structure, such as isolated, doubled, or backward pawns and holes, once created, are usually permanent. Care must therefore be taken to avoid them unless they are compensated by another valuable asset, such as the possibility to develop an attack.
 
Basic concepts of board evaluation.
A material advantage applies both strategically and tactically. Generally more pieces or an aggregate of more powerful pieces means greater chances of winning. A fundamental strategic and tactical rule is to capture opponent pieces while preserving one's own.
Bishops and knights are called "minor pieces". A knight is about as valuable as a bishop, but less valuable than a rook. Rooks and the queen are called "major pieces". Bishops are usually considered slightly better than knights in open positions, such as toward the end of the game when many of the pieces have been captured, whereas knights have an advantage in closed positions. Having two bishops (the bishop pair) is a particularly powerful weapon, especially if the opposing player lacks one or both of their bishops.
Three pawns are likely to be more useful than a knight in the endgame, but in the middlegame, a knight is often more powerful. Two minor pieces are stronger than a single rook, and two rooks are slightly stronger than a queen.
One commonly used simple scoring system is:
Under a system like this, giving up a knight or bishop to win a rook ("winning the exchange") is advantageous and is worth about two pawns. This ignores complications such as the current position and freedom of the pieces involved, but it is a good starting point. In an open position, bishops are more valuable than knights (a bishop pair can easily be worth seven points or more in some situations); conversely, in a closed position, bishops are less valuable than knights. However, a knight in the center of the board that cannot be taken is known as a knight outpost and threatens several fork instances. In such a case, a knight is worth far more than a bishop. Also, many pieces have a partner. By doubling up two knights, two rooks, rook and queen or bishop and queen the pieces can get stronger than the sum of the individual pieces alone. When pieces lose their partner, their values slightly decrease. The king is priceless since its capture results in the defeat of that player and brings about the end of that game. However, especially in the endgame, the king can also be a fighting piece, and is sometimes given a fighting value of four.
Space.
Other things being equal, the side that controls more space on the board has an advantage. More space means more options, which can be exploited both tactically and strategically. A player who has all pieces developed and no tactical tricks or promising long-term plan should try to find a move that enlarges their influence, particularly in the center. However, in some openings, one player accepts less space for a time, to set up a counterattack in the middlegame. This is one of the concepts behind hypermodern play.
The easiest way to gain space is to push the pawn skeleton forward. However, one must be careful not to over stretch. If the opponent succeeds in getting a protected piece behind enemy lines, this piece can become such a serious problem that a piece with a higher value might have to be exchanged for it.
Larry Evans gives a method of evaluating space. The method (for each side) is to count the number of squares attacked or occupied on the opponent's side of the board. In this diagram from the Nimzo-Indian Defense, Black attacks four squares on White's side of the board (d4, e4, f4, and g4). White attacks seven squares on Black's side of the board (b5, c6, e6, f5, g5, and h6 – counting b5 twice) and occupies one square (d5). White has a space advantage of eight to four and Black is cramped.
Control of the center.
The strategy consists of placing pieces so that they attack the central four squares of the board. However, a piece being placed on a central square does not necessarily mean it controls the center, e.g., a knight on a central square does not attack any central squares. Conversely, a piece does not have to be on a central square to control the center. For example, the bishop can control the center from afar.
Control of the center is generally considered important because tactical battles often take place around the central squares, from where pieces can access most of the board. Center control allows more movement and more possibility for attack and defense.
Chess openings try to control the center while developing pieces. Hypermodern openings are those that control the center with pieces from afar (usually the side, such as with a fianchetto); the older Classical (or Modern) openings control it with pawns.
Initiative.
The initiative belongs to the player who can make threats that cannot be ignored, such as checking the opponent's king. They thus put their opponent in the position of having to use their turns responding to threats rather than making their own, hindering the development of their pieces. The player with the initiative is generally attacking and the other player is generally defending.
Defending pieces.
It is important to defend one's pieces even if they are not directly threatened. This helps stop possible future campaigns from the opponent. If a defender must be added at a later time, this may cost a tempo or even be impossible due to a fork or discovered attack. The approach of always defending one's pieces has an antecedent in the theory of Aron Nimzowitsch who referred to it as "overprotection." Similarly, if one spots undefended enemy pieces, one should immediately take advantage of those pieces' weakness.
Even a defended piece can be vulnerable. If the defending piece is also defending something else, it is called an overworked piece, and may not be able to fulfill its task. When there is more than one attacking piece, the number of defenders must also be increased, and their values taken into account. In addition to defending pieces, it is also often necessary to defend key squares, open files, and the back rank. These situations can easily occur if the pawn structure is weak.
Exchanging pieces.
To exchange pieces means to capture a hostile piece and then allow a piece of the same value to be captured. As a general rule of thumb, exchanging pieces eases the task of the defender who typically has less room to operate in.
Exchanging pieces is usually desirable to a player with an existing advantage in material, since it brings the endgame closer and thereby leaves the opponent with less ability to recover ground. In the endgame even a single pawn advantage may be decisive. Exchanging also benefits the player who is being attacked, the player who controls less space, and the player with the better pawn structure.
When playing against stronger players, many beginners attempt to constantly exchange pieces "to simplify matters". However, stronger players are often relatively stronger in the endgame, whereas errors are more common during the more complicated middlegame.
Note that "the exchange" may also specifically mean a rook exchanged for a bishop or knight.
Specific pieces.
Pawns.
In the endgame, passed pawns, unhindered by enemy pawns from promotion, are strong, especially if advanced or protected by another pawn. A passed pawn on the sixth row is roughly as strong as a knight or bishop and often decides the game. (Also see isolated pawn, doubled pawns, backward pawn, connected pawns.)
Knights.
Since knights can easily be chased away by pawn moves, it is often advantageous for knights to be placed in "holes" in the enemy position as outposts—squares where they cannot be attacked by pawns. Such a knight on the fifth rank is a strong asset. The ideal position for a knight is the opponent's third rank, when it is supported by one or two pawns. A knight at the edge or corner of the board controls fewer squares than one on the board's interior, thus the saying: "A Knight on the rim is dim!"
A king and one or even both knights is not sufficient material to checkmate an opposing lone king (see Two knights endgame).
Bishops.
A bishop always stays on squares of the color it started on, so once one of them is gone, the squares of the other color become more difficult to control. When this happens, pawns moved to squares of the other color do not block the bishop, and enemy pawns directly facing them are stuck on the vulnerable color.
A "fianchettoed" bishop, e.g. at g2 after pawn g2–g3, can provide a strong defense for the castled king on g1 and often exert pressure on the long diagonal h1–a8. After a fianchetto, giving up the bishop can weaken the holes in the pawn chain; doing so in front of the castled king may thus impact its safety.
In general, a bishop is of roughly equal value to a knight. In certain circumstances, one can be more powerful than the other. If the game is "closed" with lots of interlocked pawn formations, the knight tends to be stronger, because it can hop over the pawns while they block the bishop. A bishop is also weak if it is restricted by its own pawns, especially if they are blocked and on the bishop's color. Once a bishop is lost, the remaining bishop is considered weaker since the opponent can now plan their moves to play a white or black color game.
In an open position with action on both sides of the board, the bishop tends to be stronger because of its long range. This is especially true in the endgame; if passed pawns race on opposite sides of the board, the player with a bishop usually has better winning chances than a player with a knight.
A king and a bishop is not sufficient material to checkmate an opposing lone king, but two bishops and a king checkmate an opposing lone king easily.
Rooks.
Rooks have more scope of movement on half-open files (ones with no pawns of one's own color). Rooks on the seventh rank can be very powerful as they attack pawns that can only be defended by other pieces, and they can restrict the enemy king to its back rank. A pair of rooks on the player's seventh rank is often a sign of a winning position.
In middlegames and endgames with a passed pawn, Tarrasch's rule states that rooks, both friend and foe of the pawn, are usually strongest "behind" the pawn rather than in front of it.
A king and a rook is sufficient material to checkmate an opposing lone king, although it's a little harder than checkmating with king and queen; thus the rook's distinction as a major piece above the knight and bishop.
Queen.
Queens are the most powerful pieces. They have great mobility and can make many threats at once. They can act as a rook and as a bishop at the same time. For these reasons, checkmate attacks involving a queen are easier to achieve than those without one. Although powerful, the queen is also easily harassed. Thus, it is generally wise to wait to develop the queen until after the knights and bishops have been developed to prevent the queen from being attacked by minor pieces and losing tempo. When a pawn is promoted, most of the time it is promoted to a queen.
King.
During the middle game, the king is often best protected in a corner behind its pawns. Such a position for either of the players is often achieved by castling by that player. If the rooks and queen leave the first rank (commonly called that player's "back rank"), however, an enemy rook or queen can checkmate the king by invading the first rank, commonly called a back rank checkmate. Moving one of the pawns in front of the king (making a luft) can allow it an escape square, but may weaken the king's overall safety otherwise. One must therefore wisely balance between these trade-offs.
Castling is often thought to help protect the king and often "connects" the player's two rooks together so the two rooks may protect each other. This may reduce a threat of a back rank skewer in which the king can be skewered with capture of a rook behind it.
The king can become a strong piece in the endgame. With reduced material, a quick checkmate becomes less of a concern, and moving the king towards the center of the board gives it more opportunities to make threats and actively influence play.
Considerations for a successful long term deployment.
Strategy and tactics.
Chess strategy consists of setting and achieving long-term goals during the game—for example, where to place different pieces—while tactics concentrate on immediate maneuver. These two parts of chess thinking cannot be completely separated, because strategic goals are mostly achieved by the means of tactics, while the tactical opportunities are based on the previous strategy of play.
Because of different strategic and tactical patterns, a game of chess is usually divided into three distinct phases: Opening, usually the first 10 to 25 moves, when players develop their armies and set up the stage for the coming battle; middlegame, the developed phase of the game; and endgame, when most of the pieces are gone and kings start to take an active part in the struggle.
Opening.
A chess opening is the group of initial moves of a game (the "opening moves"). Recognized sequences of opening moves are referred to as "openings" and have been given names such as the Ruy Lopez or Sicilian Defence. They are catalogued in reference works such as the "Encyclopaedia of Chess Openings". It is recommended for anyone but the chessmasters that when left with a choice to either invent a new variation or follow a standard opening, choose the latter.
There are dozens of different openings, varying widely in character from quiet positional play (e.g. the Réti Opening) to very aggressive (e.g. the Latvian Gambit). In some opening lines, the exact sequence considered best for both sides has been worked out to 30–35 moves or more. Professional players spend years studying openings, and continue doing so throughout their careers, as opening theory continues to evolve.
The fundamental strategic aims of most openings are similar:
During the opening, some pieces have a recognized optimum square they try to reach. Hence, an optimum deployment could be to push the king and queen pawn two steps followed by moving the knights so they protect the center pawns and give additional control of the center. One can then deploy the bishops, protected by the knights, to pin the opponent's knights and pawns. The optimum opening is ended with a castling, moving the king to safety and deploying for a strong back-rank and a rook along the center file.
Apart from these fundamentals, other strategic plans or tactical sequences may be employed in the opening.
Most players and theoreticians consider that White, by virtue of the first move, begins the game with a small advantage. Black usually strives to neutralize White's advantage and achieve equality, or to develop dynamic counterplay in an unbalanced position.
Middlegame.
The middlegame is the part of the game when most pieces have been developed. Because the opening theory has ended, players have to assess the position, to form plans based on the features of the positions, and at the same time to take into account the tactical possibilities in the position.
Typical plans or strategic themes—for example the minority attack, that is the attack of queenside pawns against an opponent who has more pawns on the queenside—are often appropriate just for some pawn structures, resulting from a specific group of openings. The study of openings should therefore be connected with the preparation of plans typical for resulting middlegames.
Middlegame is also the phase when most combinations occur. Middlegame combinations are often connected with the attack against the opponent's king; some typical patterns have their own names, for example the Boden's Mate or the Lasker—Bauer combination.
Another important strategical question in the middlegame is whether and how to reduce material and transform into an endgame (i.e. simplify). For example, minor material advantages can generally be transformed into victory only in an endgame, and therefore the stronger side must choose an appropriate way to achieve an ending. Not every reduction of material is good for this purpose; for example, if one side keeps a light-squared bishop and the opponent has a dark-squared one, the transformation into a "bishops and pawns" ending is usually advantageous for the weaker side only, because an endgame with bishops on opposite colors is likely to be a draw, even with an advantage of one or two pawns.
Endgame.
The endgame (or "end game" or "ending") is the stage of the game when there are few pieces left on the board. There are three main strategic differences between earlier stages of the game and endgame:
Endgames can be classified according to the type of pieces that remain on board. Basic checkmates are positions where one side has only a king and the other side has one or two pieces and can checkmate the opposing king, with the pieces working together with their king. For example, king and pawn endgames involve only kings and pawns on one or both sides and the task of the stronger side is to promote one of the pawns. Other more complicated endings are classified according to the pieces on board other than kings, e.g. "rook and pawn versus rook endgame".
References.
Notes
Bibliography
</dl>
Further reading.
</dl>

</doc>
<doc id="47376" url="http://en.wikipedia.org/wiki?curid=47376" title="100s BC (decade)">
100s BC (decade)


</doc>
<doc id="47377" url="http://en.wikipedia.org/wiki?curid=47377" title="326">
326

Year 326 (CCCXXVI) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Constantinus and Constantinus (or, less frequently, year 1079 "Ab urbe condita"). The denomination 326 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="47382" url="http://en.wikipedia.org/wiki?curid=47382" title="Edward the Martyr">
Edward the Martyr

Edward the Martyr (Old English: "Eadweard"; c. 962 – 18 March 978) was King of England from 975 until he was murdered in 978. Edward was the eldest son of King Edgar the Peaceful but was not his father's acknowledged heir. On Edgar's death, the leadership of England was contested, with some supporting Edward's claim to be king and others supporting his much younger half-brother Æthelred the Unready, recognized as a legitimate son of Edgar. Edward was chosen as king and was crowned by his main clerical supporters, the archbishops Dunstan and Oswald of Worcester.
The great nobles of the kingdom, ealdormen Ælfhere and Æthelwine, quarrelled, and civil war almost broke out. In the so-called anti-monastic reaction, the nobles took advantage of Edward's weakness to dispossess the Benedictine reformed monasteries of lands and other properties that King Edgar had granted to them.
Edward's short reign was brought to an end by his murder at Corfe Castle in 978 in circumstances that are not altogether clear. His body was reburied with great ceremony at Shaftesbury Abbey early in 979. In 1001 Edward's remains were moved to a more prominent place in the abbey, probably with the blessing of his half-brother King Æthelred. Edward was already reckoned a saint by this time.
A number of lives of Edward were written in the centuries following his death in which he was portrayed as a martyr, generally seen as a victim of the Queen Dowager Ælfthryth, mother of Æthelred. He is today recognized as a saint in the Eastern Orthodox Church, the Roman Catholic Church, and the Anglican Communion.
Ætheling (princes of succession).
Edward's date of birth is unknown, but he was the eldest of Edgar's three children. He was likely in his teens when he succeeded his father, who died at age 32 in 975. Edward was known to be King Edgar's son, but he was not the son of Queen Ælfthryth, the third wife of Edgar. This much and no more is known from contemporary charters.
Later sources of questionable reliability address the identity of Edward's mother. The earliest such source is a life of Dunstan by Osbern of Canterbury, probably written in the 1080s. Osbern writes that Edward's mother was a nun at Wilton Abbey whom the king seduced. When Eadmer wrote a life of Dunstan some decades later, he included an account of Edward's parentage obtained from Nicholas of Worcester. This denied that Edward was the son of a liaison between Edgar and a nun, presenting him as the son of Æthelflæd, daughter of Ordmær, "ealdorman of the East Anglians", whom Edgar had married in the years when he ruled Mercia (between 957 and Eadwig's death in 959). Additional accounts are offered by Goscelin in his life of Edgar's daughter Saint Edith of Wilton and in the histories of John of Worcester and William of Malmesbury. Together these various accounts suggest that Edward's mother was probably a noblewoman named Æthelflæd, surnamed "Candida" or "Eneda"—"the White" or "White Duck".
A charter of 966 describes Ælfthryth, whom Edgar had married in 964, as the king's "lawful wife", and their eldest son Edmund as the legitimate son of the king. Edward is noted as the king's son. However, a genealogy created at Glastonbury Abbey circa 969 gives Edward precedence over Edmund and Æthelred. Ælfthryth was the widow of Æthelwald, Ealdorman of East Anglia and perhaps Edgar's third wife. The contradictions regarding the identity of Edward's mother, and the fact that Edmund appears to have been regarded as the legitimate heir until his death in 971, suggest that Edward was probably illegitimate.
Edmund's full brother Æthelred may have inherited his position as heir. On a charter to the New Minster at Winchester, the names of Ælfthryth and her son Æthelred appear ahead of Edward's name. When Edgar died on 8 July 975, Æthelred was probably nine and Edward only a few years older.
Disputed succession.
Edgar had been a strong ruler who had forced monastic reforms on a probably unwilling church and nobility, aided by the leading clerics of the day, Dunstan, Archbishop of Canterbury; Oswald of Worcester, Archbishop of York; and Bishop Æthelwold of Winchester. By endowing the reformed Benedictine monasteries with the lands required for their support, he had dispossessed many lesser nobles, and had rewritten leases and loans of land to the benefit of the monasteries. Secular clergy, many of whom would have been members of the nobility, had been expelled from the new monasteries. While Edgar lived, he strongly supported the reformers, but following his death, the discontents which these changes had provoked came into the open.
The leading figures had all been supporters of the reform, but they were no longer united. Relations between Archbishop Dunstan and Bishop Æthelwold may have been strained. Archbishop Oswald was at odds with Ealdorman Ælfhere, Ealdorman of Mercia, while Ælfhere and his kin were rivals for power with the affinity of Æthelwine, Ealdorman of East Anglia. Dunstan was said to have questioned Edgar's marriage to Queen Dowager Ælfthryth and the legitimacy of their son Æthelred.
These leaders were divided as to whether Edward or Æthelred should succeed Edgar. Neither law nor precedent offered much guidance. The choice between the sons of Edward the Elder had divided his kingdom, and Edgar's elder brother Eadwig had been forced to give over a large part of the kingdom to Edgar. The Queen Dowager certainly supported the claims of her son Æthelred, aided by Bishop Æthelwold; and Dunstan supported Edward, aided by his fellow archbishop Oswald. It is likely that Ealdorman Ælfhere and his allies supported Æthelred and that Æthelwine and his allies supported Edward, although some historians suggest the opposite.
Later sources suggest that perceptions of legitimacy played a part in the arguments, as did the relative age of the two candidates. In time, Edward was anointed by Archbishops Dunstan and Oswald at Kingston upon Thames, most likely in 975. There is evidence that the settlement involved a degree of compromise. Æthelred appears to have been given lands which normally belonged to the king's sons, some of which had been granted by Edgar to Abingdon Abbey and which were forcibly repossessed for Æthelred by the leading nobles.
Edward's reign.
After recording Edward's succession, the "Anglo-Saxon Chronicle" reports that a comet appeared, and that famine and "manifold disturbances" followed. The "manifold disturbances", sometimes called the anti-monastic reaction, appear to have started soon after Edgar's death. During this time, the experienced Ealdorman Oslac of Northumbria, effective ruler of much of northern England, was exiled due to unknown circumstances. Oslac was followed as ealdorman by Thored, either Oslac's son of that name or Thored Gunnar's son mentioned by the "Chronicle" in 966.
Edward, or rather those who were wielding power on his behalf, also appointed a number of new ealdormen to positions in Wessex. Little is known of two of these men, and it is difficult to determine which faction, if any, they belonged to. Edwin, probably ruling in Sussex, and perhaps also parts of Kent and Surrey, was buried at Abingdon, an abbey patronised by Ælfhere. Æthelmær, who oversaw Hampshire, held lands in Rutland, perhaps suggesting links to Æthelwine.
The third ealdorman, Æthelweard, today best known for his Latin history, ruled in the west. Æthelweard was a descendant of King Æthelred of Wessex and probably the brother of King Eadwig's wife. He appears to have been a supporter of Edward rather than of either faction.
In some places, the secular clergy who had been driven from the monasteries returned, driving the regular clergy out in their turn. Bishop Æthelwold had been the main enemy of the seculars, and Archbishop Dunstan appears to have done little to aid his fellow reformer at this time. More generally, the magnates took the opportunity to undo many of Edgar's grants to monasteries and to force the abbots to rewrite leases and loans to favour the local nobility. Ealdorman Ælfhere was the leader in this regard, attacking Oswald's network of monasteries across Mercia. Ælfhere's rival Æthelwine, while a staunch protector of his family monastery of Ramsey Abbey, treated Ely Abbey and other monasteries harshly. At some point during these disorders, Ælfhere and Æthelwine appear to have come close to open warfare. This may well have been related to Ælfhere's ambitions in East Anglia and to attacks upon Ramsey Abbey. Æthelwine, supported by his kinsman Ealdorman Byrhtnoth of Essex and others unspecified, mustered an army and caused Ælfhere to back down.
Very few charters survive from Edward's reign, perhaps as few as three, leaving Edward's brief reign in obscurity. By contrast, numerous charters survived from the reigns of his father Edgar and half-brother Æthelred. All of the surviving Edward charters concern the royal heartland of Wessex; two deal with Crediton where Edward's former tutor Sideman was bishop. During Edgar's reign, dies for coins were cut only at Winchester and distributed from there to other mints across the kingdom. Edward's reign permitted dies to be cut locally at York and at Lincoln. The general impression is of a reduction or breakdown of royal authority in the midlands and north. The machinery of government continued to function, as councils and synods met as customary during Edward's reign, at Kirtlington in Oxfordshire after Easter 977, and again at Calne in Wiltshire the following year. During the meeting at Calne, some councillors were killed and others injured by the collapse of the floor of their room.
Death.
The version of the "Anglo-Saxon Chronicle" containing the most detailed account records that Edward was murdered in the evening of 18 March 978, while visiting Ælfthryth and Æthelred, probably at or near the mound on which the ruins of Corfe Castle now stand. It adds that he was buried at Wareham "without any royal honours". The compiler of this version of the "Chronicle", manuscript E, called the Peterborough Chronicle, says:
"No worse deed for the English race was done than this was, since they first sought out the land of Britain. Men murdered him, but God exalted him. In life he was an earthly king; after death he is now a heavenly saint. His earthly relatives would not avenge him, but his Heavenly Father has much avenged him."
Other recensions of the "Chronicle" report less detail, the oldest text stating only that he was killed, while versions from the 1040s say that he was martyred.
Of other early sources, the life of Oswald of Worcester, attributed to Byrhtferth of Ramsey, adds that Edward was killed by Æthelred's advisers, who attacked him when he was dismounting. It agrees that he was buried without ceremony at Wareham. Archbishop Wulfstan II alludes to the killing of Edward in his "Sermo Lupi ad Anglos," written not later than 1016. A recent study translates his words as follows:
"And a very great betrayal of a lord it is also in the world, that a man betray his lord to death, or drive him living from the land, and both have come to pass in this land: Edward was betrayed, and then killed, and after that burned ..."
Later sources, further removed from events, such as the late 11th-century "Passio S. Eadwardi" and John of Worcester, claim that Ælfthryth organised the killing of Edward, while Henry of Huntingdon wrote that she killed Edward herself.
Modern historians have offered a variety of interpretations of Edward's killing. Three main theories have been proposed. Firstly, that Edward was killed, as the life of Oswald claims, by nobles in Æthelred's service, either as a result of a personal quarrel, or to place their master on the throne. The life of Oswald portrays Edward as an unstable young man who, according to Frank Stenton: "had offended many important persons by his intolerable violence of speech and behavior. Long after he had passed into veneration as a saint it was remembered that his outbursts of rage had alarmed all who knew him, and especially the members of his own household." This may be a trope of hagiography.
In the second version, Ælfthryth was implicated, either beforehand by plotting the killing, or afterwards in allowing the killers to go free and unpunished.
A third alternative, noting that Edward in 978 was very close to ruling on his own, proposes that Ealdorman Ælfhere was behind the killing so as to preserve his own influence and to prevent Edward taking revenge for Ælfhere's actions earlier in the reign. John notes this and interprets Ælfhere's part in Edward's reburial as being a penance for the assassination.
Reburial and early cult.
Edward's body lay at Wareham for a year before being disinterred. Ælfhere initiated the reinterment, perhaps as a gesture of reconciliation. According to the life of Oswald, Edward's body was found to be incorrupt when it was disinterred (which was taken as a miraculous sign). The body was taken to the Shaftesbury Abbey, a nunnery with royal connections which had been endowed by King Alfred the Great and where Edward and Æthelred's grandmother Ælfgifu had spent her latter years.
Edward's remains were reburied with lavish public ceremony. Later versions, such as the "Passio S. Eadwardi," have more complicated accounts. It said that Edward's body was concealed in a marsh, where it was revealed by miraculous events. The "Passio" dates the reburial to 18 February.
In 1001, Edward's relics (for he was considered a saint, although never canonized) were translated to a more prominent place within the nunnery at Shaftesbury. The ceremonies are said to have been led by the then-Bishop of Sherborne, Wulfsige III, accompanied by a senior cleric whom the "Passio" calls Elsinus, sometimes identified with Ælfsige, the abbot of the New Minster, Winchester. King Æthelred, preoccupied with the threat of a Danish invasion, did not attend in person, but he issued a charter to the Shaftesbury nuns late in 1001 granting them lands at Bradford on Avon, which is thought to be related. A 13th-century calendar of saints gives the date of this translation as 20 June.
The rise of Edward's cult has been interpreted in various ways. It is sometimes portrayed as a popular movement, or as the product of a political attack on King Æthelred by former supporters of Edward. Alternatively, Æthelred has been seen as one of the key forces in the promotion of Edward's cult and that of their sister Eadgifu (Edith of Wilton). He was thought to make the charter in 1001 granting land to Shaftesbury at the elevation of Edward's relics, and some accounts suggest that Æthelred legislated the observation of Edward's feast days across England in a law code of 1008. It is unclear whether this innovation, seemingly drafted by Archbishop Wulfstan II, dates from Æthelred's reign. It may instead have been promulgated by King Cnut. David Rollason has drawn attention to the increased importance of the cults of other murdered royal saints in this period. Among these are the cults of King Ecgberht of Kent's nephews, whose lives form part of the Mildrith Legend, and those of the Mercian Saints Kenelm and Wigstan.
Later cult.
During the sixteenth century and English Reformation, King Henry VIII led the dissolution of the monasteries and many holy places were demolished. Edward's remains were hidden so as to avoid desecration.
In 1931, the relics were recovered by Wilson-Claridge during an archaeological excavation; their identity was confirmed by Dr. T.E.A. Stowell, an osteologist. In 1970, examinations performed on the relics suggested that the young man had died in the same manner as Edward. Wilson-Claridge wanted the relics to go to the Russian Orthodox Church Outside Russia. His brother, however, wanted them to be returned to Shaftesbury Abbey. For decades, the relics were kept in a bank vault in Woking, Surrey because of the unresolved dispute about which of two churches should have them.
In time, the Russian Orthodox Church Outside Russia was victorious and placed the relics in a church in Brookwood Cemetery in Woking, with the enshrinement ceremony occurring in September 1984. The St Edward Brotherhood of monks was organized there as well. The church is now named St Edward the Martyr Orthodox Church, and it is under the jurisdiction of a traditionalist Greek Orthodox community.
In the Orthodox Church, St Edward is ranked as a Passion-bearer, a type of saint who accepts death out of love for Christ. Edward was never officially canonized, but he is also regarded as a saint in the Eastern Orthodox Church, the Roman Catholic Church and the Anglican Communion. His feast day is celebrated on 18 March, the day of his murder. The Orthodox Church commemorates him a second time each year on 3 September and commemorates the translation of his relics into Orthodox possession on 13 February.
References.
</dl>

</doc>
<doc id="47383" url="http://en.wikipedia.org/wiki?curid=47383" title="306">
306

Year 306 (CCCVI) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Constantius and Valerius (or, less frequently, year 1059 "Ab urbe condita"). The denomination 306 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="47384" url="http://en.wikipedia.org/wiki?curid=47384" title="Brooklyn">
Brooklyn

Brooklyn is the most populous of New York City's five boroughs, with a Census-estimated 2,621,793 people in 2014. It is geographically adjacent to the borough of Queens at the western end of Long Island. Since 1896, Brooklyn has had the same boundaries as Kings County, the most populous county in the State of New York and the second-most densely populated county in the United States, after New York County (Manhattan). With a land area of 71 sqmi and water area of 26 sqmi, Kings County is the fourth-smallest county in New York State by land area and third-smallest by total area, though it is the second-largest among New York City's boroughs. Today, if it were an independent city, Brooklyn would rank as the fourth most populous city in the U.S., behind only the other boroughs of New York City combined, Los Angeles, and Chicago.
Brooklyn was an independent incorporated city (and previously an authorized village and town within the provisions of the New York State Constitution), until January 1, 1898, when, after a long political campaign and public relations battle during the 1890s, according to the new Municipal Charter of "Greater New York", Brooklyn was consolidated with the other cities, boroughs and counties to form the modern "City of New York" surrounding the Upper New York Bay with five constituent boroughs. It continues, however, to maintain a distinct culture, as befitting the former second or third largest city in America during the later 19th Century. Many Brooklyn neighborhoods are ethnic enclaves where particular ethnic and nationality groups and cultures predominate. Brooklyn's official motto is "Eendraght Maeckt Maght". Written in the (early modern spelling of the) Dutch language, it is inspired by the motto of the United Dutch Provinces (first Dutch Republic, predecessor of the current Kingdom of the Netherlands), (currently also the official motto of the neighboring Kingdom of Belgium) and translated "In unity, there is strength." The motto is displayed on the Borough seal and flag, which also feature a young robed woman bearing a bundle of bound rods known as a "fasces", a traditional emblem of Republicanism. Brooklyn's official colors are blue and gold.
History.
The history of European settlement in Brooklyn spans more than 350 years. The settlement began in the 17th century as the small Dutch-founded town of "Breuckelen" on the East River shore of Long Island, grew to be a sizable city in the 19th century, and was consolidated in 1898 with New York City (then confined to Manhattan and part of the Bronx), the remaining rural areas of Kings County, and the largely rural areas of Queens and Staten Island, to form the modern City of New York.
Colonial era.
Six Dutch towns.
The Dutch were the first Europeans to settle the area on the western edge of Long Island, which was then largely inhabited by the Lenape, a Native American people who are often referred to in contemporary colonial documents by a variation of the place name "Canarsie". The "Breuckelen" settlement, named after Breukelen in the Netherlands, was part of New Netherland, and the Dutch West India Company lost little time in chartering the six original parishes (listed here first by their later, more common English names):
Many incidents and documents relating to this period are in Gabriel Furman's early (1824) compilation.
The capital of the colony, New Amsterdam across the East River, obtained its charter later than the village of Brooklyn did, in 1653.
The neighborhood of Marine Park was home to the first tidal mill in North America. This mill was built by the Dutch, and the foundation can be seen today. However, the area was not formally settled as a town.
Six townships in an English province.
What is today Brooklyn left Dutch hands after the final English conquest of New Netherland in 1664, in a prelude to the Second Anglo–Dutch War. New Netherland was taken in a naval action, and the conquerors renamed their prize in honor of the overall English naval commander, James, Duke of York, brother of the Monarch, King Charles II of England and future king himself as King James II of England and James VII of Scotland; Brooklyn became a part of the new English and later British colony, the Province of New York.
The English reorganized the six old Dutch towns on southwestern Long Island as Kings County on November 1, 1683, one of "Original Twelve Counties" then established in New York Province. This tract of land was recognized as a political entity for the first time, and the municipal groundwork was laid for a later expansive idea of Brooklyn identity.
Lacking the patroon and tenant farmer system established along the Hudson River Valley, this agricultural county unusually came to have one of the highest percentages of slavery among the population in the "Original Thirteen Colonies" along the Atlantic Ocean eastern coast of North America.
Revolutionary War.
On August 27, 1776, the Battle of Long Island (also known as the "Battle of Brooklyn") was the first major engagement fought in the American Revolutionary War after independence was declared, and the largest of the entire conflict. British troops forced Continental Army troops under George Washington off the heights near the modern sites of Green-Wood Cemetery, Prospect Park, and Grand Army Plaza.
Washington, viewing particularly fierce fighting at the Gowanus Creek from his vantage point atop a hill near the west end of present-day Atlantic Avenue, was famously reported to have emotionally exclaimed: "What brave men I must this day lose!".
The fortified American positions at Brooklyn Heights consequently became untenable and were evacuated a few days later, leaving the British in control of New York Harbor. While Washington's defeat on the battlefield cast early doubts on his ability as commander, the subsequent tactical withdrawal of all his troops and supplies across the East River in a single night is seen by historians as one of his most brilliant triumphs.
The surrounding region was controlled by the British for the duration of the war, as New York City was soon occupied and became their military and political base of operations in North America for the remainder of the conflict. The British generally enjoyed a dominant Loyalist sentiment from the remaining residents in Kings County who did not evacuate, though the region was also the center of the fledgling — and largely successful — American intelligence network, headed by Washington himself.
The British set up a system of notorious prison ships off the coast of Brooklyn in Wallabout Bay, where more American patriots died of intentional neglect than died in combat on all the battlefields of the American Revolutionary War, combined. The Treaty of Paris in 1783 resulted, in part, in the evacuation of the British from New York City, celebrated by residents into the 20th century.
Post-colonial era.
Urbanization.
The first half of the 19th century saw the beginning of the development of urban areas on the economically strategic East River shore of Kings County, facing the adolescent City of New York confined to Manhattan Island. The New York Navy Yard operated in Wallabout Bay (border between Brooklyn and Williamsburgh) for the entire 19th century and two thirds of the 20th century.
The first center of urbanization sprang up in the Town of Brooklyn, directly across from Lower Manhattan, which saw the incorporation of the Village of Brooklyn in 1816. Reliable steam ferry service across the East River to Fulton Landing converted Brooklyn Heights into a commuter town for Wall Street. Ferry Road to Jamaica Pass became Fulton Street to East New York. Town and Village were combined to form the first, kernel incarnation of the City of Brooklyn in 1834.
In parallel development, the Town of Bushwick, a little farther up the river, saw the incorporation of the Village of Williamsburgh in 1827, which separated as the Town of Williamsburgh in 1840, only to form the short-lived City of Williamsburgh in 1851. Industrial deconcentration in mid-century was bringing shipbuilding and other manufacturing to the northern part of the county. Each of the two cities and six towns in Kings County remained independent municipalities, and purposely created non-aligning street grids with different naming systems.
But the East River shore was growing too fast for the three-year-old infant City of Williamsburgh, which, along with its Town of Bushwick hinterland, was subsumed within a greater City of Brooklyn in 1854.
By 1841, the growing city across the East River from Manhattan, boasted its own highly regarded newspaper with the appearance of the "The Brooklyn Eagle, and Kings County Democrat" published by Alfred G. Stevens. It later became the most popular and highest circulation afternoon paper in America. The publisher changed to L. Van Anden on April 19, 1842, and the paper was renamed "The Brooklyn Daily Eagle and Kings County Democrat" on June 1, 1846. On May 14, 1849, the name was shortened to "The Brooklyn Daily Eagle". On September 5, 1938, the name was further shortened, to "Brooklyn Eagle". The establishment of the paper in the 1800s set a tone for the developing separate identity for Brooklynites in the next century along with its famous National League baseball team, the Brooklyn Dodgers. Both major institutions were lost in the 1950s, when the paper closed in 1955 after unsuccessful attempts at a sale following a reporters' strike and the baseball team decamped for Los Angeles in a realignment of major league baseball in 1957.
Agitation against Southern slavery was stronger in Brooklyn than in New York, and under Republican leadership the city was fervent in the Union cause in the Civil War. After the war the Henry Ward Beecher Monument was built downtown to honor a famous local abolitionist. A great victory arch at what was then the south end of town celebrated the armed forces, the place now being called Grand Army Plaza.
The city had a population of 25,000 in 1834, but the police department only comprised 12 men on the day shift and another 12 at night. Every time a rash of burglaries broke out, officials blamed burglars coming in from New York City. Finally in 1855, a modern police force was created, employing 150 men. Voters complained of inadequate protection and excessive costs. In 1857 the state legislature merged the Brooklyn force with that of New York City.
Civil War.
Fervent in the Union cause, the city of Brooklyn played a major role in supplying troops and materiel for the American Civil War. The most well known regiment to be sent off to war from the city was the 14th Brooklyn "Red Legged Devils". They fought from 1861 to 1864 and wore red the entire war.
They were the only Regiment named after a city, and President Lincoln called them into service personally, making them part of a handful of 3 year enlisted soldiers in April 1861. Unlike other regiments during the American Civil War, the 14th wore a uniform inspired by that of the French Chasseurs, a light infantry used for quick assaults on the enemy.
As both a seaport and a manufacturing center, Brooklyn was well prepared to play to the Union's strengths in shipping and manufacturing. The two combined in shipbuilding; the ironclad "Monitor" was built in Brooklyn.
Twin city.
Taking a thirty-year break from municipal expansionism, this well-situated coastal city established itself as the third-most-populous American city for much of the 19th century. Brooklyn is referred to as a twin city of New York in the 1883 poem, "The New Colossus" by Emma Lazarus, which appears on a plaque inside the Statue of Liberty. The poem calls New York Harbor "the air-bridged harbor that twin cities frame". As a twin city to New York, it played a role in national affairs that was later overshadowed by its century-old submergence into its old partner and rival.
Economic growth continued, propelled by immigration and industrialization. The waterfront from Gowanus Bay to Greenpoint was developed with piers and factories. Industrial access to the waterfront was improved by the Gowanus Canal and the canalized Newtown Creek. The USS "Monitor" was only the most famous product of the large and growing shipbuilding industry of Williamsburg. After the Civil War, trolley lines and other transport brought urban sprawl beyond Prospect Park and into the center of the county.
The rapidly growing population needed more water, so the City built centralized waterworks including the Ridgewood Reservoir. The municipal Police Department, however, was abolished in 1854 in favor of a Metropolitan force covering also New York and Westchester Counties. In 1865 the Brooklyn Fire Department (BFD) also gave way to the new Metropolitan Fire District.
Throughout this period the peripheral towns of Kings County, far from Manhattan and even from urban Brooklyn, maintained their rustic independence. The only municipal change seen was the secession of the eastern section of the Town of Flatbush as the Town of New Lots in 1852. The building of rail links such as the Brighton Beach Line in 1878 heralded the end of this isolation.
Sports became big business, and the Brooklyn Bridegrooms played professional baseball at Washington Park in the convenient suburb of Park Slope and elsewhere. Early in the next Century they brought their new name of Brooklyn Dodgers to Ebbets Field, beyond Prospect Park. Racetracks, amusement parks and beach resorts opened in Brighton Beach, Coney Island and elsewhere in the southern part of the county.
Toward the end of the 19th century, the City of Brooklyn experienced its final, explosive growth spurt. Railroads and industrialization spread to Bay Ridge and Sunset Park. In the space of a decade, the city annexed the Town of New Lots in 1886, the Town of Flatbush, the Town of Gravesend, the Town of New Utrecht in 1894, and the Town of Flatlands in 1896. Brooklyn had reached its natural municipal boundaries at the ends of Kings County.
Mayors of the City of Brooklyn.
Brooklyn elected a mayor from 1834 until consolidation in 1898 into the City of Greater New York, whose own second mayor (1902–1903), Seth Low, had been Mayor of Brooklyn from 1882 to 1885. Since 1898, Brooklyn has, in place of a separate mayor, elected a Borough President. See the List of mayors of New York City and the list of Brooklyn Borough Presidents.
New York City borough.
In 1883, the Brooklyn Bridge was completed, transportation to Manhattan was no longer by water only, and the City of Brooklyn's ties to the City of New York were strengthened.
The question became whether Brooklyn was prepared to engage in the still-grander process of consolidation then developing throughout the region, whether to join with the county of New York, the county of Richmond and the western portion of Queens County to form the five boroughs of a united City of New York. Andrew Haskell Green and other progressives said Yes, and eventually they prevailed against the "Daily Eagle" and other conservative forces. In 1894, residents of Brooklyn and the other counties voted by a slight majority to merge, effective in 1898.
Kings County retained its status as one of New York State's counties, but the loss of Brooklyn's separate identity as a city was met with consternation by some residents at the time. The merger was called the "Great Mistake of 1898" by many newspapers of the day, and the phrase still denotes Brooklyn pride among old-time Brooklynites.
Government and politics.
Since consolidation with New York City in 1898, Brooklyn has been governed by the New York City Charter that provides for a "strong" mayor-council system. The centralized New York City government is responsible for public education, correctional institutions, libraries, public safety, recreational facilities, sanitation, water supply, and welfare services.
The office of Borough President was created in the consolidation of 1898 to balance centralization with local authority. Each borough president had a powerful administrative role derived from having a vote on the New York City Board of Estimate, which was responsible for creating and approving the city's budget and proposals for land use. In 1989, the Supreme Court of the United States declared the Board of Estimate unconstitutional because Brooklyn, the most populous borough, had no greater effective representation on the Board than Staten Island, the least populous borough, a violation of the Fourteenth Amendment's Equal Protection Clause pursuant to the high court's 1964 "one man, one vote" decision.
Since 1990 the Borough President has acted as an advocate for the borough at the mayoral agencies, the City Council, the New York state government, and corporations. Brooklyn's current Borough President is Eric Adams, elected as a Democrat in November 2013 with 90.8% of the vote. Adams replaced popular Borough President Marty Markowitz, also a Democrat, who partially used his office to promote tourism and new development for Brooklyn.
The Democratic Party holds the majority of public offices. As of 2005, 69.7% of registered voters in Brooklyn were Democrats. Party platforms center on affordable housing, education and economic development. The most controversial political issue is the proposed Atlantic Yards, a large housing and sports arena project. Pockets of majority Republican influence exist in Gravesend, Bensonhurst, Bay Ridge, Dyker Heights and Midwood by U.S. House Representative Michael Grimm & New York State Senator Marty Golden.
Each of the city's five counties (coterminous with each borough) has its own criminal court system and District Attorney, the chief public prosecutor who is directly elected by popular vote. The current District Attorney of Kings County is Kenneth P. Thompson, a Democrat elected in 2013. Brooklyn has 16 City Council members, the largest number of any of the five boroughs. Brooklyn has 18 of the city's 59 community districts, each served by an unpaid Community Board with advisory powers under the city's Uniform Land Use Review Procedure. Each board has a paid district manager who acts as an interlocutor with city agencies.
Economy.
Brooklyn's job market is driven by three main factors: the performance of the national and city economy, population flows and the borough's position as a convenient back office for New York's businesses.
Forty-four percent of Brooklyn's employed population, or 410,000 people, work in the borough; more than half of the borough's residents work outside its boundaries. As a result, economic conditions in Manhattan are important to the borough's jobseekers. Strong international immigration to Brooklyn generates jobs in services, retailing and construction.
In recent years, Brooklyn has benefited from a steady influx of financial back-office operations from Manhattan, the rapid growth of a high-tech and entertainment economy in DUMBO, and strong growth in support services such as accounting, personal supply agencies, and computer services firms.
Jobs in the borough have traditionally been concentrated in manufacturing, but since 1975, Brooklyn has shifted from a manufacturing-based to a service-based economy. In 2004, 215,000 Brooklyn residents worked in the services sector, while 27,500 worked in manufacturing. Although manufacturing has declined, a substantial base has remained in apparel and niche manufacturing concerns such as furniture, fabricated metals, and food products. The pharmaceutical company Pfizer was founded in Brooklyn in 1869 and had a manufacturing plant in the borough for many years that once employed thousands of workers, but the plant shut down in 2008. However, new light-manufacturing centered around organic and high-end food have sprung up in the old plant.
First established as a shipbuilding facility in 1801, the Brooklyn Navy Yard employed 70,000 people at its peak during World War II and was then the largest employer in the borough. The "Missouri", the ship on which the Japanese formally surrendered, was built there, as was the "Maine", whose sinking off Havana led to the start of the Spanish–American War. The iron-sided Civil War vessel the "Monitor" was built in Greenpoint. From 1968–1979 Seatrain Shipbuilding was the major employer. Later tenants include industrial design firms, food processing businesses, artisans, and the film and television production industry. About 230 private-sector firms providing 4,000 jobs are at the Yard.
Construction and services are the fastest growing sectors. Most employers in Brooklyn are small businesses. In 2000, 91% of the approximately 38,704 business establishments in Brooklyn had fewer than 20 employees. As of August 2008[ [update]], the borough's unemployment rate was 5.9%.
Brooklyn is also home to many banks and credit unions. According to the Federal Deposit Insurance Corporation, there were 37 banks and 21 credit unions operating in the borough in 2010.
Demographics.
Since 2010, the population of Brooklyn was estimated by the Census Bureau to have increased 3.5% to 2,592,149 as of 2013, representing 30.8% of New York City's population, 33.5% of Long Island's population, and 13.2% of New York State's population.
2010.
According to the United States Census 2010, the demography of Brooklyn was as follows:
2009 and 2012 estimates.
According to the 2009 American Community Survey, Brooklyn's population was 46.6% white, of which 36.9% were non-Hispanic whites. Blacks made up 34.2% of the population, of which 32.9% were non-Hispanic blacks. Native Americans represented 0.3% of the population, while Asians made up 9.5% of the populace. Pacific Islanders comprised just 0.1% of the population, and Multiracial Americans made up 1.4% of the population. Hispanics and Latinos made up 19.6% of Brooklyn's population.
According to the 2012 U.S. Census Bureau estimates, there are 2,565,635 people (up from 2.3 million in 1990), 880,727 households, and 583,922 families living in Brooklyn. The population density was 34,920/square mile (13,480/km²). There were 930,866 housing units at an average density of 13,180/square mile (5,090/km²).
Of the 880,727 households in Brooklyn, 38.6% were married couples living together, 22.3% had a female householder with no husband present, and 33.7% were non-families. 33.3% had children under the age of 18 living in them. Of all households 27.8% are made up of individuals and 9.8% had someone living alone who was 65 years of age or older. The average household size was 2.75 and the average family size was 3.41.
In Brooklyn the population was spread out with 26.9% under the age of 18, 10.3% from 18 to 24, 30.8% from 25 to 44, 20.6% from 45 to 64, and 11.5% who are 65 years of age or older. The median age was 33 years. Brooklyn has more women and girls, with 88.4 males for every 100 females. Brooklyn's lesbian community is the largest out of all the New York City boroughs.
The median income for households in Brooklyn was $32,135, and the median income for a family was $36,188. Males had a median income of $34,317, which was higher than females, whose median income was $30,516. The per capita income was $16,775. About 22% of families and 25.1% of the population were below the poverty line, including 34% of those under age 18 and 21.5% of those age 65 or over.
Languages.
Brooklyn has a high degree of linguistic diversity. As of 2010, 54.12% (1,240,416) of Brooklyn residents age 5 and older spoke English at home as a primary language, while 17.16% (393,340) spoke Spanish, 6.46% (148,012) Chinese, 5.31% (121,607) Russian, 3.47% (79,469) Yiddish, 2.75% (63,019) French Creole, 1.35% (31,004) Italian, 1.20% (27,440) Hebrew, 1.01% (23,207) Polish, 0.99% (22,763) French, 0.95% (21,773) Arabic, 0.85% (19,388) various Indic languages, 0.70% (15,936) Urdu, and African languages were spoken as a main language by 0.54% (12,305) of the population over the age of five. In total, 45.88% (1,051,456) of Brooklyn's population age 5 and older spoke a mother language other than English.
Neighborhoods.
Brooklyn's neighborhoods are ever-changing as populations move in and out. For example, during the early to mid-20th century, Brownsville had a majority of Jewish residents; since the 1970s it has been majority African American. Midwood during the early 20th century was filled with ethnic Irish, then filled with Jewish residents for nearly 50 years, and is slowly becoming a Pakistani enclave. Brooklyn's most populous racial group, white, declined from 97.2% in 1930 to 46.9% by 1990.
With gentrification, many of Brooklyn's neighborhoods are becoming increasingly mixed, with an influx of immigrants integrating its neighborhoods. What started as a trend may now be the permanent equilibrium. Brooklyn and Queens have been a worldwide example of poor immigrants getting along most of the time, often with better results than in their home countries. Presently, they have substantial populations from many countries. The borough also attracts people previously living in other cities in the United States. Of these, most come from Chicago, Detroit, San Francisco, Washington, D.C., Baltimore, Philadelphia, Boston, Cincinnati, and Seattle.
Brooklyn contains dozens of distinct neighborhoods, representing many of the major ethnic groups found within the New York City area. The borough is home to a large African American community. Bedford-Stuyvesant is home to one of the most famous African American communities in the city, along with Brownsville, East New York, and Coney Island. "Bed-Stuy" is a hub for African American culture, often referenced in hip hop and African American arts. Brooklyn's African American and Caribbean communities are spread throughout much of Brooklyn.
Brooklyn is also home to many Russians and Ukrainians, who are mainly concentrated in the areas of Brighton Beach and Sheepshead Bay. Brighton Beach features many Russian and Ukrainian businesses. Because of the large Ukrainian community, it has been nicknamed "Little Odessa". However, recently, it has been renamed to "Little Russia" because of the overwhelming presence of the Russian population. Originally these were mostly Jews; however, it is now the non-Jewish Russian and Ukrainian communities of Brighton Beach that represent various aspects of Russian and Ukrainian culture.
Bushwick is the largest hub of Brooklyn's Hispanic American community. Like other neighborhoods in New York City, Bushwick's Hispanic population is mainly Puerto Rican, with many Dominicans and peoples from several South American nations as well. As nearly 80% of Bushwick's population is Hispanic, its residents have created many businesses to support their various national and distinct traditions in food and other items. Sunset Park's population is 42% Hispanic, made up of these various ethnic groups. Brooklyn's main Hispanic groups are Puerto Ricans, Mexicans, Dominicans, and Panamanians, they are spread out throughout the borough. Puerto Ricans and Dominicans are predominant in Bushwick, Williamsburg, and East New York. While Mexicans are predominant in Sunset Park and Panamanians in Crown Heights.
Italian Americans are mainly concentrated in the neighborhoods of Bensonhurst, Dyker Heights and Bay Ridge, where there are many Italian restaurants and pizzerias. Italian Americans live throughout most of southern Brooklyn, including Bath Beach, Gravesend, Marine Park, Mill Basin, and Bergen Beach. The Carroll Gardens area, as well as the northern half of Williamsburg, also have long-standing Italian-American communities.
Orthodox Jews and Hasidic Jews have become concentrated in Borough Park, Williamsburg, and Flatbush, where there are many yeshivas, synagogues, and kosher delicatessens, as well as many other Jewish businesses. Kosher restaurants, synagogues, Jewish schools and yeshivas can be found all over New York City, and many parts as well as in Brooklyn. Other notable religious Orthodox and Hasidic Jewish neighborhoods are Kensington, Midwood, Canarsie, Sea Gate, and Crown Heights. Many hospitals in Brooklyn were started by Jewish charities, including Maimonides Medical Center in Borough Park and Brookdale Hospital in Brownsville. Many non-religious Jews are concentrated in Ditmas Park, Windsor Terrace and Park Slope.
Brooklyn's Polish are largely concentrated in Greenpoint, which is home to Little Poland. They are also scattered throughout the southern parts of Brooklyn.
Brooklyn's West Indian community is concentrated in the Crown Heights, Flatbush, East Flatbush, Kensington, and Canarsie neighborhoods in central Brooklyn. Brooklyn is home to one of the largest communities of West Indians outside of the Caribbean, being rivaled only by Toronto, Miami, Montreal,and London. Although the largest West Indian groups in Brooklyn are mostly Jamaicans, Guyanese and Haitians, there are West Indian immigrants from nearly every part of the Caribbean. Crown Heights and Flatbush are home to many of Brooklyn's West Indian restaurants and bakeries. Brooklyn has an annual, celebrated Carnival in the tradition of pre-Lenten celebrations in the islands. Started by Trinidadians, West Indian Labor Day Parade, takes place every Labor Day on Eastern Parkway.
Brooklyn's Greek Americans live throughout the borough, but their businesses today are concentrated in Downtown Brooklyn near Atlantic Avenue. Greek-owned diners, like El-Greco on Sheepshead Bay, are also throughout the borough, but many Greeks have re-located off of Atlantic Avenue due to demographic shift.
Chinese Americans live throughout the southern parts of Brooklyn, in Sunset Park, Bath Beach, Bensonhurst, Gravesend, and Homecrest. The largest concentration is in Sunset Park along 8th Avenue, which is known for Chinese culture. It is called "Brooklyn's Chinatown". Many Chinese restaurants can be found throughout Sunset Park, and the area hosts a popular Chinese New Year celebration.
Irish Americans can be found throughout Brooklyn, in low to moderate concentrations in the neighborhoods of Bay Ridge, Marine Park, Gerritsen Beach, and Vinegar Hill. Many moved east on Long Island in the mid-twentieth century.
Today, Arab Americans and Pakistanis along with other Muslim communities have moved into the southwest portion of Brooklyn, particularly to Bay Ridge, where there are many Middle Eastern restaurants, hookah lounges, halal shops, Islamic shops and mosques. Coney Island Avenue is home to Little Pakistan as Church Avenue is to Bangladeshis. Jay Street Borough Hall (Downtown Brooklyn) is little Arabia. Pakistani Independence Day is celebrated every year with parades and parties on Coney Island Avenue. Earlier, the area was known predominately for its Irish, Norwegian, and Scottish populations. There are also many Middle Eastern, particularly Yemeni, businesses, mosques, and restaurants on Atlantic Avenue west of Flatbush Avenue, near Brooklyn Heights.
The Brooklyn Heights Promenade (lower center, above the Brooklyn–Queens Expressway) and Brooklyn Bridge Park (in front of the promenade) along the East River (foreground). Brooklyn Heights can be seen in the background.
Culture.
Cultural attractions.
Brooklyn has played a major role in various aspects of American culture including literature, cinema and theater. The Brooklyn accent is often portrayed as "typical New York" in American television and film. 
Brooklyn hosts the world-renowned Brooklyn Academy of Music, the Brooklyn Philharmonic, and the second largest public art collection in the United States, housed in the Brooklyn Museum.
The Brooklyn Museum, opened in 1897, is New York City's second-largest public art museum. It has in its permanent collection more than 1.5 million objects, from ancient Egyptian masterpieces to contemporary art. The Brooklyn Children's Museum, the world's first museum dedicated to children, opened in December 1899. The only such New York State institution accredited by the American Alliance of Museums, it is one of the few globally to have a permanent collection – over 30,000 cultural objects and natural history specimens.
The Brooklyn Academy of Music (BAM) includes a 2,109-seat opera house, an 874-seat theater, and the art house BAM Rose Cinemas. Bargemusic and St. Ann's Warehouse are located on the other side of Downtown Brooklyn in the DUMBO arts district. Brooklyn Technical High School has the second-largest auditorium in New York City (after Radio City Music Hall), with a seating capacity of over 3,000.
Media.
Local periodicals.
Brooklyn has several local newspapers: The "Brooklyn Daily Eagle", "Bay Currents" (Oceanfront Brooklyn), "Brooklyn View", "The Brooklyn Paper", and Courier-Life Publications. Courier-Life Publications, owned by Rupert Murdoch's News Corporation, is Brooklyn's largest chain of newspapers. Brooklyn is also served by the major New York dailies, including "The New York Times", the "New York Daily News", and the "New York Post".
The borough is home to the bi-weekly cultural guide "The L Magazine" and the arts and politics monthly "Brooklyn Rail", as well as the arts and cultural quarterly "Cabinet".
"Brooklyn Magazine" is one of the few glossy magazines about Brooklyn. Several others, that are now defunct, include: "BKLYN Magazine" (a bimonthly lifestyle book owned by Joseph McCarthy, that saw itself as a vehicle for high-end advertisers in Manhattan and was mailed to 80,000 high-income households), "Brooklyn Bridge Magazine", "The Brooklynite" (a free, glossy quarterly edited by Daniel Treiman), and "NRG" (edited by Gail Johnson and originally marketed as a local periodical for Clinton Hill and Fort Greene, but expanded in scope to become the self-proclaimed "Pulse of Brooklyn" and then the "Pulse of New York").
Ethnic press.
Brooklyn has a thriving ethnic press. "El Diario La Prensa", the largest and oldest Spanish-language daily newspaper in the United States, maintains its corporate headquarters at 1 MetroTech Center in downtown Brooklyn. Major ethnic publications include the Brooklyn-Queens Catholic paper "The Tablet" and "Hamodia", an Orthodox Jewish daily. Many nationally distributed ethnic newspapers are based in Brooklyn. Over 60 ethnic groups, writing in 42 languages, publish some 300 non-English language magazines and newspapers in New York City. Among them the quarterly "L'Idea", a bilingual magazine printed in Italian and English since 1974. In addition, many newspapers published abroad, such as "The Daily Gleaner" and "The Star" of Jamaica, are available in Brooklyn.
Our Time Press published weekly by DBG Media covers the Village of Brooklyn with a motto of "The Local paper with the Global View".
Television.
The City of New York has an official television station, run by the NYC Media Group, which features programming based in Brooklyn. Brooklyn Community Access Television is the borough's public access channel.
Parks and other attractions.
Sports.
Brooklyn's major professional sports teams are the NBA's Brooklyn Nets and the NHL's New York Islanders. The Nets moved into the borough in 2012 and play their home games at Barclays Center in Prospect Heights. Prior to that, they had played in Long Island and New Jersey. The Islanders will begin playing in the Barclays Center in 2015 after playing in Nassau County, Long Island since their inception in 1972.
Brooklyn also has a storied sports history. It has been home to many famous sports figures such as Vince Lombardi, Mike Tyson, Joe Torre, and Vitas Gerulaitis. Basketball legend Michael Jordan was born in Brooklyn though he grew up in Wilmington, North Carolina.
In the earliest days of organized baseball, Brooklyn teams dominated the new game. The second recorded game of baseball was played near what is today Fort Greene Park on October 24, 1845. Brooklyn’s Excelsiors, Atlantics and Eckfords were the leading teams from the mid-1850s through the Civil War, and there were dozens of local teams with neighborhood league play, such as at Mapleton Oval. During this “Brooklyn era”, baseball’s rules evolved into the modern game: the first fastball, first changeup, first batting average, first triple play, first pro baseball player, first enclosed ballpark, first scorecard, first known African-American team, first black championship game, first road trip, first gambling scandal, and first eight pennant winners were all in or from Brooklyn.
Brooklyn's most famous historical team, the Brooklyn Dodgers, named for "trolley dodgers" played at Ebbets Field. In 1947 Jackie Robinson was hired by the Dodgers as the first African-American player in Major League Baseball in the modern era. In 1955, the Dodgers, perennial National League pennant winners, won the only World Series for Brooklyn against their rival New York Yankees. The event was marked by mass euphoria and celebrations. Just two years later, the Dodgers moved to Los Angeles. Walter O'Malley, the team's owner at the time, is still vilified, even by Brooklynites too young to remember the Dodgers as Brooklyn's ball club. 
After a 43-year hiatus, professional baseball returned to the borough in 2001 with the Brooklyn Cyclones, a minor league team that plays in MCU Park in Coney Island. They are an affiliate of the New York Mets.
Transportation.
About 57 percent of all households in Brooklyn were households without automobiles. The citywide rate is 55 percent in New York City.
Public transport.
Brooklyn features extensive public transit. Eighteen New York City Subway services, including the Franklin Avenue Shuttle, traverse the borough. Approximately 92.8% of Brooklyn residents traveling to Manhattan use the subway, despite the fact that some neighborhoods like Flatlands and Marine Park are poorly served by subway service. Major stations, out of the 170 currently in Brooklyn, include: 
Proposed New York City Subway lines never built include a line along Nostrand or Utica Avenues to Marine Park, as well as a subway line to Spring Creek.
The public bus network covers the entire borough. There is also daily express bus service into Manhattan. New York's famous yellow cabs also provide transportation in Brooklyn, although they are less numerous in the borough. There are three commuter rail stations in Brooklyn: East New York, Nostrand Avenue, and Atlantic Terminal, the terminus of the Atlantic Branch of the Long Island Rail Road. The terminal is located near the Atlantic Avenue – Barclays Center subway station, with ten connecting subway services.
Roadways.
The great majority of limited-access expressways and parkways are located in the western and southern sections of Brooklyn. These include the Brooklyn-Queens Expressway, the Gowanus Expressway (which is part of the Brooklyn-Queens Expressway), the Prospect Expressway (New York State Route 27), the Belt Parkway, and the Jackie Robinson Parkway (formerly the Interborough Parkway). Planned expressways that were never built include the Bushwick Expressway, an extension of I-78 and the Cross-Brooklyn Expressway, I-878. Major thoroughfares include Atlantic Avenue, Fourth Avenue, 86th Street, Kings Highway, Bay Parkway, Ocean Parkway, Eastern Parkway, Linden Boulevard, McGuinness Boulevard, Flatbush Avenue, Pennsylvania Avenue, and Nostrand Avenue.
Much of Brooklyn has only named streets, but Park Slope, Bay Ridge, Sunset Park, Bensonhurst, and Borough Park and the other western sections have numbered streets running approximately northwest to southeast, and numbered avenues going approximately northeast to southwest. East of Dahill Road, lettered avenues (like Avenue M) run east and west, and numbered streets have the prefix "East". South of Avenue O, related numbered streets west of Dahill Road use the "West" designation. This set of numbered streets ranges from West 37th Street to East 108 Street, and the avenues range from A-Z with names substituted for some of them in some neighborhoods (notably Albemarle, Beverley, Cortelyou, Dorchester, Ditmas, Foster, Farragut, Glenwood, Quentin). Numbered streets prefixed by "North" and "South" in Williamsburg, and "Bay", "Beach", "Brighton", "Plumb", "Paerdegat" or "Flatlands" along the southern and southwestern waterfront are loosely based on the old grids of the original towns of Kings County that eventually consolidated to form Brooklyn. These names often reflect the bodies of water or beaches around them, such as Plumb Beach or Paerdegat Basin.
Brooklyn is connected to Manhattan by three bridges, the Brooklyn, Manhattan, and Williamsburg bridges; a vehicular tunnel, the Hugh L. Carey Tunnel (formerly the Brooklyn-Battery Tunnel); and several subway tunnels. The Verrazano-Narrows Bridge links Brooklyn with the more suburban borough of Staten Island. Though much of its border is on land, Brooklyn shares several water crossings with Queens, including the Kosciuszko Bridge (part of the Brooklyn-Queens Expressway), the Pulaski Bridge, and the JJ Byrne Memorial Bridge, all of which carry traffic over Newtown Creek, and the Marine Parkway Bridge connecting Brooklyn to the Rockaway Peninsula.
Waterways.
Brooklyn was long a major shipping port, especially at the Brooklyn Army Terminal in Sunset Park. Most container ship cargo operations have shifted to the New Jersey side of New York Harbor, while the Brooklyn Cruise Terminal in Red Hook is a focal point for New York's growing cruise industry. The "Queen Mary 2", one of the world's largest ocean liners, was designed specifically to fit under the Verrazano-Narrows Bridge, the longest suspension bridge in the United States. She makes regular ports of call at the Red Hook terminal on her transatlantic crossings from Southampton, England.
NY Waterway offers commuter services from the western shore of Brooklyn to points in Lower Manhattan, Midtown, and Long Island City, as well as tours and charters. SeaStreak also offers weekday ferry service between the Brooklyn Army Terminal and the Manhattan ferry slips at Pier 11 downtown and East 34th Street in midtown. A Cross-Harbor Rail Tunnel, originally proposed in the 1920s as a core project for the then new Port Authority of New York is again being studied and discussed as a way to ease freight movements across a large swath of the metropolitan area.
Manhattan Bridge seen from the Brooklyn Bridge Park.
Education.
Education in Brooklyn is provided by a vast number of public and private institutions. Public schools in the borough are managed by the New York City Department of Education, the largest public school system.
Brooklyn Technical High School (commonly called Brooklyn Tech), a New York City public high school, is the largest specialized high school for science, mathematics, and technology in the United States. Brooklyn Tech opened in 1922. Brooklyn Tech is located across the street from Fort Greene Park. This high school was built from 1930 to 1933 at a cost of about $6,000,000 and is 12 stories high. It covers about half of a city block. Brooklyn Tech is noted for its famous alumni (including two Nobel Laureates), its academics, and the large number of graduates attending prestigious universities.
Higher education.
Public colleges.
Brooklyn College is a senior college of the City University of New York, and was the first public coeducational liberal arts college in New York City. The College ranked in the top 10 nationally for the second consecutive year in Princeton Review’s 2006 guidebook, "America’s Best Value Colleges". Many of its students are first and second generation Americans.
Founded in 1970, Medgar Evers College is a senior college of the City University of New York, with a mission to develop and maintain high quality, professional, career-oriented undergraduate degree programs in the context of a liberal arts education. The College offers programs both at the baccalaureate and associate degree levels, as well as Adult and Continuing Education classes for Central Brooklyn residents, corporations, government agencies, and community organizations. Medgar Evers College is a few blocks east of Prospect Park in Crown Heights.
CUNY's New York City College of Technology (City Tech) of The City University of New York (CUNY) (Downtown Brooklyn/Brooklyn Heights) is the largest public college of technology in New York State and a national model for technological education. Established in 1946, City Tech can trace its roots to 1881 when the Technical Schools of the Metropolitan Museum of Art were renamed the New York Trade School. That institution—which became the Voorhees Technical Institute many decades later—was soon a model for the development of technical and vocational schools worldwide. In 1971, Voorhees was incorporated into City Tech.
SUNY Downstate Medical Center, originally founded as the Long Island College Hospital in 1860, is the oldest hospital-based medical school in the United States. The Medical Center comprises the College of Medicine, College of Health Related Professions, College of Nursing, School of Public Health, School of Graduate Studies, and University Hospital of Brooklyn. The Nobel Prize winner Robert F. Furchgott was a member of its faculty. Half of the Medical Center's students are minorities or immigrants. The College of Medicine has the highest percentage of minority students of any medical school in New York State.
Private colleges.
Brooklyn Law School was founded in 1901 and is notable for its diverse student body. Women and African Americans were enrolled in 1909. According to the Leiter Report, a compendium of law school rankings published by Brian Leiter, Brooklyn Law School places 31st nationally for quality of students.
Long Island University is a private university in Brookville on Long Island, with a campus in Downtown Brooklyn with 6,417 undergraduate students. The Brooklyn campus has strong science and medical technology programs, at the graduate and undergraduate levels.
Pratt Institute, in Clinton Hill, is recognized by "U.S. News & World Report" as being one of the top 20 colleges in the Regional Universities North category. Pratt is a private college founded in 1887 with programs in engineering, architecture, and the arts. Many of Pratt's programs are ranked among the top ten in the country. Its graduate interior design program is ranked number one by "U.S. News & World Report" and by "DesignIntelligence". The architecture program at Pratt is ranked as being one of the top ten in the country by "DesignIntelligence". "Kiplinger's Personal Finance" also named Pratt as being one of the country’s best values in private colleges and universities. It was included as one of the top values for academic quality and affordability out of more than 600 private institutions. The school's Brooklyn campus has been named by "Architectural Digest" as being one of the top ten most architecturally significant, along with institutions such as Harvard, Princeton, and University of Virginia. Pratt has over 4700 students, with most at its Brooklyn campus. Graduate programs include library and information science, architecture, and urban planning. Undergraduate programs include architecture, construction management, writing, critical and visual studies, the arts, in total encompassing over 25 programs in all.
The New York University Polytechnic School of Engineering, the United States' second oldest private institute of technology, founded in 1854, has its main campus in Downtown's MetroTech Center, a commercial, civic and educational redevelopment project of which it was a key sponsor. NYU-Poly is one of the 18 schools and colleges that comprise New York University (NYU). NYU-Poly is considered one of the best engineering schools in the world. Forbes.com regularly ranks NYU-Poly among the top 10 in its list of “Top Colleges for Getting Rich”. NYU-Poly is regularly ranked among the top 4 in the nation for alumni with the highest mid-career salaries by CNNMoney.com In 2012, NYU-Poly was ranked #21 by graduate engineering enrollment in the United States according to the American Society for Engineering Education. As of 2013, NYU-Poly ranks #19 by graduate engineering enrollment in the United States according to U.S. News & World Report. The Institute counts 5 Nobel Prize winners (2 Nobel Prize in Physics, 2 Nobel Prize in Physiology or Medicine, 1 Nobel Prize in Chemistry), 3 Putnam Mathematical Competition winners, 2 Wolf Prize in Physics winners, (1 Russ Prize, 3 Gordon Prize, 1 Draper Prize)(also known as Nobel Prizes of Engineering) winners, 2 Turing Award (also known as Nobel Prize of Nobel Prize of computing) winners, 2 W. Wallace McDowell Award (also known as Nobel Prize of Information Technology and Computer Engineering) winners, several National Inventors Hall of Fame inductees, several Congressional Gold Medal winners, 1 List of prolific inventors inductee, multiple Technology & Engineering Emmy Award winners, 3 Israel Prize winners and many Institute of Electrical and Electronics Engineers Awards winners (including 2 IEEE Edison Medal winners and 1 IEEE Medal of Honor winner).
St. Francis College is a Catholic college located in Brooklyn Heights and was founded in 1859 by Franciscan friars. Today, there are over 2,400 students attending the small liberal arts college. St. Francis is considered by the "New York Times" as one of the more diverse colleges, and it has recently been ranked one of the best baccalaureate colleges by both "Forbes" magazine and "U.S. News & World Report".
Brooklyn also has smaller liberal arts institutions, such as Saint Joseph's College in Clinton Hill and Boricua College in Williamsburg.
Community colleges.
Kingsborough Community College is a junior college in the City University of New York system, located in Manhattan Beach.
Brooklyn Public Library.
As an independent system, separate from the New York and Queens public library systems, the Brooklyn Public Library offers thousands of public programs, millions of books, and use of more than 850 free Internet-accessible computers. It also has books and periodicals in all the major languages spoken in Brooklyn, including English, Russian, Chinese, Spanish, Hebrew, and Haitian Kreyol, as well as French, Yiddish, Hindi, Bengali, Polish, Italian, and Arabic. The Central Library is a landmarked building facing Grand Army Plaza.
There are 58 library branches, placing one within a half mile of each Brooklyn resident. In addition to its specialized Business Library in Brooklyn Heights, the Library is preparing to construct its new Visual & Performing Arts Library (VPA) in the BAM Cultural District, which will focus on the link between new and emerging arts and technology and house traditional and digital collections. It will provide access and training to arts applications and technologies not widely available to the public. The collections will include the subjects of art, theater, dance, music, film, photography and architecture. A special archive will house the records and history of Brooklyn's arts communities.
External links.
History:

</doc>
<doc id="47385" url="http://en.wikipedia.org/wiki?curid=47385" title="490s BC">
490s BC


</doc>
<doc id="47386" url="http://en.wikipedia.org/wiki?curid=47386" title="308">
308

Year 308 (CCCVIII) was a leap year starting on Thursday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Valerius and Valerius (or, less frequently, year 1061 "Ab urbe condita"). The denomination 308 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="47387" url="http://en.wikipedia.org/wiki?curid=47387" title="William III of England">
William III of England

William III (Dutch: "Willem III"; 4 November 1650 – 8 March 1702) was sovereign Prince of Orange from birth, Stadtholder of Holland, Zeeland, Utrecht, Gelderland, and Overijssel in the Dutch Republic from 1672, and King of England, Ireland, and Scotland from 1689 until his death. It is a coincidence that his regnal number (III) was the same for both Orange and England. As King of Scotland, he is known as William II. He is informally and affectionately known by sections of the population in Northern Ireland and Scotland as "King Billy". 
William inherited the principality of Orange from his father, William II, who died a week before William's birth. His mother Mary, Princess Royal, was the daughter of King Charles I of England. In 1677, he married his mother's niece and his first cousin, Mary, the daughter of his maternal uncle James, Duke of York.
A Protestant, William participated in several wars against the powerful Catholic king of France, Louis XIV, in coalition with Protestant and Catholic powers in Europe. Many Protestants heralded him as a champion of their faith. In 1685, his Catholic father-in-law, James, became king of England, Ireland and Scotland, but his reign was unpopular with the Protestant majority in Britain. William was invited to invade England by a group of influential political and religious leaders and, in what became known as the "Glorious Revolution", on 5 November 1688, William landed at the southern English port of Brixham. James was deposed and William and Mary became joint sovereigns in James's place. They reigned together until her death on 28 December 1694 after which William ruled as sole monarch.
William's reputation as a strong Protestant enabled him to take the British crowns when many were fearful of a revival of Catholicism under James. William's final victory at the Battle of the Boyne in 1690 is still commemorated by the Orange Order. His reign in Britain marked the beginning of the transition from the personal rule of the Stuarts to the more Parliament-centred rule of the House of Hanover.
Early life.
Birth and family.
William Henry of Orange was born in The Hague in the Dutch Republic on 4 November 1650. He was the only child of stadtholder William II, Prince of Orange, and Mary, Princess Royal. Mary was the eldest daughter of King Charles I of England, Scotland and Ireland, and sister of King Charles II and King James II.
Eight days before William was born, his father died of smallpox; thus William was the Sovereign Prince of Orange from the moment of his birth. Immediately, a conflict ensued between the Princess Royal and William II's mother, Amalia of Solms-Braunfels, over the name to be given to the infant. Mary wanted to name him Charles after her brother, but her mother-in-law insisted on giving him the name William or "Willem" to bolster his prospects of becoming stadtholder. William II had appointed his wife as his son's guardian in his will; however the document remained unsigned at William II's death and was void. On 13 August 1651, the "Hoge Raad van Holland en Zeeland" (Supreme Court) ruled that guardianship would be shared between his mother, his paternal grandmother and Frederick William, the Elector of Brandenburg, whose wife, Louise Henriette, was his father's eldest sister.
Childhood and education.
William's mother showed little personal interest in her son, sometimes being absent for years, and had always deliberately kept herself apart from Dutch society. William's education was first laid in the hands of several Dutch governesses, some of English descent, including Walburg Howard and the Scottish noblewoman, Lady Anna Mackenzie. From April 1656, the prince received daily instruction in the Reformed religion from the Calvinist preacher Cornelis Trigland, a follower of the Contra-Remonstrant theologian Gisbertus Voetius. The ideal education for William was described in "Discours sur la nourriture de S. H. Monseigneur le Prince d'Orange", a short treatise, perhaps by one of William's tutors, Constantijn Huygens. In these lessons, the prince was taught that he was predestined to become an instrument of Divine Providence, fulfilling the historical destiny of the House of Orange.
From early 1659, William spent seven years at the University of Leiden for a formal education, under the guidance of ethics professor Hendrik Bornius (though never officially enrolling as a student). While residing in the "Prinsenhof" at Delft, William had a small personal retinue including Hans Willem Bentinck, and a new governor, Frederick Nassau de Zuylenstein, who (as an illegitimate son of stadtholder Frederick Henry of Orange) was his paternal uncle. He was taught French by Samuel Chappuzeau (who was dismissed by William's grandmother after the death of his mother).
Grand Pensionary Johan de Witt and his uncle Cornelis de Graeff pushed the States of Holland to take charge of William's education. This was to ensure that he would acquire the skills to serve in a future—though undetermined—state function; the States acted on 25 September 1660. This first involvement of the authorities did not last long. On 23 December 1660, when William was 10 years old, his mother died of smallpox at Whitehall Palace, London while visiting her brother King Charles II. In her will, Mary requested that Charles look after William's interests, and Charles now demanded the States of Holland end their interference. To appease Charles, they complied on 30 September 1661. In 1661, Zuylenstein began to work for Charles. He induced William to write letters to Charles asking him to help William become stadtholder someday. After his mother's death, William's education and guardianship became a point of contention between his dynasty's supporters and the advocates of a more republican Netherlands.
The Dutch authorities did their best at first to ignore these intrigues, but in the Second Anglo-Dutch War one of Charles's peace conditions was the improvement of the position of his nephew. As a countermeasure in 1666, when William was 16, the States of Holland officially made him a ward of the government, or a "Child of State". All pro-English courtiers, including Zuylenstein, were removed from William's company. William begged De Witt to allow Zuylenstein to stay, but he refused. De Witt, the leading politician of the Republic, took William's education into his own hands, instructing him weekly in state matters—and joining him in a regular game of real tennis.
Early offices.
Exclusion from stadtholdership.
After William's father's death, most provinces had left the office of stadtholder vacant. The Treaty of Westminster, which ended the First Anglo-Dutch War, had a secret annexe attached on demand of Oliver Cromwell: this required the Act of Seclusion, which forbade the province of Holland from appointing a member of the House of Orange as stadtholder. After the English Restoration, the Act of Seclusion, which had not remained a secret for very long, was declared void as the English Commonwealth (with which the treaty had been concluded) no longer existed. In 1660, Mary and Amalia tried to persuade several provincial States to designate William as their future stadtholder, but they all initially refused.
In 1667, as William III approached the age of 18, the Orangist party again attempted to bring him to power by securing for him the offices of stadtholder and Captain-General. To prevent the restoration of the influence of the House of Orange, De Witt, the leader of the States Party, allowed the pensionary of Haarlem, Gaspar Fagel, to induce the States of Holland to issue the Perpetual Edict. The Edict declared that the Captain-General or Admiral-General of the Netherlands could not serve as stadtholder in any province. Even so, William's supporters sought ways to enhance his prestige and, on 19 September 1668, the States of Zeeland received him as "First Noble". To receive this honour, William had to escape the attention of his state tutors and travel secretly to Middelburg. A month later, Amalia allowed William to manage his own household and declared him to be of majority age.
The province of Holland, the center of anti-Orangism, abolished the office of stadtholder and four other provinces followed suit in March 1670, establishing the so-called "Harmony". De Witt demanded an oath from each Holland regent (city council member) to uphold the Edict; all but one complied. William saw all this as a defeat, but in fact this arrangement was a compromise: De Witt would have preferred to ignore the prince completely, but now his eventual rise to the office of supreme army commander was implicit. De Witt further conceded that William would be admitted as a member of the "Raad van State", the Council of State, then the generality organ administering the defence budget. William was introduced to the council on 31 May 1670 with full voting powers, despite De Witt's attempts to limit his role to that of an advisor.
Conflict with republicans.
In November 1670, William obtained permission to travel to England to urge Charles to pay back at least a part of the 2,797,859 guilder debt the House of Stuart owed the House of Orange. Charles was unable to pay, but William agreed to reduce the amount owed to 1,800,000 guilders. Charles found his nephew to be a dedicated Calvinist and patriotic Dutchman, and reconsidered his desire to show him the Secret treaty of Dover with France, directed at destroying the Dutch Republic and installing William as "sovereign" of a Dutch rump state. In addition to differing political outlooks, William found that Charles's and James's lifestyles differed from his own, being more concerned with drinking, gambling, and cavorting with mistresses.
The following year, the Republic's security deteriorated quickly as an Anglo-French attack became imminent. In view of the threat, the States of Gelderland wanted William to be appointed Captain-General of the Dutch States Army as soon as possible, despite his youth and inexperience. On 15 December 1671, the States of Utrecht made this their official policy. On 19 January 1672, the States of Holland made a counterproposal: to appoint William for just a single campaign. The prince refused this and on 25 February a compromise was reached: an appointment by the States General of the Netherlands for one summer, followed by a permanent appointment on his 22nd birthday. 
Meanwhile, William had written a secret letter to Charles in January 1672 asking his uncle to exploit the situation by exerting pressure on the States to appoint William stadtholder. In return, William would ally the Republic with England and serve Charles's interests as much as his "honour and the loyalty due to this state" allowed. Charles took no action on the proposal, and continued his war plans with his French ally.
Becoming stadtholder.
"Disaster year": 1672.
For the Dutch Republic, 1672 proved calamitous, becoming known as the "disaster year" (Dutch: "rampjaar") because of the Franco-Dutch War and the Third Anglo-Dutch War in which the Netherlands were invaded by France under Louis XIV, England, Münster, and Cologne. Although the Anglo-French fleet was disabled by the Battle of Solebay, in June the French army quickly overran the provinces of Gelderland and Utrecht. On 14 June, William withdrew with the remnants of his field army into Holland, where the States had ordered the flooding of the Dutch Water Line on 8 June. Louis XIV, believing the war was over, began negotiations to extract as large a sum of money from the Dutch as possible. The presence of a large French army in the heart of the Republic caused a general panic, and the people turned against de Witt and his allies.
On 4 July, the States of Holland appointed William stadtholder, and he took the oath five days later. The next day, a special envoy from Charles, Lord Arlington, met with William in Nieuwerbrug. He offered to make William Sovereign Prince of Holland in exchange for his capitulation—whereas a stadtholder was a mere civil servant. When William refused, Arlington threatened that William would witness the end of the republic's existence. William made his famous answer: "There is one way to avoid this: to die defending it in the last ditch". On 7 July, the inundations were complete and the further advance of the French army was effectively blocked. On 16 July, Zeeland offered the stadtholderate to William.
Johan de Witt had been unable to function as Grand Pensionary after having been wounded by an attempt on his life on 21 June. On 15 August, William published a letter from Charles, in which the English King stated that he had made war because of the aggression of the de Witt faction. The people thus incited, de Witt and his brother, Cornelis, were murdered by an Orangist civil militia in The Hague on 20 August. After this, William replaced many of the Dutch regents with his followers.
Though William's complicity in the lynching has never been proved (and some 19th-century Dutch historians have made an effort to disprove that he was an accessory before the fact) he thwarted attempts to prosecute the ringleaders, and even rewarded some, like Hendrik Verhoeff, with money, and others, like Johan van Banchem and Johan Kievit, with high offices. This damaged his reputation in the same fashion as his later actions at Glencoe.
William III continued to fight against the invaders from England and France, allying himself with Spain and Brandenburg. In November 1672, he took his army to Maastricht to threaten the French supply lines. By 1673, the situation further improved. Although Louis took Maastricht and William's attack against Charleroi failed, Lieutenant-Admiral Michiel de Ruyter defeated the Anglo-French fleet three times, forcing Charles to end England's involvement by the Treaty of Westminster; after 1673, France slowly withdrew from Dutch territory (with the exception of Maastricht), while making gains elsewhere.
Fagel now proposed to treat the liberated provinces of Utrecht, Gelderland and Overijssel as conquered territory (Generality Lands), as punishment for their quick surrender to the enemy. William refused but obtained a special mandate from the States General to newly appoint all delegates in the States of these provinces. William's followers in the States of Utrecht on 26 April 1674 appointed him hereditary stadtholder. On 30 January 1675, the States of Gelderland offered the titles of Duke of Guelders and Count of Zutphen. The negative reactions to this from Zeeland and the city of Amsterdam, where the stock market collapsed, made William ultimately decide to decline these honours; he was instead appointed stadtholder of Gelderland and Overijssel.
Marriage.
During the war with France, William tried to improve his position by marrying, in 1677, his first cousin Mary, elder surviving daughter of James, Duke of York, later James II of England (James VII of Scotland). Mary was eleven years his junior and he anticipated resistance to a Stuart match from the Amsterdam merchants who had disliked his mother (another Mary Stuart), but William believed that marrying Mary would increase his chances of succeeding to Charles's kingdoms, and would draw England's monarch away from his pro-French policies. James was not inclined to consent, but Charles II pressured his brother to agree. Charles wanted to use the possibility of marriage to gain leverage in negotiations relating to the war, but William insisted that the two issues be decided separately. Charles relented, and Bishop Henry Compton married the couple on 4 November 1677. Mary became pregnant soon after the marriage, but miscarried. After a further illness later in 1678, she never conceived again.
Throughout William and Mary's marriage, William had only one acknowledged mistress, Elizabeth Villiers, in contrast to the many mistresses his uncles openly kept.
Peace with France, intrigue with England.
By 1678, Louis sought peace with the Dutch Republic. Even so, tensions remained: William remained very suspicious of Louis, thinking that the French king desired "Universal Kingship" over Europe; Louis described William as "my mortal enemy" and saw him as an obnoxious warmonger. France's annexations in the Southern Netherlands and Germany (the "Réunion" policy) and the revocation of the Edict of Nantes in 1685, caused a surge of Huguenot refugees to the Republic. This led William III to join various anti-French alliances, such as the Association League, and ultimately the League of Augsburg (an anti-French coalition that also included the Holy Roman Empire, Sweden, Spain and several German states) in 1686.
After his marriage in November 1677, William became a strong candidate for the English throne if his father-in-law (and uncle) James were excluded because of his Catholicism. During the crisis concerning the Exclusion Bill in 1680, Charles at first invited William to come to England to bolster the king's position against the "exclusionists", then withdrew his invitation—after which Lord Sunderland also tried unsuccessfully to bring William over, but now to put pressure on Charles. Nevertheless, William secretly induced the States General to send the "Insinuation" to Charles, beseeching the king to prevent any Catholics from succeeding him, without explicitly naming James. After receiving indignant reactions from Charles and James, William denied any involvement.
In 1685, when James II succeeded Charles, William at first attempted a conciliatory approach, at the same time trying not to offend the Protestants in England. William, ever looking for ways to diminish the power of France, hoped that James would join the League of Augsburg, but by 1687 it became clear that James would not join the anti-French alliance. Relations worsened between William and James thereafter. In November, James's second wife Mary of Modena was announced to be pregnant. That month, to gain the favour of English Protestants, William wrote an open letter to the English people in which he disapproved of James's pro-Roman Catholic policy of religious toleration. Seeing him as a friend, and often having maintained secret contacts with him for years, many English politicians began to urge an armed invasion of England.
Glorious Revolution.
Invasion of England.
William at first opposed the prospect of invasion, but most historians now agree that he began to assemble an expeditionary force in April 1688, as it became increasingly clear that France would remain occupied by campaigns in Germany and Italy, and thus unable to mount an attack while William's troops would be occupied in Britain. Believing that the English people would not react well to a foreign invader, he demanded in a letter to Rear-Admiral Arthur Herbert that the most eminent English Protestants first invite him to invade. In June, James's second wife, Mary of Modena, after a string of miscarriages, bore a son (James Francis Edward Stuart), who displaced William's Protestant wife, Mary, to become first in the line of succession and raised the prospect of an ongoing Catholic monarchy. Public anger also increased because of the trial of seven bishops who had publicly opposed James's Declaration of Indulgence granting religious liberty to his subjects, a policy which appeared to threaten the establishment of the Anglican Church.
On 30 June 1688—the same day the bishops were acquitted—a group of political figures known afterward as the "Immortal Seven", sent William a formal invitation. William's intentions to invade were public knowledge by September 1688. With a Dutch army, William landed at Brixham in southwest England on 5 November 1688. He came ashore from the ship "Brill", proclaiming "the liberties of England and the Protestant religion I will maintain". William had come ashore with approximately 11,000-foot and 4,000 horse soldiers. James's support began to dissolve almost immediately upon William's arrival; Protestant officers defected from the English army (the most notable of whom was Lord Churchill of Eyemouth, James's most able commander), and influential noblemen across the country declared their support for the invader.
James at first attempted to resist William, but saw that his efforts would prove futile. He sent representatives to negotiate with William, but secretly attempted to flee on 11 December. He was discovered and brought back to London by a group of fishermen. He was allowed to escape to France in a second attempt on 23 December. William permitted James to leave the country, not wanting to make him a martyr for the Roman Catholic cause.
Proclaimed king.
William summoned a Convention Parliament in England, which met on 22 January 1689, to discuss the appropriate course of action following James's flight. William felt insecure about his position; though his wife ranked first in the line of succession to the throne, he wished to reign as King in his own right, rather than as a mere consort. The only precedent for a joint monarchy in England dated from the 16th century, when Queen Mary I married Philip of Spain. Philip remained king only during his wife's lifetime, and restrictions were placed on his power. William, on the other hand, demanded that he remain as king even after his wife's death. When the majority of Tory Lords proposed to acclaim her as sole ruler, William threatened to leave the country immediately. Furthermore, Mary, remaining loyal to her husband, refused.
The House of Commons, with a Whig majority, quickly resolved that the throne was vacant, and that it was safer if the ruler was Protestant. There were more Tories in the House of Lords, which would not initially agree, but after William refused to be a regent or to agree to remain king only in his wife's lifetime, there were negotiations between the two houses and the Lords agreed by a narrow majority that the throne was vacant. The Commons made William accept a Bill of Rights, and, on 13 February 1689, Parliament passed the Declaration of Right, in which it deemed that James, by attempting to flee, had abdicated the government of the realm, thereby leaving the throne vacant. 
The Crown was not offered to James's eldest son, James Francis Edward (who would have been the heir apparent under normal circumstances), but to William and Mary as joint sovereigns. It was, however, provided that "the sole and full exercise of the regal power be only in and executed by the said Prince of Orange in the names of the said Prince and Princess during their joint lives".
William and Mary were crowned together at Westminster Abbey on 11 April 1689 by the Bishop of London, Henry Compton. Normally, the coronation is performed by the Archbishop of Canterbury, but the Archbishop at the time, William Sancroft, refused to recognise James's removal. The coronation to William was "a popish mockery".
William also summoned a Convention of the Estates of Scotland, which met on 14 March 1689 and sent a conciliatory letter, while James sent haughty uncompromising orders, swaying a majority in favour of William. On 11 April, the day of the English coronation, the Convention finally declared that James was no longer King of Scotland. William and Mary were offered the Scottish Crown; they accepted on 11 May.
Revolution settlement.
William III of England encouraged the passage of the Act of Toleration, which guaranteed religious toleration to certain Protestant nonconformists. It did not, however, extend toleration as far as William wished, still restricting the religious liberty of Roman Catholics, non-trinitarians, and those of non-Christian faiths. In December 1689, one of the most important constitutional documents in English history, the Bill of Rights, was passed. The Act, which restated and confirmed many provisions of the earlier Declaration of Right, established restrictions on the royal prerogative. It provided, amongst other things, that the Sovereign could not suspend laws passed by Parliament, levy taxes without parliamentary consent, infringe the right to petition, raise a standing army during peacetime without parliamentary consent, deny the right to bear arms to Protestant subjects, unduly interfere with parliamentary elections, punish members of either House of Parliament for anything said during debates, require excessive bail or inflict cruel and unusual punishments. William was opposed to the imposition of such constraints, but he chose not to engage in a conflict with Parliament and agreed to abide by the statute.
The Bill of Rights also settled the question of succession to the Crown. After the death of either William or Mary, the other would continue to reign. Next in the line of succession was Mary II's sister, Princess Anne, and her issue. Finally, any children William might have had by a subsequent marriage were included in the line of succession. Roman Catholics, as well as those who married Catholics, were excluded.
Rule with Mary II.
Resistance to validity of rule.
Although most in Britain accepted William and Mary as sovereigns, a significant minority refused to acknowledge their claim to the throne, instead believing in the divine right of kings, which held that the monarch's authority derived directly from God rather than being delegated to the monarch by Parliament. Over the next 57 years Jacobites pressed for restoration of James and his heirs. Nonjurors in England and Scotland, including over 400 clergy and several bishops of the Church of England and Scottish Episcopal Church as well as numerous laymen, refused to take oaths of allegiance to William.
Ireland was controlled by Roman Catholics loyal to James, and Franco-Irish Jacobites arrived from France with French forces in March 1689 to join the war in Ireland and contest Protestant resistance at the Siege of Derry. William sent his navy to the city in July, and his army landed in August. After progress stalled, William personally intervened to lead his armies to victory over James at the Battle of the Boyne on 1 July 1690, after which James II fled back to France.
Upon King William's return to England, his close friend Dutch General Godert de Ginkell, who had accompanied William to Ireland and had commanded a body of Dutch cavalry at the Battle of the Boyne, was named Commander in Chief of William's forces in Ireland and entrusted with further conduct of the war there. Ginkell took command in Ireland in the spring of 1691, and following several ensuing battles, succeeded in capturing both Galway and Limerick, thereby effectively suppressing the Jacobite forces in Ireland within a few more months. After difficult negotiations a capitulation was signed on 3 October 1691—the Treaty of Limerick. Thus concluded the Williamite pacification of Ireland, and for his services the Dutch general received the formal thanks of the House of Commons, and was awarded the title of Earl of Athlone by the King.
A series of Jacobite risings also took place in Scotland, where Viscount Dundee raised Highland forces and won a victory on 27 July 1689 at the Battle of Killiecrankie, but he died in the fight and a month later Scottish Cameronian forces subdued the rising at the Battle of Dunkeld. William offered Scottish clans that had taken part in the rising a pardon provided that they signed allegiance by a deadline, and his government in Scotland punished a delay with the Massacre of Glencoe of 1692, which became infamous in Jacobite propaganda as William had countersigned the orders. Bowing to public opinion, William dismissed those responsible for the massacre, though they still remained in his favour; in the words of the historian John Dalberg-Acton, "one became a colonel, another a knight, a third a peer, and a fourth an earl."
William's reputation in Scotland suffered further damage when he refused English assistance to the Darien scheme, a Scottish colony (1698–1700) that failed disastrously.
Parliament and faction.
Although the Whigs were William's strongest supporters, he initially favoured a policy of balance between the Whigs and Tories. The Marquess of Halifax, a man known for his ability to chart a moderate political course, gained William's confidence early in his reign. The Whigs, a majority in Parliament, had expected to dominate the government, and were disappointed that William denied them this chance. This "balanced" approach to governance did not last beyond 1690, as the conflicting factions made it impossible for the government to pursue effective policy, and William called for new elections early that year.
After the Parliamentary elections of 1690, William began to favour the Tories, led by Danby and Nottingham. While the Tories favoured preserving the king's prerogatives, William found them unaccommodating when he asked Parliament to support his continuing war with France. As a result, William began to prefer the Whig faction known as the Junto. The Whig government was responsible for the creation of the Bank of England following the example of the Amsterdam Bank. William's decision to grant the Royal Charter in 1694 to the Bank, a private institution owned by bankers, is his most relevant economic legacy. It laid the financial foundation of the English take-over of the central role of the Dutch Republic and Bank of Amsterdam in global commerce in the 18th century.
William dissolved Parliament in 1695, and the new Parliament that assembled that year was led by the Whigs. There was a considerable surge in support for William following the exposure of a Jacobite plan to assassinate him in 1696. Parliament passed a bill of attainder against the ringleader, John Fenwick, and he was beheaded in 1697.
War in Europe.
William continued to be absent from the realm for extended periods during his war with France, leaving each spring and returning to England each autumn. England joined the League of Augsburg, which then became known as the Grand Alliance. Whilst William was away fighting, his wife, Mary II, governed the realm, but acted on his advice. Each time he returned to England, Mary gave up her power to him without reservation, an arrangement that lasted for the rest of Mary's life.
After the Anglo-Dutch fleet defeated a French fleet at La Hogue in 1692, the allies for a short period controlled the seas, and Ireland was pacified thereafter by the Treaty of Limerick. At the same time, the Grand Alliance fared poorly in Europe, as William lost Namur in the Spanish Netherlands in 1692, and was badly beaten at the Battle of Landen in 1693.
Later years.
Mary II died of smallpox on 28 December 1694, leaving William III to rule alone. William deeply mourned his wife's death. Despite his conversion to Anglicanism, William's popularity plummeted during his reign as a sole monarch.
Allegations of homosexuality.
During the 1690s, rumours grew of William's alleged homosexual inclinations and led to the publication of many satirical pamphlets by his Jacobite detractors. He did have several close, male associates, including two Dutch courtiers to whom he granted English titles: Hans Willem Bentinck became Earl of Portland, and Arnold Joost van Keppel was created Earl of Albemarle. These relationships with male friends, and his apparent lack of more than one mistress, led William's enemies to suggest that he might prefer homosexual relationships. William's modern biographers, however, still disagree on the veracity of these allegations. Some have suggested that there may have been some truth to the rumours, while more affirm that they were no more than figments of his enemies' imaginations, and that there was nothing unusual in someone childless like William adopting, or evincing paternal affections for, a younger man.
Bentinck's closeness to William did arouse jealousies in the Royal Court at the time, but most modern historians doubt that there was a homosexual element in their relationship. William's young protege, Keppel, aroused more gossip and suspicion, being 20 years William's junior and strikingly handsome, and having risen from being a royal page to an earldom with some ease. Portland wrote to William in 1697 that "the kindness which your Majesty has for a young man, and the way in which you seem to authorise his liberties ... make the world say things I am ashamed to hear". This, he said, was "tarnishing a reputation which has never before been subject to such accusations". William tersely dismissed these suggestions, however, saying, "It seems to me very extraordinary that it should be impossible to have esteem and regard for a young man without it being criminal."
Peace with France.
In 1696, the Dutch territory of Drenthe made William its Stadtholder. In the same year, Jacobites plotted to assassinate William III in an attempt to restore James to the English throne, but failed. In accordance with the Treaty of Rijswijk (20 September 1697), which ended the Nine Years' War, Louis recognised William III as King of England, and undertook to give no further assistance to James II. Thus deprived of French dynastic backing after 1697, Jacobites posed no further serious threats during William's reign.
As his life drew towards its conclusion, William, like many other European rulers, felt concern over the question of succession to the throne of Spain, which brought with it vast territories in Italy, the Low Countries and the New World. The King of Spain, Charles II, was an invalid with no prospect of having children; amongst his closest relatives were Louis XIV (the King of France) and Leopold I, Holy Roman Emperor. William sought to prevent the Spanish inheritance from going to either monarch, for he feared that such a calamity would upset the balance of power. William and Louis XIV agreed to the First Partition Treaty, which provided for the division of the Spanish Empire: Duke Joseph Ferdinand of Bavaria would obtain Spain, while France and the Holy Roman Emperor would divide the remaining territories between them. Charles II accepted the nomination of Joseph Ferdinand as his heir, and war appeared to be averted.
When, however, Joseph Ferdinand died of smallpox, the issue re-opened. In 1700, the two rulers agreed to the Second Partition Treaty (also called the Treaty of London), under which the territories in Italy would pass to a son of the King of France, and the other Spanish territories would be inherited by a son of the Holy Roman Emperor. This arrangement infuriated both the Spanish, who still sought to prevent the dissolution of their empire, and the Holy Roman Emperor, to whom the Italian territories were much more useful than the other lands. Unexpectedly, the invalid King of Spain, Charles II, interfered as he lay dying in late 1700. Unilaterally, he willed all Spanish territories to Philip, a grandson of Louis XIV. The French conveniently ignored the Second Partition Treaty and claimed the entire Spanish inheritance. Furthermore, Louis XIV alienated William III by recognising James Francis Edward Stuart, the son of the former King James II who had died in 1701, as King of England. The subsequent conflict, known as the War of the Spanish Succession, continued until 1713.
English succession.
The Spanish inheritance was not the only one which concerned William. His marriage with Mary II had not yielded any children, and he did not seem likely to remarry. Mary's sister, Princess Anne, had borne numerous children, all of whom died during childhood. The death of Prince William, Duke of Gloucester, in 1700 left Princess Anne as the only individual left in the line of succession established by the Bill of Rights. As the complete exhaustion of the line of succession would have encouraged a restoration of James II's line, Parliament passed the Act of Settlement 1701, which provided that if Anne died without surviving issue and William III failed to have surviving issue by any subsequent marriage, the Crown would be inherited by a distant relative, Sophia, Electress of Hanover, a granddaughter of King James VI, and her Protestant heirs. The Act debarred Roman Catholics from the throne thereby excluding the candidacy of several dozen people more closely related to Mary and Anne than Sophia. The Act extended to England and Ireland, but not to Scotland, whose Estates had not been consulted before the selection of Sophia.
Death.
In 1702, William died of pneumonia, a complication from a broken collarbone following a fall from his horse, Sorrel. Because his horse had stumbled into a mole's burrow, many Jacobites toasted "the little gentleman in the black velvet waistcoat." Years later, Sir Winston Churchill, in his "A History of the English-Speaking Peoples", stated that the fall "opened the door to a troop of lurking foes". William was buried in Westminster Abbey alongside his wife. His sister-in-law, Anne, became queen regnant of England, Scotland and Ireland.
William's death brought an end to the Dutch House of Orange, members of which had served as stadtholder of Holland and the majority of the other provinces of the Dutch Republic since the time of William the Silent (William I). The five provinces of which William III was stadtholder—Holland, Zeeland, Utrecht, Gelderland and Overijssel—all suspended the office after his death. Thus, he was the last patrilineal descendant of William I to be named stadtholder for the majority of the provinces. Under William III's will, John William Friso stood to inherit the Principality of Orange as well as several lordships in the Netherlands. He was William's closest agnatic relative, as well as son of William's aunt Albertine Agnes. However, King Frederick I of Prussia also claimed the Principality as the senior cognatic heir, his mother Louise Henriette being Albertine Agnes's older sister. Under the Treaty of Utrecht, which was agreed to in 1713, Frederick William I of Prussia ceded his territorial claim to King Louis XIV of France, keeping the title only; Friso's son, William IV, agreed to share the title of "Prince of Orange", which had accumulated high prestige in the Netherlands as well as in the entire Protestant world, with Frederick William after the Treaty of Partition (1732).
Legacy.
William's primary achievement was to contain France when it was in a position to impose its will across much of Europe. His life's aim was largely to oppose Louis XIV of France. This effort continued after his death during the War of the Spanish Succession. Another important consequence of William's reign in England involved the ending of a bitter conflict between Crown and Parliament that had lasted since the accession of the first English monarch of the House of Stuart, James I, in 1603. The conflict over royal and parliamentary power had led to the English Civil War during the 1640s and the Glorious Revolution of 1688. During William's reign, however, the conflict was settled in Parliament's favour by the Bill of Rights 1689, the Triennial Act 1694 and the Act of Settlement 1701.
William endowed the College of William and Mary (in present day Williamsburg, Virginia) in 1693. Nassau, the capital of The Bahamas, is named after Fort Nassau, which was renamed in 1695 in his honour. Similarly Nassau County, New York a county on Long Island, is a namesake. Long Island itself was also known as Nassau during early Dutch rule. Though many alumni of Princeton University think that the town of Princeton, N.J. (and hence the university) were named in his honour, this is probably untrue. Nassau Hall, at the university campus, is so named, however.
New York City was briefly renamed New Orange for him in 1673 after the Dutch recaptured the city, which had been renamed New York by the British in 1665. His name was applied to the fort and administrative center for the city on two separate occasions reflecting his different sovereign status—first as Fort Willem Hendrick in 1673, and then as Fort William in 1691 when the English evicted Colonists who had seized the fort and city. Nassau Street, NY was also named some time before 1696 in his honor. Orange County, just north of New York City, is his namesake, as was Fort Orange (now Albany).
Ireland.
The modern day Orange Order is named after William III, and makes a point of celebrating his victory at the Battle of the Boyne on 12 July with annual parades by Orangemen in Northern Ireland, Liverpool and parts of Scotland and Canada.
Titles, styles, and arms.
Titles and styles.
By 1674, William was fully styled as "Willem III, by God's grace Prince of Orange, Count of Nassau etc., Stadtholder of Holland, Zeeland, Utrecht etc., Captain- and Admiral-General of the United Netherlands". After their accession in Great Britain in 1689, William and Mary used the titles "King and Queen of England, Scotland, France and Ireland, Defenders of the Faith, etc."
Arms.
As Prince of Orange, William's arms was: Quarterly, I Azure billetty and a lion rampant Or (for Nassau); II Or a lion rampant guardant Gules crowned Azure (Katzenelnbogen); III Gules a fess Argent (Vianden), IV Gules two lions passant guardant Or (Dietz); between the I and II quarters an inescutcheon, Or a fess Sable (Moers); at the fess point an inescutcheon, quarterly I and IV Gules, a bend Or (Châlons); II and III Or a bugle horn Azure, stringed Gules (Orange) with an inescutcheon, Nine pieces Or and Azure (Geneva); between the III and IV quarters, an inescutcheon, Gules a fess counter embattled Argent (Buren).
The coat of arms used by the King and Queen was: Quarterly, I and IV Grand quarterly, Azure three fleurs-de-lis Or (for France) and Gules three lions passant guardant in pale Or (for England); II Or a lion rampant within a double tressure flory-counter-flory Gules (for Scotland); III Azure a harp Or stringed Argent (for Ireland); overall an escutcheon Azure billetty and a lion rampant Or (for Nassau). In his later coat of arms, William used the motto: "Je Maintiendrai" (medieval French for "I will maintain"). The motto represents the House of Orange-Nassau, since it came into the family with the Principality of Orange.
In popular culture.
William has been played on screen by Bernard Lee in the 1937 film "The Black Tulip", based on the novel by Alexandre Dumas, père, Henry Daniell in the 1945 film "Captain Kidd", Olaf Hytten in the 1952 film "Against All Flags", Alan Rowe in the 1969 BBC drama series "The First Churchills", Laurence Olivier in the 1986 NBC TV mini-series "Peter the Great", Thom Hoffman in the 1992 film "Orlando", based on the novel by Virginia Woolf, Corin Redgrave in the 1995 film "England, My England", the story of the composer Henry Purcell, Jochum ten Haaf in the 2003 BBC miniseries "", Bernard Hill in the 2005 film "The League of Gentlemen's Apocalypse", and Russell Pate in the 2008 BBC film "King Billy Above All". His role in Dutch politics and his alleged homosexual nature was shown in the 2015 film "Michiel de Ruyter". 
Ballads.
Seventeenth-century broadside ballads that describe the coregency between William and Mary, and the effects their rule had on English political and religious life, provide valuable insight into how the public viewed their new King. Copies of extant ballads, such as "England's Triumph", "England's Happiness in the Crowning of William and Mary", "A new loyal song, upon King William's Progress into Ireland" and "Royal Courage or King William's Happy Success in Ireland", are housed in Magdalene College's Pepys Library, the National Library of Scotland, and the British Library. Facsimiles of the ballads, as well as audio recordings, are available online.
References.
Bibliography.
</dl>

</doc>
<doc id="47388" url="http://en.wikipedia.org/wiki?curid=47388" title="Documentary Center">
Documentary Center

The Documentary Center is a part of The George Washington University in Washington, DC, USA, devoted to teaching documentary film production theory and techniques and to creating documentary films for national audiences. Inaugurated in 1991 as The Center for History in the Media, the Documentary Center is one of the few educational centers in the nation that focuses exclusively on the production of non-fiction filmmaking.

</doc>
<doc id="47390" url="http://en.wikipedia.org/wiki?curid=47390" title="309">
309

Year 309 (CCCIX) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Licinianus and Constantius (or, less frequently, year 1062 "Ab urbe condita"). The denomination 309 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="47391" url="http://en.wikipedia.org/wiki?curid=47391" title="307">
307

Year 307 (CCCVII) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Valerius and Constantius (or, less frequently, year 1060 "Ab urbe condita"). The denomination 307 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
China.
</onlyinclude>

</doc>
<doc id="47392" url="http://en.wikipedia.org/wiki?curid=47392" title="310">
310

Year 310 (CCCX) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Andronicus and Probus (or, less frequently, year 1063 "Ab urbe condita"). The denomination 310 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="47393" url="http://en.wikipedia.org/wiki?curid=47393" title="311">
311

Year 311 (CCCXI) was a common year starting on Monday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Valerius and Maximinus (or, less frequently, year 1064 "Ab urbe condita"). The denomination 311 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="47395" url="http://en.wikipedia.org/wiki?curid=47395" title="327">
327

Year 327 (CCCXXVII) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Constantius and Maximus (or, less frequently, year 1080 "Ab urbe condita"). The denomination 327 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="47396" url="http://en.wikipedia.org/wiki?curid=47396" title="328">
328

Year 328 (CCCXXVIII) was a leap year starting on Monday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Ianuarinus and Iustus (or, less frequently, year 1081 "Ab urbe condita"). The denomination 328 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="47397" url="http://en.wikipedia.org/wiki?curid=47397" title="Æthelflæd">
Æthelflæd

Æthelflæd, Lady of the Mercians, (d. 12 June 918) ruled Mercia from 911 to her death in 918. She was the eldest daughter of Alfred the Great, king of the Anglo-Saxon kingdom of Wessex, and his queen, Ealhswith. Æthelflæd was born at the height of the Viking invasions of England. Her father married her to Æthelred, Lord of the Mercians. After his death in 911, she ruled; the "Anglo-Saxon Chronicle" referred to her as the "Myrcna hlæfdige", "Lady of the Mercians".
Early life.
Ethelflaed was according to Asser the first born child of Alfred the Great and his Mercian queen Ealhswith who was a descendant of king Coenwulf of Mercia through her own mother Eadburh. Ethelflaed's date of birth is not known but it was probably sometime between the marriage of her parents in 868 and birth of her brother Edward the Elder, whose own date of birth is not known but presumably took place before 878.
Alfred was the fifth son of king Ethelwulf and was never expected to become king. However by 865 three of his brothers had died and Ethelred, the only other one still alive, was King of Wessex. Soon afterwards in the same year a great Viking army arrived and wreaked havoc in much of England. It would be the start of a long period of war between the Vikings and the Anglo-Saxons which lasted many battles until after the Battle of Edington in 878 in which Alfred was king and commander.
Lady of the Mercians.
Æthelflaed was already married to Æthelred, then ealdorman of Mercia. Æthelred and Æthelflæd are recorded as having had one daughter, Ælfwynn. Æthelstan, the son of Edward the Elder and the grandson of Alfred, was brought up in their court.
Near the end of the reign of Alfred the Great, Æthelred and Æthelflæd were requested by Werferth, the Bishop of Worcester, to fortify the town, in return for which they shared the rents and other profits which had belonged to the bishop.
Æthelflæd established garrisons in Hereford and Gloucester before 914 and repaired the old walls of Chester in 907. In 910 she built her first fortress; since her husband took no part in the campaign against the Danes, some scholars suggest that she was the real leader of the Mercian people.
On her husband's death in 911 after the Battle of Tettenhall, she was recognised as the "Lady of the Mercians". This was not a purely honorific title; Æthelflæd was a formidable military leader and tactician and ruled for eight years. Upon succeeding her husband, she began to plan and build a series of fortresses in English Mercia, ten of which can be identified: Bridgnorth (912); Tamworth (913); Stafford (913); Eddisbury (914); Warwick (914); Chirbury (915); Runcorn (915). Three other fortresses, at Bremesburh, Scergeat and Weardbyrig, have yet to be located.
Æthelflæd allied herself to her brother Edward the Elder, king over much of England. Historian Sir Frank Stenton said that Edward was able to achieve "the outstanding feature of his reign", the move against the occupying Danes in the south of England, due to being able to rely upon Æthelflæd.
In 916 she led an expedition into Wales to avenge the murder of a Mercian abbot, and succeeded in capturing the wife of the king of Brycheiniog. Edward the Elder issued coinage with novel reverses of extraordinary designs, and it is speculated that this series of coinage was for circulation in the part of Mercia under the rule of Edward and his sister, with the design of the coinage perhaps showing the influence of Æthelflæd.
Death and legacy.
In 918, the people of the region around York promised to pledge their loyalty to Æthelflæd, probably in order to secure her support against Norse raiders from Ireland, but she died on 12 June 918, less than two weeks before the city was able to pay homage to her. She was succeeded as Lady of the Mercians by her daughter, Ælfwynn, but six months later Edward deposed her and took Mercia under his personal control.
According to the Parker Chronicle (Manuscript A of the "Anglo-Saxon Chronicle"), which was strongly sympathetic to Edward, "all the people in the land of the Mercians who had been subject to Æthelflæd turned to him; and the kings among the Welsh, Hywel and Clydog and Idwal, and all the Welsh people sought to have him as their lord". Hywel Dda was king of Dyfed in south-west Wales, Clydog ap Cadell probably king of Powys in the north-east, and Idwal ab Anarawd king of Gwynedd in the north-west. Gwent in south-east Wales was already under West Saxon lordship, but in the view of historian T. M. Charles-Edwards this passage shows that the other Welsh kingdoms were under Mercian lordship until Edward took direct power by deposing Ælfwynn.
Æthelflæd died at Tamworth, Staffordshire and was buried with her husband in St Oswald's Priory, Gloucester, which they had established. A statue of her was erected outside Tamworth Castle in 1913 to commemorate the millennium of her construction of the burh of Tamworth.
Ancestors of Æthelflæd

</doc>
<doc id="47398" url="http://en.wikipedia.org/wiki?curid=47398" title="Orchestration">
Orchestration

Orchestration is the study or practice of writing music for an orchestra (or, more loosely, for any musical ensemble) or of adapting for an orchestra music composed for another medium. Only gradually over the course of music history did orchestration come to be regarded as a compositional art in itself.
Orchestration as practice.
The term "orchestration" in its specific sense refers to the way instruments are used to portray any musical aspect such as melody or harmony.
For example, a C major chord is made up of the notes C, E, and G. If the notes are held out the entire duration of a measure, the composer or orchestrator will have to decide what instrument(s) play this chord and in what register. Some instruments, including woodwinds and brass are primarily monophonic and can only play one note of the chord at a time. However in a full orchestra there is generally more than one of these instruments, so the composer may choose to outline the chord in its basic form with clarinets or trumpets. Other instruments, including the strings, piano, harp, and pitched percussion are polyphonic and may play more than one note at a time.
Additionally in orchestration, notes may be placed into another register (such as transposed down two octaves for the basses), doubled (both in the same and different octaves), and altered with various levels of dynamics. The choice of instruments, registers, and dynamics affect the overall tone color. If the C major chord was orchestrated for the trumpets and trombones playing fortissimo in their upper registers, it would sound very bright; but if the same chord was orchestrated for the celli and string basses playing sul tasto, doubled by the bassoons and bass clarinet, it might sound heavy and dark.
Note that although the above example discussed orchestrating a chord, a melody or even a single note may be orchestrated in this fashion. Also note that in this specific sense of the word, "orchestration" is not necessarily limited to an orchestra, as a composer may "orchestrate" this same C major chord for, say, a woodwind quintet.
Orchestration as adaptation.
In a more general sense, "orchestration" also refers to the re-adaptation of existing music into another medium, particularly a full or reduced orchestra. There are two general kinds of adaptation: transcription, which closely follows the original piece, and arrangement, which tends to change significant aspects of the original piece.
In terms of adaptation, "orchestration" applies, strictly speaking, only to writing for orchestra, whereas the term "instrumentation" applies to instruments used in the texture of the piece. In the study of orchestration – in contradistinction to the practice – the term "instrumentation" may also refer to consideration of the defining characteristics of individual instruments rather than to the art of combining instruments.
In commercial music, especially musical theatre and film music, independent orchestrators are often used because it is difficult to meet tight deadlines when the same person is required both to compose and to orchestrate.
Frequently, when a stage musical is adapted to film, such as "Camelot" or "Fiddler on the Roof", the orchestrations for the film version are notably different from the stage ones. In other cases, such as "Evita", they are not, and are simply expanded versions from those used in the stage production.
Most orchestrators often work from a draft (sketch), or short score, that is, a score written on limited number of independent musical staves. Some orchestrators, particularly those writing for the opera or music theatres, prefer to work from a piano vocal score up, since it is required to start rehearsing a piece long before the whole is fully completed. That was, for instance, the method of composition of Jules Massenet. In other instances simple cooperation between various creators are utilized, as when Jonathan Tunick orchestrates Stephen Sondheim's songs, or when orchestration is done from a lead sheet. In the latter case, arranging as well as orchestration will be involved.
Film orchestration.
Due to the enormous time constraints of film scoring schedules, most film composers employ orchestrators rather than doing the work themselves, although these orchestrators work under the close supervision of the composer. Some film composers have made the time to orchestrate their own music, including Ennio Morricone, Rachel Portman, Howard Shore, Abel Korzeniowski, Georges Delerue, and Bernard Herrmann.
Although there have been hundreds of orchestrators in film over the years, the most prominent film orchestrators for the latter half of the 20th century were Jack Hayes, Herbert W. Spencer, Edward Powell (who worked almost exclusively with Alfred Newman), Arthur Morton, Greig McRitchie, and Alexander Courage. Some of the most in-demand orchestrators today (and of the past 30 years) include Jeff Atmajian, Pete Anthony, Brad Dechter (James Newton Howard, Christopher Young, Theodore Shapiro, Teddy Castellucci, Danny Elfman, John Powell, Marco Beltrami, John Debney, Marc Shaiman, Michael Giacchino), Conrad Pope (John Williams, Alexandre Desplat, Jerry Goldsmith, James Newton Howard, Alan Silvestri, James Horner, Mark Isham, John Powell, Michael Convertino, Danny Elfman, Howard Shore), Eddie Karam (John Williams, James Horner), Bruce Fowler (Hans Zimmer, Klaus Badelt, Harry Gregson-Williams, Steve Jablonsky, Mark Mothersbaugh, John Powell), John Ashton Thomas (John Powell, John Debney, Alan Silvestri, James Newton Howard, Henry Jackman, Lyle Workman, Theodore Shapiro, John Ottman), Robert Elhai (Michael Kamen, Ed Shearmur, Brian Tyler, Klaus Badelt, Elliot Goldenthal, Michael Giacchino) and J.A.C. Redford (James Horner, Thomas Newman).
Conrad Salinger was the most prominent orchestrator of MGM musicals from the 1940s to 1962, orchestrating such famous films as "Singin' in the Rain", "An American in Paris", and "Gigi". In the 1950s, film composer John Williams frequently spent time with Salinger informally learning the craft of orchestration. Robert Russell Bennett (George Gershwin, Rodgers and Hammerstein) was one of America's most prolific orchestrators (particularly of Broadway shows) of the 20th century, sometimes scoring over 80 pages a day.
Process.
Most films require 30 to 120 minutes of musical score. Each individual piece of music in a film is called a "cue". There are roughly 20-80 cues per film. A dramatic film may require slow and sparse music while an action film may require 80 cues of highly active music. Each cue can range in length from five seconds to more than ten minutes as needed per scene in the film. After the composer is finished composing the cue, this sketch score is delivered to the orchestrator either as hand written or computer generated. Most composers in Hollywood today compose their music using sequencing software (e.g. Digital Performer, Logic Pro, or Cubase). A sketch score can be generated through the use of a MIDI file which is then imported into a music notation program such as Finale or Sibelius. Thus begins the job of the orchestrator.
Every composer works differently and the orchestrator's job is to understand what is required from one composer to the next. If the music is created with sequencing software then the orchestrator is given a MIDI sketch score and a synthesized recording of the cue. The sketch score only contains the musical notes (e.g. eighth notes, quarter notes, etc.) with no phrasing, articulations, or dynamics. The orchestrator studies this synthesized "mockup" recording listening to dynamics and phrasing (just as the composer has played them in). He or she then accurately tries to represent these elements in the orchestra. However some voicings on a synthesizer (synthestration) will not work in the same way when orchestrated for the live orchestra. 
The sound samples are often doubled up very prominently and thickly with other sounds in order to get the music to "speak" louder. The orchestrator sometimes changes these synth voicings to traditional orchestral voicings in order to make the music flow better. He or she may move intervals up or down the octave (or omit them entirely), double certain passages with other instruments in the orchestra, add percussion instruments to provide colour, and add Italian performance marks (e.g. Allegro con brio, Adagio, ritardando, dolce, staccato, etc.). If a composer writes a large action cue, and no woodwinds are used, the orchestrator will often add woodwinds by doubling the brass music up an octave. 
The orchestra size is determined from the music budget of the film. The orchestrator is told in advance the number of instruments he or she has to work with and has to abide by what is available. Sometimes a composer will write a three-part chord for three flutes, although only two flutes have been hired. The orchestrator decides on where to put the third note. After the orchestrated cue is complete it is delivered to the copying house (generally by placing on a server) so that each instrument of the orchestra can be extracted, printed, and delivered to the scoring stage.
The major film composers in Hollywood each have a lead orchestrator. Generally the lead orchestrator attempts to orchestrate as much of the music as possible if time allows. If the schedule is too demanding, a team of orchestrators (ranging from two to eight) will work on a film. The lead orchestrator decides on the assignment of cues to other orchestrators on the team. Most films can be orchestrated in one to two weeks with a team of five orchestrators. New orchestrators trying to obtain work will often approach a film composer asking to be hired. They are generally referred to the lead orchestrator for consideration.
At the scoring stage the orchestrator will often assist the composer in the recording booth giving suggestions on how to improve the performance, the music, or the recording. If the composer is conducting, sometimes the orchestrator will remain in the recording booth to assist as a producer. Sometimes the roles are reversed with the orchestrator conducting and the composer producing from the booth.
Payment.
Orchestrators in Hollywood have always been paid "by the page" (meaning every four measures) with rates determined by the . Score paper, usually purchased from , , or King Brand, would already have four barlines drawn on each page. However, most orchestrators today use music notation software (e.g. or ) instead of writing the music out by hand. If the union rate is $60 per page and 20 measures were orchestrated then the orchestrator's bill would be $300 plus 10% for health and pension. Orchestrators also receive royalties from films recorded with AFM American orchestras. These royalties, also called secondary payments, are generated not from the film's theatrical release, but from secondary markets (e.g. cable television and DVD sales and rental).

</doc>
<doc id="47399" url="http://en.wikipedia.org/wiki?curid=47399" title="Edgar the Peaceful">
Edgar the Peaceful

Edgar I ( 943 – 8 July 975), known as Edgar the Peaceful or the Peaceable, was King of England from 959 to 975. He was the younger son of King Edmund I and his Queen, Ælfgifu of Shaftesbury.
Accession.
Edgar was the son of Edmund I and Ælfgifu of Shaftesbury. Upon the death of King Edmund in 946, Edgar's uncle, Eadred, ruled until 955. Eadred was succeeded by his nephew, Eadwig, the son of Edmund and Edgar's older brother.
Eadwig was not a popular king, and his reign was marked by conflict with nobles and the Church, primarily St Dunstan and Archbishop Oda. In 957, the thanes of Mercia and Northumbria changed their allegiance to Edgar. A conclave of nobles declared Edgar as king of the territory north of the Thames. Edgar became King of England upon Eadwig's death in October 959.
Government.
One of Edgar's first actions was to recall Dunstan from exile and have him made Bishop of Worcester (and subsequently Bishop of London and later, Archbishop of Canterbury). Dunstan remained Edgar's advisor throughout his reign. While Edgar may not have been a particularly peaceable man, his reign was peaceful. The Kingdom of England was well established, and Edgar consolidated the political unity achieved by his predecessors. By the end of his reign, England was sufficiently unified in that it was unlikely to regress back to a state of division among rival kingships, as it had to an extent under the reign of Eadred. Blackstone mentions that King Edgar standardised measure throughout the realm.
Benedictine reform.
The Monastic Reform Movement that introduced the Benedictine Rule to England's monastic communities peaked during the era of Dunstan, Æthelwold, and Oswald (historians continue to debate the extent and significance of this movement).
Dead Man's Plack.
In 963, Edgar allegedly killed Earl Æthelwald, his rival in love, near present-day Longparish, Hampshire. The event was commemorated by the Dead Man's Plack, erected in 1825. In 1875, Edward Augustus Freeman debunked the story as a "tissue of romance" in his book, "Historic Essays"; however, his arguments were rebutted by naturalist William Henry Hudson in his 1920 book "Dead Man's Plack and an Old Thorn".
Coronation at Bath.
Edgar was crowned at Bath and anointed with his wife Ælfthryth, setting a precedent for a coronation of a queen in England itself. Edgar's coronation did not happen until 973, in an imperial ceremony planned not as the initiation, but as the culmination of his reign (a move that must have taken a great deal of preliminary diplomacy). This service, devised by Dunstan himself and celebrated with a poem in the "Anglo-Saxon Chronicle", forms the basis of the present-day British coronation ceremony.
The symbolic coronation was an important step; other kings of Britain came and gave their allegiance to Edgar shortly afterwards at Chester. Six kings in Britain, including the King of Scots and the King of Strathclyde, pledged their faith that they would be the king's liege-men on sea and land. Later chroniclers made the kings into eight, all plying the oars of Edgar's state barge on the River Dee. Such embellishments may not be factual, and what actually happened is unclear.
Death.
Edgar died on 8 July 975 at Winchester, Hampshire. He left behind Edward, who was probably his illegitimate son by Æthelflæd (not to be confused with the Lady of the Mercians), and Æthelred, the younger, the child of his wife Ælfthryth. He was succeeded by Edward. Edgar also had a possibly illegitimate daughter by Wulfthryth, who later became abbess of Wilton. She was joined there by her daughter, Edith of Wilton, who lived there as a nun until her death. Both women were later regarded as saints.
Between Edgar's death and the Norman Conquest, there were no uncontested accessions to the throne. Some see Edgar's death as the beginning of the end of Anglo-Saxon England, followed as it was by three successful 11th century conquests — two Danish and one Norman.
Appearance.
"[H]e was extremely small both in stature and bulk..."
Genealogy.
For a more complete genealogy including ancestors and descendants, see House of Wessex family tree.

</doc>
<doc id="47400" url="http://en.wikipedia.org/wiki?curid=47400" title="880">
880

Year 880 (DCCCLXXX) was a leap year starting on Friday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="47401" url="http://en.wikipedia.org/wiki?curid=47401" title="Titan (mythology)">
Titan (mythology)

In Classical Greek mythology, the Titans (Greek: Τιτάν "Tītán"; plural: Τιτᾶνες "Tītânes") and Titanesses (Greek: Τιτάνης "Tītánis"; plural: Τιτᾶνίδες "Titânídes") were members of the second order of divine beings, descending from the primordial deities and preceding the Olympian deities. Based on Mount Othrys, the Titans most famously included the first twelve children of the primordial Gaia (Mother Earth) and Uranus (Father Heaven). They were giant deities of incredible strength, who ruled during the legendary Golden Age, and also composed the first pantheon of Greek deities.
Among the first generation of twelve Titans, the females were Mnemosyne, Tethys, Theia, Phoebe, Rhea, and Themis and the males were Oceanus, Hyperion, Coeus, Cronus, Crius, and Iapetus. 
The second generation of Titans consisted of Hyperion's children Helios, Selene, and Eos; Coeus' children Lelantos, Leto, and Asteria; Iapetus' sons Atlas, Prometheus, Epimetheus, and Menoetius; Oceanus' daughter Metis; and Crius' sons Astraeus, Pallas, and Perses.
As they had overthrown the primordial deities, the Titans were overthrown by younger gods, including many of their own children - the Olympians - in the Titanomachy (or "War of the Titans"). The Greeks may have borrowed this mytheme from the Ancient Near East.
Titanomachy.
Greeks of the classical age knew of several poems about the war between the Olympians and Titans. The dominant one, and the only one that has survived, was in the "Theogony" attributed to Hesiod. A lost epic, "Titanomachia" - attributed to the legendary blind Thracian bard Thamyris - was mentioned in passing in an essay "On Music" that was once attributed to Plutarch. The Titans also played a prominent role in the poems attributed to Orpheus. Although only scraps of the Orphic narratives survive, they show interesting differences with the Hesiodic tradition.
The classical Greek myths of the Titanomachy fall into a class of similar myths throughout Europe and the Near East concerning a war in heaven, where one generation or group of gods largely opposes the dominant one. Sometimes the elders are supplanted, and sometimes the rebels lose and are either cast out of power entirely or incorporated into the pantheon. Other examples might include the wars of the Æsir with the Vanir in Scandinavian mythology, the Babylonian epic Enuma Elish, the Hittite "Kingship in Heaven" narrative, the obscure generational conflict in Ugaritic fragments, Virabhadra's conquest of the early Vedic Gods, and the rebellion of Lucifer in Christianity. The Titanomachy lasted for ten years.
In Orphic sources.
Hesiod doesn't have the last word on the Titans. Surviving fragments of poetry ascribed to Orpheus preserve some variations on the myth. In such text, Zeus does not simply set upon his father violently. Instead, Rhea spreads out a banquet for Cronus so that he becomes drunk upon fermented honey. Rather than being consigned to Tartarus, Cronus is dragged—still drunk—to the cave of Nyx (Night), where he continues to dream throughout eternity.
Another myth concerning the Titans that is not in Hesiod revolves around Dionysus. At some point in his reign, Zeus decides to give up the throne in favor of the infant Dionysus, who like the infant Zeus, is guarded by the Kouretes. The Titans decide to slay the child and claim the throne for themselves; they paint their faces white with gypsum, distract Dionysus with toys, then dismember him and boil and roast his limbs. Zeus, enraged, slays the Titans with his thunderbolt; Athena preserves the heart in a gypsum doll, out of which a new Dionysus is made. This story is told by the poets Callimachus and Nonnus, who call this Dionysus "Zagreus", and in a number of Orphic texts, which do not.
One iteration of this story, of the Late Antique Neoplatonist philosopher Olympiodorus, recounted in his commentary of Plato's "Phaedrus", affirms that humanity sprang up out of the fatty smoke of the burning Titan corpses. Pindar, Plato, and Oppian refer offhandedly to the "Titanic nature" of humans. According to them, the body is the titanic part, while soul is the divine part of humans. Other early writers imply that humanity was born out of the malevolent blood shed by the Titans in their war against Zeus. Some scholars consider that Olympiodorus' report, the only surviving explicit expression of this mythic connection, embodied a tradition that dated to the Bronze Age, while Radcliffe Edmonds has suggested an element of innovative allegorized improvisation to suit Olympiodorus' purpose.
Modern interpretations.
Some 19th- and 20th-century scholars, including Jane Ellen Harrison, have argued that an initiatory or shamanic ritual underlies the myth of the dismemberment and cannibalism of Dionysus by the Titans. She also asserts that the word "Titan" comes from the Greek τίτανος, signifying white "earth, clay or gypsum," and that the Titans were "white clay men", or men covered by white clay or gypsum dust in their rituals. Martin Litchfield West also asserts this in relation to shamanistic initiatory rites of early Greek religious practices.
According to Paul Faure, the name "Titan" can be found on Linear A written as "Tan" or "Ttan", which represents a single deity rather than a group. Other scholars believe the word is related to the Greek verb τείνω (to stretch), through an epic variation τιταίνω, and τίσις (retribution, vengeance), a view Hesiod appears to share when he narrates: "But their father, great Ouranos, called them Titans by surname, rebuking his sons, whom he had begotten himself; for he said they had strained (τιταίνοντας) in their wickedness to perform a mighty deed, and at some later time there would be vengeance (τίσιν) for this."
In popular culture.
Out of conflation with the Gigantes, various large things have been named after the Titans for their "titanic" size, for example the RMS "Titanic" or the giant predatory bird "Titanis walleri". The familiar name and large size of the Titans have made them dramatic figures suited to market-oriented popular culture. Something titanic is usually considered bigger than something gigantic.
The element titanium is named after the Titans, and many of Saturn's moons are named after various Titans.
Many professional and amateur sports teams use a titan as their mascot. One of the National Football League's teams is the Tennessee Titans, the New York Jets were originally known as the New York Titans, California State University, Fullerton and Ohio State University, Newark Campus's athletic teams are known as the Titans, and the Australian professional rugby league team Gold Coast is also known as the Titans.
The Titans have appeared as antagonists in Disney's "Hercules" and "Percy Jackson & the Olympians" series.
They have also appeared as both protagonists and antagonists in the "God of War" video game series and "Clash of the Titans" movies. While the 1981 original and 2010 remake of the latter do not actually feature any Titans, Cronus was the main antagonist of the 2012 sequel.

</doc>
<doc id="47402" url="http://en.wikipedia.org/wiki?curid=47402" title="Titan (moon)">
Titan (moon)

Titan (or Saturn VI) is the largest moon of Saturn. It is the only natural satellite known to have a dense atmosphere, and the only object other than Earth where clear evidence of stable bodies of surface liquid has been found.
Titan is the sixth ellipsoidal moon from Saturn. Frequently described as a planet-like moon, Titan's diameter is 50% larger than Earth's natural satellite, the Moon, and it is 80% more massive. It is the second-largest moon in the Solar System, after Jupiter's moon Ganymede, and is larger by volume than the smallest planet, Mercury, although only 40% as massive. Discovered in 1655 by the Dutch astronomer Christiaan Huygens, Titan was the first known moon of Saturn, and the fifth known satellite of another planet.
Titan is primarily composed of water ice and rocky material. Much as with Venus before the Space Age, the dense opaque atmosphere prevented understanding of Titan's surface until new information accumulated when the "Cassini–Huygens" mission arrived in 2004, including the discovery of liquid hydrocarbon lakes in Titan's polar regions. The geologically young surface is generally smooth, with few impact craters, although mountains and several possible cryovolcanoes have been found.
The atmosphere of Titan is largely nitrogen; minor components lead to the formation of methane and ethane clouds and nitrogen-rich organic smog. The climate—including wind and rain—creates surface features similar to those of Earth, such as dunes, rivers, lakes, seas (probably of liquid methane and ethane), and deltas, and is dominated by seasonal weather patterns as on Earth. With its liquids (both surface and subsurface) and robust nitrogen atmosphere, Titan's methane cycle is viewed as an analogy to Earth's water cycle, although at a much lower temperature.
Discovery and naming.
Titan was discovered on March 25, 1655 by the Dutch astronomer Christiaan Huygens. Huygens was inspired by Galileo's discovery of Jupiter's four largest moons in 1610 and his improvements in telescope technology. Christiaan, with the help of his brother Constantijn Huygens, Jr., began building telescopes around 1650 and discovered the first observed moon orbiting Saturn with one of the telescopes they built.
He named it simply "Saturni Luna" (or "Luna Saturni", Latin for "Saturn's moon"), publishing in the 1655 tract "De Saturni Luna Observatio Nova" ("A New Observation of Saturn's Moon"). After Giovanni Domenico Cassini published his discoveries of four more moons of Saturn between 1673 and 1686, astronomers fell into the habit of referring to these and Titan as Saturn I through V (with Titan then in fourth position). Other early epithets for Titan include "Saturn's ordinary satellite". Titan is officially numbered Saturn VI because after the 1789 discoveries the numbering scheme was frozen to avoid causing any more confusion (Titan having borne the numbers II and IV as well as VI). Numerous small moons have been discovered closer to Saturn since then.
The name "Titan", and the names of all seven satellites of Saturn then known, came from John Herschel (son of William Herschel, discoverer of Mimas and Enceladus) in his 1847 publication "Results of Astronomical Observations Made at the Cape of Good Hope". He suggested the names of the mythological Titans (Ancient Greek: Τῑτάν), brothers and sisters of Cronus, the Greek Saturn. In Greek mythology, the Titans were a race of powerful deities, descendants of Gaia and Uranus, that ruled during the legendary Golden Age.
Orbit and rotation.
Titan orbits Saturn once every 15 days and 22 hours. Like the Moon and many of the satellites of the gas giants, its rotational period is identical to its orbital period; Titan is thus tidally locked in synchronous rotation with Saturn, and permanently shows one face to the planet. Because of this, there is a sub-Saturnian point on its surface, from which the planet would always appear to hang directly overhead. Longitudes on Titan are measured westward, starting from the meridian passing through this point. Its orbital eccentricity is 0.0288, and the orbital plane is inclined 0.348 degrees relative to the Saturnian equator. Viewed from Earth, Titan reaches an angular distance of about 20 Saturn radii (just over 1200000 km) from Saturn and subtends a disk 0.8 arcseconds in diameter.
The small, irregularly shaped satellite Hyperion is locked in a 3:4 orbital resonance with Titan. A "slow and smooth" evolution of the resonance—in which Hyperion would have migrated from a chaotic orbit—is considered unlikely, based on models. Hyperion probably formed in a stable orbital island, whereas the massive Titan absorbed or ejected bodies that made close approaches.
Bulk characteristics.
Titan is 5150 km in diameter, compared to 4879 km for the planet Mercury, 3474 km for the Moon, and 12742 km for Earth. Before the arrival of "Voyager 1" in 1980, Titan was thought to be slightly larger than Ganymede (diameter 5262 km) and thus the largest moon in the Solar System; this was an overestimation caused by Titan's dense, opaque atmosphere, which extends many kilometres above its surface and increases its apparent diameter. Titan's diameter and mass (and thus its density) are similar to those of the Jovian moons Ganymede and Callisto. Based on its bulk density of 1.88 g/cm3, Titan's bulk composition is half water ice and half rocky material. Though similar in composition to Dione and Enceladus, it is denser due to gravitational compression.
Titan is likely differentiated into several layers with a 3400 km rocky center surrounded by several layers composed of different crystal forms of ice. Its interior may still be hot and there may be a liquid layer consisting of a "magma" composed of water and ammonia between the ice Ih crust and deeper ice layers made of high-pressure forms of ice. The presence of ammonia allows water to remain liquid even at a temperature as low as 176 K (for eutectic mixture with water). Evidence for such an ocean was uncovered by the "Cassini" probe in the form of natural extremely-low-frequency radio waves in Titan's atmosphere. Titan's surface is thought to be a poor reflector of extremely-low-frequency radio waves, so they may instead be reflecting off the liquid–ice boundary of a subsurface ocean. Surface features were observed by the "Cassini" spacecraft to systematically shift by up to 30 km between October 2005 and May 2007, which suggests that the crust is decoupled from the interior, and provides additional evidence for an interior liquid layer. Further supporting evidence for a liquid layer and decoupled ice shell comes from the way the gravity field varies as Titan orbits Saturn. Comparison of the gravity field with the RADAR-based topography observations also suggests that the ice shell may be substantially rigid.
Formation.
The moons of Jupiter and Saturn are thought to have formed through co-accretion, a similar process to that believed to have formed the planets in the Solar System. As the young gas giants formed, they were surrounded by discs of material that gradually coalesced into moons. However, whereas Jupiter possesses four large satellites in highly regular, planet-like orbits, Titan overwhelmingly dominates Saturn's system and possesses a high orbital eccentricity not immediately explained by co-accretion alone. A proposed model for the formation of Titan is that Saturn's system began with a group of moons similar to Jupiter's Galilean satellites, but that they were disrupted by a series of giant impacts, which would go on to form Titan. Saturn's mid-sized moons, such as Iapetus and Rhea, were formed from the debris of these collisions. Such a violent beginning would also explain Titan's orbital eccentricity.
In 2014, analysis of Titan's atmospheric nitrogen suggested that it has possibly been sourced from material similar to that found in the Oort cloud and not from sources present during co-accretion of materials around Saturn.
Atmosphere.
Titan is the only known moon with a significant atmosphere, the only nitrogen-rich dense atmosphere in the Solar System aside from Earth's. Observations of it made in 2004 by "Cassini" suggest that Titan is a "super rotator", like Venus, with an atmosphere that rotates much faster than its surface. Observations from the "Voyager" space probes have shown that Titan's atmosphere is denser than Earth's, with a surface pressure about 1.45 atm. It is also about 1.19 times as massive as Earth's overall, or about 7.3 times more massive on a per surface area basis. It supports opaque haze layers that block most visible light from the Sun and other sources and renders Titan's surface features obscure. Titan's lower gravity means that its atmosphere is far more extended than Earth's. The atmosphere of Titan is opaque at many wavelengths and a complete reflectance spectrum of the surface is impossible to acquire from orbit. It was not until the arrival of the "Cassini–Huygens" spacecraft in 2004 that the first direct images of Titan's surface were obtained.
Titan's atmospheric composition in the stratosphere is 98.4% nitrogen with the remaining 1.6% composed mostly of methane (1.4%) and hydrogen (0.1–0.2%). There are trace amounts of other hydrocarbons, such as ethane, diacetylene, methylacetylene, acetylene and propane, and of other gases, such as cyanoacetylene, hydrogen cyanide, carbon dioxide, carbon monoxide, cyanogen, argon and helium. The hydrocarbons are thought to form in Titan's upper atmosphere in reactions resulting from the breakup of methane by the Sun's ultraviolet light, producing a thick orange smog. Titan spends 95% of its time within Saturn's magnetosphere, which may help shield it from the solar wind.
Energy from the Sun should have converted all traces of methane in Titan's atmosphere into more complex hydrocarbons within 50 million years—a short time compared to the age of the Solar System. This suggests that methane must be replenished by a reservoir on or within Titan itself. The ultimate origin of the methane in its atmosphere may be its interior, released via eruptions from cryovolcanoes.
On April 3, 2013, NASA reported that complex organic chemicals could arise on Titan, based on studies simulating the atmosphere of Titan.
On June 6, 2013, scientists at the IAA-CSIC reported the detection of polycyclic aromatic hydrocarbons in the upper atmosphere of Titan.
On September 30, 2013, propene was detected in the atmosphere of Titan by NASA's "Cassini" spacecraft, using its composite infrared spectrometer (CIRS). This is the first time propene has been found on any moon or planet other than Earth and is the first chemical found by the CIRS. The detection of propene fills a mysterious gap in observations that date back to NASA's Voyager 1 spacecraft's first close flyby of Titan in 1980, during which it was discovered that many of the gases that make up Titan's hazy brown colored haze were hydrocarbons, theoretically formed via the recombination of radicals created by the ultraviolet photolysis of methane.
On October 24, 2014, methane was found in polar clouds on Titan.
Polar clouds, made of methane, on Titan (left) compared with polar clouds on Earth (right), which are made of water or water ice.
Climate.
Titan's surface temperature is about 94 K. At this temperature, water ice has an extremely low vapor pressure, so the little water vapor present appears limited to the stratosphere. Titan receives about 1% as much sunlight as Earth.
Atmospheric methane creates a greenhouse effect on Titan's surface, without which Titan would be far colder. Conversely, haze in Titan's atmosphere contributes to an anti-greenhouse effect by reflecting sunlight back into space, cancelling a portion of the greenhouse effect and making its surface significantly colder than its upper atmosphere.
Titan's clouds, probably composed of methane, ethane or other simple organics, are scattered and variable, punctuating the overall haze. The findings of the "Huygens" probe indicate that Titan's atmosphere periodically rains liquid methane and other organic compounds onto its surface.
Clouds typically cover 1% of Titan's disk, though outburst events have been observed in which the cloud cover rapidly expands to as much as 8%. One hypothesis asserts that the southern clouds are formed when heightened levels of sunlight during the southern summer generate uplift in the atmosphere, resulting in convection. This explanation is complicated by the fact that cloud formation has been observed not only after the southern summer solstice but also during mid-spring. Increased methane humidity at the south pole possibly contributes to the rapid increases in cloud size. It was summer in Titan's southern hemisphere until 2010, when Saturn's orbit, which governs Titan's motion, moved Titan's northern hemisphere into the sunlight. When the seasons switch, it is expected that ethane will begin to condense over the south pole.
Surface features.
The surface of Titan has been described as "complex, fluid-processed, [and] geologically young". Titan has been around since the Solar System's formation, but its surface is much younger, between 100 million and 1 billion years old. Geological processes may have reshaped Titan's surface. Titan's atmosphere is twice as thick as Earth's, making it difficult for astronomical instruments to image its surface in the visible light spectrum. The "Cassini" spacecraft is using infrared instruments, radar altimetry and synthetic aperture radar (SAR) imaging to map portions of Titan during its close fly-bys. The first images revealed a diverse geology, with both rough and smooth areas. There are features that may be volcanic in origin, disgorging water mixed with ammonia onto the surface. However, there is also evidence that Titan's ice shell may be substantially rigid, which would suggest little geologic activity.
There are also streaky features, some of them hundreds of kilometers in length, that appear to be caused by windblown particles. Examination has also shown the surface to be relatively smooth; the few objects that seem to be impact craters appeared to have been filled in, perhaps by raining hydrocarbons or volcanoes. Radar altimetry suggests height variation is low, typically no more than 150 meters. Occasional elevation changes of 500 meters have been discovered and Titan has mountains that sometimes reach several hundred meters to more than 1 kilometer in height.
Titan's surface is marked by broad regions of bright and dark terrain. These include Xanadu, a large, reflective equatorial area about the size of Australia. It was first identified in infrared images from the Hubble Space Telescope in 1994, and later viewed by the "Cassini" spacecraft. The convoluted region is filled with hills and cut by valleys and chasms. It is criss-crossed in places by dark lineaments—sinuous topographical features resembling ridges or crevices. These may represent tectonic activity, which would indicate that Xanadu is geologically young. Alternatively, the lineaments may be liquid-formed channels, suggesting old terrain that has been cut through by stream systems. There are dark areas of similar size elsewhere on Titan, observed from the ground and by "Cassini"; it had been speculated that these are methane or ethane seas, but "Cassini" observations seem to indicate otherwise (see below).
Liquids.
The possibility of hydrocarbon seas on Titan was first suggested based on "Voyager 1" and "2" data that showed Titan to have a thick atmosphere of approximately the correct temperature and composition to support them, but direct evidence was not obtained until 1995 when data from Hubble and other observations suggested the existence of liquid methane on Titan, either in disconnected pockets or on the scale of satellite-wide oceans, similar to water on Earth.
The "Cassini" mission confirmed the former hypothesis, although not immediately. When the probe arrived in the Saturnian system in 2004, it was hoped that hydrocarbon lakes or oceans would be detected from the sunlight reflected off their surface, but no specular reflections were initially observed. Near Titan's south pole, an enigmatic dark feature named Ontario Lacus was identified (and later confirmed to be a lake). A possible shoreline was also identified near the pole via radar imagery. Following a flyby on July 22, 2006, in which the "Cassini" spacecraft's radar imaged the northern latitudes (that were then in winter), a number of large, smooth (and thus dark to radar) patches were seen dotting the surface near the pole. Based on the observations, scientists announced "definitive evidence of lakes filled with methane on Saturn's moon Titan" in January 2007. The "Cassini–Huygens" team concluded that the imaged features are almost certainly the long-sought hydrocarbon lakes, the first stable bodies of surface liquid found outside of Earth. Some appear to have channels associated with liquid and lie in topographical depressions. The liquid erosion features appear to be a very recent occurrence: channels in some regions have created surprisingly little erosion, suggesting erosion on Titan is extremely slow, or some other recent phenomena may have wiped out older riverbeds and landforms. Overall, the Cassini radar observations have shown that lakes cover only a few percent of the surface, making Titan much drier than Earth. Although most of the lakes are concentrated near the poles (where the relative lack of sunlight prevents evaporation), a number of long-standing hydrocarbon lakes in the equatorial desert regions have also been discovered, including one near the Huygens landing site in the Shangri-La region, which is about half the size of Utah's Great Salt Lake. The equatorial lakes are probably "oases", i.e. the likely supplier is underground aquifers.
In June 2008, the Visual and Infrared Mapping Spectrometer on "Cassini" confirmed the presence of liquid ethane beyond doubt in Ontario Lacus. On December 21, 2008, "Cassini" passed directly over Ontario Lacus and observed specular reflection in radar. The strength of the reflection saturated the probe's receiver, indicating that the lake level did not vary by more than 3 mm (implying either that surface winds were minimal, or the lake's hydrocarbon fluid is viscous).
Specular reflections are indicative of a smooth, mirror-like surface, so the observation corroborated the inference of the presence of a large liquid body drawn from radar imaging. The observation was made soon after the north polar region emerged from 15 years of winter darkness.
On July 8, 2009, "Cassini's" VIMS observed a specular reflection indicative of a smooth, mirror-like surface, off what today is called Jingpo Lacus, a lake in the north polar region shortly after the area emerged from 15 years of winter darkness.
Early radar measurements made in July 2009 and January 2010 indicated that Ontario Lacus was extremely shallow, with an average depth of 0.4–3 m, and a maximum depth of 3 to. In contrast, the northern hemisphere's Ligeia Mare was initially mapped to depths exceeding 8 m, the maximum discernable by the radar instrument and the analysis techniques of the time.
Later science analysis, released in 2014, more fully mapped the depths of Titan's three methane seas and showed depths of more than 200 m. Ligeia Mare averages from 20 to in depth, while other parts of "Ligeia" did not register any radar reflection at all, indicating a depth of more than 200 m. While only the second largest of Titan's methane seas, "Ligeia" "contains enough liquid methane to fill three Lake Michigans."
During a flyby on 26 September 2012, "Cassini"'s radar detected in Titan's northern polar region what is likely a river with a length of more than 400 kilometers. It has been compared with the much larger Nile river on Earth. This feature ends in Ligeia Mare.
During six flybys of Titan from 2006 to 2011, Cassini gathered radiometric tracking and optical navigation data from which investigators could roughly infer Titan's changing shape. The density of Titan is consistent with a body that is about 60% rock and 40% water. The team's analyses suggest that Titan's surface can rise and fall by up to 10 metres during each orbit. That degree of warping suggests that Titan's interior is relatively deformable, and that the most likely model of Titan is one in which an icy shell dozens of kilometres thick floats atop a global ocean. The team's findings, together with the results of previous studies, hint that Titan's ocean may lie no more than 100 km below its surface. On July 2, 2014, NASA reported the ocean inside Titan may be as salty as the Dead Sea. On September 3, 2014, NASA reported studies suggesting methane rainfall on Titan may interact with a layer of icy materials underground, called an "alkanofer," to produce ethane and propane that may eventually feed into rivers and lakes.
Impact craters.
Radar, SAR and imaging data from "Cassini" have revealed few impact craters on Titan's surface. These impacts appear to be relatively young, compared to Titan's age. The few impact craters discovered include a 440 km wide two-ring impact basin named Menrva seen by "Cassini's" ISS as a bright-dark concentric pattern. A smaller, 60 km wide, flat-floored crater named Sinlap and a 30 km crater with a central peak and dark floor named Ksa have also been observed. Radar and "Cassini" imaging have also revealed a number of "crateriforms", circular features on the surface of Titan that may be impact related, but lack certain features that would make identification certain. For example, a 90 km wide ring of bright, rough material known as Guabonito has been observed by "Cassini". This feature is thought to be an impact crater filled in by dark, windblown sediment. Several other similar features have been observed in the dark Shangri-la and Aaru regions. Radar observed several circular features that may be craters in the bright region Xanadu during "Cassini's" April 30, 2006 flyby of Titan.
Many of Titan's craters or probable craters display evidence of extensive erosion, and all show some indication of modification. Most large craters have breached or incomplete rims, despite the fact that some craters on Titan have relatively more massive rims than those anywhere else in the Solar System. However, there is little evidence of formation of palimpsests through viscoelastic crustal relaxation, unlike on other large icy moons. Most craters lack central peaks and have smooth floors, possibly due to impact-generation or later eruption of cryovolcanic lava. Although infill from various geological processes is one reason for Titan's relative deficiency of craters, atmospheric shielding also plays a role; it is estimated that Titan's atmosphere reduces the number of craters on its surface by a factor of two.
The limited high-resolution radar coverage of Titan obtained through 2007 (22%) suggested the existence of a number of nonuniformities in its crater distribution. Xanadu has 2–9 times more craters than elsewhere. The leading hemisphere has a 30% higher density than the trailing hemisphere. There are lower crater densities in areas of equatorial dunes and in the north polar region (where hydrocarbon lakes and seas are most common).
Pre-"Cassini" models of impact trajectories and angles suggest that where the impactor strikes the water ice crust, a small amount of ejecta remains as liquid water within the crater. It may persist as liquid for centuries or longer, sufficient for "the synthesis of simple precursor molecules to the origin of life".
Cryovolcanism and mountains.
Scientists have long speculated that conditions on Titan resemble those of early Earth, though at a much lower temperature. The detection of argon-40 in the atmosphere in 2004 indicated that volcanoes had spawned plumes of "lava" composed of water and ammonia. Global maps of the lake distribution on Titan's surface revealed that there is not enough surface methane to account for its continued presence in its atmosphere, and thus that a significant portion must be added through volcanic processes.
Still, there is a paucity of surface features that can be unambiguously interpreted as cryovolcanoes. One of the first of such features revealed by "Cassini" radar observations in 2004, called Ganesa Macula, resembles the geographic features called "pancake domes" found on Venus, and was thus initially thought to be cryovolcanic in origin, although the American Geophysical Union refuted this hypothesis in December 2008. The feature was found to be not a dome at all, but appeared to result from accidental combination of light and dark patches. In 2004 "Cassini" also detected an unusually bright feature (called Tortola Facula), which was interpreted as a cryovolcanic dome. No similar features have been identified as of 2010. In December 2008, astronomers announced the discovery of two transient but unusually long-lived "bright spots" in Titan's atmosphere, which appear too persistent to be explained by mere weather patterns, suggesting they were the result of extended cryovolcanic episodes.
In March 2009, structures resembling lava flows were announced in a region of Titan called Hotei Arcus, which appears to fluctuate in brightness over several months. Though many phenomena were suggested to explain this fluctuation, the lava flows were found to rise 200 m above Titan's surface, consistent with it having been erupted from beneath the surface.
A mountain range measuring 150 km long, 30 km wide and 1.5 km high was also discovered by "Cassini" in 2006. This range lies in the southern hemisphere and is thought to be composed of icy material and covered in methane snow. The movement of tectonic plates, perhaps influenced by a nearby impact basin, could have opened a gap through which the mountain's material upwelled. Prior to Cassini, scientists assumed that most of the topography on Titan would be impact structures, yet these findings reveal that similar to Earth, the mountains were formed through geological processes. In December 2010, the "Cassini" mission team announced the most compelling possible cryovolcano yet found. Named Sotra Patera, it is one in a chain of at least three mountains, each between 1000 and 1500 m in height, several of which are topped by large craters. The ground around their bases appears to be overlaid by frozen lava flows.
If volcanism on Titan really exists, the hypothesis is that it is driven by energy released from the decay of radioactive elements within the mantle, as it is on Earth. Magma on Earth is made of liquid rock, which is less dense than the solid rocky crust through which it erupts. Because ice is less dense than water, Titan's watery magma would be denser than its solid icy crust. This means that cryovolcanism on Titan would require a large amount of additional energy to operate, possibly via tidal flexing from nearby Saturn. Alternatively, the pressure necessary to drive the cryovolcanoes may be caused by ice Ih "underplating" Titan's outer shell. The low-pressure ice, overlaying a liquid layer of ammonium sulfate, ascends buoyantly, and the unstable system can produce dramatic plume events. Titan is resurfaced through the process by grain-sized ice and ammonium sulfate ash, which helps produce a wind-shaped landscape and sand dune features.
In 2008 Jeffrey Moore (planetary geologist of Ames Research Center) proposed an alternate view of Titan's geology. Noting that no volcanic features had been unambiguously identified on Titan so far, he asserted that Titan is a geologically dead world, whose surface is shaped only by impact cratering, fluvial and eolian erosion, mass wasting and other exogenic processes. According to this hypothesis, methane is not emitted by volcanoes but slowly diffuses out of Titan's cold and stiff interior. Ganesa Macula may be an eroded impact crater with a dark dune in the center. The mountainous ridges observed in some regions can be explained as heavily degraded scarps of large multi-ring impact structures or as a result of the global contraction due to the slow cooling of the interior. Even in this case, Titan may still have an internal ocean made of the eutectic water–ammonia mixture with a temperature of 176 K, which is low enough to be explained by the decay of radioactive elements in the core. The bright Xanadu terrain may be a degraded heavily cratered terrain similar to that observed on the surface of Callisto. Indeed, were it not for its lack of an atmosphere, Callisto could serve as a model for Titan's geology in this scenario. Jeffrey Moore even called Titan "Callisto with weather".
Dark terrain.
In the first images of Titan's surface taken by Earth-based telescopes in the early 2000s, large regions of dark terrain were revealed straddling Titan's equator. Prior to the arrival of "Cassini", these regions were thought to be seas of liquid hydrocarbons. Radar images captured by the "Cassini" spacecraft have instead revealed some of these regions to be extensive plains covered in longitudinal dunes, up to 330 ft high about a kilometer wide, and tens to hundreds of kilometers long. Dunes of this type are always aligned with average wind direction. In the case of Titan, steady zonal (eastward) winds combine with variable tidal winds (approximately 0.5 meters per second). The tidal winds are the result of tidal forces from Saturn on Titan's atmosphere, which are 400 times stronger than the tidal forces of the Moon on Earth and tend to drive wind toward the equator. This wind pattern, it was theorized, causes granular material on the surface to gradually build up in long parallel dunes aligned west-to-east. The dunes break up around mountains, where the wind direction shifts.
The longitudinal (or linear) dunes were initially presumed to be formed by moderately variable winds that either follow one mean direction or alternate between two different directions. However, subsequent observations indicate that the dunes point to the east although climate simulations indicate Titan's surface winds blow toward the west. At less than 1 meter per second, they are not powerful enough to lift and transport surface material. Recent computer simulations indicate that the dunes may instead be the result of rare storm winds that happen only every fifteen years when Titan is in equinox. These storms produce strong downdrafts, flowing eastward at up to 10 meters per second when they reach the surface.
The "sand" on Titan is likely not made up of small grains of silicates like the sand on Earth, but rather might have formed when liquid methane rained and eroded the water-ice bedrock, possibly in the form of flash floods. Alternatively, the sand could also have come from organic solids produced by photochemical reactions in Titan's atmosphere. Studies of dunes' composition in May 2008 revealed that they possessed less water than the rest of Titan, and are thus most likely derived from organic soot like hydrocarbon polymers clumping together after raining onto the surface. Calculations indicate the sand on Titan has a density of one-third that of terrestrial sand.
Observation and exploration.
Titan is never visible to the naked eye, but can be observed through small telescopes or strong binoculars. Amateur observation is difficult because of the proximity of Titan to Saturn's brilliant globe and ring system; an occulting bar, covering part of the eyepiece and used to block the bright planet, greatly improves viewing. Titan has a maximum apparent magnitude of +8.2, and mean opposition magnitude 8.4. This compares to +4.6 for the similarly sized Ganymede, in the Jovian system.
Observations of Titan prior to the space age were limited. In 1907 Spanish astronomer Josep Comas i Solà observed limb darkening of Titan, the first evidence that the body has an atmosphere. In 1944 Gerard P. Kuiper used a spectroscopic technique to detect an atmosphere of methane.
The first probe to visit the Saturnian system was "Pioneer 11" in 1979, which revealed that Titan was probably too cold to support life. It took images of Titan, including Titan and Saturn together in mid to late 1979. The quality was soon surpassed by the two Voyagers, but Pioneer 11 provided data for everyone to prepare with.
Titan was examined by both "Voyager 1" and "2" in 1980 and 1981, respectively. "Voyager 1"'s course was diverted specifically to make a closer pass of Titan. Unfortunately, the craft did not possess any instruments that could penetrate Titan's haze, an unforeseen factor. Many years later, intensive digital processing of images taken through "Voyager 1"'s orange filter did reveal hints of the light and dark features now known as Xanadu and Shangri-la, but by then they had already been observed in the infrared by the Hubble Space Telescope. "Voyager 2" took only a cursory look at Titan. The "Voyager 2" team had the option of steering the spacecraft to take a detailed look at Titan or to use another trajectory that would allow it to visit Uranus and Neptune. Given the lack of surface features seen by "Voyager 1", the latter plan was implemented.
"Cassini–Huygens".
Even with the data provided by the "Voyagers", Titan remained a body of mystery—a planet-like satellite shrouded in an atmosphere making detailed observation difficult. The mystery that had surrounded Titan since the 17th-century observations of Christiaan Huygens and Giovanni Cassini was revealed by a spacecraft named in their honor.
The "Cassini–Huygens" spacecraft reached Saturn on July 1, 2004, and began the process of mapping Titan's surface by radar. A joint project of the European Space Agency (ESA) and NASA, "Cassini–Huygens" has proved a very successful mission. The "Cassini" probe flew by Titan on October 26, 2004, and took the highest-resolution images ever of Titan's surface, at only 1200 km, discerning patches of light and dark that would be invisible to the human eye. "Huygens" landed on Titan on January 14, 2005, discovering that many of its surface features seem to have been formed by fluids at some point in the past. Titan is the most distant body from Earth to have a space probe land on its surface. On July 22, 2006, "Cassini" made its first targeted, close fly-by at 950 km from Titan; the closest flyby was at 880 km on June 21, 2010. Liquid has been found in abundance on the surface in the north polar region, in the form of many lakes and seas discovered by "Cassini".
"Huygens" landing site.
The "Huygens" probe landed just off the easternmost tip of a bright region now called Adiri. The probe photographed pale hills with dark "rivers" running down to a dark plain. Current understanding is that the hills (also referred to as highlands) are composed mainly of water ice. Dark organic compounds, created in the upper atmosphere by the ultraviolet radiation of the Sun, may rain from Titan's atmosphere. They are washed down the hills with the methane rain and are deposited on the plains over geological time scales.
After landing, "Huygens" photographed a dark plain covered in small rocks and pebbles, which are composed of water ice. The two rocks just below the middle of the image on the right are smaller than they may appear: the left-hand one is 15 centimeters across, and the one in the center is 4 centimeters across, at a distance of about 85 centimeters from "Huygens". There is evidence of erosion at the base of the rocks, indicating possible fluvial activity. The surface is darker than originally expected, consisting of a mixture of water and hydrocarbon ice. The "soil" visible in the images is interpreted to be precipitation from the hydrocarbon haze above.
In March 2007, NASA, ESA, and COSPAR decided to name the "Huygens" landing site the "Hubert Curien Memorial Station" in memory of the former president of the ESA.
Proposed or conceptual missions.
There have been several conceptual missions proposed in recent years for returning a robotic space probe to Titan. Initial conceptual work has been completed for such missions by NASA, the ESA and JPL. At present, none of these proposals have become funded missions.
The Titan Saturn System Mission (TSSM) was a joint NASA/ESA proposal for exploration of Saturn's moons. It envisions a hot-air balloon floating in Titan's atmosphere for six months. It was competing against the Europa Jupiter System Mission (EJSM) proposal for funding. In February 2009 it was announced that ESA/NASA had given the EJSM mission priority ahead of the TSSM.
There was also a notional concept for a Titan Mare Explorer (TiME), which would be a low-cost lander that would splash down in a lake in Titan's northern hemisphere and float on the surface of the lake for 3 to 6 months.
Another mission to Titan proposed in early 2012 by Jason Barnes, a scientist at a University of Idaho, is the Aerial Vehicle for In-situ and Airborne Titan Reconnaissance (AVIATR): an unmanned plane (or drone) that would fly through Titan's atmosphere and take high-definition images of the surface of Titan. NASA did not approve the requested $715 million, and the future of the project is uncertain.
Another lake lander project was proposed in late 2012 by the Spanish-based private engineering firm SENER and the Centro de Astrobiología in Madrid. The concept probe is called Titan Lake In-situ Sampling Propelled Explorer (TALISE). The major difference compared to the TiME probe would be that TALISE is envisioned with its own propulsion system and would therefore not be limited to simply drifting on the lake when it splashes down.
A Discovery Program contestant for its mission #13 is Journey to Enceladus and Titan (JET), an astrobiology Saturn orbiter that would assess the habitability potential of Enceladus and Titan.
Prebiotic conditions and life.
Titan is thought to be a prebiotic environment rich in complex organic chemistry with a possible subsurface liquid ocean serving as a biotic environment.
Although the "Cassini–Huygens" mission was not equipped to provide evidence for biosignatures or complex organic compounds, it showed an environment on Titan that is similar, in some ways, to ones theorized for the primordial Earth. Scientists surmise that the atmosphere of early Earth was similar in composition to the current atmosphere on Titan, with the important exception of a lack of water vapor on Titan.
Formation of complex molecules.
The Miller–Urey experiment and several following experiments have shown that with an atmosphere similar to that of Titan and the addition of UV radiation, complex molecules and polymer substances like tholins can be generated. The reaction starts with dissociation of nitrogen and methane, forming hydrogen cyanide and acetylene. Further reactions have been studied extensively.
In October 2010, Sarah Horst of the University of Arizona reported finding the five nucleotide bases—building blocks of DNA and RNA—among the many compounds produced when energy was applied to a combination of gases like those in Titan's atmosphere. Horst also found amino acids, the building blocks of protein. She said it was the first time nucleotide bases and amino acids had been found in such an experiment without liquid water being present.
On April 3, 2013, NASA reported that complex organic chemicals could arise on Titan based on studies simulating the atmosphere of Titan.
Possible subsurface habitats.
Laboratory simulations have led to the suggestion that enough organic material exists on Titan to start a chemical evolution analogous to what is thought to have started life on Earth. Although the analogy assumes the presence of liquid water for longer periods than is currently observable, several theories suggest that liquid water from an impact could be preserved under a frozen isolation layer. It has also been theorized that liquid-ammonia oceans could exist deep below the surface. Another model suggests an ammonia–water solution as much as 200 km deep beneath a water-ice crust with conditions that, although extreme by terrestrial standards, are such that life could indeed survive. Heat transfer between the interior and upper layers would be critical in sustaining any subsurface oceanic life. Detection of microbial life on Titan would depend on its biogenic effects. That the atmospheric methane and nitrogen might be of biological origin has been examined, for example.
Methane and life at the surface.
It has been suggested that life could exist in the lakes of liquid methane on Titan, just as organisms on Earth live in water. Such organism would inhale H2 in place of O2, metabolize it with acetylene instead of glucose, and exhale methane instead of carbon dioxide.
Although all living things on Earth (including methanogens) use liquid water as a solvent, it is speculated that life on Titan might instead use a liquid hydrocarbon, such as methane or ethane. Water is a stronger solvent than methane. However, water is also more chemically reactive, and can break down large organic molecules through hydrolysis. A life-form whose solvent was a hydrocarbon would not face the risk of its biomolecules being destroyed in this way.
In 2005, astrobiologist Chris McKay argued that if methanogenic life did exist on the surface of Titan, it would likely have a measurable effect on the mixing ratio in the Titan troposphere: levels of hydrogen and acetylene would be measurably lower than otherwise expected.
In 2010, Darrell Strobel, from Johns Hopkins University, identified a greater abundance of molecular hydrogen in the upper atmospheric layers of Titan compared to the lower layers, arguing for a downward flow at a rate of roughly 1025 molecules per second and disappearance of hydrogen near Titan's surface; as Strobel noted, his findings were in line with the effects McKay had predicted if methanogenic life-forms were present. The same year, another study showed low levels of acetylene on Titan's surface, which were interpreted by McKay as consistent with the hypothesis of organisms consuming hydrocarbons. Although restating the biological hypothesis, he cautioned that other explanations for the hydrogen and acetylene findings are more likely: the possibilities of yet unidentified physical or chemical processes (e.g. a surface catalyst accepting hydrocarbons or hydrogen), or flaws in the current models of material flow. Composition data and transport models need to be substantiated, etc. Even so, despite saying that a non-biological catalytic explanation would be less startling than a biological one, McKay noted that the discovery of a catalyst effective at 95 K would still be significant.
As NASA notes in its news article on the June 2010 findings: "To date, methane-based life forms are only hypothetical. Scientists have not yet detected this form of life anywhere." As the NASA statement also says: "some scientists believe these chemical signatures bolster the argument for a primitive, exotic form of life or precursor to life on Titan's surface."
In February 2015, a hypothetical cell membrane capable of functioning in liquid methane in Titan conditions was modeled. Composed of small molecules containing carbon, hydrogen, and nitrogen, it would have the same stability and flexibility as cell membranes on Earth, which are composed of phospholipids, compounds of carbon, hydrogen, oxygen, and phosphorus. This hypothetical cell membrane was termed an "azotosome", a portmanteau from "azote", French for nitrogen, and "liposome".
Obstacles.
Despite these biological possibilities, there are formidable obstacles to life on Titan, and any analogy to Earth is inexact. At a vast distance from the Sun, Titan is frigid, and its atmosphere lacks CO2. At Titan's surface, water exists only in solid form. Because of these difficulties, scientists such as Jonathan Lunine have viewed Titan less as a likely habitat for life, than as an experiment for examining theories on the conditions that prevailed prior to the appearance of life on Earth. Although life itself may not exist, the prebiotic conditions on Titan and the associated organic chemistry remain of great interest in understanding the early history of the terrestrial biosphere. Using Titan as a prebiotic experiment involves not only observation through spacecraft, but laboratory experiments, and chemical and photochemical modeling on Earth.
Panspermia hypothesis.
It is hypothesized that large asteroid and cometary impacts on Earth's surface may have caused fragments of microbe-laden rock to escape Earth's gravity, suggesting the possibility of transpermia. Calculations indicate that a number of these would encounter many of the bodies in the Solar System, including Titan. On the other hand, Jonathan Lunine has argued that any living things in Titan's cryogenic hydrocarbon lakes would need to be so different chemically from Earth life that it would not be possible for one to be the ancestor of the other.
Future conditions.
Conditions on Titan could become far more habitable in the far future. Five billion years from now, as the Sun becomes a red giant, its surface temperature could rise enough for Titan to support liquid water on its surface making it habitable. As the Sun's ultraviolet output decreases, the haze in Titan's upper atmosphere will be depleted, lessening the anti-greenhouse effect on the surface and enabling the greenhouse created by atmospheric methane to play a far greater role. These conditions together could create a habitable environment, and could persist for several hundred million years. This was sufficient time for simple life to evolve on Earth, although the presence of ammonia on Titan would cause chemical reactions to proceed more slowly.
External links.
Listen to this article ()
This audio file was created from a revision of the "Titan (moon)" article dated 2011-10-25, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="47403" url="http://en.wikipedia.org/wiki?curid=47403" title="Instrumentation">
Instrumentation

Instrumentation is the use of measuring instruments to monitor and control a process. It is the art and science of measurement and control of process variables within a production, laboratory, or manufacturing area.
An instrument is a device that measures a physical quantity such as flow, temperature, level, distance, angle, or pressure. Instruments may be as simple as direct reading thermometers or may be complex multi-variable process analyzers. Instruments are often part of a control system in refineries, factories, and vehicles. The control of processes is one of the main branches of applied instrumentation. Instrumentation can also refer to handheld devices that measure some desired variable. Diverse handheld instrumentation is common in laboratories, but can be found in the household as well. For example, a smoke detector is a common instrument found in most western homes.
Instruments attached to a control system may provide signals used to operate solenoids, valves, regulators, circuit breakers, or relays. These devices control a desired output variable, and provide either remote or automated control capabilities. These are often referred to as final control elements when controlled remotely or by a control system.
A transmitter is a device that produces an output signal, often in the form of a 4–20 mA electrical current signal, although many other options using voltage, frequency, pressure, or ethernet are possible. This signal can be used for informational purposes, or it can be sent to a PLC, DCS, SCADA system, LabVIEW or other type of computerized controller, where it can be interpreted into readable values and used to control other devices and processes in the system.
Control instrumentation plays a significant role in both gathering information from the field and changing the
field parameters, and as such are a key part of control loops.
History.
Elements of industrial instrumentation have long histories. Scales for comparing weights and simple pointers to indicate position are ancient technologies. Some of the earliest measurements were of time. One of the oldest water clocks was found in the tomb of the Egyptian pharaoh Amenhotep I, buried around 1500 BCE.
Improvements were incorporated in the clocks. By 270 BCE they had the rudiments of an automatic control system device. In 1663 Christopher Wren presented the Royal Society with a design for a "weather clock". A drawing shows meteorological sensors moving pens over paper driven by clockwork. Such devices did not become standard in meteorology for two centuries. The concept has remained virtually unchanged as evidenced by pneumatic chart recorders, where a pressurized bellows displaces a pen. Integrating sensors, displays, recorders and controls was uncommon until the industrial revolution, limited by both need and practicality.
In the early years of process control, process indicators and control elements such as valves were monitored by an operator that walked around the unit adjusting the valves to obtain the desired temperatures, pressures, and flows. As technology evolved pneumatic controllers were invented and mounted in the field that monitored the process and controlled the valves. This reduced the amount of time process operators were needed to monitor the process. Later years the actual controllers were moved to a central room and signals were sent into the control room to monitor the process and outputs signals were sent to the final control element such as a valve to adjust the process as needed. These controllers and indicators were mounted on a wall called a control board. The operators stood in front of this board walking back and forth monitoring the process indicators. This again reduced the number and amount of time process operators were needed to walk around the units. The most standard pneumatic signal level used during these years was 3-15 psig.
Electronics enabled wiring to replace pipes. The transistor was commercialized by the mid-1950s.
Each instrument company introduced their own standard instrumentation signal, causing confusion until the 4-20 mA range was used as the standard electronic instrument signal for transmitters and valves. This signal was eventually standardized as ANSI/ISA S50, “Compatibility of Analog Signals for Electronic Industrial Process Instruments", in the 1970s. The transformation of instrumentation from mechanical pneumatic transmitters, controllers, and valves to electronic instruments reduced maintenance costs as electronic instruments were more dependable than mechanical instruments. This also increased efficiency and production due to their increase in accuracy. Pneumatics enjoyed some advantages, being favored in corrosive and explosive atmospheres.
The pneumatic and electronic signaling standards allowed centralized monitoring and control of a distributed process. The concept was limited by communication line lengths (perhaps 100 meters for pneumatics). Each pipe or wire pair carried one signal. The next evolution of instrumentation came with the production of Distributed Control Systems (DCS) which allowed monitoring and control from multiple locations which could be widely separated. A process operator could sit in front of a screen (no longer a control board) and monitor thousands of points throughout a large complex. A closely related development was termed “Supervisory Control and Data Acquisition” (SCADA). These technologies were supported by personal computers, networks and graphical user interfaces.
Definition.
The Oxford English Dictionary says (as its last definition of Instrumentation), "The design, construction, and provision of instruments for measurement, control, etc; the state of being equipped with or controlled by such instruments collectively." It notes that this use of the word originated in the U.S.A. in the early 20th century. More traditional uses of the word were associated with musical or surgical instruments. While the word is traditionally a noun, it is also used as an adjective (as instrumentation engineer, instrumentation amplifier and instrumentation system). Other dictionaries note that the word is most common in describing aeronautical, scientific or industrial instruments.
Measurement instruments have three traditional classes of use:
While these uses appear distinct, in practice they are less so. All measurements have the potential for decisions and control. A home owner may change a thermostat setting in response to a utility bill computed from meter readings.
Examples.
In some cases the sensor is a very minor element of the mechanism. Digital cameras and wristwatches might technically meet the loose definition of instrumentation because they record and/or display sensed information. Under most circumstances neither would be called instrumentation, but when used to measure the elapsed time of a race and to document the winner at the finish line, both would be called instrumentation.
Household.
A very simple example of an instrumentation system is a mechanical thermostat, used to control a household furnace and thus to control room temperature. A typical unit senses temperature with a bi-metallic strip. It displays temperature by a needle on the free end of the strip. It activates the furnace by a mercury switch. As the switch is rotated by the strip, the mercury makes physical (and thus electrical) contact between electrodes.
Another example of an instrumentation system is a home security system. Such a system consists of 
sensors (motion detection, switches to detect door openings), simple algorithms to detect intrusion, local control (arm/disarm) and remote monitoring of the system so that the police can be summoned. Communication is an inherent part of the design.
Kitchen appliances use sensors for control.
Automotive.
Modern automobiles have complex instrumentation. In addition to displays of engine rotational speed and vehicle linear speed, there are also displays of battery voltage and current, fluid levels, fluid temperatures, distance traveled and feedbacks of various controls (turn signals, parking brake, headlights, transmission position). Cautions may be displayed for special problems (fuel low, check engine, tire pressure low, door ajar, seat belt unfastened). Problems are recorded so they can be reported to diagnostic equipment. Navigation systems can provide voice commands to reach a destination. Automotive instrumentation must be cheap and reliable over long periods in harsh environments. There may be independent airbag systems which contain sensors, logic and actuators. Anti-skid braking systems use sensors to control the brakes, while cruise control affects throttle position. A wide variety of services can be provided via communication links as the OnStar system. Autonomous cars (with exotic instrumentation) have been demonstrated.
Aircraft.
Early aircraft had a few sensors. "Steam gauges" converted air pressures into needle deflections that could be interpreted as altitude and airspeed. A magnetic compass provided a sense of direction. The displays to the pilot were as critical as the measurements.
A modern aircraft has a far more sophisticated suite of sensors and displays, which are embedded into avionics systems. The aircraft may contain inertial navigation systems, global positioning systems, weather radar, autopilots, and aircraft stabilization systems. Redundant sensors are used for reliability. A subset of the information may be transferred to a crash recorder to aid mishap investigations. Modern pilot displays now include computer displays including head-up displays.
Air traffic control radar is distributed instrumentation system. The ground portion transmits an electromagnetic pulse and receives an echo (at least). Aircraft carry transponders that transmit codes on reception of the pulse. The system displays aircraft map location, an identifier and optionally altitude. The map location is based on sensed antenna direction and sensed time delay. The other information is embedded in the transponder transmission.
Laboratory instrumentation.
Among the possible uses of the term is a collection of laboratory test equipment controlled by a computer through an IEEE-488 bus (also known as GPIB for General Purpose Instrument Bus or HPIB for Hewlitt Packard Instrument Bus). Laboratory equipment is available to measure many electrical and chemical quantities. Such a collection of equipment might be used to automate the testing of drinking water for pollutants.
Measurement.
Instrumentation is used to measure many parameters (physical values). These parameters include:
Control.
In addition to measuring field parameters, instrumentation is also responsible for providing the ability to modify some field parameters. That means the instrument is not only for measuring purposes, but also for changing and modification of the process system, these instruments are generally referred to as actuators. In industries, actuators are used to regulate fluid, control flow, moderate temperatures and open/close electric circuits.
Instrumentation engineering.
Instrumentation engineering is the engineering specialization focused on the principle and operation of measuring instruments that are used in design and configuration of automated systems in electrical, pneumatic domains etc.
They typically work for industries with automated processes, such as chemical or manufacturing plants, with the goal of improving system productivity, reliability, safety, optimization, and stability.
To control the parameters in a process or in a particular system, devices such as microprocessors, microcontrollers or PLCs are used, but their ultimate aim is to control the parameters of a system.
Instrumentation engineering is loosely defined because the required tasks are very domain dependent. An expert in the biomedical instrumentation of laboratory rats has very different concerns than the expert in rocket instrumentation. Common concerns of both are the selection of appropriate sensors based on size, weight, cost, reliability, accuracy, longevity, environmental robustness and frequency response. Some sensors are literally fired in artillery shells. Others sense thermonuclear explosions until destroyed. Invariably sensor data must be recorded, transmitted or displayed. Recording rates and capacities vary enormously. Transmission can be trivial or can be clandestine, encrypted and low-power in the presence of jamming. Displays can be trivially simple or can require consultation with human factors experts. Control system design varies from trivial to a separate specialty.
Instrumentation engineers are commonly responsible for integrating the sensors with the recorders, transmitters, displays or control systems. They may design or specify installation, wiring and signal conditioning. They may be responsible for calibration, testing and maintenance of the system.
In a research environment it is common for subject matter experts to have substantial instrumentation system expertise. An astronomer knows the structure of the universe and a great deal about telescopes - optics, pointing and cameras (or other sensing elements). That often includes the hard-won knowledge of the operational procedures that provide the best results. For example, an astronomer is often knowledgeable of techniques to minimize temperature gradients that cause air turbulence within the telescope.
Instrumentation technologists and mechanics.
Instrumentation technologists, technicians and mechanics specialize in troubleshooting and repairing and maintenance of instruments and instrumentation systems.
External links.
The field of instrumentation is richly served with periodicals.
Examples:
Many other periodicals serve specific industries (hydraulics, 
pneumatics, medical, chemical, imaging, nuclear, robotics...).

</doc>
<doc id="47404" url="http://en.wikipedia.org/wiki?curid=47404" title="Anne of Cleves">
Anne of Cleves

Anne of Cleves (German: "Anna"; 22 September 1515 – 16 July 1557) was Queen of England from 6 January 1540 to 9 July 1540 as the fourth wife of King Henry VIII. The marriage was declared never consummated, and, as a result, she was not crowned queen consort. Following the annulment of their marriage, Anne was given a generous settlement by the King, and thereafter referred to as "the King's Beloved Sister". She lived to see the coronation of Queen Mary I, outliving the rest of Henry's wives.
Early life.
Anne was born on 22 September 1515 in Düsseldorf, the second daughter of John III of the House of La Marck, Duke of Jülich "jure uxoris", Cleves, Berg "jure uxoris", Count of Mark, also known as de la Marck and Ravensberg "jure uxoris" (often referred to as Duke of Cleves) who died in 1538, and his wife Maria, Duchess of Julich-Berg (1491–1543). She grew up living in Schloss Burg on the edge of Solingen. Anne's father was influenced by Erasmus and followed a moderate path within the Reformation. He sided with the Schmalkaldic League and opposed Emperor Charles V. After John's death, Anne's brother William became Duke of Jülich-Cleves-Berg, bearing the promising epithet "The Rich". In 1526, her elder sister Sibylle was married to John Frederick, Elector of Saxony, head of the Protestant Confederation of Germany and considered the "Champion of the Reformation".
At the age of 11 (1527), Anne was betrothed to Francis, son and heir of the Duke of Lorraine while he was only 10. Thus the betrothal was considered unofficial and was cancelled in 1535. Her brother William was a Lutheran but the family was unaligned religiously, with her mother, the Duchess Maria, described as a "strict Catholic". The Duke's ongoing dispute over Gelderland with Emperor Charles V made them suitable allies for England's King Henry VIII in the wake of the Truce of Nice. The match with Anne was urged on the King by his chief minister, Thomas Cromwell.
Wedding preparations.
The artist Hans Holbein the Younger was dispatched to paint portraits of Anne and her younger sister, Amalia, each of whom Henry was considering as his fourth wife. Henry required the artist to be as accurate as possible, not to flatter the sisters. The two versions of Holbein's portrait are in the Musée du Louvre in Paris and the Victoria and Albert Museum in London. Another 1539 portrait, by the school of Barthel Bruyn the Elder, is in the collection of Trinity College, Cambridge. 
Negotiations with Cleves were in full swing by March 1539. Cromwell oversaw the talks, and a marriage treaty was signed on 4 October of that year.
Henry valued education and cultural sophistication in women, but Anne lacked these: She had received no formal education but was skilled in needlework and liked playing card games. She could read and write, but only in German. Nevertheless, Anne was considered gentle, virtuous, and docile, qualities that recommended her as a suitable candidate for Henry.
Anne was described by the French ambassador, Charles de Marillac, as tall and slim, "of middling beauty, and of very assured and resolute countenance". She was fair haired and was said to have had a lovely face. In the words of the chronicler Edward Hall, "Her hair hanging down, which was fair, yellow and long ... she was apparelled after the English fashion, with a French hood, which so set forth her beauty and good visage, that every creature rejoiced to behold her". She appeared rather solemn by English standards, and looked old for her age. Holbein painted her with high forehead, heavy-lidded eyes and a pointed chin.
Henry met her privately on New Year's Day 1540 at Rochester on her journey from Dover. Henry and some of his courtiers, following a courtly-love tradition, went disguised into the room where Anne was staying, and Henry boldly kissed her. According to the testimony of his companions, he was disappointed with Anne, feeling she was not as described. According to the chronicler Charles Wriothesley Anne "regarded him little", though it is unknown if she knew if this was the king or not. Henry did then reveal his true identity to Anne, although he is said to have been put off the marriage from then on. Henry and Anne then met officially on 3 January on Blackheath outside the gates of Greenwich Park, where a grand reception was laid out.
Most historians believe that he later used Anne's alleged 'bad' appearance and failure to inspire him to consummate the marriage as excuses, saying how he felt he had been misled, for everyone had praised Anne's attractions: "She is nothing so fair as she hath been reported," he complained. Henry urged Cromwell to find a legal way to avoid the marriage but, by this point, doing so was impossible without endangering the vital alliance with the Germans.
A doomed marriage.
Despite Henry's very vocal misgivings, the two were married on 6 January 1540 at the royal Palace of Placentia in Greenwich, London by Archbishop Thomas Cranmer. The phrase "God send me well to keep" was engraved around Anne’s wedding ring. Immediately after arriving in England, Anne conformed to the Anglican form of worship, which Henry expected. The couple's first night as husband and wife was not a successful one. Henry confided to Cromwell that he had not consummated the marriage, saying, "I liked her before not well, but now I like her much worse." He described her as having unpleasant body odour and sagging breasts, among other complaints. In February 1540, Anne praised the King as a kind husband to the Countess of Rutland, saying: "When he comes to bed he kisseth me, and he taketh me by the hand, and biddeth me 'Good night, sweetheart'; and in the morning kisseth me and biddeth 'Farewell, darling.'" Lady Rutland responded: "Madam, there must be more than this, or it will be long ere we have a duke of York, which all this realm most desireth."
Anne was commanded to leave the Court on 24 June, and on 6 July she was informed of her husband's decision to reconsider the marriage. Witness statements were taken from a number of courtiers and two physicians which register the king's disappointment at her appearance. Henry had also commented to Thomas Heneage and Anthony Denny that he could not believe she was a virgin. Shortly afterwards, Anne was asked for her consent to an annulment, to which she agreed. The marriage was annulled on 9 July 1540, on the grounds of non-consummation and her pre-contract to Francis of Lorraine. Henry VIII's physician stated that after the wedding night, Henry said he was not impotent because he experienced "duas pollutiones nocturnas in somno" (two nocturnal pollutions while in sleep; i.e., two wet dreams).
After the annulment.
The former queen received a generous settlement, including Richmond Palace, and Hever Castle, home of Henry's former in-laws, the Boleyns. Anne of Cleves House, in Lewes, Sussex, is just one of many properties she owned; she never lived there. Henry and Anne became good friends—she was an honorary member of the King's family and was referred to as "the King's Beloved Sister". She was invited to court often and, out of gratitude for her not contesting the annulment, Henry decreed that she would be given precedence over all women in England save his own wife and daughters.
After Catherine Howard was beheaded, Anne and her brother, the Duke of Cleves, pressed the king to remarry Anne. Henry quickly refused to do so. She seems to have disliked Catherine Parr, and reportedly reacted to the news of Henry's sixth marriage with the remark "Madam Parr is taking a great burden on herself." 
In March 1547, Edward VI's Privy Council asked her to move out of Bletchingley Palace, her usual residence, to Penshurst Place to make way for Thomas Cawarden, Master of Revels. They pointed out that Penshurst was nearer to Hever and the move had been Henry VIII's will.
On 4 August 1553, Anne wrote to Mary to congratulate her on her marriage to Philip of Spain. On 28 September 1553, when Mary left St James's Palace for Whitehall, she was accompanied by her sister Elizabeth and Anne of Cleves. Anne also took part in Mary I's coronation procession, and may have been present at her coronation at Westminster Abbey. These were her last public appearances. As the new Queen was a strict Catholic, Anne yet again changed religion, now becoming a Roman Catholic.
After a brief return to prominence, she lost royal favour in 1554, following Wyatt's rebellion. According to Simon Renard, the imperial ambassador, Anne's close association with Elizabeth had convinced the Queen that "the Lady [Anne] of Cleves was of the plot and intrigued with the Duke of Cleves to obtain help for Elizabeth: matters in which the king of France was the prime mover". There is no evidence that Anne was invited back to court after 1554. She was compelled to live a quiet and obscure life on her estates. After her arrival as the King's bride, Anne never left England. Despite occasional feelings of homesickness, Anne was generally content in England and was described by Holinshed as "a ladie of right commendable regards, courteous, gentle, a good housekeeper and verie bountifull to hir seruants."
Death.
When Anne's health began to fail, Mary allowed her to live at Chelsea Old Manor, where Henry's last wife, Catherine Parr, had lived after her remarriage. Here, in the middle of July 1557, Anne dictated her last will. In it, she mentions her brother, sister, and sister-in-law, as well as the future Queen Elizabeth, the Duchess of Suffolk, and the Countess of Arundel. She left some money to her servants and asked Mary and Elizabeth to employ them in their households. She was remembered by everyone who served her as a particularly generous and easy-going mistress.
Anne died at Chelsea Old Manor on 16 July 1557, eight weeks before her forty-second birthday. The cause of her death was most likely to have been cancer. She was buried in Westminster Abbey, on 3 August, in what has been described as a "somewhat hard to find tomb" on the opposite side of Edward the Confessor's shrine and slightly above eye level for a person of average height. She is the only wife of Henry VIII to be buried in the Abbey.
She also has the distinction of being the last of Henry VIII's wives to die (she outlived Henry's last wife, Catherine Parr, by 9 years). She was not the longest-lived, however, since Catherine of Aragon was 50 at the time of her death.
Literature.
Anne is the subject of three biographies: Julia Hamilton's "Anne of Cleves" (1972), and Mary Saaler's "Anne of Cleves: Fourth Wife of Henry VIII" (1995), and Elizabeth Norton's "Anne of Cleves: Henry VIII's Discarded Bride" (2009). Retha Warnicke has written an academic study on Anne's marriage called "The Marrying of Anne of Cleves. Royal Protocol in Early Modern England" (2000).
Anne of Cleves appears as a character in many historical novels about Henry's reign. In "The Fifth Queen" (1906) by Ford Madox Ford she is portrayed as a sensible, practical woman who happily settles for annulment in return for material benefits. Anne of Cleves is the main character of "My Lady of Cleves" (1946) by Margaret Campbell Barnes. About a third of "The Boleyn Inheritance" (2006) by Philippa Gregory is recounted from Anne's point of view, covering the period of Henry VIII's marriages to her and to her successor Catherine Howard. The book concludes with Anne living away from court, and avoiding the execution ceremonies of Howard and of Jane Boleyn, sister-in-law to one of Henry's queens and lady-in-waiting to all the others, including Anne. Gregory includes Anne in a non-fictional review of the period at the end of the book.
Anne and her Holbein portrait in the Louvre are the focus of the novel "Amenable Women" (2009) by Mavis Cheek. Anne and Catherine Howard are the subject of "The Queen's Mistake" by Diane Haeger (2009), while Anne and Jane Seymour are covered in Volume 3 of Dixie Atkins's tetralogy "A Golden Sorrow" (2010).
References.
</dl>

</doc>
<doc id="47406" url="http://en.wikipedia.org/wiki?curid=47406" title="Catherine Parr">
Catherine Parr

Catherine Parr (alternatively Katherine or Kateryn) ( () – ) was Queen of England from 1543 until 1547, as the last of the six wives of King Henry VIII. She married him on 12 July 1543, and outlived him. She was also the most-married English queen, with four husbands, and the first English queen to be titled "Queen of Ireland".
Catherine enjoyed a close relationship with Henry's three children and was personally involved in the education of Elizabeth and Edward, both of whom became English monarchs. She was influential in Henry's passing of the Third Succession Act in 1543 that restored both his daughters, Mary and Elizabeth, to the line of succession to the throne.
Catherine was appointed Regent from July to September 1544 while Henry was on a military campaign in France and in case he lost his life, she was to rule as Regent until Edward came of age. However he did not give her any function in government in his will. In 1544, she published her first book, "Psalms or Prayers", anonymously. On account of Catherine's Protestant sympathies, she provoked the enmity of powerful Catholic officials who sought to turn the King against her—a warrant for her arrest was drawn up in 1546. However, she and the King soon reconciled. Her book "Prayers or Meditations" became the first book published by an English queen under her own name. She assumed the role of Elizabeth's guardian following the King's death, and published a third book, "The Lamentations of a Sinner".
Henry died on 28th January 1547. Six months after Henry's death, she married her fourth and final husband, Thomas Seymour, 1st Baron Seymour of Sudeley. The marriage was short-lived, as she died in September 1548, probably of complications of childbirth.
Early life.
Catherine was born in 1512, probably in August. She was the oldest surviving child of Sir Thomas Parr, lord of the manor of Kendal in Westmorland (now Cumbria), a descendant of King Edward III, and of the former Maud Green, daughter and co-heiress of Sir Thomas Green, lord of Greens Norton, Northamptonshire. The Parrs were a substantial northern family which included many knights. She had a younger brother, William, later created first Marquess of Northampton, and a sister, Anne, later Countess of Pembroke. Sir Thomas was Sheriff of Northamptonshire, Master of the Wards, and Comptroller to King Henry VIII. Parr was also a close companion of the King. Her mother was a close friend and attendant of Catherine of Aragon, and Catherine Parr was probably named after Queen Catherine, who was her godmother.
It was once thought that Catherine Parr had been born at Kendal Castle in Westmorland. However, at the time of her birth, Kendal Castle was already in very poor condition. During her pregnancy, Maud Parr was at court attending the Queen, and by necessity the Parr family was living in their town house at Blackfriars. Historians now consider it unlikely that Catherine's father, Sir Thomas Parr, would take his pregnant wife on an arduous two-week journey north over bad roads to give birth in a crumbling castle in which neither of them seemed to spend much time. Her father died when she was young, and Catherine was close to her mother as she grew up.
Catherine's initial education was similar to other well-born women, but she developed a passion for learning which would continue throughout her life. She was fluent in French, Latin, and Italian, and began learning Spanish when she was Queen. According to her biographer, Linda Porter, the story that as a child, Catherine could not tolerate sewing and often said to her mother "my hands are ordained to touch crowns and sceptres, not spindles and needles" is almost certainly apocryphal.
In 1529, when she was seventeen, Catherine married Sir Edward Borough, a grandson of Edward Burgh, 2nd Baron Burgh. Earlier biographies of Catherine mistakenly reported that she had married the grandfather. When her husband's grandfather was declared insane in December 1529, Catherine's father-in-law Sir Thomas Borough was summoned to Parliament as the first Baron Burgh.
Catherine's first husband was in his twenties and may have been in poor health. He served as a feoffee for Thomas Kiddell and as a justice of the peace. His father also secured a joint patent in survivorship with his son for the office of steward of the manor of the soke of Kirton in Lindsey. The younger Sir Edward Borough died in the spring of 1533, not surviving to inherit the title of Baron Borough.
Lady Latimer.
Following her first husband's death, Catherine may have spent time with the Dowager Lady Strickland, Catherine Neville, who was the widow of Catherine's cousin Sir Walter Strickland, at the Stricklands' family residence of Sizergh Castle in Westmorland (now in Cumbria). In the summer of 1534 Catherine married secondly John Neville, 3rd Baron Latimer, her father's second cousin and a kinsman of Lady Strickland. With this marriage, Catherine became only the second woman in the Parr family to marry into the peerage.
The twice-widowed Latimer was twice Catherine's age. From his first marriage to Dorothy de Vere, sister of John de Vere, 14th Earl of Oxford, he had two children, John and Margaret. Although Latimer was in financial difficulties after he and his brothers had pursued legal action to claim the title of Earl of Warwick, Catherine now had a home of her own, a husband with a position and influence in the north, and a title.
Latimer was a supporter of the Roman Catholic Church and had bitterly opposed the king's first divorce, his subsequent marriage to Anne Boleyn, and the religious consequences. In October 1536, during the Lincolnshire Rising, a mob of rebellious Roman Catholics appeared before the Latimers' home threatening violence if Latimer did not join their efforts to reinstate the links between England and Rome. Catherine watched as her husband was dragged away. Between October 1536 and April 1537 Catherine lived alone in fear with her step-children, struggling to survive. It is probable that, in these uncertain times, Catherine's strong reaction against the rebellion strengthened her adherence to the reformed Church of England. In January 1537, during the uprising of the North, Catherine and her step-children were held hostage at Snape Castle in Yorkshire. The rebels ransacked the house and sent word to Lord Latimer, who was returning from London, that if he did not return immediately they would kill his family. When Latimer returned to the castle, he somehow talked the rebels into releasing his family and leaving, but the aftermath would prove to be taxing on the whole family.
The King and Thomas Cromwell heard conflicting reports as to whether Latimer was a prisoner or a conspirator. As a conspirator, he could be found guilty of treason, forfeiting his estates and leaving Catherine and her step-children penniless. The King himself wrote to the Duke of Norfolk, pressing him to make sure Latimer would "condemn that villain Aske and submit to our clemency". Latimer complied. It is likely that Catherine's brother William Parr and his uncle, William Parr, 1st Baron Parr of Horton, who both fought against the rebellion, intervened to save Latimer's life.
Although no charges were laid against him, Latimer's reputation, which reflected upon Catherine, was tarnished for the rest of his life. Over the next seven years, the family spent much of their time in the south. For several years, Latimer was blackmailed by Cromwell and forced to do his bidding. After Cromwell's death in 1540, the Latimers reclaimed some dignity. In 1542 the family spent time in London as Latimer attended Parliament. Catherine visited her brother William and her sister Anne at court. It was here that Catherine became acquainted with her future fourth husband, Sir Thomas Seymour. The atmosphere of the court was greatly different from that of the rural estates she knew. There, Catherine could find the latest trends, not only in religious matters, but in less weighty secular matters such as fashion and jewellery.
By the winter of 1542, Lord Latimer's health had worsened. Catherine nursed her husband until his death in 1543. In his will, Catherine was named as guardian of his daughter, Margaret, and was put in charge of his affairs until his daughter's majority. Latimer left Catherine the manor of Stowe and other properties. He also bequeathed money for supporting his daughter, and in the case that his daughter did not marry within five years, Catherine was to take £30 a year out of the income to support her step-daughter. Catherine was left a rich widow, but after Lord Latimer's death she faced the possibility of having to return north. It is likely that Catherine sincerely mourned her husband; she kept a remembrance of him, his New Testament with his name inscribed inside, until her death.
Using her late mother's friendship with Henry's first queen, Catherine of Aragon, Catherine took the opportunity to renew her own friendship with the former queen's daughter, Lady Mary. By 16 February 1543, Catherine had established herself as part of Mary's household, and it was there that Catherine caught the attention of the King. Although she had begun a romantic friendship with Sir Thomas Seymour, the brother of the late queen Jane Seymour, she saw it as her duty to accept Henry's proposal over Seymour's. Seymour was given a posting in Brussels to remove him from the king's court.
Queen of England and Ireland.
Catherine married Henry VIII on 12 July 1543 at Hampton Court Palace. She was the first Queen of England also to be Queen of Ireland following Henry's adoption of the title King of Ireland. Catherine and her new husband shared several common ancestors making them multiple cousins. By their mothers they were third cousins sharing Sir Richard Wydeville and Joan Bedlisgate; by Henry's mother and Catherine's father they were third cousins once removed sharing Ralph Neville, 1st Earl of Westmorland and Lady Joan Beaufort; and by their fathers they were double fourth cousins once removed sharing Thomas Holland, 2nd Earl of Kent and Lady Alice FitzAlan and John of Gaunt, 1st Duke of Lancaster and Katherine Swynford.
On becoming queen, Catherine installed her former stepdaughter, Margaret Neville, as her lady-in-waiting, and gave her stepson John's wife a position in her household. Catherine was partially responsible for reconciling Henry with his daughters from his first two marriages, and also developed a good relationship with Henry's son Edward. When she became queen, her uncle Lord Parr of Horton became her Lord Chamberlain.
Henry went on his last, unsuccessful, campaign to France from July to September 1544, leaving Catherine as his regent. Because her regency council was composed of sympathetic members, including her uncle, Thomas Cranmer (the Archbishop of Canterbury) and Lord Hertford, Catherine obtained effective control and was able to rule as she saw fit. She handled provision, finances and musters for Henry's French campaign, signed five royal proclamations, and maintained constant contact with her lieutenant in the northern Marches, Lord Shrewsbury, over the complex and unstable situation with Scotland. It is thought that her actions as regent, together with her strength of character and noted dignity, and later religious convictions, greatly influenced her stepdaughter Lady Elizabeth (the future Elizabeth I of England).
The Queen's religious views were viewed with suspicion by Catholic and anti-Protestant officials such as Stephen Gardiner (the Bishop of Winchester) and Lord Wriothesley (the Lord Chancellor). Although she must have been brought up as a Catholic, given her birth before the Protestant Reformation, she later became sympathetic to and interested in the "New Faith". By the mid-1540s, she came under suspicion that she was actually a Protestant, as we would now understand the term. This view is supported by the strong reformed ideas that she revealed after Henry's death, when her second book, "Lamentacions of a synner" (Lamentations of a Sinner), was published in late 1547. The book promoted the Protestant concept of justification by faith alone, which the Catholic Church deemed to be heresy. It is unlikely that she developed these views in the short time between Henry's death and the publication of the book. Her sympathy with Anne Askew, the Protestant martyr who fiercely opposed the Catholic belief of transubstantiation, also suggests that she was more than merely sympathetic to the new religion.
In 1546, the Bishop of Winchester and Lord Wriothesley tried to turn the king against her. An arrest warrant was drawn up for her and rumours abounded across Europe that the King was attracted to her close friend, the Duchess of Suffolk. However, she saw the warrant and managed to reconcile with the King after vowing that she had only argued about religion with him to take his mind off the suffering caused by his ulcerous leg. The following day an armed guard who was unaware of the reconciliation tried to arrest her while she walked with the King.
Final marriage, childbirth and death.
Shortly before he died, Henry made provision for an allowance of £7,000 per year for Catherine to support herself. He further ordered that, after his death, Catherine, though a queen dowager, should be given the respect of a queen of England, as if he were still alive. Catherine retired from court after the coronation of her stepson, Edward VI, on 31 January 1547, to her home at Old Manor in Chelsea.
Following Henry's death, Catherine's old love, Sir Thomas Seymour, returned to court. Catherine was quick to accept when Seymour renewed his suit of marriage. Since only six months had passed since the death of King Henry, Seymour knew that the Regency council would not agree to a petition for the queen dowager to marry so soon. Sometime near the end of May, Catherine and Seymour married in secret. King Edward VI and council were not informed of the union for several months. When their union became public knowledge, it caused a small scandal. The King and Lady Mary were very much displeased by the union. After being censured and reprimanded by the council, Seymour wrote to the Lady Mary asking her to intervene on his behalf. Mary became furious at his forwardness and tasteless actions and refused to help. Mary even went as far as asking her half-sister, Lady Elizabeth, not to interact with Queen Catherine any further.
During this time, Catherine began having altercations with her brother-in-law, Edward Seymour, 1st Duke of Somerset. Like Thomas, Edward was the King's uncle, and also was the Lord Protector. A rivalry developed between Catherine and his wife, her own former lady-in-waiting, the Duchess of Somerset, which became particularly acute over the matter of Catherine's jewels. The Duchess argued that as queen dowager, Catherine was no longer entitled to wear the jewels belonging to the wife of the king. Instead she, as the wife of the protector, should be the one to wear them. Eventually, the Duchess won the argument, which left her relationship with Catherine permanently damaged; the relationship between the two Seymour brothers also worsened as a result, since Lord Thomas saw the whole dispute as a personal attack by his brother on his social standing.
In November 1547, Catherine published her second book, "Lamentations of a Sinner". The book was a success and widely praised. In early 1548, Catherine invited Lady Elizabeth and her cousin, Lady Jane Grey, to stay in the couple's household at Sudeley Castle in Gloucestershire. The dowager queen promised to provide education for both. Queen Catherine's house came to be known as a respected place of learning for young women.
In March 1548, at age 35, Catherine became pregnant. This pregnancy was a surprise as Catherine had not conceived during her first three marriages. During this time, Seymour began to take an interest in Lady Elizabeth. Seymour had reputedly plotted to marry her before marrying Catherine, and it was reported later that Catherine discovered the two in an embrace. On a few occasions before the situation risked getting completely out of hand, according to the deposition or testimony of Kat Ashley, Catherine appears not only to have acquiesced in episodes of horseplay, but actually to have assisted her husband. Whatever actually happened, Elizabeth was sent away in May 1548 to stay with Sir Anthony Denny's household at Cheshunt and never saw her beloved stepmother again, although the two corresponded. Elizabeth immediately wrote a letter to the Queen and Seymour after she left Chelsea. The letter demonstrates a sort of remorse.
One must understand that the deposition of Kat Ashley, which incorporates the queen joining her husband in his escapades, was given after Ashley was arrested, had been put in the Tower, and had been threatened to be tortured unless she confessed what she knew about Seymour and Elizabeth's relationship. At the time of the deposition, Catherine had died and Seymour had been arrested for another attempt at marrying Lady Elizabeth. It must be mentioned, though, that throughout her time at Chelsea, Ashley developed a crush on Seymour and actually encouraged her charge to "play along." At one point she even made a comment of how lucky Elizabeth would have been to have a husband like Seymour. Ashley even told Lady Elizabeth that Seymour had confided his sentiments to her of wanting to marry Elizabeth before Catherine. After Queen Catherine's death, Ashley strongly encouraged Elizabeth to write to Seymour offering her condolences; to "comfort him of his sorrow...for he would think great kindness therein." So Ashley's testimony involving the Dowager queen is extremely questionable seeing her attachment to Seymour.
Catherine gave birth to her only child — a daughter, Mary Seymour, named after Catherine's stepdaughter Mary – on 30 August 1548, and died only six days later, on 5 September 1548, at Sudeley Castle in Gloucestershire, from what is thought to be puerperal fever or puerperal sepsis, also called childbed fever. Coincidentally, this was also the illness that killed Henry's third wife, Jane Seymour. It was not uncommon, due to the lack of hygiene around childbirth. Nevertheless, a theory exists that Catherine's husband, Sir Thomas Seymour, may have poisoned her to carry out his plan to marry Lady Elizabeth Tudor. 
Lord Thomas Seymour of Sudeley was beheaded for treason on 20 March 1549, and Mary Seymour was taken to live with the Dowager Duchess of Suffolk, a close friend of Catherine's. Catherine's other jewels were kept in a coffer with five drawers at Sudeley and this was sent to the Tower of London on 20 April 1549, and her clothes and papers followed in May. After a year and a half, on 17 March 1550, Mary's property was restored to her by an Act of Parliament, easing the burden of the infant's household on the duchess. The last mention of Mary Seymour on record is on her second birthday, and although stories circulated that she eventually married and had children, most historians believe she died as a child at Grimsthorpe Castle in Lincolnshire.
Remains.
In 1782, John Locust discovered the coffin of Queen Catherine in the ruins of the Sudeley Castle chapel. He opened the coffin and observed that the body, after 234 years, was in a surprisingly good condition. Reportedly the flesh on one of her arms was still white and moist. After taking a few locks of her hair, he closed the coffin and returned it to the grave.
The coffin was opened a few more times in the next ten years and in 1792 some drunken men buried it upside down and in a rough way. When the coffin was officially reopened in 1817, nothing but a skeleton remained. Her remains were then moved to the tomb of Lord Chandos whose family owned the castle at that time. The tomb was carefully restored by order of the late Duchess of Buckingham, Lady Anne Greville, daughter of the 3rd Duke of Chandos. In later years the chapel was rebuilt by Sir George Gilbert Scott, who erected a canopied tomb with a recumbent marble figure by John Birnie Philip.
Iconography.
The full-length portrait of Catherine Parr by Master John in the National Portrait Gallery was for many years thought to represent Lady Jane Grey. The painting has recently been re-identified as Catherine Parr, with whose name it was originally associated. The full-length format was very rare in portraits of this date, and was usually used only for very important sitters. Lady Jane Grey, although of royal blood, was a relatively obscure child of eight when this was painted; it was to be another eight years before her disastrous and short-lived reign. The distinctive crown-shaped jewel the sitter wears can be traced to an inventory of jewels that belonged to Catherine Parr, and the cameo beads appear to have belonged to Catherine Howard, from whom they would have passed to her successor as queen.
In media.
Film, stage, and literature.
Catherine Parr first appeared as a character in cinemas in 1934, in Alexander Korda's film "The Private Life of Henry VIII". Charles Laughton played the king, with actress Everley Gregg appearing as Catherine. The film makes no attempt to depict the historical Parr's character, instead portraying the Queen for comic effect as an over-protective nag.
In 1952, a romanticised version of Thomas Seymour's obsession with Elizabeth I saw Stewart Granger as Seymour, Jean Simmons as the young Elizabeth and screen legend Deborah Kerr as Parr in the popular film "Young Bess".
In 1970, in "Catherine Parr", a 90-minute BBC television drama (the last in a 6-part series, entitled "The Six Wives of Henry VIII") Catherine was played by Rosalie Crutchley opposite Keith Michell's Henry. In this, Catherine's love of religion and intellectual capabilities were highlighted. Crutchley reprised her role as Catherine Parr for the first episode of the 6-part follow-up series on the life of Elizabeth I in 1971, "Elizabeth R".
In 1972, Barbara Leigh-Hunt played a matronly Catherine in "Henry VIII and his Six Wives", with Keith Michell once again playing Henry.
In 2000, Jennifer Wigmore played Catherine Parr in the American television drama aimed at teenagers, "Elizabeth: Red Rose of the House of Tudor". A year later, Caroline Lintott played Catherine in Professor David Starkey's documentary series on Henry's queens.
In October 2003, in a two-part British television series on "Henry VIII", Catherine was played by Clare Holman. The part was relatively small, given that the drama's second part focused more on the stories of Jane Seymour and Catherine Howard.
In "The Simpsons" episode "Margical History Tour," Catherine is portrayed by Agnes Skinner as an elderly widow during Marge's retelling of Henry's reign. Henry (portrayed by Homer) regrets his marriage to her because of her age.
In March 2007, Washington University in St. Louis performed the A.E. Hotchner Playwriting Competition winner "Highness," which documents the life of Catherine Parr and her relationships with King Henry and his daughter, the future Queen Elizabeth I, to whom she was a stepmother.
She was portrayed by actress Joely Richardson on the fourth and final season of Showtime's "The Tudors", which was first broadcast in Spring 2010. Richardson's portrayal was largely faithful to what has been recorded of Parr's character.
Catherine features in "The Dark Rose", Volume 2 of The Morland Dynasty a series of historical novels by author Cynthia Harrod-Eagles. The lead female character, Nanette Morland, is educated alongside Catherine and is later re-acquainted with her when she becomes Queen. She has been the subject of several novels, including two titled "The Sixth Wife," and she is a supporting character in C. J. Sansom's "Matthew Shardlake" mysteries, "Revelation", "Heartstone", and "Lamentation".
Music.
Rick Wakeman recorded the piece "Catherine Parr" for his 1973 album, "The Six Wives of Henry VIII". On his 2009 live version of the album the track's spelling is changed to "Katherine Parr".
Historiography.
The popular myth that Catherine acted more as her husband's nurse than his wife was born in the 19th century from the work of Victorian moralist and proto-feminist, Agnes Strickland. David Starkey challenged this assumption in his book "Six Wives," in which he points out that such a situation would have been vaguely obscene to the Tudors—given that Henry had a huge staff of physicians waiting on him hand and foot, and Catherine was expected to live up to the heavy expectations of Queenly dignity. Parr is usually portrayed in cinema and television by actresses who are much older than the queen, who was in her early 30s when she was Henry's wife and was about 36 years old at the time of her death. This change is usually an artistic licence taken to highlight Parr's maturity in comparison to Henry's previous queens, or at least a symptom of the longer lifespans enjoyed by modern audiences (who might be confused as to why a 30-year old is considered much older and more experienced).
Catherine's good sense, moral rectitude, compassion, firm religious commitment, and strong sense of loyalty and devotion have earned her many admirers among historians. These include David Starkey, feminist activist Karen Lindsey, Lady Antonia Fraser, Alison Weir, Carolly Erickson, Alison Plowden, Susan James, and Linda Porter. Biographers have described her as strong-willed and outspoken, physically desirable, susceptible (like Queen Elizabeth) to roguish charm, and even willing to resort to obscene language if the occasion suited.
Some of Catherine Parr's writings are available from the Women Writers Project.
Historical fiction.
Several novels also feature Catherine Parr:
References.
</dl>

</doc>
<doc id="47407" url="http://en.wikipedia.org/wiki?curid=47407" title="Gabriele D'Annunzio">
Gabriele D'Annunzio

Gabriele D'Annunzio (]; 12 March 1863 – 1 March 1938), Prince of Montenevoso, sometimes spelled d'Annunzio, was an Italian writer, poet, journalist, playwright and soldier during World War I. He occupied a prominent place in Italian literature from 1889 to 1910 and after that political life from 1914 to 1924. He was often referred to under the epithets "Il Vate" ("the Poet") or "Il Profeta" ("the Prophet").
D'Annunzio was associated with the Decadent movement in his literary works, which interplayed closely with French Symbolism and British Aestheticism. Such works represented a turn against the naturalism of the preceding romantics and was both sensuous and mystical. He came under the influence of Friedrich Nietzsche which would find outlets in his literary and later political contributions. His affairs with several women, including Eleonora Duse and Luisa Casati, received public attention.
During the First World War, perception of D'Annunzio in Italy would be transformed from literary figure into a national war hero. He was associated with the elite "Arditi" storm troops of the Italian Army and took part in actions such as the Flight over Vienna. As part of an Italian nationalist reaction against the Paris Peace Conference, he set up the short-lived Italian Regency of Carnaro in Fiume with himself as "Duce". The constitution made "music" the fundamental principle of the state and was corporatist in nature. Some of the ideas and aesthetics influenced Italian fascism and the style of Benito Mussolini.
Childhood.
He was born in Pescara, Abruzzo, the son of a wealthy landowner and mayor of the town Francesco Paolo Rapagnetta d'Annunzio (1831–1893). His father had originally been born plain Rapagnetta (the name of his single mother), but at the age of 13 had been adopted by a childless rich uncle Antonio d'Annunzio. Legend has it that he was initially baptized Gaetano and given the name of Gabriele later in childhood, because of his angelic looks. However there is wide documentation to disprove this story. His precocious talent was recognised early in life, and he was sent to school at the Liceo Cicognini in Prato, Tuscany. He published his first poetry while still at school at the age of sixteen with a small volume of verses called "Primo Vere" (1879), influenced by Giosuè Carducci's "Odi barbare", in which, side by side with some almost brutal imitations of Lorenzo Stecchetti, the fashionable poet of "Postuma", were some translations from the Latin, distinguished by such agile grace that Giuseppe Chiarini on reading them brought the unknown youth before the public in an enthusiastic article. In 1881 D'Annunzio entered the University of Rome La Sapienza, where he became a member of various literary groups, including "Cronaca Bizantina" and wrote articles and criticism for local newspapers. In those university years he started to promote Italian irredentism.
Literary work.
He published "Canto novo" (1882), "Terra vergine" (1882), "L'intermezzo di rime" (1883), "Il libro delle vergini" (1884) and the greater part of the short stories that were afterwards collected under the general title of "San Pantaleone" (1886). "Canto novo" contains poems full of pulsating youth and the promise of power, some descriptive of the sea and some of the Abruzzese landscape, commented on and completed in prose by "Terra vergine", the latter a collection of short stories dealing in radiant language with the peasant life of the author's native province. "Intermezzo di rime" is the beginning of D'Annunzio's second and characteristic manner. His conception of style was new, and he chose to express all the most subtle vibrations of voluptuous life. Both style and contents began to startle his critics; some who had greeted him as an "enfant prodige" rejected him as a perverter of public morals, whilst others hailed him as one bringing a breath of fresh air and an impulse of a new vitality into the somewhat prim, lifeless work hitherto produced.
Meanwhile, the review of Angelo Sommaruga perished in the midst of scandal, and his group of young authors found itself dispersed. Some entered the teaching career and were lost to literature, others threw themselves into journalism.
Gabriele D'Annunzio took this latter course, and joined the staff of the "Tribuna". For this paper, under the pseudonym of "Duca Minimo", he did some of his most brilliant work . To this period of greater maturity and deeper culture belongs "Il libro d'Isotta" (1886), a love poem, in which for the first time he drew inspiration adapted to modern sentiments and passions from the rich colours of the Renaissance.
"Il libro d'Isotta" is interesting also, because in it one can find most of the germs of his future work, just as in "Intermezzo melico" and in certain ballads and sonnets one can find descriptions and emotions which later went to form the aesthetic contents of "Il piacere", "Il trionfo della morte" and "Elegie romane" (1892).
D'Annunzio's first novel "Il piacere" (1889, translated into English as "The Child of Pleasure") was followed in 1891 by "Giovanni Episcopo", and in 1892 by "L'innocente" ("The Intruder"). These three novels made a profound impression. "L'innocente", admirably translated into French by Georges Herelle, brought its author the notice and applause of foreign critics. His next work, "Il trionfo della morte" ("The Triumph of Death") (1894), was followed soon by "Le vergini delle rocce" (1896) and "Il fuoco" (1900); the latter is in its descriptions of Venice perhaps the most ardent glorification of a city existing in any language.
D'Annunzio's poetic work of this period, in most respects his finest, is represented by "Il Poema Paradisiaco" (1893), the "Odi navali" (1893), a superb attempt at civic poetry, and "Laudi" (1900).
A later phase of D'Annunzio's work is his dramatic production, represented by "Il sogno di un mattino di primavera" (1897), a lyrical fantasia in one act; his "Città Morta" (1898), written for Sarah Bernhardt. In 1898 he wrote his "Sogno di un pomeriggio d'autunno" and "La Gioconda"; in the succeeding year "La gloria", an attempt at contemporary political tragedy which met with no success, probably because of the audacity of the personal and political allusions in some of its scenes; and then "Francesca da Rimini" (1901), a perfect reconstruction of medieval atmosphere and emotion, magnificent in style, and declared by an authoritative Italian critic — Edoardo Boutet — to be the first real, if imperfect, tragedy ever given to the Italian theatre.
In 1883, D'Annunzio married Maria Hardouin di Gallese, and had three sons, but the marriage ended in 1891. In 1894, he began a love affair with the famous actress Eleonora Duse which became a cause célèbre. He provided leading roles for her in his plays of the time such as "La città morta" ("The Dead City") (1898) and "Francesca da Rimini" (1901), but the tempestuous relationship finally ended in 1910. After meeting the Marchesa (Luisa) Casati in 1903, he began a lifelong turbulent on again off again affair with Luisa, that lasted until a few years before his death.
In 1897, D'Annunzio was elected to the Chamber of Deputies for a three-year term, where he sat as an independent. By 1910, his daredevil lifestyle had forced him into debt, and he fled to France to escape his creditors. There he collaborated with composer Claude Debussy on a musical play "Le martyre de Saint Sébastien" ("The Martyrdom of St Sebastian"), 1911, written for Ida Rubinstein. The Vatican reacted by placing all of his works in the Index of Forbidden Books. The work was not successful as a play, but it has been recorded in adapted versions several times, notably by Pierre Monteux (in French), Leonard Bernstein (sung in French, acted in English), and Michael Tilson Thomas (in French). In 1912 and 1913, D'Annunzio worked with opera composer Pietro Mascagni on his opera "Parisina", staying sometimes in a house rented by the composer in Bellevue, near Paris.
Flight over Vienna.
After the start of World War I, D'Annunzio returned to Italy and made public speeches in favor of Italy's entry on the side of the Triple Entente. Since taking a flight with Wilbur Wright in 1908, D'Annunzio had been interested in aviation. With the war beginning he volunteered and achieved further celebrity as a fighter pilot, losing the sight of an eye in a flying accident. In February 1918 he took part in a daring, if militarily irrelevant, raid on the harbour of Bakar (known in Italy as "La beffa di Buccari", lit. "the Bakar Mockery"), helping to raise the spirits of the Italian public, still battered by the Caporetto disaster. On 9 August 1918, as commander of the 87th fighter squadron "La Serenissima", he organized one of the great feats of the war, leading 9 planes in a 700-mile round trip to drop propaganda leaflets on Vienna. This is called in Italian "il Volo su Vienna", "the Flight over Vienna".
Fiume.
The War strengthened his ultra-nationalist and irredentist views, and he campaigned widely for Italy to assume a role alongside her wartime allies as a first-rate European power. Angered by the proposed handing over of the city of Fiume (now Rijeka in Croatia) whose population was mostly Italian, at the Paris Peace Conference, on 12 September 1919, he led the seizure by 2,000 Italian nationalist irregulars of the city, forcing the withdrawal of the inter-Allied (American, British and French) occupying forces. The plotters sought to have Italy annex Fiume, but were denied. Instead, Italy initiated a blockade of Fiume while demanding that the plotters surrender. D'Annunzio then declared Fiume an independent state, the Italian Regency of Carnaro; the Charter of Carnaro foreshadowed much of the later Italian Fascist system, with himself as "Duce" (leader). Some elements of the Royal Italian Navy, such as the destroyer "Espero" joined up with D'Annunzio's local forces. He attempted to organize an alternative to the League of Nations for (selected) oppressed nations of the world (such as the Irish, whom D'Annunzio attempted to arm in 1920), and sought to make alliances with various separatist groups throughout the Balkans (especially groups of Italians, though also some Slavic and Albanian groups), although without much success. D'Annunzio ignored the Treaty of Rapallo and declared war on Italy itself, only finally surrendering the city in December 1920 after a bombardment by the Italian navy.
Later life.
After the Fiume episode, D'Annunzio retired to his home on Lake Garda and spent his latter years writing and campaigning. Although D'Annunzio had a strong influence on the ideology of Benito Mussolini, he never became directly involved in fascist government politics in Italy, being temporarily crippled and shocked after an attempted murder in 1922. Shortly before the march on Rome, he was pushed out of a window by an unknown assailant, surviving but badly injured, and did not completely recover before Mussolini had been appointed Prime Minister and hailed by the fascists as "duce del fascismo".
In 1924 he was ennobled by King Victor Emmanuel III and given the hereditary title of Principe di Montenevoso. In 1937 he was made president of the Royal Academy of Italy. D'Annunzio died in 1938 of a stroke, at his home in Gardone Riviera. He was given a state funeral by Mussolini and was interred in a magnificent tomb constructed of white marble at Il Vittoriale degli Italiani.
He was an atheist.
His son Gabriellino D'Annunzio became a film director. His 1921 film "The Ship" was based on a novel by his father. In 1924, he co-directed the historical epic "Quo Vadis", an expensive failure, before retiring from filmmaking.
Politics.
D'Annunzio is often seen as a precursor of the ideals and techniques of Italian fascism. His political ideals emerged in Fiume when he coauthored a constitution with syndicalist Alceste de Ambris, the Charter of Carnaro. De Ambris provided the legal and political framework, to which D'Annunzio added his skills as a poet. De Ambris was the leader of a group of Italian seamen who had mutinied and then given their vessel to the service of D'Annunzio. The constitution established a corporatist state, with nine corporations to represent the different sectors of the economy (workers, employers, professionals), and a tenth (D'Annunzio's invention) to represent the "superior" human beings (heroes, poets, prophets, supermen). The "Carta" also declared that music was the fundamental principle of the state.
It was rather the culture of dictatorship that Benito Mussolini imitated and learned from D'Annunzio. D'Annunzio has been described as the John the Baptist of Italian Fascism, as virtually the entire ritual of Fascism was invented by D'Annunzio during his occupation of Fiume and his leadership of the Italian Regency of Carnaro. These included the balcony address, the Roman salute, the cries of "Eia, eia, eia! Alala!", the dramatic and rhetorical dialogue with the crowd, the use of religious symbols in new secular settings. It also included his method of government in Fiume, the economics of the corporate state; stage tricks; large emotive nationalistic public rituals; blackshirted followers, the Arditi, with their disciplined, bestial responses and strongarm repression of dissent. He was even said to have originated the practice of forcibly dosing opponents with large amounts of castor oil, a very effective laxative, to humiliate, disable or kill them, a practice which became a common tool of Mussolini's blackshirts.
D'Annunzio advocated an expansionist Italian foreign policy and applauded the invasion of Ethiopia.
Rivalry with Mussolini.
As John Whittam notes in his essay "Mussolini and The Cult of the Leader":
This famous poet, novelist and war hero was a self-proclaimed Superman. He was the outstanding interventionist in May 1915 and his dramatic exploits during the war won him national and international acclaim. In September 1919 he gathered together his 'legions' and captured the disputed seaport of Fiume. He held it for over a year and it was he who popularised the black shirts, the balcony speeches, the promulgation of ambitious charters and the entire choreography of street parades and ceremonies. He even planned a march on Rome. One historian had rightly described him as the 'First Duce' and Mussolini must have heaved a sigh of relief when he was driven from Fiume in December 1920 and his followers were dispersed. But he remained a threat to Mussolini and in 1921 Fascists like Balbo seriously considered turning to him for leadership.
In contrast Mussolini vacillated from left to right at this time. Although Mussolini's fascism was heavily influenced by the Carta del Carnaro, the constitution for Fiume written by Alceste De Ambris and D'Annunzio, neither wanted to play an active part in the new movement, both refusing when asked by Fascist supporters to run in the elections of May 15, 1921. Before the March on Rome, De Ambris even went so far as to depict the Fascist movement as: "a filthy pawn in Mister Giolitti's game of chess, and made out of the least dignified section of the bourgeoisie"
D'Annunzio was seriously injured when he fell out of a window on 13 August 1922; subsequently the planned "meeting for national pacification" with Francesco Saverio Nitti and Mussolini was cancelled. The incident was never explained and is considered by some historians an attempt to murder him, motivated by his popularity. Despite D'Annunzio's retreat from active public life after this event, the Duce still found it necessary to regularly dole out funds to D'Annunzio as a bribe for not re-entering the political arena. When asked about this by a close friend, Mussolini purportedly stated: "When you have a rotten tooth you have two possibilities open to you: either you pull the tooth or you fill it with gold. With D'Annunzio I have chosen for the latter treatment."
Nonetheless, D'Annunzio kept attempting to intervene in politics almost until his death in 1938. He wrote to Mussolini in 1933 to try to convince him not to take part in the Axis pact with Hitler. In 1934, he tried to disrupt the relationship between Hitler and Mussolini after their meeting, even writing a satirical pamphlet about Hitler. Again, in September 1937, D'Annunzio met with the Duce at the Verona train station to convince him to leave the Axis alliance. Mussolini in 1944 admitted to have made a mistake not following his advice.
Literature.
At the height of his success, D'Annunzio was celebrated for the originality, power and decadence of his writing. Although his work had immense impact across Europe, and influenced generations of Italian writers, his "fin de siècle" works are now little known, and his literary reputation has always been clouded by his fascist associations. Indeed, even before his fascist period, he had his strong detractors. A "New York Times" review in 1898 of his novel "The Intruder" referred to him as "evil", "entirely selfish and corrupt". Three weeks into its December 1901 run at the Teatro Constanzi in Rome, his tragedy "Francesca da Rimini" was banned by the censor on grounds of morality.
A prolific writer, his novels in Italian include "Il piacere" ("The Child of Pleasure", 1889), "Il trionfo della morte" ("The Triumph of Death", 1894), and "Le vergini delle rocce" ("The Virgins of the Rocks", 1896). He wrote the screenplay to the feature film "Cabiria" (1914) based on episodes from the Second Punic War. D'Annunzio's literary creations were strongly influenced by the French Symbolist school, and contain episodes of striking violence and depictions of abnormal mental states interspersed with gorgeously imagined scenes. One of D'Annunzio's most significant novels, scandalous in its day, is "Il fuoco" ("The Flame of Life") of 1900, in which he portrays himself as the Nietzschean "Superman" Stelio Effrena, in a fictionalized account of his love affair with Eleonora Duse. His short stories showed the influence of Guy de Maupassant. He was also associated with the bizarre Italian noblewoman Luisa Casati, an influence on his novels and one of his mistresses.
The 1911 "Encyclopædia Britannica" wrote of him:
The work of d' Annunzio, although by many of the younger generation injudiciously and extravagantly admired, is almost the most important literary work given to Italy since the days when the great classics welded her varying dialects into a fixed language. The psychological inspiration of his novels has come to him from many sources—French, Russian, Scandinavian, German—and in much of his earlier work there is little fundamental originality.
His creative power is intense and searching, but narrow and personal; his heroes and heroines are little more than one same type monotonously facing a different problem at a different phase of life. But the faultlessness of his style and the wealth of his language have been approached by none of his contemporaries, whom his genius has somewhat paralysed. In his later work [meaning as of 1911], when he begins drawing his inspiration from the traditions of bygone Italy in her glorious centuries, a current of real life seems to run through the veins of his personages. And the lasting merit of D'Annunzio, his real value to the literature of his country, consists precisely in that he opened up the closed mine of its former life as a source of inspiration for the present and of hope for the future, and created a language, neither pompous nor vulgar, drawn from every source and district suited to the requirements of modern thought, yet absolutely classical, borrowed from none, and, independently of the thought it may be used to express, a thing of intrinsic beauty. As his sight became clearer and his purpose strengthened, as exaggerations, affectations, and moods dropped away from his conceptions, his work became more and more typical Latin work, upheld by the ideal of an Italian Renaissance.
In Italy some of his poetic works remain popular, most notably his poem "La pioggia nel pineto" ("The Rain in the Pinewood"), which exemplifies his linguistic virtuosity as well as the sensuousness of his poetry.
Museums.
D'Annunzio's life and work are commemorated in a museum, "Il Vittoriale degli Italiani". He planned and developed it himself, adjacent to his villa at Gardone Riviera on the southwest bank of Lake Garda, between 1923 and his death. Now a national monument, it is a complex of military museum, library, literary and historical archive, theatre, war memorial and mausoleum. The museum preserves his torpedo boat "MAS 96" and the SVA-5 aircraft he flew over Vienna.
His birthplace is also open to the public as a museum, the "Casa Natale di Gabriele D'Annunzio" in Pescara
Works.
Autobiographical works.
His epistolatory work, "Solus ad solam", was published posthumously.

</doc>
<doc id="47409" url="http://en.wikipedia.org/wiki?curid=47409" title="530s BC">
530s BC


</doc>
<doc id="47410" url="http://en.wikipedia.org/wiki?curid=47410" title="540s BC">
540s BC


</doc>
<doc id="47411" url="http://en.wikipedia.org/wiki?curid=47411" title="Margaret of Anjou">
Margaret of Anjou

Margaret of Anjou (French: "Marguerite"; 23 March 1430 – 25 August 1482) was the wife of King Henry VI of England. As such, she was Queen of England from 1445 to 1461 and again from 1470 to 1471. Born in the Duchy of Lorraine, into the House of Valois-Anjou, Margaret was the second eldest daughter of René I of Naples and Isabella, Duchess of Lorraine.
She was one of the principal figures in the series of dynastic civil wars known as the Wars of the Roses and at times personally led the Lancastrian faction. Due to her husband's frequent bouts of insanity, Margaret ruled the kingdom in his place. It was she who called for a Great Council in May 1455 that excluded the Yorkist faction headed by Richard, Duke of York, and thus provided the spark that ignited a civil conflict that lasted for over thirty years, decimated the old nobility of England, and caused the deaths of thousands of men, including her only son Edward of Westminster, Prince of Wales, at the Battle of Tewkesbury in 1471.
Margaret was taken prisoner by the victorious Yorkists after the Lancastrian defeat at Tewkesbury. In 1475, she was ransomed by her cousin, King Louis XI of France. She went to live in France as a poor relation of the French king, and she died there at the age of 52.
Early life and marriage.
Margaret was born on 23 March 1430 at Pont-à-Mousson in the Duchy of Lorraine, an imperial fief east of France ruled by a cadet branch of the French kings, the House of Valois-Anjou. Margaret was the second eldest daughter of René of Anjou and of Isabella, Duchess of Lorraine. She had five brothers and four sisters, as well as three half-siblings from her father's relationships with mistresses. Her father, popularly known as "Good King René" was Duke of Anjou and titular King of Naples, Sicily and Jerusalem; he has been described as "a man of many crowns but no kingdoms". Margaret was baptised at Toul in Lorraine and, in the care of her father's old nurse Theophanie la Magine, she spent her early years at the castle of Tarascon on the River Rhône in southern France and in the old royal palace at Capua, near Naples in the Kingdom of Sicily. Her mother took care of her education and may have arranged for her to have lessons with the scholar Antoine de la Sale, who taught her brothers. In childhood Margaret was known as "la petite creature".
On 23 April 1445, Margaret married King Henry VI of England, who was eight years her senior, at Titchfield in Hampshire. Henry at the time also claimed the Kingdom of France and controlled various parts of northern France. Henry's uncle King Charles VII of France, who also claimed the crown of France, agreed to the marriage of Margaret to his rival on the condition that he would not have to provide the customary dowry and instead would receive the lands of Maine and Anjou from the English. The English government, fearing a highly negative reaction, kept this provision secret from the English public.
Margaret was crowned Queen consort of England on 30 May 1445 at Westminster Abbey by John Stafford, Archbishop of Canterbury at the age of fifteen. She was described as beautiful, and furthermore "already a woman: passionate and proud and strong-willed". Those that anticipated the future return of English claims to French territory believed that she already understood her duty to protect the interests of the Crown fervently. She seems to have inherited this indomitability from her mother, who fought to establish her husband's claim to the Kingdom of Naples, and from her paternal grandmother Yolande of Aragon, who actually governed Anjou "with a man's hand", putting the province in order and keeping out the English. Thus by family example and her own forceful personality, she was fully capable of becoming the "champion of the Crown".
Birth of a son.
Henry, who was more interested in religion and learning than in military matters, was not a successful king. He had reigned since he was only a few months old and his actions had been controlled by regents. When he married Margaret, his mental condition was already unstable and by the birth of their only son, Edward of Westminster (born 13 October 1453) he had suffered a complete breakdown. Rumours were rife that he was incapable of begetting a child and that the new Prince of Wales was the result of an adulterous liaison. Many have speculated that either Edmund Beaufort, 2nd Duke of Somerset, or James Butler, Earl of Wiltshire, both staunch allies of Margaret, was the young prince's actual father.
Although Margaret was aggressively partisan and had a volatile temperament, she shared her husband's love of learning by dint of her cultured upbringing and gave her patronage to the founding of Queens' College at Cambridge University.
Elizabeth Woodville (born ca 1437), later Queen of England as future wife of her husband's rival King Edward IV, allegedly served Margaret of Anjou as a Maid of Honour. However, the evidence is too scanty to permit historians to establish this with absolute certainty: several women at Margaret's court bore the name Elizabeth or Isabella Grey. Elizabeth Woodville married her first husband, Sir John Grey of Groby, in about 1452.
Beginnings of the dynastic civil wars.
Enmity between Margaret and the Duke of York.
After retiring from London to live in lavish state at Greenwich, Margaret was occupied with the care of her young son and did not display any signs of overt belligerence until she believed her husband was threatened with deposition by the ambitious Richard Plantagenet, 3rd Duke of York, who, to her consternation, had been appointed regent while Henry was mentally incapacitated from 1453 to 1454. The duke was a credible claimant to the English throne and by the end of his regency there were many powerful nobles and relatives prepared to back his claim. The Duke of York was powerful; Henry's advisers corrupt; Henry himself trusting, pliable, and increasingly unstable; Margaret defiantly unpopular, grimly and gallantly determined to maintain the English crown for her progeny. Yet at least one scholar identifies the source of the eventual Lancastrian downfall not as York's ambitions nearly so much as Margaret's ill-judged enmity toward York and her over-indulgence in unpopular allies. Nevertheless, Queen Margaret was a powerful force in the world of politics. King Henry was putty in her hands when she wanted something done.
Margaret's biographer Helen Maurer, however, disagrees with earlier historians having dated the much-vaunted enmity between the Queen and York to the time he obtained the office of the regency. She suggests the mutual antagonism came about two years later in 1455 in the wake of the First Battle of St. Albans, when Margaret perceived him as a challenge to the king's authority. Maurer bases this conclusion on a judicious study of Margaret's pattern of presenting gifts; this revealed that Margaret took a great deal of care to demonstrate that she favoured both York and Somerset equally in the early 1450s. Maurer also claims that Margaret appeared to accept York's regency and asserts there is no substantial evidence to back up the long-standing belief that she was responsible for the Yorkists' exclusion from the Great Council following Henry's recovery (see below).
The late historian Paul Murray Kendall, on the other hand, maintained that Margaret's allies Somerset and William de la Pole, then Earl of Suffolk, had no difficulty in persuading her that York, until then one of Henry VI's most trusted advisers, was responsible for her unpopularity and already too powerful to be trusted. Margaret not only convinced Henry to recall York from his post as governor in France and banish him instead to Ireland, she repeatedly attempted to have him assassinated during his travels to and from Ireland, once in 1449 and again in 1450. Somerset and Suffolk's joint responsibility for the secret surrender of Maine in 1448, and then the subsequent disastrous loss of the rest of Normandy in 1449 embroiled Margaret and Henry's court in riots, uprisings by the magnates, and calls for the impeachment and execution of Margaret's two strongest allies. It also might have made an ultimate battle to the death between Margaret and the House of York inevitable by making manifest Richard's dangerous popularity with the Commons. Richard of York, safely returned from Ireland in 1450, confronted Henry and was readmitted as a trusted advisor. Soon thereafter, Henry agreed to convene Parliament to address the calls for reform. When Parliament met, the demands could not have been less acceptable to Margaret: not only were both Somerset and Suffolk impeached for criminal mismanagement of French affairs and subverting justice, but it was charged as a crime against Suffolk (now a duke) that he had antagonised the king against the Duke of York. Further, the demands for reform put forward included that the Duke of York be acknowledged as the first councillor to the king, and the Speaker of Commons, perhaps with more fervour than wisdom, even proposed Richard, Duke of York, be recognised as heir to the throne. Within a few months, however, Margaret had regained control of Henry, Parliament was dissolved, the incautious Speaker thrown in prison, and Richard of York retired to Wales for the time being.
In 1457, the kingdom was again outraged when it was discovered that Pierre de Brézé, a powerful French general and an adherent of Margaret, had landed on the English coast and burnt the town of Sandwich. As leader of a French force of 4,000 men from Honfleur, he aimed at taking advantage of the chaos in England. The mayor, John Drury, was killed in this raid. It thereafter became an established tradition, which survives to this day, that the Mayor of Sandwich wears a black robe mourning this ignoble deed. Margaret, in association with de Brézé, became the object of scurrilous rumours and vulgar ballads. Public indignation was so high that Margaret, with great reluctance, was forced to give the Duke of York's kinsman Richard Neville, 16th Earl of Warwick, a commission to keep the sea for three years. He already held the post of Captain of Calais.
Leader of Lancastrian faction.
Hostilities between the rival Yorkist and Lancastrian factions soon flared into armed conflict. In May 1455, just over five months after Henry VI recovered from a bout of mental illness and Richard of York's regency had ended, Margaret called for a Great Council from which the Yorkists were excluded. The Council called for an assemblage of the peers at Leicester to protect the king "against his enemies". York apparently was prepared for conflict and soon was marching south to meet the Lancastrian army marching north. The Lancastrians suffered a crushing defeat at the First Battle of St Albans on 22 May 1455. Somerset was killed, Wiltshire fled the battlefield and King Henry was taken prisoner by the victorious Duke of York.
In 1459, hostilities resumed at the Battle of Blore Heath, where James Touchet, Lord Audley, was defeated by a Yorkist army under Richard Neville, 5th Earl of Salisbury.
The Wars of the Roses.
Military campaigns.
While she was attempting to raise further support for the Lancastrian cause in Scotland, her principal commander, Henry Beaufort, 3rd Duke of Somerset, gained a major victory for her at the Battle of Wakefield on 30 December 1460 by defeating the combined armies of the Duke of York and the Earl of Salisbury. Both men were beheaded and their heads displayed on the gates of the city of York. As Margaret was in Scotland at the time the battle had taken place, it was impossible that she issued the orders for their executions despite popular belief to the contrary. She followed up with a victory at the Second Battle of St Albans (at which she was present) on 17 February 1461. In this battle, she defeated the Yorkist forces of Richard Neville, 16th Earl of Warwick, and recaptured her husband. It was after this battle that she, in a blatant act of vengeance, ordered the execution of two Yorkist prisoners-of-war, William Bonville, 1st Baron Bonville, and Sir Thomas Kyriel, who had kept watch over King Henry to keep him out of harm's way during the battle. The king had promised the two knights immunity, but Margaret gainsaid him and ordered their executions by decapitation. It is alleged that she put the men on trial at which presided her son. "Fair son", she allegedly asked, "what death shall these knights die?" Prince Edward replied that their heads should be cut off, despite the king's pleas for mercy.
The Lancastrian army was beaten at the Battle of Towton on 29 March 1461 by the son of the late Duke of York, Edward IV of England, who deposed King Henry and proclaimed himself king. Margaret was determined to win back her son's inheritance and fled with him into Wales and later Scotland. Finding her way to France, she made an ally of her cousin, King Louis XI of France, and at his instigation she allowed an approach from Edward's former supporter, Richard Neville, Earl of Warwick, who had fallen out with his former friend as a result of Edward's marriage to Elizabeth Woodville, and was now seeking revenge for the loss of his political influence. Warwick's daughter, Anne Neville, was married to Margaret's son Edward, Prince of Wales, in order to cement the alliance, and Margaret insisted that Warwick return to England to prove himself before she followed. He did so, restoring Henry VI briefly to the throne on 3 October 1470.
Defeat at Tewkesbury.
By the time Margaret, her son and daughter-in-law were ready to follow Warwick back to England, the tables had again turned in favour of the Yorkists, and the Earl was defeated and killed by the returning King Edward IV in the Battle of Barnet on 14 April 1471. Margaret was forced to lead her own army at the Battle of Tewkesbury on 4 May 1471, at which the Lancastrian forces were defeated and her seventeen-year-old son was killed. The circumstances of Edward's death have never been made clear; it is not known whether he was killed in the actual fighting or executed after the battle by the Duke of Clarence. If he died in battle, he would have been the only Prince of Wales ever to do so. Over the previous ten years, Margaret had gained a reputation for aggression and ruthlessness, but following her defeat at Tewkesbury and the death of her only son, she was completely broken in spirit. After she was taken captive by William Stanley at the end of the battle, Margaret was imprisoned by the order of King Edward. She was sent first to Wallingford Castle and then was transferred to the more secure Tower of London. Henry VI was also imprisoned in the Tower in the wake of Tewkesbury and he died there on the night of 21 May; the cause of his death was unknown. In 1472 she was placed in the custody of her former lady-in-waiting Alice Chaucer, Duchess of Suffolk, where she remained until ransomed by Louis XI in 1475.
Death.
Margaret lived in France for the next seven years as a poor relation of the king. She died in Anjou on 25 August 1482 at the age of 52. She was entombed next to her parents in Angers Cathedral, but her remains were removed and scattered by revolutionaries who ransacked the cathedral during the French Revolution.
Margaret's letters.
There are many letters extant written by Margaret during her tenure as queen consort. One was written to the Corporation of London regarding injuries done to her tenants at the manor of Enfield, which comprised part of her dower lands. There is another letter which she wrote to the Archbishop of Canterbury. The letters are compiled in a book edited by Cecil Monro, which was published for the Camden Society in 1863. Margaret typically headed her letters with the words "By the Quene".
Depictions in fiction.
Margaret is a major character in William Shakespeare's 1st Tetrology of History plays. "Henry VI, Part 1", "Part 2", "Part 3" and "Richard III". Shakespeare portrays Margaret as an intelligent, ruthless woman who easily dominates her husband and fiercely vies for power with her enemies. In "Henry VI, Part 2" Margaret has an affair with the Duke of Suffolk and mourns his death by carrying around his severed head. In "Henry VI, Part 3" she personally stabs the Duke of York on the battlefield (after humiliatingly taunting him) and becomes suicidal when her son Edward is killed in front of her. Despite the fact that Margaret spent the rest of her life outside of England after the death of her husband and son, Shakespeare has her return to the court in "Richard III". Margaret serves as a Cassandra-like prophetess; in her first appearance she dramatically curses the majority of the nobles for their roles in the downfall of the House of Lancaster. All of her curses come to pass as the noblemen are betrayed and executed by Richard of Gloucester, and each character reflects on her curse before his execution.
Margaret is the title character of Giacomo Meyerbeer's 1820 opera "Margherita d'Anjou" and has an important role in Bulwer-Lytton's "The Last of the Barons" (1843). She is also the subject of Betty King's 1974 biographical novel "Margaret of Anjou", Alan Savage's 1994 novel "Queen of Lions", Anne Powers' historical romance "The Royal Consorts", and Susan Higginbotham's 2011 novel "The Queen of Last Hopes". Sharon Kay Penman's novel "The Sunne in Splendour" features her as an important character in the early parts of the book, up until the Battle of Tewkesbury. Jean Plaidy's "The Red Rose of Anjou" also features her.
She also is the subject of a fictional biography, "The Royal Tigress" by a fictional character, David Powlett-Jones who is the main subject of "To Serve Them All My Days", R.F. Delderfield's novel of a Welsh schoolmaster at a Devon public school from World War I to the Battle of Britain in the 1940s. Delderfield, in the person of Powlett-Jones, appears to have a very good grasp of Margaret's life and the Wars of the Roses, and the content and development of the book give us an entertaining sub-plot to the book's main narrative.
Margaret is also a major character in Sharon Penman's "The Sunne in Splendour", and "The Lady of the Rivers" by Philippa Gregory, narrated by Jacquetta of Luxembourg. In the television series "The White Queen" (2013), based on Gregory's "The Cousins' War" novels, Margaret of Anjou is portrayed by Veerle Baetens.
It has been suggested that Cersei Lannister, a major character in George R.R. Martin's "A Song of Ice and Fire" series, is modeled on Margaret.
Margaret of Anjou is one of the main characters in the two books released in Conn Igguldens trilogy on the wars of the roses.

</doc>
<doc id="47413" url="http://en.wikipedia.org/wiki?curid=47413" title="Amp">
Amp

Amp or AMP may refer to:

</doc>
<doc id="47418" url="http://en.wikipedia.org/wiki?curid=47418" title="Group psychotherapy">
Group psychotherapy

Group psychotherapy or group therapy is a form of psychotherapy in which one or more therapists treat a small group of clients together as a group. The term can legitimately refer to any form of psychotherapy when delivered in a group format, including Cognitive behavioural therapy or Interpersonal therapy, but it is usually applied to psychodynamic group therapy where the group context and group process is explicitly utilised as a mechanism of change by developing, exploring and examining interpersonal relationships within the group.
The broader concept of "group therapy" can be taken to include any helping process that takes place in a group, including support groups, skills training groups (such as anger management, mindfulness, relaxation training or social skills training), and psycho-education groups. The differences between psychodynamic groups, activity groups, support groups, problem-solving and psycoeducational groups are discussed by Montgomery (2002). Other, more specialised forms of group therapy would include non-verbal expressive therapies such as art therapy, dance therapy, or music therapy.
History.
The founders of group psychotherapy in the USA were Joseph H. Pratt, Trigant Burrow and Paul Schilder. All three of them were active and working at the East Coast in the first half of the 20th century. After World War II, group psychotherapy was further developed by Jacob L. Moreno, Samuel Slavson, Hyman Spotnitz, Irvin Yalom, and Lou Ormont. Yalom's approach to group therapy has been very influential not only in the USA but across the world. An early development in group therapy the T-group or training group (sometimes also referred to as sensitivity-training group, human relations training group or encounter group) is a form of group psychotherapy where participants themselves (typically, between eight and 15 people) learn about themselves (and about small group processes in general) through their interaction with each other. They use feedback, problem solving, and role play to gain insights into themselves, others, and groups.
It was pioneered in the mid-1940s by Kurt Lewin and Carl Rogers and his colleagues as a method of learning about human behavior in what became The National Training Laboratories (now NTL Institute) that was created by the Office of Naval Research and the National Education Association in Bethel, Maine, in 1947. Moreno developed a specific and highly structured form of group therapy known as Psychodrama (although the entry on psychodrama claims it is not a form of group therapy). Another recent development in the theory and method of group psychotherapy based on an integration of systems thinking is Yvonne Agazarian's "systems-Centered" approach (SCT), which sees groups functioning within the principles of system dynamics. Her method of "functional subgrouping" introduces a method of organizing group communication so it is less likely to react counterproductively to differences. SCT also emphasizes the need to recognize the phases of group development and the defenses related to each phase in order to best make sense and influence group dynamics.
In the United Kingdom group psychotherapy initially developed independently, with pioneers S. H. Foulkes and Wilfred Bion using group therapy as an approach to treating combat fatigue in the Second World War. Foulkes and Bion were psychoanalysts and incorporated psychoanalysis into group therapy by recognising that transference can arise not only between group members and the therapist but also among group members. Furthermore the psychoanalytic concept of the unconscious was extended with a recognition of a group unconscious, in which the unconscious processes of group members could be acted out in the form of irrational processes in group sessions. Foulkes developed the model known as Group Analysis and the Institute of Group Analysis, while Bion was influential in the development of group therapy at the Tavistock Clinic.
Bion's approach is comparable to Social Therapy, first developed in the United States in the late 1970s by Lois Holzman and Fred Newman, which is a group therapy in which practitioners relate to the group, not its individuals, as the fundamental unit of development. The task of the group is to "build the group" rather than focus on problem solving or "fixing" individuals.
In Argentina an independent school of group analysis stemmed from the work and teachings of Swiss-born Argentine psychoanalyst Enrique Pichon-Riviere. This thinker conceived of a group-centered approach which, although not directly influenced by Foulkes' work, was fully compatible with it.
Therapeutic principles.
Yalom's therapeutic factors (originally termed "curative factors" but renamed "therapeutic" factors in the 5th edition of 'The Theory and Practice of Group Psychotherapy').
Settings.
Group therapy can form part of the therapeutic milieu of a psychiatric in-patient unit or ambulatory psychiatric Partial hospitalization (also known as Day Hospital treatment).
In addition to classical "talking" therapy, group therapy in an institutional setting can also include group-based expressive therapies such as drama therapy, psychodrama, art therapy, and non-verbal types of therapy such as music therapy and dance/movement therapy. Group psychotherapy is a key component of Milieu Therapy in a Therapeutic Community. The total environment or milieu is regarded as the medium of therapy, all interactions and activities regarded as potentially therapeutic and are subject to exploration and interpretation, and are explored in daily or weekly community meetings. However, interactions between the culture of group psychotherapeutic settings and the more managerial norms of external authorities may create 'organizational turbulence' which can critically undermine a group's ability to maintain a safe yet challenging 'formative space'. Academics at the University of Oxford studied the inter-organizational dynamics of a national democratic therapeutic community over a period of four years; they found external steering by authorities eroded the community's therapeutic model, produced a crisis, and led to an intractable conflict which resulted in the community's closure.
A form of group therapy has been reported to be effective in psychotic adolescents and recovering addicts. Projective psychotherapy uses an outside text such as a novel or motion picture to provide a "stable delusion" for the former cohort and a safe focus for repressed and suppressed emotions or thoughts in the latter. Patient groups read a novel or collectively view a film. They then participate collectively in the discussion of plot, character motivation and author motivation. In the case of films, sound track, cinematography and background are also discussed and processed. Under the guidance of the therapist, defense mechanisms are bypassed by the use of signifiers and semiotic processes. The focus remains on the text rather than on personal issues. It was popularized in the science fiction novel, Red Orc's Rage.
Group therapy is now often utilized in private practice settings (Gardenswartz, 2009, Los Angeles, CA).
Group-analysis has become widespread in Europe, and especially the United Kingdom, where it has become the most common form of group psychotherapy. Interest from Australia, the former Soviet Union and the African continent is also growing.
Research on effectiveness.
There is clear evidence for the effectiveness of group psychotherapy for depression: a meta-analysis of 48 studies showed an overall effect size of 1.03, which is clinically highly significant. Similarly, a meta-analysis of five studies of group psychotherapy for adult sexual abuse survivors showed moderate to strong effect sizes, and there is also good evidence for effectiveness with chronic traumatic stress in war veterans. There is less robust evidence of good outcomes for patients with borderline personality disorder, with some studies showing only small to moderate effect sizes. The authors comment that these poor outcomes might reflect a need for additional support for some patients, in addition to the group therapy. This is borne out by the impressive results obtained using Mentalization based treatment, a model that combines dynamic group psychotherapy with individual psychotherapy and case management. Most outcome research is carried out using time-limited therapy with diagnostically homogenous groups. However, long-term intensive interactional group psychotherapy assumes diverse and diagnostically heterogeneous group membership, and an open-ended time scale for therapy. Good outcomes have also been demonstrated for this form of group therapy.
Group Therapy has been shown to be as or more effective than individual therapy for higher functioning adults (Gardenswartz, 2009, Los Angeles, CA). Clinical cases has shown that the combination of both individual and group therapy is most beneficial for such clients. (the "multiplicative" effect).

</doc>
<doc id="47419" url="http://en.wikipedia.org/wiki?curid=47419" title="Convenience store">
Convenience store

A convenience store, c-store, small grocery store, or corner shop, is a small store that stocks a range of everyday items such as groceries, snack foods, candy, toiletries, soft drinks, tobacco products, and newspapers. Such stores may also offer money order and wire transfer services. In some jurisdictions, corner stores are licensed to sell alcohol, typically beer and wine. They differ from general stores and village shops in that they are not in a rural location and are used as a convenient supplement to larger stores.
A convenience store may be part of a gas/petrol station. It may be located alongside a busy road, in an urban area, or near a railway or railroad station or other transport hub. In some countries, convenience stores have long shopping hours, some being open 24 hours.
Convenience stores usually charge significantly higher prices than conventional grocery stores or supermarkets, as convenience stores order smaller quantities of inventory at higher per-unit prices from wholesalers. However convenience stores make up for this with the convenience by having longer opening hours, serving more locations, and having shorter cashier lines.
Merchandise.
Various types exist, for example: liquor stores (off-licences—offies), mini-markets (mini-marts), general stores or party stores. Typically junk food (sweets, ice-cream, soft drinks), lottery tickets, newspapers and magazines are sold although merchandise varies widely from store to store. Unless the outlet is a liquor store, the range of alcohol beverages is likely to be limited (i.e. beer and wine) or non-existent. Most stores carry cigarettes and other tobacco products. Varying degrees of food and grocery supplies are usually available, from household products, to prepackaged foods like sandwiches and frozen burritos. Automobile-related items such as motor oil, maps and car kits may be sold. Often toiletries and other hygiene products are stocked, as well as feminine hygiene and contraception. Stores may carry apparel, home furnishings, and CDs and DVDs. Some of these stores also offer money orders and wire transfer services. Convenience stores that are near fishing destinations may carry live fishing bait as well as fishing equipment and supplies. Convenience stores may also carry small appliances as well as other household items such as coolers and back packs. Convenience stores have also been known to carry candles, stationery, artwork and dishes.
Many convenience stores offer food ready to eat, such as breakfast sandwiches and other breakfast food. Throughout Europe convenience stores now sell fresh French bread (or similar). A process of freezing parbaked bread allows easy shipment (often from France) and baking in-store. Some stores have a delicatessen counter, offering custom-made sandwiches and baguettes. Others have racks offering fresh delivered or baked doughnuts from local doughnut shops. Some stores have a self-service microwave oven for heating purchased food.
In the United States, some fast food chains offer a counter in convenience stores. Instead of cooking food in the store, these counters offer a limited menu of items delivered several times a day from a local branch of the restaurant. Convenience stores may be combined with other services, such as general stores and pawn shops, a train station ticket counter, post office counter or gasoline pumps. In Asian countries, like Japan or Taiwan, convenience stores are more common because of the higher population density. They are found with gasoline and train stations, but also can be stand-alone stores. Here, items like soft drinks or snacks are sold. Hot dogs, sausages, hard boiled tea eggs, and fish cakes can be found in stores. Delicatessens are absent, instead pre-made sandwiches can be bought. Non-food products like magazines are also sold, but at a lesser extent. Many convenience stores also have a fountain that offers a variety of beverages such as coffee, soft drinks and frozen beverages.
The smaller convenience stores typically have very few perishable items because it is not economically viable to rotate perishable items frequently with such a low number of staff. Smaller convenience stores also don't generate the business needed to sustain food spoilage rates typical of grocery stores or supermarkets. As such, products with a long shelf life are the rule unless a product is specifically aimed at attracting customers on the chance they may buy something profitable too.
Differences from supermarkets.
Although larger, newer convenience stores may have quite a broad range of items, the selection is still limited compared to supermarkets, and in many stores only one or two choices are available. Prices in a convenience store are often higher than those at a supermarket, mass merchandise store, or auto supply store, as convenience stores order smaller quantities of inventory at higher per-unit prices from wholesalers. However, there are some exceptions like milk and fuel which are priced similar to larger stores, as convenience stores traditionally do high volume in these goods and sometimes use them as loss leaders.
The average U.S. convenience store has a sales area of 2768 sqft. New stores average about 2800 sqft of sales area and about 1900 sqft of non-sales area—a nod to retailers recognizing the importance of creating destinations within the store that require additional space—whether coffee islands, food service areas with seating or financial services kiosks. Convenience stores also have expanded their offerings over the last few years, with stores become part supermarket, restaurant, gas station and even a bank or drug store.
Convenience stores sell approximately 80 percent of the fuels purchased in the United States. In the US, the stores are sometimes the only stores and services near an interstate highway exit where drivers can buy any kind of food or drink for miles. Most of the profit margin from these stores comes from beer, liquor, and cigarettes. Although those three categories themselves usually yield lower margins per item, the sales volume in these categories generally makes up for it. Profits per item are much higher on deli items (bags of ice, chicken, etc.), but sales are generally lower. In some countries, most convenience stores have longer shopping hours, some being open 24 hours.
By country.
Canada.
Alimentation Couche-Tard Inc., which operates Couche-Tard, Provi-Soir, Dépanneur 7, Mac's Convenience Stores and Beckers Milk, is the largest convenience store chain in Canada. Another large chain is Quickie Mart (whose name predates the fictitious "Kwik-E-Mart" featured on "The Simpsons"). The world's largest convenience retailer, 7-Eleven, has about 500 Canadian locations from British Columbia to Ontario. Worldwide, the highest number of the chain's Slurpee beverages are sold in Winnipeg, Manitoba, and the city has been given the title of the "Slurpee Capital of the World". Marketing itself as "more than just a convenience store", there are over 150 Hasty Market locations throughout Ontario, and Hasty Markets also exist in British Columbia.
Shoppers Drug Mart was originally a chain of pharmacies, but in recent years the retailer has decreased its reliance on pharmaceutical sales and increase sales of what it calls "front of store" items, such as food and cosmetics (by 2012, 51% of purchases came from non-pharmaceutical items). Most of its expanded merchandish offerings compete directly with convenience stores. Also, Shoppers has over 1000 stores including locations normally served by convenience stores; while convenience stores tend to be found in smaller and older strip malls, Shoppers also has a presence in larger and newer power centers alongside other big box retailers. Consequently, Shoppers has captured a significant share of the market in front store convenience; including over-the-counter medications, seasonal products and everyday household essentials. As a result of the acquisition of Shoppers by supermarket operator Loblaw Companies, Shoppers has access to Loblaw's supply chain which should ensure even lower prices relative to traditional convenience stores.
In addition to chain convenience stores, there are also many independently owned convenience stores in Canada.
Convenience stores are also commonly referred to as "corner stores", "mini-marts", or "variety stores" in some regions of Canada. In the French-speaking province of Quebec, a convenience store is known as a "dépanneur", or "dep" for short. "Dépanneur" means literally "one who gets you out of a jam".
Costa Rica.
In Costa Rica, family-owned and operated convenience stores called "pulperías" have been common since the 1900s and there are many of those stores in every neighbourhood.
In the 2010s, modern convenience stores were introduced, mainly by the AMPM company. Competitors launched brands such as Musmanni Mini Super (a chain of bakery stores promoted to convenience stores), Vindi (operated by AutoMercado supermarket company), and Fresh Market (operated by AMPM in a format appealing to prosperous neighborhoods).
France.
In France, some convenience stores are referred to as "Arabe du coin" - "Arab of the corner". These stores, not only owned by people coming from North Africa, stay open later than the other "épiceries". This name could be considered as pejorative.
Indonesia.
Convenience stores or in Indonesian "Mini market" are mostly scattered around the towns. Due to local government constriction and rules, Convenience stores in Indonesia may only be built usually 500 meters from nearest traditional market. In small city or rural areas, this means a smaller area is available as profitable. As a result, convenience stores in rural areas are often built side by side or at maximum within 50 meters of each other.
Local convenience store brands are Indomaret and Alfamart. Both targeting all public, where imported brands like 7-Eleven, Circle K or Lawson are targeting big cities and cater to a lifestyle more than "convenience". To be classified as a convenience store, the store should occupy no more than 100 meters square of service area, on in some local residence, up to 250 m.
Indonesian government restrict convenience store license, so it can only be bought by franchisee, using different name and different brand, or classifying it as cafeteria. A convenience store with a cafeteria license is only allowed to sell maximum 10% of it service space for non food/beverages product. This type of convenience store often puts some lawn chairs and desk as a decoy in front of their stores, while offering the same range of products as a holder of a mini market license.
Japan.
Convenience stores (コンビニエンスストア, konbiniensu sutoa), often shortened to konbini (コンビニ), developed tremendously in Japan. 7-Eleven Japan, while struggling to localize their service in the 1970s to 1980s, evolved its point of sale-based business, until ultimately, Seven & I Holdings Co., the parent company of 7-Eleven Japan, acquired 7-Eleven (US) from Southland Corporation in 1991. Japanese-style convenience stores also heavily influenced those stores in other Asian nations, such as Taiwan, Thailand, South Korea, and China.
Convenience stores rely heavily on the point of sale. Customers' ages and gender, as well as tomorrow's weather forecast, are important data. Stores place all orders on-line. As the store floor sizes are limited, they have to be very careful in choosing what brands to sell. In many cases, several stores from the same chain do business in neighboring areas. This strategy makes distribution to each store cheaper, as well as making multiple deliveries per day possible. Generally, food goods are delivered to each store two to five times a day from factories. Since products are delivered as needed, stores do not need large stock areas.
According to The Japan Franchise Association, as of August 2009[ [update]] (data pertaining to the month of July 2009), there are 42,345 convenience stores in Japan. 7-Eleven leads the market with 12,467 stores, followed by Lawson (9,562) and FamilyMart (7,604). Other operators include Circle K Sunkus, Daily Yamazaki, Ministop, Am/Pm Japan (acquired by Family Mart in 2009),ポプラ (Poplar (convenience store)), Coco Store and Seico Mart. Many items available in larger supermarkets can be found in Japanese convenience stores, though the selection is usually smaller. As well, the following additional services are also commonly available:
Some stores also sell charging service for electronic money and ATM services for credit card or consumer finance. Items not commonly sold include Slurpees, lottery tickets, car supplies, and gasoline.
In 1974, Japan had 1,000 convenience stores. In 1996, Japan had 47,000 convenience stores, and the number was increasing by 1,500 annually. In 1996, in Japan there was one convenience store for every 2,000 people, while in the United States there was one per 8,000 people. Peter Landers of the "Associated Press" said that the computerized distribution system allows Japanese convenience stores to stock a wider variety of products, allowing for them to be more competitive in the marketplace. Because Japan has a lower crime rate, store owners are not reluctant to keep stores open at late hours in the night, and customers are not reluctant to shop during those times.
Malaysia.
In Malaysia, 7-Eleven had the market leader in convenience store, with 1,479 stores. Other convenience stores in the country, includes KK Super Mart, Quick and Easy and MyMart (owned by Mydin). Carrefour Express is also among one of the few convenience stores that had been in existence in Malaysia in the past, but has since ceased operations.
Mexico.
Oxxo is the largest chain in the country, with more than 9,000 stores around the country. Other convenience stores, such as Tiendas Extra, 7-Eleven, SuperCity, ampm, are also found in Mexico. The first convenience store in the country, Super 7 (now a 7-Eleven), was opened in 1976 in Monterrey, Nuevo León. There are also some regional chains, like Amigo Express and CB Mas that operates in Comarca Lagunera, Super Q and El Matador in Queretaro, Coyote in Central Mexico and JV in Northeastern Mexico. Stores sell fast food like coffee, hot-dogs and nachos, cellphone refills between MXN$20 and MX$500, mainly Telcel and Movistar, newspapers, magazines, and some of their sells Panini products and other novelties.
Misceláneas (lit. meaning "place where miscellaneous items are sold", and otherwise called "tiendas de abarrotes" (grocery store) in some parts of the country) are smaller, family-run convenience stores, often found in central and southern Mexico. They operate in many locations, from rural communities to suburban residential neighborhoods, usually located in front of or below the family's residence. They often fulfil the role of neighborhood meeting points and places to disseminate community news. While offering a more limited, and sometimes varied, assortment of items than corporate chains, they fill a void in many areas in which corporate companies do not operate. Usually, they also sell home-made snacks, such as tortas and sandwiches, made by the owners themselves. They also provide items in smaller quantities than would be offered for sale in larger stores and markets, for example selling single cigarettes along with full packs.
New Zealand.
In New Zealand, convenience stores are commonly referred to as dairies and superettes. Dairies in New Zealand are generally independently owned and operated. The use of the term "dairy" to describe convenience stores was common in New Zealand by the late 1930s. Dairies carved out a niche in food retail by keeping longer trading hours than grocery stores and supermarkets – dairies were exempt from labor laws restricting trading hours and Saturday trading. With the deregulation of trading hours and in the wake of legislation in 1989 prohibiting sales of alcohol by dairies, the distinction between dairies, superettes, and grocery stores has blurred in New Zealand.
Singapore.
Major convenience stores in Singapore are 7-Eleven owned by Dairy Farm International Holdings and Cheers owned by NTUC Fairprice. Figures from the Singapore Department of Statistics showed that there are 338 7-Eleven stores and 91 Cheers outlets in 2004. Other convenience stores such as Myshop and One Plus appeared in 1983. Myshop belongs to a Japanese company, and One Plus belongs to Emporium Holdings.
Various reasons unique to Singapore have been given for the great popularity of convenience stores there. Convenience stores sell a wide range of imported goods, whereas minimarts and provision shops sell local products with a limited range of non-Asian products. Convenience stores are situated within housing estates thus reducing consumers' traveling time. Most families in Singapore are dual-income families. Since both the husband and wife are working, there is greater need for convenience in shopping for daily necessities. The 24-hour opening policy allows convenience stores to reach out to a larger group of consumers. Firstly, the policy caters to the shopping needs of consumers who work shifts or have irregular working hours. Secondly, the policy caters to the increasing number of Singaporeans who are keeping late nights. It was reported that 54% of Singaporeans stayed up past midnight in an economic review by Price Waterhouse Coopers (PWC) in 2005.
7-Eleven.
7-Eleven began the trend of convenience stores in Singapore when it opened its first store in 1982 by Jardine Matheson Group, under a franchise agreement with Southland Corporation of the United States. Dairy Farm International Holdings acquired the chain from Jardine Matheson Group in 1989.
The number of 7-Eleven outlets continued to increase in 1984 while other chains were having difficulty in expanding. One Plus was unable to expand due to the shortage of good sites. The original owners of the Myshop franchise, which had seven outlets, sold out to one of its suppliers due to a lack of demand.
However, in 1985, 7-Eleven faced difficulty in finding favourable locations and failed to meet its one-store-a-month target. The situation improved in 1986 with a new Housing Development Board (HDB) tendering system, which allowed 7-Eleven to secure shops without having to bid too high a price. 7-Eleven stores are open twenty-four hours a day, seven days a week, including Sundays and public holidays. This 24/7 policy was seen as the reason that gave 7-Eleven its edge over its competitors.
In 1990, there was a rise in the number of shop thefts in 7-Eleven. The shoplifters were usually teenagers who stole small items such as chocolates, cigarettes and beer. In response to the increase in the number of thefts, 7-Eleven stepped up security measures, which successfully lowered the crime rate by 60%
Cheers.
Cheers is owned by local corporation NTUC Fairprice, started in 1999. Cheers has adopted 7-Eleven's 24/7 model and taken similar security measures to prevent cases of shop lifting. Convenience store owners seeking franchising seem to prefer Cheers over 7-Eleven, probably due to its cheaper franchise fee.
Taiwan.
Boasting more than 10,000 convenience stores in an area of 35,980 km2 and a population of 23 million, Taiwan has Asia Pacific's and perhaps the world's highest density of convenience stores per person: one store per 2,500 people . With 4,665 7-Eleven stores, Taiwan also has the world's highest density of 7-Elevens per person: one store per 4,930 people . In Taipei, it is not unusual to see two 7-Elevens across the street or several of them within a few hundred meters of each other.
Because they are found everywhere, convenience stores in Taiwan provide services on behalf of financial institutions or government agencies such as collection of city parking fees, utility bills, traffic violation fines, and credit card payments. Eighty percent of urban household shoppers in Taiwan visit a convenience store each week .
United Kingdom.
The corner shop in the United Kingdom grew from the start of the industrial revolution, with large populations moving from the agricultural countryside to newly built model townships and later terraced housing in towns and cities. The corner shops were locally owned small businesses, started by entrepreneurs who had often had other careers before taking on the large start-up capital requirement required to establish such a trading business. Many well known high street retail brands, such as Marks and Spencer, Sainsbury's and latterly Tesco, originated during the Victorian era as simple family owned corner shops.
The reign of the corner shop and the weekly market started to fade post–World War II, with the combination of the personal motor car and the introduction from the 1950s of the American-originated supermarket format. The market shift in price and convenience lead to the establishment of common trading brands operating as virtual franchises to win back the consumer, including Budgens, Costcutter, Londis, Nisa and SPAR. There was also a consolidation of some shops under some larger corporate-owned brands, including One Stop and RS McColl.
From the late 1960s onwards, many such stores started to be owned by expatriate African-born Indians, expelled from their countries by the newly independent countries rulers. Under the Shops Act 1950, Sunday trading had been illegal for most traders, with exceptions only allowed for small stores selling perishable items (i.e.: milk, bread, butter, fresh meat and vegetables), and most shops that were not off licences had to close at 8 pm. However, the Sunday Trading Act 1994 allowed large format stores over 12000 ft2 to open on a Sunday and later extended to 24/7 opening, which has drastically cut the number of convenience and corner shops.
In more recent time, due to a combination of competition laws and a lack of large-scale development space, many of the larger retail brands have now developed shop formats based around convenience store and corner shop scale spaces, including Sainsbury's Local and Tesco Express.
United States.
In-store convenience store sales grew 2.4%, reaching a record $195.0 billion. Combined with $486.9 billion in motor fuels sales, total convenience store sales in 2011 were $681.9 billion, or one out of every 22 dollars of the overall $15.04 trillion U.S. gross domestic product. In New York City, "bodega" has come to mean any convenience store or deli.
The first chain convenience store in the United States was opened in Dallas, Texas in 1927 by the Southland Ice Company, which eventually became 7-Eleven, the largest convenience store chain. Stores connected to a service station developed into a trend, celebrated by some progressive architects: 
In the gasoline service station may be seen the beginning of an important advance agent of decentralization by way of distribution and also the beginning of the establishment of the Broadacre City. Wherever the service station happens to be naturally located, these now crude and seemingly insignificant units will grow and expand into various distributing centers for merchandise of all sorts. They are already doing so in the Southwest to a great extent.
 Frank Lloyd Wright, "The Disappearing City", 1932
In 1939, a dairy owner named J.J. Lawson started a store at his dairy plant near Akron, Ohio, to sell his milk. The Lawson's Milk Company grew to a chain of stores, primarily in Ohio. Circle K, another large company-owned convenience store chain, was founded in 1951. Since that time many different convenience store brands have developed, and their stores may either be corporate-owned or franchises. The items offered for sale tend to be similar despite store brand, and almost always include milk, bread, soft drinks, cigarettes, phone cards, coffee, slushees, candy bars, Twinkies, Slim Jims, hot dogs, ice cream, candy, gum, lip balm, chips, pretzels, popcorn, beef jerky, doughnuts, maps, magazines, newspapers, small toys, car supplies, feminine hygiene products, cat food, dog food, and toilet paper. Other less common items include sandwiches, pizza, and frozen foods. Nearly all convenience stores also have an automated teller machine (ATM), though other banking services are usually not available. State lottery tickets are also available at these stores.
In 1966, the U.S. convenience store industry first recorded $1 billion in sales. By the end of the decade, the industry had recorded $3.5 billion a year in sales. By the late 1960s, the amount of 24-hour convenience stores increased to meet the needs of a younger population and people who were working late night or early morning shifts. Not surprisingly, the first 24-hour store opened in Las Vegas in 1963.
Some convenience stores in the United States also sell gasoline. Only 2,500 stores had self-serve at the pump by 1969. It was not until the 1970s that retailers realized selling gasoline could be profitable—and competitive. In 2011, there were approximately 47,195 gas stations with convenience stores that generated $326 billion in revenue. Out of those over 3,008 of the gas stations had gas station TV installed at the gas station pumps.
Policies regarding the sale of adult magazines vary, but generally larger chains (such as 7-Eleven and Casey's General Stores) do not sell these items, while smaller independent stores may do so. One notable exception to this "rule" is fast-growing regional chain Sheetz, which does sell some soft-core pornographic material such as "Playboy" (including its various "special" issues), "Penthouse", and "Playgirl".
Because the laws regarding the sale of alcoholic beverages vary from state to state in the United States, the availability of beer, wine, and liquor varies greatly. For example, while convenience stores in Alaska, Pennsylvania and New Jersey cannot sell any kind of alcohol at all, stores in Nevada, New Mexico, and California may sell alcoholic beverages of any sort, while stores in Virginia, Idaho, or Oregon can sell beer and wine, but not liquor. Similar to grocery stores, convenience stores in New York can sell beer only, not wine or liquor. Altoona, Pennsylvania–based Sheetz tried to find a loophole in 2007 by classifying part of one of their prototype stores in Altoona as a restaurant, which would permit alcohol sales. However, state courts in Pennsylvania promptly overruled this. State law requires restaurants to have on-site consumption, but Sheetz did not do this. Sheetz continues to sell alcohol in other states.
Crime.
American convenience stores are often direct targets of armed robbery. In some areas of the United States, it is not unusual for clerks to be working behind bulletproof glass windows, even during daylight hours. Some convenience stores may even limit access inside at night, requiring customers to approach a walk-up window specifically for such situations to make purchases. The main dangers are that almost all convenience stores only have one person working night shift, most of the transactions are in cash, and easily resold merchandise, such as liquor, lottery tickets, and cigarettes are on site.
Most convenience stores have a cash drop slot into a time-delay safe so clerks may limit the amount of cash on hand. Many have installed security cameras to help deter robberies and shoplifting. Because of their vulnerability to crime, nearly all convenience stores have a friendly relationship with the local police. To reduce burglaries when the convenience store is closed, some convenience stores will have bars on the windows.
Similar concepts.
Convenience stores to some extent replaced the old-fashioned general store. They are similar but not identical to Australian milk bars. In Britain, corner shops in towns and village shops in the countryside served similar purposes and were the precursors to the modern European convenience store (e.g. SPAR). In the Canadian province of Quebec, dépanneurs (often referred to as "deps" in English) are often family-owned neighbourhood shops that serve similar purposes.
Truck stops, also known as "travel centers", combine a shop offering similar goods with a convenience store with amenities for professional drivers of semi-trailer trucks. This may include fast food restaurants, showers and facilities for buying large quantities of diesel fuel. The equivalent in Europe is the motorway service station.
Neighborhood grocery stores not big enough to be considered a supermarket often compete with convenience stores. For example, in Los Angeles, CA, a local chain operates neighborhood grocery stores that fill a niche between a traditional supermarket and convenience store. Because they stock fresh fruit and fresh meat and carry upwards of 5000 items, they have a lot in common with the supermarket. Due to the relatively small store size, customers can get in and out conveniently, or have purchases delivered. In Brussels, Belgium, convenience stores are known as night shops.

</doc>
<doc id="47420" url="http://en.wikipedia.org/wiki?curid=47420" title="7-Eleven">
7-Eleven

7-Eleven (or 7-11) is an international chain of convenience stores that operates primarily as a franchise.
7-Eleven Inc. is headquartered in Dallas, Texas, United States. Its parent company, Seven-Eleven Japan Co., is located in Tokyo, Japan. Seven-Eleven Japan is held by the Seven & I Holdings Co. holding company.
Etymology.
The company's first outlets were named "Tote'm Stores" because customers "toted" away their purchases. Some stores featured genuine Alaskan totem poles in front. In 1977, the name Tote'm was changed to 7-Eleven to reflect the company's new, extended hours, ranging from 7:00am to 11:00pm, seven days a week. The corporate name of the company was changed from "The Southland Corporation" to 7-Eleven Inc. in 1999.
History.
In May 20, 1966, Southland Ice Company employee John Jefferson Green began selling eggs, milk, and bread from the improvised ice house storefronts in Dallas, Texas, with permission from one of Southland's founding directors, Joe C. Thompson Sr. Although small grocery stores and general merchandisers were available, Thompson theorized that selling products such as bread and milk in convenience stores would reduce the need for customers to travel long distances for basic items. He eventually bought the Southland Ice Company and turned it into Southland Corporation, which oversaw several locations in the Dallas area.
In December 1969, a Southland executive brought a totem pole as a souvenir from Alaska and placed it in front of the store. The pole served as a marketing tool for the company, as it attracted a great deal of attention. Soon, executives added totem poles in front of every store and eventually adopted an Inuit-inspired theme for their stores. Later on, the stores began operating under the name "Tote'm Stores."
In the same year, the company began constructing gasoline stations in some of its Dallas locations as an experiment. Joe Thompson also provided a distinct characteristic to the company's stores, training the staff so that people would receive the same quality and service in every store. Southland also started to have a uniform for its ice station service men. This became the major factor in the company's success as a retail convenience store.
In 1977, the Great Depression greatly affected the company, sending it towards bankruptcy. Nevertheless, the company continued its operations through re-organization and receivership. A Dallas banker, W.W. Overton Jr., also helped to revive the company's finances by selling the company's bonds for seven cents on the dollar. This brought the company's ownership under the control of a board of directors.
In 1946, in an effort to continue the company's post-war recovery, the name of the franchise was changed to 7-Eleven to reflect the stores' new hours of operation, which were unprecedented at the time. By 1986, 7-Eleven had opened 100 stores. In 1989, 7-Eleven experimented with a 24-hour schedule in Austin, Texas, after an Austin store stayed open all night to satisfy customer demand. By 1963, 24-hour stores were established in Fort Worth and Dallas, Texas, as well as Las Vegas, Nevada.
In the late 1990s, The Southland Corporation was threatened by a corporate takeover, prompting the Thompson family to take steps to convert the company into a private model by buying out public shareholders in a tender offer. In 1999, John Philp Thompson, the Chairman and CEO of 7-Eleven, completed a $5.2 billion management buyout of the company. The buyout suffered from the effects of the 1987 stock market crash and after failing initially to raise high yield debt financing, the company was required to offer a portion of stock as an inducement to invest in the company's bonds.
Operating during this period with exceptionally high interest costs, the private company encountered financial difficulties. As part of the required restructuring, it sold various divisions such as the ice division and Chief Auto Parts in 1990 to General Electric and was later purchased by AutoZone. In 1998, the company was saved from bankruptcy by the Japanese Corporation Ito-Yokado. This downsizing also resulted in numerous metropolitan areas losing 7-Eleven stores to rival convenience store operators.
Ito-Yokado gained a controlling share of 7-Eleven in 1991 during the Japanese asset bubble of the early 1990s. Ito-Yokado formed Seven & I Holdings Co. and 7-Eleven became its subsidiary in 2005. In 2007, Seven & I Holdings announced that it would be expanding its American operations, with an additional 1,000 7-Eleven stores in the United States.
Seven & I was the fifth largest retailer in the world in 2013, with 35,000 stores in approximately 100 different countries.
Other products.
7-Eleven offers several brands of food and concepts, including Movie Quik, an in-store video-rental service.
7-Eleven is home to the . There are 33 flavors of SLURPEES available.
Since 2004, the company has offered a prepaid phone service where a cellphone can be purchased directly from a 7-Eleven store in the U.S. or Canada and activated on the spot.
Global.
Asia.
China.
7-Eleven opened its first store in China in Shenzhen of Guangdong Province in 1992 and later expanded to Beijing in 2004, Chengdu and Shanghai in 2011, Qingdao in 2012, and Chongqing in 2013. In China's 7-Eleven stores where Slurpees are offered, the Chinese name (sīlèbīng) is used. They also offer a wide array of warm food, including traditional items like steamed buns, and stores in Chengdu offer a full variety of onigiri (饭团). Beverages, alcohol, candy, periodicals, and other convenience items are available as well. The majority of these stores are open for 24 hours a day.
Hong Kong.
7-Eleven has operated in Hong Kong since 1981 under the ownership of Dairy Farm. With most locations being in urbanised areas, approximately 40 percent are franchised stores. In September 2004, Dairy Farm acquired Daily Stop, a convenience-store chain located mainly in the territory's MTR stations, and converted them to 7-Eleven stores. s of 2009[ [update]], Hong Kong has 950 7-Eleven stores and has the second-highest density of 7-Eleven stores after Macau, with one outlet per 1.16 km².
Indonesia.
In 2009, 7-Eleven announced plans to expand its business in Indonesia through a master franchise agreement with Modern Sevel Indonesia. Modern Sevel Indonesia's initial plans were to focus on opening stores in Jakarta, targeting densely populated commercial and business areas. Other major cities, such as Bandung, Semarang, and Surabaya have been identified as expansion opportunities. 
There are 190 7-Eleven stores in Indonesia as of 2014[ [update]].
Japan.
Japan has more 7-Eleven locations than anywhere else in the world, where they often bear the title of its current holding company "Seven & I Holdings". Of the 54,210 stores around the globe, 17,009 stores (31 percent of global stores) are located in Japan, with 2,246 stores in Tokyo alone. On September 1, 2005, Seven & I Holdings Co., Ltd., a new holding company, became the parent company of 7-Eleven, Ito Yokado, and Denny's Japan.
The aesthetics of the store are somewhat different from that of 7-Eleven stores in other countries as the stores offer a wider selection of products and services.
Following the example of other convenience stores in Japan, 7-Eleven has solar panels and LEDs installed in about 1,400 of its stores.
Macau.
7-Eleven entered the Macau market in 2005 under the ownership of Dairy Farm, the same conglomeration group operating Hong Kong's 7-Eleven. With only 25.9 square kilometres, Macau has 45 stores, making it the single market with the highest density of 7-Eleven stores, containing one store per 0.65 square kilometres.
Malaysia.
Malaysian 7-Eleven stores are owned by 7-Eleven Malaysia Sdn. Bhd., which now operates 1,472 stores nationwide (as of October 2013). 7-Eleven in Malaysia was incorporated on June 4, 1984, by the Berjaya Group Berhad. The first 7-Eleven store was opened in October 1984, in Jalan Bukit Bintang, Kuala Lumpur.
Philippines.
In the Philippines, 7-Eleven is run by the Philippine Seven Corporation (PSC). Its first store, located in Quezon City, opened in 1984. In 2000, President Chain Store Corporation (PCSC) of Taiwan, also a licensee of 7-Eleven, purchased the majority shares of PSC and thus formed a strategic alliance for the convenience store industry within the area. Now 7-Eleven abounds within the many islands of the Philippines.
Singapore.
In Singapore, 7-Eleven forms the largest chain of convenience stores island-wide. There are currently 560 7-Eleven stores scattered throughout the country. Stores in Singapore are operated by Dairy Farm International Holdings, franchised under a licensing agreement with 7-Eleven Incorporated, headquartered in Dallas, Texas, in the United States.
The first 7-Eleven stores were opened in 1983 with a franchise license under the Jardine Matheson Group. The license was then acquired by Cold Storage Singapore, a subsidiary of the Dairy Farm Group, in 1989.
7-Eleven stores in Singapore operate 24 hours a day, with the exception of stores in Biopolis, hospitals, MRT Stations, some shopping centres, ITE College West, Singapore Polytechnic, Republic Polytechnic, and Nanyang Technological University, which have shorter operating hours.
South Korea.
7-Eleven has a major presence in the South Korean convenience store market, where it competes with Ministop, GS25 (formerly LG25), FamilyMart, and independent competitors. There are 7,064 7-Eleven stores in South Korea; with only Japan, the United States, and Thailand hosting more stores. The first 7-Eleven store in South Korea opened in 1989 in Songpa-gu in Seoul with a franchise license under the Lotte Group. In January 2010, Lotte Group acquired the Buy the Way convenience store chain and rebranded its 1,000 stores under the 7-Eleven brand.
Taiwan.
In Taiwan, 7-Eleven is the largest convenience store chain and is owned by under Uni-President Enterprises Corporation. The first store opened in 1979 and the 5,000th store was opened in July 2014. Taiwan has the world's fifth-largest number of 7-Eleven convenience stores after Japan, the United States, Thailand and South Korea.
Thailand.
The first store opened in 1989 on Patpong Road in Bangkok. The franchise in Thailand is the CP ALL Public Company Limited, which in turn grants franchises to operators. There are 8,334 7-Eleven stores in Thailand as of 2013[ [update]], with approximately 50% located in Bangkok. Thailand has the 3rd largest number of 7-Eleven stores after Japan and the United States.
The company plans to spend five billion Thai baht to expand its business. Two billion will be used to open 500 new outlets, one billion to renovate existing stores, and the rest to develop a new distribution centre in the East.
United Arab Emirates.
Seven & I Holdings announced in June 2014 that they had agreed a contract with Seven Emirates Investment LLC to open the first Middle Eastern 7-Eleven in Dubai, United Arab Emirates during the summer of 2015. The company also said that they had plans to open about 100 stores in the country by the end of 2017.
Europe.
The first European 7-Eleven store was opened in Stockholm, Sweden in 1978. 7-Eleven was available in Spain until 2000 with many stores inside Repsol petrol stations, as well as some other petrol-stations across the country. 7-Eleven stores are now solely located in the Scandinavian region of Europe.
The owner of the master franchise for 7-Eleven in Scandinavia is Reitan Servicehandel, an arm of the Norwegian retail group, Reitan Group. After Reitangruppen bought the filling station chain, HydroTexaco (now YX Energy), in Norway and Sweden in 2006, it announced that several of the stores at the petrol stations would be rebranded as 7-Elevens and that the petrol would be supplied by Shell. Other stores remain under the YX brand.
Denmark.
The first 7-Eleven store in Denmark was opened at Østerbro in Copenhagen on September 14, 1993. There are currently 196 stores, mostly in Copenhagen, Aarhus, Aalborg, and Odense, including 8 stores at Copenhagen Central Station. In Denmark, 7-Eleven has an agreement with Shell, with a nationwide network of Shell/7-Eleven service stations, and an agreement with DSB to have 7-Eleven stores at most S-train stations.
Norway.
The first 7-Eleven store in Norway was opened at Grünerløkka in Oslo on September 13, 1986. As of January 2012, there are 162 7-Eleven stores in Norway, more than 50% located in Oslo. Norway has the northernmost 7-Eleven in the world, situated in Tromsø. On a per-capita basis, Norway has one 7–Eleven store for every 47,000 Norwegians, compared to Canada, which has one for every 74,000 Canadians.
Sweden.
Reitan Servicehandel Sverige has held the license in Sweden since December 1997. In the mid-1990s period, 7-Eleven in Sweden received adverse publicity due to the unfavourable labour contracts offered by its then-licensee, "Small Shops," an American-based company, resulting in many stores being sold and closed down. For a time, there were only 7-Elevens in Stockholm and Gothenburg.
7-Eleven returned to the south of Sweden in 2001, when a convenience store opened in Lund. Later in the 2000s, the Swedish 7-Eleven chain was involved in controversy when the Swedish TV channel TV3 exposed widespread fraud on the part of Reitan Servicehandel in its management of the 7-Eleven franchise, which Reitan Servicehandel eventually admitted to on its website.
North America.
Canada.
The first 7-Eleven store to open in Canada was in Calgary, Alberta, on June 29, 1969. There are 484 7-Eleven stores in Canada as of 2013[ [update]]. Winnipeg, Manitoba, has the world's largest number of Slurpee consumers, with an estimated 1,500,000 slurpees sold since the first 7-Eleven opened on March 21, 1970. All 7-Eleven locations in Canada are corporate operated.
A limited number of 7-Eleven locations feature gas stations from Shell Canada, Petro-Canada, or Esso. In November 2005, 7-Eleven started offering the Speak Out Wireless cellphone service in Canada. 7-Eleven locations also featured CIBC ATMs—in June 2012, these machines were replaced with ATMs operated by Scotiabank. 7-Eleven abandoned the Ottawa, Ontario, market in December 2009 after selling all of the six outlets to Quickie Convenience Stores, a regional chain. Following concerns over the fate of Speak Out Wireless customers, Quickie offered to assume existing SpeakOut customers and phones into its Good2Go cellphone program. 7-Eleven is similarly absent from the Quebec market due to its saturation by chains like Alimentation Couche-Tard, Boni-soir as well as independent dépanneurs.
Mexico.
In Mexico, the first 7-Eleven store opened in 1971 in Monterrey, Nuevo León, in association with Grupo Chapa (now Iconn) and 7-Eleven, Inc. under the name Super 7. In 1995, Super 7 was renamed to 7-Eleven, which has 1,552 stores in several areas of the country. When stores are located within classically designed buildings (such as in Centro Histórico buildings) or important landmarks, the storefront logo is displayed in monochrome with gold or silver lettering. The main competitors in Mexico are OXXO (Femsa), Super City (Soriana), Farmacias Guadalajara and other local competitors.
United States.
"Supermarket News" ranked 7-Eleven's North American operations No. 11 in the 2007 "Top 75 North American Food Retailers," based on the 2006 fiscal year estimated sales of US$15.0 billion. Based on the 2005 revenue, 7-Eleven is the twenty-fourth largest retailer in the United States. s of 2013[ [update]], 8,144 7-Eleven franchised units exist across the United States. Franchise fees range between US$10,000 - $1,000,000 and the ongoing royalty rate varies. 7-Eleven America has its headquarters in the One Arts Plaza building in Downtown Dallas, Texas.
7-Eleven is moving toward franchising most of its remaining corporate locations inside the United States. The 7-Eleven franchise system splits the gross profits in an arrangement that is around 50/50, between the company and the individual franchisee. The initial 7-Eleven franchise term is 15 years. The franchise fee and other upfront fees collected by 7-Eleven from a newly approved franchisee, in addition to ongoing 50:50 sharing of profits, is not transferable to another incoming franchisee in the same store, for the unexpired portion, if any, of the current 15-year contract. For example, if one pays the full franchise fee for 15 years and leaves the store after one year for any reason, they stand to lose the franchise fee for the remaining 14 years of their term.
Fuel.
In the United States, many 7-Eleven locations previously had filling stations with gasoline distributed by Citgo, which in 1983 was purchased by Southland Corporation (50% of Citgo was subsequently sold in 1986 to Petróleos de Venezuela, S.A., with the remaining 50% acquired in 1990). Although Citgo was the predominant partner of 7-Eleven, other oil companies are also co-branded with 7-Eleven, including Fina, Exxon, Gulf, Marathon, BP, Sunoco, Shell, Chevron (some former TETCO convenience stores were co-branded with Chevron, Valero and Texaco prior to the 7-Eleven purchase in late 2012CE), and Pennzoil. Alon USA is the largest 7-Eleven licensee in North America.
On September 27, 2006, 7-Eleven announced the impending cessation of its 20-year contract with Citgo and that the contract would not be renewed.
7-Eleven signed an agreement with ExxonMobil in December 2010 for the acquisition of 183 sites in Florida. This was followed by the acquisition of 51 ExxonMobil sites in North Texas in August 2011.
Oceania.
Australia.
The first 7-Eleven in Australia opened on August 24, 1977, in the Melbourne suburb of Oakleigh. The majority of stores are located in metropolitan areas, particularly in central business district areas. Stores in suburban areas often operate as petrol stations and most are owned and operated as franchises, with a central administration. In the mid-00's 7-Eleven bought out Mobil's remaining Australian petrol stations, converting them to 7-Eleven outlets, resulting in an immediate and unprecedented overnight major expansion of the brand.
7-Eleven stores in Australia sell a wide range of items, including daily newspapers, drinks, confectionery, and snack foods. They also sell gift cards, including three types of pre-paid VISA cards. The chain has also partnered with , resulting in the placement of BankWest ATMs in their stores nationwide.
On November 7 of each year, one small cup of Slurpee is given free of charge to each customer to promote the annual event, Seven Eleven Day. November is the 11th month, which makes it "7/11" in accordance with the Australian date format.
In April 2014, 7-Eleven announced plans to start operating stores in Western Australia, with 11 stores planned to operate within the first year and a total of 75 stores established within five years. The first store was opened on October 30, 2014 in the city of Fremantle.
In popular culture.
"And I went down my old neighborhood
The faces have all changed there's no one left to talk to
And the pool hall I loved as a kid
Is now a Seven Eleven"
7-Eleven outlet in Selayang, Kuala Lumpur Malaysia regularly tipped by native heroes Rogan and Dhasan. Eventually they had became the heavy-tippers in that area.

</doc>
<doc id="47422" url="http://en.wikipedia.org/wiki?curid=47422" title="Monster group">
Monster group

In the area of modern algebra known as group theory, the monster group "M" (also known as the Fischer–Griess monster, or the Friendly Giant) is a sporadic simple group of order
The finite simple groups have been completely classified. Every such group belongs to one of 18 countably infinite families, plus 26 sporadic groups that do not follow such a systematic pattern. The monster group is the largest of these sporadic groups and contains all but six of the other sporadic groups as subquotients. Robert Griess has called these 6 exceptions pariahs, and refers to the other 20 as the "happy family".
History.
The monster was predicted by Bernd Fischer (unpublished) and Robert Griess (1976) in about 1973 as a simple group containing a double cover of Fischer's Baby Monster group as a centralizer of an involution. Within a few months the order of M was found by Griess using the Thompson order formula, and Fischer, Conway, Norton and Thompson discovered other groups as subquotients, including many of the known sporadic groups, and two new ones: the Thompson group and the Harada–Norton group. constructed M as the automorphism group of the Griess algebra, a 196884-dimensional commutative nonassociative algebra. John Conway (1985) and Jacques Tits (1984, 1985) subsequently simplified this construction.
Griess's construction showed that the monster existed. Thompson (1979) showed that its uniqueness (as a simple group satisfying certain conditions coming from the classification of finite simple groups) would follow from the existence of a 196883-dimensional faithful representation. A proof of the existence of such a representation was announced by Norton (1985), though he has never published the details. gave the first complete published proof of the uniqueness of the monster (more precisely, they showed that a group with the same centralizers of involutions as the monster is isomorphic to the monster).
The Schur multiplier and the outer automorphism group of the monster are both trivial.
Representations.
The minimal degree of a faithful complex representation is 196883, which is the product of the 3 largest prime divisors of the order of M.
The character table of the monster, a 194-by-194 array, was calculated in 1979 by Fischer and Donald Livingstone using computer programs written by Michael Thorne. The smallest faithful linear representation over any field has dimension 196882 over the field with 2 elements, only 1 less than the dimension of the smallest faithful complex representation.
The smallest faithful permutation representation of the monster is on 
24 · 37 · 53 · 74 · 11 · 132 · 29 · 41 · 59 · 71 (about 1020)
points.
The monster can be realized as a Galois group over the rational numbers , and as a Hurwitz group .
The monster is unusual among simple groups in that there is no known easy way to represent its elements. This is not due so much to its size as to the absence of "small" representations. For example, the simple groups "A"100 and SL20(2) are far larger, but easy to calculate with as they have "small" permutation or linear representations. The alternating groups have permutation representations that are "small" compared to the size of the group, and all finite simple groups of Lie type have linear representations that are "small" compared to the size of the group. All sporadic groups other than the monster also have linear representations small enough that they are easy to work with on a computer (the next hardest case after the monster is the baby monster, with a representation of dimension 4370).
A computer construction.
Robert A. Wilson has found explicitly (with the aid of a computer) two 196882 by 196882 matrices (with elements in the field of order 2) which together generate the monster group; this is one dimension lower than the 196883-dimensional representation in characteristic 0. Performing calculations with these matrices is possible but is too expensive in terms of time and storage space to be useful. Wilson with collaborators has found a method of performing calculations with the monster that is considerably faster.
Let "V" be a 196882 dimensional vector space over the field with 2 elements. A large subgroup "H" (preferably a maximal subgroup) of the monster is selected in which it is easy to perform calculations. The subgroup "H" chosen is 31+12.2.Suz.2, where Suz is the Suzuki group. Elements of the monster are stored as words in the elements of "H" and an extra generator "T". It is reasonably quick to calculate the action of one of these words on a vector in "V". Using this action, it is possible to perform calculations (such as the order of an element of the monster). Wilson has exhibited vectors "u" and "v" whose joint stabilizer is the trivial group. Thus (for example) one can calculate the order of an element "g" of the monster by finding the smallest "i" > 0 such that "g""i""u" = "u" and "g""i""v" = "v".
This and similar constructions (in different characteristics) have been used to find some of its non-local maximal subgroups.
Moonshine.
The monster group is one of two principal constituents in the Monstrous moonshine conjecture by Conway and Norton, which relates discrete and non-discrete mathematics and was finally proved by Richard Borcherds in 1992.
In this setting, the monster group is visible as the automorphism group of the monster module, a vertex operator algebra, an infinite dimensional algebra containing the Griess algebra, and acts on the monster Lie algebra, a generalized Kac–Moody algebra.
McKay's E8 observation.
There are also connections between the monster and the extended Dynkin diagrams formula_1 specifically between the nodes of the diagram and certain conjugacy classes in the monster, known as "McKay's E8 observation". This is then extended to a relation between the extended diagrams formula_2 and the groups 3."Fi"24', 2."B", and "M", where these are (3/2/1-fold central extensions) of the Fischer group, baby monster group, and monster. These are the sporadic groups associated with centralizers of elements of type 1A, 2A, and 3A in the monster, and the order of the extension corresponds to the symmetries of the diagram. See ADE classification: trinities for further connections (of McKay correspondence type), including (for the monster) with the rather small simple group PSL(2,11) and with the 120 tritangent planes of a canonic sextic curve of genus 4.
Maximal subgroups.
The monster has at least 44 conjugacy classes of maximal subgroups. Non-abelian simple groups of some 60 isomorphism types are found as subgroups or as quotients of subgroups. The largest alternating group represented is A12.
The monster contains 20 of the 26 sporadic groups as subquotients. This diagram, based on one in the book "Symmetry and the monster" by Mark Ronan, shows how they fit together. The lines signify inclusion, as a subquotient, of the lower group by the upper one. The circled symbols denote groups not involved in larger sporadic groups. For the sake of clarity redundant inclusions are not shown.
44 of the classes of maximal subgroups of the monster are given by the following list, which is (as of 2013) believed to be complete except possibly for almost simple subgroups with non-abelian simple socles of the form L2(13), U3(4), U3(8) or Suz(8) , . However, tables of maximal subgroups have often been found to contain subtle errors, and in particular at least two of the subgroups on the list below were incorrectly omitted in some previous lists. 

</doc>
<doc id="47425" url="http://en.wikipedia.org/wiki?curid=47425" title="Bungee jumping">
Bungee jumping

Bungee jumping (; also spelt "Bungy" jumping, which is the usual spelling in New Zealand and several other countries)
is an activity that involves jumping from a tall structure while connected to a large elastic cord. The tall structure is usually a fixed object, such as a building, bridge or crane; but it is also possible to jump from a movable object, such as a hot-air-balloon or helicopter, that has the ability to hover above the ground. The thrill comes from the free-falling and the rebound. When the person jumps, the cord stretches and the jumper flies upwards again as the cord recoils, and continues to oscillate up and down until all the kinetic energy is dissipated.
History.
The first modern bungee jumps were made on 1 April 1979 from the 250 ft Clifton Suspension Bridge in Bristol, by David Kirke and Simon Keeling, both members of the Oxford University Dangerous Sports Club. The jumpers were arrested shortly after, but continued with jumps in the US from the Golden Gate Bridge and the Royal Gorge Bridge (this last jump sponsored by and televised on the American programme "That's Incredible"), spreading the concept worldwide. By 1982, they were jumping from mobile cranes and hot air balloons.
Organised commercial bungee jumping began with the New Zealander, A J Hackett, who made his first jump from Auckland's Greenhithe Bridge in 1986. During the following years, Hackett performed a number of jumps from bridges and other structures (including the Eiffel Tower), building public interest in the sport, and opening the world's first permanent commercial bungee site, the Kawarau Bridge Bungy at the Kawarau Gorge Suspension Bridge near Queenstown in the South Island of New Zealand. Hackett remains one of the largest commercial operators, with concerns in several countries.
Several million successful jumps have taken place since 1980. This safety record is attributable to bungee operators rigorously conforming to standards and guidelines governing jumps, such as double checking calculations and fittings for every jump. As with any sport, injuries can still occur (see below), and there have been fatalities. A relatively common mistake in fatality cases is to use a cord that is too long. The cord should be substantially shorter than the height of the jumping platform to allow it room to stretch. When the cord becomes taut and then is stretched, the tension in the cord progressively increases. Initially the tension is less than the jumper's weight and the jumper continues to accelerate downwards. At some point, the tension equals the jumper's weight and the acceleration is temporarily zero. With further stretching, the jumper has an increasing upward acceleration and at some point has zero vertical velocity before recoiling upward. See also Potential energy for a discussion of the spring constant and the force required to distort bungee cords and other spring-like objects.
The Bloukrans River Bridge was the first bridge to be 'bungee jumped off' in Africa when Face Adrenalin introduced bungee jumping to the African continent in 1990. Bloukrans Bridge Bungy has been operated commercially by Face Adrenalin since 1997, and is the highest commercial bridge bungy in the world.
In April 2008 a 37-year-old Durban man, Carl Mosca Dionisio, made bungee jumping history when he jumped off a 30 m (100 ft) tower attached to a bungee cord made entirely of 18,500 condoms.
The word "bungee".
The word "bungee" originates from West Country dialect of English language, meaning "Anything thick and squat", as defined by James Jennings in his book "Observations of Some of the Dialects in The West of England" published 1825. Around 1930, the name became used for a rubber eraser. The word bungy, as used by A J Hackett, is "Kiwi slang for an Elastic Strap".
Cloth-covered rubber cords with hooks on the ends have been available for decades under the generic name "bungy cords."
Earlier tethered jumping.
The land diving" (Sa: "Gol") of Pentecost Island in Vanuatu is an ancient ritual in which young men jump from tall wooden platforms with vines tied to their ankles as a test of their courage and passage into manhood. Unlike in modern bungee-jumping, land-divers intentionally hit the ground, but the vines absorb sufficient force to make the impact non-lethal. The land-diving ritual on Pentecost has been claimed as an inspiration by AJ Hackett, prompting calls from the islanders' representatives for compensation for what they view as the unauthorised appropriation of their cultural property.
A similar practice, only with a much slower pace for falling, has been practised as the Danza de los Voladores de Papantla or the 'Papantla flyers' of central Mexico, a tradition dating back to the days of the Aztecs.
A tower 4000 ft high with a system to drop a "car" suspended by a cable of "best rubber" was proposed for the Chicago World Fair, 1892-1893. The car, seating two hundred people, would be shoved from a platform on the tower and then bounce to a stop. The designer engineer suggested that for safety the ground below "be covered with eight feet of feather bedding". The proposal was declined by the Fair's organizers.
Equipment.
The elastic rope first used in bungee jumping, and still used by many commercial operators, is factory-produced braided shock cord. This special bungee cord consists of many latex strands enclosed in a tough outer cover. The outer cover may be applied when the latex is pre-stressed, so that the cord's resistance to extension is already significant at the cord's natural length. This gives a harder, sharper bounce. The braided cover also provides significant durability benefits. Other operators, including A. J. Hackett and most southern-hemisphere operators, use unbraided cords with exposed latex strands (pictured at top). These give a softer, longer bounce and can be home-produced.
There may be a certain elegance in using only a simple ankle attachment, but accidents where participants became detached led many commercial operators to use a body harness, if only as a backup for an ankle attachment. Body harnesses generally derive from climbing equipment rather than parachute equipment.
The highest jump.
In August 2005, AJ Hackett added a SkyJump to the Macau Tower, making it the world's highest jump at 233 m. The SkyJump did not qualify as the world's highest "bungee" as it is not strictly speaking a bungee jump, but instead what is referred to as a 'Decelerator-Descent' jump, using a steel cable and decelerator system, rather than an elastic rope. On 17 December 2006, the Macau Tower started operating a proper bungee jump, which became the "Highest Commercial Bungee Jump In The World" according to the Guinness Book of Records. The Macau Tower Bungy has a "Guide cable" system that limits swing (the jump is very close to the structure of the tower itself) but does not have any effect on the speed of descent, so this still qualifies the jump for the World Record. On 26 May 2002, Jiri Stolin from Jiri Stolin, Xtreme Sports (Czech republic) jumped from the highest place in the Czech republic, from TV Tower Prague (Zizkov) in the Czech republic.
Another commercial bungee jump currently in operation is just 13m smaller, at 220 m. This jump, made without guide ropes, is from the top of the Verzasca Dam near Locarno, Switzerland. It appears in the opening scene of the James Bond film "GoldenEye". The Bloukrans Bridge Bungy in South Africa and the Verzasca Dam jumps are pure freefall swinging bungee from a single cord.
Guinness only records jumps from fixed objects to guarantee the accuracy of the measurement. John Kockleman however recorded a 2200 ft bungee jump from a hot air balloon in California in 1989. In 1991 Andrew Salisbury jumped from 9000 ft from a helicopter over Cancun for a television program and with Reebok sponsorship. The full stretch was recorded at 3157 ft. He landed safely under parachute.
One commercial jump higher than all others is at the Royal Gorge Bridge in Colorado. The height of the platform is 321 m. However, this jump is rarely available, as part of the Royal Gorge Go Fast Games—first in 2005, then again in 2007. Previous to this the record was held in West Virginia, USA, by New Zealander Chris Allum, who bungee jumped 823 ft (251m) from the New River Gorge Bridge on "Bridge Day" 1992 to set a world's record for the longest bungee jump from a fixed structure.
In popular culture.
Several major movies have featured bungee jumps, most famously the opening sequence of the 1995 James Bond film "GoldenEye" in which Bond makes a jump over the edge of a dam in Russia (in reality the dam is in Switzerland: Verzasca Dam, and the jump was genuine, not an animated special effect). The jump in the dam later makes an appearance as a Roadblock task in the 14th season of the reality competition series "The Amazing Race".
It appears in the title of the South Korean film "Bungee Jumping of Their Own" ("Beonjijeompeureul hada 번지점프를 하다"; 2001), although it does not play a large part in the film.
A fictional proto-bungee jump is a plot point in the Michael Chabon novel "The Amazing Adventures of Kavalier and Clay".
In the film "Selena", in which Jennifer Lopez plays Selena Quintanilla-Perez, her character is shown bungee jumping at a carnival. This actual event took place shortly before Selena's murder on March 31, 1995.
In the medical drama television series Scrubs episode "My first step", characters John Dorian and Elliot Reid bungee jump after he realizes he doesn't take risks often enough.
Variations.
Catapult.
In "Catapult" (Reverse Bungee or Bungee Rocket) the 'jumper' starts on the ground. The jumper is secured and the cord is stretched, then released and shooting the jumper up into the air. This is often achieved using either a crane or a hoist attached to a (semi-)perma structure. This simplifies the action of stretching the cord and later lowering the participant to the ground.
Trampoline.
"Bungy Trampoline" uses, as its name suggests, elements from bungy and trampolining. The participant begins on a trampoline and is fitted into a body harness, which is attached via bungy cords to two high poles on either side of the trampoline. As they begin to jump, the bungy cords are tightened, allowing a higher jump than could normally be made from a trampoline alone.
Running.
"Bungee Running" involves no jumping as such. It merely consists of, as the name suggests, running along a track (often inflatable) with a bungee cord attached. One often has a velcro-backed marker that marks how far the runner got before the bungee cord pulled back. This activity can often be found at fairs and carnivals and is often most popular with children.
Ramp.
Bungee jumping off a ramp. Two rubber cords - the "bungees" - are tied around the participant's waist to a harness. Those bungee cords are linked to steel cables along which they can slide due to stainless pulleys. The participants bicycle, sled or ski before jumping.
Suspended Catch Air Device.
SCAD diving is similar to bungee jumping in that the participant is dropped from a height, but in this variation there is no cord; instead the participant falls into a net.
Safety and possible injury.
Bungee jumping injuries may be divided into those that occur after jumping secondary to equipment mishap or tragic accident, and those that occur regardless of safety measures.
In the first instance, injury can happen if the safety harness fails, the cord length is miscalculated, or the cord is not properly connected to the jump platform. In 1986, Michael Lush died of multiple injuries after bungee jumping for a stunt on a BBC television programme and in 1997 Laura Patterson, one of a 16-member professional bungee jumping team, died of massive cranial trauma when she jumped from the top level of the Louisiana Superdome and collided head-first into the concrete-based playing field. She was practicing for an exhibition intended to be performed during the halftime show of Super Bowl XXXI. In 2002, Chris Thomas died after his harness tore off during a charity jump in Swansea, Wales: it was later claimed that the harness was not safe for his weight. On New Year's Eve 2011, Erin Langworthy, an Australian woman was plunged into the Zambezi River at Victoria Falls, where she nearly drowned with her feet still tied together after her bungee rope snapped during a jump. Matthew E. Coleman, a 21-year-old man from Maryland, died at an Adventure World bungee jump after slamming into the ground with a rope that was too long.
Injuries that occur despite safety measures generally relate to the abrupt rise in upper body intravascular pressure during bungee cord recoil. Eyesight damage is the most frequently reported complication. Impaired eyesight secondary to retinal haemorrhage may be transient or take several weeks to resolve. In one case, a 26-year-old woman's eyesight was still impaired after 7 months. Whiplash injuries may occur as the jumper is jolted on the bungee cord and in at least one case, this has led to quadriplegia secondary to a broken neck. Very serious injury can also occur if the jumper's neck or body gets entangled in the cord. More recently, carotid artery dissection leading to a type of stroke after bungee jumping has also been described. All of these injuries have occurred in fit and healthy people in their twenties and thirties. Bungee jumping has also been shown to increase stress and decrease immune function.

</doc>
<doc id="47426" url="http://en.wikipedia.org/wiki?curid=47426" title="Water polo">
Water polo

Water polo is a team water sport. The game consists of 4 quarters (or periods) in which the two teams attempt to score goals by throwing the ball into their opponent's goal, with the team with the most goals at the end of the game winning the match. A team consists of 6 field players and one goalkeeper in the water at any one time. In addition to this, teams may have substitute field players and one substitute goalkeeper who are not in the water. Water polo is typically played in an all-deep pool (usually at least 1.8m deep or 5.9 feet), and players require stamina and endurance to play the game.
Water polo is a contact sport. Minor fouls occur frequently and exclusion fouls (in which a player is suspended from the game for 20 seconds) are common.
Special equipment for water polo includes a water polo ball, which floats on the water; numbered and colored caps; and goals, which either float in the water or are attached to the side of the pool.
The game consists of swimming (with and without the ball), using a special form of treading water known as the eggbeater kick , throwing, catching, and shooting the ball. All this must be done using a single hand. Each team consists of 6 field players and a goalkeeper. Except for the goalkeeper, players participate in both offensive and defensive roles. The players are also required to have some ability to play in all offensive and defensive roles.
The game is thought to have originated in Scotland in the late 19th century as a sort of "water rugby". William Wilson is thought to have developed the game during a similar period. The game thus developed with the formation of the London Water Polo League and has since expanded, becoming widely popular in various places around the world, notably mainland Europe, the United States of America, China, Canada and Australia.
History.
The history of water polo as a team sport began as a demonstration of strength and swimming skill in late 19th century England and Scotland, where water sports and racing exhibitions were a feature of county fairs and festivals. Men's water polo was among the first team sports introduced at the modern Olympic games in 1900. Water polo is now popular in many countries around the world, notably Europe (particularly in Serbia, Russia, Croatia, Italy, Montenegro, Greece and Hungary), the United States, Canada and Australia. The present-day game involves teams of seven players (plus up to six substitutes), with a water polo ball similar in size to a soccer ball but constructed of waterproof nylon.
One of the earliest recorded antecedents of the modern game of Water Polo was a game of water ‘hand-ball’ played at Bournemouth on 13 July 1876. This was a game between 12 members of the Premier Rowing Club, with goals being marked by four flags placed in the water near to the midpoint of Bournemouth Pier. The game started at 6.00pm in the evening and lasted for 15 minutes (when the ball burst) watched by a large crowd; with plans being made for play on a larger scale the following week.
The rules of water polo were originally developed in the late nineteenth century in Great Britain by William Wilson. Wilson is believed to have been the First Baths Master of the Arlington Baths Club in Glasgow. The first games of 'aquatic football' were played at the Arlington in the late 1800s (the Club was founded in 1870), with a ball constructed of India rubber. This "water rugby" came to be called "water polo" based on the English pronunciation of the Balti word for ball, "pulu". Early play allowed brute strength, wrestling and holding opposing players underwater to recover the ball; the goalie stood outside the playing area and defended the goal by jumping in on any opponent attempting to score by placing the ball on the deck.
Rules.
Number of players.
Seven players from each team (six field players and a goalkeeper) are allowed in the playing area of the pool during game play. If a player commits a major foul, then that team will only have 6 players until the player is allowed to re-enter. If a player commits a particularly violent act, such as striking a player, then the referee may signal a brutality foul, in which case that team is required to play with just 6 players in the water for a full 4 minutes, in addition to that player being disallowed any subsequent re-entry (i.e. he or she must sit out for the remainder of the game). In addition, the player may not be allowed to compete in a given number of future games depending on the governing body.
Players may be substituted in and out after goals, during timeouts, at the beginning of each quarter, after ordinary fouls and after injuries. During game play, players enter and exit in the corner of the pool, or in front of their goal; when play is stopped, they may enter or exit anywhere.
If at any time during play a team has more players in the pool than they are allowed, a penalty is given to the opposing team.
Caps.
The two opposing teams must wear caps which contrast:
In practice, one team usually wears blue caps and the other white, but some teams do choose to wear different cap colors. For instance, Australia's women's water polo team wears green caps.
Both goalies wear red caps. The first choice goalkeeper is usually marked "1" with the reserve being marked "13" (under FINA rules) or "1A" (under NCAA and NFHS rules).
Duration of game.
The game is divided into four periods; the length depends on the level of play:
Game and shot clock.
The game clock is stopped when the ball is not 'in play' (between a foul being committed and the free throw being taken, and between a goal being scored and the restart). As a result, the average quarter lasts around 12 minutes 'real time'. A team may not have possession of the ball for longer than 30 seconds without shooting for the goal unless an opponent commits an ejection foul. After 30 seconds, possession passes to the other team. However, if a team shoots the ball within the allotted time, and regains control of the ball, the shot clock is reset to 30 seconds. Each team may call 1 one-minute timeout in each period of regulation play as long as the ball is in their possession, and one timeout if the game goes into overtime. During game play, only the team in possession of the ball may call a timeout.
Pool dimensions.
Dimensions of the water polo pool are not fixed and can vary between 20×10 and 30×20 meters. Minimum water depth must be least 1.8 meters (6 feet), but this in reality is sometimes not the case due to practicalities. The goals are 3 meters wide and 90 centimeters high. Water polo balls are generally yellow and of varying size and weight for juniors, women and men. The middle of the pool is designated by a white line. Before 2005, the pool was divided by 7 and 4 meter lines (distance out from the goal line). This has been merged into one 5 meter line since the 2005–2006 season. Along the side of the pool, the center area between the 5 meter lines is marked by a green line (if marked at all). The "five meters" line is where penalties are shot and it is designated by a yellow line. The "two meter" line is designated with a red line and no player of the attacking team can receive a ball inside this zone.
Overview of game play.
In a water polo team, 6 players are assigned to attacking and defensive roles (commonly known as "fielders"), while one is assigned to the goalkeeping role. The primary aims of the fielders are to score goals and to prevent the other team scoring against their own team's goalkeeper. The goalkeeper's primary role is to stop shots from the opposing team going into his or her own goal.
Game play broadly includes swimming with and without the ball, passing both to a player's hand and onto the water and shooting. Fouls are very common, and these affect the game play, since the victim of a minor or a major foul will have the advantage of a free throw, while the victim of a penalty foul will have the opportunity of a one-on-one shot against the opposing team's goalkeeper.
Moving the ball.
Players can move the ball by throwing it to a teammate or swimming with the ball in front of them. Players are not permitted to push the ball underwater in order to keep it from an opponent, or push or hold an opposing player unless that player is holding the ball. If a player does push the ball underwater when it is in their possession, that will result in a "turnover" which is when you have to hand the ball over to the other team. Water polo is an intensely aggressive sport, so fouls are very common and result in a free throw during which the player cannot shoot at the goal unless beyond the "5 meter" line. If a foul is called outside the 5 meter line, the player is either able to shoot, pass or continue swimming with the ball. Water polo players need remarkable stamina because of the considerable amount of holding and pushing that occurs during the game, some allowed, some unseen or ignored by the referees (usually underwater).
Passing.
Water polo is a game requiring excellent hand-eye coordination. The ability to handle and pass the ball flawlessly separates the good teams from the great teams. A pass thrown to a field position player is preferably a "dry pass" (meaning the ball does not touch the water) and allows for optimal speed when passing from player to player with fluid motion between catching and throwing. A "wet pass" is a deliberate pass into the water, just out of reach of the offensive player nearest the goal (the "hole set") and his defender. The hole-set can then lunge towards the ball and out of the water to make a shot or pass. The only player who is allowed to use both hands to touch the ball is the goalkeeper. Other players who touch the ball with both hands will cause a turnover.
Fouls.
There are two types of fouls: one (like the scenario above) only results in the "fouler" giving up the ball and backing off; the other results in an ejection or kick out. Ejections are usually given if someone is being a little too aggressive; i.e. drowning or smacking someone. A player can only have 3 ejections before being majored and can not play for the rest of the game. If a player gets a brutality he or she is also not able to finish the game. An example of a brutality would be excessively cruising or intentionally punching someone. Water polo is a physically demanding activity; action is continuous, and players commonly swim 3 kilometers or more during four periods of play.
A defender will often foul the player with the ball as a tactic to disrupt the opponent's ball movement. Play continues uninterrupted in most cases, but the attacker must now pass the ball or continue swimming instead of taking a shot. (An exception allows players to quickly pick up the ball and shoot if fouled outside of the five meter mark.) However, as in ice hockey, a player caught committing a major foul, is sent out of the playing area with his team a man down for 20 seconds, but may return sooner if a goal is scored or his team regains possession. If the foul is judged to be brutal, the player is ejected for the remainder of the game, with substitution by another teammate after four minutes have elapsed. A player, coach or spectator can also be ejected for arguing with the referees. During a man up situation resulting from an ejection foul, the attacking team can expect to score by passing around to move the goalkeeper out of position. A player that has been ejected three times must sit out the whole match with substitution.
Officials.
The game of water polo requires numerous officials. The four main categories are: referee, secretary, timekeeper and goal judge. These can again be qualified into two broader categories: game officials and table officials.
Game officials.
The game officials in water polo are the referee (or referees) and the goal judges. Game officials are broadly responsible for ensuring the game runs smoothly and that correct and fair decisions relating to the game are made.
Referee.
The referees have ultimate power over decisions relating to the game, even (if necessary) overruling decisions from goal judges, secretaries or timekeepers. They have the responsibility of signalling fouls (minor, major, brutality and misconduct), goals, penalties, timeouts, start of play, end of play (to an extent), restart of play, neutral balls, corner balls and goal throws. He or she must attempt to keep all of the rules of water polo to the governing body they are using.
There are always either one or two referees in a game of water polo. At a higher level, two referees are virtually always used, but at lower levels and/or if there are limited available referees, a referee may officiate the game without another.
Originally, referees used flags to indicate possession, but these were replaced in 1997 with hand gestures.
When goal judges are not present or available, the referee(s) my take their place in that they have the decision as to whether the ball has crossed the line etc.
When there are two referees, the ball may be called out of the water for the referees to confer about an appropriate decision and come to an agreement.
Referees have dress codes at higher (and sometimes lower) levels of water polo, and are expected to abide by this. Often, the dress code is all white, but some associations variate this slightly.
Referees have a variety of methods for signalling fouls, most of which are done by hand and with the aid of a whistle. The purpose of these signals are to inform players, coaches, spectators and other officials of the decision being made, with sometimes information as to why the decision has been awarded.
Goal judge.
The goal judges are responsible for several parts of the game. These include: signalling when a goal is scored, signalling corner throws, improper re-entry (after an exclusion), to signal when play can start (at the beginning of quarters) and to signal improper restart at the beginning of quarters.
The goal judge is situated (normally sat) perfectly in line with the goal line - one at either end and usually on opposite sides. They remain seated throughout the game.
In practice goal judges are very rarely used at lower levels, but are compulsory at most international matches. Like referees, they will normally have a dress code which they are expected to abide by.
Previously, goal judges would use a red flag to indicate a corner and a white flag for a goal throw, but these have since been replaced with hand gestures.
Table officials.
The table officials in water polo are the timekeeper(s) and the secretary/secretaries. They are overall responsible for the timings of the game and keeping correct information regarding the events of the game, as well as informing of the players of very specific information (notably to do with exclusions and exclusion fouls).
Timekeeper.
The timekeeper (or timekeepers) have varying responsibilities depending on the equipment available. Only one is required if no 30 second clock is being used, with two being required otherwise. (Shot clocks are supposed to be used, but sometimes due to unavailability games are played without them). In higher level matches sometimes there are even more than two timekeepers used.
Often (though not always), one timekeeper responsible for running the shot clock. This means that he or she resets it when necessary. When this is the case, the other timekeeper is often responsible for the game clock and score. If an electronic scoreboard and game clock is being used, the timekeeper will update the score when necessary, and has a similar job with the game clock. If not, then the timekeeper will manually time the periods with a stopwatch (or similar device) and alert the players when the period is over with a whistle. If an electronic scoreboard is used, a synthetically produced sound is often produced at the end of periods to alert other officials and players of the end of the period.
Timekeepers are essentially responsible for keeping record of: the current score (though this is done more officially by the secretary), the 30 second clock, the length of the quarters (at the end of each quarter they indicate this with a whistle blow if this is not done synthetically), the time of exclusion (and when re-entry is thus allowed), the length of timeouts, the length of time between periods and to signal (if not done synthetically) by whistle 30 seconds before the end of quarter or half time and 15 seconds before the end of a timeout. Timekeepers are also responsible for the last minute bell: a bell (or other device - can be audible) showing one minute remaining before full-time.
Basic skills.
Water polo is a team water activity requiring swimming skills including treading water or wrestling before turning back for the opposing team's possession. The front crawl stroke used in water polo differs from the usual swimming style in which water polo players swim with the head out of water at all times to observe the play. The arm stroke used is also a lot shorter and quicker and is used primarily to protect the ball. Backstroke is used by defending players to look for advancing opponents and by the goalie to track the ball after passing. Water polo backstroke differs from swimming backstroke; the player sits up a bit in the water, using eggbeater leg like motions with short arm strokes to the side instead of long arm strokes. This allows the player to see the play and quickly switch positions. It also allows the player to quickly catch a pass.
As all field players are only allowed to touch the ball with one hand at a time, they must develop the ability to catch and throw the ball with either hand and also the ability to catch a ball from any direction, including across the body using the momentum of the incoming ball. Experienced water polo players can catch and release a pass or shoot with a single motion. The size of the ball can overwhelm a small child's hand, making the sport more suitable for older children. There are also smaller balls that can be used by younger children when playing.
Positions.
There are seven players in the water from each team at one time. There are six players that play out and one goalkeeper. Unlike most common team sports, there is little positional play; field players will often fill several positions throughout the game as situations demand. These positions consist of the center (or hole set), the point (who also usually plays center back or hole defender), the two wings and the two flats. Players who are skilled in all of these positions on offensive or defensive are called utility players. Utility players tend to come off of the bench, though this is not absolute. Certain body types are more suited for particular positions, and left-handed players are especially coveted on the right-hand side of the field, allowing teams to launch 2-sided attacks.
Offense.
The offensive positions include: one center (a.k.a. two-meter offense, 2-meters, hole set, set, hole man, bucket, pit player or pit-man), two wings (located on or near the 2-meter), two drivers (also called "flats," located on or near the 5-meter), and one "point" (usually just behind the 5 meter), positioned farthest from the goal. The wings, drivers and point are often called the perimeter players; while the hole-set directs play. There is a typical numbering system for these positions in U.S. NCAA men's division one polo. Beginning with the offensive wing to the opposing goalies right side is called one. The flat in a counter clockwise from one is called two. Moving along in the same direction the point player is three, the next flat is four, the final wing is five, and the hole set is called six.
The most basic positional set up is known as a 3–3, so called because there are two lines in front of the opponent's goal. Another set up, used more by professional teams, is known as an "arc," "umbrella," or "mushroom"; perimeter players form the shape of an arc around the goal, with the hole set as the handle or stalk. Yet another option for offensive set is called a 4–2 or double hole; there are two center forward offensive players in front of the goal. Double hole is most often used in "man up" situations, or when the defense has only one skilled hole D, or to draw in a defender and then pass out to a perimeter player for a shot ("kick out").
The center sets up in front of the opposing team's goalie and scores the most individually (especially during lower level play where flats do not have the required strength to effectively shoot from outside or to penetrate and then pass to teammates like the point guard in basketball). The center's position nearest to the goal allows explosive shots from close-range ("step-out" or "roll-out", "sweep," or backhand shots).
Another, albeit less common offense (violation), is the "motion offense" in which two "weak-side" (to the right of the goal for right-handed players) perimeter players set up as a wing and a flat. The remaining four players swim in square pattern in which a player swims from the point to the hole and then out to the strong side wing. The wing moves to the flat and the flat to the point. The weak side wing and flat then control the tempo of play and try to make passes into the player driving towards the center who can then either shoot or pass. This form of offense is used when no dominate hole set is available, or the hole defense is too strong. It is also seen much more often in women's water polo where teams may lack a player of sufficient size or strength to set up in the center. The best advantage to this system is it makes man-coverage much more difficult for the defender and allows the offense to control the game tempo better once the players are "set up." The main drawback is this constant motion can be very tiring as well as somewhat predictable as to where the next pass is going to go.
Defense.
Defensive positions are often the same positionally, but just switched from offense to defense. For example, the centre forward or hole set, who directs the attack on offense, on defense is known as "hole D" (also known as set guard, hole guard, hole check, pit defense or two-meter defense), and guards the opposing team's center forward (also called the hole). Defense can be played man-to-man or in zones, such as a 2–4 (four defenders along the goal line). It can also be played as a combination of the two in what is known as an "M drop" defense, in which the point defender moves away ("sloughs off") his man into a zone in order to better defend the center position. In this defense, the two wing defenders split the area furthest from the goal, allowing them a clearer lane for the counter-attack if their team recovers the ball.
Goalie.
The goalkeeper is generally one of the more challenging positions not only in the sport of water polo, but in any sport. A goalie has to be able to jump out of the water, using little more than one's core and legs, and hold the vertical position without sinking into the water, all while tracking and anticipating a shot. The goal is 2.8 m2 in face area; the goalie should also be a master of fast, effective lateral movement in the water as well as lightning fast lunges out of the water to block a shot.
Another key job that the goalkeeper is responsible for is guiding and informing his or her defense of imposing threats and gaps in the defense, and making helpful observations to identify a gap in the defense that the defenders may or can not see. The goalkeeper is also the "quarterback", as he or she usually begins the offensive play. It is not unusual for a goalie to make the assisting pass to a goal on a break away.
The goalkeeper is given several privileges above those of the other players, but only if he or she is within the five meter area in front of his or her goal:
In general, a foul that would cause an ejection of a field player might only bring on a five meter shot on the goalie. The goalkeeper also has one limitation that other players do not have: he or she cannot cross the half-distance line. Also, if a goalie pushes the ball under water, it is not a turnover like with field players. It is a penalty shot, also called a 5-meter shot, or simply, a "5-meter".
Offense strategy.
Beginning of play.
At the start of each period, teams line up on their own goal line. The most common formation is for three players to go each side of the goal, while the goalkeeper stays in the goal. If the ball is to be thrown into the center of the pool, the sprinter will often start in the goal, while the goalkeeper starts either in the goal as well or to one side of the goal.
At the referee's whistle, both teams swim to midpoint of the field (known as the sprint or the swim-off) as the referee drops the ball on to the water. Depending on the rules being played, this is either on the referee's side of the pool or in the center. In international competitions the ball is normally placed in the middle of the pool and is supported with a floating ring. The first team to recover the ball becomes the attacker until a goal is scored or the defenders recover the ball.
Exceptionally, a foul may be given before either team reaches the ball. This usually occurs when a player uses the side to assist themselves gain a speed advantage (i.e. by pulling on the side to move faster). In such scenarios, the non-offending team receives a free throw from the half way line.
It is important to note that the swimoff occurs only at the start of periods. Thus it will either occur 2, 4 or 6 times in a match, depending on whether the match is in halves, quarters or in quarters and extends to extra time.
Restart after a goal.
After a goal is scored, the teams may line up anywhere within their own half of the pool. In practice, this is usually near the center of the pool. Play resumes when the referee signals for play to restart and the team not scoring the goal puts the ball in to play by passing it backwards to a teammate.
Advancing the ball.
When the offense takes possession of the ball, the strategy is to advance the ball down the field of play and to score a goal. Players can move the ball by throwing it to a teammate or swimming with the ball in front of them (dribbling). If an attacker uses his/her arm to push away a defending player and free up space for a pass or shot, the referee will rule a turnover and the defense will take possession of the ball. If an attacker advances inside the 2-meter line without the ball or before the ball is inside the 2-meter area, (s)he is ruled offside and the ball is turned over to the defense. This is often overlooked if the attacker is well to the side of the pool or when the ball is at the other side of the pool.
Setting the ball.
The key to the offense is to accurately pass (or "set") the ball into the center forward or hole set, positioned directly in front of the goal ("the hole"). Any field player may throw the hole set a "wet pass." A wet pass is one that hits the water just outside of the hole set's reach. A dry pass may also be used. This is where the hole set receives the ball directly in his hand and then attempts a shot at the cage. This pass is much more difficult because if the pass is not properly caught, the officials will be likely to call an offensive foul resulting in a change of ball possession. The hole set attempts to take possession of the ball [after a wet pass], to shoot at the goal, or to draw a foul from his defender. A minor foul is called if his defender (called the "hole D") attempts to impede movement before the hole set has possession. The referee indicates the foul with one short whistle blow and points one hand to the spot of the foul and the other hand in the direction of the attack of the team to whom the free throw has been awarded. The hole set then has a "reasonable amount of time" (typically about three seconds) to re-commence play by making a free pass to one of the other players. The defensive team cannot hinder the hole set until the free throw has been taken, but the hole set cannot shoot a goal once the foul has been awarded until the ball has been played by at least one other player. If the hole set attempts a goal without the free throw, the goal is not counted and the defense takes possession of the ball, unless the shot is made outside the 5-meter line. As soon as the hole set has a free pass, the other attacking players attempt to swim (or "drive") away from their defenders towards the goal. The players at the flat position will attempt to set a screen (also known as a pick) for the driver. If a driver gets free from a defender, the player calls for the pass from the hole set and attempts a shot at the goal.
Man-Up (6 on 5).
If a defender interferes with a free throw, holds or sinks an attacker who is not in possession or splashes water into the face of an opponent, the defensive player is excluded from the game for twenty seconds, known as a 'kick out' or an ejection. The attacking team typically positions 4 players on the 2 meter line, and 2 players on 5 meter line (4–2), passing the ball around until an open player attempts a shot. Other formations include a 3–3 (two lines of three attackers each) or arc (attackers make an arc in front of the goal and one offensive player sits in the 'hole' or 'pit' in front of the goal). The five defending players try to pressure the attackers, block shots and prevent a goal being scored for the 20 seconds while they are a player down. The other defenders can only block the ball with one hand to help the goalie. The defensive player is allowed to return immediately if the offense scores, or if the defense recovers the ball before the twenty seconds expires.
Five meter penalty.
If a defender commits a major foul within the five meter area that prevents a likely goal, the attacking team is awarded a penalty "throw" or shot. An attacking player lines up on the five meter line in front of the opposing goal. No other player may be in front of him or within 2 meters of his position. The defending goalkeeper must be between the goal posts. The referee signals with a whistle and by lowering his arm, and the player taking the penalty shot must immediately throw the ball with an uninterrupted motion toward the goal without pumping or faking. The shooter’s body can not at any time cross the 5 meter line until after the ball is released. If the shooter carries his body over the line and shoots the result is a turn over. If the shot does not score and the ball stays in play then the play continues. Penalty shots are often successful, with 63.7% of shots being scored from them.
Scoring.
A goal is scored if the ball "completely" passes between the goal posts and is underneath the crossbar. If a shot bounces off a goal post back into the field of play, the ball is rebounded by the players and the shot clock is reset. If the shot goes outside the goal and onto the deck (outside the field of play) then the ball is automatically recovered by the defense. If the goalie, however, is the last to touch the ball before it goes out of play behind the goal line, or if a defender purposely sends the ball out, then the offense receives the ball at the two meter line for a "corner throw" or "two meter" much like a corner kick in soccer or football. When the goalie blocks a shot, the defense may gain control of the ball, and make a long pass to a teammate who stayed on his offensive end of the pool when the rest of his team was defending. This is called "cherry-picking" or "sea gulling".
Overtime.
If the score is tied at the end of regulation play, a penalty shootout will determine the winner. Five players and a goalkeeper are chosen by the coaches of each team. A player cannot be chosen if he or she was ejected three times during the match. Players shoot from the 5 meter line alternately at either end of the pool in turn until all five have taken a shot. If the score is still tied, the same players shoot alternately until one team misses and the other scores. Overtime periods are common in tournament play because of the high level of skill of these superior teams. Before September 2013 teams would play two straight 3-minute periods.
Differing from FINA rules, for which there are no shootouts, teams play two three-minute overtime periods in American college varsity water polo, and if still tied play three-minute sudden death periods until a team scores a goal and wins the game.
American High School water polo plays overtime as a "sudden death" period of a specified time limit. If this results in a tie, the teams engage in a shootout as described in FINA rules above.
Defense strategy.
On defense, the players work to regain possession of the ball and to prevent a goal in their own net. The defense attempts to knock away or steal the ball from the offense or to commit a foul in order to stop an offensive player from taking a goal shot. The defender attempts to stay between the attacker and the goal, a position known as "inside water".
Advantage rule.
If an offensive player, such as the hole set (center forward), has possession of the ball in front of the goal, the defensive player tries to steal the ball or to keep the center from shooting or passing. If the defender cannot achieve these aims, he may commit a foul intentionally. The hole set then is given a free throw but must pass off the ball to another offensive player, rather than making a direct shot at the goal. Defensive perimeter players may also intentionally cause a minor foul and then move toward the goal, away from their attacker, who must take a free throw. This technique, called sloughing, allows the defense an opportunity to double-team the hole set and possibly steal the inbound pass. The referee may refrain from declaring a foul, if in his judgment this would give the advantage to the offender's team. This is known as the "Advantage Rule".
Fouls.
Ordinary fouls occur when a player impedes or otherwise prevents the free movement of an opponent who is not holding the ball. The most common is when a player reaches over the shoulder of an opponent in order to knock the ball away while in the process hindering the opponent. Offensive players may be called for a foul by pushing off a defender to provide space for a pass or shot. The referee indicates the foul with one short whistle blow and points one hand in the direction of the attacking team (standing roughly in line with the position of the foul), who retain possession. The attacker must make a free pass without undue delay to another offensive player. If the foul has been committed outside the 5-meter line, the offensive player may also attempt a direct shot on goal, but the shot must be taken immediately and in one continuous motion. Because of this rule the hole set will often set up at or beyond the five meter mark hoping to get a foul, shoot, and score. If the offensive player fakes a shot and then shoots the ball, it is considered a turnover. If the same defender repetitively makes minor fouls, referees will exclude that player for 20 seconds. To avoid an ejection, the hole defender may foul twice, and then have a wing defender switch with him so that the defense can continue to foul the hole man without provoking an exclusion foul. The rule was altered to allow repeated fouls without exclusions, but is often still enforced by referees.
Major fouls (exclusion fouls) are committed when the defensive player holds, sinks or pulls back the offensive player away from the ball before the offensive player has had a chance to take possession of the ball. This includes dunking ("sinking" in FINA rules), intentional splashing, pulling back, swimming on the other player's back,stopping the other player from swimming or otherwise preventing the offensive player from preserving his advantage. A referee signals a major foul by two short whistle bursts and indicates that the player must leave the field of play and move to the penalty area for twenty seconds. The referee will first point to the player who commits the foul and will blow the whistle, then they will point to the ejection corner and blow the whistle again. The player must move to the penalty area without impacting the natural game play. If the player does not leave the field of play, the player will be kicked out for the remaining time of the game with substitution. The remaining five defenders, to cover the six attackers on a "man up" situation, usually set up in a zone defense in front of their goal. The attacking team can expect to score, by adopting a 4–2 or 3–3 formation, and moving the goalkeeper out of position. A player that has been ejected three times must sit out the whole match with substitution, much like the six personal fouls in basketball.
Drawing the ejection (forcing defense to commit a major foul) occurs when an offensive player takes advantage of a defensive player by using body position and/or grabbing on their wrists to make it appear as though the defensive player is committing a "major foul", therefore resulting in the ejection of that player and gaining a 6 on 5 advantage. Another common way to draw an ejection is by staggering stroke while being chased to make it appear as though the defensive player is pulling the swimmer back.
Brutality fouls A brutality is called when a player kicks or strikes an opponent or official with malicious intent. The strike must make contact with the player for a brutality to be called, and must be with intent to injure. Otherwise the player is punished with a misconduct foul, with substitution allowed after 20 seconds or a change of position. The player who is charged with a brutality is excluded from the game for 4 minutes, and the team is forced to play with one less player than the other team for that duration. In addition to the exclusion a penalty shot is also awarded to the opposing team, if the foul occurs during actual play. Previously, the team who was charged with a brutality would be required to play the remainder of the game with one less player, similar to a red card awarded in football. All brutalities have to be reported by officials and further actions may be taken by the relevant governing body. These actions could include more games added onto the one game suspension.
A misconduct foul is an unsportsmanlike act. For unacceptable language, violence or persistent fouls, taking part in the game after being excluded or showing disrespect, a player is ejected for the remainder of the game with substitution after 20 seconds has elapsed. There are two kinds of misconduct fouls that a player can incur. If a player physically assaults another player and the referee deems it not to be severe enough to warrant a charge of brutality, the lesser charge being Misconduct-Violence can be applied. If the incident does not involve physical (or attempted) contact, the referee can impose a Misconduct charge. In most competitions Misconduct-Violence carries heavier sanctions than Misconduct.
A penalty shot or 5-meter is awarded when a major foul is committed inside the 5-meter line and a probable goal was prevented by the foul. This is usually awarded if the defensive player in on another players back. This usually means that the offensive player is in front of and facing the goal. The penalty shot is attempted from 5 meters. Any defenders flanking the player taking the shot must be no closer than 2 meters. The goalkeeper must be on the goal line. In high school rules, the goalie must keep their hips even with the goal line. They are allowed to lean their upper body over in order to kick up higher. The referee blows the whistle and the player must shoot immediately.
Goalkeeper.
Even with good backup from the rest of the defenders, stopping attacks can prove very difficult if the goalkeeper remains in the middle of the goal. The most defensible position is along a semicircular line connecting the goalposts and extending out in the center. Depending on the ball carrier's location, the goalie is positioned along that semicircle roughly a meter out of the goal to reduce the attacker's shooting angle. The goalkeeper stops using his or her hands to tread water once the opponent enters at about the 7 meter mark and starts treading water much harder, elevating the body, arms ready for the block. Finally the goalie tries to block the ball down, which is often hard for the longer reaches, but prevents an offensive rebound and second shot. As is the case with other defensive players, a goalkeeper who aggressively fouls an attacker in position to score can be charged with a penalty shot for the other team. The goalkeeper can also be ejected for twenty seconds if a major foul is committed. Also inside the five meter mark, the goalie can swing at the ball with a closed fist without being penalized.
Ball handling skills.
When passing or shooting, the hips of the player should line up in the direction in which the ball is thrown. When passing, shooting or receiving a ball, the player rotates the whole of the upper body, using egg-beater which is the circling of feet under water to keep the lower body in the same position, then releasing the ball with hips lined up in the direction of the throw. For extra accuracy and speed when releasing the ball, a player uses body momentum to follow through at the end of the throw. Only one hand may come in contact with the ball at any time.
Picking up the ball.
Picking up the ball is an essential part to any water polo player. It is what is needed for almost all shots and passing the ball. When picking up the ball, it is essential that the fingers and thumb are distributed over the mass of the ball to get a grip. The player should be faced away from his or her opponent, as it is very easy to knock the ball out of the hand of a player who is holding the ball. There are two methods of picking up the ball: under water and on top of water. In the under water picking up method, the ball is picked up from underneath the water. In the on top of water picking up method, the player's hand goes on top of the ball. This is the method most often used for shooting, as it allows the player to be briefly lifted out of the water, but other players may put the ball under, giving their team a free throw.
Passing.
There are two basic passes in water polo: the "dry" pass and the "wet" pass.
Dry passing.
The passing to a field position player, a dry pass (meaning the ball does not touch the water) is thrown a few inches above the head of the catching player and to the left or right side depending on the receiver's dominant hand. The dry pass allows for optimal speed when passing from player to player, who do not have to pick the ball up out of the water to throw. A fluid motion between catching and throwing is the goal. An expert thrower's hand creates back spin, making the ball easier to catch. In order for the player to catch the ball above their head, they must egg beater harder which brings their body higher out of the water.
Wet passing.
The wet pass is a deliberate pass into the water (thus not caught in the hand). This is usually done when making a pass into the hole set. To make a successful wet pass, the ball lands just out of reach of the offensive player and defensive team. The hole set can then lunge towards the ball and out of the water to make a shot or pass. This is a very effective offensive strategy if a team has a strong hole set. The only thing the passer must look out for is a possible double-team on the hole set. If that happens, the player must look for an open player or pass the ball closer to the hole set to avoid a turnover.
Also there are about three types of set goals. First is the sweep. The sweep shot is where an outside rim player passes the ball wet into set. Then the set player will reach out for the ball while his/her hips are pointing towards the goal; the player will then come out with their arm straight will aim towards the high corner of the net and fire the ball.
Shooting.
Any part of the body can be used to score a goal except for a clenched fist.
Shots usually succeed when the goalie is out of position. At long range from the goal, shots are easy for goalkeepers to stop. If a shot is taken at a distance it is best to shoot cross cage and into one of the four corners
(SP), but closer ones are very difficult. Close-range shots tend to be harder to come by (since players close to the goalpost are usually under very great pressure), but in these situations usually a soft tap-in, with or without a feign, is enough to beat the goalkeeper. Close-range shots may come from the center-forward in open play, utilizing either quick backhand-shots, sweep-shots, layout or other creative shooting positions.
There are three basic outside water shooting techniques. The first is the power shot. Water polo players can generate ball speeds between 50–90 km/h (30–56 mph). The player propels his body out of the water and uses this to help him shoot the ball into the goal. It is powerful, but precise targeting is needed. If the shot is not in the corners, the ball can be more easily blocked by the goalie. Also there is the bounce shot or skip shot. The player throws the ball at an angle directly into the water. If done correctly and powerful enough, the ball can bounce into the goal. The bounce shot usually takes the goalie by surprise. Alternately, the ball can be thrown sidearm. This is a shot with much backspin. This can cause it, if done correctly and with enough spin, to slide along the surface of the water. The lob shot is a high arching shot intended to pass over the goalie's hands and into the goal. It can be effective if taken from an angle on either side of the goal post; this provides a large area behind the goalie into which the lob can drop on its downward arc. This shot can confuse the goalie and can force the goalie to eggbeater up too early and miss. If the does block it, he or she either has to lunge upwards and back, or go very high.
Outside water shots require a player to cease swimming, and usually occur outside the 2 meter zone. Players may perform an inside water shot, also known as a "wet shot". "Wet shots" are shot from water level by players who are currently in control of the ball. Wet shots are performed when the player has open water between him and the goal because the defender is behind him or her. A "wet shot" is valuable as the player does not have to stop and lift the ball up for a shot, making it easy for the trailing defender to steal it. Instead, the player can keep the ball in front of them while performing one of the following shots: The t-shot or bat shot is executed by scooping the ball with the non-dominant hand, "loading" the ball to the dominant hand, and propelling the ball forward. The pop shot is a quick shot executed by cupping the ball with the dominant hand from underneath the ball and releasing it, usually into a corner of the goal. This shot is timed with a player's swimming stroke, and should flow comfortably from the dribble. Other inside water shots include the screw shot, which can likewise be executed directly from the stroke, and a spring shot where the player pushes the ball slightly into the water (but avoiding a "ball under" foul) and then allows a sudden release. While beginning players will have difficulty integrating these shots into their stroke, resulting in weaker shots as compared to outside water shots, inside water shots by experienced players have sufficient force to skip past the goalkeeper. One thing the shooter must watch is how close they get to the goalie because they can come out of the goal and take the ball.
Another popular shot is the back hand. It is usually used by the 2-meter offense player. When the ball is set the hole keeps it in front of them until they reach for it and shooting it behind them while looking away from the goal. This shot is a hard one to make; their arm and elbow have to be in a perfect position in order for the ball to go towards the net, as the shot is taken "blindly". The center defender is neutralized in this shot, and the goalie is usually too close to the action and has no time to respond.
Judging exactly when to shoot can be tricky, as a blocked or a wide shot results in a turnover. This can be very risky in some situations, for example when a team has gained an advantage by swimming a counterattack. A failed shot in such a situation turns the advantage into a severe disadvantage, as the opponents left behind find themselves in numerical superiority and are thus presented with an excellent opportunity to score.
Baulking.
Baulking (also known as hezie or hesitation shot or "'pump fake'" or dummying in the UK) is a feinting tactic for outside water shots where the player gets in position to shoot but stops halfway through. This puts the defense on edge, causes the defenders to stand lower and lower in the water as their legs fatigue, and partially immobilizes the goalie by wasting his blocking lunge. This can be repeated until the player decides to release the ball. A good baulk takes a great amount of hand/arm and leg strength to maintain a high position in the water and the ball aloft in the shooting stance. The goalkeeper is particularly vulnerable to baulking as he must extend both his arms wide out of the water, which is intended to make him/her appear bigger and more imposing, thus, more difficult to beat. However, this places a massive strain on the goalie's legs, which are working in a rapid eggbeater motion. This causes the keeper to tire quicker as it is assumed a shot is imminent, thereby making them easier to beat.
Swimming with the ball.
Swimming with the ball might be the easiest way of advancing the ball down the pool when no other teammates are open for a pass. When swimming with the ball, it is important that the player keeps their elbows high in order to stop opposing players from gaining possession of the ball as well as keeping their head out of the water to see the rest of the pool and make the appropriate play. The ball should ride in the wake that comes off the chest of the player and they should use their arms to keep the ball in front of them. Players can also hold the ball in their hand and swim backstroke.
Injuries.
Water polo is a contact sport, with little protective gear besides swim suits and caps with ear protectors. Among the most common serious injuries are those affecting the head and shoulders. Those induced to the head are usually caused by elbows or the ball itself. One case would be when the defense guards the offense, the defense are right behind offense trying to steal the ball or trying to stop the ball from scoring or being passed. So as a result of the offense trying to shake off the defense to either score or pass the ball, a lot of elbowing and forceful removal from the defensive grab is needed. Many times the head being the main body part out of the water is injured in such a way. Many times these injuries are intentional and can sometimes anger many players to take revenge. Another common injury would be in the shoulder. Throwing or shooting the ball with a "cold arm" can strain the shoulder if not warmed up properly. Also occasionally, the defensive player will sometimes pull the arm to foul the offensive player. This can also injure the shoulder. With the arm, fingers are also usually harmed, due to not catching the ball right or blocking the ball. Many sprained fingers or on a more serious scale, fractured fingers have resulted from water polo. Some of the most injured players on the field are the goalies. They have to endure the ball thrown at them at a fast speed and are expected to "throw it down" to prevent the ball from going into the goal and scoring. When blocking shots the ball can hit the fingers instead of the whole hand causing fractures and strains. Goalies have also been known to suffer nosebleeds. Other injuries take place underwater as many things can not be seen from above the surface and not much padding is used to protect the players.
While playing one major injury that can occur is a labrum tear in the shoulder. The labrum is the cartilage that extends the glenohumeral joint of the shoulder which helps in stabilizing the shoulder. A labral tear can result from activities such as falling wrong, lifting heavy objects, or any other strong force running through the shoulder. Such forces exist in water polo from the continuous stresses of swimming as well as the forces caused from throwing the ball and/or having the pass or shot blocked. This is evident as labral tears are commonly found in people who participate in throwing sports. Tearing the labrum will result in the weakness of the arm. The continued weakening or injury of the labrum can ultimately cause the joint to become so weak that subluxation or dislocation of the shoulder can occur. 
Sunburn is a common minor injury in outdoor matches. The irritation of the sunburn can be restrictive because of the sheer amount of movement involved in the sport. Players will often neglect applying sunscreen as this will impair the player's ability to grip the ball and rapidly deteriorate the ball's physical grip due to the oily nature of sunscreen. Having large amounts of sunscreen on during an official match is banned by FINA and most other state/national governing bodies.
Eye irritation from pool chlorine is also common because players cannot wear goggles. They are regarded as a safety hazard because they may cause cuts, bruises or suction injuries during player-to-player contact or if the player is hit in the face by the ball.
Variations.
Inner tube water polo is a style of water polo in which players, excluding the goalkeeper, are required to float in inner tubes. By floating in an inner tube players expend less energy than traditional water polo players, not having to tread water. This allows casual players to enjoy water polo without undertaking the intense conditioning required for conventional water polo.
Surf polo, another variation of water polo, is played on surfboards. First played on the beaches of Waikiki in Hawaii in the 1930s and 1940s, it is credited to Louis Kahanamoku, Duke Kahanamoku's brother.
Canoe polo or kayak polo is one of the eight disciplines of canoeing pursued in the UK, known simply as "polo" by its aficionados. Polo combines paddling and ball handling skills with a contact team game, where tactics and positional play are as important as the speed and fitness of the individual athletes.
Water polo equipment.
Little player equipment is needed to play water polo. Items required in water polo include:
Major competitions.
Men's water polo at the Olympics was the first team sport introduced at the 1900 games, along with cricket, rugby, football, polo (with horses), rowing and tug of war. Women's water polo became an Olympic sport at the 2000 Sydney Olympic Games after political protests from the Australian women's team.
The most famous water polo match in history is probably the "Blood in the Water match", a 1956 Summer Olympics semi-final match between Hungary and the Soviet Union. As the athletes left for the games, the Hungarian revolution began, and the Soviet army crushed the uprising. The Hungarians defeated the Soviets 4–0 before the game was called off in the final minute to prevent angry Hungarians in the crowd reacting to Valentin Prokopov punching Ervin Zador.
Every 2 to 4 years since 1973, a men's Water Polo World Championship is organized within the FINA World Aquatics Championships. Women's water polo was added in 1986. A second tournament series, the FINA Water Polo World Cup, has been held every other year since 1979. In 2002, FINA organized the sport's first international league, the FINA Water Polo World League.
There is also a European Water Polo Championship that is held every other year.
Professional water polo is played in many southern and eastern European countries like Croatia, Hungary, Italy, Russia, Serbia, Spain, etc. with the LEN Euroleague tournament played amongst the best teams.
References.
PhysioAdvisor

</doc>
<doc id="47427" url="http://en.wikipedia.org/wiki?curid=47427" title="Atlas (moon)">
Atlas (moon)

Atlas is an inner satellite of Saturn.
Atlas was discovered by Richard Terrile in 1980 (some time before November 12) from Voyager photos and was designated S/1980 S 28. In 1983 it was officially named after Atlas of Greek mythology, because it "holds the rings on its shoulders" like the Titan Atlas held the sky up above the Earth. It is also designated Saturn XV.
Atlas is the closest satellite to the sharp outer edge of the A ring, and was long thought to be a shepherd satellite for this ring. However, now it is known that the outer edge of the ring is instead maintained by a 7:6 orbital resonance with the larger but more distant moons Janus and Epimetheus. In 2004 a faint, thin ring, temporarily designated R/2004 S 1, was discovered in the Atlantean orbit.
High-resolution images taken in June 2005 by "Cassini" revealed Atlas to be have a roughly spherical centre surrounded by a large, smooth equatorial ridge. The most likely explanation for this unusual and prominent structure is that ring material swept up by the moon accumulates on the moon, with a strong preference for the equator due to the ring's thinness. In fact, the size of the equatorial ridge is comparable with the expected Roche lobe of the moon. This would mean that for any additional particles impacting the equator, the centrifugal force will nearly overcome the tiny Atlantean gravity, and they will likely be lost.
Atlas is significantly perturbed by Prometheus and to a lesser degree by Pandora, leading to excursions in longitude of up to 600 km (~0.25°) away from the precessing Keplerian orbit with a rough period of about 3 years. Since the orbits of Prometheus and Pandora are chaotic, it is suspected that Atlas's may be as well.
References.
Citations
Sources
External links.
Listen to this article ()
This audio file was created from a revision of the "Atlas (moon)" article dated 2010-01-13, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="47428" url="http://en.wikipedia.org/wiki?curid=47428" title="Atlas (mythology)">
Atlas (mythology)

In Greek mythology, Atlas (; Ancient Greek: Ἄτλας) was the primordial Titan who held up the celestial spheres. He is also the titan of astronomy and navigation. Although associated with various places, he became commonly identified with the Atlas Mountains in northwest Africa (Modern-day Morocco, Algeria and Tunisia). Atlas was the son of the Titan Iapetus and the Oceanid Asia or Clymene. According to the ancient poet Hesiod Atlas stood at the ends of the earth towards the west.
In contexts where a Titan and a Titaness are assigned each of the seven planetary powers, Atlas is paired with Phoebe and governs the moon.
Hyginus emphasises the primordial nature of Atlas by making him the son of Aether and Gaia.
The first part of the term "Atlantic Ocean" refers to "Sea of Atlas", the term "Atlantis" refers to "island of Atlas".
Etymology.
The etymology of the name "Atlas" is uncertain. Virgil took pleasure in translating etymologies of Greek names by combining them with adjectives that explained them: for Atlas his adjective is "durus", "hard, enduring", which suggested to George Doig that Virgil was aware of the Greek τλῆναι "to endure"; Doig offers the further possibility that Virgil was aware of Strabo's remark that the native North African name for this mountain was "Douris". Since the Atlas mountains rise in the region inhabited by Berbers, it has been suggested that the name might be taken from one of the Berber, specifically "ádrār" 'mountain'.
Traditionally historical linguists etymologize the Ancient Greek word Ἄτλας (genitive: Ἄτλαντος) as comprised from copulative α- and the Proto-Indo-European root "*telh₂-" 'to uphold, support' (whence also τλῆναι), and which was later reshaped to an nt-stem. However, Robert Beekes argues that it cannot be expected that this ancient Titan carries an Indo-European name, and that we're rather dealing with the word of Pre-Greek origin which often end in "-ant".
Punishment.
Atlas and his brother Menoetius sided with the Titans in their war against the Olympians, the Titanomachy. When the Titans were defeated, many of them (including Menoetius) were confined to Tartarus, but Zeus condemned Atlas to stand at the western edge of Gaia (the Earth) and hold up The Heavens on his shoulders, to prevent the two from resuming their primordial embrace. Thus, he was "Atlas Telamon", "enduring Atlas," and became a doublet of Coeus, the embodiment of the celestial axis around which the heavens revolve.
A common misconception today is that Atlas was forced to hold the Earth on his shoulders, but Classical art shows Atlas holding the celestial spheres, not a globe; the solidity of the marble globe born by the renowned Farnese Atlas may have aided the conflation, reinforced in the 16th century by the developing usage of "atlas" to describe a corpus of terrestrial maps.
Variations.
In a late story, a giant named Atlas tried to drive a wandering Perseus from the place where the Atlas mountains now stand. In Ovid's telling, Perseus revealed Medusa's head, turning Atlas to stone (those very mountains) when Atlas tried to drive him away, because Perseus, who went there accidentally and asked Atlas for hospitality, named himself a son of Zeus and a prophecy said that a son of Zeus would steal the golden apples from Atlas' orchard. As is not uncommon in myth, this account cannot be reconciled with the far more common stories of Atlas' dealings with Heracles, another son of Zeus, who was Perseus' great-grandson and who sought for the golden apples.
According to Plato, the first king of Atlantis was also named Atlas, but that Atlas was a son of Poseidon and the mortal woman Cleito. A euhemerist origin for Atlas was as a legendary Atlas, king of Mauretania, an expert astronomer.
Encounter with Heracles.
One of the Twelve Labors of the hero Heracles was to fetch some of the golden apples which grow in Hera's garden, tended by Atlas' daughters, the Hesperides, and guarded by the dragon Ladon. Heracles went to Atlas and offered to hold up the heavens while Atlas got the apples from his daughters.
Upon his return with the apples, however, Atlas attempted to trick Heracles into carrying the sky permanently by offering to deliver the apples himself, as anyone who purposely took the burden must carry it forever, or until someone else took it away. Heracles, suspecting Atlas did not intend to return, pretended to agree to Atlas' offer, asking only that Atlas take the sky again for a few minutes so Heracles could rearrange his cloak as padding on his shoulders. When Atlas set down the apples and took the heavens upon his shoulders again, Heracles took the apples and ran away.
In some versions, Heracles instead built the two great Pillars of Hercules to hold the sky away from the earth, liberating Atlas much as he liberated Prometheus.
Etruscan Aril.
The identifying name "Aril" is inscribed on two 5th-century Etruscan bronze items, a mirror from Vulci and a ring from an unknown site. Both objects depict the encounter with Atlas of Hercle, the Etruscan Heracles, identified by the inscription; they represent rare instances where a figure from Greek mythology is imported into Etruscan mythology, but the name is not. The Etruscan name "aril" is etymologically independent.
Children.
Sources describe Atlas as the father, by different goddesses, of numerous children, mostly daughters. Some of these are assigned conflicting or overlapping identities or parentage in different sources. 
Cultural influence.
Atlas' best-known cultural association is in cartography. The first publisher to associate the Titan Atlas with a group of maps was the print-seller Antonio Lafreri, on the engraved title-page he applied to his "ad hoc" assemblages of maps, "Tavole Moderne Di Geografia De La Maggior Parte Del Mondo Di Diversi Autori" (1572); however, he did not use the word "atlas" in the title of his work, an innovation of Gerardus Mercator, who dedicated his "atlas" specifically "to honour the Titan, Atlas, King of Mauretania, a learned philosopher, mathematician, and astronomer"; he actually depicted the astronomer king.

</doc>
<doc id="47429" url="http://en.wikipedia.org/wiki?curid=47429" title="Atlas">
Atlas

An atlas is a collection of maps; it is typically a map of Earth or a region of Earth, but there are atlases of the other planets (and their satellites) in the Solar System. Furthermore atlases of anatomy exist, mapping out the human body or other organisms. Atlases have traditionally been bound into book form, but today many atlases are in multimedia formats. In addition to presenting geographic features and political boundaries, many atlases often feature geopolitical, social, religious and economic statistics. They also have information about the map and places in it.
Etymology.
The word atlas dates from 1636, first in reference to the English translation of "Atlas, sive cosmographicae meditationes de fabrica mundi" (1585) by Flemish geographer Gerhardus Mercator, who might have been the first to use this word in this way. A picture of the Titan Atlas holding up the world appeared on the frontispiece of this and other early map collections.
Types.
A "travel atlas" is made for easy use during travel, and often has spiral bindings so it may be folded flat. It has maps at a large zoom so the maps can be reviewed easily. A travel atlas may also be referred to as a "road map".
A "desk atlas" is made similar to a reference book. It may be in hardback or paperback form.
Modern atlas.
With the coming of the global market, publishers in different countries can reprint maps from places made elsewhere. This means that the place names on the maps often use the designations or abbreviations of the language of the country in which the feature is located, to serve the widest market. For example, islands near Russia have the abbreviation "O." for "ostrov", not "I." for "island". This practice differs from what is standard for any given language, and it reaches its extremity concerning transliterations from other languages. In particular, German mapmakers use the transliterations from Cyrillic developed by the Czechs, which are hardly used in English-speaking countries.
Selected general atlases.
Some cartographically or commercially important atlases include the following:

</doc>
<doc id="47430" url="http://en.wikipedia.org/wiki?curid=47430" title="Atlas (anatomy)">
Atlas (anatomy)

In anatomy, the atlas (C1) is the most superior (first) cervical vertebra of the spine.
It is named for the Atlas of Greek mythology, because it supports the globe of the head.
The atlas is the topmost vertebra and with the axis forms the joint connecting the skull and spine. The atlas and axis are specialized to allow a greater range of motion than normal vertebrae. They are responsible for the nodding and rotation movements of the head.
The atlanto-occipital joint allows the head to nod up and down on the vertebral column.
The dens acts as a pivot that allows the atlas and attached head to rotate on the axis, side to side.
The atlas's chief peculiarity is that it has no body. It is ring-like and consists of an anterior and a posterior arch and two lateral masses.
The atlas and axis are important neurologically because the brain stem extends down to the axis.
Structure.
Anterior arch.
The anterior arch forms about one-fifth of the ring: its anterior surface is convex, and presents at its center the anterior tubercle for the attachment of the "Longus colli" muscles and the anterior longitudinal ligament; posteriorly it is concave, and marked by a smooth, oval or circular facet ("fovea dentis"), for articulation with the odontoid process (dens) of the axis.
The upper and lower borders respectively give attachment to the anterior atlantooccipital membrane and the anterior atlantoaxial ligament; the former connects it with the occipital bone above, and the latter with the axis below.
Posterior arch.
The posterior arch forms about two-fifths of the circumference of the ring: it ends behind in the posterior tubercle, which is the rudiment of a spinous process and gives origin to the "Recti capitis posteriores minores" and the ligamentum nuchae. The diminutive size of this process prevents any interference with the movements between the atlas and the skull.
The posterior part of the arch presents above and behind a rounded edge for the attachment of the posterior atlantooccipital membrane, while immediately behind each superior articular process is a groove ("sulcus arteriae vertebralis"), sometimes converted into a foramen by a delicate bony spiculum which arches backward from the posterior end of the superior articular process.
This groove represents the superior vertebral notch, and serves for the transmission of the vertebral artery, which, after ascending through the foramen in the transverse process, winds around the lateral mass in a direction backward and medially; it also transmits the suboccipital nerve (first spinal nerve). In a common anatomic variant the vertebral artery passes through an arcuate foramen.
On the under surface of the posterior arch, behind the articular facets, are two shallow grooves, the inferior vertebral notches. The lower border gives attachment to the posterior atlantoaxial ligament, which connects it with the axis.
Lateral masses.
The lateral masses are the most bulky and solid parts of the atlas, in order to support the weight of the head.
Each carries two articular facets, a superior and an inferior.
Vertebral foramen.
Just below the medial margin of each superior facet is a small tubercle, for the attachment of the transverse atlantal ligament which stretches across the ring of the atlas and divides the vertebral foramen into two unequal parts:
This part of the vertebral canal is of considerable size, much greater than is required for the accommodation of the spinal cord.
Transverse processes.
The transverse processes are large; they project laterally and downward from the lateral masses, and serve for the attachment of muscles which assist in rotating the head. They are long, and their anterior and posterior tubercles are fused into one mass; the foramen transversarium is directed from below, upward and backward.
Development.
The atlas is usually ossified from three centers.
Of these, one appears in each lateral mass about the seventh week of fetal life, and extends backward; at birth, these portions of bone are separated from one another behind by a narrow interval filled with cartilage.
Between the third and fourth years they unite either directly or through the medium of a separate center developed in the cartilage.
At birth, the anterior arch consists of cartilage; in this a separate center appears about the end of the first year after birth, and joins the lateral masses from the sixth to the eighth year.
The lines of union extend across the anterior portions of the superior articular facets.
Occasionally there is no separate center, the anterior arch being formed by the forward extension and ultimate junction of the two lateral masses; sometimes this arch is ossified from two centers, one on either side of the middle line.
Function.
Muscular attachments.
Transverse processes.
Upper surface:
Interior and dorsal part:
Lower surface:
Posterior tubercle.
Upper surface:
Lower surface:
Clinical significance.
A break in the first vertebra is referred to as a Jefferson fracture.
References.
"This article incorporates text in the public domain from the 20th edition of Gray's Anatomy (1918)"

</doc>
<doc id="47431" url="http://en.wikipedia.org/wiki?curid=47431" title="Atlas Mountains">
Atlas Mountains

The Atlas Mountains is a mountain range which stretches across northwestern Africa extending about 2,500 km through Algeria, Morocco and Tunisia. The highest peak is Jebel Toubkal, with an elevation of 4167 m in southwestern Morocco. The Atlas ranges separate the Mediterranean and Atlantic coastlines from the Sahara Desert. The population of the Atlas Mountains is mainly Berbers. The terms for 'mountain' in some Berber languages are "adrar" and "adras", believed to be cognate with the toponym.
The mountains are home to a number of plant and animal species unique in Africa, often more like those of Europe; many of them are endangered and some have already gone extinct. Examples include the Barbary Macaque, the Atlas Bear (Africa's only species of bear; now extinct), the Barbary Leopard, the Barbary stag, Barbary Sheep, the Barbary Lion (extinct in the wild), the Atlas Mountain Badger, the North African Elephant (extinct), the African Aurochs (extinct), Cuvier's Gazelle, the Northern Bald Ibis, Dippers, the Atlas mountain viper, the Atlas Cedar, the European Black Pine, and the Algerian Oak.
Geology.
The basement rock of most of Africa was formed in the Precambrian and is much older than the Atlas mountains lying in Africa. The Atlas formed during three subsequent phases of Earth's history.
The first tectonic deformation phase involves only the Anti-Atlas, which was formed in the Paleozoic Era (~300 million years ago) as the result of continental collisions. North America, Europe and Africa were connected millions of years ago.
The Anti-Atlas mountains are believed to have originally been formed as part of Alleghenian orogeny. These mountains were formed when Africa and America collided, and were once a chain rivaling today's Himalayas. Today, the remains of this chain can be seen in the Fall Line region in the eastern United States. Some remnants can also be found in the later formed Appalachians in North America.
A second phase took place during the Mesozoic Era (before ~66 My) and consisted of a widespread extension of the Earth's crust that rifted and separated the continents mentioned above. This extension was responsible for the formation of many thick intracontinental sedimentary basins including the present Atlas. Most of the rocks forming the surface of the present High Atlas were deposited under the ocean at that time.
Finally, in the Paleogene and Neogene Periods (~66 million to ~1.8 million years ago), the mountain chains that today comprise the Atlas were uplifted as the land masses of Europe and Africa collided at the southern end of the Iberian peninsula. Such convergent tectonic boundaries occur where two plates slide towards each other forming a subduction zone (if one plate moves underneath the other) and/or a continental collision (when the two plates contain continental crust). In the case of the Africa-Europe collision, it is clear that tectonic convergence is partially responsible for the formation of the High Atlas, as well as for the closure of the Strait of Gibraltar and the formation of the Alps and the Pyrenees. However, there is a lack of evidence for the nature of the subduction in the Atlas region, or for the thickening of the Earth's crust generally associated with continental collisions. In fact, one of the most striking features of the Atlas to geologists is the relative small amount of crustal thickening and tectonic shortening despite the important altitude of the mountain range. Recent studies suggest that deep processes rooted in the Earth's mantle may have contributed to the uplift of the High and Middle Atlas.
Natural resources.
The Atlas are rich in natural resources. There are deposits of iron ore, lead ore, copper, silver, mercury, rock salt, phosphate, marble, anthracite coal, and natural gas among other resources.
Subranges of the Atlas Mountains.
The range can be divided into four general regions:
Middle Atlas range.
The Middle Atlas is a portion of the Atlas mountain range lying completely in Morocco. The Middle Atlas is the westernmost of three Atlas Mountains chains that define a large, plateaued basin extending eastward into Algeria. South of the Middle Atlas and separated by the Moulouya and Oum Er-Rbia rivers, the High Atlas stretches for 700 km with a succession of peaks among which ten reach above 4000 m. North of the Middle Atlas and separated by the Sebou River, the Rif mountains are an extension of the Baetic Cordillera (Baetic mountains, which include the Sierra Nevada) in the south of Spain.
High Atlas.
The High Atlas in central Morocco rises in the west at the Atlantic coast and stretches in an eastern direction to the Moroccan-Algerian border. At the Atlantic and to the southwest the range drops abruptly and makes an impressive transition to the coast and the Anti-Atlas range. To the north, in the direction of Marrakech, the range descends less abruptly.
On the heights of Ouarzazate the massif is cut through by the Draa valley which opens southward. Here, water runs in some places, forming clear basins. It is mainly inhabited by Berber people, who live in small villages and cultivate the high plains of Ourika Valley.
Near Barrage Cavagnac, there is a hydroelectric dam that has created the artificial lake Lalla Takerkoust. The lake serves also as a source for fish for the local fishermen.
The largest villages and towns of the area are Ouarzazate, Tahanaoute, Amizmiz, Imlil, Tin Mal and Ijoukak.
Anti-Atlas ranges.
The Anti-Atlas extends from the Atlantic Ocean in the southwest of Morocco toward the northeast to the heights of Ouarzazate and further east to the city of Tafilalt (altogether a distance of approximately 500 km). In the south it borders the Sahara. The easternmost point of the anti-Atlas is the Jbel Saghro range and its northern boundary is flanked by sections of the High Atlas range. It includes the Djebel Siroua, a massif of volcanic origin with the highest summit of the range at 3,304 m. The Jebel Bani is a much lower range running along the southern side of the Anti Atlas.
Saharan Atlas range.
The Saharan Atlas of Algeria is the eastern portion of the Atlas mountain range. Though not as high as the Grand Atlas, they are far more imposing than the Tell Atlas range that runs to the north of them and closer to the coast. The highest peak in the range is the 2236 m high Djebel Aissa. They mark the northern edge of the Sahara Desert. The mountains see some rainfall and are better suited to agriculture than the plateau region to the north. Today most of the population of the region are Berbers (Imazighen).
Tell Atlas range.
The Tell Atlas is a mountain chain over 1500 km in length, belonging to the Atlas mountain ranges and stretching from Morocco, through Algeria to Tunisia. It parallels the Mediterranean coast. Together with the Saharan Atlas to the south it forms the northernmost of two more or less parallel ranges which gradually approach one another towards the east, merging in "Eastern Algeria". At the western ends at the Middle Atlas range in Morocco. The area immediately to the south of this range is the high plateau of the Hautes Plaines, with lakes in the wet season and salt flats in the dry.
Aurès mountain range.
The Aurès Mountains of Algeria and Tunisia are the farthest eastern portion of the Atlas mountain range. The Aurès natural region is named after the range.

</doc>
<doc id="47433" url="http://en.wikipedia.org/wiki?curid=47433" title="Atlas (architecture)">
Atlas (architecture)

In classical European architecture, an atlas (also known as an atlant, or atlante or atlantid; plural atlantes) is a support sculpted in the form of a man, which may take the place of a column, a pier or a pilaster. The Roman term for such a sculptural support is telamon (plural telamones or telamons).
The term "atlantes" is the Greek plural of the name Atlas – the Titan who was forced to hold the sky on his shoulders for eternity. The alternative term, "telamones", also is derived from a later mythological hero, Telamon, one of the Argonauts, who was the father of Ajax. 
The caryatid is the female precursor of this architectural form in Greece, a woman standing in the place of each column or pillar. Caryatids are found at the treasuries at Delphi and the Erechtheion on the Acropolis at Athens for Athene. They usually are in an Ionic context and represented a ritual association with the goddesses worshiped within. 
Atlantes express extreme effort in their function, heads bent forward to support the weight of the structure above them across their shoulders, forearms often lifted to provide additional support, providing an architectural motif. 
Atlantes and caryatids were noted by the Roman late Republican architect Vitruvius, whose description of the structures, rather than surviving examples, transmitted the idea of atlantes to the Renaissance architectural vocabulary.
Origin.
Not only did the Caryatids precede them, but similar architectural figures already had been made in ancient Egypt out of monoliths. Atlantes originated in Greek Sicily and in Magna Graecia, southern Italy. The are fallen ones from the Early Classical Greek temple of Zeus, the "Olympeion", in Agrigento, Sicily. Atlantes, however, have played a more significant role in Mannerist and baroque architecture. 
During the eighteenth and nineteenth centuries, many buildings were built with glorious atlantes that look much like the Greek ones. Their selection from the two proposed designs—the other design using Caryatids—for the entrance of the Hermitage Museum that was built for Tsar Nicholas I of Russia made atlantes become even more fashionable. The portico of this building has ten enormous atlantes, approximately three times life-size, carved from Serdobol granite, which were designed by Johann Halbig and executed by the sculptor, Alexander Terebenev.

</doc>
<doc id="47434" url="http://en.wikipedia.org/wiki?curid=47434" title="Taiwan Capitalization Weighted Stock Index">
Taiwan Capitalization Weighted Stock Index

Taiwan Capitalization Weighted Stock Index (加權指數, abbr. TAIEX) is a stock market index for companies traded on the Taiwan Stock Exchange (TWSE). TAIEX covers all of the listed stocks excluding preferred stocks, full-delivery stocks and newly listed stocks, which are listed for less than one calendar month. It was first published in 1967 by TWSE with 1966 being the base year with a value of 100.

</doc>
<doc id="47436" url="http://en.wikipedia.org/wiki?curid=47436" title="Atlas (topology)">
Atlas (topology)

In mathematics, particularly topology, one describes 
a manifold using an atlas. An atlas consists of individual 
"charts" that, roughly speaking, describe individual regions
of the manifold. If the manifold is the surface of the Earth, 
then an atlas has its more common meaning. In general, 
the notion of atlas underlies the formal definition of a manifold.
Charts.
The definition of an atlas depends on the notion of a "chart". 
A chart for a topological space "M" (also called a coordinate chart, coordinate patch, coordinate map, or local frame) is a homeomorphism formula_1 from an open subset "U" of "M" to an open subset of Euclidean space. The chart is traditionally recorded as the ordered pair formula_2.
Formal definition of atlas.
An atlas for a topological space "M" is a collection formula_3 of charts on "M" such that 
formula_4. If the codomain of each chart is the "n"-dimensional Euclidean space and the atlas is connected, then "M" is said to be an "n"-dimensional manifold.
Maximal atlas.
The atlas containing all possible charts consistent with a given atlas is called the maximal atlas (i.e. an equivalence class containing that given atlas (under the already defined equivalence relation given in the previous paragraph)). Unlike an ordinary atlas, the maximal atlas of a given manifold is unique. Though it is useful for definitions, it is an abstract object and not used directly (e.g. in calculations). The completion of an atlas consists of the union of the atlas and all charts which yield an atlas of the manifold. That is, if we have an atlas formula_5 on a manifold formula_6, then the completion of the atlas consists of all those charts formula_7 such that formula_8. An atlas which is the same as its completion is a complete atlas. A complete atlas is a maximal atlas.
Transition maps.
formula_6
formula_10
formula_11
formula_12
formula_13
formula_14
formula_15
formula_16
formula_16
Two charts on a manifold
A transition map provides a way of comparing two charts of an atlas.
To make this comparison, we consider the composition of one chart
with the inverse of the other. This composition is not well-defined 
unless we restrict both charts to the intersection of their domains
of definition. (For example, if we have a chart of Europe and a chart of Russia, then we can compare these two charts on their overlap, namely the European part of Russia.)
To be more precise, suppose that formula_18 and formula_19 are two charts for a manifold "M" such that formula_20 is non-empty.
The transition map formula_21 is the map defined by
Note that since formula_23 and formula_24 are both homeomorphisms, the transition map formula_25 is also a homeomorphism.
More structure.
One often desires more structure on a manifold than simply the topological structure. For example, if one would like an unambiguous notion of differentiation of functions on a manifold, then it is necessary to construct an atlas whose transition functions are differentiable. Such a manifold is called differentiable. Given a differentiable manifold, one can unambiguously define the notion of tangent vectors and then directional derivatives.
If each transition function 
is a smooth map, then the atlas is called a 
smooth atlas, and the manifold itself is called smooth.
Alternatively, one could require that the transition maps 
have only "k" continuous derivatives in which case the atlas is 
said to be formula_26.
Very generally, if each transition function
belongs to a pseudo-group formula_27
of homeomorphisms of Euclidean space, 
then the atlas is called a formula_28-atlas.
References.
</dl>

</doc>
<doc id="47437" url="http://en.wikipedia.org/wiki?curid=47437" title="Pope Paul III">
Pope Paul III

Pope Paul III (Latin: "Paulus III"; 29 February 1468 – 10 November 1549), born Alessandro Farnese, was Pope from 13 October 1534 to his death in 1549.
He came to the papal throne in an era following the sack of Rome in 1527 and rife with uncertainties in the Catholic Church following the Protestant Reformation. During his pontificate, and in the spirit of the Counter-Reformation, new Catholic religious orders and societies, such as the Jesuits, the Barnabites, and the Congregation of the Oratory, attracted a popular following.
He convened the Council of Trent in 1545. He was a significant patron of the arts and employed nepotism to advance the power and fortunes of his family. It is to Pope Paul III that Nicolaus Copernicus dedicated "De revolutionibus orbium coelestium" ("On the Revolutions of the Celestial Spheres").
Biography.
Early life and career.
Born in 1468 at Canino, Latium (then part of the Papal States), Alessandro Farnese was the oldest son of Pier Luigi I Farnese, Signore di Montalto (1435–1487) and his wife Giovanna Caetani, a member of the Caetani family which had also produced Pope Boniface VIII. The Farnese family had prospered over the centuries but it was Alessandro’s ascendency to the papacy and his dedication to family interests which brought about the most significant increase in the family’s wealth and power.
Alessandro’s humanist education was at the University of Pisa and the court of Lorenzo de' Medici. Initially trained as an apostolic notary, he joined the Roman Curia in 1491 and in 1493 Pope Alexander VI appointed him Cardinal-Deacon of "Santi Cosma e Damiano". Farnese’s sister, Giulia was reputedly a mistress of Alexander VI and may have been instrumental in securing this appointment for her brother. For this reason, he was sometimes mockingly referred to as the "Borgia brother-in-law," just as Giulia was mocked as "the Bride of Christ." More disparagingly he was referred to as "Cardinal Fregnese" (translated as Cardinal Cunt). As bishop of Parma, he came under the influence of his vicar general, Bartolomeo Guidiccioni. This led to the future pope breaking off the relationship with his mistress and committing himself to reform in his Parma diocese. Under Pope Clement VII (1523–34) he became Cardinal Bishop of Ostia and dean of the College of Cardinals, and on the death of Clement VII in 1534, was elected as Pope Paul III.
Patron of the arts and family interests.
As a young cleric, Alessandro lived a notably dissolute life, taking for himself a mistress and having three sons and two daughters with her. By Silvia Ruffini, he fathered Pier Luigi Farnese, whom he created Duke of Parma; others included Ranuccio Farnese and Costanza Farnese. The elevation to the cardinalate of his grandsons, Alessandro Farnese, aged fourteen, and Guido Ascanio Sforza, aged sixteen, displeased the reform party and drew a protest from the emperor, but this was forgiven, when shortly after, he introduced into the Sacred College men of the calibre of Reginald Pole, Gasparo Contarini, Jacopo Sadoleto, and Giovanni Pietro Caraffa, who became Pope Paul IV.
One of the most significant artistic works of Paul's reign was the depiction of the Last Judgement by Michelangelo in the Sistine Chapel of the Vatican Palace. Although the work was commissioned by Paul III’s predecessor, it was finished in 1541.
As a cardinal, Alessandro had begun construction of a palace, the Palazzo Farnese, in central Rome. On his election to the papacy, the size and magnificence of this building programme was increased to reflect his change in status. The palace was initially designed by the architect Antonio da Sangallo the Younger, received further architectural refinement from Michelangelo, and was completed by Giacomo della Porta. Like other Farnese family buildings, the palace imposes its presence on its surroundings in an expression of the family’s power and wealth. Alessandro's Villa Farnese at Caprarola has a similar presence.
In 1546, after the death of Sangallo, Paul appointed the elderly Michelangelo to take over the supervision of the building of St. Peter's Basilica. Michelangelo was also commissioned by Paul to paint the 'Crucifixion of St. Peter' and the 'Conversion of St. Paul' (1542–50), Michelangelo's last frescoes, in the Pauline Chapel of the Vatican.
Paul III's artistic and architectural commissions were numerous and varied. The Venetian artist Titian painted a portrait of the Pope in 1543, and in 1546, the well-known portrait of Paul III with his grandsons Cardinal Alessandro Farnese and Ottavio Farnese, Duke of Parma. Both are now in the Capodimonte Museum, Naples. The military fortifications in Rome and the Papal States were strengthened during his reign. He had Michelangelo relocate the ancient bronze of the Emperor Marcus Aurelius to the Capitoline Hill, where it became the centerpiece to the Piazza del Campidoglio.
Paul III’s bronze tomb, executed by Guglielmo della Porta, is in St. Peter's.
Politics and religion during the papacy of Paul III.
The fourth pope during the period of the Reformation, Paul III became the first to take proactive reform measures in response to Protestantism. Soon after his elevation, 2 June 1536, Paul III summoned a general council to meet at Mantua in the following May; but the opposition of the Protestant princes and the refusal of the Duke of Mantua to assume the responsibility of maintaining order frustrated the project. Paul III first deferred for a year and then discarded the whole project.
In 1536, Paul III invited nine eminent prelates, distinguished by learning and piety alike, to act in committee and to report on the reformation and rebuilding of the Church. In 1537 they turned in their celebrated Consilium de emendenda ecclesia, exposing gross abuses in the Curia, in the church administration and public worship; and proffering many a bold and earnest word on behalf of abolishing such abuses. This report was printed not only at Rome, but at Strasburg and elsewhere.
But to the Protestants it seemed far from thorough; Martin Luther had his edition (1538) prefaced with a vignette showing the cardinals cleaning the Augean stable of the Roman Church with foxtails instead of brooms. Yet the Pope was in earnest when he took up the problem of reform. He clearly perceived that the emperor, Charles V would not rest until the problems were grappled in earnest, and a council was an unequivocal procedure that should leave no room for doubt of his own readiness to make changes. Yet it is clear that the "Concilium" bore no fruit in the actual situation, and that in Rome no results followed from the committee's recommendations.
On the other hand, serious political complications resulted. In order to vest his grandson Ottavio Farnese with the dukedom of Camerino, Paul forcibly wrested the same from the duke of Urbino (1540). He also incurred virtual war with his own subjects and vassals by the imposition of burdensome taxes. Perugia, renouncing its obedience, was besieged by Paul's son, Pier Luigi, and forfeited its freedom entirely on its surrender. The burghers of Colonna were duly vanquished, and Ascanio was banished (1541). After this the time seemed ripe for annihilating heresy.
In 1540, the Church officially recognized the young society forming about Ignatius of Loyola, (founder of the Society of Jesus).
The second visible stage in the process becomes marked by the institution, or reorganization, in 1542, of the Congregation of the Holy Office of the Inquisition (see Inquisition).
On another side, the Emperor was insisting that Rome should forward his designs toward a peaceable recovery of the German Protestants. Accordingly the Pope despatched Giovanni Morone (not yet a cardinal) as nuncio to Hagenau and Worms, in 1540; while, in 1541, Cardinal Gasparo Contarini took part in the adjustment proceedings at the Conference of Regensburg. It was Contarini who led to the stating of a definition in connection with the article of justification in which occurs the famous formula "by faith alone are we justified," with which was combined, however, the Roman Catholic doctrine of good works. At Rome, this definition was rejected in the consistory of 27 May, and Luther declared that he could accept it only provided the opposers would admit that hitherto they had taught differently from what was meant in the present instance.
Yet, even now, and particularly after the Regensburg Conference had proved in vain, the Emperor did not cease to insist on convening the council, the final result of his insistence being the Council of Trent, which, after several postponements, was finally convoked by the bull "Laetare Hierusalem", 15 March 1545.
Meanwhile, after the peace of Crespy (September 1544), the situation had so shaped itself that Emperor Charles V (1519–56) began to put down Protestantism by force. Pending the diet of 1545 in Worms, the emperor concluded a covenant of joint action with the papal legate, Cardinal Alessandro Farnese. Paul III was to aid in the projected war against the German Evangelical princes and estates. The prompt acquiescence of Paul III in the war project was probably grounded on personal motives. The moment now seemed opportune for him, since the Emperor was sufficiently preoccupied in the German realm, to acquire for his son Pier Luigi the duchies of Parma and Piacenza. Although these belonged to the Papal States, Paul III thought to overcome the reluctance of the Cardinals by exchanging the duchies for the less valuable domains of Camerino and Nepi. The Emperor agreed, because of his prospective compensation to the extent of 12,000 infantry, 500 mounted troops, and considerable sums of money.
In Germany the campaign began in the west, where Protestant movements had been at work in the archbishopric of Cologne since 1542. The Reformation was not a complete success there, because the city council and the majority of the chapter opposed it; whereas on 16 April 1546, Hermann of Wied was excommunicated, his rank forfeited, and, in February 1547, was compelled by the Emperor to abdicate.
In the meantime open warfare had begun against the Evangelical princes, estates, and cities allied in the Schmalkaldic League (see Philip of Hesse). By the close of 1546, Charles V succeeded in subjugating South Germany, while the victory at the Battle of Mühlberg, on 24 April 1547, established his imperial sovereignty everywhere in Germany and delivered into his hands the two leaders of the league.
But while north of the Alps, in virtue of his preparations for the Augsburg Interim and its enforcement, the Emperor was widely instrumental in recovering Germany to Roman Catholicism, the Pope now held aloof from him because Charles V himself had stood aloof in the matter of endowing Pier Luigi with Parma and Piacenza, and the situation came to a total rupture when the imperial vice-regent, Ferrante Gonzaga, proceeded forcibly to expel Pier Luigi.
The Pope's son was assassinated, 1547, at Piacenza, and Paul III believed that this had not come to pass without the emperor's foreknowledge. In the same year, however, and after the death of Francis I of France (1515–47), with whom the Pope had once again sought an alliance, the stress of circumstances compelled him to do the Emperor's will and accept the ecclesiastical measures adopted during the Interim.
With reference to the assassinated prince's inheritance, the restitution of which Paul III demanded ostensibly in the name and for the sake of the Church, the Pope's design was thwarted by the Emperor, who refused to surrender Piacenza, and by Pier Luigi's heir in Parma, Ottavio Farnese.
In consequence of a violent altercation on this account with Cardinal Farnese, Paul III, at the age of eighty-one years, became so overwrought that an attack of sickness ensued from which he died, 10 November 1549.
Paul III proved unable to suppress the Protestant Reformation, although it was during his pontificate that the foundation was laid for the Counter-Reformation. He decreed the second and final excommunication of King Henry VIII of England in December 1538.
Pope Paul III and slavery.
In May–June 1537 Paul issued three documents: the bulls "Sublimus Dei" (also known as "Unigenitus" and "Veritas ipsa"); "Altituda divini consolii"; and "Pastorale officium", the brief for the execution of "Sublimus Dei".
"Altituda divini consolii" was essentially a bull to settle a difference between the Franciscans and Dominicans over baptism, but "Sublimus Dei" is described by Prein (2008) as the "Magna Carta" for the human rights of the indigenous peoples of the Americas in its declaration that "the Indians were human beings and they were not to be robbed of their freedom or possessions".
"Pastorale officium" declared automatic excommunication for anyone who failed to abide by the new ruling. Stogre (1992) notes that "Sublimus Dei" is not present in Denzinger, the authoritative compendium of official teachings of the Catholic Church, and that the executing brief for it ("Pastorale officium") was annulled the following year in "Non Indecens Videtur". Davis (1988) asserts it was annulled due to a dispute with the Spanish crown. The Council of The West Indies and the Crown concluded that the documents broke their patronato rights and the Pope withdrew them, though they continued to circulate and be quoted by Las Casas and others who supported Indian rights.
According to Falkowski (2002) "Sublimus Dei" had the effect of revoking the bull of Alexander VI "Inter Caetera" but still leaving the colonizers the duty of converting the native people. Prein (2008) observes the difficulty in reconciling these decrees with "Inter Caetera".
Father Gustavo Gutierrez describes "Sublimus Dei" as "the most important papal document relating to the condition of native Indians and that it was addressed to all Christians". Maxwell (1975) notes that the bull did not change the traditional teaching that the enslavement of Indians was permissible if they were considered "enemies of Christendom" as this would be considered by the Church as a "just war". He further argues that the Indian nations had every right to self-defense. Stark (2003) describes the bull as "magnificent" and believes the reason that, in his opinion, it has belatedly come to light is due to the neglect of Protestant historians. Falola notes that the bull related to the native populations of the New World and did not condemn the transatlantic slave trade stimulated by the Spanish monarchy and the Holy Roman Emperor.
In 1545 Paul repealed an ancient law that allowed slaves to claim their freedom under the Emperor's statue on Capital Hill, in view of the number of homeless people and tramps in the city of Rome. The decree included those who had become Christians after their enslavement and those born to Christian slaves. The right of inhabitants of Rome to publicly buy and sell slaves of both sexes was affirmed. Stogre (1992) asserts that the lifting of restrictions was due to a shortage of slaves in Rome. In 1548 Paul authorized the purchase and possession of Muslim slaves in the Papal states.
Fictional portrayals.
The character of Pope Paul III, played by Peter O'Toole in the Showtime series "The Tudors", is loosely inspired by him.
The young Alessandro Farnese is played by Diarmuid Noyes in the StudioCanal serial "Borgia", and Cyron Melville in Showtime's "The Borgias".

</doc>
<doc id="47442" url="http://en.wikipedia.org/wiki?curid=47442" title="Roc">
Roc

Roc or may refer to:
As an acronym or initialism.
ROC or R.O.C. may refer to:

</doc>
<doc id="47443" url="http://en.wikipedia.org/wiki?curid=47443" title="Vera Wang">
Vera Wang

Vera Ellen Wang (, ]; born June 27, 1949) is an American fashion designer based in New York City and former figure skater. She is known for her wide range of haute couture bridesmaid gowns and wedding gown collections, as well as for her clientele of elite lady figure skaters, designing dresses for competitions and exhibitions.
Early life and education.
Vera Ellen Wang was born and raised in New York City, and is of Chinese descent. Her parents were born in Shanghai, China, and came to the United States in the mid-1940s. Her mother, Florence Wu, worked as a translator for the United Nations, while her father, Cheng Ching Wang, owned a medicine company. Wang has one younger brother, Kenneth.
Wang graduated from The Chapin School in 1967, attended the University of Paris and earned a degree in art history from Sarah Lawrence College. Wang began figure skating at the age of eight. While in high school, she trained with pairs partner James Stuart, and competed at the 1968 U.S. Figure Skating Championships. She was featured in "Sports Illustrated"'s Faces in the Crowd in the January 9, 1968 issue. When she failed to make the US Olympics team, she entered the fashion industry. Wang continues to enjoy skating, saying, "Skating is multidimensional".
In 1968, Wang was presented as a debutante at the International Debutante Ball at the Waldorf Astoria Hotel in New York City. At the time of her presentation, Wang explained that "the whole idea of a debutante affair is for a girl to be presented who is available for dating”. However, Wang had not formally announced her two-week-old engagement to Thomas Bermingham of Chicago and Phoenix. Wang, who was representing China at the ball, also admitted that she had never actually been to China, but that her parents had “homes in several areas in the Far East”.
Career.
Beginning in 1970, Wang was a senior fashion editor for "Vogue" but left "Vogue" after being turned down for the editor-in-chief position currently filled by Anna Wintour and joined Ralph Lauren as a design director for two years.
Wang has made wedding gowns for many well-known public figures, such as Chelsea Clinton, Karenna Gore, Ivanka Trump, Campbell Brown, Alicia Keys, Mariah Carey, Victoria Beckham, Avril Lavigne, Jennifer Lopez, Jennifer Garner, Sharon Stone, Sarah Michelle Gellar, Hilary Duff, Uma Thurman, Holly Hunter, Kate Hudson, Khloe Kardashian and Kim Kardashian. Wang's evening wear has also been worn by Michelle Obama.
She has designed costumes for figure skaters, including Nancy Kerrigan, Michelle Kwan and Evan Lysacek. Silver medalist Nancy Kerrigan wore a unique design of Vera's for the 1994 Olympics. She designed the two-piece uniforms currently worn by the Philadelphia Eagles Cheerleaders.
On October 23, 2001, her book, "Vera Wang on Weddings", was released. In June 2005, she won the CFDA (Council of Fashion Designers of America) Womenswear Designer of the Year. On May 27, 2006, Wang was awarded the André Leon Talley Lifetime Achievement Award from the Savannah College of Art and Design. Wang was inducted into the U.S. Figure Skating Hall of Fame in 2009, and was honored for her contribution to the sport as a costume designer.
Twenty years after opening her first bridal boutique, Wang was awarded the Leadership in the Arts Award by the Harvard-Radcliffe Asian American Association. She accepted the award on April 17, 2010 at "Identities", the Harvard association's annual charity fashion show.
Wang's evening wear has been worn by stars at many red carpet events, including Viola Davis at the 2012 Academy Awards, Sandra Bullock at the Oscars in 2011 and Sofia Vergara at the 65th Emmy Awards.
She was honored with the Geoffrey Beene Lifetime Achievement Award in 2013.
Retail.
In 1990, she opened her own design salon in the Carlyle Hotel in New York that features her trademark bridal gowns. She has since opened bridal boutiques in New York, London, Tokyo and Sydney, Australia. Wang has also expanded her brand name through her fragrance, jewelry, eyewear, shoe and houseware collections.
'White by Vera Wang' launched on February 11, 2011 at David's Bridal. Prices of the bridal gowns range from $600–$1,400 which gives more brides a more affordable way to wear Vera's designs. In 2002, Wang began to enter the home fashion industry and launched The Vera Wang China and Crystal Collection, followed by the 2007 release of her diffusion line called Simply Vera, which are sold exclusively by Kohl's.
In June 2012, she expanded in Australia with the opening of "Vera Wang Bride Sydney" and her first Asian flagship store 'Vera Wang Bridal Korea', helmed by President Jung Mi-ri, in upmarket neighbourhood Cheongdam-dong in Gangnam-gu, Seoul.
Personal life.
In 1989, she married Arthur Becker in an interfaith Baptist and Jewish ceremony. They resided in Manhattan with their two adopted daughters: Cecilia (born 1990), who currently resides in New York City, and Josephine (born 1993), who attended The Chapin School and currently attends Harvard University. Becker was the CEO of an information technology services company called NaviSite until August 2010. In July 2012, Vera Wang Co. announced that the couple have separated. The separation was amicable.
In popular culture.
Several movies and television shows have mentioned Wang's works.
In the "Sex and the City" TV series, Charlotte York found Wang's wedding dress to be the perfect wedding dress, and wore it for her wedding to Trey MacDougal. Wang's design was mentioned in the NBC television show "The West Wing" in the episode "The Black Vera Wang". In ABC's "Ugly Betty" TV series, Vera Wang makes a cameo as herself, designing a dress for Wilhelmina Slater's wedding to Bradford Meade.
In the film "Sex and the City", Vera Wang was among the bridal gowns Carrie Bradshaw wore in her "Vogue" photo shoot. In the film "Bride Wars", Anne Hathaway and Kate Hudson wore custom-made Vera Wang gowns.
In one of Totally Spies! episodes, "The Wedding Crasher", a villain named Wera Van (parody of Vera Wang), desires revenge on those who rejected her wedding dress designs.
Vera Wang also designed a wedding dress for Sarah Michelle Gellar's character Buffy Summers in the TV Series "Buffy The Vampire Slayer" episode "The Prom".
In the series, "How to Get Away with Murder", the character Michaela Pratt mentioned a custom Vera Wang wedding gown when confronting her fiancé, Aiden Walker, about his sexuality.
In the TV Series, "Gossip Girl," Vera Wang is mentioned in multiple episodes and is one of Blair Warldorf's favourite designers. Blair has a wedding dress designed for her wedding to Prince Louis by Vera Wang but decides she can no longer wear it after losing their baby in a car accident, and has another designed instead.
Also in Revenge (TV series), Victoria Grayson played by Madeleine Stowe wears a steel gray Vera Wang mermaid gown in her second wedding with Conrad Grayson (Henry Czerny). The dress was actually shown in black at Wang's Fall 2012 show.

</doc>
<doc id="47444" url="http://en.wikipedia.org/wiki?curid=47444" title="Copycat suicide">
Copycat suicide

A copycat suicide is defined as an emulation of another suicide that the person attempting suicide knows about either from local knowledge or due to accounts or depictions of the original suicide on television and in other media.
A spike of emulation suicides after a widely publicized suicide is known as the Werther effect, following Goethe's novel "The Sorrows of Young Werther".
The well-known suicide serves as a model, in the absence of protective factors, for the next suicide. This is referred to as suicide contagion. They occasionally spread through a school system, through a community, or in terms of a celebrity suicide wave, nationally. This is called a suicide cluster. Suicide clusters are caused by the social learning of suicide related behaviors, or "copycat suicides". Point clusters are clusters of suicides in both time and space, and have been linked to direct social learning from nearby individuals. Mass clusters are clusters of suicides in time but not space, and have been linked to the broadcasting of information concerning celebrity suicides via the mass media Examples of celebrities whose suicides have inspired suicide clusters include Ruan Lingyu, the Japanese musicians Yukiko Okada and hide, and Marilyn Monroe, whose death was followed by an increase of 200 more suicides than average for that August month.
Another famous case is the self-immolation of Mohamed Bouazizi, a Tunisian street vendor who set himself on fire on December 17, 2010, an act that was a catalyst for the Tunisian Revolution and sparked the Arab Spring, including several men who emulated Bouazizi's act.
To prevent this type of suicide, it is customary in some countries for the media to discourage suicide reports except in special cases.
History.
One of the earliest known associations between the media and suicide arose from Goethe's novel "Die Leiden des jungen Werthers" ("The Sorrows of Young Werther"). Soon after its publication in 1774, young men began to mimic the main character by dressing in yellow pants and blue jackets. In the novel, Werther shoots himself with a pistol after he is rejected by the woman he loves, and shortly after its publication there were many reports of young men using the same method to kill themselves in an act of hopelessness. 
This resulted in the book being banned in several places. Hence the term "Werther effect", used in the technical literature to designate copycat suicides. The term was coined by researcher David Phillips in 1974. 
Two centuries after Goethe's novel was published, David Phillips confirmed imitative suicides as the "Werther effect." Reports in 1985 and 1989 by Phillips and his colleagues found that suicides and other accidents seem to incline after a well publicized suicide. Copycat suicide is mostly blamed on the media. "Hearing about a suicide seems to make those who are vulnerable feel they have permission to do it," Phillips said. He cited studies that showed that people were more likely to engage in dangerous deviant behavior, such as drug taking, if someone else had set the example first.
Factors in suicide reporting.
The Werther effect not only predicts an increase in suicide, but the majority of the suicides will take place in the same or a similar way as the one publicized. The more similar the person in the publicized suicide is to the people exposed to the information about it, the more likely the age group or demographic is to die by suicide. The increase generally happens only in areas where the suicide story was highly publicized. Upon learning of someone else's suicide, many people decide that action is appropriate for them as well, especially if the publicized suicide was of someone in a similar situation as them.
Publishing the means of suicides, romanticized and sensationalized reporting, particularly about celebrities, suggestions that there is an epidemic, glorifying the deceased and simplifying the reasons all lead to increases in the suicide rate. People may see suicide as a glamorous ending — with youth getting a lot of attention, lots of sympathy, lots of national concern that they never got in life. The second possible factor is that vulnerable youth may feel like, "If they couldn't cut it, neither can I". Increased rate of suicides has been shown to occur up to ten days after a television report. Studies in Japan and Germany have replicated findings of an imitative effect. Etzersdorfer et al. in an Austrian study showed a strong correlation between the number of papers distributed in various areas and the number of subsequent firearm suicides in each area after a related media report. Higher rates of copycat suicides have been found in those with similarities in race, age, and gender to the victim in the original report. 
Stack analyzed the results from 42 studies and found that those measuring the effect of a celebrity suicide story were 14.3 times more likely to find a copycat effect than studies that did not. Studies based on a real as opposed to fictional story were 4.03 times more likely to uncover a copycat effect and research based on televised stories was 82% less likely to report a copycat effect than research based on newspapers. Other scholars have been less certain about whether copycat suicides truly happen or are selectively hyped. For instance, fears of a suicide wave following the death of Kurt Cobain never materialized in an actual increase in suicides. Furthermore, there is evidence for an indirect Werther effect, i.e. the perception that suicidal media content has an impact on others which, in turn, can concurrently or additionally influence one person's own future thoughts and behaviors. Similarly the researcher Gerard Sullivan has critiqued research on copycat suicides, suggesting that data analyses have been selective and misleading, and that the evidence for copycat suicides are much less consistent than suggested by some researchers.
Many people interviewed after the suicide of a relative or friend have a tendency to simplify the issues; their grief can lead to their minimizing or ignoring significant factors. Studies show a high incidence of psychiatric disorders in suicide victims at the time of their death with the total figure ranging from 98% to 87.3% with mood disorders and substance abuse being the two most common. These are often undiagnosed or untreated and treatment can result in reductions in the suicide rate. Reports that minimize the impact of psychiatric disorders contribute to copycat suicides whereas reports that mention this factor and provide help-line contact numbers and advice for where sufferers may gain assistance can reduce suicides.
Social proof model.
An alternate model to explain copycat suicide, called "social proof" by Cialdini, goes beyond the theories of glorification and simplification of reasons to look at why copycat suicides are so similar, demographically and in actual methods, to the original publicized suicide. In the social proof model, people imitate those who seem similar, despite or even because of societal disapproval. This model is important because it has nearly opposite ramifications for what the media ought to do about the copycat suicide effect than the standard model does. To deal with this problem, Alex Mesoudi of Queen Mary University, London, developed a computer model of a community of 1000 people, to examine how copycat suicides occur.
These were divided into 100 groups of 10, in a model designed to represent different levels of social organization, such as schools or hospitals within a town or state.
Mesoudi then circulated the simulation through 100 generations. He found the simulated people acted just as sociologists' theory predicted. They were more likely to die by suicide in clusters, either because they had learned this trait from their friends, or because suicidal people are more likely to be like one another.
Journalism codes.
Various countries have national journalism codes which range from one extreme of, "Suicide and attempted suicide should in general never be given any mention" (Norway) to a more moderate, "In cases of suicide, publishing or broadcasting information in an exaggerated way that goes beyond normal dimensions of reporting with the purpose of influencing readers or spectators should not occur." The study's author, University of London psychologist Alex Mesoudi, recommends that reporters follow the sort of guidelines the World Health Organization and others endorse for coverage of any suicide: Use extreme restraint in covering these deaths — keep the word "suicide" out of the headline, don't romanticize the death, and limit the number of stories. Photography, pictures, visual images or film depicting such cases should not be made public" (Turkey). While many countries do not have national codes, media outlets still often have in-house guidelines along similar lines. In the United States there are no industrywide standards and a survey of inhouse guides of 16 US daily newspapers showed that only three mentioned the word "suicide" and none gave guidelines about publishing the method of suicide. Craig Branson, online director of the American Society of News Editors (ASNE), has been quoted as saying, "Industry codes are very generic and totally voluntary. Most ethical decisions are left to individual editors at individual papers. The industry would fight any attempt to create more specific rules or standards, and editors would no doubt ignore them." Guidelines on the reporting of suicides in Ireland were introduced recently which attempt to remove any positive connotations the act might have (e.g. using the term "completed" rather than "successful" when describing a suicide attempt which resulted in a death).
The Canadian Broadcasting Corporation's journalistic standards and practices manual discourages the reporting of the details of suicide.
Journalist training.
Australia is one of the few countries where there is a concerted effort to teach journalism students about this subject. The Mindframe national media initiative followed an ambivalent response by the Australian Press Council to an earlier media resource kit issued by Suicide Prevention Australia and the Australian Institute for Suicide Research and Prevention. The UK-based media ethics charity provides training for journalists on reporting suicide and related issues, and has compiled other useful information on the topic at 
Headline is Ireland's media monitoring programme for suicide and mental health issues, set up by Shine and the Health Service Executives National Office for Suicide Prevention as part of 'Reach Out: National Strategy for action on Suicide Prevention.' Headline works with media professionals and students to find ways to collaborate to ensure that suicide, mental health and mental illness are responsibly covered in the media and provides information on reporting on mental health and suicidal behavior, literature and daily analysis of news stories. Headline also serves as a vehicle for the public to become involved in helping to monitor the Irish media on issues relating to mental health and suicide.

</doc>
<doc id="47449" url="http://en.wikipedia.org/wiki?curid=47449" title="Chessboard">
Chessboard

A chessboard is the type of checkerboard used in the very famous board game chess, and consists of 64 squares (eight rows and eight columns)and 32 pieces.The squares are arranged in two alternating colors (light and dark). Wooden boards may use naturally light and dark brown woods, while plastic and vinyl boards often use brown or green for the dark squares and shades such as buff or cream for the light squares. Materials vary widely; while wooden boards are generally used in high-level games, vinyl, plastic, and cardboard are common for low-level and informal play. Decorative glass and marble boards are available but not usually accepted for sanctioned games.Each square on the board has a name from a1 to h8.
 According to FIDE equipment standards, the size of a square should be twice the diameter of a pawn's base. 
In "modern commentary", the columns (called "files") are labeled by the letters a to h from left to right from the white player's point of view, and the rows (called "ranks") by the numbers 1 to 8, with 1 being closest to the white player, thus providing a standard notation called algebraic chess notation.
In older "English commentary", the files are labeled by the piece originally occupying its first rank (e.g. queen, king's rook, queen's bishop), and ranks by the numbers 1 to 8 from each player's point of view, depending on the move being described. This is called descriptive chess notation and is no longer commonly used.

</doc>
<doc id="47454" url="http://en.wikipedia.org/wiki?curid=47454" title="Stratosphere">
Stratosphere

The stratosphere is the second major layer of Earth's atmosphere, just above the troposphere, and below the mesosphere. It is stratified in temperature, with warmer layers higher up and cooler layers farther down. This is in contrast to the troposphere near the Earth's surface, which is cooler higher up and warmer farther down. The border of the troposphere and stratosphere, the tropopause, is marked by where this inversion begins, which in terms of atmospheric thermodynamics is the equilibrium level. At moderate latitudes the stratosphere is situated between about 10 - and 50 km altitude above the surface, while at the poles it starts at about 8 km altitude, and near the equator it may start at altitudes as high as 18 km.
Ozone and temperature.
Within this layer, temperature increases as altitude increases (see temperature inversion); the top of the stratosphere has a temperature of about 270 K (−3°C or 26.6°F), just slightly below the freezing point of water. The stratosphere is layered in temperature because ozone (O3) here absorbs high energy UVB and UVC energy waves from the Sun and is broken down into atomic oxygen (O) and diatomic oxygen (O2). Atomic oxygen is found prevalent in the upper stratosphere due to the bombardment of UV light and the destruction of both ozone and diatomic oxygen. The mid stratosphere has less UV light passing through it, O and O2 are able to combine, and is where the majority of natural ozone is produced. It is when these two forms of oxygen recombine to form ozone that they release the heat found in the stratosphere. The lower stratosphere receives very low amounts of UVC, thus atomic oxygen is not found here and ozone is not formed (with heat as the byproduct). This vertical stratification, with warmer layers above and cooler layers below, makes the stratosphere dynamically stable: there is no regular convection and associated turbulence in this part of the atmosphere. The top of the stratosphere is called the stratopause, above which the temperature decreases with height.
Methane (CH4), while not a direct cause of ozone destruction in the stratosphere, does lead to the formation of compounds that destroy ozone. Monatomic oxygen (O) in the upper stratosphere reacts with methane (CH4) to form a hydroxyl radical (OH·). This hydroxyl radical is then able to interact with non-soluble compounds like chlorofluorocarbons, and UV light breaks off chlorine radicals (Cl·). These chlorine radicals break off an oxygen atom from the ozone molecule, creating an oxygen molecule (O2) and a hypochloryl radical (ClO·). The hypochloryl radical then reacts with an atomic oxygen creating another oxygen molecule and another chlorine radical, thereby preventing the reaction of monatomic oxygen with O2 to create natural ozone.
Aircraft flight.
Commercial airliners typically cruise at altitudes of 9 – in temperate latitudes (in the lower reaches of the stratosphere). This optimizes fuel burn, mostly due to the low temperatures encountered near the tropopause and low air density, reducing parasitic drag on the airframe. (Stated another way, it allows the airliner to fly faster for the same amount of drag.) It also allows them to stay above hard weather (extreme turbulence).
Concorde would cruise at mach 2 at about 18000 m, and the SR-71 would cruise at mach 3 at 26000 m, all still in the stratosphere.
Because the temperature in the tropopause and lower stratosphere remains constant (or slightly decreases) with increasing altitude, very little convective turbulence occurs at these altitudes. Though most turbulence at this altitude is caused by variations in the jet stream and other local wind shears, areas of significant convective activity (thunderstorms) in the troposphere below may produce convective overshoot.
Although a few gliders have achieved great altitudes in the powerful thermals in thunderstorms, this is dangerous. Most high altitude flights by gliders use lee waves from mountain ranges and were used to set the current record of 15,447 m.
On October 24, 2014, Alan Eustace became the record holder for reaching the altitude record for a manned balloon at 135,890 feet. Mr Eustace also broke the world records for vertical speed reached with a peak velocity of 1,321 km/h (822 mph) and total freefall distance of 123,414 feet - lasting four minutes and 27 seconds.
Circulation and mixing.
The stratosphere is a region of intense interactions among radiative, dynamical, and chemical processes, in which the horizontal mixing of gaseous components proceeds much more rapidly than in vertical mixing.
An interesting feature of stratospheric circulation is the quasi-biennial oscillation (QBO) in the tropical latitudes, which is driven by gravity waves that are convectively generated in the troposphere. The QBO induces a secondary circulation that is important for the global stratospheric transport of tracers, such as ozone or water vapor.
In northern hemispheric winter, sudden stratospheric warmings, caused by the absorption of Rossby waves in the stratosphere, can be observed in approximately half of winters when easterly winds develop in the stratosphere. These events often precede unusual winter weather and may even be responsible for the cold European winters of the 1960s.
Life.
Bacteria.
Bacterial life survives in the stratosphere, making it a part of the biosphere. In 2001 an Indian experiment, involving a high-altitude balloon, was carried out at a height of 41 kilometres and a sample of dust was collected with bacterial material inside.
Birds.
Also, some bird species have been reported to fly at the lower levels of the stratosphere. On November 29, 1973, a Rüppell's vulture was ingested into a jet engine 37900 ft above the Ivory Coast, and bar-headed geese reportedly overfly Mount Everest's summit, which is 8,848 m.
Discovery.
Léon Teisserenc de Bort and Richard Assmann, in separate publications and following years of observations, announced the discovery of an isothermal layer at around 11–14 km, which is the base of the lower stratosphere. This was based on temperature profiles from unmanned instrumented balloons.

</doc>
<doc id="47456" url="http://en.wikipedia.org/wiki?curid=47456" title="Pater Noster (disambiguation)">
Pater Noster (disambiguation)

Pater Noster or "Our Father" is probably the best-known prayer in Christianity.
Pater Noster or Paternoster may also refer to:

</doc>
<doc id="47459" url="http://en.wikipedia.org/wiki?curid=47459" title="James T. Kirk">
James T. Kirk

James Tiberius "Jim" Kirk is a fictional character in the "Star Trek" media franchise, appearing in numerous television episodes, films, books, comics, and video games. As the captain of the starship USS "Enterprise", Kirk leads his crew as they explore "where no man has gone before". Often, the characters of Spock and Leonard McCoy act as his logical and emotional sounding boards, respectively.
Kirk, played by William Shatner, first appears in the broadcast pilot episode of ', "The Man Trap", originally broadcast on September 8, 1966. Shatner continued in the role for the show's three seasons, and later provided the voice of the animated version of Kirk in ' (1973–74). Shatner returned to the role for "" (1979) and in six subsequent films. Chris Pine portrays a young version of the character in the 2009 reboot "Star Trek" film, with Jimmy Bennett playing Kirk as a child. Other actors have played the character in fan-created media, and the character has been the subject of multiple spoofs and satires. The character has been praised for his leadership traits, and also criticized for his relationships with women.
Depiction.
James Tiberius Kirk was born on March 22, 2233, in Riverside, Iowa. He was raised there by his parents, George and Winona Kirk. Although born on Earth, Kirk for a time lived on , where he was one of nine surviving witnesses to the massacre of 4,000 colonists by . James Kirk's brother George Samuel Kirk is first mentioned in "What Are Little Girls Made Of?" and introduced and killed in "", leaving behind three children.
At Starfleet Academy, Kirk became the only student to defeat the "Kobayashi Maru" test, garnering a commendation for original thinking by reprogramming the computer to make the "no-win scenario" winnable. Kirk was granted a field commission as an ensign and posted to advanced training aboard the USS "Republic". He then was promoted to lieutenant junior grade and returned to Starfleet Academy as a student instructor. Students could either "think or sink" in his class, and Kirk himself was "a stack of books with legs". Upon graduating in the top five percent, Kirk was promoted to lieutenant and served aboard the USS "Farragut". While assigned to the "Farragut", Kirk commanded his first planetary survey and survived a deadly attack that killed a large portion of the "Farragut"‍ '​s crew, including his commanding officer, Captain Garrovick. He received his first command, a spaceship roughly equivalent to a destroyer, while still quite young.
Kirk became Starfleet's youngest captain when he received command of the USS "Enterprise" for a five-year mission, three years of which are depicted in the original "Star Trek" series. Kirk's most significant relationships in the television series are with first officer Spock and chief medical officer Dr. Leonard "Bones" McCoy. McCoy is someone to whom Kirk unburdens himself and is a foil to Spock. Robert Jewett and John Shelton Lawrence's "The Myth of the American Superhero" describes Kirk as "a hard-driving leader who pushes himself and his crew beyond human limits". Terry J. Erdman and Paula M. Block, in their "Star Trek 101" primer, note that while "cunning, courageous and confident", Kirk also has a "tendency to ignore Starfleet regulations when he feels the end justifies the means"; he is "the quintessential officer, a man among men and a hero for the ages". Although Kirk throughout the series becomes romantically involved with various women, when confronted with a choice between a woman and the "Enterprise", "his ship always won". Roddenberry wrote in a production memo that Kirk is not afraid of being fallible, but rather is afraid of the consequences to his ship and crew should he make an error in judgment. Roddenberry also wrote that Kirk
has any normal man's insecurities and doubts, but he knows he cannot ever show them — except occasionally in private with ship's surgeon McCoy or in subsequent moments with Mr. Spock whose opinions Kirk has learned to value so highly.
J. M. Dillard's novel "The Lost Years" describes Kirk's promotion to rear admiral and unfulfilling duties as a diplomatic troubleshooter after the "Enterprise"‍ '​s five-year mission. In ', Kirk is chief of Starfleet operations, and he takes command of the "Enterprise" from Captain Willard Decker. "Star Trek" creator Gene Roddenberry's novelization of "The Motion Picture" depicts Kirk married to a Starfleet officer killed during a transporter accident. At the beginning of ', Kirk takes command of the "Enterprise" from Captain Spock to pursue his enemy from "Space Seed", Khan Noonien Singh. The movie introduces Kirk's son, David Marcus. Spock, who notes that "commanding a starship is [Kirk's] first, best destiny", dies at the end of "Star Trek II". In ', Kirk leads his surviving officers in a successful mission to rescue Spock from a planet on which he is reborn. Although Kirk is demoted to captain in ' for disobeying Starfleet orders, he also receives command of a new USS "Enterprise". The ship is ordered decommissioned at the end of "".
In "Star Trek Generations", Captain Picard finds Kirk alive in the timeless Nexus, despite the fact that history recorded his death during the "Enterprise"-B's maiden voyage, Kirk having fallen into the Nexus in the incident that caused his "death". Picard convinces Kirk to return to Picard's present to help stop the villain Soran from destroying Veridian III's sun. Although Kirk initially refuses the offer, he agrees when he realises that the Nexus cannot give him the one thing he has always sought: the ability to make a difference. The two leave the Nexus and stop Soran. However, Kirk is mortally wounded; as he dies, Picard assures him that he helped to "make a difference". Picard buries Kirk on the planet. Shatner and Judith and Garfield Reeves-Stevens wrote a series of novels that depict Kirk's resurrection by the Borg and his ongoing adventures after the events of "Generations".
Reboot films.
The 2009 film "Star Trek" introduces an alternative timeline that reveals different origins for Kirk, the formation of his association with Spock, and how they came to serve together on the "Enterprise". The point of divergence between "The Original Series" and the film occurs on the day of Kirk's birth in 2233. Although the film treats specific details from Star Trek as mutable, characterizations are meant to "remain the same". In the film, George and Winona Kirk name their son "James Tiberius" after his maternal and paternal grandfathers, respectively. He is born on a shuttle escaping the starship USS "Kelvin", on which his father is killed. The character begins as "a reckless, bar-fighting rebel" who eventually matures. According to Pine, the character is "a 25-year-old [who acts like a] 15-year-old" and who is "angry at the world". Kirk and Spock clash at Starfleet Academy, but, over the course of the film, Kirk focuses his "passion and obstinance and the spectrum of emotions" and becomes captain of the "Enterprise". The alternate timeline continues in the 2013 sequel, "Star Trek Into Darkness", in which Pine reprises his role.
Development.
Conception and television.
Jeffrey Hunter played Captain Christopher Pike, commanding officer of the USS "Enterprise", in the rejected "Star Trek" television pilot "". In developing a new pilot episode, called "Where No Man Has Gone Before", series creator Gene Roddenberry changed the captain's name to "James Kirk" after rejecting other options like Hannibal, Timber, Flagg and Raintree. The name was inspired by Captain James Cook, whose journal entry "ambition leads me ... farther than any other man has been before me" inspired the episode title. The character is in part based on C. S. Forester's Horatio Hornblower hero, and NBC wanted the show to emphasize the captain's "rugged individualism". Jack Lord was Desilu Productions' original choice to play Kirk, but his demand for fifty-percent ownership of the show led to him not being hired. The second pilot episode was successful, and "Where No Man Has Gone Before" was broadcast as the third episode of "Star Trek" on September 22, 1966.
William Shatner tried to imbue the character with qualities of "awe and wonder" absent from "The Cage". He also drew upon his experiences as a Shakespearean actor to invigorate the character, whose dialogue at times is laden with jargon. Not only did Shatner take inspiration from Roddenberry's suggestion of Hornblower, but also from Alexander the Great – "the athlete and the intellectual of his time" – whom Shatner had played for an unsold television pilot two years earlier. In addition, the actor based Kirk partly on himself because "the fatigue factor [after weeks of daily filming] is such that you try to be as honest about yourself as possible". A comedy veteran, Shatner suggested making the show's characters as comfortable working in space as they would be at sea, thus having Kirk be a humorous "good-pal-the-captain, who in time of need would snap to and become the warrior". Changing the character to be "a man with very human emotions" also allowed for the development of the Spock character. Shatner wrote that "Kirk was a man who marveled and greatly appreciated the endless surprises presented to him by the universe ... He didn't take things for granted and, more than anything else, respected life in every one of its weird weekly adventure forms".
Films.
Shatner did not expect "Star Trek" to become a success; when "Star Trek" was cancelled in 1969, Shatner assumed it would be the end of his association with the show. However, Shatner went on to voice Kirk in the animated "Star Trek" series, star in the first seven "Star Trek" films, and provide voice acting for several games. "" director and writer Nicholas Meyer, who had never seen an episode of "Star Trek" before he was assigned to direct, conceived a ""Hornblower" in outer space" atmosphere, unaware that those books had been an influence on the show. Meyer also emphasized parallels to Sherlock Holmes, in that both characters waste away in the absence of their stimuli: new cases for Holmes; starship adventures for Kirk.
Meyer's "The Wrath of Khan" script focuses on Kirk's age, with McCoy giving Kirk a pair of glasses as a birthday present. The script states that Kirk is 49, but Shatner was unsure about being specific about Kirk's age because he was hesitant to portray a middle-aged version of himself. Shatner changed his mind when producer Harve Bennett convinced Shatner that he could age gracefully like Spencer Tracy. Spock's sacrifice at the end of the film allows for Kirk's spiritual rebirth; after commenting earlier that he feels old and worn out, Kirk states in the final scene that he feels "young." Additionally, Spock's self-sacrificing solution to the no-win "Kobayashi Maru" scenario, which Kirk had cheated his way through, forces Kirk to confront death and to grow as a character.
Both Shatner and test audiences were dissatisfied that Kirk was fatally shot in the back in the original ending of the film "Star Trek Generations". An addendum inserted while Shatner's "Star Trek Movie Memories" memoir was being printed expresses his enthusiasm at being called back to film a rewritten ending. Despite the rewrite, "Generations" co-writer Ronald D. Moore said that Kirk's death, which was intended to "resonate throughout the Star Trek franchise", failed to "pay off the themes [of death and mortality] in the way we wanted". Malcolm McDowell, whose character kills Kirk, was dissatisfied with both versions of Kirk's death: he believed Kirk should have been killed "in a big way". McDowell claims to have received death threats after "Generations" was released.
Franchise "reboot".
In the 2009 film "Star Trek", screenwriters Alex Kurtzman and Roberto Orci focused their story on Kirk and Spock in the movie's alternative timeline while attempting to preserve key character traits from the previous depictions. Kurtzman said casting someone whose portrayal of Kirk would show that the character "is being honored and protected" was "tricky", but that the "spirit of Kirk is very much alive and well" in Pine's depiction. Due to his belief that he could not take himself seriously as a leader, Pine recalled having difficulty with his audition, which required him "to bark ‍ '​"Trek" jargon'", but his charisma impressed director J. J. Abrams. Pine's chemistry with Zachary Quinto, playing Spock, led Abrams to offer Pine the role. Jimmy Bennett played Kirk in scenes depicting the character's childhood. The writers turned to material such as "Best Destiny" for inspiration as to Kirk's childhood.
In preparing to play Kirk, Pine decided to embrace the character's key traits – "charming, funny, leader of men" – rather than try to fit the "predigested image" of Shatner's portrayal. Pine specifically did not try to mirror Shatner's cadence, believing that doing so would become "an impersonation". Pine said he wanted his portrayal of Kirk to most resemble Harrison Ford's Indiana Jones or Han Solo characters, highlighting their humor and "accidental hero" traits.
A misunderstanding arose during the film's production about the possibility of Shatner making a cameo appearance. According to Abrams, the production team considered ways to resurrect Shatner's deceased Kirk character, but could not devise a way that was not "lame". However, Abrams believed Shatner misinterpreted language about trying to get "him" into the movie as a reference to Shatner, and not his character. Shatner released a YouTube video expressing disappointment at not being approached for a cameo. Although Shatner questioned the wisdom of not including him in the film, he predicted the movie would be "wonderful" and that he was "kidding" Abrams about not offering him a cameo.
Reception.
According to Shatner, early "Star Trek" reviews called his performance "wooden", with most of the show's acting praise and media interest going to Nimoy. However, Shatner's mannerisms when portraying Kirk have become "instantly recognizable" and Shatner won a Saturn Award for Best Actor in 1982 for "The Wrath of Khan". "Star Trek II" director Nicholas Meyer said Shatner "gives the best performance of his life" in "The Wrath of Khan". "The Guardian" called Pine's performance of Kirk an "unqualified success", and "The Boston Globe" said Pine is "a fine, brash boy Kirk". "Slate", which called Pine "a jewel", described his performance as "channel[ing]" Shatner without being an impersonation.
Slate.com described Shatner's depiction of Kirk as an "expansive, randy, faintly ridiculous, and yet supremely capable leader of men, Falstaffian in his love of life and largeness of spirit". "The Myth of the American Superhero" refers to Kirk as a "superhuman redeemer" who "like a true superhero ... regularly escapes after risking battle with monsters or enemy spaceships". Although some episodes question Kirk's position as a hero, "Star Trek" "never left the viewer in doubt for long". Others have commented that Kirk's exaggerated "strength, intelligence, charm, and adventurousness" make him unrealistic. Kirk is described as able to find ways "through unanticipated problems to reach [his] goals" and his leadership style is most "appropriate in a tight, geographically identical team with a culture of strong leadership." Although Roddenberry conceived the character as being "in a very real sense ... 'married' " to the "Enterprise", Kirk has been noted for "his sexual exploits with gorgeous females of every size, shape and type"; he has been called "promiscuous" and labeled a "womanizer". "The Last Lecture" author Randy Pausch believed he became a better teacher, colleague, and husband because he watched Kirk run the "Enterprise"; Pausch wrote that "for ambitious boys with a scientific bent, there could be no greater role model than James T. Kirk".
Cultural impact.
The town of Riverside, Iowa, petitioned Roddenberry and Paramount Pictures in 1985 for permission to "adopt" Kirk as their town's "Future Son". Paramount wanted $40,000 for a license to reproduce a bust of Kirk, but the city instead set a plaque and built a replica of the "Enterprise" (named the "USS "Riverside""), and the Riverside Area Community Club holds an annual "Trek Fest" in anticipation of Kirk's birthday.
Kirk has been the target of spoofs in a wide range of television programs in many countries, including "The Carol Burnett Show" and KI.KA's "Bernd das Brot". John Belushi's impression of Kirk for "Saturday Night Live", which he described as his favorite role, was "dead-on". Jim Carrey has been praised for his satire of the character in a 1992 episode of "In Living Color". Comedian Kevin Pollak is well known for his impressions of Shatner as Kirk. Kirk has also been mentioned in song, such as the 1984 English adaptation of "99 Luftballons" by Nena and the 1979 song "Where's Captain Kirk?" by Spizzenergi.
Kirk has been merchandised in a variety of ways, including collectible busts, action figures, mugs, t-shirts, and Christmas tree ornaments. A Kirk Halloween mask was altered and used as the mask worn by the character Michael Myers in the "Halloween" film franchise. In 2002, Kirk's captain's chair from the original "Star Trek" was auctioned for $304,000.
In a 2010 Space Foundation survey, Kirk was ranked as the No. 6 (tied with cosmonaut Yuri Gagarin) most popular space hero.
The character of J.T. from the show was named after Kirk.
Fan productions.
The "" fan production portrays the further voyages of the original "Enterprise" crew. The series' creators feel that "Kirk, Spock, McCoy and the rest should be treated as 'classic' characters like Willy Loman from "Death of a Salesman", Gandalf from "The Lord of the Rings" or even Hamlet, Othello or Romeo. Many actors have and can play the roles, each offering a different interpretation of said character".
James Cawley has played Kirk in the "Phase II" series since it began in 2004. "Wired" observes that while Cawley's depiction "lacks Shatner's vulnerability", the actor has enough swagger "to be passable in the role of Captain Kirk". Cawley's portrayal was well-known enough at Paramount that a group of "" writers called for Cawley's attention at a science fiction convention by shouting "Hey, Kirk!" at him while Shatner sat nearby.

</doc>
<doc id="47460" url="http://en.wikipedia.org/wiki?curid=47460" title="Mesosphere">
Mesosphere

The mesosphere (; from Greek "mesos" "middle" and "sphaira" "ball") is the layer of the Earth's atmosphere that is directly above the stratopause and directly below the mesopause. In the mesosphere temperature decreases as the altitude increases. The upper boundary of the mesosphere is the mesopause, which can be the coldest naturally occurring place on Earth with temperatures below 130 K. The exact upper and lower boundaries of the mesosphere vary with latitude and with season, but the lower boundary of the mesosphere is usually located at heights of about 50 km above the Earth's surface and the mesopause is usually at heights near 100 km, except at middle and high latitudes in summer where it descends to heights of about 85 km.
The stratosphere, mesosphere and lowest part of the thermosphere are collectively referred to as the "middle atmosphere", which spans heights from approximately 10 km to 100 km. The mesopause, at an altitude of 80 -, separates the mesosphere from the thermosphere—the second-outermost layer of the Earth's atmosphere. This is also around the same altitude as the turbopause, below which different chemical species are well mixed due to turbulent eddies. Above this level the atmosphere becomes non-uniform; the scale heights of different chemical species differ by their molecular masses.
Temperature.
Within the mesosphere, temperature decreases with increasing height. This is due to decreasing solar heating and increasing cooling by CO2 radiative emission. The top of the mesosphere, called the mesopause, is the coldest part of Earth's atmosphere. Temperatures in the upper mesosphere fall as low as -100 C, varying according to latitude and season.
Dynamic features.
The main dynamic features in this region are strong zonal (East-West) winds, atmospheric tides, internal atmospheric gravity waves (commonly called "gravity waves") and planetary waves. Most of these tides and waves are excited in the troposphere and lower stratosphere, and propagate upward to the mesosphere. In the mesosphere, gravity-wave amplitudes can become so large that the waves become unstable and dissipate. This dissipation deposits momentum into the mesosphere and largely drives global circulation.
Noctilucent clouds are located in the mesosphere. The upper mesosphere is also the region of the ionosphere known as the "D layer". The D layer is only present during the day, when some ionization occurs with nitric oxide being ionized by Lyman series-alpha hydrogen radiation. The ionization is so weak that when night falls, and the source of ionization is removed, the free electron and ion form back into a neutral molecule. The mesosphere is also known as the "Ignorosphere" because it is poorly studied compared to the stratosphere (which can be accessed with high-altitude balloons) and the thermosphere (in which satellites can orbit).
A 5 km deep sodium layer is located between 80 -. Made of unbound, non-ionized atoms of sodium, the sodium layer radiates weakly to contribute to the airglow.
Millions of meteors enter the atmosphere, an average of 40 tons per year.
Uncertainties.
The mesosphere lies above the maximum altitude for aircraft and nearly all balloons, and below the minimum altitude for orbital spacecraft. Above the 53.0 km balloon altitude record,
the mesosphere has only been accessed through the use of sounding rockets. As a result, it is the most poorly understood part of the atmosphere. The presence of red sprites and blue jets (electrical discharges or lightning within the lower mesosphere), noctilucent clouds and density shears within the poorly understood layer are of current scientific interest.

</doc>
<doc id="47463" url="http://en.wikipedia.org/wiki?curid=47463" title="Thermosphere">
Thermosphere

The thermosphere is the layer of the Earth's atmosphere directly above the mesosphere and directly below the exosphere. Within this layer, ultraviolet radiation causes ionization. Called from the Greek θερμός ("pronounced thermos") meaning heat, the thermosphere begins about 85 km above the Earth. At these high altitudes, the residual atmospheric gases sort into strata according to molecular mass (see turbosphere). Thermospheric temperatures increase with altitude due to absorption of highly energetic solar radiation. Temperatures are highly dependent on solar activity, and can rise to 2000 C. Radiation causes the atmosphere particles in this layer to become electrically charged (see ionosphere), enabling radio waves to bounce off and be received beyond the horizon. In the exosphere, beginning at 500 to above the Earth's surface, the atmosphere turns into space. 
The highly diluted gas in this layer can reach 2500 C during the day. Even though the temperature is so high, one would not feel warm in the thermosphere, because it is so near vacuum that there is not enough contact with the few atoms of gas to transfer much heat. A normal thermometer would be significantly below 0 C, because the energy lost by thermal radiation would exceed the energy acquired from the atmospheric gas by direct contact. In the anacoustic zone above 160 km, the density is so low that molecular interactions are too infrequent to permit the transmission of sound. 
The dynamics of the thermosphere are dominated by atmospheric tides, which are driven by the very significant diurnal heating. Atmospheric waves dissipate above this level because of collisions between the neutral gas and the ionospheric plasma. 
The International Space Station has a stable orbit within the middle of the thermosphere, between 320 and, whereas the Gravity Field and Steady-State Ocean Circulation Explorer satellite at 260 km utilized winglets and an innovative ion engine to maintain a stable orientation and orbit. 
It is convenient to separate the atmospheric regions according to the two temperature minima at about 12 km altitude (the tropopause) and at about 85 km (the mesopause) (Figure 1). The thermosphere (or the upper atmosphere) is the height region above 85 km, while the region between the tropospause and the mesopause is the middle atmosphere (stratosphere and mesosphere) where absorption of solar UV radiation generates the temperature maximum near 45 km altitude and causes the ozone layer.
The density of the Earth's atmosphere decreases nearly exponentially with altitude. The total mass of the atmosphere is M = ρA H  ≃ 1 kg/cm2 within a column of one square centimeter above the ground (with ρA = 1.29 kg/m3 the atmospheric density on the ground at z = 0 m altitude, and H ≃ 8 km the average atmospheric scale height). 80% of that mass already concentrated within the troposphere. The mass of the thermosphere above about 85 km is only 0.002% of the total mass. Therefore, no significant energetic feedback from the thermosphere to the lower atmospheric regions can be expected.
Turbulence causes the air within the lower atmospheric regions below the turbopause at about 110 km to be a mixture of gases that does not change its composition. Its mean molecular weight is 29 g/mol with molecular oxygen (O2) and nitrogen (N2) as the two dominant constituents. Above the turbopause, however, diffusive separation of the various constituents is significant, so that each constituent follows its own barometric height structure with a scale height inversely proportional to its molecular weight. The lighter constituents atomic oxygen (O), helium (He), and hydrogen (H) successively dominate above about 200 km altitude and vary with geographic location, time, and solar activity. The ratio
N2/O which is a measure of the electron density at the ionospheric F region is highly affected by these variations. These changes follow from the diffusion of the minor constituents through the major gas component during dynamic processes.
Energy input.
Energy budget.
The thermospheric temperature can be determined from density observations as well as from direct satellite measurements. The temperature vs. altitude z in Fig. 1 can be simulated by the so-called Bates profile 
(1)   formula_1
with T∞ the exospheric temperature above about 400 km altitude, 
To = 355 K, and zo = 120 km reference temperature and height, and s an empirical parameter depending on T∞ and decreasing with T∞. That formula is derived from a simple equation of heat conduction. One estimates a total heat input of qo≃ 0.8 to 1.6 mW/m2 above zo = 120 km altitude. In order to obtain equilibrium conditions, that heat input qo above zo is lost to the lower atmospheric regions by heat conduction. 
The exospheric temperature T∞ is a fair measurement of the solar XUV radiation. Since solar radio emission F at 10.7 cm wavelength is a good indicator of solar activity, one can apply the empirical formula for quiet magnetospheric conditions.
(2)   formula_2
with T∞ in K, Fo in 10−2 W m−2 Hz−1 (the Covington index) a value of F averaged over several solar cycles. The Covington index varies typically between 70 and 250 during a solar cycle, and never drops below about 50. Thus, T∞ varies between about 740 and 1350 K. During very quiet magnetospheric conditions, the still continuously flowing magnetospheric energy input contributes by about 250 K to the residual temperature of 500 K in eq.(2). The rest of 250 K in eq.(2) can be attributed to atmospheric waves generated within the troposphere and dissipated within the lower thermosphere.
Solar XUV radiation.
The solar X-ray and extreme ultraviolet radiation (XUV) at wavelengths < 170 nm is almost completely absorbed within the thermosphere. This radiation causes the various ionospheric layers as well as a temperature increase at these heights (Figure 1).
While the solar visible light (380 to 780 nm) is nearly constant with a variability of not more than about 0.1% of the solar constant, the solar XUV radiation is highly variable in time and space. For instance, X-ray bursts associated with solar flares can dramatically increase their intensity over preflare levels by many orders of magnitude over a time span of tens of minutes. In the extreme ultraviolet, the Lyman α line at 121.6 nm represents an important source of ionization and dissociation at ionospheric D layer heights. During quiet periods of solar activity, it alone contains more energy than the rest of the spectrum at lower wavelengths. Quasi-periodic changes of the order of 100% and more with period of 27 days and 11 years belong to the prominent variations of solar XUV radiation. However, irregular fluctuations over all time scales are present all the time. During low solar activity, about one half of the total energy input into the thermosphere is thought to be solar XUV radiation. Evidently, that solar XUV energy input occurs only during daytime conditions, maximizing at the equator during equinox.
Solar wind.
A second source of energy input into the thermosphere is solar wind energy which is transferred to the magnetosphere by mechanisms that are not completely understood. One possible way to transfer energy is via a hydrodynamic dynamo process. Solar wind particles penetrate into the polar regions of the magnetosphere where the geomagnetic field lines are essentially vertically directed. An electric field is generated, directed from dawn to dusk. Along the last closed geomagnetic field lines with their footpoints within the auroral zones, field aligned electric currents can flow into the ionospheric dynamo region where they are closed by electric Pedersen and Hall currents. Ohmic losses of the Pedersen currents heat the lower thermosphere (see e.g., Magnetospheric electric convection field). In addition, penetration of high energetic particles from the magnetosphere into the auroral regions enhance drastically the electric conductivity, further increasing the electric currents and thus Joule heating. During quiet magnetospheric activity, the magnetosphere contributes perhaps by a quarter to the energy budget of the thermosphere. This is about 250 K of the exospheric temperature in eq.(2). During very large activity, however, this heat input can increase substantially, by a factor of four or more. That solar wind input occurs mainly in the auroral regions during the day as well as during the night.
Atmospheric waves.
Two kinds of large scale atmospheric waves within the lower atmosphere exist: internal waves with finite vertical wavelengths which can transport wave energy upward and external waves with infinitely large wavelengths which cannot transport wave energy. Atmospheric gravity waves and most of the atmospheric tides generated within the troposphere belong to the internal waves. Their density amplitudes increase exponentially with height, so that at the mesopause these waves become turbulent and their enery is dissipated (similar to breaking of ocean waves at the coast), thus contributing to the heating of the thermosphere by about 250 K in eq.(2). On the other hand, the fundamental diurnal tide labelled (1, −2) which is most efficiently excited by solar irradiance is an external wave and plays only a marginal role within lower and middle atmosphere. However, at thermospheric altitudes, it becomes the predominant wave. It drives the electric Sq-current within the ionospheric dynamo region between about 100 and 200 km height.
Heating, predominately by tidal waves, occurs mainly at lower and middle latitudes. The variability of this heating depends in general on the meteorological conditions within troposphere and middle atmosphere, and may not exceed about 50%.
Dynamics.
Within the thermosphere above about 150 km height, all atmospheric waves successively become external waves, and no signifiant vertical wave structure is visible. The atmospheric wave modes degenerate to the spherical functions Pnm with m a meridional wave number and n the zonal wave number (m = 0: zonal mean flow; m = 1: diurnal tides; m = 2: semidiurnal tides; etc.). The thermosphere becomes a damped oscillator system with low pass filter characteristics. This means that smaller scale waves (greater numbers of (n,m)) and higher frequencies are suppressed in favor of large scale waves and lower frequencies. If one considers very quiet magnetospheric disturbances and a constant mean exospheric temperature (averaged over the sphere), the observed temporal and spatial distribution of the exospheric temperature distribution can be described by a sum of spheric functions:
(3)   formula_3
Here, it is φ latitude, λ longitude, and t time, ωa the angular frequency of one year, ωd the angular frequency of one solar day, and τ = ωdt + λ the local time. ta = June, 21 is the time of northern summer solstice, and τd = 15:00 is the local time of maximum diurnal temperature.
The first term in (3) on the right is the global mean of the exospheric temperature (of the order of 1000 K). The second term [with P20 = 0.5(3 sin2(φ)−1)] represents the heat surplus at lower latitudes and a corresponding heat deficit at higher latitudes (Fig. 2a). A thermal wind system develops with winds toward the poles in the upper level and wind away from the poles in the lower level. The coefficient ΔT20 ≈ 0.004 is small because Joule heating in the aurora regions compensates that heat surplus even during quiet magnetospheric conditions. During disturbed conditions, however, that term becomes dominant changing sign so that now heat surplus is transported from the poles to the equator. The third term (with P10 = sin φ) represents heat surplus on the summer hemisphere and is responsible for the transport of excess heat from the summer into the winter hemisphere (Fig. 2b). Its relative amplitude is of the order ΔT10 ≃ 0.13. The fourth term (with P11(φ) = cos φ) is the dominant diurnal wave (the tidal mode (1,−2)). It is responsible for the transport of excess heat from the day time hemisphere into the night time hemisphere (Fig. 2d). Its relative amplitude is ΔT11≃ 0.15, thus of the order of 150 K. Additional terms (e.g., semiannual, semidiurnal terms and higher order terms) must be added to eq.(3). They are, however, of minor importance. Corresponding sums can be developed for density, pressure, and the various gas constituents.
Thermospheric storms.
In contrast to solar XUV radiation, magnetospheric disturbances, indicated on the ground by geomagnetic variations, show an unpredictable impulsive character, from short periodic disturbances of the order of hours to long standing giant storms of several day's duration. 
The reaction of the thermosphere to a large magnetospheric storm is called thermospheric storm. Since the heat input into the thermosphere occurs at high latitudes (mainly into the auroral regions), the heat transport represented by the term P20 in eq.(3) is reversed. In addition, due to the impulsive form of the disturbance, higher order terms are generated which, however, possess short decay times and thus quickly disappear. The sum of 
these modes determines the "travel time" of the disturbance to the lower latitudes, and thus the response time of the thermosphere with respect to the magnetospheric disturbance. Important for the development of an ionospheric storm is the increase of the ratio N2/O
during a thermospheric storm at middle and higher latitude. An increase of 
N2 increases the loss process of the ionospheric plasma and causes therefore a decrease of the electron density within the ionospheric F-layer (negative ionospheric storm).

</doc>
<doc id="47465" url="http://en.wikipedia.org/wiki?curid=47465" title="Aten">
Aten

Aten (also Aton, Egyptian "jtn") is the disk of the sun in ancient Egyptian mythology, and originally an aspect of Ra. The deified Aten is the focus of the monolatristic, henotheistic, monistic or monotheistic religion of Atenism established by Amenhotep IV, who later took the name Akhenaten in worship and recognition of Aten. In his poem "Great Hymn to the Aten", Akhenaten praises Aten as the creator, giver of life, and nurturing spirit of the world. Aten does not have a Creation Myth or family, but is mentioned in the Book of the Dead.
The worship of Aten was eradicated by Horemheb.
Overview.
The Aten, the sun-disk, is first referred to as a deity in The Story of Sinuhe from the 12th dynasty, in which the deceased king is described as rising as god to the heavens and uniting "with the sun-disk, the divine body merging with its maker." By analogy, the term "silver aten" was sometimes used to refer to the moon. The solar Aten was extensively worshipped as a god in the reign of Amenhotep III, when it was depicted as a falcon-headed man much like Ra. In the reign of Amenhotep III's successor, Amenhotep IV, the Aten became the central god of Egyptian state religion, and Amenhotep IV changed his name to Akhenaten to reflect his close link with the new supreme deity.
The full title of Akhenaten's god was "Ra-Horakhty who rejoices in the horizon, in his Name as the Light which is in the sun disc." (This is the title of the god as it appears on the numerous stelae which were placed to mark the boundaries of Akhenaten's new capital at Akhetaten, modern Amarna.) 
This lengthy name was often shortened to "Ra-Horus-Aten" or just "Aten" in many texts, but the god of Akhenaten raised to supremacy is considered a synthesis of very ancient gods viewed in a new and different way. The god is also considered to be both masculine and feminine simultaneously. All creation was thought to emanate from the god and to exist within the god. In particular, the god was not depicted in anthropomorphic (human) form, but as rays of light extending from the sun's disk.
Furthermore, the god's name came to be written within a cartouche, along with the titles normally given to a Pharaoh, another break with ancient tradition. Ra-Horus, more usually referred to as "Ra-Horakhty" ("Ra, who is Horus of the two horizons"), is a synthesis of two other gods, both of which are attested from very early on. During the Amarna period, this synthesis was seen as the invisible source of energy of the sun god, of which the visible manifestation was the Aten, the solar disk. Thus Ra-Horus-Aten was a development of old ideas which came gradually. The real change, as some see it, was the apparent abandonment of all other gods, especially Amun, and the debatable introduction of monotheism by Akhenaten. The syncretism is readily apparent in the Great Hymn to the Aten in which Re-Herakhty, Shu and Aten are merged into the creator god. Others see Akhenaten as a practitioner of an Aten monolatry, as he did not actively deny the existence of other gods; he simply refrained from worshipping any but the Aten.
Religion.
Principles of Aten's religion were recorded on the rock tomb walls of Akhetaten.
In the religion of Aten (Atenism), night is a time to fear. Work is done best when the sun, Aten, is present. Aten cares for every creature, and created a Nile river in the sky (rain) for the Syrians. Aten created all countries and people. The rays of the sun disk only holds out life to the royal family; everyone else receives life from Akhenaten and Nefertiti in exchange of loyalty for Aten.
When a good person dies, he/she continues to live in the City of Light for the dead in Akhetaten. The conditions are the same after death. Akhenaten judged whether someone should be granted an afterlife, and operated the scale of justice.
The explanation to why Aten could not be fully represented was that the god has gone beyond creation.
Worship.
The cult centre of Aten was at the new city Akhetaten; some other cult cities include Thebes, and Heliopolis. The principles of Aten's cult were recorded on the rock walls of tombs of Tall al-Amarnah. Significantly different from other ancient Egyptian temples, temples of Aten were colorful and open-roofed to allow the rays of the sun. Doorways had broken lintels and raised thresholds. No statues of Aten were allowed; those were seen as idolatry. Priests had less to do, since offerings (fruits, flowers, cakes) were limited, and oracles were not needed. Temples of Aten did not collect tax.
In the worship of Aten, the daily service of purification, anointment and clothing of the divine image was not performed. Incense was burnt several times a day. Hymns sang to Aten were accompanied by harp music. Aten's ceremonies in Akhetaten involved giving offerings to Aten with a swipe of the royal scepter.
Instead of barque processions, the royal family rode on a chariot on festival days.
Royal Titulary.
During the Amarna Period, the Aten was given a Royal Titulary (as he was considered to be king of all), with his names drawn in a cartouche. There were two forms of this title, the first had the names of other gods, and the second later one which was more 'singular' and referred only to the Aten himself. The early form has Re-Horakhti who rejoices in the Horizon, in his name Shu which is the Aten. The later form has Re, ruler of the two horizons who rejoices in the Horizon, in his name of light which is the Aten.
Variant vocalizations.
Egyptologists have vocalized the word variously as Aten, Aton, Atonu, and Itn.

</doc>
<doc id="47466" url="http://en.wikipedia.org/wiki?curid=47466" title="Mesopause">
Mesopause

The mesopause is the temperature minimum at the boundary between the mesosphere and the thermosphere atmospheric regions. Due to the lack of solar heating and very strong radiative cooling from carbon dioxide, the mesosphere is the coldest region on Earth with temperatures as low as -100 °C (-146 °F or 173 K). The altitude of the mesopause for many years was assumed to be at around 85 km (52 mi.), but observations to higher altitudes and modeling studies in the last 10 years have shown that in fact the mesopause consists of two minima - one at about 85 km and a stronger minimum at about 100 km. (62 mi.)
An interesting feature is that the summer mesopause is cooler than the winter. This is sometimes referred to as the "mesopause anomaly". It is due to a summer-to-winter circulation giving rise to upwelling at the summer pole and downwelling at the winter. Air rising will expand and cool resulting in a cold summer mesopause and conversely downwelling air results in compression and associated increase in temperature at the winter mesopause. In the mesosphere the summer-to-winter circulation is due to gravity wave dissipation, which deposits momentum against the mean east-west flow, resulting in a small north-south circulation.
In recent years the mesopause has also been the focus for studies on global climate change associated with increases in CO2. Unlike the troposphere, where greenhouse gases result in the atmosphere heating up, increased CO2 in the mesosphere acts to cool the atmosphere due to increased radiative emission by CO2. This results in a measurable effect - the mesopause should become cooler with increased CO2. Observations do show a decrease of temperature of the mesopause, though the magnitude of this decrease varies and is subject to further study. Modeling studies of this phenomenon have also been carried out.

</doc>
<doc id="47469" url="http://en.wikipedia.org/wiki?curid=47469" title="Amor">
Amor

Amor may refer to:

</doc>
<doc id="47470" url="http://en.wikipedia.org/wiki?curid=47470" title="List of Apollo asteroids">
List of Apollo asteroids

The Apollo asteroids are a group of near-Earth asteroids named after 1862 Apollo, the first asteroid of this group which was discovered by Karl Wilhelm Reinmuth. They are Earth-crosser asteroids that have orbital semi-major axis greater than that of the Earth (> 1 AU) but perihelion distances less than the Earth's aphelion distance (q < 1.017 AU). Some can get very close to the Earth, making them a potential threat to our planet (the closer their semi-major axis is to Earth's, the less eccentricity is needed for the orbits to cross). The February 15, 2013 Chelyabinsk meteor that exploded over the city of Chelyabinsk in the southern Urals region of Russia, injuring an estimated one thousand people with flying glass from broken windows, was an Apollo class asteroid.
The largest known Apollo asteroid is 1866 Sisyphus, with a diameter of about 8.5 km.
s of 2014[ [update]], there are 5766 known Apollo-class asteroids of which 832 are numbered. Near-Earth asteroids are not numbered until they have been observed at two or more oppositions.
Examples of known Apollo asteroids include:

</doc>
<doc id="47473" url="http://en.wikipedia.org/wiki?curid=47473" title="Algal bloom">
Algal bloom

An algal bloom is a rapid increase or accumulation in the population of algae (typically microscopic) in a water system. Cyanobacteria blooms are often called blue-green algae. Algal blooms may occur in freshwater as well as marine environments. Typically, only one or a small number of phytoplankton species are involved, and some blooms may be recognized by discoloration of the water resulting from the high density of pigmented cells.
Blooming.
Since 'algae' is a broad term including organisms of widely varying sizes, growth rates and nutrient requirements, there is no officially recognized threshold level as to what is defined as a bloom. For some species, algae can be considered to be blooming at concentrations reaching millions of cells per milliliter, while others form blooms of tens of thousands of cells per liter. The photosynthetic pigments in the algal cells determine the color of the algal bloom, and are thus often a greenish color, but they can also be a wide variety of other colors such as yellow, brown or red, depending on the species of algae and the type of pigments contained therein.
Bright green blooms in freshwater systems are frequently a result of cyanobacteria (colloquially known as blue-green algae) such as "Microcystis". Blooms may also consist of macroalgal (non-phytoplanktonic) species. These blooms are recognizable by large blades of algae that may wash up onto the shoreline.
Of particular note are harmful algal blooms (HABs), which are algal bloom events involving toxic or otherwise harmful phytoplankton such as dinoflagellates of the genus "Alexandrium" and "Karenia", or diatoms of the genus "Pseudo-nitzschia". Such blooms often take on a red or brown hue and are known colloquially as red tides.
Freshwater algal blooms.
Freshwater algal blooms are the result of an excess of nutrients, particularly some phosphates. The excess of nutrients may originate from fertilizers that are applied to land for agricultural or recreational purposes. They may also originate from household cleaning products containing phosphorus. These nutrients can then enter watersheds through water runoff. Excess carbon and nitrogen have also been suspected as causes. Presence of residual sodium carbonate acts as catalyst for the algae to bloom by providing dissolved carbon dioxide for enhanced photo synthesis in the presence of nutrients.
When phosphates are introduced into water systems, higher concentrations cause increased growth of algae and plants. Algae tend to grow very quickly under high nutrient availability, but each alga is short-lived, and the result is a high concentration of dead organic matter which starts to decay. The decay process consumes dissolved oxygen in the water, resulting in hypoxic conditions. Without sufficient dissolved oxygen in the water, animals and plants may die off in large numbers. Use of an Olszewski tube can help combat these problems with hypolimnetic withdrawal.
Blooms may be observed in freshwater aquariums when fish are overfed and excess nutrients are not absorbed by plants. These are generally harmful for fish, and the situation can be corrected by changing the water in the tank and then reducing the amount of food given.
Harmful algal blooms.
A "harmful algal bloom" (HAB) is an algal bloom that causes negative impacts to other organisms via production of natural toxins, mechanical damage to other organisms, or by other means. HABs are often associated with large-scale marine mortality events and have been associated with various types of shellfish poisonings.
Background.
In the marine environment, single-celled, microscopic, plant-like organisms naturally occur in the well-lit surface layer of any body of water. These organisms, referred to as phytoplankton or microalgae, form the base of the food web upon which nearly all other marine organisms depend. Of the 5000+ species of marine phytoplankton that exist worldwide, about 2% are known to be harmful or toxic. Blooms of harmful algae can have large and varied impacts on marine ecosystems, depending on the species involved, the environment where they are found, and the mechanism by which they exert negative effects.
Harmful algal blooms have been observed to cause adverse effects to a wide variety of aquatic organisms, most notably marine mammals, sea turtles, seabirds and finfish. The impacts of HAB toxins on these groups can include harmful changes to their developmental, immunological, neurological, or reproductive capacities. The most conspicuous effects of HABs on marine wildlife are large-scale mortality events associated with toxin-producing blooms. For example, a mass mortality event of 107 bottlenose dolphins occurred along the Florida panhandle in the spring of 2004 due to ingestion of contaminated menhaden with high levels of brevetoxin. Manatee mortalities have also been attributed to brevetoxin but unlike dolphins, the main toxin vector was endemic seagrass species ("Thalassia testudinum") in which high concentrations of brevetoxins were detected and subsequently found as a main component of the stomach contents of manatees.
Additional marine mammal species, like the highly endangered North Atlantic Right Whale, have been exposed to neurotoxins by preying on highly contaminated zooplankton. With the summertime habitat of this species overlapping with seasonal blooms of the toxic dinoflagellate "Alexandrium fundyense", and subsequent copepod grazing, foraging right whales will ingest large concentrations of these contaminated copepods. Ingestion of such contaminated prey can affect respiratory capabilities, feeding behavior, and ultimately the reproductive condition of the population.
Immune system responses have been affected by brevetoxin exposure in another critically endangered species, the Loggerhead sea turtle. Brevetoxin exposure, via inhalation of aerosolized toxins and ingestion of contaminated prey, can have clinical signs of increased lethargy and muscle weakness in loggerhead sea turtles causing these animals to wash ashore in a decreased metabolic state with increases of immune system responses upon blood analysis.
Examples of common harmful effects of HABs include:
Due to their negative economic and health impacts, HABs are often carefully monitored.
HABs occur in many regions of the world, and in the United States are recurring phenomena in multiple geographical regions. The Gulf of Maine frequently experiences blooms of the dinoflagellate "Alexandrium fundyense", an organism that produces saxitoxin, the neurotoxin responsible for paralytic shellfish poisoning. The well-known "Florida red tide" that occurs in the Gulf of Mexico is a HAB caused by "Karenia brevis", another dinoflagellate which produces brevetoxin, the neurotoxin responsible for neurotoxic shellfish poisoning. California coastal waters also experience seasonal blooms of "Pseudo-nitzschia", a diatom known to produce domoic acid, the neurotoxin responsible for amnesic shellfish poisoning. Off the west coast of South Africa, HABs caused by "Alexandrium catanella" occur every spring. These blooms of organisms cause severe disruptions in fisheries of these waters as the toxins in the phytoplankton cause filter-feeding shellfish in affected waters to become poisonous for human consumption.
If the HAB event results in a high enough concentration of algae the water may become discoloured or murky, varying in colour from purple to almost pink, normally being red or green. Not all algal blooms are dense enough to cause water discolouration.
Red tides.
Red tide is a term often used synonymously with HABs in marine coastal areas, however the term is misleading since algal blooms can be a wide variety of colors and growth of algae is unrelated to the tides. The term 'algal bloom' or 'harmful algal bloom' has since replaced 'red tide' as the appropriate description of this phenomenon.
Causes of HABs.
It is unclear what causes HABs; their occurrence in some locations appears to be entirely natural, while in others they appear to be a result of human activities. Furthermore, there are many different species of algae that can form HABs, each with different environmental requirements for optimal growth. The frequency and severity of HABs in some parts of the world have been linked to increased nutrient loading from human activities. In other areas, HABs are a predictable seasonal occurrence resulting from coastal upwelling, a natural result of the movement of certain ocean currents. The growth of marine phytoplankton (both non-toxic and toxic) is generally limited by the availability of nitrates and phosphates, which can be abundant in coastal upwelling zones as well as in agricultural run-off. The type of nitrates and phosphates available in the system are also a factor, since phytoplankton can grow at different rates depending on the relative abundance of these substances (e.g. ammonia, urea, nitrate ion). A variety of other nutrient sources can also play an important role in affecting algal bloom formation, including iron, silica or carbon. Coastal water pollution produced by humans (including iron fertilization) and systematic increase in sea water temperature have also been suggested as possible contributing factors in HABs. Other factors such as iron-rich dust influx from large desert areas such as the Sahara are thought to play a role in causing HABs. Some algal blooms on the Pacific coast have also been linked to natural occurrences of large-scale climatic oscillations such as El Niño events. While HABs in the Gulf of Mexico have been occurring since the time of early explorers such as Cabeza de Vaca, it is unclear what initiates these blooms and how large a role anthropogenic and natural factors play in their development. It is also unclear whether the apparent increase in frequency and severity of HABs in various parts of the world is in fact a real increase or is due to increased observation effort and advances in species identification technology.
Researching Solutions.
The decline of filter-feeding shellfish populations, such as oysters, likely contribute to HAB occurrence. As such, numerous research projects are assessing the potential of restored shellfish populations to reduce HAB occurrence.
Since many Algal blooms are caused by a major influx of nutrient-rich runoff into a water body, programs to treat wastewater, reduce the overuse of fertilizers in agriculture and reducing the bulk flow of runoff can be effective for reducing severe algal blooms at river mouths, estuaries, and the ocean directly in front of the river's mouth.
See also.
</dl>

</doc>
<doc id="47474" url="http://en.wikipedia.org/wiki?curid=47474" title="Aperture">
Aperture

In optics, an aperture is a hole or an opening through which light travels. More specifically, the aperture of an optical system is the opening that determines the cone angle of a bundle of rays that come to a focus in the image plane. The aperture determines how collimated the admitted rays are, which is of great importance for the appearance at the image plane. If an aperture is narrow, then highly collimated rays are admitted, resulting in a sharp focus at the image plane. If an aperture is wide, then uncollimated rays are admitted, resulting in a sharp focus only for rays with a certain focal length. This means that a wide aperture results in an image that is sharp around what the lens is focusing on. The aperture also determines how many of the incoming rays are actually admitted and thus how much light reaches the image plane (the narrower the aperture, the darker the image for a given exposure time). In the human eye, the pupil is the aperture.
An optical system typically has many openings, or structures that limit the ray bundles (ray bundles are also known as "pencils" of light). These structures may be the edge of a lens or mirror, or a ring or other fixture that holds an optical element in place, or may be a special element such as a diaphragm placed in the optical path to limit the light admitted by the system. In general, these structures are called stops, and the aperture stop is the stop that determines the ray cone angle, or equivalently the brightness, at an image point.
In some contexts, especially in photography and astronomy, "aperture" refers to the "diameter" of the aperture stop rather than the physical stop or the opening itself. For example, in a telescope the aperture stop is typically the edges of the objective lens or mirror (or of the mount that holds it). One then speaks of a telescope as having, for example, a 100 centimeter "aperture". Note that the aperture stop is not necessarily the smallest stop in the system. Magnification and demagnification by lenses and other elements can cause a relatively large stop to be the aperture stop for the system.
Sometimes stops and diaphragms are called apertures, even when they are not the aperture stop of the system.
The word aperture is also used in other contexts to indicate a system which blocks off light outside a certain region. In astronomy for example, a photometric aperture around a star usually corresponds to a circular window around the image of a star within which the light intensity is assumed.
Application.
The aperture stop is an important element in most optical designs. Its most obvious feature is that it limits the amount of light that can reach the image/film plane. This can be either unavoidable, as in a telescope where one wants to collect as much light as possible; or deliberate, to prevent saturation of a detector or overexposure of film. In both cases, the size of the aperture stop is constrained by things other than the amount of light admitted; however:
In addition to an aperture stop, a photographic lens may have one or more "field stops", which limit the system's field of view. When the field of view is limited by a field stop in the lens (rather than at the film or sensor) vignetting results; this is only a problem if the resulting field of view is less than was desired.
The biological pupil of the eye is its aperture in optics nomenclature; the iris is the diaphragm that serves as the aperture stop. Refraction in the cornea causes the effective aperture (the entrance pupil in optics parlance) to differ slightly from the physical pupil diameter. The entrance pupil is typically about 4 mm in diameter, although it can range from 2 mm (f/8.3) in a brightly lit place to 8 mm (f/2.1) in the dark.
In astronomy, the diameter of the aperture stop (called the "aperture") is a critical parameter in the design of a telescope. Generally, one would want the "aperture" to be as large as possible, to collect the maximum amount of light from the distant objects being imaged. The size of the aperture is limited, however, in practice by considerations of cost and weight, as well as prevention of aberrations (as mentioned above).
Apertures are also used in laser energy control, focusing, diffractions/patterns, and beam cleaning. Laser applications include spatial filters, Q-switching, high intensity x-ray control.
In light microscopy, the word aperture may be used with reference to either the condenser (changes angle of light onto specimen field), field iris (changes area of illumination) or possibly objective lens (forms primary image). "See" Optical microscope.
In photography.
The aperture stop of a photographic lens can be adjusted to control the amount of light reaching the film or image sensor. In combination with variation of shutter speed, the aperture size will regulate the film's or image sensor's degree of exposure to light. Typically, a fast shutter will require a larger aperture to ensure sufficient light exposure, and a slow shutter will require a smaller aperture to avoid excessive exposure.
A device called a diaphragm usually serves as the aperture stop, and controls the aperture. The diaphragm functions much like the iris of the eye – it controls the effective diameter of the lens opening. Reducing the aperture size increases the depth of field, which describes the extent to which subject matter lying closer than or farther from the actual plane of focus appears to be in focus. In general, the smaller the aperture (the larger the number), the greater the distance from the plane of focus the subject matter may be while still appearing in focus.
The lens aperture is usually specified as an f-number, the ratio of focal length to effective aperture diameter. A lens typically has a set of marked "f-stops" that the f-number can be set to. A lower f-number denotes a greater aperture opening which allows more light to reach the film or image sensor. The photography term "one f-stop" refers to a factor of √2 (approx. 1.41) change in f-number, which in turn corresponds to a factor of 2 change in light intensity.
Aperture priority is a semi-automatic shooting mode used in cameras. It permits the photographer to select an aperture setting and let the camera to decide the shutter speed and sometimes also ISO sensitivity for the correct exposure. This is also referred to as Aperture Priority Auto Exposure, A mode, AV mode (aperture-value mode), or semi-auto mode.
Typical ranges of apertures used in photography are about f/2.8–f/22 or f/2–f/16, covering 6 stops, which may be divided into wide, middle, and narrow of 2 stops each, roughly (using round numbers) f/2–f/4, f/4–f/8, and f/8–f/16 or (for a slower lens) f/2.8–f/5.6, f/5.6–f/11, and f/11–f/22. These are not sharp divisions, and ranges for specific lenses vary.
Maximum and minimum apertures.
The specifications for a given lens typically include the maximum and minimum aperture sizes, for example, f/1.4–f/22. In this case f/1.4 is the maximum aperture (the widest opening), and f/22 is the minimum aperture (the smallest opening). The maximum aperture opening tends to be of most interest, and is always included when describing a lens. This value is also known as the lens "speed", as it affects the exposure time. The aperture is proportional to the square root of the light admitted, and thus inversely proportional to the square root of required exposure time, such that an aperture of f/2 allows for exposure times one quarter that of f/4.
Lenses with apertures opening f/2.8 or wider are referred to as "fast" lenses, although the specific point has changed over time (for example, in the 1911 Encyclopaedia Britannica aperture openings wider than f/6 were considered fast). The fastest lenses for the common 35 mm film format in general production have apertures of f/1.2 or f/1.4, with more at f/1.8 and f/2.0, and many at f/2.8 or slower; f/1.0 is unusual, though sees some use. When comparing "fast" lenses, the image format used must be considered. Lenses designed for a small format such as half frame or APS-C need to project only a much smaller image circle than any lens used for example for large format photography. Thus the optical elements built into the lens can be far smaller and cheaper.
In exceptional circumstances lenses can have even wider apertures with f-numbers smaller than 1.0; see lens speed: fast lenses for a detailed list. For instance, in photography, both the current Leica Noctilux-M 50mm ASPH and a 1960s-era Canon 50mm rangefinder lens have a maximum aperture of f/0.95. Such lenses tend to be optically complex and very expensive; at launch, in September 2008, the Leica Noctilux retailed for $11,000. However, significantly more affordable examples have appeared in recent years, such as the Voigtlander 17.5mm f/0.95, 25mm f/0.95 and 42.5mm f/0.95 manual focus lenses for the Micro Four Thirds System, each of which retails for approximately US$1,000. 
Professional lenses for some movie cameras have f-numbers as small as f/0.75. Stanley Kubrick's film "Barry Lyndon" has scenes shot by candlelight with a NASA/Zeiss 50mm f/0.7, the fastest lens in film history. Beyond the expense, these lenses have limited application due to the correspondingly shallower depth of field – the scene must either be shallow, shot from a distance, or will be significantly defocused, though this may be a desired effect.
Zoom lenses typically have a maximum relative aperture (minimum f-number) of f/2.8 to f/6.3 through their range. High-end lenses will have a constant aperture, such as f/2.8 or f/4, which means that the relative aperture will stay the same throughout the zoom range. A more typical consumer zoom will have a variable maximum relative aperture, since it is harder and more expensive to keep the maximum relative aperture proportional to focal length at long focal lengths; f/3.5 to f/5.6 is an example of a common variable aperture range in a consumer zoom lens.
By contrast, the minimum aperture does not depend on the focal length – it is limited by how narrowly the aperture closes, not the lens design – and is instead generally chosen based on practicality: very small apertures have lower sharpness due to diffraction, while the added depth of field is not generally useful, and thus there is generally little benefit in using such apertures. Accordingly, DSLR lens typically have minimum aperture of f/16, f/22, or f/32, while large format may go down to f/64, as reflected in the name of Group f/64. Depth of field is a significant concern in macro photography, however, and there one sees smaller apertures. For example, the Canon MP-E 65mm can have effective aperture (due to magnification) as small as f/96. The pinhole optic for Lensbaby creative lenses has an aperture of just f/177.
Aperture area.
The amount of light captured by a lens is proportional to the area of the aperture, equal to:
Where the two equivalent forms are related via the f-number "N = f / D", with focal length "f" and aperture diameter "D".
The focal length value is not required when comparing two lenses of the same focal length; a value of 1 can be used instead, and the other factors can be dropped as well, leaving area proportion to the reciprocal square of the f-number "N".
If two cameras of different format sizes and focal lengths have the same angle of view, and the same aperture area, they gather the same amount of light from the scene. In that case, the relative focal-plane illuminance, however, would depend only on the f-number "N", so it is less in the camera with the larger format, longer focal length, and higher f-number. This assumes both lenses have identical transmissivity.
Aperture control.
Most SLR cameras provide "automatic aperture control", which allows viewing and metering at the lens’s maximum aperture, stops the lens down to the working aperture during exposure, and returns the lens to maximum aperture after exposure.
The first SLR cameras with internal (“through-the-lens” or “TTL”) meters (e.g., the Pentax Spotmatic) required that the lens be stopped down to the working aperture when taking a meter reading. With a small aperture, this darkened the
viewfinder, making viewing, focusing, and composition difficult.
Subsequent models soon incorporated mechanical coupling between the lens and the camera body, indicating the working aperture to the camera while allowing the lens to be at its maximum aperture for composition and focusing; this feature became known as automatic aperture control or automatic diaphragm control.
For some lenses, including a few long telephotos, lenses mounted on bellows, and perspective-control and tilt/shift lenses, the mechanical linkage was impractical, and automatic aperture control was not provided. Many such lenses incorporated a feature known as a "preset" aperture, which allows the lens to be set to working aperture and then quickly switched between working aperture and full aperture without looking at the aperture control. Typical operation might be to establish rough composition, set the working aperture for metering, return to full aperture for a final check of focus and composition, and focusing, and finally, return to working aperture just before exposure. Although slightly easier than stopped-down metering, operation is less convenient than automatic operation. Preset aperture controls have taken several forms; the most common has been the use of essentially two lens aperture rings, with one ring setting the aperture and the other serving as a limit stop when switching to working aperture. Examples of lenses with this type of preset aperture control are the Nikon PC Nikkor 28 mm f/3.5 and the SMC Pentax Shift 6×7 75 mm f/4.5. The Nikon PC Micro-Nikkor 85 mm f/2.8D lens incorporates a mechanical pushbutton that sets working aperture when pressed and restores full aperture when pressed a second time.
Canon EF lenses, introduced in 1987, have electromagnetic diaphragms, eliminating the need for a mechanical linkage between the camera and the lens, and allowing automatic aperture control with the Canon TS-E tilt/shift lenses. Nikon PC-E perspective-control lenses, introduced in 2008, also have electromagnetic diaphragms. Automatic aperture control is provided with the newer Nikon digital SLR cameras; with some earlier cameras, the lenses offer preset aperture control by means of a pushbutton that controls the electromagnetic diaphragm.
Optimal aperture.
Optimal aperture depends both on optics (the depth of the scene versus diffraction), and on the performance of the lens.
Optically, as a lens is stopped down, the defocus blur at the Depth of Field (DOF) limits decreases but diffraction blur increases. The presence of these two opposing factors implies a point at which the combined blur spot is minimized (Gibson 1975, 64); at that point, the f-number is optimal for image sharpness, for this given depth of field – a wider aperture (lower "f"-number) causes more defocus, while a narrower aperture (higher "f"-number) causes more diffraction.
As a matter of performance, lenses often do not perform optimally when fully opened, and thus generally have better sharpness when stopped down some – note that this is sharpness in the plane of critical focus, setting aside issues of depth of field. Beyond a certain point there is no further sharpness benefit to stopping down, and the diffraction begins to become significant. There is accordingly a sweet spot, generally in the f/4 – f/8 range, depending on lens, where sharpness is optimal, though some lenses are designed to perform optimally when wide open. How significant this is varies between lenses, and opinions differ on how much practical impact this has.
While optimal aperture can be determined mechanically, how much sharpness is "required" depends on how the image will be used – if the final image is viewed under normal conditions (e.g., an 8″×10″ image viewed at 10″), it may suffice to determine the f-number using criteria for minimum required sharpness, and there may be no practical benefit from further reducing the size of the blur spot. But this may not be true if the final image is viewed under more demanding conditions, e.g., a very large final image viewed at normal distance, or a portion of an image enlarged to normal size (Hansma 1996). Hansma also suggests that the final-image size may not be known when a photograph is taken, and obtaining the maximum practicable sharpness allows the decision to make a large final image to be made at a later time; see also critical sharpness.
Equivalent aperture range.
In digital photography, the 35mm-equivalent aperture range is sometimes considered to be more important than the actual f-number. Equivalent aperture is the f-number adjusted to correspond to the f-number of the same size absolute aperture diameter on a lens with a 35mm equivalent focal length. Smaller equivalent f-numbers are expected to lead to higher image quality based on more total light from the subject, as well as lead to reduced depth of field. For example, a Sony Cyber-shot DSC-RX10 uses a 1" sensor, 24–200 mm with maximum aperture constant along the zoom range; f/2.8 has equivalent aperture range f/7.6, which is a lower equivalent f-number than some other f/2.8 cameras with smaller sensors.
In scanning or sampling.
The terms "scanning aperture" and "sampling aperture" are often used to refer to the opening through which an image is sampled, or scanned, for example in a Drum scanner, an image sensor, or a television pickup apparatus. The sampling aperture can be a literal optical aperture, that is, a small opening in space, or it can be a time-domain aperture for sampling a signal waveform.
For example, film grain is quantified as "graininess" via a measurement of film density fluctuations as seen through a 0.048 mm sampling aperture.

</doc>
<doc id="47476" url="http://en.wikipedia.org/wiki?curid=47476" title="Altimeter">
Altimeter

An altimeter or an altitude meter is an instrument used to measure the altitude of an object above a fixed level. The measurement of altitude is called altimetry, which is related to the term bathymetry, the measurement of depth underwater.
Pressure altimeter.
Altitude can be determined based on the measurement of atmospheric pressure. The greater the altitude the lower the pressure. When a barometer is supplied with a nonlinear calibration so as to indicate altitude, the instrument is called a pressure altimeter or barometric altimeter. A pressure altimeter is the altimeter found in most aircraft, and skydivers use wrist-mounted versions for similar purposes. Hikers and mountain climbers use wrist-mounted or hand-held altimeters, in addition to other navigational tools such as a map, magnetic compass, or GPS receiver.
The calibration of an altimeter follows the equation
where c is a constant, T is the absolute temperature, P is the pressure at altitude z, and Po is the pressure at sea level. The constant c depends on the acceleration of gravity and the molar mass of the air.
However, one must be aware that this type of altimeter relies on "density altitude" and its readings can vary by hundreds of feet owing to a sudden change in air pressure, such as from a cold front, without any actual change in altitude.
Use in hiking and climbing.
A barometric altimeter, used along with a topographic map, can help to verify one's location. It is more reliable, and often more accurate, than a GPS receiver for measuring altitude; GPS altimeters may be unavailable, for example, when one is deep in a canyon, or may give wildly inaccurate altitudes when all available satellites are near the horizon. Because barometric pressure changes with the weather, hikers must periodically recalibrate their altimeters when they reach a known altitude, such as a trail junction or peak marked on a topographical map.
Skydiving.
Altimeter is the most important piece of skydiving equipment, after the parachute itself. Altitude awareness is crucial at all times during the jump, and determines the appropriate response to maintain safety.
Since altitude awareness is so important in skydiving, there is a wide variety of altimeter designs made specifically for use in the sport, and a non-student skydiver will typically use two or more altimeters in a single jump:
The exact choice of altimeters depends heavily on the individual skydiver's preferences, experience level, primary disciplines, as well as the type of the jump. On one end of the spectrum, a low-altitude demonstration jump with water landing and no free fall might waive the mandated use of altimeters and use none at all. In contrast, a jumper doing freeflying jumps and flying a high performance canopy might use a mechanical analogue altimeter for easy reference in free fall, an in-helmet audible for breakaway altitude warning, additionally programmed with swoop guide tones for canopy flying, as well as a digital altimeter on an armband for quickly glancing the precise altitude on approach. Another skydiver doing similar types of jumps might wear a digital altimeter for their primary visual one, preferring the direct altitude readout of a numeric display.
Use in aircraft.
In aircraft, an aneroid barometer measures the atmospheric pressure from a static port outside the aircraft. Air pressure decreases with an increase of altitude—approximately 100 hectopascals per 800 meters or one inch of mercury per 1000 feet near sea level.
The aneroid altimeter is calibrated to show the pressure directly as an altitude above mean sea level, in accordance with a mathematical model defined by the International Standard Atmosphere (ISA). Older aircraft used a simple aneroid barometer where the needle made less than one revolution around the face from zero to full scale. This design evolved to altimeters with a primary needle and one or more secondary needles that show the number of revolutions, similar to a clock face. In other words, each needle points to a different digit of the current altitude measurement. However this design has fallen out of favor due to the risk of misreading in stressful situations. The design evolved further to drum-type altimeters, the final step in analogue instrumentation, where each revolution of a single needle accounted for 1,000 feet, with thousand foot increments recorded on a numerical odometer-type drum. To determine altitude, a pilot had first to read the drum to determine the thousands of feet, then look at the needle for the hundreds of feet. Modern analogue altimeters in transport aircraft are typically drum-type. The latest development in clarity is an Electronic flight instrument system with integrated digital altimeter displays. This technology has trickled down from airliners and military planes until it is now standard in many general aviation aircraft.
Modern aircraft use a "sensitive altimeter,". On a sensitive altimeter, the sea-level reference pressure can be adjusted with a setting knob. The reference pressure, in inches of mercury in Canada and the US, and hectopascals (previously millibars) elsewhere, is displayed in the small "Kollsman window," on the face of the aircraft altimeter. This is necessary, since sea level reference atmospheric pressure at a given location varies over time with temperature and the movement of pressure systems in the atmosphere.
In aviation terminology, the regional or local air pressure at mean sea level (MSL) is called the QNH or "altimeter setting", and the pressure that will calibrate the altimeter to show the height above ground at a given airfield is called the QFE of the field. An altimeter cannot, however, be adjusted for variations in air temperature. Differences in temperature from the ISA model will accordingly cause errors in indicated altitude.
In aerospace, the mechanical stand-alone altimeters which are based on diaphragm bellows were replaced by integrated measurement system which is called Air data computer (ADC). This module measures altitude, speed of flight and outside temperature to provide more precise output data allowing automatic flight control and Flight level division. Multiple altimeters can be used to design a Pressure Reference System to provide information about airplane's position angles to further support Inertial navigation system calculations.
Use in ground effect vehicle.
After extensive research and experimentation, it has been shown that "phase radio-altimeters" are most suitable for ground effect vehicles, as compared to laser, isotropic or ultrasonic altimeters.
Sonic altimeter.
In 1931, the US Army Air Corps and General Electric tested a sonic altimeter for aircraft, which was considered more reliable and accurate than one that relied on air pressure, when heavy fog or rain was present. The new altimeter used a series of high-pitched sounds like those made by a bat to measure the distance from the aircraft to the surface, which on return to the aircraft was converted to feet shown on a gauge inside the aircraft cockpit.
Radar altimeter.
A radar altimeter measures altitude more directly, using the time taken for a radio signal to reflect from the surface back to the aircraft. Alternatively, Frequency Modulated Continuous-wave radar can be used. The greater the frequency shift the further the distance traveled. This method can achieve much better accuracy than the pulsed radar for the same outlay and radar altimeters that use frequency modulation are industry standard. The radar altimeter is used to measure height above ground level during landing in commercial and military aircraft. Radar altimeters are also a component of terrain avoidance warning systems, warning the pilot if the aircraft is flying too low, or if there is rising terrain ahead. Radar altimeter technology is also used in terrain-following radar allowing fighter aircraft to fly at very low altitude.
Global Positioning System.
Global Positioning System (GPS) receivers can also determine altitude by trilateration with four or more satellites. In aircraft, altitude determined using autonomous GPS is not precise or accurate enough to supersede the pressure altimeter without using some method of augmentation. In hiking and climbing, it is not uncommon to find that the altitude measured by GPS is off by as much as 400 feet depending on satellite orientation. 
Other modes of transport.
The altimeter is an instrument optional in off-road vehicles to aid in navigation. Some high-performance luxury cars that were never intended to leave paved roads, such as the Duesenberg in the 1930s, have also been equipped with altimeters.
Hikers and mountaineers use hand-held or wrist-mounted barometric altimeters, as do skydivers.
Diesel submarines have barometers installed on them to monitor vacuum being pulled in the event that the snorkel closes while the diesels are running and, as a consequence, sucking the air out of the boat.
Satellites.
Satellites such as Seasat and TOPEX/Poseidon use advanced dual-band radar altimeters to measure height from a spacecraft. That measurement, coupled with orbital elements (possibly augmented by GPS), enables determination of the terrain. The two different wavelengths of radio waves used permit the altimeter to automatically correct for varying delays in the ionosphere.
Spaceborne radar altimeters have proven to be superb tools for mapping ocean-surface topography, the hills and valleys of the sea surface. These instruments send a microwave pulse to the ocean's surface and record the time it takes to return. A microwave radiometer corrects any delay that may be caused by water vapor in the atmosphere. Other corrections are also required to account for the influence of electrons in the ionosphere and the dry air mass of the atmosphere. Combining these data with the precise location of the spacecraft makes it possible to determine sea-surface height to within a few centimetres (about one inch). The strength and shape of the returning signal also provides information on wind speed and the height of ocean waves. These data are used in ocean models to calculate the speed and direction of ocean currents and the amount and location of heat stored in the ocean, which in turn reveals global climate variations.

</doc>
<doc id="47477" url="http://en.wikipedia.org/wiki?curid=47477" title="Ames Research Center">
Ames Research Center

Ames Research Center (ARC), commonly known as NASA Ames, is a major NASA research center at Moffett Federal Airfield in California's Silicon Valley. Originally founded as the second National Advisory Committee for Aeronautics laboratory (NACA), that agency was dissolved and its assets and personnel transferred to the newly created National Aeronautics and Space Administration (NASA) on October 1, 1958. NASA Ames is named in honor of Joseph Sweetman Ames, a physicist and one of the founding members of NACA. At last estimate NASA has over US$3.0 billion in capital equipment, 2,300 research personnel and a US$860 million annual budget.
Ames was originally founded to conduct wind-tunnel research on the aerodynamics of propeller-driven aircraft; however, its role has expanded to encompass spaceflight and information technology. Ames plays a role in many NASA missions. It provides leadership in astrobiology; small satellites; robotic lunar exploration; the search for habitable planets; supercomputing; intelligent/adaptive systems; advanced thermal protection; and airborne astronomy. Ames also develops tools for a safer, more efficient national airspace. The center's current director is Eugene Tu.
The site is mission center for several key current missions ("Kepler", the "Lunar CRater Observation and Sensing Satellite" (LCROSS) mission, "Stratospheric Observatory for Infrared Astronomy" (SOFIA), Interface Region Imaging Spectrograph) and a major contributor to the "new exploration focus" as a participant in the Orion crew exploration vehicle.
Missions.
Although Ames is a NASA Research Center, and not a flight center, it has nevertheless been closely involved in a number of astronomy and space missions.
The Pioneer program's eight successful space missions from 1965 to 1978 were managed by Charles Hall at Ames, initially aimed at the inner solar system. By 1972, it supported the bold flyby missions to Jupiter and Saturn with Pioneer 10 and Pioneer 11. Those two missions were trail blazers (radiation environment, new moons, gravity-assist flybys) for the planners of the more complex Voyager 1 and Voyager 2 missions, launched five years later. In 1978, the end of the program saw a return to the inner solar system, with the Pioneer Venus Orbiter and Multiprobe, this time using orbital insertion rather than flyby missions.
Lunar Prospector was the third mission selected by NASA for full development and construction as part of the Discovery Program. At a cost of $62.8 million, the 19-month mission was put into a low polar orbit of the Moon, accomplishing mapping of surface composition and possible polar ice deposits, measurements of magnetic and gravity fields, and study of lunar outgassing events. Based on Lunar Prospector Neutron Spectrometer (NS) data, mission scientists have determined that there is indeed water ice in the polar craters of the Moon. The mission ended July 31, 1999 when the orbiter was guided to an impact into a crater near the lunar south pole in an (unsuccessful) attempt to analyze lunar polar water by vaporizing it to allow spectroscopic characterization from Earth telescopes.
The 11-pound (5 kg) GeneSat-1, carrying bacteria inside a miniature laboratory, was launched on December 16, 2006. The very small NASA satellite has proven that scientists can quickly design and launch a new class of inexpensive spacecraft—and conduct significant science.
The "Lunar CRater Observation and Sensing Satellite" (LCROSS) mission to look for water on the moon was a 'secondary payload spacecraft.' LCROSS began its trip to the moon on the same rocket as the Lunar Reconnaissance Orbiter (LRO), which continues to conduct a different lunar task. It launched in April 2009 on an Atlas V rocket from Kennedy Space Center, Florida.
"Kepler" is NASA's first mission capable of finding Earth-size and smaller planets. The Kepler mission will monitor the brightness of stars to find planets that pass in front of them during the planets' orbits. During such passes or 'transits,' the planets will slightly decrease the star's brightness.
"Stratospheric Observatory for Infrared Astronomy" (SOFIA) is a joint venture of the U.S. and German aerospace agencies, NASA and the DLR to make an infrared telescope platform that can fly at altitudes high enough to be in the infrared-transparent regime above the water vapor in the Earth's atmosphere. The aircraft is supplied by the U.S., and the infrared telescope by Germany. Modifications of the Boeing 747SP airframe to accommodate the telescope, mission-unique equipment and large external door were made by L-3 Communications Integrated Systems of Waco, Texas.
The Interface Region Imaging Spectrograph mission is a partnership with the Lockheed Martin Solar and Astrophysics Laboratory to understand the processes at the boundary between the Sun's chromosphere and corona. This mission is sponsored by the NASA Small Explorer program.
The "Lunar Atmosphere Dust Environment Explorer" (LADEE) mission has been developed by NASA Ames. This successfully launched to the Moon on September 6, 2013.
In addition, Ames has played a support role in a number of missions, most notably the Mars Pathfinder and Mars Exploration Rover missions, where the Ames Intelligent Robotics Laboratory played a key role. NASA Ames was a partner on the Mars "Phoenix", a Mars Scout Program mission to send a high-latitude lander to Mars, deployed a robotic arm to dig trenches up to 1.6 feet (one half meter) into the layers of water ice and analyzing the soil composition. Ames is also a partner on the Mars Science Laboratory, a next generation Mars rover to explore for signs of organics and complex molecules.
Air traffic control automation research.
The Aviation Systems Division conducts research and development in two primary areas: air traffic management, and high-fidelity flight simulation. For air traffic management, researchers are creating and testing concepts to allow for up to three times today's level of aircraft in the national airspace. Automation and its attendant safety consequences are key foundations of the concept development. Historically, the division has developed products that have subsequently been implemented for the flying public, such as the Traffic Management Advisor, which is being deployed nationwide. For high-fidelity flight simulation, the division operates the world's largest flight simulator (the Vertical Motion Simulator), a Level-D 747-400 simulator, and a panoramic air traffic control tower simulator. These simulators have been used for a variety of purposes including continued training for space shuttle pilots, development of future spacecraft handling qualities, helicopter control system testing, Joint Strike Fighter evaluations, and accident investigations. Personnel in the division have a variety of technical backgrounds, including guidance and control, flight mechanics, flight simulation, and computer science. Customers outside NASA have included the FAA, DOD, DHS, DOT, NTSB, Lockheed Martin, and Boeing.
Information technology.
Ames is the home of NASA's large research and development divisions in Advanced Supercomputing, "Human Factors", and Artificial Intelligence ( Intelligent Systems). These Research & Development organizations support NASA's Exploration efforts, as well as the continued operations of the International Space Station, and the space science and Aeronautics work across NASA. The center also runs and maintains the E Root nameserver of the DNS System.
The Intelligent Systems Division is NASA's leading R&D Division developing advanced intelligent software and systems for all of NASA Mission Directorates. It provides software expertise for aeronautics, space science missions, International Space Station, and the Crewed Exploration Vehicle (CEV. The first AI in space (Deep Space 1) was developed from Code TI, as is the MAPGEN software that daily plans the activities for the Mars Exploration Rovers, the same core reasoner is used for Ensemble to operate Phoenix Lander, and the planning system for the International Space Station's solar arrays. Integrated System Health Management for the International Space Station's control moment gyroscopes, collaborative systems with semantic search tools, and robust software engineering round out the scope of Code TI's work.
The Human Systems Integration Division "advances human-centered design and operations of complex aerospace systems through analysis, experimentation, and modeling of human performance and human-automation interaction to make dramatic improvements in safety, efficiency and mission success". For decades, the Human Systems Integration Division has been on the leading edge of human-centered aerospace research. The Division is home to over 100 researchers, contractors and administrative staff.
Ames operates one of the world′s fastest supercomputers, Pleiades, which will be further enhanced and is scheduled to reach 10 petaflops of processing power by 2012.
In September 2009, Ames launched NEBULA as a fast and powerful Cloud Computing Platform to handle NASA's massive data sets that complied with security requirements. This innovative pilot uses open-source components, complies with FISMA and can scale to Government-sized demands while being extremely energy efficient. In July 2010, NASA CTO Chris C. Kemp open sourced Nova, the technology behind the NEBULA Project in collaboration with Rackspace, launching OpenStack. OpenStack has subsequently become one of the largest and fastest growing open source projects in the history of computing, and as of 2014[ [update]] has been included in most major distributions of linux including Red Hat, Oracle, HP, SUSE, and Canonical.
Image processing.
NASA Ames was one of the first locations in the world to conduct research on image processing of satellite-platform aerial photography. Some of the pioneering techniques of contrast enhancement using Fourier analysis were developed at Ames in conjunction with researchers at ESL Inc.
Wind tunnels.
The NASA Ames Research Center wind tunnels are known not only for their immense size, but also for their diverse characteristics that enable various kinds of scientific and engineering research.
ARC Unitary Plan Wind Tunnel.
The Unitary Plan Wind Tunnel (UPWT) was completed in 1956 at a cost of $27 million under the Unitary Plan Act of 1949. Since its completion, the UPWT facility has been the most heavily used NASA wind tunnel. Every major commercial transport and almost every military jet built in the United States over the last 40 years has been tested in this facility. Mercury, Gemini, and Apollo spacecraft space shuttle models were also tested in this tunnel complex.
National Full-Scale Aerodynamics Complex (NFAC).
Ames Research Center also houses the world's largest wind tunnel, part of the National Full-Scale Aerodynamic Complex (NFAC): it is large enough to test full-sized planes, rather than scale models.
The 40 by 80 foot wind tunnel circuit was originally constructed in the 1940s and is now capable of providing test velocities up to 300 kn. It is used to support an active research program in aerodynamics, dynamics, model noise, and full-scale aircraft and their components. The aerodynamic characteristics of new configurations are investigated with an emphasis on estimating the accuracy of computational methods. Aeromechanical stability boundaries of advanced rotorcraft and rotor-fuselage interactions are explored. Stability and control derivatives are also determined, including the static and dynamic characteristics of new aircraft configurations. The acoustic characteristics of most of the full-scale vehicles are also determined, as well as acoustic research aimed at discovering and reducing aerodynamic sources of noise. In addition to the normal data gathering methods (e.g., balance system, pressure measuring transducers, and temperature sensing thermocouples), state-of-the-art, non-intrusive instrumentation (e.g., laser velocimeters and shadowgraphs) are available to help determine flow direction and velocity in and around the lifting surfaces of models or aircraft undergoing investigation. The 40 by 80 Foot Wind Tunnel is primarily used for determining the low- and medium-speed aerodynamic characteristics of high-performance aircraft, rotorcraft, and fixed wing, powered-lift V/STOL aircraft.
The 80 by 120 Foot Wind Tunnel at NASA Ames Research Center is the largest wind tunnel test section in the world. This open circuit leg was added and a new fan drive system was installed in the 1980s. The 80 by 120 Foot Wind Tunnel is used to support an active research program in aerodynamics, dynamics, model noise, and full-scale aircraft and their components. The aerodynamic characteristics of new configurations are investigated, with an emphasis on estimating the accuracy of computational methods. Aeromechanical stability boundaries of advanced rotorcraft and rotor-fuselage interactions are explored. The acoustic characteristics of most of the full-scale vehicles are also determined, as well as acoustic research aimed at discovering and reducing aerodynamic sources of noise. In addition to the normal data gathering methods (e.g., balance system, pressure measuring transducers, and temperature sensing thermocouples), state-of-the-art non-intrusive instrumentation (e.g., laser velocimeters and shadowgraphs) are available to help determine flow direction and velocity in and around the lifting surfaces of models or aircraft undergoing investigation. Some of the test programs that have come through the 80 by 120 Foot include: F-18 High Angle of Attack Vehicle, DARPA/Lockheed Common Affordable Lightweight Fighter, XV-15 Tilt Rotor, and Advance Recovery System Parafoil. The 80 by 120 foot test section is capable of testing a full size Boeing 737 at velocities up to 100 kn.
Although decommissioned by NASA in 2003, the NFAC is now being operated by the United States Air Force as a satellite facility of the Arnold Engineering Development Center (AEDC).
Arc Jet Complex.
The Ames Arc Jet Complex is an advanced thermophysics facility where sustained hypersonic- and hyperthermal testing of vehicular thermoprotective systems takes place under a variety of simulated flight- and re-entry conditions. Of its seven available test bays, four currently contain Arc Jet units of differing configurations, serviced by common facility support equipment. These are the Aerodynamic Heating Facility (AHF), the Turbulent Flow Duct (TFD), the Panel Test Facility (PTF), and the Interaction Heating Facility (IHF). The support equipment includes two D.C. power supplies, a steam ejector-driven vacuum system, a water-cooling system, high-pressure gas systems, data acquisition system, and other auxiliary systems.
The magnitude and capacity of these systems makes the Ames Arc Jet Complex unique in the world. The largest power supply can deliver 75 megawatts (MW) for a 30 minute duration or 150 MW for a 15 second duration. This power capacity, in combination with a high-volume 5-stage steam ejector vacuum-pumping system, enables facility operations to match high-altitude atmospheric flight conditions with samples of relatively large size. The Thermo-Physics Facilities Branch operates four arc jet facilities. The Interaction Heating Facility (IHF), with an available power of over 60-MW, is one of the highest-power arc jets available. It is a very flexible facility, capable of long run times of up to one hour, and able to test large samples in both a stagnation and flat plate configuration. The Panel Test Facility (PTF) uses a unique semielliptic nozzle for testing panel sections. Powered by a 20-MW arc heater, the PTF can perform tests on samples for up to 20 minutes. The Turbulent Flow Duct provides supersonic, turbulent high temperature air flows over flat surfaces. The TFD is powered by a 20-MW Hüls arc heater and can test samples 203 by in size. The Aerodynamic Heating Facility (AHF) has similar characteristics to the IHF arc heater, offering a wide range of operating conditions, samples sizes and extended test times. A cold-air-mixing plenum allows for simulations of ascent or high-speed flight conditions. Catalycity studies using air or nitrogen can be performed in this flexible rig. A 5-arm model support system allows the user to maximize testing efficiency. The AHF can be configured with either a Hüls or segmented arc heater, up to 20-MW. 1 MW is enough power to supply 750 homes.
Range complex.
Ames Vertical Gun Range.
The Ames Vertical Gun Range (AVGR) was designed to conduct scientific studies of lunar impact processes in support of the Apollo missions. In 1979, it was established as a National Facility, funded through the Planetary Geology and Geophysics Program. In 1995, increased scientific needs across various disciplines resulted in joint core funding by three different science programs at NASA Headquarters (Planetary Geology and Geophysics, Exobiology, and Solar System Origins). In addition, the AVGR provides programmatic support for various proposed and ongoing planetary missions (e.g. Stardust, Deep Impact).
Using its 0.30 cal light-gas gun and powder gun, the AVGR can launch projectiles to velocities ranging from 500 to. By varying the gun’s angle of elevation with respect to the target vacuum chamber, impact angles from 0° to 90° relative to the gravitational vector are possible. This unique feature is extremely important in the study of crater formation processes.
The target chamber is approximately 2.5 m in diameter and height and can accommodate a wide variety of targets and mounting fixtures. It can maintain vacuum levels below 0.03 Torr, or can be back filled with various gases to simulate different planetary atmospheres. Impact events are typically recorded with high-speed video/film, or Particle Image Velocimetry (PIV).
Hypervelocity Free-Flight Range.
The Hypervelocity Free-Flight (HFF) Range currently comprises two active facilities: the Aerodynamic Facility (HFFAF) and the Gun Development Facility (HFFGDF). The HFFAF is a combined Ballistic Range and Shock-tube Driven Wind Tunnel. Its primary purpose is to examine the aerodynamic characteristics and flow-field structural details of free-flying aeroballistic models.
The HFFAF has a test section equipped with 16 shadowgraph-imaging stations. Each station can be used to capture an orthogonal pair of images of a hypervelocity model in flight. These images, combined with the recorded flight time history, can be used to obtain critical aerodynamic parameters such as lift, drag, static and dynamic stability, flow characteristics, and pitching moment coefficients. For very high Mach number (M > 25) simulations, models can be launched into a counter-flowing gas stream generated by the shock tube. The facility can also be configured for hypervelocity impact testing and has an aerothermodynamic capability as well. The HFFAF is currently configured to operate the 1.5 in light-gas gun in support of continuing thermal imaging and transition research for NASA's hypersonics program.
The HFFGDF is used for gun performance enhancement studies, and occasional impact testing. The Facility uses the same arsenal of light-gas and powder guns as the HFFAF to accelerate particles that range in size from 3.2 to diameter to velocities ranging from 0.5 to 8.5 km/s (1,500 to 28,000 ft/s). Most of the research effort to date has centered on Earth atmosphere entry configurations (Mercury, Gemini, Apollo, and Shuttle), planetary entry designs (Viking, Pioneer Venus, Galileo and MSL), and aerobraking (AFE) configurations. The facility has also been used for scramjet propulsion studies (National Aerospace Plane (NASP)) and meteoroid/orbital debris impact studies (Space Station and RLV). In 2004, the facility was utilized for foam-debris dynamics testing in support of the Return To Flight effort. As of March 2007, the GDF has been reconfigured to operate a cold gas gun for subsonic CEV capsule aerodynamics.
Electric Arc Shock Tube.
The Electric Arc Shock Tube (EAST) Facility is used to investigate the effects of radiation and ionization that occur during very high velocity atmospheric entries. In addition, the EAST can also provide air-blast simulations requiring the strongest possible shock generation in air at an initial pressure loading of 1 atm or greater. The facility has three separate driver configurations, to meet a range of test requirements: the driver can be connected to a diaphragm station of either a 102 mm or a 610 mm shock tube, and the high-pressure 102 mm shock tube can also drive a 762 mm shock tunnel. Energy for the drivers is supplied by a 1.25-MJ-capacitor storage system.
Education.
NASA Ames Exploration Center.
The NASA Ames Exploration Center is a science museum and education center for NASA. There are displays and interactive exhibits about NASA technology, missions and space exploration. A moon rock, meteorite and other geologic samples are on display. The theater shows movies with footage from NASA's explorations of Mars and the planets, and about the contributions of the scientists at Ames. The facility is free and open to the public. Hours: Tuesday – Friday: 10am – 4pm; Sat & Sun: 12 noon – 4pm.
Robotics Alliance Project.
In 1999, Mark León developed NASA's Robotics Education Project (now called the Robotics Alliance Project) under his mentor Dave Lavery, which has reached over 100,000 students nationwide using FIRST robotics and BOTBALL robotics competitions. The Project's FIRST branch originally comprised FRC , an all-male team from Bellarmine High School in San Jose, CA. In 2006, , an all-female team, was founded in collaboration with the Girl Scouts. In 2012, of Mountain View High School joined the Project, though the team continues to operate at their school. All three teams are highly decorated. All three have won Regional competitions, two have won the FIRST Championship, two of which have won the Regional Chairman's Award, and one is a Hall of Fame team. The three teams are collectively referred to as "House teams."
The mission of the project is "To create a human, technical, and programmatic resource of robotics capabilities to enable the implementation of future robotic space exploration missions."
Recent events.
Although the Bush administration slightly increased funding
for NASA overall, the substantial realignment in research priorities that followed the announcement of the Vision for Space Exploration in 2004 led to a significant number of layoffs at Ames.
On October 22, 2006, NASA opened the Carl Sagan Center for the Study of Life in the Cosmos. The center continued work that Sagan undertook, including the Search for Extraterrestrial Intelligence.
In 2008, the Lunar Orbiter Image Recovery Project (LOIRP) was given space in the old McDonald's (the building was renamed McMoons) to digitize data tapes from the five 1966 and 1967 Lunar Orbiter spacecraft that were sent to the Moon.
Also in 2008, it was announced that former Ames director Henry McDonald was a 60th Anniversary Class inductee of the Ames Hall of Fame for providing, "...exceptional leadership and keen technical insight to NASA Ames as the Center re-invented itself in the late 1990s."
In 2010, scientists at the Fluid Mechanics Laboratory at Ames studied the aerodynamics of the Jabulani World Cup soccer ball, concluding that it tends to "knuckle under" at speeds of 45 to. Aerospace engineer Rabi Mehta attributed this effect to asymmetric flow due to the ball's seam construction.
In March 2015, scientists at Ames announced that they had synthesized "...uracil, cytosine, and thymine, all three components of RNA and DNA, non-biologically in a laboratory under conditions found in space." 
Public-private partnerships.
The federal government has re-tasked portions of the facility and human resources to private sector industry, research, and education.
HP became the first corporate affiliate of a new Bio-Info-Nano Research and Development Institute (BIN-RDI); a collaborative venture established by the University of California Santa Cruz and NASA, based at Ames. The Bio|Info|Nano R&D Institute is dedicated to creating scientific breakthroughs by the convergence of biotechnology, information technology, and nanotechnology.
Singularity University hosts its leadership and educational program at the facility. The Organ Preservation Alliance is also headquartered there; the Alliance is a nonprofit organization that works in partnership with the Methuselah Foundation's New Organ Prize "to catalyze breakthroughs on the remaining obstacles towards the long-term storage of organs" to overcome the drastic unmet medical need for viable organs for transplantation. Kleenspeed Technologies is headquartered there.
Google.
On September 28, 2005, both Google and Ames Research Center disclosed details to a long-term research partnership. In addition to pooling engineering talent, Google planned to build a 1000000 sqft facility on the ARC campus. One of the projects between Ames, Google, and Carnegie Mellon University is the Gigapan Project—a robotic platform for creating, sharing, and annotating terrestrial gigapixel images. The Planetary Content Project seeks to integrate and improve the data that Google uses for its Google Moon and Google Mars projects. On June 4, 2008 Google announced it had leased 42 acre from NASA, at Moffett Field, for use as office space and employee housing.
Construction of the new Google project which is near Google's Googleplex headquarters began in 2013 and has a target opening date of 2015. It is called "Bay View" as it overlooks San Francisco Bay.
In May 2013, Google Inc announced that it was launching the Quantum Artificial Intelligence Lab, to be hosted by NASA’s Ames Research Center. The lab will house a 512-qubit quantum computer from D-Wave Systems, and the USRA (Universities Space Research Association) will invite researchers from around the world to share time on it. The goal being to study how quantum computing might advance machine learning.
Living and working at Ames.
There are a myriad of activities inside the research center and around for full-time workers and interns alike. The website new2nasa.wikispaces.com offers updated information. There is a Parcourse trail, also known as a fitness trail inside the base. An official NASA ID is required to enter Ames.

</doc>
<doc id="47481" url="http://en.wikipedia.org/wiki?curid=47481" title="Aquifer">
Aquifer

An aquifer is an underground layer of water-bearing permeable rock or unconsolidated materials (gravel, sand, or silt) from which groundwater can be extracted using a water well. The study of water flow in aquifers and the characterization of aquifers is called hydrogeology. Related terms include aquitard, which is a bed of low permeability along an aquifer, and aquiclude (or "aquifuge"), which is a solid, impermeable area underlying or overlying an aquifer. If the impermeable area overlies the aquifer, pressure could cause it to become a confined aquifer.
Depth.
Aquifers may occur at various depths. Those closer to the surface are not only more likely to be used for water supply and irrigation, but are also more likely to be topped up by the local rainfall. Many desert areas have limestone hills or mountains within them or close to them that can be exploited as groundwater resources. Parts of the Atlas Mountains in North Africa, the Lebanon and Anti-Lebanon ranges of Syria, Palestine and Lebanon, the Jebel Akhdar (Oman) in Oman, parts of the Sierra Nevada and neighboring ranges in the United States' Southwest, have shallow aquifers that are exploited for their water. Overexploitation can lead to the exceeding of the practical sustained yield; i.e., more water is taken out than can be replenished. Along the coastlines of certain countries, such as Libya and Israel, increased water usage associated with population growth has caused a lowering of the water table and the subsequent contamination of the groundwater with saltwater from the sea.
The beach provides a model to help visualize an aquifer. If a hole is dug into the sand, very wet or saturated sand will be located at a shallow depth. This hole is a crude well, the wet sand represents an aquifer, and the level to which the water rises in this hole represents the water table.
In 2013 large freshwater aquifers were discovered under continental shelves off Australia, China, North America and South Africa. They contain an estimated half a million cubic kilometers of “low salinity” water that could be economically processed into potable water. The reserves formed when ocean levels were lower and rainwater made its way into the ground in land areas that were not submerged until the ice age ended 20,000 years ago. The volume is estimated to be 100x the amount of water extracted from other aquifers since 1900.
Classification.
The above diagram indicates typical flow directions in a cross-sectional view of a simple confined or unconfined aquifer system. The system shows two aquifers with one aquitard (a confining or impermeable layer) between them, surrounded by the bedrock "aquiclude", which is in contact with a gaining stream (typical in humid regions). The water table and unsaturated zone are also illustrated. 
An "aquitard" is a zone within the earth that restricts the flow of groundwater from one aquifer to another. An aquitard can sometimes, if completely impermeable, be called an "aquiclude" or "aquifuge". Aquitards are composed of layers of either clay or non-porous rock with low hydraulic conductivity.
Saturated versus unsaturated.
Groundwater can be found at nearly every point in the Earth's shallow subsurface to some degree, although aquifers do not necessarily contain fresh water. The Earth's crust can be divided into two regions: the "saturated zone" or "phreatic zone" (e.g., aquifers, aquitards, etc.), where all available spaces are filled with water, and the "unsaturated zone" (also called the vadose zone), where there are still pockets of air that contain some water, but can be filled with more water.
Saturated means the pressure head of the water is greater than atmospheric pressure (it has a gauge pressure > 0). The definition of the water table is the surface where the pressure head is equal to atmospheric pressure (where gauge pressure = 0).
Unsaturated conditions occur above the water table where the pressure head is negative (absolute pressure can never be negative, but gauge pressure can) and the water that incompletely fills the pores of the aquifer material is under suction. The water content in the unsaturated zone is held in place by surface adhesive forces and it rises above the water table (the zero-gauge-pressure isobar) by capillary action to saturate a small zone above the phreatic surface (the capillary fringe) at less than atmospheric pressure. This is termed tension saturation and is not the same as saturation on a water-content basis. Water content in a capillary fringe decreases with increasing distance from the phreatic surface. The capillary head depends on soil pore size. In sandy soils with larger pores, the head will be less than in clay soils with very small pores. The normal capillary rise in a clayey soil is less than 1.80 m (six feet) but can range between 0.3 and 10 m (one and 30 ft).
The capillary rise of water in a small-diameter tube involves the same physical process. The water table is the level to which water will rise in a large-diameter pipe (e.g., a well) that goes down into the aquifer and is open to the atmosphere.
Aquifers versus aquitards.
Aquifers are typically saturated regions of the subsurface that produce an economically feasible quantity of water to a well or spring (e.g., sand and gravel or fractured bedrock often make good aquifer materials).
An aquitard is a zone within the earth that restricts the flow of groundwater from one aquifer to another. A completely impermeable aquitard is called an aquiclude or aquifuge. Aquitards comprise layers of either clay or non-porous rock with low hydraulic conductivity.
In mountainous areas (or near rivers in mountainous areas), the main aquifers are typically unconsolidated alluvium, composed of mostly horizontal layers of materials deposited by water processes (rivers and streams), which in cross-section (looking at a two-dimensional slice of the aquifer) appear to be layers of alternating coarse and fine materials. Coarse materials, because of the high energy needed to move them, tend to be found nearer the source (mountain fronts or rivers), whereas the fine-grained material will make it farther from the source (to the flatter parts of the basin or overbank areas - sometimes called the pressure area). Since there are less fine-grained deposits near the source, this is a place where aquifers are often unconfined (sometimes called the forebay area), or in hydraulic communication with the land surface.
Confined versus unconfined.
There are two end members in the spectrum of types of aquifers; "confined" and "unconfined" (with semi-confined being in between). Unconfined aquifers are sometimes also called "water table" or "phreatic" aquifers, because their upper boundary is the water table or phreatic surface. (See Biscayne Aquifer.) Typically (but not always) the shallowest aquifer at a given location is unconfined, meaning it does not have a confining layer (an aquitard or aquiclude) between it and the surface. The term "perched" refers to ground water accumulating above a low-permeability unit or strata, such as a clay layer. This term is generally used to refer to a small local area of ground water that occurs at an elevation higher than a regionally extensive aquifer. The difference between perched and unconfined aquifers is their size (perched is smaller).
If the distinction between confined and unconfined is not clear geologically (i.e., if it is not known if a clear confining layer exists, or if the geology is more complex, e.g., a fractured bedrock aquifer), the value of storativity returned from an aquifer test can be used to determine it (although aquifer tests in unconfined aquifers should be interpreted differently than confined ones). Confined aquifers have very low storativity values (much less than 0.01, and as little as 10−5), which means that the aquifer is storing water using the mechanisms of aquifer matrix expansion and the compressibility of water, which typically are both quite small quantities. Unconfined aquifers have storativities (typically then called specific yield) greater than 0.01 (1% of bulk volume); they release water from storage by the mechanism of actually draining the pores of the aquifer, releasing relatively large amounts of water (up to the drainable porosity of the aquifer material, or the minimum volumetric water content).
Isotropic versus anisotropic.
In isotropic aquifers or aquifer layers the hydraulic conductivity (K) is equal for flow in all directions, while in anisotropic conditions it differs, notably in horizontal (Kh) and vertical (Kv) sense.
Semi-confined aquifers with one or more aquitards work as an anisotropic system, even when the separate layers are isotropic, because the compound Kh and Kv values are different (see hydraulic transmissivity and hydraulic resistance).
When calculating flow to drains or flow to wells in an aquifer, the anisotropy is to be taken into account lest the resulting design of the drainage system may be faulty.
Groundwater in rock formations.
Groundwater may exist in "underground rivers" (e.g., caves where water flows freely underground). This may occur in eroded limestone areas known as karst topography, which make up only a small percentage of Earth's area. More usual is that the pore spaces of rocks in the subsurface are simply saturated with water — like a kitchen sponge — which can be pumped out for agricultural, industrial, or municipal uses.
If a rock unit of low porosity is highly fractured, it can also make a good aquifer (via fissure flow), provided the rock has a hydraulic conductivity sufficient to facilitate movement of water. Porosity is important, but, "alone", it does not determine a rock's ability to act as an aquifer. Areas of the Deccan Traps (a basaltic lava) in west central India are good examples of rock formations with high porosity but low permeability, which makes them poor aquifers. Similarly, the micro-porous (Upper Cretaceous) Chalk of south east England, although having a reasonably high porosity, has a low grain-to-grain permeability, with its good water-yielding characteristics mostly due to micro-fracturing and fissuring.
Human dependence on groundwater.
Most land areas on Earth have some form of aquifer underlying them, sometimes at significant depths. These aquifers are rapidly being depleted by the human population.
Fresh-water aquifers, especially those with limited recharge by snow or rain, also known as meteoric water, can be over-exploited and depending on the local hydrogeology, may draw in non-potable water or saltwater intrusion from hydraulically connected aquifers or surface water bodies. This can be a serious problem, especially in coastal areas and other areas where aquifer pumping is excessive. In some areas, the ground water can be contaminated by mineral poisons, such as arsenic - see Arsenic contamination of groundwater.
Aquifers are critically important in human habitation and agriculture. Deep aquifers in arid areas have long been water sources for irrigation (see Ogallala below). Many villages and even large cities draw their water supply from wells in aquifers.
Municipal, irrigation, and industrial water supplies are provided through large wells. Multiple wells for one water supply source are termed "wellfields", which may withdraw water from confined or unconfined aquifers. Using ground water from deep, confined aquifers provides more protection from surface water contamination. Some wells, termed "collector wells," are specifically designed to induce infiltration of surface (usually river) water.
Aquifers that provide sustainable fresh groundwater to urban areas and for agricultural irrigation are typically close to the ground surface (within a couple of hundred metres) and have some recharge by fresh water. This recharge is typically from rivers or meteoric water (precipitation) that percolates into the aquifer through overlying unsaturated materials.
Occasionally, sedimentary or "fossil" aquifers are used to provide irrigation and drinking water to urban areas. In Libya, for example, Muammar Gaddafi's Great Manmade River project has pumped large amounts of groundwater from aquifers beneath the Sahara to populous areas near the coast. Though this has saved Libya money over the alternative, desalination, the aquifers are likely to run dry in 60 to 100 years. Aquifer depletion has been cited as one of the causes of the food price rises of 2011.
Subsidence.
In unconsolidated aquifers, groundwater is produced from pore spaces between particles of gravel, sand, and silt. If the aquifer is confined by low-permeability layers, the reduced water pressure in the sand and gravel causes slow drainage of water from the adjoining confining layers. If these confining layers are composed of compressible silt or clay, the loss of water to the aquifer reduces the water pressure in the confining layer, causing it to compress from the weight of overlying geologic materials. In severe cases, this compression can be observed on the ground surface as subsidence. Unfortunately, much of the subsidence from groundwater extraction is permanent (elastic rebound is small). Thus, the subsidence is not only permanent, but the compressed aquifer has a permanently reduced capacity to hold water.
Saltwater intrusion.
Aquifers near the coast have a lens of freshwater near the surface and denser seawater under freshwater. Seawater penetrates the aquifer diffusing in from the ocean and is denser than freshwater. For porous (i.e., sandy) aquifers near the coast, the thickness of freshwater atop saltwater is about 40 ft for every 1 ft of freshwater head above sea level. This relationship is called the Ghyben-Herzberg equation. If too much ground water is pumped near the coast, salt-water may intrude into freshwater aquifers causing contamination of potable freshwater supplies. Many coastal aquifers, such as the Biscayne Aquifer near Miami and the New Jersey Coastal Plain aquifer, have problems with saltwater intrusion as a result of overpumping.
Salination.
Aquifers in surface irrigated areas in semi-arid zones with reuse of the unavoidable irrigation water losses percolating down into the underground by supplemental irrigation from wells run the risk of salination.
Surface irrigation water normally contains salts in the order of 0.5 g/l or more and the annual irrigation requirement is in the order of 10000 m³/ha or more so the annual import of salt is in the order of 5000 kg/ha or more.
Under the influence of continuous evaporation, the salt concentration of the aquifer water may increase continually and eventually cause an environmental problem.
For salinity control in such a case, annually an amount of drainage water is to be discharged from the aquifer by means of a subsurface drainage system and disposed of through a safe outlet. The drainage system may be "horizontal" (i.e. using pipes, tile drains or ditches) or "vertical" (drainage by wells). To estimate the drainage requirement, the use of a groundwater model with an agro-hydro-salinity component may be instrumental, e.g. SahysMod.
Examples.
The Great Artesian Basin situated in Australia is arguably the largest groundwater aquifer in the world (over 1.7 million km²). It plays a large part in water supplies for Queensland and remote parts of South Australia.
The Guarani Aquifer, located beneath the surface of Argentina, Brazil, Paraguay, and Uruguay, is one of the world's largest aquifer systems and is an important source of fresh water. Named after the Guarani people, it covers 1,200,000 km², with a volume of about 40,000 km³, a thickness of between 50 m and 800 m and a maximum depth of about 1,800 m.
Aquifer depletion is a problem in some areas, and is especially critical in northern Africa; see the Great Manmade River project of Libya for an example. However, new methods of groundwater management such as artificial recharge and injection of surface waters during seasonal wet periods has extended the life of many freshwater aquifers, especially in the United States.
The Ogallala Aquifer of the central United States is one of the world's great aquifers, but in places it is being rapidly depleted by growing municipal use, and continuing agricultural use. This huge aquifer, which underlies portions of eight states, contains primarily fossil water from the time of the last glaciation. Annual recharge, in the more arid parts of the aquifer, is estimated to total only about 10 percent of annual withdrawals. According to a 2013 report by research hydrologist Leonard F. Konikow at the United States Geological Survey (USGC), the depletion between 2001–2008, inclusive, is about 32 percent of the cumulative depletion during the entire 20th century (Konikow 2013:22)." In the United States, the biggest users of water from aquifers include agricultural irrigation and oil and coal extraction. "Cumulative total groundwater depletion in the United States accelerated in the late 1940s and continued at an almost steady linear rate through the end of the century. In addition to widely recognized environmental consequences, groundwater depletion also adversely impacts the long-term sustainability of groundwater supplies to help meet the Nation’s water needs."
An example of a significant and sustainable carbonate aquifer is the Edwards Aquifer in central Texas. This carbonate aquifer has historically been providing high quality water for nearly 2 million people, and even today, is full because of tremendous recharge from a number of area streams, rivers and lakes. The primary risk to this resource is human development over the recharge areas.
Discontinuous sand bodies at the base of the McMurray Formation in the Athabasca Oil Sands region of northeastern Alberta, Canada, are commonly referred to as the Basal Water Sand (BWS) aquifers. Saturated with water, they are confined beneath impermeable bitumen-saturated sands that are exploited to recover bitumen for synthetic crude oil production. Where they are deep-lying and recharge occurs from underlying Devonian formations they are saline, and where they are shallow and recharged by meteoric water they are non-saline. The BWS typically pose problems for the recovery of bitumen, whether by open-pit mining or by "in situ" methods such as steam-assisted gravity drainage (SAGD), and in some areas they are targets for waste-water injection.

</doc>
<doc id="47484" url="http://en.wikipedia.org/wiki?curid=47484" title="Atmospheric pressure">
Atmospheric pressure

Atmospheric pressure is the pressure exerted by the weight of air in the atmosphere of Earth (or that of another planet). In most circumstances atmospheric pressure is closely approximated by the hydrostatic pressure caused by the weight of air above the measurement point. On a given plane, low-pressure areas have less atmospheric mass above their location, whereas high-pressure areas have more atmospheric mass above their location. Likewise, as elevation increases, there is less overlying atmospheric mass, so that atmospheric pressure decreases with increasing elevation. On average, a column of air one square centimeter in cross-section, measured from sea level to the top of the atmosphere, has a mass of about 1.03 kg and weight of about 10.1 N (2.28 lbf). (A column one square inch in cross-section would have a weight of about 14.7 lbs, or about 65.4 N.) Atmospheric pressure is sometimes called barometric pressure. 
Standard atmosphere.
The standard atmosphere (symbol: atm) is a unit of pressure equal to 101325 Pa or 1013.25 hectopascals or millibars. Equivalent to 760 mmHg (torr), 29.92 inHg, 14.696 psi. (The pascal is a newton per square meter or in terms of SI base units, kilogram per meter per second-squared.)
Mean sea level pressure.
The mean sea level pressure (MSLP) is the atmospheric pressure at sea level or (when measured at a given elevation on land) the station pressure adjusted to sea level assuming that the temperature falls at a lapse rate of 6.5 K per km in the fictive layer of air between the station and sea level.
This is the atmospheric pressure normally given in weather reports on radio, television, and newspapers or on the Internet. When barometers in the home are set to match the local weather reports, they measure pressure adjusted to sea level, not the actual local atmospheric pressure.
The adjustment to sea level means that the "normal range of fluctuations" in atmospheric pressure is the same for everyone. The pressures that are considered "high pressure" or "low pressure" do not depend on geographical location. This makes isobars on a weather map meaningful and useful tools.
The "altimeter setting" in aviation, set to either QNH or QFE setting, is another atmospheric pressure adjustment, but the methods of making these adjustments are different:
QFE and QNH are arbitrary Q codes rather than abbreviations, but the mnemonics "nautical height" (for QNH) and "field elevation" (for QFE) are often used by pilots to distinguish them.
Average "sea-level pressure" is 101.325 kPa (1013.25 hPa or mbar) or 29.92 inches (inHg) or 760 millimetres of mercury (mmHg). In aviation weather reports (METAR), QNH is transmitted around the world in hectopascals or millibars (1 hectopascal = 1 millibar), except in the United States, Canada, and Colombia where it is reported in inches (to two decimal places) of mercury. (The United States and Canada also report "sea level pressure" SLP, which is adjusted to sea level by a different method, in the remarks section, not in the internationally transmitted part of the code, in hectopascals or millibars. However, in Canada's public weather reports, sea level pressure is instead reported in kilopascals, while Environment Canada's standard unit of pressure is the same.)
In the US weather code remarks, three digits are all that are transmitted; decimal points and the one or two most significant digits are omitted: 1013.2 mbar or 101.32 kPa is transmitted as 132; 1000.0 mbar or 100.00 kPa is transmitted as 000; 998.7 mbar or 99.87 kPa is transmitted as 987; etc. The highest "sea-level pressure" on Earth occurs in Siberia, where the Siberian High often attains a "sea-level pressure" above 1050.0 mbar (105.00 kPa, 30.01 inHg), with record highs close to 1085.0 mbar (108.50 kPa, 32.04 inHg). The lowest measurable "sea-level pressure" is found at the centers of tropical cyclones and tornadoes, with a record low of 870 mbar (87 kPa, 25.69 inHg) (see Atmospheric pressure records).
Altitude variation.
Pressure varies smoothly from the Earth's surface to the top of the mesosphere. Although the pressure changes with the weather, NASA has averaged the conditions for all parts of the earth year-round. As altitude increases, atmospheric pressure decreases. One can calculate the atmospheric pressure at a given altitude. Temperature and humidity also affect the atmospheric pressure, and it is necessary to know these to compute an accurate figure. The graph at right was developed for a temperature of 15 °C and a relative humidity of 0%.
At low altitudes above the sea level, the pressure decreases by about 1.2 kPa for every 100 meters. For higher altitudes within the troposphere, the following equation (the barometric formula) relates atmospheric pressure "p" to altitude "h"
where the constant parameters are as described below:
Local variation.
Atmospheric pressure varies widely on Earth, and these changes are important in studying weather and climate. See pressure system for the effects of air pressure variations on weather.
Atmospheric pressure shows a diurnal or semidiurnal (twice-daily) cycle caused by global atmospheric tides. This effect is strongest in tropical zones, with an amplitude of a few millibars, and almost zero in polar areas. These variations have two superimposed cycles, a circadian (24 h) cycle and semi-circadian (12 h) cycle.
Records.
The highest adjusted-to-sea level barometric pressure ever recorded on Earth (above 750 meters) was 1085.7 hPa measured in Tosontsengel, Mongolia on 19 December 2001. The highest adjusted-to-sea level barometric pressure ever recorded (below 750 meters) was at Agata, Evenhiyskiy, Russia [66°53’N, 93°28’E, elevation: 261 m (856.3 ft)] on 31 December 1968 of 1083.3 hPa. The discrimination is due to the problematic assumptions (assuming a standard lapse rate) associated with reduction of sea level from high elevations. The lowest non-tornadic atmospheric pressure ever measured was 870 hPa (25.69 inHg), set on 12 October 1979, during Typhoon Tip in the western Pacific Ocean. The measurement was based on an instrumental observation made from a reconnaissance aircraft. The normal high barometric pressure at the Dead Sea, as measured by a standard mercury manometer and blood gas analyzer, was found to be 799 mmHg (1065 hPa).
Measurement based on depth of water.
One atmosphere (101 kPa or 14.7 psi) is the pressure caused by the weight of a column of fresh water of approximately 10.3 m (33.8 ft). Thus, a diver 10.3 m underwater experiences a pressure of about 2 atmospheres (1 atm of air plus 1 atm of water). Conversely, 10.3 m is the maximum height to which water can be raised using suction under standard atmospheric conditions.
Low pressures such as natural gas lines are sometimes specified in inches of water, typically written as "w.c." (water column) or "w.g." (inches water gauge). A typical gas-using residential appliance in the US is rated for a maximum of 14 w.c., which is approximately 35 hPa. Similar metric units with a wide variety of names and notation based on millimetres, centimetres or metres are now less commonly used.
Boiling point of water.
Clean fresh water boils at about 100 C at earth's standard atmospheric pressure. The boiling point is the temperature at which the vapor pressure is equal to the atmospheric pressure around the water. Because of this, the boiling point of water is lower at lower pressure and higher at higher pressure. This is why cooking at elevations more than 1100 m above sea level requires adjustments to recipes. A rough approximation of elevation can be obtained by measuring the temperature at which water boils; in the mid-19th century, this method was used by explorers.
Measurement and maps.
An important application of the knowledge that atmospheric pressure varies directly with altitude was in determining the height of hills and mountains thanks to the availability of reliable pressure measurement devices. While in 1774 Maskelyne was confirming Newton's theory of gravitation at and on Schiehallion in Scotland (using plumb bob deviation to show the effect of "gravity") and accurately measure elevation, William Roy using barometric pressure was able to confirm his height determinations, the agreement being to within one meter (3.28 feet). This was then a useful tool for survey work and map making and long has continued to be useful. It was part of the "application of science" which gave practical people the insight that applied science could easily and relatively cheaply be "useful".

</doc>
<doc id="47485" url="http://en.wikipedia.org/wiki?curid=47485" title="King (chess)">
King (chess)

In chess, the king (♔, ♚) is the most important piece. The object of the game is to trap the opponent's king so that its escape is not possible (checkmate). If a player's king is threatened with capture, it is said to be "in check", and the player must remove the threat of capture on the next move. If this cannot be done, the king is said to be in checkmate. Although the king is the most important piece, it is usually the weakest piece in the game until a later phase, the endgame.
Movement.
White starts with the king on the first rank to the right of the queen. Black starts with the king directly across from the white king. With the squares labeled as in algebraic notation, the white king starts on e1 and the black king on e8.
A king can move one square in any direction (horizontally, vertically, or diagonally) unless the square is already occupied by a friendly piece or the move would place the king in check. As a result, the opposing kings may never occupy adjacent squares (see opposition), but the king can give discovered check by unmasking a bishop, rook, or queen. The king is also involved in the special move of castling. 
Castling.
In conjunction with a rook, the king may make a special move called castling, in which the king moves two squares toward one of its rooks and then the rook is placed on the square over which the king crossed. Castling is allowed only when neither the king nor the castling rook has previously moved, when no squares between them are occupied, when the king is not in check, and when the king will not move across or end its movement on a square that is under enemy attack.
Status in games.
Check and checkmate.
If a player's move places the opponent's king under attack, that king is said to be "in check", and the player in check is required to immediately remedy the situation. There are three possible methods to remove the king from check:
If none of these three options are possible, the player's king has been "checkmated" and the player loses the game.
Stalemate.
A stalemate occurs when, for the player with the move:
If this happens, the king is said to have been stalemated and the game ends in a draw. A player who has very little or no chance of winning will often try to entice the opponent to inadvertently place the player's king in stalemate in order to avoid a loss.
Role in gameplay.
In the opening and middlegame, the king will rarely play an active role in the development of an offensive or defensive position. Instead, a player will normally try to castle and seek safety on the edge of the board behind friendly pawns. In the endgame, however, the king emerges to play an active role as an offensive piece as well as assisting in the promotion of their remaining pawns.
It is not meaningful to assign a value to the king relative to the other pieces, as it cannot be captured or exchanged. In this sense, its value could be considered infinite. As an assessment of the king's capability as an offensive piece in the endgame, it is often considered to be slightly stronger than a bishop or knight – Emanuel Lasker gave it the value of a knight plus a pawn (i.e. four points on the scale of chess piece relative value) . It is better at defending nearby pawns than the knight is, and it is better at attacking them than the bishop is .
Unicode.
Unicode defines two codepoints for king:
♔ U+2654 White Chess King (HTML &#9812;)
♚ U+265A Black Chess King (HTML &#9818;)

</doc>
<doc id="47486" url="http://en.wikipedia.org/wiki?curid=47486" title="Atoll">
Atoll

An atoll ( or ), sometimes called a coral atoll, is a ring-shaped coral reef including a coral rim that encircles a lagoon partially or completely. There may be coral islands/cays on the rim.(p60)
 The coral of the atoll often sits atop the rim of an extinct seamount or volcano which has eroded or subsided partially beneath the water. The lagoon forms over the volcanic crater or caldera while the higher rim remains above water or at shallow depths that permit the coral to grow and form the reefs. For the atoll to persist, continued erosion or subsidence must be at a rate slow enough to permit reef growth upwards and outwards to replace the lost height.
Usage.
The word "atoll" comes from the Dhivehi (an Indo-Aryan language spoken on the Maldive Islands) word "atholhu" (Dhivehi: އަތޮޅު, ]), meaning an administrative subdivision.OED Its first recorded use in English was in 1625 as "atollon" – Charles Darwin recognized its indigenous origin and coined, in his "The Structure and Distribution of Coral Reefs", the definition of atolls as "circular groups of coral islets" that is synonymous with "lagoon-island".(p2)
More modern definitions of "atoll" describe them as "annular reefs enclosing a lagoon in which there are no promontories other than reefs and islets composed of reef detritus" or "in an exclusively morphological sense, [as] a ring-shaped ribbon reef enclosing a lagoon".
Distribution and size.
The distribution of atolls around the globe is instructive: most of the world's atolls are in the Pacific Ocean (with concentrations in the Tuamotu Islands, Caroline Islands, Marshall Islands, Coral Sea Islands, and the island groups of Kiribati, Tuvalu and Tokelau) and Indian Ocean (the Atolls of the Maldives, the Lakshadweep Islands, the Chagos Archipelago and the Outer Islands of the Seychelles). The Atlantic Ocean has no large groups of atolls, other than eight atolls east of Nicaragua that belong to the Colombian department of San Andres and Providencia in the Caribbean.
Reef-building corals will thrive only in warm tropical and subtropical waters of oceans and seas, and therefore atolls are only found in the tropics and subtropics. The northernmost atoll of the world is Kure Atoll at 28°24' N, along with other atolls of the Northwestern Hawaiian Islands. The southernmost atolls of the world are Elizabeth Reef at 29°58' S, and nearby Middleton Reef at 29°29' S, in the Tasman Sea, both of which are part of the Coral Sea Islands Territory. The next southerly atoll is Ducie Island in the Pitcairn Islands Group, at 24°40' S. Bermuda is sometimes claimed as the "northernmost atoll" at a latitude of 32°24' N. At this latitude coral reefs would not develop without the warming waters of the Gulf Stream. However, Bermuda is termed a "pseudo-atoll" because its general form, while resembling that of an atoll, has a very different mode of formation. While there is no atoll directly on the equator, the closest atoll to the Equator is Aranuka of Kiribati, with its southern tip just 12 km north of the equator.
The largest atolls by total area (lagoon plus reef and dry land) are listed below:
In most cases, the land area of an atoll is very small in comparison to the total area. Atoll islands are low lying, with their elevations less than 5 meters (9). Measured by total area, Lifou (1146 km²) is the largest raised coral atoll of the world, followed by Rennell Island (660 km²). More sources however list as the largest atoll in the world in terms of land area Kiritimati, which is also a raised coral atoll (321.37 km² land area; according to other sources even 575 km²), 160 km² main lagoon, 168 km² other lagoons (according to other sources 319 km² total lagoon size). The remains of an ancient atoll as a hill in a limestone area is called a reef knoll. The second largest atoll by dry land area is Aldabra with 155 km². The largest atoll in terms of island numbers is Huvadhu Atoll in the south of the Maldives with 255 islands.
Formation.
In 1842, Charles Darwin explained the creation of coral atolls in the southern Pacific Ocean based upon observations made during a five-year voyage aboard the from 1831 to 1836. Accepted as basically correct, his explanation involved considering that several tropical island types—from high volcanic island, through barrier reef island, to atoll—represented a sequence of gradual subsidence of what started as an oceanic volcano. He reasoned that a fringing coral reef surrounding a volcanic island in the tropical sea will grow upwards as the island subsides (sinks), becoming an "almost atoll", or barrier reef island, as typified by an island such as Aitutaki in the Cook Islands, Bora Bora and others in the Society Islands. The fringing reef becomes a barrier reef for the reason that the outer part of the reef maintains itself near sea level through biotic growth, while the inner part of the reef falls behind, becoming a lagoon because conditions are less favorable for the coral and calcareous algae responsible for most reef growth. In time, subsidence carries the old volcano below the ocean surface and the barrier reef remains. At this point, the island has become an atoll.
Atolls are the product of the growth of tropical marine organisms, and so these islands are only found in warm tropical waters. Volcanic islands located beyond the warm water temperature requirements of hermatypic (reef-building) organisms become seamounts as they subside and are eroded away at the surface. An island that is located where the ocean water temperatures are just sufficiently warm for upward reef growth to keep pace with the rate of subsidence is said to be at the Darwin Point. Islands in colder, more polar regions evolve towards seamounts or guyots; warmer, more equatorial islands evolve towards atolls, for example Kure Atoll.
Reginald Aldworth Daly offered a somewhat different explanation for atoll formation: islands worn away by erosion, by ocean waves and streams, during the last glacial stand of the sea of some 900 ft below present sea level developed as coral islands (atolls), or barrier reefs on a platform surrounding a volcanic island not completely worn away, as sea level gradually rose from melting of the glaciers. Discovery of the great depth of the volcanic remnant beneath many atolls such as at Midway Atoll favors the Darwin explanation, although there can be little doubt that fluctuating sea level has had considerable influence on atolls and other reefs.
Coral atolls are also an important place where dolomitization of calcite occurs. At certain depths water is undersaturated in calcium carbonate but saturated in dolomite. Convection created by tides and sea currents enhance this change. Hydrothermal currents created by volcanoes under the atoll may also play an important role.
Investigation by the Royal Society of London into the formation of coral reefs.
In 1896, 1897 and 1898, the Royal Society of London carried out drilling on Funafuti atoll in Tuvalu for the purpose of investigating the formation of coral reefs to determine whether traces of shallow water organisms could be found at depth in the coral of Pacific atolls. This investigation followed the work on the structure and distribution of coral reefs conducted by Charles Darwin in the Pacific.
The first expedition in 1896 was led by Professor William Johnson Sollas of the University of Oxford. The geologists included Walter George Woolnough and Edgeworth David of the University of Sydney. Professor Edgeworth David led the expedition in 1897. The third expedition in 1898 was led by Alfred Edmund Finckh.
United States national monuments.
On January 6, 2009, U.S. President George W. Bush announced that several remote Pacific islands under U.S. jurisdiction were now national monuments, protecting coral reefs.(Number 1, page 14)

</doc>
<doc id="47487" url="http://en.wikipedia.org/wiki?curid=47487" title="Azimuth">
Azimuth

An azimuth () (from Arabic "al-sumūt", meaning "the directions") is an angular measurement in a spherical coordinate system. The vector from an observer (origin) to a point of interest is projected perpendicularly onto a reference plane; the angle between the projected vector and a reference vector on the reference plane is called the azimuth.
An example is the position of a star in the sky. The star is the point of interest, the reference plane is the horizon or the surface of the sea, and the reference vector points north. The azimuth is the angle between the north vector and the perpendicular projection of the star down onto the horizon.
Azimuth is usually measured in degrees (°). The concept is used in navigation, astronomy, engineering, mapping, mining and artillery.
Navigation.
In land navigation, azimuth is usually denoted "alpha", formula_1, and defined as a horizontal angle measured clockwise from a north base line or "meridian". "Azimuth" has also been more generally defined as a horizontal angle measured clockwise from any fixed reference plane or easily established base direction line.
Today the reference plane for an azimuth is typically true north, measured as a 0° azimuth, though other angular units (grad, mil) can be used. Moving clockwise on a 360 degree circle, east has azimuth 90°, south 180°, and west 270°. There are exceptions: some navigation systems use south as the reference vector. Any direction can be the reference vector, as long as it is clearly defined.
Quite commonly, azimuths or compass bearings are stated in a system in which either north or south can be the zero, and the angle may be measured clockwise or anticlockwise from the zero. For example, a bearing might be described as "(from) south, (turn) thirty degrees (toward the) east" (the words in brackets are usually omitted), abbreviated "S30°E", which is the bearing 30 degrees in the eastward direction from south, i.e. the bearing 150 degrees clockwise from north. The reference direction, stated first, is always north or south, and the turning direction, stated last, is east or west. The directions are chosen so that the angle, stated between them, is positive, between zero and 90 degrees. If the bearing happens to be exactly in the direction of one of the cardinal points, a different notation, e.g. "due east", is used instead.
Cartographical azimuth.
The cartographical azimuth (in decimal degrees) can be calculated when the coordinates of 2 points are known in a flat plane (cartographical coordinates):
Remark that the reference axes are swapped relative to the (counterclockwise) mathematical polar coordinate system and that the azimuth is clockwise relative to the north.
Calculating azimuth.
We are standing at latitude formula_3, longitude zero; we want to find the azimuth from our viewpoint to Point 2 at latitude formula_4, longitude L (positive eastward). We can get a fair approximation by assuming the Earth is a sphere, in which case the azimuth formula_5 is given by
A better approximation assumes the Earth is a slightly-squashed sphere (an oblate spheroid); "azimuth" then has at least two very slightly different meanings. "Normal-section azimuth" is the angle measured at our viewpoint by a theodolite whose axis is perpendicular to the surface of the spheroid; "geodetic azimuth" is the angle between north and the "geodesic" – that is, the shortest path on the surface of the spheroid from our viewpoint to Point 2. The difference is usually unmeasurably small; if Point 2 is not more than 100 km away the difference will not exceed 0.03 arc second.
Various websites will calculate geodetic azimuth – e.g. . Formulas for calculating geodetic azimuth are linked in the distance article.
Normal-section azimuth is simpler to calculate; Bomford says Cunningham's formula is exact for any distance. If formula_7 is the flattening for the chosen spheroid (e.g. 1/298.257223563 for WGS84) then
If formula_12 = 0 then
To calculate the azimuth of the sun or a star given its declination and hour angle at our location, we modify the formula for a spherical earth. Replace formula_14 with declination and longitude difference with hour angle, and change the sign (since hour angle is positive westward instead of east).
Mapping.
There are a wide variety of azimuthal map projections. They all have the property that directions (the azimuths) from a central point are preserved. Some navigation systems use south as the reference plane. However, any direction can serve as the plane of reference, as long as it is clearly defined for everyone using that system.
Astronomy.
Used in celestial navigation, an "azimuth" is the direction of a celestial body from the observer. In astronomy, an "azimuth" is sometimes referred to as a bearing. In modern astronomy azimuth is nearly always measured from the north.
(The article on coordinate systems, for example, uses a convention measuring from the south.) In former times, it was common to refer to azimuth from the south, as it was then zero at the same time that the hour angle of a star was zero. This assumes, however, that the star (upper) culminates in the south, which is only true if the star's declination is less than (i.e. further south than) the observer's latitude.
Other systems.
Right ascension.
If instead of measuring from and along the horizon the angles are measured from and along the celestial equator, the angles are called right ascension if referenced to the Vernal Equinox, or hour angle if referenced to the celestial meridian.
Horizontal coordinate.
In the horizontal coordinate system, used in celestial navigation and satellite dish installation, azimuth is one of the two coordinates. The other is altitude, sometimes called elevation above the horizon. See also: Sat finder.
Polar coordinate.
In mathematics the azimuth angle of a point in cylindrical coordinates or spherical coordinates is the anticlockwise angle between the positive x-axis and the projection of the vector onto the xy-plane. The angle is the same as an angle in polar coordinates of the component of the vector in the xy-plane and is normally measured in radians rather than degrees. As well as measuring the angle differently, in mathematical applications theta, formula_15, is very often used to represent the azimuth rather than the symbol phi formula_16.
Other uses of the word.
For magnetic tape drives, "azimuth" refers to the angle between the tape head(s) and tape.
In sound localization experiments and literature, the "azimuth" refers to the angle the sound source makes compared to the imaginary straight line that is drawn from within the head through the area between the eyes.
An azimuth thruster in shipbuilding is a propeller that can be rotated horizontally.
Etymology of the word.
The word azimuth is in all European languages today. It originates from medieval Arabic "al-sumūt", pronounced "as-sumūt" in Arabic, meaning "the directions" (plural of Arabic "al-samt" = "the direction"). The Arabic word entered late medieval Latin in an astronomy context and in particular in the use of the Arabic version of the Astrolabe astronomy instrument. The word's first record in English is in the 1390s in "Treatise on the Astrolabe" by Geoffrey Chaucer. The first known record in any Western language is in Spanish in the 1270s in an astronomy book that was largely derived from Arabic sources, the "Libros del saber de astronomía" commissioned by King Alfonso X of Castile.

</doc>
<doc id="47488" url="http://en.wikipedia.org/wiki?curid=47488" title="Barometer">
Barometer

A barometer is a scientific instrument used in meteorology to measure atmospheric pressure. Pressure tendency can forecast short term changes in the weather. Numerous measurements of air pressure are used within surface weather analysis to help find surface troughs, high pressure systems and frontal boundaries.
Barometers and pressure altimeters (the most basic and common type of altimeter) are essentially the same instrument, but used for different purposes. An altimeter is intended to be transported from place to place matching the atmospheric pressure to the corresponding altitude, while a barometer is kept stationary and measures subtle pressure changes caused by weather. The main exception to this is ships at sea, which can use a barometer because their elevation does not change. Due to the presence of weather systems, aircraft altimeters may need to be adjusted as they fly between regions of varying normalized atmospheric pressure.
History.
Although Evangelista Torricelli is universally credited with inventing the barometer in 1643,<ref name="http://www.islandnet.com/~see/weather/history/barometerhistory1.htm"></ref><ref name="http://www.barometerfair.com/history_of_the_barometer.htm"></ref><ref name="http://www.juliantrubin.com/bigten/torricellibarometer.html"></ref> historical documentation also suggests Gasparo Berti, an Italian mathematician and astronomer, unintentionally built a water barometer sometime between 1640 and 1643. French scientist and philosopher René Descartes described the design of an experiment to determine atmospheric pressure as early as 1631, but there is no evidence that he built a working barometer at that time.
On July 27, 1630, Giovanni Battista Baliani wrote a letter to Galileo Galilei explaining an experiment he had made in which a siphon, led over a hill about twenty-one meters high, failed to work. Galileo responded with an explanation of the phenomenon: he proposed that it was the power of a vacuum that held the water up, and at a certain height the amount of water simply became too much and the force could not hold any more, like a cord that can support only so much weight. This was a restatement of the theory of "horror vacui" ("nature abhors a vacuum"), which dates to Aristotle, and which Galileo restated as "resistenza del vacuo".
Galileo's ideas reached Rome in December 1638 in his "Discorsi". Raffaele Magiotti and Gasparo Berti were excited by these ideas, and decided to seek a better way to attempt to produce a vacuum than with a siphon. Magiotti devised such an experiment, and sometime between 1639 and 1641, Berti (with Magiotti, Athanasius Kircher and Niccolò Zucchi present) carried it out.
Four accounts of Berti's experiment exist, but a simple model of his experiment consisted of filling with water a long tube that had both ends plugged, then standing the tube in a basin already full of water. The bottom end of the tube was opened, and water that had been inside of it poured out into the basin. However, only part of the water in the tube flowed out, and the level of the water inside the tube stayed at an exact level, which happened to be 10.3 m, the same height Baliani and Galileo had observed that was limited by the siphon. What was most important about this experiment was that the lowering water had left a space above it in the tube which had no intermediate contact with air to fill it up. This seemed to suggest the possibility of a vacuum existing in the space above the water.
Torricelli, a friend and student of Galileo, interpreted the results of the experiments in a novel way. He proposed that the weight of the atmosphere, not an attracting force of the vacuum, held the water in the tube. In a letter to Michelangelo Ricci in 1644 concerning the experiments, he wrote:
Many have said that a vacuum does not exist, others that it does exist in spite of the repugnance of nature and with difficulty; I know of no one who has said that it exists without difficulty and without a resistance from nature. I argued thus: If there can be found a manifest cause from which the resistance can be derived which is felt if we try to make a vacuum, it seems to me foolish to try to attribute to vacuum those operations which follow evidently from some other cause; and so by making some very easy calculations, I found that the cause assigned by me (that is, the weight of the atmosphere) ought by itself alone to offer a greater resistance than it does when we try to produce a vacuum.
It was traditionally thought (especially by the Aristotelians) that the air did not have lateral weight: that is, that the kilometers of air above the surface did not exert any weight on the bodies below it. Even Galileo had accepted the weightlessness of air as a simple truth. Torricelli questioned that assumption, and instead proposed that air had weight and that it was the latter (not the attracting force of the vacuum) which held (or rather, pushed) up the column of water. He thought that the level the water stayed at (c. 10.3 m) was reflective of the force of the air's weight pushing on it (specifically, pushing on the water in the basin and thus limiting how much water can fall from the tube into it). In other words, he viewed the barometer as a balance, an instrument for measurement (as opposed to merely being an instrument to create a vacuum), and because he was the first to view it this way, he is traditionally considered the inventor of the barometer (in the sense in which we use the term now).
Because of rumors circulating in Torricelli's gossipy Italian neighborhood, which included that he was engaged in some form of sorcery or witchcraft, Torricelli realized he had to keep his experiment secret to avoid the risk of being arrested. He needed to use a liquid that was heavier than water, and from his previous association and suggestions by Galileo, he deduced by using mercury, a shorter tube could be used. With mercury, then called "quicksilver", which is about 14 times heavier than water, a tube only 80 cm was now needed, not 10.5 m.
In 1646, Blaise Pascal along with Pierre Petit, had repeated and perfected Torricelli's experiment after hearing about it from Marin Mersenne, who himself had been shown the experiment by Torricelli toward the end of 1644. Pascal further devised an experiment to test the Aristotelian proposition that it was vapors from the liquid that filled the space in a barometer. His experiment compared water with wine, and since the latter was considered more "spiritous", the Aristotelians expected the wine to stand lower (since more vapors would mean more pushing down on the liquid column). Pascal performed the experiment publicly, inviting the Aristotelians to predict the outcome beforehand. The Aristotelians predicted the wine would stand lower. It did not.
However, Pascal went even further to test the mechanical theory. If, as suspected by mechanical philosophers like Torricelli and Pascal, air had lateral weight, the weight of the air would be less at higher altitudes. Therefore, Pascal wrote to his brother-in-law, Florin Perier, who lived near a mountain called the Puy de Dome, asking him to perform a crucial experiment. Perier was to take a barometer up the Puy de Dome and make measurements along the way of the height of the column of mercury. He was then to compare it to measurements taken at the foot of the mountain to see if those measurements taken higher up were in fact smaller. In September 1648, Perier carefully and meticulously carried out the experiment, and found that Pascal's predictions had been correct. The mercury barometer stood lower the higher one went.
Types.
Water-based barometers.
The concept that decreasing atmospheric pressure predicts stormy weather, postulated by Lucien Vidi, provides the theoretical basis for a weather prediction device called a "storm glass" or a "Goethe barometer" (named for Johann Wolfgang Von Goethe, the renowned German writer and polymath who developed a simple but effective weather ball barometer using the principles developed by Torricelli). The French name, "le baromètre Liégeois", is used by some English speakers. This name reflects the origins of many early weather glasses - the glass blowers of Liége, Belgium.
The weather ball barometer consists of a glass container with a sealed body, half filled with water. A narrow spout connects to the body below the water level and rises above the water level. The narrow spout is open to the atmosphere. When the air pressure is lower than it was at the time the body was sealed, the water level in the spout will rise above the water level in the body; when the air pressure is higher, the water level in the spout will drop below the water level in the body. A variation of this type of barometer can be easily made at home.
Mercury barometers.
A mercury barometer has a glass tube with a height of at least 84 cm, closed at one end, with an open mercury-filled reservoir at the base. The weight of the mercury creates a vacuum in the top of the tube. Mercury in the tube adjusts until the weight of the mercury column balances the atmospheric force exerted on the reservoir. High atmospheric pressure places more force on the reservoir, forcing mercury higher in the column. Low pressure allows the mercury to drop to a lower level in the column by lowering the force placed on the reservoir. Since higher temperature levels around the instrument will reduce the density of the mercury, the scale for reading the height of the mercury is adjusted to compensate for this effect.
Torricelli documented that the height of the mercury in a barometer changed slightly each day and concluded that this was due to the changing pressure in the atmosphere. He wrote: "We live submerged at the bottom of an ocean of elementary air, which is known by incontestable experiments to have weight".
The mercury barometer's design gives rise to the expression of atmospheric pressure in inches or millimeters or feet (torr): the pressure is quoted as the level of the mercury's height in the vertical column. Typically, atmospheric pressure is measured between 26.5 to 31.5 inches of Hg. One atmosphere (1 atm) is equivalent to 29.92 inches of mercury.
Design changes to make the instrument more sensitive, simpler to read, and easier to transport resulted in variations such as the basin, siphon, wheel, cistern, Fortin, multiple folded, stereometric, and balance barometers. Fitzroy barometers combine the standard mercury barometer with a thermometer, as well as a guide of how to interpret pressure changes. Fortin barometers use a variable displacement mercury cistern, usually constructed with a thumbscrew pressing on a leather diaphragm bottom. This compensates for displacement of mercury in the column with varying pressure. To use a Fortin barometer, the level of mercury is set to the zero level before the pressure is read on the column. Some models also employ a valve for closing the cistern, enabling the mercury column to be forced to the top of the column for transport. This prevents water-hammer damage to the column in transit.
On June 5, 2007, a European Union directive was enacted to restrict the sale of mercury, thus effectively ending the production of new mercury barometers in Europe.
Vacuum pump oil barometer.
Using vacuum pump oil as the working fluid in a barometer has led to the creation of the new "World's Tallest Barometer" in February 2013. The barometer at Portland State University (PSU) uses doubly distilled vacuum pump oil and has a nominal height of ~12.4 m for the oil column height; expected excursions are in the range of ±0.4 m over the course of a year. Vacuum pump oil has very low vapor pressure and it is available in a range of densities; the lowest density vacuum oil was chosen for the PSU barometer to maximize the oil column height.
Aneroid barometers.
An aneroid barometer is an instrument for measuring pressure as a method that does not involve liquid. Invented in 1844 by French scientist Lucien Vidi, the aneroid barometer uses a small, flexible metal box called an aneroid cell (capsule), which is made from an alloy of beryllium and copper. The evacuated capsule (or usually more capsules) is prevented from collapsing by a strong spring. Small changes in external air pressure cause the cell to expand or contract. This expansion and contraction drives mechanical levers such that the tiny movements of the capsule are amplified and displayed on the face of the aneroid barometer. Many models include a manually set needle which is used to mark the current measurement so a change can be seen. In addition, the mechanism is made deliberately "stiff" so that tapping the barometer reveals whether the pressure is rising or falling as the pointer moves. This type of barometer is common in homes and in recreational boats, as well as small aircraft. It is also used in meteorology, mostly in barographs and as a pressure instrument in radiosondes.
Barographs.
A barograph records a graph of some atmospheric pressure and uses an aneroid barometer mechanism to move a needle on a smoked foil or to move a pen upon paper, both of which are attached to a drum moved by clockwork.
MEMS Barometers.
Microelectromechanical systems (or MEMS) barometers are extremely small devices between 1 to 100 micrometres in size (i.e. 0.001 to 0.1 mm). They are created via photolithography or photochemical machining. Typical applications include miniaturized weather stations, electronic barometers and altimeters.
More unusual barometers.
There are many other more unusual types of barometer. From variations on the storm barometer, such as the Collins Patent Table Barometer, to more traditional looking designs such as Hooke's Otheometer and the Ross Sympiesometer. Some, such as the Shark Oil barometer, work only in a certain temperature range, achieved in warmer climates.
A barometer can also be found in smartphones such as the Samsung Galaxy Nexus, Samsung Galaxy S3-S6, Motorola Xoom and Apple iPhone 6 smartphones, to provide a faster GPS lock.
Applications.
Using barometric pressure and the pressure tendency (the change of pressure over time) has been used in weather forecasting since the late 19th century. When used in combination with wind observations, reasonably accurate short-term forecasts can be made. Simultaneous barometric readings from across a network of weather stations allow maps of air pressure to be produced, which were the first form of the modern weather map when created in the 19th century. Isobars, lines of equal pressure, when drawn on such a map, gives a contour map showing areas of high and low pressure. Localized high atmospheric pressure acts as a barrier to approaching weather systems, diverting their course. Atmospheric lift caused by low-level wind convergence into the surface low brings clouds and potentially precipitation. The larger the change in pressure, especially if more than 3.5 hPa, the larger the change in weather can be expected. If the pressure drop is rapid, a low pressure system is approaching, and there is a greater chance of rain . Rapid pressure rises, such as in the wake of a cold front, are associated with improving weather conditions, such as clearing skies.
Compensations.
Temperature.
The density of mercury will change with temperature, so a reading must be adjusted for the temperature of the instrument. For this purpose a mercury thermometer is usually mounted on the instrument. Temperature compensation of an aneroid barometer is accomplished by including a bi-metal element in the mechanical linkages. Aneroid barometers sold for domestic use typically have no compensation under the assumption that they will be used within a controlled room temperature range.
Altitude.
As the air pressure will be decreased at altitudes above sea level (and increased below sea level) the uncorrected reading of the barometer will be dependent upon its location. This pressure measurement is then converted to an equivalent sea-level pressure for purposes of reporting. For example, if a barometer located at sea level and under fair weather conditions is moved to an altitude of 1,000 feet (305 m), about 1 inch of mercury (~35 hPa) must be added on to the reading. The barometer readings at the two locations should be the same if there are negligible changes in time, horizontal distance, and temperature. If this were not done, there would be a false indication of an approaching storm at the higher elevation.
Aneroid barometers have a mechanical adjustment that allows the equivalent sea level pressure to be read directly and without further adjustment if the instrument is not moved to a different altitude. Setting an aneroid barometer is similar to setting an analog clock that is not at the correct time. Its dial is rotated so that the current atmospheric pressure from a known accurate and nearby barometer (such as the local weather station) is displayed. No calculation is needed, as the source barometer reading has already been converted to equivalent sea-level pressure, and this is transferred to the barometer being set—regardless of its altitude. Though somewhat rare, a few aneroid barometers intended for monitoring the weather are calibrated to manually adjust for altitude. In this case, knowing "either" the altitude or the current atmospheric pressure would be sufficient for future accurate readings.
The table below shows examples for three locations in the city of San Francisco, California. Note the corrected barometer readings are identical, and based on equivalent sea-level pressure. (Assume a temperature of 15 °C.)
Barometers and atmospheric pressure calculations.
When atmospheric pressure is measured by a barometer, the pressure is also referred to as the "barometric pressure". Assume a barometer with a cross-sectional area, A, a height, h, filled with mercury from the bottom at Point B to the top at Point C. The pressure at the bottom of the barometer, Point B, is equal to the atmospheric pressure. The pressure at the very top, Point C, can be taken as zero because there is only mercury vapor above this point and its pressure is very low relative to the atmospheric pressure. Therefore, one can find the atmospheric pressure using the barometer and this equation:
Patm = ρgh
where ρ is the density of mercury, g is the gravitational acceleration, and h is the height of the mercury column above the free surface area. Note that the physical dimensions (length of tube and cross-sectional area of the tube) of the barometer itself have no effect on the height of the fluid column in the tube.
In thermodynamic calculations, a commonly used pressure unit is the "standard atmosphere". This is the pressure resulting from a column of mercury of 760mm in height at 0 °C. For the density of mercury, use ρHg = 13,595 kg/m3 and for gravitational acceleration use g = 9.807 m/s2.
If water were used (instead of mercury) to meet the standard atmospheric pressure, a water column of roughly 10.3 m (33.8 ft) would be needed.
Standard atmospheric pressure as a function of elevation.
Note: 1 torr = 133.3 Pa = 0.03937 In Hg

</doc>
<doc id="47489" url="http://en.wikipedia.org/wiki?curid=47489" title="Bioassay">
Bioassay

Bioassay (commonly used shorthand for biological assay or assessment), or biological standardization is a type of scientific experiment. A bioassay involves the use of live animal or plant ("in vivo") or tissue or cell ("in vitro") to determine the biological activity of a substance, such as a hormone or drug. Bioassays are typically conducted to measure the effects of a substance on a living organism and are essential in the development of new drugs and in monitoring environmental pollutants. Both are procedures by which the potency or the nature of a substance is estimated by studying its effects on living matter. A bioassay can also be used to determine the concentration of a particular constitution of a mixture that may cause harmful effects on organisms or the environment.
Use.
Bioassays are procedures that can determine the concentration or purity or biological activity of a substance such as vitamin, hormone or plant growth factor by measuring the effect on an organism, tissue, cells, enzyme or receptor. Bioassays may be or . Qualitative bioassays are used for assessing the physical effects of a substance that may not be quantified, such as seeds fail to germinate or develop abnormally deformity. An example of a qualitative bioassay includes Arnold Adolph Berthold's famous experiment on castrated chickens. This analysis found that by removing the testicles of a chicken, it would not develop into a rooster because the endocrine signals necessary for this process were not available. Quantitative bioassays involve estimation of the concentration or potency of a substance by measurement of the biological response that it produces. Quantitative bioassays are typically analyzed using the methods of biostatistics. For more information Look up Basic and Clinical Pharmacology by Bertram G. Katzung.
Definition.
""The determination of the relative strength of a substance (e.g., a drug or hormone or toxicant) by comparing its effect on a test organism with that of a standard preparation." is called bioassay.
Types.
Bioassays are of two types:
Quantal.
A quantal assay involves an "all or none response".
Graded.
Graded assays are based on the observation that there is a proportionate increase in the observed response following an increase in the concentration or dose. The parameters employed in such bioassays are based on the nature of the effect the substance is expected to produce. For example: contraction of smooth muscle preparation for assaying histamine or the study of blood pressure response in case of adrenaline.
A graded bioassay can be performed by employing any of the below-mentioned techniques. The choice of procedure depends on:
Techniques.
Matching Bioassay:
It is the simplest type of the bioassay. In this type of bioassay, response of the test substance taken first and the observed response is tried to match with the standard response. Several responses of the standard drug are recorded till a close matching point to that of the test substance is observed. A corresponding concentration is thus calculated. This assay is applied when the sample size is too small. Since the assay does not involve the recording of concentration response curve, the sensitivity of the preparation is not taken into consideration. Therefore, precision and reliability is not very good.
Interpolation bioassay:
Bioassays are conducted by determining the amount of preparation of unknown potency required to produce a definite effect on suitable test animals or organs or tissue under standard conditions. This effect is compared with that of a standard. Thus the amount of the test substance required to produce the same biological effect as a given quantity the unit of a standard preparation is compared and the potency of the unknown is expressed as a % of that of the standard by employing a simple formula.
Many times, a reliable result cannot be obtained using this calculation. Therefore it may be necessary to adopt more precise methods of calculating potency based upon observations of relative, but not necessarily equal effects, likewise, statistical methods may also be employed.
The data (obtained from either of assay techniques used) on which bioassay are based may be classified as quantal or graded response.
Both these depend ultimately on plotting or making assumption concerning the form of DRC.
Environmental bioassays.
Environmental bioassays are generally a broad-range survey of toxicity. A toxicity identification evaluation is conducted to determine what the relevant toxicants are. Although bioassays are beneficial in determining the biological activity within an organism, they can often be time-consuming and laborious. Organism-specific factors may result in data that is not applicable to others in that species. For these reasons, other biological techniques are often employed, including radioimmunoassays. "See bioindicator."
Water pollution control requirements in the United States require some industrial dischargers and municipal sewage treatment plants to conduct bioassays. These procedures, called whole effluent toxicity tests, include acute toxicity tests as well as chronic test methods. The methods involve exposing living aquatic organisms to samples of wastewater. For example the bioassay ECOTOX uses the microalgae Euglena gracilis to test the toxicity of water samples. (See Bioindicator#Microalgae as bioindicators and water quality)

</doc>
<doc id="47490" url="http://en.wikipedia.org/wiki?curid=47490" title="Biodegradation">
Biodegradation

IUPAC definition
"Degradation" caused by enzymatic process resulting from the action of cells.
"Note": Modified to exclude "abiotic enzymatic" processes.
 
Biodegradation is the chemical dissolution of materials by bacteria, fungi or other biological means. Although often conflated, biodegradable is distinct in meaning from compostable. While biodegradable simply means to be consumed by microorganisms and return to compounds found in nature, "compostable" makes the specific demand that the object break down under composting conditions. The term is often used in relation to ecology, waste management, biomedicine, and the natural environment (bioremediation) and is now commonly associated with environmentally friendly products that are capable of decomposing back into natural elements. Organic material can be degraded aerobically with oxygen, or anaerobically, without oxygen. Biosurfactant, an extracellular surfactant secreted by microorganisms, enhances the biodegradation process.
Biodegradable matter is generally organic material such as plant and animal matter and other substances originating from living organisms, or artificial materials that are similar enough to plant and animal matter to be put to use by microorganisms. Some microorganisms have a naturally occurring, microbial catabolic diversity to degrade, transform or accumulate a huge range of compounds including hydrocarbons (e.g. oil), polychlorinated biphenyls (PCBs), polyaromatic hydrocarbons (PAHs), pharmaceutical substances, radionuclides, pesticides, and metals. Decomposition of biodegradable substances may include both biological and abiotic steps. Products that contain biodegradable matter and non-biodegradable matter are often marketed as biodegradable.
Monona Rossol wrote that "biodegradable substances break down into more than one set of chemicals, which are usually called primary and secondary degradation products. Any of these may be toxic."
Meteorology.
In nature, different materials biodegrade at different rates, and a number of factors are important in the rate of degradation of organic compounds. To be able to work effectively, most microorganisms that assist the biodegradability need light, water and oxygen. Temperature is also an important factor in determining the rate of biodegradability. This is because microorganisms tend to reproduce faster in warmer conditions. The rate of degradation of many soluble organic compounds is limited by bioavailability when the compounds have a strong affinity for surfaces in the environment, and thus must be released to solution before organisms can degrade them.
Biodegradability can be measured in a number of ways. Scientists often use respirometry tests for aerobic microbes. First one places a solid waste sample in a container with microorganisms and soil, and then aerate the mixture. Over the course of several days, microorganisms digest the sample bit by bit and produce carbon dioxide – the resulting amount of CO2 serves as an indicator of degradation. Biodegradability can also be measured by anaerobic microbes and the amount of methane or alloy that they are able to produce. In formal scientific literature, the process is termed bio-remediation.
Plastics.
Biodegradable plastic is plastic that has been treated to be easily broken down by microorganisms and return to nature. Many technologies exist today that allow for such treatment. Currently there are some synthetic polymers that can be broken down by microorganisms such as polycaprolactone, others are polyesters and aromatic-aliphatic esters, due to their ester bonds being susceptible to attack by water. Some examples of these are the bio-derived poly-3-hydroxybutyrate, the renewably derived polylactic acid, and the synthetic polycaprolactone. Others are the cellulose-based cellulose acetate and celluloid (cellulose nitrate).
Under low oxygen conditions biodegradable plastics break down slower and with the production of methane, like other organic materials do. The breakdown process is accelerated in a dedicated compost heap. Starch-based plastics will degrade within two to four months in a home compost bin, while polylactic acid is largely undecomposed, requiring higher temperatures. Polycaprolactone and polycaprolactone-starch composites decompose slower, but the starch content accelerates decomposition by leaving behind a porous, high surface area polycaprolactone. Nevertheless, it takes many months.
Many plastic companies have gone so far even to say that their plastics are compostable, typically listing corn starch as an ingredient. However, these claims are questionable because the plastics industry operates under its own definition of compostable:
"that which is capable of undergoing biological decomposition in a compost site such that the material is not visually distinguishable and breaks down into carbon dioxide, water, inorganic compounds and biomass at a rate consistent with known compostable materials." (Ref: ASTM D 6002)
Using this new definition, "carbon dioxide, water, inorganic compounds and biomass" encompasses every substance in the known universe, it makes no restriction on what the plastic leaves behind after it has biodegraded. So, while plastic manufacturers may legally be on solid ground, "compostable plastics" can not be said to be compostable in the traditional sense. However the word biodegradable does still apply.
Biodegradable technology.
In 1973 it was proved for first time that polyester degrades when disposed in bioactive material such as soil. As a result, polyesters are water resistant and can be melted and shaped into sheets, bottles, and other products, making certain plastics now available as a biodegradable product. Following, Polyhydroxylalkanoates (PHAs) were produced directly from renewable resources by microbes. They are approximately 95% cellular bacteria and can be manipulated by genetic strategies. The composition and biodegradability of PHAs can be regulated by blending it with other natural polymers. In the 1980s the company ICI Zenecca commercialized PHAs under the name Biopol. It was used for the production of shampoo bottles and other cosmetic products. Consumer response was unusual. Consumers were willing to pay more for this product because it was natural and biodegradable, which had not occurred before.
Now biodegradable technology is a highly developed market with applications in product packaging, production and medicine. Biodegradable technology is concerned with the manufacturing science of biodegradable materials. It imposes science based mechanisms of plant genetics into the processes of today. Scientists and manufacturing corporations can help impact climate change by developing a use of plant genetics that would mimic some technologies. By looking to plants, such as biodegradable material harvested through photosynthesis, waste and toxins can be minimized.
Oxo-biodegradable technology, which has further developed biodegradable plastics, has also emerged. Oxo-biodegradation is defined by CEN (the European Standards Organisation) as "degradation resulting from oxidative and cell-mediated phenomena, either simultaneously or successively." Whilst sometimes described as "oxo-fragmentable," and "oxo-degradable" this describes only the first or oxidative phase. These descriptions should not be used for material which degrades by the process of oxo-biodegradation defined by CEN, and the correct description is "oxo-biodegradable."
By combining plastic products with very large polymer molecules, which contain only carbon and hydrogen, with oxygen in the air, the product is rendered capable of decomposing in anywhere from a week to one to two years. This reaction occurs even without prodegradant additives but at a very slow rate. That is why conventional plastics, when discarded, persist for a long time in the environment. Oxo-biodegradable formulations catalyze and accelerate the biodegradation process but it takes considerable skill and experience to balance the ingredients within the formulations so as to provide the product with a useful life for a set period, followed by degradation and biodegradation.
Biodegradable technology is especially utilized by the bio-medical community. Biodegradable polymers are classified into three groups:
medical, ecological, and dual application, while in terms of origin they are divided into two groups: natural and synthetic. The Clean Technology Group is exploiting the use of supercritical carbon dioxide, which under high pressure at room temperature is a solvent that can use biodegradable plastics to make polymer drug coatings. The polymer (meaning a material composed of molecules with repeating structural units that form a long chain) is used to encapsulate a drug prior to injection in the body and is based on lactic acid, a compound normally produced in the body, and is thus able to be excreted naturally. The coating is designed for controlled release over a period of time, reducing the number of injections required and maximizing the therapeutic benefit. Professor Steve Howdle states that biodegradable polymers are particularly attractive for use in drug delivery, as once introduced into the body they require no retrieval or further manipulation and are degraded into soluble, non-toxic by-products. Different polymers degrade at different rates within the body and therefore polymer selection can be tailored to achieve desired release rates.
Other biomedical applications include the use of biodegradable, elastic shape-memory polymers. Biodegradable implant materials can now be used for minimally invasive surgical procedures through degradable thermoplastic polymers. These polymers are now able to change their shape with increase of temperature, causing shape memory capabilities as well as easily degradable sutures. As a result, implants can now fit through small incisions, doctors can easily perform complex deformations, and sutures and other material aides can naturally biodegrade after a completed surgery.
Etymology of "biodegradable".
The first known use of the word in biological text was in 1961 when employed to describe the breakdown of material into the base components of carbon, hydrogen, and oxygen by microorganisms. Now biodegradable is commonly associated with environmentally friendly products that are part of the earth's innate cycle and capable of decomposing back into natural elements.

</doc>
<doc id="47492" url="http://en.wikipedia.org/wiki?curid=47492" title="Biomass (ecology)">
Biomass (ecology)

Biomass, in ecology, is the mass of living biological organisms in a given area or ecosystem at a given time. Biomass can refer to "species biomass", which is the mass of one or more species, or to "community biomass", which is the mass of all species in the community. It can include microorganisms, plants or animals. The mass can be expressed as the average mass per unit area, or as the total mass in the community.
How biomass is measured depends on why it is being measured. Sometimes, the biomass is regarded as the natural mass of organisms "in situ", just as they are. For example, in a salmon fishery, the salmon biomass might be regarded as the total wet weight the salmon would have if they were taken out of the water. In other contexts, biomass can be measured in terms of the dried organic mass, so perhaps only 30% of the actual weight might count, the rest being water. For other purposes, only biological tissues count, and teeth, bones and shells are excluded. In stricter scientific applications, biomass is measured as the mass of organically bound carbon (C) that is present.
Apart from bacteria, the total live biomass on Earth is about 560 billion tonnes C, and the total annual primary production of biomass is just over 100 billion tonnes C/yr. However, the total live biomass of bacteria may exceed that of plants and animals.
Ecological pyramids.
An ecological pyramid is a graphical representation that shows, for a given ecosystem, the relationship between biomass or biological productivity and trophic levels.
An ecological pyramid provides a snapshot in time of an ecological community.
The bottom of the pyramid represents the primary producers (autotrophs). The primary producers take energy from the environment in the form of sunlight or inorganic chemicals and use it to create energy-rich molecules such as carbohydrates. This mechanism is called primary production. The pyramid then proceeds through the various trophic levels to the apex predators at the top.
When energy is transferred from one trophic level to the next, typically only ten percent is used to build new biomass. The remaining ninety percent goes to metabolic processes or is dissipated as heat. This energy loss means that productivity pyramids are never inverted, and generally limits food chains to about six levels. However, in oceans, biomass pyramids can be wholly or partially inverted, with more biomass at higher levels.
Terrestrial biomass.
Terrestrial biomass generally decreases markedly at each higher trophic level (plants, herbivores, carnivores). Examples of terrestrial producers are grasses, trees and shrubs. These have a much higher biomass than the animals that consume them, such as deer, zebras and insects. The level with the least biomass are the highest predators in the food chain, such as foxes and eagles.
In a temperate grassland, grasses and other plants are the primary producers at the bottom of the pyramid. Then come the primary consumers, such as grasshoppers, voles and bison, followed by the secondary consumers, shrews, hawks and small cats. Finally the tertiary consumers, large cats and wolves. The biomass pyramid decreases markedly at each higher level.
Ocean biomass.
 
The marine food chain
  predatory fish
  filter feeders
predatory zooplankton
zooplankton
phytoplankton
 
Ocean biomass, in a reversal of terrestrial biomass, can increase at higher trophic levels. In the ocean, the food chain typically starts with phytoplankton, and follows the course:
Phytoplankton → zooplankton → predatory zooplankton → filter feeders → predatory fish
Phytoplankton are the main primary producers at the bottom of the marine food chain. Phytoplankton use photosynthesis to convert inorganic carbon into protoplasm. They are then consumed by microscopic animals called zooplankton.
Zooplankton comprise the second level in the food chain, and includes small crustaceans, such as copepods and krill, and the larva of fish, squid, lobsters and crabs.
In turn, small zooplankton are consumed by both larger predatory zooplankters, such as krill, and by forage fish, which are small schooling filter feeding fish. This makes up the third level in the food chain.
The fourth trophic level consists of predatory fish, marine mammals and seabirds that consume forage fish. Examples are swordfish, seals and gannets.
Apex predators, such as orcas, which can consume seals, and shortfin mako sharks, which can consume swordfish, make up the fifth trophic level. Baleen whales can consume zooplankton and krill directly, leading to a food chain with only three or four trophic levels.
Marine environments can have inverted biomass pyramids. In particular, the biomass of consumers (copepods, krill, shrimp, forage fish) is larger than the biomass of primary producers. This happens because the ocean's primary producers are tiny phytoplankton that grow and reproduce rapidly, so a small mass can have a fast rate of primary production. In contrast, terrestrial primary producers grow and reproduce slowly.
There is an exception with cyanobacteria. Marine cyanobacteria are the smallest known photosynthetic organisms; the smallest of all, "Prochlorococcus", is just 0.5 to 0.8 micrometres across. Prochlorococcus is possibly the most plentiful species on Earth: a single millilitre of surface seawater may contain 100,000 cells or more. Worldwide, there are estimated to be several octillion (~1027) individuals. "Prochlorococcus" is ubiquitous between 40°N and 40°S and dominates in the oligotrophic (nutrient poor) regions of the oceans. The bacterium accounts for an estimated 20% of the oxygen in the Earth's atmosphere, and forms part of the base of the ocean food chain.
Bacterial biomass.
There are typically 50 million bacterial cells in a gram of soil and a million bacterial cells in a millilitre of fresh water. In all, it has been estimated that there are about five million trillion trillion, or 5 × 1030 (5 nonillion) bacteria on Earth with a total biomass equaling that of plants. Some researchers believe that the total biomass of bacteria exceeds that of all plants and animals.
Global biomass.
Estimates for the global biomass of species and higher level groups are not always consistent across the literature. Apart from bacteria, the total global biomass has been estimated at about 560 billion tonnes C. Most of this biomass is found on land, with only 5 to 10 billion tonnes C found in the oceans. On land, there is about 1,000 times more plant biomass ("phytomass") than animal biomass ("zoomass"). About 18% of this plant biomass is eaten by the land animals. However, in the ocean, the animal biomass is nearly 30 times larger than the plant biomass. Most ocean plant biomass is eaten by the ocean animals.
Humans comprise about 100 million tonnes of the Earth's dry biomass, domesticated animals about 700 million tonnes, and crops about 2 billion tonnes.
The most successful animal species, in terms of biomass, may well be Antarctic krill, "Euphausia superba", with a fresh biomass approaching 500 million tonnes, although domestic cattle may also reach these immense figures. However, as a group, the small aquatic crustaceans called copepods may form the largest animal biomass on earth. A 2009 paper in "Science" estimates, for the first time, the total world fish biomass as somewhere between 0.8 and 2.0 billion tonnes. It has been estimated that about 1% of the global biomass is due to phytoplankton, and a staggering 25% is due to fungi.
Global rate of production.
Net primary production is the rate at which new biomass is generated, mainly due to photosynthesis. Global primary production can be estimated from satellite observations. Satellites scan the normalised difference vegetation index (NDVI) over terrestrial habitats, and scan sea-surface chlorophyll levels over oceans. This results in 56.4 billion tonnes C/yr (53.8%), for terrestrial primary production, and 48.5 billion tonnes C/yr for oceanic primary production. Thus, the total photoautotrophic primary production for the Earth is about 104.9 billion tonnes C/yr. This translates to about 426 gC/m²/yr for land production (excluding areas with permanent ice cover), and 140 gC/m²/yr for the oceans.
However, there is a much more significant difference in standing stocks—while accounting for almost half of total annual production, oceanic autotrophs account for only about 0.2% of the total biomass. Autotrophs may have the highest global proportion of biomass, but they are closely rivaled or surpassed by microbes.
Terrestrial freshwater ecosystems generate about 1.5% of the global net primary production.
Some global producers of biomass in order of productivity rates are

</doc>
<doc id="47496" url="http://en.wikipedia.org/wiki?curid=47496" title="Biota">
Biota

Biota may refer to:

</doc>
<doc id="47497" url="http://en.wikipedia.org/wiki?curid=47497" title="Doctor Doom">
Doctor Doom

Doctor Victor von Doom is a fictional supervillain that appears in publications by Marvel Comics. The son of Romani witch Cynthia Von Doom, Doctor Doom is a recurring archenemy of the Fantastic Four, and leader of the fictional nation of Latveria. He is both a genius inventor and a sorcerer. While his chief opponents have been the Fantastic Four, he has also come into conflict with the Avengers and other superheroes in the Marvel Universe.
Doctor Doom has made many appearances in video games, television series, and merchandise such as action figures and trading cards. He was ranked as the 4th Greatest Villain by "Wizard" on its 100 Greatest Villains of All Time list. IGN's list of the Top 100 Comic Book Villains of All Time ranked Doctor Doom as #3.
Doctor Doom has also been featured in other Marvel-endorsed feature films such as "The Fantastic Four" played by Joseph Culp, while actor Julian McMahon played him in the 2005 film "Fantastic Four" and its 2007 sequel "" as the film's antagonist. Toby Kebbell will portray the character in the upcoming 2015 reboot.
Publication history.
Created by writer-editor Stan Lee and artist/co-plotter Jack Kirby, the character first appeared in "The Fantastic Four" #5 (July 1962) wearing his trademark metal mask and green cloak.
Creation and development.
Like many of Marvel's Silver Age characters, Doctor Doom was conceived by writer Stan Lee and artist Jack Kirby. With the "Fantastic Four" title performing well, Lee and Kirby were trying to dream up a "soul-stirring…super sensational new villain." Looking for a name, Lee latched onto "Doctor Doom" as "eloquent in its simplicity — magnificent in its implied menace."
Due to the rush to publish, the character was not given a full origin story until "Fantastic Four Annual" #2, two years after his debut.
Jack Kirby modeled Doctor Doom after Death, with the armor standing in for that character's skeleton; "It was the reason for the armor and the hood. Death is connected with armor and the inhuman-like steel. Death is something without mercy, and human flesh contains that mercy." Kirby further described Doctor Doom as being "paranoid", wrecked by his twisted face and wanting the whole world to be like him. Kirby went on to say that "Doctor Doom is an evil person, but he's not always been evil. He was [respected]…but through a flaw in his own character, he was a perfectionist." At one point in the 1970s, Kirby drew his interpretation of what Doctor Doom would look like under the mask, giving Doctor Doom only "a tiny scar on his cheek." Due to this slight imperfection, Doctor Doom hides his face not from the world, but from himself. To Kirby, this is the motivation for Doctor Doom's vengeance against the world; because others are superior due to this slight scar, Doom wants to elevate himself above them. Typical of Lee's writing characterization of Doctor Doom is his arrogance; his pride leads to Doctor Doom's disfigurement at the hands of his own machine, and to the failures of many of his schemes. There is also an idea that Doctor Doom placed his mask on his face before it was fully cool, burning his face. In some early stories glimpses of his face are shown, in which he appears to be bald.
While the Fantastic Four had fought various villains such as the Mole Man, Skrulls, the Miracle Man, and Namor the Sub-Mariner, Doctor Doom managed to overshadow them all and became the Fantastic Four's archnemesis.
During the 1970s, Doctor Doom branched out to more Marvel titles such as "Astonishing Tales", "The Incredible Hulk", and "Super-Villain Team-Up", starting in 1975, as well as appearances in "Marvel Team-Up", beginning with issue #42 (February 1976). Doctor Doom's origin was also a feature in "Astonishing Tales" when his ties to the villain Mephisto were revealed.
1980s-1990s.
1981 saw Marvel and DC Comics collaborate on another project. In 1976 the two companies had published "Superman vs. the Amazing Spider-Man", and seeking to replicate that success the two companies again teamed the characters up, in "Superman and Spider-Man". Marvel editor in chief Jim Shooter co-wrote the story alongside Marv Wolfman, and recalled choosing Doctor Doom based on his iconic status: "I figured I needed the heaviest-duty bad guy we had to offer — Doctor Doom. Their greatest hero against our greatest villain."
The same year saw John Byrne begin his six-year run writing and illustrating "Fantastic Four" in 1981, sparking a "second golden age" for the title but also attempting to ""turn the clock back [...] get back and see fresh what it was that made the book great at its inception." Doctor Doom made his first appearance under Byrne's tenure with issue #236. Whereas Kirby had intimated that Doom's disfigurement was more a figment of Victor's vain personality, Byrne expressed that Doctor Doom 's face was truly ravaged; only Doctor Doom 's own robot slaves are allowed to see the monarch without his helmet. Byrne also emphasized other aspects of Doom's personality; despite his ruthless nature, Doctor Doom is a man of honor. returning to Latveria after being temporarily deposed, Doctor Doom abandons a scheme to wrest mystical secrets from Doctor Strange in order to oversee his land's reconstruction. Though possessing a tempestuous temper, Doctor Doom also occasionally shows warmth and empathy to others; he tries to free his mother from Mephisto and treats Kristoff Vernard like his own son. Byrne also gave further detail regarding Doom's scarring; Byrne used the idea that the accident at Empire State University only left Doctor Doom with a small scar that was exaggerated into a more disfiguring accident by Doctor Doom's own arrogance; when Doctor Doom puts on the armor forged for him when it had yet to cool, however, he truly damages his face.
After Byrne's departure Doctor Doom continued to be a major villain in "Fantastic Four", and as the 1980s continued Doom appeared in other comics such as "Punisher", "The Spectacular Spider-Man", and "Excalibur". Under Fantastic Four writer Steven Englehart, Doctor Doom became exiled from Latveria by his heir Kristoff, who was brainwashed into thinking he was Doctor Doom. Doctor Doom would spend most of his time in exile planning his return, but Englehart left the title before he could resolve the storyline. This storyline ultimately ended with the controversial "Fantastic Four" #350, where writer Walt Simonson had the Doctor Doom who had been seen in the book during the Englehart run being revealed to be a Doctor Doom bot and the real Doctor Doom, in a newly redesigned armor, returning to claim his country from his usurper. Simonson's retcon stated that Doctor Doom 's last real appearance was in the famous "Battle of Baxter Building" though with occasional trips back home, though Doctor Victor Von Doom was shown to be unaware of certain major changes at the time to the Fantastic Four. An urban legend states that Simonson drew up a list of official stories which featured the real Doctor Doom and those which did not but this plotline was dropped and never mentioned again by later writers, who ignored Simonson's declaration as subterfuge Doctor Doom stated for the sake of blaming past failures on Doombots.
Modern depictions.
In 2003 Doctor Doom was the villain in the "Fantastic Four" story arc "Unthinkable", in which Doctor Victor Von Doom viscerally skins his childhood love Valeria alive to the bone, and turns her flesh into mystic armour, imprisons Franklin Richards in Hell, captures Valeria Richards, and succeeds in de-powering and imprisoning the Fantastic Four. Writer Mark Waid sought to redefine Doctor Doom 's character in a way that had not been seen before. Waid punctuated this reinterpretation of Doctor Doom during his "Unthinkable" saga (Vol 2 #66-70 & Vol 1 (restart) #500) as an absolute sadist by having Von Doom ruthlessly murder Valeria (namesake of the Richards's daughter), his first love and granddaughter to his long serving faithful retainer Boris, in order to be granted access to powerful magic by a trio of demons, Valeria being the treasured possession that they demanded in exchange. He subsequently attempted to prove his superiority to Reed by giving him the chance to find his way out of a prison that could only be escaped by mastering magic, in the belief that Reed would fail to do so, but with the aid of the astral projection of Doctor Strange Reed learned to master magic by accepting that he could not understand it. This fight resulted in Doctor Doom being trapped in Hell when Reed tricked him into rejecting the demons, until the events of Ragnarok, when Thor's hammer Mjolnir fell through dimensions and gave Doctor Doom a way out of Hell when it was lost after Thor's apparent death.
In 2005 and 2006, Doctor Doom was featured in his own limited series, "Books of Doom ", a retelling of the origin story by Ed Brubaker. In an interview, Brubaker said the series was a way to elaborate on the earlier portions of Doctor Doom 's life which had not been seen often in the comics. The series also set out to determine if Doctor Doom 's path from troubled child to dictator was fated or Doctor Doom 's own faults led to his corruption — in essence, a nature versus nurture question. Brubaker's version of Doctor Doom was heavily influenced by the original Lee/Kirby version; responding to a question if he would show Doctor Doom 's face, Brubaker stated "[F]ollowing Kirby's example, I think it's better not to show it."
The Mighty Avengers invaded Latveria, Doctor Doom 's nation, due to his involvement in creating a chemical bomb that would infect people with the symbiote (though it was recently revealed that this attack was actually set up by Kristoff Vernard to put Doctor Doom out of the picture prior to Kristoff's future attempt at a coup). Due to Ultron's interference, the bomb was dropped on Manhattan, but the Mighty Avengers are able to stop the effects on the people. The Mighty Avengers proceed to invade Latveria. During the invasion, the Sentry, Iron Man, and Doctor Doom are sent to the past thanks to Doctor Doom ’s time platform. Eventually, the trio breaks into the Baxter Building and make use of a confiscated time machine to return to the present era, the Sentry taking advantage of the fact he will soon be forgotten by the world to easily defeat the Thing. Doctor Doom transports himself to Morgana's castle to summon up a magical army and captures the Avengers, but they free themselves and he is arrested for terrorist crimes against humanity after a brief struggle that culminated with the Sentry tearing off Doctor Doom 's mask.
During Dark Reign when Norman Osborn is in charge, Doctor Doom is released and sent back to Latveria. However, Morgana le Fay engages him in a magical battle, which he is losing until the Dark Avengers rescue him. He then magically rebuilds his kingdom.
The character is also featured in "Siege" storyline and in the five issue mini-series Doomwar written by Jonathan Maberry.
Doctor Doom soon allies himself with the isolationist group known as the Desturi, to take control of Wakanda. He attacked and wounded T'Challa, the current Black Panther, maiming him enough to prevent him from holding the mantle again. Doctor Doom 's main objective was to secure Wakanda's store of vibranium, which he could mystically enhance to make himself unstoppable.
In the Mark Millar penned "Fantastic Four" 566-569 Doctor Doom received a significant power upgrade. He was thrown back in time (perhaps about 50 million years) by the Marquis of Death. Doctor Doom then fought through time and space to get back to the present to seek revenge on the Marquis of Death. Doctor Doom stated, as he killed the Marquis, he had rebuilt every molecule of his being and increased his power all to destroy the Marquis. In later issues this seems to have been ignored however, with writers treating Doctor Doom the way they have always before in terms of power.
 Doctor Doom later joins the supervillain group known as the Intelligencia, but is betrayed when they captured him to complete their plan. With the help of Bruce Banner, he escapes and returns to Latveria. He appears to have been damaged by this experience.
At the start of the story arc "Fantastic Four: Three," Doctor Doom feels that he needs to be "reborn" and makes plans to abdicate his throne and give it to Kristoff when Valeria teleports to his room unexpectedly asking for his assistance to help her father. Valeria quickly notices that Doctor Doom has suffered brain damage and makes a deal with him to restore his mental capacities if he helps Reed and the Fantastic Four. Doctor Doom agrees to her proposition. Later, Doctor Doom appears among those in attendance at Johnny Storm's funeral.
Due to the agreement, Doctor Doom is recommended by Nathaniel and Valeria Richards to be a member of the Future Foundation. Objecting, Thing attacks Doctor Doom out of anger, but the fight is stopped by Mister Fantastic and the Invisible Woman, who welcomes Doctor Doom to their group. In an issue of the Hulk series, it is revealed that Doctor Doom performed brain surgery on Hulk to separate him from Banner, extracting the uniquely Banner elements from Hulk's brain and cloning a new body for Banner, in return for an initially-unspecified favor from the Hulk. However, when Doctor Doom demands to keep Banner for his own purposes, the Hulk reneges on the deal and flees with Banner's body, leaving his alter ego in the desert where he was created to ensure that Doctor Doom cannot use Banner's intellect.
Fictional character biography.
Victor von Doom was born decades ago to a tribe of Latverian Romani people under the rule of an unnamed nobleman called the Baron. Victor's mother was witch Cynthia Von Doom who died by Mephisto's hand while von Doom was young. His father, Werner, was the leader of the tribe and a renowned medicine man who kept his wife's sorceress life quiet in order to protect Victor from a similar fate. Soon after Cynthia's death, the Baron's wife grew incurably ill from cancer and Werner was called to the capital to heal her. When she succumbed to illness, the Baron labeled Werner a murderer and called for his death. Werner escaped with young Victor, having realized the night before the woman would die. He goes on to die of exposure on the mountainside, cradling the boy in a final embrace and giving him his garments to keep him warm. Victor survived and, on return to the Romani camp, discovered his mother's occult instruments and swore revenge on the Baron. Victor grew into a headstrong and brilliant man, combining sorcery and technology to create fantastic devices to keep the Baron's men at bay and protect the gypsies. His exploits attracted the attention of the dean of Empire State University, who sent someone to the camp. Offered the chance to study in the United States, von Doom chooses to leave his homeland and his love, Valeria, behind.
Once in the United States, Victor met fellow student and future nemesis Reed Richards, who was intended to be his roommate, but von Doom disliked him and asked for another roommate. After a time, Victor constructed a machine intended to communicate with the dead. Though Richards tried to warn him about a flaw in the machine, seeing his calculations were a few decimals off, Victor continued on with disastrous results. The machine violently failed and the resulting explosion seemingly severely damaged his face. It is later revealed that Ben Grimm, a friend of Richards who despised Victor for his superior attitude, tampered with the machine. He would later blame himself for Doctor Doom's eventual rise to power, but never revealed this information to anyone. Expelled after the accident, Victor traveled the world until he collapsed on a Tibetan mountainside. Rescued by a clan of monks, Victor quickly mastered the monks' disciplines as well as the monks themselves. Victor then forged himself a suit of armor, complete with a scowling mask, and took the name Doctor Doom. As Doctor Doom, he would go on to menace those he felt responsible for his accident—primarily, Reed Richards of the Fantastic Four. He succeeded in taking over Latveria, taking an interest in the welfare of the Roma.
In his first appearance, Doctor Doom captures the Invisible Girl, using her as a hostage so the Fantastic Four will travel back in time to steal the enchanted treasure of Blackbeard which will help him conquer the world, but he is fooled by Reed Richards, who swaps the treasure with worthless chains. Doctor Doom then forms an alliance with the Sub-Mariner, who places a magnetic device in the Baxter Building. However Doctor Doom uses this to pull him and the Fantastic Four into space, thinking this will rid him of those capable of preventing him conquering the world. But the Sub-Mariner gets to Doctor Doom 's ship and returns the Baxter Building to New York, while Doctor Doom is left on an asteroid. Returning to Earth after learning the secrets of an advanced alien race, the Ovids, Doctor Doom exchanges consciousnesses with Mr. Fantastic; Richards, inhabiting Doctor Doom 's body, switches the two back, and Doctor Doom ends up trapped in a micro-world when he is hit with a shrinking ray he had intended to use on the rest of the Fantastic Four. Doctor Doom takes over the micro-world, but leaves after the FF end his rule. He is then thrown into space when he attempts to do this to the FF. Doctor Doom is saved by Rama-Tut, and he returns to Earth to destroy the Fantastic Four by turning each member against the other using a special berry juice. Richards outwits Doctor Doom by using the hallucinogenic juice against the villain. Doctor Doom, believing he has killed Richards in a test of willpower, departs certain of his victory and superior intelligence.
During the 1960s, Doctor Doom tricks Spider-Man into joining forces with him, and he also menaces the Avengers when Quicksilver and Scarlet Witch travel to Latveria to find a long-lost relative. He steals the Silver Surfer's powers in 1967, but he loses them after breaching a barrier Galactus set for the Surfer on Earth.
During the 1970s, Doctor Doom branched out to more Marvel titles, with a battle between Doctor Doom and Prince Rudolfo over control of Latveria being featured in "Astonishing Tales". Doctor Doom also attempts to use the Hulk as his slave during two issues of "The Incredible Hulk". The character also made several appearances in the story arcs of "Super-Villain Team-Up", starting in 1975, as well as appearances in "Marvel Team-Up", beginning with issue #42 (February 1976). In August 1981, Doctor Doom also made an appearance in "Iron Man" when the two travel to Camelot where Stark thwarted Doctor Doom 's attempt to solicit the aide of Morgan le Fay and Doctor Doom swore deadly vengeance for that interference, which had to be indefinitely delayed in the interest of returning to the present day.
During John Byrne's run in the 1980s, Doctor Doom attempts to steal the cosmic powers of Terrax, but Doctor Doom's body is destroyed in the resulting fight between Terrax and the Silver Surfer. Doctor Doom survives by transferring his consciousness to another human, and is returned to his original body by the Beyonder.
In the 2000s, Doctor Doom rediscovered his mystical heritage, using his powers in an attempt to destroy the Fantastic Four after making a deal with a group of demons called the Hazarath Three, but later is consigned to a hell dimension after Professor Richards manages to trick him into rejecting the idea that the demons aided him. He escapes and attempts to claim Thor's mystical hammer Mjolnir for himself, after the rift the hammer created as it fell to Earth following the destruction of Asgard allowed him to escape. The plot fails due to his inability to lift the hammer, and Doctor Doom returns to Latveria to rule once again.
When Franklin Richards was kidnapped by Onslaught, Doctor Doom joined the Fantastic Four, Avengers and the X-Men to battle Onslaught in Central Park. An enraged Hulk was able to crack open Onslaught's shell. However, Onslaught remained as pure psionic energy, separated Hulk and Banner, planning to spread across the planet. Thor plunged into Onslaught, trying to contain him. The Fantastic Four, the majority of Avengers, and the Hulk-Less Banner followed in short order, with Doom being forced to join the sacrifice when Iron Man tackled the villain into the energy mass. Thanks to this sacrifice, the X-Men finally managed to destroy Onslaught. Doom, the Fantastic Four, and the Avengers and Banner were believed dead, but were instead saved by Franklin, who created a pocket dimension called Counter-Earth to keep them safe. After several months away, the missing heroes returned from Counter-Earth, except for Doom, who remained there for a time. Doom uncovers the secret power at the heart of the planet, an avatar of his arch-foe Reed Richards' son, Franklin, the super-powered youth who conjured this globe and left a bit of himself behind to guide it from within. Doom manages to convince the little boy to relinquish control of this world with little more than a few errant promises of a better life.
Later, a Doombot was taken down by Reed Richards, Henry Pym, Iron Man, She-Hulk and others in New York City. Whether or not it was sent by Doctor Doom himself remains to be seen, as does his role in the overall conflict. Doctor Doom was not invited to the wedding of Storm and the Black Panther. However, he did send a present: an invitation to form an alliance with Latveria, using the Civil War currently going on among the hero community as a reason to quite possibly forge an alliance between their two countries. When Black Panther, on a diplomatic mission to other countries with Storm, did show up in Latveria, he presented them with a real present, and extended another invitation to form an alliance with Black Panther. He demonstrated behavior very uncharacteristic of him, however, which may or may not become a plot point later. Panther spurned the invitation, detonating an EMP that blacked out a local portion of Latveria before Doctor Doom 's robots could destroy his ship. Later on, Doctor Doom is then shown collaborating with the Red Skull on a weapon which will only "be the beginning" of Captain America's suffering. Doctor Doom gave the Red Skull the weapon because the Red Skull gave Victor pieces of technology from an old German castle. The castle was owned by a "Baron of Iron" centuries prior, who had used his technological genius to protect himself and his people. The map the Red Skull used to find the castle bore a picture of Doctor Doom. Doctor Doom states that the technology the Red Skull gave him is more advanced than what he currently has, and that he will become the Baron of Iron in his future. The Red Skull is currently in the process of reverse-engineering Doctor Doom 's weapon for multiple uses, rather than the single use Doctor Doom agreed to.
At the end of the first chapter of the X-Men event ", Doctor Doom is among the supervillain geniuses that Beast contacts to help him reverse the effects of Decimation. He spurns Beast by stating that genetics do not number among his talents.
In ", Doctor Doom was among those that Spider-Man contacts to help save Aunt May.
Doctor Doom also makes Latveria into a refugee camp for the Atlanteans following the destruction of their underwater kingdom as well as becoming allies with Loki in his plot to manipulate Thor into unwittingly release his Asgardian enemies.
Doctor Doom later defends Latveria from the Mighty Avengers, following a revelation that it was one of Doctor Doom's satellites that carried the 'Venom Virus' released in New York City. In a battle with Iron Man and the Sentry, the time travel mechanism within his armor overloads, trapping Doctor Doom and his opponents at some point in the past. Doctor Doom continues a relationship with Morgan le Fay using his time machine. He and Iron Man managed to get back to the present, but Doctor Doom has left Iron Man in his exploding castle. Despite this, Doctor Doom ended up incarcerated at The Raft.
During the "Secret Invasion" storyline, Doctor Doom was among the villains who escaped the Raft when a virus was uploaded into its systems by the Skrulls.
In the aftermath of the Secret Invasion, Doctor Doom is a member of the Dark Illuminati alongside Norman Osborn, Emma Frost, Namor, Loki's female form, and Hood. At the end of this meeting, Namor and Doctor Doom are seen having a discussion of their own plans that have all ready been set in motion.
Doctor Doom soon allies himself with the isolationist group known as the Desturi, to take control of Wakanda. He attacked and wounded T'Challa, the current Black Panther, maiming him enough to prevent him from holding the mantle again. Doctor Doom 's main objective was to secure Wakanda's store of vibranium, which he could mystically enhance to make himself unstoppable. Doctor Doom was also a part of the supervillain group known as the Intelligencia, but was betrayed when they captured him to complete their plan. With the help of Bruce Banner, he escaped, and returned to Latveria. He appears to have been damaged by this experience.
At the start of the "Siege" storyline, Doctor Doom is with the Cabal discussing the current problems with the X-Men and both Avengers teams. Doctor Doom demands that Osborn at once reverse his course of action against his ally Namor, to which Osborn refuses, saying that he and Emma Frost had "crossed the line" with him. Doctor Doom, loathing Thor and the Asgardians all the more due to his recent defeat at their hands, claims that he will support Osborn's "madness" should Namor be returned to him, but Osborn refuses. Osborn's mysterious ally, the Void, violently attacks Doctor Doom, and an apparently amused Loki tells the Hood that he should go, as there is nothing here for either of them, which the Hood, now loyal to Loki due to his hand in the restoration of his mystical abilities, agrees. However, it is revealed that "Doctor Doom" is actually an upgraded Doctor Doom bot, which releases swarms of Doctor Doom bot nanites against the Cabal, tearing down Avengers Tower and forcing its denizens, such as the Dark Avengers, to evacuate. Osborn is rescued by the Sentry, who destroys the body. When Osborn contacts Doctor Doom, Doctor Doom tells him not to ever strike him again or he is willing to go further.
It has been revealed that the Scarlet Witch seen in Wundagore Mountain is actually a Doctor Doom bot which apparently means that the real one has been captured by Doctor Doom sometime after the House of M event. It is revealed that Wanda's enhanced powers were a result of her and Doctor Doom's combined attempt to channel the Life Force in order to resurrect her children. This proves to be too much for Wanda to contain and it overtook her. With Wiccan and Doctor Doom's help, they seek to use the entity that is possessing Wanda to restore mutantkinds' powers. This is stopped by the Young Avengers (who are concerned at the fall-out that would ensue if the powerless mutants are suddenly re-powered) only to find out Doctor Doom 's real plan: to transfer the entity into his own body and gaining Wanda's god-like powers for himself. Doctor Doom becomes omnipotent with powers surpassing those of beings as the Beyonder or the Cosmic Cube. The Young Avengers confront him, but Doctor Doom kills Cassie just before Wanda and Wiccan stole his new-found powers.
At the start of the story arc "Fantastic Four: Three," Doctor Doom felt that he needed to be "reborn" and was making plans to abdicate his throne and give it to Kristoff when Valeria teleported to his room unexpectedly asking for his assistance to help her father. Valeria quickly notices that Doctor Doom has suffered brain damage and makes a deal with him to restore his mental capacities if he helps Reed and the Fantastic Four. Doctor Doom agrees to her proposition. Later, Doctor Doom appears among those in attendance at Johnny Storm's funeral.
Due to the agreement, Doctor Doom was recommended by Nathaniel and Valeria Richards to be a member of the Future Foundation. Objecting, Thing attacks Doctor Doom out of anger, but the fight was stopped by Mister Fantastic and the Invisible Woman, who welcomes Doctor Doom to their group. When Valeria asks Doctor Doom if he has a backup for restoring his memories, he reveals that Kristoff Vernard is his backup. Afterward, Mr. Fantastic, Spider-Man, Nathaniel, Valeria, and Doctor Doom head to Latveria to meet with Kristoff and request his help. Mister Fantastic sets up a brain transfer machine in order to help restore Doctor Doom's memories and knowledge, which is successful. When Kristoff wants to return the throne to him, Doctor Doom states that it is not time yet because of a promise he made to Valeria. When Mister Fantastic asks what promise Doctor Doom made to Valeria, Doctor Doom states that he made a promise to help defeat Mister Fantastic. Doctor Doom decides to hold a symposium on how to finally defeat Reed Richards. The Thing and the evolved Moloids give an invitation to the High Evolutionary. Dragon Man and Alex Power give an invitation to Diablo. Upon receiving an invitation from Spider-Man, Mad Thinker is convinced to take part in the event. Bentley 23 even gives an invitation to his creator, the Wizard, along with two A.I.M. lieutenants. However, it is subsequently revealed that the 'Richards' they have been invited to defeat are actually members of the "Council of Reeds" (alternate versions of Reed who were trapped in this universe by Valeria a while back, possessing Reed's intellect while lacking his conscience). While Spider-Man and Invisible Woman make sandwiches for the kids, Mister Fantastic, Doctor Doom, Valeria, and Nathaniel Richards meet with the supervillain geniuses and Uatu the Watcher about what to do with the Council of Reeds.
The child members of the Future Foundation used the panic room system to teleport themselves the top of the Baxter Building to near Latveria where they help Nathaniel Richards, Kristoff Vernard, Alpha-Reed Richards, and Doctor Doom to rebuild the Bridge, and the Alpha-Reed Richards could return home. The Mad Celestials from Earth-4280 try to enter Earth through the Bridge in order to destroy it. Doctor Doom and Alpha-Reed Richards tried to stop them although Alpha-Reed Richards was killed using the Ultimate Nullifier while Doctor Doom was apparently killed by the Mad Celestials.
With no knowledge as to how he survived the blast from the Mad Celestials, Doom woke up in the middle of the ruins of the Interdimensional Council of Reeds, where Valeria had left him a present: the full army of lobotomized Doctor Dooms from alternate realities who were previously captured by the Council, along with two Infinity Gauntlets from alternate universes. With these resources, Doom created the Parliament of Doom. He later returned to again rule Latveria, and was targeted by Lucia von Bardas and the Red Ghost, who wanted to get revenge on Doom for past discretions.
Doom later journeyed to the Universe that one of his Infinity Gauntlets had belonged to, which was now empty and desolate, and used the gauntlet to create it anew. He separated magic and science, creating the basic rules for their existence, created new life, made himself its ruler. On a world where science and magic were wed, his creations turned on him and six rulers divided Doom's infinity gems between them. Reed and Nathaniel Richards entered this Universe to save Doom after being prompted by Valeria that he was in grave danger. They managed to convince five of the rulers to pardon Doom and managed to escape the clutches of the sixth, bringing Doom back to their universe. Upon their return, Doom declared that he and Richards were again even.
When Latveria became the site of an Incursion, a collision between Earth and one of its alternate universe counterparts, this Incursion was revealed to be different however, in that it was controlled by a mysterious group known as The Mapmakers, who had rigged the other Earth to explode and wished to mark Doom's Earth for potential expansion. Doom fought off the Mapmakers with the help of his adopted son Kristoff Vernard, whilst unbeknownst to him, the Illuminati blew up the other Earth. After the Incursion ended, Doom was alerted to a rock that had fallen from the sky, which was in truth the Mapmaker's beacon. Doom contacted Reed Richards and Stephen Strange in order to confront them about the Illuminati's presence in Latveria and the incursion, but to his fury, they refused to give him answers and Reed warned him not to contact him in regard to the Incursions again.
During the "AXIS" storyline, Doctor Doom appears as a member of Magneto's unnamed supervillain group during the fight against Red Skull's Red Onslaught form. Doctor Doom works with Scarlet Witch in order to use a spell to awaken the dormant part of Professor X's brain within Red Onslaught. The spell also caused some inversions which made the Avengers and X-Men evil and the bad guys good. In order to combat the now-evil Avengers and X-Men, Doctor Doom forms his team of Avengers by recruiting 3D Man, Elsa Bloodstone, Stingray, Valkyrie, and U.S. Agent. Their first mission is to fight a now-evil Scarlet Witch when she invades Latveria. It is later revealed that he intends to use Scarlet's attack for his goal to atone for his sins, by absorbing during the fight her reality-altering powers with which he can undo all his crimes; being forced to choose only one act to set straight, as he obtains only a fraction of the power, and despite being tempted to use it to repair his face and resurrect his beloved mother, Doctor Doom elects to revive Cassie Lang. He subsequently makes a Faustian deal with an unspecified demon to resurrect Brother Voodoo to take control of the Scarlet Witch and undo the inversion. Having returned to normal, Doctor Doom is shown with the Red Skull in captivity, contained by various telepathy-blocking machines, although his long-term goal is unknown.
Over the following months after the Incursion in Latveria, Doctor Doom had been working with a team of scientists to reverse-engineer one of the pieces of a Mapmaker he gathered from the Incursion, and successfully mapped their entire network. Doctor Doom planned to use the Molecule Man to oppose whatever was the origin of the Incursions.
When Doctor Strange and the Black Priests began searching for Rabum Alal, the man whose birth was supposedly one of the main causes for the collapse and the decay of the Multiverse, the Black Priest finds a door to the Library of Worlds but unlike before, they now have a key to get in and Doctor Strange to guide them through the Library where they are ambushed by the Black Swans in a vacuum where their words cannot be used, but Doctor Strange uses his other magics to fight back but are defeated nevertheless. He is taken to Rabum Alal who is revealed to be Doctor Doom himself. He then reveals to Doctor Strange that as Rabum Alal he had gathered a number Black Swans who revere him as a mythic figure. It was also revealed that Doctor Doom has been opposing the Beyonders with the Molecule Man, by his side.
Powers and abilities.
Doctor Doom is a polymath and scientific genius. Throughout most of his publication history, he has been depicted as one of the most intelligent humans in the Marvel Universe. This is shown on many occasions, most famously by actually curing Ben Grimm of his Thing form, which Reed Richards has never repeated. On the other hand, Reed Richards managed to process all the computer calculations necessary to save the life of a disintegrating Kitty Pryde by himself, which is a feat that Doctor Doom at the time professed to be unable to do.
Doctor Doom also possesses originally minor mystical capabilities due to teachings from Tibetan monks, but later increased them to a considerable extent due to tutoring from his lover Morgan Le Fay. He is capable of energy projection, creating protective shields, and summoning hordes of demonic creatures. Even at a time his abilities were consistently referred to as minor, with assistance from his technology and by tag-teaming with Doctor Strange, Doctor Doom managed to come second in a magic tournament held by the ancient sorcerer the Aged Genghis.
Doctor Doom has also used his scientific talents to steal or replicate the power of other beings such as the Silver Surfer, or in one case the entity Galactus' world-ship.
The alien Ovoids taught Doctor Doom the process of psionically transferring his consciousness into another nearby being through a simple eye contact, as well as showing him other forms of technology which Doctor Doom uses to escape from incarcerations and to avoid getting killed; however, if his concentration is broken, it can transfer his mind back, and he rarely uses this power unless absolutely necessary due to his own ego about his appearance.
Doctor Doom can exert technopathic control over certain machines, most notably the Doom bots. In addition, Doctor Doom has a remarkably strong will, as demonstrated in the graphic novel, "Emperor Doom " when he dared his prisoner, the mind controlling Purple Man, to attempt to control him and he successfully resists.
Without his armor he proved himself to be a skilled bare-handed fighter, even capable of killing a lion.
Doctor Doom’s armor augments his natural physical strength to superhuman levels, to the point where he is able to hold his own against Spider-Man in hand-to-hand combat, although he tends to rely on long-range tactics when engaging physically stronger foes. It is also highly resistant to harm, sufficient to withstand blows from Iron Man’s armor. The armor can generate a defensive force field and a lethal electric shock killing anyone who might come in contact with Doctor Doom. The armor is self-supporting, equipped with internal stores and recycling systems for air, food, water, and energy, allowing the wearer to survive lengthy periods of exposure underwater or in outer space.
As the absolute monarch of Latveria, Dr. Doom has diplomatic immunity – allowing him to escape prosecution for most of his crimes – and total control of the nation’s natural and technological resources, along with its manpower, economy, and military.
Doctor Doom is known for the frequent plot device wherein it is revealed that his actions were actually those of a "Doombot", one of Doctor Doom’s many robot doubles, either working on his behalf or as a result of rogue artificial intelligence.
On many occasions, Doctor Doom’s only real weakness has been his arrogance. Layla Miller once reflecting that Doctor Doom is incapable of accepting that he himself might be the reason for his failures. This is most keenly reflected in Doctor Doom’s continued refusal to accept responsibility for the accident that scarred his face, instead preferring to blame Reed Richards for sabotaging his experiment. While his high opinion of himself is generally accurate, he is generally unable to accept when others may have a better understanding of a situation than he does – with the occasional exception of hearing out the recommendations of heroes such as Mr. Fantastic or the Thing when it is to his advantage. Even when teaming up with others against a greater threat, Doctor Doom will often try to subvert the alliance for personal gain. For instance, while allied with Adam Warlock and other heroes against the titan Thanos, he attempted to steal Thanos’ Infinity Gauntlet before its owner had been defeated.
Doctor Doom adheres to a strict code of honor at all times. However, Von Doom will keep his "exact" word, which may or may not be beneficial to the person to whom he has given his promise. For example, Doctor Doom may swear that he will not harm an individual, but that only means he will not personally harm that person, it does not mean he will prevent others from harming that person.
Doctor Doom’s honor code led him to save Captain America from drowning because Captain America had earlier saved his life, and on another occasion he thanked Spider-Man for saving him from terrorists attacking him in an airport by allowing him to leave alive despite Spider-Man subsequently insulting him. His code of honor also means that he will not attack a respected opponent who is weakened or at a severe disadvantage, as he regards any victory resulting from such circumstances as hollow and meaningless. He has even on several occasions battled opponents who were intent on killing the Fantastic Four, for no other reason than the fact that he does not want the ultimate defeat of the Fantastic Four to come from anyone’s hands but his own.
Doctor Doom is shown to be devoted to the welfare and well being of his subjects.
Inventions.
Doctor Doom has constructed numerous devices in order to defeat his foes or gain more power including:
Other versions.
Doctor Doom's status as one of the Fantastic Four's greatest villains has led to his appearance in many of Marvel's alternate universes and spinoffs, in which the character's history, circumstances and behavior vary from the mainstream setting.
In other media.
Doctor Doom has been included in almost every media adaptation of the "Fantastic Four" franchise, including film, television, and computer and video games.
Cultural impact.
In the book "Superhero: The Secret Origin of a Genre", Peter Coogan writes that Doctor Doom's appearance was representative of a change in the portrayal of "mad scientists" to full-fledged villains, often with upgraded powers. Doctor Doom is also emblematic of a specific "subset" of supervillain, which comic book critic Peter Sanderson describes as a "megavillain". These supervillains are genre-crossing villains who exist in adventures "in a world in which the ordinary laws of nature are slightly suspended"; characters such as Professor Moriarty, Count Dracula, Auric Goldfinger, Hannibal Lecter, Lex Luthor, and Darth Vader, also fit this description. Sanderson also found traces of William Shakespeare’s characters Richard III and Iago in Doctor Doom ; all of them "are descended from the 'vice' figure of medieval drama", who address the audience in monologues detailing their thoughts and ambitions.
Described as "iconic", Doctor Doom is one of the most well-received supervillains of the Marvel universe, as well as one of the most recurring; in his constant battles with heroes and other villains, Doctor Doom has appeared more times than any other villain. The comics site Panels of Awesome ranked Doctor Doom as the number one villain in their listing of the top ten villains in comics; "Wizard Magazine" went a step further by declaring Doctor Doom the fourth greatest villain of all time.
Comic Book Resources ranks Doctor Doom as their fourth favorite Marvel character. Journalist Brent Ecenbarger cited him being able to "stand up against entities like Mephisto, the Beyonder, and Galactus and often come out on top", as well as the tragedy of any "other number of circumstances could have led to Doctor Doom being a savior, but as it is, instead he remains Marvel’s greatest villain." Fellow journalist Jason Stanhope called his "master[ing] of sorcery and technology an unusual combination", and also felt "his inner sense of nobility sets him apart from lesser villains, in a similar manner to Magneto." Doctor Doom has also been favorably regarded by those who wrote for the character; Stan Lee declared Doctor Doom his favorite villain, saying that Doom "could come to the United States and he could do almost anything, and we could not arrest him because he has diplomatic immunity. Also, he wants to rule the world and if you think about it, wanting to rule the world is not a crime." Mark Waid echoed Lee's assessment of the character, stating that Doom "[has] got a great look, a great visual design [and] a dynamite origin."
A ride called "Doctor Doom's Fearfall" is located at Islands of Adventure in the Universal Orlando Resort.

</doc>
<doc id="47498" url="http://en.wikipedia.org/wiki?curid=47498" title="All your base are belong to us">
All your base are belong to us

"All your base are belong to us" is a broken English phrase found in the opening cutscene of the 1991 video game "Zero Wing" which became a popular Internet meme. The quote is included in the European version of the game, which features poorly translated English from the original Japanese version.
The meme developed from this as the result of a GIF animation depicting the opening text which was initially popularized on the Something Awful message forums.
Mentions in media.
The phrase or some variation of lines from the game has appeared in numerous articles, books, comics, clothing, movies, radio shows, songs, television shows, video games, webcomics, and websites.
In late 2000, Kansas City computer programmer and part-time DJ Jeffrey Ray Roberts of the Gabber band The Laziest Men on Mars made a techno dance track, "Invasion of the Gabber Robots", which remixed some of the "Zero Wing" video game music by Tatsuya Uemura with a voice-over phrase "All your base are belong to us."
On February 23, 2001, "Wired" provided an early report on the phenomenon, covering it from the Flash animation to its spread through e-mail and Internet forums to T-shirts bearing the phrase.
On April 1, 2003, in Sturgis, Michigan, seven people aged 17 to 20 placed signs all over town that read: "All your base are belong to us. You have no chance to survive make your time." They claimed to be playing an April Fool's joke but most people who saw the signs were unfamiliar with the phrase. Many residents were upset that the signs appeared while the U.S. was at war with Iraq and police chief Eugene Alli said the signs could be "a borderline terrorist threat depending on what someone interprets it to mean."
In February 2004, North Carolina State University students and members of TheWolfWeb in Raleigh, North Carolina exploited a web-based service provided for local schools and businesses to report a weather-related closing to display the phrase within a news ticker on a live news broadcast on News 14 Carolina.
On June 1, 2006, YouTube, the video-hosting website, was taken down temporarily for maintenance. The phrase "ALL YOUR VIDEO ARE BELONG TO US" appeared below the YouTube logo as a placeholder while the site was down. Some users believed the site had been hacked, leading YouTube to add the message "No, we haven't be [sic] hacked. Get a sense of humor."
On June 12, 2014, Tesla Motors CEO, Elon Musk announced that the company was releasing all its patented material under the Open Source licenses. The title of his blog post was "All Our Patent Are Belong To You" which was wordplay on the term used in the game here.
External links.
Listen to this article ()
This audio file was created from a revision of the "All your base are belong to us" article dated 2010-10-22, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="47499" url="http://en.wikipedia.org/wiki?curid=47499" title="Boreal">
Boreal

Boreal or boreale may refer to:

</doc>
<doc id="47500" url="http://en.wikipedia.org/wiki?curid=47500" title="Northern">
Northern

Northern may refer to the following:

</doc>
<doc id="47501" url="http://en.wikipedia.org/wiki?curid=47501" title="Brightness temperature">
Brightness temperature

Brightness temperature is the temperature a black body in thermal equilibrium with its surroundings would have to be to duplicate the observed intensity of a grey body object at a frequency formula_1.
This concept is extensively used in radio astronomy and planetary science.
For a black body, Planck's law gives:
where
formula_3 (the Intensity or Brightness) is the amount of energy emitted per unit surface area per unit time per unit solid angle and in the frequency range between formula_1 and formula_5; formula_6 is the temperature of the black body; formula_7 is Planck's constant; formula_1 is frequency; formula_9 is the speed of light; and formula_10 is Boltzmann's constant.
For a grey body the spectral radiance is a portion of the black body radiance, determined by the emissivity formula_11.
That makes the reciprocal of the brightness temperature:
At low frequency and high temperatures, when formula_13, we can use the Rayleigh–Jeans law:
so that the brightness temperature can be simply written as:
In general, the brightness temperature is a function of formula_1, and only in the case of blackbody radiation it is the same at all frequencies. The brightness temperature can be used to calculate the spectral index of a body, in the case of non-thermal radiation.
Calculating by frequency.
The brightness temperature of a source with known spectral radiance can be expressed as:
When formula_13 we can use the Rayleigh–Jeans law:
For narrowband radiation with very low relative spectral linewidth formula_20 and known radiance formula_21 we can calculate the brightness temperature as:
Calculating by wavelength.
Spectral radiance of black-body radiation is expressed by wavelength as:
So, the brightness temperature can be calculated as:
For long-wave radiation formula_25 the brightness temperature is:
For almost monochromatic radiation, the brightness temperature can be expressed by the radiance formula_21 and the coherence length formula_28:
It should be noted that the brightness temperature is not a temperature as ordinarily understood. It characterizes radiation, and depending on the mechanism of radiation can differ considerably from the physical temperature of a radiating body (though it is theoretically possible to construct a device which will heat up by a source of radiation with some brightness temperature to the actual temperature equal to brightness temperature). Nonthermal sources can have very high brightness temperatures. In pulsars the brightness temperature can reach 1026 K. For the radiation of a typical helium–neon laser with a power of 60 mW and a coherence length of 20 cm, focused in a spot with a diameter of 10 µm, the brightness temperature will be nearly .

</doc>
<doc id="47502" url="http://en.wikipedia.org/wiki?curid=47502" title="Calibration">
Calibration

Calibration is a comparison between measurements – one of known magnitude or correctness made or set with one device and another measurement made in as similar a way as possible with a second device.
The device with the known or assigned correctness is called the standard. The second device is the unit under test, test instrument, or any of several other names for the device being calibrated.
The formal definition of calibration by the International Bureau of Weights and Measures is the following: "Operation that, under specified conditions, in a first step, establishes a relation between the quantity values with measurement uncertainties provided by measurement standards and corresponding indications with associated measurement uncertainties (of the calibrated instrument or secondary standard) and, in a second step, uses this information to establish a relation for obtaining a measurement result from an indication."
History.
Origins.
The words "calibrate" and "calibration" entered the English language as recent as the American Civil War, in descriptions of artillery. Some of the earliest known systems of measurement and calibration seem to have been created between the ancient civilizations of Egypt, Mesopotamia and the Indus Valley, with excavations revealing the use of angular gradations for construction. The term "calibration" was likely first associated with the precise division of linear distance and angles using a dividing engine and the measurement of gravitational mass using a weighing scale. These two forms of measurement alone and their direct derivatives supported nearly all commerce and technology development from the earliest civilizations until about AD 1800.
Calibration of weights and distances (c. 1100 CE–).
Early measurement devices were "direct", i.e. they had the same units as the quantity being measured. Examples include length using a yardstick and mass using a weighing scale. At the beginning of the twelfth century, during the reign of Henry I (1100-1135), it was decreed that a yard be "the distance from the tip of the King's nose to the end of his outstretched thumb." However, it wasn't until the reign of Richard I (1197) that we find documented evidence.
Other standardization attempts followed, such as the Magna Carta (1225) for liquid measures, until the Mètre des Archives from France and the establishment of the Metric system.
The Industrial Revolution and the calibration of pressure (c. 1600 CE–).
One of the earliest pressure measurement devices was the Mercury barometer, credited to Torricelli (1643), which read atmospheric pressure using Mercury. Soon after, hydrostatic manometers were designed, with a linear calibration for measuring lower pressures ranges. The Industrial Revolution (c. 1760 CE – c. 1840 CE) saw widespread use of "indirect" measuring devices, in which the quantity being measured was derived functionally based on direct measurements of dependent quantities. During this time, scientists discovered the energy stored in compressed steam and other gases, leading to the development of gauges more practical than hydrostatic manometers at measuring higher pressures. One such invention was Eugene Bourdon's indirect design Bourdon tube.
In the direct reading hydrostatic manometer design on the left, an unknown applied pressure Pa pushes the liquid down the right side of the manometer U-tube, while a length scale next to the tube measures the pressure, referenced to the other, open end of the manometer on the left side of the U-tube (P0). The resulting height difference "H" is a direct measurement of the pressure or vacuum with respect to atmospheric pressure. The absence of pressure or vacuum would make H=0. The self-applied calibration would only require the length scale to be set to zero at that same point.
This direct measurement of pressure as a height difference depends on both the density of the manometer fluid, and a calibrated means of measuring the height difference.
In a Bourdon tube (shown in the two views on the right), applied pressure entering from the bottom on the silver barbed pipe tries to straighten a curved tube (or vacuum tries to curl the tube to a greater extent), moving the free end of the tube that is mechanically connected to the pointer. This is indirect measurement that depends on calibration to read pressure or vacuum correctly. No self-calibration is possible, but generally the zero pressure state is correctable by the user, as shown below.
Even in recent times, direct measurement is used to increase confidence in the validity of the measurements.
The age of the automobiles (c. 1800 CE–).
In the early days of US automobile use, people wanted to see the gasoline they were about to buy in a big glass pitcher, a direct measure of volume and quality via appearance. By 1930, rotary flowmeters were accepted as indirect substitutes. A hemispheric viewing window allowed consumers to see the blade of the flowmeter turn as the gasoline was pumped (see image on the right). By 1970, the windows were gone and the measurement was totally indirect.
Indirect measurement always involve linkages or conversions of some kind. It is seldom possible to intuitively monitor the measurement. These facts intensify the need for calibration.
Most measurement techniques used today are indirect.
Modern Calibration.
Calibration methods can be both manual and automatic, depending on what kind of device is being calibrated. The picture on the left shows a U.S. Navy Airman performing a manual calibration procedure on a pressure test gauge. The procedure is complex, but overall it involves the following: (i) depressurizing the system, and turning the screw, if necessary, to ensure that the needle reads zero, (ii) fully pressurizing the system and ensuring that the needle reads maximum, within acceptable tolerances, (iii) replacing the gauge if the error in the calibration process is beyond tolerance, as this may indicate signs of failure such as corrosion or material fatigue.
In contrast, the picture on the right shows the use of a 3666C automatic pressure calibrator, which is a device that consists of a control unit housing the electronics that drive the system, a pressure intensifier used to compress a gas such as Nitrogen, a pressure transducer used to detect desired levels in a hydraulic accumulator, and accessories such as liquid traps and gauge fittings.
Modern Calibration
Simply,Calibration means standardization with respect to master equipment,the only condition is master device must have higher accuracy and precision than the test instrument.According to the nature of industrial instrumentation calibration, different types are there,mainly,
1. Electro Technical
2.Mechanical
3.Thermal
1.ELECTRO TECHNICAL
In Electro Technical calibration,its mainly deals with the calibration of electronic and electrical instruments,such as Temperature Controllers,Pressure transmitters,Limit switches etc.So in electro technical calibration,mainly we have to simulate electrical signal or measure the electrical signal of test instrument with respect to a master instrument.
example 1;Temperature Controller
For the calibration of the temperature controller the standard input values are feed from the calibrator and corresponding test instrument reading are taken.
2.MECHANICAL
As name indicates Mechanical Calibration always deals with instruments which have purely mechanical parts or semi electrical-mechanical parts. Examples are Pressure Gauges,Pressure Switches etc.
For the calibration mechanical instruments we need to apply the appropriate standard mechanical input from a master instrument.Example,A Pressure gauge is calibrated by using Dead weight tester or by using a standard pressure source.
3. THERMAL 
In this category,the instruments which senses directly thermal parameters in a process line.Examples are Temperature Gauges,Temperature Controller with sensor,Temperature elements etc
So for the calibration,we have to provide standard heat sources to the test instruments for master temperature sources.Example ,A temperature gauge is calibrated by using a Dry Block Temperature Tester,which can create temperature are compare it with the test instrument,here the test instrument is temperature gauge.
Basic calibration process.
Purpose and scope.
The calibration process begins with the design of the measuring instrument that needs to be calibrated. The design has to be able to "hold a calibration" through its calibration interval. In other words, the design has to be capable of measurements that are "within engineering tolerance" when used within the stated environmental conditions over some reasonable period of time. Having a design with these characteristics increases the likelihood of the actual measuring instruments performing as expected.
Basically,the purpose of calibration is for maintaining the quality of measurement as well as to ensure the proper working of particular instrument.
Frequency.
The exact mechanism for assigning tolerance values varies by country and industry type. The measuring equipment manufacturer generally assigns the measurement tolerance, suggests a calibration interval (CI) and specifies the environmental range of use and storage. The using organization generally assigns the actual calibration interval, which is dependent on this specific measuring equipment's likely usage level. The assignment of calibration intervals can be a formal process based on the results of previous calibrations. The standards themselves are not clear on recommended CI values:
Standards required and accuracy.
The next step is defining the calibration process. The selection of a standard or standards is the most visible part of the calibration process. Ideally, the standard has less than 1/4 of the measurement uncertainty of the device being calibrated. When this goal is met, the accumulated measurement uncertainty of all of the standards involved is considered to be insignificant when the final measurement is also made with the 4:1 ratio. This ratio was probably first formalized in Handbook 52 that accompanied MIL-STD-45662A, an early US Department of Defense metrology program specification. It was 10:1 from its inception in the 1950s until the 1970s, when advancing technology made 10:1 impossible for most electronic measurements.
Maintaining a 4:1 accuracy ratio with modern equipment is difficult. The test equipment being calibrated can be just as accurate as the working standard. If the accuracy ratio is less than 4:1, then the calibration tolerance can be reduced to compensate. When 1:1 is reached, only an exact match between the standard and the device being calibrated is a completely correct calibration. Another common method for dealing with this capability mismatch is to reduce the accuracy of the device being calibrated.
For example, a gage with 3% manufacturer-stated accuracy can be changed to 4% so that a 1% accuracy standard can be used at 4:1. If the gage is used in an application requiring 16% accuracy, having the gage accuracy reduced to 4% will not affect the accuracy of the final measurements. This is called a limited calibration. But if the final measurement requires 10% accuracy, then the 3% gage never can be better than 3.3:1. Then perhaps adjusting the calibration tolerance for the gage would be a better solution. If the calibration is performed at 100 units, the 1% standard would actually be anywhere between 99 and 101 units. The acceptable values of calibrations where the test equipment is at the 4:1 ratio would be 96 to 104 units, inclusive. Changing the acceptable range to 97 to 103 units would remove the potential contribution of all of the standards and preserve a 3.3:1 ratio. Continuing, a further change to the acceptable range to 98 to 102 restores more than a 4:1 final ratio.
This is a simplified example. The mathematics of the example can be challenged. It is important that whatever thinking guided this process in an actual calibration be recorded and accessible. Informality contributes to tolerance stacks and other difficult to diagnose post calibration problems.
Also in the example above, ideally the calibration value of 100 units would be the best point in the gage's range to perform a single-point calibration. It may be the manufacturer's recommendation or it may be the way similar devices are already being calibrated. Multiple point calibrations are also used. Depending on the device, a zero unit state, the absence of the phenomenon being measured, may also be a calibration point. Or zero may be resettable by the user-there are several variations possible. Again, the points to use during calibration should be recorded.
There may be specific connection techniques between the standard and the device being calibrated that may influence the calibration. For example, in electronic calibrations involving analog phenomena, the impedance of the cable connections can directly influence the result.
Process description and documentation.
All of the information above is collected in a calibration procedure, which is a specific test method. These procedures capture all of the steps needed to perform a successful calibration. The manufacturer may provide one or the organization may prepare one that also captures all of the organization's other requirements. There are clearinghouses for calibration procedures such as the Government-Industry Data Exchange Program (GIDEP) in the United States.
This exact process is repeated for each of the standards used until transfer standards, certified reference materials and/or natural physical constants, the measurement standards with the least uncertainty in the laboratory, are reached. This establishes the traceability of the calibration.
See Metrology for other factors that are considered during calibration process development.
After all of this, individual instruments of the specific type discussed above can finally be calibrated. The process generally begins with a basic damage check. Some organizations such as nuclear power plants collect "as-found" calibration data before any routine maintenance is performed. After routine maintenance and deficiencies detected during calibration are addressed, an "as-left" calibration is performed.
More commonly, a calibration technician is entrusted with the entire process and signs the calibration certificate, which documents the completion of a successful calibration.
Success factors.
The basic process outlined above is a difficult and expensive challenge. The cost for ordinary equipment support is generally about 10% of the original purchase price on a yearly basis, as a commonly accepted rule-of-thumb. Exotic devices such as scanning electron microscopes, gas chromatograph systems and laser interferometer devices can be even more costly to maintain.
The extent of the calibration program exposes the core beliefs of the organization involved. The integrity of organization-wide calibration is easily compromised. Once this happens, the links between scientific theory, engineering practice and mass production that measurement provides can be missing from the start on new work or eventually lost on old work.
The 'single measurement' device used in the basic calibration process description above does exist. But, depending on the organization, the majority of the devices that need calibration can have several ranges and many functionalities in a single instrument. A good example is a common modern oscilloscope. There easily could be 200,000 combinations of settings to completely calibrate and limitations on how much of an all inclusive calibration can be automated.
Every organization using oscilloscopes has a wide variety of calibration approaches open to them. If a quality assurance program is in force, customers and program compliance efforts can also directly influence the calibration approach. Most oscilloscopes are capital assets that increase the value of the organization, in addition to the value of the measurements they make. The individual oscilloscopes are subject to depreciation for tax purposes over 3, 5, 10 years or some other period in countries with complex tax codes. The tax treatment of maintenance activity on those assets can bias calibration decisions.
New oscilloscopes are supported by their manufacturers for at least five years, in general. The manufacturers can provide calibration services directly or through agents entrusted with the details of the calibration and adjustment processes.
Very few organizations have only one oscilloscope. Generally, they are either absent or present in large groups. Older devices can be reserved for less demanding uses and get a limited calibration or no calibration at all. In production applications, oscilloscopes can be put in racks used only for one specific purpose. The calibration of that specific scope only has to address that purpose.
This whole process in repeated for each of the basic instrument types present in the organization, such as the digital multimeter pictured below.
Also the picture above shows the extent of the integration between Quality Assurance and calibration. The small horizontal unbroken paper seals connecting each instrument to the rack prove that the instrument has not been removed since it was last calibrated. These seals are also used to prevent undetected access to the adjustments of the instrument. There also are labels showing the date of the last calibration and when the calibration interval dictates when the next one is needed. Some organizations also assign unique identification to each instrument to standardize the record keeping and keep track of accessories that are integral to a specific calibration condition.
When the instruments being calibrated are integrated with computers, the integrated computer programs and any calibration corrections are also under control.
Quality.
To improve the quality of the calibration and have the results accepted by outside organizations it is desirable for the calibration and subsequent measurements to be "traceable" to the internationally defined measurement units. Establishing traceability is accomplished by a formal comparison to a standard which is directly or indirectly related to national standards ( such as NIST in the USA), international standards, or certified reference materials. This may be done by national standards laboratories operated by the government or by private firms offering metrology services.
Quality management systems call for an effective metrology system which includes formal, periodic, and documented calibration of all measuring instruments. ISO 9000 and ISO 17025 standards require that these traceable actions are to a high level and set out how they can be quantified.
Instrument calibration.
Calibration may be called for:
In general use, calibration is often regarded as including the process of adjusting the output or indication on a measurement instrument to agree with value of the applied standard, within a specified accuracy. For example, a thermometer could be calibrated so the error of indication or the correction is determined, and adjusted (e.g. via calibration constants) so that it shows the true temperature in Celsius at specific points on the scale. This is the perception of the instrument's end-user. However, very few instruments can be adjusted to exactly match the standards they are compared to. For the vast majority of calibrations, the calibration process is actually the comparison of an unknown to a known and recording the results.
International.
In many countries a National Metrology Institute (NMI) will exist which will maintain primary standards of measurement (the main SI units plus a number of derived units) which will be used to provide traceability to customer's instruments by calibration. The NMI supports the metrological infrastructure in that country (and often others) by establishing an unbroken chain, from the top level of standards to an instrument used for measurement. Examples of National Metrology Institutes are NPL in the UK, NIST in the United States, PTB in Germany and many others. Since the Mutual Recognition Agreement was signed it is now straightforward to take traceability from any participating NMI and it is no longer necessary for a company to obtain traceability for measurements from the NMI of the country in which it is situated.
To communicate the quality of a calibration the calibration value is often accompanied by a traceable uncertainty statement to a stated confidence level. This is evaluated through careful uncertainty analysis.
Some times a DFS (Departure From Spec) is required to operate machinery in a degraded state. Whenever this does happen, it must be in writing and authorized by a manager with the technical assistance of a calibration technician.
References.
Crouch, Stanley & Skoog, Douglas A. (2007). Principles of Instrumental Analysis. Pacific Grove: Brooks Cole. ISBN 0-495-01201-7.
IS:ISO:ISI:17025:2005

</doc>
<doc id="47503" url="http://en.wikipedia.org/wiki?curid=47503" title="Carbon cycle">
Carbon cycle

The carbon cycle is the biogeochemical cycle by which carbon is exchanged among the biosphere, pedosphere, geosphere, hydrosphere, and atmosphere of the Earth. Along with the nitrogen cycle and the water cycle, the carbon cycle comprises a sequence of events that are key to making the Earth capable of sustaining life; it describes the movement of carbon as it is recycled and reused throughout the biosphere.
The "global carbon budget" is the balance of the exchanges (incomes and losses) of carbon between the carbon reservoirs or between one specific loop (e.g., atmosphere <-> biosphere) of the carbon cycle. An examination of the carbon budget of a pool or reservoir can provide information about whether the pool or reservoir is functioning as a source or sink for carbon dioxide.
The carbon cycle was initially discovered by Joseph Priestley and Antoine Lavoisier, and popularized by Humphry Davy.
Relevance for the global climate.
Carbon-based molecules are crucial for life on earth, because it is the main component of biological compounds. Carbon is also a major component of many minerals. Carbon also exists in various forms in the atmosphere. Carbon dioxide (CO2) is partly responsible for the greenhouse effect and is the most important human-contributed greenhouse gas.
In the past two centuries, human activities have seriously altered the global carbon cycle, most significantly in the atmosphere. Although carbon dioxide levels have changed naturally over the past several thousand years, human emissions of carbon dioxide into the atmosphere exceed natural fluctuations. Changes in the amount of atmospheric CO2 are considerably altering weather patterns and indirectly influencing oceanic chemistry. Records from ice cores have shown that, although global temperatures can change without changes in atmospheric CO2 levels, CO2 levels cannot change significantly without affecting global temperatures. Current carbon dioxide levels in the atmosphere exceed measurements from the last 420,000 years and levels are rising faster than ever recorded, making it of critical importance to better understand how the carbon cycle works and what its effects are on the global climate.
Main components.
The global carbon cycle is now usually divided into the following major reservoirs of carbon interconnected by pathways of exchange:
The carbon exchanges between reservoirs occur as the result of various chemical, physical, geological, and biological processes. The ocean contains the largest active pool of carbon near the surface of the Earth.
The natural flows of carbon between the atmosphere, ocean, and sediments is fairly balanced, so that carbon levels would be roughly stable without human influence.
Atmosphere.
Carbon in the earth's atmosphere exists in two main forms: carbon dioxide and methane. Both of these gases absorb and retain heat in the atmosphere and are partially responsible for the greenhouse effect. Methane produces a large greenhouse effect per volume as compared to carbon dioxide, but it exists in much lower concentrations and is more short-lived than carbon dioxide, making carbon dioxide the more important greenhouse gas of the two.
Carbon dioxide leaves the atmosphere through photosynthesis, thus entering the terrestrial and oceanic biospheres. Carbon dioxide also dissolves directly from the atmosphere into bodies of water (oceans, lakes, etc.), as well as dissolving in precipitation as raindrops fall through the atmosphere. When dissolved in water, carbon dioxide reacts with water molecules and forms carbonic acid, which contributes to ocean acidity. It can then be absorbed by rocks through weathering. It also can acidify other surfaces it touches or be washed into the ocean.
Human activity over the past two centuries has significantly increased the amount of carbon in the atmosphere, mainly in the form of carbon dioxide, both by modifying ecosystems' ability to extract carbon dioxide from the atmosphere and by emitting it directly, e.g. by burning fossil fuels and manufacturing concrete.
Terrestrial biosphere.
The terrestrial biosphere includes the organic carbon in all land-living organisms, both alive and dead, as well as carbon stored in soils. About 500 gigatons of carbon are stored above ground in plants and other living organisms, while soil holds approximately 1,500 gigatons of carbon. Most carbon in the terrestrial biosphere is organic carbon, while about a third of soil carbon is stored in inorganic forms, such as calcium carbonate. Organic carbon is a major component of all organisms living on earth. Autotrophs extract it from the air in the form of carbon dioxide, converting it into organic carbon, while heterotrophs receive carbon by consuming other organisms.
Because carbon uptake in the terrestrial biosphere is dependent on biotic factors, it follows a diurnal and seasonal cycle. In CO2 measurements, this cycle is often called a Keeling curve. It is strongest in the northern hemisphere, because this hemisphere has more land mass than the southern hemisphere and thus more room for ecosystems to absorb and emit carbon.
Carbon leaves the terrestrial biosphere in several ways and on different time scales. The combustion or respiration of organic carbon releases it rapidly into the atmosphere. It can also be exported into the oceans through rivers or remain sequestered in soils in the form of inert carbon. Carbon stored in soil can remain there for up to thousands of years before being washed into rivers by erosion or released into the atmosphere through soil respiration. Between 1989 and 2008 soil respiration increased by about 0.1% per year. In 2008, the global total of CO2 released from the soil reached roughly 98 billion tonnes, about 10 times more carbon than humans are now putting into the atmosphere each year. There are a few plausible explanations for this trend, but the most likely explanation is that increasing temperatures have increased rates of decomposition of soil organic matter, which has increased the flow of CO2. The length of carbon sequestering in soil is dependent on local climatic conditions and thus changes in the course of climate change. From pre-industrial era to 2010, the terrestrial biosphere represented a net source of atmospheric CO2 prior to 1940, switching subsequently to a net sink.
Oceans.
Oceans contain the greatest quantity of actively cycled carbon in this world and are second only to the lithosphere in the amount of carbon they store. The oceans' surface layer holds large amounts of dissolved organic carbon that is exchanged rapidly with the atmosphere. The deep layer's concentration of dissolved inorganic carbon (DIC) is about 15% higher than that of the surface layer. DIC is stored in the deep layer for much longer periods of time. Thermohaline circulation exchanges carbon between these two layers.
Carbon enters the ocean mainly through the dissolution of atmospheric carbon dioxide, which is converted into carbonate. It can also enter the oceans through rivers as dissolved organic carbon. It is converted by organisms into organic carbon through photosynthesis and can either be exchanged throughout the food chain or precipitated into the ocean's deeper, more carbon rich layers as dead soft tissue or in shells as calcium carbonate. It circulates in this layer for long periods of time before either being deposited as sediment or, eventually, returned to the surface waters through thermohaline circulation.
Oceanic absorption of CO2 is one of the most important forms of carbon sequestering limiting the human-caused rise of carbon dioxide in the atmosphere. However, this process is limited by a number of factors. Because the rate of CO2 dissolution in the ocean is dependent on the weathering of rocks and this process takes place slower than current rates of human greenhouse gas emissions, ocean CO2 uptake will decrease in the future. CO2 absorption also makes water more acidic, which affects ocean biosystems. The projected rate of increasing oceanic acidity could slow the biological precipitation of calcium carbonates, thus decreasing the ocean's capacity to absorb carbon dioxide.
Geological carbon cycle.
The geologic component of the carbon cycle operates slowly in comparison to the other parts of the global carbon cycle. It is one of the most important determinants of the amount of carbon in the atmosphere, and thus of global temperatures.
Most of the earth's carbon is stored inertly in the earth's lithosphere. Much of the carbon stored in the earth's mantle was stored there when the earth formed. Some of it was deposited in the form of organic carbon from the biosphere. Of the carbon stored in the geosphere, about 80% is limestone and its derivatives, which form from the sedimentation of calcium carbonate stored in the shells of marine organisms. The remaining 20% is stored as kerogens formed through the sedimentation and burial of terrestrial organisms under high heat and pressure. Organic carbon stored in the geosphere can remain there for millions of years.
Carbon can leave the geosphere in several ways. Carbon dioxide is released during the metamorphosis of carbonate rocks when they are subducted into the earth's mantle. This carbon dioxide can be released into the atmosphere and ocean through volcanoes and hotspots. It can also be removed by humans through the direct extraction of kerogens in the form of fossil fuels. After extraction, fossil fuels are burned to release energy, thus emitting the carbon they store into the atmosphere.
Human influence.
Since the industrial revolution, human activity has modified the carbon cycle by changing its component's functions and directly adding carbon to the atmosphere.
The largest and most direct human influence on the carbon cycle is through direct emissions from burning fossil fuels, which transfers carbon from the geosphere into the atmosphere. Humans also influence the carbon cycle indirectly by changing the terrestrial and oceanic biosphere.
Over the past several centuries, human-caused land use and land cover change (LUCC) has led to the loss of biodiversity, which lowers ecosystems' resilience to environmental stresses and decreases their ability to remove carbon from the atmosphere. More directly, it often leads to the release of carbon from terrestrial ecosystems into the atmosphere. Deforestation for agricultural purposes removes forests, which hold large amounts of carbon, and replaces them, generally with agricultural or urban areas. Both of these replacement land cover types store comparatively small amounts of carbon, so that the net product of the process is that more carbon stays in the atmosphere.
Other human-caused changes to the environment change ecosystems' productivity and their ability to remove carbon from the atmosphere. Air pollution, for example, damages plants and soils, while many agricultural and land use practices lead to higher erosion rates, washing carbon out of soils and decreasing plant productivity.
Higher temperatures and CO2 levels in the atmosphere increase decomposition rates in soil, thus returning CO2 stored in plant material more quickly to the atmosphere.
However, increased levels of CO2 in the atmosphere can also lead to higher gross primary production. It increases photosynthesis rates by allowing plants to more efficiently use water, because they no longer need to leave their stomata open for such long periods of time in order to absorb the same amount of carbon dioxide. This type of carbon dioxide fertilization affects mainly C3 plants, because C4 plants can already concentrate CO2 effectively.
Humans also affect the oceanic carbon cycle. Current trends in climate change lead to higher ocean temperatures, thus modifying ecosystems. Also, acid rain and polluted runoff from agriculture and industry change the ocean's chemical composition. Such changes can have dramatic effects on highly sensitive ecosystems such as coral reefs, thus limiting the ocean's ability to absorb carbon from the atmosphere on a regional scale and reducing oceanic biodiversity globally.

</doc>
<doc id="47505" url="http://en.wikipedia.org/wiki?curid=47505" title="Queen (chess)">
Queen (chess)

The queen (♕,♛) is the most powerful piece in the game of chess, able to move any number of squares vertically, horizontally or diagonally. Each player starts the game with one queen, placed in the middle of the first rank next to the king. Because of the value of a queen, it is sometimes used as bait to lure an opponent into a trap by a queen sacrifice. Another tactic is to use the queen to threaten the opponent's queen, to either retreat or to exchange the queen (losing both of them) to reduce the game to less powerful pieces. The queen is often used in conjunction with another piece, such as teamed with a bishop or rook, where the pieces could guard each other while threatening the opponent pieces.
With the chessboard oriented correctly, the white queen starts on a white square and the black queen on a black square. (Thus the mnemonics "queen gets her color", or "queen on [her] [own] color", or "the dress [queen piece] matches the shoes [square]", Latin "servat regina colorem".) In algebraic notation, the white queen starts on d1 and the black queen starts on d8. Because the queen is the most powerful piece, when a pawn is promoted, it is almost always promoted to a queen.
In the game "shatranj", an ancestor of chess, the queen was a fairly weak piece called a "fers" or "vizier", only able to move or capture one step diagonally and not at all in any other direction. The modern queen's move arose in 15th-century Europe.
The piece is archaically known as the "minister". In Polish it is known as the "Hetman" – the name of a major historical military-political office. In Russian it is known as "ferz'" (ферзь). The Arabic name of the piece is "Wazïr", the same as in shatranj.
 
Movement.
The queen can be moved any number of unoccupied squares in a straight line vertically, horizontally, or diagonally, thus combining the moves of the rook and bishop. The queen captures by occupying the square on which an enemy piece sits.
Although both players start with one queen each, a player can promote a pawn to any of several types of pieces, including a queen, when the pawn is moved to the player's furthest rank (the opponent's first rank). Such a queen created by promotion can be an additional queen, or if the player's queen has been captured, a replacement queen. Pawn promotion to a queen is colloquially called "queening", which is by far the most common type of piece a pawn is promoted to because of the relative power of a queen.
General remarks.
Piece value.
Ordinarily the queen is slightly more powerful than a rook and a bishop together, while slightly less powerful than two rooks. It is almost always disadvantageous to exchange the queen for a single piece other than the enemy's queen.
The reason the queen is more powerful than a combination of a rook and bishop, even though they control the same number of squares, is twofold. First, the queen is a more mobile unit than the rook and bishop, as the entire power of the queen can be transferred to another location in one move, while transferring the entire firepower of a rook and bishop requires two moves, and the bishop is always restricted to squares of one color. Second, the queen is not hampered by the bishop's inability to control squares of the opposite color to the square on which it stands. A factor in favor of the rook and bishop is that they can attack (or defend) a square twice, while a queen can only do so only once, but experience has shown that this factor is usually less significant than the points favoring the queen.
The queen is at her most powerful when the board is open, when the enemy king is not well defended, or when there are "loose" (i.e. undefended) pieces in the enemy camp. Because of her long range and ability to move in multiple directions, the queen is well equipped to execute forks. Compared to other long range pieces (i.e. rooks and bishops) the queen is less restricted and more powerful also in closed positions.
Strategy.
Beginners often develop the queen early in the game, hoping to plunder the enemy position and deliver an early checkmate such as Scholar's Mate. This can expose the easily harassed queen to attacks by weaker pieces causing the player to lose time. Experienced players generally prefer to delay developing the queen, and instead develop minor pieces in the opening.
Early queen attacks are rare in high level chess, but there are some openings with early queen development that are used by high level players. For example, the Scandinavian Defense, which features queen moves by Black on the second and third moves is considered sound, and has been played at the world championship level. Some less common examples have also been observed in high level games. The Parham Attack, which is widely considered an opening suitable only for beginners, has occasionally been played by the strong American Grandmaster Hikaru Nakamura.
A queen exchange often marks the beginning of the endgame, but there are queen endgames, and sometimes queens are exchanged in the opening, long before the endgame. A common goal in the endgame is to promote a pawn to a queen. As the queen has the largest range and mobility, queen and king vs. lone king is an easy win when compared to some other basic mates.
Queen sacrifice.
A "queen sacrifice" is the deliberate sacrifice of a queen in order to gain a more favorable tactical position.
History.
The queen was originally the counsellor or prime minister or vizier (Sanskrit "mantri", Persian "farzīn", Arabic "firzān" or "firz"). Initially it could move only one square diagonally. About 1300 its move was enhanced to allow it to move two squares with jump onto a same-colored square for its first move, to help the sides to come into contact sooner.
The "fers" changed into the queen over time. The first surviving mention of this piece as a queen or similar was "regina" in the Einsiedeln Poem, written in Latin around 997 and preserved in a monastery at Einsiedeln in Switzerland. Some surviving early medieval pieces depict the piece as a queen, and the word "fers" became grammatically feminized in several languages, for example "alferza" in Spanish and "fierce" or "fierge" in French, before it was replaced with names such as "reine" or "dame" (lady). The "Carmina Burana" also refer to the queen as "femina" (woman) and "coniunx" (spouse), and the name "Amazon" has sometimes been seen.
In Russian it keeps its Persian name of "ferz" to this day; "koroleva" (queen) is colloquial and is never used by professional chess players. However, the names "korolevna" (king's daughter), "tsaritsa" (tsar's wife), and "baba" (old woman) are attested as early as 1694. In Arabic countries the queen remains termed, and in some cases depicted as, a vizier.
Historian Marilyn Yalom proposes that the prominence of medieval queens such as Eleanor of Aquitaine and Blanche of Castile and Isabella I of Castile, the cult of the Virgin Mary, and the power ascribed to women in the troubadour tradition of courtly love, might have been partly responsible for influencing the piece towards its identity as a queen and later its modern great power on the board, as might the medieval popularity of chess as a game particularly suitable for women to play on equal terms with men. She points to medieval poetry depicting the Virgin as the chess-queen of God or "Fierce Dieu". Significantly, the earliest surviving treatise to describe the modern movement of the queen (as well as the bishop and pawn), "Repetición de amores e arte de axedres con CL iuegos de partido" ("Discourses on Love and the Art of Chess with 150 Problems") by Luis Ramírez de Lucena, was published during the reign of Isabella I of Castile. Well before the queen's powers expanded, it was already being romantically described as essential to the king's survival, so that when the queen was lost, there was nothing more of value on the board.
Marilyn Yalom wrote that:
During the 16th century the queen's move took its modern form as a combination of the move of the rook and the current move of the bishop. Starting from Spain, this new version - called "queen's chess" ("scacchi de la donna"), or pejoratively "madwoman's chess" ("scacchi alla rabiosa") - spread throughout Europe rapidly, partly due to the advent of the printing press and the popularity of new books on chess. The new rules faced a backlash in some quarters, ranging from anxiety over a powerful female warrior figure to frank abuse against women in general.
At various times, the ability of pawns to be queened was restricted while the original queen was still on the board, so as not to cause scandal by providing the king with more than one queen. An early twelfth-century Latin poem refers to a queened pawn as a "ferzia", as opposed to the original queen or "regina", to account for this.
In Russia for a long time the queen could also move like a knight; some players disapproved of this ability to "gallop like the horse" (knight). The book "A History of Chess" by H.J.R.Murray, page 384, says that a Mr.Coxe who was in Russia in 1772 saw chess played with the queen also moving like a knight.
Unicode.
Unicode defines two codepoints for queen:
♕ U+2655 White Chess Queen (HTML &#9813;)
♛ U+265B Black Chess Queen (HTML &#9819;)

</doc>
<doc id="47506" url="http://en.wikipedia.org/wiki?curid=47506" title="520s BC">
520s BC


</doc>
<doc id="47507" url="http://en.wikipedia.org/wiki?curid=47507" title="510s BC">
510s BC

Deaths.
Evans, John Karl. "War, Women, and Children in Ancient Rome" (London, 1991).

</doc>
<doc id="47510" url="http://en.wikipedia.org/wiki?curid=47510" title="Cirrus cloud">
Cirrus cloud

Cirrus (cloud classification symbol: Ci) is a genus of atmospheric cloud generally characterized by thin, wispy strands, giving the type its name from the Latin word "cirrus" meaning a ringlet or curling lock of hair. The strands of cloud sometimes appear in tufts of a distinctive form referred to by the common name of "mares' tails".
On planet Earth, cirrus generally appears white or light gray in color. It forms when water vapor undergoes deposition at altitudes above 16500 ft in temperate regions and above 20000 ft in tropical regions. It also forms from the outflow of tropical cyclones or the anvils of cumulonimbus cloud. Since cirrus clouds arrive in advance of the frontal system or tropical cyclone, it indicates that weather conditions may soon deteriorate. While it indicates the arrival of precipitation (rain), cirrus clouds per se produce only fall streaks (falling ice crystals that evaporate before landing on the ground).
Jet stream-powered cirrus can grow long enough to stretch across continents while remaining only a few kilometers deep. When visible light interacts with the ice crystals in cirrus cloud, it produces optical phenomena such as sun dogs and haloes. Cirrus is known to raise the temperature of the air beneath the main cloud layer by an average of 10 °C (18 °F). When the individual filaments become so extensive that they are virtually indistinguishable from one another, they form a sheet of high cloud called cirrostratus. Convection at high altitudes can produce another high-based genus called cirrocumulus, a pattern of small cloud tufts that contain droplets of supercooled water.
Cirrus clouds form on other planets, including Mars, Jupiter, Saturn, Uranus, and possibly Neptune. They have even been seen on Titan, one of Saturn's moons. Some of these extraterrestrial cirrus clouds are composed of ammonia or methane ice rather than water ice. The term "cirrus" is also used for certain interstellar clouds composed of sub-micrometer-sized dust grains.
Description.
Cirrus cloud ranges in thickness from 100 m to 8000 m, with an average thickness of 1500 m. There are, on average, 30 ice crystals per liter (96 ice crystals per gallon), but this ranges from one ice crystal per 10,000 liters (3.7 ice crystals per 10,000 gallons) to 10,000 ice crystals per liter (37,000 ice crystals per gallon), a difference of eight orders of magnitude. The length of each of these ice crystals is usually 0.25 millimeters long, but they range from as short as 0.01 millimeters or as long as several millimeters. The ice crystals in contrails are much smaller than those in naturally-occurring cirrus cloud, as they are around 0.001 millimeters to 0.1 millimeters in length. Cirrus can vary in temperature from -20 C to -30 C.
The ice crystals in cirrus clouds have different shapes in addition to different sizes. Some shapes include solid columns, hollow columns, plates, rosettes, and conglomerations of the various other types. The shape of the ice crystals is determined by the air temperature, atmospheric pressure, and ice supersaturation. Cirrus in temperate regions typically have the shapes segregated by type: the columns and plates tend to be at the top of the cloud, whereas the rosettes and conglomerations tend to be near the base. In the northern Arctic region, cirrus tend to be composed of only the columns, plates, and conglomerations, and these crystals tend to be at least four times larger than the minimum size. In Antarctica, cirrus are usually composed of only the columns, and these columns are much longer than normal.
Scientists have studied the characteristics of cirrus using several different methods. One, Light Detection and Ranging (LiDAR), gives highly accurate information on the cloud's altitude, length, and width. Balloon-carried hygrometers give information on the humidity of the cirrus cloud but are not accurate enough to measure the depth of the cloud. Radar units give information on the altitudes and thicknesses of cirrus clouds. Another data source is satellite measurements from the Stratospheric Aerosol and Gas Experiment (SAGE) program. These satellites measure where infrared radiation is absorbed in the atmosphere, and if it is absorbed at cirrus altitudes, then it is assumed that there are cirrus clouds in that location. The United States National Aeronautics and Space Administration's (NASA) MODerate resolution Imaging Spectroradiometer (MODIS) also gives information on the cirrus cloud cover by measuring reflected infrared radiation of various specific frequencies during the day. During the night, it determines cirrus cover by detecting the Earth's infrared emissions. The cloud reflects this radiation back to the ground, thus enabling satellites to see the "shadow" it casts into space. Visual observations from aircraft or the ground provide additional information about cirrus clouds.
Based upon data taken from the United States using these methods, cirrus cloud cover was found to vary diurnally and seasonally. The researchers found that in the summer, at noon, the cover is the lowest, with an average of 23% of the United States' land area covered by cirrus. Around midnight, the cloud cover increases to around 28%. In winter, the cirrus cloud cover did not vary appreciably from day to night. These percentages include clear days and nights, as well as days and nights with other cloud types, as lack of cirrus cloud cover. When these clouds are present, the typical coverage ranges from 30% to 50%. Based on satellite data, cirrus covers an average of 20% to 25% of the Earth's surface. In the tropical regions, this cloud covers around 70% of the region's surface area.
Cirrus clouds often produce hair-like filaments—similar to the virga produced in liquid–water clouds—called fall streaks, and they are made of heavier ice crystals that fall from the cloud. The sizes and shapes of fall streaks are determined by the wind shear.
Cirrus comes in four distinct species; Cirrus "castellanus", "fibratus", "spissatus", and "uncinus"; which are each divided into four varieties: "intortus", "vertebratus", "radiatus", and "duplicatus". "Cirrus castellanus" is a species that has cumuliform tops caused by high-altitude convection rising up from the main cloud body. "Cirrus fibratus" looks striated and is the most common cirrus species. "Cirrus uncinus" clouds are hooked and are the form that is usually called "mare's tails". Of the varieties, "Cirrus intortus" has an extremely contorted shape, and "cirrus radiatus" has large, radial bands of cirrus clouds that stretch across the sky. Kelvin-Helmholtz waves are a form of cirrus intortus that has been twisted into loops by vertical wind shear.
Formation.
Cirrus clouds are formed when water vapor undergoes deposition at high altitudes where the atmospheric pressure ranges from 600 mbar at 4000 m above sea level to 200 mbar at 12000 m above sea level. These conditions commonly occur at the leading edge of a warm front. Because humidity is low at such high altitudes, this genus-type tends to be very thin.
Cyclones.
Cirrus forms from tropical cyclones, and is commonly seen fanning out from the eyewalls of hurricanes. A large shield of cirrus and cirrostratus typically accompanies the high altitude outflow of hurricanes or typhoons, and these can make the underlying rain bands—and sometimes even the eye—difficult to detect in satellite photographs.
Thunderstorms.
Thunderstorms can form dense cirrus at their tops. As the cumulonimbus cloud in a thunderstorm grows vertically, the liquid water droplets freeze when the air temperature reaches the freezing point. The anvil cloud takes its shape because the temperature inversion at the tropopause prevents the warm, moist air forming the thunderstorm from rising any higher, thus creating the flat top. In the tropics, these thunderstorms occasionally produce copious amounts of cirrus from their anvils. High-altitude winds commonly push this dense mat out into an anvil shape that stretches downwind as much as several kilometers.
Individual cirrus cloud formations can be the remnants of anvil clouds formed by thunderstorms. In the dissipating stage of a cumulonimbus cloud, when the normal column rising up to the anvil has evaporated or dissipated, the mat of cirrus in the anvil is all that is left.
Contrails.
Contrails are a manmade type of cirrus cloud formed when water vapor from the exhaust of a jet engine condenses on particles, which come from either the surrounding air or the exhaust itself, and freezes, leaving behind a visible trail. The exhaust can also trigger the formation of cirrus by providing ice nuclei when there is an insufficient naturally-occurring supply in the atmosphere. One of the environmental impacts of aviation is that persistent contrails can form into large mats of cirrus, and increased air traffic has been implicated as one possible cause of the increasing frequency and amount of cirrus in Earth's atmosphere.
Use in forecasting.
Random, isolated cirrus do not have any particular significance. A large number of cirrus clouds can be a sign of an approaching frontal system or upper air disturbance. This signals a change in weather in the near future, which usually becomes stormier. If the cloud is a cirrus castellanus, there might be instability at the high altitude level. When the clouds deepen and spread, especially when they are of the "cirrus radiatus" variety or "cirrus fibratus" species, this usually indicates an approaching weather front. If it is a warm front, the cirrus clouds spread out into cirrostratus, which then thicken and lower into altocumulus and altostratus. The next set of clouds are the rain-bearing nimbostratus clouds. When cirrus clouds precede a cold front, squall line or multicellular thunderstorm, it is because they are blown off the anvil, and the next to arrive are the cumulonimbus clouds. Kelvin-Helmholtz waves indicate extreme wind shear at high levels.
Within the tropics, 36 hours prior to the center passage of a tropical cyclone, a veil of white cirrus clouds approaches from the direction of the cyclone. In the mid to late 19th century, forecasters used these cirrus veils to predict the arrival of hurricanes. In the early 1870s the president of Belén College in Havana, Cuba, Father Benito Viñes, developed the first hurricane forecasting system, and he mainly used the motion of these clouds in formulating his predictions. He would observe the clouds hourly from 4:00 am to 10:00 pm. After accumulating enough information, Viñes began accurately predicting the paths of hurricanes, and he eventually summarized his observations in his book, "Apuntes Relativos a los Huracanes de las Antilles".
Effects on climate.
Cirrus clouds cover up to 25% of the Earth and have a net heating effect. When they are thin and translucent, the clouds efficiently absorb outgoing infrared radiation while only marginally reflecting the incoming sunlight. When cirrus clouds are 100 m thick, they reflect only around 9% of the incoming sunlight, but they prevent almost 50% of the outgoing infrared radiation from escaping, thus raising the temperature of the atmosphere beneath the clouds by an average of 10 °C (18 °F)—a process known as the greenhouse effect. Averaged worldwide, cloud formation results in a temperature loss of 5 °C (9 °F) at the earth's surface, mainly the result of stratocumulus clouds.
As a result of their warming effects when relatively thin, cirrus clouds have been implicated as a potential partial cause of global warming. Scientists have speculated that global warming could cause high thin cloud cover to increase, thereby increasing temperatures and humidity. This, in turn, would increase the cirrus cloud cover, effectively creating a positive feedback circuit. A prediction of this hypothesis is that the cirrus would move higher as the temperatures rose, increasing the volume of air underneath the clouds and the amount of infrared radiation reflected back down to earth. In addition, the hypothesis suggests that the increase in temperature would tend to increase the size of the ice crystals in the cirrus cloud, possibly causing the reflection of solar radiation and the reflection of the Earth's infrared radiation to balance out.
A similar hypothesis put forth by Richard Lindzen is the iris hypothesis in which an increase in tropical sea surface temperatures results in less cirrus clouds and thus more infrared radiation emitted to space.
Optical phenomena.
Cirrus clouds, like cirrostratus clouds, can produce several optical effects, such as halos around the sun and moon. Halos are caused by interaction of the light with hexagonal ice crystals present in the clouds, which, depending on their shape and orientation, can result in a wide variety of white and colored rings, arcs and spots in the sky. Common halo varieties are the 22° halo, sun dogs, the circumzenithal arc and the circumhorizontal arc. Halos produced by cirrus clouds tend to be more pronounced and colorful than those caused by cirrostratus.
More rarely, cirrus clouds are capable of producing glories, more commonly associated with liquid water-based clouds such as stratus. A glory is a set of concentric, faintly-colored glowing rings that appear around the shadow of the observer, and are best observed from a high viewpoint or from a plane. Cirrus clouds only form glories when the constituent ice crystals are aspherical, and researchers suggest that the ice crystals must be between 0.009 millimeters and 0.015 millimeters in length. 
Relation to other clouds.
Cirrus clouds are one of three different genera of high-étage (high-level) clouds. High-étage clouds form at 16500 ft and above in temperate regions. The other two genera, cirrocumulus and cirrostratus, are also high clouds.
In the intermediate range, from 6500 ft to 23000 ft in temperate regions, are the mid-étage clouds. They comprise two or three genera depending on the system of height classification being used: altostratus, altocumulus, and, according to WMO classification, nimbostratus. These clouds are formed from ice crystals, supercooled water droplets, or liquid water droplets.
Low-étage clouds, form at less than 6500 ft. The two genera that are strictly low-étage are stratus, and stratocumulus. These clouds are composed of water droplets, except during winter when they are formed of supercooled waterdroplets or ice crystals if the temperature at cloud level is below freezing. Two additional genera usually form in the low altitude range, but may be based at higher levels under conditions of very low humidity. They comprise the genera cumulus, and cumulonimbus, which along with nimbostratus, are often classified separately as clouds of vertical development, especially when their tops are high enough to be composed of super-cooled water droplets or ice crystals.
The altitudes of high-étage clouds like cirrus vary considerably with latitude. In the polar regions, they are at their lowest, with a minimum altitude of only 10000 ft to a maximum of 25000 ft. In tropical regions, they are at their highest, ranging in altitude from about 20000 ft to around 60000 ft. In temperate regions, they range in altitude from 16500 ft to 45000 ft—a variation in contrast to low-étage clouds, which do not appreciably change altitude with latitude.
Summary of high cloud genera.
There are three main genera in the family of high clouds: cirrus, cirrocumulus, and cirrostratus. Cirrostratus clouds commonly produce halos because they are composed almost entirely of ice crystals. Cirrocumulus and cirrostratus are sometimes informally referred to as "cirriform clouds" because of their frequent association with cirrus. They are given the prefix "cirro-", but this refers more to their altitude range than their physical structure. Cirrocumulus in its pure form is actually a high cumuliform genus, and cirrostratus is stratiform, like altostratus and lower based sheet clouds.
Cirrocumulus.
Cirrocumulus clouds form in sheets or patches and do not cast shadows. They commonly appear in regular, rippling patterns or in rows of clouds with clear areas between. Cirrocumulus are, like other members of the cumuliform category, formed via convective processes. Significant growth of these patches indicates high-altitude instability and can signal the approach of poorer weather. The ice crystals in the bottoms of cirrocumulus clouds tend to be in the form of hexagonal cylinders. They are not solid, but instead tend to have stepped funnels coming in from the ends. Towards the top of the cloud, these crystals have a tendency to clump together. These clouds do not last long, and they tend to change into cirrus because as the water vapor continues to deposit on the ice crystals, they eventually begin to fall, destroying the upward convection. The cloud then dissipates into cirrus. Cirrocumulus clouds come in four species: "stratiformis", "lenticularis", "castellanus", and "floccus". They are iridescent when the constituent supercooled water droplets are all about the same size.
Cirrostratus.
Cirrostratus clouds can appear as a milky sheen in the sky or as a striated sheet. They are sometimes similar to altostratus and are distinguishable from the latter because the sun or moon is always clearly visible through transparent cirrostratus, in contrast to altostratus which tends to be opaque or translucent. Cirrostratus come in two species, "fibratus" and "nebulosus". The ice crystals in these clouds vary depending upon the height in the cloud. Towards the bottom, at temperatures of around -35 C to -45 C, the crystals tend to be long, solid, hexagonal columns. Towards the top of the cloud, at temperatures of around -47 C to -52 C, the predominant crystal types are thick, hexagonal plates and short, solid, hexagonal columns. These clouds commonly produce halos, and sometimes the halo is the only indication that such clouds are present. They are formed by warm, moist air being lifted slowly to a very high altitude. When a warm front approaches, cirrostratus clouds become thicker and descend forming altostratus clouds, and rain usually begins 12 to 24 hours later.
Extraterrestrial.
Cirrus clouds have been observed on several other planets. On September 18, 2008, the Martian Lander "Phoenix" took a time-lapse photograph of a group of cirrus clouds moving across the Martian sky using LiDAR. Near the end of its mission, the Phoenix Lander detected more thin clouds close to the north pole of Mars. Over the course of several days, they thickened, lowered, and eventually began snowing. The total precipitation was only a few thousandths of a millimeter. James Whiteway from York University concluded that "precipitation is a component of the [Martian] hydrologic cycle." These clouds formed during the Martian night in two layers, one around 4000 m above ground and the other at surface level. They lasted through early morning before being burned away by the sun. The crystals in these clouds were formed at a temperature of -65 C, and they were shaped roughly like ellipsoids 0.127 millimeters long and 0.042 millimeters wide.
On Jupiter, cirrus clouds are composed of ammonia. When Jupiter's South Equatorial Belt disappeared, one hypothesis put forward by Glenn Orten was that a large quantity of ammonia cirrus clouds had formed above it, hiding it from view. NASA's Cassini probe detected these clouds on Saturn and thin water-ice cirrus on Saturn's moon Titan. Cirrus clouds composed of methane ice exist on Uranus. On Neptune, thin wispy clouds which could possibly be cirrus have been detected over the Great Dark Spot. As on Uranus, these are probably methane crystals.
Interstellar cirrus clouds are composed of tiny dust grains smaller than a micrometer and are therefore not true clouds of this genus which are composed of ice crystals or other frozen liquids. They range from a few light years to dozens of light years across. While they are not technically cirrus clouds, the dust clouds are referred to as "cirrus" because of their similarity to the clouds on Earth. They also emit infrared radiation, similar to the way cirrus clouds on Earth reflect heat being radiated out into space.
Sources.
Footnotes
Bibliography
</dl>

</doc>
<doc id="47512" url="http://en.wikipedia.org/wiki?curid=47512" title="Climate change">
Climate change

Climate change is a change in the statistical distribution of weather patterns when that change lasts for an extended period of time (i.e., decades to millions of years). Climate change may refer to a change in average weather conditions, or in the time variation of weather around longer-term average conditions (i.e., more or fewer extreme weather events). Climate change is caused by factors such as biotic processes, variations in solar radiation received by Earth, plate tectonics, and volcanic eruptions. Certain human activities have also been identified as significant causes of recent climate change, often referred to as "global warming".
Scientists actively work to understand past and future climate by using observations and theoretical models. A climate record — extending deep into the Earth's past — has been assembled, and continues to be built up, based on geological evidence from borehole temperature profiles, cores removed from deep accumulations of ice, floral and faunal records, glacial and periglacial processes, stable-isotope and other analyses of sediment layers, and records of past sea levels. More recent data are provided by the instrumental record. General circulation models, based on the physical sciences, are often used in theoretical approaches to match past climate data, make future projections, and link causes and effects in climate change.
Terminology.
The most general definition of "climate change" is a change in the statistical properties of the climate system when considered over long periods of time, regardless of cause. Accordingly, fluctuations over periods shorter than a few decades, such as El Niño, do not represent climate change.
The term sometimes is used to refer specifically to climate change caused by human activity, as opposed to changes in climate that may have resulted as part of Earth's natural processes.
In this sense, especially in the context of environmental policy, the term "climate change" has become synonymous with anthropogenic global warming. Within scientific journals, "global warming" refers to surface temperature increases while "climate change" includes global warming and everything else that increasing greenhouse gas levels will affect.
Causes.
On the broadest scale, the rate at which energy is received from the sun and the rate at which it is lost to space determine the equilibrium temperature and climate of Earth. This energy is distributed around the globe by winds, ocean currents, and other mechanisms to affect the climates of different regions.
Factors that can shape climate are called climate forcings or "forcing mechanisms". These include processes such as variations in solar radiation, variations in the Earth's orbit, variations in the albedo or reflectivity of the continents and oceans, mountain-building and continental drift and changes in greenhouse gas concentrations. There are a variety of climate change feedbacks that can either amplify or diminish the initial forcing. Some parts of the climate system, such as the oceans and ice caps, respond more slowly in reaction to climate forcings, while others respond more quickly. There are also key threshold factors which when exceeded can produce rapid change.
Forcing mechanisms can be either "internal" or "external". Internal forcing mechanisms are natural processes within the climate system itself (e.g., the thermohaline circulation). External forcing mechanisms can be either natural (e.g., changes in solar output) or anthropogenic (e.g., increased emissions of greenhouse gases).
Whether the initial forcing mechanism is internal or external, the response of the climate system might be fast (e.g., a sudden cooling due to airborne volcanic ash reflecting sunlight), slow (e.g. thermal expansion of warming ocean water), or a combination (e.g., sudden loss of albedo in the arctic ocean as sea ice melts, followed by more gradual thermal expansion of the water). Therefore, the climate system can respond abruptly, but the full response to forcing mechanisms might not be fully developed for centuries or even longer.
Internal forcing mechanisms.
Scientists generally define the five components of earth's climate system to include atmosphere, hydrosphere, cryosphere, lithosphere (restricted to the surface soils, rocks, and sediments), and biosphere. Natural changes in the climate system ("internal forcings") result in internal "climate variability". Examples include the type and distribution of species, and changes in ocean currents.
Ocean variability.
The ocean is a fundamental part of the climate system, some changes in it occurring at longer timescales than in the atmosphere, massing hundreds of times more and having very high thermal inertia (such as the ocean depths still lagging today in temperature adjustment from the Little Ice Age).
Short-term fluctuations (years to a few decades) such as the El Niño-Southern Oscillation, the Pacific decadal oscillation, the North Atlantic oscillation, and the Arctic oscillation, represent climate variability rather than climate change. On longer time-scales, alterations to ocean processes such as thermohaline circulation play a key role in redistributing heat by carrying out a very slow and extremely deep movement of water and the long-term redistribution of heat in the world's oceans.
Life.
Life affects climate through its role in the carbon and water cycles and such mechanisms as albedo, evapotranspiration, cloud formation, and weathering. Examples of how life may have affected past climate include: glaciation 2.3 billion years ago triggered by the evolution of oxygenic photosynthesis, glaciation 300 million years ago ushered in by long-term burial of decomposition-resistant detritus of vascular land plants (forming coal), termination of the Paleocene-Eocene Thermal Maximum 55 million years ago by flourishing marine phytoplankton, reversal of global warming 49 million years ago by 800,000 years of arctic azolla blooms, and global cooling over the past 40 million years driven by the expansion of grass-grazer ecosystems.
External forcing mechanisms.
Orbital variations.
Slight variations in Earth's orbit lead to changes in the seasonal distribution of sunlight reaching the Earth's surface and how it is distributed across the globe. There is very little change to the area-averaged annually averaged sunshine; but there can be strong changes in the geographical and seasonal distribution. The three types of orbital variations are variations in Earth's eccentricity, changes in the tilt angle of Earth's axis of rotation, and precession of Earth's axis. Combined together, these produce Milankovitch cycles which have a large impact on climate and are notable for their correlation to glacial and interglacial periods, their correlation with the advance and retreat of the Sahara, and for their appearance in the stratigraphic record.
The IPCC notes that Milankovitch cycles drove the ice age cycles, CO2 followed temperature change "with a lag of some hundreds of years," and that as a feedback amplified temperature change. The depths of the ocean have a lag time in changing temperature (thermal inertia on such scale). Upon seawater temperature change, the solubility of CO2 in the oceans changed, as well as other factors impacting air-sea CO2 exchange.
Solar output.
The Sun is the predominant source of energy input to the Earth. Both long- and short-term variations in solar intensity are known to affect global climate.
Three to four billion years ago the sun emitted only 70% as much power as it does today. If the atmospheric composition had been the same as today, liquid water should not have existed on Earth. However, there is evidence for the presence of water on the early Earth, in the Hadean and Archean eons, leading to what is known as the faint young Sun paradox. Hypothesized solutions to this paradox include a vastly different atmosphere, with much higher concentrations of greenhouse gases than currently exist. Over the following approximately 4 billion years, the energy output of the sun increased and atmospheric composition changed. The Great Oxygenation Event – oxygenation of the atmosphere around 2.4 billion years ago – was the most notable alteration. Over the next five billion years the sun's ultimate death as it becomes a red giant and then a white dwarf will have large effects on climate, with the red giant phase possibly ending any life on Earth that survives until that time.
Solar output also varies on shorter time scales, including the 11-year solar cycle and longer-term modulations. Solar intensity variations possibly as a result of the Wolf, Spörer and Maunder Minimum are considered to have been influential in triggering the Little Ice Age, and some of the warming observed from 1900 to 1950. The cyclical nature of the sun's energy output is not yet fully understood; it differs from the very slow change that is happening within the sun as it ages and evolves. Research indicates that solar variability has had effects including the Maunder minimum from 1645 to 1715 A.D., part of the Little Ice Age from 1550 to 1850 A.D. that was marked by relative cooling and greater glacier extent than the centuries before and afterward. Some studies point toward solar radiation increases from cyclical sunspot activity affecting global warming, and climate may be influenced by the sum of all effects (solar variation, anthropogenic radiative forcings, etc.).
Interestingly, a 2010 study "suggests", “that the effects of solar variability on temperature throughout the atmosphere may be contrary to current expectations.”
In an Aug 2011 Press Release, CERN announced the publication in the Nature journal the initial results from its CLOUD experiment. The results indicate that ionisation from cosmic rays significantly enhances aerosol formation in the presence of sulfuric acid and water, but in the lower atmosphere where ammonia is also required, this is insufficient to account for aerosol formation and additional trace vapours must be involved. The next step is to find more about these trace vapours, including whether they are of natural or human origin.
Volcanism.
The eruptions considered to be large enough to affect the Earth's climate on a scale of more than 1 year are the ones that inject over 0.1 Mt of SO2 into the stratosphere. This is due to the optical properties of SO2 and sulfate aerosols, which strongly absorb or scatter solar radiation, creating a global layer of sulfuric acid haze. On average, such eruptions occur several times per century, and cause cooling (by partially blocking the transmission of solar radiation to the Earth's surface) for a period of a few years.
The eruption of Mount Pinatubo in 1991, the second largest terrestrial eruption of the 20th century, affected the climate substantially, subsequently global temperatures decreased by about 0.5 °C (0.9 °F) for up to three years. Thus, the cooling over large parts of the Earth reduced surface temperatures in 1991-93, the equivalent to a reduction in net radiation of 4 watts per square meter. The Mount Tambora eruption in 1815 caused the Year Without a Summer. Much larger eruptions, known as large igneous provinces, occur only a few times every fifty - hundred million years - through flood basalt, and caused in Earth past global warming and mass extinctions.
Small eruptions, with injections of less than 0.1 Mt of sulfur dioxide into the stratosphere, impact the atmosphere only subtly, as temperature changes are comparable with natural variability. However, because smaller eruptions occur at a much higher frequency, they too have a significant impact on Earth's atmosphere.
Seismic monitoring maps current and future trends in volcanic activities, and tries to develop early warning systems. In climate modelling the aim is to study the physical mechanisms and feedbacks of volcanic forcing.
Volcanoes are also part of the extended carbon cycle. Over very long (geological) time periods, they release carbon dioxide from the Earth's crust and mantle, counteracting the uptake by sedimentary rocks and other geological carbon dioxide sinks. The US Geological Survey estimates are that volcanic emissions are at a much lower level than the effects of current human activities, which generate 100–300 times the amount of carbon dioxide emitted by volcanoes. A review of published studies indicates that annual volcanic emissions of carbon dioxide, including amounts released from mid-ocean ridges, volcanic arcs, and hot spot volcanoes, are only the equivalent of 3 to 5 days of human caused output. The annual amount put out by human activities may be greater than the amount released by supererruptions, the most recent of which was the Toba eruption in Indonesia 74,000 years ago.
Although volcanoes are technically part of the lithosphere, which itself is part of the climate system, the IPCC explicitly defines volcanism as an external forcing agent.
Plate tectonics.
Over the course of millions of years, the motion of tectonic plates reconfigures global land and ocean areas and generates topography. This can affect both global and local patterns of climate and atmosphere-ocean circulation.
The position of the continents determines the geometry of the oceans and therefore influences patterns of ocean circulation. The locations of the seas are important in controlling the transfer of heat and moisture across the globe, and therefore, in determining global climate. A recent example of tectonic control on ocean circulation is the formation of the Isthmus of Panama about 5 million years ago, which shut off direct mixing between the Atlantic and Pacific Oceans. This strongly affected the ocean dynamics of what is now the Gulf Stream and may have led to Northern Hemisphere ice cover. During the Carboniferous period, about 300 to 360 million years ago, plate tectonics may have triggered large-scale storage of carbon and increased glaciation. Geologic evidence points to a "megamonsoonal" circulation pattern during the time of the supercontinent Pangaea, and climate modeling suggests that the existence of the supercontinent was conducive to the establishment of monsoons.
The size of continents is also important. Because of the stabilizing effect of the oceans on temperature, yearly temperature variations are generally lower in coastal areas than they are inland. A larger supercontinent will therefore have more area in which climate is strongly seasonal than will several smaller continents or islands.
Human influences.
In the context of climate variation, anthropogenic factors are human activities which affect the climate. The scientific consensus on climate change is "that climate is changing and that these changes are in large part caused by human activities,"
and it "is largely irreversible."
 “Science has made enormous inroads in understanding climate change and its causes, and is beginning to help develop a strong understanding of current and potential impacts that will affect people today and in coming decades. This understanding is crucial because it allows decision makers to place climate change in the context of other large challenges facing the nation and the world. There are still some uncertainties, and there always will be in understanding a complex system like Earth’s climate. Nevertheless, there is a strong, credible body of evidence, based on multiple lines of research, documenting that climate is changing and that these changes are in large part caused by human activities. While much remains to be learned, the core phenomenon, scientific questions, and hypotheses have been examined thoroughly and have stood firm in the face of serious scientific debate and careful evaluation of alternative explanations.”
 — United States National Research Council, "Advancing the Science of Climate Change"
Of most concern in these anthropogenic factors is the increase in CO2 levels due to emissions from fossil fuel combustion, followed by aerosols (particulate matter in the atmosphere) and the CO2 released by cement manufacture. Other factors, including land use, ozone depletion, animal agriculture and deforestation, are also of concern in the roles they play – both separately and in conjunction with other factors – in affecting climate, microclimate, and measures of climate variables.
Physical evidence.
Evidence for climatic change is taken from a variety of sources that can be used to reconstruct past climates. Reasonably complete global records of surface temperature are available beginning from the mid-late 19th century. For earlier periods, most of the evidence is indirect—climatic changes are inferred from changes in proxies, indicators that reflect climate, such as vegetation, ice cores, dendrochronology, sea level change, and glacial geology.
Temperature measurements and proxies.
The instrumental temperature record from surface stations was supplemented by radiosonde balloons, extensive atmospheric monitoring by the mid-20th century, and, from the 1970s on, with global satellite data as well. The 18O/16O ratio in calcite and ice core samples used to deduce ocean temperature in the distant past is an example of a temperature proxy method, as are other climate metrics noted in subsequent categories.
Historical and archaeological evidence.
Climate change in the recent past may be detected by corresponding changes in settlement and agricultural patterns. Archaeological evidence, oral history and historical documents can offer insights into past changes in the climate. Climate change effects have been linked to the collapse of various civilizations.
Glaciers.
Glaciers are considered among the most sensitive indicators of climate change. Their size is determined by a mass balance between snow input and melt output. As temperatures warm, glaciers retreat unless snow precipitation increases to make up for the additional melt; the converse is also true.
Glaciers grow and shrink due both to natural variability and external forcings. Variability in temperature, precipitation, and englacial and subglacial hydrology can strongly determine the evolution of a glacier in a particular season. Therefore, one must average over a decadal or longer time-scale and/or over a many individual glaciers to smooth out the local short-term variability and obtain a glacier history that is related to climate.
A world glacier inventory has been compiled since the 1970s, initially based mainly on aerial photographs and maps but now relying more on satellites. This compilation tracks more than 100,000 glaciers covering a total area of approximately 240,000 km2, and preliminary estimates indicate that the remaining ice cover is around 445,000 km2. The World Glacier Monitoring Service collects data annually on glacier retreat and glacier mass balance. From this data, glaciers worldwide have been found to be shrinking significantly, with strong glacier retreats in the 1940s, stable or growing conditions during the 1920s and 1970s, and again retreating from the mid-1980s to present.
The most significant climate processes since the middle to late Pliocene (approximately 3 million years ago) are the glacial and interglacial cycles. The present interglacial period (the Holocene) has lasted about 11,700 years. Shaped by orbital variations, responses such as the rise and fall of continental ice sheets and significant sea-level changes helped create the climate. Other changes, including Heinrich events, Dansgaard–Oeschger events and the Younger Dryas, however, illustrate how glacial variations may also influence climate without the orbital forcing.
Glaciers leave behind moraines that contain a wealth of material—including organic matter, quartz, and potassium that may be dated—recording the periods in which a glacier advanced and retreated. Similarly, by tephrochronological techniques, the lack of glacier cover can be identified by the presence of soil or volcanic tephra horizons whose date of deposit may also be ascertained.
Arctic sea ice loss.
The decline in Arctic sea ice, both in extent and thickness, over the last several decades is further evidence for rapid climate change. Sea ice is frozen seawater that floats on the ocean surface. It covers millions of square miles in the polar regions, varying with the seasons. In the Arctic, some sea ice remains year after year, whereas almost all Southern Ocean or Antarctic sea ice melts away and reforms annually. Satellite observations show that Arctic sea ice is now declining at a rate of 11.5 percent per decade, relative to the 1979 to 2000 average.
Vegetation.
A change in the type, distribution and coverage of vegetation may occur given a change in the climate. Some changes in climate may result in increased precipitation and warmth, resulting in improved plant growth and the subsequent sequestration of airborne CO2. A gradual increase in warmth in a region will lead to earlier flowering and fruiting times, driving a change in the timing of life cycles of dependent organisms. Conversely, cold will cause plant bio-cycles to lag. Larger, faster or more radical changes, however, may result in vegetation stress, rapid plant loss and desertification in certain circumstances. An example of this occurred during the Carboniferous Rainforest Collapse (CRC), an extinction event 300 million years ago. At this time vast rainforests covered the equatorial region of Europe and America. Climate change devastated these tropical rainforests, abruptly fragmenting the habitat into isolated 'islands' and causing the extinction of many plant and animal species.
Satellite data available in recent decades indicates that global terrestrial net primary production increased by 6% from 1982 to 1999, with the largest portion of that increase in tropical ecosystems, then decreased by 1% from 2000 to 2009.
Pollen analysis.
Palynology is the study of contemporary and fossil palynomorphs, including pollen. Palynology is used to infer the geographical distribution of plant species, which vary under different climate conditions. Different groups of plants have pollen with distinctive shapes and surface textures, and since the outer surface of pollen is composed of a very resilient material, they resist decay. Changes in the type of pollen found in different layers of sediment in lakes, bogs, or river deltas indicate changes in plant communities. These changes are often a sign of a changing climate. As an example, palynological studies have been used to track changing vegetation patterns throughout the Quaternary glaciations and especially since the last glacial maximum.
Cloud cover and precipitation.
Past precipitation can be estimated in the modern era with the global network of precipitation gauges. Surface coverage over oceans and remote areas is relatively sparse, but, reducing reliance on interpolation, satellite clouds and precipitation data has been available since the 1970s. Quantification of climatological variation of precipitation in prior centuries and epochs is less complete but approximated using proxies such as marine sediments, ice cores, cave stalagmites, and tree rings.
Climatological temperatures substantially affect cloud cover and precipitation. For instance, during the Last Glacial Maximum of 18,000 years ago, thermal-driven evaporation from the oceans onto continental landmasses was low, causing large areas of extreme desert, including polar deserts (cold but with low rates of cloud cover and precipitation). In contrast, the world's climate was cloudier and wetter than today near the start of the warm Atlantic Period of 8000 years ago.
Estimated global land precipitation increased by approximately 2% over the course of the 20th century, though the calculated trend varies if different time endpoints are chosen, complicated by ENSO and other oscillations, including greater global land cloud cover precipitation in the 1950s and 1970s than the later 1980s and 1990s despite the positive trend over the century overall.
Similar slight overall increase in global river runoff and in average soil moisture has been perceived.
Dendroclimatology.
Dendroclimatology is the analysis of tree ring growth patterns to determine past climate variations. Wide and thick rings indicate a fertile, well-watered growing period, whilst thin, narrow rings indicate a time of lower rainfall and less-than-ideal growing conditions.
Ice cores.
Analysis of ice in a core drilled from an ice sheet such as the Antarctic ice sheet, can be used to show a link between temperature and global sea level variations. The air trapped in bubbles in the ice can also reveal the CO2 variations of the atmosphere from the distant past, well before modern environmental influences. The study of these ice cores has been a significant indicator of the changes in CO2 over many millennia, and continues to provide valuable information about the differences between ancient and modern atmospheric conditions.
Animals.
Remains of beetles are common in freshwater and land sediments. Different species of beetles tend to be found under different climatic conditions. Given the extensive lineage of beetles whose genetic makeup has not altered significantly over the millennia, knowledge of the present climatic range of the different species, and the age of the sediments in which remains are found, past climatic conditions may be inferred.
Similarly, the historical abundance of various fish species has been found to have a substantial relationship with observed climatic conditions. Changes in the primary productivity of autotrophs in the oceans can affect marine food webs.
Sea level change.
Global sea level change for much of the last century has generally been estimated using tide gauge measurements collated over long periods of time to give a long-term average. More recently, altimeter measurements — in combination with accurately determined satellite orbits — have provided an improved measurement of global sea level change. To measure sea levels prior to instrumental measurements, scientists have dated coral reefs that grow near the surface of the ocean, coastal sediments, marine terraces, ooids in limestones, and nearshore archaeological remains. The predominant dating methods used are uranium series and radiocarbon, with cosmogenic radionuclides being sometimes used to date terraces that have experienced relative sea level fall. In the early Pliocene, global temperatures were 1–2˚C warmer than the present temperature, yet sea level was 15–25 meters higher than today.
See also.
Climate of recent glaciations
Climate of the past
Recent climate
Further reading.
</dl>
External links.
Listen to this article ()
This audio file was created from a revision of the "Climate change" article dated 2010-03-19, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="47513" url="http://en.wikipedia.org/wiki?curid=47513" title="Climate model">
Climate model

Climate models use quantitative methods to simulate the interactions of the atmosphere, oceans, land surface, and ice. They are used for a variety of purposes from study of the dynamics of the climate system to projections of future climate. The most talked-about use of climate models in recent years has been to project temperature changes resulting from increases in atmospheric concentrations of greenhouse gases.
All climate models take account of incoming energy from the sun as short wave electromagnetic radiation, chiefly visible and short-wave (near) infrared, as well as outgoing energy as long wave (far) infrared electromagnetic radiation from the earth. Any imbalance results in a change in temperature.
Models can range from relatively simple to quite complex:
This is not a full list; for example "box models" can be written to treat flows across and within ocean basins. Furthermore, other types of modelling can be interlinked, such as land use, allowing researchers to predict the interaction between climate and ecosystems.
Box models.
Box models are simplified versions of complex systems, reducing them to boxes (or reservoirs) linked by fluxes. The boxes are assumed to be mixed homogeneously. Within a given box, the concentration of any chemical species is therefore uniform. However, the abundance of a species within a given box may vary as a function of time due to the input to (or loss from) the box or due to the production, consumption or decay of this species within the box.
Simple box models, i.e. box model with a small number of boxes whose properties (e.g. their volume) do not change with time, are often useful to derive analytical formulas describing the dynamics and steady-state abundance of a species. More complex box models are usually solved using numerical techniques.
Box models are used extensively to model environmental systems or ecosystems and in studies of ocean circulation and the carbon cycle.
Zero-dimensional models.
A very simple model of the radiative equilibrium of the Earth is
where
and
The constant "πr"2 can be factored out, giving
Solving for the temperature,
This yields an apparent effective average earth temperature of 288 K. This is because the above equation represents the effective "radiative" temperature of the Earth (including the clouds and atmosphere). The use of effective emissivity and albedo account for the greenhouse effect.
This very simple model is quite instructive, and the only model that could fit on a page. For example, it easily determines the effect on average earth temperature of changes in solar constant or change of albedo or effective earth emissivity.
The average emissivity of the earth is readily estimated from available data. The emissivities of terrestrial surfaces are all in the range of 0.96 to 0.99 (except for some small desert areas which may be as low as 0.7). Clouds, however, which cover about half of the earth’s surface, have an average emissivity of about 0.5 (which must be reduced by the fourth power of the ratio of cloud absolute temperature to average earth absolute temperature) and an average cloud temperature of about 258 K. Taking all this properly into account results in an effective earth emissivity of about 0.64 (earth average temperature 285 K).
This simple model readily determines the effect of changes in solar output or change of earth albedo or effective earth emissivity on average earth temperature. It says nothing, however about what might cause these things to change. Zero-dimensional models do not address the temperature distribution on the earth or the factors that move energy about the earth.
Radiative-convective models.
The zero-dimensional model above, using the solar constant and given average earth temperature, determines the effective earth emissivity of long wave radiation emitted to space. This can be refined in the vertical to a one-dimensional radiative-convective model, which considers two processes of energy transport:
The radiative-convective models have advantages over the simple model: they can determine the effects of varying greenhouse gas concentrations on effective emissivity and therefore the surface temperature. But added parameters are needed to determine local emissivity and albedo and address the factors that move energy about the earth.
Effect of ice-albedo feedback on global sensitivity in a one-dimensional radiative-convective climate model.
Higher-dimension models.
The zero-dimensional model may be expanded to consider the energy transported horizontally in the atmosphere. This kind of model may well be zonally averaged. This model has the advantage of allowing a rational dependence of local albedo and emissivity on temperature – the poles can be allowed to be icy and the equator warm – but the lack of true dynamics means that horizontal transports have to be specified.
EMICs (Earth-system models of intermediate complexity).
Depending on the nature of questions asked and the pertinent time scales, there are, on the one extreme, conceptual, more inductive models, and, on the other extreme, general circulation models operating at the highest spatial and temporal resolution currently feasible. Models of intermediate complexity bridge the gap. One example is the Climber-3 model. Its atmosphere is a 2.5-dimensional statistical-dynamical model with 7.5° × 22.5° resolution and time step of half a day; the ocean is MOM-3 (Modular Ocean Model) with a 3.75° × 3.75° grid and 24 vertical levels.
GCMs (global climate models or general circulation models).
General Circulation Models (GCMs) discretise the equations for fluid motion and energy transfer and integrate these over time. Unlike simpler models, GCMs divide the atmosphere and/or oceans into grids of discrete "cells," which represent computational units. Unlike simpler models which make mixing assumptions, processes internal to a cell—such as convection—that occur on scales too small to be resolved directly are parameterised at the cell level, while other functions govern the interface between cells.
Atmospheric GCMs (AGCMs) model the atmosphere and impose sea surface temperatures as boundary conditions. Coupled atmosphere-ocean GCMs (AOGCMs, e.g. HadCM3, EdGCM, GFDL CM2.X, ARPEGE-Climat) combine the two models. The first general circulation climate model that combined both oceanic and atmospheric processes was developed in the late 1960s at the NOAA Geophysical Fluid Dynamics Laboratory AOGCMs represent the pinnacle of complexity in climate models and internalise as many processes as possible. However, they are still under development and uncertainties remain. They may be coupled to models of other processes, such as the carbon cycle, so as to better model feedback effects. Such integrated multi-system models are sometimes referred to as either "earth system models" or "global climate models."
Research and development.
There are three major types of institution where climate models are developed, implemented and used:
The World Climate Research Programme (WCRP), hosted by the World Meteorological Organization (WMO), coordinates research activities on climate modelling worldwide.
A 2012 U.S. National Research Council report discussed how the large and diverse U.S. climate modeling enterprise could evolve to become more unified. Efficiencies could be gained by developing a common software infrastructure shared by all U.S. climate researchers, and holding an annual climate modeling forum, the report found.

</doc>
<doc id="47514" url="http://en.wikipedia.org/wiki?curid=47514" title="Cloud albedo">
Cloud albedo

Cloud albedo is a measure of the albedo of a cloud - higher values mean that the cloud reflects more solar radiation, or less radiation is transmitted. 
Cloud albedo varies from less than 10% to more than 90% and depends on drop sizes, liquid water or ice content, thickness of the cloud, and the sun's zenith angle. The smaller the drops and the greater the liquid water content, the greater the cloud albedo, if all other factors are the same.
Low, thick clouds (such as stratocumulus) primarily reflect incoming solar radiation, causing it to have a high albedo, whereas high, thin clouds (such as Cirrus) tend to transmit it to the surface but then trap outgoing infrared radiation, causing it to have low albedo. It contributes to the greenhouse effect.

</doc>
<doc id="47515" url="http://en.wikipedia.org/wiki?curid=47515" title="Cloud">
Cloud

In meteorology, a cloud is a visible mass of liquid droplets or frozen crystals made of water or various chemicals suspended in the atmosphere above the surface of a planetary body. These suspended particles are also known as aerosols and are studied in the cloud physics branch of meteorology.
Terrestrial cloud formation is the result of air in any of the lower three principal layers of Earth's atmosphere (collectively known as the homosphere) becoming saturated due to either or both of two processes: cooling of the air and adding water vapor. With sufficient saturation in the troposphere, precipitation will fall to the surface; an exception is virga, which evaporates before reaching the surface. Clouds that form at very high altitudes in the stratosphere and mesosphere do not contain sufficient moisture to generate any outfall of droplets or crystals.
Clouds in the troposphere, the atmospheric layer closest to Earth's surface, have Latin names due to the universal adaptation of Luke Howard's nomenclature. It was introduced in December 1802 and became the basis of a modern international system that classifies these tropospheric aerosols into several physical "forms", then cross-classifies them as low, middle and high-"étage" according to cloud-base altitude range above Earth's surface. Clouds with significant "vertical" extent occupying more than one étage are often considered a distinct group or sub-group.
One physical form shows free-convective upward growth into low or vertical "cumuliform" heaps. Other more layered types appear as non-convective "stratiform" sheets, and as limited-convective "stratocumuliform" rolls or ripples. Both these layered forms have low, middle, and high-étage variants with the latter two identified respectively by the prefixes "alto"- and "cirro"-. Thin "cirriform" wisps are found only at high altitudes of the troposphere. In the case of clouds with vertical extent, prefixes are used whenever necessary to express variations or complexities in their physical structures. These include "cumulo-" for complex highly convective "cumulonimbiform" storm clouds, and "nimbo-" for thick stratiform layers with sufficient vertical depth to produce moderate to heavy "precipitation".
This process of cross-classification produces ten basic genus-types or "genera", most of which can be divided into subtypes consisting of "species" that are often subdivided into "varieties" where applicable. Synoptic surface weather observations use code numbers to record and report any type of tropospheric cloud visible at scheduled observation times based on its height and physical appearance.
Clouds that form above the troposphere have common names for their main types, but are sub-classified "alpha-numerically" rather than with the elaborate system of Latin names given to cloud types in the troposphere. Clouds have been observed on other planets and moons within the Solar System, but, due to their different temperature characteristics, they are often composed of other substances such as methane, ammonia, and sulfuric acid as well as water.
Etymology.
The origin of the term "cloud" can be found in the old English "clud" or "clod", meaning a hill or a mass of rock. Around the beginning of the 13th century, it was extended metaphorically to include rainclouds as masses of evaporated water in the sky because of the similarity in appearance between a mass of rock and a cumulus heap cloud. Over time, the metaphoric term replaced the original old English "weolcan" to refer to clouds in general.
The science of clouds is nephology.
History of cloud science and nomenclature.
Aristotle and Theophrastus.
Ancient cloud studies were not made in isolation, but were observed in combination with other weather elements and even other natural sciences. In about 340 BC the Greek philosopher Aristotle wrote Meteorologica, a work which represented the sum of knowledge of the time about natural science, including weather and climate. For the first time, precipitation and the clouds from which precipitation fell were called meteors, which originate from the Greek word meteoros, meaning 'high in the sky'. From that word came the modern term meteorology, the study of clouds and weather. Meteorologica was a work of intuitive rather than scientific study. Nevertheless, it was the first known work that attempted to treat a broad range of meteorological topics.
Several years after Aristotle's book, his pupil Theophrastus put together a book on weather forecasting called The Book of Signs. Various indicators such as solar and lunar halos formed by high clouds were presented as ways to forecast the weather. The combined works of Aristotle and Theophrastus had such authority they became the main influence in the study of clouds, weather and weather forecasting for nearly 2000 years.
Luke Howard and Jean-Baptiste Lamarck.
After centuries of speculative theories about the formation and behavior of clouds, the first truly scientific studies were undertaken by Luke Howard in England and Jean-Baptiste Lamark in France. Howard was a methodical observer with a strong grounding in the Latin language and used his background to classify the various tropospheric cloud types during 1803. He believed that the changing cloud forms in the sky could unlock the key to weather forecasting. Lamarck had worked independently on cloud classification the previous year and had come up with a different naming scheme that failed to make an impression even in his home country of France because it used unusual French names for cloud types. His system of nomenclature included twelve categories of clouds, with such names as (translated from French) hazy clouds, dappled clouds and broom-like clouds. By contrast, Howard used universally accepted Latin, which caught on quickly. As a sign of the popularity of the naming scheme, the German dramatist and poet Johann Wolfgang von Goethe composed four poems about clouds, dedicating them to Howard. An elaboration of Howard's system was formally adopted by the International Meteorological Conference in 1891.
First comprehensive classification.
Howard's original system established three general cloud "forms" based on physical appearance and process of formation: "cirriform" (mainly detached and wispy), "cumuliform" or convective (mostly detached and heaped, rolled, or rippled), and non-convective "stratiform" (mainly continuous layers in sheets). These were cross-classified into "lower" and "upper" étages. Cumuliform clouds forming in the lower level were given the genus name cumulus from the Latin word for "heap", and low stratiform clouds the genus name stratus from the Latin word for "sheet" or "layer". Physically similar clouds forming in the upper étage were given the genus names cirrocumulus (generally showing more limited convective activity than low level cumulus) and cirrostratus, respectively. Cirriform clouds were identified as always upper level and given the genus name cirrus from the Latin for 'fibre' ot 'hair. To these, Howard added the genus nimbus for clouds of complex structure producing significant precipitation that came to be identified as a distinct "nimbiform" physical category.
Howard's successors.
In 1840, German meteorologist Ludwig Kaemtz added stratocumulus to Howard's canon as a mostly detached low-étage genus of limited convection with both cumuliform- and stratiform characteristics. This led to the recognition of a "stratocumuliform" category that included rolled and rippled clouds classified separately from the more freely convective heaped cumuliform clouds. An alternative to Howard's classification system had been proposed earlier by Heinrich Dove of Germany in 1828, and Elias Loomis of the United States came up with another scheme in 1841, but neither met with international success.
During the mid 1850s, Emilien Renou, director of the Parc Saint-Maur and Montsouris observatories, began work on an elaboration of Howard's classifications that would lead to the introduction during the 1870s of altocumulus (physically more closely related to stratocumulus than to cumulus) and altostratus. These were respectively stratocumuliform and stratiform cloud genera of a newly defined "middle" étage above stratocumulus and stratus but below cirrocumulus and cirrostratus.
Additional proposals were made by Andre Poey (1863), Clemment Ley (1894), and H.H. Clayton (1896), but their systems, like earlier alternative schemes, differed too much from Howard's to have any success beyond the adoption of some secondary cloud types. However, Clayton's idea to formalize the division of clouds by their physical structures into cumuliform, stratiform, "flocciform" (stratocumuliform), and cirriform eventually found favor as an aid in the analysis of satellite cloud images.
In 1880, Philip Weilbach, secretary and librarian at the Art Academy in Copenhagen, and like Luke Howard, an amateur meteorologist, unsuccessfully propoosed an alternative to Howard's classification. However, he also proposed and had accepted by the permanent committee of the International Meteorological Organization (IMO), a forerunner of the present-day World Meteorological Organization (WMO), the designation of a new free-convective vertical or multi-étage genus type, cumulonimbus, which would be distinct from cumulus and nimbus and identifiable by its often very complex structure (frequently including a cirriform top and what are now recognized as multiple accessory clouds), and its ability to produce thunder. With this addition, a canon of ten tropospheric cloud "genera" was established that came to be officially and universally accepted.
In 1890, Otto Jesse revealed the discovery and idenification of the first clouds known to form above the troposphere. He proposed the name "noctilucent" which is Latin for "night shining". Because of the extremely high altitudes of these clouds in what is now known to be the mesosphere, they could become illuminated by the a sun's rays when the sky was nearly dark after sunset and before sunrise. Three years later, Henrik Mohn revealed a similar discovery of nacreous clouds in what is now considered the stratosphere.
In 1896, the first cloud atlas sanctioned by the IMO was produced by Teisserenc de Borte based on collaborations with Hugo H. Hildebrandsson. The latter had become the first researcher to use photography for the study and classification of clouds in 1879.
20th century developments.
A further modification of the genus classification system came when an IMC commission for the study of clouds put forward a refined and more restricted definition of the genus nimbus which was effectively reclassified as a stratiform cloud type. It was then renamed nimbostratus and published with the new name in the 1932 edition of the "International Atlas of Clouds and of States of the Sky". This left cumulonimbus as the only nimbiform type as indicated by its root-name.
On April 1, 1960, the first successful weather satellite, TIROS I (Television Infrared Observation Satellite), was launched from Cape Canaveral, Florida by the National Aeronautics and Space Administration (NASA) with the participation of The US Army Signal Research and Development Lab, RCA, the US Weather Bureau, and the US Naval Photographic Center. During its 78-day mission, it relayed thousands of pictures showing the structure of large-scale cloud regimes, and proved that satellites could provide useful surveillance of global weather conditions from space.
In 1976, the United Kingdom Department of Industry published a modification of the international cloud classification system adapted for satellite cloud observations. It was co-sponsored by NASA and showed a change in name of the nimbiform type to "cumulonimbiform", although the earlier name and original meaning pertaining to all rain clouds can still be found in some classifications.
Tropospheric clouds.
Physical forms, étages, and cross-classification into genera.
Clouds can be divided into five physical forms based on physical structure and process of formation. These forms are commonly used for the purpose of satellite analysis.
The individual "genus" types result from the physical forms being cross-classified by étage within the troposphere. The base-height range for each étage varies depending on the latitudinal geographical zone. A consensus exists as to the designation of high, middle, and low étages, the makeup of the basic canon of ten cloud genera that results from this cross-classification, and the étage designations of non-vertical genus types. Clouds with significant vertical extent occupy more than one étage and are commonly (but not always) treated as a separate group or sub-group, or given separate descriptions within the context of the standard étages.
Physical forms.
Cirriform.
Genus cirrus (high-étage)
Cirriform clouds generally have the appearance of detached or semi-merged filaments and form at high tropospheric altitudes.
Cumuliform.
Genus cumulus (low or multi-étage)
Cumuliform clouds typically appear in isolated heaps. They are the product of localized but generally free-convective lift and can vary in vertical extent depending on the stability characteristics of the air mass where they are forming.
Cumulonimbiform.
Genus cumulonimbus (vertical/multi-étage)
The largest free-convective clouds occur in very unstable air and often have complex structures that include cirriform tops and multiple accessory clouds.
Stratiform.
Genera cirrostratus (high-étage), altostratus (middle-étage), stratus (low-étage), nimbostratus (multi-étage)
In general, stratiform clouds have a flat sheet-like structure and form at any altitude in the troposphere. Very low stratiform cloud can form when advection fog is lifted above surface level during breezy conditions.
Stratocumuliform.
Genera Cirrocumulus (high-étage), altocumulus (middle-étage), stratocumulus (low-étage)
Clouds of this physical structure have both cumuliform and stratiform characteristics in the form of rolls or ripples and generally form as a result of limited convection in slightly unstable air topped by an inversion layer. They can form at any altitude in the troposphere whenever there is sufficient moisture and lift. Layered altocumulus and cirrocumulus are physically more closely related to stratocumulus than to cumulus heaps. However, some ambiguity regarding the physical relationships between these stratocumuliform and cumuliform clouds can arise because the strato- prefix is dropped from the names of the alto- and cirro- genus types to avoid double prefixing.
Étages.
The genus types can be grouped by étage. This is generally done for the purpose of cloud atlases, surface weather observations and weather maps.
High cirriform, stratocumuliform, and stratiform.
Clouds of the high-étage form at altitudes of 10000 to in the polar regions, 16500 to in the temperate regions and 20000 to in the tropical region. All cirriform clouds are classified as high and thus constitute a single genus "cirrus" (Ci). Stratocumuliform and stratiform clouds in the high-étage carry the prefix "cirro-", yielding the respective genus names "cirrocumulus" (Cc) and "cirrostratus" (Cs).
Middle stratocumuliform and stratiform.
Clouds in the middle-étage are prefixed by "alto-", yielding the genus names "altocumulus" (Ac) and "altostratus" (As). These clouds can form as low as 6500 ft above surface at any latitude, but may be based as high as 13000 ft near the poles, 23000 ft at mid latitudes, and 25000 ft in the tropics.
 
Low stratocumuliform, stratiform, and cumuliform.
Low-étage clouds are found from near surface up to 6500 ft. Genus types in this étage either have no prefix or carry one that refers to a characteristic other than altitude.
Vertical or multi-étage stratiform, cumuliform, and cumulonimbiform (low to middle cloud base).
These clouds have low to middle-étage bases that form anywhere from near surface to about 8000 ft. 
Some classifications limit the term "vertical" to upward-growing free-convective cumuliform and cumulonimbiform genera. Downward-growing nimbostratus can be as thick as most upward-growing vertical cumulus, but its horizontal extent tends to be even greater. This sometimes leads to the exclusion of this genus type from the group of vertical clouds. Classifications that follow this approach usually show nimbostratus either as low-étage to denote its normal base height range, or as middle, based on the altitude range at which it normally forms. Sometimes the terms "multi-level" or "multi-étage " are used for all very thick or tall cloud types including nimbostratus to avoid the association of 'vertical' with free-convective cumuliform only. Alternatively, some classifications do not recognize a vertical or multi-étage designation and include all vertical free-convective cumuliform and cumulonimbiform types with the low-étage clouds.
Nimbostratus and some cumulus in this group usually achieve moderate or deep vertical extent, but without towering structure. However, with sufficient airmass instability, upward-growing cumuliform clouds can grow to high towering proportions. Although genus types with vertical extent are often considered a single group, the International Civil Aviation Organization (ICAO) further distinguishes towering vertical clouds as a separate group or sub-group by specifying that these very large cumuliform and cumulonimbiform types must be identified by their standard names or abbreviations in all aviation observations (METARS) and forecasts (TAFS) to warn pilots of possible severe weather and turbulence. When towering vertical types are considered separately, they comprise the aforementioned cumulonimbus genus and one cumulus genus subtype, cumulus congestus (Cu con). This subtype is designated "towering cumulus" (Tcu) by ICAO. There is no stratiform type in this group because by definition, even very thick stratiform clouds cannot have towering vertical structure, although they may be accompanied by embedded towering cumuliform or cumulonimbiform types.
These clouds are sometimes classified separately from the other vertical or multi-étage types because of their ability to produce severe turbulance.
Species.
Genus types are comminly divided into subtypes called "species" that indicate specific structural details. However, because these latter types are not always restricted by étage, some species can be common to several genera that are differentiated mainly by altitude.
Mostly stable stratocumuliform.
Good examples of species common to more than one genus are the "stratiformis" and "lenticularis" types, each of which is common to mostly stable stratocumuliform genera in the high, middle, and low étages. (cirrocumulus, altocumulus, and stratocumulus, respectively). Stratiformis species normally occur in extensive sheets or in smaller patches where there is only minimal convective activity. Lenticularis species tend to have lens-like shapes tapered at the ends. They are most commonly seen as orographic mountain-wave clouds, but can occur anywhere in the troposphere where there is strong wind shear combined with sufficient airmass stability to maintain a generally flat cloud structure.
Stable cirriform and stratiform.
Cirrus clouds have a couple of species that are unique to the wispy structures of this genus and an additional species which is also seen with high-étage stratiform clouds. "Uncinus" filaments with upturned hooks and "spissatus" filaments that merge into dense patches are both considered cirriform species. However, the species "fibratus" can be seen with cirrus and with cirrostratus that is transitional to or from cirrus. Cirrostratus at its most characteristic tends to be mostly of the stratiform species "nebulosus", which creates a rather diffuse appearance lacking in structural detail. Altostratus and nimbostratus clouds always have this physical appearance without significant variation or deviation and, therefore, do not need to be subdivided into species. Low-étage stratus is also of the species nebulosus except when broken up into ragged sheets of stratus fractus.
Partly or mostly unstable cirriform and stratocumuliform.
With increasing airmass instability, "castellanus" structures, which resemble the turrets of a castle when viewed from the side, can be found with any stratocumuliform genus. This species is also sometimes seen with convective patches of cirrus, as are the more detached tufted "floccus" species, which are common to cirrus, cirrocumulus, and altocumulus, but not stratocumulus.
Mostly unstable cumuliform and cumulonimbiform.
Except for stratocumulus castellanus, local airmass instability in the low étage tends to produce clouds of the more freely convective cumulus and cumulonimbus genera, whose species are mainly indicators of degrees of vertical development. A cumulus cloud initially forms as a cloudlet of the species "fractus" or "humilis" that shows only slight vertical development. If the air becomes more unstable, the cloud tends to grow vertically into the species "mediocris", then "congestus", the tallest cumulus species. With further instability, the cloud may continue to grow into cumulonimbus "calvus" (essentially a very tall congestus cloud that produces thunder), then ultimately "capillatus" when supercooled water droplets at the top turn into ice crystals giving it a cirriform appearance.
Varieties.
Genus and species types are further subdivided into "varieties" whose names can appear after the species name to provide a fuller description of a cloud. Some cloud varieties are not restricted to a specific étage or physical structure, and can therefore be common to more than one genus or species.
Opacity-based.
All cloud varieties fall into one of two main groups. One group identifies the opacities of particular low and middle étage cloud structures and comprises the varieties "translucidus" (thin translucent), "perlucidus" (thick opaque with translucent breaks), and "opacus" (thick opaque). These varieties are always identifiable for cloud genera and species with variable opacity. All three are associated with the stratiformis species of altocumulus and stratocumulus. However, only two are seen with altostratus and stratus nebulosus whose uniform structures prevent the formation of a perlucidus variety. Opacity-based varieties are not applied to high-étage clouds because they are always translucent, or in the case of cirrus spissatus, always opaque. Similarly, these varieties are also not associated with moderate and towering vertical clouds because they are always opaque.
Pattern-based.
A second group describes the occasional arrangements of cloud structures into particular patterns that are discernable by a surface-based observer (cloud fields usually being visible only from a significant altitude above the formations). These varieties are not always present with the genera and species with which they are otherwise associated, but only appear when atmospheric conditions favor their formation. "Intortus" and "vertebratus" varieties occur on occasion with cirrus fibratus. They are respectively filaments twisted into irregular shapes, and those that are arranged in fishbone patterns, usually by uneven wind currents that favor the formation of these varieties. The variety "radiatus" is associated with cloud rows of a particular type that appear to converge at the horizon. It is sometimes seen with the fibratus and uncinus species of cirrus, the stratiformis species of altocumulus and stratocumulus, the mediocris and sometimes humilis species of cumulus, and with the genus altostratus.
Another variety, "duplicatus" (closely spaced layers of the same type, one above the other), is sometimes found with cirrus of both the fibratus and uncinus species, and with altocumulus and stratocumulus of the species stratiformis and lenticularis. The variety "undulatus" (having a wavy undulating base) can occur with any clouds of the species stratiformis or lenticularis, and with altostratus. It is only rarely observed with stratus nebulosus. The variety "lacunosus" is caused by localized downdrafts that create circular holes in the form of a honeycomb or net. It is occasionally seen with cirrocumulus and altocumulus of the species stratiformis, castellanus, and floccus, and with stratocumulus of the species stratiformis and castellanus.
Combinations.
It is possible for some species to show combined varieties at one time, especially if one variety is opacity-based and the other is pattern-based. An example of this would be an opaque layer of altocumulus stratiformis arranged in seemingly converging rows. The full technical name of a cloud in this configuration would be "altocumulus stratiformis opacus radiatus", which would identify respectively its genus, species, and two combined varieties.
Accessory clouds and supplementary features.
Supplementary features are not further subdivisions of cloud types below the species and variety level. Rather, they are either "hydrometeors" or special cloud formations with their own Latin names that form in association with certain cloud genera, species, and varieties. Supplemenary features, whether in the form of clouds or precipitation, are directly attached to the main genus-cloud. Accessory clouds, by contrast, are generally detached from the main cloud.
Precipitation-based supplementary features.
One group of supplementary features are not actual cloud formations but rather precipitation that falls when water droplets that make up visible clouds have grown too heavy to remain aloft. "Virga" is a feature seen with clouds producing precipitation that evaporates before reaching the ground, these being of the genera cirrocumulus, altocumulus, altostratus, nimbostratus, stratocumulus, cumulus, and cumulonimbus.
When the precipitation reaches the ground without completely evaporating, it is designated as the feature "praecipitatio". This normally occurs with altostratus opacus, which can produce widespread but usually light precipitation, and with thicker clouds that show significant vertical development. Of the latter, "upward-growing" cumulus mediocris produces only isolated light showers, while "downward growing" nimbostratus is capable of heavier, more extensive precipitation. Towering vertical clouds have the greatest ability to produce intense precipitation events, but these tend to be localized unless organized along fast-moving cold fronts. Showers of moderate to heavy intensity can fall from cumulus congestus clouds. Cumulonimbus, the largest of all cloud genera, has the capacity to produce very heavy showers. Low stratus clouds usually produce only light precipitation, but this always occurs as the feature praecipitatio due to the fact this cloud genus lies too close to the ground to allow for the formation of virga.
Cloud-based supplementary features.
"Incus" is the most type-specific supplementary feature, seen only with cumulonimbus of the species capillatus. A cumulonimbus incus cloud top is one that has spread out into a clear anvil shape as a result of rising air currents hitting the stability layer at the tropopause where the air no longer continues to get colder with increasing altitude.
The "mamma" feature forms on the bases of clouds as downward-facing bubble-like protuberances caused by localized downdrafts within the cloud. It is also sometimes called "mammatus", an earlier version of the term used before a standardization of Latin nomenclature brought about by the World Meterorological Organization during the 20th century. The best-known is cumulonimbus with mammatus, but the mamma feature is also seen occasionally with cirrus, cirrocumulus, altocumulus, altostratus, and stratocumulus.
A "tuba" feature is a cloud column that may hang from the bottom of a cumulus or cumulonimbus. A newly formed or poorly organized column might be comparatively benign, but can quickly intensify into a funnel cloud or tornado.
An "arcus" feature is a roll cloud with ragged edges attached to the lower front part of cumulus congestus or cumulonimbus that forms along the leading edge of a squall line or thunderstorm outflow. A large arcus formation can have the appearance of a dark menacing arch.
There are some arcus-like clouds that form as a consequence of interactions with specific geographical features rather than with a parent cloud. Perhaps the strangest geographically specific cloud of this type is the Morning Glory, a rolling cylindrical cloud that appears unpredictably over the Gulf of Carpentaria in Northern Australia. Associated with a powerful "ripple" in the atmosphere, the cloud may be "surfed" in glider aircraft. It has been officially suggested that roll clouds of this type that are not attached to a parent cloud be reclassified as a new species of stratocumulus, possibly with the Latin name "volutus".
Accessory clouds.
Supplementary cloud formations detached from the main cloud are known as accessory clouds. The heavier precipitating clouds, nimbostratus, towering cumulus (cumulus congestus), and cumulonimbus typically see the formation in precipitation of the "pannus" feature, low ragged clouds of the genera and species cumulus fractus or stratus fractus.
After the pannus types, the remaining accessory clouds comprise formations that are associated mainly with upward-growing cumuliform and cumulonimbiform clouds of free convection. "Pileus" is a cap cloud that can form over a cumulonimbus or large cumulus cloud, whereas a "velum" feature is a thin horizontal sheet that sometimes forms like an apron around the middle or in front of the parent cloud.
Under conditions of strong atmospheric wind shear and instability, wave-like undulatus formations may break into regularly spaced crests. This variant has no separate WMO Latin designation, but is sometimes known informally as a Kelvin-Helmholtz wave cloud. This phenomenon has also been observed in cloud formations over other planets and even in the sun's atmosphere. It has been formally suggested that this wave cloud be classified as a supplementary feature, possibly with the Latin name "fluctus". Another wave-like cloud feature that is distinct from the variety undulatus has been given the Latin name "asperatus". It has been recommended for formal classification as a supplementary feature using its suggested Latin name.
A circular fall-streak hole occasionally forms in a thin layer of supercooled altocumulus or cirrocumulus. Fall streaks consisting of virga or wisps of cirrus are usually seen beneath the hole as ice crystals fall out to a lower altitude. This type of hole is usually larger than typical lacunosus holes, and a formal recommendation has been made to classify it as a supplementary feature, possibly with the Latin name "cavus".
Mother clouds.
Clouds initially form in clear air or become clouds when fog rises above surface level. The genus of a newly formed cloud is determined mainly by air mass characteristics such as stability and moisture content. If these characteristics change over time, the genus tends to change accordingly. When this happens, the original genus is called a "mother cloud". If the mother cloud retains much of its original form after the appearance of the new genus, it is termed a "genitus" cloud. One example of this is "stratocumulus cumulogenitus", a stratocumulus cloud formed by the partial spreading of a cumulus type when there is a loss of convective lift. If the mother cloud undergoes a complete change in genus, it is considered to be a "mutatus" cloud.
It has been officially recommended that the genitus category be expanded to include certain types that do not originate from pre-existing clouds or as the result of any natural atmospheric processes. Among vertically developed clouds, these may include "flammagenitus" for cumulus congestus or cumulonimbus that are formed by large scale fires or volcanic eruptions. Smaller low-étage "pyrocumulus" or "fumulus" clouds formed by contained industrial activity could be classified as cumulus "homogenitus". Contrails formed from the exhaust of aircraft flying in the high étage can persist and spread into formations resembling any of the high cloud genus-types. These variants have no special WMO designations, but are sometimes given the faux-Latin name Aviaticus. Persistent contrails have been identified as candidates for possible inclusion in the genitus category as cirrus, cirrostratus, or cirrocumulus homogenitus
Stratocumulus fields.
Stratocumulus clouds can be organized into "fields" that take on certain specially classified shapes and characteristics. In general, these fields are more discernible from high altitudes than from ground level. They can often be found in the following forms:
Reporting and mapping cloud types.
The international synoptic code (or SYNOP) is used to report weather conditions at regular intervals by professionally trained staff at major weather stations. It provides for reporting of cloud types in any of the three basic étages for tropospheric clouds, but makes no special provision for vertical or multi-level clouds that can occupy more than one étage at a particular time. Consequently, cloud genera with significant vertical development are coded as low when they form in the low étage of the troposphere and achieve vertical extent by growing upward into the middle or high étage, as is the case with cumulus and cumulonimbus. Conversely, nimbostratus is coded as middle because it usually initially forms in the middle étage of the troposphere and becomes vertically developed by growing downward into the low étage, and ofter upward in to the high étage as well. Although the SYNOP code has no separate formal group classification for vertical or multi-level clouds, the observer procedure for selecting numerical codes is designed to give high reporting priority to those genera or species that show significant vertical development. Cloud codes are translated into symbols and plotted on weather maps along with other meteorological data that make up a complete synoptic message.
Formation: how the air becomes saturated.
Cooling air to its dew point.
 
Adiabatic cooling.
This process occurs when one or more of three possible lifting agents���cyclonic/frontal, convective, or orographic—causes air containing invisible water vapor to rise and cool to its dew point, the temperature at which the air becomes saturated. The main mechanism behind this process is adiabatic cooling. Atmospheric pressure decreases with altitude, so the rising air expands in a process that expends energy and causes the air to cool, which reduces its capacity to hold water vapor. If the air is cooled to its dew point and becomes saturated, it normally sheds vapor it can no longer retain, which condenses into cloud. The water droplets in a cloud have a normal radius of about 0.002 mm (0.00008 in). The droplets may collide to form larger droplets which remain aloft as long as the drag force of the air dominates over the gravitational force for small particles.
For non-convective cloud, the altitude at which condensation begins to happen is called the lifted condensation level (LCL), which roughly determines the height of the cloud base. Free convective clouds generally form at the altitude of the convective condensation level (CCL). Water vapor in saturated air is normally attracted to condensation nuclei such as salt particles that are small enough to be held aloft by normal circulation of the air. If the condensation process occurs below the freezing level in the troposphere, the nuclei help transform the vapor into very small water droplets. Clouds that form just above the freezing level are composed mostly of supercooled liquid droplets, while those that condense out at higher altitudes where the air is much colder generally take the form of ice crystals. An absence of sufficient condensation particles at and above the condensation level causes the rising air to become supersaturated and the formation of cloud tends to be inhibited.
Frontal and cyclonic lift.
Frontal and cyclonic lift occur in their purist manifestations when stable air, which has been subjected to little or no surface heating, is forced aloft at weather fronts and around centers of low pressure. Warm fronts associated with extratropical cyclones tend to generate mostly cirriform and stratiform clouds over a wide area unless the approaching warm airmass is unstable, in which case cumulus congestus or cumulonimbus clouds will usually be embedded in the main precipitating cloud layer. Cold fronts are usually faster moving and generate a narrower line of clouds which are mostly stratocumuliform, cumuliform, or cumulonimbiform depending on the stability of the warm air mass just ahead of the front.
Convective lift.
Another agent is the buoyant convective upward motion caused by significant daytime solar heating at surface level, or by relatively high absolute humidity. Incoming short-wave radiation generated by the sun is re-emitted as long-wave radiation when it reaches Earth's surface. This process warms the air closest to ground and increases air mass instability by creating a steeper temperature gradient from warm or hot at surface level to cold aloft. This causes it to rise and cool until temperature equilibrium is achieved with the surrounding air aloft. Moderate instability allows for the formation of cumuliform clouds of moderate size that can produce light showers if the airmass is sufficiently moist. Typical convection upcurrents may allow the droplets to grow to a radius of about 0.015 mm before precipitating as showers. The equivalent diameter of these droplets is about 0.03 mm. If air near the surface becomes extremely warm and unstable, its upward motion can become quite explosive resulting in towering cumulonimbiform clouds that can cause severe weather.
Convective lift can occur in an unstable air mass well away from any fronts. However very warm unstable air can also be present around fronts and low-pressure centers, often producing cumuliform and cumulonimbiform clouds in heavier and more active concentrations because of the combined frontal and convective lifting agents. As with non-frontal convective lift, increasing instability promotes upward vertical cloud growth and raises the potential for severe weather. On comparatively rare occasions, convective lift can be powerful enough to penetrate the tropopause and push the cloud top into the stratosphere.
Orographic lift.
A third source of lift is wind circulation forcing air over a physical barrier such as a mountain (orographic lift). If the air is generally stable, nothing more than lenticular cap clouds will form. However, if the air becomes sufficiently moist and unstable, orographic showers or thunderstorms may appear.
Non-adiabatic cooling.
Along with adiabatic cooling that requires a lifting agent, there are three other main mechanisms for lowering the temperature of the air to its dew point, all of which occur near surface level and do not require any lifting of the air. Conductive, radiational, and evaporative cooling can cause condensation at surface level resulting in the formation of fog. Conductive cooling takes place when air from a relatively mild source area comes into contact with a colder surface, as when mild marine air moves across a colder land area. Radiational cooling occurs due to the emission of infrared radiation, either by the air or by the surface underneath. This type of cooling is common during the night when the sky is clear. Evaporative cooling happens when moisture is added to the air through evaporation, which forces the air temperature to cool to its wet-bulb temperature, or sometimes to the point of saturation.
Adding moisture to the air.
There are five main ways water vapor can be added to the air. Increased vapor content can result from wind convergence over water or moist ground into areas of upward motion. Precipitation or virga falling from above also enhances moisture content. Daytime heating causes water to evaporate from the surface of oceans, water bodies or wet land. Transpiration from plants is another typical source of water vapor. Lastly, cool or dry air moving over warmer water will become more humid. As with daytime heating, the addition of moisture to the air increases its heat content and instability and helps set into motion those processes that lead to the formation of cloud or fog.
Distribution: variable global prevalence and association with planetary latitudinal pressure zones.
Convergence along low-pressure zones.
Although the local distribution of clouds can be significantly influenced by topography, the global prevalence of cloud cover tends to vary more by latitude. It is most prevalent globally in and along low pressure zones of surface atmospheric convergence which encircle the Earth close to the equator and near the 50th parallels of latitude in the northern and southern hemispheres. The adiabatic cooling processes that lead to the creation of clouds by way of lifting agents are all associated with convergence; a process that involves the horizontal inflow and accumulation of air at a given location, as well as the rate at which this happens.<ref name="Convergence/divergence"></ref> Near the equator, increased cloudiness is due to the presence of the low-pressure Intertropical Convergence Zone (ITCZ) where very warm and unstable air promotes mostly cumuliform and cumulonimbiform clouds. Clouds of virtually any type can form along the mid-latitude convergence zones depending on the stability and moisture content of the air. These extratropical convergence zones are occupied by the polar fronts where air masses of polar origin meet and clash with those of tropical or subtropical origin. This leads to the formation of weather-making extratropical cyclones composed of cloud systems that may be stable or unstable to varying degrees according to the stability characteristics of the various airmasses that are in conflict.
Divergence along high pressure zones.
Divergence is the opposite of convergence. In the Earth's atmosphere, it involves the horizontal outflow of air from the upper part of a rising column of air, or from the lower part of a subsiding column often associated with an area or ridge of high pressure. Cloudiness tends to be least prevalent near the poles and in the subtropics close to the 20th parallels, north and south. The latter are sometimes referred to as the horse latitudes. The presence of a large-scale high-pressure subtropical ridge on each side of the equator reduces cloudiness at these low latitudes. Similar patterns also occur at higher latitudes in both hemispheres. After the passage of a cold front or other organized weather disturbance, the sky usually clears as high pressure builds in behind the system, although significant amounts of cumulus or stratocumulus, often in the form of long bands called "cloud streets" may persist if the air mass behind the front remains humid. Small and unchanging amounts of cumulus or cirrus clouds in an otherwise clear sky are usually indications of continuing fair weather as long as the barometric pressure remains comparatively high.
Determination of properties.
Satellites are used to gather data about cloud properties and other information such as Cloud Amount, height, IR emissivity, visible optical depth, effective particle size for both liquid and ice, and cloud top temperature and pressure.
Icing is another property that can be analysed using satellite information. It has been determined that low stratocumulus and stratus can cause icing at a temperature range of 0 to −10 °C. For middle etage altocumulus and altostratus, the range is 0 to −20 °C. Vertical or multi-etage cumulus, cumulonimbus, and nimbostatus, create icing at a range of 0 to −25 °C. High etage cirrus, cirrocumulus, and cirrostratus generally cause no icing because they are made mostly of ice crystals colder that −25 °C.
Polar stratospheric clouds.
Polar stratospheric clouds show little variation in structure and are limited to a single very high range of altitude of about 15000 -, so they are not classified into étages, genus types, species, or varieties in the manner of tropospheric clouds. Instead, the classification is alpha-numeric and is based on chemical makeup rather than variations in physical appearance.
Formation and distribution.
Polar stratospheric clouds form in the lowest part of the statosphere during the winter, at the altitude and during the season that produces the coldest temperatures and therefore the best chances of triggering condensation caused by adiabatic cooling. They are typically very thin with an undulating cirriform appearance. Moisture is scarce in the stratosphere, so nacreous and non-nacreous cloud at this altitude range is rare and is usually restricted to polar regions in the winter where the air is coldest.
Polar mesospheric clouds.
Polar mesospheric clouds form at a single extreme altitude range of about 80 to and are consequently not classified into more than one étage. They are given the Latin name Noctilucent because of their illumination well after sunset and before sunrise. An alpha-numeric classification is used to identify variations in physical appearance.
Formation and distribution.
Polar mesospheric clouds are the highest in the atmosphere and form near the top of the mesosphere at about ten times the altitude of tropospheric high clouds. From ground level, they can occasionally be seen illuminated by the sun during deep twilight. Ongoing research indicates that convective lift in the mesophere is strong enough during the polar summer to cause adiabatic cooling of small amount of water vapour to the point of saturation. This tends to produce the coldest temperatures in the entire atmosphere just below the mesopause. These conditions result in the best environment for the formation of polar mesospheric clouds. There is also evidence that smoke particles from burnt-up meteors provide much of the condensation nuclei required for the formation of noctilucent cloud.
Distribution in the mesosphere is similar to the stratosphere except at much higher altitudes. Because of the need for maximum cooling of the water vapor to produce noctilucent clouds, their distribution tends to be restricted to polar regions of Earth. A major seasonal difference is that convective lift from below the mesosphere pushes very scarce water vapor to higher colder altitudes required for cloud formation during the respective summer seasons in the northern and southern hemispheres. Sightings are rare more than 45 degrees south of the north pole or north of the south pole.
Clouds throughout the homosphere.
Cohesion and dissolution.
There are forces throughout the homosphere (which includes the troposphere, stratosphere, and mesophere) that can impact the structural integrity of a cloud. However, as long as the air remains saturated, the natural force of cohesion that hold the molecules of a substance together acts to keep the cloud from breaking up. Dissolution of the cloud can occur when the process of adiabatic cooling ceases and upward lift of the air is replaced by subsidence. This leads to at least some degree of adiabatic warming of the air which can result in the cloud droplets or crystals turning back into invisible water vapor. Stronger forces such as wind shear and downdrafts can impact a cloud, but these are largely confined to the troposphere where nearly all the Earth's weather takes place.
Luminance and reflectivity.
The luminance or brightness of a cloud is determined by how light is reflected, scattered, and transmitted by the cloud's particles. Its brightness may also be affected by the presence of haze or photometeors such as halos and rainbows. In the troposphere, dense, deep clouds exhibit a high reflectance (70% to 95%) throughout the visible spectrum. Tiny particles of water are densely packed and sunlight cannot penetrate far into the cloud before it is reflected out, giving a cloud its characteristic white color, especially when viewed from the top. Cloud droplets tend to scatter light efficiently, so that the intensity of the solar radiation decreases with depth into the gases. As a result, the cloud base can vary from a very light to very-dark-grey depending on the cloud's thickness and how much light is being reflected or transmitted back to the observer. High thin tropospheric clouds reflect less light because of the comparatively low concentration of constituent ice crystals or supercooled water droplets which results in a slightly off-white appearance. However, a thick dense ice-crystal cloud appears brilliant white with pronounced grey shading because of its greater reflectivity.
As a tropospheric cloud matures, the dense water droplets may combine to produce larger droplets. If the droplets become too large and heavy to be kept aloft by the air circulation, they will fall from the cloud as rain. By this process of accumulation, the space between droplets becomes increasingly larger, permitting light to penetrate farther into the cloud. If the cloud is sufficiently large and the droplets within are spaced far enough apart, a percentage of the light that enters the cloud is not reflected back out but is absorbed giving the cloud a darker look. A simple example of this is one's being able to see farther in heavy rain than in heavy fog. This process of reflection/absorption is what causes the range of cloud color from white to black.
Coloration.
Striking cloud colorations can be seen at many altitudes in the "homosphere". The color of a cloud is usually the same as the incident light.
During daytime when the sun is relatively high in the sky, clouds generally appear bright white on top with varying shades of grey underneath. Thin clouds may look white or appear to have acquired the color of their environment or background. Other colors occur naturally in tropospheric clouds that can tell much about what is going on inside the cloud. Bluish-grey is the result of light scattering within the cloud. In the visible spectrum, blue and green are at the short end of light's visible wavelengths, whereas red and yellow are at the long end. The short rays are more easily scattered by water droplets, and the long rays are more likely to be absorbed. The bluish color is evidence that such scattering is being produced by rain-size droplets in the cloud.
A cumulonimbus cloud that appears to have a greenish/bluish tint is a sign that it contains extremely high amounts of water; hail or rain which scatter light in a way that gives the cloud a blue color. A green colorization occurs mostly late in the day when the sun is comparatively low in the sky and the incident sunlight has a reddish tinge that appears green when illuminating a very tall bluish cloud. Supercell type storms are more likely to be characterized by this but any storm can appear this way. Coloration such as this does not directly indicate that it is a severe thunderstorm, it only confirms its potential. Since a green/blue tint signifies copious amounts of water, a strong updraft to support it, high winds from the storm raining out, and wet hail; all elements that improve the chance for it to become severe, can all be inferred from this. In addition, the stronger the updraft is, the more likely the storm is to undergo tornadogenesis and to produce large hail and high winds.
Yellowish clouds may occur in the late spring through early fall months during forest fire season. The yellow color is due to the presence of pollutants in the smoke. Yellowish clouds caused by the presence of nitrogen dioxide are sometimes seen in urban areas with high air pollution levels.
Within the troposphere, red, orange, and pink clouds occur almost entirely at sunrise/sunset and are the result of the scattering of sunlight by the atmosphere. When the angle between the sun and the horizon is less than 10 percent, as it is just after sunrise or just prior to sunset, sunlight becomes too red due to refraction for any colors other than those with a reddish hue to be seen. The clouds do not become that color; they are reflecting long and unscattered rays of sunlight, which are predominant at those hours. The effect is much like if one were to shine a red spotlight on a white sheet. In combination with large, mature thunderheads, this can produce blood-red clouds. Clouds look darker in the near-infrared because water absorbs solar radiation at those wavelengths. When the sun is just below the horizon, low-etage clouds are gray, middle clouds appear rose-colored, and high-etage clouds are white or off-white.
Clouds at night are black or dark grey in a moonless sky, or whitish when illuminated by the moon. They may also reflect the colors of large fires, city lights, or auroras that might be present.
In high latitude regions of the stratosphere, nacreous clouds occasionally found there during the polar winter tend to display quite striking displays of mother-of-pearl colorations. This is due to the refraction and diffusion of the sun's rays through thin clouds with supercooled droplets that often contain compounds other than water. At still higher altitudes up in the mesospere, noctilucent clouds made of ice crystals are sometimes seen in polar regions in the summer. They typically have a bluish or silvery white coloration that can resemble brightly illuminated cirrus. Noctilucent clouds may occasionally take on more of a red or orange hue.
Effects on climate and the atmosphere.
The role of tropospheric clouds in regulating weather and climate remains a leading source of uncertainty in projections of global warming. This uncertainty arises because of the delicate balance of processes related to clouds, spanning scales from millimeters to planetary. Hence, interactions between the large-scale (synoptic meteorology) and clouds becomes difficult to represent in global models. The complexity and diversity of clouds, as outlined above, adds to the problem. On the one hand, white-colored cloud tops promote cooling of Earth's surface by reflecting "short-wave" radiation from the sun. Most of the sunlight that reaches the ground is absorbed, warming the surface, which emits radiation upward at longer, "infrared", wavelengths. At these wavelengths, however, water in the clouds acts as an efficient absorber. The water reacts by radiating, also in the infrared, both upward and downward, and the downward "long-wave" radiation results in some warming at the surface. This is analogous to the greenhouse effect of greenhouse gases and water vapor.
High-étage tropospheric genus-types, "cirrus", "cirrocumulus", and "cirrostratus", particularly show this duality with both short-wave albedo cooling and long-wave greenhouse warming effects. On the whole though, "ice-crystal" clouds in the upper troposphere tend to favor net "warming". However, the "cooling" effect is dominant with low-étage stratocumuliform and stratiform clouds made of very small "water droplets" that have an average radius of about 0.002 mm (0.00008 in)., especially when they form in extensive sheets that block out more of the sun. These include middle-étage layers of "altocumulus" and "altostratus" as well as low "stratocumulus", and "stratus". Small-droplet aerosols are not good at absorbing long-wave radiation reflected back from Earth, so there is a net cooling with almost no long-wave effect. This effect is particularly pronounced with low-étage clouds that form over water. Low and vertical heaps of "cumulus", "towering cumulus", and "cumulonimbus" are made of larger water droplets ranging in radius from 0.005 to about 0.015 mm. Nimbostratus cloud droplets can also be quite large, up to 0.015mm radius. These larger droplets associated with vertically developed clouds are better able to trap the long-wave radiation thus migitating the cooling effect to some degree. However, these large often precipitating clouds are variable or unpredictable in their overall effect because of variations in their concentration, distribution, and vertical extent. Measurements taken by NASA indicate that on the whole, the effects of low and middle étage clouds that tend to promote cooling are outweighing the warming effects of high layers and the variable outcomes associated with multi-level or vertically developed clouds.
As difficult as it is to evaluate the effects of current cloud cover characteristics on climate change, it is even more problematic to predict the outcome of this change with respect to future cloud patterns and events. As a consequence, much research has focused on the response of low and vertical clouds to a changing climate. Leading global models can produce quite different results, however, with some showing increasing low-étage clouds and others showing decreases.
In the stratosphere, Type I non-nacreous clouds are known to have harmful effects over the polar regions of Earth. They become catalysts which convert relatively benign man-made chlorine into active free radicals like chlorine monoxide which are destructive of the stratospheric ozone layer.
Polar mesospheric clouds are not common or widespread enough to have a significant effect on climate. However, an increasing frequency of occurrence of noctilucent clouds since the 19th century may be the result of climate change.
Global brightening.
New research indicates a global brightening trend. The details are not fully understood, but much of the global dimming (and subsequent reversal) is thought to be a consequence of changes in aerosol loading in the atmosphere, especially sulfur-based aerosol associated with biomass burning and urban pollution. Changes in aerosol burden can have indirect effects on clouds by changing the droplet size distribution or the lifetime and precipitation characteristics of clouds.
Extraterrestrial.
Cloud cover has been seen on most other planets in the solar system. Venus's thick clouds are composed of sulfur dioxide and appear to be almost entirely stratiform. They are arranged in three main layers at altitudes of 45 to 65 km that obscure the planet's surface and can produce virga. No embedded cumuliform types have been identified, but broken stratocumuliform wave formations are sometimes seen in the top layer that reveal more continuous layer clouds underneath. On Mars, cirrus, cirrocumulus and stratocumulus composed of water-ice have been detected mostly near the poles. Water-ice fogs have also been detected on this planet.
Both Jupiter and Saturn have an outer cirriform cloud deck composed of ammonia, an intermediate stratiform haze-cloud layer made of ammonium hydrosulfide, and an inner deck of cumulus water clouds. Embedded cumulonimbus are known to exist near the Great Red Spot on Jupiter. The same category-types can be found covering Uranus, and Neptune, but are all composed of Methane. Saturn's moon Titan has cirrus clouds believed to be composed largely of methane. The Cassini–Huygens Saturn mission uncovered evidence of a fluid cycle on Titan, including lakes near the poles and fluvial channels on the surface of the moon.
In October 2013, the detection of high altitude optically thick clouds in the atmosphere of Kepler-7b was announced, and, in December 2013, also in the atmospheres of GJ 436 b and GJ 1214 b.

</doc>
<doc id="47517" url="http://en.wikipedia.org/wiki?curid=47517" title="Planetary body">
Planetary body

A planetary body or planetary object is any secondary body in the Solar System that is geologically differentiated or in hydrostatic equilibrium and thus has a planet-like geology: a planet, dwarf planet, or the larger moons and asteroids.

</doc>
<doc id="47518" url="http://en.wikipedia.org/wiki?curid=47518" title="Cloud feedback">
Cloud feedback

Cloud feedback is the coupling between cloudiness and surface air temperature in which a change in radiative forcing perturbs the surface air temperature, leading to a change in clouds, which could then amplify or diminish the initial temperature perturbation.
Global warming is expected to change the distribution and type of clouds. Seen from below, clouds emit infrared radiation back to the surface, and so exert a warming effect; seen from above, clouds reflect sunlight and emit infrared radiation to space, and so exert a cooling effect. Cloud representations vary among global climate models, and small changes in cloud cover have a large impact on the climate. Differences in planetary boundary layer cloud modeling schemes can lead to large differences in derived values of climate sensitivity. A model that decreases boundary layer clouds in response to global warming has a climate sensitivity twice that of a model that does not include this feedback. However, satellite data show that cloud optical thickness actually increases with increasing temperature. Whether the net effect is warming or cooling depends on details such as the type and altitude of the cloud; details that are difficult to represent in climate models.
In addition to how clouds themselves will respond to increased temperatures, other feedbacks affect clouds properties and formation. The amount and vertical distribution of water vapor is closely linked to the formation of clouds. Ice crystals have been shown to largely influence the amount of water vapor. Water vapor in the subtropical upper troposphere has been linked to the convection of water vapor and ice. Changes in subtropical humidity could provide a negative feedback that decreases the amount of water vapor which in turn would act to mediate global climate transitions.
Changes in cloud cover are closely coupled with other feedback, including the water vapor feedback and ice-albedo feedback. Changing climate is expected to alter the relationship between cloud ice and supercooled cloud water, which in turn would influence the microphysics of the cloud which would result in changes in the radiative properties of the cloud. Climate models suggest that a warming will increase fractional cloudiness. Increased cloudiness cools the climate, resulting in a negative feedback. Increasing temperatures in the polar regions is expected in increase the amount of low-level clouds, whose stratification prevents the convection of moisture to upper levels. This feedback would partially cancel the increased surface warming due to the cloudiness. This negative feedback has less effect than the positive feedback. The upper atmosphere more than cancels negative feedback that causes cooling, and therefore the increase of CO2 is actually exacerbating the positive feedback. Therefore as the cited paper notes, Global Warming will continue unabated as more CO2 enters the system

</doc>
<doc id="47519" url="http://en.wikipedia.org/wiki?curid=47519" title="Cloud forcing">
Cloud forcing

Cloud forcing (sometimes described as cloud radiative forcing) is, in meteorology, the difference between the radiation budget components for average cloud conditions and cloud-free conditions. Much of the interest in cloud forcing relates to its role as a feedback process in the present period of global warming. 
All global climate models used for climate change projections include the effects of water vapor and cloud forcing. The models include the effects of clouds on both incoming (solar) and emitted (terrestrial) radiation.
Clouds increase the global reflection of solar radiation from 15% to 30%, reducing the amount of solar radiation absorbed by the Earth by about 44 W/m². This cooling is offset somewhat by the greenhouse effect of clouds which reduces the outgoing longwave radiation by about 31 W/m². Thus the net cloud forcing of the radiation budget is a loss of about 13 W/m². If the clouds were removed with all else remaining the same, the Earth would gain this last amount in net radiation and begin to warm up. These numbers should not be confused with the usual radiative forcing concept, which is for the "change" in forcing related to climate change. 
Without the inclusion of clouds, water vapor alone contributes 36% to 70% of the greenhouse effect on Earth. When water vapor and clouds are considered together, the contribution is 66% to 85%. The ranges come about because there are two ways to compute the influence of water vapor and clouds: the lower bounds are the reduction in the greenhouse effect if water vapor and clouds are "removed" from the atmosphere leaving all other greenhouse gases unchanged, while the upper bounds are the greenhouse effect introduced if water vapor and clouds are "added" to an atmosphere with no other greenhouse gases. The two values differ because of overlap in the absorption and emission by the various greenhouse gases. Trapping of the long-wave radiation due to the presence of clouds reduces the radiative forcing of the greenhouse gases compared to the clear-sky forcing. However, the magnitude of the effect due to clouds varies for different greenhouse gases. Relative to clear skies, clouds reduce the global mean radiative forcing due to CO2 by about 15%, that due to CH4 and N2O by about 20%, and that due to the halocarbons by up to 30%. 
Clouds remain one of the largest uncertainties in future projections of climate change by global climate models, owing to the physical complexity of cloud processes and the small scale of individual clouds relative to the size of the model computational grid.

</doc>
<doc id="47520" url="http://en.wikipedia.org/wiki?curid=47520" title="Coccolithophore">
Coccolithophore

A coccolithophore (the adjective coccolithophorid is often incorrectly used as if it were a noun) is a unicellular, eukaryotic phytoplankton (alga). They belong either to the kingdom Protista, according to Robert Whittaker's Five kingdom classification, or Chromalveolata, according to the newer Thomas Cavalier-Smith Biological Classification system. Within the Chromalveolata, the coccolithophorids are in the phylum or division Haptophyta, class Coccolithophyceae. Coccolithophorids are distinguished by special calcium carbonate plates (or scales) of uncertain function called "coccoliths", which are also important microfossils. However, there are Coccolithophyceae species lacking coccoliths (e.g. in genus "Prymnesium"), so not every member of Coccolithophyceae (Prymnesiophyceae) is coccolithophorid. Coccolithophores are almost exclusively marine and are found in large numbers throughout the sunlight zone of the ocean.
The most abundant species of coccolithophore, "Emiliana huxleyi", belongs to the order Isochrysidales and family Noëlaerhabdaceae. It is found in temperate, subtropical, and tropical oceans. This makes "E. huxleyi" an important part of the planktonic base of a large proportion of marine food webs. It is also the fastest growing coccolithophore in laboratory cultures. It is studied for the extensive blooms it forms in nutrient depleted waters after the reformation of the summer thermocline. and for its production of a group resistant alkenones commonly used by earth scientists as a means to estimate past sea surface temperatures. Coccolithophores are of particular interest to those studying global climate change because as ocean acidity increases, their coccoliths may become even more important as a carbon sink. Furthermore, management strategies are being employed to prevent eutrophication-related coccolithophore blooms, as these blooms lead to a decrease in nutrient flow to lower levels of the ocean.
Structure.
Coccolithophores are spherical cells about 5–100 micrometres across, enclosed by calcareous plates called coccoliths, which are about 2–25 micrometres across. Each cell contains two brown chloroplasts which surround the nucleus.
Exoskeleton (coccosphere).
Each unicellular plankton is enclosed in its own collection of coccoliths, the calcified scales, which make up its exoskeleton or coccosphere. The coccoliths are created inside the cell and while some species maintain a single layer throughout life only producing new coccoliths as the cell grows, others continually produce and shed coccoliths.
Composition.
The primary constituent of coccoliths is calcium carbonate, or chalk. Calcium carbonate is transparent, so the organisms’ photosynthetic activity is not compromised by encapsulation in a coccosphere.
Formation.
Coccoliths are produced by a biomineralization process known as coccolithogenesis. Generally, calcification of coccoliths occurs in the presence of light, and these scales are produced much more during the exponential phase of growth than the stationary phase. Although not yet entirely understood, the biomineralization process is tightly regulated by calcium signaling. Calcite formation begins in the golgi complex where protein templates nucleate the formation of CaCO3 crystals and complex acidic polysaccharides control the shape and growth of these crystals. As each scale is produced, it is exported in a Golgi-derived vesicle and added to the inner surface of the coccosphere. This means that the most recently produced coccoliths may lie beneath older coccoliths.
Depending upon the phytoplankton’s stage in the life cycle, two different types of coccoliths may be formed. Holococcoliths are produced only in the haploid phase, lack radial symmetry, and are composed of anywhere from hundreds to thousands of similar minute (ca 0.1 µm) rhombic calcite crystals. These crystals are thought to form at least partially outside the cell. Heterococcoliths occur only in the diploid phase, have radial symmetry, and are composed of relatively few complex crystal units (<100) . Although they are rare, combination coccospheres, which contain both holococcoliths and heterococcoliths, have been observed in the plankton recording coccolithophore life cycle transitions. Finally, the coccospheres of some species are highly modified with various appendages made of specialized coccoliths.
Function.
While the exact function of the coccosphere is unclear, many potential functions have been proposed. Most obviously coccoliths may protect the phytoplankton from predators. In addition, these exoskeletons may confer an advantage in energy production, as coccolithogenesis seems highly coupled with photosynthesis. Organic precipitation of calcium carbonate from bicarbonate solution produces free carbon dioxide directly within the cellular body of the alga, this additional source of gas is then available to the Coccolithophore for photosynthesis. It has been suggested that they may provide a cell-wall like barrier to isolate intracellular chemistry from the marine environment. More specific, defensive properties of coccoliths may include protection from osmotic changes, chemical or mechanical shock, and short-wavelength light. It has also been proposed that the added weight of multiple layers of coccoliths allows the organism to sink to lower, more nutrient rich layers of the water and conversely, that coccoliths add buoyancy, stopping the cell from sinking to dangerous depths. Coccolith appendages have also been proposed to serve several functions, such as inhibiting grazing by zooplankton.
Uses.
Coccoliths are the main component of the Chalk, a Late Cretaceous rock formation which outcrops widely in southern England and forms the White Cliffs of Dover, and of other similar rocks in many other parts of the world. At the present day sedimented coccoliths are a major component of the calcareous oozes that cover up to 35% of the ocean floor and is kilometres thick in places. Because of their abundance and wide geographic ranges, the coccoliths which make up the layers of this ooze and the chalky sediment formed as it is compacted serve as valuable microfossils.
Cellular anatomy.
Enclosed in each coccosphere is a single cell with membrane bound organelles. Two large chloroplasts with brown pigment are located on either side of the cell and surround the nucleus, mitochondria, golgi apparatus, endoplasmic reticulum, and other organelles. Each cell also has two flagellar structures, which are involved not only in motility, but also in mitosis and formation of the cytoskeleton. In some species, a functional or vestigial haptonema is also present. This structure, which is unique to haptophytes, coils and uncoils in response to environmental stimuli. Although poorly understood, it has been proposed to be involved in prey capture. 
Ecology.
Life history strategy.
The life cycle of coccolithophores is characterized by an alternation of diploid and haploid phases. They alternate from the haploid to diploid phase through syngamy and from diploid to haploid through meiosis. In contrast with most organisms with alternating life cycles, asexual reproduction by mitosis is possible in both phases of the life cycle. Both abiotic and biotic factors may affect the frequency with which each phase occurs.
Coccolithophores reproduce asexually through binary fission. In this process the coccoliths from the parent cell are divided between the two daughter cells. There have been suggestions stating the possible presence of a sexual reproduction process due to the diploid stages of the coccolithophores, but this process has never been observed.
K or r- selected strategies of coccolithophores depend on their life cycle stage. When coccolithophores are diploid, they are r-selected. In this phase they tolerate a wider range of nutrient compositions. When they are haploid they are K- selected and are often more competitive in stable low nutrient environments. Most coccolithophores are K strategist and are usually found on nutrient-poor surface waters. They are poor competitors when compared to other phytoplankton and thrive in habitats where other phytoplankton would not survive.
These two stages in the life cycle of coccolithophores occur seasonally, where more nutrition is available in warmer seasons and less is available in cooler seasons. This type of life cycle is known as a complex heteromorphic life cycle.
Global distribution.
Coccolithophores occur throughout the world ocean. Their distribution varies vertically by stratified layers in the ocean and geographically by different temporal zones. While most modern coccolithophores can be located in their associated stratified oligotrophic conditions, the most abundant areas of coccolithophores where there is the highest species diversity are located in subtropical zones with a temperate climate. While water temperature and the amount of light intensity entering the water’s surface are the more influential factors in determining where species are located, the ocean currents also can determine the location where certain species of coccolithophores are found.
Although motility and colony formation vary according to the life cycle of different coccolithophore species, there is often alternation between a motile, haploid phase, and a non-motile diploid phase. In both phases, the organism’s dispersal is largely due to ocean currents and circulation patterns.
Within the Pacific Ocean, approximately 90 species have been identified with six separate zones relating to different Pacific currents that contain unique groupings of different species of coccolithophores. The highest diversity of coccolithophores in the Pacific Ocean was in an area of the ocean considered the Central North Zone which is an area between 30 oN and 5 oN, composed of the North Equatorial Current and the Equatorial Countercurrent. These two currents move in opposite directions, east and west, allowing for a strong mixing of waters and allowing a large variety of species to populate the area.
In the Atlantic Ocean, the most abundant species are "E. huxleyi" and "Florisphaera profunda" with smaller concentrations of the species "Umbellosphaera" "irregularis", "Umbellosphaera tenuis" and different species of "Gephyrocapsa". Deep-dwelling coccolithophore species abundance is greatly affected by nutricline and thermocline depths. These coccolithophores increase in abundance when the nutricline and thermocline are deep and decrease when they are shallow.
The complete distribution of coccolithophores is currently not known and some regions, such as the Indian Ocean, are not as well known as other locations in the Pacific and Atlantic Oceans. It is also very hard to explain distributions due to multiple constantly changing factors involving the ocean properties, such as coastal and equatorial upwelling, frontal systems, benthic environments, unique oceanic topography, and pockets of isolated high or low water temperatures.
Table 1 and Table 2 show simplified specific species preferences based on geographic location and water depth respectively. These do not contain all coccolithophore species and are just a simplification of major, more well-known species.
The upper photic zone is low in nutrient concentration, high in light intensity and penetration, and usually higher in temperature. The lower photic zone is high in nutrient concentration, low in light intensity and penetration and relatively cool. The middle photic zone is an area that contains the same values in between that of the lower and upper photic zones.
Effect of global climate change on distribution and abundance.
Recent studies show that climate change has direct and indirect impacts on Coccolithophore distribution and productivity. They will inevitably be affected by the increasing temperatures and thermal stratification of the top layer of the ocean, since these are prime controls on their ecology, although it is not clear whether global would result in net increase or decrease of coccolithophores. It has been suggested that since they calcify ocean acidification due to increasing carbon dioxide could severely affect coccolithophores.
Role in the food web.
Coccolithophores are one of the more abundant primary producers in the ocean. As such, they are a large contributor to the primary productivity of the tropical and subtropical oceans, however, exactly how much has yet to have been recorded.
Dependence on nutrients.
The ratio between the concentrations of nitrogen, phosphorus and silicate in particular areas of the ocean dictates competitive dominance within phytoplankton communities. Each ratio essentially tips the odds in favor of either diatoms or more other groups of phytoplankton, such as coccolithophores. A low silicate to nitrogen and phosphorus ratio allows coccolithophores to outcompete other phytoplankton species; however, when silicate to phosphorus to nitrogen ratios are high coccolithophores are outcompeted by diatoms. The increase in agricultural processes lead to eutrophication of waters and thus, coccolithophore blooms in these high nitrogen and phosphorus, low silicate environments.
Impact on water column productivity.
The calcite in calcium carbonate allows coccoliths to scatter more light than they absorb. This has two important consequences: 1) Surface waters become brighter, meaning they have a higher albedo, and 2) there is induced photoinhibition, meaning deeper waters become darker. Hence, a high concentration of coccoliths leads to a simultaneous increase in surface water temperature and decrease in the temperature of deeper waters. This results in more stratification in the water column and a decrease in the vertical mixing of nutrients. However, a recent study estimated that the overall effect of coccolithophores on the increased in radiative forcing of the ocean is less than that from anthropogenic factors. Therefore, the overall result of large blooms of coccolithophores is a decrease in water column productivity, rather than a contribution to global warming.
Predator-prey interactions.
Their predators include the common predators of all phytoplankton including small fish, zooplankton, and shellfish larvae. Viruses specific to this species have been isolated from several locations worldwide and appear to play a major role in spring bloom dynamics.
Toxicity.
No environmental evidence of coccolithophore toxicity has been reported, but they belong to the class Prymnesiophyceae which contain orders with toxic species. Toxic species have been found in the genera "Prymnesium" Massart and "Chrysochromulina" Lackey. Members of the genus "Prymnesium" have been found to produce haemolytic compounds, the agent responsible for toxicity. Some of these toxic species are responsible for large fish kills and can be accumulated in organisms such as shellfish; transferring it through the food chain. In laboratory tests for toxicity members of the oceanic coccolithophore genera "Emiliania, Gephyrocapsa, Calcidiscus" and "Coccolithus" were shown to be non-toxic as were species of the coastal genus "Hymenomonas", however several species of "Pleurochrysis" and "Jomonlithus", both coastal genera were toxic to "Artemia".
Community interactions.
Coccolithophorids are predominantly found as single, free-floating haploid or diploid cells.
Competition.
Most phytoplankton need sunlight and nutrients from the ocean to survive, so they thrive in areas with large inputs of nutrient rich water upwelling from the lower levels of the ocean. Most coccolithophores, only require sunlight for energy production and have a higher ratio of nitrate uptake over ammonium uptake (nitrogen is required for growth and can be used directly from nitrate but not ammonium). Because of this they thrive in still, nutrient-poor environments where other phytoplankton are starving. Trade-offs associated with these faster growth rates, however, include a smaller cell radius and lower cell volume than other types of phytoplankton.
Viral infection and coevolution.
Giant DNA-containing viruses are known to lytically infect coccolithophores, particularly "E. huxleyi". These viruses, known as E. huxleyi viruses (EhVs), appear to infect the coccosphere coated diploid phase of the life cycle almost exclusively. It has been proposed that as the haploid organism is not infected and therefore not affected by the virus, the co-evolutionary “arms race” between coccolithophores and these viruses does not follow the classic Red Queen evolutionary framework, but instead a “Cheshire Cat” ecological dynamic. More recent work has suggested that viral synthesis of sphingolipids and induction of programmed cell death provides a more direct link to study a Red Queen-like coevolutionary arms race at least between the coccolithoviruses and diploid organism.
Importance in global climate change.
Impact on the carbon cycle.
Coccolithophores have both long and short term effects on the carbon cycle. The production of coccoliths requires the uptake of dissolved inorganic carbon and calcium. Calcium carbonate and carbon dioxide are produced from calcium and bicarbonate by the following chemical reaction:
Because coccolithophores are photosynthetic organisms, they are able to use some of the CO2 released in the calcification reaction for photosynthesis.
However, the production of calcium carbonate drives surface alkalinity down, and in conditions of low alkalinity the CO2 is instead released back into the atmosphere.
As a result of this, researchers have postulated that large blooms of coccolithophores may contribute to global warming in the short term. A more widely accepted idea, however, is that over the long term coccolithophores contribute to an overall decrease in atmospheric CO2 concentrations. During calcification two carbon atoms are taken up and one of them becomes trapped as calcium carbonate. This calcium carbonate sinks to the bottom of the ocean in the form of coccoliths and becomes part of sediment; thus, coccolithophores provide a sink for emitted carbon, mediating the effects of greenhouse gas emissions.
Evolutionary responses to ocean acidification.
Research also suggests that ocean acidification due to increasing concentrations of CO2 in the atmosphere may affect the calcification machinery of coccolithophores. This may not only affect immediate events such as increases in population or coccolith production, but also may induce evolutionary adaptation of coccolithophore species over longer periods of time. For example, coccolithophores use H+ ion channels in to constantly pump H+ ions out of the cell during coccolith production. This allows them to avoid acidosis, as coccolith production would otherwise produce a toxic excess of H+ ions. When the function of these ion channels is disrupted, the coccolithophores stop the calcification process to avoid acidosis, thus forming a feedback loop. Low ocean alkalinity, impairs ion channel function and therefore places evolutionary selective pressure on coccolithophores and makes them (and other ocean calcifiers) vulnerable to ocean acidification. In 2008, field evidence indicating an increase in in calcification of newly formed ocean sediments containing coccolithophores bolstered the first ever experimental data showing that an increase in ocean CO2 concentration results in an increase in calcification of these organisms.
Decreasing coccolith mass is related to both the increasing concentrations of CO2 and decreasing concentrations of CO3 in the world’s oceans. This lower calcification is assumed to put coccolithophores at ecological disadvantage. Some species like "Calcidiscus" "leptoporus", however, are not affected in this way, while the most abundant coccolithophore species, "E. huxleyi" is. Also, highly calcified coccolithophorids have been found in conditions of low CO−3 contrary to predictions. Understanding the effects of increasing ocean acidification on coccolithophore species is absolutely essential to predicting the future chemical composition of the ocean, particularly its carbonate chemistry. Viable conservation and management measures will come from future research in this area. Groups like the European-based CALMARO are monitoring the responses of coccolithophore populations to varying pH’s and working to determine environmentally sound measures of control.
Impact on nannofossil record.
Coccolith fossils are prominent and valuable calcareous nannofossils (see Micropaleontology). Of particular interest are fossils dating back to the Palaeocene-Eocene Thermal Maximum 55 million years ago. This period is thought to correspond most directly to the current levels of CO2 in the ocean. Finally, field evidence of coccolithophore fossils in rock were used to show that the deep-sea fossil record bears a rock record bias similar to the one that is widely accepted to affect the land-based fossil record.
Impact on the oceans.
The coccolithophorids help in regulating the temperature of the oceans. They thrive in warm seas and release DMS (demethyl sulphide) into the air whose nuclei help to produce thicker clouds to block the sun. When the oceans cool, the number of coccolithophorids decrease and the amount of clouds also decrease. When there are fewer clouds blocking the sun, the temperature also rises. This, therefore, maintains the balance and equilibrium of nature.
Modern Culture.
The coccolithophores are featured on the new design for the 500 Kroner bank note.
External links.
Sources of detailed information
Introductions to coccolithophores

</doc>
<doc id="47521" url="http://en.wikipedia.org/wiki?curid=47521" title="Condensation">
Condensation

Condensation is the change of the physical state of matter from gas phase into liquid phase, and is the reverse of evaporation. Mostly refers to the water cycle. It can also be defined as the change in the state of water vapor to liquid water when in contact with a liquid or solid surface or cloud condensation nuclei within the atmosphere. When the transition happens from the gaseous phase into the solid phase directly, the change is called deposition.
Initiation.
Condensation is initiated by the formation of atomic/molecular clusters of that species within its gaseous volume—like rain drop or snow flake formation within clouds—or at the contact between such gaseous phase and a liquid or solid surface.
Reversibility scenarios.
A few distinct reversibility scenarios emerge here with respect to the nature of the surface.
Most common scenarios.
Condensation commonly occurs when a vapor is cooled and/or compressed to its saturation limit when the molecular density in the gas phase reaches its maximal threshold. Vapor cooling and compressing equipment that collects condensed liquids is called a "condenser".
How condensation is measured.
Psychrometry measures the rates of condensation from and evaporation into the air moisture at various atmospheric pressures and temperatures. Water is the product of its vapor condensation—condensation is the process of such phase conversion.
Applications of condensation.
Condensation is a crucial component of distillation, an important laboratory and industrial chemistry application.
Because condensation is a naturally occurring phenomenon, it can often be used to generate water in large quantities for human use. Many structures are made solely for the purpose of collecting water from condensation, such as air wells and fog fences. Such systems can often be used to retain soil moisture in areas where active desertification is occurring—so much so that some organizations educate people living in affected areas about water condensers to help them deal effectively with the situation.
It is also a crucial process in forming particle tracks in a cloud chamber. In this case, ions produced by an incident particle act as nucleation centers for the condensation of the vapor producing the visible "cloud" trails.
Biological adaptation.
Numerous living beings use water made accessible by condensation. A few examples of these are the Australian Thorny Devil, the darkling beetles of the Namibian coast, and the Coast Redwoods of the West Coast of the United States.
Condensation in building construction.
Condensation in building construction is an unwanted phenomenon as it may cause dampness, mold health issues, wood rot, corrosion and energy penalties due to increased heat transfer. To alleviate these issues, the indoor air humidity needs to be lowered, or air ventilation in the building needs to be improved. This can be done in a number of ways; opening windows, turning on extractor fans, using dehumidifiers, drying clothes outside and covering pots and pans whilst cooking to name a few. Air conditioning or ventilation systems can be installed that help remove moisture from the air, and move air throughout a building. The amount of water vapour that can be stored in the air can be increased simply by increasing the temperature
Interstructure condensation may be caused by thermal bridges, insufficient or lacking insulation, damp proofing or insulated glazing.

</doc>
