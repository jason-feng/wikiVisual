<doc id="42856" url="http://en.wikipedia.org/wiki?curid=42856" title="The Bridge on the River Kwai">
The Bridge on the River Kwai

The Bridge on the River Kwai is a 1957 World War II epic film directed by David Lean, based on the novel "Le Pont de la Rivière Kwai" (1952) by Pierre Boulle. The film is a work of fiction but borrows the construction of the Burma Railway in 1942–43 for its historical setting. It stars William Holden, Jack Hawkins, Alec Guinness and Sessue Hayakawa. The movie was filmed in Ceylon (now known as Sri Lanka). The bridge in the film was located near Kitulgala.
The film was widely praised, winning seven Academy Awards (including Best Picture) at the 30th Academy Awards; in 1997 this film was deemed "culturally, historically, or aesthetically significant" and selected for preservation in the United States Library of Congress National Film Registry. It is widely considered to be one of the greatest films in history.
Plot.
In World War II, British prisoners arrive by train at a Japanese prison camp in Thailand. The commandant, Colonel Saito (Sessue Hayakawa), informs them that all prisoners, regardless of rank, are to work on the construction of a railway bridge over the River Kwai that will connect Bangkok and Rangoon. The senior British officer, Lieutenant Colonel Nicholson (Alec Guinness), reminds Saito that the Geneva Conventions exempt officers from manual labour.
At the following morning's assembly, Nicholson orders his officers to remain behind when the enlisted men are sent off to work. Saito slaps him across the face with his copy of the conventions and threatens to have them shot, but Nicholson refuses to back down. When Major Clipton (James Donald), the British medical officer, intervenes, Saito leaves the officers standing all day in the intense tropical heat. That evening, the officers are placed in a punishment hut, while Nicholson is locked in an iron box.
Meanwhile, three prisoners attempt to escape. Two are shot dead, but United States Navy Commander Shears (William Holden), gets away, although badly wounded. He stumbles into a village. The villagers help him escape by boat.
Nicholson refuses to compromise. Meanwhile, the prisoners are working as little as possible and sabotaging whatever they can. Should Saito fail to meet his deadline, he would be obliged to commit ritual suicide. Desperate, Saito uses the anniversary of Japan's victory in the Russo-Japanese War as an excuse to save face and announces a general amnesty, releasing Nicholson and his officers.
Nicholson conducts an inspection and is shocked by the poor job being done by his men. Over the protests of some of his officers, he orders Captain Reeves (Peter Williams) and Major Hughes (John Boxer) to design and build a proper bridge, despite its military value to the Japanese, for the sake of maintaining his men's morale. The Japanese engineers had chosen a poor site, so the original construction is abandoned and a new bridge is begun downstream.
Shears is enjoying his hospital stay in Ceylon, when British Major Warden (Jack Hawkins) asks him to volunteer for a commando mission to destroy the bridge before it's completed. Shears is appalled at the idea and reveals that he is not an officer at all. He switched uniforms with a dead officer after the sinking of their cruiser, USS Houston (CL-30), as a ploy to get better treatment. Warden already knows this. Faced with the prospect of being charged with impersonating an officer, Shears volunteers.
Meanwhile, Nicholson drives his men hard to complete the bridge on time. For him, its completion will exemplify the ingenuity and hard work of the British Army for generations. When he asks that their Japanese counterparts join in as well, a resigned Saito replies that he has already given the order.
The commandos parachute in, with one man killed on landing. Later, Warden is wounded in an encounter with a Japanese patrol and has to be carried on a litter. He, Shears, and Canadian Lieutenant Joyce (Geoffrey Horne) reach the river in time with the assistance of Siamese women bearers and their village chief, Khun Yai. Under cover of darkness, Shears and Joyce plant explosives on the bridge towers below the water line.
A train carrying soldiers and important dignitaries is scheduled to be the first to cross the bridge the following day, so Warden waits to destroy both. However, at daybreak the commandos are horrified to see that the water level has dropped, exposing the wire connecting the explosives to the detonator. Making a final inspection, Nicholson spots the wire and brings it to Saito's attention. As the train is heard approaching, they hurry down to the riverbank to investigate.
Joyce, manning the detonator, breaks cover and stabs Saito to death. Aghast, Nicholson yells for help, while attempting to stop Joyce from reaching the detonator. When Joyce is shot dead by Japanese fire, Shears swims across the river, but is fatally wounded as he reaches Nicholson. Recognizing the dying Shears, Nicholson exclaims, "What have I done?" Warden fires his mortar, mortally wounding Nicholson. The dazed colonel stumbles towards the detonator and collapses on the plunger, just in time to blow up the bridge and send the train hurtling into the river below. Witnessing the carnage, Clipton shakes his head uttering, "Madness! ... Madness!"
Historical parallels.
The largely fictional film plot is loosely based on the building in 1943 of one of the railway bridges over the Mae Klong—renamed Khwae Yai in the 1960s—at a place called Tha Ma Kham, five kilometres from the Thai town of Kanchanaburi.
According to the Commonwealth War Graves Commission:
"The notorious Burma-Siam railway, built by Commonwealth, Dutch and American prisoners of war, was a Japanese project driven by the need for improved communications to support the large Japanese army in Burma. During its construction, approximately 13,000 prisoners of war died and were buried along the railway. An estimated 80,000 to 100,000 civilians also died in the course of the project, chiefly forced labour brought from Malaya and the Dutch East Indies, or conscripted in Siam (Thailand) and Burma. Two labour forces, one based in Siam and the other in Burma, worked from opposite ends of the line towards the centre."
The incidents portrayed in the film are mostly fictional, and though it depicts bad conditions and suffering caused by the building of the Burma Railway and its bridges, historically the conditions were much worse than depicted. The real senior Allied officer at the bridge was British Lieutenant Colonel Philip Toosey. Some consider the film to be an insulting parody of Toosey. On a BBC "Timewatch" programme, a former prisoner at the camp states that it is unlikely that a man like the fictional Nicholson could have risen to the rank of lieutenant colonel; and if he had, due to his collaboration he would have been "quietly eliminated" by the other prisoners. Julie Summers, in her book "The Colonel of Tamarkan", writes that Pierre Boulle, who had been a prisoner of war in Thailand, created the fictional Nicholson character as an amalgam of his memories of collaborating French officers. He strongly denied the claim that the book was anti-British, although many involved in the film itself (including Alec Guinness) felt otherwise.
Toosey was very different from Nicholson and was certainly not a collaborator who felt obliged to work with the Japanese. Toosey in fact did as much as possible to delay the building of the bridge. While Nicholson disapproves of acts of sabotage and other deliberate attempts to delay progress, Toosey encouraged this: termites were collected in large numbers to eat the wooden structures, and the concrete was badly mixed.
In an interview that forms part of the 1969 BBC2 documentary "Return to the River Kwai" made by former POW John Coast, Boulle outlined the reasoning that led him to conceive the character of Nicholson. A transcript of the interview and the documentary as a whole can be found in the new edition of John Coast's book "Railroad of Death". Coast's documentary sought to highlight the real history behind the film (partly through getting ex-POWs to question its factual basis, for example Dr Hugh de Wardener and Lt-Col Alfred Knights), which had angered so many former POWs. The documentary itself was described by one newspaper reviewer when it was shown on Boxing Day 1974 ("The Bridge on the River Kwai" had been shown on BBC1 on Christmas Day 1974) as "Following the movie, this is a rerun of the antidote."
Some of the characters in the film use the names of real people who were involved in the Burma Railway. Their roles and characters, however, are fictionalised. For example, a Sergeant-Major Risaburo Saito was in real life second in command at the camp. In the film, a Colonel Saito is camp commandant. In reality, Risaburo Saito was respected by his prisoners for being comparatively merciful and fair towards them; Toosey later defended him in his war crimes trial after the war, and the two became friends.
The destruction of the bridge as depicted in the film is entirely fictional. In fact, two bridges were built: a temporary wooden bridge and a permanent steel/concrete bridge a few months later. Both bridges were used for two years, until they were destroyed by Allied aerial bombing. The steel bridge was repaired and is still in use today.
Japanese views of the book and movie.
The Japanese resented the conclusion in the movie that their engineers were less capable than British engineers. In fact, Japanese engineers had been surveying the route of the railway since 1937 and they were highly organized. The Japanese also resented the "glorification of the superiority of Western civilization" represented in the movie by the British being able to build a bridge that the Japanese could not.
Production.
Screenplay.
The screenwriters, Carl Foreman and Michael Wilson, were on the Hollywood blacklist and could only work on the film in secret. The two did not collaborate on the script; Wilson took over after Lean was dissatisfied with Foreman's work. The official credit was given to Pierre Boulle (who did not speak English), and the resulting Oscar for Best Screenplay (Adaptation) was awarded to him. Only in 1984 did the Academy rectify the situation by retroactively awarding the Oscar to Foreman and Wilson, posthumously in both cases. Subsequent releases of the film finally gave them proper screen credit. David Lean himself also claimed that producer Sam Spiegel cheated him out of his rightful part in the credits since he had had a major hand in the script.
The film was relatively faithful to the novel, with two major exceptions. Shears, who is a British commando officer like Warden in the novel, became an American sailor who escapes from the POW camp. Also, in the novel, the bridge is not destroyed: the train plummets into the river from a secondary charge placed by Warden, but Nicholson (never realising "what have I done?") does not fall onto the plunger, and the bridge suffers only minor damage. Boulle nonetheless enjoyed the film version though he disagreed with its climax.
Filming.
Many directors were considered for the project, among them: John Ford, William Wyler, Howard Hawks, Fred Zinnemann and Orson Welles.
The film was an international co-production between companies in Britain and the United States. It is set in Thailand, but was filmed mostly near Kitulgala, Ceylon (now Sri Lanka), with a few scenes shot in England.
Director David Lean clashed with his cast members on multiple occasions, particularly Alec Guinness and James Donald, who thought the novel was anti-British. Lean had a lengthy row with Guinness over how to play the role of Nicholson; Guinness wanted to play the part with a sense of humour and sympathy, while Lean thought Nicholson should be "a bore." On another occasion, Lean and Guinness argued over the scene where Nicholson reflects on his career in the army. Lean filmed the scene from behind Guinness, and exploded in anger when Guinness asked him why he was doing this. After Guinness was done with the scene, Lean said "Now you can all fuck off and go home, you English actors. Thank God that I'm starting work tomorrow with an American actor (William Holden)."
Alec Guinness later said that he subconsciously based his walk while emerging from "the Oven" on that of his son Matthew when Matthew was recovering from polio. Guinness called his walk from the Oven to Saito's hut while being saluted by his men the "finest work I'd ever done."
Lean nearly drowned when he was swept away by a river current during a break from filming; Geoffrey Horne saved his life.
The filming of the bridge explosion was to be done on 10 March 1957, in the presence of S.W.R.D. Bandaranaike, then Prime Minister of Ceylon, and a team of government dignitaries. However, cameraman Freddy Ford was unable to get out of the way of the explosion in time, and Lean had to stop filming. The train crashed into a generator on the other side of the bridge and was wrecked. It was repaired in time to be blown up the next morning, with Bandaranaike and his entourage present.
According to the supplemental material in the Blu-ray digipak, a thousand "tons" of explosives were used to blow up the bridge. This is highly unlikely, as the film shows roughly 50 kg of plastic explosive being used simply to knock down the bridge's supports.
According to Turner Classic Movies, the producers nearly suffered a catastrophe following the filming of the bridge explosion. To ensure they captured the one-time event, multiple cameras from several angles were used. Ordinarily, the film would have been taken by boat to London, but due to the Suez crisis this was impossible; therefore the film was taken by air freight. When the shipment failed to arrive in London, a worldwide search was undertaken. To the producers' horror the film containers were found a week later on an airport tarmac in Cairo, sitting in the hot Egyptian sun. Although it was not exposed to sunlight, the heat-sensitive colour film stock should have been hopelessly ruined; however, when processed the shots were perfect and appeared in the film.
Music.
A memorable feature of the film is the tune that is whistled by the POWs—the first strain of the march "Colonel Bogey"—when they enter the camp. The march was written in 1914 by Kenneth J. Alford, a pseudonym of British Bandmaster Frederick J. Ricketts. The Colonel Bogey strain was accompanied by a counter-melody using the same chord progressions, then continued with film composer Malcolm Arnold's own composition, "The River Kwai March," played by the off-screen orchestra taking over from the whistlers, though Arnold's march was not heard in completion on the soundtrack. Mitch Miller had a hit with a recording of both marches.
Besides serving as an example of British fortitude and dignity in the face of privation, the "Colonel Bogey March" suggested a specific symbol of defiance to British film-goers, as its melody was used for the song "Hitler Has Only Got One Ball." Lean wanted to introduce Nicholson and his soldiers into the camp singing this song, but Sam Spiegel thought it too vulgar, and whistling was substituted. However, the lyrics were, and continue to be, so well known to the British public that they didn't need to be laboured.
The soundtrack of the film is largely diegetic; background music is not widely used. In many tense, dramatic scenes, only the sounds of nature are used. An example of this is when commandos Warden and Joyce hunt a fleeing Japanese soldier through the jungle, desperate to prevent him from alerting other troops.
Arnold won an Academy Award for the film's score.
Lean later used another Allford march, "The Voice of the Guns," in "Lawrence of Arabia."
Box office performance.
"Variety" reported that this film was the #1 moneymaker of 1958, with a US take of $18,000,000. The second highest moneymaker of 1958 was "Peyton Place" at $12,000,000; in third place was "Sayonara" at $10,500,000.
The movie was re-released in 1964 and earned an estimated $2.6 million in North American rentals.
Awards.
Academy Awards.
"The Bridge on the River Kwai" won seven Oscars:
It was nominated for
BAFTA Awards.
Winner of 3 BAFTA Awards
Golden Globe Awards.
Winner of 3 Golden Globes
Recipient of one nomination
Recognition.
The film has been selected for preservation in the United States National Film Registry.
Channel 4 held a poll in 2005 to find the 100 Greatest War Movies: "The Bridge on the River Kwai" came in at #10, behind "Black Hawk Down" and in front of "The Dam Busters".
The British Film Institute placed "The Bridge on the River Kwai" as the eleventh greatest British film.
First TV broadcast.
The 167-minute film was first telecast, uncut, by ABC-TV in color on the evening of 25 September 1966, as a three hours-plus ABC Movie Special. The telecast of the film lasted more than three hours because of the commercial breaks. It was still highly unusual at that time for a television network to show such a long film in one evening; most films of that length were still generally split into two parts and shown over two evenings. But the unusual move paid off for ABC—the telecast drew huge ratings. On the evenings of 28 and 29 January 1973, ABC broadcast another David Lean colour spectacular, "Lawrence of Arabia", but that broadcast 'was split into two parts over two evenings, due to the film's nearly four-hour length.
Restorations.
The film was restored in 1985 by Columbia Pictures. The separate dialogue, music and effects were located and remixed with newly recorded "atmospheric" sound effects. The image was restored by OCS, Freeze Frame, and Pixel Magic with George Hively editing.
On 2 November 2010 Columbia Pictures released a newly restored "The Bridge on the River Kwai" for the first time on Blu-ray. According to Columbia Pictures, they followed an all-new 4K digital restoration from the original negative with newly restored 5.1 audio. The original negative for the feature was scanned at 4k (roughly four times the resolution in High Definition), and the colour correction and digital restoration were also completed at 4k. The negative itself manifested many of the kinds of issues one would expect from a film of this vintage: torn frames, imbedded emulsion dirt, scratches through every reel, colour fading. Unique to this film, in some ways, were other issues related to poorly made optical dissolves, the original camera lens and a malfunctioning camera. These problems resulted in a number of anomalies that were very difficult to correct, like a ghosting effect in many scenes that resembles color mis-registration, and a tick-like effect with the image jumping or jerking side-to-side. These issues, running throughout the film, were addressed to a lesser extent on various previous DVD releases of the film and might not have been so obvious in standard definition.

</doc>
<doc id="42858" url="http://en.wikipedia.org/wiki?curid=42858" title="Sudetenland">
Sudetenland

The Sudetenland (Czech and Slovak: "Sudety", Polish: "Kraj Sudetów") is the German name (used in English in the first half of the 20th century) to refer to those northern, southwest, and western areas of Czechoslovakia which were inhabited primarily by German speakers, specifically the border districts of Bohemia, Moravia, and those parts of Silesia located within Czechoslovakia.
The name is derived from that of the Sudetes mountains, which run along the northern Czech border as far as Silesia and contemporary Poland, although it encompassed areas well beyond those mountains. 
The word "Sudetenland" only came into existence in the early 20th century, and only came to prominence after the First World War, when the German-dominated Austria-Hungary was dismembered and the Sudeten Germans found themselves living in the new country of Czechoslovakia. The "Sudeten crisis" of 1938 was provoked by the demands of Nazi Germany that the Sudetenland be annexed to Germany, which in fact took place after the later infamous Munich Agreement. When Czechoslovakia was reconstituted after the Second World War, the Sudeten Germans were largely expelled, and the region today is inhabited primarily by Czech speakers. 
Parts of the current Czech regions of Karlovy Vary, Liberec, Olomouc, Moravia-Silesia, and Ústí nad Labem are situated within the former Sudetenland.
History.
The areas later known as Sudetenland never formed a single historical region, which makes it difficult to distinguish the history of the Sudetenland apart from that of Bohemia, until the advent of nationalism in the 19th century.
Early origins.
The Celtic and Boii tribes settled there and the region was first mentioned on the map of Ptolemaios in the 2nd century AD. The Germanic tribe of the Marcomanni dominated the entire core of the region in later centuries. Those tribes already built cities like Brno, but moved west during the Migration Period. In the 7th century AD Slavic people moved in and were united under Samo's realm. Later in the High Middle Ages Germans settled into the less populated border region.
In the Middle Ages the regions situated on the mountainous border of the Duchy and the Kingdom of Bohemia had since the Migration Period been settled mainly by western Slavic Czechs. Along the Bohemian Forest in the west, the Czech lands bordered on the German Slavic tribes (German Sorbs) stem duchies of Bavaria and Franconia; marches of the medieval German kingdom had also been established in the adjacent Austrian lands south of the Bohemian-Moravian Highlands and the northern Meissen region beyond the Ore Mountains. In the course of the "Ostsiedlung" (settlement of the east) German settlement from the 13th century onwards continued to move into the Upper Lusatia region and the duchies of Silesia north of the Sudetes mountain range.
From as early as the second half of the 13th century onwards these Bohemian border regions were settled by ethnic Germans, who were invited by the Přemyslid Bohemian kings—especially by Ottokar II (1253–1278) and Wenceslaus II (1278–1305). After the extinction of the Přemyslid dynasty in 1306, the Bohemian nobility backed John of Luxembourg as king against his rival Duke Henry of Carinthia. In 1322 King John of Bohemia acquired (for the third time) the formerly Imperial Egerland region in the west and was able to vassalize most of the Piast Silesian duchies, acknowledged by King Casimir III of Poland by the 1335 Treaty of Trentschin. His son, Bohemian King Charles IV, was elected King of the Romans in 1346 and crowned Holy Roman Emperor in 1355. He added the Lusatias to the Lands of the Bohemian Crown, which then comprised large territories with a significant German population.
In the hilly border regions German settlers established major manufactures of forest glass. The situation of the German population was aggravated by the Hussite Wars (1419–1434), though there were also some Germans among the Hussite insurgents.
By then Germans largely settled the hilly Bohemian border regions as well as the cities of the lowlands; mainly people of Bavarian descent in the South Bohemian and South Moravian Region, in Brno, Jihlava, České Budějovice and the West Bohemian Plzeň Region; Franconian people in Žatec; Upper Saxons in adjacent North Bohemia, where the border with the Saxon Electorate was fixed by the 1459 Peace of Eger; Germanic Silesians in the adjacent Sudetes region with the County of Kladsko, in the Moravian–Silesian Region, in Svitavy and Olomouc. The city of Prague had a German-speaking majority from the last third of the 17th century until 1860, but after 1910 had the proportion of German speakers decreased to 6.7% of the population.
From the Luxembourgs, the rule over Bohemia passed through George of Podiebrad to the Jagiellon dynasty and finally to the House of Habsburg in 1526. Both Czech and German Bohemians suffered heavily in the Thirty Years War. Bohemia lost 70% of its population. From the defeat of the Bohemian Revolt that collapsed at the 1620 Battle of White Mountain, the Habsburgs gradually integrated the Kingdom of Bohemia into their monarchy. During the subsequent Counter-Reformation, less populated areas were resettled with Catholic Germans from the Austrian lands. From 1627 the Habsburgs enforced the so-called "Verneuerte Landesordnung" ("Renewed Land's Constitution") and one of its consequences was that German according to mother tongue gradually became the primary and official language while Czech declined to a secondary role in the Empire. Also in 1749 Austrian Empire enforced German as the official language again. Emperor Joseph II in 1780 renounced the coronation ceremony as Bohemian king and unsuccessfully tried to push German through as sole official language in all Habsburg lands (including Hungary). Nevertheless German cultural influence grew stronger during the Age of Enlightenment and Weimar Classicism.
On the other hand, in the course of the Romanticism movement national tensions arose, both in the form of the Austroslavism ideology developed by Czech politicians like František Palacký and Pan-Germanist activist raising the German question. Conflicts between Czech and German nationalists emerged in the 19th century, for instance in the Revolutions of 1848: while the German-speaking population of Bohemia and Moravia wanted to participate in the building of a German nation state, the Czech-speaking population insisted on keeping Bohemia out of such plans. The Bohemian Kingdom remained a part of the Austrian Empire and Austria-Hungary until its dismemberment after the First World War.
Emergence of the term.
In the wake of growing nationalism, the name ""Sudetendeutsche" (Sudeten Germans) emerged by the early 20th century. It originally constituted part of a larger classification of three groupings of Germans within the Austro-Hungarian Empire, which also included "Alpine Deutschen" (English: Alpine Germans) in what later became the Republic of Austria and "Balkandeutsche" (English: Balkan Germans) in Hungary and the regions east of it. Of these three terms, only the term "Sudetendeutsche"" survived, because of the ethnic and cultural conflicts within Bohemia.
World War I and its aftermath.
During World War I, what would later be known as the Sudetenland experienced a rate of war deaths higher than most other German speaking areas of Austria-Hungary and exceeded only by German South Moravia and Carinthia. Thirty-four of each 1,000 inhabitants were killed.
Austria-Hungary broke apart at the end of World War I. Late in October 1918, an independent Czechoslovak state, consisting of the lands of the Bohemian kingdom and areas belonging to the Kingdom of Hungary, was proclaimed. The German deputies of Bohemia, Moravia, and Silesia in the Imperial Council ("Reichsrat") referred to the Fourteen Points of U.S. President Woodrow Wilson and the right proposed therein to self-determination, and attempted to negotiate the union of the German-speaking territories with the new Republic of German Austria, which itself aimed at joining Weimar Germany.
The German-speaking parts of the former Lands of the Bohemian Crown remained in a newly created Czechoslovakia, a multiethnic state of several nations: Czechs, Germans, Slovaks, Hungarians, Poles and Ruthenians. On 20 September 1918, the Prague government asked the United States's opinion for the Sudetenland. President Woodrow Wilson sent Ambassador Archibald Coolidge into Czechoslovakia. After Coolidge became witness of German Bohemian demonstrations, Coolidge suggested the possibility of ceding certain German-speaking parts of Bohemia to Germany (Cheb) and Austria (South Moravia and South Bohemia). He also insisted that the German-inhabited regions of West and North Bohemia remain within Czechoslovakia. The American delegation at the Paris talks, with Allen Dulles as the American's chief diplomat in the Czechoslovak Commission who emphasized preserving the unity of the Czech lands, decided not to follow Coolidge's proposal.
Four regional governmental units were established:
The U.S. commission to the Paris Peace Conference issued a declaration which gave unanimous support for "unity of Czech lands". In particular the declaration stated:
The Commission was...unanimous in its recommendation that the separation of all areas inhabited by the German-Bohemians would not only expose Czechoslovakia to great dangers but equally create great difficulties for the Germans themselves. The only practicable solution was to incorporate these Germans into Czechoslovakia.
Several German minorities according to their mother tongue in Moravia—including German-speaking populations in Brno, Jihlava, and Olomouc—also attempted to proclaim their union with German Austria, but failed. The Czechs thus rejected the aspirations of the German Bohemians and demanded the inclusion of the lands inhabited by ethnic Germans in their state, despite the presence of more than 90% (as of 1921) ethnic Germans (which led to the presence of 23.4% of Germans in all of Czechoslovakia), on the grounds they had always been part of lands of the Bohemian Crown. The Treaty of Saint-Germain in 1919 affirmed the inclusion of the German-speaking territories within Czechoslovakia. Over the next two decades, some Germans in the Sudetenland continued to strive for a separation of the German-inhabited regions from Czechoslovakia.
Within the Czechoslovak Republic (1918–1938).
According to the February 1921 census, 3,123,000 native German speakers lived in Czechoslovakia—23.4% of the total population. The controversies between the Czechs and the German-speaking minority (which constituted a majority in the Sudetenland areas) lingered on throughout the 1920s, and intensified in the 1930s.
During the Great Depression the mostly mountainous regions populated by the German minority, together with other peripheral regions of Czechoslovakia, were hurt by the economic depression more than the interior of the country. Unlike the less developed regions (Ruthenia, Moravian Wallachia), the Sudetenland had a high concentration of vulnerable export-dependent industries (such as glass works, textile industry, paper-making, and toy-making industry). Sixty percent of the bijouterie and glass-making industry were located in the Sudetenland, 69% of employees in this sector were Germans speaking according to mother tongue, and 95% of bijouterie and 78% of other glassware was produced for export. The glass-making sector was affected by decreased spending power and also by protective measures in other countries and many German workers lost their work.
The high unemployment made people more open to populist and extremist movements such as fascism, communism, and German irredentism. In these years, the parties of German nationalists and later the Nazi Sudeten German National Socialist Party (SdP) with its radical demands gained immense popularity among Germans in Czechoslovakia. After 1933 Czechoslovakia remained the only democracy in central and eastern Europe.
Sudeten Crisis.
The increasing aggressiveness of Hitler prompted the Czechoslovak military to build extensive Czechoslovak border fortifications starting in 1936 to defend the troubled border region.
Immediately after the "Anschluss" of Austria into the Third Reich in March 1938, Hitler made himself the advocate of ethnic Germans living in Czechoslovakia, triggering the "Sudeten Crisis". The following month, Sudeten Nazis, led by Konrad Henlein, agitated for autonomy. On 24 April 1938 the SdP proclaimed the "Karlsbader Programm", which demanded in eight points the complete equality between the Sudeten Germans and the Czech people. The government accepted these claims on 30 June 1938.
In August, British Prime Minister, Neville Chamberlain, sent Lord Runciman to Czechoslovakia in order to see if he could obtain a settlement between the Czechoslovak government and the Germans in the Sudetenland. Lord Runciman's first day included meetings with President Beneš and Prime Minister Milan Hodža as well as a direct meeting with the Sudeten Germans from Henlein's SdP. On the next day he met with Dr and Mme Beneš and later met non-Nazi Germans in his hotel.
A full account of his report—including summaries of the conclusions of his meetings with the various parties—which he made in person to the Cabinet on his return to Britain is found in the Document CC 39(38). Lord Runciman expressed sadness that he could not bring about agreement with the various parties, but he agreed with Lord Halifax that the time gained was important. He reported on the situation of the Sudeten Germans, and he gave details of four plans which had been proposed to deal with the crisis, each of which had points which, he reported, made it unacceptable to the other parties to the negotiations. 
The four were: Transfer of the Sudetenland to the Reich; hold a plebiscite on the transfer of the Sudetenland to the Reich, organize a Four Power Conference on the matter, create a federal Czechoslovakia. At the meeting, he said that he was very reluctant to offer his own solution; he had not seen this as his task. The most that he said was that the great centres of opposition were in Eger and Asch, in the northwestern corner of Bohemia, which contained about 800,000 Germans and very few others. 
He did say that the transfer of these areas to Germany would almost certainly be a good thing; he added that the Czechoslovak army would certainly oppose this very strongly, and that Beneš had said that they would fight rather than accept it.
British Prime Minister Neville Chamberlain met with Adolf Hitler in "Berchtesgaden" on 15 September and agreed to the cession of the Sudetenland; three days later, French Prime Minister Édouard Daladier did the same. No Czechoslovak representative was invited to these discussions. Germany was now able to walk into the Sudetenland without firing a shot.
Chamberlain met Hitler in Godesberg on 22 September to confirm the agreements. Hitler, aiming to use the crisis as a pretext for war, now demanded not only the annexation of the Sudetenland but the immediate military occupation of the territories, giving the Czechoslovak army no time to adapt their defence measures to the new borders. To achieve a solution, Italian dictator Benito Mussolini suggested a conference of the major powers in Munich and on 29 September, Hitler, Daladier and Chamberlain met and agreed to Mussolini's proposal (actually prepared by Hermann Göring) and signed the Munich Agreement, accepting the immediate occupation of the Sudetenland. The Czechoslovak government, though not party to the talks, submitted to compulsion and promised to abide by the agreement on 30 September.
The Sudetenland was relegated to Germany between 1 October and 10 October 1938. The Czech part of Czechoslovakia was subsequently invaded by Germany in March 1939, with a portion being annexed and the remainder turned into the Protectorate of Bohemia and Moravia. The Slovak part declared its independence from Czechoslovakia, becoming the Slovak Republic (Slovak State), a satellite state and ally of Nazi Germany. (The Ruthenian part — Subcarpathian Rus — made also an attempt to declare its sovereignty as Carpatho-Ukraine but only with ephemeral success. This area was annexed by Hungary.)
Part of the borderland was also invaded and annexed by Poland.
Sudetenland as part of Nazi Germany.
The Sudetenland was initially put under military administration, with General Wilhelm Keitel as military governor. On 21 October 1938, the annexed territories were divided, with the southern parts being incorporated into the neighbouring Reichsgaue "Niederdonau", "Oberdonau" and "Bayerische Ostmark".
The northern and western parts were reorganized as the Reichsgau "Sudetenland", with the city of Reichenberg (present-day Liberec) established as its capital. Konrad Henlein (now openly a NSDAP member) administered the district first as "Reichskommissar" (until 1 May 1939) and then as "Reichsstatthalter" (1 May 1939 – 4 May 1945). Sudetenland consisted of three political districts: Eger (with Karlsbad as capital), Aussig (Aussig) and Troppau (Troppau).
Shortly after the annexation, the Jews living in the Sudetenland were widely persecuted. Only a few weeks afterwards, the Kristallnacht occurred. As elsewhere in Germany, many synagogues were set on fire and numerous leading Jews were sent to concentration camps. In later years, the Nazis transported up to 300,000 Czech and Slovak Jews to concentration camps, where many of them were killed or died. Jews and Czechs were not the only afflicted peoples; German socialists, communists and pacifists were widely persecuted as well. Some of the German socialists fled the Sudetenland via Prague and London to other countries. The Gleichschaltung would permanently alter the community in the Sudetenland.
Despite this, on 4 December 1938 there were elections in Reichsgau Sudetenland, in which 97.32% of the adult population voted for NSDAP. About a half million Sudeten Germans joined the Nazi Party which was 17.34% of the total German population in Sudetenland (the average NSDAP membership participation in Nazi Germany was merely 7.85% in 1944). This means the Sudetenland was one of the most pro-Nazi regions of the Third Reich. Because of their knowledge of the Czech language, many Sudeten Germans were employed in the administration of the ethnic Czech Protectorate of Bohemia and Moravia as well as in Nazi organizations (Gestapo, etc.). The most notable was Karl Hermann Frank: the SS and Police general and Secretary of State in the Protectorate.
Expulsions and resettlement after World War II.
After World War II in summer 1945 the Potsdam Conference decided that Sudeten Germans would have to leave Czechoslovakia (see Expulsion of Germans after World War II). As a consequence of the immense hostility against all Germans that had grown within Czechoslovakia due to Nazi behavior, the overwhelming majority of Germans were expelled (while the relevant Czechoslovak legislation provided for the remaining Germans who were able to prove their anti-Nazi affiliation). 
The number of expelled Germans in the early phase (spring–summer 1945) is estimated to be around 500,000 people. Following the Beneš decrees and starting in 1946, the majority of the Germans were expelled and in 1950 only 159,938 (from 3,149,820 in 1930) still lived in the Czech Republic. The remaining Germans, proven anti-fascists and forced laborers, were allowed to stay in Czechoslovakia, but were later forcefully dispersed within the country (Přesun v rámci rozptylu občanů německé národnosti). Some German refugees from Czechoslovakia are represented by the Sudetendeutsche Landsmannschaft.
Many of the Germans who stayed in Czechoslovakia later emigrated to West Germany (more than 100,000). As the German population was transferred out of the country, the former Sudetenland was resettled, mostly by Czechs but also by other nationalities of Czechoslovakia: Slovaks, Greeks (arriving in the wake of the Greek Civil War 1946–49), Volhynian Czechs, Gypsies, Jews and Hungarians (though the Hungarians were forced into this and later returned home—see Hungarians in Slovakia: Population exchanges).
Some areas—such as part of Czech Silesian-Moravian borderland, southwestern Bohemia (Šumava National Park), western and northern parts of Bohemia—remained depopulated for several strategic reasons (extensive mining and military interests) or are now protected national parks and landscapes. Moreover, before the establishment of the Iron Curtain in 1952–55, the so-called "forbidden zone" was established (by means of engineer equipment) up to 2 km (1.2 mi) from the border in which no civilians could reside. A wider region, or "border zone" existed, up to 12 km from the border, in which no "disloyal" or "suspect" civilians could reside or work. Thus, the entire Aš-Bulge fell within the border zone; this status remained until the Velvet Revolution in 1989.
There remained areas with noticeable German minorities in the westernmost borderland around Cheb, where skilled forced labour of remaining ethnic German men remained necessary in mining and industry until 1955 ; in the Egerland, German minority organizations continue to exist. Also, the small town of Kravaře (German: "Deutsch Krawarn") in the multiethnic Hlučín Region of Czech Silesia has an ethnic German majority (2006), including an ethnic German mayor.
In the 2001 census, approximately 40,000 people in the Czech Republic claimed German ethnicity.
Suppression of the term.
Shortly after the liberation of Czechoslovakia in May 1945, the use of the term "Sudety" (Sudetenland) in the official communication was banned and replaced by the term "pohraniční území" (border territory).

</doc>
<doc id="42861" url="http://en.wikipedia.org/wiki?curid=42861" title="Chang (film)">
Chang (film)

 
Chang: A Drama of the Wilderness (1927) is a silent film about a poor farmer in Issan (Northeastern Thailand) and his daily struggle for survival in the jungle, the film was directed by Merian C. Cooper and Ernest B. Schoedsack. It was released by Famous Players-Lasky, a division of Paramount Pictures.
Plot.
In the directors' own words, "Chang" is a "melodrama with man, the jungle, and wild animals as its cast." Kru, the farmer depicted in the film, battles leopards, tigers, and even a herd of elephants, all of which pose a constant threat to his livelihood. As filmmakers, Cooper and Schoedsack attempted to capture real life with their cameras, though they often re-staged events that had not been captured adequately on film. The danger was real to all the people and animals involved. Tigers, leopards, and bears are slaughtered on camera, while the film's climax shows Kru's house being demolished by a stampeding elephant.
Awards.
"Chang" was nominated for the Academy Award for Unique and Artistic Production at the first Academy Awards in 1929, the only year when that award was presented.

</doc>
<doc id="42863" url="http://en.wikipedia.org/wiki?curid=42863" title="A Streetcar Named Desire (play)">
A Streetcar Named Desire (play)

A Streetcar Named Desire is a 1947 play written by American playwright Tennessee Williams for which he received the Pulitzer Prize for Drama in 1948. The play opened on Broadway on December 3, 1947, and closed on December 17, 1949, in the Ethel Barrymore Theatre. The Broadway production was directed by Elia Kazan and starred Marlon Brando, Jessica Tandy, Kim Hunter, and Karl Malden. The London production opened in 1949 with Bonar Colleano, Vivien Leigh, and Renee Asherson and was directed by Laurence Olivier. The drama "A Streetcar Named Desire" is often numbered on the short list of being among the finest American plays in the 20th century, alongside "Long Day's Journey into Night" and "Death of a Salesman".
Plot.
After the loss of her family home, Belle Reve, to creditors, Blanche DuBois travels from the small town of Laurel, Mississippi to the New Orleans French Quarter to live with her younger, married sister, Stella, and brother-in-law, Stanley Kowalski. Blanche is in her thirties, and with no money, she has nowhere else to go.
Blanche tells Stella that she has taken a leave of absence from her English teaching position because of her nerves. Blanche laments the shabbiness of her sister’s two-room flat. She finds Stanley loud and rough, eventually referring to him as “common.” Stanley, in return, does not care for Blanche's manners and dislikes her presence.
Stanley later questions Blanche about her earlier marriage. Blanche had married when she was very young, but her husband died, leaving her widowed and alone. The memory of her dead husband causes Blanche some obvious distress. Stanley, worried that he has been cheated out of an inheritance, demands to know what happened to Belle Reve, once a large plantation and the DuBois family home. Blanche hands over all the documents pertaining to Belle Reve. While looking at the papers, Stanley notices a bundle of letters that Blanche emotionally proclaims are personal love letters from her dead husband. For a moment, Stanley seems caught off guard over her proclaimed feelings. Afterwards, he informs Blanche that Stella is going to have a baby.
The night after Blanche’s arrival, during one of Stanley’s poker parties, Blanche meets Mitch, one of Stanley’s poker player buddies. His courteous manner sets him apart from the other men. Their chat becomes flirtatious and friendly, and Blanche easily charms him; they like each other. Suddenly becoming upset over multiple interruptions, Stanley explodes in a drunken rage and strikes Stella. Blanche and Stella take refuge with the upstairs neighbor, Eunice. When Stanley recovers, he cries out from the courtyard below for Stella to come back by repeatedly calling her name until she comes down and allows herself to be carried off to bed. After Stella returns to Stanley, Blanche and Mitch sit at the bottom of the steps in the courtyard, where Mitch apologizes for Stanley's coarse behavior.
Blanche is bewildered that Stella would go back with him after such violence. The next morning, Blanche rushes to Stella and describes Stanley as a subhuman animal, though Stella assures Blanche that she and Stanley are fine. Stanley overhears the conversation but keeps silent. When Stanley comes in, Stella hugs and kisses him, letting Blanche know that her low opinion of Stanley does not matter.
As the weeks pass, Blanche and Stanley continue to not get along. Blanche has hope in Mitch, and tells Stella that she wants to go away with him and not be anyone’s problem. During a meeting between the two, Blanche confesses to Mitch that once she was married to a young man, Allan Grey, whom she later discovered in a sexual encounter with an older man. Grey later committed suicide when Blanche told him she was disgusted with him. The story touches Mitch, who tells Blanche that they need each other. It seems certain that they will get married.
Later on, Stanley repeats gossip to Stella that he has gathered on Blanche, telling her that Blanche was fired from her teaching job for having sex with a student and that she lived at a hotel known for prostitutes. Stella erupts in anger over Stanley’s cruelty after he states that he has also told Mitch about the rumors, but the fight is cut short as she goes into labor and is sent to the hospital.
As Blanche waits at home alone, Mitch arrives and confronts Blanche with the stories that Stanley has told him. At first she denies everything, but eventually confesses that the stories are true. She pleads for forgiveness, but an angry and humiliated Mitch rejects her. He then advances towards her as though to rape her; in response, Blanche screams "fire", and he runs away in fright.
When Stella has the baby, Stanley and Blanche are left alone in the apartment. In their final confrontation, it is strongly implied that Stanley rapes Blanche, imminently resulting in her psychotic crisis. 
Weeks later, at another poker game at the Kowalski apartment, Stella and her neighbor, Eunice, are packing Blanche's belongings. Blanche has suffered a complete mental breakdown and is to be committed to a mental hospital. Although Blanche has told Stella about Stanley's assault, Stella cannot bring herself to believe her sister's story. When a doctor and a nurse arrive to take Blanche to the hospital, she initially resists them and collapses on the floor in confusion. Mitch, present at the poker game, breaks down in tears. When the doctor helps Blanche up, she goes willingly, with him, saying, “whoever you are, I have always depended upon the kindness of strangers.” The play ends with Stanley continuing to comfort Stella while the poker game continues uninterrupted.
Stage productions.
Original Broadway production.
The original Broadway production was produced by Irene Mayer Selznick. It opened at the Shubert in New Haven shortly before moving to the Ethel Barrymore Theatre on December 3, 1947. Selznick originally wanted to cast Margaret Sullavan and John Garfield, but settled on Marlon Brando and Jessica Tandy, who were virtual unknowns at the time. Brando was given car fare to Tennessee Williams' home in Provincetown, Massachusetts, where he not only gave a sensational reading, but did some house repairs as well.The problem with casting Brando as Stanley was that he was much younger than the character as written by Williams. However, after the meeting between Brando and Williams, the playwright eagerly agreed that Brando would make an ideal Stanley. Williams believed that by casting a younger actor, the Neanderthalish Kowalski would evolve from being a vicious older man to someone whose unintentional cruelty can be attributed to his youthful ignorance. Brando ultimately was dissatisfied with his performance, though, saying he never was able to bring out the humor of the character, which was ironic as his characterization often drew laughs from the audience at the expense of Jessica Tandy's Blanche Dubois. Tandy was cast after Williams saw her performance in a West Coast production of his one-act play "Portrait of a Madonna". The opening night cast also included Kim Hunter as Stella and Karl Malden as Mitch. Despite its shocking scenes and gritty dialogue, the audience applauded for half an hour after the debut performance ended.
Brooks Atkinson, reviewing the opening in "The New York Times" described Tandy's "superb performance" as "almost incredibly true," concluding that Williams "has spun a poignant and luminous story."
Later in the run, Uta Hagen replaced Tandy, Carmelita Pope replaced Hunter, and Anthony Quinn replaced Brando. Hagen and Quinn took the show on a national tour and then returned to Broadway for additional performances. Early on, when Brando broke his nose, Jack Palance took over his role. Ralph Meeker also took on the part of Stanley both in the Broadway and touring companies. Tandy received a Tony Award for Best Actress in a Play in 1948, sharing the honor with Judith Anderson's portrayal of Medea and with Katharine Cornell. Brando portrayed Stanley with an overt sexuality combined with a boyish vulnerability that made his portrait of Stanley, and especially the moment where he howls "Stella!" for his wife, into cultural touchstones. Brando's appearance as Stanley on stage and on screen revolutionized American acting by introducing "The Method" or "Method acting" into American consciousness and culture. 
Uta Hagen's Blanche on the national tour was directed not by Elia Kazan, who had directed the Broadway production, but by Harold Clurman, and it has been reported, both in interviews by Hagen and observations by contemporary critics, that the Clurman-directed interpretation shifted the focus of audience sympathy back to Blanche and away from Stanley (where the Kazan/Brando/Tandy version had located it).
The original Broadway production closed after 855 performances in 1949.
Original cast.
Original London production.
The London production, directed by Laurence Olivier, opened on October 12, 1949, and starred Bonar Colleano, Vivien Leigh, and Renee Asherson.
"Belle Reprieve".
Bette Bourne and Paul Shaw of the British gay theater company, Bloolips, and Peggy Shaw and Lois Weaver of the American lesbian theater company, Split Britches, collaborated and performed a gender-bent production of "Belle Reprieve", a gender twisted adaption of "Streetcar". “This theatrical piece creates a Brechtian,” an “epic drama” that relies on the reflective attachment of the audience rather than emotional involvement, created by the German poet and playwright, Bertolt Brecht, “commentary on the sexual roles and games in Williams’s text.” Blanche was played by Bette Bourne as “man in a dress”, Stanley was played by Peggy Shaw as a “butch lesbian”, Mitch was played by Paul Shaw as a “fairy disguised as a man”, and Stella was played by Lois Weaver as a “woman disguised as a woman”. 
Influence on 20th-century theatre.
By the close of the 19th century, melodrama began to disappear from the theater. More and more, the focus was on a style of acting called dramatic naturalism.
By the time "A Streetcar Named Desire" was written and produced, melodrama was in its last stages and Blanche DuBois's memorable personality used it to illustrate exactly how misleading melodramatic acting could be.
Exaggerated sighs, unnecessary screams of distress, and fluttery hand gestures are all employed by Blanche throughout the play. Dramatic lines about needing rescuing (which are now often seen as clichéd) are an internal part of Blanche's working. They veil her true personality (that of a sick, unbalanced woman) and allow her to play with men like Mitch, who falls for her histrionics and becomes convinced he will be her savior.
With the 20th century's arrival came dramatic naturalism, based on Constantin Stanislavski's method-acting system. Unlike melodrama, dramatic naturalism focused on realistic acting, where actors were asked to recall memories to help them emote realistically during scenes, as per Stanislavski's method. "Streetcar"′s first director, Elia Kazan, employed a Stanislavski reading on every play he worked on and his notes on "Streetcar" depicted not a melodramatic villainous Stanley Kowalski, but a defensive, flawed, and relatable Stanley, whom Marlon Brando portrayed well.
The biggest example of dramatic naturalism is Blanche's opponent, Stanley, who in the first production of "Streetcar" was played by method-actor Marlon Brando. After his exemplary performance as a lustful, animal-like, yet needy Stanley, American theater saw a significant shift away from melodrama and toward dramatic naturalism. Brando has been hailed as the father of theatrical stars like James Dean and Jack Nicholson.
Revivals.
Tallulah Bankhead, whom Williams had in mind when writing the play, starred in a 1956 New York City Center Company production directed by Herbert Machiz. The production, which was staged at the Coconut Grove Playhouse in Miami, was not well received and only ran 300 performances.
The first Broadway revival of the play was in 1973. It was produced by the Lincoln Center, at the Vivian Beaumont Theater, and starred Rosemary Harris as Blanche, James Farentino as Stanley and Patricia Conolly as Stella.
Famously, The Simpsons also did an episode in which the play was featured. Ned Flanders took the leading role as Stanley and Marge playing Blanche
The Spring 1988 revival at the Circle in the Square Theatre starred Aidan Quinn opposite Blythe Danner as Blanche and Frances McDormand as Stella.
A highly publicized revival in 1992 starred Alec Baldwin as Stanley and Jessica Lange as Blanche. It was staged at the Ethel Barrymore Theatre, the same theatre that the original production was staged in. This production proved so successful that it was filmed for television. It featured Timothy Carhart as Mitch and Amy Madigan as Stella, as well as future "Sopranos" stars James Gandolfini and Aida Turturro. Gandolfini was Carhart's understudy.
In 1997, Le Petit Theatre du Vieux Carre in New Orleans mounted a 50th Anniversary production, with music by the Marsalis family, starring Michael Arata and Shelly Poncy. In 2009, the Walnut Street Theatre in Philadelphia, where the original pre-Broadway tryout occurred, began a production of the play for its 200th anniversary season.
The 2005 Broadway revival was directed by Edward Hall and produced by The Roundabout Theater Company. It starred John C. Reilly as Stanley, Amy Ryan as Stella, and Natasha Richardson as Blanche. The production would mark Natasha Richardson's final appearance on Broadway owing to her death in 2009 in a skiing accident.
In January 2009, an African-American production of "A Streetcar Named Desire" premiered at Pace University, directed by Steven McCasland. The production starred Lisa Lamothe as Blanche, Stephon O'Neal Pettway as Stanley, and Jasmine Clayton as Stella, and featured Sully Lennon as Allan Gray, the ghost of Blanche's dead husband. The first all-black production of "Streetcar" was probably the one performed by the Summer Theatre Company at Lincoln University in Jefferson City, Missouri, in August 1953 and directed by one of Williams's former classmates at Iowa, Thomas D. Pawley, as noted in the "Streetcar" edition of the "Plays in Production" series published by Cambridge University Press. The black and cross-gendered productions of "Streetcar" since the mid-1950s are much too numerous to list here.
The Sydney Theatre Company production of "A Streetcar Named Desire" premiered on September 5 and ran until October 17, 2009. This production, directed by Liv Ullmann, starred Cate Blanchett as Blanche, Joel Edgerton as Stanley, Robin McLeavy as Stella and Tim Richards as Mitch.
From July 2009 until October 2009, Rachel Weisz and Ruth Wilson starred in a hugely acclaimed revival of the play in London's West End at the Donmar Warehouse directed by Rob Ashford.
The 2010 Writers' Theater of Chicago production of "A Streetcar Named Desire" was located in Glencoe, Illinois. The final performance of this play was on August 15, 2010.
A production at the Guthrie Theater in Minneapolis, starring Ricardo Antonio Chavira as Stanley, Gretchen Egolf as Blanche and Stacia Rice as Stella, ran from July through August 2010.
In November 2010, an Oxford University student production was staged at the Oxford Playhouse which sold out and was critically acclaimed.
In April 2012, Blair Underwood, Nicole Ari Parker, Daphne Rubin-Vega and Wood Harris starred in a multiracial adaptation at the Broadhurst Theatre. Theatre review aggregator "Curtain Critic" gave the production a score of 61 out of 100 based on the opinions of 17 critics.
A production at the Young Vic, London, opened on July 23, 2014, and closed on September 19, 2014. Directed by Benedict Andrews and starring Gillian Anderson, Ben Foster and Vanessa Kirby, this production garnered critical acclaim and is the fastest selling show ever produced by the Young Vic. On September 16, 2014, the performance was relayed live to over one thousand cinemas in the UK as part of the National Theatre Live project to broadcast the best of British theatre live from the London stage to cinemas across the UK and around the world. 
Adaptations.
Film.
In 1951, a film adaptation of the play, directed by Elia Kazan, with Brando, Malden and Hunter reprising their Broadway roles, joined by Vivien Leigh from the London production for the part of Blanche. The movie won four Academy Awards, including three acting awards (Leigh for Best Actress, Malden for Best Supporting Actor and Hunter for Best Supporting Actress), the first time a film won three out of four acting awards (Brando was nominated for Best Actor but lost). Jessica Tandy was the only lead actor from the original Broadway production not to appear in the 1951 film. References to Allan Grey's sexual orientation are essentially removed, due to Motion Picture Production Code restrictions. Instead, the reason for his suicide is changed to a general "weakness". The ending itself was also slightly altered. Stella does not remain with Stanley, as she does in the play.
Pedro Almodóvar's 1999 Academy Award-winning film, "All About My Mother", features a Spanish-language version of the play being performed by some of the supporting characters. However, some of the film's dialogue is taken from the 1951 film version, not the original stage version.
It was noted by many critics that the 2013 Academy Award-winning Woody Allen film, "Blue Jasmine", had much in common with Streetcar and is most likely to be a loose adaptation. It shares a very similar plot and characters, although it has been suitably updated for modern film audiences.
Opera.
In 1995, an opera was adapted and composed by André Previn with a libretto by Philip Littell. It had its premiere at the San Francisco Opera during the 1998–99 season, and featured Renée Fleming as Blanche.
Ballet.
A 1952 ballet production, which was staged at Her Majesty's Theatre in Montreal, featured the music of Alex North, who had composed the music for the 1951 film.
Another ballet production was staged by John Neumeier in Frankfurt in 1983. Music included "Visions fugitives" by Prokofiev and Alfred Schnittke's First Symphony.
In 2012, Scottish Ballet collaborated with theatre and film director Nancy Meckler and international choreographer Annabelle Lopez Ochoa to create a new staging of "A Streetcar Named Desire".
Television.
In 1955, the television program "Omnibus" featured Jessica Tandy reviving her original Broadway performance as Blanche, with her husband, Hume Cronyn, as Mitch. It aired only portions of the play that featured the Blanche and Mitch characters.
The multi-Emmy Award-winning 1984 television version featured Ann-Margret as Blanche, Treat Williams as Stanley, Beverly D'Angelo as Stella and Randy Quaid as Mitch. It was directed by John Erman and the teleplay was adapted by Oscar Saul. The music score by composed by Marvin Hamlisch. Ann-Margret, D'Angelo and Quaid were all nominated for Emmy Awards, but none won. However, it did win four Emmys, including one for cinematographer Bill Butler. Ann-Margret won a Golden Globe award for her performance and Treat Williams was nominated for Best Actor in a Miniseries or TV Movie.
A 1995 television version was based on the highly successful Broadway revival that starred Alec Baldwin and Jessica Lange. However, only Baldwin and Lange were from the stage production. The TV version added John Goodman as Mitch and Diane Lane as Stella. This production was directed by Glenn Jordan. Baldwin, Lange and Goodman all received Emmy Award nominations. Lange won a Golden Globe award (for Best Actress in a Miniseries or TV Movie), while Baldwin was nominated for Best Actor, but did not win.
In 1998, PBS aired a taped version of the opera adaptation that featured the original San Francisco Opera cast. The program received an Emmy Award nomination for Outstanding Classical Music/Dance Program.
Inspirations.
The Desire Line ran from 1920 to 1948, at the height of streetcar use in New Orleans. The route ran down Bourbon, through the Quarter, to Desire Street in the Bywater district, and back up to Canal. Blanche's route in the play—"They told me to take a streetcar named Desire, transfer to one called Cemeteries and ride six blocks and get off at—Elysian Fields!"—is allegorical, taking advantage of New Orleans's colorful street names.
The character of Blanche is thought to be based on Williams' sister, Rose Williams, who struggled with mental health issues and became incapacitated after a lobotomy.
Theatre critic and former actress Blanche Marvin, a friend of Williams, says the playwright used her name for the character Blanche DuBois, named the character's sister Stella after Marvin's former surname "Zohar" (which means "Star"), and took the play's line "I've always depended on the kindness of strangers" from something she said to him.
"A Streetcar Named Success".
"A Streetcar Named Success" is an essay by Tennessee Williams about art and the artist's role in society. It is often included in paper editions of "A Streetcar Named Desire". A version of this essay first appeared in "The New York Times" on November 30, 1947, four days before the opening of "A Streetcar Named Desire". Another version of this essay, entitled "The Catastrophe of Success", is sometimes used as an introduction to "The Glass Menagerie".
Auction record.
On October 1, 2009, Swann Galleries auctioned an unusually fine copy of "A Streetcar Named Desire", New York, 1947, signed by Williams and dated 1976 for $9,000, a record price for a signed copy of the play.

</doc>
<doc id="42866" url="http://en.wikipedia.org/wiki?curid=42866" title="Java Message Service">
Java Message Service

The Java Message Service (JMS) API is a Java Message Oriented Middleware (MOM) API for sending messages between two or more clients. JMS is a part of the Java Platform, Enterprise Edition, and is defined by a specification developed under the Java Community Process as JSR 914. It is a messaging standard that allows application components based on the Java Enterprise Edition (Java EE) to create, send, receive, and read messages. It allows the communication between different components of a "distributed application" to be loosely coupled, reliable, and asynchronous.
General idea of messaging.
Messaging is a form of "loosely coupled" distributed communication, where in this context the term 'communication' can be understood as an exchange of messages between software components. Message-oriented technologies attempt to relax "tightly coupled" communication (such as TCP network sockets, CORBA or RMI) by the introduction of an intermediary component. This approach allows software components to communicate 'indirectly' with each other. Benefits of this include message senders not needing to have precise knowledge of their receivers.
The advantages of messaging include the ability to integrate heterogeneous platforms, reduce system bottlenecks, increase scalability, and respond more quickly to change.
Version history.
JMS 2.0 is maintained under the Java Community Process as JSR 343.
Elements.
The following are JMS elements:
Models.
The JMS API supports two models:
Point-to-point model.
In point-to-point messaging system, messages are routed to an individual consumer which maintains a queue of "incoming" messages. This messaging type is built on the concept of message queues, senders, and receivers. Each message is addressed to a specific queue, and the receiving clients extract messages from the queues established to hold their messages. While any number of producers can send messages to the queue, each message is guaranteed to be delivered, and consumed by one consumer. Queues retain all messages sent to them until the messages are consumed or until the messages expire. If no consumers are registered to consume the messages, the queue holds them until a consumer registers to consume them.
Publish/subscribe model.
The publish/subscribe model supports publishing messages to a particular message topic. "Subscribers" may register interest in receiving messages on a particular message topic. In this model, neither the "publisher" nor the subscriber knows about each other. A good analogy for this is an anonymous bulletin board
JMS provides a way of separating the application from the transport layer of providing data. The same Java classes can be used to communicate with different JMS providers by using the Java Naming and Directory Interface (JNDI) information for the desired provider. The classes first use a "connection factory" to connect to the queue or topic, and then use populate and send or publish the messages. On the receiving side, the clients then receive or subscribe to the messages.
URI scheme.
RFC 6167 defines a jms: URI scheme for the Java Message Service.
Provider implementations.
To use JMS, one must have a JMS provider that can manage the sessions, queues and topics. Starting from Java EE version 1.4, JMS provider has to be contained in "all" Java EE application servers. This can be implemented using the message inflow management of the Java EE Connector Architecture, which was first made available in that version.
The following is a list of JMS providers:
A historical comparison matrix of JMS providers from 2005 is available at http://www.theserverside.com/reviews/matrix.tss

</doc>
<doc id="42869" url="http://en.wikipedia.org/wiki?curid=42869" title="Java Platform, Enterprise Edition">
Java Platform, Enterprise Edition

Java Platform, Enterprise Edition or Java EE is Oracle's enterprise Java computing platform. The platform provides an API and runtime environment for developing and running enterprise software, including network and web services, and other large-scale, multi-tiered, scalable, reliable, and secure network applications. Java EE extends the Java Platform, Standard Edition (Java SE), providing an API for object-relational mapping, distributed and multi-tier architectures, and web services. The platform incorporates a design based largely on modular components running on an application server. Software for Java EE is primarily developed in the Java programming language. The platform emphasizes convention over configuration and annotations for configuration. Optionally XML can be used to override annotations or to deviate from the platform defaults.
Version history.
The platform was known as "Java 2 Platform, Enterprise Edition" or "J2EE" until the name was changed to "Java Platform, Enterprise Edition" or "Java EE" in version 5. The current version is called "Java EE 7".
Standards and specifications.
Java EE is defined by its specification. As with other Java Community Process specifications, providers must meet certain conformance requirements in order to declare their products as "Java EE compliant".
Java EE includes several API specifications, such as RMI, e-mail, JMS, web services, XML, etc., and defines how to coordinate them. Java EE also features some specifications unique to Java EE for components. These include Enterprise JavaBeans, connectors, servlets, JavaServer Pages and several web service technologies. This allows developers to create enterprise applications that are portable and scalable, and that integrate with legacy technologies. A Java EE application server can handle transactions, security, scalability, concurrency and management of the components it is deploying, in order to enable developers to concentrate more on the business logic of the components rather than on infrastructure and integration tasks.
General APIs.
The Java EE APIs includes several technologies that extend the functionality of the base Java SE APIs.
The servlet specification defines a set of APIs to service mainly HTTP requests. It includes the JavaServer Pages (JSP) specification.
The Java API for WebSocket specification defines a set of APIs to service WebSocket connections.
This package defines the root of the JavaServer Faces ("JSF") API. JSF is a technology for constructing user interfaces out of components.
This package defines the component part of the JavaServer Faces API. Since JSF is primarily component oriented, this is one of the core packages. The package overview contains a UML diagram of the component hierarchy.
This package defines the classes and interfaces for Java EE's Expression Language. The Expression Language ("EL") is a simple language originally designed to satisfy the specific needs of web application developers. It is used specifically in JSF to bind components to (backing) beans and in CDI to name beans, but can be used throughout the entire platform.
These packages define the injection annotations for the APIs.
These packages define the context annotations and interfaces for the API.
The Enterprise JavaBean ("EJB") specification defines a set of lightweight APIs that an object container (the EJB container) will support in order to provide transactions (using JTA), remote procedure calls (using RMI or RMI-IIOP), concurrency control, dependency injection and access control for business objects. This package contains the Enterprise JavaBeans classes and interfaces that define the contracts between the enterprise bean and its clients and between the enterprise bean and the ejb container.
This package contains the annotations and interfaces for the declarative validation support offered by the Bean Validation API. Bean Validation provides a unified way to provide constraints on beans (e.g. JPA model classes) that can be enforced cross-layer. In Java EE, JPA honors bean validation constraints in the persistence layer, while JSF does so in the view layer.
This package contains the contracts between a persistence provider and the managed classes and the clients of the Java Persistence API (JPA).
This package provides the Java Transaction API ("JTA") that contains the interfaces and annotations to interact with the transaction support offered by Java EE. Even though this API abstracts from the really low-level details, the interfaces are also considered somewhat low-level and the average application developer in Java EE is either assumed to be relying on transparent handling of transactions by the higher level EJB abstractions, or using the annotations provided by this API in combination with CDI managed beans.
This package provides the core of the Java Authentication SPI ("JASPIC") that contains the interfaces and classes to build authentication modules for secure Java EE applications. Authentication modules are responsible for the interaction dialog with a user (e.g. redirecting to a Form or to an OpenID provider), verifying the user's input (e.g. by doing an LDAP lookup, database query or contacting the OpenID provider with a token) and retrieving a set of groups/roles that the authenticated user is in or has (e.g. by again doing an LDAP lookup or database query).
This package provides the interfaces for interacting directly with Java EE's platform default managed thread pool. A higher-level executor service working on this same thread pool can be used optionally. The same interfaces can be used for user-defined managed thread pools, but this relies on vendor specific configuration and is not covered by the Java EE specification.
This package defines the Java Message Service ("JMS") API. The JMS API provides a common way for Java programs to create, send, receive and read an enterprise messaging system's messages.
This package defines the entry AP for Java EE Batch Applications. The Batch Applications API provides the means to run long running background tasks that possibly involve a large volume of data and which may need to be periodically executed.
This package defines the Java EE Connector Architecture ("JCA") API. Java EE Connector Architecture (JCA) is a Java-based technology solution for connecting application servers and enterprise information systems ("EIS") as part of enterprise application integration ("EAI") solutions. This is a low-level API aimed at vendors that the average application developer typically does not come in contact with.
Web profile.
In an attempt to limit the footprint of web containers, both in physical and in conceptual terms, the web profile was created, a subset of the Java EE specifications.
The Java EE web profile comprises the following:
Certified application servers.
Differences between implementations.
Although by definition all Java EE implementations provide the same base level of technologies (namely, the Java EE spec and the associated APIs), they can differ considerably with respect to extra features (like connectors, clustering, fault tolerance, high availability, security, etc.), installed size, memory footprint, startup time, etc.
Code sample.
The code sample shown below demonstrates how various technologies in Java EE 7 are used together to build a web form for editing a user.
In Java EE a (web) UI can be built using Servlet, JavaServer Pages ("JSP"), or JavaServer Faces ("JSF") with Facelets. The example below uses JSF and Facelets. Not explicitly shown is that the input components use the Java EE Bean Validation API under the covers to validate constraints.
Example Backing Bean class.
To assist the view, Java EE uses a concept called a "Backing Bean". The example below uses and Enterprise JavaBean ("EJB").
Example DAO class.
To implement business logic, Enterprise JavaBean ("EJB") is the dedicated technology in Java EE. For the actual persistence, JDBC or Java Persistence API ("JPA") can be used. The example below uses EJB and JPA. Not explicitly shown is that JTA is used under the covers by EJB to control transactional behavior.
Example Entity class.
For defining entity/model classes Java EE provides the Java Persistence API ("JPA"), and for expressing constraints on those entities it provides the Bean Validation API. The example below uses both these technologies.

</doc>
<doc id="42870" url="http://en.wikipedia.org/wiki?curid=42870" title="Java Platform, Micro Edition">
Java Platform, Micro Edition

Java Platform, Micro Edition, or Java ME, is a Java platform designed for embedded systems (mobile devices are one kind of such systems). Target devices range from industrial controls to mobile phones (especially feature phones) and set-top boxes. Java ME was formerly known as Java 2 Platform, Micro Edition (J2ME).
Java ME was designed by Sun Microsystems, acquired by Oracle Corporation in 2010; the platform replaced a similar technology, PersonalJava. Originally developed under the Java Community Process as JSR 68, the different flavors of Java ME have evolved in separate JSRs. Sun provides a reference implementation of the specification, but has tended not to provide free binary implementations of its Java ME runtime environment for mobile devices, rather relying on third parties to provide their own.
As of 22 December 2006, the Java ME source code is licensed under the GNU General Public License, and is released under the project name phoneME.
As of 2008, all Java ME platforms are currently restricted to JRE 1.3 features and use that version of the class file format (internally known as version 47.0). Should Oracle ever declare a new round of Java ME configuration versions that support the later class file formats and language features, such as those corresponding to JRE 1.5 or 1.6 (notably, generics), it will entail extra work on the part of all platform vendors to update their JREs.[]
Java ME devices implement a "profile". The most common of these are the Mobile Information Device Profile aimed at mobile devices, such as cell phones, and the Personal Profile aimed at consumer products and embedded devices like set-top boxes and PDAs. Profiles are subsets of "configurations", of which there are currently two: the Connected Limited Device Configuration (CLDC) and the Connected Device Configuration (CDC).
There are more than 2.1 billion Java ME enabled mobile phones and PDAs. It is popular in sub $200 devices such as Nokia's Series 40. It was also used on the Bada operating system and on Symbian OS along with native software. Also, there are implementations for Windows CE, Windows Mobile, Maemo, MeeGo and Android available for separate download.
Connected Limited Device Configuration.
The Connected Limited Device Configuration (CLDC) contains a strict subset of the Java-class libraries, and is the minimum amount needed for a Java virtual machine to operate. CLDC is basically used for classifying myriad devices into a fixed configuration.
A configuration provides the most basic set of libraries and virtual-machine features that must be present in each implementation of a J2ME environment. When coupled with one or more profiles, the Connected Limited Device Configuration gives developers a solid Java platform for creating applications for consumer and embedded devices.
The configuration is designed for devices with 160KB to 512KB total memory, which has a minimum of 160KB of ROM and 32KB of RAM available for the Java platform.
Mobile Information Device Profile.
Designed for mobile phones, the Mobile Information Device Profile includes a GUI, and a data storage API, and MIDP 2.0 includes a basic 2D gaming API. Applications written for this profile are called MIDlets. Almost all new cell phones come with a MIDP implementation, and it is now the de facto standard for downloadable cell phone games. However, many cellphones can run only those MIDlets that have been approved by the carrier, especially in North America.
"JSR 271: Mobile Information Device Profile 3" (Final release on 09 Dec, 2009) specified the 3rd generation Mobile Information Device Profile (MIDP3), expanding upon the functionality in all areas as well as improving interoperability across devices. A key design goal of MIDP3 is backward compatibility with MIDP2 content.
Information Module Profile.
The Information Module Profile (IMP) is a profile for embedded, "headless" devices such as vending machines, industrial embedded applications, security systems, and similar devices with either simple or no display and with some limited network connectivity.
Originally introduced by Siemens Mobile and Nokia as JSR-195, IMP 1.0 is a strict subset of MIDP 1.0 except that it doesn't include user interface APIs — in other words, it doesn't include support for the Java package codice_1. JSR-228, also known as IMP-NG, is IMP's next generation that is based on MIDP 2.0, leveraging MIDP 2.0's new security and networking types and APIs, and other APIs such as codice_2 and codice_3, but again it doesn't include UI APIs, nor the game API.
Connected Device Configuration.
The Connected Device Configuration is a subset of Java SE, containing almost all the libraries that are not GUI related. It is richer than CLDC.
Foundation Profile.
The Foundation Profile is a Java ME Connected Device Configuration (CDC) profile. This profile is intended to be used by devices requiring a complete implementation of the Java virtual machine up to and including the entire Java Platform, Standard Edition API. Typical implementations will use some subset of that API set depending on the additional profiles supported. This specification was developed under the Java Community Process.
Personal Basis Profile.
The Personal Basis Profile extends the Foundation Profile to include lightweight GUI support in the form of an AWT subset. This is the platform that BD-J is built upon.
Implementations.
Sun provides a reference implementation of these configurations and profiles for MIDP and CDC. Starting with the JavaME 3.0 SDK, a NetBeans-based IDE will support them in a single IDE.
In contrast to the numerous binary implementations of the Java Platform built by Sun for servers and workstations, Sun does not provide any binaries for the platforms of Java ME targets with the exception of an MIDP 1.0 JRE (JVM) for Palm OS. Sun provides no J2ME JRE for the Microsoft Windows Mobile (Pocket PC) based devices, despite an open-letter campaign to Sun to release a rumored internal implementation of PersonalJava known by the code name "Captain America". Third party implementations like JBlend and JBed are widely used by Windows Mobile vendors like HTC and Samsung.
Operating systems targeting Java ME have been implemented by DoCoMo in the form of DoJa, and by SavaJe as SavaJe OS. The latter company was purchased by Sun in April 2007 and now forms the basis of Sun's JavaFX Mobile. The company IS2T provides a Java ME virtual machine (MicroJvm) for any RTOS and even with no RTOS (then qualified as baremetal). When baremetal, the virtual machine is the OS/RTOS: the device boots in Java.
 provides an open source (LGPL) implementation of an MIDP emulator. This is a Java Applet based emulator and can be embedded in web pages.
The open-source Mika VM aims to implement JavaME CDC/FP, but is not certified as such (certified implementations are required to charge royalties, which is impractical for an open-source project). Consequently devices which use this implementation are not allowed to claim JavaME CDC compatibility.
ESR.
The ESR consortium is devoted to Standards for embedded Java. Especially cost effective Standards.
Typical applications domains are industrial control, machine-to-machine, medical, e-metering, home automation, consumer, human-to-machine-interface, ...
Bibliography.
</dl>

</doc>
<doc id="42871" url="http://en.wikipedia.org/wiki?curid=42871" title="Java Platform, Standard Edition">
Java Platform, Standard Edition

Java Platform, Standard Edition or Java SE is a widely used platform for development and deployment of portable applications for desktop and server environments. Java SE uses the object-oriented Java programming language. It is part of the Java software platform family. Java SE defines a wide range of general purpose APIs – such as Java APIs for the Java Class Library – and also includes the Java Language Specification and the Java Virtual Machine Specification. One of the most well-known implementations of Java SE is Oracle Corporation's Java Development Kit (JDK).
Nomenclature, standards and specifications.
Java SE was known as Java 2 Platform, Standard Edition or J2SE from version 1.2 until version 1.5. The "SE" is used to distinguish the base platform from the Enterprise Edition (Java EE) and Micro Edition (Java ME) platforms. The "2" was originally intended to emphasize the major changes introduced in version 1.2, but was removed in version 1.6. The naming convention has been changed several times over the Java version history. Starting with J2SE 1.4 (Merlin), Java SE has been developed under the Java Community Process, which produces descriptions of proposed and final specifications for the Java platform called Java Specification Requests (JSR). JSR 59 was the umbrella specification for J2SE 1.4 and JSR 176 specified J2SE 5.0 (Tiger). Java SE 6 (Mustang) was released under JSR 270.
Java Platform, Enterprise Edition (Java EE) is a related specification that includes all the classes in Java SE, plus a number that are more useful to programs that run on servers as opposed to workstations.
Java Platform, Micro Edition (Java ME) is a related specification intended to provide a certified collection of Java APIs for the development of software for small, resource-constrained devices such as cell phones, PDAs and set-top boxes.
The Java Runtime Environment (JRE) and Java Development Kit (JDK) are the actual files downloaded and installed on a computer to run or develop Java programs, respectively.
General purpose packages.
java.lang.
The Java package contains fundamental classes and interfaces closely tied to the language and runtime system. This includes the root classes that form the class hierarchy, types tied to the language definition, basic exceptions, math functions, threading, security functions, as well as some information on the underlying native system. This package contains 22 of 32 codice_1 classes provided in JDK 6.
The main classes and interfaces in codice_2 are:
Classes in codice_2 are automatically imported into every source file.
java.lang.ref.
The package provides more flexible types of references than are otherwise available, permitting limited interaction between the application and the Java Virtual Machine (JVM) garbage collector. It is an important package, central enough to the language for the language designers to give it a name that starts with "java.lang", but it is somewhat special-purpose and not used by a lot of developers. This package was added in J2SE 1.2.
Java has an expressive system of references and allows for special behavior for garbage collection. A normal reference in Java is known as a "strong reference." The codice_7 package defines three other types of references — soft, weak, and phantom references. Each type of reference is designed for a specific use.
Each of these reference types extends the class, which provides the method to return a strong reference to the referent object (or codice_8 if the reference has been cleared or if the reference type is phantom), and the method to clear the reference.
The codice_7 also defines the class , which can be used in each of the applications discussed above to keep track of objects that have changed reference type. When a codice_10 is created it is optionally registered with a reference queue. The application polls the reference queue to get references that have changed reachability state.
java.lang.reflect.
Reflection is a constituent of the Java API that lets Java code examine and "reflect" on Java components at runtime and use the reflected members. Classes in the package, along with codice_11 and accommodate applications such as debuggers, interpreters, object inspectors, class browsers, and services such as object serialization and JavaBeans that need access to either the public members of a target object (based on its runtime class) or the members declared by a given class. This package was added in JDK 1.1.
Reflection is used to instantiate classes and invoke methods using their names, a concept that allows for dynamic programming. Classes, interfaces, methods, fields, and constructors can all be discovered and used at runtime. Reflection is supported by metadata that the JVM has about the program.
Techniques.
There are two basic techniques involved in reflection:
Discovery.
Discovery typically starts with an object and calling the method to get the object's codice_12. The codice_12 object has several methods for discovering the contents of the class, for example:
Use by name.
The codice_12 object can be obtained either through discovery, by using the "class literal" (e.g. codice_19) or by using the name of the class (e.g. ). With a codice_12 object, member codice_21, codice_22, or codice_23 objects can be obtained using the symbolic name of the member. For example:
codice_21, codice_22, and codice_23 objects can be used to dynamically access the represented member of the class. For example:
Arrays and proxies.
The codice_46 package also provides an class that contains static methods for creating and manipulating array objects, and since J2SE 1.3, a class that supports dynamic creation of proxy classes that implement specified interfaces.
The implementation of a codice_47 class is provided by a supplied object that implements the interface. The codice_48's method is called for each method invoked on the proxy object—the first parameter is the proxy object, the second parameter is the codice_21 object representing the method from the interface implemented by the proxy, and the third parameter is the array of parameters passed to the interface method. The codice_39 method returns an codice_32 result that contains the result returned to the code that called the proxy interface method.
java.io.
The package contains classes that support input and output. The classes in the package are primarily stream-oriented; however, a class for random access files is also provided. The central classes in the package are and , which are abstract base classes for reading from and writing to byte streams, respectively. The related classes and are abstract base classes for reading from and writing to character streams, respectively. The package also has a few miscellaneous classes to support interactions with the host file system.
Streams.
The stream classes follow the decorator pattern by extending the base subclass to add features to the stream classes. Subclasses of the base stream classes are typically named for one of the following attributes:
The stream subclasses are named using the naming pattern codice_52 where codice_53 is the name describing the feature and codice_54 is one of codice_55, codice_56, codice_57, or codice_58.
The following table shows the sources/destinations supported directly by the codice_59 package:
Other standard library packages provide stream implementations for other destinations, such as the codice_55 returned by the method or the Java EE class.
Data type handling and processing or filtering of stream data is accomplished through stream filters. The filter classes all accept another compatible stream object as a parameter to the constructor and "decorate" the enclosed stream with additional features. Filters are created by extending one of the base filter classes , , , or .
The codice_57 and codice_58 classes are really just byte streams with additional processing performed on the data stream to convert the bytes to characters. They use the default character encoding for the platform, which as of J2SE 5.0 is represented by the returned by the static method. The class converts an codice_55 to a codice_57 and the class converts an codice_56 to a codice_58. Both these classes have constructors that support specifying the character encoding to use. If no encoding is specified, the program uses the default encoding for the platform.
The following table shows the other processes and filters that the codice_59 package directly supports. All these classes extend the corresponding codice_68 class.
Random access.
The class supports "random access" reading and writing of files. The class uses a "file pointer" that represents a byte-offset within the file for the next read or write operation. The file pointer is moved implicitly by reading or writing and explicitly by calling the or methods. The current position of the file pointer is returned by the method.
File system.
The class represents a file or directory path in a file system. codice_69 objects support the creation, deletion and renaming of files and directories and the manipulation of file attributes such as "read-only" and "last modified timestamp". codice_69 objects that represent directories can be used to get a list of all the contained files and directories.
The class is a file descriptor that represents a source or sink (destination) of bytes. Typically this is a file, but can also be a console or network socket. codice_71 objects are used to create codice_69 streams. They are obtained from codice_69 streams and codice_74 sockets and datagram sockets.
java.nio.
In J2SE 1.4, the package (NIO or New I/O) was added to support memory-mapped I/O, facilitating I/O operations closer to the underlying hardware with sometimes dramatically better performance. The codice_75 package provides support for a number of buffer types. The subpackage provides support for different character encodings for character data. The subpackage provides support for "channels," which represent connections to entities that are capable of performing I/O operations, such as files and sockets. The codice_76 package also provides support for fine-grained locking of files.
java.math.
The package supports multiprecision arithmetic (including modular arithmetic operations) and provides multiprecision prime number generators used for cryptographic key generation. The main classes of the package are:
java.net.
The package provides special IO routines for networks, allowing HTTP requests, as well as other common transactions.
java.text.
The package implements parsing routines for strings and supports various human-readable languages and locale-specific parsing.
java.util.
Data structures that aggregate objects are the focus of the package. Included in the package is the Collections API, an organized data structure hierarchy influenced heavily by the design patterns considerations.
Special purpose packages.
java.applet.
Created to support Java applet creation, the package lets applications be downloaded over a network and run within a guarded sandbox. Security restrictions are easily imposed on the sandbox. A developer, for example, may apply a digital signature to an applet, thereby labeling it as safe. Doing so allows the user to grant the applet permission to perform restricted operations (such as accessing the local hard drive), and removes some or all the sandbox restrictions. Digital certificates are issued by certificate authorities.
java.beans.
Included in the package are various classes for developing and manipulating beans, reusable components defined by the JavaBeans architecture. The architecture provides mechanisms for manipulating properties of components and firing events when those properties change.
The APIs in codice_80 are intended for use by a bean editing tool, in which beans can be combined, customized, and manipulated. One type of bean editor is a GUI designer in an integrated development environment.
java.awt.
The , or Abstract Window Toolkit, provides access to a basic set of GUI widgets based on the underlying native platform's widget set, the core of the GUI event subsystem, and the interface between the native windowing system and the Java application. It also provides several basic layout managers, a datatransfer package for use with the Clipboard and Drag and Drop, the interface to input devices such as mice and keyboards, as well as access to the system tray on supporting systems. This package, along with codice_81 contains the largest number of enums (7 in all) in JDK 6.
java.rmi.
The package provides Java remote method invocation to support remote procedure calls between two java applications running in different JVMs.
java.security.
Support for security, including the message digest algorithm, is included in the package.
java.sql.
An implementation of the JDBC API (used to access SQL databases) is grouped into the package.
javax.rmi.
The package provides the support for the remote communication between applications, using the RMI over IIOP protocol. This protocol combines RMI and CORBA features.
javax.swing.
Swing is a collection of routines that build on codice_82 to provide a platform independent widget toolkit. uses the 2D drawing routines to render the user interface components instead of relying on the underlying native operating system GUI support.
This package contains the largest number of classes (133 in all) in JDK 6. This package, along with codice_82 also contains the largest number of enums (7 in all) in JDK 6. It supports pluggable looks and feels (PLAFs) so that widgets in the GUI can imitate those from the underlying native system. Design patterns permeate the system, especially a modification of the model-view-controller pattern, which loosens the coupling between function and appearance. One inconsistency is that (as of J2SE 1.3) fonts are drawn by the underlying native system, and not by Java, limiting text portability. Workarounds, such as using bitmap fonts, do exist. In general, "layouts" are used and keep elements within an aesthetically consistent GUI across platforms.
javax.swing.text.html.parser.
The package provides the error tolerant HTML parser that is used for writing various web browsers and web bots.
javax.xml.bind.annotation.
The package contains the largest number of Annotation Types (30 in all) in JDK 6. It defines annotations for customizing Java program elements to XML Schema mapping.
OMG packages.
org.omg.CORBA.
The package provides the support for the remote communication between applications using the General Inter-ORB Protocol and supports other features of the common object request broker architecture. Same as RMI and RMI-IIOP, this package is for calling remote methods of objects on other virtual machines (usually via network).
This package contains the largest number of codice_84 classes (45 in all) in JDK 6. From all communication possibilities CORBA is portable between various languages; however, with this comes more complexity.
org.omg.PortableInterceptor.
The package contains the largest number of interfaces (39 in all) in JDK 6. It provides a mechanism to register ORB hooks through which ORB services intercept the normal flow of execution of the ORB.
Critical security issues with the Java SE plugin.
Several critical security vulnerabilities have been reported, the most recent in January 2013. Security alerts from Oracle announce critical security-related patches to Java SE.

</doc>
<doc id="42874" url="http://en.wikipedia.org/wiki?curid=42874" title="Elia Kazan">
Elia Kazan

Elia Kazan (born Elias Kazantzoglou, Greek: Ηλίας Καζαντζόγλου; September 7, 1909 – September 28, 2003) was a Greek-American director, producer, writer and actor, described by "The New York Times" as "one of the most honored and influential directors in Broadway and Hollywood history".
He was born in Istanbul, to Cappadocian Greek parents. After studying acting at Yale, he acted professionally for eight years, later joining the Group Theater in 1932, and co-founded the Actors Studio in 1947. With Robert Lewis and Cheryl Crawford, he introduced Method acting to the American stage and cinema as a new form of self-expression and psychological "realism." Kazan acted in only a few films, including "City for Conquest" (1940).
Kazan introduced a new generation of unknown young actors to the movie audiences, including Marlon Brando and James Dean. Noted for drawing out the best dramatic performances from his actors, he directed 21 actors to Oscar nominations, resulting in nine wins. He became "one of the consummate filmmakers of the 20th century" after directing a string of successful films, including "A Streetcar Named Desire" (1951), "On the Waterfront" (1954), and "East of Eden" (1955). During his career, he won two Oscars as Best Director and received an Honorary Oscar, won three Tony Awards, and four Golden Globes. Among the other actors he introduced to movie audiences were Warren Beatty, Carroll Baker, Julie Harris, Andy Griffith, Lee Remick, Rip Torn, Eli Wallach, Eva Marie Saint, Martin Balsam, Fred Gwynne, and Pat Hingle.
His films were concerned with personal or social issues of special concern to him. Kazan writes, "I don't move unless I have some empathy with the basic theme." His first such "issue" film was "Gentleman's Agreement" (1947), with Gregory Peck, which dealt with anti-Semitism in America. It received 8 Oscar nominations and 3 wins, including Kazan's first for Best Director. It was followed by "Pinky", one of the first films to address racial prejudice against blacks. In 1954, he directed "On the Waterfront", a film about union corruption on the New York harbor waterfront, which some consider "one of the greatest films in the history of international cinema." "A Streetcar Named Desire" (1951), an adaptation of the stage play which he had also directed, received 12 Oscar nominations, winning 4, and was Marlon Brando's breakthrough role. In 1955, he directed John Steinbeck's "East of Eden", which introduced James Dean to movie audiences, making him an overnight star.
A turning point in Kazan's career came with his testimony as a "friendly witness" before the House Committee on Un-American Activities in 1952 at the time of the Hollywood blacklist, which brought him strong negative reactions from many liberal friends and colleagues. Kazan later explained that he took "only the more tolerable of two alternatives that were either way painful and wrong." Kazan influenced the films of the 1950s and '60s with his provocative, issue-driven subjects. Director Stanley Kubrick called him, "without question, the best director we have in America, [and] capable of performing miracles with the actors he uses.":36 Film author Ian Freer concludes that "if his achievements are tainted by political controversy, the debt Hollywood—and actors everywhere—owes him is enormous." In 2010, Martin Scorsese co-directed the documentary film "A Letter to Elia" as a personal tribute to Kazan.
Early life.
Elia Kazan was born in the Fener district of Istanbul, to Cappadocian Greek parents originally from Kayseri in Anatolia. His parents, George and Athena Kazantzoglou (née Shishmanoglou), emigrated to the United States when he was four years old. He was named after his paternal grandfather, Elia Kazantzoglou. His maternal grandfather was Isaak Shishmanoglou. Elia's brother, Avraam, was born in Berlin and later became a psychiatrist.:21
As a young boy, he was remembered as being shy, and his college classmates described him as more of a loner. Much of his early life was portrayed in his autobiographical book, "America America", which he made into a film in 1963. In it, he describes his family as "alienated" from both their parents' Greek Orthodox values and from those of mainstream America.:23 His mother's family were cotton merchants who imported cotton from England, and sold it wholesale. His father became a rug merchant after emigrating to the United States, and expected that his son would go into the family business.
After attending public schools in New York, he enrolled at Williams College in Massachusetts, where he helped pay his way by waiting tables and washing dishes, although he still graduated cum laude. He also worked as a bartender at various fraternities, but never joined one. While a student at Williams, he earned the nickname "Gadg," for gadget, because, he said, "I was small, compact, and handy to have around."
In "America America" he tells how, and why, his family left Turkey and moved to America. Kazan notes that much of it came from stories that he heard as a young boy. He says during an interview that "it's all true: the wealth of the family was put on the back of a donkey, and my uncle, really still a boy, went to Istanbul ... to gradually bring the family there to escape the oppressive circumstances... It's also true that he lost the money on the way, and when he got there he swept rugs in a little store."
Kazan notes some of the controversial aspects of what he put in the film. He writes, "I used to say to myself when I was making the film that America was a dream of total freedom in all areas." To make his point, the character who portrays Kazan's uncle Avraam kisses the ground when he gets through customs, while the Statue of Liberty and the American flag are in the background. Kazan had considered whether that kind of scene might be too much for American audiences:
Before undertaking the film, Kazan wanted to confirm many of the details about his family's background. At one point, he sat his parents down and recorded their answers to his questions. He remembers eventually asking his father a "deeper question: 'Why America? What were you hoping for?'" His mother gave him the answer, however: "A.E. brought us here." Kazan states that "A.E. was my uncle Avraam Elia, the one who left the Anatolian village with the donkey. At twenty-eight, somehow—this was the wonder—he made his way to New York. He sent home money and in time brought my father over. Father sent for my mother and my baby brother and me when I was four."
Kazan writes of the movie, "It's my favorite of all the films I've made; the first film that was entirely mine."
Stage career.
Group Theater.
In 1932, after spending two years at the Yale University School of Drama, he moved to New York City to become a professional stage actor. His first opportunity came with a small group of actors engaged in presenting plays containing “social commentary”. They were called the Group Theater, which showcased many lesser known plays with deep social or political messages. After struggling to be accepted by them, he discovered his first strong sense of self in America within the "family of the Group Theater, and more loosely in the radical social and cultural movements of the time," writes film author Joanna E. Rapf.:23
In Kazan's autobiography, Kazan writes of the "lasting impact on him of the Group," noting in particular, Lee Strasberg and Harold Clurman as "father figures", along with his close friendship with playwright Clifford Odets. Kazan, during an interview with Michel Ciment, describes the Group:
Kazan, in his autobiography, also describes Strasberg as a vital leader of the group:
Kazan's first national success came as New York theatrical director. Although initially he worked as an actor on stage, and told early in his acting career that he had no acting ability, he surprised many critics by becoming one of the Group’s most capable actors. In 1935 he played the role of a strike-leading taxi driver in a drama by Clifford Odets, "Waiting for Lefty", and his performance was called "dynamic," leading some to describe him as the "proletarian thunderbolt.":23
Among the themes that would run through all of his work were "personal alienation and an outrage over social injustice", writes film critic William Baer. Other critics have likewise noted his "strong commitment to the social and social psychological—rather than the purely political—implications of drama".:33
By the mid-1930s, when he was 26, he began directing a number of the Group Theater's plays. In 1942 he achieved his first notable success by directing a Pulitzer prize-winning play by Thornton Wilder, "The Skin of Our Teeth", starring Montgomery Clift and Tallulah Bankhead. He then went on to direct "Death of a Salesman," by Arthur Miller, and then directed "A Streetcar Named Desire", written by Tennessee Williams. Kazan's wife, Molly Thacher, the reader for the Group, discovered Williams and awarded him a "prize that launched his career."
The Group Theater's summer rehearsal headquarters was at Pine Brook Country Club, located in the countryside of Nichols, Connecticut, during the 1930s and early 1940s. Along with Kazan were numerous other artists: Harry Morgan, John Garfield, Luise Rainer, Frances Farmer, Will Geer, Howard Da Silva, Clifford Odets, Lee J. Cobb and Irwin Shaw.
Actors Studio.
In 1947, he founded the Actors Studio, a non-profit workshop, with actors Robert Lewis and Cheryl Crawford. It soon became famous for promoting "Method," a style of theater and acting involving "total immersion of actor into character," writes film author Ian Freer. According to Rapf, "the Studio rode the bandwagon of method fashionability, and Kazan was its clear star and attraction.":97 Within a short time, as word spread, "everyone wanted to be at the Studio—not least because of the chance of being in a Kazan production in one medium or another.":97
Among its first students were Marlon Brando, Montgomery Clift, Julie Harris, Eli Wallach, Karl Malden, Patricia Neal, Mildred Dunnock, James Whitmore, and Maureen Stapleton. In 1951, Lee Strasberg became its director, and it remained a non-profit enterprise, eventually considered "the nation's most prestigious acting school," according to film historian James Lipton.
Student James Dean, in a letter home to his parents, writes that Actors Studio was "the greatest school of the theater [and] the best thing that can happen to an actor". Playwright Tennessee Williams said of its actors: "They act from the inside out. They communicate emotions they really feel. They give you a sense of life." Contemporary directors like Sidney Lumet, a former student, have intentionally used actors such as Al Pacino, a former student skilled in "Method".
Kazan directed one of the Studio's brightest young talents, Marlon Brando, in the Tennessee Williams play "A Streetcar Named Desire". He cast him again in the film version in 1951, which made Brando a star and won 4 Oscars, and was nominated for 12.
Among the other Broadway plays he directed were "Cat on a Hot Tin Roof", "Sweet Bird of Youth", "The Dark at the Top of the Stairs" and "Tea and Sympathy". This led some, such as theater critic Eric Bentley, to write that "the work of Elia Kazan means more to the American theater than that of any current writer." Film critic David Richard Jones adds that Kazan, during the 1940s and 1950s, was one of America's foremost Stanislavskians, and "influenced thousands of contemporaries" in the theatre, film, and the Actors Studio that he helped found.
Film career.
At the height of his stage success, Kazan then turned to Hollywood where he soon demonstrated equal skill as director of motion pictures. He first directed two short films, but his first feature film was "A Tree Grows in Brooklyn" (1945), one his first attempts to film dramas focused on contemporary concerns, which became his forte. Two years later he directed "Gentleman's Agreement", where he tackled a seldom-discussed topic in America, antisemitism, for which he won his first Oscar as Best Director. In 1949 he again dealt with controversial subject when he directed "Pinky", which dealt with issues of racism in America, and was nominated for 3 Academy Awards.
In 1947, he directed the courtroom drama "Boomerang!", and in 1950 he directed "Panic in the Streets", starring Richard Widmark, in a thriller shot on the streets of New Orleans. In that film, Kazan experimented with a documentary style of cinematography, which succeeded in "energizing" the action scenes. He won the Venice Film Festival, International Award as director, and the film also won two Academy Awards. Kazan had requested that Zero Mostel also act in the film, despite Mostel being "blacklisted" as a result of HUAC testimony a few years earlier. Kazan writes of his decision:
Marlon Brando.
In 1951, after introducing and directing one of the Actors Studio's brightest young talents, Marlon Brando, in the stage version, he went on to cast him in the film version of the play, "A Streetcar Named Desire", which made Brando a star and won 4 Oscars, being nominated for 12. The film popularized Method acting with Brando's role as the earthy and unmannered Stanley Kowalski opposite the classical dignity of British actress, Vivien Leigh, as his sister-in-law. Despite the plaudits, the film was considered a step back cinematically with the feel of filmed theater, however Kazan did at first use a more open setting but then felt compelled to revert to the stage atmosphere to remain true to the script. He explains:
Brando's role as a virtually unknown actor at age 27, would "catapult him to stardom."
His next film was "Viva Zapata!" (1952) which also starred Marlon Brando playing the role of Mexican revolutionary Emiliano Zapata. The film added real atmosphere with the use of location shots and strong character accents. Kazan called this his "first real film" because of those factors.
In 1954 he again used Brando as co-star in "On the Waterfront". As a continuation of the socially relevant themes that he developed in New York, the film exposed corruption within New York’s longshoremen’s union. It too was nominated for 12 Academy Awards, but won 8, including Best Picture, Best Director and Best Actor, for Marlon Brando. To some critics, Brando gives the "best performance in American film history," playing an ex-boxer, Terry Malloy, who is persuaded by a priest to inform on corrupt unions. Surprisingly, Brando writes that he was actually disappointed with his acting upon first watching the screening:
Karl Malden.
Actor Karl Malden became an early student at the Group Theater in 1937, where he first began acting under Kazan's direction. Kazan would play a "prominent role in Malden's stage and film career", including convincing him to change his name from Mladen Sekulovich. He played a drunken sailor in Kazan's "Truckline Cafe," which also included a young Marlon Brando. In 1947, he co-starred in the stage play "All My Sons," written by Arthur Miller, with Kazan directing, and began being recognized as a serious actor.
However, his first major stage success was his role as an awkward suitor of Jessica Tandy in the Broadway production of "A Streetcar Named Desire," which also helped make Brando a star on stage. After two years in the role, he played the same part in the 1951 film version, this time playing opposite Vivien Leigh, where he won his first Oscar for Best Supporting Actor. Kazan next directed him in "On the Waterfront" (1954), where he was also nominated as Best Supporting Actor for his role as a sympathetic priest. In 1956, Kazan directed him in a starring role in "Baby Doll", alongside Carroll Baker and Eli Wallach, a controversial story written by Tennessee Williams, and he was nominated for a Golden Globe Award for Best Actor.
Malden remained friends with Kazan despite his unpopular appearance at the House Un-American Activities Committee in 1952. Many mutual "friends who turned on Kazan also refused to speak to Malden." He furthered his support in 1999, when, as a member of the Board of Governors of the Academy of Motion Picture Arts and Sciences, he proposed that they give Kazan an honorary Oscar for "lifetime achievement". Malden's proposal was bold, as film festivals, critics associations, and the American Film Institute, had already refused to bestow similar honors because of Kazan's testimony given nearly 50 years earlier. Malden recalled giving his proposal:
According to the "Los Angeles Times", when Malden finished speaking, "he was greeted by a rousing burst of applause."
Eva Marie Saint.
"On the Waterfront" was also the screen debut for Eva Marie Saint, who won the Oscar for Best Supporting Actress for her role. Saint recalls that Kazan selected her for the role after he had her do an improvisational skit with Brando playing the other character. She had no idea that he was looking to fill any particular film part, however, but remembers that Kazan set up the scenario with Brando which brought out surprising emotions:
"Life" magazine described "On the Waterfront" as the "most brutal movie of the year" but with "the year's tenderest love scenes," and stating that Saint was a "new discovery" in films. In its cover story about Saint, it speculated that it will probably be as Edie in "On the Waterfront" that she "starts her real trip to fame."
The film made use of extensive on-location street scenes and waterfront shots, and included a notable score by composer Leonard Bernstein.
James Dean.
After the success of "On the Waterfront" he went on to direct the screen adaptation of John Steinbeck's novel, "East of Eden" in 1955. As director, Kazan again used another unknown actor, James Dean. Kazan had seen Dean on stage in New York and after an audition gave him the starring role along with an exclusive contract with Warner Bros. Dean flew back to Los Angeles with Kazan in 1954, the first time he had ever flown in a plane, bringing his clothes in a brown paper bag.:194 The film's success introduced James Dean to the world and established him as a popular actor. He went on to star in "Rebel Without a Cause" (1955), directed by Kazan's friend, Nicholas Ray, and then "Giant", (dir. George Stevens, 1956)
Author Douglas Rathgeb describes the difficulties Kazan had in turning Dean into a new star, noting how Dean was a controversial figure at Warner Bros. from the time he arrived. There were rumors that he "kept a loaded gun in his studio trailer; that he drove his motorcycle dangerously down studio streets or sound stages; that he had bizarre and unsavory friends." As a result, Kazan was forced to "baby-sit the young actor in side-by-side trailers," so he wouldn't run away during production. Co-star Julie Harris worked overtime to quell Dean's panic attacks. In general, Dean was oblivious to Hollywood's methods, and Rathgeb notes that "his radical style did not mesh with Hollywood's corporate gears."
Dean himself was amazed at his own performance on screen when he later viewed a rough cut of the film. Kazan had invited director Nicholas Ray to a private showing, with Dean, as Ray was looking for someone to play the lead in "Rebel Without a Cause". Ray watched Dean's powerful acting on the screen; but it didn't seem possible that it was the same person in the room. Ray felt Dean was shy and totally withdrawn as he sat there hunched over. "Dean himself did not seem to believe it," notes Rathgeb. "He watched himself with an odd, almost adolescent fascination, as if he were admiring someone else."
The film also made good use of on-location and outdoor scenes, along with an effective use of early widescreen format, making the film one of Kazan's most accomplished works. James Dean died the following year, at the age of 24, in an accident with his sports car outside of Los Angeles. He had only made three films, and the only completed film he ever saw was "East of Eden".
Warren Beatty.
In 1961, he introduced Warren Beatty in his first screen appearance with a starring role in "Splendor in the Grass" (1961), with Natalie Wood; the film was nominated for two Oscars and won one. Author Peter Biskind points out that Kazan "was the first in a string of major directors Beatty sought out, mentors or father figures from whom he wanted to learn." Biskind notes also that they "were wildly dissimilar—mentor vs. protege, director vs. actor, immigrant outsider vs. native son. Kazan was armed with the confidence born of age and success, while Beatty was virtually aflame with the arrogance of youth." Kazan recalls his impressions of Beatty:
Biskind describes an episode during the first week of shooting, where Beatty was angered at something Kazan said: "The star lashed out at the spot where he knew Kazan was most vulnerable, the director's friendly testimony before the HUAC. He snapped, 'Lemme ask you something—why did you name all those names?'"
Beatty himself recalled the episode: "In some patricidal attempt to stand up to the great Kazan, I arrogantly and stupidly challenged him on it." Biskind describes how "Kazan grabbed his arm, asking, 'What did you say?' and dragged him off to a tiny dressing room ... whereupon the director proceeded to justify himself for two hours." Beatty, years later, during a Kennedy Center tribute to Kazan, stated to the audience that Kazan "had given him the most important break in his career.":23
Natalie Wood.
Beatty's costar, Natalie Wood, was in a transition period in her career, having mostly been cast in roles as a child or teenager, and she was now hoping to be cast in adult roles. Biographer Suzanne Finstad notes that a "turning point" in her life as an actress was upon seeing the film "A Streetcar Named Desire": "She was transformed, in awe of Kazan and of Vivien Leigh's performance... [who] became a role model for Natalie.":107 In 1961, after a "series of bad films, her career was already in decline," notes Rathgeb.:199 Kazan himself writes that the "sages" of the film community declared her as "washed up" as an actress, although he still wanted to interview her for his next film:
Kazan cast her as the female lead in "Splendor in the Grass", and her career rebounded. Finstad feels that despite Wood never receiving training in Method acting techniques, "working with Kazan brought her to the greatest emotional heights of her career. The experience was exhilarating but wrenching for Natalie, who faced her demons on "Splendor."":259 She adds that a scene in the film, as a result of "Kazan's wizardry ... produced a hysteria in Natalie that may be her most powerful moment as an actress.":260
Actor Gary Lockwood, who also acted in the film, felt that "Kazan and Natalie were a terrific marriage, because you had this beautiful girl, and you had somebody that could get things out of her." Kazan's favorite scene in the movie was the last one, when Wood goes back to see her lost first love, Bud (Beatty). "It's terribly touching to me. I still like it when I see it," writes Kazan.:263 "And I certainly didn't need to tell her how to play it. She understood it perfectly."
Screenwriters.
Another aspect that contributed to the power and intensity of his films was his close collaboration with writers. On Broadway, he worked with Arthur Miller, Tennessee Williams, and William Inge; in film, he worked again with Willams ("A Streetcar Named Desire" and "Baby Doll"), Inge ("Splendor in the Grass"), Budd Schulberg ("On the Waterfront" and "A Face in the Crowd"), John Steinbeck ("Viva Zapata!"), and Harold Pinter ("The Last Tycoon"). As an instrumental figure in the careers of many of the best writers of his time, "he always treated them and their work with the utmost respect." In 2009, a previously unproduced screenplay by Williams, "The Loss of a Teardrop Diamond", was released as a film. Williams wrote the screenplay specifically for Kazan to direct during the 1950s.
Williams became one of Kazan's closest and most loyal friends, and Kazan often pulled Williams out of "creative slumps" by redirecting his focus with new ideas. In 1959, in a letter to Kazan, he writes, “Some day you will know how much I value the great things you did with my work, how you lifted it above its measure by your great gift.”
Among Kazan's other films were "Panic in the Streets" (1950), "East of Eden" (1955), "Baby Doll" (1956), "Wild River" (1960), and "The Last Tycoon" (1976).
Literary career.
In between his directing work he wrote four best-selling novels, including "America, America", and "The Arrangement", both of which tell the story of Kazan's Greek immigrant ancestors. Both novels were later made into films.
Directing style.
Preference for unknown actors.
Kazan strove for "cinematic realism," a quality he often achieved by discovering and working with unknown actors, many of whom treated him as their mentor, which gave him the flexibility to depict "social reality with both accuracy and vivid intensity." He also felt that casting the right actors accounted for 90% of a movie's ultimate success or failure. As a result of his efforts, he also gave actors such as Lee Remick, Jo Van Fleet, Warren Beatty, Andy Griffith, James Dean, and Jack Palance, their first major movie roles. He explained to director and producer George Stevens, Jr. that he felt that "big stars are barely trained or not very well trained. They also have bad habits... they're not pliable anymore." Kazan also describes how and why he gets to know his actors on a personal level:
Kazan goes on to describe how he got to understand James Dean, as an example:
Topics of personal and social realism.
Kazan chooses his subjects to express personal and social events that he is familiar with. He describes his thought process before taking on a project:
Film historian Joanna E. Rapf notes that among the methods Kazan used in his work with actors, was his initial focus on "reality", although his style was not defined as "naturalistic." She adds: "He respects his script, but casts and directs with a particular eye for expressive action and the use of emblematic objects.":33 Kazan himself states that "unless the character is somewhere in the actor himself, you shouldn't cast him.":33
In his later years he changed his mind about some of the philosophy behind the Group Theater, in that he no longer felt that the theater was a
"collective art," as he once believed:
Film author Peter Biskind described Kazan's career as "fully committed to art and politics, with the politics feeding the work.":22 Kazan, however, has downplayed that impression:
Nonetheless, there have been clear messages in some of his films that involved politics in various ways. In 1954, he directed "On the Waterfront", written by screenwriter Budd Schulberg, which was a film about union corruption in New York. Some critics consider it "one of the greatest films in the history of international cinema." Another political film was "A Face in the Crowd" (1957). His protagonist, played by Andy Griffith (in his film debut) is not a politician, yet his career suddenly becomes deeply involved in politics. According to film author Harry Keyishian, Kazan and screenwriter Budd Schulberg were using the film to warn audiences about the dangerous potential of the new medium of television. Kazan explains that he and Schulberg were trying to warn "of the power TV would have in the political life of the nation." Kazan states, "Listen to what the candidate says; don't be taken in by his charm or his trust-inspiring personality. Don't buy the advertisement; buy what's in the package."
Use of "Method" acting.
As a product of the Group Theater and Actors Studio, he was most noted for his use of "Method" actors, especially Brando and Dean. During an interview in 1988, Kazan said, "I did whatever was necessary to get a good performance "including" so-called Method acting. I made them run around the set, I scolded them, I inspired jealousy in their girlfriends... The director is a "desperate beast!" ... You don't deal with actors as dolls. You deal with them as people who are poets to a certain degree." Actor Robert De Niro called him a "master of a new kind of psychological and behavioral faith in acting."
Kazan was aware of the limited range of his directing abilities:
He explained that he tried to inspire his actors to offer ideas:
Kazan, however, held strong ideas about the scenes, and would try to merge an actor's suggestions and inner feelings with his own. Despite the strong eroticism created in "Baby Doll", for example, he set limits. Before shooting a seduction scene between Eli Wallach and Carroll Baker, he privately asked Wallach, "Do you think you actually go through with seducing that girl?" Wallach writes, "I hadn't thought about that question before, but I answered ... 'No.'" Kazan replies, "Good idea, play it that way." Kazan, many years later, explained his rationale for scenes in that film:
Being an "actor's director".
Joanna Rapf adds that Kazan was most admired for his close work with actors, noting that director Nicholas Ray considered him "the best actor's director the United States has ever produced.":22 Film historian Foster Hirsch explains that "he created virtually a new acting style, which was the style of the Method... [that] allowed for the actors to create great depth of psychological realism."
Among the actors who describe Kazan as an important influence in their career were Patricia Neal, who co-starred with Andy Griffith in "A Face in the Crowd" (1957): "He was very good. He was an actor and he knew how we acted. He would come and talk to you privately. I liked him a lot." Anthony Franciosa, a supporting actor in the film, explains how Kazan encouraged his actors:
However, in order to get quality acting from Andy Griffith, in his first screen appearance, and achieve what Schickel calls "an astonishing movie debut,":338 Kazan would often take surprising measures. In one important and highly emotional scene, for example, Kazan had to give Griffith fair warning: "I may have to use extraordinary means to make you do this. I may have to get out of line. I don't know any other way of getting an extraordinary performance out of an actor."
Actress Terry Moore calls Kazan her "best friend," and notes that "he made you feel better than you thought you could be. I never had another director that ever touched him. I was spoiled for life." "He would find out if your life was like the character," says Carroll Baker, star of "Baby Doll", "he was the best director with actors."
Kazan's need to remain close to his actors continued up to his last film, "The Last Tycoon" (1976). He remembers that Robert De Niro, the star of the film, "would do almost anything to succeed," and even cut his weight down from 170 to 128 pounds for the role. Kazan adds that De Niro "is one of a select number of actors I've directed who work hard at their trade, and the only one who asked to rehearse on Sundays. Most of the others play tennis. Bobby and I would go over the scenes to be shot.":766
The powerful dramatic roles Kazan brought out from many of his actors was due, partly, to his ability to recognize their personal character traits. Although he didn't know De Niro before this film, for example, Kazan later writes, "Bobby is more meticulous ... he's very imaginative. He's very precise. He figures everything out both inside and outside. He has good emotion. He's a character actor: everything he does he calculates. In a good way, but he calculates.":210 Kazan developed and used those personality traits for his character in the film.:766 Although the film did poorly at the box office, some reviewers praised De Niro's acting. Film critic Marie Brenner writes that "for De Niro, it is a role that surpasses even his brilliant and daring portrayal of Vito Corleone in "The Godfather, part II", ... [his] performance deserves to be compared with the very finest."
Marlon Brando, in his autobiography, goes into detail about the influence Kazan had on his acting:
HUAC testimony.
Until his death, Kazan remained controversial in some circles for testimony he gave before the House Committee on Un-American Activities (HUAC) in 1952, a period that many, such as journalist Michael Mills, feel was "the most controversial period in Hollywood history." When he was in his mid-20s, during the Depression years 1934 to 1936, he had been a member of the American Communist Party in New York, for a year and a half.
In April 1952, the Committee called on Kazan, under oath, to identify Communists from that period 16 years earlier. Kazan initially refused to provide names, but eventually named eight former Group Theater members who he said had been Communists: Clifford Odets, J. Edward Bromberg, Lewis Leverett, Morris Carnovsky, Phoebe Brand, Tony Kraber, Ted Wellman, and Paula Miller, who later married Lee Strasberg. He testified that Odets quit the party at the same time that he did. All the persons named were already known to HUAC, however. The move cost Kazan many friends within the film industry, including playwright Arthur Miller.
Kazan would later write in his autobiography of the "warrior pleasure at withstanding his 'enemies.'"
When Kazan received an Honorary Academy Award in 1999, the audience was noticeably divided in their reaction, with some including Nick Nolte, Ed Harris, Ian McKellen and Amy Madigan refusing to applaud, and many others, such as actors Kathy Bates, Meryl Streep and Warren Beatty and producer George Stevens, Jr. standing and applauding. Stevens speculates on why he, Beatty, and many others in the audience chose to stand and applaud:
"Los Angeles Times" film critic Kenneth Turan, agreed, writing "The only criterion for an award like this is the work". Kazan was already "denied accolades" from the American Film Institute, and other film critics associations. According to Mills, "It’s time for the Academy to recognize this genius," adding that "We applauded when the great Chaplin finally had his hour."
In later interviews, Kazan explained some of the early events that made him decide to become a friendly witness, most notably in relation to the Group Theater, which he called his first "family," and the "best thing professionally" that ever happened to him:
Mills notes that prior to becoming a "friendly witness," Kazan discussed the issues with Miller:
Miller put his arm around Kazan and retorted, "don’t worry about what I’ll think. Whatever you do is okay with me, because I know that your heart is in the right place."
In his memoirs, Kazan writes that his testimony meant that "the big shot had become the outsider." He also notes that it strengthened his friendship with another outsider, Tennessee Williams, with whom he collaborated on numerous plays and films. He called Williams "the most loyal and understanding friend I had through those black months.":495
Personal life.
Elia Kazan was married three times. His first wife was playwright Molly Day Thacher. They were married from 1932 until her death in 1963; this marriage produced two daughters and two sons, including screenwriter Nicholas Kazan. His second marriage, to the actress Barbara Loden, lasted from 1969 until her death in 1980, and produced one son. His marriage, in 1982, to Frances Rudge continued until his death, in 2003, aged 94.
In 1978, the U.S. government paid for Kazan and his family to travel to Kazan's birthplace where many of his films were to be shown. During a speech in Athens, he discussed his films and his personal and business life in the U.S., along with the messages he tried to convey:
In my own view, the solution is to talk about human beings and not about abstracts, to reveal the culture and the social moment as it is reflected in the behavior and the lives of individual people. Not to be "correct." To be total. So I do not believe in any ideology that does not permit—no encourage—the freedom of the individual.
He also offered his opinions about the role of the U.S. as a world model for democracy:
I think you and I, all of us, have some sort of stake in the United States. If it fails, the failure will be that of us all. Of mankind itself. It will cost us all. . . . I think of the United States as a country which is an arena and in that arena there is a drama being played out. . . . . I have seen that the struggle is the struggle of free men.
Elia Kazan died from natural causes in his Manhattan apartment, 28 September 2003 aged 94.
Legacy.
Kazan became known as an "actor's director" because he was able to elicit some of the best performances in the careers of many of his stars, such as Marlon Brando, Rod Steiger, Karl Malden, James Dean, Julie Harris, Carroll Baker, Eli Wallach and Natalie Wood. Under his direction, his actors received 21 Academy Award nominations and won nine Oscars.
He won as Best Director for "Gentleman's Agreement" (1947) and for "On the Waterfront" (1954), which is considered "one of the greatest films in the history of international cinema." Both "A Streetcar Named Desire" (1951) and "On the Waterfront" were nominated for twelve Academy Awards, respectively winning four and eight.
Kazan never lost his identification with the oppressed people he remembered from the depths of the Great Depression. With his many years with the Group Theater and Actors Studio in New York City and later triumphs on Broadway, he became famous "for the power and intensity of his actors' performances." He was the pivotal figure in launching the film careers of Marlon Brando, James Dean, Julie Harris, Eva Marie Saint, Warren Beatty, Lee Remick, Karl Malden, and many others. Seven of Kazan's films won a total of 20 Academy Awards. Dustin Hoffman commented that he "doubted whether he, Robert De Niro, or Al Pacino, would have become actors without Mr. Kazan's influence."
Upon his death, at the age of 94, the "New York Times" described him as "one of the most honored and influential directors in Broadway and
Hollywood history." His stage direction of "Death of a Salesman" and "A Streetcar Named Desire" is considered a "high point of world theater" in the 20th century. Although he became a "legendary director on Broadway", he made an equally impressive transition into one of the major filmmakers of his time. Critic William Baer notes that throughout his career "he constantly rose to the challenge of his own aspirations", adding that "he was a pioneer and visionary who greatly affected the history of both stage and cinema". Certain of his film-related material and personal papers are contained in the Wesleyan University Cinema Archives to which scholars and media experts from around the world may have full access.
His controversial stand during his testimony in front of the House Committee on Un-American Activities (HUAC) in 1952, became the low point in his career, although he remained convinced that he made the right decision to give the names of Communist Party members. He stated in an interview in 1976:
During his career, Kazan won both Tony and Oscar Awards for excellence on stage and screen. In 1982, President Ronald Reagan presented him with the Kennedy Center honors award, a national tribute for life achievement in the arts. At the ceremony, screenwriter Budd Schulberg, who wrote "On the Waterfront", thanks his lifelong friend saying, “Elia Kazan has touched us all with his capacity to honor not only the heroic man, but the hero in every man.” In an interview with the American Film Institute in 1976, Kazan spoke of his love of the cinema: "I think it's the most wonderful art in the world."
In 1999, when he was 90 years old, Kazan received an honorary Oscar for lifetime achievement. During the ceremony, he was accompanied by Martin Scorsese and Robert De Niro. The propriety of such an honor for Kazan who "named names" at the HUAC hearings remains a "contentious subject" according to the "New York Times". Many in Hollywood felt that enough time had passed that it was appropriate to finally recognize Kazan's great artistic accomplishments, although others did not and would not applaud. Kazan appreciated the award:
In his autobiography, "A Life", he sums up the influence of filmmaking on his life:
Martin Scorsese has directed a film documentary, "A Letter to Elia" (2010), considered to be an "intensely personal and deeply moving tribute" to Kazan. Scorsese was "captivated" by Kazan's films as a young man, and the documentary mirrors his own life story while he also credits Kazan as the inspiration for his becoming a filmmaker. It won a Peabody Award in 2010.
Awards and honors.
In addition to these awards, Kazan has a star on the Hollywood Walk of Fame, which is located on 6800 Hollywood Boulevard. He is also a member of the American Theater Hall of Fame.

</doc>
<doc id="42877" url="http://en.wikipedia.org/wiki?curid=42877" title="Ninety-ninety rule">
Ninety-ninety rule

In computer programming and software engineering, the ninety-ninety rule is a humorous aphorism that states:
The first 90 percent of the code accounts for the first 90 percent of the development time. The remaining 10 percent of the code accounts for the other 90 percent of the development time.—Tom Cargill, Bell Labs
That the total development time sums to 180% is a wry allusion to the notorious tendency of software development projects to significantly overrun their original schedules. It expresses both the rough allocation of time to easy and hard portions of a programming project and the cause of the lateness of many projects (that is, failure to anticipate the hard parts). In other words, it takes both more time and more coding than expected to make a project work.
The rule is attributed to Tom Cargill of Bell Labs and was made popular by Jon Bentley's September 1985 "Programming Pearls" column in "Communications of the ACM", in which it was titled the "Rule of Credibility".

</doc>
<doc id="42878" url="http://en.wikipedia.org/wiki?curid=42878" title="Nevil Maskelyne">
Nevil Maskelyne

The Reverend Dr Nevil Maskelyne FRS (6 October 1732 – 9 February 1811) was the fifth British Astronomer Royal. He held the office from 1765 to 1811.
Biography.
Maskelyne was born in London, the third son of Edmund Maskelyne of Purton, Wiltshire. Maskelyne's father died when he was 12, leaving the family in reduced circumstances. Maskelyne attended Westminster School and was still a pupil there when his mother died in 1748. His interest in astronomy had begun while at Westminster School, shortly after the eclipse of 25 July 1748.
Maskelyne entered St Catharine's College, Cambridge in 1749, graduating as seventh wrangler in 1754. Ordained as a minister in 1755, he became a fellow of Trinity College, Cambridge in 1756 and a Fellow of the Royal Society in 1758.
About 1785 Maskelyne married Sophia Rose of Cotterstock, Northamptonshire. Their only child, Margaret (1786–1858), was the mother of Mervyn Herbert Nevil Story-Maskelyne (1823–1911) professor of mineralogy at Oxford (1856–95). Maskelyne's sister, Margaret (1735-1817), married Robert Clive.
Nevil Maskelyne is buried in the churchyard of St Mary the Virgin, the parish church of the village of Purton, Wiltshire, England.
Career.
Measurement of longitude.
In 1760 the Royal Society appointed Maskelyne as an astronomer on one of their expeditions to observe the 1761 transit of Venus. He and Robert Waddington were sent to the island of St. Helena. This was an important observation since accurate measurements would allow the accurate calculation of Earth's distance from the Sun, which would in turn allow the actual rather than the relative scale of the solar system to be calculated. This would allow, it was argued, the production of more accurate astronomical tables, in particular those predicting the motion of the Moon.
Bad weather prevented observation of the transit; however, Maskelyne used his journey to trial a method of determining longitude using the position of the moon, which became known as the lunar distance method. He returned to England, resuming his position as curate at Chipping Barnet in 1761, and began work on a book, publishing the lunar-distance method of longitude calculation and providing tables to facilitate its use in 1763 in "The British Mariner's Guide", which included the suggestion that to facilitate the finding of longitude at sea, lunar distances should be calculated beforehand for each year and published in a form accessible to navigators.
In 1763 the Board of Longitude sent Maskelyne to Barbados in order to carry out an official trial of three contenders for a Longitude reward. He was to carry out observations on board ship and to calculate the longitude of the capital, Bridgetown by observation of Jupiter's satellites. The three methods on trial were John Harrison's sea watch (now known as H4), Tobias Mayer's lunar tables and a marine chair made by Christopher Irwin, intended to help observations of Jupiter's satellites on board ship. Both Harrison's watch and lunar-distance observations based on Mayer's lunar tables produced results within the terms of the Longitude Act, although the former appeared to be more accurate. Harrison's watch had produced Bridgetown's longitude with an error of less than ten miles, while the lunar-distance observations were accurate to within 30 nautical miles.
Maskelyne reported the results of the trial to the Board of Longitude on 9 February 1765, by which time he had been appointed Astronomer Royal and was, ex officio, a Commissioner of Longitude. The Commissioners understood that the timekeeping and astronomical methods of finding longitude were complementary. The lunar-distance method could more quickly be rolled out, with Maskelyne's proposal that tables like those in his "The British Mariner's Guide" be published for each year. This proposal led to the establishment of The Nautical Almanac, the production of which, as Astronomer Royal, Maskelyne oversaw. Taking even occasional astronomical observations was also the only way to check that a timekeeper was keeping good time over the course of a long voyage. The Commissioners also needed to know that more than one sea watch could be made, and that Harrison's methods could be communicated to other watchmakers.
The Board of Longitude therefore decided that rewards should be given Harrison (£10,000), Mayer (£3000, posthumously) and others involved in helping to develop the lunar-distance method. Harrison was told that a further reward of £10,000 would be forthcoming if he could demonstrate the replicability of his watch. It is worth noting that although Harrison and his son later accused Maskelyne of bias against the timekeeping method, charges repeated by authors such as Dava Sobel and Rupert Gould, Maskelyne never submitted a method or an idea of his own for consideration by the Board of Longitude. He was to play a significant role in having marine timekeepers, as well as the lunar-distance method, developed, tested and used on board voyages of exploration.
Since the observations that fed into the Nautical Almanac were made at the Royal Observatory, Greenwich, the Greenwich meridian became the reference for measurements of longitude in the Royal Navy, and on British Admiralty Charts. It was recommended for adoption as the international Prime Meridian in 1884.
Measurement of latitude.
Maskelyne took a great interest in various geodetical operations, notably the measurement of the length of a degree of latitude in Maryland and Pennsylvania, executed by Mason and Dixon in 1766 – 1768, and later the determination of the relative longitude of Greenwich and Paris. On the French side the work was conducted by Count Cassini, Legendre, and Méchain; on the English side by General Roy. This triangulation was the beginning of the great trigonometrical survey which was subsequently extended all over Britain. His observations appeared in four large folio volumes from 1776–1811, some of them being reprinted in Samuel Vince's "Elements of Astronomy".
Schiehallion experiment.
In 1772 Maskelyne proposed to the Royal Society what was to become known as the "Schiehallion experiment" (named after the mountain on which it was performed), for the determination of the Earth’s density using a plumb line. He was not the first to suggest this, Pierre Bouguer and Charles-Marie de la Condamine having attempted the same experiment in 1738.
Maskelyne performed his experiment in 1774 on Schiehallion in Perthshire, Scotland, the mountain being chosen due to its regular conical shape which permitted a reasonably accurate determination of its volume. The apparent difference of latitude between two stations on opposite sides of the mountain were compared with the real difference of latitude obtained by triangulation.
From Maskelyne’s observations Charles Hutton deduced a density for the earth 4.5 times that of water (the modern value is 5.515).
Other work.
Maskelyne’s first contribution to astronomical literature was "A Proposal for Discovering the Annual Parallax of Sirius", published in 1760. Subsequent contributions to the "Transactions" contained his observations of the transits of Venus (1761 and 1769), on the tides at Saint Helena (1762), and on various astronomical phenomena at Saint Helena (1764) and at Barbados (1764).
Maskelyne also introduced several practical improvements, such as the measurement of time to tenths of a second and prevailed upon the government to replace Bird’s mural quadrant by a repeating circle 6 feet (1.8 m) in diameter. The new instrument was constructed by Edward Troughton but Maskelyne did not live to see it completed.

</doc>
<doc id="42880" url="http://en.wikipedia.org/wiki?curid=42880" title="John Flamsteed">
John Flamsteed

John Flamsteed FRS (19 August 1646 – 31 December 1719) was an English astronomer and the first Astronomer Royal. He catalogued over 3000 stars.
Life.
Flamsteed was born in Denby, Derbyshire, England, the only son of Stephen Flamsteed and his first wife, Mary Spadman. He was educated at the free school of Derby, and was educated at Derby School, in St Peter's Churchyard, Derby, near where his father carried on a malting business. At that time, most masters of the school were Puritans. Flamsteed had a solid knowledge of Latin, essential for reading the literature of the day, and a love of history, leaving the school in May, 1662.:3–4
His progress to Jesus College, Cambridge, recommended by the Master of Derby School, was delayed by some years of chronic ill health. During those years, Flamsteed gave his father some help in his business, and from his father learnt arithmetic and the use of fractions, developing a keen interest in mathematics and astronomy. In July 1662, he was fascinated by the thirteenth century work of Johannes de Sacrobosco, "De sphaera mundi", and on 12 September 1662 observed his first partial solar eclipse. Early in 1663, he read Thomas Fale's "The Art of Dialling", which set off an interest in sundials. In the summer of 1663, he read Wingate's "Canon", William Oughtred's "Canon", and Thomas Stirrup's "Art of Dialling". At about the same time, he acquired Thomas Street's "Astronomia Carolina, or A New Theory of the Celestial Motions" ("Caroline Tables"). He associated himself with local gentlemen interested in astronomy, including William Litchford, whose library included the work of the astrologer John Gadbury which included astronomical tables by Jeremiah Horrocks, who had died in 1641 at the age of twenty-two. Flamsteed was greatly impressed (as Isaac Newton had been) by the work of Horrocks.:8–11
In August 1665, at the age of nineteen and as a gift for his friend Litchford, Flamsteed wrote his first paper on astronomy, entitled "Mathematical Essays", concerning the design, use and construction of an astronomer's quadrant, including tables for the latitude of Derby.:11
In September 1670, Flamsteed visited Cambridge and entered his name as an undergraduate at Jesus College. While it seems he never took up full residence, he was there for two months in 1674, and had the opportunity to hear Isaac Newton's "Lucasian Lectures".:26
Ordained a deacon, he was preparing to take up a living in Derbyshire when he was invited to London by his patron Jonas Moore, Surveyor-General of the Ordnance. Moore had recently made an offer to the Royal Society to pay for the establishment of an observatory. These plans were, however, preempted when Charles II was persuaded by his mistress, Louise de Kérouaille, Duchess of Portsmouth, to hear about a proposal to find longitude by the position of the Moon from an individual known as Le Sieur de St. Pierre. Charles appointed a Royal Commission to examine the proposal in December 1674, consisting of Lord Brouncker, Seth Ward, Samuel Moreland, Christopher Wren, Silius Titus, John Pell and Robert Hooke.
Having arrived in London on 2 February 1675, and staying with Jonas Moore at the Tower of London, Flamsteed had the opportunity to be taken by Titus to meet the King. He was subsequently admitted as an official Assistant to the Royal Commission and supplied observations in order to test St Pierre's proposal and to offer his own comments. The Commission's conclusions were that, although St Pierre's proposal was not worth further consideration, the King should consider establishing an observatory and appointing an observer in order to better map the stars and the motions of the Moon in order to underpin the successful development of the lunar-distance method of finding longitude.
On 4 March 1675 Flamsteed was appointed by royal warrant "The King's Astronomical Observator" — the first English Astronomer Royal, with an allowance of £100 a year. The warrant stated his task as "rectifieing the Tables of the motions of the Heavens, and the places of the fixed stars, so as to find out the so much desired Longitude of places for Perfecteing the Art of Navigation". In June 1675, another royal warrant provided for the founding of the Royal Greenwich Observatory, and Flamsteed laid the foundation stone on 10 August. 
In February 1676, he was admitted a Fellow of the Royal Society, and in July, he moved into the Observatory where he lived until 1684, when he was finally appointed priest to the parish of Burstow, Surrey. He held that office, as well as that of Astronomer Royal, until his death. He is buried at Burstow, and the east window in the church was dedicated to him as a memorial. 
 The will of Flamsteed’s widow, Margaret, left instructions for her own remains to be deposited “in the same Grave in which Mr John Flamsteed is buryed in the Chancell of Burstow Church.” Intriguingly, she also left instructions, and twenty five pounds, for the executor of her will to place “in the aforesaid Chancell of Burstow… A Marble stone or Monument, with an inscription in Latin, in memory of the late Reverend Mr. John Flamsteed.” It seems no such monument was created, and almost 200 years later, a plaque was placed to mark his burial in the Chancel.
After his death, his papers and scientific instruments were taken by his widow. The papers were returned many years later, but the instruments disappeared.
Scientific work.
Flamsteed accurately calculated the solar eclipses of 1666 and 1668. He was responsible for several of the earliest recorded sightings of the planet Uranus, which he mistook for a star and catalogued as '34 Tauri'. The first of these was in December 1690, which remains the earliest known sighting of Uranus by an astronomer.
On 16 August 1680 Flamsteed catalogued a star, 3 Cassiopeiae, that later astronomers were unable to corroborate. Three hundred years later, the American astronomical historian William Ashworth suggested that what Flamsteed may have seen was the most recent supernova in the galaxy's history, an event which would leave as its remnant the strongest radio source outside of the solar system, known in the third Cambridge (3C) catalogue as 3C 461 and commonly called Cassiopeia A by astronomers. Because the position of "3 Cassiopeiae" does not precisely match that of Cassiopeia A, and because the expansion wave associated with the explosion has been worked backward to the year 1667 and not 1680, some historians feel that all Flamsteed may have done was incorrectly note the position of a star already known.
In 1681 Flamsteed proposed that the two great comets observed in November and December 1680 were not separate bodies, but rather a single comet travelling first towards the Sun and then away from it. Although Isaac Newton first disagreed with Flamsteed, he later came to agree with him and theorized that comets, like planets, moved around the sun in large, closed elliptical orbits. An angry Flamsteed later learned that Newton had gained access to Flamsteed's observations and data, with the aid of Edmund Halley.
As Astronomer Royal, Flamsteed spent some forty years observing and making meticulous records for his star catalogue, which would eventually triple the number of entries in Tycho Brahe's sky atlas. Unwilling to risk his reputation by releasing unverified data, he kept the incomplete records under seal at Greenwich. In 1712, Isaac Newton, then President of the Royal Society, and Edmund Halley again obtained Flamsteed's data and published a pirated star catalogue. Flamsteed managed to gather three hundred of the four hundred printings and burned them. "If Sir I.N. would be sensible of it, I have done both him and Dr. Halley a great kindness," he wrote to his assistant Abraham Sharp.
In 1725 Flamsteed's own version of "Historia Coelestis Britannica" was published posthumously, edited by his wife, Margaret. This contained Flamsteed's observations, and included a catalogue of 2,935 stars to much greater accuracy than any prior work. It was considered the first significant contribution of the Greenwich Observatory, and the numerical Flamsteed designations for stars that were added subsequently to a French edition are still in use. In 1729 his wife published his "Atlas Coelestis", assisted by Joseph Crosthwait and Abraham Sharp, who were responsible for the technical side.

</doc>
<doc id="42882" url="http://en.wikipedia.org/wiki?curid=42882" title="Cosmogony">
Cosmogony

Cosmogony (or cosmogeny) is any model concerning the coming-into-existence (i.e. origin) of either the cosmos (i.e. universe), or the so-called reality of sentient beings. Developing a complete theoretical model has implications in both the philosophy of science and epistemology.
Etymology.
The word comes from the Koine Greek κοσμογονία (from κόσμος "cosmos, the world") and the root of γί(γ)νομαι / γέγονα ("come into a new state of being"). In astronomy, cosmogony refers to the study of the origin of particular astrophysical objects or systems, and is most commonly used in reference to the origin of the universe, the solar system, or the earth-moon system.
Overview.
The Big Bang theory is the prevailing cosmological model of the early development of the universe. The most commonly held view is that the universe was once a gravitational singularity, which expanded extremely rapidly from its hot and dense state. However, while this expansion is well-modeled by the Big Bang theory, the origins of the singularity remain as one of the unsolved problems in physics.
Cosmologist and science communicator Sean M. Carroll explains two competing types of explanations for the origins of the singularity which is the main disagreement between the scientists who study cosmogony and centers on the question of whether time existed "before" the emergence of our universe or not. One cosmogonical view sees time as fundamental and even eternal: The universe could have contained the singularity because the universe evolved or changed from a prior state (the prior state was "empty space", or maybe a state that could not be called "space" at all). The other view, held by proponents like Stephen Hawking, says that there was no change through time because "time" itself emerged along with this universe (in other words, there can be no "prior" to the universe). Thus, it remains unclear what combination of "stuff", space, or time emerged with the singularity and this universe.
One problem in cosmogony is that there is currently no theoretical model that explains the earliest moments of the universe's existence (during the Planck time) because of a lack of a testable theory of quantum gravity. Researchers in string theory and its extensions (for example, M theory), and of loop quantum cosmology, have nevertheless proposed solutions of the type just discussed.
Another issue facing the field of particle physics is a need for more expensive and technologically advanced particle accelerators to test proposed theories (for example, that the universe was caused by colliding membranes).
Developing a complete theoretical model has implications in both the philosophy of science and epistemology.
Compared with cosmology.
Cosmology is the study of the structure and changes in the present universe, while the scientific field of cosmogony is concerned with the origin of the universe. Observations about our present universe may not only allow predictions to be made about the future, but they also provide clues to events that happened long ago when ... the cosmos began. So the work of cosmologists and cosmogonists overlaps. 
”
 National Aeronautics and Space Administration (NASA)
Cosmogony can be distinguished from cosmology, which studies the universe at large and throughout its existence, and which technically does not inquire directly into the source of its origins. There is some ambiguity between the two terms. For example, the cosmological argument from theology regarding the existence of God is technically an appeal to cosmogonical rather than cosmological ideas. In practice, there is a scientific distinction between cosmological and cosmogonical ideas. Physical cosmology is the science that attempts to explain all observations relevant to the development and characteristics of the universe as a whole. Questions regarding why the universe behaves in such a way have been described by physicists and cosmologists as being extra-scientific (i.e., metaphysical), though speculations are made from a variety of perspectives that include extrapolation of scientific theories to untested regimes (i.e., at Planck scales), and philosophical or religious ideas.
Theoretical scenarios.
Cosmogonists have only tentative theories for the early stages of the universe and its beginning. s of 2011[ [update]], no accelerator experiments probe energies of sufficient magnitude to provide any experimental insight into the behavior of matter at the energy levels that prevailed shortly after the Big Bang. Furthermore, since astronomical observations imply a singularity at the origin of the universe, experiments at any given high energy level will always be dwarfed by the infinite energy level predicted by Big Bang Theory. Therefore, significant technological and conceptual advances would be needed to propose a scientific test for cosmogonical theories.
Proposed theoretical scenarios differ radically, and include string theory and M-theory, the Hartle–Hawking initial state, string landscape, brane inflation, the Big Bang, and the ekpyrotic universe. Some of these models are mutually compatible, whereas others are not.

</doc>
<doc id="42886" url="http://en.wikipedia.org/wiki?curid=42886" title="Gattaca">
Gattaca

Gattaca is a 1997 American science fiction film written and directed by Andrew Niccol. It stars Ethan Hawke and Uma Thurman, with Jude Law, Loren Dean, Ernest Borgnine, Gore Vidal, and Alan Arkin appearing in supporting roles. The film presents a biopunk vision of a future society driven by eugenics where potential children are conceived through genetic manipulation to ensure they possess the best hereditary traits of their parents. The film centers on Vincent Freeman, played by Hawke, who was conceived outside the eugenics program and struggles to overcome genetic discrimination to realize his dream of traveling into space.
The movie draws on concerns over reproductive technologies which facilitate eugenics, and the possible consequences of such technological developments for society. It also explores the idea of destiny and the ways in which it can and does govern lives. Characters in "Gattaca" continually battle both with society and with themselves to find their place in the world and who they are destined to be according to their genes.
The film's title is based on the first letters of guanine, adenine, thymine, and cytosine, the four nucleobases of DNA. It was a 1997 nominee for the Academy Award for Best Art Direction and the Golden Globe Award for Best Original Score.
The film flopped at the box office, but it received generally positive reviews and has since gained a cult following.
Plot.
In "the not-too-distant future", eugenics (in the form of conceiving "improved" children by genetic manipulation) is common, and DNA plays the primary role in determining social class. A genetic registry database uses biometrics to instantly identify and classify those so created as "valids" while those conceived by traditional means and more susceptible to genetic disorders are derisively known as "in-valids". Genetic discrimination is forbidden by law, but in practice genotype profiling is used to identify valids to qualify for professional employment while in-valids are relegated to menial jobs.
Vincent Freeman is conceived naturally without the aid of genetic selection, and immediately after birth, his genetics indicate a high probability of several disorders and an estimated life span of 30.2 years. His parents, regretting their decision, use genetic selection to give birth to their next child Anton who is genetically superior to Vincent. Growing up, the two brothers often play a game of "chicken" by swimming out to sea with the first one giving up and returning to shore determined the loser, which Vincent always loses. Vincent dreams of a career in space travel but is reminded of his genetic inferiority. One day, Vincent challenges Anton to a game of chicken and bests him before Anton starts to drown. Vincent saves Anton and then leaves home on his own.
Vincent works as an in-valid, cleaning office spaces including that of Gattaca Aerospace Corporation, a space-flight conglomerate. He gets a chance to become a "borrowed ladder", posing as a valid by using genetic hair, skin, blood and urine samples from a donor, Jerome Eugene Morrow, who is a former swimming star paralyzed by a car accident. With Jerome's "second to none" genetic makeup, Vincent gains employment at Gattaca, and is assigned to be navigator for an upcoming trip to Saturn's moon Titan. To keep his identity hidden, Vincent must meticulously groom and scrub down daily to remove his own genetic material, and pass daily DNA scanning and frequent urine tests using Jerome's samples.
Gattaca becomes embroiled in controversy when one of its administrators is murdered a week before the planned flight. The police find a fallen eyelash of Vincent's at the scene. An investigation is launched to find the murderer, Vincent being the top suspect. Through this, Vincent becomes close to a co-worker, Irene Cassini, and falls in love with her. Though a valid, Irene is aware that her higher risk of heart failure will prevent her from joining any deep space Gattaca mission. Vincent also learns more about Jerome, and discovers that his paralysis is by his own hand; after coming in second place in a swim meet, Jerome became depressed and threw himself in front of a car to attempt suicide.
Vincent is able to repeatedly evade scrutiny from the investigation, and soon it is revealed that Gattaca's mission director was the killer, as the administrator was threatening to pull the mission. Vincent learns the identity of the detective that closed the case, his brother Anton, who has become aware of Vincent's presence. The brothers meet, and Anton warns Vincent what he is doing is illegal, but Vincent asserts he has gotten to this position on his own, and did not need help as Anton did the last time they played chicken. Anton challenges Vincent to one more game of chicken. As the two swim out in the dead of night, Anton is surprised at Vincent's stamina, and Vincent reveals that his trick to winning was never saving any energy for the swim back. Anton turns back and starts to drown, but Vincent rescues him and swims them both back to shore safely using celestial navigation.
The day of the launch arrives. Jerome reveals that he has stored enough DNA samples for Vincent to last several lifetimes upon his return and gives him an envelope to open once in flight. After saying his goodbyes to Irene, Vincent prepares to board but discovers there is one final genetic test before he can board, and he currently lacks any of Jerome's samples. He is surprised when Dr. Lamar, the person in charge of background checks, reveals that he has been aware that Vincent has been posing as a valid. Lamar admits that his son looks up to Vincent and wonders if his son, genetically selected but "not all that they promised", could break the limits just as Vincent has. He then wipes the information from the test, passing Vincent as a valid. As the rocket launches, Jerome dons his swimming medal and burns himself in his home's incinerator; Vincent opens the note from Jerome to find only a lock of Jerome's hair attached to it. Vincent muses on this, stating "For someone not meant for this world, I must confess, I’m suddenly having a hard time leaving it. Of course, they say every atom in our bodies was once a part of a star. Maybe I'm not leaving; maybe I'm going home."
Production.
The exteriors (including the roof scene), and some of the interior shots, of the Gattaca complex were filmed at Frank Lloyd Wright's 1960 Marin County Civic Center in San Rafael, California. The parking lot scenes were shot at the Otis College of Art and Design, distinguished by its punch card-like windows, located near LAX in Los Angeles. The exterior of Vincent Freeman's house was shot at the CLA Building on the campus of California State Polytechnic University, Pomona (Cal Poly Pomona). Other exterior shots were filmed at the bottom of the spillway of the Sepulveda Dam and outside the The Forum in Inglewood. The solar power plant mirrors sequence was filmed at the Kramer Junction Solar Electric Generating Station.
Design.
The movie uses a swimming treadmill in the opening minutes to punctuate the swimming and futuristic themes. The futuristic turbine cars are based on 1960s car models like Rover P6, Citroën DS19 and Studebaker Avanti, and futuristic buildings represent modern architecture of the 1950s.
Release.
"Gattaca" was released in theaters on October 24, 1997, and opened at number 5 at the box office; trailing "I Know What You Did Last Summer", "The Devil's Advocate", "Kiss the Girls", and "Seven Years in Tibet". Over the first weekend the film brought in $4.3 million. It ended its theatrical run with a domestic total of $12.5 million against a reported production budget of $36 million.
Home media.
"Gattaca" was released on DVD on July 1, 1998, and was also released on Superbit DVD. Special Edition DVD and Blu-ray versions were released on March 11, 2008. Both editions contain a deleted scene featuring historical figures like Einstein, Lincoln, etc., who according to the texts are supposedly genetically deficient.
Critical reception.
"Gattaca" received positive reviews from critics; the film received an 82% "fresh" rating from Rotten Tomatoes, based on 55 reviews, with a rating average of 7.1/10. The critical consensus states that "Intelligent and scientifically provocative, Gattaca is an absorbing sci fi drama that poses important interesting ethical questions about the nature of science." On Metacritic, the film received "generally favorable reviews" with a score of 64 out of 100. Roger Ebert stated, "This is one of the smartest and most provocative of science fiction films, a thriller with ideas." James Berardinelli praised it for "energy and tautness" and its "thought-provoking script and thematic richness."
Despite critical acclaim, "Gattaca" was not a box office success but it is said to have crystallized the debate over the controversial topic of human genetic engineering. The film's dystopian depiction of "genoism" has been cited by many bioethicists and laymen in support of their hesitancy about, or opposition to, eugenics and the societal acceptance of the genetic-determinist ideology that may frame it. In a 1997 review of the film for the journal "Nature Genetics", molecular biologist Lee M. Silver stated that ""Gattaca" is a film that all geneticists should see if for no other reason than to understand the perception of our trade held by so many of the public-at-large".
In 2004, bioethicist James Hughes criticized the premise and influence of the film "Gattaca", arguing that:
Soundtrack.
The score for "Gattaca" was composed by Michael Nyman, and the original soundtrack was released on October 21, 1997.
Legacy.
Television series.
On October 30, 2009, "Variety" reported that Sony Pictures was developing a television adaptation of the feature film as a one-hour police procedural set in the future. The show was to be written by Gil Grant, who has written for "24" and "NCIS".
Political references.
U.S. Senator Rand Paul (R-KY) used near-verbatim portions of the plot summary from the Wikipedia entry on "Gattaca" in a speech at Liberty University on October 28, 2013 supporting Virginia Attorney General Ken Cuccinelli's campaign for Governor of Virginia. Paul accused pro-choice politicians of advocating eugenics in a manner similar to the events in "Gattaca".
Further reading.
Jon Frauley. 2010. "Biopolitics and the Governance of Genetic Capital in "Gattaca"." "Criminology, Deviance and the Silver Screen: The Fictional Reality and the Criminological Imagination". New York: Palgrave Macmillan.

</doc>
<doc id="42888" url="http://en.wikipedia.org/wiki?curid=42888" title="Human genome">
Human genome

The human genome is the complete set of genetic information for humans ("Homo sapiens"). This information is encoded as DNA sequences within the 23 chromosome pairs in cell nuclei and in a small DNA molecule found within individual mitochondria. Human genomes include both protein-coding DNA genes and noncoding DNA. Haploid human genomes (contained in egg and sperm cells) consist of three billion DNA base pairs, while diploid genomes (found in somatic cells) have twice the DNA content. While there are significant differences among the genomes of human individuals (on the order of 0.1%), these are considerably smaller than the differences between humans and their closest living relatives, the chimpanzees (approximately 4%) and bonobos.
The Human Genome Project produced the first complete sequences of individual human genomes. As of 2012, thousands of human genomes have been completely sequenced, and many more have been mapped at lower levels of resolution. The resulting data are used worldwide in biomedical science, anthropology, forensics and other branches of science. There is a widely held expectation that genomic studies will lead to advances in the diagnosis and treatment of diseases, and to new insights in many fields of biology, including human evolution.
Although the sequence of the human genome has been (almost) completely determined by DNA sequencing, it is not yet fully understood. Most (though probably not all) genes have been identified by a combination of high throughput experimental and bioinformatics approaches, yet much work still needs to be done to further elucidate the biological functions of their protein and RNA products. Recent results suggest that most of the vast quantities of noncoding DNA within the genome have associated biochemical activities, including regulation of gene expression, organization of chromosome architecture, and signals controlling epigenetic inheritance.
There are an estimated 20,000-25,000 human protein-coding genes. The estimate of the number of human genes has been repeatedly revised down from initial predictions of 100,000 or more as genome sequence quality and gene finding methods have improved, and could continue to drop further. Protein-coding sequences account for only a very small fraction of the genome (approximately 1.5%), and the rest is associated with non-coding RNA molecules, regulatory DNA sequences, LINEs, SINEs, introns, and sequences for which as yet no function has been elucidated.
Molecular organization and gene content.
The total length of the human genome is over 3 billion base pairs. The genome is organized into 22 paired chromosomes, the X chromosome (one in males, two in females) and, in males only, one Y chromosome, all being large linear DNA molecules contained within the cell nucleus. It also includes the mitochondrial DNA, a comparatively small circular molecule present in each mitochondrion. Basic information about these molecules and their gene content, based on a reference genome that does not represent the sequence of any specific individual, are provided in the following table. (Data source: , July 2012)
Table 1 (above) summarizes the physical organization and gene content of the human reference genome, with links to the original analysis, as published in the Ensembl database at the European Bioinformatics Institute (EBI) and Wellcome Trust Sanger Institute. Chromosome lengths were estimated by multiplying the number of base pairs by 0.34 nanometers, the distance between base pairs in the DNA double helix. The number of proteins is based on the number of initial precursor mRNA transcripts, and does not include products of alternative pre-mRNA splicing, or modifications to protein structure that occur after translation.
The number of variations is a summary of unique DNA sequence changes that have been identified within the sequences analyzed by Ensembl as of July, 2012; that number is expected to increase as further personal genomes are sequenced and examined. In addition to the gene content shown in this table, a large number of non-expressed functional sequences have been identified throughout the human genome (see below). Links open windows to the reference chromosome sequence in the EBI genome browser. The table also describes prevalence of genes encoding structural RNAs in the genome.
MiRNA, or MicroRNA, functions as a post-transcriptional regulator of gene expression. Ribosomal RNA, or rRNA, makes up the RNA portion of the ribosome and is critical in the synthesis of proteins. Small nuclear RNA, or snRNA, is found in the nucleus of the cell. Its primary function is in the processing of pre-mRNA molecules and also in the regulation of transcription factors. SnoRNA, or Small nucleolar RNA, primarily functions in guiding chemical modifications to other RNA molecules.
Completeness of the human genome sequence.
Although the human genome has been completely sequenced for all practical purposes, there are still hundreds of gaps in the sequence. A recent study noted more than 160 euchromatic gaps of which 50 gaps were closed. However, there are still numerous gaps in the heterochromatic parts of the genome which is much harder to sequence due to numerous repeats and other intractable sequence features.
Coding vs. noncoding DNA.
The content of the human genome is commonly divided into coding and noncoding DNA sequences. Coding DNA is defined as those sequences that can be transcribed into mRNA and translated into proteins during the human life cycle; these sequences occupy only a small fraction of the genome (<2%). Noncoding DNA is made up of all of those sequences (ca. 98% of the genome) that are not used to encode proteins.
Some noncoding DNA contains genes for RNA molecules with important biological functions (noncoding RNA, for example ribosomal RNA and transfer RNA). The exploration of the function and evolutionary origin of noncoding DNA is an important goal of contemporary genome research, including the ENCODE (Encyclopedia of DNA Elements) project, which aims to survey the entire human genome, using a variety of experimental tools whose results are indicative of molecular activity.
Because non-coding DNA greatly outnumbers coding DNA, the concept of the sequenced genome has become a more focused analytical concept than the classical concept of the DNA-coding gene.
Mutation Rate of Human Genome.
Mutation rate of human genome is a very important factor in calculating evolutionary time points. Researchers calculated the number of genetic variations between human and apes. Dividing that number by age of fossil of most recent common ancestor of humans and ape, researchers calculated the mutation rate. Recent studies using next generation sequencing technologies concluded a slow mutation rate which doesn't add up with human migration pattern time points and suggesting a new evolutionary time scale. 100,000 year old human fossil found in Israel threw more questions on human migration time points.
Coding sequences (protein-coding genes).
Protein-coding sequences represent the most widely studied and best understood component of the human genome. These sequences ultimately lead to the production of all human proteins, although several biological processes (e.g. DNA rearrangements and alternative pre-mRNA splicing) can lead to the production of many more unique proteins than the number of protein-coding genes.
The complete modular protein-coding capacity of the genome is contained within the exome, and consists of DNA sequences encoded by exons that can be translated into proteins. Because of its biological importance, and the fact that it constitutes less than 2% of the genome, sequencing of the exome was the first major milepost of the Human Genome Project.
Number of protein-coding genes. About 20,000 human proteins have been annotated in databases such as Uniprot. Historically, estimates for the number of protein genes have varied widely, ranging up to 2,000,000 in the late 1960s, but several researchers pointed out in the early 1970s that the estimated mutational load from deleterious mutations placed an upper limit of approximately 40,000 for the total number of functional loci (this includes protein-coding and functional non-coding genes).
The number of human protein-coding genes is not significantly larger than that of many less complex organisms, such as the roundworm and the fruit fly. This difference may result from the extensive use of alternative pre-mRNA splicing in humans, which provides the ability to build a very large number of modular proteins through the selective incorporation of exons.
Protein-coding capacity per chromosome. Protein-coding genes are distributed unevenly across the chromosomes, ranging from a few dozen to more than 2000, with an especially high gene density within chromosomes 19, 11, and 1 (Table 1). Each chromosome contains various gene-rich and gene-poor regions, which may be correlated with chromosome bands and GC-content . The significance of these nonrandom patterns of gene density is not well understood.
Size of protein-coding genes. The size of protein-coding genes within the human genome shows enormous variability (Table 2). For example, the gene for histone H1a (HIST1HIA) is relatively small and simple, lacking introns and encoding mRNA sequences of 781 nt and a 215 amino acid protein (648 nt open reading frame). Dystrophin (DMD) is the largest protein-coding gene in the human reference genome, spanning a total of 2.2 MB, while Titin (TTN) has the longest coding sequence (80,780 bp), the largest number of exons (364), and the longest single exon (17,106 bp). Over the whole genome, the median size of an exon is 122 bp (mean = 145 bp), the median number of exons is 7 (mean = 8.8), and the median coding sequence encodes 367 amino acids (mean = 447 amino acids; Table 21 in ).
Table 2. Examples of human protein-coding genes. Chrom, chromosome. Alt splicing, alternative pre-mRNA splicing. (Data source: release 68, July 2012)
Noncoding DNA (ncDNA).
Noncoding DNA is defined as all of the DNA sequences within a genome that are not found within protein-coding exons, and so are never represented within the amino acid sequence of expressed proteins. By this definition, more than 98% of the human genomes is composed of ncDNA.
Numerous classes of noncoding DNA have been identified, including genes for noncoding RNA (e.g. tRNA and rRNA), pseudogenes, introns, untranslated regions of mRNA, regulatory DNA sequences, repetitive DNA sequences, and sequences related to mobile genetic elements.
Numerous sequences that are included within genes are also defined as noncoding DNA. These include genes for noncoding RNA (e.g. tRNA, rRNA), and untranslated components of protein-coding genes (e.g. introns, and 5' and 3' untranslated regions of mRNA).
Protein-coding sequences (specifically, coding exons) constitute less than 1.5% of the human genome. In addition, about 26% of the human genome is introns. Aside from genes (exons and introns) and known regulatory sequences (8–20%), the human genome contains regions of noncoding DNA. The exact amount of noncoding DNA that plays a role in cell physiology has been hotly debated. Recent analysis by the ENCODE project indicates that 80% of the entire human genome is either transcribed, binds to regulatory proteins, or is associated with some other biochemical activity.
It however remains controversial whether all of this biochemical activity contributes to cell physiology, or whether a substantial portion of this is the result transcriptional and biochemical noise, which must be actively filtered out by the organism. Excluding protein-coding sequences, introns, and regulatory regions, much of the non-coding DNA is composed of:
Many DNA sequences that do not play a role in gene expression have important biological functions. Comparative genomics studies indicate that about 5% of the genome contains sequences of noncoding DNA that are highly conserved, sometimes on time-scales representing hundreds of millions of years, implying that these noncoding regions are under strong evolutionary pressure and positive selection.
Many of these sequences regulate the structure of chromosomes by limiting the regions of heterochromatin formation and regulating structural features of the chromosomes, such as the telomeres and centromeres. Other noncoding regions serve as origins of DNA replication. Finally several regions are transcribed into functional noncoding RNA that regulate the expression of protein-coding genes (for example ), mRNA translation and stability (see miRNA), chromatin structure (including histone modifications, for example ), DNA methylation (for example ), DNA recombination (for example ), and cross-regulate other noncoding RNAs (for example ). It is also likely that many transcribed noncoding regions do not serve any role and that this transcription is the product of non-specific RNA Polymerase activity.
Pseudogenes.
Pseudogenes are inactive copies of protein-coding genes, often generated by gene duplication, that have become nonfunctional through the accumulation of inactivating mutations. Table 1 shows that the number of pseudogenes in the human genome is on the order of 13,000, and in some chromosomes is nearly the same as the number of functional protein-coding genes. Gene duplication is a major mechanism through which new genetic material is generated during molecular evolution.
For example, the olfactory receptor gene family is one of the best-documented examples of pseudogenes in the human genome. More than 60 percent of the genes in this family are non-functional pseudogenes in humans. By comparison, only 20 percent of genes in the mouse olfactory receptor gene family are pseudogenes. Research suggests that this is a species-specific characteristic, as the most closely related primates all have proportionally fewer pseudogenes. This genetic discovery helps to explain the less acute sense of smell in humans relative to other mammals.
Genes for noncoding RNA (ncRNA).
Noncoding RNA molecules play many essential roles in cells, especially in the many reactions of protein synthesis and RNA processing. The human genome contains genes encoding 18,400 ncRNAs, including tRNA, ribosomal RNA, microRNA, and other non-coding RNA genes.
One historical misconception regarding the ncRNAs is that they lack critical genetic information or function. Rather, these ncRNAs are often critical elements in gene regulation and expression. Noncoding RNA also contributes to epigenetics, transcription, RNA splicing, and the translational machinery. The role of RNA in genetic regulation and disease offers a new potential level of unexplored genomic complexity.
Introns and untranslated regions of mRNA.
In addition to the ncRNA molecules that are encoded by discrete genes, the initial transcripts of protein coding genes usually contain extensive noncoding sequences, in the form of introns, 5'-untranslated regions (5'-UTR), and 3'-untranslated regions (3'-UTR). Within most protein-coding genes of the human genome, the length of intron sequences is 10- to 100-times the length of exon sequences (Table 2).
Regulatory DNA sequences.
The human genome has many different regulatory sequences which are crucial to controlling gene expression. Conservative estimates indicate that these sequences make up 8% of the genome, however extrapolations from the ENCODE project give that 20-40% of the genome is gene regulatory sequence. Some types of non-coding DNA are genetic "switches" that do not encode proteins, but do regulate when and where genes are expressed (called enhancers).
Regulatory sequences have been known since the late 1960s. The first identification of regulatory sequences in the human genome relied on recombinant DNA technology. Later with the advent of genomic sequencing, the identification of these sequences could be inferred by evolutionary conservation. The evolutionary branch between the primates and mouse, for example, occurred 70–90 million years ago. So computer comparisons of gene sequences that identify conserved non-coding sequences will be an indication of their importance in duties such as gene regulation.
Other genomes have been sequenced with the same intention of aiding conservation-guided methods, for exampled the pufferfish genome. However, regulatory sequences disappear and re-evolve during evolution at a high rate.
As of 2012, the efforts have shifted toward finding interactions between DNA and regulatory proteins by the technique ChIP-Seq, or gaps where the DNA is not packaged by histones (DNase hypersensitive sites), both of which tell where there are active regulatory sequences in the investigated cell type.
Repetitive DNA sequences.
Repetitive DNA sequences comprise approximately 50% of the human genome.
About 8% of the human genome consists of tandem DNA arrays or tandem repeats, low complexity repeat sequences that have multiple adjacent copies (e.g. "CAGCAGCAG..."). The tandem sequences may be of variable lengths, from two nucleotides to tens of nucleotides. These sequences are highly variable, even among closely related individuals, and so are used for genealogical DNA testing and forensic DNA analysis.
Repeated sequences of fewer than ten nucleotides (e.g. the dinucleotide repeat (AC)n) are termed microsatellite sequences. Among the microsatellite sequences, trinucleotide repeats are of particular importance, as sometimes occur within coding regions of genes for proteins and may lead to genetic disorders. For example, Huntington's disease results from an expansion of the trinucleotide repeat (CAG)n within the "Huntingtin" gene on human chromosome 4. Telomeres (the ends of linear chromosomes) end with a microsatellite hexanucleotide repeat of the sequence (TTAGGG)n.
Tandem repeats of longer sequences (arrays of repeated sequences 10–60 nucleotides long) are termed minisatellites.
Mobile genetic elements (transposons) and their relics.
Transposable genetic elements, DNA sequences that can replicate and insert copies of themselves at other locations within a host genome, are an abundant component in the human genome. The most abundant transposon lineage, "Alu", has about 50,000 active copies, and can be inserted into intragenic and intergenic regions. One other lineage, LINE-1, has about 100 active copies per genome (the number varies between people). Together with non-functional relics of old transposons, they account for over half of total human DNA. Sometimes called "jumping genes", transposons have played a major role in sculpting the human genome. Some of these sequences represent endogenous retroviruses, DNA copies of viral sequences that have become permanently integrated into the genome and are now passed on to succeeding generations.
Mobile elements within the human genome can be classified into LTR retrotransposons (8.3% of total genome), SINEs (13.1% of total genome) including Alu elements, LINEs (20.4% of total genome), SVAs and Class II DNA transposons (2.9% of total genome).
Genomic variation in humans.
Human Reference Genome.
With the exception of identical twins, all humans show significant variation in genomic DNA sequences. The Human Reference Genome (HRG) is used as a standard sequence reference.
There are several important points concerning the Human Reference Genome--
Measuring human genetic variation.
Most studies of human genetic variation have focused on single-nucleotide polymorphisms (SNPs), which are substitutions in individual bases along a chromosome. Most analyses estimate that SNPs occur 1 in 1000 base pairs, on average, in the euchromatic human genome, although they do not occur at a uniform density. Thus follows the popular statement that "we are all, regardless of race, genetically 99.9% the same", although this would be somewhat qualified by most geneticists. For example, a much larger fraction of the genome is now thought to be involved in copy number variation. A large-scale collaborative effort to catalog SNP variations in the human genome is being undertaken by the International HapMap Project.
The genomic loci and length of certain types of small repetitive sequences are highly variable from person to person, which is the basis of DNA fingerprinting and DNA paternity testing technologies. The heterochromatic portions of the human genome, which total several hundred million base pairs, are also thought to be quite variable within the human population (they are so repetitive and so long that they cannot be accurately sequenced with current technology). These regions contain few genes, and it is unclear whether any significant phenotypic effect results from typical variation in repeats or heterochromatin.
Most gross genomic mutations in gamete germ cells probably result in inviable embryos; however, a number of human diseases are related to large-scale genomic abnormalities. Down syndrome, Turner Syndrome, and a number of other diseases result from nondisjunction of entire chromosomes. Cancer cells frequently have aneuploidy of chromosomes and chromosome arms, although a cause and effect relationship between aneuploidy and cancer has not been established.
Mapping human genomic variation.
Whereas a genome sequence lists the order of every DNA base in a genome, a genome map identifies the landmarks. A genome map is less detailed than a genome sequence and aids in navigating around the genome.
An example of a variation map is the HapMap being developed by the International HapMap Project. The HapMap is a haplotype map of the human genome, "which will describe the common patterns of human DNA sequence variation." It catalogs the patterns of small-scale variations in the genome that involve single DNA letters, or bases.
Researchers published the first sequence-based map of large-scale structural variation across the human genome in the journal "Nature" in May 2008. Large-scale structural variations are differences in the genome among people that range from a few thousand to a few million DNA bases; some are gains or losses of stretches of genome sequence and others appear as re-arrangements of stretches of sequence. These variations include differences in the number of copies individuals have of a particular gene, deletions, translocations and inversions.
Personal genomes.
A personal genome sequence is a (nearly) complete sequence of the chemical base pairs that make up the DNA of a single person. Because medical treatments have different effects on different people because of genetic variations such as single-nucleotide polymorphisms (SNPs), the analysis of personal genomes may lead to personalized medical treatment based on individual genotypes.
The first personal genome sequence to be determined was that of Craig Venter in 2007. Personal genomes had not been sequenced in the public Human Genome Project to protect the identity of volunteers who provided DNA samples. That sequence was derived from the DNA of several volunteers from a diverse population. However, early in the Venter-led Celera Genomics genome sequencing effort the decision was made to switch from sequencing a composite sample to using DNA from a single individual, later revealed to have been Venter himself. Thus the Celera human genome sequence released in 2000 was largely that of one man. Subsequent replacement of the early composite-derived data and determination of the diploid sequence, representing both sets of chromosomes, rather than a haploid sequence originally reported, allowed the release of the first personal genome. In April 2008, that of James Watson was also completed. Since then hundreds of personal genome sequences have been released, including those of Desmond Tutu, and of a Paleo-Eskimo. In November 2013, a Spanish family made their personal genomics data obtained by direct-to-consumer genetic testing with 23andMe publicly available under a Creative Commons public domain license. This is believed to be the first such public genomics dataset for a whole family.
The sequencing of individual genomes further unveiled levels of genetic complexity that had not been appreciated before. Personal genomics helped reveal the significant level of diversity in the human genome attributed not only to SNPs but structural variations as well. However, the application of such knowledge to the treatment of disease and in the medical field is only in its very beginnings. Exome sequencing has become increasingly popular as a tool to aid in diagnosis of genetic disease because the exome contributes only 1% of the genomic sequence but accounts for roughly 85% of mutations that contribute significantly to disease.
Human genetic disorders.
Most aspects of human biology involve both genetic (inherited) and non-genetic (environmental) factors. Some inherited variation influences aspects of our biology that are not medical in nature (height, eye color, ability to taste or smell certain compounds, etc.). Moreover, some genetic disorders only cause disease in combination with the appropriate environmental factors (such as diet). With these caveats, genetic disorders may be described as clinically defined diseases caused by genomic DNA sequence variation. In the most straightforward cases, the disorder can be associated with variation in a single gene. For example, cystic fibrosis is caused by mutations in the CFTR gene, and is the most common recessive disorder in caucasian populations with over 1,300 different mutations known.
Disease-causing mutations in specific genes are usually severe in terms of gene function, and are fortunately rare, thus genetic disorders are similarly individually rare. However, since there are many genes that can vary to cause genetic disorders, in aggregate they constitute a significant component of known medical conditions, especially in pediatric medicine. Molecularly characterized genetic disorders are those for which the underlying causal gene has been identified, currently there are approximately 2,200 such disorders annotated in the OMIM database.
Studies of genetic disorders are often performed by means of family-based studies. In some instances population based approaches are employed, particularly in the case of so-called founder populations such as those in Finland, French-Canada, Utah, Sardinia, etc. Diagnosis and treatment of genetic disorders are usually performed by a geneticist-physician trained in clinical/medical genetics. The results of the Human Genome Project are likely to provide increased availability of genetic testing for gene-related disorders, and eventually improved treatment. Parents can be screened for hereditary conditions and counselled on the consequences, the probability it will be inherited, and how to avoid or ameliorate it in their offspring.
As noted above, there are many different kinds of DNA sequence variation, ranging from complete extra or missing chromosomes down to single nucleotide changes. It is generally presumed that much naturally occurring genetic variation in human populations is phenotypically neutral, i.e. has little or no detectable effect on the physiology of the individual (although there may be fractional differences in fitness defined over evolutionary time frames). Genetic disorders can be caused by any or all known types of sequence variation. To molecularly characterize a new genetic disorder, it is necessary to establish a causal link between a particular genomic sequence variant and the clinical disease under investigation. Such studies constitute the realm of human molecular genetics.
With the advent of the Human Genome and International HapMap Project, it has become feasible to explore subtle genetic influences on many common disease conditions such as diabetes, asthma, migraine, schizophrenia, etc. Although some causal links have been made between genomic sequence variants in particular genes and some of these diseases, often with much publicity in the general media, these are usually not considered to be genetic disorders "per se" as their causes are complex, involving many different genetic and environmental factors. Thus there may be disagreement in particular cases whether a specific medical condition should be termed a genetic disorder. The categorized table below provides the prevalence as well as the genes or chromosomes associated with some human genetic disorders.
Evolution.
Comparative genomics studies of mammalian genomes suggest that approximately 5% of the human genome has been conserved by evolution since the divergence of extant lineages approximately 200 million years ago, containing the vast majority of genes. The published chimpanzee genome differs from that of the human genome by 1.23% in direct sequence comparisons. Around 20% of this figure is accounted for by variation within each species, leaving only ~1.06% consistent sequence divergence between humans and chimps at shared genes. This nucleotide by nucleotide difference is dwarfed, however, by the portion of each genome that is not shared, including around 6% of functional genes that are unique to either humans or chimps.
In other words, the considerable observable differences between humans and chimps may be due as much or more to genome level variation in the number, function and expression of genes rather than DNA sequence changes in shared genes. Indeed, even within humans, there has been found to be a previously unappreciated amount of copy number variation (CNV) which can make up as much as 5 – 15% of the human genome. In other words, between humans, there could be +/- 500,000,000 base pairs of DNA, some being active genes, others inactivated, or active at different levels. The full significance of this finding remains to be seen. On average, a typical human protein-coding gene differs from its chimpanzee ortholog by only two amino acid substitutions; nearly one third of human genes have exactly the same protein translation as their chimpanzee orthologs. A major difference between the two genomes is human chromosome 2, which is equivalent to a fusion product of chimpanzee chromosomes 12 and 13 (later renamed to chromosomes 2A and 2B, respectively).
Humans have undergone an extraordinary loss of olfactory receptor genes during our recent evolution, which explains our relatively crude sense of smell compared to most other mammals. Evolutionary evidence suggests that the emergence of color vision in humans and several other primate species has diminished the need for the sense of smell.
Mitochondrial DNA.
The human mitochondrial DNA is of tremendous interest to geneticists, since it undoubtedly plays a role in mitochondrial disease. It also sheds light on human evolution; for example, analysis of variation in the human mitochondrial genome has led to the postulation of a recent common ancestor for all humans on the maternal line of descent (see Mitochondrial Eve).
Due to the lack of a system for checking for copying errors, mitochondrial DNA (mtDNA) has a more rapid rate of variation than nuclear DNA. This 20-fold increase in the mutation rate allows mtDNA to be used for more accurate tracing of maternal ancestry. Studies of mtDNA in populations have allowed ancient migration paths to be traced, such as the migration of Native Americans from Siberia or Polynesians from southeastern Asia. It has also been used to show that there is no trace of Neanderthal DNA in the European gene mixture inherited through purely maternal lineage. Due to the restrictive all or none manner of mtDNA inheritance, this result (no trace of Neanderthal mtDNA) would be likely unless there were a large percentage of Neanderthal ancestry, or there was strong positive selection for that mtDNA (for example, going back 5 generations, only 1 of your 32 ancestors contributed to your mtDNA, so if one of these 32 was pure Neanderthal you would expect that ~3% of your autosomal DNA would be of Neanderthal origin, yet you would have a ~97% chance to have no trace of Neanderthal mtDNA).
Epigenome.
Epigenetics describes a variety of features of the human genome that transcend its primary DNA sequence, such as chromatin packaging, histone modifications and DNA methylation, and which are important in regulating gene expression, genome replication and other cellular processes. Epigenetic markers strengthen and weaken transcription of certain genes but do not affect the actual sequence of DNA nucleotides. DNA methylation is a major form of epigenetic control over gene expression and one of the most highly studied topics in epigenetics. During development, the human DNA methylation profile experiences dramatic changes. In early germ line cells, the genome has very low methylation levels. These low levels generally describe active genes. As development progresses, parental imprinting tags lead to increased methylation activity.
Epigenetic patterns can be identified between tissues within an individual as well as between individuals themselves. Identical genes that have differences only in their epigenetic state are called epialleles. Epialleles can be placed into three categories: those directly determined by an individual’s genotype, those influenced by genotype, and those entirely independent of genotype. The epigenome is also influenced significantly by environmental factors. Diet, toxins, and hormones impact the epigenetic state. Studies in dietary manipulation have demonstrated that methyl-deficient diets are associated with hypomethylation of the epigenome. Such studies establish epigenetics as an important interface between the environment and the genome.

</doc>
<doc id="42889" url="http://en.wikipedia.org/wiki?curid=42889" title="Fusor">
Fusor

A fusor is a device that uses an electric field to heat ions to conditions suitable for nuclear fusion. The machine has a voltage between two metal cages inside a vacuum. Positive ions fall down this voltage drop, building up speed. If they collide in the center, they can fuse. This is a type of Inertial electrostatic confinement device.
A Farnsworth–Hirsch fusor is the most common type of fusor. This design came from work by Philo T. Farnsworth (in 1964) and Robert L. Hirsch in 1967. A variant of fusor had been proposed previously by William Elmore, James L. Tuck, and Ken Watson at the Los Alamos National Laboratory though they never built the machine.
Fusors have been built by various institutions. These include academic institutions such as the University of Wisconsin–Madison, the Massachusetts Institute of Technology and government entities, such as the Atomic Energy Organization of Iran and the Turkish Atomic Energy Authority. Fusors have also been developed commercially, as sources for neutrons by DaimlerChrysler Aerospace and as a method for generating medical isotopes. Fusors have also become very popular for hobbyists and amateurs. A growing number of amateurs have performed nuclear fusion using simple fusor machines.
Mechanism.
For every volt that an ion of ±1 charge is accelerated across, it gains 11,604 kelvins in temperature. For example, a typical magnetic confinement fusion plasma is 15 keV, or 170 megakelvins. An ion with a charge of one can reach this temperature by being accelerated across a fifteen thousand volt drop. In fusors, the voltage drop is made with a wire cage. Because most of the ions fall into the wires of the cage, fusors suffer from high conduction losses. Hence, no fusor has ever come close to break-even energy output.
History.
"See also, history of IEC"
The fusor was originally conceived by Philo T. Farnsworth, better known for his pioneering work in television. In the early 1930s, he investigated a number of vacuum tube designs for use in television, and found one that led to an interesting effect. In this design, which he called the "multipactor", electrons moving from one electrode to another were stopped in mid-flight with the proper application of a high-frequency magnetic field. The charge would then accumulate in the center of the tube, leading to high amplification. Unfortunately it also led to high erosion on the electrodes when the electrons eventually hit them, and today the multipactor effect is generally considered a problem to be avoided.
What particularly interested Farnsworth about the device was its ability to focus electrons at a particular point. One of the biggest problems in fusion research is to keep the hot fuel from hitting the walls of the container. If this is allowed to happen, the fuel cannot be kept hot enough for the fusion reaction to occur. Farnsworth reasoned that he could build an electrostatic plasma confinement system in which the "wall" fields of the reactor were electrons or ions being held in place by the "multipactor". Fuel could then be injected through the wall, and once inside it would be unable to escape. He called this concept a virtual electrode, and the system as a whole the "fusor".
Design.
Farnsworth's original fusor designs were based on cylindrical arrangements of electrodes, like the original multipactors. Fuel was ionized and then fired from small accelerators through holes in the outer (physical) electrodes. Once through the hole they were accelerated towards the inner reaction area at high velocity. Electrostatic pressure from the positively charged electrodes would keep the fuel as a whole off the walls of the chamber, and impacts from new ions would keep the hottest plasma in the center. He referred to this as inertial electrostatic confinement, a term that continues to be used to this day.
Work at Farnsworth Television labs.
All of this work had taken place at the Farnsworth Television labs, which had been purchased in 1949 by ITT Corporation, as part of its plan to become the next RCA. However, a fusion research project was not regarded as immediately profitable. In 1965, the board of directors started asking Geneen to sell off the Farnsworth division, but he had his 1966 budget approved with funding until the middle of 1967. Further funding was refused, and that ended ITT's experiments with fusion.
Things changed dramatically with the arrival of Robert Hirsch, and the introduction of the modified Hirsch-Meeks fusor patent. New fusors based on Hirsch's design were first constructed between 1964 and 1967. Hirsch published his design in a paper in 1967. His design included ion beams to shoot ions into the vacuum chamber.
The team then turned to the AEC, then in charge of fusion research funding, and provided them with a demonstration device mounted on a serving cart that produced more fusion than any existing "classical" device. The observers were startled, but the timing was bad; Hirsch himself had recently revealed the great progress being made by the Soviets using the tokamak. In response to this surprising development, the AEC decided to concentrate funding on large tokamak projects, and reduce backing for alternative concepts.
Recent developments.
In the early 1980s, disappointed by the slow progress on "big machines", a number of physicists took a fresh look at alternative designs. George H. Miley at the University of Illinois picked up on the fusor and re-introduced it into the field. A low but steady interest in the fusor has persisted since. An important development was the successful commercial introduction of a fusor-based neutron generator. From 2006 until his death in 2007, Robert W. Bussard gave talks on a reactor similar in design to the fusor, now called the polywell, that he stated would be capable of useful power generation. Most recently, the fusor has gained popularity among amateurs, who choose them as home projects due to their relatively low space, money, and power requirements. An online community of "fusioneers", The Open Source Fusor Research Consortium, or Fusor.net, is dedicated to reporting developments in the world of fusors and aiding other amateurs in their projects. The site includes forums, articles and papers done on the fusor, including Farnsworth's original patent, as well as Hirsch's patent of his version of the invention.
Fusion in fusors.
Basic fusion.
Nuclear fusion refers to reactions in which lighter nuclei are combined to become heavier nuclei. This process changes mass into energy which in may be captured to provide fusion power. Many types of atoms can be fused. The easiest to fuse are deuterium and tritium. This happens when the ions have to have a temperature of at least 4 keV (kiloelectronvolts) or about 45 million kelvins. The second easiest reaction is fusing deuterium with itself. Because this gas is cheaper, it is the fuel commonly used by amateurs. The ease of doing a fusion reaction is measured by its cross section.
Net power.
At such conditions, the atoms are ionized and make a plasma. The energy generated by fusion, inside a hot plasma cloud can be found with the following equation.
where:
This equation shows that energy varies with the temperature, density, speed of collision, and fuel used. To reach net power, fusion reactions have to occur fast enough to make up for energy losses. Any power plant using fusion will hold in this hot cloud. Plasma clouds lose energy through conduction and radiation. Conduction is when ions, electrons or neutrals touch a surface and leak out. Energy is lost with the particle. Radiation is when energy leaves the cloud as light. Radiation increases as the temperature rises. To get net power from fusion, you must overcome these losses. This leads to an equation for power output.
where:
John Lawson used this equation to estimate some conditions for net power based on a Maxwellian cloud. This is the Lawson criterion. Fusors typically suffer from conduction losses due to the wire cage being in the path of the recirculating plasma.
In fusors.
In the original fusor design, several small particle accelerators, essentially TV tubes with the ends removed, inject ions at a relatively low voltage into a vacuum chamber. In the Hirsch version of the fusor, the ions are produced by ionizing a dilute gas in the chamber. In either version there are two concentric spherical electrodes, the inner one being charged negatively with respect to the outer one (to about 80 kV). Once the ions enter the region between the electrodes, they are accelerated towards the center.
In the fusor, the ions are accelerated to several keV by the electrodes, so heating as such is not necessary (as long as the ions fuse before losing their energy by any process). Whereas 45 megakelvins is a very high temperature by any standard, the corresponding voltage is only 4 kV, a level commonly found in such devices as neon lights and televisions. To the extent that the ions remain at their initial energy, the energy can be tuned to take advantage of the peak of the reaction cross section or to avoid disadvantageous (for example neutron-producing) reactions that might occur at higher energies.
Various attempts have been made at increasing deuterium ionization rate, including heaters within "ion-guns", (similar to the "electron gun" which forms the basis for old-style television display tubes), as well as magnetron type devices, (which are the power sources for microwave ovens), which can enhance ion formation using high-voltage electro-magnetic fields. Any method which increases ion density (within limits which preserve ion mean-free path), or ion energy, can be expected to enhance the fusion yield, typically measured in the number of neutrons produced per second.
The ease with which the ion energy can be increased appears to be particularly useful when "high temperature" fusion reactions are considered, such as proton-boron fusion, which has plentiful fuel, requires no radioactive tritium, and produces no neutrons in the primary reaction.
Common considerations.
Modes of operation.
Fusors have at least two modes of operation (possibly more): Star Mode and Halo Mode. Halo mode is characterized by a broad symmetric glow, with one or two electron beams exiting the structure. There is little fusion. The halo mode occurs in higher pressure tanks, and as the vacuum improves, the device transitions to star mode. Star mode appears as bright beams of light emanating from the device center.
Power density.
Because the electric field made by the cages is negative, it cannot simultaneously trap both positively charged ions and negative electrons. Hence, there must be some regions of charge accumulation, which will result in an upper limit on the achievable density. This could place an upper limit on the machine's power density, which may keep it too low for power production.
Thermalization of the ion velocities.
When they first fall into the center of the fusor, the ions will all have the same energy, but the velocity distribution will rapidly approach a Maxwell–Boltzmann distribution. This would occur through simple Coulomb collisions in a matter of milliseconds, but beam-beam instabilities will occur orders of magnitude faster still. In comparison, any given ion will require a few minutes before undergoing a fusion reaction, so that the monoenergetic picture of the fusor, at least for power production, is not appropriate. One consequence of the thermalization is that some of the ions will gain enough energy to leave the potential well, taking their energy with them, without having undergone a fusion reaction.
Electrodes.
There are a number of unsolved challenges with the electrodes in a fusor power system. To begin with, the electrodes cannot influence the potential within themselves, so it would seem at first glance that the fusion plasma would be in more or less direct contact with the inner electrode, resulting in contamination of the plasma and destruction of the electrode. However, the majority of the fusion tends to occur in microchannels formed in areas of minimum electric potential, seen as visible "rays" penetrating the core. These form because the forces within the region correspond to roughly stable "orbits". Approximately 40% of the high energy ions in a typical grid operating in star mode may be within these microchannels. Nonetheless, grid collisions remain the primary energy loss mechanism for Farnsworth-Hirsch fusors. Complicating issues is the challenge in cooling the central electrode; any fusor producing enough power to run a power plant seems destined to also destroy its inner electrode. As one fundamental limitation, any method which produces a neutron flux that is captured to heat a working fluid will also bombard its electrodes with that flux, heating them as well.
Attempts to resolve these problems include Bussard's Polywell system, D. C. Barnes' modified Penning trap approach, and the University of Illinois's fusor which retains grids but attempts to more tightly focus the ions into microchannels to attempt to avoid losses. While all three are IEC devices, only the last is actually a "fusor".
Radiation.
Nonrelativistic particles will radiate energy as light when they change speed. This loss rate can be estimated using the Larmor formula. Inside a fusor there is a cloud of ions and electrons. These particles will accelerate or decelerate as they move about. These changes in speed make the cloud lose energy as light. The radiation from a fusor can (at least) be in the visible, ultraviolet and X-ray spectrum, depending on the type of fusor used. These changes in speed can be due to electrostatic interactions between particles (ion to ion, ion to electron, electron to electron). This is referred to bremsstrahlung radiation, and is common in fusors. Changes in speed can also be due to interactions between the particle and the electric field. Since there are no magnetic fields, fusors emit no Cyclotron radiation at slow speeds, or synchrotron radiation at high speeds.
In , Todd Rider argues that a quasineutral isotropic plasma will lose energy due to Bremsstrahlung at a rate prohibitive for any fuel other than D-T (or possibly D-D or D-He3). This paper is not applicable to IEC fusion, as a quasineutral plasma cannot be contained by an electric field, which is a fundamental part of IEC fusion. However, in an earlier paper, , Rider addresses the common IEC devices directly, including the fusor. In the case of the fusor the electrons are generally separated from the mass of the fuel isolated near the electrodes, which limits the loss rate. However, Rider demonstrates that practical fusors operate in a range of modes that either lead to significant electron mixing and losses, or alternately lower power densities. This appears to be a sort of catch-22 that limits the output of any fusor-like system.
Commercial Applications.
Neutron Source.
The fusor has been demonstrated as a viable neutron source. Typical fusors cannot reach fluxes as high as nuclear reactor or particle accelerator sources, but are sufficient for many uses. Importantly, the neutron generator easily sits on a benchtop, and can be turned off at the flick of a switch. A commercial fusor was developed as a non-core business within DaimlerChrysler Aerospace - Space Infrastructure, Bremen between 1996 and early 2001. After the project was effectively ended, the former project manager established a company which is called NSD-Fusion. To date, the highest neutron flux achieved by a fusor-like device has been 3 × 1011 neutrons per second with the deuterium-deuterium fusion reaction.
Medical isotopes.
Commercial startups have used the neutron fluxes generated by fusors to generate Mo-99, an isotope used for medical care.
Fusor examples.
Professional.
Fusors have been theoretically studied at multiple institutions, including: Kyoto University, and Kyushu University. Researchers meet annually at the US-Japan Workshop on Inertial Electrostatic Confinement Fusion. Listed here, are actual machines built.
Amateur.
A number of amateurs have built working fusors and detected neutrons. Many fusor enthusiasts connect on forums and message boards online. Below are some examples of working fusors. 
Further reading.
</dl>

</doc>
<doc id="42890" url="http://en.wikipedia.org/wiki?curid=42890" title="Philo Farnsworth">
Philo Farnsworth

Philo Taylor Farnsworth (August 19, 1906 – March 11, 1971) was an American inventor and television pioneer. He made many contributions that were crucial to the early development of all-electronic television. He is perhaps best known for inventing the first fully functional all-electronic image pickup device (video camera tube), the "image dissector", as well as the first fully functional and complete all-electronic television system. He was also the first person to demonstrate such a system to the public. Farnsworth developed a television system complete with receiver and camera, which he produced commercially in the firm of the Farnsworth Television and Radio Corporation, from 1938 to 1951.
In later life, Farnsworth invented a small nuclear fusion device, the Farnsworth–Hirsch fusor, or simply "fusor", employing inertial electrostatic confinement (IEC). Although not a practical device for generating nuclear energy, the fusor serves as a viable source of neutrons. The design of this device has been the acknowledged inspiration for other fusion approaches including the Polywell reactor concept in terms of a general approach to fusion design. Farnsworth held 165 patents, mostly in radio and television.
Early life.
Philo T. Farnsworth was born August 19, 1906, the eldest of five children of Lewis Edwin Farnsworth and Serena Amanda Bastian, a Mormon couple then living in a small log cabin built by Lewis's father in a place called Indian Creek near Beaver, Utah. In 1918, the family moved to a relative's 240-acre ranch near Rigby, Idaho, where Lewis supplemented his farming income by hauling freight with his horse-drawn wagon. Philo was excited to find his new home was wired for electricity, with a Delco generator providing power for lighting and farm machinery. He was a quick study in mechanical and electrical technology, repairing the troublesome generator, and upon finding a burned out electric motor among some items discarded by the previous tenants, proceeding to rewind the armature and convert his mother's hand-powered washing machine into an electric-powered one. Philo developed an early interest in electronics after his first telephone conversation with an out-of-state relative and the discovery of a large cache of technology magazines in the attic of the family’s new home, and won a $25 first prize in a pulp-magazine contest for inventing a magnetized car lock.
Farnsworth excelled in chemistry and physics at Rigby High School. He asked his high school science teacher, Justin Tolman, for advice about an electronic television system he was contemplating. He provided the teacher with sketches and diagrams covering several blackboards to show how it might be accomplished electronically. He asked his teacher if he should go ahead with his ideas, and he was encouraged to do so.
One of the drawings he did on a blackboard for his chemistry teacher was recalled and reproduced for a patent interference case between Farnsworth and Radio Corporation of America (RCA).
In 1923, the Farnsworths moved to Provo, Utah, and Farnsworth attended Brigham Young High School beginning that fall. His father died of pneumonia in January 1924, at age 58, and Farnsworth, as eldest son, assumed responsibility for sustaining the family while still attending high school and graduating in June 1924. He went on to attend Brigham Young University that year, and to earn Junior Radio-Trician certification from the National Radio Institute, adding a full certification in 1925. While attending college, he met Provo High School student Elma “Pem” Gardner, (February 25, 1908 – April 27, 2006), whom he would later marry.
Later in 1924, Farnsworth applied to the United States Naval Academy in Annapolis, Maryland, where he was recruited after he earned the nation's second highest score on academy tests. However, he was already thinking ahead to his television projects and, upon learning the government would own his patents if he stayed in the military, he sought and received an honorable discharge within months, under a provision in which the eldest child in a fatherless family could be excused from military service in order to provide for his family. He returned to Provo and enrolled again at Brigham Young University, where he was allowed to take advanced science classes.
Philo worked while his sister Agnes, the elder of the two sisters, took charge of the family home and the second-floor boarding house (with the help of a cousin then living with the family). The Farnsworths later moved into half of a duplex, with family friends the Gardners moving into the other side when it became vacant. Philo developed a close friendship with Pem Gardner's brother, Cliff Gardner, who shared Farnsworth's interest in electronics. The two moved to Salt Lake City to start a radio repair business.
The business failed and Gardner returned to Provo. Farnsworth remained in Salt Lake City, and through enrollment in a University of Utah job-placement service became acquainted with Leslie Gorrell and George Everson, a pair of San Francisco philanthropists who were then conducting a Salt Lake City Community Chest fundraising campaign.
They agreed to fund Farnsworth's early television research with an initial $6,000 in backing, and set up a laboratory in Los Angeles for Farnsworth to carry out his experiments. Before relocating to California, Farnsworth married Pem Gardner Farnsworth (February 25, 1908 – April 27, 2006), on May 27, 1926, and the two traveled to the West Coast in a Pullman coach.
Career.
A few months after arriving in California, Farnsworth was prepared to show his models and drawings to a patent attorney who was nationally recognized as an authority on electrophysics. Everson and Gorrell agreed that Farnsworth should apply for patents for his designs, a decision which proved crucial in later disputes with RCA. Most television systems in use at the time used image scanning devices ("rasterizers") employing rotating "Nipkow disks" comprising lenses arranged in spiral patterns such that they swept across an image in a succession of short arcs while focusing the light they captured on photosensitive elements, thus producing a varying electrical signal corresponding to the variations in light intensity. Farnsworth recognized the limitations of the mechanical systems, and that an all-electronic scanning system could produce a superior image for transmission to a receiving device.
On September 7, 1927, Farnsworth's image dissector camera tube transmitted its first image, a simple straight line, to a receiver in another room of his laboratory at 202 Green Street in San Francisco. Pem Farnsworth recalled in 1985 that her husband broke the stunned silence of his lab assistants by saying, "There you are — electronic television!" The source of the image was a glass slide, backlit by an arc lamp. An extremely bright source was required because of the low light sensitivity of the design. By 1928, Farnsworth had developed the system sufficiently to hold a demonstration for the press. His backers had demanded to know when they would see dollars from the invention, so the first image shown was, appropriately, a dollar sign. In 1929, the design was further improved by elimination of a motor-generator, so the television system now had no mechanical parts. That year, Farnsworth transmitted the first live human images using his television system, including a three and a half-inch image of his wife Pem with her eyes closed because of the blinding light required.
Many inventors had built electromechanical television systems before Farnsworth's seminal contribution, but Farnsworth designed and built the world's first working all-electronic television system, employing electronic scanning in both the pickup and display devices. He first demonstrated his system to the press on September 3, 1928, and to the public at the Franklin Institute in Philadelphia on August 25, 1934.
In 1930, Vladimir Zworykin, who had been developing his own all-electronic television system at Westinghouse in Pittsburgh since 1923, but which he had never been able to make work or satisfactorily demonstrate to his superiors, was recruited by RCA to lead its television development department. Before leaving his old employer, Zworykin visited Farnsworth's laboratory and was sufficiently impressed with the performance of the Image Dissector that he reportedly had his team at Westinghouse make several copies of the device for experimentation. But Zworykin later abandoned research on the Image Dissector, which at the time required extremely bright illumination of its subjects to be effective, and turned his attention to what would become the Iconoscope. In a 1970s series of videotaped interviews, Zworykin recalled that, "Farnsworth was closer to this thing you're using
now [i.e., a video camera] than anybody, because he used the cathode-ray tube for transmission. But, Farnsworth didn't have the mosaic [of discrete light elements], he didn't have storage. Therefore, [picture] definition was very low... But he was very proud, and he stuck to his method."
In 1931, David Sarnoff of RCA offered to buy Farnsworth's patents for US$100,000, with the stipulation that he become an employee of RCA, but Farnsworth refused. In June of that year, Farnsworth joined the Philco company and moved to Philadelphia along with his wife and two children. RCA would later file an interference suit against Farnsworth, claiming Zworykin's 1923 patent had priority over Farnsworth's design, despite the fact it could present no evidence that Zworykin had actually produced a functioning transmitter tube before 1931. Farnsworth had lost two interference claims to Zworykin in 1928, but this time he prevailed and the U.S. Patent Office rendered a decision in 1934 awarding priority of the invention of the image dissector to Farnsworth. RCA lost a subsequent appeal, but litigation over a variety of issues continued for several years with Sarnoff finally agreeing to pay Farnsworth royalties. Zworykin received a patent in 1928 for a color transmission version of his 1923 patent application; he also divided his original application in 1931, receiving a patent in 1935, while a second one was eventually issued in 1938 by the Court of Appeals on a non-Farnsworth-related interference case, and over the objection of the Patent Office.
In 1932, while in England to raise money for his legal battles with RCA, Farnsworth met with John Logie Baird, a Scottish inventor who had given the world's first public demonstration of a working television system in London in 1926, using an electro-mechanical imaging system, and who was seeking to develop electronic television receivers. Baird demonstrated his mechanical system for Farnsworth. Baird's company directors pursued a merger with Farnsworth, paying $50,000 to supply electronic television equipment and provide access to Farnsworth patents. Baird and Farnsworth competed with EMI for the U.K. standard television system, but EMI merged with the Marconi Company in 1934, gaining access to the RCA Iconoscope patents. After trials of both systems, the BBC committee chose the Marconi-EMI system, which was by then virtually identical to RCA's system. The image dissector scanned well but had poor light sensitivity compared to the Marconi-EMI Iconoscopes, dubbed "Emitrons".
After sailing to Europe in 1934, Farnsworth secured an agreement with Goerz-Bosch-Fernseh in Germany. Some image dissector cameras were used to broadcast the 1936 Olympic Games in Berlin.
In March 1932, Philco denied Farnsworth time to travel to Utah to bury his young son Kenny, placing a strain on Farnsworth's marriage, and possibly marking the beginning of his struggle with depression. In May 1933, the Philco Corporation severed their relationship with Farnsworth because, in George Everson's words, "it [had] become apparent that Philo's aim at establishing a broad patent structure through research [was] not identical with the production program of Philco." Many sources paint this breakup as Philco's idea, but in Everson's view the decision was mutual and amicable.
Farnsworth returned to his laboratory, and by 1936 his company was regularly transmitting entertainment programs on an experimental basis. That same year, while working with University of Pennsylvania biologists, Farnsworth developed a process to sterilize milk using radio waves. He also invented a fog-penetrating beam for ships and airplanes.
In 1936 he attracted the attention of "Collier's Weekly", which described his work in glowing terms. "One of those amazing facts of modern life that just don't seem possible – namely, electrically scanned television that seems destined to reach your home next year, was largely given to the world by a nineteen-year-old boy from Utah ... Today, barely thirty years old he is setting the specialized world of science on its ears."
In 1938, Farnsworth established the Farnsworth Television and Radio Corporation in Fort Wayne, Indiana, with E. A. Nicholas as president and himself as director of research. In September 1939, after a more than decade-long legal battle, RCA finally conceded to a multi-year licensing agreement concerning Farnsworth's 1927 patent for television totaling $1 million. RCA was then free, after showcasing electronic television at New York World's Fair on April 20, 1939, to sell electronic television cameras to the public.
Farnsworth Television and Radio Corporation was purchased by International Telephone and Telegraph (ITT) in 1951. During his time at ITT, Farnsworth worked in a basement laboratory known as "the cave" on Pontiac Street in Fort Wayne. From there he introduced a number of breakthrough concepts, including a defense early warning signal, submarine detection devices, radar calibration equipment and an infrared telescope. "Philo was a very deep person – tough to engage in conversation, because he was always thinking about what he could do next," said Art Resler, an ITT photographer who documented Farnsworth’s work in pictures. One of Farnsworth's most significant contributions at ITT was the PPI Projector, an enhancement on the iconic "circular sweep" radar display, which allowed safe air traffic control from the ground. This system developed in the 1950s was the forerunner of today’s air traffic control systems.
In addition to his electronics research, ITT management agreed to nominally fund Farnsworth's nuclear fusion research. He and staff members invented and refined a series of fusion reaction tubes called "fusors". For scientific reasons unknown to Farnsworth and his staff, the necessary reactions lasted no longer than thirty seconds. In December 1965, ITT came under pressure from its board of directors to terminate the expensive project and sell the Farnsworth subsidiary. It was only due to the urging of president Harold Geneen that the 1966 budget was accepted, extending ITT's fusion research for an additional year. The stress associated with this managerial ultimatum, however, caused Farnsworth to suffer a relapse. A year later he was terminated and eventually allowed medical retirement.
In the spring of 1967, Farnsworth and his family moved back to Utah to continue his fusion research at Brigham Young University, which presented him with an honorary doctorate. The university also offered him office space and an underground concrete bunker for the project. Realizing the fusion lab was to be dismantled at ITT, Farnsworth invited staff members to accompany him to Salt Lake City, as team members in Philo T. Farnsworth Associates (PTFA). By late 1968, the associates began holding regular business meetings and PTFA was underway. Although a contract with the National Aeronautics and Space Administration (NASA) was promptly secured, and more possibilities were within reach, financing stalled for the $24,000 in monthly expenses required to cover salaries and equipment rental.
By Christmas 1970, PTFA had failed to secure the necessary financing, and the Farnsworths had sold all their own ITT stock and cashed in Philo's life insurance policy to maintain organizational stability. The underwriter had failed to provide the financial backing that was to have supported the organization during its critical first year. The banks called in all outstanding loans, repossession notices were placed on anything not previously sold, and the Internal Revenue Service put a lock on the laboratory door until delinquent taxes were paid. In January 1971, PTFA disbanded. Farnsworth had begun abusing alcohol in his later years, and as a consequence he became seriously ill with pneumonia, and died on March 11, 1971.
Farnsworth's wife Elma Gardner "Pem" Farnsworth fought for decades after his death to assure his place in history. Farnsworth always gave her equal credit for creating television, saying, "my wife and I started this TV." She died on April 27, 2006, at age 98. The inventor and wife were survived by two sons, Russell (then living in New York), and Kent (then living in Fort Wayne, Indiana).
In 1999, "Time" magazine included Farnsworth in the "".
Inventions.
Electronic television.
Farnsworth worked out the principle of the image dissector in the summer of 1921, not long before his fifteenth birthday, and demonstrated the first working version on September 7, 1927, having turned 21 the previous August. A farm boy, his inspiration for scanning an image as series of lines came from the back-and-forth motion used to plow a field. In the course of a patent interference suit brought by RCA in 1934 and decided in February 1935, his high school chemistry teacher, Justin Tolman, produced a sketch he had made of a blackboard drawing Farnsworth had shown him in spring 1922. Farnsworth won the suit; RCA appealed the decision in 1936 and lost. Although Farnsworth was paid royalties by RCA, he never became wealthy. The video camera tube that evolved from the combined work of Farnsworth, Zworykin and many others was used in all television cameras until the late 20th century, when alternate technologies such as charge-coupled devices started to appear.
Farnsworth also developed the "image oscillite", a cathode ray tube that displayed the images captured by the image dissector.
Farnsworth called his device an image dissector because it converted individual elements of the image into electricity one at a time. He replaced the spinning disks with caesium, an element that emits electrons when exposed to light.
Fusor.
The Farnsworth–Hirsch fusor is an apparatus designed by Farnsworth to create nuclear fusion. Unlike most controlled fusion systems, which slowly heat a magnetically confined plasma, the fusor injects high temperature ions directly into a reaction chamber, thereby avoiding a considerable amount of complexity.
When the Farnsworth-Hirsch fusor was first introduced to the fusion research world in the late 1960s, the fusor was the first device that could clearly demonstrate it was producing fusion reactions at all. Hopes at the time were high that it could be quickly developed into a practical power source. However, as with other fusion experiments, development into a power source has proven difficult. Nevertheless, the fusor has since become a practical neutron source and is produced commercially for this role.
Other inventions.
At the time he died, Farnsworth held 300 U.S. and foreign patents. His inventions contributed to the development of radar, infra-red night vision devices, the electron microscope, the baby incubator, the gastroscope, and the astronomical telescope.
TV appearance.
Although he was the man responsible for its technology, Farnsworth appeared only once on a television program. On July 3, 1957, he was a mystery guest ("Doctor X") on the CBS quiz show "I've Got A Secret". He fielded questions from the panel as they unsuccessfully tried to guess his secret ("I invented electronic television."). For stumping the panel, he received $80 and a carton of Winston cigarettes. Moore then spent a few minutes discussing with Farnsworth his research on such projects as high definition television, flat-screen receivers, and fusion power.
Farnsworth said, "There had been attempts to devise a television system using mechanical disks and rotating mirrors and vibrating mirrors — all mechanical. My contribution was to take out the moving parts and make the thing entirely electronic, and that was the concept that I had when I was just a freshman in high school in the Spring of 1921 at age 14." When Moore asked about others' contributions, Farnsworth agreed, "There are literally thousands of inventions important to television. I hold something in excess of 165 American patents." The host then asked about his current research, and the inventor replied, "In television, we're attempting first to make better utilization of the bandwidth, because we think we can eventually get in excess of 2000 lines instead of 525 ... and do it on an even narrower channel ... which will make for a much sharper picture. We believe in the picture-frame type of a picture, where the visual display will be just a screen. And we hope for a memory, so that the picture will be just as though it's pasted on there."
A letter to the editor of the Idaho Falls "Post Register" disputed that Farnsworth had made only one television appearance. Roy Southwick claimed "... I interviewed Mr. [Philo] Farnsworth back in 1953 - the first day KID-TV went on the air." KID-TV, which later became KIDK-TV, was then located near the Rigby area where Farnsworth grew up.
Memorials and legacy.
In a 1996 videotaped interview by the Academy of Television Arts & Sciences, available on YouTube, Elma Farnsworth recounts Philo's change of heart about the value of television, after seeing how it showed man walking on the moon, in real time, to millions of viewers:
In fiction, Farnsworth appeared in the "Futurama" episode "All The Presidents' Heads" as an ancestor of Professor Farnsworth and Philip J. Fry, and was referred to as having invented the television.
Farnsworth and the introduction of television are significant characters in "Carter Beats the Devil", a novel by Glen David Gold published in 2001 by Hyperion.
A fictionalized representation of Farnsworth appears in Canadian writer Wayne Johnston's 1994 novel, "Human Amusements". The main character in the novel appears as the protagonist in a television show that features Farnsworth as the main character. In the show, an adolescent Farnsworth invents many different devices (television among them) while being challenged at every turn by a rival inventor.
Misquote.
Farnsworth is sometimes quoted as telling his son Kent, with regard to television: "There's nothing on it worthwhile, and we're not going to watch it in this household, and I don't want it in your intellectual diet." Yet, his family's website makes it clear that this is "Kent's" summation of his father's view, rather than a direct quote.
Farnsworth television factory history.
In May and June 2010 the Farnsworth TV production factory in Fort Wayne, Indiana, was razed, eliminating the "cave" where many of Farnsworth's inventions were created, and mass-produced for sale to the public. It was located in the International Harvester Industrial Park on Pontiac Street. The Philo T. Farnsworth Television Museum and his residence, while living in Fort Wayne, still stands on the corner of State and St. Joseph Boulevards, in Fort Wayne.
Further reading.
</dl>

</doc>
<doc id="42891" url="http://en.wikipedia.org/wiki?curid=42891" title="Adjustable spanner">
Adjustable spanner

An adjustable wrench (US) or adjustable spanner (UK) is a wrench with a "jaw" of adjustable width, allowing it to be used with different sizes of fastener head (nut, bolt, etc.) rather than just one fastener, as with a conventional fixed spanner. An adjustable spanner may also be called a Bahco (European usage, see below), crescent wrench (US, Canada and New Zealand usage - see Famous brands section), adjustable end wrench (US), wrench, shifter, shifting spanner (UK, Australia), shifting adjustable, fit-all or adjustable angle-head wrench.
Forms and names.
In many European as well as Middle Eastern countries (e.g. France, Germany, Portugal, Spain, Italy, Syria, Lebanon, Turkey, etc.) the adjustable wrench is called an "English key" as it was first invented in 1842 by the English engineer Richard Clyburn. Another English engineer, Edwin Beard Budding, is also credited with the invention. Improvements followed: on 22 September 1885 Enoch Harris received US patent 326868 for his spanner that permitted both the jaw width and the angle of the handles to be adjusted and locked. Other countries, like Denmark, Poland and Israel, refer to it as a "Swedish key" as its invention has been attributed to the Swedish inventor Johan Petter Johansson, who in 1891 received a patent for an improved design of the adjustable spanner that is still used today. Johansson's spanner was a further development of Clyburn's original "screw spanner". In some countries (e.g. Czech Republic, Egypt, Greece, Hungary, Serbia, Iran, Slovakia, Slovenia, Poland, Romania, Bulgaria) it is called "French key" (in Poland, "Swedish" or "French" key depending on type). In the USA, the tool is known as a Crescent wrench or an adjustable wrench.
There are many forms of adjustable spanners, from the taper locking spanners which needed a hammer to set the movable jaw to the size of the nut, to the modern screw adjusted spanner. Some adjustable spanners automatically adjust to the size of the nut. Simpler models use a serrated edge to lock the movable jaw to size, while more sophisticated versions are digital types that use sheets or feelers to set the size.
Monkey wrenches are another type of adjustable spanner with a long history; the origin of the name is unclear.
The type of straight adjustable spanner with jaws at right angles to the handle shown here as an "English Key" is mainly called a "King Dick" spanner in the United Kingdom because of a popular British brand of small, handy and reliable adjustable spanner used throughout the 1900s and used in great numbers during World War 2.
Proper use.
The movable jaw should be snugly adjusted to the nut or bolt head in order to prevent damage to the fastener's head, or "rounding". In addition, it is important when applying significant force to ensure that the fixed jaw "leads" the rotation (it follows its tip) and the movable jaw "trails" the rotation, leaving its tip behind, so to speak. The area of contact for the fixed jaw should be farther from the body of the tool. That means the movable jaw's area of contact is relatively close to the body of the tool, which means less bending stress. The fixed jaw can withstand bending stress far better than can the movable jaw, because the latter is supported only by the flat surfaces on either side of the guide slot, not the full thickness of the tool.
This type of spanner should not be used on a rounded off nut, as this can overload the movable jaw. Nor should such a wrench be used "end on" in cramped quarters (except perhaps when the nut is barely more than finger-tight), where a socket wrench is more appropriate.
Some cheaper brands' jaws move when twisting on tight nuts.
Famous brands.
In the United States and Canada, the adjustable spanner (adjustable wrench) is colloquially referred to as a "crescent wrench" due to the widespread Crescent brand of adjustable wrenches. The Crescent brand of hand tools is owned and marketed by Apex Tool Group, LLC. In some parts of Europe, adjustable spanners are often called a Bahco. This term refers to the company of the Swedish inventor Johan Petter Johansson, which was originally called B.A. (Bernt August) Hjort & Company.
The Swedes themselves call the key "skiftnyckel" which is translated into adjustable key (shifting key).

</doc>
<doc id="42895" url="http://en.wikipedia.org/wiki?curid=42895" title="Walvis Bay">
Walvis Bay

Walvis Bay (Afrikaans "Walvisbaai", German "Walfischbucht" or "Walfischbai", all meaning "Whale Bay") is a city in Namibia and the name of the bay on which it lies. The town has 85,000 inhabitants and has a total area of 29 km2 of land.
The bay is a safe haven for sea vessels because of its natural deepwater harbour, protected by the Pelican Point sand spit, being the only natural harbour of any size along the country's coast. Being rich in plankton and marine life, these waters also drew large numbers of Southern right whales, attracting whalers and fishing vessels. The Dutch referred to it as Walvisch Baye and the English as Whale Bay. In its eventual formal incorporation, it was named Walfish Bay, which was changed to Walvish Bay, and ultimately to Walvis Bay. It has also been referred to as Walwich Bay or Walwisch Bay.
A succession of colonists developed the location and resources of this strategic harbour settlement. The harbour's value in relation to the sea route around the Cape of Good Hope had caught the attention of world powers since it was discovered in 1485. This explains the complicated political status of Walvis Bay down the years.
The town is situated just north of the Tropic of Capricorn in the Kuiseb River delta and lies at the end of the TransNamib Railway to Windhoek, and on the B2 road.
Walvis Bay, with its large bay and sand dunes, is the center of tourism activity in Namibia. Attractions include the artificial Bird Island, centre of a guano collection industry, the Dune 7 sand dune, the salt works, the abundant birdlife, and a museum. Kuisebmund Stadium, home to two clubs in the Namibia Premier League, is also located in the city. The beach resort of Langstrand lies just a few kilometres north. The Walvis Bay Export Processing Zone is an important facet of the local economy.
History.
The Portuguese navigator Diogo Cão reached Cape Cross, north of the bay, in 1485. There followed Bartolomeu Dias, who anchored his flagship "São Cristóvão" in what is now Walvis Bay on 8 December 1487, on his expedition to discover a sea route to the East via the Cape of Good Hope. He named the bay "O Golfo de Santa Maria da Conceição." However, the Portuguese did not formally stake a claim to Walvis Bay.
The Herero called the place "Ezorongondo". 
Little commercial development occurred on the site until the late 19th century. During the scramble for Africa, the United Kingdom occupied Walvis Bay and a small area surrounding the territory, and permitted the Cape Colony to annexe it in 1878, both to forestall German ambitions in the region, and to ensure safe passage of British ships around the Cape. Walvis Bay was the only known natural harbour on the Namibian coast. The Cape government, correctly predicting a German invasion of the region and desiring protection for its Griqualand diamond fields, originally requested permission to incorporate the whole of South West Africa, but this was blocked by Britain. Consequently when the Germans later colonised the region, only Walvis Bay remained as an enclave of the Cape Colony, and out of German control. 
In 1910, Walvis Bay, as well as the Cape Colony, became part of the newly-formed Union of South Africa. Subsequently, a dispute arose with Germany over the enclave's boundaries. This was eventually settled in 1911 and Walvis Bay was allocated an area of 434 sqmi.
The enclave was overrun by the Germans during the South-West Africa Campaign early in World War I. But South African Forces eventually ousted the Germans in 1915 and Walvis Bay was quickly integrated into the new martial law regime established in South-West Africa. South Africa was later awarded control (a "C" class mandate) over South-West Africa by the League of Nations to administer South-West Africa as an integral part of South Africa. Civilian rule was restored in South-West Africa in 1921 and administration of Walvis Bay was transferred to South-West Africa by an act of the South African parliament in 1922.
In 1971, anticipating an imminent cession of its control over South-West Africa, South Africa transferred control of Walvis Bay back to its Cape Province, thus making it an exclave. In 1977, in an attempt to avoid losing control of Walvis Bay to a possibly hostile South-West Africa People's Organisation-led government, the South African government reimposed direct rule and reasserted its claim of sovereignty based on the original annexation. In 1978, the United Nations Security Council provided for bilateral negotiations between South Africa and a future Namibia to resolve the political status of Walvis Bay.
In 1990 South-West Africa gained independence as Namibia. Walvis Bay remained under South African sovereignty until midnight on 28 February 1994 when South Africa formally transferred sovereignty over Walvis Bay and the Penguin Islands to Namibia.
Education.
Walvis Bay has a number of public (government-run) and private schools, among them Duinesig Primary School, , International School of Walvis Bay, Kuisebmond Secondary School, Walvis Bay Private High School and others. A number of kindergartens cater for young children.
The Namibian Maritime and Fisheries Institute (NAMFI) is a tertiary education institution based in town. International University of Management (IUM) and Monitronics Success College both have branches in Walvis Bay.
Fishing.
In Walvis Bay there are different fishing companies like Hangana Seafood,Caroline Fishing, Benguella Fishing Company, Etale Fishing Company, Cadilu Fishing, Etosha Fisheries, Kuiseb Fishing Enterprises, Blue Ocean Products, Benguella Sea Products, Consortium Fisheries, Talanam Fish Processor. These companies catch different types of fish like snoek, horse mackerel, anchovy, steenbras, kabeljou, kingklip, hake, catfish, tuna, and sardines. Hangana Seafood are processors and exporters of fish and fish products.
As such, the fishing enterprise accounts for a major part of Walvis Bay's economy.
Entertainment and sport.
Walvis Bay contains open spaces, scenic beauty and unique marine and plant life. It is well suited for the outdoor lifestyle, boasting sports such as sandboarding, kiting, surfing, swimming, angling, sailing, golf and other indoor and outdoor sport codes. There is Walvis Bay Lagoon and Aquatic Activities, Kuiseb River Delta and the beach itself where people enjoy swimming and catching fish.
The 2 km sand spit allows the adjacent water to remain smooth in very strong winds, ideal for record attempting vessels like Vestas Sailrocket
It is home to Eleven Arrows F.C. and Blue Waters F.C., local football clubs that compete in the Namibia Premier League.
Climate.
Walvis Bay features the very rare mild variation of the arid climate ("BWn", according to the Köppen climate classification). Walvis Bay receives an average of less than 10 mm of precipitation per year, making it one of the driest cities on the planet. Despite the fact that it has an arid climate, Walvis Bay seldom gets very hot or very cold, an extremely unusual feature for a city featuring this climate; this is primarily due to cold offshore currents near Walvis Bay.
Twin towns – Sister cities.
Walvis Bay is twinned with:
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="42896" url="http://en.wikipedia.org/wiki?curid=42896" title="Viscometer">
Viscometer

A viscometer (also called viscosimeter) is an instrument used to measure the viscosity of a fluid. For liquids with viscosities which vary with flow conditions, an instrument called a rheometer is used. Viscometers only measure under one flow condition.
In general, either the fluid remains stationary and an object moves through it, or the object is stationary and the fluid moves past it. The drag caused by relative motion of the fluid and a surface is a measure of the viscosity. The flow conditions must have a sufficiently small value of Reynolds number for there to be laminar flow.
At 20.00 degrees Celsius the dynamic viscosity (kinematic viscosity x density) of water is 1.0038mPa·s and its kinematic viscosity (product of flow time x Factor) is 1.0022mm2/sec.;mm2/s. These values are used for calibrating certain types of viscometers.
Standard laboratory viscometers for liquids.
U-tube viscometers.
These devices are also known as glass capillary viscometers or Ostwald viscometers, named after Wilhelm Ostwald. Another version is the Ubbelohde viscometer, which consists of a U-shaped glass tube held vertically in a controlled temperature bath. In one arm of the U is a vertical section of precise narrow bore (the capillary). Above there is a bulb, with it is another bulb lower down on the other arm. In use, liquid is drawn into the upper bulb by suction, then allowed to flow down through the capillary into the lower bulb. Two marks (one above and one below the upper bulb) indicate a known volume. The time taken for the level of the liquid to pass between these marks is proportional to the kinematic viscosity. Most commercial units are provided with a conversion factor, or can be calibrated by a fluid of known properties.
The time required for the test liquid to flow through a capillary of a known diameter of a certain factor between two marked points is measured. By multiplying the time taken by the factor of the viscometer, the kinematic viscosity is obtained.
Such viscometers can be classified as direct flow or reverse flow. Reverse flow viscometers have the reservoir above the markings and direct flow are those with the reservoir below the markings. Such classifications exist so that the level can be determined even when opaque or staining liquids are measured, otherwise the liquid will cover the markings and make it impossible to gauge the time the level passes the mark. This also allows the viscometer to have more than 1 set of marks to allow for an immediate timing of the time it takes to reach the 3rd mark, therefore yielding 2 timings and allowing for subsequent calculation of Determinability to ensure accurate results. The use of two timings in one viscometer in a single run is only possible if the sample being measured has Newtonian properties. Otherwise the change in driving head which in turn changes the shear rate will produce a different viscosity for the two bulbs.
Falling sphere viscometers.
Stokes' law is the basis of the falling sphere viscometer, in which the fluid is stationary in a vertical glass tube. A sphere of known size and density is allowed to descend through the liquid. If correctly selected, it reaches terminal velocity, which can be measured by the time it takes to pass two marks on the tube. Electronic sensing can be used for opaque fluids. Knowing the terminal velocity, the size and density of the sphere, and the density of the liquid, Stokes' law can be used to calculate the viscosity of the fluid. A series of steel ball bearings of different diameter are normally used in the classic experiment to improve the accuracy of the calculation. The school experiment uses glycerine as the fluid, and the technique is used industrially to check the viscosity of fluids used in processes. It includes many different oils, and polymer liquids such as solutions.
In 1851, George Gabriel Stokes derived an expression for the frictional force (also called drag force) exerted on spherical objects with very small Reynolds numbers (e.g., very small particles) in a continuous viscous fluid by changing the small fluid-mass limit of the generally unsolvable Navier-Stokes equations:
where:
If the particles are falling in the viscous fluid by their own weight, then a terminal velocity, also known as the settling velocity, is reached when this frictional force combined with the buoyant force exactly balance the gravitational force. The resulting settling velocity (or terminal velocity) is given by:
where:
Note that Stokes flow is assumed, so the Reynolds number must be small.
A limiting factor on the validity of this result is the roughness of the sphere being used.
A modification of the straight falling sphere viscometer is a rolling ball viscometer which times a ball rolling down a slope whilst immersed in the test fluid. This can be further improved by using a patented V plate which increases the number of rotations to distance traveled, allowing smaller more portable devices. This type of device is also suitable for ship board use.
Falling Ball Viscometer.
In 1932 Fritz Höppler got a patent for the Falling Ball viscometer, named after him - the worldwide first viscometer to determine the dynamic viscosity. More other world-firsts viscometers which were developed by Fritz Höppler in Medingen (Germany) are the Ball Pressure types Consistometer and Rheoviscometer, see Kugeldruckviskosimeter = Ball Pressure Viscometer.
Falling Piston Viscometer.
Also known as the Norcross viscometer after its inventor, Austin Norcross. The principle of viscosity measurement in this rugged and sensitive industrial device is based on a piston and cylinder assembly. The piston is periodically raised by an air lifting mechanism, drawing the material being measured down through the clearance (gap) between the piston and the wall of the cylinder into the space which is formed below the piston as it is raised. The assembly is then typically held up for a few seconds, then allowed to fall by gravity, expelling the sample out through the same path that it entered, creating a shearing effect on the measured liquid, which makes this viscometer particularly sensitive and good for measuring certain thixotropic liquids. The time of fall is a measure of viscosity, with the clearance between the piston and inside of the cylinder forming the measuring orifice. The viscosity controller measures the time of fall (time-of-fall seconds being the measure of viscosity) and displays the resulting viscosity value. The controller can calibrate the time-of-fall value to cup seconds (known as efflux cup), Saybolt universal second (SUS) or centipoise.
Industrial use is popular due to simplicity, repeatability, low maintenance and longevity. This type of measurement is not affected by flow rate or external vibrations. The principle of operation can be adapted for many different conditions, making it ideal for process control environments.
Oscillating Piston Viscometer.
Sometimes referred to as electromagnetic viscometer or EMV viscometer, was invented at in 1986. The sensor (see figure below) comprises a measurement chamber and magnetically influenced piston. Measurements are taken whereby a sample is first introduced into the thermally controlled measurement chamber where the piston resides. Electronics drive the piston into oscillatory motion within the measurement chamber with a controlled magnetic field. A shear stress is imposed on the liquid (or gas) due to the piston travel and the viscosity is determined by measuring the travel time of the piston. The construction parameters for the annular spacing between the piston and measurement chamber, the strength of the electromagnetic field, and the travel distance of the piston are used to calculate the viscosity according to Newton’s Law of Viscosity.
The oscillating piston viscometer technology has been adapted for small sample viscosity and micro-sample viscosity testing in laboratory applications. It has also been adapted to measure high pressure viscosity and high temperature viscosity measurements in both laboratory and process environments. The viscosity sensors have been scaled for a wide range of industrial applications such as small size viscometers for use in compressors and engines, flow-through viscometers for dip coating processes, in-line viscometers for use in refineries, and hundreds of other applications. Improvements in sensitivity from modern electronics, is stimulating a growth in oscillating piston viscometer popularity with academic laboratories exploring gas viscosity.
Vibrational viscometers.
Vibrational viscometers date back to the 1950s Bendix instrument, which is of a class that operates by measuring the damping of an oscillating electromechanical resonator immersed in a fluid whose viscosity is to be determined. The resonator generally oscillates in torsion or transversely (as a cantilever beam or tuning fork). The higher the viscosity, the larger the damping imposed on the resonator. The resonator's damping may be measured by one of several methods:
The vibrational instrument also suffers from a lack of a defined shear field, which makes it unsuited to measuring the viscosity of a fluid whose flow behaviour is not known before hand.
Vibrating viscometers are rugged industrial systems used to measure viscosity in the process condition. The active part of the sensor is a vibrating rod. The vibration amplitude varies according to the viscosity of the fluid in which the rod is immersed. These viscosity meters are suitable for measuring clogging fluid and high-viscosity fluids, including those with fibers (up to 1,000 Pa·s). Currently, many industries around the world consider these viscometers to be the most efficient system with which to measure the viscosities of a wide range of fluids; by contrast, rotational viscometers require more maintenance, are unable to measure clogging fluid, and require frequent calibration after intensive use. Vibrating viscometers have no moving parts, no weak parts and the sensitive part is very small. Even very basic or acidic fluids can be measured by adding a protective coating such as enamel, or by changing the material of the sensor to a material such as 316L stainless steel.
Rotational viscometers.
Rotational viscometers use the idea that the torque required to turn an object in a fluid is a function of the viscosity of that fluid. They measure the torque required to rotate a disk or bob in a fluid at a known speed.
'Cup and bob' viscometers work by defining the exact volume of a sample which is to be sheared within a test cell; the torque required to achieve a certain rotational speed is measured and plotted. There are two classical geometries in "cup and bob" viscometers, known as either the "Couette" or "Searle" systems - distinguished by whether the cup or bob rotates. The rotating cup is preferred in some cases because it reduces the onset of Taylor vortices, but is more difficult to measure accurately in insument.
'Cone and Plate' viscometers use a cone of very shallow angle in bare contact with a flat plate. With this system the shear rate beneath the plate is constant to a modest degree of precision and deconvolution of a flow curve; a graph of shear stress (torque) against shear rate (angular velocity) yields the viscosity in a straightforward manner.
Electromagnetically Spinning Sphere Viscometer (EMS Viscometer).
The EMS Viscometer measures the viscosity of liquids through observation of the rotation of a sphere which is driven by electromagnetic interaction: Two magnets attached to a rotor create a rotating magnetic field. The sample (3) to be measured is in a small test tube (2). Inside the tube is an aluminium sphere (4). The tube is located in a temperature controlled chamber (1) and set such that the sphere is situated in the centre of the two magnets.
The rotating magnetic field induces eddy currents in the sphere. The resulting Lorentz interaction between the magnetic field and these eddy currents generate torque that rotates the sphere. The rotational speed of the sphere depends on the rotational velocity of the magnetic field, the magnitude of the magnetic field and the viscosity of the sample around the sphere. The motion of the sphere is monitored by a video camera (5) located below the cell. The torque applied to the sphere is proportional to the difference in the angular velocity of the magnetic field ΩB and the one of the sphere ΩS. There is thus a linear relationship between (ΩB−ΩS)/ΩS and the viscosity of the liquid.
This new measuring principle was developed by Sakai et al. at the University of Tokyo. The EMS viscometer distinguishes itself from other rotational viscometers by three main characteristics: 
Stabinger viscometer.
By modifying the classic Couette type rotational viscometer, it is possible to combine the accuracy of kinematic viscosity determination with a wide measuring range.
The outer cylinder of the Stabinger Viscometer is a tube that rotates at constant speed in a temperature-controlled copper housing. The hollow internal cylinder – shaped as a conical rotor – is specifically lighter than the filled samples and therefore floats freely within them, centered by centrifugal forces. In this way all bearing friction, an inevitable factor in most rotational devices, is fully avoided. The rotating fluid's shear forces drive the rotor, while a magnet inside the rotor forms an eddy current brake with the surrounding copper housing. An equilibrium rotor speed is established between driving and retarding forces, which is an unambiguous measure of the dynamic viscosity. The speed and torque measurement is implemented without direct contact by a Hall effect sensor counting the frequency of the rotating magnetic field. This allows for a highly precise torque resolution of 50 pN·m and a wide measuring range from 0.2 to 20,000 mPa•s with a single measuring system. A built-in density measurement based on the oscillating U-tube principle allows the determination of kinematic viscosity from the measured dynamic viscosity employing the relation
where:
Bubble viscometer.
Bubble viscometers are used to quickly determine kinematic viscosity of known liquids such as resins and varnishes. The time required for an air bubble to rise is directly proportional to the viscosity of the liquid, so the faster the bubble rises, the lower the viscosity. The Alphabetical Comparison Method uses 4 sets of lettered reference tubes, A5 through Z10, of known viscosity to cover a viscosity range from 0.005 to 1,000 stokes. The Direct Time Method uses a single 3-line times tube for determining the "bubble seconds", which may then be converted to stokes.
This method is considerably accurate, but the measurements can vary due to variances in buoyancy because of the changing in shape of the bubble in the tube However, this does not cause any sort of serious miscalculation.
Rectangular-Slit Viscometer.
The basic design of a rectangular-slit viscometer/rheometer, as commercially developed by of San Ramon, CA, consists of a rectangular, slit channel with uniform cross-sectional area. A test liquid is pumped at a constant flow rate through this channel. Multiple pressure sensors flush mounted at linear distances along the stream-wise direction measure pressure drop as depicted in "Figure 1".
Measuring principle: The slit viscometer/rheometer is based on the fundamental principle that a viscous liquid resists flow, exhibiting a decreasing pressure along the length of the slit. The pressure decrease or drop (Δ"P") is correlated with the shear stress at the wall boundary.  The apparent shear rate is directly related to the flow rate and the dimension of the slit. The apparent shear rate, the shear stress, and the apparent viscosity are calculated:
= Apparent Shear Rate (s−1)
σ = Shear Stress (Pa)
"η"a = Apparent Viscosity (Pa-s)
"Δ"P = Pressure difference between the leading pressure sensor and the last pressure sensor (Pa)
"Q" = Flow Rate (ml/s)
"w" = width of the flow channel (mm)
"h" = depth of the flow channel (mm)
"l" = the distance between the leading pressure sensor and the last pressure sensor (mm)
To determine the viscosity of a liquid, pump the liquid sample to flow through the slit channel at a constant flow rate and measure the pressure drop.  Following these equations, calculate the apparent viscosity for the apparent shear rate.  For a Newtonian liquid, the apparent viscosity is the same as the true viscosity and the single shear rate measurement is sufficient.  For non-Newtonian liquids, the apparent viscosity is not true viscosity.  In order to obtain true viscosity, measure the apparent viscosities at multiple apparent shear rates. Then calculate true viscosities, "η", at various shear rates using Weissenberg-Rabinowitsch-Mooney correction factor:
The calculated true viscosity will be the same as the cone and plate value at the same shear rate. 
A modified version of the rectangular-slit viscometer/rheometer can also be used to determine apparent extensional viscosity.
Miscellaneous viscometer types.
Other viscometer types use balls or other objects. Viscometers that can characterize non-Newtonian fluids are usually called "rheometers" or "plastometers".
In the I.C.I "Oscar" viscometer, a sealed can of fluid was oscillated torsionally, and by clever measurement techniques it was possible to measure both viscosity and elasticity in the sample.
The Marsh funnel viscometer measures viscosity from the time ("efflux time") it takes a known volume of liquid to flow from the base of a cone through a short tube. This is similar in principle to the flow cups (efflux cups) like the Ford, Zahn and Shell cups which use different shapes to the cone and various nozzle sizes. The measurements can be done according to ISO 2431, ASTM D1200 - 10 or DIN 53411.
The improves the accuracy of measurements for the lower viscosity range liquids utilizing the subtle changes in the flow field due to the flexibility of the moving or stationary blade (sometimes called wing or single side clamped cantilever).

</doc>
<doc id="42897" url="http://en.wikipedia.org/wiki?curid=42897" title="Alexander Dubček">
Alexander Dubček

Alexander Dubček (]; 27 November 1921 – 7 November 1992) was a Slovak politician and, briefly, leader of Czechoslovakia (1968–1969). He attempted to reform the communist regime during the Prague Spring but he was forced to resign following Warsaw Pact invasion of Czechoslovakia. Later, after the overthrow of the government in 1989, he was Chairman of the federal Czechoslovak parliament. He was the recipient of the Sakharov Prize for Freedom of Thought of the European Parliament, in 1989.
Early life.
Dubček was born in Uhrovec, Czechoslovakia, on 27 November 1921 and raised in the Kyrgyz SSR of the Soviet Union (now Kyrgyzstan) as a member of the Esperantist and Idist industrial cooperative Interhelpo. Alexander Dubček was conceived in Chicago, but born after the family relocated to Czechoslovakia. When Alexander Dubček was three, the family moved to the Soviet Union, in part to help build socialism and in part because jobs were scarce in Czechoslovakia. In 1938, the family returned to Czechoslovakia.
During the Second World War, Alexander Dubček joined the underground resistance against the wartime pro-German Slovak state headed by Jozef Tiso. In August 1944, Dubček fought in the Slovak National Uprising and was wounded. His brother, Július, was killed.
Political career.
During the war, Alexander Dubček joined the Communist Party of Slovakia (KSS), which had been created after the formation of the Slovak state and in 1948 was transformed into the Slovak branch of the Communist Party of Czechoslovakia (KSČ).
After the war, he steadily rose through the ranks in Communist Czechoslovakia. From 1951 to 1955 he was a member of the National Assembly, the parliament of Czechoslovakia. In 1953, he was sent to the Moscow Political College, where he graduated in 1958. In 1955 he joined the Central Committee of the Slovak branch and in 1962 became a member of the presidium. In 1958 he also joined the Central Committee of the Communist Party of Czechoslovakia, which he served as a secretary from 1960 to 1962 and as a member of the presidium after 1962. From 1960 to 1968 he once more was a member of the federal parliament.
In 1963, a power struggle in the leadership of the Slovak branch unseated Karol Bacílek and Pavol David, hard-line allies of Antonín Novotný, First Secretary of the KSČ and President of Czechoslovakia. In their place, a new generation of Slovak Communists took control of party and state organs in Slovakia, led by Alexander Dubček, who became First Secretary of the Slovak branch of the party.
Under Dubček's leadership, Slovakia began to evolve toward political liberalization. Because Novotný and his Stalinist predecessors had denigrated Slovak "bourgeois nationalists", most notably Gustáv Husák and Vladimír Clementis, in the 1950s, the Slovak branch worked to promote Slovak identity. This mainly took the form of celebrations and commemorations, such as the 150th birthdays of 19th century leaders of the Slovak National Revival Ľudovít Štúr and Jozef Miloslav Hurban, the centenary of the Matica slovenská in 1963, and the twentieth anniversary of the Slovak National Uprising. At the same time, the political and intellectual climate in Slovakia became freer than that in the Czech Lands. This was exemplified by the rising readership of "Kultúrny život", the weekly newspaper of the Union of Slovak Writers, which published frank discussions of liberalization, federalization and democratization, written by the most progressive or controversial writers – both Slovak and Czech. "Kultúrny život" consequently became the first Slovak publication to gain a wide following among Czechs.
Prague Spring.
 The Czechoslovak planned economy in the 1960s was in serious decline and the imposition of central control from Prague disappointed local Communists while the destalinization program caused further disquiet. In October 1967, a number of reformers, most notably Ota Šik and Alexander Dubček, took action: they challenged First Secretary Antonín Novotný at a Central Committee meeting. Novotný faced a mutiny in the Central Committee, so he secretly invited Leonid Brezhnev, the Soviet leader, to make a whirlwind visit to Prague in December 1967 in order to shore up his own position. When Brezhnev arrived in Prague and met with the Central Committee members, he was stunned to learn of the extent of the opposition to Novotný, leading Brezhnev to withhold support and paving the way for the Central Committee to force Novotný's resignation. Dubček became the new First Secretary of the Communist Party of Czechoslovakia on 5 January 1968.
The period following Novotný's downfall became known as the Prague Spring. During this time, Dubček and other reformers sought to liberalize the Communist government--creating "socialism with a human face". Though this loosened the party's influence on the country, Dubček remained a devoted Communist and intended to preserve the party's rule. However, during the Prague Spring, he and other reform-minded Communists sought to win popular support for the Communist government by eliminating its worst, most repressive features, allowing greater freedom of expression and tolerating political and social organizations not under Communist control. "Dubček! Svoboda!" became the popular refrain of student demonstrations during this period. Yet Dubček found himself in an increasingly untenable position. The program of reform gained momentum, leading to pressures for further liberalization and democratization. At the same time, hard-line Communists in Czechoslovakia and the leaders of other Warsaw Pact countries pressured Dubček to rein in the Prague Spring. Though Dubček wanted to oversee the reform movement, he refused to resort to any draconian measures to do so.
The Soviet leadership tried to slow down or stop the changes in Czechoslovakia through a series of negotiations. The Soviet Union agreed to bilateral talks with Czechoslovakia in July at Čierna nad Tisou, near the Slovak-Soviet border. At the meeting, Dubček tried to reassure the Soviets and the Warsaw Pact leaders that he was still friendly to Moscow, arguing that the reforms were an internal matter. He thought he had learned an important lesson from the failing of the Hungarian Revolution of 1956, in which the leaders had gone as far as withdrawing from the Warsaw Pact. Dubček believed that the Kremlin would allow him a free hand in pursuing domestic reform as long as Czechoslovakia remained a faithful member of the Soviet bloc. Despite Dubček's continuing efforts to stress these commitments, Brezhnev and other Warsaw Pact leaders remained wary.
Downfall.
On the night of 20–21 August 1968, Warsaw Pact forces entered Czechoslovakia. The occupying armies quickly seized control of Prague and the Central Committee's building, taking Dubček and other reformers into Soviet custody. But, before they were arrested, Dubček urged the people not to resist militarily. Later in the day, Dubček and the others were taken to Moscow on a Soviet military transport aircraft (reportedly one of the aircraft used in the Soviet invasion).
Despite the inspired non-violent resistance of the Czech and Slovak population, which delayed full loss of control to the Warsaw Pact forces for a full eight months (in contrast to the Soviet military's estimate of four days) and became a prime example of Civilian-based defense, the reformers ultimately were forced to accede to Soviet demands, signing the Moscow protocols. (Only František Kriegel refused to sign.)
Dubček and most of the reformers were returned to Prague on 27 August and Dubček retained his post as the party's first secretary for a while. Indeed, the achievements of the Prague Spring were not reversed overnight, but over a period of several months.
In January 1969, Dubček was hospitalized in Bratislava complaining of a cold and had to cancel a speech. Rumours sprang up that his illness was radiation sickness and that it was caused by radioactive strontium being placed in his soup during his stay in Moscow in an attempt to kill him. However, a U.S. intelligence report discounted this for lack of evidence.
Dubček was forced to resign as First Secretary in April 1969, following the Czechoslovak Hockey Riots. He was re-elected to the Federal Assembly (as the federal parliament was now called) and became its Speaker. He was later sent as ambassador to Turkey (1969–70), allegedly in the hope that he would defect to the West, which however did not occur. In 1970, he was expelled from the Communist party and lost his seats in the Slovak parliament (which he had held continuously since 1964) and the Federal Assembly.
Private citizen.
After his expulsion from the party, Dubček worked in the Forestry Service in Slovakia. He remained a popular figure among the Slovaks and Czechs he encountered on the job, using this reverence to procure scarce and hard-to-find materials for his workplace. Dubček and his wife, Anna, continued to live in a comfortable villa in a nice neighbourhood in Bratislava. In 1988, Dubček was allowed to travel to Italy to accept an honorary doctorate from Bologna University, and, while there, he gave an interview with Italian newspaper "L'Unità", his first public remarks to the press since 1970. Dubček's appearance and interview helped to return him to international prominence.
In 1989, he was awarded the annual Sakharov Prize in its second year of existence.
Velvet Revolution.
During the Velvet Revolution of 1989, he supported the Public against Violence (VPN) and the Civic Forum. On the night of 24 November, Dubček appeared with Václav Havel on a balcony overlooking Wenceslas Square, where he was greeted with uproarious applause from the throngs of protesters below and embraced as a symbol of democratic freedom. Several onlookers even chanted, "Dubček na hrad!" ("Dubček to the Castle"--i. e., Dubček for President). He disappointed the crowd somewhat by calling the revolution a chance to continue the work he had started 20 years earlier, and prune out what was wrong with contemporary communist governments; by that time the demonstrators in Prague did not support the Czechoslovakian communist leadership or the planned economy. Later that night, Dubček was on stage with Havel at the Laterna Magika theatre, the headquarters of Civic Forum, when the entire leadership of the Communist Party resigned, in effect ending Communist rule in Czechoslovakia.
Dubček was elected Chairman of the Federal Assembly (the Czechoslovak Parliament) on 28 December 1989, and re-elected in 1990 and 1992.
At the time of the overthrow of Communist party rule, Dubček described the Velvet Revolution as a victory for his "humanistic socialist outlook". In 1990, he received the International Humanist Award from the International Humanist and Ethical Union. He also gave the commencement address to the graduates of the Class of 1990 at The American University in Washington, D.C.; it was his first trip to the United States.
In 1992, he became leader of the Social Democratic Party of Slovakia and represented that party in the Federal Assembly. At that time, Dubček passively supported the union between Czechs and Slovaks in a single Czecho-Slovak federation against the (ultimately successful) push towards an independent Slovak state.
Death.
Dubček died on 7 November 1992, as a result of injuries sustained in a car crash that took place on 1 September on the Czech D1 highway, near Humpolec. He was buried in Slávičie údolie cemetery in Bratislava, Slovakia.

</doc>
<doc id="42898" url="http://en.wikipedia.org/wiki?curid=42898" title="Anthrax">
Anthrax

Anthrax is an acute disease caused by the bacterium "Bacillus anthracis". Most forms of the disease are lethal, and it affects mostly animals. It is not contagious but can be transmitted through contact or consumption of infected meat. Effective vaccines against anthrax are available, and some forms of the disease respond well to antibiotic treatment.
Like many other members of the genus "Bacillus", "B. anthracis" can form dormant endospores (often referred to as "spores" for short, but not to be confused with fungal spores) that are able to survive in harsh conditions for decades or even centuries. Such spores can be found on all continents, even Antarctica. When spores are inhaled, ingested, or come into contact with a skin lesion on a host, they may become reactivated and multiply rapidly.
Anthrax commonly infects wild and domesticated herbivorous mammals that ingest or inhale the spores while grazing. Ingestion is thought to be the most common route by which herbivores contract anthrax. Carnivores living in the same environment may become infected by consuming infected animals. Diseased animals can spread anthrax to humans, either by direct contact (e.g., inoculation of infected blood to broken skin) or by consumption of a diseased animal's flesh.
Anthrax does not spread directly from one infected animal or person to another, it is spread by spores. These spores can be transported by clothing or shoes. The body of an animal that had active anthrax at the time of death can also be a source of anthrax spores. Owing to the hardiness of anthrax spores, and their ease of production "in vitro", they are extraordinarily well suited to use (in powdered and aerosol form) as biological weapons. Such weaponization has been accomplished in the past by at least five state bioweapons programs — those of the United Kingdom, Japan, the United States, Russia, and Iraq — and has been attempted by several others.
Until the 20th century, anthrax infections killed hundreds of thousands of animals and people worldwide each year. French scientist Louis Pasteur developed the first effective vaccine for anthrax in 1881. As a result of over a century of animal vaccination programs, sterilization of raw animal waste materials, and anthrax eradication programs in United States, Canada, Russia, Eastern Europe, Oceania, and parts of Africa and Asia, anthrax infection is now relatively rare in domestic animals. Anthrax is especially rare in dogs and cats, as is evidenced by a single reported case in the United States in 2001.
Anthrax outbreaks occur in some wild animal populations with some regularity.
The disease is more common in countries without widespread veterinary or human public health programs. In the 21st century, anthrax is still a problem in less developed countries. An outbreak of anthrax in humans who had eaten meat from a dead carabao was reported in Cagayan Province in the Philippines in early 2010, with over 400 cases of illness and at least two fatalities.
"B. anthracis" bacterial spores are soil-borne. Because of their long lifespan, spores are present globally and remain at the burial sites of animals killed by anthrax for many decades. Disturbed grave sites of infected animals have caused reinfection over 70 years after the animal's interment.
Signs and symptoms.
Pulmonary.
Respiratory infection in humans is relatively rare and initially presents with cold or flu-like symptoms for several days, followed by pneumonia and severe (and often fatal) respiratory collapse. Historical mortality rates were over 85%, but, when treated early (seen in the 2001 anthrax attacks), observed case fatality rate dropped to 45%. Distinguishing pulmonary anthrax from more common causes of respiratory illness is essential to avoiding delays in diagnosis and thereby improving outcomes. An algorithm for this purpose has been developed.
A lethal infection is reported to result from inhalation of about 10,000–20,000 spores, though this dose varies among host species. As with all diseases, a wide variation in susceptibility is presumed, with evidence indicating some people may die from much lower exposures; little documented evidence is available to verify the exact or average number of spores needed for infection. Inhalational anthrax is also known as woolsorters' or ragpickers' disease. These professions were more susceptible to the disease due to their exposure to infected animal products. Other practices associated with exposure include the slicing up of animal horns for the manufacture of buttons, the handling of hair bristles used for the manufacturing of brushes, and the handling of animal skins. Whether these animal skins came from animals that died of the disease or from animals that had simply lain on ground with spores on it is unknown. 
Gastrointestinal.
Gastrointestinal (GI) infection in humans is most often caused by consuming anthrax-infected meat and is characterized by serious GI difficulty, vomiting of blood, severe diarrhea, acute inflammation of the intestinal tract, and loss of appetite. Lesions have been found in the intestines and in the mouth and throat. After the bacterium invades the bowel system, it spreads through the bloodstream throughout the body, while also continuing to make toxins. GI infections can be treated, but usually result in fatality rates of 25% to 60%, depending upon how soon treatment commences. This form of anthrax is the rarest form. In the United States, only two official cases have occurred, the first reported in 1942 by the CDC and the second reported in 2010 that was treated at the Massachusetts General Hospital. It is the only known case of survival from GI anthrax in the United States. An outbreak of anthrax among people who had eaten meat from a dead carabao was reported in Cagayan Province in the Philippines in early 2010, with over 400 cases of illness and at least two fatalities.
Cutaneous.
Cutaneous anthrax, also known as Hide porter's disease, is the cutaneous (on the skin) manifestation of anthrax infection in humans. It presents as a boil-like skin lesion that eventually forms an ulcer with a black center (eschar). The black eschar often shows up as a large, painless necrotic ulcer (beginning as an irritating and itchy skin lesion or blister that is dark and usually concentrated as a black dot, somewhat resembling bread mold) at the site of infection. In general, cutaneous infections form within the site of spore penetration between two and five days after exposure. Unlike bruises or most other lesions, cutaneous anthrax infections normally do not cause pain.
Cutaneous anthrax is typically caused when "B. anthracis" spores enter through cuts on the skin. This form is found most commonly when humans handle infected animals and/or animal products.
Cutaneous anthrax is rarely fatal if treated, because the infection area is limited to the skin, preventing the lethal factor, edema factor, and protective antigen from entering and destroying a vital organ. Without treatment, about 20% of cutaneous skin infection cases progress to toxemia and death.
Cause.
Bacteria.
"Bacillus anthracis" is a rod-shaped, Gram-positive, aerobic bacterium about 1 by 9 μm in size. It was shown to cause disease by Robert Koch in 1876 when he took a blood sample from an infected cow, isolated the bacteria and put them into a mouse. The bacterium normally rests in endospore form in the soil, and can survive for decades in this state. Herbivores are often infected whilst grazing, especially when eating rough, irritant, or spiky vegetation: the vegetation has been hypothesized to cause wounds within the gastrointestinal tract permitting entry of the bacterial endospores into the tissues, though this has not been proven. Once ingested or placed in an open wound, the bacterium begins multiplying inside the animal or human and typically kills the host within a few days or weeks. The endospores germinate at the site of entry into the tissues and then spread by the circulation to the lymphatics, where the bacteria multiply.
The production of two powerful exotoxins and lethal toxin by the bacteria causes death. Veterinarians can often tell a possible anthrax-induced death by its sudden occurrence, and by the dark, nonclotting blood that oozes from the body orifices. Most anthrax bacteria inside the body after death are outcompeted and destroyed by anaerobic bacteria within minutes to hours "post mortem". However, anthrax vegetative bacteria that escape the body via oozing blood or through the opening of the carcass may form hardy spores. One spore forms per one vegetative bacterium. The triggers for spore formation are not yet known, though oxygen tension and lack of nutrients may play roles. Once formed, these spores are very hard to eradicate.
The infection of herbivores (and occasionally humans) by the inhalational route normally proceeds as follows: Once the spores are inhaled, they are transported through the air passages into the tiny air sacs (alveoli) in the lungs. The spores are then picked up by scavenger cells (macrophages) in the lungs and are transported through small vessels (lymphatics) to the lymph nodes in the central chest cavity (mediastinum). Damage caused by the anthrax spores and bacilli to the central chest cavity can cause chest pain and difficulty in breathing. Once in the lymph nodes, the spores germinate into active bacilli that multiply and eventually burst the macrophages, releasing many more bacilli into the bloodstream to be transferred to the entire body. Once in the blood stream, these bacilli release three proteins named lethal factor, edema factor, and protective antigen. The three are not toxic by themselves, but the combination is incredibly lethal to humans. Protective antigen combines with these other two factors to form lethal toxin and edema toxin, respectively. These toxins are the primary agents of tissue destruction, bleeding, and death of the host. If antibiotics are administered too late, even if the antibiotics eradicate the bacteria, some hosts will still die of toxemia because the toxins produced by the bacilli remain in their system at lethal dose levels.
The lethality of the anthrax disease is due to the bacterium's two principal virulence factors: the poly-D-glutamic acid capsule, which protects the bacterium from phagocytosis by host neutrophils, and the tripartite protein toxin, called anthrax toxin. Anthrax toxin is a mixture of three protein components: protective antigen (PA), edema factor (EF), and lethal factor (LF). PA plus LF produces lethal toxin, and PA plus EF produces edema toxin. These toxins cause death and tissue swelling (edema), respectively.
To enter the cells, the edema and lethal factors use another protein produced by "B. anthracis" called protective antigen, which binds to two surface receptors on the host cell. A cell protease then cleaves PA into two fragments: PA20 and PA63. PA20 dissociates into the extracellular medium, playing no further role in the toxic cycle. PA63 then oligomerizes with six other PA63 fragments forming a heptameric ring-shaped structure named a prepore. Once in this shape, the complex can competitively bind up to three EFs or LFs, forming a resistant complex. Receptor-mediated endocytosis occurs next, providing the newly formed toxic complex access to the interior of the host cell. The acidified environment within the endosome triggers the heptamer to release the LF and/or EF into the cytosol. It is unknown how exactly the complex results in the death of the cell.
Edema factor is a calmodulin-dependent adenylate cyclase. Adenylate cyclase catalyzes the conversion of ATP into cyclic AMP (cAMP) and pyrophosphate. The complexation of adenylate cyclase with calmodulin removes calmodulin from stimulating calcium-triggered signaling, thus inhibiting the immune response. To be specific, LF inactivates neutrophils (a type of phagocytic cell) by the process just described so they cannot phagocytose bacteria. Throughout history, lethal factor was presumed to caused macrophages to make TNF-alpha and interleukin 1, beta (IL1B). TNF-alpha is a cytokine whose primary role is to regulate immune cells, as well as to induce inflammation and apoptosis or programmed cell death. Interleukin 1, beta is another cytokine that also regulates inflammation and apoptosis. The overproduction of TNF-alpha and IL1B ultimately leads to septic shock and death. However, recent evidence indicates anthrax also targets endothelial cells that line serous cavities such as the pericardial cavity, pleural cavity, and the peritoneal cavity, lymph vessels, and blood vessels, causing vascular leakage of fluid and cells, and ultimately hypovolemic shock and septic shock.
Exposure.
Occupational exposure to infected animals or their products (such as skin, wool, and meat) is the usual pathway of exposure for humans. Workers who are exposed to dead animals and animal products are at the highest risk, especially in countries where anthrax is more common. Anthrax in livestock grazing on open range where they mix with wild animals still occasionally occurs in the United States and elsewhere. Many workers who deal with wool and animal hides are routinely exposed to low levels of anthrax spores, but most exposure levels are not sufficient to develop anthrax infections. The body's natural defenses presumably can destroy low levels of exposure. These people usually contract cutaneous anthrax if they catch anything. Throughout history, the most dangerous form of inhalational anthrax was called woolsorters' disease because it was an occupational hazard for people who sorted wool. Today, this form of infection is extremely rare, as almost no infected animals remain.
The last fatal case of natural inhalational anthrax in the United States occurred in California in 1976, when a home weaver died after working with infected wool imported from Pakistan. To minimize the chance of spreading the disease, the deceased was transported to UCLA in a sealed plastic body bag within a sealed metal container for autopsy.
In November 2008, a drum maker in the United Kingdom who worked with untreated animal skins died from anthrax. Gastrointestinal anthrax is exceedingly rare in the United States, with only one case on record, reported in 1942, according to the Centers for Disease Control and Prevention. In December 2009, an outbreak of anthrax occurred amongst heroin addicts in Glasgow, Scotland, resulting in 14 deaths. The source of the anthrax is believed to be dilution of the heroin with bone meal in Afghanistan.
Also during December 2009, the New Hampshire Department of Health and Human Services confirmed a case of gastrointestinal anthrax in an adult female. The CDC investigated the source and the possibility that it was contracted from an African drum recently used by the woman taking part in a drumming circle. The woman apparently inhaled anthrax [in spore form] from the hide of the drum. She became critically ill, but with gastrointestinal anthrax rather than inhaled anthrax, which made her unique in American medical history. The building where the infection took place was cleaned and reopened to the public and the woman recovered. Jodie Dionne-Odom, New Hampshire state epidemiologist, stated, "It is a mystery. We really don't know why it happened."
Mode of infection.
Anthrax can enter the human body through the intestines (ingestion), lungs (inhalation), or skin (cutaneous) and causes distinct clinical symptoms based on its site of entry. In general, an infected human will be quarantined. However, anthrax does not usually spread from an infected human to a noninfected human. But, if the disease is fatal to the person's body, its mass of anthrax bacilli becomes a potential source of infection to others and special precautions should be used to prevent further contamination. Inhalational anthrax, if left untreated until obvious symptoms occur, may be fatal.
Anthrax can be contracted in laboratory accidents or by handling infected animals or their wool or hides. It has also been used in biological warfare agents and by terrorists to intentionally infect as exemplified by the 2001 anthrax attacks.
Diagnosis.
Various techniques are used for the direct identification of "B. anthracis" in clinical material. Firstly, specimens may be Gram stained. "Bacillus" spp. are quite large in size (3 to 4 μm long), they grow in long chains, and they stain Gram-positive. To confirm the organism is "B. anthracis", rapid diagnostic techniques such as polymerase chain reaction-based assays and immunofluorescence microscopy may be used.
All "Bacillus" species grow well on 5% sheep blood agar and other routine culture media. Polymyxin-lysozyme-EDTA-thallous acetate can be used to isolate "B. anthracis" from contaminated specimens, and bicarbonate agar is used as an identification method to induce capsule formation. "Bacillus" spp. usually grow within 24 hours of incubation at 35 °C, in ambient air (room temperature) or in 5% CO2. If bicarbonate agar is used for identification, then the medium must be incubated in 5% CO2. "B. anthracis" colonies are medium-large, gray, flat, and irregular with swirling projections, often referred to as having a "medusa head" appearance, and are not hemolytic on 5% sheep blood agar. The bacteria are not motile, susceptible to penicillin, and produce a wide zone of lecithinase on egg yolk agar. Confirmatory testing to identify "B. anthracis" includes gamma bacteriophage testing, indirect hemagglutination, and enzyme linked immunosorbent assay to detect antibodies. The best confirmatory precipitation test for anthrax is the Ascoli test.
Prevention.
Vaccines.
Vaccines against anthrax for use in livestock and humans have had a prominent place in the history of medicine, from Pasteur's pioneering 19th-century work with cattle (the second effective vaccine ever) to the controversial 20th century use of a modern product (BioThrax) to protect American troops against the use of anthrax in biological warfare. Human anthrax vaccines were developed by the Soviet Union in the late 1930s and in the US and UK in the 1950s. The current FDA-approved US vaccine was formulated in the 1960s.
Currently administered human anthrax vaccines include acellular (United States) and live spore (Russia) varieties. All currently used anthrax vaccines show considerable local and general reactogenicity (erythema, induration, soreness, fever) and serious adverse reactions occur in about 1% of recipients. The American product, BioThrax, is licensed by the FDA and was formerly administered in a six-dose primary series at 0, 2, 4 weeks and 6, 12, 18 months, with annual boosters to maintain immunity. In 2008, the FDA approved omitting the week-2 dose, resulting in the currently recommended five-dose series. New second-generation vaccines currently being researched include recombinant live vaccines and recombinant subunit vaccines.
Prophylaxis.
If a person is suspected as having died from anthrax, every precaution should be taken to avoid skin contact with the potentially contaminated body and fluids exuded through natural body openings. The body should be put in strict quarantine. A blood sample should then be collected and sealed in a container and analyzed in an approved laboratory to ascertain if anthrax is the cause of death. Then, the body should be incinerated. Microscopic visualization of the encapsulated bacilli, usually in very large numbers, in a blood smear stained with polychrome methylene blue (McFadyean stain) is fully diagnostic, though culture of the organism is still the gold standard for diagnosis. Full isolation of the body is important to prevent possible contamination of others. Protective, impermeable clothing and equipment such as rubber gloves, rubber apron, and rubber boots with no perforations should be used when handling the body. No skin, especially if it has any wounds or scratches, should be exposed. Disposable personal protective equipment is preferable, but if not available, decontamination can be achieved by autoclaving. Disposable personal protective equipment and filters should be autoclaved, and/or burned and buried. "B. anthracis" bacillii range from 0.5–5.0 μm in size. Anyone working with anthrax in a suspected or confirmed victim should wear respiratory equipment capable of filtering this size of particle or smaller. The US National Institute for Occupational Safety and Health – and Mine Safety and Health Administration-approved high-efficiency respirator, such as a half-face disposable respirator with a high-efficiency particulate air filter, is recommended. All possibly contaminated bedding or clothing should be isolated in double plastic bags and treated as possible biohazard waste. The victim should be sealed in an airtight body bag. Dead victims who are opened and not burned provide an ideal source of anthrax spores. Cremating victims is the preferred way of handling body disposal. No embalming or autopsy should be attempted without a fully equipped biohazard laboratory and trained, knowledgeable personnel.
Delays of only a few days may make the disease untreatable, so treatment should be started even without symptoms if possible contamination or exposure is suspected. Animals with anthrax often just die without any apparent symptoms. Initial symptoms may resemble a common cold—sore throat, mild fever, muscle aches, and malaise. After a few days, the symptoms may progress to severe breathing problems and shock, and ultimately death. Death can occur from about two days to a month after exposure, with deaths apparently peaking at about eight days after exposure. Antibiotic-resistant strains of anthrax are known.
Early detection of sources of anthrax infection can allow preventive measures to be taken. In response to the anthrax attacks of October 2001, the United States Postal Service (USPS) installed biodetection systems (BDSs) in their large-scale mail cancellation facilities. BDS response plans were formulated by the USPS in conjunction with local responders including fire, police, hospitals and public health. Employees of these facilities have been educated about anthrax, response actions, and prophylactic medication. Because of the time delay inherent in getting final verification that anthrax has been used, prophylactic antibiotic treatment of possibly exposed personnel must be started as soon as possible.
Treatment.
Anthrax cannot be spread directly from person to person, but a person's clothing and body may be contaminated with anthrax spores. Effective decontamination of people can be accomplished by a thorough wash-down with antimicrobial soap and water. Waste water should be treated with bleach or other antimicrobial agent. Effective decontamination of articles can be accomplished by boiling them in water for 30 minutes or longer. Chlorine bleach is ineffective in destroying spores and vegetative cells on surfaces, though formaldehyde is effective. Burning clothing is very effective in destroying spores. After decontamination, there is no need to immunize, treat, or isolate contacts of persons ill with anthrax unless they were also exposed to the same source of infection.
Antibiotics.
Early antibiotic treatment of anthrax is essential; delay significantly lessens chances for survival.
Treatment for anthrax infection and other bacterial infections includes large doses of intravenous and oral antibiotics, such as fluoroquinolones (ciprofloxacin), doxycycline, erythromycin, vancomycin, or penicillin. FDA-approved agents include ciprofloxacin, doxycycline, and penicillin.
In possible cases of pulmonary anthrax, early antibiotic prophylaxis treatment is crucial to prevent possible death.
In May 2009, Human Genome Sciences submitted a Biologic License Application (BLA, permission to market) for its new drug, raxibacumab (brand name ABthrax) intended for emergency treatment of inhaled anthrax. If death occurs from anthrax, the body should be isolated to prevent possible spread of anthrax germs. Burial does not kill anthrax spores.
In recent years, many attempts have been made to develop new drugs against anthrax, but existing drugs are effective if treatment is started soon enough.
Monoclonal antibodies.
On 14 December 2012, the US Food and Drug Administration approved raxibacumab injection to treat inhalational anthrax. Raxibacumab is a monoclonal antibody that neutralizes toxins produced by "B. anthracis" that can cause massive and irreversible tissue injury and death. A monoclonal antibody is a protein that closely resembles a human antibody, and identifies and neutralizes foreign material such as bacteria and viruses.
History.
Etymology.
The name comes from "anthrax" [άνθραξ], the Greek word for coal, because of the black skin lesions developed by victims with a cutaneous anthrax infection. It was discovered and analyzed by Robert Koch (1876).
Alternative names include Siberian plague, charbon, splenic fever, malignant edema, and woolsorter's disease.
Discovery.
Robert Koch, a German physician and scientist, first identified the bacterium that caused the anthrax disease in 1875 in Wolsztyn. His pioneering work in the late 19th century was one of the first demonstrations that diseases could be caused by microbes. In a groundbreaking series of experiments, he uncovered the lifecycle and means of transmission of anthrax. His experiments not only helped create an understanding of anthrax, but also helped elucidate the role of microbes in causing illness at a time when debates still took place over spontaneous generation versus cell theory. Koch went on to study the mechanisms of other diseases and won the 1905 Nobel Prize in Physiology or Medicine for his discovery of the bacterium causing tuberculosis.
First vaccination.
In May 1881, Louis Pasteur performed a public experiment to demonstrate his concept of vaccination. He prepared two groups of 25 sheep, one goat, and several cows. The animals of one group were injected with an anthrax vaccine prepared by Pasteur twice, at an interval of 15 days; the control group was left unvaccinated. Thirty days after the first injection, both groups were injected with a culture of live anthrax bacteria. All the animals in the unvaccinated group died, while all of the animals in the vaccinated group survived.
The human vaccine for anthrax became available in 1954. This was a cell-free vaccine instead of the live-cell Pasteur-style vaccine used for veterinary purposes. An improved cell-free vaccine became available in 1970.
Society and culture.
The virulent Ames strain, which was used in the 2001 anthrax attacks in the United States, has received the most news coverage of any anthrax outbreak. The Ames strain contains two virulence plasmids, which separately encode for a three-protein toxin, called anthrax toxin, and a polyglutamic acid capsule. Nonetheless, the Vollum strain, developed but never used as a biological weapon during the Second World War, is much more dangerous. The Vollum (also incorrectly referred to as Vellum) strain was isolated in 1935 from a cow in Oxfordshire. This same strain was used during the Gruinard bioweapons trials. A variation of Vollum known as "Vollum 1B" was used during the 1960s in the US and UK bioweapon programs. Vollum 1B is widely believed to have been isolated from William A. Boyles, a 46-year-old scientist at the U.S. Army Biological Warfare Laboratories at Camp (later Fort) Detrick, Maryland, (precursor to USAMRIID), who died in 1951 after being accidentally infected with the Vollum strain. The Sterne strain, named after the Trieste-born immunologist Max Sterne, is an attenuated strain used as a vaccine, which contains only the anthrax toxin virulence plasmid and not the polyglutamic acid capsule expressing plasmid.
Site cleanup and decontamination.
Anthrax spores can survive for very long periods of time in the environment after release. Chemical methods for cleaning anthrax-contaminated sites or materials may use oxidizing agents such as peroxides, ethylene oxide, Sandia Foam, chlorine dioxide (used in the Hart Senate Office Building), peracetic acid, ozone gas, hypochlorous acid, sodium persulfate, and liquid bleach products containing sodium hypochlorite. Nonoxidizing agents shown to be effective for anthrax decontamination include methyl bromide, formaldehyde, and metam sodium. These agents destroy bacterial spores. All of the aforementioned anthrax decontamination technologies have been demonstrated to be effective in laboratory tests conducted by the US EPA or others.
A bleach solution for treating hard surfaces has been approved by the EPA.
Chlorine dioxide has emerged as the preferred biocide against anthrax-contaminated sites, having been employed in the treatment of numerous government buildings over the past decade. Its chief drawback is the need for "in situ" processes to have the reactant on demand.
To speed the process, trace amounts of a nontoxic catalyst composed of iron and tetroamido macrocyclic ligands are combined with sodium carbonate and bicarbonate and converted into a spray. The spray formula is applied to an infested area and is followed by another spray containing tert-butyl hydroperoxide.
Using the catalyst method, a complete destruction of all anthrax spores can be achieved in under 30 minutes. A standard catalyst-free spray destroys fewer than half the spores in the same amount of time. They can be heated and exposed to the harshest chemicals, but they do not easily die.
Cleanups at a Senate office building, several contaminated postal facilities, and other US government and private office buildings showed decontamination to be possible, but it is time-consuming and costly. Clearing the Senate office building of anthrax spores cost $27 million, according to the Government Accountability Office. Cleaning the Brentwood postal facility outside Washington cost $130 million and took 26 months. Since then, newer and less costly methods have been developed.
Cleanup of anthrax-contaminated areas on ranches and in the wild is much more problematic. Carcasses may be burned, though it often takes up to three days to burn a large carcass and this is not feasible in areas with little wood. Carcasses may also be buried, though the burying of large animals deeply enough to prevent resurfacing of spores requires much manpower and expensive tools. Carcasses have been soaked in formaldehyde to kill spores, though this has environmental contamination issues. Block burning of vegetation in large areas enclosing an anthrax outbreak has been tried; this, while environmentally destructive, causes healthy animals to move away from an area with carcasses in search of fresh grass. Some wildlife workers have experimented with covering fresh anthrax carcasses with shadecloth and heavy objects. This prevents some scavengers from opening the carcasses, thus allowing the putrefactive bacteria within the carcass to kill the vegetative "B. anthracis" cells and preventing sporulation. This method also has drawbacks, as scavengers such as hyenas are capable of infiltrating almost any exclosure.
The experimental site at Gruinard Island is said to have been decontaminated with a mixture of formaldehyde and seawater by the Ministry of Defence. It is not clear whether similar treatments had been applied to US test sites.
Biological warfare.
Anthrax spores can and have been used as a biological warfare weapon. Its first modern incidence occurred when Scandinavian rebels, supplied by the German General Staff, used anthrax with unknown results against the Imperial Russian Army in Finland in 1916. Anthrax was first tested as a biological warfare agent by Unit 731 of the Japanese Kwantung Army in Manchuria during the 1930s; some of this testing involved intentional infection of prisoners of war, thousands of whom died. Anthrax, designated at the time as Agent N, was also investigated by the Allies in the 1940s.
A long history of practical bioweapons research exists in this area. For example, in 1942, British bioweapons trials severely contaminated Gruinard Island in Scotland with anthrax spores of the Vollum-14578 strain, making it a no-go area until it was decontaminated in 1990. The Gruinard trials involved testing the effectiveness of a submunition of an "N-bomb" — a biological weapon containing dried anthrax spores. Additionally, five million "cattle cakes" (animal feed pellets impregnated with anthrax spores) were prepared and stored at Porton Down for "Operation Vegetarian" — antilivestock attacks against Germany to be made by the Royal Air Force. The plan was for anthrax-based biological weapons to be dropped on Germany in 1944. However, the edible cattle cakes and the bomb were not used; the cattle cakes were incinerated in late 1945.
Weaponized anthrax was part of the US stockpile prior to 1972, when the United States signed the Biological Weapons Convention. President Nixon ordered the dismantling of US biowarfare programs in 1969 and the destruction of all existing stockpiles of bioweapons. In 1978–1979, the Rhodesian government used anthrax against cattle and humans during its campaign against black rebels. The Soviet Union created and stored 100 to 200 tons of anthrax spores at Kantubek on Vozrozhdeniya Island. They were abandoned in 1992 and destroyed in 2002.
American military and British Army personnel are routinely vaccinated against anthrax prior to active service in places where biological attacks are considered a threat.
Despite signing the 1972 agreement to end bioweapon production, the government of the Soviet Union had an active bioweapons program that included the production of hundreds of tons of weapons-grade anthrax after this period. On 2 April 1979, some of the over one million people living in Sverdlovsk (now called Ekaterinburg, Russia), about 850 miles east of Moscow, were exposed to an accidental release of anthrax from a biological weapons complex located near there. At least 94 people were infected, of whom at least 68 died. One victim died four days after the release, 10 over an eight-day period at the peak of the deaths, and the last six weeks later. Extensive cleanup, vaccinations, and medical interventions managed to save about 30 of the victims. Extensive cover-ups and destruction of records by the KGB continued from 1979 until Russian President Boris Yeltsin admitted this anthrax accident in 1992. Jeanne Guillemin reported in 1999 that a combined Russian and United States team investigated the accident in 1992.
Nearly all of the night-shift workers of a ceramics plant directly across the street from the biological facility (compound 19) became infected, and most died. Since most were men, some NATO governments suspected the Soviet Union had developed a sex-specific weapon. The government blamed the outbreak on the consumption of anthrax-tainted meat, and ordered the confiscation of all uninspected meat that entered the city. They also ordered all stray dogs to be shot and people not have contact with sick animals. Also, a voluntary evacuation and anthrax vaccination program was established for people from 18–55.
To support the cover-up story, Soviet medical and legal journals published articles about an outbreak in livestock that caused GI anthrax in people having consumed infected meat, and cutaneous anthrax in people having come into contact with the animals. All medical and public health records were confiscated by the KGB. In addition to the medical problems the outbreak caused, it also prompted Western countries to be more suspicious of a covert Soviet bioweapons program and to increase their surveillance of suspected sites. In 1986, the US government was allowed to investigate the incident, and concluded the exposure was from aerosol anthrax from a military weapons facility. In 1992, President Yeltsin admitted he was "absolutely certain" that "rumors" about the Soviet Union violating the 1972 Bioweapons Treaty were true. The Soviet Union, like the US and UK, had agreed to submit information to the UN about their bioweapons programs, but omitted known facilities and never acknowledged their weapons program.
In theory, anthrax spores can be cultivated with minimal special equipment and a first-year collegiate microbiological education.
To make large amounts of an aerosol form of anthrax suitable for biological warfare requires extensive practical knowledge, training, and highly advanced equipment.
Concentrated anthrax spores were used for bioterrorism in the 2001 anthrax attacks in the United States, delivered by mailing postal letters containing the spores. The letters were sent to several news media offices and two Democratic senators: Tom Daschle of South Dakota and Patrick Leahy of Vermont. As a result, 22 were infected and five died. Only a few grams of material were used in these attacks and in August 2008, the US Department of Justice announced they believed that Dr. Bruce Ivins, a senior biodefense researcher employed by the United States government, was responsible. These events also spawned many anthrax hoaxes.
Due to these events, the U.S. Postal Service installed biohazard detection systems at its major distribution centers to actively scan for anthrax being transported through the mail.
In response to the postal anthrax attacks and hoaxes, the Postal Service sterilized some mail using gamma irradiation and treatment with a proprietary enzyme formula supplied by Sipco Industries Ltd.
A scientific experiment performed by a high school student, later published in "The Journal of Medical Toxicology", suggested a domestic electric iron at its hottest setting (at least 400 °F) used for at least 5 minutes should destroy all anthrax spores in a common postal envelope.
2014 anthrax outbreak in India.
In October 2014, an outbreak of anthrax in a village in India allegedly killed seven people. The village was located in the Simdega district within the Indian state of Jharkhand. Indian government health personnel quarantined 30 houses as a result. Officials traced the anthrax spores to a cow, and found that people who had touched the dead cow or eaten from it became infected. The Indian equivalent of the CDC said at the time that the outbreak was one of the biggest in recent years in terms of deaths.
Government officials sent samples of the suspected anthrax to a laboratory in Delhi for confirmation testing. People with the victims reported that the victims vomited blood and complained of chest and stomach aches.
The Hindustan Times, an English-language newspaper in India, reported that village residents murdered by lynching a man who had treated some anthrax patients with herbs.

</doc>
<doc id="42899" url="http://en.wikipedia.org/wiki?curid=42899" title="Anthrax (American band)">
Anthrax (American band)

Anthrax is an American thrash metal band from New York City, formed in 1981 by guitarist Scott Ian and bassist Dan Lilker. The group was considered one of the leaders of the thrash metal scene during the 1980s. When the genre's popularity increased, Anthrax was one of its "big four" with Metallica, Megadeth and Slayer. As of 2014 the band has released ten studio albums, a number of singles and an EP with American hip hop group Public Enemy. According to Nielsen SoundScan Anthrax sold 2.5 million records in the United States from 1991 to 2004, with worldwide sales of over 15 million.
Noted for its live performances, Anthrax signed with the independent label Megaforce Records (which released the band's debut studio album in 1984). Lilker soon left the band to form Nuclear Assault, and was replaced by roadie Frank Bello. Vocalist Neil Turbin was replaced after two years by Matt Fallon who was then subsequently replaced in 1985 by Joey Belladonna. With a new lineup, the band recorded "Spreading the Disease" (distributed by Island Records) in 1985. Anthrax's third album, "Among the Living", was released in 1987 to critical praise. The band experienced another lineup change in 1992, when John Bush replaced Belladonna as lead vocalist. "Sound of White Noise" was released the following year, peaking at number seven on the "Billboard" 200. Studio recordings during the 1990s saw the band, influenced by other genres, experimenting with its sound.
Anthrax's lineup has changed several times. Paul Kahn and Greg Walls were early replacements for drummer Dave Weiss and bassist Kenny Kushner; the band has had a number of vocalists, including Neil Turbin, Matt Fallon, Joey Belladonna, Dan Nelson and John Bush. Scott Ian and Charlie Benante, who joined Anthrax in 1983, are the only band members to appear on every album; bassist Frank Bello has played on every album except the band's first. Belladonna returned to Anthrax for "Worship Music", his first studio album with the band since 1990's "Persistence of Time".
History.
Formation (1981).
Anthrax was formed in mid-1981 by guitarists Scott Ian and Dan Lilker. The band was named for the disease Ian saw in a biology textbook because it sounded "sufficiently evil". Anthrax's initial lineup was completed with drummer Dave Weiss and bassist Kenny Kushner. Kushner was soon briefly replaced by bassist Paul Kahn before Lilker took over on bass and Greg Walls joined as lead guitarist. Weiss was replaced early by Greg D'Angelo, who was recommended by Greg Walls. Scott Ian's younger brother Jason Rosenfeld was a temporary vocalist until Neil Turbin joined the band in late August 1982.
Neil Turbin Era Debut Album and First North American Tour (1982–1984).
The band's first performance with Neil Turbin was at Great Gildersleeves, a New York club, in September 1982. This lineup played regularly in the New York-New Jersey area, often with Metallica. Walls left Anthrax during the summer of 1983 after not receiving a writing credit for "Panic" (from the band's first album). Bob Berry, recommended to Turbin by Rhett Forrester of Riot, temporarily replaced Walls on guitar. Berry was in turn replaced by Dan Spitz, who previously of the New Jersey thrash band Overkill, for Anthrax's second demo.
Charlie Benante replaced drummer Greg D'Angelo (who left for White Lion) in September 1983. This lineup recorded "Soldiers of Metal", produced by Ross the Boss of Manowar. The B-side was "Howling Furies", from a previous demo with D'Angelo on drums (his only Anthrax recording), and the single secured the band a record deal with Megaforce Records. Anthrax recorded its debut album, "Fistful of Metal", in late 1983; released in January 1984, it was followed by a very successful US tour amidst great tension between the band and Neil Turbin. The road crew which included Joe Allen, Tom Browne were very close friends of Charlie Benante and Scott Ian, knew their agenda was to undermine Neil Turbin as singer, since he didn't march to Scott Ian and Charlie Benante's orders. Tensions between Lilker and the rest of the band arose because of his tardiness, sloppiness and unprofessional behavior. Lilker left, forming Nuclear Assault with former guitar roadie John Connelly, and was replaced by Charlie Benante's nephew and roadie Frank Bello.
In August 1984 Turbin and Anthrax went their separate ways, as Neil Turbin was receiving prominent attention and some band members were displeased with Turbin's growing popularity. Turbin went on to form Deathriders. In his book "Eddie Trunk’s Essential Hard Rock and Heavy Metal", music journalist Eddie Trunk admits pressuring Jon Zazula, Scott Ian and Anthrax into firing Turbin because of his personal taste in vocals. Matt Fallon was hired in late 1984 as the second vocalist of Anthrax and announcements were made all throughout the press and media. Things didn't work out with former Skid Row vocalist Matt Fallon either. Soon after the four-piece band, billed as "The Diseased" with Scott Ian on vocals, performed hardcore punk covers.
First Joey Belladonna era (1984–1992).
In 1985 after Matt Fallon parted ways with Anthrax, Joey Belladonna was chosen as the new vocalist, and this lineup debuted in February 1985. An EP, "Armed and Dangerous", was recorded and released that year featuring two live 1984 tracks and two songs from the "Soldiers of Metal" single. Later in 1985, Ian, Benante and Lilker collaborated with vocalist Billy Milano to produce the satirical album "Speak English or Die" as Stormtroopers of Death. However, Dan Lilker was not invited to rejoin Anthrax as permanent bassist, since Charlie and Scott wanted Charlie's nephew Frank Bello in the band in order to have unanimous control of Anthrax.
Anthrax's second album, "Spreading the Disease", was released in 1985; US and European tours followed the next year. The US tour, with Black Sabbath, was cancelled after four dates due to singer Glenn Hughes' voice problems. In April 1986 Anthrax attempted its first tour of Europe, including a show near Chernobyl immediately after the Chernobyl disaster. In May the band played its first European concert in Bochum, Germany, supported by Overkill and Agent Steel. Later that year, Anthrax toured Europe with Metallica. The tour began on September 10 at St David's Hall and ended on September 26 in Solnahallen, Sweden. The Swedish show was Anthrax's last performance before the bus accident the following day which killed Metallica bassist Cliff Burton.
The band's third studio album, "Among the Living", was recorded in 1986 and released in March 1987; it showcased the band's humorous, experimental side. Anthrax departed from its heavy-metal look in favor of brightly-colored surfer shorts, and began a lyrical trend focusing on movies, comic books and Stephen King novels. The album was dedicated to Burton's memory. "I Am the Law" was issued as a single backed with "I'm the Man", a rap-metal hybrid. Anthrax further indulged its appreciation for rap by appearing on the title track of U.T.F.O.'s album, "Lethal". The band toured Europe with Metallica and Metal Church to promote "Among the Living".
Anthrax returned to its thrash formula for its 1988 album, "State of Euphoria". The single "Antisocial", originally by French heavy-metal band Trust, became an MTV staple as part of the rotation on "Headbangers Ball". The band expanded its horizons by touring the US with the funk metal band Living Colour and embarking on the Headbangers Ball Tour with Exodus and Helloween. In 1989, MTV sponsored a contest in which the winner had her home trashed by the band this inspired Anthrax's 1992 appearance on "Married... with Children", in which the Bundys win a similar TV contest.
In 1990 Anthrax released the more serious "Persistence of Time", which surpassed "State of Euphoria"‍ '​s success. The album was darker, more technical and more progressive than the band's previous work, striking a chord with metal fans wary of Anthrax's "silly" side. The most successful single from the album was a cover of Joe Jackson's "Got the Time", which Jackson said he enjoyed. In 1991, Anthrax collaborated with Public Enemy on a version of "Bring the Noise". This was a hit, and a successful tour with Public Enemy followed. The EP "Attack of the Killer B's" was recorded in 1991, with a new version of "I'm the Man" and a cover of "Bring the Noise" on which Ian did some vocals. In late 1992, Belladonna was fired from the band.
First John Bush era (1992–2005).
Former Armored Saint John Bush joined Anthrax shortly after Belladonna's dismissal. The band left Island Records to sign with Elektra, releasing "Sound of White Noise" in 1993. A change from Anthrax's earlier work, with a dark, rockier feel, "Sound of White Noise" received mostly-positive reviews. The single "Only" was a hit; in the liner notes for "Return of the Killer A's", Ian said that James Hetfield told him it was a "perfect song". In keeping with the band's eye for unlikely collaborations, classical composer Angelo Badalamenti provided music for "Black Lodge" (a nod to "Twin Peaks"). This album demonstrated that Anthrax had fully shed its cartoonish persona in favor of mature, thoughtful songwriting, which began with "Persistence of Time".
After "Sound of White Noise" longtime guitarist Dan Spitz left the band to become a watchmaker, leaving Anthrax a quartet for two years. In 1995 Anthrax released "Stomp 442", on which Charlie Benante played most of the lead-guitar parts. Benante was assisted by Paul Crook, later the band's touring lead guitarist for several years, and Dimebag Darrell of Pantera. Because Elektra did not promote the album it was less commercially successful than its predecessor, and Anthrax severed its ties with the label.
The band signed with independent label Ignition Records, releasing "" in 1998. As on "Stomp 442", Benante played lead guitar with Crook and Darrell; Pantera vocalist Phil Anselmo also appeared. After the album's release the label went bankrupt, disrupting its distribution. Although Anthrax then signed with Beyond Records, releasing the greatest-hits album "Return of the Killer A's", Beyond went out of business as well. During this period a two-vocalist tour with Belladonna and Bush was planned, but Belladonna quit at the last minute.
During the 2001 anthrax attacks in the United States the band changed its website, providing information about the disease after people began typing "anthrax.com" into search engines. Amid a potential PR nightmare, Anthrax issued a press release on October 10, 2001 joking that the band's name would be changed to "something more friendly, like 'Basket Full of Puppies'." Anthrax dispelled any name-change rumors derived from the press release at the November 2001 New York Steel 9/11 benefit concert, when they took the stage in boiler suits with a different word on each one (reading "We're not changing our name"). Bello has stated they did so after receiving support from members of the NYPD and NYFD, who believed that changing the name of the band would send the wrong message. A picture of the band in the suits is on the inner tray card of "We've Come for You All".
Despite hardships and legal entanglements over album rights, Anthrax continued. In 2001 Rob Caggiano joined on lead guitar; two years later the band released "We've Come for You All", praised by metal journalists as a return to form, on Sanctuary Records. In early 2004 Anthrax released "The Greater of Two Evils", a "live in the studio" re-recording of the earlier work with the band's current lineup. Bassist Frank Bello announced shortly afterwards that he was leaving the band to join Helmet, and was replaced by Fates Warning and Armored Saint member Joey Vera.
Reunions with Belladonna and Bush (2005–2009).
In April 2005, Anthrax announced that the "classic" lineup of Scott Ian, Charlie Benante, Dan Spitz, Joey Belladonna and Frank Bello would reform. At some shows on the following tour, they played "Among the Living" in its entirety. Although the lineup was expected to record a new album after the tour, in January 2007 Ian said that Belladonna had not agreed to a reunion. After that announcement it was uncertain if John Bush would return, since Bush said he was unready to re-commit to Anthrax.
In May 2007 Ian said the decision of who would be singing for Anthrax would be made at the end of June, but the announcement was delayed until December. In June, Bush was asked by "Rock Hard" if he was bitter about the Anthrax reunion. He replied that he was asked to return to the band, but declined. Asked if he wanted to rejoin the band when Belladonna left, Bush said that he "just didn't feel right to do that."
In December 2007 it was announced that the band's new vocalist would be Dan Nelson, formerly of Devilsize, and Rob Caggiano would return as lead guitarist. In May 2008, Anthrax played its first show in 19 months at Double Door in Chicago. Appearing before a sold-out audience with Nelson, the band played new material which was well received (despite equipment problems).
In his monthly Food Coma column posted on December 22, 2008, Scott Ian wrote that he had "been in the studio working on the new Anthrax album since November 4"; drums, bass and rhythm had been recorded on 19 tracks, and the addition of vocals had begun. "We should be mixing at the end of January and soon after that giving birth to a really pissed off, loud, fast and heavy child." In a May 2009 Food Coma column Ian wrote that the album was being mixed by Dave Fortman, who had worked with Evanescence and Slipknot. In a post on the Anthrax website, Charlie Benante said that "Worship Music" would probably be out in May.
In early 2009, Anthrax began a brief tour opening for Iron Maiden in South America. In July, band manager Izvor Zivkovic confirmed the departure of Dan Nelson due to illness. Nelson denied this, saying that he was fired. All subsequent performances were canceled except the August UK Sonisphere Festival, with John Bush on vocals. Due to fan response after his performance, a "Bring Back Bush" campaign began and was endorsed by Ian.
In September 2009, it was announced that Bush would again sing with Anthrax at the October Loud Park '09 Festival in Japan. Soon afterwards, Benante said that Bush had rejoined the band. In February 2010, Anthrax performed five shows as part of Soundwave in Australia. After the Australian shows, Bush said the band intended to re-record the vocals of several tracks from the upcoming album.
"Worship Music" and upcoming album (2010–present).
In late 2009, Anthrax confirmed a "Big Four" event (with Metallica, Megadeth and Slayer) as part of the 2010 Sonisphere Festival. Bush decided that he did not want to commit to the band full-time, and again left. Joey Belladonna returned to Anthrax in early 2010 for shows that summer, and committed to record a studio album with the band. Anthrax, Metallica, Slayer and Megadeth performed on the same bill during that summer's Sonisphere Festival series, the first time all members of the thrash-metal "Big Four" played together. The Sofia, Bulgaria show was broadcast in cinemas and later .
In April 2011, Anthrax headlined in the Philippines for the first time at the annual Pulp Summer Slam with Death Angel and Hellyeah. The band also headlined the Jägermeister side stage at Mayhem Festival 2012, co-headlined by Slayer and Slipknot. In June Anthrax released "Fight 'Em 'Til You Can't" (from its upcoming album) on its website as a free download, to thank fans for their patience in waiting several years for "Worship Music"; the album was released on September 13.
In January 2013, Anthrax announced that lead guitarist Rob Caggiano had left the band. A week later it was announced that Jonathan Donais of Shadows Fall would be the band's touring lead guitarist, and on August 13 it was announced that Donais had joined the band. In March 2013, Anthrax released its "Anthems" EP featuring covers of 1970s rock songs. According to Scott Ian, the band began working on its next studio album in late 2013; in September, he confirmed that drummer Charlie Benante and bassist Frank Bello were "coming over to start writing for the next record." The album is scheduled to be released in early 2015.
Style and influences.
Anthrax is one of the bands responsible for the emergence of speed and thrash metal. It exhibited a typical thrash-metal sound on its early albums and was known for humor and comic-book references in the lyrics, distinguishing the band from its contemporaries. According to "Rolling Stone", Anthrax was one of the few heavy-metal bands to receive critical praise and redefine the genre during the 1980s. Original guitarists Scott Ian and Dan Spitz' styles were described as "aggressive and head pounding", with power chords and "chugging" pedal points providing the sonic drive. Author Thomas Harrison wrote that Anthrax played metal at a faster tempo because of its punk influences, noting the group's "antimetal stage persona" with "bright clothes more fit to surf culture than to metal". The band's sixth album, 1993's "Sound of White Noise" (its first with singer John Bush), incorporated grunge and alternative metal influences in a darker vein. Critics consider the band's studio releases from the Bush era as having a more alternative and groove metal sound. The band's latest album, "Worship Music", marked a return to thrash metal and the return of singer Joey Belladonna.
Anthrax was influenced by classic rock artists on its "Anthems" EP, which includes covers of 1970s bands Rush, Cheap Trick, AC/DC, Thin Lizzy, Boston and Journey. The band has been influenced by punk bands Bad Brains, the Sex Pistols and Discharge and traditional heavy metal bands Black Sabbath, Kiss, Judas Priest, Anvil, Iron Maiden and Motörhead. Anthrax is a member the "big four" of thrash metal with Metallica, Megadeth and Slayer. The band has been credited for laying the groundwork for rap and nu metal, and was one of the first bands combining rap and hard rock. According to Nielsen SoundScan, Anthrax sold 2.5 million records in the United States from 1991 to 2004; worldwide sales were over 15 million.

</doc>
<doc id="42900" url="http://en.wikipedia.org/wiki?curid=42900" title="Pythagorean tuning">
Pythagorean tuning

Pythagorean tuning (Greek: Πυθαγόρεια κλίμακα) is a tuning of the syntonic temperament in which the generator is the ratio 3:2 (i.e., the untempered perfect fifth), which is 702 cents wide (see the figure labelled "The syntonic tuning continuum" below). 
Hence, it is a system of musical tuning in which the frequency ratios of all intervals are based on the ratio 3:2, "found in the harmonic series." This ratio, also known as the "pure" perfect fifth, is chosen because it is one of the most consonant and easy to tune by ear.
The system had been mainly attributed to Pythagoras (sixth century BC) by modern authors of music theory, while Ptolemy, and later Boethius, ascribed the division of the tetrachord by only two intervals, called "semitonium", "tonus", "tonus" in Latin (256:243 x 9:8 x 9:8), to Eratosthenes. The so-called "Pythagorean tuning" was used by musicians up to the beginning of the 16th century.
The Pythagorean scale is any scale which may be constructed from only pure perfect fifths (3:2) and octaves (2:1) or the gamut of twelve pitches constructed from only pure perfect fifths and octaves, and from which specific scales may be drawn (see Generated collection). For example, the series of fifths generated above gives seven notes, a diatonic major scale on C in Pythagorean tuning, shown in notation on the top right. In Greek music it was used to tune tetrachords and the twelve tone Pythagorean system was developed by medieval music theorists using the same method of tuning in perfect fifths, however there is no evidence that Pythagoras himself went beyond the tetrachord.
Method.
Pythagorean tuning is based on a stack of intervals called perfect fifths, each tuned in the ratio 3:2, the next simplest ratio after 2:1. Starting from D for example ("D-based" tuning), six other notes are produced by moving six times a ratio 3:2 up, and the remaining ones by moving the same ratio down:
This succession of eleven 3:2 intervals spans across a wide range of frequency (on a piano keyboard, it encompasses 77 keys). Since notes differing in frequency by a factor of 2 are given the same name, it is customary to divide or multiply the frequencies of some of these notes by 2 or by a power of 2. The purpose of this adjustment is to move the 12 notes within a smaller range of frequency, namely within the interval between the base note D and the D above it (a note with twice its frequency). This interval is typically called the basic octave (on a piano keyboard, an octave encompasses only 13 keys ).
For instance, the A is tuned such that its frequency equals 3:2 times the frequency of D—if D is tuned to a frequency of 288 Hz, then A is tuned to 432 Hz. Similarly, the E above A is tuned such that its frequency equals 3:2 times the frequency of A, or 9:4 times the frequency of D—with A at 432 Hz, this puts E at 648 Hz. Since this E is outside the above-mentioned basic octave (i.e. its frequency is more than twice the frequency of the base note D), it is usual to halve its frequency to move it within the basic octave. Therefore, E is tuned to 324 Hz, a 9:8 (= one epogdoon) above D. The B at 3:2 above that E is tuned to the ratio 27:16 and so on. Starting from the same point working the other way, G is tuned as 3:2 below D, which means that it is assigned a frequency equal to 2:3 times the frequency of D—with D at 288 Hz, this puts G at 192 Hz. This frequency is then doubled (to 384 Hz) to bring it into the basic octave.
When extending this tuning however, a problem arises: no stack of 3:2 intervals (perfect fifths) will fit exactly into any stack of 2:1 intervals (octaves). For instance a stack such as this, obtained by adding one more note to the stack shown above
will be similar but not identical in size to a stack of 7 octaves. More exactly, it will be about a quarter of a semitone larger (see Pythagorean comma). Thus, A♭ and G♯, when brought into the basic octave, will not coincide as expected. The table below illustrates this, showing for each note in the basic octave the conventional name of the interval from D (the base note), the formula to compute its frequency ratio, its size in cents, and the difference in cents (labeled ET-dif in the table) between its size and the size of the corresponding one in the equally tempered scale.
In the formulas, the ratios 3:2 or 2:3 represent an ascending or descending perfect fifth (i.e. an increase or decrease in frequency by a perfect fifth), while 2:1 or 1:2 represent an ascending or descending octave.
The major scale based on C, obtained from this tuning is:
In equal temperament, pairs of enharmonic notes such as A♭ and G♯ are thought of as being exactly the same note—however, as the above table indicates, in Pythagorean tuning they have different ratios with respect to D, which means they are at a different frequency. This discrepancy, of about 23.46 cents, or nearly one quarter of a semitone, is known as a "Pythagorean comma".
To get around this problem, Pythagorean tuning constructs only twelve notes as above, with eleven fifths between them. For example, one may use only the 12 notes from E♭ to G♯. This, as shown above, implies that only eleven just fifths are used to build the entire chromatic scale. The remaining interval (the diminished sixth from G♯ to E♭) is left badly out-of-tune, meaning that any music which combines those two notes is unplayable in this tuning. A very out-of-tune interval such as this one is known as a "wolf interval". In the case of Pythagorean tuning, all the fifths are 701.96 cents wide, in the exact ratio 3:2, except the wolf fifth, which is only 678.49 cents wide, nearly a quarter of a semitone flatter.
If the notes G♯ and E♭ need to be sounded together, the position of the wolf fifth can be changed. For example, a C-based Pythagorean tuning would produce a stack of fifths running from D♭ to F♯, making F♯-D♭ the wolf interval. However, there will always be one wolf fifth in Pythagorean tuning, making it impossible to play in all keys in tune.
Size of intervals.
The table above shows only intervals from D. However, intervals can be formed by starting from each of the above listed 12 notes. Thus, twelve intervals can be defined for each interval type (twelve unisons, twelve semitones, twelve intervals composed of 2 semitones, twelve intervals composed of 3 semitones, etc.).
As explained above, one of the twelve fifths (the wolf fifth) has a different size with respect to the other eleven. For a similar reason, each of the other interval types, except for the unisons and the octaves, has two different sizes in Pythagorean tuning. This is the price paid for seeking just intonation. The tables on the right and below show their frequency ratios and their approximate sizes in cents. Interval names are given in their standard shortened form. For instance, the size of the interval from D to A, which is a perfect fifth (P5), can be found in the seventh column of the row labeled D. Strictly just (or pure) intervals are shown in bold font. Wolf intervals are highlighted in red.
The reason why the interval sizes vary throughout the scale is that the pitches forming the scale are unevenly spaced. Namely, the frequencies defined by construction for the twelve notes determine two different semitones (i.e. intervals between adjacent notes):
Conversely, in an equally tempered chromatic scale, by definition the twelve pitches are equally spaced, all semitones having a size of exactly
As a consequence all intervals of any given type have the same size (e.g., all major thirds have the same size, all fifths have the same size, etc.). The price paid, in this case, is that none of them is justly tuned and perfectly consonant, except, of course, for the unison and the octave.
For a comparison with other tuning systems, see also this table.
By definition, in Pythagorean tuning 11 perfect fifths (P5 in the table) have a size of approximately 701.955 cents (700+ε cents, where ε ≈ 1.955 cents). Since the average size of the 12 fifths must equal exactly 700 cents (as in equal temperament), the other one must have a size of 700−11ε cents, which is about 678.495 cents (the wolf fifth). Notice that, as shown in the table, the latter interval, although enharmonically equivalent to a fifth, is more properly called a diminished sixth (d6). Similarly,
In short, similar differences in width are observed for all interval types, except for unisons and octaves, and they are all multiples of ε, the difference between the Pythagorean fifth and the average fifth.
Notice that, as an obvious consequence, each augmented or diminished interval is exactly 12ε (≈ 23.460) cents narrower or wider than its enharmonic equivalent. For instance, the d6 (or wolf fifth) is 12ε cents narrower than each P5, and each A2 is 12ε cents wider than each m3. This interval of size 12ε is known as a Pythagorean comma, exactly equal to the opposite of a diminished second (≈ −23.460 cents). This implies that ε can be also defined as one twelfth of a Pythagorean comma.
Pythagorean intervals.
Four of the above-mentioned intervals take a specific name in Pythagorean tuning. In the following table, these specific names are provided, together with alternative names used generically for some other intervals. Notice that the Pythagorean comma does not coincide with the diminished second, as its size (524288:531441) is the reciprocal of the Pythagorean diminished second (531441:524288). Also "ditone" and "semiditone" are specific for Pythagorean tuning, while "tone" and "tritone" are used generically for all tuning systems. Interestingly, despite its name, a semiditone (3 semitones, or about 300 cents) can hardly be viewed as half of a ditone (4 semitones, or about 400 cents). All the intervals with prefix "sesqui-" are justly tuned, and their frequency ratio, shown in the table, is a superparticular number (or epimoric ratio). The same is true for the octave.
History.
Because of the wolf interval, this tuning is rarely used nowadays, although it is thought to have been widespread. In music which does not change key very often, or which is not very harmonically adventurous, the wolf interval is unlikely to be a problem, as not all the possible fifths will be heard in such pieces.
Because most fifths in Pythagorean tuning are in the simple ratio of 3:2, they sound very "smooth" and consonant. The thirds, by contrast, most of which are in the relatively complex ratios of 81:64 (for major thirds) and 32:27 (for minor thirds), sound less smooth. For this reason, Pythagorean tuning is particularly well suited to music which treats fifths as consonances, and thirds as dissonances. In western classical music, this usually means music written prior to the 15th century.
From about 1510 onward, as thirds came to be treated as consonances, meantone temperament, and particularly quarter-comma meantone, which tunes thirds to the relatively simple ratio of 5:4, became the most popular system for tuning keyboards. At the same time, syntonic-diatonic just intonation was posited by Zarlino as the normal tuning for singers.
However, meantone presented its own harmonic challenges. Its wolf intervals proved to be even worse than those of the Pythagorean tuning (so much so that it often required 19 keys to the octave as opposed to the 12 in Pythagorean tuning). As a consequence, meantone was not suitable for all music.
From around the 18th century, as the desire grew for instruments to change key, and therefore to avoid a wolf interval, this led to the widespread use of well temperaments and eventually equal temperament.
In 2007, the discovery of the syntonic temperament exposed the Pythagorean tuning as being a point on the syntonic temperament's tuning continuum.

</doc>
<doc id="42901" url="http://en.wikipedia.org/wiki?curid=42901" title="Indian Trade">
Indian Trade

The Native American Trade refers to historic trade between Europeans and their North American descendants and the Indigenous people of North America (today known as Native Americans in the United States, and First Nations in Canada, but formerly as "Indians"), beginning before the colonial period and continuing through the nineteenth century, although declining before mid-century.
The term Indian Trade describes the people involved in the trade. The products involved varied by region and era. In most of Canada the term is synonymous with the fur trade, since fur for making beaver hats was by far the most valuable product of the trade, from the European point of view. Demand for other products resulted in trade in those items: Europeans asked for deerskin in the Southeast coast of the United States, and for buffalo skins and meat, and pemmican and on the Great Plains. In turn, Native American demand influenced the trade goods brought by Europeans.
Economic contact between Native Americans and European colonists began in the 1500s and lasted until the late 1800s. Although the relationship between Europeans and Indians was often marred by conflicts, many tribes established peaceful trade relations with the new colonists during the early stages of European settlement. From the seventeenth to the nineteenth century, the English and French mainly traded for animal pelts and fur with Native Americans. On the other hand, trading between the Spanish and Native Americans was sporadic and lasted only for a couple of decades. Eventually, wars, the dwindling of Native American populations and the westward expansion of the United States led to the confinement of tribes to reservations and the end of this kind of economic relations between Indians and European Americans.
Other economic relations continued, especially in the alcohol trade around many reservations, and for Native arts and crafts. Today, many Native Americans satisfy a different kind of demand with the associated trades of their gaming casinos on sovereign land. These have been developed as entertainment and conference resorts, serving a wide market of customers, and generating substantial revenues for tribes to use for economic development, as well as welfare and education of their people.
Pre-European settlements (1500-early 1600s).
Economic contact between Native Americans and Europeans can be traced back to the 1500s when English and French fishermen fishing off the coast of Canada, traded guns and other weapons for beaver fur. Before Europeans settled permanently in North America, many European fishermen regularly made voyages to the shores of Canada to trade for furs from Native Americans. By the 1600s, the Eurasian beaver was almost extinct in France and England.[4] Due to this shortage of fur, many fur traders began to look to the New World for pelts.
The first explorers to conduct trade with Native Americans were Giovanni da Verrazano and Jacques Cartier in the 1520s-1530s. Verrazano noted in his book, “If we wanted to trade with them for some of their things, they would come to the seashore on some rocks where the breakers were most violent while we remained on the little boat, and they sent us what they wanted to give on a rope, continually shouting to us not to approach the land.” As visits from Europeans became more frequent and some Europeans began to settle in North America, Indians began to establish regular trade relations with these new colonists. The ideal locations for fur trading were near harbors where ships could come in.
Trade with early European settlers.
Plymouth and Jamestown
In order to set up a thriving colony, settlers in the New World needed the five factors of production that contribute to the creation of wealth: land (natural resources), labor, capital, entrepreneurship and knowledge. Often, trading with Native Americans resulted in colonists gaining needed knowledge and natural resources. Examples of this can be seen in the English settlements of Plymouth Bay and Jamestown. Chief Massasoit, a Wampanoag, and Squanto, a Patuxet Indian, helped the Pilgrims of Plymouth Bay establish their colony by teaching them skills in cultivating this land and hunting. In return for weapons and tools, these Native Americans provided the colonists with important natural resources, including food. In 1621 Chief Massasoit established one of the earliest trading pacts between Europeans and Indians by signing a treaty with Plymouth Colony to engage in peaceful trade. As the number of English colonists in the New England area began to grow, the Wampanoag became uneasy of losing their land to these new settlers. Gradually, tensions escalated, leading to King Philip's War, an armed conflict between the Pilgrims and the Native Americans in the area. The war ended with the defeat of the Indian tribe, causing a serious fracture amongst relations between the Pilgrims and Native Americans.
Relations between settlers in the Jamestown area and Native Americans ended similarly. Initially, the Powahatan aided the English settlers with food and clothing, helping them survive the early difficult years. However, relations between the two groups deteriorated after three years, resulting in a war.
Fur trading posts
Fur trading was one of the main economic activities in Northern America from the late 1500s to the mid-1800s. At the time, demand for fur was surging in Europe as it was used to make cloth and fancy hats. Data collected from England in the eighteenth century highlights that the years from 1746 to 1763 saw an increase of 12 shillings per pelt. It has been calculated that over 20 million beaver hats were exported from England alone from 1700 to 1770. Both trading partners, Native Americans and Europeans, provided the other a comparative advantage in the fur trade industry. The opportunity cost of hunting beavers in Europe was extremely high: by the seventeenth and eighteenth centuries, the Eurasian beaver was near extinction in England and France. On the other hand, traders and trappers thought the wildlife in the New World was essentially limitless. Native Americans made use of the trade goods received, particularly knives, axes, and guns. The fur trade provided a stable source of income for many Native Americans until the mid-1800s, when changing fashion trends in Europe and a decline in the beaver population in North America brought about a collapse in demand for fur.
Trade with the Spanish
Trading between Spanish settlers and Native Americans was rare and occurred in parts of New Mexico and California. The Spanish mainly intended to spread the Christian faith to Indians and to use them as slaves for work. The most significant effect of trading with the Spanish was the introduction of the horse to the Ute in New Mexico. Gradually, horses bred and their use was adopted across the Great Plains, dramatically altering the lifestyles and customs of many Native American tribes. Many Indians switched from a hunter- gatherer economy to a nomadic lifestyle after they began using horses for transportation. They had a greater range for hunting bison and trading with other tribes.
Relationship between Europeans and Indians
It took time for Europeans and Native Americans to learn the customs of the other side. When Europeans first encountered a tribe, they would often be offered fur, food or other items as gifts. The Europeans did not understand they were supposed to take on an alliance with the natives, including helping them against their enemies. Native American tribes regularly practice gift giving as part of their social relations. Because the Europeans did not (or most of them), they were considered to be rude and crude.
After observing that Europeans wanted to trade goods for the skins and other items, Native Americans entered into that. Both sides became involved in the conflicts of the other. In New France, in Carolina, Virginia, and New England and in New Netherland, the Europeans became drawn into the endemic warfare of their trading partners. As Native Americans were pressed into alliances by the Europeans for Queen Anne's War, the Seven Years' War, the Nine Years' War, and other standing competitions among the European powers: France, Great Britain and Spain, with whom they were dealing in North America, they felt drawn into the Europeans' endemic warfare.
Late 1800s to present.
After the United States became independent, it enacted legislation to regulate trading with the Indians/Native Americans, under the Trade and Intercourse Act, first passed on July 22, 1790. Later the Indian Office, then part of the War Department, issued licenses to traders in the Indian Territory. Under removal, the largest tribes from the Southeast and north of the Ohio were moved west of the Mississippi river. By 1834 Indian Territory had been designated as what was then most of the United States west of the Mississippi, primarily what became Arkansas, Kansas and Oklahoma. Territories of the upper West were still occupied by native tribes as well. Mountain men and traders from Mexico freely operated there independently of the US.
After the formation of the United States, the commerce clause of the constitution gave Congress the power to “regulate Commerce with foreign Nations, and among the several States, and with the Indian tribes.” In the 1800s, the American government passed legislation to support relocation of tribes to reservations in order to extinguish their title to lands that could be sold to European Americans. The Indian Removal Act of 1830 forced tribes such as the Cherokee and the Choctaw to move out of their homelands. Resistance by Native Americans to relocate resulted in conflicts such as the Second Seminole War, that caused the deaths of 3000 Native Americans. Forcing tribes to relocate and to adjust to isolated reservations often unsuitable for the subsistence farming they were encouraged to undertake, made many of them dependent on the U.S. government for annuities and supplies. They had difficulty trying to develop economic systems of their own.
As outlined by Kalt and Cornell in their book, "What Can Tribes Do? Strategies and Institutions in American Indian Economic Development," on reservations, tribes lacked access to capital, were assigned to areas with poor natural resources (or had their resources stolen or kept from their control), and did not possess skilled labor.
Today, many programs, such as the Harvard Project on American Indian Economic Development, exist to foster conditions that will help reservations become independent and financially stable communities. Since the late 20th century, many tribes have established gaming casinos. The most successful ones use part of the revenues for economic development of their nations, as well as for welfare and education for all their tribal members.
References.
N.p.: W.W. Norton &, 2011. Print.

</doc>
<doc id="42903" url="http://en.wikipedia.org/wiki?curid=42903" title="Meantone temperament">
Meantone temperament

Meantone temperament is a musical temperament, which is a system of musical tuning. In general, a meantone is constructed the same way as Pythagorean tuning, as a stack of perfect fifths, but in meantone, each fifth is "narrow" compared to the ratio 27/12:1 used in 12 equal temperament. The meantone temperament:
Quarter-comma meantone is the best known type of meantone temperament, and the term "meantone temperament" is often used to refer to it specifically.
Meantone temperaments.
Though quarter-comma meantone is the most common type, other systems that flatten the fifth by some amount, but that still equate the major whole tone (9/8 in just intonation) with the minor whole tone (10/9 in just intonation), are also called meantone systems. Since (9/8) / (10/9) = (81/80)—the syntonic comma—the fundamental characteristics of meantone systems are that all intervals are generated from fifths, and the syntonic comma is tempered to a unison.
All meantone temperaments fall on the syntonic temperament's tuning continuum, and as such are "syntonic tunings". The distinguishing feature of each unique syntonic tuning is the width of its generator in cents, as shown in the central column of Figure 1. Historically notable meantone temperaments, discussed below, occupy a narrow portion of the syntonic temperament's tuning continuum, ranging from approximately 695 to 699 cents. The criteria which define the limits (if any) of the meantone range of tunings within the syntonic temperament's tuning continuum are not yet well-defined.
While the term "meantone temperament" refers primarily to the tempering of 5-limit musical intervals, optimum values for the 5-limit also work well for the 7-limit, defining septimal meantone temperament. In Figure 1, the valid tuning ranges of 5-limit, 7-limit, and 11-limit syntonic tunings are shown, and can be seen to include many notable meantone tunings.
Meantone temperaments can be specified in various ways: by what fraction (logarithmically) of a syntonic comma the fifth is being flattened (as above), what equal temperament has the meantone fifth in question, the width of the tempered perfect fifth in cents, or the ratio of the whole tone to the diatonic semitone. This last ratio was termed "R" by American composer, pianist and theoretician Easley Blackwood, but in effect has been in use for much longer than that. It is useful because it gives us an idea of the melodic qualities of the tuning, and because if R is a rational number N/D, so is (3R+1)/(5R+2) or (3N+D)/(5N+2D), which is the size of fifth in terms of logarithms base 2, and which immediately tells us what division of the octave we will have. If we multiply by 1200, we have the size of fifth in cents.
In these terms, some historically notable meantone tunings are listed below. The relationship between the first two columns is exact, while that between them and the third is closely approximate.
Equal temperaments.
Neither the just fifth nor the quarter-comma meantone fifth is a rational fraction of the octave, but several tunings exist which approximate the fifth by such an interval; these are a subset of the equal temperaments (""N"-ET"), in which the octave is divided into some number ("N") of equally wide intervals.
Equal temperaments useful as meantone tunings include (in order of increasing generator width) 19-ET, 50-ET, 31-ET, 43-ET, and 55-ET. The farther the tuning gets away from quarter-comma meantone, however, the less related the tuning is to harmonic timbres, which can be overcome by tempering the timbre to match the tuning.
Wolf intervals.
A whole number of just perfect fifths will never add up to a whole number of octaves, because they are incommensurable (see Fundamental theorem of arithmetic). If a stacked-up whole number of perfect fifths is to close with the octave, then one of the fifths must have a different width than all of the others. For example, to make the 12-note chromatic scale in Pythagorean tuning close at the octave, one fifth must be out of tune by the Pythagorean comma; this altered fifth is called a wolf fifth.
Wolf intervals are an artifact of keyboard design. This can be shown most easily using an isomorphic keyboard, such as that shown in Figure 2.
 On an isomorphic keyboard, any given musical interval has the same shape wherever it appears, except at the edges. Here's an example. On the keyboard shown in Figure 2, from any given note, the note that's a perfect fifth higher is always up-and-rightwardly adjacent to the given note. There are no wolf intervals within the note-span of this keyboard. The problem is at the edge, on the note E♯. The note that's a perfect fifth higher than E♯ is B♯, which is not included on the keyboard shown (although it could be included in a larger keyboard, placed just to the right of A♯, hence maintaining the keyboard's consistent note-pattern). Because there is no B♯ button, when playing an E♯ power chord, one must choose some other note, such as C, to play instead of the missing B♯.
Even edge conditions produce wolf intervals only if the isomorphic keyboard has fewer buttons per octave than the tuning has enharmonically-distinct notes (Milne, 2007). For example, the isomorphic keyboard in Figure 2 has 19 buttons per octave, so the above-cited edge-condition, from E♯ to C, is "not" a wolf interval in 12-ET, 17-ET, or 19-ET; however, it "is" a wolf interval 26-ET, 31-ET, and 50-ET. In these latter tunings, using electronic transposition could keep the current key's notes on the isomorphic keyboard's white buttons, such that these wolf intervals would very rarely be encountered in tonal music, despite modulation to exotic keys.
Isomorphic keyboards expose the invariant properties of the meantone tunings of the syntonic temperament isomorphically (that is, for example, by exposing a given interval with a single consistent inter-button shape in every octave, key, and tuning) because both the isomorphic keyboard and temperament are two-dimensional ("i.e.", rank-2) entities (Milne, 2007). One-dimensional N-key keyboards can expose accurately the invariant properties of only a single one-dimensional N-ET tuning; hence, the one-dimensional piano-style keyboard, with 12 keys per octave, can expose the invariant properties of only one tuning: 12-ET.
When the perfect fifth is exactly 700 cents wide (that is, tempered by approximately 1/11 of a syntonic comma, or exactly 1/12 of a Pythagorean comma) then the tuning is identical to the familiar 12-tone equal temperament. This appears in the table above when R = 2/1.
Because of the compromises (and wolf intervals) forced on meantone tunings by the one-dimensional piano-style keyboard, well temperaments and eventually equal temperament became more popular.
Using standard interval names, twelve fifths equal six octaves plus one augmented seventh; seven octaves are equal to eleven fifths plus one diminished sixth. Given this, three "minor thirds" are actually augmented seconds (for example, B♭ to C♯), and four "major thirds" are actually diminished fourths (for example, B to E♭). Several triads (like B–E♭–F♯ and B♭–C♯–F) contain both these intervals and have normal fifths.
Extended meantones.
All meantone tunings fall into the valid tuning range of the syntonic temperament, so all meantone tunings are syntonic tunings. All syntonic tunings, including the meantones, have a conceptually infinite number of notes in each octave, that is, seven natural notes, seven sharp notes (F♯ to B♯), seven flat notes (B♭ to F♭), double sharp notes, double flat notes, triple sharps and flats, and so on. In fact, double sharps and flats are uncommon, but still needed; triple sharps/flats are almost never seen. In any syntonic tuning that happens to divide the octave into a small number of equally wide smallest intervals (such as 12, 19, or 31), this infinity of notes still exists, although some notes will be enharmonic. For example, in 19-ET, E♯ and F♭ are the same pitch.
Many musical instruments are capable of very fine distinctions of pitch, such as the human voice, the trombone, unfretted strings such as the violin, and lutes with tied frets. These instruments are well-suited to the use of meantone tunings.
On the other hand, the piano keyboard has only twelve physical note-controlling devices per octave, making it poorly suited to any tunings other than 12-ET. Almost all of the historic problems with the meantone temperament are caused by the attempt to map meantone's infinite number of notes per octave to a finite number of piano keys. This is, for example, the source of the "wolf fifth" discussed above. When choosing which notes to map to the piano's black keys, it is convenient to choose those notes that are common to a small number of closely related keys, but this will only work up to the edge of the octave; when wrapping around to the next octave, one must use a "wolf fifth" that is not as wide as the others, as discussed above.
The existence of the "wolf fifth" is one of the reasons why, before the introduction of well temperament, instrumental music generally stayed in a number of "safe" tonalities that did not involve the "wolf fifth" (which was generally put between G♯/A♭ and D♯/E♭).
Throughout the Renaissance and Enlightenment, theorists as varied as Nicola Vicentino, Francisco de Salinas, Fabio Colonna, Marin Mersenne, Constantijn Huygens, and Isaac Newton advocated the use of meantone tunings that were extended beyond the keyboard's twelve notes, and hence have come to be called "extended" meantone tunings. These efforts required a concomitant extension of keyboard instruments to offer means of controlling more than 12 notes per octave, including Vincento's Archicembalo (shown in Figure 3), Mersenne's 19-ET harpsichord, Colonna's 31-ET sambuca, and Huygens' 31-ET harpsichord. Other instruments extended the keyboard by only a few notes. Some period harpsichords and organs have split D♯/E♭ keys, such that both E major/C♯ minor (4 sharps) and E♭ major/C minor (3 flats) can be played without wolf fifths. Many of those instruments also have split G♯/A♭ keys, and a few have all the five accidental keys split.
All of these alternative instruments were "complicated" and "cumbersome" (Isacoff, 2003), due to (a) not being isomorphic, and (b) not having the ability to transpose electronically, which can significantly reduce the number of note-controlling buttons needed on an isomorphic keyboard (Plamondon, 2009). Both of these criticisms could be addressed by electronic isomorphic keyboard instruments (such as the open source jammer keyboard), which could be simpler, less cumbersome, and more expressive than existing keyboard instruments.
Use of meantone temperament.
References to tuning systems that could possibly refer to meantone were published as early as 1496 (Gafori) and Aron (1523) is unmistakably referring to meantone. However, the first mathematically precise Meantone tuning descriptions are found in late 16th century treatises by Francisco de Salinas and Gioseffo Zarlino. Salinas (in De musica libra septum) describes three different mean tone temperaments: the 1/3 comma system, the 2/7 comma system, and the 1/4 comma system. He is the likely inventor of the 1/3 system, while he and Zarlino both wrote on the 2/7 system, apparently independently. Lodovico Fogliano mentions the 1/4 comma system, but offers no discussion of it.
In the past, meantone temperaments were sometimes used or referred to under other names or descriptions. For example, in 1691 Christiaan Huygens wrote his "Lettre touchant le cycle harmonique" ("Letter concerning the harmonic cycle") with the purpose of introducing what he believed to be a new division of the octave. In this letter Huygens referred several times, in a comparative way, to a conventional tuning arrangement, which he indicated variously as "temperament ordinaire", or "the one that everyone uses". But Huygens' description of this conventional arrangement was quite precise, and is clearly identifiable with what is now classified as (quarter-comma) meantone temperament.
Although meantone is best known as a tuning environment associated with earlier music of the Renaissance and Baroque, there is evidence of continuous usage of meantone as a keyboard temperament well into the middle of the 19th century. Meantone temperament has had considerable revival for early music performance in the late 20th century and in newly composed works specifically demanding meantone by composers including John Adams, György Ligeti and Douglas Leedy.
New uses of meantone tunings.
Meantone is one of many possible tuning effects found in Dynamic Tonality (Plamondon, 2009).

</doc>
<doc id="42904" url="http://en.wikipedia.org/wiki?curid=42904" title="Suva">
Suva

Suva is the capital and the second most populated municipality of Fiji, after Nasinu. It is on the southeast coast of the island of Viti Levu, in the Rewa Province, Central Division. In 1877, it was decided to make Suva the capital of Fiji when the geography of former main European settlement at Levuka on the island of Ovalau proved too restrictive. The administration of the colony was moved from Levuka to Suva in 1882.
Suva is Fiji's political and administrative capital. It is the largest and the most cosmopolitan city in the South Pacific and has become an important regional centre; students from the Pacific region and a growing expatriate community make up a significant portion of the city's population. Under authority of local government act Suva is governed and administratively looked after by Suva City Council.
At the 2007 census, the city of Suva had a population of 85,691. Including independent suburbs, the population of the Greater Suva urban area was 172,399 at the 2007 census. Suva, along with the bordering cities of Lami, Nasinu, and Nausori have a total urban population of around 330,000, over a third of the nation's population. This urban complex is known also as the Suva–Nausori corridor (not including Lami).
History.
In return for a promise to pay off debts owed to the United States by the Bauan chieftain, Seru Epenisa Cakobau, the Australian-based Polynesia Company was granted 5000 km² of land, 575 km² of it near what was then the village of Suva, in 1868. The original intention was to develop a cotton farming industry, but the land and climate proved unsuitable.
Following the annexation of the Fiji Islands by the United Kingdom in 1874, the colonial authorities decided to move the capital to Suva from Levuka in 1877. The transfer was made official in 1882. Colonel F.E. Pratt of the Royal Engineers was appointed Surveyor-General in 1875 and designed the new capital, assisted by W. Stephens and Colonel R.W. Stewart.
Following the promulgation of the Municipal Constitution Ordinance of 1909, Suva acquired municipal status in 1910. The town initially comprised one square mile; these boundaries remained intact until 1952 when the Muanikau and Samabula wards were annexed, expanding its territory to 13 square kilometers. In October that year, Suva was proclaimed a city — Fiji's first. Tamavua was subsequently annexed; the most recent extension of the city boundaries has been to incorporate the Cunningham area to the north of the city. Urban sprawl has resulted in a number of suburbs that remain outside of the city limits; together with the city itself, they form a metropolitan area known as the Greater Suva Area.
The city hosted the 2003 South Pacific Games, being the third time in the event's 40-year history that they had been held in Suva. As part of the hosting of the event, a new gymnasium and indoor sports center, swimming pool, and stadium; field hockey pitch; and stands were built in the area around Suva, funded by the government and a $16 million People's Republic of China aid package.
Geography and physical characteristics.
Suva is the capital of Fiji and is a harbour city built on a peninsula reaching out into the sea. It has a mix of modern buildings and traditional colonial architecture.
The city is perched on a hilly peninsula between Laucala Bay and Suva Harbour in the southeast corner of Viti Levu. The mountains north and west catch the southeast trade winds, producing moist conditions year round.
Suva is the commercial and political centre of Fiji, though not necessarily the cultural centre, and the largest urban area in the South Pacific outside of Australia and New Zealand. It is Fiji's main port city.
Although Suva is on a peninsula, and almost surrounded by sea, the nearest beach is 40 kilometres (25 mi) away at Pacific Harbour and the nearby coast is lined by mangroves. A significant part of the city centre, including the old Parliament buildings, is built on reclaimed mangrove swamp.
Central.
The Central Business District encompasses an area known as the Central Ward; one of Suva's six wards, Central occupies close to the whole south-western side of the Suva Peninsula.
City wards.
The city's six wards beginning from the city centre, then north, then clockwise rotation.
Suva–Nausori Corridor.
Suva sits in the middle of an urban conurbation that stretches from Lami, to the immediate west of the city, along the Queens Highway and Nasinu, on its eastern border all the way to the Rewa River, along the Kings Highway. This conurbation, sometimes known as the Suva Urban Complex, continues till Nausori, over the Rewa River. The north of the city to its northeast contains the rainforest park areas of Colo-i-Suva and Sawani, along the Princes Road, connecting at the Rewa River Bridge. This entire conurbation, is generally referred to by locals as Suva, although it contains four local government areas. In formal reference, this complex has come to be known as the Suva–Nausori Corridor (where Lami is generally excluded) and is the most populous area in Fiji, with close to 350,000 people.
Climate.
Suva features a tropical rainforest climate under the Köppen climate classification. The city sees a copious amount of precipitation during the course of the year, with no true dry season due to no month having an average rainfall below 60 mm. Suva averages 3000 mm of precipitation annually with its driest month, July averaging 125 mm. In fact, during all 12 months of the year, Suva receives substantial precipitation. Like many other cities with a tropical rainforest climate, temperatures are relatively constant throughout the year, with an average high of about 28 °C and an average low of about 22 °C.
Suva is noted for its considerable rainfall, it has a markedly higher rainfall than Nadi and the western side of Viti Levu, which is known to Suva citizens as "the burning west". The First Governor of Fiji, Sir Arthur Gordon, allegedly remarked that it rained in Suva like he had seen nowhere else before and that there was hardly a day without rain. The most copious rainfall is observed from November to May, while the slightly cooler months from June to October are considerably drier.
Demographics.
Suva is a multiracial and multicultural city. Indigenous Fijians and Indo-Fijians, the two principal ethnic groups of Fiji, comprise the bulk of Suva's population, and the city is home to the majority of Fiji's ethnic minority populations, which include Caucasians (Europeans or "Kaivalagi"), part-Europeans (of European and Fijian descent), and Chinese, amongst others. The majority of expatriates working in Fiji are based in Suva. The most widely spoken language is English, but Fijian, Hindustani, Cantonese, and other Indian languages are also spoken by their respective communities. 
Municipal government.
Suva has municipal status and is governed by a Lord Mayor and a 20-member city council. The Suva City Council is the municipal law-making body of the city of Suva, Fiji's capital. It consists of 20 Councillors, elected for three-year terms from four multi-member constituencies called wards. Councillors, who are elected by residents, landowners, and representatives of corporations owning or occupying ratable property in Suva, elect a Lord Mayor and Deputy Lord Mayor from among their own members; they serve one-year terms and are eligible for re-election. However, The current interim-government has reformed and restructured all municipal councils as of October, 2008 and the position of Mayor is now void. The position of Special administrator is currently in place, equivalent to that of mayor, but rather than elected, the administrator is selected by the Ministry of Local Government. Currently, Suva City's special administrator is Chandu Umaria, Former Lord Mayor and was responsible, in his term, for many of the city's beautification works.
Landmarks.
A well-known landmark is the Suva City Library or the Carnegie Library, built in 1909 as well as many other colonial buildings.
The government buildings complex sits on what was once the flowing waters of a creek. It was drained in 1935 and over five kilometres of reinforced concrete pilings were driven into the creek bed to support the massive buildings to be erected. After the foundation stone was laid in 1937, the building was completed in 1939; a new wing was completed in 1967. Parliament, however, was moved to a new complex on Ratu Sukuna Road in 1992.
Government House was formerly the residence of Fiji's colonial Governors and, following independence in 1970, Governors-General. It is now the official residence of Fiji's President. Originally erected in 1882, it had to be rebuilt in 1928, following its destruction by lightning in 1921.
The Suva campus of the University of the South Pacific (USP) occupies what was once a New Zealand military base. It is the largest of the many USP campuses dotted throughout the South Pacific and the largest university in the Pacific Islands outside Hawai'i. It offers courses which are internationally recognized and endorsed.
The Fiji Museum, in Thurston Gardens, was founded in 1904 and originally occupied the old town hall, but moved to its present location in 1954. The museum houses the most extensive collection of Fijian artefacts in the world, and is also a research and educational institution, specializing in archaeology, the preservation of Fiji's oral tradition, and the publication of material on Fiji's language and culture.
Suva has around 78 parks, these include the new Takashi Suzuki Garden, Apted Park at Suva Point which is a popular spot for viewing sunrise and sunset, Thurston Gardens which was opened in 1913 and has flora from throughout the South Pacific.
Suva has many shopping and retail areas, notably Cumming street, which has since colonial times been a vibrant and colourful shopping area. Features of these streets include the original colonial buildings and narrow roads. More modern shopping malls, such as the Suva Central Shopping Mall, Mid-City Mall as well as MHCC are all part of the developments to give the city a modern and sophisticated look.
In December 2009, there was an addition to Suva's skyline with the opening of TappooCity valued at USD25.7 million (FJD50 million) a joint venture six storey low-rise building project by FNPF & Tappoo Group of Companies as Fiji's (and South Pacific's) largest department mall at present outside Australia & New Zealand.
Construction work began in January 2011 for a FJD30 million mini-mall complex at Grantham Road behind the Sports-City Complex and close proximity to University of the South Pacific, which will house restaurants, retail outlets and cinemas. Although construction was scheduled for end 2011, this complex will now be ready mid-2012.
Economy.
Unlike most cities and towns in Fiji and indeed many around the world, Suva did not grow around one industry but has gradually developed as a hub and one of the largest cities in the Pacific Islands. Suva is the commercial centre of Fiji with most banks having their Pacific headquarters here, including ANZ and the Westpac Bank. Most national financial institutions, non-governmental organizations and government ministries and departments are headquartered here. At one point both Air Pacific (now called Fiji Airways and Air Fiji were headquartered in Suva.
A large part of Fiji's international shipping is conducted at Suva's Kings Wharf as well as docking of international cruise ships, which has led to a growth in Suva's tourism industry. Many services are provided in Suva and is the basis of Suva's industrial and commercial activity.
There are large industrial areas, the largest being at Walu Bay, where there is a saturation of factories and warehouses and import and export companies. This area contains many shipyards for ship building and repairs as well as container yards. There is a brewery and many printeries. Other notable industrial zones are located in Vatuwaqa, Raiwaqa and Laucala Beach.
There is a large commercial and shopping scene in Suva with streets such as Cumming Street and Victoria Parade being popular. There are many shopping complexes to visit and many markets.
Institutions.
Suva is host to more international and regional intergovernmental agencies and NGOs than any other Pacific Island capital. Some of the bodies with a presence in Suva are:
Entertainment and culture.
Suva is host to many regional, national and local events and has very developed and advanced venue options.
Venues.
Suva has many multipurpose venues, the main ones being:
Parks and gardens.
Suva has a number of parks and a few gardens. Albert Park, in the City centre, is famous as the stage for many national-historical events such as the Independence of Fiji, the landing by Kingsford Smith on the Southern Cross and many parades and carnivals. Sukuna Park, also in the CBD is a popular recreational park and has many performances and events on a weekly basis. Thurston Gardens is the city's main botanical garden and the location of the Fiji Museum. Queen Elizabeth drive is popular as a scenic walk along Suva's foreshore. Many city residents go to the Colo-i-Suva Forest Reserve, a short drive from the city center, to swim under the waterfalls.
Music.
Many concerts are held in Suva, some coming from other countries to enjoy world-class performances. Concerts and shows are usually staged at one of the above-mentioned venues on a monthly basis. Some of the famous music artists to hold shows in Suva include UB40, Lucky Dube, O'Yaba, Sean Kingston and many others. Due to a favoured interests in Bollywood by all, some prominent singers and actors have held shows in the capital which includes singers like Shaan, Sonu Nigam, Sunidhi Chauhan and movie artists like Shah Rukh Khan, Priyanka Chopra, Johnny Lever, Dino Morea, Rajpal Yadav and the like.
Food.
Suva offers a varied and interesting culinary experience where almost every if not all major cuisines are represented. Particularly popular cuisines are Chinese, Indian, Fijian and Italian. At nights, especially on weekends, food stalls and outlets cater to the city's nightlife.
Festivals.
During the course of the year, arts, music and trade festivals are held in Suva, albeit on a small scale. There are a few large and notable festivals that occur annually and these include the Hibiscus Festival (largest carnival in the South Pacific islands), the New Years Street Party, and the Fiji Show Case tradeshow that includes carnival rides, food as well as magic and circus performances.
Night life.
Suva has a vibrant nightlife where most nightclubs and bars open in the late afternoon and remain open till 5 am. Suva's nightlife caters to all tastes, moods and likes. Food stalls are open throughout the night and the city is well policed at night. Apart from nightclubs, there are lounges and bars that cater to those seeking low-key entertainment
The seedier side of Suva is Victoria Parade where night clubs such Signals Night Club, East Court Restaurant, Angel Night Club are located in the same stretch.
Cinema.
Downtown Suva has one main cinema complex, Village Six, owned by the Damodar Brothers. The Regal and Phoenix theaters, once prominent cinema/theatre haunts before the new millennium owned by the Sharan Brothers, have since closed down. A development expected to be finished at the beginning of 2013 is the Damodar city complex (also owned by the Damodar brothers), in the shopping area of Laucala Bay, which will have a further six screens, along with shopping and eating outlets and cafes.
Another interesting feature of Suva is the increasing number of Bollywood films being shot in the capital: By the middle 2012 alone, there have been around six movies partly shot in Suva.
Sports.
Suva plays host to many regional and national sporting events. A special highlight is the Coca-Cola Games, the largest secondary school athletics meet in the world. The Capital City is represented in major sporting events by its respective rugby, netball and soccer teams.
Suva was the host of the first Pacific Games, in 1963. Forty years later in 2003 the Games returned to Fiji's capital, with a full program of 32 sports introduced for the first time. Suva held the games for the second time in 1979. Having hosted the event three times, Suva has held the Pacific Games the more often than any other city.
Mass media.
Headquartered in Suva are the three main national television stations, Fiji One, FBC TV and MAI TV along with the Fiji Ministry of Information, which produces government programs as well as national news and current affairs bulletins. Fiji One produces and airs its evening 'National News' bulletin from its studios in Gladstone Road in Central; FBC TV airs its 'FBC News' bulletin from its studios, also on Gladstone Road. Sky Pacific and Pacific Broadcasting Services Fiji are the two pay satellite television company headquartered here.
Suva is home to the national radio broadcasters Fiji Broadcasting Corporation (FBC) and Communications Fiji Limited (CFL), between them providing 12 of the national radio stations.
The two dailies, "The Fiji Times" and "The Fiji Sun" are printed here (and, formerly, the "Fiji Post"). Many other weekly newspapers are headquartered and published in Suva, including "Inside Fiji", "Nai Lalakai" (iTaukei language weekly), "Shanti Dut" (Fiji Hindi weekly), national magazines such as "Repúblika" and "Mai Life" as well as regional magazines such as "Islands Business".
Shopping and fashion.
Suva is one of the most shopper friendly cities in the Pacific and has long been known for its affordable and unique offerings, often duty-free. The city offers its shopping paradise in a cluster that is referred to as Suva Central. Areas like Cumming Street and Marks Street are for clothing, jewellery, food, electronics, pharmaceuticals and more. Terry Walk and the Flea Market offer handicrafts and local ware. Close by, huge, new shopping complexes dominate the canal area, such as MHCC, Tappoo City and Suva Central. The general outer areas of this radius are telecommunication and electronic stores, tourist favourite - Jack's and sporting gear stores.
Every year, Suva plays host to Fiji Fashion Week (FJFW) usually held in Suva and a condensed version in Nadi over consecutive weeks in late October. The shows offer the creations of local and overseas designers as well as shows focused on wearable art. FJFW began in 2008, with a show at the Hilton Hotel in Denarau, with the 2009 show at Albert Park. In 2010, Fiji Fashion Week was a bigger extravaganza with a Fashion Film Festival as well. FJFW 2010 was held at Boron House (a state mansion) where the shows were broadcast internationally by world-renowned Fashion TV. In 2011, the show took on a distinct urban theme with the show held on the sixth floor of the Tappoo City building with the city of Suva as a backdrop and for the first time, included models from overseas (mostly, international models who were in Fiji for the World Supermodel Competition). Fiji Fashion Week 2012 brings together designers from France, USA, India, Australia, New Zealand, Philippines, Nigeria, Samoa, Cook Islands, Wallis & Futuna and Fiji. It will once again be shown on Fashion TV.
Transportation.
Nausori International Airport caters mainly to the domestic market, connecting Suva with Fiji's other international airport, Nadi International Airport, and serves smaller international aircraft, at one time servicing Brisbane and Sydney routes. As of August 2010, Air Pacific will operate a twice weekly flight from Nausori International Airport to Auckland, New Zealand to complement its 13 weekly flights from Nadi to Auckland, furthermore, the Nausori - Sydney route is expected to resume in the later half of 2012. The airport provides services to its immediate Pacific neighbours Tonga and Tuvalu as well as the dependency of Rotuma.
Suva has a public transport system consisting of buses and taxis servicing the metropolitan area as well as the cities of Nasinu, Nausori and Lami town. There are bus services connecting Suva with other towns and cities on Viti levu by way of either the Kings, Queens or Princes highways, all originating within Suva, although the latter terminates at Rewa Bridge in Nausori. As of January 2012, a feasibility study was being conducted by JRK and Associates, in partnership with Canadian company Hatch Mott McDonald, to construct and operate a monorail train network from Suva, across the Suva - Nausori Corridor to ease congestion and traffic problems. The construction of the monorail system is expected to begin in the first quarter of 2013.
There is a domestic ferry services from the Princess Wharf to the outer islands of Fiji as well as Vanua Levu. International ships and cruise liners dock at Suva's Kings Wharf.
Notable residents and/or people from Suva.
This is a list of famous people who are either currently living in, or are originally from Suva.

</doc>
<doc id="42905" url="http://en.wikipedia.org/wiki?curid=42905" title="North American Aerospace Defense Command">
North American Aerospace Defense Command

North American Aerospace Defense Command (NORAD, ) is a combined organization of the United States and Canada that provides aerospace warning, air sovereignty, and defense for Northern America. Headquarters for NORAD and the NORAD/United States Northern Command (USNORTHCOM) center are located at Peterson Air Force Base in El Paso County, near Colorado Springs, Colorado. The nearby Cheyenne Mountain nuclear bunker has the Alternate Command Center. The NORAD commander and deputy commander (CINCNORAD) are, respectively, a United States four-star general or equivalent and a Canadian three-star general or equivalent.
Organization.
CINCNORAD maintains the NORAD headquarters at Peterson Air Force Base near Colorado Springs, Colorado. The NORAD and USNORTHCOM Command Center at Peterson AFB serves as a central collection and coordination facility for a worldwide system of sensors designed to provide the commander and the leadership of Canada and the U.S. with an accurate picture of any aerospace or maritime threat. NORAD has administratively divided the North American landmass into three regions: the Alaska NORAD (ANR) Region, under Eleventh Air Force (11 AF); the Canadian NORAD (CANR) Region, under 1 Canadian Air Division, and the Continental U.S. (CONR) Region, under 1 AF/CONR-AFNORTH. Both the CONR and CANR regions are divided into eastern and western sectors.
Alaska NORAD Region.
The Alaska NORAD Region (ANR) maintains continuous capability to detect, validate and warn of any atmospheric threat in its area of operations from its Regional Operations Control Center (ROCC) at Elmendorf Air Force Base, Alaska.
ANR also maintains the readiness to conduct a continuum of aerospace control missions, which include daily air sovereignty in peacetime, contingency and/or deterrence in time of tension, and active air defense against manned and unmanned air-breathing atmospheric vehicles in times of crisis.
ANR is supported by both active duty and reserve units. Active duty forces are provided by 11 AF and the Canadian Armed Forces (CAF), and reserve forces provided by the Alaska Air National Guard. Both 11 AF and the CAF provide active duty personnel to the ROCC to maintain continuous surveillance of Alaskan airspace.
Canadian NORAD Region.
1 Canadian Air Division/Canadian NORAD Region Headquarters is at CFB Winnipeg, Manitoba. It is responsible for providing surveillance and control of Canadian airspace. The Royal Canadian Air Force provides alert assets to NORAD. CANR is divided into two sectors, which are designated as the Canada East Sector and Canada West Sector. Both Sector Operations Control Centers (SOCCs) are co-located at CFB North Bay Ontario. The routine operation of the SOCCs includes reporting track data, sensor status and aircraft alert status to NORAD headquarters.
Canadian air defense forces assigned to NORAD include 409 Tactical Fighter Squadron at CFB Cold Lake, Alberta and 425 Tactical Fighter Squadron at CFB Bagotville, Quebec. All squadrons fly the McDonnell Douglas CF-18 Hornet fighter aircraft.
To monitor for drug trafficking, in cooperation with the Royal Canadian Mounted Police and the United States drug law enforcement agencies, the Canadian NORAD Region monitors all air traffic approaching the coast of Canada. Any aircraft that has not filed a flight plan may be directed to land and be inspected by RCMP and Canada Border Services Agency.
Continental United States NORAD Region.
The Continental NORAD Region (CONR) is the component of NORAD that provides airspace surveillance and control and directs air sovereignty activities for the Contiguous United States (CONUS).
CONR is the NORAD designation of the United States Air Force First Air Force/AFNORTH. Its headquarters is located at Tyndall Air Force Base, Florida. The First Air Force (1 AF) became responsible for the USAF air defense mission on 30 September 1990. AFNORTH is the United States Air Force component of United States Northern Command (NORTHCOM),
1 AF/CONR-AFNORTH comprises State Air National Guard Fighter Wings assigned an air defense mission to 1 AF/CONR-AFNORTH, made up primarily of citizen Airmen. The primary weapons systems are the McDonnell Douglas F-15 Eagle and General Dynamics F-16 Fighting Falcon aircraft.
It plans, conducts, controls, coordinates and ensures air sovereignty and provides for the unilateral defense of the United States. It is organized with a combined First Air Force command post at Tyndall Air Force Base and two Sector Operations Control Centers (SOCC) at Rome, New York for the US East ROCC and McChord Field, Washington for the US West ROCC manned by active duty personnel to maintain continuous surveillance of CONUS airspace.
In its role as the CONUS NORAD Region, 1 AF/CONR-AFNORTH also performs counter-drug surveillance operations.
History.
NORAD (originally known as the North American Air Defense Command), was recommended by the Joint Canadian-U.S. Military Group in late 1956, approved by the United States JCS in February 1957, and announced on 1 August 1957; the "establishment of [NORAD] command headquarters" was on 12 September 1957, at Ent Air Force Base's 1954 blockhouse. The 1958 international agreement designated the NORAD commander always be a United States officer (Canadian vice commander), and "RCAF officers ... agreed the command's primary purpose would be…early warning and defense for SAC's retaliatory forces.":252 In late 1958, Canada and the United States started the Continental Air Defense Integration North (CADIN) for the SAGE air defense network:253 (initial CADIN cost sharing agreement between the countries was on 5 January 1959), and two December 1958 plans submitted by NORAD had "average yearly expenditure of around five and one half billions", including "cost of the accelerated Nike Zeus program" and three Ballistic Missile Early Warning System (BMEWS) sites.
Canada's NORAD bunker with a SAGE AN/FSQ-7 Combat Direction Central computer was constructed from 1959 to 1963, and each of the USAF's eight smaller AN/FSQ-8 Combat Control Central systems provided NORAD with data and could command the entire United States air defense. The RCAF's 1950 "ground observer system, the Long Range Air Raid Warning System," was discontinued and on 31 January 1959, the United States Ground Observer Corps was deactivated.:222 The Cheyenne Mountain nuclear bunker's planned mission was expanded in August 1960 to "a hardened center from which CINCNORAD would supervise and direct operations "against space attack" as well as air attack" (NORAD would be renamed North American Aero"space" Defense Command in March 1981). The Secretary of Defense assigned on 7 October 1960, "operational command of all space surveillance to Continental Air Defense Command (CONAD) and operational control to North American Air Defense Command (NORAD)".
The JCS placed the Ent Air Force Base Space Detection and Tracking System (496L System with Philco 2000 Model 212 computer) "under the operational control of CINCNORAD on December 1, 1960"; during Cheyenne Mountain nuclear bunker excavation, and the joint SAC-NORAD exercise "Sky Shield II" — and on 2 September 1962 — "Sky Shield III" were conducted for mock penetration of NORAD sectors.
NORAD command center operations moved from Ent Air Force Base to the 1963 partially underground "Combined Operations Center" for Aerospace Defense Command and NORAD at the Chidlaw Building. President John F. Kennedy visited "NORAD headquarters" after the 5 June 1963 United States Air Force Academy graduation. NORAD had an exhibit at the 1964 New York World's Fair, and on 30 October 1964, "NORAD began manning" the Cheynne Mountain Combat Operations Center. By 1965, about 250,000 United States and Canadian personnel were involved in the operation of NORAD, On 1 January 1966, Air Force Systems Command turned the COC over to NORAD The NORAD Cheyenne Mountain Complex was accepted on 8 February 1966.:319
1968 reorganization.
United States Department of Defense realignments for the NORAD command organization began by 15 November 1968 ("e.g.", Army Air Defense Command (ARADCOM)) and by 1972, there were eight NORAD "regional areas ... for all air defense", and the NORAD Cheyenne Mountain Complex Improvements Program (427M System) became operational in 1979.
False alarms.
On at least three occasions, NORAD systems failed, such as on 9 November 1979, when a technician in NORAD loaded a test tape, but failed to switch the system status to "test", causing a stream of constant false warnings to spread to two "continuity of government" bunkers as well as command posts worldwide. On 3 June 1980, and again on 6 June 1980, a computer communications device failure caused warning messages to sporadically flash in U.S. Air Force command posts around the world that a nuclear attack was taking place. During these incidents, Pacific Air Forces (PACAF) properly had their planes (loaded with nuclear bombs) in the air; Strategic Air Command (SAC) did not and took criticism, because they did not follow procedure, even though the SAC command knew these were almost certainly false alarms, as did PACAF. Both command posts had recently begun receiving and processing direct reports from the various radar, satellite, and other missile attack detection systems, and those direct reports simply did not match anything about the erroneous data received from NORAD.
1980 reorganization.
Following the 1979 Joint US-Canada Air Defense Study, the command structure for aerospace defense was changed, "e.g.", "SAC assumed control of ballistic missile warning and space surveillance facilities" on 1 December 1979 from ADCOM.:48 The Aerospace Defense Command major command ended 31 March 1980. and its organizations in Cheyenne Mountain became the "ADCOM" "specified" command under the same commander as NORAD, "e.g.", HQ NORAD/ADCOM J31 manned the Space Surveillance Center. By 1982, a NORAD Off-site Test Facility was located at Peterson AFB. The DEW Line was to be replaced with the North Warning System (NWS); the Over-the-Horizon Backscatter (OTH-B) radar was to be deployed; more advanced fighters were deployed, and E-3 Sentry AWACS aircraft were planned for greater use. These recommendations were accepted by the governments in 1985. The United States Space Command was formed in September 1985 as an adjunct, but not a component of NORAD.
Post–Cold War.
In 1989 NORAD operations expanded to cover counter-drug operations, "e.g.", tracking of small aircraft entering and operating within the United States and Canada. DEW line sites were replaced between 1986 and 1995 by the North Warning System. The Cheyenne Mountain site was also upgraded, but none of the proposed OTH-B radars are currently in operation.
After the September 11, 2001 attacks, the NORAD Air Warning Center's mission "expanded to include the interior airspace of North America."
The Cheyenne Mountain Realignment was announced on 28 July 2006, to consolidate NORAD's day-to-day operations at Peterson Air Force Base with Cheyenne Mountain in "warm standby" staffed with support personnel.
In popular culture.
Movies and television.
The NORAD command center was depicted in the satirical film "Dr. Strangelove" that starred Peter Sellers and in the drama "Fail-Safe," which starred Henry Fonda. Both films were released in 1964; a television adaptation of "Fail-Safe" starring George Clooney and Richard Dreyfuss was broadcast live on CBS in 2000.
Cheyenne Mountain is a setting of the 1983 film "WarGames" and the "Jeremiah" and "Stargate" television series.
NORAD HQ is a retreat facility for NASA in the mid/late 21st century in the 2014 science fiction film Interstellar.
NORAD Tracks Santa.
On December 24, 1955, a Sears department store placed an advertisement in a Colorado Springs newspaper that told children they could telephone Santa Claus and included a number for them to call. However, the telephone number printed was erroneously that of the Colorado Springs command center of NORAD's predecessor, the Continental Air Defense Command (CONAD). Colonel Harry Shoup, who was on duty that night, told his staff to give all children who called in a "current location" for Santa Claus — and a Christmas Eve tradition was born, now known as the "NORAD Tracks Santa" program. Every year on Christmas Eve, "NORAD Tracks Santa" purports to track Santa Claus as he leaves the North Pole and delivers presents to children around the world. Today, NORAD relies on volunteers to make the program possible.

</doc>
<doc id="42906" url="http://en.wikipedia.org/wiki?curid=42906" title="The Queen of the Damned">
The Queen of the Damned

The Queen of the Damned (1988) is the third novel of Anne Rice's "The Vampire Chronicles" series. It follows "Interview with the Vampire" and "The Vampire Lestat". This novel is a continuation of the story that ends in a cliffhanger in "The Vampire Lestat" and explores the rich history and mythology of the origin of the vampires, which dates back to Ancient Egypt.
On March 9, 2014, it was announced that there would be another installment of "The Vampire Chronicles" titled "Prince Lestat" which would be a sequel to "The Queen of the Damned". Tentative release date is said to be October 28, 2014. There was also mention of a second book, a sequel to "Prince Lestat", tentatively titled "Blood Paradise".
Plot summary.
Part One follows several different people over the same period of several days. Several of the characters appear in the two previous books, including Armand, Daniel (the "boy reporter" of "Interview with the Vampire"), Marius, Louis, Gabrielle and Santino. Each of the six chapters in Part One tells a different story about a different person or group of people. Two things unify these chapters: a series of dreams about red-haired twin sisters, and the fact that a powerful being is killing vampires around the world by means of spontaneous combustion.
Pandora and Santino rescue Marius, having answered his telepathic call for help. Marius informs his rescuers that Akasha has been awakened by Lestat, or rather his rock music, for he has joined a rock band of mortals whose names are Alex, Larry and Tough Cookie. Having been awakened by Lestat's rebellious music, Akasha destroys her husband Enkil and plots to rule the world. Akasha is also revealed as the source of the attacks on other vampires.
Part Two takes place at Lestat's concert. Jesse Reeves, a member of the secret Talamasca and relative of Maharet, is mortally injured while attending the concert, and is taken to Maharet's Sonoma compound where she is made into a vampire. The vampires from Part One later congregate in the Sonoma compound. The only vampires not present are Akasha and Lestat. Akasha has abducted Lestat and takes him as an unwilling consort to various locations in the world, inciting women to rise up and kill the men who have oppressed them.
Part Three takes place at Maharet's home in a Sonoma forest. There Maharet tells the story of Akasha and the red-haired twins (who are, in fact, Maharet and her sister, Mekare) to Pandora, Jesse, Marius, Santino, Eric, Armand, Daniel, Louis and Gabrielle. Also present are Mael and Khayman, who already know the story.
In Part Four, Akasha confronts the gathered vampires at Maharet's compound. There she explains her plans and offers the vampires a chance to be her "angels" in her New World Order. Akasha plans to kill 90 percent of the world's human men, and to establish a new Eden in which women will worship Akasha as a goddess. If the assembled vampires refuse to follow her, she will destroy them. The vampires refuse, but before Akasha can destroy them, Mekare enters. Mekare kills Akasha by severing her head. Mekare then consumes Akasha's brain, while Maharet eats her heart. Amel (who was in the brain), passes into Mekare, thereby saving the lives of the remaining vampires. She becomes the new Queen of the Damned.
In Part Five, the vampires leave Maharet's compound and assemble at Armand's resort, the Night Island, (according to Anne Rice, inspired by Fire Island) in Florida to recover. They eventually go their separate ways (as told in "The Tale of the Body Thief"). Lestat takes Louis to see David Talbot in London. After their brief visit with Talbot they depart into the night, an incensed Louis and his angry words filling Lestat with glee.
The Origin of Vampires.
"The Queen of the Damned", deals with the origins of vampires themselves. The mother of all vampires, Akasha, begins as a pre-Egyptian queen, in a land called Kemet (which will become Egypt), many thousands of years ago. During this time two powerful witches (Maharet and Mekare) live in the mountains of an unnamed region. The witches are able to communicate with invisible spirits and gain simple favors from them. During this period there is a bloodthirsty, invisible spirit known as Amel who continually asks the two witches if they need his assistance, although they prudently decline the offer. The witches' village is destroyed and they are incarcerated by the king and queen, who desire their knowledge. When the witches offend Akasha, the Queen condemns the twins. Enkil then orders his chief steward (who is Khayman as a mortal man) to rape the twins in his stead, which would prove their lack of power, before the eyes of the court. Afterward the witches are cast out into the desert. While making her way back home with a pregnant Maharet, Mekare curses the king and queen secretly with the bloodthirsty spirit. Eventually this spirit inflicts such torment on Akasha and Enkil that they again demand advice and help from the two witches.
Conspirators, unhappy with the young king's policies, assassinate the royal couple in Khayman's house while they were attempting to exorcise Amel, who had been tormenting Khayman. While the king and queen lie dying, the evil spirit sees its chance to ensnare the soul of the dying queen and pulls it back into her body. The spirit combines itself with the flesh and blood of the queen, transforming her into a vampire. Akasha allows the king to drink her blood, which saves his life. They then order Khayman to find the witches and bring them back to Egypt so that they could use their knowledge of spirits to help them, as they feel guilty because of their thirst for blood. However, when the witches admit that they cannot help the monarchs, Akasha orders the mutilation of the witches: Maharet loses her eyes and Mekare her tongue. Afterward, Khayman, who had been turned into a vampire by Akasha, comes to the witches' cell and turns them too. The three flee together, but are caught by Akasha's soldiers. Khayman escapes, but Maharet and Mekare are further punished. The witches are put into two separate coffins which are then set afloat on two separate bodies of water. They are only reunited near the end of the novel "Queen of the Damned".
In Mekare's absence, Maharet returns to watch over her daughter and her descendants. Maharet's descendants become what she calls the Great Family. A maternal line, the Great Family includes every culture, religion, ethnicity, and race. The Great Family represents all humanity and shows the vampires what Akasha would destroy with the creation of her New World Order.
As the source of all vampires, Akasha is connected to all vampires by the blood and spirit they collectively share. In an experiment by the first Keeper, Akasha and Enkil are exposed to sunlight when they are several thousand years old. This merely darkens their skin. However, the result on all other vampires is extreme, and many of the weakest vampires die, thus confirming the legend that anything that harms Akasha will also directly affect all of her progeny.

</doc>
<doc id="42909" url="http://en.wikipedia.org/wiki?curid=42909" title="Jacobus Henricus van 't Hoff">
Jacobus Henricus van 't Hoff

Jacobus Henricus van 't Hoff, Jr. (]; 30 August 1852 – 1 March 1911) was a Dutch physical and organic chemist and the first winner of the Nobel Prize in Chemistry. He is best known for his discoveries in chemical kinetics, chemical equilibrium, osmotic pressure, and stereochemistry. Van 't Hoff's work in these subjects helped found the discipline of physical chemistry as it is today.
Biography.
The third of seven children, Van 't Hoff was born in Rotterdam, Netherlands. His father was Jacobus Henricus van 't Hoff, Sr., a physician, and his mother was Alida Kolff van 't Hoff. From a young age, he was interested in science and nature, and frequently took part in botanical excursions. In his early school years, he showed a strong interest in poetry and philosophy. He considered Lord Byron to be his idol.
Against the wishes of his father, Van 't Hoff chose to study chemistry. First, he enrolled at Delft University of Technology in September 1869, and studied until 1871, when he passed his final exam on at 8 July and obtained a degree of chemical technologist. He passed all his courses in two years, although the time assigned to study was three years. Then he enrolled at University of Leiden to study chemistry. He then studied in Bonn, Germany with Friedrich Kekulé and in Paris with C. A. Wurtz. He received his doctorate under Eduard Mulder at the University of Utrecht in 1874.
In 1878, Van 't Hoff married Johanna Francina Mees. They had two daughters, Johanna Francina (b. 1880) and Aleida Jacoba (b. 1882), and two sons, Jacobus Henricus van 't Hoff III (b. 1883) and Govert Jacob (b. 1889). Van 't Hoff died at the age of 58, on 1 March 1911, at Steglitz, near Berlin, from tuberculosis.
Career.
Van 't Hoff earned his earliest reputation in the field of organic chemistry. In 1874, he accounted for the phenomenon of optical activity by assuming that the chemical bonds between carbon atoms and their neighbors were directed towards the corners of a regular tetrahedron. This three-dimensional structure accounted for the isomers found in nature. He shares credit for this with the French chemist Joseph Le Bel, who independently came up with the same idea.
Three months before his doctoral degree was awarded Van 't Hoff published this theory, which today is regarded as the foundation of stereochemistry, first in a Dutch pamphlet in the fall of 1874, and then in the following May in a small French book entitled "La chimie dans l'espace". A German translation appeared in 1877, at a time when the only job van 't Hoff could find was at the Veterinary School in Utrecht. In these early years his theory was largely ignored by the scientific community, and was sharply criticized by one prominent chemist, Hermann Kolbe. Kolbe wrote:
"A Dr. J. H. van ’t Hoff of the Veterinary School at Utrecht has no liking, apparently, for exact chemical investigation. He has considered it more convenient to mount Pegasus (apparently borrowed from the Veterinary School) and to proclaim in his "‘La chimie dans l’espace’" how, in his bold flight to the top of the chemical Parnassus, the atoms appeared to him to be arranged in cosmic space." However, by about 1880 support for van 't Hoff's theory by such important chemists as Johannes Wislicenus and Viktor Meyer brought recognition.
In 1884, Van 't Hoff published his research on chemical kinetics, titled "Études de Dynamique chimique" ("Studies in Chemical Dynamics"), in which he described a new method for determining the order of a reaction using graphics and applied the laws of thermodynamics to chemical equilibria. He also introduced the modern concept of chemical affinity. In 1886, he showed a similarity between the behaviour of dilute solutions and gases. In 1887, he and German chemist Wilhelm Ostwald founded an influential scientific magazine named "Zeitschrift für physikalische Chemie" ("Journal of Physical Chemistry"). He worked on Svante Arrhenius's theory of the dissociation of electrolytes and in 1889 provided physical justification for the Arrhenius equation. In 1896, he became a professor at the Prussian Academy of Sciences in Berlin. His studies of the salt deposits at Stassfurt were an important contribution to Prussia's chemical industry.
Van 't Hoff became a lecturer in chemistry and physics at the Veterinary College in Utrecht. He then worked as a professor of chemistry, mineralogy, and geology at the University of Amsterdam for almost 18 years before eventually becoming the chairman of the chemistry department. In 1896, Van 't Hoff moved to Germany, where he finished his career at the University of Berlin in 1911. In 1901, he received the first Nobel Prize in Chemistry for his work with solutions. His work showed that very dilute solutions follow mathematical laws that closely resemble the laws describing the behavior of gases.
Honours and awards.
In 1885, Van 't Hoff was appointed as a member of the Royal Netherlands Academy of Sciences. Other distinctions include honorary doctorates from Harvard and Yale (1901), Victoria University, Manchester University (1903), and University of Heidelberg (1908). He was awarded the Davy Medal of the Royal Society in 1893 (along with Le Bel), and the Helmholtz Medal of the Prussian Academy of Sciences (1911). He was also appointed "Chevalier de la Légion d'honneur" (1894) and "Senator der Kaiser-Wilhelm-Gesellschaft" (1911). Van 't Hoff became an honorary member of the British Chemical Society in London, the Royal Dutch Academy of Sciences (1892), American Chemical Society (1898), and the Académie des Sciences, in Paris (1905). Of his numerous distinctions, Van 't Hoff regarded winning the first Nobel Prize in Chemistry as the culmination of his career. 
References.
</dl>

</doc>
<doc id="42910" url="http://en.wikipedia.org/wiki?curid=42910" title="JavaServer Pages">
JavaServer Pages

JavaServer Pages (JSP) is a technology that helps software developers create dynamically generated web pages based on HTML, XML, or other document types. Released in 1999 by Sun Microsystems, JSP is similar to PHP, but it uses the Java programming language.
To deploy and run JavaServer Pages, a compatible web server with a servlet container, such as Apache Tomcat or Jetty, is required.
Overview.
Architecturally, JSP may be viewed as a high-level abstraction of Java servlets. JSPs are translated into servlets at runtime; each JSP servlet is cached and re-used until the original JSP is modified.
JSP can be used independently or as the view component of a server-side model–view–controller design, normally with JavaBeans as the model and Java servlets (or a framework such as Apache Struts) as the controller. This is a type of Model 2 architecture.
JSP allows Java code and certain pre-defined actions to be interleaved with static web markup content, with the resulting page being compiled and executed on the server to deliver a document. The compiled pages, as well as any dependent Java libraries, use Java bytecode rather than a native software format. Like any other Java program, they must be executed within a Java virtual machine (JVM) that integrates with the server's host operating system to provide an abstract platform-neutral environment.
JSPs are usually used to deliver HTML and XML documents, but through the use of OutputStream, they can deliver other types of data as well.
The Web container creates JSP implicit objects like pageContext, servletContext, session, request & response.
Syntax.
JSP pages use several delimiters for scripting functions. The most basic is <% ... %>, which encloses a JSP "scriptlet." A scriptlet is a fragment of Java code that is run when the user requests the page. Other common delimiters include <%= ... %> for "expressions," where the scriptlet and delimiters are replaced with the result of evaluating the expression, and "directives", denoted with <%@ ... %>.
Java code is not required to be complete or self-contained within its scriptlet element block, but can straddle markup content providing the page as a whole is syntactically correct. For example, any Java if/for/while blocks opened in one scriptlet element must be correctly closed in a later element for the page to successfully compile. Markup which falls inside a split block of code is subject to that code, so markup inside an "if" block will only appear in the output when the "if" condition evaluates to true; likewise, markup inside a loop construct may appear multiple times in the output depending upon how many times the loop body runs.
The following would be a valid for loop in a JSP page:
The output displayed in the user's web browser would be:
<samp style="padding-left:0.4em; padding-right:0.4em; color:#666666;" {lang="" xml:lang=" >Counting to three:
This number is 1.
This number is 2.
This number is 3.
OK.</samp>
Expression Language.
Version 2.0 of the JSP specification added support for the Expression Language (EL), used to access data and functions in Java objects. In JSP 2.1, it was folded into the Unified Expression Language, which is also used in JavaServer Faces.
An example of EL syntax:
 The value of "variable" in the object "javabean" is ${javabean.variable}.
Additional tags.
The JSP syntax adds additional tags, called JSP actions, to invoke built-in functionality. Additionally, the technology allows for the creation of custom JSP "tag libraries" that act as extensions to the standard JSP syntax. One such library is the JSTL, with support for common tasks such as iteration and conditionals (the equivalent of "for" and "if" statements in Java.)
Compiler.
A JavaServer Pages compiler is a program that parses JSPs, and transforms them into executable Java Servlets. A program of this type is usually embedded into the application server and run automatically the first time a JSP is accessed, but pages may also be precompiled for better performance, or compiled as a part of the build process to test for errors.
Some JSP containers support configuring how often the container checks JSP file timestamps to see whether the page has changed. Typically, this timestamp would be set to a short interval (perhaps seconds) during software development, and a longer interval (perhaps minutes, or even never) for a deployed Web application.
Criticism.
In 2000, Jason Hunter, author of "Java Servlet Programming", criticized JSP for either tempting or requiring the programmer to mix Java code and HTML markup, although he acknowledged it would "wean people off" of Microsoft's Active Server Pages. Later, he added a note to his site saying that JSP had improved since 2000, but also cited its competitors, Apache Velocity and Tea (template language).

</doc>
<doc id="42914" url="http://en.wikipedia.org/wiki?curid=42914" title="Concordia University">
Concordia University

Concordia University (commonly referred to as Concordia) is a Canadian public comprehensive university with campuses and facilities in Montreal, Quebec, Canada.
Founded in 1974 following the merger of Loyola College and Sir George Williams University, Concordia is one of the two universities in Montreal where English is the primary language of instruction. As of the 2011-2012 academic year, there were 45,954 students enrolled at Concordia, making the university among the largest in Canada by enrollment. The university has two campuses, set approximately seven km apart: Sir George Williams Campus in the downtown core of Montreal, in an area known as Quartier Concordia and Loyola Campus in the residential district of Notre-Dame-de-Grâce. With four faculties, a school of graduate studies and numerous colleges, centres and institutes, Concordia offers over 300 undergraduate and 100 graduate programs and courses.
The university was ranked 11th among Canada's comprehensive universities in the Maclean's 24th annual rankings. Internationally, Concordia was ranked 461-470th overall in the 2014 QS World University Rankings and is also included in Times Higher Education's list of the top 100 universities worldwide under 50 years old. Nationally, the 2012 Higher Education Strategy Associates' University Rankings placed Concordia 9th in the field of social science and 20th in science and engineering. The university's John Molson School of Business is consistently ranked within the top ten Canadian business schools, and within the top 100 worldwide. Furthermore, Concordia was ranked 7th among Canadian and 229th among world universities in the International Professional Classification of Higher Education Institutions, a worldwide ranking compiled by the École des Mines de Paris that uses as its sole criterion the number of graduates occupying the rank of Chief Executive Officer at Fortune 500 companies.
Concordia is a non-sectarian and coeducational institution, with over 175,000 living alumni worldwide.
The University is a member of the Association of Universities and Colleges of Canada, the International Association of Universities, the Association of Commonwealth Universities, the Canadian Association of Research Libraries, the Canadian University Society for Intercollegiate Debate as well as the Canadian Bureau for International Education and the Canadian University Press. The university's varsity teams, known as the Stingers, compete in the Quebec Student Sport Federation of Canadian Interuniversity Sport.
History.
Although the roots of its founding institutions go back more than 160 years, Concordia University was formed on August 24, 1974 through the merger of Loyola College (1896) and Sir George Williams University (1926). Since its inception, Concordia has changed its logo four times.
Loyola College.
Loyola College traces its roots to an English-language program at the Jesuit Collège Sainte-Marie de Montréal (today part of the Université du Québec à Montréal) at the Sacred Heart Convent. In 1896, Loyola College was established at the corner of Bleury Street and Saint Catherine Street. Loyola College was named in honour of Ignatius of Loyola, founder of the Society of Jesus. On March 10, 1898, the institution was incorporated by the Government of Quebec and became a full-fledged college. The same year, following a fire, the college was relocated, further west on Drummond Street, south of Saint Catherine. Although founded as a "collège classique" (the forerunners of Quebec's college system), Loyola began granting university degrees through Université Laval in 1903.
The college moved into the present west-end campus on Sherbrooke Street West in Notre-Dame-de-Grâce in 1916. The School of Sociology opened in 1918. In 1920, the institution became affiliated with the Université de Montréal, which began granting degrees instead of Université Laval.
Memorial bronze honour roll plaques in the Entrance hall, administrative offices are dedicated to those from Loyola College who fought in the First and Second World Wars and the Korean War. 
The inter-war period was marked by the shift of education in the institution, the "collège classique" education was replaced by humanistic education (Liberal Arts College) in 1940, and Loyola became a four-year university. Loyola College never became a chartered university, and never had the ability to grant its own university degrees. Theology and philosophy were subjects taught to all students until 1972.
In 1940, the Faculty of Science and the Department of Engineering, which became a faculty in 1964, were created. In addition to providing the same undergraduate programs as other colleges, the institution also offered innovative fields of study at the time, such as exercise science and communication studies. Students could enroll in Academic majors starting in 1953 and honors programs in 1958. Students graduating from Loyola could afterwards pursue graduate-level education in other universities, with a few earning Rhodes Scholarships.
Starting in 1958, Loyola also began offering its first evening courses for students not being able to go to school full-time. New courses were given in library science and faith community nursing. Since its creation, Loyola College had welcomed almost exclusively young English-speaking Catholic men as students. It became co-ed in 1959 and became less homogeneous with the ever increasing number of foreign students.
Obtaining a university charter was an important issue in the 1960s. Although many wanted the Loyola College to become Loyola University, the Quebec government preferred to annex it to Sir George William University. Negotiations began in 1968 and ended with the creation of Concordia University on August 24, 1974.
Sir George Williams University.
In 1851, the first YMCA in North America was established on Ste-Helene street in Old Montreal. Beginning in 1873, the YMCA offered evening classes to allow working people in the English-speaking community to pursue their education while working during the day. Sixty years later, the Montreal YMCA relocated to its current location on Stanley Street in Downtown Montreal. In 1926, the education program at the YMCA was re-organized as Sir George Williams College, named after George Williams, founder of the original YMCA in London, upon which the Montreal YMCA was based. In 1934, Sir George Williams College offered the first undergraduate credit course in adult education in Canada.
The Sir George Williams College became Sir George Williams University (SGWU) in 1948, when it received a university charter from the provincial government, though it remained the education arm of the Montreal YMCA. SGWU expanded into its first standalone building, the Norris Building, in 1956. It established a Centre for Human Relations and Community Studies in 1963. SGWU continued to hold classes in the YMCA building until the construction of the Henry F. Hall Building in 1966.
The university gained international attention in 1969, when a group of students occupied the Hall Building's 9th floor computer lab (see Sir George Williams Computer Riot).
Following several years of discussions and planning, Sir George Williams University merged with Loyola College to create Concordia University in 1974. Concordia provided students with representative student organizations and greater power over administrative decisions at the University.
Merger.
In 1968, in the wake of the Parent Commission Report, which recommended for the secularization of Quebec's educational system, the Quebec government asked Loyola College and Sir George Williams University to consider some form of union. The proposed merger was discussed by the "Loyola-Sir George Williams Joint Steering Committee", a committee created to analyze all forms of possible mergers of the two institutions. It was proposed, in 1969, to create a university federation which allowed students to take courses at both campuses without paying additional fees. There is also mention of a shuttle bus service linking the remote facilities 7 km apart.
Criticized for the difficulties encountered by the cohesion of the various departments and faculties, this option was set aside, but not totally rejected by the Joint Steering Committee. The "Joint Committee of Representatives of the Board of Trustees of Loyola College and the Board of Governors of Sir George Williams University" was formed in December 1971 and produced in the fall of 1972, a document outlining the basis of a university with two campuses. While a number of possible models were considered, including that of a loose federation, the solution finally adopted was that of an integrated institution, Concordia University, operating under the existing charter of Sir George Williams University. Following several revisions in November 1972, the document became the main plan of the proposed merger. It was accepted by both institutions, which begun the process of consolidating their operations.
In early 1973, the two institutions announced the merger would take place that fall. However, legal and administrative procedures delayed the merger for another year. On August 24, 1974, the Government of Quebec recognized the merger, thus creating Concordia University. The name was taken from the motto of the city of Montreal, "Concordia salus" (meaning 'well-being through harmony').
"When you join together two lively institutions, each with its own philosophies and ways of doing things, each firmly dedicated to freedom of thought and speech, you must expect a measure of friction. We look forward now to a new period of creative friction."—Concordia Rector and Vice-Chancellor John O'Brien, on the finalization of the merger, August 16, 1974, 
Post merger.
The legal existence of Concordia dates from August 24, 1974. The integration of the various faculties of the two institutions into a coherent whole took several years. The five faculties of the new university were a combination of existing faculties and departments prior to the merger. There was a Faculty of Commerce, a Faculty of Science and Faculty of Arts at Sir George William University. Additionally, there was a Faculty of Arts and Science from Loyola College. The Faculty of Engineering of both institutions had previously been combined.
The Faculty of Fine Arts was created in 1976.
The first phase of combination of the Faculties of Arts and Science began in 1977 and ended in 1985.
In the late 1980s, the Vanier Library on the Loyola campus was enlarged, while, in 1992, the library on Sir George Williams campus moved to the new JW McConnell Building. The Norris Building was closed the same year.
On August 24, 1992, Valery Fabrikant, a Mechanical Engineering professor, shot five colleagues, killing four, on the ninth floor of the Hall Building. Fabrikant was convicted of the murders and sentenced to life imprisonment. The university erected a memorial to the slain professors (four granite tables) in the Hall Building lobby.
Starting in 1998, the University entered a major phase of expansion to meet its growing student enrollment. In August 2003, Concordia inaugurated the Richard J. Renaud Science Complex on Loyola campus.
On September 9, 2002, a scheduled visit from the then former (and now current) Israeli Prime Minister Benjamin Netanyahu was cancelled after Montreal Police and pro-Palestinian protestors clashed inside the Henry F. Hall Building.
In 2005, the University launched a major urban redevelopment project in the neighbourhood surrounding the Sir George Williams campus known as the Quartier Concordia. That same year, the Engineering, Computer Science and Visual Arts Complex opened its doors on Saint Catherine Street West between Guy Street and Mackay Street.
In September 2009, the University marked the opening of the new building for the John Molson School of Business.
Campuses.
The university has two campuses, set approximately seven km apart: Sir George Williams Campus in the downtown core of Montreal, in an area known as Quartier Concordia (at Guy-Concordia Metro station), and Loyola Campus in the residential west-end district of Notre-Dame-de-Grâce. They are connected by free shuttle-bus service for students, faculty and staff.
Libraries, Archives and Galleries.
Concordia University has two library locations, Webster Library located in the McConnell Building of the Sir George Williams Campus and Vanier Library on the Loyola Campus. Concordia Libraries house several special and unique collections including the Azrieli Holocaust Collection and the Irving Layton Collection. Most Special Collections are located in the Vanier Library. The Libraries also maintains the University's institutional repository, Spectrum. The Concordia Libraries are members of the Canadian Association of Research Libraries. Concordia University Libraries also has partnerships with the Canadian Research Knowledge Network and The Data Liberation Initiative.
Corcordia University's Hall Building houses The Leonard & Bina Ellen Art Gallery. Samuel Schecter, an art enthusiast and businessman, set up two funds in 1962 to be used for the purchase of Canadian art at Sir George Williams University and at Loyola College (Montreal). When Sir George Williams University and Loyola College merged under the name Concordia in 1974 their respective art collections were also combined. The Collection of the Leonard & Bina Ellen Gallery consists of 1700 paintings, sculptures, prints, photographs and videos, many of the works by 20th-century Canadian artists.
The Concordia University Archives house official records of, or relating to, or people/activities connected with Concordia University and its two founding institutions. The collection consists of manuscripts, texts, photographs, audio-visual material and artifacts.
New buildings.
In 2001, Concordia embarked on a mission to develop and expand the quality of the downtown campus, and to revive the west end in Montreal.
The university has also acquired the historic Grey Nuns motherhouse near its Sir George Williams Campus, for $18 million. Built in 1871, it would alone double the size of the current downtown campus. From 2007 to 2022, the university will begin occupying the building in 4 separate phases. The large property will house the faculty of Fine Arts and possibly the Mel Hoppenheim School of Cinema, and other departments. Currently the Grey Nuns building is only partially owned by Concordia (about 1/3 of the building on Saint-Mathieu Road), however full control of the building will be given to Concordia University in 2011. Concordia Residence Life currently houses nearly 250 students each year in the Grey Nuns building. The dorm-rooms are among the largest in the country, as many of the rooms have been transformed from when the section of the Grey Nuns building was occupied by the Grey Nuns. The site was designated a National Historic Site of Canada in 2011.
The Integrated Engineering, Computer Science and Visual Arts Complex at Saint Catherine Street and Guy Street was opened in September 2005. The building is directly connected to the Guy-Concordia metro station and also houses Le Gym, a facility of Athletics and Recreation. Across the street, the 100-year-old TD Canada Trust building was donated to Concordia in 2005 by the Toronto-Dominion Bank. The university had planned to begin using this space in 2006.
Construction of the new John Molson School of Business Building that is located on the corner of Guy and de Maisonneuve streets began in February 2007. The Quebec Minister of Education, Recreation and Sports, Jean-Marc Fournier, on October 30, 2006 announced an investment of $60 million towards the construction of the new building. The minister made the announcement during a ceremony at Concordia. The government's $60 million represents about half of the total construction costs. Construction started on January 22, 2006 and the building was completed and opened in September 2009. The fifteen-story building now houses the JMSB's 6,000 full and part-time students under the same roof for the very first time. The Departments of Contemporary Dance, Theater, and Music at Concordia have also moved into the new JMSB building. It is connected to the EV building by a tunnel under Guy Street.
In April 2010, a 120-metre tunnel completed the underground connections of the Guy-Concordia metro station with the Hall Building and the McConnell Library building.
Quartier Concordia.
Quartier Concordia is a neighbourhood redevelopment project centred around Concordia University's Sir George Williams campus in downtown Montreal, Quebec, Canada. Bordered by Sherbrooke Street, Saint-Mathieu Street, René Lévesque Boulevard and Bishop Street, the district is designed to be a green urban campus that will improve the use and quality of public places and spaces, student life on campus and transportation.
As part of the redesign, the small Norman Bethune Square has been redesigned and enlarged. Sidewalks in the area will also be widened, with additional trees.
As of September 2010, an underground tunnel links the university's Hall and J.W. McConnell buildings with the Guy-Concordia metro station. The hallway was completed in Spring 2010. However, a project to create a green space on Mackay Street has been put on hold.
Academics.
Students enter the university in September, or, in some cases, in January or May. An undergraduate degree normally takes three or four years studying full-time to complete, a Master's takes from a year and a half to three years, and a Ph.D. is at least four years long. Certificates and diplomas usually take no longer than a year and a half to complete.
Concordia has more than 285 undergraduate programs, divided into four faculties. The faculties are the Faculty of Arts and Science, the Faculty of Engineering and Computer Science, the Faculty of Fine Arts and the John Molson School of Business Students are normally enrolled in one of these Faculties, but they may take courses from any of the others as part of their studies. Class sizes vary from 85-400 students.
The School of Graduate Studies offers about 70 programs leading to Master's and doctoral degrees, as well as graduate diplomas and certificates for professionals seeking to upgrade their knowledge and skills.
The School of Extended Learning offers programs and services designed to make it easier for students to attend the university and be successful at their studies.
The administers more than 35 bachelor's and master's programs in an alternating co-op work study format. Concordia's co-op programs enable students to enrich their learning by participating in career-relevant 12-17 week full-time, paid work terms. Depending on their faculty and major, co-op students will usually graduate with a minimum of 12 months of academically relevant work experience. There are also Industrial Experience and Professional Experience options in certain disciplines that enable students to participate in a summer-only work term. Concordia is a member of the Canadian Association for Co-operative Education .
Faculty of Arts and Science.
Concordia University's Faculty of Arts and Science contains 21 departments in the humanities, sciences and social sciences at the undergraduate and graduate levels. There are over 293 programs, offering more than 2,400 courses. There are 500 full-time and 400 part-time faculty members. During the 2010-2011 academic year, there were 15,767 undergraduate and 2,103 graduate students enrolled in the faculty.
In addition to regular academic programs, the Faculty of Arts and Science also includes three colleges, two schools and one institute. These are the Liberal Arts College, the Loyola College for Diversity and Sustainability, the School of Community and Public Affairs, the School of Canadian Irish Studies, the Science College and the Simone de Beauvoir Institute.
The Loyola College for Diversity and Sustainability (formerly Loyola International College) is an interdisciplinary college of Concordia University on the Loyola campus, the original site of Loyola College. It offers minor programs in "Diversity and the Contemporary World" and "Sustainability Studies".
At the undergraduate level, the Faculty of Arts and Science offers both Bachelor of Arts (B.A.) and Bachelor of Science (BSc) programs with majors ranging from economics, political science and sociology to actuarial mathematics, biology and ecology.
Faculty of Engineering and Computer Science.
The Faculty of Engineering and Computer Science (ENCS) offers 86 undergraduate and graduate-level programs in the following disciplines: Building Engineering, Civil Engineering, Computer Engineering, Computer Science, Electrical Engineering, Industrial Engineering, Information Systems Security, Mechanical Engineering, Quality Systems Engineering and Software Engineering. The engineering programs are all accredited by the Canadian Engineering Accreditation Board (CAEB). During the 2010-2011 academic year, there were 3,501 undergraduate and 2,438 graduate students enrolled in the faculty.
The Troitsky Bridge Building Competition brings together engineering students from across Canada and parts of the United States. Teams of students representing their universities must build a 1-metre-long bridge using only regular popsicle sticks, toothpicks, dental floss, and white glue. A panel of judges grades the bridges based on originality and presentation while a hydraulic loading device is used to determine the maximum load and performance.
Faculty of Fine Arts.
The Faculty of Fine Arts offers 76 programs at the undergraduate and graduate levels. It includes nine departments and three research institutes. During the 2010-2011 academic year, there were 3,153 undergraduate and 555 graduate students enrolled in the faculty. Among the departments is the The Mel Hoppenheim School of Cinema. It is informally identified as MHSoC, and accepts 200 students a year, for study in the fields of animation, film production and film studies. It is the largest, university-based centre for the study of film animation, film production and film studies in Canada.
John Molson School of Business.
The John Molson School of Business (JMSB) (formerly the Faculty of Commerce and Administration) offers 48 different programs at the undergraduate and graduate levels from six different departments. The departments are Accountancy, Decision Sciences and MIS, Finance, International Business, Management and Marketing. During the 2010-2011 academic year, there were 7,508 undergraduate students and 1,470 graduate students enrolled as well as 37,788 alumni. The JMSB is accredited by the Association to Advance Collegiate Schools of Business (AACSB). The business school has been located in a LEED silver-certified building.
Reputation.
The Academic Ranking of World Universities ranked Concordia University 19-23 overall in Canada. In addition, the Maclean's Guide to Canadian Universities ranked Concordia 11th nationally out of comprehensive universities in its 2015 edition. On an international scale, QS World University Rankings ranked Concordia 461-470 overall. Within specific fields in the QS World University Rankings, Concordia placed 51-100 for the fields of Education and Training and English Language and Literature, 101-150 for Communication and Media Studies, and 151-200 in the world for the field of Accounting and Finance. Additionally, Concordia placed 151-200 in the world for the fields of Linguistics, Psychology and Sociology. Also, Times Higher Education World University Rankings ranked Concordia's arts and humanities programs 79th worldwide. Concordia University was ranked 91st in Times Higher Education World University Rankings' "100 Under 50" ranking. Concordia University's John Molson School of Business has fared well in academic rankings. In 2012, The Economist ranked JMSB's Master of Business Administration program 78th in the world. The Academic Ranking of World Universities ranked Concordia University 101-150 worldwide in Economics and Business.
Concordia University has also gained a considerable level of prestige in social sciences and humanities. Higher Education Strategy Associates' rankings of Canadian universities in 2012 placed Concordia 9th nationally in social sciences and humanities. Concordia placed 20th in Canada in natural sciences and engineering according to rankings of Canadian universities by the Higher Education Strategy Associates. Finally, Concordia's computer science programs were ranked 151-200 worldwide by ARWU. Also, in 2014, LinkedIn ranked schools based on how successful recent graduates have been at landing desirable jobs. For the Software Development category, Concordia University was ranked as the sixth best university in Canada.
Student life.
Athletics.
Concordia University's athletic teams are called the Concordia Stingers. They compete with other schools in Canadian Interuniversity Sport, and more specifically, in the Quebec Student Sports Federation and the Quebec University Football League. The university has ten varsity teams. In the fall, teams compete in Canadian football, men's and women's soccer, men's and women's rugby union and sport wrestling. There are female and male wrestlers on the team from year to year, however they compete as one team. In the winter, teams compete in men's and women's ice hockey and men's and women's basketball.
Concordia won a national championship in 1999, when the women's hockey team beat the University of Alberta in the final game of the season. Recently, the Stingers beat Cape Breton University Capers 12-2 and won the 2009 National Baseball Crown.
Student organizations.
The Concordia Student Union (usually referred to as the CSU) is the organization representing undergraduate students at Concordia University in Montreal, Quebec, Canada. Its membership totals more than 33,000 students. Concordia students voted in favor of accreditation of their student union in a referendum in December 2000. As a result, the CSU is now legally accountable only to its student constituents.
Another noteworthy aspect of Concordia University is the number of longstanding fee-levy groups which provide numerous services, funded by the student population in the form of per-credit fees. These include the People's Potato which offers a four-course vegan meal, the anti-capitalist grocery store, The Frigo Vert, and the Coop Bookstore.
Concordia University has a campus radio station, (CJLO) and television station, (CUTV). Concordia also has three student-run newspapers, "The Link", "The Concordian" and French-language "L'Organe". "The Concordian" and "L'Organe" are members of Canadian University Press (CUP). The University also assists in the publishing of the only student-run, bilingual literary/arts magazine ', founded in 2002, as well as arts magazine '. "The Link" left the CUP network in 2012.
Concordia University is home to local and international fraternities and sororities. The Delta Phi Epsilon Sorority, represented by the Beta Pi chapter, was established at Concordia in 1994. The Zeta Tau Omega sorority (ZTΩ) was founded in 1968 by six women studying at Montreal. Mu Omicron Zeta fraternity, commonly referred to as MOZ (pronounced like "moes"), was founded in 1992. The Brotherhood of Omicron is another locally based fraternity at Concordia. Tau Kappa Epsilon (TKE) Fraternity has its Kappa Chi (KX) Chapter at Concordia, which was founded in 1967 at Loyola College.
Notable alumni and faculty.
Concordia's alumni and faculty have achieved fame for their accomplishments in many fields. Distinguished alumni include, a former governor general (Georges Vanier), a former prime minister of Dominica (Rosie Douglas), presidents and Chief Executive Officers of major businesses (Dominic D'Alessandro, Mireille Gingras, Gerald T. McCaughey), authors (E. Annie Proulx, Mordecai Richler, Nino Ricci, Chandra Venugopal, James Cummins), political leaders and ministers, academics Kim Sawchuk, scientists, actors (Adam Kelly), filmmakers (Will Arnett, Moyra Davey, René Balcer, Peter Lenkov, Alex Rice, Lynne Stopkewich, B. P. Paquette, Donald Tarlton, James Tupper, Steven Woloshen), and musicians (Emily Haines, Prita Chhabra, Régine Chassagne, Richard Reed Parry, Amy Millan, Matthew Otto of Majical Cloudz).

</doc>
<doc id="42918" url="http://en.wikipedia.org/wiki?curid=42918" title="JSP">
JSP

JSP may stand for:

</doc>
<doc id="42922" url="http://en.wikipedia.org/wiki?curid=42922" title="Comparison of Java and C++">
Comparison of Java and C++

This is a comparison of the Java programming language with the C++ programming language.
Design aims.
The differences between the C++ and Java programming languages can be traced to their heritage, as they have different design goals.
The different goals in the development of C++ and Java resulted in different principles and design tradeoffs between the languages.
The differences are as follows :
Language features.
Templates vs. generics.
Both C++ and Java provide facilities for generic programming, templates and generics, respectively. Although they were created to solve similar kinds of problems, and have similar syntax, they are actually quite different.
Miscellaneous.
An example comparing and exists in Wikibooks.
Performance.
In addition to running a compiled Java program, computers running Java applications generally must also run the Java virtual machine (JVM), while compiled C++ programs can be run without external applications. Early versions of Java were significantly outperformed by statically compiled languages such as C++. This is because the program statements of these two closely related languages may compile to a few machine instructions with C++, while compiling into several byte codes involving several machine instructions each when interpreted by a JVM. For example:
Since performance optimization is a very complex issue, it is very difficult to quantify the performance difference between C++ and Java in general terms, and most benchmarks are unreliable and biased. And given the very different natures of the languages, definitive qualitative differences are also difficult to draw. In a nutshell, there are inherent inefficiencies as well as hard limitations on optimizations in Java given that it heavily relies on flexible high-level abstractions, however, the use of a powerful JIT compiler (as in modern JVM implementations) can mitigate some issues. And, in any case, if the inefficiencies of Java are too much to bear, compiled C or C++ code can be called from Java by means of the JNI.
Certain inefficiencies that are inherent to the Java language itself include, primarily:
However, there are a number of benefits to Java's design, some realized, some only theorized:
Additionally, some performance problems exist in C++ as well:
Official standard and reference of the language.
Language specification.
The C++ language is defined by "ISO/IEC 14882", an ISO standard, which is published by the "ISO/IEC JTC1/SC22/WG21" committee. The latest, post-standardization draft of C++11 is available as well.
The C++ language evolves through an open steering committee called the C++ Standards Committee. The committee is composed of the creator of C++ Bjarne Stroustrup, the convener Herb Sutter, and other prominent figures, including many representatives of industries and user-groups (i.e., the stake-holders). Being an open committee, anyone is free to join, participate, and contribute proposals for upcoming releases of the standard and technical specifications. The committee now aims to release a new standard every few years, although in the past strict review processes and discussions have meant longer delays between publication of new standards (1998, 2003, and 2011).
The Java language is defined by the "Java Language Specification", a book which is published by Oracle.
The Java language continuously evolves through a process called the Java Community Process, and the world's programming community is represented by a group of people and organizations - the Java Community members—which is actively engaged into the enhancement of the language, by sending public requests - the Java Specification Requests - which must pass formal and public reviews before they get integrated into the language.
The lack of a firm standard for Java and the somewhat more volatile nature of its specifications have been a constant source of criticism by stake-holders wanting more stability and more conservatism in the addition of new language and library features. On the other hand, C++ committee also receives constant criticism for the opposite reason, i.e., being too strict and conservative, and taking too long to release new versions.
Trademarks.
"C++" is not a trademark of any company or organization and is not owned by any individual.
"Java" is a trademark of Oracle Corporation.

</doc>
<doc id="42925" url="http://en.wikipedia.org/wiki?curid=42925" title="Pine (email client)">
Pine (email client)

Pine is a freeware, text-based email client which was developed at the University of Washington. The first version was written in 1989, and announced to the public in March, 1992. Source code was available for only the Unix version under a license written by the University of Washington. Pine is no longer under development, and has been replaced by the Alpine client, which is available under the Apache License .
Supported platforms.
There are Unix, Windows, and Linux versions of Pine. The Unix/Linux version is text user interface based—its message editor inspired the text editor Pico. The Windows (and formerly DOS) version is called PC-Pine. WebPine is available to individuals associated with the University of Washington (students, faculty, etc.)—a version of Pine implemented as a web application.
Most moved over to Alpine, however there are still many users of this software.
Etymology.
Many people believe that Pine stands for "Pine Is Not Elm". One of its original authors, Laurence Lundblade, insists this was never the case and that it started off simply as a word and not an acronym, and that his first choice of a backronym for pine would be "Pine Is Nearly Elm". Over time, it was changed by the university to mean "Program for Internet News and E-mail". The original announcement said: "Pine was originally based on Elm, but it has evolved much since, ('Pine Is No-longer Elm')."
Licensing and clones.
Up to version 3.91, the Pine license was similar to BSD, and it stated that
The University registered a trademark for the name Pine.
From version 3.92, the holder of the copyright, the University of Washington, changed the license so that even if the source code was still available, they did "not" allow modifications and changes to Pine to be distributed by anyone other than themselves. They also claimed that even the old license never allowed distribution of modified versions.
The trademark for the Pine name was part of their position in this matter.
In reaction, some developers forked version 3.91 under the name MANA (for "Mail And News Agent") to avoid the trademark issue and the GNU project adopted it as GNU Mana. Richard Stallman claims that the University of Washington threatened to sue the Free Software Foundation for distributing the modified Pine program, resulting in the development of MANA ceasing and no versions being released.
The University of Washington later modified their license somewhat to allow unmodified distribution of Pine alongside collections of free software, but the license still does not conform to the Open Source and the Free Software Guidelines so it is semi-free software, effectively proprietary software.
Alpine.
In 2006, the University of Washington announced that it stopped development of Pine with Pine 4.64, although Pine continues to be supported.
In its place is a new family of email tools based upon Pine, called Alpine and licensed under the Apache License, version 2. November 29, 2006 saw the first public alpha release, which forms a new approach, since the alpha test of Pine was always non-public.
Alpine 1.0 was publicly released on December 20, 2007. Version 2.0 was released in 26 August 2008.

</doc>
<doc id="42927" url="http://en.wikipedia.org/wiki?curid=42927" title="John of Damascus">
John of Damascus

Saint John of Damascus (; Latin: "Ioannes Damascenus"; Arabic: يوحنا الدمشقي‎, ALA-LC: "Yūḥannā ad-Dimashqī"), also known as John Damascene and as Χρυσορρόας / "Chrysorrhoas" (literally "streaming with gold"—i.e., "the golden speaker"; c. 675 or 676 – 4 December 749) was a Syrian monk and priest. Born and raised in Damascus, he died at his monastery, Mar Saba, near Jerusalem.
A polymath whose fields of interest and contribution included law, theology, philosophy, and music, he is said by some sources to have served as a Chief Administrator to the Muslim caliph of Damascus before his ordination. He wrote works expounding the Christian faith, and composed hymns which are still used both liturgically in Eastern Christian practice throughout the world as well as in western Lutheranism at Easter. He is one of the Fathers of the Eastern Orthodox church and is best known for his strong defense of icons. The Catholic Church regards him as a Doctor of the Church, often referred to as the "Doctor of the Assumption" due to his writings on the Assumption of Mary.
The most common source of information for the life of John of Damascus is a work attributed to one John of Jerusalem, identified therein as the Patriarch of Jerusalem. This is an excerpted translation into Greek of an earlier Arabic text. The Arabic original contains a prologue not found in most other translations, and was written by an Arab monk, Michael. Michael explained that he decided to write his biography in 1084 because none was available in his day. However, the main Arabic text seems to have been written by an earlier author sometime between the early 9th and late 10th centuries AD. Written from a hagiographical point of view and prone to exaggeration and some legendary details, it is not the best historical source for his life, but is widely reproduced and considered to contain elements of some value. The hagiographic novel "Barlaam and Josaphat", traditionally attributed to John, is in fact a work of the 10th century.
Family background.
John was born into a prominent family known as Mansour (Arabic: المنصور‎ / "al-Manṣūr", "the victorious one") in Damascus in the 7th century AD. His full name was Yuhanna (or Yanah) ibn Mansur ibn Sarjun (Arabic: منصور بن سرجون‎), named for his grandfather Mansur, who had been responsible for the taxes of the region under the Emperor Heraclius. The lack of documentation attesting to his specific tribal lineage has led a number of scholars to assign him either to the Taghlib or the Kalb, two prominent Bedouin tribes in the Syrian desert. Others suggest that he may have been of Syrian non-Arab origin. Whatever the case, John of Damascus had two names: John, his Christian name, and his Arabic name, given as "Qurein" or "Yana" or "Iyanis".
Eutychius, a 10th-century Melkite patriarch mentions a certain Arab governor of the city who surrendered the city to the Muslims, probably John's grandfather Mansur Bin Sargun. When the region came under Arab Muslim rule in the late 7th century AD, the court at Damascus retained its large complement of Christian civil servants, John's grandfather among them. John's father, Sarjun (Sergius) or Ibn Mansur, went on to serve the Umayyad caliphs. According to John of Jerusalem and some later versions of his life, after his father's death John also served as an official to the caliphal court before leaving to become a monk. This claim, that John actually served in a Muslim court, has been questioned since he is never mentioned in Muslim sources, which however do refer to his father Sarjun (Sergius) as a secretary in the caliphal administration. In addition, John's own writings never refer to any experience in a Muslim court. It is believed that John became a monk at Mar Saba, and that he was ordained as a priest in 735.
Education.
One of the "vitae" describes his father's desire for him to "learn not only the books of the Muslims, but those of the Greeks as well." From this it has been suggested that John may have grown up bilingual. John does indeed show some knowledge of the Quran, which he criticizes harshly.
Other sources describes his education in Damascus as having been conducted in accordance with the principles of Hellenic education, termed "secular" by one source and "Classical Christian" by another. One account identifies his tutor as a monk by the name of Cosmas, who had been kidnapped by Arabs from his home in Sicily, and for whom John's father paid a great price. Under the instruction of Cosmas, who also taught John's orphan friend (the future St. Cosmas of Maiuma), John is said to have made great advances in music, astronomy and theology, soon rivalling Pythagoras in arithmetic and Euclid in geometry. As a refugee from Italy, Cosmas brought with him the scholarly traditions of Western Christianity.
Career.
John had two careers: one as a civil servant for the Caliph in Damascus, and the other as a priest and monk at the Mar Saba monastery near Jerusalem. One source believes John left Damascus to become a monk around 706, when al-Walid I increased the Islamicisation of the Caliphate's administration. Muslim sources only mention that his father Sarjun (Sergius) left the administration around this time, and fail to name John at all. During the next two decades, culminating in the Siege of Constantinople (717-718), the Umayyad Caliphate progressively occupied the borderlands of the Byzantine Empire. An editor of John's works, Father Le Quien, has shown that John was already a monk at Mar Saba before the dispute over iconoclasm, explained below.
In the early 8th century AD, iconoclasm, a movement opposed to the veneration of icons, gained acceptance in the Byzantine court. In 726, despite the protests of St. Germanus, Patriarch of Constantinople, Emperor Leo III (who had forced the emperor to abdicate and himself assumed the throne in 717 immediately before the great siege) issued his first edict against the veneration of images and their exhibition in public places. 
All agree that John of Damascus undertook a spirited defence of holy images in three separate publications. The earliest of these works, his "Apologetic Treatises against those Decrying the Holy Images", secured his reputation. He not only attacked the Byzantine emperor, but adopted a simplified style that allowed the controversy to be followed by the common people, stirring rebellion among those of Christian faith. Decades after his death, John's writings would play an important role during the Second Council of Nicaea (787), which convened to settle the icon dispute.
John's biography recounts at least one episode deemed improbable or legendary. Leo III reportedly sent forged documents to the caliph which implicated John in a plot to attack Damascus. The caliph then ordered John's right hand be cut off and hung up in public view. Some days afterwards, John asked for the restitution of his hand, and prayed fervently to the Theotokos before her icon: thereupon, his hand is said to have been miraculously restored. In gratitude for this miraculous healing, he attached a silver hand to the icon, which thereafter became known as the "Three-handed", or Tricheirousa.
Last days.
John died in 749 as a revered Father of the Church, and is recognized as a saint. He is sometimes called the last of the Church Fathers by the Roman Catholic Church. In 1883 he was declared a Doctor of the Church by Pope Leo XIII.
Veneration.
When the name of Saint John of Damascus was inserted in the General Roman Calendar in 1890, it was assigned to 27 March. The feast day was moved in 1969 to the day of the saint's death, 4 December, the day on which his feast day is celebrated also in the Byzantine Rite calendar, Lutheran Commemorations. and the Anglican Communion and Episcopal Church 
List of works.
Besides his purely textual works, many of which are listed below, John of Damascus also composed hymns, perfecting the canon, a structured hymn form used in Eastern Orthodox church services.
The Arabic translation.
It is believed that the homily on the Annunciation was the first work to be translated into Arabic. We can find a big part of this text in the manuscript 4226 of the Library of Strasbourg (France), a copy achieved in 885 AD.
Later in the 10th century, Antony, superior of the monastery of St. Simon (near Antioch) translated a corpus of saint John Damascene. In his introduction to John's work, Sylvestre patriarch of Antioch (1724-1766) said that Antony was monk at Saint Saba. This could be a misunderstanding of the title Superior of Saint Simon probably because Saint Simon's monastery was in ruins in the 18th century.
Most manuscripts give the text of the letter to Cosmas, the philosophical chapters, the theological chapters and five other small works. Since March 2013, a first edition of this translation is available on the web.
In 1085, Mikhael, a monk from Antioch wrote the Arabic life of the Chrysorrhoas. This work was first edited by Bacha in 1912 and then translated in many languages (German, Russian and English).
Modern English translations.
2 translations exist of the 10th century hagiographic novel "Barlaam and Josaphat", traditionally attributed to John:

</doc>
<doc id="42934" url="http://en.wikipedia.org/wiki?curid=42934" title="Cryostasis (clathrate hydrates)">
Cryostasis (clathrate hydrates)

The term cryostasis was introduced to name the reversible preservation technology for live biological objects which is based on using clathrate-forming gaseous substances under increased hydrostatic pressure and hypothermic temperatures.
Living tissues cooled below the freezing point of water are damaged by the dehydration of the cells as ice is formed between the cells. The mechanism of freezing damage in living biological tissues has been elucidated by Renfret (1968) (Renfret A.P. Cryobiology: some fundamentals in surgical context. In: Cryosurgery. Rand R.W., Rinfret A.P., von Lode H., Eds. Springfield, IL: Charles C. Thomas, 1968) and by Mazur (1984): ice formation begins in the intercellular spaces.
The vapor pressure of the ice is lower than the vapor pressure of the solute water in the surrounding cells and as heat is removed at the freezing point of the solutions, the ice crystals grow between the cells, extracting water from them. As the ice crystals grow, the volume of the cells shrinks, and the cells are crushed between the ice crystals. Additionally, as the cells shrink, the solutes inside the cells are concentrated in the remaining water, increasing the intracellular ionic strength and interfering with the organization of the proteins and other organized intercellular structures. Eventually, the solute concentration inside the cells reaches the eutectic and freezes. The final state of frozen tissues is pure ice in the former extracellular spaces, and inside the cell membranes a mixture of concentrated cellular components in ice and bound water. In general, this process is not reversible to the point of restoring the tissues to life.
Cryostasis utilizes using clathrate-forming gases that penetrate and saturate the biological tissues causing clathrate hydrates formation (under specific pressure-temperature conditions) inside the cells and in the extracellular matrix. Clathrate hydrates are a class of solids in which gas molecules occupy "cages" made up of hydrogen-bonded water molecules. These "cages" are unstable when empty, collapsing into conventional ice crystal structure, but they are stabilised by the inclusion of the gas molecule within them. Most low molecular weight gases (including CH4, H2S, Ar, Kr, and Xe) will form a hydrate under some pressure-temperature conditions.
Clathrates formation will prevent the biological tissues from dehydration which will cause irreversible inactivation of intracellular enzymes.

</doc>
<doc id="42935" url="http://en.wikipedia.org/wiki?curid=42935" title="Detection">
Detection

In general, detection is the extraction of particular information from a larger stream of information without specific cooperation from or synchronization with the sender.
In the history of radio communications, the term "detector" was first used for a device that detected the simple presence or absence of a radio signal, since all communications were in Morse code. The term is still in use today to describe a component that extracts a particular signal from all of the electromagnetic waves present. Detection is usually based on the frequency of the carrier wave, as in the familiar frequencies of radio broadcasting, but it may also involve filtering a faint signal from noise, as in radio astronomy, or reconstructing a hidden signal, as in steganography.
In optoelectronics, "detection" means converting a received optical input to an electrical output. For example, the light signal received through an optical fiber is converted to an electrical signal in a detector such as a photodiode.
In steganography, attempts to detect hidden signals in suspected carrier material is referred to as steganalysis. Steganalysis has an interesting difference from most other types of detection, in that it can often only determine the probability that a hidden message exists; this is in contrast to the detection of signals which are simply encrypted, as the ciphertext can often be identified with certainty, even if it cannot be decoded. 
In the military, detection refers to the special discipline of reconnaissance with the aim to recognize the presence of an object in a location or ambiance.
Finally, the art of detection, also known as "following clues", is the work of a detective in attempting to reconstruct a sequence of events by identifying the relevant information in a situation.

</doc>
<doc id="42937" url="http://en.wikipedia.org/wiki?curid=42937" title="Photodiode">
Photodiode

A photodiode is a semiconductor device that converts light into current. The current is generated when photons are absorbed in the photodiode. A small amount of current is also produced when no light is present. Photodiodes may contain optical filters, built-in lenses, and may have large or small surface areas. Photodiodes usually have a slower response time as their surface area increases. The common, traditional solar cell used to generate electric solar power is a large area photodiode. 
Photodiodes are similar to regular semiconductor diodes except that they may be either exposed (to detect vacuum UV or X-rays) or packaged with a window or optical fiber connection to allow light to reach the sensitive part of the device. Many diodes designed for use specifically as a photodiode use a PIN junction rather than a p–n junction, to increase the speed of response. A photodiode is designed to operate in reverse bias.
Principle of operation.
A photodiode is a p–n junction or PIN structure. When a photon of sufficient energy strikes the diode, it creates an electron-hole pair. This mechanism is also known as the inner photoelectric effect. If the absorption occurs in the junction's depletion region, or one diffusion length away from it, these carriers are swept from the junction by the built-in electric field of the depletion region. Thus holes move toward the anode, and electrons toward the cathode, and a photocurrent is produced. The total current through the photodiode is the sum of the dark current (current that is generated in the absence of light) and the photocurrent, so the dark current must be minimized to maximize the sensitivity of the device.
Photovoltaic mode.
When used in zero bias or "photovoltaic mode", the flow of photocurrent out of the device is restricted and a voltage builds up. This mode exploits the photovoltaic effect, which is the basis for solar cells – a traditional solar cell is just a large area photodiode.
Photoconductive mode.
In this mode the diode is often reverse biased (with the cathode driven positive with respect to the anode). This reduces the response time because the additional reverse bias increases the width of the depletion layer, which decreases the junction's capacitance. The reverse bias also increases the dark current without much change in the photocurrent. For a given spectral distribution, the photocurrent is linearly proportional to the illuminance (and to the irradiance).
Although this mode is faster, the photoconductive mode tends to exhibit more electronic noise. The leakage current of a good PIN diode is so low (<1 nA) that the Johnson–Nyquist noise of the load resistance in a typical circuit often dominates.
Other modes of operation.
Avalanche photodiodes have a similar structure to regular photodiodes, but they are operated with much higher reverse bias. This allows each "photo-generated" carrier to be multiplied by avalanche breakdown, resulting in internal gain within the photodiode, which increases the effective "responsivity" of the device.
A phototransistor is a light-sensitive transistor. A common type of phototransistor, called a photobipolar transistor, is in essence a bipolar transistor encased in a transparent case so that light can reach the "base–collector junction". It was invented by Dr. John N. Shive (more famous for his wave machine) at Bell Labs in 1948,:205 but it wasn't announced until 1950. The electrons that are generated by photons in the base–collector junction are injected into the base, and this photodiode current is amplified by the transistor's current gain β (or hfe). If the emitter is left unconnected, the phototransistor becomes a photodiode. While phototransistors have a higher responsivity for light they are not able to detect low levels of light any better than photodiodes. Phototransistors also have significantly longer response times. Field-effect phototransistors, also known as photoFETs, are light-sensitive field-effect transistors. Unlike photobipolar transistors, photoFETs control drain-source current by creating a gate voltage.
Materials.
The material used to make a photodiode is critical to defining its properties, because only photons with sufficient energy to excite electrons across the material's bandgap will produce significant photocurrents.
Materials commonly used to produce photodiodes include:
Because of their greater bandgap, silicon-based photodiodes generate less noise than germanium-based photodiodes.
Unwanted photodiode effects.
Any p–n junction, if illuminated, is potentially a photodiode. Semiconductor devices such as transistors and ICs contain p–n junctions, and will not function correctly if they are illuminated by unwanted electromagnetic radiation (light) of wavelength suitable to produce a photocurrent; this is avoided by encapsulating devices in opaque housings. If these housings are not completely opaque to high-energy radiation (ultraviolet, X-rays, gamma rays), transistors and ICs can malfunction due to induced photo-currents. Background radiation from the packaging is also significant. Radiation hardening mitigates these effects.
Features.
Critical performance parameters of a photodiode include:
When a photodiode is used in an optical communication system, all these parameters contribute to the "sensitivity" of the optical receiver, which is the minimum input power required for the receiver to achieve a specified "bit error rate".
Applications.
P–n photodiodes are used in similar applications to other photodetectors, such as photoconductors, charge-coupled devices, and photomultiplier tubes. They may be used to generate an output which is dependent upon the illumination (analog; for measurement and the like), or to change the state of circuitry (digital; either for control and switching, or digital signal processing).
Photodiodes are used in consumer electronics devices such as compact disc players, smoke detectors, and the receivers for infrared remote control devices used to control equipment from televisions to air conditioners. For many applications either photodiodes or photoconductors may be used. Either type of photosensor may be used for light measurement, as in camera light meters, or to respond to light levels, as in switching on street lighting after dark.
Photosensors of all types may be used to respond to incident light, or to a source of light which is part of the same circuit or system. A photodiode is often combined into a single component with an emitter of light, usually a light-emitting diode (LED), either to detect the presence of a mechanical obstruction to the beam (slotted optical switch), or to couple two digital or analog circuits while maintaining extremely high electrical isolation between them, often for safety (optocoupler).
Photodiodes are often used for accurate measurement of light intensity in science and industry. They generally have a more linear response than photoconductors.
They are also widely used in various medical applications, such as detectors for computed tomography (coupled with scintillators), instruments to analyze samples (immunoassay), and pulse oximeters.
PIN diodes are much faster and more sensitive than p–n junction diodes, and hence are often used for optical communications and in lighting regulation.
P–n photodiodes are not used to measure extremely low light intensities. Instead, if high sensitivity is needed, avalanche photodiodes, intensified charge-coupled devices or photomultiplier tubes are used for applications such as astronomy, spectroscopy, night vision equipment and laser rangefinding.
Pinned photodiode is not a PIN photodiode, it has p+/n/p regions in it.
It has a shallow P+ implant in N type diffusion layer over a P-type epitaxial substrate layer. It is used in CMOS Active pixel sensor.
Comparison with photomultipliers.
Advantages compared to photomultipliers:
Disadvantages compared to photomultipliers:
Photodiode array.
A one-dimensional array of hundreds or thousands of photodiodes can be used as a position sensor, for example as part of an angle sensor. 
One advantage of photodiode arrays (PDAs) is that they allow for high speed parallel read out since the driving electronics may not be built in like a traditional CMOS or CCD sensor.
References.
 This article incorporates public domain material from websites or documents of the .
</dl>

</doc>
<doc id="42939" url="http://en.wikipedia.org/wiki?curid=42939" title="Autolysin">
Autolysin

An autolysin is an enzyme (EC , "gametolysin, Chlamydomonas cell wall degrading protease", "lysin", "Chlamydomonas reinhardtii metalloproteinase", "gamete lytic enzyme", "gamete autolysin") that hydrolyzes (and breaks down) the components of a biological cell or a tissue in which it is produced. It is similar in function to a lysozyme. This enzyme catalyses the following chemical reaction
This glycoprotein is present in "Chlamydomonas reinhardtii" gametes.
Autolysins exist in all bacteria containing peptidoglycan. The peptidoglycan matrix is very rigid, so these enzymes break down the peptidoglycan matrix in small sections so that growth and division of cells can occur. Autolysins do this by hydrolyzing the β-(1,4) bond between N-acetylmuramic acid and N-acetylglucosamine molecules. Autolysins are naturally produced by peptidoglycan containing bacteria, but excessive amounts will degrade the peptidoglycan matrix and cause the cell to burst due to osmotic pressure. Gram-positive bacteria regulate autolysins with teichoic acid molecules attached to the tetrapeptide of the peptidoglycan matrix.
Tariq Bangash Microbiology Kohat University of Science & Technology 2014
References.
Bao Quoc Tran. Autolysin and its affect on herpes. Microbiology text book ed 5 2012

</doc>
<doc id="42940" url="http://en.wikipedia.org/wiki?curid=42940" title="Biostasis">
Biostasis

Biostasis is the ability of an organism to tolerate environmental changes without having to actively adapt to them. The word is also used as a synonym for cryostasis or cryonics. It is found in organisms that live in habitats that may encounter unfavourable living conditions (i.e. drought, freezing, a change in pH, pressure, or temperature). Insects undergo diapause, which allows them to survive winter and other events. Diapause may be obligatory (required for the insect to survive) or facultative (the insect is able to undergo change before the initiating event arrives).
Medical Biostasis can be put to use in humans to help repair brain damage. 
Depending on where medicine is in the next decade medical biostasis procedures can be performed by trauma surgeons by 2026.

</doc>
<doc id="42942" url="http://en.wikipedia.org/wiki?curid=42942" title="Chitinase">
Chitinase

Chitinases (EC , "chitodextrinase", "1,4-beta-poly-N-acetylglucosaminidase", "poly-beta-glucosaminidase", "beta-1,4-poly-N-acetyl glucosamidinase", "poly[1,4-(N-acetyl-beta-D-glucosaminide)] glycanohydrolase", "(1->4)-2-acetamido-2-deoxy-beta-D-glucan glycanohydrolase") are hydrolytic enzymes that break down glycosidic bonds in chitin.
As chitin is a component of the cell walls of fungi and exoskeletal elements of some animals (including worms and arthropods), chitinases are generally found in organisms that either need to reshape their own chitin or dissolve and digest the chitin of fungi or animals.
Species distribution.
Chitinivorous organisms include many bacteria (Aeromonads, "Bacillus", "Vibrio", among others), which may be pathogenic or detritivorous. They attack living arthropods, zooplankton or fungi or they may degrade the remains of these organisms.
Fungi, such as "Coccidioides immitis", also possess degradative chitinases related to their role as detritivores and also to their potential as arthropod pathogens.
Chitinases are also present in plants (barley seed chitinase: PDB , EC ); some of these are pathogenesis related (PR) proteins that are induced as part of systemic acquired resistance. Expression is mediated by the NPR1 gene and the salicylic acid pathway, both involved in resistance to fungal and insect attack. Other plant chitinases may be required for creating fungal symbioses.
Although mammals do not produce chitin, they have two functional chitinases (Chitotriosidase - CHIT1 and acidic mammalian chitinase - AMCase) as well as chitinase-like proteins (such as YKL-40) that have high sequence similarity but lack chitinase activity.
Function.
Like cellulose, chitin is an abundant biopolymer that is relatively resistant to degradation. It is typically not digested by animal, though certain fish are able to digest chitin. It is currently assumed that chitin digestion by animals requires bacterial symbionts and lengthy fermentations, similar to cellulase digestion by ruminants. Nevertheless, chitinases have been isolated from the stomachs of certain mammals, including humans.
Chitinase activity can also be detected in human blood and possibly cartilage. As in plant chitinases this may be related to pathogen resistance.
Clinical significance.
Human chitinases may be related to allergies, and asthma has been linked to enhanced chitinase expression levels.
Human chitinases may explain the link between some of the most common allergies (dust mites, mold spores - both of which contain chitin) and worm (helminth) infections, as part of one version of the hygiene hypothesis (worms have chitinous mouthparts to hold the intestinal wall). Finally, the link between chitinases and salicylic acid in plants is well established - but there is a hypothetical link between salicylic acid and allergies in humans.
Presence in food.
Chitinase occurs naturally in many common foods. This is at least one cause of the cross-reaction phenomenon in latex-fruit syndrome. Bananas, chestnuts, kiwis, avocados, papaya, and tomatoes, for example, all contain significant levels of chitinase.

</doc>
<doc id="42946" url="http://en.wikipedia.org/wiki?curid=42946" title="Encyclopedia Astronautica">
Encyclopedia Astronautica

The Encyclopedia Astronautica is a reference web site on space travel. A comprehensive catalog of vehicles, technology, astronauts, and flights, it includes information from most countries that have had an active rocket research program, from Robert Goddard to the NASA Space shuttle to the Soviet Shuttle Buran.
It is maintained by space enthusiast and author Mark Wade. He has been collecting such information for most of his life.
On July 12, 2011, a message appeared on the website stating that its content had been taken down due to persistent denial of service attacks; however, the website was restored about two weeks later. The site is also mirrored at an older url.

</doc>
<doc id="42948" url="http://en.wikipedia.org/wiki?curid=42948" title="Lip piercing">
Lip piercing

A lip piercing is a type of body piercing that penetrates the lips or the area surrounding the lips, which can be pierced in a variety of ways.
Procedure and healing.
Approximate healing time for most lip piercings is between 1 to 3 months; however, there is a possibility of serious infection if the piercing is not properly taken care of. After healing is complete, other jewelry may be used. After this time, some scar tissue may be present, but the fistula is normally fully developed and mostly healed. Aftercare consists of hot saline soaks two to three times daily. Soaking the wound for three to five minutes with a weak saline solution softens any blood and lymph discharge attached to the jewelry. Afterwards, taking a hot shower and using clean hands and a small amount of a mild soap such as castile soap removes excess matter from the site. Turning or otherwise moving jewelry on a fresh piercing is not advised, as it can irritate and lengthen swelling and healing time. Diluted mouthwash or salt water solution can also be used after meals along with toothbrushing to help remove debris and flush the piercing and is recommended by practitioners.
Initial jewelry is usually a labret stud or a captive bead ring, manufactured from high-grade surgical stainless steel, implant-grade titanium, or similar lightweight and inert metal. No matter which type of jewelry is used, the jewelry's diameter and length will be intentionally over sized to allow room for initial swelling. After healing, the jewelry can be replaced with a closer-fitting piece.
A home-made saline solution made from non-iodized sea salt and hot distilled or filtered water is a common way to heal a lip piercing and avoid infection. This solution can be used to rinse out the mouth after eating (or the mouth can be rinsed with non-alcoholic, non-antimicrobial mouth wash) and to soak the outside of the piercing. Anything with alcohol, peroxide, iodine, or any strong soaps should be avoided because they may irritate the fresh piercing, and cause additional swelling and trauma during the healing process. Using peroxide, iodine, teatree oil, conventional antibacterial soap, or dish soap can damage or kill the skin in and around the piercing, extending the healing process. The ornament should be periodically cleansed to prevent bacterial plaque accumulation.
Types.
Lip piercings can be placed anywhere around the mouth, but the surface of the lip is not typically pierced itself, except for horizontal lip piercings and canine bites. Piercings in specific positions have certain names. Monroe piercings, for example, are labret studs worn on the upper lip where Marilyn Monroe had her famous beauty mark. Medusa piercings go through the center of the upper lip (the philtrum), perpendicular to the tissue. Labret piercings are pierced with a labret stud and can pierced in the center or off-center. A variation of this is the lowbret, a lower labret. Vertical labret piercings go through the center of the bottom lip, parallel to the tissue. The variation is called the vertical lowbret, which starts inside the mouth between the lower lip and the teeth and travels straight down, exiting on the lower edge of the jawline. Horizontal lip piercings are very rare, and include a horizontal bar on the lower lip that goes through the lip surface. Another variation of the labret is known as the dahlia piercing. The piercings, placed at or very near the corners of the mouth, are named in reference to the murder of Black Dahlia, in which the victim's mouth was cut along the same horizontal line along which these piercings are placed. (See Glasgow smile)
History and culture.
Precolumbian cultures of South America historically used lip piercing called Tembetá. Lip piercing continues to be practiced by many people, the most well-known of which are certain African tribes, who wear large decorative lip plates or discs, usually in the lower lip.
In contemporary society, lip piercings are relatively common. In a study among Israeli young-adults, 4.3% had present or past body piercing (not included earlobe, lip or intra-oral piercing), and 5.7%, 6.2% and 15.7% had present or past lip piercing, body tattooing and intra-oral piercing, respectively.

</doc>
<doc id="42950" url="http://en.wikipedia.org/wiki?curid=42950" title="301">
301

Year 301 (CCCI) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Postumius and Nepotianus (or, less frequently, year 1054 "Ab urbe condita"). The denomination 301 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="42951" url="http://en.wikipedia.org/wiki?curid=42951" title="302">
302

Year 302 (CCCII) was a common year starting on Thursday of the Julian calendar. At the time, it was known as the Year of the Consulship of Constantius and Valerius or, less frequently, year 1055 "Ab urbe condita". The denomination 302 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42952" url="http://en.wikipedia.org/wiki?curid=42952" title="303">
303

Year 303 (CCCIII) was a common year starting on Friday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Valerius and Valerius (or, less frequently, year 1056 "Ab urbe condita"). The denomination 303 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42953" url="http://en.wikipedia.org/wiki?curid=42953" title="Irish Free State">
Irish Free State

 |style="width:1.0em; padding:0 0 0 0.6em;"| - 
 |style="padding-left:0;text-align:left;"| 1922–1936
 |- class="mergedrow"
 |style="width:1.0em; padding:0 0 0 0.6em;"| - ||style="padding-left:0;text-align:left;"|1922–1927|| 
 |- class="mergedrow"
 | style="width:1.0em; padding:0 0 0 0.6em;"|  -  ||style="padding-left:0;text-align:left;"| Until 8 December 1922 
 |  km² ( sq mi)
 |- class="mergedbottomrow"
 | style="width:1.0em; padding:0 0 0 0.6em;"|  -  ||style="padding-left:0;text-align:left;"| After 8 December 1922 
 |  km² ( sq mi)
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |- class="mergedbottomrow"
 |  Ireland<br> United Kingdom
The Irish Free State (Irish: "Saorstát Éireann" ]; 6 December 1922 – 29 December 1937) was the state established in 1922 as a Dominion of the British Commonwealth of Nations under the Anglo-Irish Treaty signed by British and Irish representatives exactly twelve months beforehand. On the day the Irish Free State was established, it comprised the entire island of Ireland; but, as expected, Northern Ireland immediately exercised its right under the treaty to remove itself from the new state. The Irish Free State effectively replaced both the self-proclaimed Irish Republic (founded 21 January 1919) and the Provisional Government of the Irish Free State. W. T. Cosgrave, the first President of the Executive Council of the Irish Free State, had led both of these governments since August 1922.
The Free State came to an end in 1937, when the citizens voted by plebiscite to adopt a new constitution. Under the new constitution the Irish state was named Ireland.
Historical background.
The Easter Rising of 1916, and particularly the execution of fifteen people by firing squad, the imprisonment or internment of hundreds more, and the imposition of martial law caused a profound shift in public opinion towards the republican cause in Ireland. Meanwhile opposition increased to Ireland's participation in World War I in Europe and the Middle East. This came about when the Irish Parliamentary Party supported the Allied cause in World War I in response to the passing of the Third Home Rule Bill in 1914. Many people had begun to doubt whether the Bill, passed by Westminster in September 1914 but suspended for the duration of the war, would ever come into effect. Due to the war situation deteriorating badly on the Western Front in April 1918, which coincided with the publication of the final report and recommendations of the Irish Convention, the British Cabinet drafted a doomed "dual policy" of introducing Home Rule linked to compulsory military service for Ireland which it eventually had to drop. Sinn Féin, the Irish Party and all other Nationalist elements joined forces in opposition to the idea during the Conscription Crisis of 1918. At the same time the Irish Parliamentary lost in support on account of the crisis. Irish republicans felt further emboldened by successful anti-monarchical revolutions in the Russian Empire (1917), the German Empire (1918), and the Austro-Hungarian Empire (1918). In the December 1918 General Election, Sinn Féin won a large majority of the Irish seats in the Westminster parliament of the United Kingdom of Great Britain and Ireland: 73 of the 105 constituencies returned Sinn Féin members (25 uncontested). The Sinn Féin party, founded by Arthur Griffith in 1905, had previously espoused non-violent separatism. Under Éamon de Valera's leadership from 1917, it campaigned aggressively and militantly for an Irish republic.
On 21 January 1919, Sinn Féin MPs (who became known as "Teachta Dála", TDs), refusing to sit at Westminster, assembled in Dublin and formed a single-chamber Irish parliament called Dáil Éireann (Assembly of Ireland). It affirmed the formation of an Irish Republic and passed a Declaration of Independence, 
the irish people is resolved...to promote the common weal, to re-establish justice ...with equal rights and equal opportunity for every citizen. and calling itself "Saorstát Éireann" in Irish. Although the less than overwhelming majority of Irish people accepted this course, America and Soviet Russia were targeted to recognise the Irish Republic internationally.The Message to the Free Nations of the World called on every free nation to support the Irish Republic by recognizing Ireland's national status...the last outpost of Europe towards the West...demanded by the Freedom of the Seas.
Cathal Brugha elected President of the Ministry Pro-Tem warned, "Deputies you understand from this that we are now done with England."
A war for a new independent Ireland.
The War of Independence (1919–21) pitted the army of the Irish Republic, the Irish Republican Army (known subsequently as the "Old IRA" to distinguish it from later organisations of that name), against the British Army, the Black and Tans, the Royal Irish Constabulary, the Auxiliary Division, the Dublin Metropolitan Police, the Ulster Special Constabulary and the Ulster Volunteer Force. On 9 July 1921 a truce came into force. By this time the Ulster Parliament had been opened, established under the Government of Ireland Act 1920, presenting the republican side with a "fait accompli" and guaranteeing the British a permanent entanglement in Ireland. On 11 October negotiations opened between Prime Minister David Lloyd George and Arthur Griffith, who headed the Irish Republic's delegation. The Irish Treaty delegation set up its headquarters in Hans Place, Knightsbridge. On 5 December 1921 at 11:15 am the delegation decided during private discussions at 22 Hans Place to recommend the negotiated agreement to the Dáil Éireann; negotiations continued until 2:30 am on 6 December 1921, after which the parties signed Anglo-Irish Treaty.
Nobody had doubted that these negotiations would produce a form of Irish government short of the independence wished for by republicans. The United Kingdom could not offer a republican form of government without losing prestige and risking demands for something similar throughout the Empire. Furthermore, as one of the negotiators, Michael Collins, later admitted (and he would have known, given his leading role in the independence war), the IRA at the time of the truce was weeks, if not days, from collapse, with a chronic shortage of ammunition. "Frankly, we thought they were mad", Collins said of the sudden British offer of a truce - although the republicans would probably have continued the struggle in one form or another, given the level of public support. The President of the Republic, Éamon de Valera, realising that Westminster would not accept an Irish republic, decided not to become a member of the treaty delegation (Griffith, Collins, Duggan, Barton, and Gavan Duffy) and so not to become accused by more militant republicans as a "sellout". Yet his own proposals - published in January 1922 - fell far short of an autonomous all-Ireland republic.Sinn Féin's abstention was unambiguous.
As expected, the Anglo-Irish Treaty explicitly ruled out a republic. It offered Ireland dominion status, as a state within the then British Empire, equal to Canada, Newfoundland, Australia, New Zealand and South Africa. Though less than expected by the Sinn Féin leadership, this deal offered substantially more than the initial form of home rule within the United Kingdom sought by Charles Stewart Parnell from 1880, and represented a serious advance on the Home Rule Bill of 1914 that the Irish nationalist leader John Redmond had achieved through parliamentary proceedings. However, it all but confirmed the partition of Ireland between Northern Ireland and the Irish Free State. The Second Dáil in Dublin ratified the Treaty (7 January 1922), splitting Sinn Féin in the process.
Northern Ireland "opts out".
For about two days from 6 December 1922 Northern Ireland became part of the newly created Irish Free State. This constitutional episode arose because of the Anglo-Irish Treaty and the legislation introduced to give that Treaty legal effect.
The Treaty was given legal effect in the United Kingdom through the Irish Free State Constitution Act 1922. That Act established, on 6 December 1922, the new Dominion for the whole island of Ireland. Legally therefore, on 6 December 1922, Northern Ireland became an autonomous region of the newly created Irish Free State. However, the Treaty and the laws which implemented it also allowed Northern Ireland to "opt out" of the Irish Free State. Under Article 12 of the Treaty, Northern Ireland could exercise its opt out by presenting an address to the King requesting not to be part of the Irish Free State. Once the Treaty was ratified, the Houses of Parliament of Northern Ireland had one month (dubbed the "Ulster month") to exercise this opt out during which month the Irish Free State Government could not legislate for Northern Ireland, holding the Free State's effective jurisdiction in abeyance for a month.
Realistically it was always certain that Northern Ireland would opt out of the Free State. The Prime Minister of Northern Ireland, Sir James Craig, speaking in the Parliament in October 1922 said that "when 6 December is passed the month begins in which we will have to make the choice either to vote out or remain within the Free State". He said it was important that that choice be made as soon as possible after 6 December 1922 "in order that it may not go forth to the world that we had the slightest hesitation". On 7 December 1922 (the day after the establishment of the Irish Free State) the Parliament demonstrated its lack of hesitation by resolving to make the following address to the King so as to opt out of the Irish Free State:
MOST GRACIOUS SOVEREIGN, We, your Majesty's most dutiful and loyal subjects, the Senators and Commons of Northern Ireland in Parliament assembled, having learnt of the passing of the Irish Free State Constitution Act, 1922, being the Act of Parliament for the ratification of the Articles of Agreement for a Treaty between Great Britain and Ireland, do, by this humble Address, pray your Majesty that the powers of the Parliament and Government of the Irish Free State shall no longer extend to Northern Ireland.
Discussion in the Parliament of the address was short. Prime Minister Craig left for London with the memorial embodying the address on the night boat that evening, 7 December 1922. The King received it the following day, "The Times" reporting:
YORK COTTAGE, SANDRINGHAM, DEC. 8. The Earl of Cromer (Lord Chamberlain) was received in audience by The King this evening and presented an Address from the Houses of Parliament of Northern Ireland, to which His Majesty was graciously pleased to make reply.
With this, Northern Ireland had left the Irish Free State. If the Houses of Parliament of Northern Ireland had not made such a declaration, under Article 14 of the Treaty Northern Ireland, its Parliament and government would have continued in being but the Oireachtas would have had jurisdiction to legislate for Northern Ireland in matters not delegated to Northern Ireland under the Government of Ireland Act. This, of course, never came to pass.
On 13 December 1922 Prime Minister Craig addressed the Parliament informing them that the King had responded to the Parliament's address as follows:
I have received the Address presented to me by both Houses of the Parliament of Northern Ireland in pursuance of Article 12 of the Articles of Agreement set forth in the Schedule to the Irish Free State (Agreement) Act, 1922, and of Section 5 of the Irish Free State Constitution Act, 1922, and I have caused my Ministers and the Irish Free State Government to be so informed.
Governmental and constitutional structures.
The Treaty established that the new Irish Free State would be a constitutional monarchy, with a Governor-General. The "Constitution of the Irish Free State" made more detailed provision for the state's system of government, with a three-tier parliament, called the Oireachtas, made up of the King and two houses, Dáil Éireann and Seanad Éireann (the Irish Senate). Executive authority was vested in the King, and exercised by a cabinet called the Executive Council, presided over by a prime minister called the President of the Executive Council.
The Representative of the Crown.
The King in Ireland was represented by a Governor-General of the Irish Free State. The office replaced the previous Lord Lieutenant, who had headed English and British administrations in Ireland since the Middle Ages. Governors-General were appointed by the King initially on the advice of the British Government, but with the consent of the Irish Government. From 1927 the Irish Government alone had the power to advise the King whom to appoint.
Oath of Allegiance.
As with all dominions, provision was made for an Oath of Allegiance. Within dominions, such oaths were taken by parliamentarians personally towards the monarch. The Irish Oath of Allegiance was fundamentally different. It had two elements; the first, an "oath to the Free State, as by law established", the second part a promise of "fidelity, to His Majesty, King George V, his heirs and successors". That second fidelity element, however, was qualified in two ways. It was to the King "in" Ireland, not specifically to the King of the United Kingdom. Secondly, it was to the King explicitly in his role as part of the Treaty settlement, not in terms of pre-1922 British rule. The Oath itself came from a combination of three sources, and was largely the work of Michael Collins in the Treaty negotiations. It came in part from a draft oath suggested prior to the negotiations by President de Valera. Other sections were taken by Collins directly from the Oath of the Irish Republican Brotherhood (IRB), of which he was the secret head. In its structure, it was also partially based on the form and structure used for 'Dominion status'.
Although 'a new departure', and notably indirect in its reference to the monarchy, it was criticised by nationalists and republicans for making any reference to the Crown, the claim being that it "was" a direct oath to the Crown, a fact demonstrably incorrect by an examination of its wording. But in 1922 Ireland and beyond, it was the perception, not the reality, that influenced public debate on the issue. Had its original author, Michael Collins, survived, he might have been able to clarify its actual meaning, but with his assassination in August 1922, no major negotiator to the Oath's creation on the Irish side was still alive, available or pro-Treaty. (The leader of the Irish delegation, Arthur Griffith, had also died in August 1922). The Oath became a key issue in the resulting Irish Civil War that divided the pro- and anti-treaty sides in 1922–23.
The Irish Civil War.
The compromises contained in the agreement caused the civil war in the 26 counties in June 1922 – April 1923, in which the pro-Treaty Provisional Government defeated the anti-Treaty Republican forces. The latter were led, nominally, by Éamon de Valera, who had resigned as President of the Republic on the treaty's ratification. His resignation outraged some of his own supporters, notably Seán T. O'Kelly, the main Sinn Féin organizer. On resigning, he then sought re-election but was defeated two days later on a vote of 60–58. The pro-Treaty Arthur Griffith followed as President of the Irish Republic. Michael Collins was chosen at a meeting of the members elected to sit in the House of Commons of Southern Ireland (a body set up under the Government of Ireland Act 1920) to become Chairman of the Provisional Government of the Irish Free State in accordance with the Treaty. The general election in June gave overwhelming support for the pro-Treaty parties. W. T. Cosgrave's Crown-appointed Provisional Government effectively subsumed Griffith's republican administration with the death of both Collins and Griffith in August 1922.
The "freedom to achieve freedom".
Governance.
The following were the principal parties of government of the Irish Free State between 1922 and 1937:
Constitutional evolution.
Michael Collins described the Treaty as 'the freedom to achieve freedom'. In practice, the Treaty offered most of the symbols and powers of independence. These included a functioning, if disputed, parliamentary democracy with its own executive, judiciary and written constitution which could be changed by the Oireachtas. However, a number of conditions existed:
The Statute of Westminster (of 1931), embodying a decision of an Imperial Conference, enabled each dominion to enact new legislation or to change any extant legislation, without resorting to any role for the British parliament that may have enacted the original legislation in the past.
The Free State symbolically marked these changes in two mould-breaking moves:
When Éamon de Valera became President of the Executive Council (prime minister) in 1932 he described Cosgrave's ministers' achievements simply. Having read the files, he told his son, Vivion, "they were magnificent, son". 
The Statute of Westminster allowed de Valera, on becoming President of the Executive Council (February 1932), to go even further. With no ensuing restrictions on his policies, he abolished the Oath of Allegiance (which Cosgrave intended to do had he won the 1932 general election), the Senate, university representation in the Dáil, and appeals to the Privy Council.
One major policy error occurred in 1936 when he attempted to use the abdication of King Edward VIII to abolish the crown and governor-general in the Free State with the "Constitution (Amendment No. 27 Act)". He was advised by senior law officers and other constitutional experts that, as the crown and governor-generalship existed separately from the constitution in a vast number of acts, charters, orders-in-council, and letters patent, they both still existed. A second bill, the "Executive Powers (Consequential Provisions) Act, 1937" was quickly introduced to repeal the necessary elements. De Valera retroactively dated the second act back to December 1936.
Currency.
The new state continued to use sterling from its inception; there is no reference in the Treaty or in either of the enabling Acts to currency. Nonetheless and within a few years, the Dáil passed the (which provided for a "Saorstát" [Free State] coinage) and the (which provided "inter alia" for banknotes of the Saorstát pound). The new Saorstát pound was defined by the 1927 Act to have exactly the same weight and fineness of gold as was the sovereign at the time, making the new currency pegged at 1:1 with sterling. The State circulated its new national coinage in 1928, marked "Saorstát Éireann" and a national series of banknotes. British coinage remained acceptable in the Free State at an equal rate. In 1937, when the Free State was superseded by Ireland ("Éire"), the pound became known as the "Irish pound" and the coins were marked "Éire".
Demographics.
According to one report, in 1924, shortly after the Irish Free State's establishment, the new dominion had the "lowest birth-rate in the world". The report noted that amongst countries for which statistics were available (Ceylon, Chile, Japan, Spain, South Africa, Netherlands, Canada, Germany, Australia, United States, Britain, New Zealand, Finland and the Irish Free State). Ceylon had the highest birth rate at 40.8 per 1,000 while the Irish Free State had a birth rate of just 18.6 per 1,000.
After the Irish Free State.
In 1937 the Fianna Fáil government presented a draft of an entirely new Constitution to Dáil Éireann. An amended version of the draft document was subsequently approved by the Dáil. A referendum was then held on the same day as the 1937 general election, when a relatively narrow majority approved it. The new Constitution of Ireland ("Bunreacht na hÉireann") repealed the 1922 Constitution, and came into effect on 29 December 1937.
The state was named Ireland (Éire in the Irish language), and a new office of President of Ireland was instituted in place of the Governor-General of the Irish Free State. The new constitution claimed jurisdiction over all of Ireland while recognising that legislation would not apply in Northern Ireland (see Articles 2 and 3). Articles 2 and 3 were reworded in 1998 to remove jurisdictional claim over the entire island and to recognise that "a united Ireland shall be brought about only by peaceful means with the consent of a majority of the people, democratically expressed, in both jurisdictions in the island".
With respect to religion, a section of Article 44 included the following:
The State recognises the special position of the Holy Catholic Apostolic and Roman Church as the guardian of the Faith professed by the great majority of the citizens. The State also recognises the Church of Ireland, the Presbyterian Church in Ireland, the Methodist Church in Ireland, the Religious Society of Friends in Ireland, as well as the Jewish Congregations and the other religious denominations existing in Ireland at the date of the coming into operation of this Constitution.
 Following a referendum, this section was deleted in 1973.
It was left to the initiative of de Valera's successors in government to achieve the country's formal transformation into a republic. A small but significant minority of Irish people, usually attached to parties like Sinn Féin and the smaller Republican Sinn Féin, denied the right of the twenty-six county state to use the name "Ireland" and continued to refer to the state as the Free State. With Sinn Féin's entry into Dáil Éireann and the Northern Ireland Executive at the close of the 20th century, the number of those who refuse to accept the legitimacy of the state, which was already in a minority, declined further. After the setting up of the Free State in 1923, most Protestants left southern Ireland and unionism there came to an end; this contrasted markedly to De Valera's opinion that Ireland could not stand divided. The Boundary Commission of 1924 confirmed that the two nations would live apart.
Further reading.
<includeonly></includeonly>

</doc>
<doc id="42954" url="http://en.wikipedia.org/wiki?curid=42954" title="304">
304

Year 304 (CCCIV) was a leap year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Valerius and Valerius (or, less frequently, year 1057 "Ab urbe condita"). The denomination 304 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
China.
</onlyinclude>

</doc>
<doc id="42957" url="http://en.wikipedia.org/wiki?curid=42957" title="Endospore">
Endospore

An endospore is a dormant, tough, and non-reproductive structure produced by certain bacteria from the Firmicute phylum. The name "endospore" is suggestive of a spore or seed-like form ("endo" means within), but it is not a true spore (i.e., not an offspring). It is a stripped-down, dormant form to which the bacterium can reduce itself. Endospore formation is usually triggered by a lack of nutrients, and usually occurs in Gram-positive bacteria. In endospore formation, the bacterium divides within its cell wall. One side then engulfs the other. Endospores enable bacteria to lie dormant for extended periods, even centuries. Revival of spores millions of years old has been claimed. When the environment becomes more favorable, the endospore can reactivate itself to the vegetative state. Most types of bacteria cannot change to the endospore form. Examples of bacteria that can form endospores include "Bacillus" and "Clostridium".
The endospore consists of the bacterium's DNA, ribosomes and large amounts of dipicolinic acid. Dipicolinic acid is a spore-specific chemical that appears to help in the ability for endospores to maintain dormancy. This chemical comprises up to 10% of the spore's dry weight.
Endospores can survive without nutrients. They are resistant to ultraviolet radiation, desiccation, high temperature, extreme freezing and chemical disinfectants. Thermo-resistant endospores were first hypothesized by Ferdinand Cohn after studying "Bacillus subtilis" (pictured to the right) growth on cheese after boiling the cheese. His notion of spores being the reproductive mechanism for the growth was a large blow to the previous suggestions of spontaneous generation. Astrophysicist Steinn Sigurdsson said "There are viable bacterial spores that have been found that are 40 million years old on Earth – and we know they're very hardened to radiation." Common anti-bacterial agents that work by destroying vegetative cell walls do not affect endospores. Endospores are commonly found in soil and water, where they may survive for long periods of time. A variety of different microorganisms form "spores" or "cysts," but the endospores of low G+C Gram-positive bacteria are by far the most resistant to harsh conditions.
Some classes of bacteria can turn into exospores, also known as microbial cysts, instead of endospores. Exospores and endospores are two kinds of "hibernating" or dormant stages seen in some classes of microorganisms.
Structure.
Bacteria produce a single endospore internally. The spore is sometimes surrounded by a thin covering known as the exosporium, which overlies the "spore coat". The spore coat, which acts like a sieve that excludes large toxic molecules like lysozyme, is resistant to many toxic molecules and may also contain enzymes that are involved in germination. The "cortex" lies beneath the spore coat and consists of peptidoglycan. The "core wall" lies beneath the cortex and surrounds the protoplast or "core" of the endospore. The core contains the spore chromosomal DNA which is encased in chromatin-like proteins known as SASPs (small acid-soluble spore proteins), that protect the spore DNA from UV radiation and heat. The core also contains normal cell structures, such as ribosomes and other enzymes, but is not metabolically active.
Up to 20% of the dry weight of the endospore consists of calcium dipicolinate within the core, which is thought to stabilize the DNA. Dipicolinic acid could be responsible for the heat resistance of the spore, and calcium may aid in resistance to heat and oxidizing agents. However, mutants resistant to heat but lacking dipicolinic acid have been isolated, suggesting other mechanisms contributing to heat resistance are also at work. Small acid-soluble proteins (SASPs) are found in endospores. These proteins tightly bind and condense the DNA, and are in part responsible for resistance to UV light and DNA-damaging chemicals.
Visualising endospores under light microscopy can be difficult due to the impermeability of the endospore wall to dyes and stains. While the rest of a bacterial cell may stain, the endospore is left colourless. To combat this, a special stain technique called a Moeller stain is used. That allows the endospore to show up as red, while the rest of the cell stains blue. Another staining technique for endospores is the Schaeffer-Fulton stain, which stains endospores green and bacterial bodies red. The arrangement of spore layers is as follows:
Location.
The position of the endospore differs among bacterial species and is useful in identification. The main types within the cell are terminal, subterminal, and centrally placed endospores. Terminal endospores are seen at the poles of cells, whereas central endospores are more or less in the middle. Subterminal endospores are those between these two extremes, usually seen far enough towards the poles but close enough to the center so as not to be considered either terminal or central. Lateral endospores are seen occasionally.
Examples of bacteria having terminal endospores include "Clostridium tetani", the pathogen that causes the disease tetanus. Bacteria having a centrally placed endospore include "Bacillus cereus", and those having a subterminal endospore include "Bacillus subtilis". Sometimes the endospore can be so large the cell can be distended around the endospore, this is typical of "Clostridium tetani".
Formation and destruction.
Under conditions of starvation, especially the lack of carbon and nitrogen sources, a single endospore forms within some of the bacteria. The process is called sporulation (def).(Kaiser, 2011)
When a bacterium detects environmental conditions are becoming unfavourable it may start the process of endosporulation, which takes about eight hours. The DNA is replicated and a membrane wall known as a "spore septum" begins to form between it and the rest of the cell. The plasma membrane of the cell surrounds this wall and pinches off to leave a double membrane around the DNA, and the developing structure is now known as a forespore. Calcium dipicolinate, a molecule of calcium and dipicolinic acid, is incorporated into the forespore during this time. The dipicolinic acid helps stabilize the proteins and DNA in the endospore. Next the peptidoglycan cortex forms between the two layers and the bacterium adds a spore coat to the outside of the forespore. In the final stages of endospore formation the newly forming endospore is dehydrated and allowed to mature before being released from the mother cell. The cortex is what makes the endospore so resistant to temperature. The cortex contains an inner membrane known as the core. The inner membrane that surrounds this core leads to the endospore's resistance against UV light and harsh chemicals that would normally destroy microbes. Sporulation is now complete, and the mature endospore will be released when the surrounding vegetative cell is degraded.
Endospores are resistant to most agents that would normally kill the vegetative cells they formed from. Unlike persister cells, endospores are the result of a morphological differentiation process triggered by nutrient limitation (starvation) in the environment; endosporulation is initiated by quorum sensing within the "starving" population. Most disinfectants such as household cleaning products, alcohols, quaternary ammonium compounds and detergents have little effect on endospores. However, sterilant alkylating agents (e.g. ethylene oxide), and 10% bleach are effective against endospores. To kill all anthrax spores, standard household bleach (with 5% sodium hypochlorite) diluted freshly 1:10 in water (100ml of bleach in 900ml of water) must be in contact with the spore for at least several minutes; a very small proportion of spores can survive longer than 10 minutes in such a solution. Higher concentrations of bleach are not more effective, and can cause some types of bacteria to aggregate and thus survive.
While significantly resistant to heat and radiation, endospores can be destroyed by burning or by autoclaving at a temperature exceeding the boiling point of water, 100 °C. Endospores are able to survive at 100 °C for hours, although the longer the number of hours the fewer that will survive. An indirect way to destroy them is to place them in an environment that reactivates them to their vegetative state. They will germinate within a day or two with the right environmental conditions, and then the vegetative cells, not as hardy as endospores, can be straightforwardly destroyed. This indirect method is called Tyndallization. It was the usual method for a while in the late 19th century before the introduction of inexpensive autoclaves. Prolonged exposure to ionising radiation, such as x-rays and gamma rays, will also kill most endospores.
Certain types of endospores are used to ensure that an autoclaved item has been rendered truly sterile: a small capsule containing the spores is put into the autoclave with the items; after the cycle it is checked to see if anything will grow from it. If nothing will grow, then the spores were destroyed and the sterilization was successful.
In hospitals, endospores on delicate invasive instruments (e.g., video endoscopes) are killed by "low-temperature" and non-corrosive, non-toxic, plasma-activated concentrated hydrogen peroxide vapor in sterilizers. In contrast, "high level disinfection" does not kill endospores but is used for instruments that don't enter sterile bodily cavities (e.g., a colonoscope). This latter method uses only warm water, enzymes, and detergents.
Bacterial endospores are resistant to antibiotics, most disinfectants, and physical agents such as radiation, boiling, and drying. The impermeability of the spore coat is thought to be responsible for the endospore's resistance to chemicals. The heat resistance of endospores is due to a variety of factors:
Reactivation.
Reactivation of the endospore occurs when conditions are more favourable and involves "activation", "germination", and "outgrowth". Even if an endospore is located in plentiful nutrients, it may fail to germinate unless activation has taken place. This may be triggered by heating the endospore. Germination involves the dormant endospore starting metabolic activity and thus breaking hibernation. It is commonly characterised by rupture or absorption of the spore coat, swelling of the endospore, an increase in metabolic activity, and loss of resistance to environmental stress.
Outgrowth follows germination and involves the core of the endospore manufacturing new chemical components and exiting the old spore coat to develop into a fully functional vegetative bacterial cell, which can divide to produce more cells.
Endospores possess five times more sulfur than vegetative cells. This excess sulfur is concentrated in spore coats as an amino acid, cystine. It is believed that the macromolecule accountable for maintaining the dormant state has a protein coat rich in cystine, stabilized by S-S linkages. A reduction in these linkages has the potential to change the tertiary structure, causing the protein to unfold. This conformational change in the protein is thought to be responsible for exposing active enzymatic sites necessary for endospore germination.
Endospores can stay dormant for a very long time. For instance, Endospores were found in the tombs of the Egyptian Pharaohs. When placed in appropriate medium, under appropriate conditions, they were able to be reactivated. In 1995, Raul Cano of California Polytechnic State University found bacterial spores in the gut of a fossilized bee trapped in amber from a tree in the Dominican Republic. The bee fossilized in amber was dated to being about 25 million years old. The spores germinated when the amber was cracked open and the material from the gut of the bee was extracted and placed in nutrient medium. After the spores were analyzed by microscopy, it was determined that the cells were very similar to "Bacillus sphericus" which is found in bees in the Dominican Republic today.
Importance.
As a simplified model for cellular differentiation, the molecular details of endospore formation have been extensively studied, specifically in the model organism "Bacillus subtilis". These studies have contributed much to our understanding of the regulation of gene expression, transcription factors, and the sigma factor subunits of RNA polymerase.
Endospores of the bacterium "Bacillus anthracis" were used in the 2001 anthrax attacks. The powder found in contaminated postal letters was composed of extracellular anthrax endospores. This intentional distribution lead to 22 known cases of anthrax (11 inhalation and 11 cutaneous) making the case fatality rate among patients with inhalation anthrax 45% (5/11). The six other individuals with inhalation anthrax and all the individuals with cutaneous anthrax recovered. Had it not been for antibiotic therapy many more might have been stricken.
According to WHO veterinary documents, B. anthracis sporulates when it sees oxygen instead of the carbon dioxide present in mammal blood; this signals to the bacteria that it has reached the end of the animal, and an inactive dispersable morphology is useful.
"Sporulation requires the presence of free oxygen. in the natural situation, this means the vegetative cycles occur within the low oxygen environment of the infected host and, within the host, the organism is exclusively in the vegetative form. once outside the host, sporulation commences upon exposure to the air and the spore forms are essentially the exclusive phase in the environment."
"Geobacillus stearothermophilus" endospores are used as biological indicators when an autoclave is used in sterilization procedures.
Biotechnology.
"Bacillus subtilis" spores are useful for the expression of recombinant proteins and in particular for the surface display of peptides and proteins as a tool for fundamental and applied research in the fields of microbiology, biotechnology and vaccination.
Endospore-forming bacteria.
Examples of endospore-forming bacteria include the genera:

</doc>
<doc id="42958" url="http://en.wikipedia.org/wiki?curid=42958" title="Theodore the Studite">
Theodore the Studite

Theodore the Studite (also known as Theodorus Studita, St. Theodore of Stoudios, and St. Theodore of Studium; 759–826) was a Byzantine Greek monk and abbot of the Stoudios Monastery in Constantinople. Theodore's letter, containing suggested monastery reform rules, is the first recorded stand against slavery. He played a major role in the revivals both of Byzantine monasticism and of classical literary genres in Byzantium. He is known as a zealous opponent of iconoclasm, one of several conflicts that set him at odds with both emperor and patriarch.
Biography.
Family and childhood.
Theodore was born in Constantinople in 759. He was the oldest son of Photeinos, an important financial official in the palace bureaucracy, and Theoktiste, herself the offspring of a distinguished Constantinopolitan family. The brother of Theoktiste, Theodore's uncle Platon, was himself an important official in the imperial financial administration. The family therefore controlled a significant portion, if not all, of the imperial financial administration during the reign of Constantine V (r. 741–775). Theodore had two younger brothers (Joseph, later Archbishop of Thessaloniki, and Euthymios) and one sister, whose name we do not know.
It has often been assumed that Theodore's family belonged to the iconodule party during the first period of Byzantine Iconoclasm. There is however no evidence to support this, and their high position in the imperial bureaucracy of the time renders any openly iconodule position highly unlikely. Furthermore, when Platon left his office and entered the priesthood in 759, he was ordained by an abbot who, if he was not actively iconoclastic himself, at the very least offered no resistance to the iconoclastic policies of Constantine V. The family as a whole was most likely indifferent to the question of icons during this period.
According to the later hagiographical literature, Theodore received an education befitting his family's station, and from the age of seven was instructed by a private tutor, eventually concentrating in particular on theology. It is however not clear that these opportunities were available to even the most well-placed Byzantine families of the eighth century, and it is possible that Theodore was at least partially an autodidact.
Early monastic career.
Following the death of Emperor Leo IV (r. 775–780) in 780, Theodore's uncle Platon, who had lived as a monk in the Symbola Monastery in Bithynia since 759, visited Constantinople, and persuaded the entire family of his sister, Theoktiste, to likewise take monastic vows. Theodore, together with his father, brothers, sailed back to Bithynia with Platon in 781, where they set about transforming the family estate into a religious establishment, which became known as the Sakkudion Monastery. Platon became abbot of the new foundation, and Theodore was his "right hand." The two sought to order the monastery according to the writings of Basil of Caesarea.
During the period of the regency of Eirene, Abbot Platon emerged as a supporter of the Patriarch Tarasios, and was a member of Tarasios's iconodule party at the Second Council of Nicaea, where the veneration of icons was declared orthodox. Shortly thereafter Tarasios himself ordained Theodore as a priest. In 794, Theodore became abbot of the Sakkudion Monastery, while Platon withdrew from the daily operation of the monastery and dedicated himself to silence.
Conflict with Constantine VI.
Also in 794, Emperor Constantine VI (r. 776–797) decided to separate from his first wife, Maria of Amnia, and to marry Maria's "kubikularia" (Lady-in-waiting), Theodote, a cousin of Theodore the Studite. Although the Patriarch may initially have resisted this development, as a divorce without proof of adultery on the part of the wife could be construed as illegal, he ultimately gave way. The marriage of Constantine and Theodote was celebrated in 795, although not by the patriarch, as was normal, but by a certain Joseph, a priest of Hagia Sophia.
A somewhat obscure chain of events followed (the so-called "Moechian controversy," from the Greek "moichos", "adulterer"), in which Theodore initiated a protest against the marriage from the Sakkudion Monastery, and appears to have demanded the excommunication, not only of the priest Joseph, but also of all who had received communion from him, which, as Joseph was a priest of the imperial church, included implicitly the emperor and his court. This demand had no official weight, however, and Constantine appears to have attempted to make peace with Theodore and Platon (who, on account of his marriage, were now his relatives), inviting them to visit him during a sojourn at the imperial baths of Prusa in Bithynia. In the event neither appeared.
As a result, imperial troops were sent to the Sakkudion Monastery, and the community was dispersed. Theodore was flogged, and, together with ten other monks, banished to Thessaloniki, while Platon was imprisoned in Constantinople. The monks arrived in Thessaloniki in March 797, but did not remain for long; in August of the same year Constantine VI was blinded and overthrown, and his mother Irene, the new empress, lifted the exile.
Abbot of the Studites.
Following the accession of Irene, the priest Joseph was stripped of his office, and Theodoros was received in the imperial palace. The monks then returned to the Sakkudion Monastery, but were forced back to the capital in either 797 or 798 on account of an Arab raid on Bithynia. At this time, Irene offered Theodore the leadership of the ancient Stoudios Monastery in Constantinople, which he accepted. Theodore then set about building various workshops within the monastery to guarantee autarky, constructing a library and a scriptorium, and restoring and decorating the church. He also composed a series of poems on the duties of the various members of the community, which were likely inscribed and displayed within the monastery. He furthermore composed a rule for the governance of the monastery, and made the Studios community the center of an extensive congregation of dependent monasteries, including the Sakkudion. He maintained contact with these other monasteries above all through his prodigious literary output (letters as well as catechisms), which reached a quantitative peak at this time, and developed a system of messengers that was so elaborate as to resemble a private postal service.
To this period may also date the so-called iconophile epigrams, iambic acrostics composed by Theodore that replaced the "iconoclastic epigrams" which were previously exhibited on the Chalke gate of the Great Palace. It has been suggested that these were commissioned by Irene, as another sign of her good favor toward Theodore, although a commission under Michael I Rangabe (r. 811–813) is also possible; in any case, they were removed in 815 by Leo V the Armenian (r. 813–820) and replaced by new "iconoclastic" verses.
In 806, the Patriarch Tarasios died, and Emperor Nikephoros I (r. 802–811) set about seeking his replacement. It appears likely that Platon at this time put forth Theodore's name, but Nikephoros, a layman who held the rank of "asekretis" in the imperial bureaucracy, was chosen instead. The selection of Nikephoros gave rise to an immediate protest on the part of the Studites, and in particular Theodore and Platon, who objected to the elevation of a layman to the patriarchal throne. Theodore and Platon were jailed for 24 days before the Emperor Nikephoros allowed them to return to their congregations.
Conflict with Nikephoros.
Emperor Nikephoros soon requested that his new patriarch rehabilitate the priest Joseph, who had officiated at the wedding of Constantine and Theodote, possibly because Joseph had aided in the peaceful resolution of the revolt of Bardanes Tourkos. In 806, the Patriarch Nikephoros convened a synod to address the case, at which Theodore was present. The Synod decided to readmit Joseph to the priesthood, a decision to which Theodore did not at the time object.
Therefore, relations between the Studite Abbot and the Patriarch appear to have been initially untroubled, an impression which is reinforced by the choice (806/807) of Theodore's brother, Joseph, as Archbishop of Thesaloniki. However, soon after this ordination, perhaps in 808, Theodore began to express his unwillingness to associate with the rehabilitated priest Joseph, or for that matter with anyone else who knowingly associated with him, as he held the rehabilitation for uncanonical. As in the first dispute over the priest Joseph, the extension of this refusal beyond Joseph to those who associated with him included implicitly the patriarch and the emperor himself.
Early in 808, Theodoros offered in a series of letters to explain his position to the emperor, and furthermore to perform the customary proskynesis at his feet, which offer Nikephoros declined, instead setting off for the summer military campaign. In the winter of the same year, Theodore's brother Joseph visited him in Constantinople, but refused to attend the Christmas mass in Hagia Sophia, at which the emperor, the patriarch, and the priest Joseph would have been present. As a result he was stripped of his archbishopric. At around the same time a small military division was dispatched to the Studios Monastery to arrest Theodore, Joseph, and Platon. A synod was then held in January of 809, at which Theodore and his followers were anathematized as schismatic. Theodore, Joseph, and Platon, were thereafter banished to the Princes' Islands: Theodore to Chalke, Joseph to Prote, and Platon to Oxeia.
Theodore maintained an extensive literary activity in exile, writing numerous letters to correspondents including his brother, various Studite monks, influential family members, and even Pope Leo III. He also continued to compose catechisms for the Studite congregation, as well as a number of poems.
Rehabilitation under Michael I.
In 811, the new emperor Michael I Rangabe called the Studites back from exile. The priest Joseph was once more defrocked, and Theodore was, at least superficially, reconciled with the Patriarch Nikephoros.
There are, however, indications that a certain rivalry between the Studite Abbot and the Patriarch persisted. In 812, Michael I resolved to persecute certain heretics in Phrygia and Lycaonia, namely the Paulicians and the "Athinganoi" (sometimes identified with the Roma). Theodore and Nikephoros were called before the emperor to debate the legality of punishing heresy by death, Theodore arguing against and Nikephoros for. Theodore is said to have won the day.
The second affair concerned a peace treaty proposed by Krum of Bulgaria (r. 803–814), also in 812, according to which the Byzantine and Bulgarian states should exchange refugees. It is likely that Krum sought the return of certain Bulgarians who had betrayed him to the Byzantines. In this instance Theodore argued against the exchange, as it would require that Christians be cast to barbarians, while Nikephoros urged the emperor to accept the treaty. Once more Theodore's opinion prevailed, although this time with serious consequences; Krum attacked and took Mesembria in November of the same year. Michael led a military campaign against the Bulgarians in 813, which ended in defeat, and as a result he abdicated in July and Leo V was crowned emperor.
On April 4, 814, Theodore's uncle Platon died in the Studios Monastery after a long illness. Theodore composed a long funeral oration, the "Laudatio Platonis", which remains one of the most important sources for the history of the family.
Second Iconoclasm.
At the very beginning of his reign, Leo V faced a new Bulgarian offensive that reached the walls of Constantinople and ravaged large sections of Thrace. This came to an end with the death of Krum on April 13, 814, and the internal power struggles that followed. However, as the previous 30 years since the approval of icon-veneration at the Synod of 787 had represented for the Byzantines a string of military catastrophes, Leo resolved to reach back to the policies of the more successful Isaurian dynasty. He renamed his son Constantine, thus drawing a parallel to Leo III (r. 717–741) and Constantine V, and beginning in 814 began to discuss with various clerics and senators the possibility of reviving the iconoclastic policy of the Isaurians. This movement met with strong opposition from the Patriarch Nikephoros, who himself gathered a group of bishops and abbots about him and swore them to uphold the veneration of images. The dispute came to a head in a debate between the two parties before the emperor in the Great Palace on Christmas 814, at which Theodore and his brother Joseph were present, and took the side of the iconophiles.
Leo held fast by his plan to revive iconoclasm, and in March 815 the Patrarch Nikephoros was stripped of his office and exiled to Bithynia. At this point Theodore remained in Constantinople, and assumed a leading role in the iconodule opposition. On March 25, Palm Sunday, he commanded his monks to process through the monastery's vineyard, holding up icons so that they could be seen over the walls by the neighbors. This provocation elicited only a rebuke from the emperor.
A new patriarch, Theodotos, was selected, and in April a synod was convened in Hagia Sophia, at which iconoclasm was re-introduced as dogma. Theodore composed a series of letters in which he called on "all, near and far," to revolt against the decision of the synod. Not long thereafter he was exiled by imperial command to a Metopa, a fortress on the eastern shore of Lake Apollonia in Bithynia. Shortly thereafter Leo had Theodore's poems removed from the Chalke Gate and replaced by a new set of "iconoclastic" epigrams.
While Theodore was in exile, the leadership of the Studite congregation was assumed by the Abbot Leontios, who for a time adopted the iconoclast position and won over many individuals monks to his party. He was, however, eventually won back to the iconodule party. The Studite situation mirrored a general trend, with a number of bishops and abbots at first willing to reach a compromise with the iconoclasts, but then in the years between 816 and 819 renouncing the iconoclast position, a movement that was perhaps motivated by the martyrdom of the Studite monk Thaddaios. It was during this upswell in icondule sentiment that Theodore began to compose his own polemic against the iconoclasts, the "Refutatio", concentrating in particular on refuting the arguments and criticizing the literary merits of the new iconoclastic epigrams on the Chalke.
Theodore exercised a wide influence during the first year of his exile, primarily through a massive letter-writing campaign. Accordingly, he was transferred in 816 to Boneta, a fortress in the more remote Anatolic theme, whence he nevertheless remained abreast of developments in the capital and maintained a regular correspondence. This continued activity led to an imperial order that Theodore be whipped, which his captors however refused to carry out. In 817, Theodore wrote two letters to Pope Paschal I, which were co-signed by several fellow iconophile abbots, in the first requesting that he summon an anti-iconoclastic Synod; letters to the Patriarchs of Alexandria and Jerusalem, among other "foreign" clerics, followed. As a result the emperor ordered at least once more that Theodore be flogged, and the command was this time carried out, with the result that Theodore became quite ill. After his recovery Theodore was moved to Smyrna. Early in 821, however, Leo V fell victim to a grisly murder at the altar of the Church of St. Stephen in the imperial palace; Theodore was released from exile shortly thereafter.
Final years.
Following his release, Theodore made his way back to Constantinople, travelling through north-western Anatolia and meeting with numerous monks and abbots on the way. At the time he appears to have believed that the new emperor, Michael II (r. 820–829), would adopt a pro-icons policy, and he expressed this hope in two letters to Michael. An imperial audience was arranged for a group of iconodule clerics, including Theodore, at which however Michael expressed his attention to "leave the church as he had found it." The abbots were to be allowed to venerate images if they so wished, as long as they remained outside of Constantinople. Theodore returned to Anatolia, in what seems to have been a sort of self-imposed exile.
Theodore's activities in his final years are somewhat difficult to trace. He continued to write numerous letters supporting the use of icons, and appears to have remained an important leader of the opposition to imperial iconoclasm. He was present at a meeting of "more than a hundred" iconodule clerics in 823 or 824, which ended in an argument between the Studites and the host, one Ioannikos, which may have represented a power struggle within the movement. Theodore also spoke against the second marriage of Michael II to the nun Euphrosyne, a daughter of Constantine VI, although in a very moderate fashion, and with none of the passion or effect of the Moechian controversy.
Theodore's years of exile, regular fasting, and exceptional exertions had taken their toll, and in 826 he became quite ill. In this year, he dictated his "Testament", a form of spiritual guidance for the future abbots of the Studios monastery, to his disciple Naukratios. He died on the 11 of November of that same year, while celebrating mass, apparently in the monastery of Hagios Tryphon on Cape Akritas in Bithynia. Eighteen years later, his remains, along with those of his brother Joseph, were brought back to the Studios Monastery, were they were interred beside the grave of their uncle Platon.
Legacy.
Theodore's revival of the Studios monastery had a major effect on the later history of Byzantine monasticism. His disciple, Naukratios, recovered control of the monastery after the end of iconoclasm in 842, and throughout the remainder of the ninth century the Studite abbots continued Theodore's tradition of opposition to patriarchal and imperial authority. Elements of Theodore's "Testament" were incorporated verbatim in the typika of certain early Athonite monasteries. The most important elements of his reform were its emphases on cenobitic (communal) life, manual labor, and a carefully defined administrative hierarchy.
Theodore also built the Studios monastery into a major scholarly center, in particular through its library and scriptorium, which certainly surpassed all other contemporary Byzantine ecclesiastical institutions in this regard. Theodore himself was a pivotal figure in the revival of classical literary forms, in particular iambic verse, in Byzantium, and his criticisms of the iconoclastic epigrams drew a connection between literary skill and orthodox faith. After his death the Studios monastery continued to be a vital center for Byzantine hymnography and hagiography, as well as for the copying of manuscripts.
Following the "triumph of Orthodoxy" (i.e. the reintroduction of icons) in 843, Theodore became one of the great heroes of the iconodule opposition. There was no formal process of canonization in Byzantium, but Theodore was soon recognized as a saint. In the Latin West, a tradition arose according to which Theodore had recognized papal primacy, on the basis of his letters to Pope Paschal I, and he was formally canonized by the Catholic Church. His feast day is November 11 in the East and November 12 in the West.
Works.
Theodore was an immensely prolific author; among his most important works are:
Commentary on Theodore.
As also mentioned by Kirby Page in "Jesus or Christianity", Charles Loring Brace tells us in "Gesta Christ" that it was not until the 9th century that the first recorded stand against slavery itself was taken by St. Theodore:
No direct word against slavery, however, came forth from the great Teacher [Jesus Christ]. It was not until the ninth century after, that one of his humble followers, Saint Theodore of Studium (Constantinople), ventured to put forth the command "Thou shalt possess no slave, neither for domestic service nor for the labor of the fields, for man is made in the image of God."
References.
Studies.
</dl>

</doc>
<doc id="42959" url="http://en.wikipedia.org/wiki?curid=42959" title="Çaro, Pyrénées-Atlantiques">
Çaro, Pyrénées-Atlantiques

 
Çaro (Basque: "Zaro") is a commune in the Pyrénées-Atlantiques department in south-western France.
It is located in the former province of Lower Navarre.

</doc>
<doc id="42960" url="http://en.wikipedia.org/wiki?curid=42960" title="Fractal transform">
Fractal transform

The fractal transform is a technique invented by Michael Barnsley "et al." to perform lossy image compression.
This first practical fractal compression system for digital images resembles a vector quantization system using the image itself as the codebook.
Fractal transform compression.
Start with a digital image A1.
Downsample it by a factor of 2 to produce image A2.
Now, for each block B1 of 4x4 pixels in A1, find the corresponding block B2 in A2 most similar to B1, and then find the grayscale or RGB offset and gain from A2 to B2.
For each destination block, output the positions of the source blocks and the color offsets and gains.
Fractal transform decompression.
Starting with an empty destination image A1, repeat the following algorithm several times:
Downsample A1 down by a factor of 2 to produce image A2. Then copy blocks from A2 to A1 as directed by the compressed data, multiplying by the respective gains and adding the respective color offsets.
This algorithm is guaranteed to converge to an image, and it should appear similar to the original image.
In fact, a slight modification of the decompressor to run at block sizes larger than 4x4 pixels produces a method of stretching images without causing the blockiness or blurriness of traditional linear resampling algorithms.
Patents.
The basic patents covering Fractal Image Compression, U.S. Patents 4,941,193, 5,065,447, 5,384,867, 5,416,856, and 5,430,812 appear to be expired.

</doc>
<doc id="42964" url="http://en.wikipedia.org/wiki?curid=42964" title="Snell's law">
Snell's law

Snell's law (also known as the Snell–Descartes law and the law of refraction) is a formula used to describe the relationship between the angles of incidence and refraction, when referring to light or other waves passing through a boundary between two different isotropic media, such as water, glass, or air. 
In optics, the law is used in ray tracing to compute the angles of incidence or refraction, and in experimental optics to find the refractive index of a material. The law is also satisfied in metamaterials, which allow light to be bent "backward" at a negative angle of refraction with a negative refractive index.
Although named after Dutch astronomer Willebrord Snellius (1580–1626), the law was first accurately described by the scientist Ibn Sahl at the Baghdad court in 984. In the manuscript "On Burning Mirrors and Lenses", Sahl used the law to derive lens shapes that focus light with no geometric aberrations.
Snell's law states that the ratio of the sines of the angles of incidence and refraction is equivalent to the ratio of phase velocities in the two media, or equivalent to the reciprocal of the ratio of the indices of refraction:
with each formula_2 as the angle measured from the normal of the boundary, formula_3 as the velocity of light in the respective medium (SI units are meters per second, or m/s) and formula_4 as the refractive index (which is unitless) of the respective medium.
The law follows from Fermat's principle of least time, which in turn follows from the propagation of light as waves.
History.
Ptolemy, a Greek living in Alexandria, Egypt, had found a relationship regarding refraction angles, but it was inaccurate for angles that were not small. Ptolemy was confident he had found an accurate empirical law, partially as a result of fudging his data to fit theory (see: confirmation bias). Alhazen, in his "Book of Optics" (1021), came closer to discovering the law of refraction, though he did not take this step.
The law of refraction was first accurately described by Ibn Sahl, of Baghdad, in the manuscript "On Burning Mirrors and Lenses" (984). He made use of it to work out the shapes of lenses that focus light with no geometric aberrations, known as anaclastic lenses.
The law was rediscovered by Thomas Harriot in 1602, who however did not publish his results although he had corresponded with Kepler on this very subject. In 1621, Willebrord Snellius (Snell) derived a mathematically equivalent form, that remained unpublished during his lifetime. René Descartes independently derived the law using heuristic momentum conservation arguments in terms of sines in his 1637 essay "Dioptrics", and used it to solve a range of optical problems. Rejecting Descartes' solution, Pierre de Fermat arrived at the same solution based solely on his principle of least time. Interestingly, Descartes assumed the speed of light was infinite, yet in his derivation of Snell's law he also assumed the denser the medium, the greater the speed of light. Fermat supported the opposing assumptions, i.e., the speed of light is finite, and his derivation depended upon the speed of light being slower in a denser medium. Fermat's derivation also utilized his invention of adequality, a mathematical procedure equivalent to differential calculus, for finding maxima, minima, and tangents.
In his influential mathematics book "Geometry", Descartes solves a problem that was worked on by Apollonius of Perga and Pappus of Alexandria. Given n lines L and a point P(L) on each line, find the locus of points Q such that the lengths of the line segments QP(L) satisfy certain conditions. For example, when n = 4, given the lines a, b, c, and d and a point A on a, B on b, and so on, find the locus of points Q such that the product QA*QB equals the product QC*QD. When the lines are not all parallel, Pappus showed that the loci are conics, but when Descartes considered larger n, he obtained cubic and higher degree curves. To show that the cubic curves were interesting, he showed that they arose naturally in optics from Snell's law.
According to Dijksterhuis, "In "De natura lucis et proprietate" (1662) Isaac Vossius said that Descartes had seen Snell's paper and concocted his own proof. We now know this charge to be undeserved but it has been adopted many times since." Both Fermat and Huygens repeated this accusation that Descartes had copied Snell. In French, Snell's Law is called "la loi de Descartes" or "loi de Snell-Descartes."
In his 1678 "Traité de la Lumière", Christiaan Huygens showed how Snell's law of sines could be explained by, or derived from, the wave nature of light, using what we have come to call the Huygens–Fresnel principle.
Explanation.
Snell's law is used to determine the direction of light rays through refractive media with varying indices of refraction. The indices of refraction of the media, labeled formula_5, formula_6 and so on, are used to represent the factor by which a light ray's speed decreases when traveling through a refractive medium, such as glass or water, as opposed to its velocity in a vacuum.
As light passes the border between media, depending upon the relative refractive indices of the two media, the light will either be refracted to a lesser angle, or a greater one. These angles are measured with respect to the "normal line", represented perpendicular to the boundary. In the case of light traveling from air into water, light would be refracted towards the normal line, because the light is slowed down in water; light traveling from water to air would refract away from the normal line.
Refraction between two surfaces is also referred to as "reversible" because if all conditions were identical, the angles would be the same for light propagating in the opposite direction.
Snell's law is generally true only for isotropic or specular media (such as glass). In anisotropic media such as some crystals, birefringence may split the refracted ray into two rays, the "ordinary" or "o"-ray which follows Snell's law, and the other "extraordinary" or "e"-ray which may not be co-planar with the incident ray.
When the light or other wave involved is monochromatic, that is, of a single frequency, Snell's law can also be expressed in terms of a ratio of wavelengths in the two media, λ1 and λ2:
Derivations and formula.
Snell's law may be derived from Fermat's principle, which states that the light travels the path which takes the least time. By taking the derivative of the optical path length, the stationary point is found giving the path taken by the light (though it should be noted that the result does not show light taking the least time path, but rather one that is stationary with respect to small variations as there are cases where light actually takes the greatest time path, as in a spherical mirror). In a classic analogy, the area of lower refractive index is replaced by a beach, the area of higher refractive index by the sea, and the fastest way for a rescuer on the beach to get to a drowning person in the sea is to run along a path that follows Snell's law.
Alternatively, Snell's law can be derived using interference of all possible paths of light wave from source to observer—it results in destructive interference everywhere except extrema of phase (where interference is constructive)—which become actual paths.
Another way to derive Snell’s Law involves an application of the general boundary conditions of Maxwell equations for electromagnetic radiation.
Yet another way to derive Snell's law is based on translation symmetry considerations. For example, a homogeneous surface perpendicular to the z direction cannot change the transverse momentum. Since the propagation vector formula_8 is proportional to the photon's momentum, the transverse propagation direction formula_9 must remain the same in both regions. Assuming without loss of generality a plane of incidence in the formula_10 plane formula_11. Using the well known dependence of the wave number on the refractive index of the medium, we derive Snell's law immediately.
where formula_15 is the wavenumber in vacuum. Note that no surface is truly homogeneous, in the least at the atomic scale. Yet full translational symmetry is an excellent approximation whenever the region is homogeneous on the scale of the light wavelength.
Vector form.
Given a normalized light vector l (pointing from the light source toward the surface) and a normalized plane normal vector n, one can work out the normalized reflected and refracted rays, via the cosines of the angle of incidence formula_16 and angle of refraction formula_17, without explicitly using the sine values or any trigonometric functions or angles:
Note: formula_19 must be positive, which it will be if n is the normal vector that points from the surface toward the side where the light is coming from, the region with index formula_5. If formula_19 is negative, then n points to the side without the light, so start over with n replaced by its negative. 
This reflected direction vector points back toward the side of the surface where the light came from.
Now apply Snell's law to the ratio of sines to derive the formula for the refracted ray's direction vector:
The formula may appear simpler in terms of renamed simple values formula_26 and formula_27, avoiding any appearance of trig function names or angle names:
Example:
The cosine values may be saved and used in the Fresnel equations for working out the intensity of the resulting rays.
Total internal reflection is indicated by a negative radicand in the equation for formula_32, which can only happen for rays crossing into a less-dense medium (formula_33).
Total internal reflection and critical angle.
When light travels from a medium with a higher refractive index to one with a lower refractive index, Snell's law seems to require in some cases (whenever the angle of incidence is large enough) that the sine of the angle of refraction be greater than one. This of course is impossible, and the light in such cases is completely reflected by the boundary, a phenomenon known as total internal reflection. The largest possible angle of incidence which still results in a refracted ray is called the critical angle; in this case the refracted ray travels along the boundary between the two media.
For example, consider a ray of light moving from water to air with an angle of incidence of 50°. The refractive indices of water and air are approximately 1.333 and 1, respectively, so Snell's law gives us the relation
which is impossible to satisfy. The critical angle θcrit is the value of θ1 for which θ2 equals 90°:
Dispersion.
In many wave-propagation media, wave velocity changes with frequency or wavelength of the waves; this is true of light propagation in most transparent substances other than a vacuum. These media are called dispersive. The result is that the angles determined by Snell's law also depend on frequency or wavelength, so that a ray of mixed wavelengths, such as white light, will spread or disperse. Such dispersion of light in glass or water underlies the origin of rainbows and other optical phenomena, in which different wavelengths appear as different colors.
In optical instruments, dispersion leads to chromatic aberration; a color-dependent blurring that sometimes is the resolution-limiting effect. This was especially true in refracting telescopes, before the invention of achromatic objective lenses.
Lossy, absorbing, or conducting media.
In a conducting medium, permittivity and index of refraction are complex-valued. Consequently, so are the angle of refraction and the wave-vector. This implies that, while the surfaces of constant real phase are planes whose normals make an angle equal to the angle of refraction with the interface normal, the surfaces of constant amplitude, in contrast, are planes parallel to the interface itself. Since these two planes do not in general coincide with each other, the wave is said to be inhomogeneous. The refracted wave is exponentially attenuated, with exponent proportional to the imaginary component of the index of refraction.

</doc>
<doc id="42965" url="http://en.wikipedia.org/wiki?curid=42965" title="Éire">
Éire

Éire (; ]) is Irish for "Ireland", the name of an island and a sovereign state.
Etymology.
The modern Irish "Éire" evolved from the Old Irish word "Ériu", which was the name of a Gaelic goddess. "Ériu" is generally believed to have been the matron goddess of Ireland, a goddess of sovereignty, or simply a goddess of the land. The origin of "Ériu" has been traced to the Proto-Celtic reconstruction *"Φīwerjon-" (nominative singular "Φīwerjū" < Pre-Proto-Celtic "-jō"). This suggests a descent from the Proto-Indo-European reconstruction *"piHwerjon-", likely related to the adjectival stem *"piHwer-" (cf. Sanskrit "pīvan", "pīvarī" and "pīvara" meaning "fat, full, abounding"). This would suggest a meaning of "abundant land".
This Proto-Celtic form became "Īweriū" or "Īveriū" in Proto-Goidelic. It is highly likely that explorers borrowed and modified this term. During his exploration of northwest Europe (circa 320 BC), Pytheas of Massilia called the island "Ierne" (written Ἰέρνη). In his book "Geographia" (circa 150 AD), Claudius Ptolemaeus called the island "Iouernia" (written Ἰουερνία). Based on these historical accounts, the Roman Empire called the island "Hibernia".
Thus, the evolution of the word would follow as such:
A 19th century proposal, which does not follow modern standards of etymology, derives the name from Scottish Gaelic:
This is similar in meaning to the Norse name for Irish people, "west men", which subsequently gave its name to the Icelandic island of Vestmannaeyjar.
Difference between "Éire" and "Erin".
While "Éire" is simply the name for the island of Ireland in the Irish language, and sometimes used in English, "Erin" is a common poetic name for Ireland, as in "Erin go bragh". The distinction between the two is one of the difference between cases of nouns in Irish. "Éire" is the nominative case, the case that (in the modern Gaelic languages) is used for nouns that are the subject of a sentence, i.e., the noun that is "doing" something as well as the direct object of a sentence. "Erin" derives from "Éirinn", the Irish dative case of "Éire", which has replaced the nominative case in Déise Irish and some non-standard sub-dialects elsewhere, in Scottish Gaelic (where the usual word for Ireland is "Èirinn") and Manx (a form of Gaelic), where the word is spelled "Nerin," with the initial "n-" probably representing a fossilisation of the preposition "in"/"an" "in" (cf. Irish "in Éirinn", Scottish "an Èirinn"/"ann an Èirinn" "in Ireland"). The genitive case, "Éireann", is used in the Gaelic forms of the titles of companies and institutions in Ireland e.g. "Iarnród Éireann" ("Irish Rail"), "Dáil Éireann" ("Irish Parliament") or "Poblacht na hÉireann" ("The Republic of Ireland").
Éire as a state name.
Article 4 of the Irish constitution adopted in 1937 by the government under Éamon de Valera states that "Éire" is the name of the state, or in the English language, "Ireland". The Constitution's English-language preamble also described the population as "We, the people of Éire". Despite the fact that Article 8 designated Irish as the "national" and "first official" language, "Éire" has to some extent passed out of everyday conversation and literature, and the state is referred to as "Ireland" or its equivalent in all other languages.
The name "Éire" has been used on Irish postage stamps since 1922; on all Irish coinage (including Irish euro coins); and together with "Ireland" on passports and other official state documents issued since 1937. "Éire" is used on the Seal of the President of Ireland. 
The United Kingdom insisted on using only the name "Eire" and refused to accept the name "Ireland". It adopted the Eire (Confirmation of Agreements) Act 1938 putting in law that position. At the 1948 Summer Olympics the organisers insisted that the Irish team march under the banner "Eire" notwithstanding that every other team was marching according to what their name was in English. The UK Government used what some Irish politicians stated were "sneering titles such as Eirish". The UK Government would refer to "Eire Ministers" and the "Eireann Army" and generally avoid all reference to "Ireland" in connection with the state.
Before the 1937 Constitution, "Saorstát Éireann" (the Irish name of the Irish Free State) was generally used.
During the Emergency (as World War II was known), Irish ships had "EIRE" (and the Irish tricolour) painted large on their sides and deck, to identify them as neutrals.
In 1922–1938 the international plate on Irish cars was "SE". From 1938 to 1962 it was marked "EIR", short for Éire. In 1961 statutory instrument no. 269 allowed "IRL", and by 1962 "IRL" had been adopted. Irish politician Bernard Commons TD suggested to the Dáil in 1950 that the government examine "the tourist identification plate bearing the letters EIR ... with a view to the adoption of identification letters more readily associated with this country by foreigners". "EIR" is also shown in other legislation such as the car insurance statutory instrument no. 383 of 1952 and no. 82 of 1958.
Under the 1947 Convention Irish-registered aircraft have carried a registration mark starting "EI" for Éire.
From January 2007, the Irish government nameplates at meetings of the European Union have borne both "Éire" and "Ireland", following the adoption of Irish as a working language of the European Union.
Spelling "Eire" rather than "Éire".
When Irish-language texts were printed in Gaelic type, diacritics were retained on upper-case letters as for lower-case letters. From the later 1940s, in conjunction with other reforms, printing switched to the same "Roman type" used for most other Latin alphabet languages. There was some uncertainty about whether the "síneadh fada" (acute accent) should be written on upper-case letters. While it was preserved in all-Irish texts, it was often omitted when short fragments of Irish appeared alone or in English texts. Noel Davern asked in the Dáil in 1974 why Irish stamps had "EIRE" rather than "ÉIRE". The reply from the Minister for Posts and Telegraphs was:
The spelling "Eire" is generally deplored by Irish-speakers as worse than a misspelling, because "eire" is a separate word, meaning "a burden, load or encumbrance". The minister in 1974 stated, "The word on the stamp ... does not mean 'eire' and it is not understood to mean 'eire' by anybody except Davern." Stamps later reverted to a Gaelic type with the accent preserved.
In 1938 the British government provided in the Eire (Confirmation of Agreements) Act 1938 that British legislation could henceforth refer to the Irish Free State as "Eire" (but not as "Ireland"). The 1938 Act was repealed in 1981, and in 1996 a British journalist described "Eire" as "now an oddity rarely used, an out-of-date reference".
Founded in 1937, the Eire Society of Boston is an influential Irish-American group.
Other uses.
Éire has also been incorporated into the names of Irish commercial and social entities, such as Eircom Group plc (formerly "Telecom Éireann") and its former mobile phone network, Eircell. In 2006 the Irish electricity network was devolved to EirGrid. The company "BetEire Flow" (eFlow), named as a pun on "better", is a French consortium running the electronic tolling system at the West-Link bridge west of Dublin. According to the Dublin Companies Registration Office in 2008, over 500 company names incorporate the word Éire in some form.
Sometimes the incorporation is humorous or ironic, such as the Hip Hop group named "ScaryÉire", or Cormac Ó Gráda's "Éirvana" paper in 2007 on the Celtic Tiger economy.

</doc>
<doc id="42966" url="http://en.wikipedia.org/wiki?curid=42966" title="Montreal Canadiens">
Montreal Canadiens

The Montreal Canadiens (French: "Les Canadiens de Montréal") are a professional ice hockey team based in Montreal, Quebec, Canada. They are members of the Atlantic Division in the Eastern Conference of the National Hockey League (NHL). The club's official name is le Club de hockey Canadien. French nicknames for the team include "Les Canadiens" (or "Le Canadien"), "Le Bleu-Blanc-Rouge", "La Sainte-Flanelle", "Le Tricolore", "Les Glorieux" (or "Nos Glorieux"), "Les Habitants", "Le CH" and "Le Grand Club". The team's main English nickname is the "Habs", an abbreviation of "Les Habitants".
Founded in 1909, the Canadiens are the longest continuously operating professional ice hockey team and the only existing NHL club to predate the founding of the NHL. One of the oldest North American professional sports franchises, the Canadiens' history predates that of every other Canadian franchise outside of football as well as every American franchise outside of baseball and the National Football League's Arizona Cardinals. The franchise is one of the "Original Six" teams, a description used for the teams that made up the NHL from 1942 until the 1967 expansion. The team's championship season in 1992–93 was the last time a Canadian team won the Stanley Cup.
The Canadiens have won the Stanley Cup more times than any other franchise. They have won 24 championships, 22 of them since 1927, when NHL teams became the only ones to compete for the Stanley Cup. On a percentage basis, as of 2014, the franchise has won 25.3% of all Stanley Cup championships contested after the Challenge Cup era, making it the second most successful professional sports team of the traditional four major sports of Canada and the United States, behind only the Boston Celtics.
Since 1996, the Canadiens have played their home games at the Bell Centre, originally known as the Molson Centre. The team previously played at the Montreal Forum which housed the team for seven decades and all but their first two Stanley Cup championships.
History.
The Canadiens were founded by J. Ambrose O'Brien on December 4, 1909, as a charter member of the National Hockey Association,
the forerunner to the National Hockey League. It was to be the team of the francophone community in Montreal, composed of francophone players, and under francophone ownership as soon as possible. The team's first season was not a success, as they placed last. After the first year, ownership was transferred to George Kennedy of Montreal and the team's fortunes improved over the next seasons. The team won its first Stanley Cup championship in the 1915–16 season. In 1917, with four other NHA teams, the Canadiens formed the NHL, and they won their first NHL Stanley Cup during the 1923–24 season, led by Howie Morenz. The team moved from the Mount Royal Arena to the Montreal Forum for the 1926–27 season.
In the 1930s, the club started the decade successfully with Stanley Cup wins in 1930 and 1931. However, the club and its then Montreal rival, the Montreal Maroons, declined both on the ice and economically during the Depression. Losses grew to the point where the team owners considering selling the team to Cleveland, Ohio interests. However, local investors were found to maintain the Canadiens. The Maroons still suspended operations, and several of their players moved to the Canadiens.
Led by the "Punch Line" of Maurice "Rocket" Richard, Toe Blake and Elmer Lach in the 1940s, the Canadiens enjoyed success again atop the NHL. From 1953 to 1960, the franchise won six Stanley Cups, including a record five straight from 1956 to 1960, with a new set of stars coming to prominence: Jean Beliveau, Dickie Moore, Doug Harvey, Bernie "Boom Boom" Geoffrion, Jacques Plante, and Richard's younger brother, Henri.
The Canadiens added ten more championships in fifteen seasons from 1965 to 1979, with another dynastic run of four straight Cups from 1976 to 1979. In the 1976–77 season, the Canadiens set two still-standing team records, for most points, with 132, and fewest losses, by only losing eight games in an 80-game season. The next season 1977–78, they had a 28-game unbeaten streak, the second-longest in NHL history. The next generation of stars included Guy Lafleur, Yvan Cournoyer, Ken Dryden, Pete Mahovlich, Jacques Lemaire, Pierre Larouche, Steve Shutt, Bob Gainey, Serge Savard, Guy Lapointe and Larry Robinson. Scotty Bowman, who would later set a record for most NHL victories by a coach, was the team's head coach for its last five Stanley Cup victories in the 1970s.
The Canadiens won Stanley Cups in 1986, led by rookie star goaltender Patrick Roy, and in 1993, continuing their streak of winning at least one championship in every decade from the 1910s to the 1990s (this streak ended in the 2000s). In 1996, the Habs moved from the Montreal Forum, their home during 70 seasons and 22 Stanley Cups, to the Molson Centre (now the Bell Centre).
Following Roy's departure in 1995, the Canadiens fell into an extended stretch of mediocrity, missing the playoffs in four of their next ten seasons and failing to advance past the second round of the playoffs until 2010. By the late 1990s, with both an ailing team and monetary losses helped by a record-low value of the Canadian dollar, Montreal fans feared their team would end up relocated to the United States. But following the acquisition of the franchise and its stadium by American businessman George N. Gillett Jr. in 2001, the Canadiens returned to being a lucrative enterprise, with enhanced income from broadcasting and arena events, and president Pierre Boivin leading the team improvement. Gillett sold the franchise to the Molson family in 2009 for $575 million, more than double the $275 million he spent on the purchase eight years prior.
During the 2008-09 NHL season, the Canadiens celebrated their 100th anniversary with various events, including hosting both the 2009 NHL All-Star Game, and the 2009 NHL Entry Draft. Said season also marked the Canadiens as the first team in NHL history to reach 3,000 victories, reaching the milestone on their 5-2 defeat of the Florida Panthers on December 29, 2008.
Team identity.
Logo and jersey design.
One of sport's oldest and most recognizable logos, the classic 'C' and 'H' of the Montreal Canadiens was first used together in the 1917–18 season, when the club changed its name to "Club de hockey Canadien" from "Club athlétique Canadien", before evolving to its current form in 1952–53. The "H" stands for "hockey", not "Habitants", a popular misconception. According to NHL.com, the first man to refer to the team as "the Habs" was American Tex Rickard, owner of the Madison Square Garden, in 1924. Rickard apparently told a reporter that the "H" on the Canadiens' sweaters was for "Habitants".
The team's colours since 1911 are blue, red, and white. The home sweater is predominantly red in colour. There are four blue and white stripes, one across each arm, one across the chest and the other across the waistline. The main road sweater is mainly white with a red and blue stripe across the waist, red at the end of both arm sleeves red shoulder yokes. The basic design has been in use since 1914 and took its current form in 1925, generally evolving as materials changed. Because of the team's lengthy history and significance in Quebec, the sweater has been referred to as 'La Sainte-Flanelle' (the holy flannel sweater).
The Canadiens used multiple designs prior to adopting the aforementioned design in 1914. The original shirt of the 1909–10 season was blue with a white C. The second season had a red shirt featuring a green maple leaf with the C logo, and green pants. Lastly, the season before adopting the current look the Canadiens wore a "barber pole" design jersey with red, white and blue stripes, and the logo being a white maple leaf reading "CAC", "Club athlétique Canadien". All three designs were worn during the 2009–10 season as part of the Canadiens centenary.
The Canadiens' colours are a readily identifiable aspect of French Canadian culture. In the short story "The Hockey Sweater", Roch Carrier described the influence of the Canadiens and their jersey within rural Quebec communities during the 1940s.
The story was later made into an animated short, "The Sweater", narrated by Carrier.
A passage from the short story appears on the 2002 issue of the Canadian five dollar bill.
Motto.
"Nos bras meurtris vous tendent le flambeau, à vous toujours de le porter bien haut."
"To you from failing hands we throw the torch. Be yours to hold it high."
The motto is from the poem "In Flanders Fields" by John McCrae which was written in 1915, the year the Canadiens won their first Stanley Cup championship. The motto appears on the wall of the Canadiens dressing room, originally at the Montreal Forum and currently at the Bell Centre.
Mascot.
Beginning in the 2004–05 NHL season, the Canadiens adopted Youppi as their official mascot, the first costumed mascot in their long history. Youppi was the longtime mascot for the Montreal Expos baseball team, but was dropped from the franchise when they moved to Washington, D.C. in 2004 and became the Washington Nationals. With the switch, Youppi became the first mascot in professional sports to switch leagues.
Rivalries.
The Canadiens have developed strong rivalries with two fellow Original Six franchises, with whom they frequently shared divisions and had playoff confrontations. The oldest is with the Toronto Maple Leafs, who first faced the Canadiens in 1917 (still under the name Toronto Arenas), and have met the team 15 times in the playoffs, including five Stanley Cup finals; the rivalry has the symbolism of featuring the two biggest Canadian cities, each of whom is a center of the country's main ethnicities (English and French Canadians). Another equally notable rivalry concerns the Boston Bruins, that since their NHL debut in 1924 have played the Canadiens in both regular season play and the playoffs combined, more than any other two teams in NHL history. These include 34 playoff series, seven of which were in the finals. Between 1979 and 1995, the Canadiens also held an intraprovincial rivalry with the Quebec Nordiques, which earned the nickname "Battle of Quebec". This waned off once the Nordiques moved to Denver and became the Colorado Avalanche.
Broadcasting.
Montreal Canadiens games are broadcast locally in both the French and English languages. On radio, Canadiens games are broadcast in French by CHMP 98.5, and in English by CKGM, "TSN Radio 690", who acquired the English broadcast rights under a 7-year deal which began in the 2011-12 season.
Regional television rights in French are currently held by Réseau des sports under a 12-year deal, effective as of the 2014–15 NHL season. A sister to the English-language network TSN, RDS was the only French-language sports channel in Canada until the 2011 launch of TVA Sports, and was also the previous national French rightsholder of the NHL. As such, the team forewent a separate regional contract, and allowed its games to be televised nationally as part of RDS's rights. As TVA Sports is now the national French rightsholder through a sub-licensing agreement with Rogers Communications, as of the 2014–15 season, RDS parent company Bell Media subsequently announced a 12-year deal to maintain regional rights to Canadiens games not shown on TVA Sports. Games on RDS are now blacked out outside of the Canadiens' home market of Quebec, Atlantic Canada and parts of Ontario. A total of 22 Canadiens games per season will be televised nationally by TVA Sports, primarily through its Saturday night "La super soirée LNH".
Regional television rights in English are held by Sportsnet East in a 3-year deal announced by Rogers on September 2, 2014, with selected games (three in its inaugural season) airing on CJNT "City Montreal". The remaining games are aired nationally through Rogers' aforementioned NHL rights deal (including additional games on Sportsnet, City, or on CBC during "Hockey Night in Canada"), thus giving Rogers rights to over all English-language telecasts of the Canadiens. Regional Canadiens games on Sportsnet East and City are called by John Bartlett and Jason York. TSN previously held regional, English-language television rights to the Canadiens from 2010 through 2014. They were broadcast on a part-time TSN feed with Dave Randorf on play-by-play; these rights were not renewed by Bell Media past the 2013–14 season.
Seasons and records.
Season by season results.
"This is a list of the last five seasons completed by the Canadiens. For the full season-by-season history, see List of Montreal Canadiens seasons."
"Note: GP = Games played, W = Wins, L = Losses, T = Ties, OTL = Overtime Losses, Pts = Points, GF = Goals for, GA = Goals against, PIM = Penalties in minutes"
Franchise individual records.
Franchise scoring leaders.
These are the top-ten point-scorers in franchise history. Figures are updated after each completed NHL regular season.
"Note: Pos = Position; GP = Games Played; G = Goals; A = Assists; Pts = Points; P/G = Points per game
Honoured members.
Retired numbers.
The Canadiens have retired fifteen numbers in honour of nineteen players, the most of any team in the National Hockey League. All of the honourees were born in Canada. Howie Morenz was the first honouree on November 2, 1937.
Hockey Hall of Fame.
Sixty-two people associated with the Canadiens have been inducted into the Hockey Hall of Fame. Thirty-six of these players are from three separate notable dynasties: 12 from 1955–60, 11 from 1964–69 and 13 from 1975–79. Howie Morenz and Georges Vezina were the first Canadiens given the honour in 1945, while Chris Chelios was the most recently inducted, in 2013.
The following are members of the Hockey Hall of Fame in the Builders category. The first inductee was Vice President William Northy in 1945. The most recent inductee was coach Pat Burns in 2014.
External links.
class="wikitable succession-box" style="margin:0.5em auto; font-size:95%;clear:both;"

</doc>
<doc id="42967" url="http://en.wikipedia.org/wiki?curid=42967" title="Ornithology">
Ornithology

Ornithology is a branch of zoology that concerns the study of birds. Etymologically, the word "ornithology" derives from the ancient Greek ὄρνις "ornis" ("bird") and λόγος "logos" ("rationale" or "explanation"). Several aspects of ornithology differ from related disciplines, due partly to the high visibility and the aesthetic appeal of birds. Most marked among these is the extent of studies undertaken by amateurs working within the parameters of strict scientific methodology.
The science of ornithology has a long history and studies on birds have helped develop several key concepts in evolution, behaviour and ecology such as the definition of species, the process of speciation, instinct, learning, ecological niches, guilds, island biogeography, phylogeography and conservation. While early ornithology was principally concerned with descriptions and distributions of species, ornithologists today seek answers to very specific questions, often using birds as models to test hypotheses or predictions based on theories. Most modern biological theories apply across taxonomic groups and the number of professional scientists who identify themselves as "ornithologists" has therefore declined. A wide range of tools and techniques are used in ornithology, both inside the laboratory and out in the field, and innovations are constantly made.
Etymology.
The origins of the word "ornithology" come from the Greek "ornithologos" and late 17th century Latin "ornithologia" meaning "bird science".
History.
The history of ornithology largely reflects the trends in the history of biology, as well as many other scientific disciplines, including ecology, anatomy, physiology, paleontology, and more recently molecular biology. Trends include the move from mere descriptions to the identification of patterns, and thus towards elucidating the processes that produce these patterns.
Early knowledge and study.
Humans have had an observational relationship with birds since prehistory, with some stone age drawings being amongst the oldest indications of an interest in birds. Birds were perhaps important as a food source, and bones of as many as 80 species have been found in excavations of early Stone Age settlements. Waterbird and seabird remains have also been found in shell mounds on the island of Oronsay off the coast of Scotland.
Cultures around the world have rich vocabularies related to birds. Traditional bird names are often based on detailed knowledge of the behaviour, with many names being onomatopoeic, many still in use. Traditional knowledge may also involve the use of birds in folk medicine and knowledge of these practices are passed on through oral traditions (see ethno-ornithology). Hunting of wild birds as well as their domestication would have required considerable knowledge of their habits. Poultry farming and falconry were practised from early times in many parts of the world. Artificial incubation of poultry was practised in China around 246 BC and around at least 400 BC in Egypt. The Egyptians also made use of birds in their hieroglyphic scripts, many of which, though stylized, are still identifiable to species.
Early written records provide valuable information on the past distributions of species. For instance Xenophon records the abundance of the ostrich in Assyria (Anabasis, i. 5); this subspecies from Asia minor is extinct and all extant ostrich races are today restricted to Africa. Other old writings such as the "Vedas" (1500–800 BC) demonstrate the careful observation of avian life histories and includes the earliest reference to the habit of brood parasitism by the Asian koel ("Eudynamys scolopacea"). Like writing, the early art of China, Japan, Persia and India also demonstrate knowledge, with examples of scientifically accurate bird illustrations.
Aristotle in 350 BC in his "Historia Animalium" noted the habit of bird migration, moulting, egg laying and life spans, as well as compiling a list of 170 different bird species. However, he also introduced and propagated several myths, such as the idea that swallows hibernated in winter, although he noted that cranes migrated from the steppes of Scythia to the marshes at the headwaters of the Nile. The idea of swallow hibernation became so well established that, even as late as in 1878, Elliott Coues could list as many as 182 contemporary publications dealing with the hibernation of swallows and little published evidence to contradict the theory. Similar misconceptions existed regarding the breeding of barnacle geese. Their nests had not been seen and it was believed that they grew by transformations of goose barnacles, an idea that became prevalent from around the 11th century and noted by Bishop Giraldus Cambrensis (Gerald of Wales) in "Topographia Hiberniae" (1187). Around 77 AD, Pliny the Elder described birds, among other creatures, in his "Historia Naturalis".
The origins of falconry have been traced to Mesopotamia and the earliest record comes from the reign of Sargon II (722–705 BC). Falconry made its entry to Europe only after AD 400, brought in from the East after invasions by the Huns and Allans. Frederick II of Hohenstaufen (1194–1250) learnt about Arabian falconry during wars in the region and obtained an Arabic treatise on falconry by Moamyn. He had this work translated into Latin and also conducted experiments on birds in his menagerie. By sealing the eyes of vultures and placing food nearby, he concluded that they found food by sight, and not by smell. He also developed methods to keep and train falcons. The studies that he undertook over nearly 30 years, were published in 1240 as De Arte Venandi cum Avibus (The Art of Hunting with Birds), considered one of the earliest studies on bird behaviour, and the first work known to include illustrations of birds.
Several early German and French scholars compiled old works and conducted new research on birds. These included Guillaume Rondelet who described his observations in the Mediterranean and Pierre Belon who described the fish and birds that he had seen in France and the Levant. Belon's Book of Birds (1555) is a folio volume with descriptions of some two hundred species. His comparison of the skeleton of humans and birds is considered as a landmark in comparative anatomy. Volcher Coiter (1534–1576), a Dutch anatomist made detailed studies of the internal structures of birds and produced a classification of birds, "De Differentiis Avium" (around 1572), that was based on structure and habits. Konrad Gesner wrote the "Vogelbuch" and "Icones avium omnium" around 1557. Like Gesner, Ulisse Aldrovandi, an encyclopedic naturalist began a 14-volume natural history with three volumes on birds, entitled "ornithologiae hoc est de avibus historiae libri XII" which was published from 1599 to 1603. Aldrovandi showed great interest in plants and animals and his work included 3000 drawings of fruits, flowers, plants and animals, published in 363 volumes. His "Ornithology" alone covers 2000 pages and included such aspects as the chicken and poultry techniques. He used a number of traits including behaviour, particularly bathing and dusting, to classify bird groups.
William Turner's "Historia Avium" ("History of Birds"), published at Cologne in 1544, was an early ornithological work from England. He noted the commonness of kite in English cities where they snatched food out of the hands of children. He included folk beliefs such as those of anglers. Anglers believed that the osprey emptied their fishponds and would kill them, mixing the flesh of the osprey into their fish bait. Turner's work reflected the violent times that he lived in and stands in contrast to later works such as Gilbert White's "The Natural History and Antiquities of Selborne" that were written in a tranquil era.
In the 17th century Francis Willughby (1635–1672) and John Ray (1627–1705) came up with the first major system of bird classification that was based on function and morphology rather than on form or behaviour. Willughby's "Ornithologiae libri tres" (1676) completed by John Ray is sometimes considered to mark the beginning of scientific ornithology. Ray also worked on "Ornithologia" which was published posthumously in 1713 as "Synopsis methodica avium et piscium". The earliest list of British birds, "Pinax Rerum Naturalium Britannicarum" was written by Christopher Merrett in 1667, but authors such as John Ray considered it of little value. Ray did however, value the expertise of the naturalist Sir Thomas Browne (1605–82) who, not only answered his queries on ornithological identification and nomenclature, but also those of Willoughby and Merrett in letter correspondence. Browne himself in his lifetime kept an eagle, owl, cormorant, bittern and ostrich, penned a tract on falconry, and introduced the words "incubation" and "oviparous" into the English language. 
Towards the late 18th century, Mathurin Jacques Brisson (1723–1806) and Comte de Buffon (1707–1788) began new works on birds. Brisson produced a six-volume work "Ornithologie" in 1760 and Buffon's included nine volumes (volumes 16–24) on birds "Histoire naturelle des oiseaux" (1770–1785) in his work on science "Histoire naturelle générale et particulière" (1749–1804). Coenraad Jacob Temminck (1778–1858) sponsored François Le Vaillant [1753–1824] to collect bird specimens in Africa and this resulted in Le Vaillant's six-volume "Histoire naturelle des oiseaux d'Afrique" (1796–1808). Louis Jean Pierre Vieillot (1748–1831) spent ten years studying North American birds and wrote the "Histoire naturelle des oiseaux de l'Amerique septentrionale" (1807–1808?). Vieillot pioneered in the use of life-histories and habits in classification. Alexander Wilson composed a nine-volume work, American Ornithology, published 1808-14—the first such record of North American birds, significantly predating Audubon. In the early 19th century, Lewis and Clark studied and identified many birds in the western United States. John James Audubon, born in 1785, observed and painted birds in France and later in the Ohio and Mississippi valleys. From 1827 to 1838, Audubon published "The Birds of America", which was engraved by Robert Havell, Sr. and his son Robert Havell, Jr.. Containing 435 engravings, it is often regarded as the greatest ornithological work in history.
Scientific studies.
The emergence of ornothology as a scientific discipline began in the 18th century when Mark Catesby published his two-volume "Natural History of Carolina, Florida and the Bahama Islands", a landmark work which included 220 hand-painted engravings and was the basis for many of the species Carolus Linnaeus described in the 1758 "Systema Naturae". Linnaeus' work revolutionised bird taxonomy by assigning every species a binomial name, categorising them into different genera.
However, it was not until the Victorian era—with the concept of natural history, and the collection of natural objects such as bird eggs and skins—that ornithology emerged as a specialised science. This specialization led to the formation in Britain of the British Ornithologists' Union in 1858. In 1859 the members founded its journal "The Ibis". The sudden spurt in ornithology was also due in part to colonialism. A hundred years later, in 1959, R. E. Moreau noted that ornithology in this period was preoccupied with the geographical distributions of various species of birds.
No doubt the preoccupation with widely extended geographical ornithology, was fostered by the immensity of the areas over which British rule or influence stretched during the 19th century and for some time afterwards.—Moreau
The bird collectors of the Victorian era observed the variations in bird forms and habits across geographic regions, noting local specialization and variation in widespread species. The collections of museums and private collectors grew with contributions from various parts of the world. The naming of species with binomials and the organization of birds into groups based on their similarities became the main work of museum specialists. The variations in widespread birds across geographical region caused the introduction of trinomial names.
The search for patterns in the variations of birds was attempted by many. Friedrich Wilhelm Joseph Schelling (1775–1854), his student Johann Baptist von Spix (1781–1826) and several others believed that there was a hidden and innate mathematical order in the forms of birds. They believed that there was a "natural" classification that was superior to "artificial" ones. A particularly popular idea was the Quinarian system popularised by Nicholas Aylward Vigors (1785–1840), William Sharp Macleay (1792–1865), William Swainson and others. The idea was that nature followed a "rule of five" with five groups nested hierarchically. Some had attempted a rule of four, but Johann Jakob Kaup (1803–1873) insisted that the number five was special noting that other natural entities such as the senses also came in fives. He followed this idea and demonstrated his view of the order within the crow family. Where he failed to find 5 genera, he left a blank insisting that a new genus would found to fill these gaps. These ideas were replaced by more complex "maps" of affinities in works by Hugh Edwin Strickland and Alfred Russel Wallace.
The Galapagos finches were especially influential in the development of Charles Darwin's theory of evolution. His contemporary Alfred Russel Wallace also noted these variations and the geographical separations between different forms leading to the study of biogeography. Wallace was influenced by the work of Philip Lutley Sclater on the distribution patterns of birds.
For Darwin, the problem was how species arose from a common ancestor, but he did not attempt to find rules for delineation of species. The species problem was tackled by the ornithologist Ernst Mayr. Mayr was able to demonstrate that geographical isolation and the accumulation of genetic differences led to the splitting of species.
Early ornithologists were preoccupied with matters of species identification. Only systematics counted as true science and field studies were considered inferior through much of the 19th century. In 1901 Robert Ridgway wrote in the introduction to "The Birds of North and Middle America" that:
There are two essentially different kinds of ornithology: systematic or scientific, and popular. The former deals with the structure and classification of birds, their synonymies and technical descriptions. The latter treats of their habits, songs, nesting, and other facts pertaining to their life histories.
This early idea that the study of "living birds" was merely recreation held sway until ecological theories became the predominant focus of ornithological studies. The study of birds in their habitats was particularly advanced in Germany with bird ringing stations established as early as 1903. By the 1920s the "Journal für Ornithologie" included many papers on the behaviour, ecology, anatomy and physiology, many written by Erwin Stresemann. Stresemann changed the editorial policy of the journal, leading both to a unification of field and laboratory studies and a shift of research from museums to universities. Ornithology in the United States continued to be dominated by museum studies of morphological variations, species identities and geographic distributions, until it was influenced by Stresemann's student Ernst Mayr. In Britain, some of the earliest ornithological works that used the word ecology appeared in 1915. "The Ibis" however resisted the introduction of these new methods of study and it was not until 1943 that any paper on ecology appeared. The work of David Lack on population ecology was pioneering. Newer quantitative approaches were introduced for the study of ecology and behaviour and this was not readily accepted. For instance, Claud Ticehurst wrote:
Sometimes it seems that elaborate plans and statistics are made to prove what is commonplace knowledge to the mere collector, such as that hunting parties often travel more or less in circles.—Ticehurst
David Lack's studies on population ecology sought to find the processes involved in the regulation of population based on the evolution of optimal clutch sizes. He concluded that population was regulated primarily by density-dependent controls, and also suggested that natural selection produces life-history traits that maximize the fitness of individuals. Others like Wynne-Edwards interpreted population regulation as a mechanism that aided the "species" rather than individuals. This led to widespread and sometimes bitter debate on what constituted the "unit of selection". Lack also pioneered the use of many new tools for ornithological research, including the idea of using radar to study bird migration.
Birds were also widely used in studies of the niche hypothesis and Georgii Gause's competitive exclusion principle. Work on resource partitioning and the structuring of bird communities through competition were made by Robert MacArthur. Patterns of biodiversity also became a topic of interest. Work on the relationship of the number of species to area and its application in the study of island biogeography was pioneered by E. O. Wilson and Robert MacArthur. These studies led to the development of the discipline of landscape ecology.
John Hurrell Crook studied the behaviour of weaverbirds and demonstrated the links between ecological conditions, behaviour and social systems. Principles from economics were introduced to the study of biology by Jerram L. Brown in his work on explaining territorial behaviour. This led to more studies of behaviour that made use of cost-benefit analyses. The rising interest in sociobiology also led to a spurt of bird studies in this area.
The study of imprinting behaviour in ducks and geese by Konrad Lorenz and the studies of instinct in herring gulls by Nicolaas Tinbergen, led to the establishment of the field of ethology. The study of learning became an area of interest and the study of bird song has been a model for studies in neuro-ethology. The role of hormones and physiology in the control of behaviour has also been aided by bird models. These have helped in the study of circadian and seasonal cycles. Studies on migration have attempted to answer questions on the evolution of migration, orientation and navigation.
The growth of genetics and the rise of molecular biology led to the application of the gene-centered view of evolution to explain avian phenomena. Studies on kinship and altruism, such as helpers, became of particular interest. The idea of inclusive fitness was used to interpret observations on behaviour and life-history and birds were widely used models for testing hypotheses based on theories postulated by W. D. Hamilton and others.
The new tools of molecular-biology changed the study of bird systematics. Systematics changed from being based on phenotype to the underlying genotype. The use of techniques such as DNA-DNA hybridization to study evolutionary relationships was pioneered by Charles Sibley and Jon Edward Ahlquist resulting in what is called the Sibley-Ahlquist taxonomy. These early techniques have been replaced by newer ones based on mitochondrial DNA sequences and molecular phylogenetics approaches that make use of computational procedures for sequence alignment, construction of phylogenetic trees and calibration of molecular clocks to infer evolutionary relationships. Molecular techniques are also widely used in studies of avian population biology and ecology.
Rise to popularity.
The use of field glasses or telescopes for bird observation began in the 1820s and 1830s with pioneers like J. Dovaston (who also pioneered in the use of bird-feeders), but it was not until the 1880s that instruction manuals began to insist on the use of optical aids such as "a first-class telescope" or "field glass."
The rise of field guides for the identification of birds was another major innovation. The early guides such as those of Thomas Bewick (2 volumes) and William Yarrell (3 volumes) were cumbersome, and mainly focused on identifying specimens in the hand. The earliest of the new generation of field guides was prepared by Florence Merriam, sister of Clinton Hart Merriam, the mammalogist. This was published in 1887 in a series "Hints to Audubon Workers:Fifty Birds and How to Know Them" in Grinnell's "Audubon Magazine". These were followed by new field guides including classics by Roger Tory Peterson.
The interest in birdwatching grew in popularity in many parts of the world and it was realized that there was a possibility for amateurs to contribute to biological studies. As early as 1916, Julian Huxley wrote a two part article in the "Auk", noting the tensions between amateurs and professionals and suggested the possibility that the "vast army of bird-lovers and bird-watchers could begin providing the data scientists needed to address the fundamental problems of biology."
Organizations were started in many countries and these grew rapidly in membership, most notable among them being the Royal Society for the Protection of Birds (RSPB) in Britain and the Audubon Society in the US. The Audubon Society started in 1885. Both these organizations were started with the primary objective of conservation. The RSPB, born in 1889, grew from a small group of women in Croydon who met regularly and called themselves the "Fur, Fin and Feather Folk" and who took a pledge "to refrain from wearing the feathers of any birds not killed for the purpose of food, the Ostrich only exempted." The organization did not allow men as members initially, avenging a policy of the British Ornithologists' Union to keep out women. Unlike the RSPB, which was primarily conservation oriented, the British Trust for Ornithology (BTO) was started in 1933 with the aim of advancing ornithological research. Members were often involved in collaborative ornithological projects. These projects have resulted in atlases which detail the distribution of bird species across Britain. In the United States, the Breeding Bird Surveys, conducted by the US Geological Survey have also produced atlases with information on breeding densities and changes in the density and distribution over time. Other volunteer collaborative ornithology projects were subsequently established in other parts of the world.
Techniques.
The tools and techniques of ornithology are varied and new inventions and approaches are quickly incorporated. The techniques may be broadly dealt under the categories of those that are applicable to specimens and those that are used in the field, however the classification is rough and many analysis techniques are usable both in the laboratory and field or may require a combination of field and laboratory techniques.
Collections.
The earliest approaches to modern bird study involved the collection of eggs, a practice known as oology. While collecting became a pastime for many amateurs, the labels associated with these early egg collections made them unreliable for the serious study of bird breeding. In order to preserve eggs, a tiny hole was pierced and the contents extracted. This technique became standard with the invention of the blow drill around 1830. Egg collection is no longer popular; however historic museum collections have been of value in determining the effects of pesticides such as DDT on physiology. Museum bird collections continue to act as a resource for taxonomic studies.
The use of bird skins to document species has been a standard part of systematic ornithology. Bird skins are prepared by retaining the key bones of the wings, leg and skull along with the skin and feathers. In the past, they were treated with arsenic to prevent fungal and insect (mostly dermestid) attack. Arsenic, being toxic, was replaced by borax. Amateur and professional collectors became familiar with these skinning techniques and started sending in their skins to museums, some of them from distant locations. This led to the formation of huge collections of bird skins in museums in Europe and North America. Many private collections were also formed. These became references for comparison of species and the ornithologists at these museums were able to compare species from different locations, often places that they themselves never visited. Morphometrics of these skins, particularly the lengths of the tarsus, bill, tail and wing became important in the descriptions of bird species. These skin collections have been utilized in more recent times for studies on molecular phylogenetics by the extraction of ancient DNA. The importance of type specimens in the description of species make skin collections a vital resource for systematic ornithology. However, with the rise of molecular techniques, it has now become possible to establish the taxonomic status of new discoveries, such as the Bulo Burti Boubou "Laniarius liberatus" (no longer a valid species) and the Bugun Liocichla "Liocichla bugunorum", using blood, DNA and feather samples as the holotype material.
Other methods of preservation include the storage of specimens in spirit. Such wet-specimens have special value in physiological and anatomical study, apart from providing better quality of DNA for molecular studies. Freeze drying of specimens is another technique that has the advantage of preserving stomach contents and anatomy, although it tends to shrink making it less reliable for morphometrics.
In the field.
The study of birds in the field was helped enormously by improvements in optics. Photography made it possible to document birds in the field with great accuracy. High power spotting scopes today allow observers to detect minute morphological differences that were earlier possible only by examination of the specimen "in the hand".
The capture and marking of birds enables detailed studies of life-history. Techniques for capturing birds are varied and include the use of bird liming for perching birds, mist nets for woodland birds, cannon netting for open area flocking birds, the bal-chatri trap for raptors, decoys and funnel traps for water birds.
The bird in the hand may be examined and measurements can be made including standard lengths and weight. Feather moult and skull ossification provide indications of age and health. Sex can be determined by examination of anatomy in some sexually non-dimorphic species. Blood samples may be drawn to determine hormonal conditions in studies of physiology, identify DNA markers for studying genetics and kinship in studies of breeding biology and phylogeography. Blood may also be used to pathogens and arthropod borne viruses. Ectoparasites may be collected for studies of coevolution and zoonoses. In many of cryptic species, measurements (such as the relative lengths of wing feathers in warblers) are vital in establishing identity.
Captured birds are often marked for future recognition. Rings or bands provide long-lasting identification but require capture for the information on them to be read. Field identifiable marks such as coloured bands, wing tags or dyes enable short-term studies where individual identification is required. Mark and recapture techniques make demographic studies possible. Ringing has traditionally been used in the study of migration. In recent times satellite transmitters provide the ability to track migrating birds in near real-time.
Techniques for estimating population density include point counts, transects and territory mapping. Observations are made in the field using carefully designed protocols and the data may be analysed to estimate bird diversity, relative abundance or absolute population densities. These methods may be used repeatedly over large time spans to monitor changes in the environment. Camera traps have been found to be a useful tool for the detection and documentation of elusive species, nest predators and in the quantitative analysis of frugivory, seed dispersal and behaviour.
In the laboratory.
Many aspects of bird biology are difficult to study in the field. These include the study of behavioural and physiological changes that require a long duration of access to the bird. Non-destructive samples of blood or feathers taken during field studies may be studied in the laboratory. For instance, the variation in the ratios of stable hydrogen isotopes across latitudes makes it possible to roughly establish the origins of migrant birds using mass spectrometric analysis of feather samples. These techniques can be used in combination with other techniques such as ringing.
The first attenuated vaccine developed by Louis Pasteur was for fowl cholera and was tested on poultry in 1878. Poultry continues to be used as a model for many studies in non-mammalian immunology.
Studies in bird behaviour include the use of tamed and trained birds in captivity. Studies on bird intelligence and song learning have been largely laboratory based. Field researchers may make use of a wide range of techniques such as the use of dummy owls to elicit mobbing behaviour, dummy males or the use of call playback to elicit territorial behaviour and thereby to establish the boundaries of bird territories.
Studies of bird migration including aspects of navigation, orientation and physiology are often studied using captive birds in special cages that record their activities. The Emlen funnel for instance makes use of a cage with an inkpad at the centre and a conical floor where the ink marks can be counted to identify the direction in which the bird attempts to fly. The funnel can have a transparent top and visible cues such as the direction of sunlight may be controlled using mirrors or the positions of the stars simulated in a planetarium.
The entire genome of the domestic fowl "Gallus gallus" was sequenced in 2004 and was followed in 2008 by the genome of the zebra finch ("Taeniopygia guttata"). Such whole genome sequencing projects allow for studies on evolutionary processes involved in speciation. Associations between the expression of genes and behaviour may be studied using candidate genes. Variations in the exploratory behaviour of great tits ("Parus major") have been found to be linked with a gene orthologous to the human gene "DRD4" (Dopamine receptor D4) which is known to be associated with novelty-seeking behaviour. The role of gene expression in developmental differences and morphological variations have been studied in Darwin's finches. The difference in the expression of "Bmp4" have been shown to be associated with changes in the growth and shape of the beak.
The chicken has long been a model organism for studying vertebrate developmental biology. As the embryo is readily accessible, its development can be easily followed (unlike mice). This also allows the use of electroporation for studying the effect of adding or silencing a gene. Other tools for perturbing their genetic makeup are chicken embryonic stem cells and viral vectors.
Collaborative studies.
With the widespread interest in birds, it has been possible to use a large number of people to work on collaborative ornithological projects that cover large geographic scales. These citizen science projects include nationwide projects such as the "Christmas Bird Count", "Backyard Bird Count", the North American "Breeding Bird Survey", the Canadian EPOQ or regional projects such as the "Asian Waterfowl Census" and "" in Europe. These projects help to identify distributions of birds, their population densities and changes over time, arrival and departure dates of migration, breeding seasonality and even population genetics. The results of many of these projects are published as bird atlases. Studies of migration using bird ringing or colour marking often involve the cooperation of people and organizations in different countries.
Applications.
Wild birds impact many human activities while domesticated birds are important sources of eggs, meat, feathers and other products. Applied and economic ornithology aim to reduce the ill effects of problem birds and enhance gains from beneficial species.
The role of some species of birds as pests has been well known, particularly in agriculture. Granivorous birds such as the queleas in Africa are among the most numerous birds in the world and foraging flocks can cause devastation. Many insectivorous birds are also noted as beneficial in agriculture. Many early studies on the benefits or damages caused by birds in fields were made by analysis of stomach contents and observation of feeding behaviour. Modern studies aimed to manage birds in agriculture make use of a wide range of principles from ecology. Intensive aquaculture has brought humans in conflict with fish-eating birds such as cormorants.
Large flocks of pigeons and starlings in cities are often considered as a nuisance and techniques to reduce their populations or their impacts are constantly innovated. Birds are also of medical importance and their role as carriers of human diseases such as Japanese Encephalitis, West Nile Virus and H5N1 have been widely recognised. Bird strikes and the damage they cause in aviation are of particularly great importance, due to the fatal consequences and the level of economic losses caused. It has been estimated that the airline industry incurs worldwide damages of US$1.2 billion each year.
Many species of birds have been driven to extinction by human activities. Being conspicuous elements of the ecosystem, they have been considered as indicators of ecological health. They have also helped in gathering support for habitat conservation. Bird conservation requires specialized knowledge in aspects of biology, ecology and may require the use of very location specific approaches. Ornithologists contribute to conservation biology by studying the ecology of birds in the wild and identifying the key threats and ways of enhancing the survival of species. Critically endangered species such as the California condor have had to be captured and bred in captivity. Such ex-situ conservation measures may be followed by re-introduction of the species into the wild.

</doc>
<doc id="42969" url="http://en.wikipedia.org/wiki?curid=42969" title="Dodge">
Dodge

Dodge is an American brand of automobiles, minivans, and sport utility vehicles manufactured by FCA US LLC (formerly known as Chrysler Group LLC), based in Auburn Hills, Michigan. Dodge vehicles presently include the lower-priced badge variants of Chrysler-badged vehicles as well as performance cars, though for much of its existence Dodge was Chrysler's mid-priced brand above Plymouth.
Founded as the Dodge Brothers Company by brothers Horace Elgin Dodge and John Francis Dodge in 1900, Dodge was originally a supplier of parts and assemblies for Detroit-based automakers and began building complete automobiles in 1915, predating the founding of Chrysler Corporation. The Dodge brothers died suddenly in 1920 and the company was sold to Dillon, Read & Co. in 1925 before being sold to Chrysler in 1928. Dodge vehicles mainly consisted of trucks and full-sized passenger cars through the 1970s, though it did make some inroads into the compact car market during this time. The 1973 oil crisis and its subsequent impact on the American automobile industry led Chrysler to develop the K platform of compact to midsize cars for the 1981 model year. The K platform and its derivatives are credited with reviving Chrysler's business in the 1980s; one such derivative became the Dodge Caravan.
The Dodge brand has withstood the multiple ownership changes at Chrysler from 1998–2009, including its short-lived merger with Daimler-Benz AG from 1998–2007, its subsequent sale to Cerberus Capital Management, its 2009 bailout by the United States government, and its subsequent Chapter 11 bankruptcy and acquisition by Fiat.
In 2011, Dodge, Ram, and Dodge's Viper were separated. Dodge said that the Dodge Viper will now be an SRT product and Ram will be a manufacturer. In 2014, SRT was merged back into Dodge. Later that year, Chrysler Group was renamed FCA US LLC, corresponding with the merger of Fiat S.p.A. and Chrysler Group into the single corporate structure of Fiat Chrysler Automobiles.
History.
Founding and early years.
After the founding of the Dodge Brothers Company by Horace and John Dodge in 1900, the Detroit-based company quickly found work producing precision engine and chassis components for the city’s growing number of automobile firms. Chief among these customers were the established Olds Motor Vehicle Company and the then-new Ford Motor Company.
By 1914, Horace had created the new four-cylinder Dodge Model 30. Marketed as a slightly more upscale competitor to the ubiquitous Ford Model T, it pioneered or made standard many features later taken for granted: all-steel body construction (the vast majority of cars worldwide still used wood-framing under steel panels, though Stoneleigh and BSA used steel bodies as early as 1911); 12-volt electrical system (6-volt systems would remain the norm until the 1950s); 35 horsepower (versus the Model T's 20), and sliding-gear transmission (the best-selling Model T would retain an antiquated planetary design until its demise in 1927). As a result of this, and the brothers' well-earned reputation for the highest quality truck, transmission and motor parts they made for other successful vehicles, Dodge Brothers cars were ranked at second place for U.S. sales as early as 1916. That same year, Henry Ford decided to stop paying stock dividends to finance the construction of his new River Rouge complex. This led the Dodges to file suit to protect their annual stock earnings of approximately one million dollars, leading Ford to buy out his shareholders; the Dodges were paid some US$25 million.
Also in 1916, the Dodge Brothers' vehicles won acclaim for durability while in service with the U.S. Army's Pancho Villa Expedition into Mexico. One notable instance was in May when the 6th Infantry received a reported sighting of Julio Cárdenas, one of Villa's most trusted subordinates. Lt. George S. Patton led ten soldiers and two civilian guides in three Dodge Model 30 touring cars to conduct a raid at a ranch house in San Miguelito, Sonora. During the ensuing firefight the party killed three men, of whom one was identified as Cárdenas. Patton's men tied the bodies to the hoods of the Dodges, returning to headquarters in Dublán and an excited reception from US newspapermen.
Death of the Dodge brothers, Sale to Chrysler.
Dodge Brothers cars continued to rank second place in American sales in 1920. However, the same year, tragedy struck as John Dodge was felled by pneumonia in January. His brother Horace then died of cirrhosis in December of the same year (reportedly out of grief at the loss of his brother, to whom he was very close). With the loss of both founders, the Dodge Brothers Company passed into the hands of the brothers' widows, who promoted long-time employee Frederick Haynes to the company presidency. During this time, the Model 30 was evolved to become the new Series 116 (though it retained the same basic construction and engineering features). However, throughout the 1920s Dodge gradually lost its ranking as the third best-selling automobile manufacturer, slipping down to seventh in the U.S. market.
Dodge Brothers emerged as a leading builder of light trucks. They also entered into an agreement whereby they marketed trucks built by Graham Brothers of Evansville, Indiana. The three Graham brothers would later produce Graham-Paige and Graham automobiles.
Stagnation in development was becoming apparent, however, and the public responded by dropping Dodge Brothers to fifth place in the industry by 1925. That year, the Dodge Brothers company was sold by the widows to the well-known investment group Dillon, Read & Co. for no less than US$146 million (at the time, the largest cash transaction in history).
Dillon, Read & Co. offered non-voting stock on the market in the new Dodge Brothers, Inc., firm, and along with the sale of bonds was able to raise $160 million, reaping a $14 million (net) profit. All voting stock was retained by Dillon, Read. Frederick Haynes remained as company head until E.G. Wilmer was named board chairman in November, 1926. Wilmer was a banker with no auto experience and Haynes remained as president. Changes to the car, save for superficial things like trim levels and colors, remained minimal until 1927, when the new Senior six-cylinder line was introduced. The former four-cylinder line was kept on, but renamed the Fast Four line until it was dropped in favor of two lighter six-cylinder models (the Standard Six and Victory Six) for 1928.
On October 1, 1925, Dodge Brothers, Inc., acquired a 51% interest in Graham Brothers, Inc., for $13 million and the remaining 49% on May 1, 1926. The three Graham brothers, Robert, Joseph and Ray, assumed management positions in Dodge Brothers before departing early in 1927.
Despite all this, Dodge Brothers’ sales had already dropped to seventh place in the industry by 1927, and Dillon, Read began looking for someone to take over the company on a more permanent basis. Eventually Dodge was sold to Chrysler Corporation in 1928.
Pre-war years.
To fit better in the Chrysler Corporation lineup, alongside low-priced Plymouth and medium-priced DeSoto, Dodge’s lineup for early 1930 was trimmed down to a core group of two lines and thirteen models (from three lines and nineteen models just over a year previous). Prices started out just above DeSoto but were somewhat less than top-of-the-line Chrysler, in a small-scale recreation of General Motors’ “step-up” marketing concept. (DeSoto and Dodge would swap places in the market for the 1933 model year, Dodge dropping down between Plymouth and DeSoto.) 
For 1930, Dodge took another step up by adding a new eight-cylinder line to replace the existing Senior six-cylinder. This basic format of a dual line with Six and Eight models continued through 1933, and the cars were gradually streamlined and lengthened in step with prevailing trends of the day. The Dodge Eight was replaced by a larger Dodge DeLuxe Six for 1934, which was dropped for 1935. A long-wheelbase edition of the remaining Six was added for 1936 and would remain a part of the lineup for many years. 
The Dodge line, along with most of the corporation’s output, was restyled in the so-called “Wind Stream” look for 1935. This was a mild form of streamlining, which saw sales jump remarkably over the previous year (even though Dodge as a whole still dropped to fifth place for the year after two years of holding down fourth). Dodge never got the radical Airflow styling that was the cause of depressed sales of Chryslers and DeSotos from 1934 to 1937. 
 Dodge (along with the rest of Chrysler) added safety features such as a smooth, flat dashboard with no protruding knobs, curved in door handles, and padded front-seat backs for the benefit of the rear-seat occupants. 
Another major restyle arrived for the 25th-anniversary 1939 models, which Dodge dubbed the Luxury Liner series. These were once again completely redesigned, with new bodies for 1940, again in 1941, and a refreshing for 1942. However, just after the 1942 models were introduced, Japan’s attack on Pearl Harbor forced the shutdown of Dodge’s passenger car assembly lines in favor of war production in February 1942.
World War II.
Chrysler was prolific in its production of war materiel from 1942 to 1945, and Dodge in particular was well known to both average citizens and thankful soldiers for their tough military-spec truck models and ambulances like the WC54. Starting with the hastily converted VC series and evolving into the celebrated WC series, Dodge built a strong reputation for itself that readily carried over into civilian models after the war.
Post-war years.
Civilian production at Dodge was restarted by late 1945, in time for the 1946 model year. The "seller's market" of the early postwar years, brought on by the lack of any new cars throughout the war, meant that every automaker found it easy to sell vehicles regardless of any drawbacks they might have. Like almost every other automaker, Dodge sold lightly facelifted revisions of its 1942 design through the 1948 season. As before, these were a single series of six-cylinder models with two trim levels (basic Deluxe or plusher Custom).
Styling was not initially Dodge’s strong point during this period, though that began to change by 1953 under the direction of corporate design chief Virgil Exner. At the same time, Dodge also introduced its first V8 engine – the Red Ram Hemi, a smaller version of the original design of the famed Hemi. The new 1953 bodies were smaller and based on the Plymouth. For 1954, sales dropped, the stubby styling not going over well with the public. 1954 also saw the introduction of the fully automatic PowerFlite transmission.
New corporate “Forward Look” styling for 1955 began a new era for Dodge. With steadily upgraded styling and ever-stronger engines every year through 1960, Dodge found a ready market for its products as America discovered the joys of freeway travel. This situation improved when Dodge introduced a new line of Dodges called the Dart to do battle against Ford, Chevrolet and Plymouth. The result was that Dodge sales in the middle price class collapsed. Special and regional models were sold as well, including the LaFemme (a white and orchid-trimmed hardtop marketed toward women) and the Texan, a gold-accented Dodge sold in the Lone Star State.
Dodge entered the compact car field for 1961 with their new Lancer, a variation on Plymouth's Valiant. Though it was 
not initially successful, the Dart range that succeeded the Lancer in 1963 would prove to be one of the division's top sellers for many years.
Chrysler did make an ill-advised move to downsize the Dodge and Plymouth full-size lines for 1962, which resulted in a loss of sales. However, they turned this around in 1965 by turning those former full-sizes into "new" mid-size models; Dodge revived the Coronet nameplate in this way and later added a sporty fastback version called the Charger that became both a sales leader and a winner on the NASCAR circuit. Not only did this style dominate the racetrack for 4 full years, its aerodynamic improvements forever changed the face a NASCAR racing.
Full-size models evolved gradually during this time. After Dodge dealers complained about not having a true full-size car in the fall of 1961, the Custom 880 was hurried into production. The Custom 880 used the 1962 Chrysler Newport body with the 1961 Dodge front end and interior. The 880 continued into 1965, the year a completely new full-size body was put into production, the Polara entered the medium price class and the Monaco was added as the top series. The Polara and Monaco were changed mostly in appearance for the next ten years or so. Unique "fuselage" styling was employed for 1969 through 1973 and then was toned down again for the 1974 to 1977 models.
Dodge is well-known today for being a player in the muscle car market of the late 1960s and early 1970s. Along with the Charger, models like the Coronet R/T and Super Bee were popular with buyers seeking performance. The pinnacle of this effort was the introduction of the Challenger sports coupe and convertible (Dodge's entry into the "pony car" class ) in 1970, which offered everything from mild economy engines up to the wild race-ready Hemi V8 in the same package.
In an effort to reach every segment of the market, Dodge even reached a hand across the Pacific to its partner, Mitsubishi Motors, and marketed their subcompact as the Colt to compete with the AMC Gremlin, Chevrolet Vega, and Ford Pinto. Chrysler would over the years come to rely heavily on their relationship with Mitsubishi.
Times of crisis.
The 1973 oil crisis caused significant changes at Dodge, as well as Chrysler as a whole. Except for the Colt and Slant Six models of the Dart, Dodge's lineup was quickly seen as extremely inefficient. In fairness, this was true of most American automakers at the time, but Chrysler was also not in the best financial shape to do anything about it. Consequently, while General Motors and Ford were quick to begin downsizing their largest cars, Chrysler (and Dodge) moved more slowly out of necessity.
At the very least, Chrysler was able to use some of its other resources. Borrowing the recently introduced Chrysler Horizon from their European division, Dodge was able to get its new Omni subcompact on the market fairly quickly. At the same time, they increased the number of models imported from Mitsubishi: first came a smaller Colt (based on Mitsubishi's Lancer line), then a revival of the Challenger as a Mitsubishi compact hardtop coupe with nothing more than a four-cylinder under the hood, rather than the booming V8s of yore.
Bigger Dodges, though, remained rooted in old habits. The Dart was replaced by a new Aspen for 1976, and Coronet and Charger were effectively replaced by the Diplomat for 1977, which was actually a fancier Aspen. While the Aspen got accolades for styling and handling, build quality was problematic, sullying the car's reputation at the time when sales were desperately needed. Meanwhile, the huge Monaco (Royal Monaco beginning in 1977 when the mid-sized Coronet was renamed "Monaco") models hung around through 1977, losing sales every year, until finally being replaced by the St. Regis for 1979 following a one-year absence from the big car market. In a reversal of what happened for 1965, the St. Regis was an upsized Coronet. Buyers, understandably, were confused and chose to shop the competition rather than figure out what was going on at Dodge.
Everything came to a head in 1979 when Chrysler's new chairman, Lee Iacocca, requested and received federal loan guarantees from the United States Congress in an effort to save the company from having to file bankruptcy. With a Federal Loan in hand, Chrysler quickly set to work on new models that would leave the past behind, while reorganizing to pay the government loan which stood at 29%.
K-Cars and minivans.
The first fruit of Chrysler's crash development program was the "K-Car", the Dodge version of which was the Dodge Aries. This basic and durable front-wheel drive platform spawned a whole range of new models at Dodge during the 1980s, including the groundbreaking Dodge Caravan. The Caravan not only helped save Chrysler as a serious high-volume American automaker, but also spawned an entirely new market segment that remains popular today: the minivan.
Through the late 1980s and 1990s, Dodge's designation as the sporty-car division was backed by a succession of high-performance and/or aggressively styled models including the Daytona, mid-sized 600 and several versions of the Lancer. The Dodge Spirit sedan was well received in numerous markets worldwide. The Omni remained in the line through 1990. Dodge-branded Mitsubishi vehicles were phased out by 1993 except for the Dodge Stealth running through 1996, though Mitsubishi-made engines and electrical components were still widely used in American domestic Chrysler products. In 1992, Dodge moved their performance orientation forward substantially with the Viper, which featured an aluminum V10 engine and composite sports roadster body. This was the first step in what was marketed as "The New Dodge", which was an aggressive advertising campaign with a litany of new models, with television ads narrated by Edward Herrmann that pointed out the innovations in the vehicles and challenged their competitors. Later that year, was the introduction of new Intrepid sedan, totally different from its boxy Dynasty predecessor and for 1993, the new Dodge Ram pickup was introduced with bold styling. The Intrepid used what Chrysler called "cab forward" styling, with the wheels pushed out to the corners of the chassis for maximum passenger space. They followed up on this idea in a smaller scale with the Stratus and Neon, both introduced for 1995. The Neon in particular was a hit, buoyed by a clever marketing campaign and good performance. 
The modern era.
DaimlerChrysler & private ownership.
In a move that never lived up to the expectations of its driving forces, Chrysler Corporation merged with Daimler-Benz AG in 1998 to form DaimlerChrysler. Rationalizing Chrysler's broad lineup was a priority, and Dodge's sister brand Plymouth was withdrawn from the market. With this move, Dodge became 
DaimlerChrysler's low-price division as well as its performance division.
The Intrepid, Stratus, and Neon updates of the 1998 to 2000 timeframe were largely complete before Daimler's presence, and Dodge's first experience of any platform sharing with the German side of the company was the 2005 Magnum station wagon, introduced as a replacement for the Intrepid. Featuring Chrysler's first mainstream rear-wheel drive platform since the 1980s and a revival of the Hemi V8 engine. The Charger was launched in 2006 on the same platform.
Further cost savings were explored in the form of an extensive platform-sharing arrangement with Mitsubishi, which spawned the Caliber subcompact as a replacement for the Neon and the Avenger sedan. The rear-drive chassis was then used in early 2008 to build a new Challenger, with styling reminiscent of the original 1970 Challenger. Like its predecessor, the new Challenger coupe was available with a powerful V8 engine (base models featured a V6). In Spring 2007, DaimlerChrysler reached an agreement with Cerberus Capital Management to dump its Chrysler Group subsidiary, of which the Dodge division was a part. Soon after, the housing bubble began to collapse the American market, and on May 1, 2009, Chrysler and GM filed for bankruptcy on the same day.
Fiat ownership.
On June 10, 2009, Italian automaker Fiat formed a partnership with Chrysler under Sergio Marchionne, with the UAW, and the US Government to form Chrysler Group LLC, of which Dodge remained fully integrated. For its part, the US Government provided more than $6 billion in loans at 21%, called a "bridge loan" or "bailout". The newly formed company went on to fully repay that loan, remortgaging to reduce the interest rate several times down to 6%. They fully paid back the loan with interest to the U.S. Government on May 24, 2011, a full five years early. The UAW, being partners throughout the process, were paid well and above $3.9 billion in 2013 as Sergio's plan for full consolidation has continued on schedule. This has allowed Chrysler LLC to fully merge with Fiat to form FCA, Fiat Chrysler Automobiles, in 2014. The combined company will be based in London.
In 2013, Dodge re-introduced a compact car based on an Alfa-Romeo design called the Dart. It was the first new Dodge model produced under FCA.
On May 6, 2014, FCA announced a major restructuring, in which Dodge would focus solely on performance vehicles and will be positioned between Chrysler (which is moving downmarket into mainstream vehicles) and a relaunched Alfa Romeo (making its return to North America after a 20-year absence) in the FCA lineup. This is a set up similar to PSA Peugeot Citroën, which positions Peugeot as its conservative mainstream brand while Citroën is more performance-based, as well as Hyundai Motor Group having its two mainstream brands, Kia Motors and Hyundai Motor Company focusing on performance and mid-luxury, respectively. (Among the American press, it has drawn comparisons to the decades-long set up of Chevrolet and Pontiac at General Motors before the phase-out of Pontiac in 2010.) As part of the restructuring, Dodge will discontinue the Dodge Grand Caravan (after 32 years) and Dodge Avenger without replacements, while launching a sporty subcompact below the Dart in 2018. Additionally, while the Ram Trucks division will remain separate (although the Dodge Durango will remain in production as a Dodge), the SRT division was merged back into Dodge.
Dodge trucks.
Over the years, Dodge has become at least as well known for its many truck models as for its prodigious passenger car output. In 2009, trucks were spun off into the Ram brand, named after the brand's most popular truck, the Dodge Ram. However, it should be noted that even though the Ram trucks are marketed separately from Dodge cars, Ram President Fred Diaz has stated that "Ram trucks will always and forever be Dodges. Ram will always have the Dodge emblem inside and outside and they will be 'vinned' (from the acronym VIN, or Vehicle Identification Number) as a Dodge. We need to continue to market as Ram so Dodge can have a different brand identity: hip, cool, young, energetic. That will not fit the campaign for truck buyers. The two should have distinct themes."
Pickups and medium to heavy trucks.
Ever since the beginning of its history in 1914, Dodge has offered light truck models. For the first few years, these were based largely on the existing 
passenger cars, but eventually gained their own chassis and body designs as the market matured. Light- and medium-duty models were offered first, then a heavy-duty range was added during the 1930s and 1940s.
Following World War II and the successful application of four-wheel drive to the truck line, Dodge introduced a civilian version that it called the Power Wagon. At first based almost exactly on the military-type design, variants of the standard truck line were eventually given 4WD and the same “Power Wagon” name.
Dodge was among the first to introduce car-like features to its trucks, adding the plush Adventurer package during the 1960s and offering sedan-like space in its Club Cab bodies of the 1970s. Declining sales and increased competition during the 1970s eventually forced the company to drop its medium- and heavy-duty models, an arena the company has only recently begun to reenter.
Dodge introduced what they called the "Adult Toys" line to boost its truck sales in the late 1970s, starting off with the limited edition Lil' Red Express pickup (featuring, a 360 c.i. police interceptor engine and visible big rig-style exhaust stacks). Later came the more widely available Warlock. Other "Adult Toys" from Dodge included the "Macho Power Wagon" and "Street Van".
As part of a general decline in the commercial vehicle field during the 1970s, Dodge eliminated their LCF Series heavy-duty trucks in 1975, along with the Bighorn and medium-duty D-Series trucks, and affiliated S Series school buses were dropped in 1978. On the other hand, Dodge produced several thousand pickups for the United States Military under the CUCV program from the late 1970s into the early 1980s.
Continuing financial problems meant that even Dodge’s light-duty models – renamed as the Ram Pickup line for 1981 – were carried over with the most minimal of updates until 1993. Two things helped to revitalize Dodge’s fortunes during this time. First was their introduction of Cummins’ powerful and reliable B Series turbo-diesel engine as an option for 1989. This innovation raised Dodge’s profile among serious truck buyers who needed power for towing or large loads. A mid-size Dakota pickup, which later offered a class-exclusive V8 engine, was also an attractive draw.
Dodge introduced the Ram's all-new “big-rig” styling treatment for 1994. Besides its instantly polarizing looks, exposure was also gained by usage of the new truck on the hit TV show "Walker, Texas Ranger" starring Chuck Norris. The new Ram also featured a totally new interior with a console box big enough to hold a laptop computer, or ventilation and radio controls that were designed to be easily used even with gloves on. A V10 engine derived from that used in the Viper sports car was also new, and the previously offered Cummins turbo-diesel remained available. The smaller Dakota was redesigned in the same vein for 1997, thus giving Dodge trucks a definitive “face” that set them apart from the competition.
The Ram was redesigned again for 2002 (the Dakota in 2005), basically as an evolution of the original but now featuring the revival of Chrysler’s legendary Hemi V8 engine. New medium-duty chassis-cab models were introduced for 2007 (with standard Cummins turbo-diesel power), as a way of gradually getting Dodge back in the business truck market again.
For a time during the 1980s, Dodge also imported a line of small pickups from Mitsubishi. Known as the D50 or (later) the Ram 50, they were carried on as a stopgap until the Dakota’s sales eventually made the imported trucks irrelevant. (Mitsubishi has more recently purchased Dakota pickups from Dodge and restyled them into their own Raider line for sale in North America.)
Vans.
Dodge had offered panel delivery models for many years since its founding, but their first purpose-built van model arrived for 1964 with the compact A Series. Based on the Dodge Dart platform and using its proven six-cylinder or V8 engines, the A-series was a strong competitor for both its domestic rivals (from Ford and Chevrolet/GMC) and the diminutive Volkswagen Transporter line.
As the market evolved, however, Dodge realized that a bigger and stronger van line would be needed in the future. Thus the B Series, introduced for 1971, offered both car-like comfort in its Sportsman passenger line or expansive room for gear and materials in its Tradesman cargo line. A chassis-cab version was also offered, for use with bigger cargo boxes or flatbeds.
Like the trucks, though, Chrysler’s dire financial straits of the late 1970s precluded any major updates for the vans for many years. Rebadged as the Ram Van and Ram Wagon for 1981, this venerable design carried on for 33 years with little more than cosmetic and safety updates all the way to 2003.
The DaimlerChrysler merger of 1999 made it possible for Dodge to explore new ideas; hence the European-styled Mercedes-Benz Sprinter line of vans was brought over and given a Dodge styling treatment. Redesigned for 2006 as a 2007 model, the economical diesel-powered Sprinters have become very popular for city usage among delivery companies like FedEx and UPS in recent years. Because of their fuel efficiency major motorhome manufacturer made several Class C and Class A Motorhomes available on the Dodge Sprinter Chassis including their popular Four Winds Siesta & Chateau Citation product lines.
Dodge also offered a cargo version of its best-selling Caravan for many years, at first calling it the Mini Ram Van (a name originally applied to short-wheelbase B-Series Ram Vans) and later dubbing it the Caravan C/V (for "Cargo Van"). However, for model year 2011, the Caravan C/V was rebranded as a Ram, called the Ram C/V.
Sport utility vehicles.
Dodge’s first experiments with anything like a sport utility vehicle appeared in the late 1950s with a windowed version of their standard panel-truck - known as the Town Wagon. These were built in the same style through the mid-1969s.
But the division didn't enter the SUV arena in earnest until 1974, with the purpose-built Ramcharger. Offering the then-popular open body style and Dodge's powerful V8 engines, the Ramcharger was a strong competitor for trucks like the Ford Bronco, Chevrolet Blazer and International Harvester Scout II.
Once again, though, Dodge was left with outdated products during the 1980s as the market evolved. The Ramcharger hung on through 1993 with only minor updates. When the Ram truck was redesigned for the 1994 model year, the Ramcharger was discontinued in the American and Canadian markets. A version using the updated styling was made for the Mexican Market but was never imported to the U.S. or Canada.
Instead, Dodge tried something new in 1998. Using the mid-sized Dakota pickup's chassis as a base, they built the four-door Durango SUV with seating for eight people and created a new niche. Sized between smaller SUVs (like the Chevrolet Blazer and Ford Explorer) and larger models (like the Chevrolet Tahoe and Ford Expedition), Durango was both a bit more and bit less of everything. The redesigned version for 2004 grew a little bit in every dimension, becoming a full-size SUV (and thus somewhat less efficient), but was still sized between most of its competitors on either side of the aisle. For 2011 a new unibody Durango based on the Jeep Grand Cherokee was released. The 2011 Durango shrank slightly to size comparable to the original model.
Dodge also imported a version of Mitsubishi’s popular Montero (Pajero in Japan) as the Raider from 1987 to 1989.
International markets.
Dodge vehicles are now available in many countries throughout the world.
Argentina.
Dodge came to Argentina in the early 20th century with imported cars & trucks. But, since 1960, has partnered with a local representative: Fevre-Basset. The first vehicle made in Argentina was the pick up D-100 "Sweptilte". For 1961 to 1980 arrives the trucks, like: D-400/DP-400 D-500/DP-500 DP600, DD900 & DD1000 (the last two with one curiosity: the aircooled Deutz engine rather Perkins or Chrysler). Respecting the passenger cars, made the Valiant I & II, and the local versions of the 1966 Dodge Dart (called Valiant III & IV). For 1971, arrives the Dodge 1500, a rebadged Hillman Avenger from UK. In 1982, ceased the Dodge brand, because Volkswagen bought the Fevre plant and the shares.
In 1993, Dodge cars and pick-ups began to be marketed in the country. Currently, the Journey and the Ram sold in Argentina by Dodge.
Asia.
Dodge entered the Japanese market in mid-2007, and re-entered the Chinese market in late 2007. Soueast Motors of China assembles the Caravan for the Chinese market. Dodge had already been marketing its vehicles in South Korea since 2004, starting with the Dakota.
Dodge vehicles have been sold in the Middle East for a considerably longer period.
Australia.
Dodge re-entered the Australian market in 2006 with the Caliber, their first offering since the AT4/D5N trucks in 1979 and the first Dodge passenger car to be marketed in Australia since the Phoenix sedan was discontinued in 1973. The second model to be introduced was the Nitro, with the Avenger and Journey followed. Dodge chose not use the full model lines and engines available to them, the 2.7L V6 being available in the Journey and Avenger instead of the 3.2 in the North American versions. However they did introduce diesel engines in all their cars.
Following the Global Financial Crisis, Chrysler introduced the facelifted model of the Caliber and discontinued the Avenger imports. From early 2012 on, model year 2010 cars were available. By early 2012 no new cars were being brought into Australia aside from the new facelifted 2012 Journey.
There are now rumours that Dodge cars will be re-badged as Fiats in the Australian market as has happened in Europe. In contrast, recent speculation has suggested that the Dodge nameplate would continue on until at least 2015, due to consistent sales of the Journey.
Brazil.
In Brazil, Dodge cars were produced between 1969 and 1981 with the models Dart, Charger, Magnum, LeBaron (all powered by the same 318 cid V8 engine), and the compact 1800/Polara, based on the British Hillman Avenger. The manufacturer was acquired by Volkswagen in 1981. In 1998, the Dakota pickup started production in a new plant in Campo Largo, Paraná by Mercedes-Benz, which belongs to its former partner Daimler AG. It was built there until 2001 with petrol and diesel engines and regular, extended and crew cabs. In 2010, Dodge started sales of the imported pickup Ram 2500. The model portfolio is being expanded, starting with the Journey crossover for the 2009 model year.
Canada.
In Canada, the Dodge lineup of cars started down the road to elimination along with the Plymouth line when in 1988 the Dodge Dynasty was sold in Canada as the Chrysler Dynasty and sold at both Plymouth and Dodge dealers. Similarly, the new Dodge Intrepid, the Dynasty's replacement, was sold as the Chrysler Intrepid.
For 2000, the new Neon became the Chrysler Neon. The Chrysler Cirrus and Mitsubishi-built Dodge Avenger were dropped. Dodge trucks, which have been sold at Canadian Plymouth dealers since 1973, continued without change. All Plymouth-Chrysler and Dodge-Chrysler dealers became Chrysler-Dodge-Jeep dealers.
The diluting of the Chrysler name did not go well in Canada, especially as the nameplate had been pushed as a luxury line since the 1930s. For 2003, the revamped Neon appeared in Canada as the Dodge SX 2.0. Since then all new Dodge models have been sold in Canada under the Dodge name.
Europe.
Dodge started assembling lorries (trucks) in the United Kingdom, from imported parts, in 1922. In 1933 it began to manufacture a British chassis, at its works in Kew, using American engines and gearboxes.
Following Chrysler's takeover of the British Rootes Group, Simca of France, and Barreiros of Spain, and the resultant establishment of Chrysler Europe in the late 1960s, the Dodge brand was used on light commercial vehicles, most of which were previously branded Commer or Karrier, on pickup and van versions of the Simca 1100, on the Spanish Dodge Dart, and on heavy trucks built in Spain. The most common of these was the Dodge 50 series, widely used by utility companies and the military, but rarely seen outside the UK, and the Spanish-built heavy-duty 300 series available as 4x2, 6x4, 8x2, and 8x4 rigids, as well as 4x2 semi-trailer tractors. All of these were also sold in selected export markets badged either as Fargo or De Soto.
Following Chrysler Europe's collapse in 1977, and the sale of their assets to Peugeot, the Chrysler/Dodge British and Spanish factories were quickly passed on to Renault Véhicules Industriels. Chrysler licensed the Dodge name to be used on Renault trucks sold in certain European markets - most notably the United Kingdom, although it eventually reverted back to Renault when the associated models were discontinued. They would eventually drop these products altogether and used the plants to produce engines (in the UK) and "real" Renault truck models in Spain. Dodge vehicles would not return to the UK until the introduction of the Neon, badged as a Chrysler, in the mid-1990s.
The Dodge marque was reintroduced to Europe on a broad scale in 2006. Currently, the Dodge lineup in Europe consists of the Caliber, Avenger, Viper SRT-10, Nitro and Dodge Journey. However, in 2010 Chrysler pulled the Dodge marque from the UK lineup due to poor sales.
On June 1, 2011 the Dodge name was dropped from the rest of Europe when it was replaced by the Fiat brand, where Fiat rebadged the Dodge Journey as the Fiat Freemont. However, the Freemont is not available in the Ireland or UK Fiat lineup.
Mexico.
In Mexico, the Hyundai Accent, Hyundai Atos, and Hyundai H100 are branded as "Dodge" or "Verna by Dodge", "Atos by Dodge" and "Dodge H100" respectively, and sold at Chrysler/Dodge dealers. Current models are marketed with Hyundai logos instead of the "Ram" logo on previous model years. Dodge and Hyundai will end the venture and Dodge will use rebadged and reworked Fiats.
Logos.
A second emblem was revealed during the unveiling of the 2011 Durango, which used the same five-point shield-shaped outline of the old emblem, but with the ram's head replaced with a chrome cross reminiscent of the brand's signature cross-haired grille. This was only used on the steering wheel. In 2014, the cross logo was replaced by the word "DODGE" on the Durango steering wheel. A modified version of the Ram's head logo is still used for the Ram brand, with "RAM" written across the bottom in bold white or black lettering.

</doc>
<doc id="42970" url="http://en.wikipedia.org/wiki?curid=42970" title="Future history">
Future history

A future history is a postulated history of the future and is used by authors in the subgenre of speculative fiction (or science fiction) to construct a common background for fiction. Sometimes the author publishes a timeline of events in the history, while other times the reader can reconstruct the order of the stories from information provided therein.
Background.
The term appears to have been coined by John W. Campbell, Jr., the editor of "Astounding Science Fiction", in the February 1941 issue of that magazine, in reference to Robert A. Heinlein's "Future History". Neil R. Jones is generally credited as the first author to create a future history.
A set of stories which share a backdrop but are not really concerned with the sequence of history in their universe are rarely considered future histories. For example, neither Lois McMaster Bujold's "Vorkosigan Saga" nor George R. R. Martin's 1970s short stories which share a backdrop are generally considered future histories. Standalone stories which trace an arc of history are rarely considered future histories. For example, Walter M. Miller Jr.'s "A Canticle for Leibowitz" is not generally considered a future history.
Earlier, some works were published which constituted "future history" in a more literal sense—i.e., stories or whole books purporting to be excerpts of a history book from the future and which are written in the form of a history book—i.e., having no personal protagonists but rather describing the development of nations and societies over decades and centuries.
Such works include:
Future history and alternate history.
Unlike alternate history, where alternative outcomes are ascribed to past events, future history postulates certain outcomes to events in the writer's present and future.
The essential difference is that the writer of alternate history is in possession of knowledge of the actual outcome of a certain event, and that knowledge influences also the description of the event's alternate outcome. The writer of future history does not have such knowledge, such works being based on speculations and predictions current at the time of writing—which often turn out to be wildly inaccurate.
For example, in 1933 H. G. Wells postulated in "The Shape of Things to Come" a Second World War in which Nazi Germany and Poland are evenly matched militarily, fighting an indecisive war over ten years; and Poul Anderson's early 1950s Psychotechnic League depicted a world undergoing a devastating nuclear war in 1958, yet by the early 21st century managing not only to rebuild the ruins on Earth but also engage in extensive space colonization of the Moon and several planets. A writer possessing knowledge of the actual swift collapse of Poland in World War II and the enormous actual costs of far less ambitious space programs in a far less devastated world would have been unlikely to postulate such outcomes. "" was set in the future and featured developments in space travel and habitation which have not occurred on the timescale postulated.
A problem with future history science fiction is that it will date and be overtaken by real historical events, for instance H. Beam Piper's future history, which included a nuclear war in 1973, and much of the future history of "Star Trek". Jerry Pournelle's "CoDominium" future history assumed that the Cold War would end with the USA and Soviet Union establishing a co-rule of the world, the CoDominium of the title, which would last into the 22nd Century—rather than the Soviet Union collapsing in 1991. 
There are several ways this is dealt with. One solution to the problem is when some authors set their stories in an indefinite future, often in a society where the current calendar has been disrupted due to a societal collapse or undergone some form of distortion due to the impact of technology. Related to the first, some stories are set in the very remote future and only deal with the author's contemporary history in a sketchy fashion, if at all (e.g. the original "Foundation" Trilogy by Asimov). Another related case is where stories are set in the near future, but with an explicitly allohistorical past, as in Ken MacLeod's "Engines of Light" series.
In other cases, such as the "Star Trek" universe, the merging of the fictional history and the known history is done through extensive use of retroactive continuity. In yet other cases, such as the "Doctor Who" television series and the fiction based on it, much use is made of secret history, in which the events that take place are largely secret and not known to the general public.
As with Heinlein, some authors simply write a detailed future history and accept the fact that events will overtake it, making the sequence into a "de facto" alternate history.
Lastly, some writers formally transform their future histories into alternate history, once they had been overtaken by events. For example, Poul Anderson started The Psychotechnic League history in the early 1950s, assuming a nuclear war in 1958—then a future date. When it was republished in the 1980s, a new foreword was added explaining how that history's timeline diverged from ours and led to war.

</doc>
<doc id="42971" url="http://en.wikipedia.org/wiki?curid=42971" title="The Dukes of Hazzard">
The Dukes of Hazzard

The Dukes of Hazzard is an American television series that aired on the CBS television network from January 26, 1979 to February 8, 1985. The series was inspired by the 1975 film "Moonrunners", which was also created by Gy Waldron and had many identical or similar character names and concepts.
Overview.
"The Dukes of Hazzard" follows the adventures of "The Duke Boys", cousins Bo Duke (John Schneider) and Luke Duke (Tom Wopat), who live in a rural part of the fictional Hazzard County, Georgia, with their attractive female cousin Daisy (Catherine Bach) and their wise old Uncle Jesse (Denver Pyle). They race around in their customized 1969 Dodge Charger stock car, dubbed "(The) General Lee", evading crooked county commissioner Boss Hogg (Sorrell Booke) and his inept county Sheriff Rosco P. Coltrane (James Best), and always managing to get caught in the middle of the various escapades and incidents that often occur in the area. Bo and Luke had previously been sentenced to probation for illegal transportation of moonshine; their Uncle Jesse made a plea bargain with the U.S. Government to stop distilling moonshine in exchange. As a result, Bo and Luke are not allowed to carry firearms — instead, they often use compound bows, sometimes with arrows tipped with dynamite — or to leave Hazzard County, although the exact details of their probation terms vary from episode to episode. Sometimes it is implied that they would be jailed for merely crossing the county line; on other occasions, it is shown that they may leave Hazzard, as long as they are back within a certain time limit. Several other technicalities of their probation also came into play at various times.
Corrupt county commissioner Jefferson Davis "Boss" Hogg, who either runs or has fingers in just about everything in Hazzard County is forever angry with the Dukes, in particular Bo and Luke for always foiling his crooked scams. He is always looking for ways to get them out of the picture so that his plots have a chance of succeeding. Many episodes revolve around Hogg trying to engage in an illegal scheme, sometimes with aid of hired criminal help. Some of these are get-rich-quick schemes, though many others affect the financial security of the Duke farm, which Hogg has long wanted to acquire for nefarious reasons. Other times, Hogg hires criminals from out of town to do his dirty work for him, and often tries to frame Bo and Luke for various crimes as part of these plots. Bo and Luke always seem to stumble over Hogg's latest scheme, sometimes by curiosity, and often by sheer luck, and put it out of business. Despite the Dukes often coming to his rescue (see below), Hogg forever seems to have an irrational dislike of the clan, particularly Bo and Luke, often accusing them of spying on him, robbing or planning to rob him, and other supposedly nefarious actions as he believes they are generally out to get him.
The other main players of the show are Cooter Davenport (Ben Jones), who in very early episodes was seen to be a wild, unshaven rebel, often breaking or treading on the edge of the law, before settling down and becoming much more laid-back, and who owns the local garage and is the Duke family's best friend (he is often referred to as an "honorary Duke"), and Enos Strate (Sonny Shroyer), an honest but naive young Deputy who often finds his morals conflicted as he is reluctantly forced to take part in Hogg and Rosco's crooked schemes. In the third and fourth season, when Enos leaves for his own show, he is replaced by Deputy Cletus Hogg (Rick Hurst), Boss's cousin, who is slightly more wily than Enos but is generally also a reluctant player in Hogg's plots.
Owing to their fundamentally good natures, the Dukes often wind up helping Boss Hogg, albeit grudgingly. More than once Hogg is targeted by former associates who are either seeking revenge or have turned against him after a scheme has unraveled in one way or another. Sheriff Rosco also finds himself in trouble more than once. On such occasions, Bo and Luke usually have to rescue their adversaries as an inevitable precursor to defeating the bad guys; in other instances, the Dukes and Hogg and Rosco mutually join forces to tackle bigger threats to Hazzard or one of their respective parties. These instances became more frequent as the show progressed, and later seasons saw a number of stories where the Dukes and Hogg (and Rosco) temporarily work together.
Production.
The series was developed from the 1975 B movie "Moonrunners". Created by Gy Waldron in collaboration with ex-moonshiner Jerry Rushing, this movie shares many identical and very similar names and concepts with the subsequent TV series. Although itself essentially a comedy, this original movie was much cruder and edgier than the family-friendly TV series that would evolve from it.
In 1977, Waldron was approached by Warner Bros. with the idea of developing "Moonrunners" into a television series. Waldron reworked various elements from "Moonrunners", and from it was devised what would become "The Dukes of Hazzard". Production began in October 1978 with the original intention of only nine episodes being produced as mid-season filler. The first five episodes were filmed in Covington and Conyers, Georgia and surrounding areas, including some location work in nearby Atlanta. Although most of the familiar "Dukes" elements are present in these initial early episodes, these first five stories feature a noticeably different tone from the rest of the series, including some more adult-oriented humor, with some raunchier elements and slightly coarser language; several of the characters, noticeably those of Rosco and Cooter, are also given different interpretation to their more recognized roles. After completing production on the fifth episode, "High Octane", the cast and crew broke for Christmas break, expecting to return in several weeks' time to complete the ordered run of episodes. In the meantime, executives at Warner Bros. were impressed by the rough preview cuts of the completed episodes and saw potential in developing the show into a full-running series; part of this plan was to move production from Georgia to the Warner Bros. lot in Burbank, California, primarily to simplify production as well as develop a larger workshop to service the large number of automobiles needed for the series.
Rushing appeared as shady used car dealer Ace Parker in the third episode produced, "Repo Men" (the fourth to be broadcast). Rushing believed this to be the start of a recurring role, in return for which he would supply creative ideas from his experiences: many of the "Dukes" (and thus "Moonrunners") characters and situations were derived from Rushing's experiences as a youth, and much of the character of Bo Duke he states to be based on him. However, "Repo Men" would turn out to be the character's only appearance in the entire show's run, leading to a legal dispute in the following years over the rights to characters and concepts between Rushing and Warner Bros., although he remained on good terms with cast and crew and in recent years has made appearances at several fan conventions.
By the end of the first (half) season, the family-friendly tone of "The Dukes of Hazzard" was very much in place. When the show returned for a second season in Fall 1979 (its first full season), with a few further minor tweaks, the show quickly found its footing as a family-friendly comedy-adventure series. By the third season, starting in Fall 1980, the template was well set in place for that which would be widely associated with the show.
As well as its regular car chases, jumps and stunts, "The Dukes of Hazzard" relied on character familiarity, with each character effectively serving the same role within a typical episode, and with Deputy Cletus replacing Deputy Enos in Seasons 3 and 4, and Coy and Vance Duke temporarily replacing Bo and Luke (due to a salary dispute) in Season 5, being the only major cast changes through the show's run (Ben Jones and James Best both left temporarily during the second season due to different disputes with producers, but both returned within a couple of episodes). Of the characters, only Uncle Jesse and Boss Hogg appeared in all 145 episodes; Daisy appears in all but one, the third season's "To Catch a Duke". The General Lee also appears in all but one (the early first season episode "Mary Kaye's Baby", the fourth to be produced and the third broadcast).
Characters.
Notable guest appearances.
Throughout its network television run, "The Dukes of Hazzard" had a consistent mix of up-and-comers and established stars make guest appearances.
Others.
NASCAR driver Terry Labonte makes a brief, uncredited appearance as a crewman in the episode Undercover Dukes Part 1. The race cars supplied for both part 1 and part 2 of Undercover Dukes were supplied by Labonte's race owner, Billy Hagan. However, the emblems of the sponsors of the cars (at that time Labonte was sponsored by Budweiser) were covered to avoid paying royalties.
The celebrity speed trap.
During the show's second season, the show's writers began flirting with the idea of incorporating a "celebrity speed trap" into some of the episodes, as a means to feature top country stars of the day performing their hits. On its first couple of instances, the "Speed Trap" was featured early in the story, but for most of the cases, it was featured in the last few minutes of an episode, often used when the main story was running too short to fill episode time.
The "celebrity speed trap" feature was essentially similar: Aware that a big-name country star was passing through the area, Boss Hogg would order Rosco to lower the speed limit on a particular road to an unreasonable level (using a reversible sign, with one speed limit on one side and another, far lower, on the back), so that the targeted singer would be in violation of the posted limit. The singer would be required to perform at the Boar's Nest in exchange for having their citations forgiven; the performer would then perform one of their best-known hits or other popular country music standard, while the Dukes, Boss, Rosco, Cletus, Cooter, and other patrons whooped and hollered in enjoyment of the performance. More often than not, the performer would give a parting shot to Boss and Rosco.
Singers who were featured in the "speed trap" segments were:
Honorable Mentions: Mickey Gilley, Loretta Lynn
Gilley's and Lynn's appearances were not solely for the celebrity speed trap. After performing a concert in Hazzard, Gilley was nabbed while leaving and forced to do a second show to nullify his citation. Lynn was kidnapped by criminals wanting to break into the music business. Loretta Lynn was the very first country music guest star on the show in 1979 and had an entire show titled "Find Loretta Lynn."
Coy and Vance.
"The Dukes of Hazzard" was consistently among the top-rated television series (at one point, ranking second only to "Dallas", which immediately followed the show on CBS' Friday night schedule). With that success came huge profits in merchandising, with a huge array of "Dukes of Hazzard" toys and products being licensed and becoming big sellers. However, over the course of the show's fourth season, series stars Tom Wopat and John Schneider became increasingly concerned about a contract dispute over their salaries and merchandising royalties owed to them over the high sales of "Dukes" products. Neither were being paid what was owed to them and this became very frustrating to the duo. As a result, in the spring of 1982, as filming was due to begin on the fifth season, Wopat and Schneider did not report to the set in protest over the matter. Catherine Bach also considered walking out due to similar concerns, but Wopat and Schneider convinced her to stay, insisting that if she left then there may not be a show to come back to, and that settling the issue was up to them.
Production was pushed back by a few weeks as two lookalike replacements were subsequently, hastily hired: Byron Cherry as Coy Duke and Christopher Mayer as Vance Duke. Bo and Luke were said to have gone to race on the NASCAR circuit; how they managed to do this, bearing in mind the terms of their probation, was never mentioned. Cherry and Mayer were originally contracted at just ten episodes as stand-ins, still with hope that a settlement might be reached with Wopat and Schneider (in total, they made 17 episodes). The scripts for Coy and Vance were originally written for Bo and Luke but with their names quite literally crossed out and Coy and Vance penned in. The new Dukes — previously-unmentioned nephews of Uncle Jesse, who were said to have left the farm in 1976, before the show had started — were unpopular with the great majority of viewers, and the ratings immediately sank. Much of the criticism was that Coy and Vance were nothing but direct clones of Bo and Luke, with Coy a direct "carbon copy" replacement for Bo and Vance for Luke, with little variation in character. This was something that even show creator Gy Waldron has said was wrong, and that he insisted, unsuccessfully, that audiences would not accept direct character clones and the two replacements should be taken in a different direction characterwise, but was overridden by producers. Waldron also commented that if Bach too had walked, the show would have most probably been cancelled. It was reported that prior to filming, Cherry and Mayer were given Bo and Luke episodes to watch, to study and learn to emulate them, although Cherry has said in interviews that he doesn't recall this ever happening .
Hit hard by the significant drop in ratings, Warner Bros. renegotiated with Wopat and Schneider, and eventually a settlement was reached, and the original Duke boys returned to the series in early 1983, four episodes from the conclusion of the fifth season. Initially, part of the press release announcing Wopat and Schneider's return suggested that Cherry and Mayer would remain as part of the cast (though presumably in a reduced role), but it was quickly realized that "four Duke boys" would not work within the context of the series, and due to the huge unpopularity associated with their time on the show, they were quickly written out of the same episode in which Bo and Luke returned.
Bo and Luke return.
Although Coy and Vance were never popular with the majority, many viewers were disappointed by their departure episode, "Welcome Back, Bo 'N' Luke", which was very much a standard episode, with the return of Bo and Luke and the departure of Coy and Vance tacked onto the beginning (Bo and Luke return from their NASCAR tour just as Coy and Vance leave Hazzard to tend to a sick relative). Many viewers commented that they were disappointed by this, and that they would have liked to have seen both pairs of Duke boys team up to tackle a particularly dastardly plot by Boss Hogg before Coy and Vance's departure, but as it turned out, Coy and Vance had very little dialogue and were gone by the first commercial break, never to be seen nor mentioned again.
While the return of Bo and Luke was welcomed by ardent and casual viewers alike, and as a result ratings recovered slightly, the show never completely regained its former popularity. One of Wopat and Schneider's disputes even before they left was what they considered to be increasingly weak and formulaic scripts and episode plots. With Wopat and Schneider's return, the producers agreed to try a wider scope of storylines, even including some science fiction elements in certain episodes. However, although it continued for two more seasons, the show never fully returned to its former glory. As well as what was widely recognized to be increasingly inferior scripts, many fans, or indeed cast members, did not take to the miniature car effects used to make it appear as if the General Lee was performing even more breathtaking feats (in part to compete with TV's newer supercar, KITT from the NBC series "Knight Rider"). Finally, at the end of its seventh season, in early February 1985, "The Dukes of Hazzard" quietly ended its run.
Vehicles.
The General Lee.
The General Lee was based on a 1969 Dodge Charger owned by Bo and Luke (the series had 1968-1970 Chargers to look like a 1969). It was orange with a Confederate battle flag painted on the roof, the words "GENERAL LEE" over each door, and the number "01" on each door. In the original five Georgia-filmed episodes, a Confederate flag along with a checkered racing flag in a criss-cross pattern could be seen behind the rear window; this was removed when it was felt that this extra detail did not show up enough on-screen enough to warrant the already very tight time constraints of preparing and repairing each example of the car. The name refers to the American Civil War Confederate General Robert E. Lee. The television show was based on the movie "Moonrunners", in turn based on actual moonshine runners who used a 1958 Chrysler named Traveler, after General Lee's horse. Traveler was originally intended to be the name of the Duke boys' stock car too, until producers agreed that General Lee had more punch to it.
Since it was built as a race car, the windows were regularly open (except for several shots in early episodes). Through the history of the show, an estimated 309 Chargers were used; 17 are still known to exist in various states of repair. A replica was owned by John Schneider, known as "Bo's General Lee". In 2008, Schneider sold "Bo's General Lee" at the Barrett-Jackson automobile auction for $450,000. An eBay auction which garnered a bid of $9,900,500 for the car was never finalized, with the purported bidder claiming his account had been hacked. The underside of the hood has the signatures of the cast from the 1997 TV movie. Schneider has also restored over 20 other "General Lee"s to date. The show also used 1968 Chargers (which shared the same sheet metal) by changing the grille and taillight panel to the 1969 style, and removing the round side marker lights. These Chargers performed many record-breaking jumps throughout the show, almost all of them resulting in a completely destroyed car.
The 1970s were modified by removing the chrome bumper and changing the taillights.
The Duke boys added a custom air horn to the General Lee that played the first twelve notes of the song "Dixie". The Dixie horn was not originally planned, until a Georgia local hot rod racer drove by and sounded his car's Dixie horn. The producers immediately rushed after him asking where he had bought the horn. Warner Bros. purchased several Chargers for stunts, as they generally destroyed at least one or two cars per episode. By the end of the show's sixth season, the Chargers were becoming harder to find, and more expensive. In addition, the television series "Knight Rider" began to rival the General Lee's stunts. As such, the producers used 1:8 scale miniatures, filmed by Jack Sessums' crew, or recycled stock jump footage — the latter being a practice that had been in place to an extent since the second season, and had increased as the seasons passed.
Some of the 01 and Confederate flag motifs were initially hand painted, but as production sped up, these were replaced with vinyl decals for quick application (and removal), as needed.
During the first five episodes of the show that were filmed in Georgia, the cars involved with filming were given to the crew at H&H body shop near the filming location. At this shop, the men worked day and night to prepare the wrecked cars for the next day while still running their body shop during the day. Time was of the essence, and the men that worked at this shop worked hard hours to get the cars prepared for the show.
The third episode "Mary Kaye's Baby" is the only one in which the General Lee does not appear. Instead, the Dukes drove around in a blue 1975 Plymouth Fury borrowed from Cooter that Luke later destroyed by shooting an arrow at the car, whose trunk had been leaking due to the moonshine stowed in the back.
The Duke boys' CB handle was (jointly) "Lost Sheep". Originally when the show was conceived, their handle was to be "General Lee" to match their vehicle, but this was only ever used on-screen on one occasion, in the second episode, "Daisy's Song", when Cooter calls Bo and Luke over the CB by this handle, although they were actually driving Daisy's Plymouth Roadrunner (see below) at the time. As it became obvious that the "General Lee" handle would be out of place when the Duke boys were in another vehicle, the "Lost Sheep" handle was devised (with Uncle Jesse being "Shepherd" and Daisy being "Bo Peep").
AMC Matador.
The 1975 AMC Matador was one of many different Hazzard County police cars used on the series, mostly in the first season; they had light bars and working radios. A 1972 Dodge Polara and a 1974 Dodge Monaco were used during the pilot episode "One Armed Bandits", these were also seen in the show's title sequence. From the second season, the 1977 Dodge Monaco was mostly used. From mid-season four the similar looking 1978 Plymouth Fury was used instead. The Matadors were former Los Angeles Police Department vehicles, while the Monacos and Furies were former California Highway Patrol units.
Plymouth Road Runner.
A 1974 Plymouth Road Runner (yellow with a black stripe) was used by Daisy Duke in the first five episodes of the first season. For the last episodes of the first season and the second season, a similarly painted 1971 Plymouth Satellite with a matching "Road Runner" stripe was used until Bo and Luke sent it off a cliff in "The Runaway" after the accelerator became stuck.
Dixie.
Dixie was the name given to Daisy Duke's white 1980 Jeep CJ-7 "Golden Eagle" which had a golden eagle emblem on the hood and the name "Dixie" on the sides. Like other vehicles in the show, there was actually more than one Jeep used throughout the series. Sometimes it would have an automatic transmission, and other times it would be a manual. The design of the roll cage also varied across the seasons. When the Jeep was introduced at the end of the second season's "The Runaway", it was seen to have doors and a slightly different paint job, but, bar one appearance in the next produced episode, "Arrest Jesse Duke" (actually broadcast before "The Runaway", causing a continuity error), thereafter the doors were removed and the paint job was made all-white, with "Dixie" painted on the sides of the hood. These Jeeps were leased to the producers of the show by American Motors Corporation in exchange for a brief mention in the closing credits of the show.
Uncle Jesse's truck.
Uncle Jesse's Truck was a white Ford pickup truck, most commonly a Sixth generation (1973–1977) F100 Styleside. However, in the earliest episodes it had a Flareside bed, and varied between F100 and F250 models throughout the show's run. Bo, Luke and Daisy also drove Jesse's truck on occasion.
Boss Hogg's Cadillac.
A White 1970 Cadillac Coupe de Ville convertible was used as Hogg's car, notably with large bull horns as a hood ornament. In earlier seasons Hogg was driven by a chauffeur, who was normally nameless and had little or no dialogue, but identified on occasion as being called "Alex"; and played by several different uncredited actors, including stuntman Gary Baxley. This chauffeur would often be dressed in a red plaid shirt and deep brown or black Stetson hat, but on occasion would be an older man, sometimes dressed in more typical chauffeur attire. Hogg is first seen to drive for himself in the second season opener "Days of Shine and Roses", where he and Jesse challenge each other to one last moonshine race. From the fourth season onwards, bar a couple of brief reappearances of the chauffeur (during the fourth season), Hogg drove himself around in his Cadillac (or occasionally driven by Rosco and, in the series' finale, by Uncle Jesse) and frequently challenged others by invoking his driving expertise from his days as a ridge-runner. Unlike other vehicles in the series, Boss Hogg's Cadillac is typically treated with kid gloves. The car is almost always seen with its convertible top down, with the top only being seen in two episodes, "Daisy's Song" (the chauffeur was called called "Eddie" in this episode), the second to be produced and broadcast, and briefly in the second season episode "Witness For the Persecution", when Cooter is returning it to the Court House after repairs.
Tourist attraction.
Artifacts from the show are on display in Nashville, Tennessee and in Gatlinburg, Tennessee. Cooter's Place in Gatlinburg is overseen by Ben "Cooter" Jones from the series and features a gift shop, "Dukes of Hazzard" themed indoor mini golf and go-cart track, with a small display of costumes, collectables and artifacts from the show.
Covington and Conyers, Georgia, where the original five episodes were produced, have been two major tourist attractions for "Dukes of Hazzard" fans.
Dixie Outfitters in Branson, Missouri on Hwy 76 has the General Lee and Roscoe Police car, signed by Daisy, Cooter, Cletus and Enos.
Theme song.
The theme song "The Good Ol' Boys" was written and performed by Waylon Jennings. He was also "The Balladeer" (as credited), and served as narrator of the show. However, the Jennings theme song that is currently available for purchase is not the same version that was used in the show's opening credits. The differences are that the show version featured a different verse ["...Fightin' the system like a true modern-day Robin Hood"], an enhanced bass line, a shorter length, and the famous "Yee-haw" yell at the end.
In 1980, the song reached #1 on the American Country chart and peaked at #21 on the "Billboard" Hot 100.
Broadcast history.
Syndication.
Soon before the series ended its original run on CBS, "The Dukes of Hazzard" went into off-network syndication. Although not as widely run as it was back in the 1980s and the years since, reruns of the program do continue to air in various parts of the United States.
Notably, television stations that aired the show in syndication include KCOP Los Angeles, WGN-TV Chicago, KBHK San Francisco, WKBD Detroit, WTAF/WTXF Philadelphia, KTXL Sacramento, WVTV Milwaukee, KMSP Minneapolis–Saint Paul, among others. It has also aired on ABC Family (2000–2001, 2004) and CMT (2005–2007, 2010–2012, 2014-), WMAZ-TV in Macon, Georgia 1979–1985,
WGXA-TV Macon, Georgia 1984–1990.
Nielsen ratings.
Season one (a mid-season debut) began with 21.0 rating. In season 2, the series managed to average 18.39 million viewers in 1979. Season 3 grew 15.6% to 21.81 million viewers while Season 4 dropped 15.5% to 18.41 million viewers in 1980–1981. Season 5 dropped extensively to below 14.327 million viewers but as ratings below the top 30, Seasons 6 and Season 7 ratings are unknown.
Episode list.
The show ran for seven seasons and a total of 145 episodes. Many of the episodes followed a similar structure "out-of-town crooks pull a robbery, Duke boys blamed, spend the rest of the hour clearing their names, the General Lee flies and the squad cars crash".
Spin-off.
The second season episodes "Jude Emery", about a Texas Ranger, and "Mason Dixon's Girls", about a travelling private investigator and his female associates, were both pilots written by "Dukes" creator Gy Waldron for proposed new shows. Both failed to sell.
Films.
There were two made-for-TV reunion movies that aired on CBS, ' (1997) and ' (2000).
DVD releases.
Warner Home Video has released all seven seasons of The Dukes of Hazzard on DVD in regions 1 and 2. The two TV-movies that followed the series were released on DVD in Region 1 on June 10, 2008. In Region 4, Warner has released only the first six seasons on DVD.
Legacy and influence in popular culture.
In 2005, Tom Wopat and John Schneider were reunited during "Exposed", a fifth season episode of the television series "Smallville". Wopat guest-starred as Kansas State Senator Jack Jennings, an old friend of Clark Kent's adoptive father Jonathan Kent (portrayed by Schneider). In the episode, Jennings drives a 1968 Dodge Charger—the same body style as "The General Lee".
"Lizard Lick Towing" featured an episode with its repossession specialists Ronnie Shirley and Bobby Brantley commandeering a General Lee replica.
John Schneider and Tom Wopat have recently been featured reprising their roles as Bo and Luke Duke in ads for Auto-Trader.com.

</doc>
<doc id="42975" url="http://en.wikipedia.org/wiki?curid=42975" title="Hubble's law">
Hubble's law

Hubble's law is the name for the observation in physical cosmology that: (1) objects observed in deep space (extragalactic space, ~10 megaparsecs or more) are found to have a Doppler shift interpretable as relative velocity away from the Earth; and (2) that this Doppler-shift-measured velocity, of various galaxies receding from the Earth, is approximately proportional to their distance from the Earth for galaxies up to a few hundred megaparsecs away. This is normally interpreted as a direct, physical observation of the expansion of the spatial volume of the observable universe.
The motion of astronomical objects due solely to this expansion is known as the Hubble flow. Hubble's law is considered the first observational basis for the expanding space paradigm and today serves as one of the pieces of evidence most often cited in support of the Big Bang model.
Although widely attributed to Edwin Hubble, the law was first derived from the general relativity equations by Georges Lemaître in a 1927 article where he proposed the expansion of the universe and suggested an estimated value of the rate of expansion, now called the Hubble constant. Two years later Edwin Hubble confirmed the existence of that law and determined a more accurate value for the constant that now bears his name. Hubble inferred the recession velocity of the objects from their redshifts, many of which were earlier measured and related to velocity by Vesto Slipher in 1917.
The law is often expressed by the equation , with "H"0 the constant of proportionality (Hubble constant) between the "proper distance" "D" to a galaxy (which can change over time, unlike the comoving distance) and its velocity "v" (i.e. the derivative of proper distance with respect to cosmological time coordinate; see "Uses of the proper distance" for some discussion of the subtleties of this definition of 'velocity'). The SI unit of "H"0 is s−1 but it is most frequently quoted in (km/s)/Mpc, thus giving the speed in km/s of a galaxy 1 Mpc away. The reciprocal of "H"0 is the Hubble time.
Discovery.
A decade before Hubble made his observations, a number of physicists and mathematicians had established a consistent theory of the relationship between space and time by using Einstein's field equations of general relativity. Applying the most general principles to the nature of the universe yielded a dynamic solution that conflicted with the then-prevailing notion of a static universe.
FLRW equations.
In 1922, Alexander Friedmann derived his Friedmann equations from Einstein's field equations, showing that the Universe might expand at a rate calculable by the equations. The parameter used by Friedmann is known today as the scale factor which can be considered as a scale invariant form of the proportionality constant of Hubble's law. Georges Lemaître independently found a similar solution in 1927. The Friedmann equations are derived by inserting the metric for a homogeneous and isotropic universe into Einstein's field equations for a fluid with a given density and pressure. This idea of an expanding spacetime would eventually lead to the Big Bang and Steady State theories of cosmology.
Lemaitre's Equation.
In 1927, two years before Hubble published his own article, the Belgian priest and astronomer Georges Lemaître was the first to publish research deriving what is now known as Hubble's Law. Unfortunately, for reasons unknown, "all discussions of radial velocities and distances (and the very first empirical determination of "H") were omitted". It is speculated that these omissions were deliberate. According to the Canadian astronomer Sidney van den Bergh, "The 1927 discovery of the expansion of the Universe by Lemaitre was published in French in a low-impact journal. In the 1931 high-impact English translation of this article a critical equation was changed by omitting reference to what is now known as the Hubble constant. That the section of the text of this paper dealing with the expansion of the Universe was also deleted from that English translation suggests a deliberate omission by the unknown translator."
Shape of the universe.
Before the advent of modern cosmology, there was considerable talk about the size and shape of the universe. In 1920, the famous Shapley-Curtis debate took place between Harlow Shapley and Heber D. Curtis over this issue. Shapley argued for a small universe the size of the Milky Way galaxy and Curtis argued that the Universe was much larger. The issue was resolved in the coming decade with Hubble's improved observations.
Cepheid variable stars outside of the Milky Way.
Edwin Hubble did most of his professional astronomical observing work at Mount Wilson Observatory, the world's most powerful telescope at the time. His observations of Cepheid variable stars in spiral nebulae enabled him to calculate the distances to these objects. Surprisingly, these objects were discovered to be at distances which placed them well outside the Milky Way. They continued to be called "nebulae" and it was only gradually that the term "galaxies" took over.
Combining redshifts with distance measurements.
The parameters that appear in Hubble’s law: velocities and distances, are not directly measured. In reality we determine, say, a supernova brightness, which provides information about its distance, and the redshift "z" = ∆"λ"/"λ" of its spectrum of radiation. Hubble correlated brightness and parameter "z".
Combining his measurements of galaxy distances with Vesto Slipher and Milton Humason's measurements of the redshifts associated with the galaxies, Hubble discovered a rough proportionality between redshift of an object and its distance. Though there was considerable scatter (now known to be caused by peculiar velocities – the 'Hubble flow' is used to refer to the region of space far enough out that the recession velocity is larger than local peculiar velocities), Hubble was able to plot a trend line from the 46 galaxies he studied and obtain a value for the Hubble constant of 500 km/s/Mpc (much higher than the currently accepted value due to errors in his distance calibrations). (See cosmic distance ladder for details.)
At the time of discovery and development of Hubble's law, it was acceptable to explain redshift phenomenon as a Doppler shift in the context of special relativity, and use the Doppler formula to associate redshift "z" with velocity. Today, the velocity-distance relationship of Hubble's law is viewed as a theoretical result with velocity to be connected with observed redshift not by the Doppler effect, but by a cosmological model relating recessional velocity to the expansion of the Universe. Even for small "z" the velocity entering the Hubble law is no longer interpreted as a Doppler effect, although at small "z" the velocity-redshift relation for both interpretations is the same.
Hubble Diagram.
Hubble's law can be easily depicted in a "Hubble Diagram" in which the velocity (assumed approximately proportional to the redshift) of an object is plotted with respect to its distance from the observer. A straight line of positive slope on this diagram is the visual depiction of Hubble's law.
Cosmological constant abandoned.
After Hubble's discovery was published, Albert Einstein abandoned his work on the cosmological constant, which he had designed to modify his equations of general relativity, to allow them to produce a static solution which, in their simplest form, model either an expanding or contracting universe. After Hubble's discovery that the Universe was, in fact, expanding, Einstein called his faulty assumption that the Universe is static his "biggest mistake". On its own, general relativity could predict the expansion of the Universe, which (through observations such as the bending of light by large masses, or the precession of the orbit of Mercury) could be experimentally observed and compared to his theoretical calculations using particular solutions of the equations he had originally formulated.
In 1931, Einstein made a trip to Mount Wilson to thank Hubble for providing the observational basis for modern cosmology.
The cosmological constant has regained attention in recent decades as a hypothesis for dark energy.
Interpretation.
The discovery of the linear relationship between redshift and distance, coupled with a supposed linear relation between recessional velocity and redshift, yields a straightforward mathematical expression for Hubble's Law as follows:
where
Hubble's law is considered a fundamental relation between recessional velocity and distance. However, the relation between recessional velocity and redshift depends on the cosmological model adopted, and is not established except for small redshifts.
For distances "D" larger than the radius of the Hubble sphere "r"HS , objects recede at a rate faster than the speed of light ("See" Uses of the proper distance for a discussion of the significance of this):
Since the Hubble "constant" is a constant only in space, not in time, the radius of the Hubble sphere may increase or decrease over various time intervals. The subscript '0' indicates the value of the Hubble constant today. Current evidence suggests that the expansion of the Universe is accelerating ("see" Accelerating universe), meaning that, for any given galaxy, the recession velocity dD/dt is increasing over time as the galaxy moves to greater and greater distances; however, the Hubble parameter is actually thought to be decreasing with time, meaning that if we were to look at some "fixed" distance D and watch a series of different galaxies pass that distance, later galaxies would pass that distance at a smaller velocity than earlier ones.
Redshift velocity and recessional velocity.
Redshift can be measured by determining the wavelength of a known transition, such as hydrogen α-lines for distant quasars, and finding the fractional shift compared to a stationary reference. Thus redshift is a quantity unambiguous for experimental observation. The relation of redshift to recessional velocity is another matter. For an extensive discussion, see Harrison.
Redshift velocity.
The redshift "z" is often described as a "redshift velocity", which is the recessional velocity that would produce the same redshift "if" it were caused by a linear Doppler effect (which, however, is not the case, as the shift is caused in part by a cosmological expansion of space, and because the velocities involved are too large to use a non-relativistic formula for Doppler shift). This redshift velocity can easily exceed the speed of light. In other words, to determine the redshift velocity "v"rs, the relation:
is used. That is, there is "no fundamental difference" between redshift velocity and redshift: they are rigidly proportional, and not related by any theoretical reasoning. The motivation behind the "redshift velocity" terminology is that the redshift velocity agrees with the velocity from a low-velocity simplification of the so-called Fizeau-Doppler formula
Here, "λ"o, "λ"e are the observed and emitted wavelengths respectively. The "redshift velocity" "v"rs is not so simply related to real velocity at larger velocities, however, and this terminology leads to confusion if interpreted as a real velocity. Next, the connection between redshift or redshift velocity and recessional velocity is discussed. This discussion is based on Sartori.
Recessional velocity.
Suppose "R(t)" is called the "scale factor" of the Universe, and increases as the Universe expands in a manner that depends upon the cosmological model selected. Its meaning is that all measured distances "D(t)" between co-moving points increase proportionally to "R". (The co-moving points are not moving relative to each other except as a result of the expansion of space.) In other words:
where "t0" is some reference time. If light is emitted from a galaxy at time "te" and received by us at "t0", it is red shifted due to the expansion of space, and this redshift "z" is simply:
Suppose a galaxy is at distance "D", and this distance changes with time at a rate "dtD ". We call this rate of recession the "recession velocity" "vr":
We now define the Hubble constant as
and discover the Hubble law:
From this perspective, Hubble's law is a fundamental relation between (i) the recessional velocity contributed by the expansion of space and (ii) the distance to an object; the connection between redshift and distance is a crutch used to connect Hubble's law with observations. This law can be related to redshift "z" approximately by making a Taylor series expansion:
If the distance is not too large, all other complications of the model become small corrections and the time interval is simply the distance divided by the speed of light:
According to this approach, the relation "cz" = "v"r is an approximation valid at low redshifts, to be replaced by a relation at large redshifts that is model-dependent. See velocity-redshift figure.
Observability of parameters.
Strictly speaking, neither "v" nor "D" in the formula are directly observable, because they are properties "now" of a galaxy, whereas our observations refer to the galaxy in the past, at the time that the light we currently see left it.
For relatively nearby galaxies (redshift "z" much less than unity), "v" and "D" will not have changed much, and "v" can be estimated using the formula formula_16 where "c" is the speed of light. This gives the empirical relation found by Hubble.
For distant galaxies, "v" (or "D") cannot be calculated from "z" without specifying a detailed model for how "H" changes with time. The redshift is not even directly related to the recession velocity at the time the light set out, but it does have a simple interpretation: "(1+z)" is the factor by which the Universe has expanded while the photon was travelling towards the observer.
Expansion velocity vs relative velocity.
In using Hubble's law to determine distances, only the velocity due to the expansion of the Universe can be used. Since gravitationally interacting galaxies move relative to each other independent of the expansion of the Universe, these relative velocities, called peculiar velocities, need to be accounted for in the application of Hubble's law.
The Finger of God effect is one result of this phenomenon. In systems that are gravitationally bound, such as galaxies or our planetary system, the expansion of space is a much weaker effect than the attractive force of gravity.
Idealized Hubble's Law.
The mathematical derivation of an idealized Hubble's Law for a uniformly expanding universe is a fairly elementary theorem of geometry in 3-dimensional Cartesian/Newtonian coordinate space, which, considered as a metric space, is entirely homogeneous and isotropic (properties do not vary with location or direction). Simply stated the theorem is this:
In fact this applies to non-Cartesian spaces as long as they are locally homogeneous and isotropic; specifically to the negatively- and positively-curved spaces frequently considered as cosmological models (see shape of the universe).
An observation stemming from this theorem is that seeing objects recede from us on Earth is not an indication that Earth is near to a center from which the expansion is occurring, but rather that "every" observer in an expanding universe will see objects receding from them.
Ultimate fate and age of the universe.
The value of the Hubble parameter changes over time, either increasing or decreasing depending on the value of the so-called deceleration parameter formula_17, which is defined by
In a universe with a deceleration parameter equal to zero, it follows that "H" = 1/"t", where "t" is the time since the Big Bang. A non-zero, time-dependent value of formula_17 simply requires integration of the Friedmann equations backwards from the present time to the time when the comoving horizon size was zero.
It was long thought that "q" was positive, indicating that the expansion is slowing down due to gravitational attraction. This would imply an age of the Universe less than 1/"H" (which is about 14 billion years). For instance, a value for "q" of 1/2 (once favoured by most theorists) would give the age of the Universe as 2/(3"H"). The discovery in 1998 that "q" is apparently negative means that the Universe could actually be older than 1/"H". However, estimates of the age of the universe are very close to 1/"H".
Olbers' paradox.
The expansion of space summarized by the Big Bang interpretation of Hubble's Law is relevant to the old conundrum known as Olbers' paradox: if the Universe were infinite, static, and filled with a uniform distribution of stars, then every line of sight in the sky would end on a star, and the sky would be as bright as the surface of a star. However, the night sky is largely dark. Since the 17th century, astronomers and other thinkers have proposed many possible ways to resolve this paradox, but the currently accepted resolution depends in part on the Big Bang theory and in part on the Hubble expansion. In a universe that exists for a finite amount of time, only the light of a finite number of stars has had a chance to reach us yet, and the paradox is resolved. Additionally, in an expanding universe, distant objects recede from us, which causes the light emanating from them to be redshifted and diminished in brightness.
Dimensionless Hubble parameter.
Instead of working with Hubble's constant, a common practice is to introduce the dimensionless Hubble parameter, usually denoted by "h", and to write the Hubble's parameter "H"0 as "h" × 100 km s−1 Mpc−1, all the uncertainty relative of the value of "H"0 being then relegated on "h". Currently "h" = 0.678. This should not be confused with the dimensionless value of Hubble's constant, usually expressed in terms of Planck units, with current value of "H"0×"t"P = 1.18 × 10−61.
Determining the Hubble constant.
The value of the Hubble constant is estimated by measuring the redshift of distant galaxies and then determining the distances to the same galaxies (by some other method than Hubble's law). Uncertainties in the physical assumptions used to determine these distances have caused varying estimates of the Hubble constant.
Earlier measurement and discussion approaches.
For most of the second half of the 20th century the value of formula_20 was estimated to be between 50 and 90 (km/s)/Mpc.
The value of the Hubble constant was the topic of a long and rather bitter controversy between Gérard de Vaucouleurs, who claimed the value was around 100, and Allan Sandage, who claimed the value was near 50. In 1996, a debate moderated by John Bahcall between Gustav Tammann and Sidney van den Bergh was held in similar fashion to the earlier Shapley-Curtis debate over these two competing values.
This previously wide variance in estimates was partially resolved with the introduction of the ΛCDM model of the Universe in the late 1990s. With the ΛCDM model observations of high-redshift clusters at X-ray and microwave wavelengths using the Sunyaev-Zel'dovich effect, measurements of anisotropies in the cosmic microwave background radiation, and optical surveys all gave a value of around 70 for the constant.
More recent measurements from the Planck mission indicate a lower value of around 67.
"See table of measurements above for many recent and older measurements."
Acceleration of the expansion.
A value for formula_17 measured from standard candle observations of Type Ia supernovae, which was determined in 1998 to be negative, surprised many astronomers with the implication that the expansion of the Universe is currently "accelerating" (although the Hubble factor is still decreasing with time, as mentioned above in the Interpretation section; see the articles on dark energy and the ΛCDM model).
Derivation of the Hubble parameter.
Start with the Friedmann equation:
where formula_3 is the Hubble parameter, formula_24 is the scale factor, G is the gravitational constant, formula_25 is the normalised spatial curvature of the Universe and equal to −1, 0, or +1, and formula_26 is the cosmological constant.
Matter-dominated universe (with a cosmological constant).
If the Universe is matter-dominated, then the mass density of the Universe formula_27 can just be taken to include matter so
where formula_29 is the density of matter today. We know for nonrelativistic particles that their mass density decreases proportional to the inverse volume of the Universe, so the equation above must be true. We can also define (see density parameter for formula_30)
so formula_33 Also, by definition,
and
where the subscript nought refers to the values today, and formula_36. Substituting all of this into the Friedmann equation at the start of this section and replacing formula_24 with formula_38 gives
Matter- and dark energy-dominated universe.
If the Universe is both matter-dominated and dark energy- dominated, then the above equation for the Hubble parameter will also be a function of the equation of state of dark energy. So now:
where formula_41 is the mass density of the dark energy. By definition, an equation of state in cosmology is formula_42, and if we substitute this into the fluid equation, which describes how the mass density of the Universe evolves with time,
If w is constant,
Therefore, for dark energy with a constant equation of state w, formula_47. If we substitute this into the Friedman equation in a similar way as before, but this time set formula_48, which is assuming we live in a spatially flat universe, (see Shape of the Universe)
If dark energy does not have a constant equation-of-state w, then
and to solve this we must parametrize formula_51, for example if formula_52, giving
Other ingredients have been formulated recently. In certain era, where the high energy experiments seem to have a reliable access in analyzing the property of the matter dominating the background geometry, with this era we mean the quark-gluon plasma, the transport properties have been taken into consideration. Therefore, the evolution of the Hubble parameter and of other essential cosmological parameters, in such a background are found to be considerably (non-negligibly) different than their evolution in an ideal, gaseous, non-viscous background.
Units derived from the Hubble constant.
Hubble time.
The Hubble constant formula_20 has units of inverse time, i.e. formula_55 or 14.4 billion years. This is somewhat longer than the age of the universe of 13.8 billion years. The Hubble time is the age it would have had if the expansion had been linear, and it is different from the real age of the universe because the expansion isn't linear.
We currently appear to be approaching a period where the expansion is exponential due to the increasing dominance of vacuum energy. In this regime, the Hubble parameter is constant, and the universe grows by a factor e each Hubble time:
Over long periods of time, the dynamics are complicated by general relativity, dark energy, inflation, etc., as explained above.
Hubble length.
The Hubble length or Hubble distance is a unit of distance in cosmology, defined as "cH"0−1 — the speed of light multiplied by the Hubble time. It is equivalent to 4,228 million parsecs or 13.8 billion light years. (The numerical value of the Hubble length in light years is, by definition, equal to that of the Hubble time in years.) The Hubble distance would be the distance between the Earth and the galaxies which are "currently" receding from us at the speed of light, as can be seen by substituting into the equation for Hubble's law, .
Hubble volume.
The Hubble volume is sometimes defined as a volume of the Universe with a comoving size of "c/H"0. The exact definition varies: it is sometimes defined as the volume of a sphere with radius "c/H"0, or alternatively, a cube of side "c/H"0. Some cosmologists even use the term Hubble volume to refer to the volume of the observable universe, although this has a radius approximately three times larger.

</doc>
<doc id="42977" url="http://en.wikipedia.org/wiki?curid=42977" title="Daimler AG">
Daimler AG

   (]) is a German multinational automotive corporation. Daimler AG is headquartered in Stuttgart, Baden-Württemberg, Germany. By unit sales, it is the thirteenth-largest car manufacturer and second-largest truck manufacturer in the world. In addition to automobiles, Daimler manufactures buses and provides financial services through its Daimler Financial Services arm.
As of 2014, Daimler owns or has shares in a number of car, bus, truck and motorcycle brands including Mercedes-Benz, Mercedes-AMG, Smart Automobile, Freightliner, Western Star, Thomas Built Buses, Setra, BharatBenz, Mitsubishi Fuso, MV Agusta as well as shares in Denza, KAMAZ, Beijing Automotive Group, Tesla Motors, and Renault-Nissan Alliance. The Maybach brand was closed at the end of 2012, but was revived in November 2014 as "Mercedes-Maybach", an ultra luxury edition of the Mercedes-Benz S-Class. In 2014 Daimler sold 2.5 million vehicles.
History.
Daimler AG is a German manufacturer of automobiles, motor vehicles, and engines, which dates back more than a century.
An "Agreement of Mutual Interest" was signed on 1 May 1924 between Benz & Cie (founded 1883 by Karl Benz) and Daimler Motoren Gesellschaft (founded 1890 by Gottlieb Daimler and Wilhelm Maybach).
Both companies continued to manufacture their separate automobile and internal combustion engine brands until, on 28 June 1926, when Benz & Cie. and Daimler Motoren Gesellschaft AG formally merged—becoming Daimler-Benz AG—and agreed that, thereafter, all of the factories would use the brand name of Mercedes-Benz on their automobiles.
In 1998, Daimler-Benz and Chrysler Corporation announced the world's largest cross-border deal ever, valued at US$38billion, and the resulting change in company name to "DaimlerChrysler AG".
In 2007, when the Chrysler group was sold off to Cerberus Capital Management (see below), the name of the parent company was changed to simply "Daimler AG".
In November 2014, Daimler announced it would acquire 25 percent of Italian motorcycle producer MV Agusta for an undisclosed fee.
Timeline of Daimler AG.
Benz & Company, 1883–1926 
Daimler Motoren Gesellschaft AG, 1890–1926 
Daimler-Benz AG, 1926–1998 
DaimlerChrysler AG, 1998–2007 
Daimler AG, 2007–present
Merger with Chrysler.
In a so-called "Merger of Equals," Daimler-Benz AG and United States-based automobile manufacturer Chrysler Corporation, the third-largest American automaker, merged in 1998 in an exchange of shares as Daimler-Benz AG bought 92% of Chrysler, and 8% of Chrysler remained independent and formed DaimlerChrysler AG. The terms of the merger allowed Daimler-Benz's non-automotive businesses such as Daimler-Benz InterServices AG (created in 1989 to handle data processing, financial and insurance services, and real estate management for the Daimler group) to continue to pursue their respective strategies of expansion. Debis reported revenues of $8.6 bn (DM 15.5 bn) in 1997.
The merger was contentious with investors launching lawsuits over whether the transaction was the 'merger of equals' that senior management claimed or actually amounted to a Daimler-Benz takeover of Chrysler. A class action investor lawsuit was settled in August 2003 for US$300 million while a suit by billionaire investor activist Kirk Kerkorian was dismissed on 7 April 2005. The transaction claimed the job of its architect, Chairman Jürgen E. Schrempp, who resigned at the end of 2005 in response to the fall of the company's share price following the transaction. The merger was also the subject of a book "Taken for a Ride: How Daimler-Benz Drove Off With Chrysler", (2000) by Bill Vlasic and Bradley A. Stertz.
Another issue of contention is whether the merger delivered promised synergies and successfully integrated the two businesses. Martin H. Wiggers' concept of a platform strategy like the VW Group, was implemented only for a few models, so the synergy effects in development and production were too low. As late as 2002, DaimlerChrysler appeared to run two independent product lines. Later that year, the company launched products that appear to integrate elements from both sides of the company, including the Chrysler Crossfire, which was based on the Mercedes SLK platform and utilized Mercedes's 3.2L V6, and the Dodge Sprinter/Freightliner Sprinter, a re-badged Mercedes-Benz Sprinter van.
Sale of Chrysler.
Daimler agreed to sell the Chrysler unit to Cerberus Capital Management in May 2007 for US$6 billion. Through most of its history, Chrysler has been the third largest of the "Big 3" U.S. automakers, but in January 2007, DaimlerChrysler, excluding its luxury Mercedes and Maybach lines, also outsold traditionally second place Ford, though behind General Motors and Toyota.
Chrysler reported losses of US$1.5 billion in 2006. It then announced plans to lay off 13,000 employees in mid-February 2007, close a major assembly plant and reduce production at other plants in order to restore profitability by 2008.
DaimlerChrysler had reportedly approached other carmakers and investment groups to sell Chrysler in early 2007. General Motors was reported to be a suitor, but on 3 August 2007, DaimlerChrysler completed the sale of Chrysler Group to Cerberus Capital Management. The original agreement stated that Cerberus would take an 80.1 percent stake in the new company, Chrysler Holding LLC. DaimlerChrysler changed its name to Daimler AG and retained the remaining 19.9% stake in the separated Chrysler.
The terms saw Daimler pay Cerberus US$650 million to take Chrysler and associated liabilities off its hands. Of the US$7.4 billion purchase price, Cerberus Capital Management will invest US$5 billion in Chrysler Holdings and US$1.05 billion in Chrysler's financial unit. The de-merged Daimler AG received US$1.35 billion directly from Cerberus but directly invested US$2 billion in Chrysler itself.
Since Chrysler's 2009 bankruptcy filing in the United States, Chrysler has been controlled by Italian automaker Fiat and plans to integrate Chrysler's products into the Fiat portfolio, such as Lancia and Chrysler's namesake brand, and Fiat's namesake brand with Dodge. Despite the fact it had been nearly seven years after the Daimler/Chrysler split, the fourth-generation Jeep Grand Cherokee shares a platform with the Mercedes-Benz M-Class. This also includes the Chrysler LX platform vehicles which initially used Mercedes-Benz components since its 2005 introduction.
Corporate affairs.
Management.
Dieter Zetsche has been the Chairman of Daimler and Head of Mercedes-Benz Cars since 1 January 2006 as well as member of the Board of Management since 1998. He was former President and CEO of the Chrysler, LLC (previously owned by Daimler AG), he may be best known in the United States as "Dr. Z" from a Chrysler advertising campaign called "Ask Dr. Z".
Current members of the Board of Management of Daimler AG are:
The Board of Management total members of seven, after the unexpected resignation on 28 January 2014 of Andreas Renschler, former head of Manufacturing and Procurement Mercedes-Benz Cars & Mercedes-Benz Vans, has been brought back to eight after the nomination on 1 January 2015 of Swedish-born Ola Källenius to the Board of Management as Head of Mercedes-Benz Cars Marketing and Sales.
, the members of Daimler AG's Supervisory Board are: Manfred Bischoff (Chairman), Michael Brecht (Deputy Chairman), Paul Achleitner, Sari Baldauf, Clemens Börsig, Bernd Bohr, Jürgen Hambrecht, Petraea Heynike, Jörg Hofmann, Andrea Jung, Joe Kaeser, Jürgen Langer, Ergun Lümali, Sabine Maaßen, Wolfgang Nieke, Bernd Pischetsrieder, Valter Sanches, Jörg Spies, Frank Weber, Elke Tönjes-Werner.
Shareholder Structure.
"by Ownership"
"by Region"
29.7% Europe (excluding Germany),
32.1% German,
25.5% United States,
6.8% Kuwait,
5.4% Asia,
0.5% Others.
EADS shareholding.
As of March 2010, Daimler owned a 22.5% share of EADS, of which the public sector held 40%. 
On the side of the public sector, the KfW banking group holds 13%, HGV Hamburger Gesellschaft fur Vermogens- und Beteiligungsverwaltung (State of Hamburg) holds 10%, Hannoversche Beteiligungsgesellschaft (State of Lower Saxony) holds 5%, Bayerische Landesbodenkreditanstalt, Anstalt der Bayerischen Landesbank holds 3.5%, LfA Forderbank Bayern holds 1.5%, Landesbank Baden-Württemberg and Landeskreditbank Baden-Württemberg – Forderbank (L-Bank) each holds 2.5%, and Bremer Investitions-Gesellschaft (State of Bremen) holds 2%. 
North Charleston Expansion.
Daimler AG has announced a 1,200 jobs package to the North Charleston region for its van plant. This will allow the company to start manufacturing Mercedes-Benz Sprinter vans from scratch in a North Charleston plant to meet demand in North America. Currently, these vans are set up in Germany, then shipped to the United States partially disassembled for reassembly. This is all to avoid import tariffs, a practice that started in 2010. An official said that the Sprinter’s popularity in North America is making that process less efficient. he North Charleston plant employed only 100 workers. The Sprinter is available on the U.S. market as a panel van, crew bus and chassis in several variants with three lengths and roof heights, with six-cylinder diesel or gasoline engines. The Sprinter has been assembled and sold in the United States since 2001.
Brands.
Daimler sells automobiles under the following brands worldwide:
Locations.
The Daimler AG has a worldwide networks of production plants and research centres. The following list is a description of all locations worldwide, that include a Daimler plant, including plants for Daimler subsidiaries EvoBus, Daimler Trucks North America, Detroit Diesel, Freightliner Trucks and Mitsubishi Fuso Truck and Bus Corporation. The list excludes the location of Daimler Financial Services locations.
Holdings.
Daimler currently holds interests in the following companies:
At the end of 2011 McLaren Group completely bought back the stocks from Daimler.
17 April 2013, Daimler AG exits EADS, the parent company of Airbus of Europe.
Joint ventures and alliances.
Beijing Automotive Group.
In February 2013, Daimler acquired a 12% stake in Beijing Automotive Industry Holding Co Ltd (BAIC), becoming the first western car manufacturer to own a stake in a Chinese company.
Daimler works with China's Beiqi Foton (a subsidiary of BAIC) to build Auman trucks.
Denza.
In 2010 BYD Auto and Daimler AG created a new joint venture Shenzhen BYD Daimler New Technology Co., Ltd. In 2012 a the new brand Denza was launched by the joint venture to specialise in electric cars.
Fujian Benz.
In 2007 Daimler created a joint venture with Fujian Motors Group and China Motor Corporation and created Fujian Benz (originally Fujian Daimler Automotive Co.).
Renault-Nissan and Daimler Alliance.
On 7 April 2010 Renault-Nissan executives, Carlos Ghosn and Dr. Dieter Zetsche announced a partnership between the three companies. The first fruits of the alliance in 2012 included engine sharing (Infiniti Q50 utilising Mercedes diesel engines) and a re-badged Renault Kangoo being sold as a Mercedes-Benz Citan.
Alternative propulsion.
Biofuel research.
Daimler AG is involved in a joint project with Archer Daniels Midland Company and Bayer CropScience to develop the semi-evergreen shrub jatropha curcas as a biofuel.
Electric.
Daimler AG and the utility company RWE AG were set in 2009 to begin a joint electric car and charging station test project in the German capital, Berlin, called "E-Mobility Berlin". 
Following trials in 2007 and then with Tesla in 2009, Daimler is building a production Smart electric drive car using Tesla's battery technology.
Daimler's joint venture with BYD has resulted in the creation of the new brand Denza.
Fuel cell.
Daimler has been involved with fuel cell development for some time, with a number of research and concept vehicles shown and demonstrated, the first being the 2002 Mercedes-Benz F-Cell car and the Mercedes-Benz Citaro Hydrogen bus. In 2013, the Renault-Nissan/Daimler alliance was joined by Ford to further develop the fuel cell technology with an aim for production by 2017.
Hybrid.
Mercedes-Benz launched its first passenger car model equipped with a hybrid drive system in summer 2009, the Mercedes-Benz S-Class 400 Hybrid. and the Citaro Hybrid bus in 2007. Daimler Trucks and Mitusbishi Fuso have also trailed various hybrid models including the Mitsubishi Fuso Canter Eco Hybrid and Mitsubishi Fuso Aero Star Aero Star Eco Hybrid bus.
Formula One.
On 16 November 2009 Daimler (45.1%) och Aabar Investments (30%) purchased a 75.1% stake in Brawn GP. The company was rebranded as Mercedes GP with its base in Brackley, UK, with Ross Brawn remaining team principal., However the purchase of Brawn meant that Daimler sold back its stake in McLaren in stages that ended in 2011. Mercedes will continue to provide sponsorship and engines to McLaren until 2015, when McLaren will switch to engines from Honda.
Prior to the 2011 season, Daimler and Aabar Investments purchased the remaining 24.9% stake owned by the team management in February 2011. In November 2012 Aabar Investments sold its remaining shares and the team (rebranded as Mercedes AMG Petronas F1 Team) is now wholly owned by Daimler.
Daimler also owns Mercedes AMG High Performance Powertrains, which as of 2014 supplies engines to Force India, McLaren and Williams, in addition to Mercedes AMG Petronas.
Bribery and corruption.
On 1 April 2010, Daimler AG's German and Russian subsidiaries each plead guilty to two counts of bribery charges brought by the U.S. Justice Department and the U.S. Securities and Exchange Commission. Daimler itself had to pay US$185 million as a settlement, but the company and its Chinese subsidiary remained subject to a two-year deferred prosecution agreement requiring further cooperation with regulators, adherence to internal controls and meeting other terms before final sentencing. Daimler would face harsher penalties should it fail to meet the terms of the agreement during the two-year period.
Additionally, Louis J. Freeh, a former director of the Federal Bureau of Investigation, served as an independent monitor to oversee Daimler's compliance with anti-bribery laws.
U.S. prosecutors accused key executives of Daimler, Daimler subsidiaries, and Daimler affiliates of illegally showering foreign officials with money and gifts between 1998 and 2008 to secure government contracts around the world. The investigation for the case revealed that Daimler improperly paid some $56 million in bribes related to more than 200 transactions in at least 22 countries (including China, Russia, Turkey, Hungary, Greece, Latvia, Serbia and Montenegro, Egypt and Nigeria, among other places) that, in return, awarded the company $1.9 billion in revenue and at least $91.4 million in illegal profits.
The SEC case was sparked in 2004 after David Bazzetta, a former auditor at then DaimlerChrysler Corp, filed a whistleblower complaint after he was fired for raising questions about bank accounts controlled by Mercedes-Benz units in South America. Bazzetta alleged that he learned in a July 2001 corporate audit executive committee meeting in Stuttgart that business units "continued to maintain secret bank accounts to bribe foreign government officials", though the company knew the practice violated U.S. laws.
In another attempt to silence Bazzetta, Daimler later offered to settle his termination of employment suit out of court and he eventually accepted a settlement. But Daimler's strategy with Bazzetta proved to be a failure as the U.S. criminal investigation for violating anti-bribery laws was already underway in what has been one of the most wide-ranging cases brought against a foreign corporation.
According to the charges, the bribes were frequently made by over-invoicing customers and paying the excess back to top government officials or their proxies. The bribes also took the form of luxury European vacations, armored Mercedes vehicles for high-ranking government officials and a birthday gift to the then notorious dictator of Turkmenistan, Turkmenbashi (Saparmurat Niyazov), including a golden box and 10,000 copies of his personal manifesto, Ruhnama, translated into German.
Investigators also found that the firm violated the terms of the United Nations' Oil-for-Food Programme with Iraq by giving kickbacks worth 10% of the contract values to officials within the Iraqi government, then led by Saddam Hussein. The SEC said the company made more than $4 million in profit from the sale of vehicles and spare parts in the corrupt Oil-for-Food deals.
U.S. prosecutors further alleged that some bribes were paid through shell companies based in the U.S. "In some cases Daimler wired these improper payments to U.S. bank accounts or to the foreign bank accounts of U.S. shell companies in order to transmit the bribe," the court papers said.
Prosecutors said that Daimler engaged in a "long-standing practice" of paying bribes, due in part to a corporate culture that encouraged the practice.
"Using offshore bank accounts, third-party agents and deceptive pricing practices, these companies [Daimler AG, its subsidiaries and affiliates] saw foreign bribery as a way of doing business," said Mythili Raman, a principal deputy in the Justice Department's criminal division.
"It is no exaggeration to describe corruption and bribe-paying at Daimler as a standard business practice," Robert Khuzami, director of the SEC's enforcement division, said in a statement.
"We have learned a lot from past experience," Dieter Zetsche, chairman of Daimler's board, said in a statement.
As per the agreement with prosecutors, the two Daimler subsidiaries admitted to knowingly violating the Foreign Corrupt Practices Act, which bars companies and their officials from paying bribes to foreign officials to win business. The Foreign Corrupt Practices Act applies to any company that lists its shares on U.S. stock exchanges. Daimler AG was listed with the symbol "DAI" on the New York Stock Exchange, giving the Justice Department jurisdiction over the German car maker's payments in countries around the globe.
Judge Richard J. Leon of the United States District Court in Washington, D.C., approved the plea agreement and settlement, calling it a "just resolution."
The primary case is "USA v. Daimler AG," United States District Court for the District of Columbia, No. 1:10-cr-00063-RJL .

</doc>
<doc id="42978" url="http://en.wikipedia.org/wiki?curid=42978" title="Jack London">
Jack London

John Griffith "Jack" London (born John Griffith Chaney, January 12, 1876 – November 22, 1916) was an American author, journalist, and social activist. He was a pioneer in the then-burgeoning world of commercial magazine fiction and was one of the first fiction writers to obtain worldwide celebrity and a large fortune from his fiction alone. Some of his most famous works include "The Call of the Wild" and "White Fang", both set in the Klondike Gold Rush, as well as the short stories "To Build a Fire", "An Odyssey of the North", and "Love of Life". He also wrote of the South Pacific in such stories as "The Pearls of Parlay" and "The Heathen", and of the San Francisco Bay area in "The Sea Wolf".
London was a passionate advocate of unionization, socialism, and the rights of workers. He wrote several powerful works dealing with these topics, such as his dystopian novel "The Iron Heel", his non-fiction exposé "The People of the Abyss", and "The War of the Classes".
Family.
Jack London's mother, Flora Wellman, was the fifth and youngest child of Pennsylvania Canal builder Marshall Wellman and his first wife, Eleanor Garrett Jones. Marshall Wellman was descended from Thomas Wellman, an early Puritan settler in the Massachusetts Bay Colony. Flora left Ohio and moved to the Pacific coast when her father remarried after her mother died. In San Francisco, Flora worked as a music teacher and spiritualist claiming to channel the spirit of an Indian chief.
Biographer Clarice Stasz and others believe London's father was astrologer William Chaney. Flora Wellman was living with Chaney in San Francisco when she became pregnant. Whether Wellman and Chaney were legally married is unknown. Most San Francisco civil records were destroyed by the extensive fires that followed the 1906 earthquake; nobody knows what name appeared on his birth certificate. Stasz notes that in his memoirs, Chaney refers to London's mother Flora Wellman as having been his "wife"; he also cites an advertisement in which Flora called herself "Florence Wellman Chaney."
According to Flora Wellman's account, as recorded in the "San Francisco Chronicle" of June 4, 1875, Chaney demanded that she have an abortion. When she refused, he disclaimed responsibility for the child. In desperation, she shot herself. She was not seriously wounded, but she was temporarily deranged. After giving birth, Flora turned the baby over to ex-slave Virginia Prentiss, who remained a major maternal figure throughout London's life. Late in 1876, Flora Wellman married John London, a partially disabled Civil War veteran, and brought her baby John, later known as Jack, to live with the newly married couple. The family moved around the San Francisco Bay Area before settling in Oakland, where London completed grade school.
In 1897, when he was 21 and a student at the University of California, Berkeley, London searched for and read the newspaper accounts of his mother's suicide attempt and the name of his biological father. He wrote to William Chaney, then living in Chicago. Chaney responded that he could not be London's father because he was impotent; he casually asserted that London's mother had relations with other men and averred that she had slandered him when she said he insisted on an abortion. Chaney concluded that he was more to be pitied than London. London was devastated by his father's letter; in the months following, he quit school at Berkeley and went to the Klondike.
Early life.
London was born near Third and Brannan Streets in San Francisco. The house burned down in the fire after the 1906 San Francisco earthquake; the California Historical Society placed a plaque at the site in 1953. Although the family was working class, it was not as impoverished as London's later accounts claimed. London was essentially self-educated.
In 1885, London found and read Ouida's long Victorian novel "Signa". He credited this as the seed of his literary success. In 1886, he went to the Oakland Public Library and found a sympathetic librarian, Ina Coolbrith, who encouraged his learning. (She later became California's first "poet laureate" and an important figure in the San Francisco literary community).
In 1889, London began working 12 to 18 hours a day at Hickmott's Cannery. Seeking a way out, he borrowed money from his black foster mother Virginia Prentiss, bought the sloop "Razzle-Dazzle" from an oyster pirate named French Frank, and became an oyster pirate. In his memoir, "John Barleycorn", he claims to have stolen French Frank's mistress Mamie. After a few months, his sloop became damaged beyond repair. London hired on as a member of the California Fish Patrol.
In 1893, he signed on to the sealing schooner "Sophie Sutherland", bound for the coast of Japan. When he returned, the country was in the grip of the panic of '93 and Oakland was swept by labor unrest. After grueling jobs in a jute mill and a street-railway power plant, he joined Kelly's Army and began his career as a tramp. In 1894, he spent 30 days for vagrancy in the Erie County Penitentiary at Buffalo, New York. In "The Road", he wrote:
After many experiences as a hobo and a sailor, he returned to Oakland and attended Oakland High School. He contributed a number of articles to the high school's magazine, "The Aegis". His first published work was "Typhoon off the Coast of Japan", an account of his sailing experiences.
As a schoolboy, London often studied at Heinold's First and Last Chance Saloon, a port-side bar in Oakland. At 17, he confessed to the bar's owner, John Heinold, his desire to attend university and pursue a career as a writer. Heinold lent London tuition money to attend college.
London desperately wanted to attend the University of California, Berkeley. In 1896, after a summer of intense studying to pass certification exams, he was admitted. Financial circumstances forced him to leave in 1897 and he never graduated. No evidence suggests that London wrote for student publications while studying at Berkeley.
While at Berkeley, London continued to study and spend time at Heinold's saloon, where he was introduced to the sailors and adventurers who would influence his writing. In his autobiographical novel, "John Barleycorn," London mentioned the pub's likeness seventeen times. Heinold's was the place where London met Alexander McLean, a captain known for his cruelty at sea, on whom the protagonist in London's novel "The Sea-Wolf", Wolf Larsen, is based.
Heinold's First and Last Chance Saloon is now unofficially named Jack London's Rendezvous in his honor.
Gold rush and first success.
On July 12, 1897, London (age 21) and his sister's husband Captain Shepard sailed to join the Klondike Gold Rush. This was the setting for some of his first successful stories. London's time in the Klondike, however, was detrimental to his health. Like so many other men who were malnourished in the goldfields, London developed scurvy. His gums became swollen, leading to the loss of his four front teeth. A constant gnawing pain affected his hip and leg muscles, and his face was stricken with marks that always reminded him of the struggles he faced in the Klondike. Father William Judge, "The Saint of Dawson," had a facility in Dawson that provided shelter, food and any available medicine to London and others. His struggles there inspired London's short story, "To Build a Fire" (1902, revised in 1908), which many critics assess as his best.
His landlords in Dawson were mining engineers Marshall Latham Bond and Louis Whitford Bond, educated at Yale and Stanford. The brothers' father, Judge Hiram Bond, was a wealthy mining investor. The Bonds, especially Hiram, were active Republicans. Marshall Bond's diary mentions friendly sparring with London on political issues as a camp pastime.
London left Oakland with a social conscience and socialist leanings; he returned to become an activist for socialism. He concluded that his only hope of escaping the work "trap" was to get an education and "sell his brains". He saw his writing as a business, his ticket out of poverty, and, he hoped, a means of beating the wealthy at their own game. On returning to California in 1898, London began working deliberately to get published, a struggle described in his novel, "Martin Eden" (serialized in 1908, published in 1909). His first published story since high school was "To the Man On Trail", which has frequently been collected in anthologies. When "The Overland Monthly" offered him only five dollars for it—and was slow paying—London came close to abandoning his writing career. In his words, "literally and literarily I was saved" when "The Black Cat" accepted his story "A Thousand Deaths", and paid him $40—the "first money I ever received for a story."
London began his writing career just as new printing technologies enabled lower-cost production of magazines. This resulted in a boom in popular magazines aimed at a wide public and a strong market for short fiction. In 1900, he made $2,500 in writing, about $ in today's currency.
Among the works he sold to magazines was a short story known as either "Diable" (1902) or "Bâtard" (1904), in two editions of the same basic story; London received $141.25 for this story on May 27, 1902. In the text, a cruel French Canadian brutalizes his dog, and the dog retaliates and kills the man. London told some of his critics that man's actions are the main cause of the behavior of their animals, and he would show this in another story, "The Call of the Wild".
In early 1903, London sold "The Call of the Wild" to "The Saturday Evening Post" for $750, and the book rights to Macmillan for $2,000. Macmillan's promotional campaign propelled it to swift success.
While living at his rented villa on Lake Merritt in Oakland, London met poet George Sterling; in time they became best friends. In 1902, Sterling helped London find a home closer to his own in nearby Piedmont. In his letters London addressed Sterling as "Greek", owing to Sterling's aquiline nose and classical profile, and he signed them as "Wolf". London was later to depict Sterling as Russ Brissenden in his autobiographical novel "Martin Eden" (1910) and as Mark Hall in "The Valley of the Moon" (1913).
In later life London indulged his wide-ranging interests by accumulating a personal library of 15,000 volumes. He referred to his books as "the tools of my trade".
First marriage (1900–1904).
London married Elizabeth "Bessie" Maddern on April 7, 1900, the same day "The Son of the Wolf" was published. Bess had been part of his circle of friends for a number of years. She was related to stage actresses Minnie Maddern Fiske and Emily Stevens. Stasz says, "Both acknowledged publicly that they were not marrying out of love, but from friendship and a belief that they would produce sturdy children.", Kingman says, "they were comfortable together... Jack had made it clear to Bessie that he did not love her, but that he liked her enough to make a successful marriage."
During the marriage, London continued his friendship with Anna Strunsky, co-authoring "The Kempton-Wace Letters", an epistolary novel contrasting two philosophies of love. Anna, writing "Dane Kempton's" letters, arguing for a romantic view of marriage, while London, writing "Herbert Wace's" letters, argued for a scientific view, based on Darwinism and eugenics. In the novel, his fictional character contrasted two women he had known.
London's pet name for Bess was "Mother-Girl" and Bess's for London was "Daddy-Boy". Their first child, Joan, was born on January 15, 1901 and their second, Bessie (later called Becky), on October 20, 1902. Both children were born in Piedmont, California. Here London wrote one of his most celebrated works, "The Call of the Wild."
While London had pride in his children, the marriage was strained. Kingman says that by 1903, the couple were close to separation as they were "extremely incompatible". Nevertheless, "Jack was still so kind and gentle with Bessie that when Cloudsley Johns was a house guest in February 1903 he didn't suspect a breakup of their marriage."
London reportedly complained to friends Joseph Noel and George Sterling: [Bessie] is devoted to purity. When I tell her morality is only evidence of low blood pressure, she hates me. She'd sell me and the children out for her damned purity. It's terrible. Every time I come back after being away from home for a night she won't let me be in the same room with her if she can help it. Stasz writes that these were "code words for [Bess's] fear that [Jack] was consorting with prostitutes and might bring home venereal disease."
On July 24, 1903, London told Bessie he was leaving and moved out. During 1904, London and Bess negotiated the terms of a divorce, and the decree was granted on November 11, 1904.
War correspondent (1904).
London accepted an assignment of the "San Francisco Examiner" to cover the Russo-Japanese War in early 1904, arriving in Yokohama on January 25, 1904. He was arrested by Japanese authorities in Shimonoseki, but released through the intervention of American ambassador Lloyd Griscom. After travelling to Korea, he was again arrested by Japanese authorizes for straying too close to the border with Manchuria without official permission and was sent back to Seoul. Released again, he was permitted to travel with the Imperial Japanese Army to the border, and to observe the Battle of the Yalu. London then made a request to William Randolph Hearst, the owner of the "San Francisco Examiner", to be allowed to transfer to the Imperial Russian Army, where he felt that restrictions on his reporting and his movements would be less severe. However, before this could be arranged, he was arrested for a third time in four months, this time for assaulting his Japanese assistants, whom he accused of stealing the fodder for his horse. Released through the personal intervention of President Theodore Roosevelt, London departed the front on June 1904.
Bohemian Club.
On August 18, 1904, London went with his close friend, the poet George Sterling, to "Summer High Jinks" at the Bohemian Grove. London was elected to honorary membership in the Bohemian Club and took part in many activities. Other noted members of the Bohemian Club during this time included Ambrose Bierce, Gelett Burgess, Allan Dunn, John Muir, and Frank Norris.
Beginning in December 1914, London worked on "The Acorn Planter, A California Forest Play", to be performed as one of the annual Grove Plays, but it was never selected; it was described as too difficult to set to music. London published "The Acorn Planter" in 1916.
Second marriage.
After divorcing Maddern, London married Charmian Kittredge in 1905. London was introduced to Kittredge by his MacMillan publisher, George Platt Brett, Sr., while Kittredge served as Brett's secretary. Biographer Russ Kingman called Charmian "Jack's soul-mate, always at his side, and a perfect match." Their time together included numerous trips, including a 1907 cruise on the yacht "Snark" to Hawaii and Australia. Many of London's stories are based on his visits to Hawaii, the last one for 10 months beginning in December 1915.
The couple also visited Goldfield, Nevada, in 1907, where they were guests of the Bond brothers, London's Dawson City landlords. The Bond brothers were working in Nevada as mining engineers.
London had contrasted the concepts of the "Mother Woman" and the "Mate Woman" in "The Kempton-Wace Letters." His pet name for Bess had been "Mother-Girl;" his pet name for Charmian was "Mate-Woman." Charmian's aunt and foster mother, a disciple of Victoria Woodhull, had raised her without prudishness. Every biographer alludes to Charmian's uninhibited sexuality.
Joseph Noel calls the events from 1903 to 1905 "a domestic drama that would have intrigued the pen of an Ibsen... London's had comedy relief in it and a sort of easy-going romance." In broad outline, London was restless in his first marriage, sought extramarital sexual affairs, and found, in Charmian Kittredge, not only a sexually active and adventurous partner, but his future life-companion. They attempted to have children; one child died at birth, and another pregnancy ended in a miscarriage.
In 1906, London published in "Collier's" magazine his eye-witness report of the San Francisco earthquake.
Beauty Ranch (1905–1916).
In 1905, London purchased a 1000 acre ranch in Glen Ellen, Sonoma County, California, on the eastern slope of Sonoma Mountain, for $26,450. He wrote: "Next to my wife, the ranch is the dearest thing in the world to me." He desperately wanted the ranch to become a successful business enterprise. Writing, always a commercial enterprise with London, now became even more a means to an end: "I write for no other purpose than to add to the beauty that now belongs to me. I write a book for no other reason than to add three or four hundred acres to my magnificent estate." After 1910, his literary works were mostly potboilers, written out of the need to provide operating income for the ranch.
Stasz writes that London "had taken fully to heart the vision, expressed in his agrarian fiction, of the land as the closest earthly version of Eden ... he educated himself through the study of agricultural manuals and scientific tomes. He conceived of a system of ranching that today would be praised for its ecological wisdom." He was proud to own the first concrete silo in California, a circular piggery that he designed. He hoped to adapt the wisdom of Asian sustainable agriculture to the United States. He hired both Italian and Chinese stonemasons, whose distinctly different styles are obvious.
The ranch was an economic failure. Sympathetic observers such as Stasz treat his projects as potentially feasible, and ascribe their failure to bad luck or to being ahead of their time. Unsympathetic historians such as Kevin Starr suggest that he was a bad manager, distracted by other concerns and impaired by his alcoholism. Starr notes that London was absent from his ranch about six months a year between 1910 and 1916, and says, "He liked the show of managerial power, but not grinding attention to detail ... London's workers laughed at his efforts to play big-time rancher [and considered] the operation a rich man's hobby."
London spent $80,000 ($ in current value) to build a 15000 sqft stone mansion called Wolf House on the property. Just as the mansion was nearing completion, two weeks before the Londons planned to move in, it was destroyed by fire.
London's last visit to Hawaii, beginning in December 1915, lasted eight months. He met with Duke Kahanamoku, Prince Jonah Kūhiō Kalaniana'ole, Queen Lili‘uokalani and many others, before returning to his ranch in July 1916. He was suffering from kidney failure, but he continued to work.
The ranch (abutting stone remnants of Wolf House) is now a National Historic Landmark and is protected in Jack London State Historic Park.
Animal activism.
London witnessed animal cruelty in the training of circus animals, and his subsequent novels "Jerry of the Islands" and "Michael, Brother of Jerry" included a foreword entreating the public to become more informed about this practice. In 1918, the Massachusetts Society for the Prevention of Cruelty to Animals and the American Humane Education Society teamed up to create the Jack London Club, which sought to inform the public about cruelty to circus animals and encourage them to protest this establishment. Support from Club members led to a temporary cessation of trained animal acts at Ringling-Barnum and Bailey in 1925.
Death.
London died November 22, 1916, in a sleeping porch in a cottage on his ranch. London had been a robust man but had suffered several serious illnesses, including scurvy in the Klondike. Additionally, during travels on the "Snark", he and Charmian may have picked up unspecified tropical infections. At the time of his death, he suffered from dysentery, uremia, and late stage alcoholism; he was in extreme pain and taking morphine, and it is possible that a morphine overdose, accidental or deliberate, may have contributed to his death.
London's ashes were buried, together with those of his second wife Charmian (who died in 1955), in Jack London State Historic Park, in Glen Ellen, California, not far from the Wolf House. Jack's funeral took place on November 26, 1916, attended only by close friends, relatives, and workers of the property. In accordance with his wishes, he was cremated and buried next to some pioneer children, under a rock that belonged to the Wolf House. After Charmian's death in 1955, she was also cremated and then buried with her husband in the same simple spot that her husband chose. The simple grave is marked by only a mossy boulder.
Suicide debate.
Many older sources describe London's death as a suicide, and some still do. This conjecture appears to be a rumor, or speculation based on incidents in his fiction writings. His death certificate gives the cause as uremia, following acute renal colic, a type of pain often described as "the worst pain...ever experienced", commonly caused by kidney stones. Uremia is also known as uremic poisoning.
The biographer Stasz writes, "Following London's death, for a number of reasons, a biographical myth developed in which he has been portrayed as an alcoholic womanizer who committed suicide. Recent scholarship based upon firsthand documents challenges this caricature." Most biographers, including Russ Kingman, now agree he died of uremia aggravated by an accidental morphine overdose.
London's fiction featured several suicides. In his autobiographical memoir "John Barleycorn", he claims, as a youth, to have drunkenly stumbled overboard into the San Francisco Bay, "some maundering fancy of going out with the tide suddenly obsessed me". He said he drifted and nearly succeeded in drowning before sobering up and being rescued by fishermen. In the dénouement of "The Little Lady of the Big House", the heroine, confronted by the pain of a mortal gunshot wound, undergoes a physician-assisted suicide by morphine. Also, in "Martin Eden", the principal protagonist, who shares certain characteristics with London, drowns himself.
Accusations of plagiarism.
London was vulnerable to accusations of plagiarism, both because he was such a conspicuous, prolific, and successful writer and because of his methods of working. He wrote in a letter to Elwyn Hoffman, "expression, you see—with me—is far easier than invention." He purchased plots and novels from the young Sinclair Lewis and used incidents from newspaper clippings as writing material.
In July 1901, two pieces of fiction appeared within the same month: London's "Moon-Face", in the "San Francisco Argonaut," and Frank Norris' "The Passing of Cock-eye Blacklock", in "Century Magazine". Newspapers showed the similarities between the stories, which London said were "quite different in manner of treatment, [but] patently the same in foundation and motive." London explained both writers based their stories on the same newspaper account. A year later, it was discovered that Charles Forrest McLean had published a fictional story also based on the same incident.
Egerton Ryerson Young claimed "The Call of the Wild" (1903) was taken from Young's book "My Dogs in the Northland" (1902). London acknowledged using it as a source and claimed to have written a letter to Young thanking him.
In 1906, the "New York World" published "deadly parallel" columns showing eighteen passages from London's short story "Love of Life" side by side with similar passages from a nonfiction article by Augustus Biddle and J. K Macdonald, titled "Lost in the Land of the Midnight Sun". London noted the "World" did not accuse him of "plagiarism", but only of "identity of time and situation", to which he defiantly "pled guilty".
The most serious charge of plagiarism was based on London's "The Bishop's Vision", Chapter 7 of his novel "The Iron Heel" (1908). The chapter is nearly identical to an ironic essay that Frank Harris published in 1901, titled "The Bishop of London and Public Morality". Harris was incensed and suggested he should receive 1/60th of the royalties from "The Iron Heel," the disputed material constituting about that fraction of the whole novel. London insisted he had clipped a reprint of the article, which had appeared in an American newspaper, and believed it to be a genuine speech delivered by the Bishop of London.
Views.
Atheism.
London was an atheist. He is quoted as saying,“I believe that when I am dead, I am dead. I believe that with my death I am just as much obliterated as the last mosquito you and I squashed.” 
Socialism.
London wrote from a socialist viewpoint, which is evident in his novel "The Iron Heel". Neither a theorist nor an intellectual socialist, London's socialism grew out of his life experience. As London explained in his essay, "How I Became a Socialist", his views were influenced by his experience with people at the bottom of the social pit. His optimism and individualism faded, and he vowed never to do more hard physical work than necessary. He wrote that his individualism was hammered out of him, and he was politically reborn. He often closed his letters "Yours for the Revolution."
London joined the Socialist Labor Party in April 1896. In the same year, the "San Francisco Chronicle" published a story about the twenty-year-old London giving nightly speeches in Oakland's City Hall Park, an activity he was arrested for a year later. In 1901, he left the Socialist Labor Party and joined the new Socialist Party of America. He ran unsuccessfully as the high-profile Socialist nominee for mayor of Oakland in 1901 (receiving 245 votes) and 1905 (improving to 981 votes), toured the country lecturing on socialism in 1906, and published two collections of essays about socialism: "The War of the Classes" (1905) and "Revolution, and other Essays" (1906).
Stasz notes that "London regarded the Wobblies as a welcome addition to the Socialist cause, although he never joined them in going so far as to recommend sabotage." Stasz mentions a personal meeting between London and Big Bill Haywood in 1912.
In his late (1913) book "The Cruise of the Snark", London writes, about appeals to him for membership of the Snark's crew from office workers and other "toilers" who longed for escape from the cities, and of being cheated by workmen.
In his Glen Ellen ranch years, London felt some ambivalence toward socialism and complained about the "inefficient Italian labourers" in his employ. In 1916, he resigned from the Glen Ellen chapter of the Socialist Party, but stated emphatically he did so "because of its lack of fire and fight, and its loss of emphasis on the class struggle." In an unflattering portrait of London's ranch days, California cultural historian Kevin Starr refers to this period as "post-socialist" and says "... by 1911 ... London was more bored by the class struggle than he cared to admit." Starr maintains London's socialism
always had a streak of elitism in it, and a good deal of pose. He liked to play working class intellectual when it suited his purpose. Invited to a prominent Piedmont house, he featured a flannel shirt, but, as someone there remarked, London's badge of solidarity with the working class "looked as if it had been specially laundered for the occasion. [Mark Twain said] "It would serve this man London right to have the working class get control of things. He would have to call out the militia to collect his royalties."
George Orwell, however, identified a fascist strain in London's outlook:
Racial views.
London shared common Californian concerns about Asian immigration and "the yellow peril", which he used as the title of a 1904 essay. This theme was also the subject of a story he wrote in 1910 called "The Unparalleled Invasion". Presented as an historical essay set in the future, the story narrates events between 1976 and 1987, in which China, with an ever-increasing population, is taking over and colonizing its neighbors with the intention of taking over the entire Earth. The western nations respond with biological warfare and bombard China with dozens of the most infectious diseases. On his fears about China, he admits, "it must be taken into consideration that the above postulate is itself a product of Western race-egotism, urged by our belief in our own righteousness and fostered by a faith in ourselves which may be as erroneous as are most fond race fancies."
Many of London's short stories are notable for their empathetic portrayal of Mexican ("The Mexican"), Asian ("The Chinago"), and Hawaiian ("Koolau the Leper") characters. London's war correspondence from the Russo-Japanese War, as well as his unfinished novel "Cherry", show he admired much about Japanese customs and capabilities, which has led to big popularity of London (considered a Japanophile) among the Japanese, who consider themselves rather positively portrayed by him.
In London's 1902 novel "Daughter of the Snows", the character Frona Welse has a speech about Teutonic virtues in contrast to the characteristics of other "races". The scholar Andrew Furer, in a long essay exploring the complexity of London's views, says there is no doubt that Frona Welse is acting as a mouthpiece for London in this passage.
In "Koolau the Leper", London describes Koolau, who is a Hawaiian leper—and thus a very different sort of "superman" than Martin Eden—and who fights off an entire cavalry troop to elude capture, as "indomitable spiritually—a ... magnificent rebel". This character is based on Hawaiian leper Kaluaikoolau who revolted and resisted capture from forces of the Provisional Government of Hawaii in the Kalalau Valley in 1893.
An amateur boxer and avid boxing fan, London reported on the 1910 Johnson-Jeffries fight, in which the black boxer Jack Johnson vanquished Jim Jeffries, the "Great White Hope". In 1908, according to Furer, London praised Johnson highly, contrasting the black boxer's coolness and intellectual style, with the apelike appearance and fighting style of his white opponent, Tommy Burns: "what . . . [won] on Saturday was bigness, coolness, quickness, cleverness, and vast physical superiority... Because a white man wishes a white man to win, this should not prevent him from giving absolute credit to the best man, even when that best man was black. All hail to Johnson." Johnson was "superb. He was impregnable . . . as inaccessible as Mont Blanc."
Those who defend London against charges of racism cite the letter he wrote to the "Japanese-American Commercial Weekly" in 1913:
In reply to yours of August 16, 1913. First of all, I should say by stopping the stupid newspaper from always fomenting race prejudice. This of course, being impossible, I would say, next, by educating the people of Japan so that they will be too intelligently tolerant to respond to any call to race prejudice. And, finally, by realizing, in industry and government, of socialism—which last word is merely a word that stands for the actual application of in the affairs of men of the theory of the Brotherhood of Man.
In the meantime the nations and races are only unruly boys who have not yet grown to the stature of men. So we must expect them to do unruly and boisterous things at times. And, just as boys grow up, so the races of mankind will grow up and laugh when they look back upon their childish quarrels.
In Yukon in 1996, after the City of Whitehorse renamed two streets to honor London and Robert W. Service, protests over London's alleged racism forced the city to change the name of "Jack London Boulevard" back to "Two-mile Hill".
Works.
Short stories.
Western writer and historian Dale L. Walker writes:
London's true métier was the short story ... London's true genius lay in the short form, 7,500 words and under, where the flood of images in his teeming brain and the innate power of his narrative gift were at once constrained and freed. His stories that run longer than the magic 7,500 generally—but certainly not always—could have benefited from self-editing.
London's "strength of utterance" is at its height in his stories, and they are painstakingly well-constructed. "To Build a Fire" is the best known of all his stories. Set in the harsh Klondike, it recounts the haphazard trek of a new arrival who has ignored an old-timer's warning about the risks of traveling alone. Falling through the ice into a creek in seventy-five-below weather, the unnamed man is keenly aware that survival depends on his untested skills at quickly building a fire to dry his clothes and warm his extremities. After publishing a tame version of this story—with a sunny outcome—in "The Youth's Companion" in 1902, London offered a second, more severe take on the man's predicament in "The Century Magazine" in 1908. Reading both provides an illustration of London's growth and maturation as a writer. As Labor (1994) observes: "To compare the two versions is itself an instructive lesson in what distinguished a great work of literary art from a good children's story."
Other stories from the Klondike period include: "All Gold Canyon", about a battle between a gold prospector and a claim jumper; "The Law of Life", about an aging American Indian man abandoned by his tribe and left to die; "Love of Life", about a trek by a prospector across the Canadian tundra; "To the Man on Trail," which tells the story of a prospector fleeing the Mounted Police in a sled race, and raises the question of the contrast between written law and morality; and "An Odyssey of the North," which raises questions of conditional morality, and paints a sympathetic portrait of a man of mixed White and Aleut ancestry.
London was a boxing fan and an avid amateur boxer. "A Piece of Steak" is a tale about a match between older and younger boxers. It contrasts the differing experiences of youth and age but also raises the social question of the treatment of aging workers. "The Mexican" combines boxing with a social theme, as a young Mexican endures an unfair fight and ethnic prejudice in order to earn money with which to aid the revolution.
Several of London's stories would today be classified as science fiction. "The Unparalleled Invasion" describes germ warfare against China; "Goliath" about an irresistible energy weapon; "The Shadow and the Flash" is a tale about two brothers who take different routes to achieving invisibility; "A Relic of the Pliocene" is a tall tale about an encounter of a modern-day man with a mammoth. "The Red One" is a late story from a period when London was intrigued by the theories of the psychiatrist and writer Jung. It tells of an island tribe held in thrall by an extraterrestrial object.
Some nineteen original collections of short stories were published during London's brief life or shortly after his death. There have been several posthumous anthologies drawn from this pool of stories. Many of these stories were located in the Klondike and the Pacific. A collection of "Jack London's San Francisco Stories" was published in October 2010 by Sydney Samizdat Press.
Novels.
London's most famous novels are "The Call of the Wild", "White Fang", "The Sea-Wolf", "The Iron Heel", and "Martin Eden".
In a letter dated Dec 27, 1901, London's Macmillan publisher George Platt Brett, Sr. said "he believed Jack's fiction represented 'the very best kind of work' done in America."
Critic Maxwell Geismar called "The Call of the Wild" "a beautiful prose poem"; editor Franklin Walker said that it "belongs on a shelf with "Walden" and "Huckleberry Finn""; and novelist E.L. Doctorow called it "a mordant parable ... his masterpiece."
The historian Dale L. Walker commented:
Jack London was an uncomfortable novelist, that form too long for his natural impatience and the quickness of his mind. His novels, even the best of them, are hugely flawed.
Some critics have said that his novels are episodic and resemble linked short stories. Dale L. Walker writes:
The Star Rover, that magnificent experiment, is actually a series of short stories connected by a unifying device ... Smoke Bellew is a series of stories bound together in a novel-like form by their reappearing protagonist, Kit Bellew; and "John Barleycorn" ... is a synoptic series of short episodes.
Ambrose Bierce said of "The Sea-Wolf" that "the great thing—and it is among the greatest of things—is that tremendous creation, Wolf Larsen ... the hewing out and setting up of such a figure is enough for a man to do in one lifetime." However, he noted, "The love element, with its absurd suppressions, and impossible proprieties, is awful."
"The Iron Heel" is interesting as an example of a dystopian novel that anticipates and influenced George Orwell's "Nineteen Eighty-Four". London's socialist politics are explicitly on display here. "The Iron Heel" meets the contemporary definition of soft science fiction.
Apocrypha.
Jack London Credo.
London's literary executor, Irving Shepard, quoted a "Jack London Credo" in an introduction to a 1956 collection of London stories:
<poem style="margin-left: 3em;">
I would rather be ashes than dust!
I would rather that my spark should burn out in a brilliant blaze than it should be stifled by dry-rot.
I would rather be a superb meteor, every atom of me in magnificent glow, than a sleepy and permanent planet.
The function of man is to live, not to exist.
I shall not waste my days in trying to prolong them.
I shall use my time.
</poem>
The biographer Stasz notes that the passage "has many marks of London's style" but the only line that could be safely attributed to London was the first. The words Shepard quoted were from a story in the "San Francisco Bulletin", December 2, 1916 by journalist Ernest J. Hopkins, who visited the ranch just weeks before London's death. Stasz notes "Even more so than today journalists' quotes were unreliable or even sheer inventions" and says no direct source in London's writings has been found. However, at least one line, according to Stasz, is authentic, being referenced by London, and written in his own hand, in the autograph book of Australian suffragette Vida Goldstein:
<poem style="margin-left: 3em;">
Dear Miss Goldstein:–
Seven years ago I wrote you that I'd rather be ashes than dust. I still subscribe to that sentiment.
Sincerely yours,
Jack London
Jan. 13, 1909
</poem>
Furthermore, in his short story "By The Turtles of Tasman", a character, defending her ne'er-do-well grasshopperish father to her antlike uncle, says: "... my father has been a king. He has lived ... Have you lived merely to live? Are you afraid to die? I'd rather sing one wild song and burst my heart with it, than live a thousand years watching my digestion and being afraid of the wet. When you are dust, my father will be ashes."
"The Scab".
A short diatribe on "The Scab" is often quoted within the U.S. labor movement and frequently attributed to London. It opens:
After God had finished the rattlesnake, the toad, and the vampire, he had some awful substance left with which he made a scab. A scab is a two-legged animal with a corkscrew soul, a water brain, a combination backbone of jelly and glue. Where others have hearts, he carries a tumor of rotten principles. When a scab comes down the street, men turn their backs and Angels weep in Heaven, and the Devil shuts the gates of hell to keep him out..."
In 1913 and 1914, a number of newspapers printed the first three sentences with varying terms used instead of "scab", such as
"knocker",
"stool pigeon"
or "scandal monger".
This passage as given above was the subject of a 1974 Supreme Court case, "Letter Carriers v. Austin" 418 U.S. 264 (1974), in which Justice Thurgood Marshall referred to it as "a well-known piece of trade union literature, generally attributed to author Jack London". A union newsletter had published a "list of scabs," which was granted to be factual and therefore not libelous, but then went on to quote the passage as the "definition of a scab". The case turned on the question of whether the "definition" was defamatory. The court ruled that "Jack London's... 'definition of a scab' is merely rhetorical hyperbole, a lusty and imaginative expression of the contempt felt by union members towards those who refuse to join", and as such was not libelous and was protected under the First Amendment.
Despite being frequently attributed to London, the passage does not appear at all in the extensive collection of his writings at Sonoma State University's website. However, in his book "The War of the Classes" he published a 1903 speech entitled "The Scab", which gave a much more balanced view of the topic:
To strike at a man's food and shelter is to strike at his life; and in a society organized on a tooth-and-nail basis, such an act, performed though it may be under the guise of generosity, is none the less menacing and terrible. It is for this reason that a laborer is so fiercely hostile to another laborer who offers to work for less pay or longer hours. To hold his place, (which is to live), he must offset this offer by another equally liberal, which is equivalent to giving away somewhat from the food and shelter he enjoys.
<br>[...]<br>
When a striker kills with a brick the man who has taken his place, he has no sense of wrong-doing. In the deepest holds of his being, though he does not reason the impulse, he has an ethical sanction. He feels dimly that he has justification, just as the home-defending Boer felt, though more sharply, with each bullet he fired at the invading English. Behind every brick thrown by a striker is the selfish will "to live" of himself, and the slightly altruistic will "to live" of his family. The family group came into the world before the State group, and society, being still on the primitive basis of tooth and nail, the will "to live" of the State is not so compelling to the striker as is the will "to live" of his family and himself.
<br>[...]<br>
The laborer who gives more time or strength or skill for the same wage than another, or equal time or strength or skill for a less wage, is a scab. The generousness on his part is hurtful to his fellow-laborers, for it compels them to an equal generousness which is not to their liking, and which gives them less of food and shelter. But a word may be said for the scab. Just as his act makes his rivals compulsorily generous, so do they, by fortune of birth and training, make compulsory his act of generousness.
<br>[...]<br>
Nobody desires to scab, to give most for least. The ambition of every individual is quite the opposite, to give least for most; and, as a result, living in a tooth-and-nail society, battle royal is waged by the ambitious individuals. But in its most salient aspect, that of the struggle over the division of the joint product, it is no longer a battle between individuals, but between groups of individuals. Capital and labor apply themselves to raw material, make something useful out of it, add to its value, and then proceed to quarrel over the division of the added value. Neither cares to give most for least. Each is intent on giving less than the other and on receiving more.
"Might Is Right".
Anton LaVey's Church of Satan claims that "Ragnar Redbeard", pseudonymous author of the 1896 book "Might Is Right", was London. No London biographers mention any such possibility. Rodger Jacobs published an essay ridiculing this theory, arguing that in 1896 London was unfamiliar with philosophers heavily cited by Redbeard, such as Nietzsche, and had not even begun to develop his mature literary style.
Bibliography.
Source unless otherwise specified: 
Footnotes.
Note
Citations
References.
The Jack London Online Collection

</doc>
<doc id="42980" url="http://en.wikipedia.org/wiki?curid=42980" title="Suzuki (disambiguation)">
Suzuki (disambiguation)

Suzuki may refer to:

</doc>
<doc id="42986" url="http://en.wikipedia.org/wiki?curid=42986" title="Alternating current">
Alternating current

In alternating current (AC), the flow of electric charge periodically reverses direction, whereas in direct current (DC, also dc), the flow of electric charge is only in one direction. The abbreviations "AC" and "DC" are often used to mean simply "alternating" and "direct", as when they modify "current" or "voltage".
AC is the form in which electric power is delivered to businesses and residences. The usual waveform of an AC power circuit is a sine wave. In certain applications, different waveforms are used, such as triangular or square waves. Audio and radio signals carried on electrical wires are also examples of alternating current. In these applications, an important goal is often the recovery of information encoded (or modulated) onto the AC signal.
Transmission, distribution, and domestic power supply.
AC voltage may be increased or decreased with a transformer. Use of a higher voltage leads to significantly more efficient transmission of power. The power losses (formula_1) in a conductor are a product of the square of the current (I) and the resistance (R) of the conductor, described by the formula
This means that when transmitting a fixed power on a given wire, if the current is doubled, the power loss will be four times greater.
The power transmitted is equal to the product of the current and the voltage (assuming no phase difference); that is,
Thus, the same amount of power can be transmitted with a lower current by increasing the voltage. It is therefore advantageous when transmitting large amounts of power to distribute the power with high voltages (often hundreds of kilovolts).
However, high voltages also have disadvantages, the main one being the increased insulation required, and generally increased difficulty in their safe handling. In a power plant, power is generated at a convenient voltage for the design of a generator, and then stepped up to a high voltage for transmission. Near the loads, the transmission voltage is stepped down to the voltages used by equipment. Consumer voltages vary depending on the country and size of load, but generally motors and lighting are built to use up to a few hundred volts between phases.
The utilization voltage delivered to equipment such as lighting and motor loads is standardized, with an allowable range of voltage over which equipment is expected to operate. Standard power utilization voltages and percentage tolerance vary in the different mains power systems found in the world.
Modern high-voltage direct-current (HVDC) electric power transmission systems contrast with the more common alternating-current systems as a means for the efficient bulk transmission of electrical power over long distances. HVDC systems, however, tend to be more expensive and less efficient over shorter distances than transformers. Transmission with high voltage direct current was not feasible when Edison, Westinghouse and Tesla were designing their power systems, since there was then no way to economically convert AC power to DC and back again at the necessary voltages.
Three-phase electrical generation is very common. The simplest case is three separate coils in the generator stator that are physically offset by an angle of 120° to each other. Three current waveforms are produced that are equal in magnitude and 120° out of phase to each other. If coils are added opposite to these (60° spacing), they generate the same phases with reverse polarity and so can be simply wired together.
In practice, higher "pole orders" are commonly used. For example, a 12-pole machine would have 36 coils (10° spacing). The advantage is that lower speeds can be used. For example, a 2-pole machine running at 3600 rpm and a 12-pole machine running at 600 rpm produce the same frequency. This is much more practical for larger machines.
If the load on a three-phase system is balanced equally among the phases, no current flows through the neutral point. Even in the worst-case unbalanced (linear) load, the neutral current will not exceed the highest of the phase currents. Non-linear loads (e.g., computers) may require an oversized neutral bus and neutral conductor in the upstream distribution panel to handle harmonics. Harmonics can cause neutral conductor current levels to exceed that of one or all phase conductors.
For three-phase at utilization voltages a four-wire system is often used. When stepping down three-phase, a transformer with a Delta (3-wire) primary and a Star (4-wire, center-earthed) secondary is often used so there is no need for a neutral on the supply side.
For smaller customers (just how small varies by country and age of the installation) only a single phase and the neutral or two phases and the neutral are taken to the property. For larger installations all three phases and the neutral are taken to the main distribution panel. From the three-phase main panel, both single and three-phase circuits may lead off.
Three-wire single-phase systems, with a single center-tapped transformer giving two live conductors, is a common distribution scheme for residential and small commercial buildings in North America. This arrangement is sometimes incorrectly referred to as "two phase". A similar method is used for a different reason on construction sites in the UK. Small power tools and lighting are supposed to be supplied by a local center-tapped transformer with a voltage of 55 V between each power conductor and earth. This significantly reduces the risk of electric shock in the event that one of the live conductors becomes exposed through an equipment fault whilst still allowing a reasonable voltage of 110 V between the two conductors for running the tools.
A third wire, called the bond (or earth) wire, is often connected between non-current-carrying metal enclosures and earth ground. This conductor provides protection from electric shock due to accidental contact of circuit conductors with the metal chassis of portable appliances and tools. Bonding all non-current-carrying metal parts into one complete system ensures there is always a low electrical impedance path to ground sufficient to carry any fault current for as long as it takes for the system to clear the fault. This low impedance path allows the maximum amount of fault current, causing the overcurrent protection device (breakers, fuses) to trip or burn out as quickly as possible, bringing the electrical system to a safe state. All bond wires are bonded to ground at the main service panel, as is the Neutral/Identified conductor if present.
AC power supply frequencies.
The frequency of the electrical system varies by country and sometimes within a country; most electric power is generated at either 50 or 60 hertz. Some countries have a mixture of 50 Hz and 60 Hz supplies, notably electricity power transmission in Japan.
A low frequency eases the design of electric motors, particularly for hoisting, crushing and rolling applications, and commutator-type traction motors for applications such as railways. However, low frequency also causes noticeable flicker in arc lamps and incandescent light bulbs. The use of lower frequencies also provided the advantage of lower impedance losses, which are proportional to frequency. The original Niagara Falls generators were built to produce 25 Hz power, as a compromise between low frequency for traction and heavy induction motors, while still allowing incandescent lighting to operate (although with noticeable flicker). Most of the 25 Hz residential and commercial customers for Niagara Falls power were converted to 60 Hz by the late 1950s, although some 25 Hz industrial customers still existed as of the start of the 21st century. 16.7 Hz power (formerly 16 2/3 Hz) is still used in some European rail systems, such as in Austria, Germany, Norway, Sweden and Switzerland.
Off-shore, military, textile industry, marine, aircraft, and spacecraft applications sometimes use 400 Hz, for benefits of reduced weight of apparatus or higher motor speeds.
Computer mainframe systems are often powered by 415 Hz, using customer-supplied 35 or 70 KVA motor-generator sets. Smaller mainframes may have an internal 415 Hz M-G set. In any case, the input to the M-G set is the local customary voltage and frequency, variously 200 (Japan), 208, 240 (North America), 380, 400 or 415 (Europe) volts, and variously 50 or 60 Hz.
Effects at high frequencies.
A direct current flows uniformly throughout the cross-section of a uniform wire. An alternating current of any frequency is forced away from the wire's center, toward its outer surface. This is because the acceleration of an electric charge in an alternating current produces waves of electromagnetic radiation that cancel the propagation of electricity toward the center of materials with high conductivity. This phenomenon is called skin effect.
At very high frequencies the current no longer flows "in" the wire, but effectively flows "on" the surface of the wire, within a thickness of a few skin depths. The skin depth is the thickness at which the current density is reduced by 63%. Even at relatively low frequencies used for power transmission (50–60 Hz), non-uniform distribution of current still occurs in sufficiently thick conductors. For example, the skin depth of a copper conductor is approximately 8.57 mm at 60 Hz, so high current conductors are usually hollow to reduce their mass and cost.
Since the current tends to flow in the periphery of conductors, the effective cross-section of the conductor is reduced. This increases the effective AC resistance of the conductor, since resistance is inversely proportional to the cross-sectional area. The AC resistance often is many times higher than the DC resistance, causing a much higher energy loss due to ohmic heating (also called I2R loss).
Techniques for reducing AC resistance.
For low to medium frequencies, conductors can be divided into stranded wires, each insulated from one other, and the relative positions of individual strands specially arranged within the conductor bundle. Wire constructed using this technique is called Litz wire. This measure helps to partially mitigate skin effect by forcing more equal current throughout the total cross section of the stranded conductors. Litz wire is used for making high-Q inductors, reducing losses in flexible conductors carrying very high currents at lower frequencies, and in the windings of devices carrying higher radio frequency current (up to hundreds of kilohertz), such as switch-mode power supplies and radio frequency transformers.
Techniques for reducing radiation loss.
As written above, an alternating current is made of electric charge under periodic acceleration, which causes radiation of electromagnetic waves. Energy that is radiated is lost. Depending on the frequency, different techniques are used to minimize the loss due to radiation.
Twisted pairs.
At frequencies up to about 1 GHz, pairs of wires are twisted together in a cable, forming a twisted pair. This reduces losses from electromagnetic radiation and inductive coupling. A twisted pair must be used with a balanced signalling system, so that the two wires carry equal but opposite currents. Each wire in a twisted pair radiates a signal, but it is effectively cancelled by radiation from the other wire, resulting in almost no radiation loss.
Coaxial cables.
Coaxial cables are commonly used at audio frequencies and above for convenience. A coaxial cable has a conductive wire inside a conductive tube, separated by a dielectric layer. The current flowing on the inner conductor is equal and opposite to the current flowing on the inner surface of the tube. The electromagnetic field is thus completely contained within the tube, and (ideally) no energy is lost to radiation or coupling outside the tube. Coaxial cables have acceptably small losses for frequencies up to about 5 GHz. For microwave frequencies greater than 5 GHz, the losses (due mainly to the electrical resistance of the central conductor) become too large, making waveguides a more efficient medium for transmitting energy. Coaxial cables with an air rather than solid dielectric are preferred as they transmit power with lower loss.
Waveguides.
Waveguides are similar to coax cables, as both consist of tubes, with the biggest difference being that the waveguide has no inner conductor. Waveguides can have any arbitrary cross section, but rectangular cross sections are the most common. Because waveguides do not have an inner conductor to carry a return current, waveguides cannot deliver energy by means of an electric current, but rather by means of a "guided" electromagnetic field. Although surface currents do flow on the inner walls of the waveguides, those surface currents do not carry power. Power is carried by the guided electromagnetic fields. The surface currents are set up by the guided electromagnetic fields and have the effect of keeping the fields inside the waveguide and preventing leakage of the fields to the space outside the waveguide.
Waveguides have dimensions comparable to the wavelength of the alternating current to be transmitted, so they are only feasible at microwave frequencies. In addition to this mechanical feasibility, electrical resistance of the non-ideal metals forming the walls of the waveguide cause dissipation of power (surface currents flowing on lossy conductors dissipate power). At higher frequencies, the power lost to this dissipation becomes unacceptably large.
Fiber optics.
At frequencies greater than 200 GHz, waveguide dimensions become impractically small, and the ohmic losses in the waveguide walls become large. Instead, fiber optics, which are a form of dielectric waveguides, can be used. For such frequencies, the concepts of voltages and currents are no longer used.
Mathematics of AC voltages.
Alternating currents are accompanied (or caused) by alternating voltages. An AC voltage "v" can be described mathematically as a function of time by the following equation:
where
The peak-to-peak value of an AC voltage is defined as the difference between its positive peak and its negative peak. Since the maximum value of formula_10 is +1 and the minimum value is −1, an AC voltage swings between formula_11 and formula_12. The peak-to-peak voltage, usually written as formula_13 or formula_14, is therefore formula_15.
Power.
The relationship between voltage and the power delivered is
Rather than using instantaneous power, formula_18, it is more practical to use a time averaged power (where the averaging is performed over any integer number of cycles). Therefore, AC voltage is often expressed as a root mean square (RMS) value, written as formula_19, because
By the following trigonometric identity, the power oscillation is double frequency of the voltage.
Root mean square.
For a sinusoidal voltage:
The factor formula_26 is called the crest factor, which varies for different waveforms.
Example.
To illustrate these concepts, consider a 230 V AC mains supply used in many countries around the world. It is so called because its root mean square value is 230 V. This means that the time-averaged power delivered is equivalent to the power delivered by a DC voltage of 230 V. To determine the peak voltage (amplitude), we can rearrange the above equation to:
For 230 V AC, the peak voltage formula_33 is therefore formula_34, which is about 325 V.
History.
The first alternator to produce alternating current was a dynamo electric generator based on Michael Faraday's principles constructed by the French instrument maker Hippolyte Pixii in 1832. Pixii later added a commutator to his device to produce the (then) more commonly used direct current. The earliest recorded practical application of alternating current is by Guillaume Duchenne, inventor and developer of electrotherapy. In 1855, he announced that AC was superior to direct current for electrotherapeutic triggering of muscle contractions.
Alternating current technology had first developed in Europe due to the work of Guillaume Duchenne (1850s), The Hungarian Ganz Works (1870s), Sebastian Ziani de Ferranti (1880s), Lucien Gaulard, and Galileo Ferraris.
In 1876, Russian engineer Pavel Yablochkov invented a lighting system based on a set of induction coils where the primary windings were connected to a source of AC. The secondary windings could be connected to several 'electric candles' (arc lamps) of his own design. The coils Yablochkov employed functioned essentially as transformers.
In 1878, the Ganz factory, Budapest, Hungary, began manufacturing equipment for electric lighting and, by 1883, had installed over fifty systems in Austria-Hungary. Their AC systems used arc and incandescent lamps, generators, and other equipment.
A power transformer developed by Lucien Gaulard and John Dixon Gibbs was demonstrated in London in 1881, and attracted the interest of Westinghouse. They also exhibited the invention in Turin in 1884.
DC distribution systems.
During the initial years of electricity distribution, Edison's direct current was the standard for the United States, and Edison did not want to lose all his patent royalties.
Direct current worked well with incandescent lamps, which were the principal load of the day, and with motors. Direct-current systems could be directly used with storage batteries, providing valuable load-leveling and backup power during interruptions of generator operation. Direct-current generators could be easily paralleled, allowing economical operation by using smaller machines during periods of light load and improving reliability. At the introduction of Edison's system, no practical AC motor was available. Edison had invented a meter to allow customers to be billed for energy proportional to consumption, but this meter worked only with direct current.
The principal drawback of direct-current distribution was that customer loads, distribution and generation were all at the same voltage. Generally, it was uneconomical to use a high voltage for transmission and reduce it for customer uses. Even with the Edison 3-wire system (placing two 110-volt customer loads in series on a 220-volt supply), the high cost of conductors required generation to be close to customer loads, otherwise losses made the system uneconomical to operate.
Transformers.
Alternating current systems can use transformers to change voltage from low to high level and back, allowing generation and consumption at low voltages but transmission, possibly over great distances, at high voltage, with savings in the cost of conductors and energy losses.
A bipolar open-core power transformer developed by Lucien Gaulard and John Dixon Gibbs was demonstrated in London in 1881, and attracted the interest of Westinghouse. They also exhibited the invention in Turin in 1884. However these early induction coils with open magnetic circuits are inefficient at transferring power to loads. Until about 1880, the paradigm for AC power transmission from a high voltage supply to a low voltage load was a series circuit. Open-core transformers with a ratio near 1:1 were connected with their primaries in series to allow use of a high voltage for transmission while presenting a low voltage to the lamps. The inherent flaw in this method was that turning off a single lamp (or other electric device) affected the voltage supplied to all others on the same circuit. Many adjustable transformer designs were introduced to compensate for this problematic characteristic of the series circuit, including those employing methods of adjusting the core or bypassing the magnetic flux around part of a coil.<br>The direct current systems did not have these drawbacks, giving it significant advantages over early AC systems.
Pioneers.
In the autumn of 1884, Károly Zipernowsky, Ottó Bláthy and Miksa Déri (ZBD), three engineers associated with the Ganz factory, had determined that open-core devices were impracticable, as they were incapable of reliably regulating voltage. In their joint 1885 patent applications for novel transformers (later called ZBD transformers), they described two designs with closed magnetic circuits where copper windings were either a) wound around iron wire ring core or b) surrounded by iron wire core. In both designs, the magnetic flux linking the primary and secondary windings traveled almost entirely within the confines of the iron core, with no intentional path through air (see Toroidal cores below). The new transformers were 3.4 times more efficient than the open-core bipolar devices of Gaulard and Gibbs.
The Ganz factory in 1884 shipped the world's first five high-efficiency AC transformers. This first unit had been manufactured to the following specifications: 1,400 W, 40 Hz, 120:72 V, 11.6:19.4 A, ratio 1.67:1, one-phase, shell form.
The ZBD patents included two other major interrelated innovations: one concerning the use of parallel connected, instead of series connected, utilization loads, the other concerning the ability to have high turns ratio transformers such that the supply network voltage could be much higher (initially 1,400 to 2,000 V) than the voltage of utilization loads (100 V initially preferred). When employed in parallel connected electric distribution systems, closed-core transformers finally made it technically and economically feasible to provide electric power for lighting in homes, businesses and public spaces.
The other essential milestone was the introduction of 'voltage source, voltage intensive' (VSVI) systems' by the invention of constant voltage generators in 1885. Ottó Bláthy also invented the first AC electricity meter.
The AC power systems was developed and adopted rapidly after 1886 due to its ability to distribute electricity efficiently over long distances, overcoming the limitations of the direct current system. In 1886, the ZBD engineers designed, and the Ganz factory supplied electrical equipment for, the world's first power station that used AC generators to power a parallel connected common electrical network, the steam-powered Rome-Cerchi power plant. The reliability of the AC technology received impetus after the Ganz Works electrified a large European metropolis: Rome in 1886.
In the UK Sebastian de Ferranti, who had been developing AC generators and transformers in London since 1882, redesigned the AC system at the Grosvenor Gallery power station in 1886 for the London Electric Supply Corporation (LESCo) including alternators of his own design and transformer designs similar to Gaulard and Gibbs. In 1890 he designed their power station at Deptford and converted the Grosvenor Gallery station across the Thames into an electrical substation, showing the way to integrate older plants into a universal AC supply system.
In the US William Stanley, Jr. designed one of the first practical devices to transfer AC power efficiently between isolated circuits. Using pairs of coils wound on a common iron core, his design, called an induction coil, was an early (1885) transformer. Stanley also worked on engineering and adapting European designs such as the Gaulard and Gibbs transformer for US entrepreneur George Westinghouse who started building AC systems in 1886. The spread of Westinghouse and other AC systems triggered a push back in late 1887 by Thomas Edison (a proponent of direct current) who attempted to discredit alternating current as too dangerous in a public campaign called the "War of Currents".
In 1888 alternating current systems gained further viability with introduction of a functional AC motor, something these systems had lacked up till then. The design, an induction motor, was independently invented by Galileo Ferraris and Nikola Tesla (with Tesla's design being licensed by Westinghouse in the US). This design was further developed into the modern practical three-phase form by Mikhail Dolivo-Dobrovolsky and Charles Eugene Lancelot Brown.
The Ames Hydroelectric Generating Plant (spring of 1891) and the original Niagara Falls Adams Power Plant (August 25, 1895) were among the first hydroelectric AC-power plants. The first commercial power plant in the United States using three-phase alternating current was the hydroelectric Mill Creek No. 1 Hydroelectric Plant near Redlands, California, in 1893 designed by Almirian Decker. Decker's design incorporated 10,000-volt three-phase transmission and established the standards for the complete system of generation, transmission and motors used today.
The Jaruga Hydroelectric Power Plant in Croatia was set in operation on 28 August 1895. The two generators (42 Hz, 550 kW each) and the transformers were produced and installed by the Hungarian company Ganz. The transmission line from the power plant to the City of Šibenik was 11.5 km long on wooden towers, and the municipal distribution grid 3000 V/110 V included six transforming stations.
Alternating current circuit theory developed rapidly in the latter part of the 19th and early 20th century. Notable contributors to the theoretical basis of alternating current calculations include Charles Steinmetz, Oliver Heaviside, and many others. Calculations in unbalanced three-phase systems were simplified by the symmetrical components methods discussed by Charles Legeyt Fortescue in 1918. 
Further reading.
</dl>

</doc>
<doc id="42990" url="http://en.wikipedia.org/wiki?curid=42990" title="Convoy">
Convoy

A convoy is a group of vehicles, typically motor vehicles or ships, traveling together for mutual support and protection. Often, a convoy is organized with armed defensive support. It may also be used in a non-military sense, for example when driving through remote areas. Arriving at the scene of a major emergency with a well-ordered unit and intact command structure can be another motivation.
Naval convoys.
Age of sail.
Naval convoys have been in use for centuries, with examples of merchant ships traveling under naval protection dating to the 12th century. The use of organized naval convoys dates from when ships began to be separated into specialist classes and national navies were established.
By the French Revolutionary Wars of the late 18th century, effective naval convoy tactics had been developed to ward off pirates and privateers. Some convoys contained several hundred merchant ships. The most enduring system of convoys were the Spanish treasure fleets, that sailed from the 1520s until 1790.
When merchant ships sailed independently, a privateer could cruise a shipping lane and capture ships as they passed. Ships sailing in convoy presented a much smaller target: a convoy was as hard to find as a single ship. Even if the privateer found a convoy and the wind was favourable for an attack, it could hope to capture only a handful of ships before the rest managed to escape, and a small escort of warships could easily thwart it. As a result of the convoy system's effectiveness, wartime insurance premiums were consistently lower for ships that sailed in convoys.
Many naval battles in the Age of Sail were fought around convoys, including:
By the end of the Napoleonic Wars the Royal Navy had in place a sophisticated convoy system to protect merchant ships. Losses of ships traveling out of convoy however were so high that no merchant ship was allowed to sail unescorted.
World War I.
In the early 20th century, the dreadnought changed the balance of power in convoy battles. Steaming faster than merchant ships and firing at long ranges, a single battleship could destroy many ships in a convoy before the others could scatter over the horizon. To protect a convoy against a capital ship required providing it with an escort of another capital ship, at very high opportunity cost (i.e. potentially tying down multiple capital ships to defend different convoys against one opponent ship).
Battleships were the main reason that the British Admiralty did not adopt convoy tactics at the start of the first Battle of the Atlantic in World War I. But the German capital ships had been bottled up in the North Sea, and the main threat to shipping came from U-boats. From a tactical point of view, World War I–era submarines were similar to privateers in the age of sail: only a little faster than the merchant ships they were attacking, and capable of sinking only a small number of vessels in a convoy because of their limited supply of torpedoes and shells. The Admiralty took a long time to respond to this change in the tactical position, and in April 1917 convoy was trialled, before being officially introduced in the Atlantic in September 1917.
Other arguments against convoy were raised. The primary issue was the loss of productivity, as merchant shipping in convoy has to travel at the speed of the slowest vessel in the convoy and spent a considerable amount of time in ports waiting for the next convoy to depart. Further, large convoys were thought to overload port resources.
Actual analysis of shipping losses in World War I disproved all these arguments, at least so far as they applied to transatlantic and other long-distance traffic. Ships sailing in convoys were far less likely to be sunk, even when not provided with any escort at all. The loss of productivity due to convoy delays was small compared with the loss of productivity due to ships being sunk. Ports could deal more easily with convoys because they tended to arrive on schedule and so loading and unloading could be planned.
In his book "On the Psychology of Military Incompetence", Norman Dixon suggested that the hostility towards convoys in the naval establishment were in part caused by a (sub-conscious) perception of convoys as effeminating, due to warships having to care for civilian merchant ships. Convoy duty also exposes the escorting warships to the sometimes hazardous conditions of the North Atlantic, with only rare occurrences of visible achievement (i.e. fending off a submarine assault).
World War II.
Atlantic.
The British adopted a convoy system, initially voluntary and later compulsory for almost all merchant ships, the moment that World War II was declared. Each convoy consisted of between 30 and 70 mostly unarmed merchant ships. Canadian, and later American, supplies were vital for Britain to continue its war effort. The course of the second Battle of the Atlantic was a long struggle as the Germans developed anti-convoy tactics and the British developed counter-tactics to thwart the Germans.
The capability of a heavily armed warship against a convoy was dramatically illustrated by the fate of Convoy HX-84. On November 5, 1940, the German heavy cruiser "Admiral Scheer" encountered the convoy. "Maiden", "Trewellard", "Kenbame Head", "Beaverford", and "Fresno" were quickly sunk, and other ships were damaged. Only the sacrifice of the Armed Merchant Cruiser HMS "Jervis Bay" and failing light allowed the rest of the convoy to escape.
The deterrence value of a battleship in protecting a convoy was also dramatically illustrated when the German light battleships (referred by some as battlecruisers) "Scharnhorst" and "Gneisenau", mounting 11 in guns, came upon an eastbound British convoy (HX-106, with 41 ships) in the North Atlantic on February 8, 1941. When the Germans detected the slow but well-protected battleship HMS "Ramillies" escorting the convoy, they fled the scene rather than risk damage from her 15 in guns.
The enormous number of vessels involved and the frequency of engagements meant that statistical techniques could be applied to evaluate tactics: an early use of operational research in war.
Prior to overt participation in WWII, the US was actively engaged in convoys with the British in the North Atlantic Ocean, primarily supporting British activities in Iceland. This was discussed by John T. Flynn in his 1944 work "The Truth About Pearl Harbor" 
On the entry of the U.S. into World War II, the U.S. Navy decided not to instigate convoys on eastern seaboard of the U.S. Fleet Admiral Ernest King ignored advice on this subject from the British as he had formed a poor opinion of the Royal Navy early in his career. The result was what the U-boat crews called their Second Happy Time, which did not end until convoys were introduced.
Pacific.
In the Pacific Theater of World War II, Japanese merchant ships rarely traveled in convoys. Japanese destroyers were generally deficient in antisubmarine weaponry compared to their Allied counterparts, and the Japanese navy did not develop an inexpensive convoy escort like the Allies' destroyer escort/frigate until it was too late. In the early part of the conflict, American submarines in the Pacific were ineffective as they suffered from timid tactics, faulty torpedoes, and poor deployment, while there were only small numbers of British and Dutch boats. U.S. Admiral Charles A. Lockwood's efforts, coupled with strenuous complaints from his captains, rectified these problems and U.S. submarines became much more successful by war's end. As a result, the Japanese merchant fleet was largely destroyed by the end of the war. Japanese submarines, unlike their U.S. and German equivalents, focused on U.S. battle fleets rather than merchant convoys, and while they did manage some early successes, sinking two U.S. carriers, they failed to significantly inhibit the invasion convoys carrying troops and equipment in support of the U.S. island-hopping campaign.
Several notable battles in the South Pacific involved Allied bombers interdicting Japanese troopship convoys which were often defended by Japanese fighters, notable Guadalcanal (13 November 1942), Battle of Wau (5 January 1943), and the Battle of the Bismarck Sea (2–4 March 1943).
At the Battle off Samar, the effectiveness of the U.S. Navy's escorts was demonstrated when they managed to defend their troop convoy from a much larger and more powerful Japanese battle fleet. The Japanese force was centered four battleships and numerous heavy cruisers, while the U.S. forces consisted of escort carriers, destroyers, and destroyer escorts; large numbers American of aircraft (albeit without much anti-ship ordinance other than torpedoes) and aggressive tactics of the destroyers (with their radar-directed gunfire) allowed the U.S. to sink three Japanese heavy cruisers at the cost of one escort carrier and three destroyers.
Tactics.
The German anti-convoy tactics included:
The Allied responses included:
They were also aided by
Convoy battles.
Many naval battles of World War II were fought around convoys, including:
The convoy prefix indicates the route of the convoy. For example, 'PQ' would be Iceland to Northern Russia and 'QP' the return route.
Analysis.
The success of convoys as an anti-submarine tactic during the world wars can be ascribed to several reasons related to U-boat capabilities, the size of the ocean and convoy escorts.
In practice, Type VII and Type IX U-boats were limited in their capabilities. Submerged speed and endurance was limited and not suited for overhauling many ships. Even a surfaced U-boat could take several hours to gain an attack position. Torpedo capacity was also restricted to around fourteen (Type VII) or 24 (Type IX), thus limiting the number of attacks that could be made, particularly when multiple firings were necessary for a single target. There was a real problem for the U-boats and their adversaries in finding each other; with a tiny proportion of the ocean in sight, without intelligence or radar, warships and even aircraft would be fortunate in coming across a submarine. The Royal Navy and later the United States Navy each took time to learn this lesson. Conversely, a U-boat's radius of vision was even smaller and had to be supplemented by regular long-range reconnaissance flights.
For both major allied navies, it had been difficult to grasp that, however large a convoy, its "footprint" (the area within which it could be spotted) was far smaller than if the individual ships had traveled independently. In other words, a submarine had less chance of finding a single convoy than if it were scattered as single ships. Moreover, once an attack had been made, the submarine would need to regain an attack position on the convoy. If, however, an attack were thwarted by escorts, even if the submarine had escaped damage, it would have to remain submerged for its own safety and might only recover its position after many hours' hard work. U-boats patrolling areas with constant and predictable flows of sea traffic, such as the United States Atlantic coast in early 1942, could dismiss a missed opportunity in the certain knowledge that another would soon present itself.
The destruction of submarines required their discovery, an improbable occurrence on aggressive patrols, by chance alone. Convoys, however, presented irresistible targets and could not be ignored. For this reason, the U-boats presented themselves as targets to the escorts with increasing possibility of destruction. In this way, the Ubootwaffe suffered severe losses, for little gain, when pressing pack attacks on well-defended convoys.
Post-World War II.
The largest convoy effort since World War II was Operation Earnest Will, the U.S. Navy's 1987–88 escort of reflagged Kuwaiti tankers in the Persian Gulf during the Iran–Iraq War.
In the present day, convoys are used as a tactic by navies to deter pirates off the coast of Somalia from capturing unarmed civilian freighters who would otherwise pose easy targets if they sailed alone.
Road convoys.
Humanitarian aid convoys.
The word "convoy" is also associated with groups of road vehicles being driven, mostly by volunteers, to deliver humanitarian aid, supplies, and—a stated objective in some cases—"solidarity".
In the 1990s these convoys became common traveling from Western Europe to countries of the former Yugoslavia, in particular Bosnia and Kosovo, to deal with the aftermath of the wars there. They also travel to countries where standards of care in institutions such as orphanages are considered low by Western European standards, such as Romania; and where other disasters have led to problems, such as around the Chernobyl disaster in Belarus and Ukraine.
The convoys are made possible partly by the relatively small geographic distances between the stable and affluent countries of Western Europe, and the areas of need in Eastern Europe and, in a few cases, North Africa and even Iraq. They are often justified because although less directly cost-effective than mass freight transport, they emphasise the support of large numbers of small groups, and are quite distinct from multinational organisations such as United Nations humanitarian efforts.
Truckers' convoys.
Truckers' convoys consisting of semi-trailer trucks are more similar to a caravan than a military convoy.
Truckers' convoys were created as a byproduct of the 55 mph speed limit and 18-wheelers becoming the prime targets of speed traps. Most truckers had difficult schedules to keep and as a result had to maintain a speed above the posted speed limit to reach their destinations on time. Convoys were started so that multiple trucks could run together at a high speed with the rationale being that if they passed a speed trap the police would only be able to pull over one of the trucks in the convoy. When driving on a highway, convoys are also useful to conserve fuel by drafting.
The film "Convoy", inspired by a 1975 song of the same name, explores the camaraderie between truck drivers, where the culture of the CB radio encourages truck drivers to travel in convoys.
Special Convoy Rights.
The Highway Code of several European countries (Norway, Italy, Greece, Netherlands, Germany, Austria, Switzerland, possibly more) includes special rights for marked convoys. They have to be treated like a single vehicle. If the first vehicle has passed an intersection, all others may do so without interruption. If other road users overtake the convoy, they aren't allowed to split into the queue. Clear and uniform marking has been required in court decisions for these rights to apply. Operating such convoy usually needs special permission, but there are exemptions for emergency and catastrophe intervention. Common practice is, to operate with the same style of marking as NATO convoys: STANAG 2154 marking plus country-specific augmentation listed in Annex B to the STANAG.
During the Cold War with its high number of military exercises, the military was the main user of convoy rights. Today, catastrophes like large-scale flooding might bring a high number of flagged convoys to the roads. Large-scale evacuations for the disarming of WWII bombs are another common reason for Non-governmental organization (NGO) unit movements under convoy rights.

</doc>
<doc id="42991" url="http://en.wikipedia.org/wiki?curid=42991" title="Sugar glider">
Sugar glider

The sugar glider ("Petaurus breviceps") is a small, omnivorous, arboreal and nocturnal gliding possum belonging to the marsupial infraclass. The common name refers to its preference for sugary nectarous foods and ability to glide through the air, much like a flying squirrel. Due to convergent evolution, they have very similar appearance and habits to the flying squirrel, but are not closely related. The scientific name, "Petaurus breviceps", translates from Latin as "short-headed rope-dancer", a reference to their canopy acrobatics.
The sugar glider is native to eastern and northern mainland Australia, and was introduced to Tasmania. The sugar glider is also native to various islands in the region.
Distribution and habitat.
Sugar gliders are found throughout the northern and eastern parts of mainland Australia, and in Tasmania, Papua New Guinea and several associated isles, the Bismarck Archipelago, Louisiade Archipelago, and certain isles of Indonesia, Halmahera Islands of the North Moluccas. The sugar glider was introduced to Tasmania in 1835. This is supported by the absence of skeletal remains in subfossil bone deposits and the lack of an Aboriginal Tasmanian name for the animal. They can be found in any forest where there is a suitable food supply, but most are commonly found in forests with eucalyptus trees. Being nocturnal, they sleep in their nests during the day and are active at night. During the night they hunt insects and small vertebrates, and feed on the sweet sap of certain species of eucalyptus, acacia and gum trees.
They are arboreal, spending most of their lives in trees. When suitable habitats are present, sugar gliders can be seen 1 per 1,000 square metres, provided there are tree hollows available for shelter.
Native owls ("Ninox" sp.) are their primary predators; others in their range include kookaburras, goannas, snakes, and quolls. Feral cats ("Felis catus") also represent a significant threat.
Appearance and anatomy.
The sugar glider has a squirrel-like body with a long, partially (weakly) prehensile tail. The males are larger than the females and have bald patches on their head and chest; their length from the nose to the tip of the tail is about 24 to 30 cm (12–13 inches, the body itself is approx. 5–6 inches). A sugar glider has a thick, soft fur coat that is usually blue-grey; some have been known to be yellow, tan or (rarely) albino. A black stripe is seen from its nose to midway on its back. Its belly, throat, and chest are cream in colour.
Being nocturnal, its large eyes help it to see at night, and its ears swivel to help locate prey in the dark.
It has five digits on each foot, each having a claw, except for the opposable toe on the hind feet. Also on the hind feet, the second and third digits are partially syndactylous (fused together), forming a grooming comb.
Its most striking feature is the patagium, or membrane, that extends from the fifth finger to the first toe. When legs are stretched out, this membrane allows the sugar glider to glide a considerable distance.
There are four scent glands, located frontal (forehead), sternal (chest), and two paracloacal (associated with, but not part of the cloaca). These are used for marking purposes, mainly by the male. The frontal gland is easily seen on an adult male as a bald spot. The female has a marsupium (pouch) in the middle of her abdomen to carry offspring.
Data averages
Biology and behaviour.
Gliding.
The sugar glider is one of a number of volplane (gliding) possums in Australia. This remarkable ability to glide is achieved through flaps or membranes of loose skin (patagia) which extend between the fifth finger of each hand to the first toe of each foot. The animal launches itself from a tree, spreading its limbs to expose the gliding membranes. This creates an aerofoil enabling them to glide 50 metres or more. This gliding flight is regulated by changing the curvature of the membrane or moving the legs and tail.
This form of arboreal locomotion is typically used to travel from tree to tree; the species rarely descends to the ground. Gliding serves as an efficient means of both locating food and evading predators.
Torpor.
During the cold season, drought, or rainy nights, a sugar glider's activity is reduced. The animal may even become immobile and unresponsive due to torpor. This differs from hibernation in that torpor is usually a short-term daily cycle. In the winter season or drought, there is a decrease in food supply, which is a challenge for this marsupial because of the energy cost for the maintenance of its metabolism, locomotion, and thermoregulation. With energetic constraints, the sugar glider will enter into daily torpor for 2–23 hours while in rest phase. However, before entering torpor, a sugar glider will reduce activity and body temperature normally in order to lower energy expenditure and avoid torpor.
Torpor, which is seen as an emergency measure, saves energy for the animal by allowing its body temperature to fall to a minimum of 10.4 °C to 19.6 °C. When food is scarce, as in winter, heat production is lowered in order to reduce energy expenditure. With low energy and heat production, it is important for the sugar glider to peak its body mass by fat content in the autumn (May/June) in order to survive the following cold season. In the wild, sugar gliders enter into daily torpor more often than sugar gliders in captivity.
Diet and nutrition.
Sugar gliders are seasonally adapted omnivores with a wide variety of foods in their diet. In summer they are primarily insectivorous, and in the winter when insects (and other arthropods) are scarce, they are mostly exudativorous (feeding on acacia gum, eucalyptus sap, manna, honeydew or lerp). They are opportunistic feeders and can be carnivorous (preying mostly on lizards and small birds), and eat many other foods when available, such as nectar, acacia seeds, bird eggs, pollen, fungi and native fruits. Carnivorous behaviour in the form of predating upon the nests of the Swift parrot is a serious threat to the parrot's survival (see "Conservation" below).
Reproduction.
The age of sexual maturity in sugar gliders varies slightly between the males and females. The males reach maturity at 4 to 12 months of age, while females require from 8 to 12 months. In the wild, sugar gliders breed once or twice a year depending on the climate and habitat conditions, while they can breed multiple times a year in captivity as a result of consistent living conditions and proper diet.
A sugar glider female gives birth to one (19%) or two (81%) babies (joeys) per litter. The gestation period is 15 to 17 days, after which the tiny joey (0.2 g) will crawl into a mother's pouch for further development. They are born with a continuous arc of cartilage in their shoulder girdle to provide support for climbing into the pouch. This structure breaks down immediately after birth.
It is virtually unnoticeable that the female is pregnant until after the joey has climbed into her pouch and begins to grow, forming bumps in her pouch. Once in the pouch, the joey will attach itself to its mother's nipple, where it will stay for about 60 to 70 days. The mother can get pregnant while her joeys are still ip (in pouch) and hold the pregnancy until the pouch is available. The joey gradually spills out of the pouch until it falls out completely. It emerges virtually without fur, and the eyes will remain closed for another 12–14 days. During this time, the joey will begin to mature by growing fur and increasing gradually in size. It takes about two months for the offspring to be completely weaned, and at four months, the young glider is self-sufficient, however it will continue to live in the nest for ten months.
Socialisation.
Sugar gliders are highly social animals. They live in family groups or colonies consisting of up to seven adults, plus the current season's young, which leave as soon as they are able to, all sharing a nest and defending their territory, an example of helping at the nest. They engage in social grooming, which in addition to improving hygiene and health, helps bond the colony and establish group identity.
A dominant adult male will mark his territory and members of the group with saliva and a scent produced by separate glands on the forehead and chest. Intruders who lack the appropriate scent marking are expelled violently. Each colony defends a territory of about 2.5 acres where eucalyptus trees provide a staple food source. Within the colony, typically no fighting takes place beyond threatening behaviour.
Conservation.
The sugar glider is not considered endangered, and its conservation rank is "Least Concern (LC)" on the IUCN Red List. Despite the loss of natural habitat in Australia over the last 200 years, it is adaptable and capable of living in small patches of remnant bush, particularly if it does not have to cross large expanses of cleared land to reach them. However, several close relatives are endangered, particularly Leadbeater's possum and the mahogany glider.
Conservation in Australia is enacted at the federal, state and local levels, where sugar gliders are protected as a native species. The central conservation law in Australia is the Environment Protection and Biodiversity Conservation Act 1999 (EPBC Act). The National Parks and Wildlife Act 1974 is an example of conservation law in the state of South Australia, where it is legal to keep (only) one sugar glider without a permit, provided it was acquired legally from a source with a permit. A permit is required to obtain or possess more than one glider, or if one wants to sell or give away any glider in their possession. It is illegal to capture or sell wild sugar gliders without a permit.
Although not itself endangered, the species has been identified as a severe threat to the survival of the Swift parrot. Reduction in mature forest cover has left Swift parrot nests highly vulnerable to predation by sugar gliders, and it is estimated that the parrot could be extinct within 16 years.
In captivity.
In captivity, they can suffer from calcium deficiencies if not fed an adequate diet. A lack of calcium in the diet causes the body to leach calcium from the bones, with the hind legs first to show noticeable dysfunction. Calcium to phosphorus ratios should be 2:1 to prevent hypocalcemia, sometimes known as hind leg paralysis (HLP). Their diet should be 50% insects (gut-loaded) or other sources of protein, 25% fruit and 25% vegetables. Some of the more recognised diets are Bourbon's Modified Leadbeaters (BML), High Protein Wombaroo (HPW) and various calcium rich diets with Leadbeaters Mixture (LBM).
As a pet.
Around the world, the sugar glider is popular as an exotic pet. It is also one of the most commonly traded wild animals in the illegal pet trade, where animals are plucked directly from their natural habitats.
In Australia, sugar gliders can be kept in Victoria, South Australia, and the Northern Territory. However, they are not allowed to be kept as pets in Western Australia, New South Wales, the Australian Capital Territory, Queensland or Tasmania.
Sugar gliders are popular as pets in the United States, where they are bred in large numbers. Most states and cities allow sugar gliders as pets, with some exceptions including California, Hawaii, and Alaska. In 2014, Massachusetts changed its law, allowing sugar gliders to be kept as pets. Some other states require permits or licensing. Many states also require permits and/or licensing for breeding large numbers of sugar gliders. Due to the Animal Welfare Act, the USDA and APHIS regulate the licensing of breeders. 
Taxonomy.
Further taxonomic study is needed because "P. breviceps" might be composed of more than one species.

</doc>
<doc id="42993" url="http://en.wikipedia.org/wiki?curid=42993" title="Back to the Future">
Back to the Future

Back to the Future is a 1985 American comic science fiction film. It was directed by Robert Zemeckis, written by Zemeckis and Bob Gale, produced by Steven Spielberg, and stars Michael J. Fox, Christopher Lloyd, Lea Thompson, Crispin Glover and Thomas F. Wilson. Fox plays Marty McFly, a teenager who is sent back in time to 1955. He meets his future parents in high school and accidentally becomes his mother's romantic interest. Marty must repair the damage to history by causing his parents-to-be to fall in love, and with the help of scientist Dr. Emmett "Doc" Brown (Christopher Lloyd), he must find a way to return to 1985.
Zemeckis and Gale wrote the script after Gale mused upon whether he would have befriended his father if they attended school together. Various film studios rejected the script until the financial success of Zemeckis' "Romancing the Stone". Zemeckis approached Spielberg, who agreed to produce the project at Amblin Entertainment, with Universal Pictures as distributor. The first choice for the role of Marty McFly was Michael J. Fox. However, he was busy filming his television series "Family Ties" and the show's producers would not allow him to star in the film. Consequently, Eric Stoltz was cast in the role. During filming, Stoltz and the filmmakers decided that the role was miscast, and Fox was again approached for the part. Now with more flexibility in his schedule and the blessing of his show's producers, Fox managed to work out a timetable in which he could give enough time and commitment to both.
"Back to the Future" was released on July 3, 1985, and became the most successful film of the year, grossing more than $383 million worldwide and receiving widespread critical acclaim. It won the Hugo Award for Best Dramatic Presentation, the Saturn Award for Best Science Fiction Film, and the Academy Award for Best Sound Effects Editing, as well as receiving three additional Academy Award nominations, five BAFTA nominations, and four Golden Globe nominations. Ronald Reagan even quoted the film in his 1986 State of the Union Address. In 2007, the Library of Congress selected it for preservation in the National Film Registry, and in June 2008 the American Film Institute's special AFI's 10 Top 10 designated the film as the 10th-best film in the science fiction genre. The film marked the beginning of a franchise, with sequels "Back to the Future Part II" and "III" released in 1989 and 1990, as well as an animated series, , several video games and a forthcoming musical.
Plot.
Teenager Marty McFly is an aspiring musician dating girlfriend Jennifer Parker in Hill Valley, California. His family is less ambitious; his father George is bullied by his supervisor, Biff Tannen, while his mother Lorraine is an overweight alcoholic who mainly reminisces about the past, such as how she met George in high school when he was hit by her father's car.
Marty meets his scientist friend "Doc" Brown late at night in the parking lot of a shopping mall, where Doc unveils a time machine built from a modified DeLorean. The vehicle's "flux capacitor" is powered by plutonium that he's stolen from Libyan terrorists. Doc tests the time machine by accelerating it to 88 m.p.h., sending it one minute into the future, and demonstrates the time circuits by entering an example date of November 5, 1955, the day he invented the flux capacitor. Before Doc can make his first trip, the Libyans appear in a van and gun him down. Marty attempts to escape in the DeLorean but inadvertently activates the time machine, finding himself transported to 1955.
Wandering in 1955 Hill Valley, Marty encounters the teenage George, who is still bullied by Biff, now a classmate. After Marty saves George from an oncoming car and is knocked unconscious, he awakens to find himself tended to by an infatuated Lorraine. Marty goes in search of the 1955 Doc, asking for his help to get back to 1985. With no plutonium, Doc explains that the only power source capable of generating the necessary 1.21 gigawatts of electricity is a bolt of lightning. Marty shows Doc a flyer from the future that recounts a lightning strike at the town's courthouse the coming Saturday night. Doc formulates a plan to harness the power of the lightning, while Marty sets about introducing his parents to each other to ensure his own existence.
Marty makes several attempts to set George up with Lorraine, but only antagonizes Biff and his gang in the process, causing Biff to crash his car into a manure truck. Marty also attempts to warn Doc about his death in the future, but Doc refuses to hear it, fearing it will alter the future.
When Lorraine asks Marty to the upcoming school dance, Marty plans to have George attend as well and "rescue" Lorraine from Marty's inappropriate advances. The plan goes awry when a drunken Biff shows up, pulls Marty from his car, and attempts to force himself on Lorraine. George arrives to rescue her from Marty but finds Biff instead; standing up to him for the first time, George knocks out Biff. A smitten Lorraine follows George to the dance floor, while Marty helps the band and ensures that his parents kiss for the first time.
As the storm gathers, Marty arrives at the clock tower. Doc angrily tears up a warning letter Marty has written him, still fearing it will alter the future, and a fallen branch suddenly disconnects the massive wire Doc has run from the clock tower to the street. As Marty races the DeLorean at 88 m.p.h. toward the clock tower, Doc climbs across the face of the clock to reconnect the cable. The lightning strikes on cue, sending Marty back to 1985 ten minutes before he left it. Marty runs to the shopping mall, but arrives too late only to watch Doc get gunned down and his counterpart escape to 1955. After a moment, Doc arises with a bullet-proof vest thanks to Marty's letter, which he kept. Doc then takes Marty home and departs to 2015.
Marty awakens the next morning to find his family changed: George is a self-confident, successful author and Lorraine is physically fit. Biff, instead of being a bullying superior, is now an obsequious subordinate to George and Marty. As Marty reunites with Jennifer, the DeLorean appears with Doc, dressed in a futuristic outfit, insisting they accompany him in the future. The DeLorean disappears into the future with Doc, Marty and Jennifer.
Production.
Development.
Writer and producer Bob Gale conceived the idea after he visited his parents in St. Louis, Missouri after the release of "Used Cars". Searching their basement, Gale found his father's high school yearbook and discovered he was president of his graduating class. Gale thought about the president of his own graduating class, who was someone he had nothing to do with. Gale wondered whether he would have been friends with his father if they went to high school together. When he returned to California, he told Robert Zemeckis his new concept. Zemeckis subsequently thought of a mother claiming she never kissed a boy at school, when in reality she was highly promiscuous. The two took the project to Columbia Pictures, and made a development deal for a script in September 1980.
Zemeckis and Gale said that they had set the story in 1955 because a 17-year-old traveling to meet his parents at the same age arithmetically required the script to travel to that decade. The era also marked the rise of teenagers as an important cultural element, the birth of rock n' roll, and suburb expansion, which would flavor the story. In an early script, the time machine was designed as a refrigerator, and its user needed to use the power of an atomic explosion at the Nevada Test Site to return home. Zemeckis was "concerned that kids would accidentally lock themselves in refrigerators", and found that it would be more convenient if the time machine were mobile. The DeLorean was chosen because its design made the gag about the family of farmers mistaking it for a flying saucer believable. In addition the original climax was deemed too expensive by the executives of Universal and was simplified. Spielberg later used the omitted refrigerator and Nevada nuclear site elements in his film "Indiana Jones and the Kingdom of the Crystal Skull". The writers found it difficult to create a believable friendship between Marty and Brown before they created the giant guitar amplifier, and only resolved his Oedipal relationship with his mother when they wrote the line "It's like I'm kissing my brother." Biff Tannen was named after Universal executive Ned Tanen, who behaved aggressively toward Zemeckis and Gale during a script meeting for "I Wanna Hold Your Hand".
The first draft of "Back to the Future" was finished in February 1981. Columbia Pictures put the film in turnaround. "They thought it was a really nice, cute, warm film, but not sexual enough," Gale said. "They suggested that we take it to Disney, but we decided to see if any other of the major studios wanted a piece of us." Every major film studio rejected the script for the next four years, while "Back to the Future" went through two more drafts. During the early 1980s, popular teen comedies (such as "Fast Times at Ridgemont High" and "Porky's") were risqué and adult-aimed, so the script was commonly rejected for being too light. Gale and Zemeckis finally decided to pitch "Back to the Future" to Disney. "They told us that a mother falling in love with her son was not appropriate for a family film under the Disney banner," Gale said.
The two were tempted to ally themselves with Steven Spielberg, who produced "Used Cars" and "I Wanna Hold Your Hand", which both flopped. Spielberg was initially absent from the project because Zemeckis felt if he produced another flop under him, he would never be able to make another film. Gale said "we were afraid that we would get the reputation that we were two guys who could only get a job because we were pals with Steven Spielberg." One producer was interested, but changed his mind when he learned Spielberg was not involved. Zemeckis chose to direct "Romancing the Stone" instead, which was a box office success. Now a high-profile director, Zemeckis approached Spielberg with the concept, and the project was set up at Universal Pictures.
Executive Sidney Sheinberg made some suggestions to the script, changing Marty's mother's name from Meg to Lorraine (the name of his wife, actress Lorraine Gary), to change Brown's name from Professor Brown to Doc Brown and replace his pet chimpanzee with a dog. Sheinberg also wanted the title changed to "Spaceman from Pluto", convinced no successful film ever had "future" in the title. He suggested Marty introduce himself as "Darth Vader from the planet Pluto" while dressed as an alien forcing his dad to ask out his mom (rather than "the planet Vulcan"), and that the farmer's son's comic book be titled "Spaceman from Pluto" rather than "Space Zombies from Pluto". Appalled by the new title that Sheinberg wanted to impose, Zemeckis asked Spielberg for help. Spielberg subsequently dictated a memo back to Sheinberg, wherein Spielberg convinced him they thought his title was just a joke, thus embarrassing him into dropping the idea.
Casting.
Michael J. Fox was the first choice to play Marty McFly, but he was committed to the show "Family Ties". "Family Ties" producer Gary David Goldberg felt that Fox was essential to the show's success. With co-star Meredith Baxter on maternity leave, he refused to allow Fox time off to work on a film. "Back to the Future" was originally scheduled for a May 1985 release and it was late 1984 when it was learned that Fox would be unable to star in the film. Zemeckis' next two choices were C. Thomas Howell and Eric Stoltz. Eric Stoltz impressed the producers enough with his earlier portrayal of Roy L. Dennis in "Mask" – which had yet to be released – that they selected him to play Marty McFly. Because of the difficult casting process, the start date was pushed back twice.
Four weeks into filming, Zemeckis determined Stoltz had been miscast. Although he and Spielberg realized reshooting the film would add $3 million to the $14 million budget, they decided to recast. Spielberg explained Zemeckis felt Stoltz was too humorless and gave a "terrifically "dramatic" performance". Gale further explained they felt Stoltz was simply acting out the role, whereas Fox himself had a personality like Marty McFly. He felt Stoltz was uncomfortable riding a skateboard, whereas Fox was not. Stoltz confessed to director Peter Bogdanovich during a phone call, two weeks into the shoot, that he was unsure of Zemeckis and Gale's direction, and concurred that he was wrong for the role.
Fox's schedule was opened up in January 1985 when Meredith Baxter returned to "Family Ties" following her pregnancy. The "Back to the Future" crew met with Goldberg again, who made a deal that Fox's main priority would be "Family Ties", and if a scheduling conflict arose, "we win". Fox loved the script and was impressed by Zemeckis and Gale's sensitivity in releasing Stoltz, because they nevertheless "spoke very highly of him". Per Welinder and Bob Schmelzer assisted on the skateboarding scenes. Fox found his portrayal of Marty McFly to be very personal. "All I did in high school was skateboard, chase girls and play in bands. I even dreamed of becoming a rock star."
Christopher Lloyd was cast as Doc Brown after the first choice, John Lithgow, became unavailable. Having worked with Lloyd on "The Adventures of Buckaroo Banzai" (1984), producer Neil Canton suggested him for the part. Lloyd originally turned down the role, but changed his mind after reading the script and at the persistence of his wife. He improvised some of his scenes, taking inspiration from Albert Einstein and conductor Leopold Stokowski. Brown pronounces gigawatts as "jigawatts", which was the way a physicist said the word when he met with Zemeckis and Gale as they researched the script, rather than with an initial hard "g", although both pronunciations are acceptable. Doc Brown's notable hunch came about because at 6'1" Lloyd was considerably taller than Fox at 5'5", and they needed to look closer in height.
Crispin Glover played George McFly. Zemeckis said Glover improvised much of George's nerdy mannerisms, such as his shaky hands. The director joked he was "endlessly throwing a net over Crispin because he was completely off about fifty percent of the time in his interpretation of the character". Due to a contract disagreement, Glover was replaced by Jeffrey Weissman in "Part II" and "Part III".
Lea Thompson was cast as Lorraine McFly because she had acted opposite Stoltz in "The Wild Life"; the producers noticed her as they had watched the film while casting Stoltz. Her prosthetic makeup for scenes at the beginning of the film, set in 1985, took three and a half hours to apply.
Thomas F. Wilson was cast as Biff Tannen because the producers felt that the original choice, J. J. Cohen, wasn't physically imposing enough to bully Stoltz. Cohen was recast as Skinhead, one of Biff's cohorts. Had Fox been cast from the beginning, Cohen probably would have won the part because he was sufficiently taller than Fox.
Melora Hardin was originally cast in the role of Marty's girlfriend Jennifer, but was let go after Eric Stoltz was dismissed, with the explanation that the actress was now too tall to be playing against Michael J. Fox. Hardin was dismissed before she had a chance to shoot a single scene and was replaced with Claudia Wells. Actress Jill Schoelen had also been considered to play Marty's girlfriend.
Filming.
Following Stoltz's departure, Fox's schedule during weekdays consisted of filming "Family Ties" during the day, and "Back to the Future" from 6:30 pm to 2:30 am. He averaged five hours of sleep each night. During Fridays, he shot from 10 pm to 6 or 7 am, and then moved on to film exterior scenes throughout the weekend, as only then was he available during daytime hours. Fox found it exhausting, but "it was my dream to be in the film and television business, although I didn't know I'd be in them simultaneously. It was just this weird ride and I got on." Zemeckis concurred, dubbing "Back to the Future" "the film that would not wrap". He recalled that because they shot night after night, he was always "half asleep" and the "fattest, most out-of-shape and sick I ever was".
The Hill Valley town square scenes were shot at Courthouse Square, located in the Universal Studios back lot (). Bob Gale explained it would have been impossible to shoot on location "because no city is going to let a film crew remodel their town to look like it's in the 1950s." The filmmakers "decided to shoot all the 50s stuff first, and make the town look real beautiful and wonderful. Then we would just totally trash it down and make it all bleak and ugly for the 1980s scenes." The interiors for Doc Brown's house were shot at the Robert R. Blacker House, while exteriors took place at Gamble House. The exterior shots of the Twin Pines Mall, and later the Lone Pine Mall (from 1985) were shot at the Puente Hills Mall in City of Industry, California. The exterior shots and some interior scenes at Hill Valley High School were filmed at Whittier High School in Whittier, California, while the band tryouts and the "Enchantment Under the Sea" dance were filmed in the gymnasium at Hollywood United Methodist Church. The scenes outside of the Baines' house in the 50s were shot at Bushnell Avenue, South Pasadena, California.
Filming wrapped after 100 days on April 20, 1985, and the film was delayed from May to August. But after a highly positive test screening ("I'd never seen a preview like that," said Frank Marshall, "the audience went up to the ceiling"), Sheinberg chose to move the release date to July 3. To make sure the film met this new date, two editors, Arthur Schmidt and Harry Keramidas, were assigned to the picture, while many sound editors worked 24-hour shifts on the film. Eight minutes were cut, including Marty watching his mom cheat during an exam, George getting stuck in a telephone booth before rescuing Lorraine, as well as much of Marty pretending to be Darth Vader. Zemeckis almost cut out the "Johnny B. Goode" sequence as he felt it did not advance the story, but the preview audience loved it, so it was kept. Industrial Light & Magic created the film's 32 effects shots, which did not satisfy Zemeckis and Gale until a week before the film's completion date.
Music.
Alan Silvestri collaborated with Zemeckis on "Romancing the Stone", but Spielberg disliked that film's score. Zemeckis advised Silvestri to make his compositions grand and epic, despite the film's small scale, to impress Spielberg. Silvestri began recording the score two weeks before the first preview. He also suggested Huey Lewis and the News create the theme song. Their first attempt was rejected by Universal, before they recorded "The Power of Love". The studio loved the final song, but were disappointed it did not feature the film's title, so they had to send memos to radio stations to always mention its association with "Back to the Future". In the end, the track "Back in Time" was featured in the film, playing during the scene when Marty wakes up after his return to 1985 and also during the end credits.
Although it appears that Michael J. Fox is actually playing a guitar, music supervisor Bones Howe hired Hollywood guitar coach and musician Paul Hanson to teach Fox to simulate playing all the parts so it would look realistic, including playing behind his head. Fox lip-synched "Johnny B. Goode" to vocals by Mark Campbell (of Jack Mack and the Heart Attack fame), with the guitar solo played by Tim May.
The original 1985 soundtrack album only included two tracks culled from Silvestri's compositions for the film, both Huey Lewis tracks, the songs played in the film by the fictional band "Marvin Berry and The Starlighters" (and Marty McFly), one of the vintage 1950s songs in the movie, and two pop songs that are only very briefly heard in the background of the film . On November 24, 2009, an authorized, limited-edition two-CD set of the entire score was released by Intrada Records.
Release.
"Back to the Future" opened on July 3, 1985, on 1,200 screens in North America. Zemeckis was concerned the film would flop because Fox had to film a "Family Ties" special in London and was unable to promote the film. Gale was also dissatisfied with Universal Pictures' tagline "Are you telling me my mother's got the hots for me?".
When the film was released on VHS, Universal added a "To be continued..." graphic at the end to increase awareness of production on "Part II". This caption is omitted on the film's DVD release of 2002 and on subsequent Blu-ray and DVD releases.
This film received a 25th anniversary theatrical re-release in the UK and the US in October 2010 to coincide with Universal Pictures Home Entertainment's 25th Anniversary DVD and Blu-ray Disc releases of the trilogy. For its re-issue, "Back to the Future" was restored and remastered.
Reception.
Box office.
"Back to the Future" spent 11 weeks at number one. Gale recalled "Our second weekend was higher than our first weekend, which is indicative of great word of mouth. "National Lampoon's European Vacation" came out in August and it kicked us out of number one for one week and then we were back to number one." The film went on to gross $210.61 million in North America and $173.2 million in foreign countries, accumulating a worldwide total of $383.87 million. "Back to the Future" had the fourth-highest opening weekend of 1985 and was the top grossing film of the year. 
Critical response.
According to review aggregator Metacritic, which assigns a normalized rating out of 100, the film received an average score of 86/100, which indicates "universal acclaim", based on 12 reviews. s of 2013[ [update]], review aggregator Rotten Tomatoes reported that 96% of critics gave the film a positive review, based on 68 reviews, certifying it "Fresh", with an average rating of 8.6 out of 10 and the consensus: "Inventive, funny, and breathlessly constructed, "Back to the Future" is a rousing time-travel adventure with an unforgettable spirit."
Roger Ebert of the "Chicago Sun-Times" felt "Back to the Future" had similar themes to the films of Frank Capra, especially "It's a Wonderful Life". Ebert commented "[Producer] Steven Spielberg is emulating the great authentic past of Classical Hollywood cinema, who specialized in matching the right director (Robert Zemeckis) with the right project." Janet Maslin of "The New York Times" believed the film had a balanced storyline: "It's a cinematic inventing of humor and whimsical tall tales for a long time to come." Christopher Null, who first saw the film as a teenager, called it "a quintessential 1980s flick that combines science fiction, action, comedy, and romance all into a perfect little package that kids and adults will both devour." Dave Kehr of "Chicago Reader" felt Gale and Zemeckis wrote a script that perfectly balanced science fiction, seriousness and humor. "Variety" applauded the performances, arguing Fox and Lloyd imbued Marty and Doc Brown's friendship with a quality reminiscent of King Arthur and Merlin. BBC News applauded the intricacies of the "outstandingly executed" script, remarking that "nobody says anything that doesn't become important to the plot later." "Back to the Future" appeared on Gene Siskel's top ten film list of 1985.
Accolades.
At the 58th Academy Awards, "Back to the Future" won for Best Sound Effects Editing while "The Power of Love" was nominated for Best Song and Bill Varney, B. Tennyson Sebastian II, Robert Thirlwell and William B. Kaplan were nominated for Best Sound Mixing. Zemeckis and Gale were nominated for Best Original Screenplay, but lost to the critically acclaimed thriller "Witness". The film won the Hugo Award for Best Dramatic Presentation and the Saturn Award for Best Science Fiction Film. Michael J. Fox and the visual effects designers won categories at the Saturn Awards. Zemeckis, composer Alan Silvestri, the costume design and supporting actors Christopher Lloyd, Lea Thompson, Crispin Glover and Thomas F. Wilson were also nominated. The film was nominated for numerous BAFTAs at the 39th British Academy Film Awards, including Best Film, original screenplay, visual effects, production design and editing. At the 43rd Golden Globe Awards, "Back to the Future" was nominated for Best Motion Picture (Musical or Comedy), original song (for "The Power of Love"), Best Actor in a Motion Picture Musical or Comedy (Fox) and Best Screenplay for Zemeckis and Gale.
Legacy.
President Ronald Reagan, a fan of the film, referred to the movie in his 1986 State of the Union Address when he said, "Never has there been a more exciting time to be alive, a time of rousing wonder and heroic achievement. As they said in the film "Back to the Future", 'Where we're going, we don't need roads'." When he first saw the joke about him being president, he ordered the projectionist of the theater to stop the reel, roll it back, and run it again.
The movie ranked number 28 on "Entertainment Weekly"‍ '​s list of the 50 Best High School Movies. In 2008, "Back to the Future" was voted the 23rd greatest film ever made by readers of "Empire". It was also placed on a similar list by "The New York Times", a list of 1000 movies. In January 2010, "Total Film" included the film on its list of "The 100 Greatest Movies of All Time". On December 27, 2007, "Back to the Future" was selected for preservation in the United States National Film Registry by the Library of Congress as being "culturally, historically, or aesthetically significant". In 2006, the original screenplay for "Back to the Future" was selected by the Writers Guild of America as the 56th best screenplay of all time.
In June 2008, the American Film Institute revealed the AFI's 10 Top 10 – the best ten films in ten classic American film genres – after polling more than 1,500 people from the creative community. "Back to the Future" was acknowledged as the 10th best film in the science fiction genre.
A musical theater production, also called "Back to the Future", is in development for a debut in London's West End theatre during the film's 30th anniversary in 2015. Zemeckis and Gale reunited to write the play, while Silvestri and Glen Ballard provide music.
The scenes of the Marty McFly character skateboarding in the film occurred during the infancy of the skateboarding sub-culture and numerous skateboarders, as well as companies in the industry, pay tribute to the film for its influence in this regard. Examples can be seen in promotional material, in interviews in which professional skateboarders cite the film as an initiation into the action sport, and in the public's recognition of the film's influence.
"Back to the Future" is also among Film4's "50 Films to See Before You Die", being ranked 10th.
Sequels.
"Back to the Future"‍ '​s success led to two film sequels: "Back to the Future Part II" and "Back to the Future Part III". "Part II" was released on November 22, 1989 to similar financial and critical success as the original, finishing as the third highest-grossing film of the year worldwide. The film continues directly from the ending of "Back to the Future" and follows Marty and Doc as they travel into the future of 2015, an alternative 1985, and 1955 where Marty must repair the future while avoiding his past self from the original film. "Part II" became notable for its 2015-setting and predictions of technology such as hoverboards. "Part III", released on May 25, 1990, continued the story, following Marty as he travels back to 1885 to rescue a time-stranded Doc. "Part III" was less financially successful than its predecessors despite being better received by critics.
References.
</dl>

</doc>
<doc id="42994" url="http://en.wikipedia.org/wiki?curid=42994" title="Chain letter">
Chain letter

A typical chain letter consists of a message that attempts to convince the recipient to make a number of copies of the letter and then pass them on to as many recipients as possible. In reality, the "chain" is actually a geometrically progressing pyramid that cannot be sustained indefinitely. Common methods used in chain letters include emotionally manipulative stories, get-rich-quickly pyramid schemes, and the exploitation of superstition to threaten the recipient with bad luck or even physical violence or death if he or she "breaks the chain" and refuses to adhere to the conditions set out in the letter. Chain letters started as actual letters that one received in the mail. Today, chain letters are generally no longer actual letters. They are sent through email messages, postings on social network sites, and text messages.
There are two main types of chain letters:
In the United States, chain letters that request money or other items of value and promise a substantial return to the participants (such as the infamous "Make Money Fast" scheme) are illegal. Other types of chain letters are viewed as a general nuisance in that frequently multiplying letters clog up the postal system and do not function as correspondence mail, but rather, a game. Some colleges and military bases have passed regulations stating that in the private mail of college students and military personnel, respectively, chain letters are not authorized and will be thrown out. However, it is often difficult to distinguish chain letters from genuine correspondence.
Channels.
Print.
The oldest known channel for chain letters is written, or printed, on letters on paper. These might be exchanged hand-to-hand or distributed through the mail. One notorious early example was the "Prosperity Club" or "Send-a-Dime" letter. This letter started in Denver, Colorado in 1935, based on an earlier luck letter. It soon swamped the Denver post office with hundreds of thousands of letters before spilling into St. Louis and other cities.
Chain letters take religious perspectives especially relating to Christianity. Often these letters originate from Photocopy centers, claiming to have originated from the Pope, with the intent of persuading people to make copies of such letters. The content usually gives one or two examples of people, sometimes public figures who obeyed and were rewarded and others who disobeyed and suffered heavily, which may even include cases of deaths and of someone becoming a millionaire overnight. These types of letters will flourish for some days and will die out naturally, partly based on the economic realities of the people, and maybe many would also reason that if that was truly the original letter, then it cannot contain cases of people who had broken or continued the chain.
Email.
Some email messages sent as chain letters may seem fairly harmless, for example, a grammar school student wishing to see how many people can receive his/her email for a science project, but can grow exponentially and be hard to stop. Messages sometimes include phony promises from companies or wealthy individuals (such as Bill Gates) promising a monetary reward to everyone who receives the message. They may also be politically motivated, such as "Save the Scouts, forward this to as many friends as possible" or a concept that a popular TV or radio show may be forced off the air. Some, like the "Hawaiian Good Luck Totem" which has spread in thousands of forms, threaten users with bad luck if not forwarded.
There are many forms of chain email that threaten death or the taking of one's soul by telling tales of others' deaths, such as the Katu Lata Kulu chain email, stating that if it is not forwarded, the receivers of the message will be killed by the spirit.
Platforms like Facebook, YouTube can host chain letters playing with users' emotions. They may also be in the form of a warning, such as stories of escaped convicts et cetera which urge the reader to pass the message on. One chain letter distributed on MSN Hotmail began, "Hey it's Tara and John the directors of MSN"... and tells you that your account will be deleted if you don't send that message to everyone.
Another common form of email chain letter is the virus hoax and a form of cyberbullying.
Web communities.
Chain letters have become widespread on Myspace (in the form of Myspace bulletins) and YouTube (in the form of video comments) as well as on Facebook through messages or applications. For instance, the chain post/email of Carmen Winstead, a girl who was pushed down a sewage drain in a firedrill, states that, "if you do not repost/send this to 10 people, Carmen will find you and kill you." Chain letters are often coupled with intimidating hoaxes or the promise of providing the sender with "secret" information once they've forwarded the message.
Legality.
A chain letter may qualify as a fraudulent activity, as in the case of a pyramid scheme which asks recipients to funnel money up the chain while requesting the letter be distributed to multiple new recipients.
The legality of chain letters comes into question when they attempt to funnel monetary value to a single source. When a chain letter suggests a game of chance or a lottery with an opportunity for financial gain it is considered fraudulent under Title 18, United States Code, Section 1302, the Postal Lottery Statute. Chain letters that ask for items of minor value such as business cards or recipes are not covered by this law.
If pyramid scheme chain letters are sent through email it may constitute wire fraud. An email chain letter may contain trojans or another type of computer virus which is covered under the Computer Fraud and Abuse Act (CFAA) [18 U.S.C. Section 1030]. This law makes it illegal to distribute computer codes or place them in the stream of commerce if their intent is to cause damage or economic loss.

</doc>
<doc id="42996" url="http://en.wikipedia.org/wiki?curid=42996" title="Rebecca (disambiguation)">
Rebecca (disambiguation)

Rebecca, from Rebekah, is a biblical matriarch from the Book of Genesis and a common first name. As a name it is often shortened to Becky, Becki or Becca; see "Rebecca (given name)".
Rebecca (and related names) may also refer to:

</doc>
<doc id="42997" url="http://en.wikipedia.org/wiki?curid=42997" title="Flevoland">
Flevoland

Flevoland (]) is a province of the Netherlands. Located in the centre of the country, at the location of the former Zuiderzee, the province was established on 1 January 1986; the twelfth province of the country, with Lelystad as its capital. The province has approximately 394,758 inhabitants (2011[ [update]]) and consists of 6 municipalities.
History.
After a flood in 1916, it was decided that the Zuiderzee, an inland sea within the Netherlands, would be enclosed and reclaimed: the Zuiderzee Works started. Other sources indicate other times and reasons, but also agree that in 1932, the Afsluitdijk was completed, which closed off the sea completely. The Zuiderzee was subsequently called IJsselmeer (lake at the end of the river IJssel).
The first part of the new lake that was reclaimed was the Noordoostpolder (Northeast polder) in 1939. This new land included the former islands of Urk and Schokland and it was included in the province of Overijssel. After this, other parts were reclaimed: the Southeastern part in 1957 and the Southwestern part in 1968. There was an important change in these post-war projects from the earlier Noordoostpolder reclamation: a narrow body of water was preserved along the old coast to stabilise the water table and to prevent coastal towns from losing their access to the sea. Thus Flevopolder became an artificial island joined to the mainland by bridges. The municipalities on the three parts voted to become a separate province, which happened in 1986.
Flevoland was named after Lacus Flevo, a name recorded in Roman sources for a large inland lake at the southern end of the later-formed Zuiderzee. Draining the Flevoland polders revealed many wrecks of aircraft that had crashed into the IJsselmeer during World War II, and also fossils of Pleistocene mammals.
In February 2011, Flevoland, together with the provinces of Utrecht and North Holland, showed a desire to investigate the feasibility of a merger between the three provinces. This has been positively received by the Dutch cabinet, for the desire to create one Randstad province has already been mentioned in the coalition agreement. The province of South Holland, part of the Randstad urban area, visioned to be part of the Randstad province, and very much supportive of the idea of a merger into one province, is not named. With or without South Holland, if created, the new province would be the largest in the Netherlands in both area and population.
Geography.
Flevolands, Zuiderzee works.
Eastern Flevoland ("Oostelijk Flevoland" or "Oost-Flevoland") and Southern Flevoland ("Zuidelijk Flevoland" or "Zuid-Flevoland"), unlike the Noordoostpolder, have peripheral lakes between them and the mainland: the Veluwemeer and Gooimeer respectively, making them, together, the world's largest artificial island.
They are two polders with a joint hydrological infrastructure, with a dividing dike in the middle, the "Knardijk", that will keep one polder safe if the other is flooded. The two main drainage canals that traverse the dike can be closed by floodgates in such an event. The pumping stations are the "Wortman" (diesel powered) at Lelystad-Haven, the "Lovink" near Harderwijk on the mainland and the "Colijn" (both electrically powered) along the northern dike beside the Ketelmeer.
A new element in the design of Eastern Flevoland is the larger city Lelystad (1966), named after Cornelis Lely, the man who had played a crucial role in designing and realising the Zuiderzee Works. Other more conventional settlements already existed by then; Dronten, the major local town, was founded in 1962, followed by two smaller satellite villages, Swifterbant and Biddinghuizen, in 1963. These three were incorporated in the new municipality of Dronten on 1 January 1972.
Southern Flevoland has only one pumping station, the diesel powered "De Blocq van Kuffeler". Because of the hydrological union of the two Flevolands it simply joins the other three in maintaining the water-level of both polders. Almere relieves the housing shortage and increasing overcrowding on the old land. Its name is derived from the early medieval name for Lacus Flevo. Almere was to be divided into 3 major settlements initially; the first, "Almere-Haven" (1976) situated along the coast of the Gooimeer (one of the peripheral lakes), the second and largest was to fulfill the role of city centre as "Almere-Stad" (1980) and the third was "Almere-Buiten" (1984) to the northwest towards Lelystad. In 2003, the municipality made a new "Structuurplan" which started development of three new settlements: "Overgooi" in the southeast, "Almere-Hout" in the east, and "Almere-Poort" in the West. In time, "Almere-Pampus" could be developed in the northwest, with possibly a new bridge over the IJmeer towards Amsterdam.
The Oostvaardersplassen is a landscape of shallow pools, islets and swamps. Originally, this low part of the new polder was destined to become an industrial area. Spontaneous settlement of interesting flora & fauna turned the area into a nature park, of such importance that the new railway-line was diverted. The recent decline in agricultural land use will in time make it possible to expand "natural" land use, and connect the Oostvaardersplassen to the Veluwe.
The centre of the polder most closely resembles the pre-war polders in that it is almost exclusively agricultural. In contrast, the southeastern part is dominated by extensive forests. Here is also found the only other settlement of the polder, Zeewolde (1984), again a more conventional town acting as the local centre. Zeewolde became a municipality at the same time as Almere on 1 January 1984, which in the case of Zeewolde meant that the municipality existed before the town itself, with only farms in the surrounding land to be governed until the town started to grow.
Government.
The King's Commissioner of Flevoland is Leen Verbeek, who is a member of the Labour Party (Netherlands) (PvDA). The States of Flevoland have 39 seats. Since the 2011 provincial elections, the People's Party for Freedom and Democracy has been the largest party in The States, with 9 seats. The second largest parties are the Labour Party and the Party for Freedom, both with 6 seats.
Since the 2011 provincial elections, the seats of the States of Flevoland are as following:
Transport.
Rail.
The Flevopolder is served by the Flevolijn, running from Weesp to Lelystad, and the Hanzelijn, continuing from Lelystad towards Zwolle. The two railways stations of the province with intercity services are Almere Centrum and Lelystad Centrum.
Furthermore, Lelystad Zuid is a planned railway station between Almere Oostvaarders and Lelystad Centrum. It has been partially constructed preceding the opening of the railway in 1988, but construction has been put on indefinite hold because of slower-than-expected development of the city of Lelystad.
Amongst the cities with direct train connections to Flevoland are Amsterdam, Utrecht, The Hague, Zwolle, Groningen, Leeuwarden and Schiphol Airport.
Airports.
There are two airports in the province: Lelystad Airport and Noordoostpolder Airport.
References.
Zuiderzeemuseum

</doc>
<doc id="43000" url="http://en.wikipedia.org/wiki?curid=43000" title="Sucre">
Sucre

Sucre (]), also known historically as Charcas ], La Plata ] and Chuquisaca ] (population 247,300 in 2006) is the constitutional capital of Bolivia, the capital of the department of Chuquisaca, and the 6th most populated city in Bolivia. Located in the south-central part of the country, Sucre lies at an elevation of 2,810 meters (9,214 feet). This relatively high altitude gives the city a cool temperate climate year-round.
History.
On November 30, 1538, Sucre was founded under the name "Ciudad de la Plata de la Nueva Toledo" (City of Silver of New Toledo) by Pedro Anzures, Marqués de Campo Redondo. In 1559, the Spanish King Philip II established the "Audiencia de Charcas" in La Plata with authority over an area which covers what is now Paraguay, southeastern Peru, Northern Chile and Argentina, and much of Bolivia. The "Audiencia de Charcas" was a subdivision of the Viceroyalty of Peru until 1776, when it was transferred to the newly created Viceroyalty of the Río de la Plata. In 1601 the Recoleta Monastery was founded by the Franciscans and in 1609 an archbishopric was founded in the city. In 1624 St Francis Xavier University of Chuquisaca was founded.
Very much a Spanish city during the colonial era, the narrow streets of the city centre are organised in a grid, reflecting the Andalusian culture that is embodied in the architecture of the city's great houses and numerous convents and churches. Sucre remains the seat of the Roman Catholic Church in Bolivia, and a common sight is members of religious orders dressed in traditional costume. For much of its colonial history, Sucre's temperate climate was preferred by the Spanish royalty and wealthy families involved in silver trade coming from Potosí. Testament to this is the Glorieta Castle. Sucre's University (Universidad Mayor Real y Pontificia de San Francisco Xavier de Chuquisaca) is one of the oldest universities in the new world.
On May 25, 1809 the Bolivian independence movement was started with the ringing of the bell of the Basilica of Saint Francisco. This bell was rung to the point of breakage, but it can still be found in the Basilica today: it is one of the most precious relics of the city.
Until the 19th century, La Plata was the judicial, religious and cultural centre of the region. It was proclaimed provisional capital of the newly independent Alto Peru (later, Bolivia) on July 1826. On July 12, 1839, President José Miguel de Velasco proclaimed a law naming the city as the capital of Bolivia, and renaming it in honor of the revolutionary leader Antonio José de Sucre. Sucre, after the economic decline of Potosí and its silver industry, saw the Bolivian seat of government move to La Paz in 1898. Many argue Sucre was the location of the beginning of the Latin American independence movement against Spain. The first "Grito Libertario" (Shout for Freedom) in any Western Hemisphere Spanish colony is said to have taken place in Sucre in 1809. Ironically from that point of view, Bolivia was the last Spanish imperial territory in South America to gain its independence, in 1825. In 1991 Sucre became a UNESCO World Heritage Site.
The city attracts thousands of tourists every year due to its well-preserved downtown with buildings from the 18th and 19th centuries. Nestled at the foot of the twin hills of Churuquella and Sika Sika, Sucre is the gateway to numerous small villages that date from the colonial era, the most well-known of which is Tarabuco, home of the colorful "Pujllay" festival held each March. Most of these villagers are members of one of the indigenous ethnicities. Many dress in clothing distinctive to their respective villages.
Government.
Sucre is the capital of Chuquisaca department and the capital of Bolivia, where the Supreme Court is located. The government of the City of Sucre is divided into the executive and legislative branches. The Mayor of Sucre is the head of the city government, elected for a term of five years by general election. The legislative branch consists of the Municipal Council, which elects a President, Vice President and Secretary from a group of eleven members.
The current mayor of Sucre is Moisés Torres Chivé, who was elected in the special municipal election on 18 December 2011 and sworn in January 2012. The mayor elect is Iván Arciénega, who defeated former mayor Jaime Barrón on March 30, 2015; he is scheduled to take office on May 25.
The current Municipal Council was elected in the regional election of April 4, 2010. The election was by proportional representation with the Pact of Social Integration and the Movement Towards Socialism gaining the largest and second largest shares of the vote.
The council elected in April 2010 and seated in late December 2010 is as follows:
Geography and territorial organization.
Sucre is divided into eight numbered districts: the first five of these are urban districts, while Districts 6, 7, and 8 are rural districts. Each is administered by a Sub-Mayor (Spanish: "Subalcalde"), appointed by the Mayor of Sucre. The rural districts include numerous rural communities outside the urban area.
Sucre is served by Juana Azurduy de Padilla International Airport, situated 5 km to the Northwest and connected by Avenida Juana Azurduy de Padilla.
Climate.
Sucre has a subtropical highland climate (Köppen: Cwb), with mild temperatures year round.
The highest record temperature was 34.7 C while the lowest record temperature was -6 C 
The City of Four Names.
Each of the well known names represent a specific era of the city's history.
Sports.
Sucre has the most important sport facilities in Bolivia, and the most practiced sport in the city is football. Sucre has the second-biggest football and Olympic stadium in the country, the Estadio Patria. It is the home ground of Sucre's first-division team in the Bolivian professional league, Universitario, the 2008 champions.
Other sports are also practiced, such as swimming at la Piscina Bolivariana, basketball at numerous courts around the city, as well as taekwondo, kung fu, volleyball, tennis and racquetball.
Architecture.
The city of Sucre contains many old and classic buildings:
The House of Freedom.
Built in 1621, it is perhaps the most important building of the nation. The republic was founded in this building by Simón Bolívar who wrote the Bolivian Constitution.
The "Salón de la Independencia" houses the Bolivian Declaration of Independence.
National Library.
Built on the same year of the foundation of the Republic, it is the first and the most important historical, bibliographical and documentation center of the country. The National Library has documents that date from 90th century.
Metropolitan Cathedral.
Built between 1559 to 1712, the cathedral has the "Museo Catedraliceo" which is the first and most important religious museum of the country. The "Pinacoteca" has a vast collection of paintings by Colonial and Republican masters and also by Europeans such as Bitti, Fourchaudt and Van Dyck. The Cathedral contains a vast amount of jewelry made of gold, silver and gemstones.
Archbishop's Palace.
Built in 1609, was an important religious and historic institution during colonial times.
Departmental Autonomous Government of Chuquisaca.
One of the best buildings of republican architecture, this was completed in 1896. It was the first Palace of Government of Bolivia but when the government was moved to La Paz it became the Chuquisaca Governorship Palace.
Supreme Court of Justice.
On July 16, 1827 the Supreme Court of the Nation was established. Its first president was Dr. Manuel Maria Urcullo. Others prominent in its history include Dr. Pantaleon Dalence, who was twice president of the Supreme Court and through his qualities became known as the 'Father of Bolivian Justice'. This institution was installed in several places before moving to its current building. It was designed in the neoclassical style under the canons of French academicism and was inaugurated on May 25, 1945.
General Cemetery.
Constructive order and harmony predominates in the different areas, some of which date from the late nineteenth century. Ornate mausoleums, tombs and gardens with magnificent old trees populate the space that is home to the graves of important people in the arts, sciences and the history both of Bolivia and of Latin America. Because of the tranquility offered by the site, many students choose to study here.

</doc>
<doc id="43001" url="http://en.wikipedia.org/wiki?curid=43001" title="Guar gum">
Guar gum

Guar gum, also called guaran, is a galactomannan. It is primarily the ground endosperm of guar beans. The guar seeds are dehusked, milled and screened to obtain the guar gum. It is typically produced as a free-flowing, off-white powder.
Production and trade.
The guar bean is principally grown in India, Pakistan, US, Australia and Africa. India produces 2.5 - 3.5 million tons of guar annually, making it the largest producer with about 80% of world production. 
In India, Rajasthan, Gujarat and Haryana are the main producing regions, and Jodhpur, Sri Ganganagar and Hanumangarh in Rajasthan are the major Guar trading markets. Hanumangarh - Ganganagar is the main production area for best quality guar beans, so called "Fast-Hydrating Guar".
The United States has produced 4,600 to 14,000 tonnes of guar over the last 5 years. As many as 50,000 acres of guar have been grown in West Texas over the last 50 years.
The world production for guar gum and its derivatives is about 1.0 Million tonnes. Industrial guar gum accounts for about 70% of the total demand. Mainly it is used as a proppant transport / proppant suspending agent in Hydraulic Fracturing Process. In 2012 guar prices increased by 900-1000%. The main reason for this large scale price rise was the inventory build up by companies like Halliburton and Schlumberger, amidst the fear of shortage of guar gum for drilling due to ongoing drought in Rajasthan. 2013 was a strong year for guar sowing and production in India.In 2013 the export of Guar and Guar Gum was so strong that this humble cash crop of India topped the agro export commodity list of INDIA. The total sowing area rose by 21 percent in 2013 to reach 10.6 million acres. Rajasthan, Haryana, and Gujarat – the three key guar-producing states – exceeded the sowing area target set by their respective agriculture departments. Non-traditional guar cultivators in other Indian states also showed keen interest in the crop in 2013.
Current guar gum rates can be found online at http://www.agriwatch.com/guar/guar-gum/ 
Properties.
Chemical composition.
Chemically, guar gum is a polysaccharide composed of the sugars galactose and mannose. The backbone is a linear chain of β 1,4-linked mannose residues to which galactose residues are 1,6-linked at every second mannose, forming short side-branches.
Solubility and viscosity.
Guar gum is more soluble than locust bean gum and is a better stabilizer, as it has more galactose branch points. Unlike locust bean gum, it is not self-gelling. However, either borax or calcium can cross-link guar gum, causing it to gel. In water, it is nonionic and hydrocolloidal. It is not affected by ionic strength or pH, but will degrade at extreme pH and temperature (e.g. pH 3 at 50 °C). It remains stable in solution over pH range 5-7. Strong acids cause hydrolysis and loss of viscosity, and alkalies in strong concentration also tend to reduce viscosity. It is insoluble in most hydrocarbon solvents.
The viscosity attained is dependent on time, temperature, concentration, pH, rate of agitation and practical size of the powdered gum used. The lower the temperature lower the rate at which viscosity increases and the lower the final viscosity.
Above 80º the final viscosity is slightly reduced. The finer guar powders swells more rapidly than coarse powdered gum.
the -OH structure forms crosslinking epoxy linkage, which gives guar its unique characteristic property 
Guar gum shows high low-shear viscosity but is strongly shear-thinning. It is very thixotropic above 1% concentration, but below 0.3%, the thixotropy is slight. It has much greater low-shear viscosity than that of locust bean gum, and also generally greater than that of other hydrocolloids. Guar gum shows viscosity synergy with xanthan gum. Guar gum and micellar casein mixtures can be slightly thixotropic if a biphase system forms.
Thickening.
Guar gum is economical because it has almost eight times the water-thickening potency of cornstarch - only a very small quantity is needed for producing sufficient viscosity. Thus, it can be used in various multiphase formulations: as an emulsifier because it helps to prevent oil droplets from coalescing, and/or as a stabilizer because it helps to prevent solid particles from settling.
Ice crystal growth.
Guar gum retards ice crystal growth nonspecifically by slowing mass transfer across the solid/liquid interface. It shows good stability during freeze-thaw cycles.It is therefore used in gluten free ice cream. Synergies with other ingredients: Has synergistic effects with locust bean gum and sodium alginate. May be synergistic with xanthan. Use together with Xanthan for thicker results (0.5% Guar Gum / 0.35% Xanthan Gum) in applications such as soups that do not require clear results.Guar gum is a hydrocolloid particularly useful for making thick pastes without forming a gel and for keeping water bound in a sauce or emulsion. Guar gum can be used for thickening cold and hot liquids, to make hot gels, light foams and as an emulsion stabilizer. Guar gum can be used for cottage cheese ie PANEER,DAHI ie curd or yougart, sauces, soups and frozen desserts. Guar gum is also a good source of fiber with 80
% soluble dietary fiber on a dry weight basis. 
Grading.
Guar gum is analysed for
Guar gum powder standards are:
Manufacturing process.
Depending upon the requirement of end product, various processing techniques are used. The commercial production of guar gum normally uses roasting, differential attrition, sieving, and polishing.
Food-grade guar gum is manufactured in stages. Guar split selection is important in this process. The split is screened to clean it and then soaked to prehydrate it in a double-cone mixer. The prehydrating stage is very important because it determines the rate of hydration of the final product.
The soaked splits, which have reasonably high moisture content, are passed through a flaker. The flaked guar split is ground and then dried. The powder is screened through rotary screens to deliver the required particle size. Oversize particles are either recycled to main ultra fine or reground in a separate regrind plant, according to the viscosity requirement.
This stage helps to reduce the load at the grinder. The soaked splits are difficult to grind. Direct grinding of those generates more heat in the grinder, which is not desired in the process, as it reduces the hydration of the product. Through the heating, grinding, and polishing process, the husk is separated from the endosperm halves and the refined guar split is obtained. Through the further grinding process, the refined guar split is then treated and converted into powder.
The split manufacturing process yields husk and germ called “guar meal”, widely sold in the international market as cattle feed. It is high in protein and contains oil and albuminoids, about 50% in germ and about 25% in husks. The quality of the food-grade guar gum powder is defined from its particle size, rate of hydration, and microbial content.
Manufacturers define different grades and qualities of guar gum by the particle size, the viscosity generated with a given concentration, and the rate at which that viscosity develops. Coarse-mesh guar gums will typically, but not always, develop viscosity more slowly. They may achieve a reasonably high viscosity, but will take longer to achieve. On the other hand, they will disperse better than fine-mesh, all conditions being equal. A finer mesh, such as a 200 mesh, requires more effort to dissolve.
Modified forms of guar gum are available commercially, including enzyme-modified, cationic and hydropropyl guar.
Food applications.
The largest market for guar gum is in the food industry. In the US, differing percentages are set for its allowable concentration in various food applications. In Europe, guar gum has EU food additive code E412. Xanthan gum and guar gum are the most frequently used gums in gluten-free recipes and gluten-free products.
Applications include:
Nutritional and medicinal effects.
Guar gum, as a water-soluble fiber, acts as a bulk-forming laxative, so is claimed to be effective in promoting regular bowel movements and relieving constipation and chronic related functional bowel ailments, such as diverticulosis, Crohn's disease, colitis and irritable bowel syndrome.
Several studies have found significant decreases in human serum cholesterol levels following guar gum ingestion. These decreases are thought to be a function of its high soluble fiber content.
Guar gum has been considered of interest in regard to both weight loss and diabetic diets. It is a thermogenic substance. Moreover, its low digestibility lends its use in recipes as a filler, which can help to provide satiety, or slow the digestion of a meal, thus lowering the glycemic index of that meal. In the late 1980s, guar gum was used and heavily promoted in several weight-loss products. The US Food and Drug Administration eventually recalled these due to reports of esophageal blockage from insufficient fluid intake, after one brand alone caused at least 10 users to be hospitalized, and a death. For this reason, guar gum is no longer approved for use in over-the-counter weight loss aids in the United States. Moreover, a meta-analysis combining the results of 11 randomized, controlled trials found guar gum supplements were not effective in reducing body weight.
Two Japanese studies using rats showed guar gum supports increased absorption of calcium occurring in the colon instead of in the small intestine. This means lesser amounts of calcium may be consumed to obtain its recommended minimum daily intake. This has obvious implications for reduced calorie diets, since some calcium-rich dairy products tend to be high in calories.
Guar gum, though, is also capable of reducing the absorbability of dietary minerals (other than calcium), when foods or nutritional supplements containing them are consumed concomitantly with it, but this is less of a concern with guar gum than with various insoluble dietary fibers.
Some studies have found guar gum to improve dietary glucose tolerance. Research has revealed the water-soluble fiber in it may help people with diabetes by slowing the absorption of sugars by the small intestine. Although the rate of absorption is reduced, the amount of sugar absorbed is the same overall. This may help diabetic patients by moderating glucose "spikes".
Allergies.
Some studies have found an allergic sensitivity to guar gum developed in a few individuals working in an industrial environment where airborne concentrations of the substance were present. In those affected by the inhalation of the airborne particles, common adverse reactions were occupational rhinitis and asthma.
Soy protein occurs as an impurity in manufactured guar gum, and can make up as much as 10%. The guar gum can therefore adversely affect those with sensitivity to soy.
A few individuals are allergic to guar gum and experience reactions such as flushing, itchiness and diarrhea. Consumption for allergic individuals is also known to cause small sores or lesions. 
Dioxin contamination.
In July 2007, the European Commission issued a health warning to its member states after high levels of dioxins were detected in a food additive - guar gum - used as thickener in small quantities in meat, dairy, dessert or delicatessen products. The source was traced to guar gum from India that was contaminated with pentachlorophenol, a pesticide no longer in use. PCP contains dioxins as contamination. Dioxins damage the human immune system.

</doc>
<doc id="43005" url="http://en.wikipedia.org/wiki?curid=43005" title="Rio Grande">
Rio Grande

The Rio Grande ( or ; Spanish: Río Bravo del Norte, ] or simply Río Bravo) is a river that flows from south central Colorado in the United States to the Gulf of Mexico. Along the way it forms part of the Mexico–United States border. According to the International Boundary and Water Commission, its total length was 1896 mi in the late 1980s, though course shifts occasionally result in length changes. Depending on how it is measured, the Rio Grande is the fourth or fifth longest river system in North America.
The river serves as a natural border between the U.S. state of Texas and the Mexican states of Chihuahua, Coahuila, Nuevo León, and Tamaulipas. A very short stretch of the river serves as the boundary between the U.S. states of Texas and New Mexico. Since the mid–20th century, heavy water consumption of farms and cities along with many large hydroelectric dams on the river has left only 20 percent of its natural discharge to flow to the Gulf. Near the river's mouth, the heavily irrigated Rio Grande Valley is an important agricultural region. The Rio Grande is one of 19 Great Waters recognized by America's Great Waters Coalition.
The Rio Grande's watershed covers 182200 sqmi. Many endorheic basins are situated within, or adjacent to, the Rio Grande's basin, and these are sometimes included in the river basin's total area, increasing its size to about 336000 sqmi.
Geography.
The Rio Grande rises in the western part of the Rio Grande National Forest in the U.S. state of Colorado. The river is formed by the joining of several streams at the base of Canby Mountain in the San Juan Mountains, just east of the Continental Divide. From there, it flows through the San Luis Valley, then south into New Mexico, passing through Española, Albuquerque, and Las Cruces to El Paso, Texas and Ciudad Juárez, Chihuahua. Below El Paso it serves as part of the border between the United States and Mexico.
The official river border measurement ranges from 889 mi to 1248 mi, depending on how the river is measured. A major tributary, the Rio Conchos, enters at Ojinaga, Chihuahua, below El Paso, and supplies most of the water in the border segment. Other well-known tributaries include the Pecos and the smaller Devils, which join the Rio Grande on the site of Amistad Dam. Despite its name and length, the Rio Grande is not navigable by ocean-going ships, nor do smaller passenger boats or cargo barges use it as a route. It is barely navigable at all, except by small boats in a few places.
The Rio Grande rises in high mountains and flows for much of its length at high elevation; El Paso is 3762 ft above sea level. In New Mexico, the river flows through the Rio Grande rift from one sediment-filled basin to another, cutting canyons between the basins and supporting a fragile "bosque" ecosystem on its flood plain. From El Paso eastward, the river flows through desert. Only in the sub-tropical lower Rio Grande Valley is there extensive irrigated agriculture. The river ends in a small sandy delta at the Gulf of Mexico. During portions of 2001 and 2002 the mouth of the Rio Grande was blocked by a sandbar. In the fall of 2003 the sandbar was cleared by high river flows of about 7063 cuft/s.
Navigation.
Navigation was active during much of the 19th century, with over 200 different steamboats operating between the river's mouth close to Brownsville, and Rio Grande City, Texas. Many steamboats from the Ohio and Mississippi rivers were requisitioned by the US Government and moved to the Rio Grande during the Mexican War in 1846. They provided transport for the U.S. Army, under General Zachary Taylor to invade Monterrey, Mexico, via Camargo Municipality, Tamaulipas. Army engineers recommended that with small improvements the river could easily be made navigable as far north as El Paso. Those recommendations were never acted upon.
The Brownsville & Matamoros International Bridge is a large swing bridge that dates back to 1910 and is still in use today by automobiles and railroad trains, connecting Brownsville, Texas with Matamoros, Tamaulipas. It has not been opened since the early 1900s however, when the last of the big steamboats disappeared. The bridge is now operated by the Brownsville and Matamoros Bridge Company, a joint venture between the Mexican government and the Union Pacific Railroad.
At the mouth of the Rio Grande, on the Mexican side, was the large commercial port of Bagdad. During the American Civil War, this was the only legitimate port of the Confederacy. European warships anchored offshore to maintain the port's neutrality, and managed to do so successfully throughout that conflict, despite occasional stare downs with blockading ships from the US Navy. It was a shallow draft river port, with several smaller vessels that hauled cargo to and from the deeper draft cargo ships anchored off shore. These deeper draft ships could not cross the shallow sandbar at the mouth of the river. The port's commerce was European military supplies, in exchange for bales of cotton.
History.
In the 1800s, the river marked the disputed border between Mexico and the nascent Republic of Texas; Mexico marked the border at the Nueces River. The disagreement provided part of the rationale for the US invasion of Mexico in 1846, after Texas had been admitted as a new state. Since 1848, the Rio Grande has marked the boundary between Mexico and the United States from the twin cities of El Paso, Texas, and Ciudad Juárez, Chihuahua, to the Gulf of Mexico. As such, crossing the river was the escape route used by some Texan slaves to seek freedom. Mexico had liberal colonization policies and had abolished slavery in 1828.
In 1944 the US and Mexico signed a treaty regarding the river, and in 1997 the US designated the Rio Grande as one of the American Heritage Rivers. Two portions of the Rio Grande are designated National Wild and Scenic Rivers System, one in northern New Mexico and the other in Texas, at Big Bend National Park.
In the summer of 2001, a 328 ft wide sandbar formed at the mouth of the river, marking the first time in recorded history that the Rio Grande failed to empty into the Gulf of Mexico. The sandbar was subsequently dredged, but it re-formed almost immediately. Spring rains the following year flushed the re-formed sandbar out to sea, but it returned in the summer of 2002. As of the fall of 2003, the river once again reaches the Gulf.
River modifications.
The United States and Mexico share the water of the river under a series of agreements administered by the International Boundary and Water Commission,US-Mexico. The most notable of these treaties were signed in 1906 and 1944. The IBWC traces its institutional roots to 1889, when the International Boundary Committee was established to maintain the border. The IBWC today also allocates river waters between the two nations, and provides for flood control and water sanitation.
Use of that water belonging to the United States is regulated by the Rio Grande Compact, an interstate pact between Colorado, New Mexico, and Texas. The water of the Rio Grande is over-appropriated: that is, there are more users for the water than there is water in the river. Because of both drought and overuse, the section from El Paso downstream through Ojinaga was recently tagged "The Forgotten River" by those wishing to bring attention to the river's deteriorated condition.
There are a number of dams on the Rio Grande, including Rio Grande Dam, Cochiti Dam, Elephant Butte Dam, Caballo Dam, Amistad Dam, Falcon Dam, Anzalduas Dam, and Retamal Dam. In southern New Mexico and the upper portion of the Texas border segment, the river's discharge dwindles. Diversions, mainly for agricultural irrigation, have increased the natural decrease in flow such that by the time the river reaches Presidio, Texas, there is little or no water. Below Presidio the Rio Conchos restores the flow of water. Near Presidio the river's discharge is frequently zero. Its average discharge is 178 cuft/s, down from 945 cuft/s at Elephant Butte Dam. Supplemented by other tributaries the Rio Grande's discharge increases to its maximum annual average of 3504 cuft/s near Rio Grande City, Texas. Large diversions for irrigation below Rio Grande City reduce the river's average flow to 889 cuft/s at Brownsville and Matamoros.
Crossings.
The major international border crossings along the river are at Ciudad Juárez and El Paso; Presidio, Texas, and Ojinaga, Chihuahua; Laredo, Texas, and Nuevo Laredo, Tamaulipas; McAllen, Texas, and Reynosa, Tamaulipas; and Brownsville, Texas, and Matamoros, Tamaulipas. Other notable border towns are the Texas/Coahuila pairings of Del Rio–Ciudad Acuña and Eagle Pass–Piedras Negras.
Names and pronunciation.
"Río Grande" is Spanish for "Big River" and "Río Grande del Norte" means "Great River of the North". In English, Rio Grande is pronounced either or . Because "río" means "river" in Spanish, the phrase "Rio Grande River" is redundant.
In Mexico it is known as Río Bravo or "Río Bravo del Norte", "bravo" meaning (among other things) "furious" or "agitated".
Historically, the Pueblo and Navajo peoples also had names for the Rio Grande/Rio Bravo:
The four Pueblo names likely predated the Spanish entrada by several centuries.
"Rio del Norte" was most commonly used for the upper Rio Grande (roughly, within the present-day borders of New Mexico) from Spanish colonial times to the end of the Mexican period in the mid-19th century. This use was first documented by the Spanish in 1582. Early American settlers in south Texas began to use the modern 'English' name "Rio Grande". By the late 19th century, in the United States, the name Rio Grande had become standard in being applied to the entire river, from Colorado to the sea.
By 1602, "Río Bravo" had become the standard Spanish name for the lower river, below its confluence with the Rio Conchos.
Tributaries.
The largest tributary of the Rio Grande by discharge is the Rio Conchos, which contributes almost twice as much water as any other. In terms of drainage basin size the Pecos River is the largest.

</doc>
<doc id="43006" url="http://en.wikipedia.org/wiki?curid=43006" title="Britpop">
Britpop

Britpop is a subgenre of rock and pop music that originated in the United Kingdom. Britpop emerged from the British independent music scene of the early 1990s and was characterised by bands influenced by British guitar pop music of the 1960s and 1970s and indie rock from the 1980s, notably The Smiths. Britpop focused on bands, singing in regional British accents and making references to British places and British culture, particularly working class culture. The movement developed as a reaction against various musical and cultural trends in the late 1980s and early 1990s, particularly the grunge phenomenon from the United States.
In the wake of the musical invasion into the United Kingdom of American grunge bands, new British groups such as Suede and Blur launched the movement by positioning themselves as opposing musical forces, referencing British guitar music of the past and writing about uniquely British topics and concerns. These bands were soon joined by others including Oasis, The Verve, Pulp, Placebo, Supergrass, Cast, Space, Sleeper and Elastica.
Britpop groups brought British alternative rock into the mainstream and formed the backbone of a larger British cultural movement called Cool Britannia. A chart battle between Blur and Oasis dubbed "The Battle of Britpop" brought Britpop to the forefront of the British press in 1995. By 1997, however, the movement began to slow down, many acts began to falter and broke up. The popularity of the pop group the Spice Girls captured the "spirit of the age from those responsible for Britpop." Although its more popular bands were able to spread their commercial success overseas, especially to the United States, the movement largely fell apart by the end of the decade.
Style, roots and influences.
Britpop bands were influenced by British guitar music of the past, particularly movements and genres such as the British Invasion, glam rock, and punk rock. Specific influences varied: Blur and Oasis drew from The Kinks and The Beatles, respectively, while Elastica had a fondness for arty punk rock. Regardless, all Britpop artists projected a sense of reverence for the sounds of the past.
Alternative rock acts from the 1980s and early 1990s indie scene were the direct ancestors of the Britpop movement. The influence of The Smiths was common to the majority of Britpop artists. The Madchester scene, fronted by The Stone Roses, Happy Mondays and Inspiral Carpets (for whom Oasis's Noel Gallagher had worked as a roadie during the Madchester years), was the immediate root of Britpop since its emphasis on good times and catchy songs provided an alternative to the alternative rock style known as shoegazing.
Britpop groups were defined by being focused on bands rather than solo artists; having drums/bass/guitar/vocals (and sometimes keyboards) line-ups; writing original material and playing instruments themselves; singing in regional British accents; references to British places and culture in lyrics and image; and fashion consciousness. Stylistically, Britpop bands relied on catchy hooks and wrote lyrics that were meant to be relevant to British young people of their own generation. Britpop bands conversely denounced grunge as irrelevant and having nothing to say about their lives. Damon Albarn of Blur summed up the attitude in 1993 when after being asked if Blur were an "anti-grunge band" he said, "Well, that's good. If punk was about getting rid of hippies, then I'm getting rid of grunge." In spite of the professed disdain for the genres, some elements of both crept into the more enduring facets of Britpop. Noel Gallagher has since championed Ride. Noel Gallagher stated in a 1996 interview that Nirvana's Kurt Cobain was the only songwriter he had respect for in the last ten years, and that he felt their music was similar enough that Cobain could have written "Wonderwall".
The imagery associated with Britpop was equally British and working class. Music critic Jon Savage asserted that Britpop was "an outer-suburban, middle-class fantasy of central London streetlife, with exclusively metropolitan models." A rise in unabashed maleness, exemplified by "Loaded" magazine and lad culture in general, would be very much part of the Britpop era. The Union Jack also became a prominent symbol of the movement (as it had a generation earlier with mod bands such as The Who) and its use as a symbol of pride and nationalism contrasted deeply with the controversy that erupted just a few years before when former Smiths singer Morrissey performed draped in it. The emphasis on British reference points made it difficult for the genre to achieve success in the US.
History.
Origins and first years.
The origins of Britpop lie primarily in the indie scene of the early 1990s, and in particular around a group of bands involved in a vibrant social scene focused in the Camden Town area of London. This scene was dubbed "The Scene That Celebrates Itself" by "Melody Maker". Some members of this scene (Blur, Lush, Suede) would go on to play a leading part in Britpop. Others such as Kingmaker, Slowdive, Spitfire and Ride would not. The dominant musical force of the period was the grunge invasion from the United States, which filled the void left in the indie scene by The Stone Roses' inactivity.
Journalist John Harris has suggested that Britpop began when Blur's single "Popscene" and Suede's "The Drowners" were released around the same time in the spring of 1992. He stated, "[I]f Britpop started anywhere, it was the deluge of acclaim that greeted Suede's first records: all of them audacious, successful and very, very British". Suede were the first of the new crop of guitar-orientated bands to be embraced by the UK music media as Britain's answer to Seattle's grunge sound. Their debut album "Suede" became the fastest-selling debut album in the history of the UK. In April 1993, "Select" magazine featured Suede's lead singer Brett Anderson on the cover with a Union Flag in the background and the headline "Yanks go home!". The issue included features on Suede, The Auteurs, Denim, Saint Etienne and Pulp and helped foment the idea of an emerging movement.
Blur, a group that formerly mixed elements of shoegazing and baggy, took on an Anglocentric aesthetic with their second album "Modern Life Is Rubbish" (1993). Blur's new approach was inspired by their tour of the United States in the spring of 1992. During the tour, frontman Damon Albarn began to resent American culture and found the need to comment on that culture's influence seeping into Britain. Justine Frischmann, formerly of Suede and leader of Elastica (and at the time in a relationship with Damon Albarn) explained, "Damon and I felt like we were in the thick of it at that point [. . .] it occurred to us that Nirvana were out there, and people were very interested in American music, and there should be some sort of manifesto for the return of Britishness." John Harris wrote in an "NME" article just prior to the release of "Modern Life is Rubbish", "[Blur's] timing has been fortuitously perfect. Why? Because, as with baggies and shoegazers, loud, long-haired Americans have just found themselves condemned to the ignominious corner labeled 'yesterday's thing'". The music press also fixated on what the "NME" had dubbed the New Wave of New Wave, a term applied to the more punk-derivative acts such as Elastica, S*M*A*S*H and These Animal Men.
While "Modern Life Is Rubbish" was a moderate success, it was Blur's third album "Parklife" that made them arguably the most popular band in the UK in 1994. "Parklife" continued the fiercely British nature of its predecessor, and coupled with the death of Nirvana's Kurt Cobain in April of that year it seemed that British alternative rock had finally turned back the tide of grunge dominance. That same year Oasis released their debut album "Definitely Maybe", which broke Suede's record for fastest-selling debut album.
The movement was soon dubbed Britpop. The term "Britpop" had been used in the late 1980s (in "Sounds" magazine by journalist, Goldblade frontman and TV pundit John Robb referring to bands such as The La's, The Stone Roses, Inspiral Carpets and The Bridewell Taxis). "Britpop" arose around the same time as the term "Britart" (which referred to the work of British modern artists such as Damien Hirst). However, it would not be until 1994 when the term entered the popular consciousness, being used extensively by the music press and radio DJs. A rash of bands emerged aligned with the new movement. At the start of 1995 Britpop bands including Sleeper, Supergrass, and Menswear scored pop hits. Elastica released their debut album "Elastica" that March; its first week sales surpassed the record set by "Definitely Maybe" the previous year. The music press viewed the scene around Camden Town as a musical centre; frequented by Britpop groups like Blur, Elastica, and Menswear, "Melody Maker" declared "Camden is to 1995 what Seattle was to 1992, what Manchester was to 1989, and what Mr Blobby was to 1993."
Peak of success.
A chart battle between Blur and Oasis dubbed "The Battle of Britpop" brought Britpop to the forefront of the British press in 1995. The bands had initially praised each other but over the course of the year antagonisms between the two increased. Spurred on by the media, the groups became engaged in what the "NME" dubbed on the cover of its 12 August issue the "British Heavyweight Championship" with the pending release of Oasis' single "Roll with It", and Blur's "Country House" on the same day. The battle pitted the two bands against each other, with the conflict as much about British class and regional divisions as it was about music. Oasis were taken as representing the North of England, while Blur represented the South. The event caught the public's imagination and gained mass media attention in national newspapers, tabloids, and even the BBC News. The "NME" wrote about the phenomenon, "Yes, in a week where news leaked that Saddam Hussein was preparing nuclear weapons, everyday folks were still getting slaughtered in Bosnia and Mike Tyson was making his comeback, tabloids and broadsheets alike went Britpop crazy." Blur won the battle of the bands, selling 274,000 copies to Oasis' 216,000 - the songs charting at number one and number two respectively. However, in the long run Oasis became more commercially successful than Blur. Unlike Blur, Oasis were able to achieve sustained sales in the United States thanks to the singles "Wonderwall" and "Champagne Supernova". Oasis's second album "(What's the Story) Morning Glory?" (1995) eventually sold over four million copies in the UK, becoming the third best-selling album in British history.
By the summer of 1996 Oasis's prominence was such that "NME" termed a number of Britpop bands (including The Boo Radleys, Ocean Colour Scene and Cast) as "Noelrock", citing Gallagher's influence on their success. John Harris typified this wave of Britpop bands, and Gallagher, of sharing "a dewy-eyed love of the 1960s, a spurning of much beyond rock's most basic ingredients, and a belief in the supremacy of 'real music'". Starting on 10 August 1996, Oasis played a two-night set at Knebworth to a combined audience of 250,000 people, with one journalist commenting; "(Knebworth) could be seen as the last great Britpop performance; nothing after would match its scale." The demand for these gigs was and still is the largest ever for a concert on British soil; over 2.6 million people had applied for tickets.
Decline.
Oasis' third album "Be Here Now" (1997) was highly anticipated. Despite initially attracting positive reviews and selling strongly, the record was soon subjected to strong criticism from music critics, record-buyers and even Noel Gallagher himself for its overproduced and bloated sound. Music critic Jon Savage pinpointed "Be Here Now" as the moment where Britpop ended; Savage said that while the album "isn't the great disaster that everybody says," he noted that "[i]t was supposed to be the big, big triumphal record" of the period. At the same time, Damon Albarn sought to distance Blur from Britpop with the band's fifth album, "Blur" (1997). On guitarist Graham Coxon's suggestion, Blur moved away from their "Parklife"-era sound, and their music began to assimilate American lo-fi influences, particularly that of Pavement. Albarn explained to the "NME" in January 1997 that "We created a movement: as far as the lineage of British bands goes, there'll always be a place for us", but added, "We genuinely started to see that world in a slightly different way."
As the movement began to slow down, many acts began to falter and broke up. The popularity of the pop group the Spice Girls has been seen as having "snatched the spirit of the age from those responsible for Britpop." While established acts struggled, attention began to turn to the likes of Radiohead and The Verve, who had been previously overlooked by the British media. These two bands—in particular Radiohead—showed considerably more esoteric influences from the 1960s and 1970s, influences that were uncommon among earlier Britpop acts. In 1997, Radiohead and The Verve released their respective efforts "OK Computer" and "Urban Hymns", both of which were widely acclaimed. Post-Britpop bands like Travis, Stereophonics and Coldplay, influenced by Britpop acts, particularly Oasis, with more introspective lyrics, were some of the most successful rock acts of the late 1990s and early 2000s.

</doc>
<doc id="43007" url="http://en.wikipedia.org/wiki?curid=43007" title="Blur (band)">
Blur (band)

Blur are an English rock band, formed in London in 1988. The group consists of singer/keyboardist Damon Albarn, guitarist/singer Graham Coxon, bassist Alex James and drummer Dave Rowntree. Blur's debut album "Leisure" (1991) incorporated the sounds of Madchester and shoegazing. Following a stylistic change influenced by English guitar pop groups such as the Kinks, the Beatles and XTC, Blur released "Modern Life Is Rubbish" (1993), "Parklife" (1994) and "The Great Escape" (1995). As a result, the band helped to popularise the Britpop genre and achieved mass popularity in the UK, aided by a chart battle with rivals Oasis in 1995 dubbed the "Battle of Britpop".
In recording their follow-up, "Blur" (1997), the band underwent another reinvention, showing influence from the lo-fi style of American indie rock groups. The album, including the "Song 2" single, brought Blur mainstream success in the United States. Their next album, "13" (1999) saw the band members experimenting with electronic and gospel music, and featured more personal lyrics from Albarn. In May 2002, Coxon left Blur during the recording of their seventh album "Think Tank" (2003). Containing electronic sounds and more minimal guitar work, the album was marked by Albarn's growing interest in hip hop and African music. After a 2003 tour without Coxon, Blur did no studio work or touring as a band, as members engaged in other projects.
Blur reunited, with Coxon back in the fold, for a series of concerts in 2009. In the following years they released several singles and retrospective compilations, and toured internationally. In 2012, the group received a Brit Award for Outstanding Contribution to Music. Blur have released their first studio album in twelve years, "The Magic Whip", on 27 April 2015 promoting it with live performances on BBC TV programmes such as "Later... with Jools Holland" and "The Graham Norton Show".
History.
Formation and "Leisure": 1988–91.
Childhood friends Damon Albarn and Graham Coxon met Alex James when they began studying at London's Goldsmiths College in 1988. Albarn was in a group named Circus, who were joined by drummer Dave Rowntree that October. Circus requested the services of Coxon after the departure of their guitarist. That December, Circus fired two members and James joined as the group's bassist. This new group named themselves Seymour in December 1988, inspired by J.D. Salinger's "". The group performed live for the first time in summer 1989. In November, Food Records' A&R representative Andy Ross attended a Seymour performance that convinced him to court the group for his label. The only concern held by Ross and Food was that they disliked the band's name. Food drew up a list of alternatives, from which the group decided on "Blur". Food Records finally signed the newly christened band in March 1990.
From March to July 1990, Blur toured Britain, opening for the Cramps, and testing out new songs. In October 1990, after their tour was over, Blur released the "She's So High" single, which reached number 48 in the UK Singles Chart. The band had trouble creating a follow-up single, but they made progress when paired with producer Stephen Street. The resulting single release, "There's No Other Way", became a hit, peaking at number eight. As a result of the single's success, Blur became pop stars and were accepted into a clique of bands who frequented the Syndrome club in London dubbed "The Scene That Celebrates Itself". "NME" magazine wrote in 1991, "[Blur] are [the] acceptable pretty face of a whole clump of bands that have emerged since the whole Manchester thing started to run out of steam."
The band's third single, "Bang", performed relatively disappointingly, reaching only number 24. Andy Ross and Food owner David Balfe were convinced Blur's best course of action was to continue drawing influence from the Madchester genre. Blur attempted to expand their musical sound, but the recording of the group's debut album was hindered by Albarn having to write his lyrics in the studio. Although the resulting album "Leisure" (1991) peaked at number seven on the UK Albums Chart, it received mixed reviews, and according to journalist John Harris, "could not shake off the odour of anti-climax".
Britpop years: 1992–95.
After discovering they were £60,000 in debt, Blur toured the United States in 1992 in an attempt to recoup their financial losses. The group released the single "Popscene" to coincide with the start of the tour. Featuring "a rush of punk guitars, '60s pop hooks, blaring British horns, controlled fury, and postmodern humor", "Popscene" was a turning point for the band musically. However, upon its release it only charted at number 32. "We felt 'Popscene' was a big departure; a very, very English record", Albarn told the "NME" in 1993, "But that annoyed a lot of people ... We put ourselves out on a limb to pursue this English ideal and no-one was interested." As a result of the single's lacklustre performance, plans to release a single named "Never Clever" were scrapped and work on Blur's second album was pushed back.
During the two-month American tour, the band became increasingly unhappy, often venting frustrations on each other, leading to several physical confrontations. The band members were homesick; Albarn said, "I just started to miss really simple things ... I missed everything about England so I started writing songs which created an English atmosphere." Upon the group's return to the United Kingdom, Blur (Albarn in particular) were upset by the success rival group Suede had achieved while they were gone. After a poor performance at a 1992 gig that featured a well-received set by Suede on the same bill, Blur were in danger of being dropped by Food. By that time, Blur had undergone an ideological and image shift intended to celebrate their English heritage in contrast to the popularity of American grunge bands like Nirvana. Although sceptical of Albarn's new manifesto for Blur, Balfe gave assent for the band's choice of Andy Partridge (of XTC) to produce their follow-up to "Leisure". The sessions with Partridge proved unsatisfactory, but a chance reunion with Stephen Street resulted in him returning to produce the group.
Blur completed their second album "Modern Life Is Rubbish" in December 1992, but Food Records said the album required more potential hit singles and asked them to return to the studio for a second time. The band complied and Albarn wrote "For Tomorrow", which became the album's lead single. "For Tomorrow" was a minor success, reaching number 28 on the charts. "Modern Life Is Rubbish" was released in May 1993. The announcement of the album's release included a press photo which featured Blur, dressed in a mix of mod and skinhead attire, posing alongside a mastiff with the words "British Image 1" spraypainted behind them. At the time, such imagery was viewed as nationalistic and racially insensitive by the British music press; to quieten concerns, Blur released the "British Image 2" photo, which was "a camp restaging of a pre-war aristocratic tea party". "Modern Life Is Rubbish" peaked at number 15 on the British charts, but failed to break into the US "Billboard" 200, selling only 19,000 copies there.
The success of "Parklife" (1994) revived Blur's commercial fortunes. The album's first single, the disco-influenced "Girls & Boys", found favour on BBC Radio 1 and peaked at number 5 on the UK Singles Chart, and number 59 in the US "Billboard" Hot 100 where it remains the band's highest-charting single. "Parklife" entered the British charts at number one and stayed on the album charts for 90 weeks. Enthusiastically greeted by the music press—the "NME" called it "a Great Pop Record ... bigger, bolder, narkier and funnier [than "Modern Life is Rubbish"]"—"Parklife" is regarded as one of Britpop's defining records. Blur won four awards at the 1995 Brit Awards, including Best Band and Best Album for "Parklife". Coxon later pointed to "Parklife" as the moment when "[Blur] went from being regarded as an alternative, left field arty band to this amazing new pop sensation".
Blur began working on their fourth album "The Great Escape" at the start of 1995. Building upon the band's previous two albums, Albarn's lyrics for the album consisted of several third-person narratives. James reflected, "It was all more elaborate, more orchestral, more theatrical, and the lyrics were even more twisted ... It was all dysfunctional, misfit characters fucking up." The release of the album's lead single "Country House" played a part in Blur's public rivalry with Manchester band Oasis termed the "Battle of Britpop". Partly due to increasing antagonisms between the groups, Blur and Oasis ultimately decided to release their new singles on the same day, an event the "NME" called "The British Heavyweight Championship". The debate over which band would top the British singles chart became a media phenomenon, and Albarn appeared on the "News at Ten". At the end of the week, "Country House" ultimately outsold Oasis' "Roll With It" by 274,000 copies to 216,000, becoming Blur's first number one single.
"The Great Escape", which Albarn told the public was the last installment in the band's Life Trilogy, was released in September 1995 to ecstatic reviews. The "NME" hailed it as "spectacularly accomplished, sumptuous, heart-stopping and inspirational", while "Mojo" argued "Blur are the very best that '95 Britpop has to offer and this is a most gong-worthy sound, complete with head-slicing guitars, catchy tunes and very funny words". Entering the UK charts at number one, the album sold nearly half a million copies in its first month of sale. However, opinion quickly changed and Blur found themselves largely out of favour with the media once again. Following the worldwide success of Oasis' "(What's the Story) Morning Glory?" (which went quadruple platinum in America), the media quipped "[Blur] wound up winning the battle but losing the war." Blur became perceived as an "inauthentic middle class pop band" in comparison to the "working class heroes" Oasis, which Albarn said made him feel "stupid and confused". Alex James later summarised, "After being the People's Hero, Damon was the People's Prick for a short period ... basically, he was a loser – very publicly."
Reinvention after Britpop: 1996–2000.
An early 1996 "Q" magazine interview revealed that relations between Blur members had become very strained; journalist Adrian Deevoy wrote that he found them "on the verge of a nervous breakup". Coxon, in particular, began to resent his band mates: James for his playboy lifestyle, and Albarn for his control over Blur's musical direction and public image. The guitarist struggled with drinking problems and, in a rejection of the group's Britpop aesthetic, made a point of listening to noisy American alternative rock bands such as Pavement. In February 1996, when Coxon and James were absent for a lip-synced Blur performance broadcast on Italian television, they were replaced by a cardboard cutout and a roadie, respectively. Blur biographer Stuart Maconie later wrote that, at the time, "Blur were sewn together very awkwardly".
Although he had previously dismissed it, Albarn grew to appreciate Coxon's tastes in lo-fi and underground music, and recognised the need to significantly change Blur's musical direction once again. "I can sit at my piano and write brilliant observational pop songs all day long but you've got to move on", he said. He subsequently approached Street, and argued for a more stripped-down sound on the band's next record. Coxon, recognising his own personal need to—as Rowntree put it—"work this band", wrote a letter to Albarn, describing his desire for their music "to scare people again". After initial sessions in London, the band left to record the rest of the album in Iceland, away from the Britpop scene.
The result was "Blur", the band's fifth studio album, released in February 1997. Although the music press predicted that the lo-fi sonic experimentation would alienate Blur's teenage girl fan-base, they generally applauded the effort. Pointing out lyrics such as "Look inside America/She's alright", and noting Albarn's "obligatory nod to Beck, [and promotion of] the new Pavement album as if paid to do so", reviewers felt the band had come to accept American values during this time—an about-face of their attitude during the Britpop years. Despite cries of "commercial suicide", the album and its first single, "Beetlebum", debuted at number one in the UK. Although the album could not match the sales of their previous albums in the UK, "Blur" became the band's most successful internationally. In the US, the album received strong reviews; "Blur" reached number 61 on the "Billboard" 200 and was certified gold. The album's "Song 2" single was also popular on alternative radio, reaching number six on the Modern Rock chart. After it was licensed for use in various media—such as soundtracks, advertisements and television shows—"Song 2" became the most recognisable Blur song in the US. After the success of "Blur", the band embarked on a nine-month world tour.
In February 1998, a few months after completing the tour, Blur released "Bustin' + Dronin"' for the Japanese market. The album is a collection of Blur songs remixed by artists such as Thurston Moore, William Orbit and Moby. Among the tracks, the band were most impressed by Orbit's effort and enlisted him to replace Street as producer for their next album, citing a need to approach the recording process from a fresh perspective.
Released in March 1999, Blur's sixth studio album "13" saw them drift still further away from their Britpop-era attitude and sound. Orbit's production style allowed for more jamming, and incorporated a "variety of emotions, atmospheres, words and sounds" into the mix. "13" was creatively dominated by Coxon, who "was simply allowed to do whatever he chose, unedited", by Orbit. Albarn's lyrics—more heart-felt, personal and intimate than on previous occasions—were reflective of his break-up with Elastica frontwoman Justine Frischmann, his partner of eight years. The album received generally favourable reviews from the press. While "Q" called it "a dense, fascinating, idiosyncratic and accomplished art rock album", the "NME" felt it was inconsistent and "(at least) a quarter-of-an-hour too long". "13" debuted at the top of the UK charts, staying at that position for two weeks. The album's lead single, the gospel-based "Tender", opened at the second spot on the charts. After "Coffee & TV", the first Blur single to feature Coxon on lead vocals, managed to only reach number 11 in the UK, manager Chris Morrison demanded a chart re-run because of what he deemed was a sales miscalculation.
In July 1999, in celebration of their tenth anniversary, Blur released a 22-CD limited edition box-set of their singles. The accompanying tour saw Blur play the A-sides of the 22 singles in their chronological order of release. In October 2000, the group released the compilation ", which debuted at number three in the UK and received a Platinum certification for 300,000 copies shipped. Dismissed by the band as "the first record we have seen as product", the tracklisting and release dates of "Blur: The Best of" were determined on the basis of market research and focus groups conducted by Blur's record label, EMI. By this time, the group had largely disowned the upbeat pop singles from the Britpop era, and favoured the more arty, experimental work on "Blur" and "13". In an otherwise highly enthusiastic review of the best-of for the "NME", Steve Sutherland criticised the band's "sheer disregard" for their earlier work; "Just because these songs embarrassed them once they started listening to broadsheet critics and retreated wounded from the big-sales battle with Oasis doesn't mean that we're morons to love them."
Coxon's departure, "Think Tank" and hiatus: 2001–07.
After "13" and the subsequent tours in 1999–2000, band members pursued other projects. Graham Coxon recorded a string of solo albums, while Damon Albarn dedicated his time to Gorillaz, the animated band he had created with Jamie Hewlett. Alex James worked with Fat Les and co-wrote several songs with Sophie Ellis-Bextor and Marianne Faithfull.
Recording for Blur's next album began in London in November 2001, but concerted work started in June 2002, with the sessions moving to Marrakech, Morocco soon after, and then to Devon back in the UK. Not long after the sessions began, Coxon left the group. Coxon said "there were no rows" and "[the band] just recognised the feeling that we needed some time apart". Before the album was released, Blur released a new single, "Don't Bomb When You Are the Bomb" as a very limited white label release. The song is largely electronic, and was part of the band's protest against war in the Middle East. Albarn, however, attempted to assuage fans' fears that the album would be electronic by providing reassurances that the band's new album would be "a rockin' record", and also said that it has "a lot of finely crafted pop songs". Early in 2002, Blur recorded a song that would be played by European Space Agency's Beagle 2 lander once it touched down; however, attempts to locate the probe after it landed on Mars were fruitless.
"Think Tank", released in May 2003, was filled with atmospheric, brooding electronic sounds, featuring simpler guitar lines played by Albarn, and largely relying on other instruments to replace Coxon. The guitarist's absence also meant that "Think Tank" was almost entirely written by Albarn. Its sound was seen as a testament to Albarn's increasing interest in African and Middle Eastern music, and to his complete control over the group's creative direction. "Think Tank" was yet another UK number one and managed Blur's highest US position of number 56. The album was also nominated for best album at the 2004 Brit Awards. The band did a successful tour in 2003, anchored by former The Verve guitarist and keyboardist Simon Tong.
In 2005, XFM News reported that Blur would record an EP, and denied that they would hire a replacement guitarist for Coxon. There were also some aborted recordings made in 2005. Overall the band kept a low profile and did no studio or touring work as a three-piece. After Coxon significantly thawed on the subject of rejoining Blur, in 2007 band members announced that they would reunite, and that they intended to record together first in August, with the date later being pushed back to September, then October. Though the band members finally met up in October, they posted on their website that they had only "met up for an enjoyable lunch" and that there were no "other music plans for Blur".
Reunion and "The Magic Whip": 2008–present.
In December 2008, Blur announced they would reunite for a concert at London's Hyde Park on 3 July 2009. Days later, the band added a second date, for 2 July. A series of June preview shows were also announced, ending at Manchester Evening News arena on the 26th. All the shows were well received; "The Guardian"‍ '​s music critic Alexis Petridis gave their performance at Goldsmiths college a full five stars, and wrote "Blur's music seems to have potentiated by the passing of years ... they sound both more frenetic and punky and more nuanced and exploratory than they did at the height of their fame". Blur headlined the Glastonbury Festival on 28 June, where they played for the first time since their headline slot in 1998. Reviews of the Glastonbury performance were enthusiastic; "The Guardian" called them "the best Glastonbury headliners in an age". The band released their second greatest hits album " in June 2009.
Blur also headlined at other summer festivals, including Oxegen 2009 in Ireland, and the Scottish outdoor show of T in the Park. Their T in the Park headline slot was put in jeopardy after Graham Coxon was admitted to hospital with food poisoning. Ultimately, the band did play, albeit an hour and a half after they were scheduled to appear. After the completion of the reunion dates, James said the group had not discussed further plans, and Albarn told "Q" soon after that Blur had no intention of recording or touring again. He said, "I just can't do it anymore", and explained that the main motivation for participating in the reunion was to repair his relationship with Coxon, which he succeeded at. Coxon also said that no further Blur activity was planned, telling NME.com in September, "We're in touch and we say 'Wotcha' and all that but nothing has been mentioned about any more shows or anything else".
In January 2010, "No Distance Left to Run", a documentary about the band, was released in cinemas and a month later on DVD. In April 2010, Blur released their first new recording since 2003, "Fool's Day", for the Record Store Day event, as a vinyl record limited to 1000 copies; it was later made available as a free download on their website. "No Distance Left to Run" was nominated as Best Long Form Music Video for the 53rd Grammy Awards, Blur's first-ever Grammy nomination.
In February 2012, Blur were awarded the Outstanding Contribution to Music award at the 2012 Brit Awards. Later that month, Albarn and Coxon premiered a new track together live, "Under the Westway". In April, the band announced that a box-set entitled "Blur 21"—containing all seven Blur studio albums, four discs of unreleased rarities and three DVDs—would be released in July.
Blur entered the studio early that year to record material for a new album, but in May producer William Orbit told the "NME" that Albarn had halted recording. Blur's official Twitter and Facebook pages announced that the band would release two singles "The Puritan" and "Under the Westway" on 2 July. That August, Blur headlined a show at Hyde Park for the 2012 Summer Olympics closing ceremony. In 2013, the band performed at the Rock Werchter in Belgium, the Spanish and Portuguese dates of the Primavera Sound festival, and the Coachella Valley Music and Arts Festival in the United States.
In February 2015, Blur announced their first album in twelve years, "The Magic Whip", released on 27 April. Conceived over five days in Hong Kong after a cancelled Japan tour in 2013, the album was inspired by the city as well. "There's nothing pastoral about it", Albarn said, "it's very urban". "The Magic Whip" also marks the return of Coxon, absent on all but one track on "Think Tank", and Stephen Street, Blur's producer during the Britpop era.

</doc>
<doc id="43008" url="http://en.wikipedia.org/wiki?curid=43008" title="Supergrass">
Supergrass

Supergrass were an English alternative rock band from Oxford. The band consisted of brothers Gaz (guitar and lead vocals) and Rob Coombes (keyboards and backing vocals), Mick Quinn (bass and backing vocals) and Danny Goffey (drums and backing vocals).
Gaz Coombes, Mick Quinn and Danny Goffey formed Supergrass in 1993 in Oxford with Gaz's brother Rob Coombes officially joining the band in 2002. The band signed to Parlophone records in 1994 and produced "I Should Coco" (1995), the biggest selling debut album for the label since the Beatles' "Please Please Me". Their first album's fourth single "Alright" was a huge international hit that established the band's reputation. Since then the band have released five albums: "In It for the Money" (1997), "Supergrass" (1999), "Life on Other Planets" (2002), "Road to Rouen" (2005) and "Diamond Hoo Ha" (2008), as well as a decade-ending compilation called "Supergrass is 10" (2004).
In August 2009 they signed to Cooking Vinyl and began work on their seventh studio album "Release the Drones". It remained unreleased and unfinished as, on 12 April 2010, the band announced that it was splitting up due to musical and creative differences. The group disbanded after four farewell gigs, the final one at La Cigale, Paris on 11 June 2010.
History.
The Jennifers and formation (1990–1993).
At the age of 16 and 18 respectively Gaz Coombes and Danny Goffey were members of shoegaze band The Jennifers along with Nick Goffey and Andy Davis. The band played gigs at various venues around Oxfordshire, often public houses and clubs. One pub the band played at was the Jericho Tavern in Oxford. The band enjoyed enough success to release one single in 1992, "Just Got Back Today", on Nude Records before they disbanded.
When Coombes began working at the local Harvester he befriended co-worker Mick Quinn. The two realised they had common music interests and Coombes invited Quinn to come and jam with himself and Goffey. In February 1993 they formed Theodore Supergrass, "for about two months" Quinn explains, "then we realized that Theodore was a bit rubbish so we took that off."
Goffey claims that the name was his idea and says; "Although the others will dispute it, it was me. We were Theodore Supergrass and the idea was the band would be a little black character, and we wouldn't ever have to do interviews. We'd get the questions in advance, script the answers and then animate Theodore Supergrass answering them. But it cost too much money."
Gaz's brother, Rob Coombes, played flute for the band's début gig at the Co-Op Hall, Oxford in 1993. In January 1995 he first performed as keyboardist with the band for a live Radio 1 John Peel session. His role in the band progressed over the years, post I Should Coco material is credited to "Supergrass and Rob Coombes", however, he wasn't introduced as a band member until almost a decade later.
Britpop years and stardom (1994–1998).
In mid-1994, Supergrass issued their debut single "Caught by the Fuzz" on the small independent local label Backbeat Records. The song recounts lead singer and guitarist Gaz Coombes's experience of being arrested by the police for possession of cannabis. The limited release of vinyl copies sold out quickly, thanks in part to support from John Peel on his Radio One show. The Parlophone label signed the band and re-released the single in the autumn of the same year. It achieved the rare feat of both NME and Melody Maker "Single Of The Week" status in the same week.
"Mansize Rooster", released in February 1995, peaked at number 20 in the UK Single Charts and "Lenny", the band's first top 10 single. "Lenny" was followed soon afterwards by the band's debut album, "I Should Coco" (May 1995), which entered the UK Album Charts at number one. It achieved a half-a-million sales in the UK and over a million worldwide. "NME" reviewer Steve Sutherland gave the album a nine out of ten rating, writing, "These freaks shall inherit the earth." The album's fourth single, the double A-side release "Alright"/"Time", stayed in British Top Three for a month, peaking at number two.
Supergrass followed "I Should Coco" with 18 months of heavy touring, appearing at festivals such as Scotland's "T In The Park" and the "Glastonbury Festival". After Performing in Rio's "Hollywood Rock Festival" in April 1996, Supergrass met the infamous train robber Ronnie Biggs, and apparently said to him, "I was frightened for my life when I heard there was a supergrass in the area." A photograph of Ronnie Biggs and Gaz together was subsequently included in the music video for their 1996 single "Going Out". Recorded at Great Linford Manor the single peaked at number five in the UK charts, but was the last song produced by Sam Williams. Supergrass returned to Sawmills studio to co-produce follow up album, "In It For The Money" (released April 1997), with John Cornfield. The album was a huge success, and went platinum in the UK, but confused some fans expecting I Should Coco part 2. The single, "Richard III" reached number two. Subsequent releases, "Sun Hits the Sky" and "Late In The Day" reached numbers 10 and 18 respectively.
Around this time Supergrass also appeared on the front cover of "The Big Issue", interviewed for the magazine at Heathrow Airport by infamous ex-drug smuggler Howard Marks.
Further musical growth (1999–2004).
The band again took a short break before returning in 1999 with the single "Pumping on Your Stereo". The promo video, produced in conjunction with the Jim Henson's Creature Shop, featured the band with comical "muppet" bodies. The single generated welcome publicity following their time out of the limelight, as did a small sold-out tour scheduled around the single release, the final night of which was at Shepherds Bush Empire as part of MTV's "Five Night Stand" festival. The single and the tour were followed by their third LP "Supergrass" (1999). The following spring the record was released in the U.S. Once more, the album was recorded at Sawmills Studio with longtime associate Cornfield producing. "Supergrass" was well received critically and commercially and it has since gone platinum in the UK, but did not reap the same level of success as its predecessors. Critics claimed the album was "hit and miss" which showed up particularly as the "also-rans are surrounded by songs that are as great as anything Supergrass has ever recorded". Their next single, "Moving", proved popular and reached the Top Ten in the UK. And their third single, "Mary" entered the Top 40. There followed a long hiatus.
After three years out of the limelight, the band returned with "Life on Other Planets" (September 2002). Recorded at "Heliocentric", "Rockfield" and "Mayfair" Studios and produced by Beck collaborator Tony Hoffer. The album was released in the UK on Parlophone but in US, on the Island Def Jam imprint. The record was not as commercially successful as Supergrass' first three albums, failing to make the Top Three in the UK album chart. However, the critical response to the album was generally very positive, with Stephen Thomas Erlewine from AllMusic claiming "The world is a better place for having Supergrass in it.". It has since gone gold in the UK. "Life on Other Planets" was also notable as it was the first Supergrass album to recognise Rob Coombes as an official member. For the band's first three albums, Supergrass officially consisted of Gaz Coombes, Goffey and Quinn although Rob Coombes contributed to many of the band's songs and videos, and toured with them. Tracks recorded before this were often credited to "Supergrass and Rob Coombes". The band followed "Life on Other Planets" with another extended three-year hiatus, devoting to touring and personal engagements.
In June 2004 the band's record company suggested the band release a singles compilation "Supergrass is 10", spawning two new self-produced tracks: "Kiss of Life" and "Bullit". The companion DVD contained 'Home Movie', a humorous documentary charting the band's first 10 years achievements, made in collaboration with "Seen the Light" video director Simon Hilton. The record entered the UK album chart at number four and has since gone gold in the UK.
Development in recent years (2005–2008).
Recording of fifth studio album, "Road to Rouen" began in France in a studio built by the band in Normandy. Working with French engineer Pierre-Olivier Margerand it represented a significant change in direction and was perceived as a more mature body of work.
"St. Petersburg", the string laden first single, was released on 8 August 2005. The album followed a week later ( released 27 September in North America ) and reached No. 9 on the UK charts, going on to achieve silver status in the UK. Opinion at the time was divided, but the album garnered the band many new fans and a measure of creative respect, some even embraced it as "the sound of a band at last hitting their stride".
Second single, "Low C", featured a video by acclaimed "Pumping On Your Stereo" video director Garth Jennings, shot in Weeki Wachee Springs Florida. Third single "Fin" interpreted as a missive to the Coombes brothers recently deceased mother, received much critical praise; "The Guardian" referring to it as "so gorgeously light and airy that listening to it is like sleepwalking in space".
The band toured the songs in both acoustic and electric formats with percussionist Satin Singh joining the live band throughout. From August 2005 to September 2006 they performed in Japan, South America, USA, and Europe, finishing with a memorable gig at the Beijing Pop Festival.
The follow up album, "Diamond Hoo Ha" was recorded at Hansa Tonstudio, Berlin, with producer Nick Launey and mixed at Seedy Underbelly Studios in Los Angeles. The band toured in the summer 2007, headlining Guilfest among others and debuting new material, with the youngest sibling of the Coombes brothers ex-22-20s keyboardist Charly on second guitar, percussion and backing vocals.
On 27 September 2007 bassist Mick Quinn sustained a broken heel bone and two spinal fractures in a sleepwalking accident whilst on holiday in France. During his recuperation, Gaz and Danny promoted first single "Diamond Hoo Ha Man" as the Diamond Hoo Ha Men, with a run of small club shows in December/January. To celebrate the single release, Mick Quinn appeared as Diamond Ho Ha Man "Biff Hymenn" at the Apple Store, Regent Street, London, marking his return to touring duties on 15 January. Charly directed "Glange Fever" (under pseudonym "Chas Harrison") a rockumentary which followed their exploites.
The single was released as a limited edition, chocolate vinyl 7". The inner photo pictures a sign on a striped wall reading "ON A&R" were taken by fine art photographer (also responsible for the photography book ""), which documented the recording sessions in Hansa Studios.
In February 2008, the video of their second single "Bad Blood" was released on the band's official web-site, winning Best Rock Video at the UK Music Video Awards and the single followed on 17 March.
In 2008, Parlophone was taken over by venture capitalist group, Terra Firma and Supergrass ended their contract with the label. "Rebel In You", final single from the Diamond Hoo Ha album, was released, under licence from Parlophone, on the band's own imprint, 'Supergrass Records'.
Independent career and split (2009–2010).
The band headlined Wychwood Festival on 30 May and also Sellindge Music Festival (6 June), Provinssirock Festival (13 June) and a short European trek in July at BBK Live (10 July) at Bilbao, Bikini Festival (11 July) in Toulouse, Festival Les Ardentes (12 July) in Liège (Belgium) and Paredes de Coura Festival (30 July) in Portugal. Also a co-headlining date at 2009's Truck Festival along with Ash, on 25 July and 26 at Hill Farm in Steventon, Oxfordshire.
On 12 April 2010, the band announced they would split after a series of four farewell shows, with their final gig in Paris on 11 June 2010.
At the time of the split, Supergrass were working on their seventh studio album, tentatively titled "Release the Drones". In early 2010, the band revealed that the album had been influenced by krautrock bands such as Can, and drone music, and that the members had swapped instruments on several tracks during its recording. Coombes said of the approach to the album: "This record's actually been very collaborative. It's been cool to try something different and chaotic." Coombes stated that the album was "nearly finished", and it was scheduled for release in May. The album remains unfinished and unreleased.
Solo projects.
During 1998, Coombes and Quinn were invited to play on Dr John's "Anutha Zone" album (they appear on the track "Voices In My Head"), whilst Goffey contributed to the debut album by Lodger (which also featured his partner Pearl Lowe and members of the band Delicatessen).
Danny Goffey has also embarked on a solo project between Supergrass engagements called "Van Goffey" which saw tracks being released via MySpace in August 2006, the first three being "Crack House Blues", "I Feel so Gaye" and "Natalie Loves the F". He plays drums on the charity football song "Born In England" by a collective of musicians called Twisted X, which charted at number 8 in the UK Charts in 2004. Danny Goffey was also a drummer in the 2004 charity single "Do They Know It's Christmas?", along with members of Radiohead and The Darkness under the name Band Aid 20.
In July 2008, Coombes joined Foo Fighters on stage during a show that saluted The Who's musical career, VH1 Rock Honors. Coombes performed vocals on The Who's classic song "Bargain".
In 2007 and 2008, while Mick Quinn was still recuperating from his injury, Danny Goffey and Gaz Coombes were performing as the duo Diamond Hoo Ha Men, the name taken from the band's sixth album and its lead single (see ). They appeared in character as Duke Diamond and Randy Hoo Ha to play gigs at small venues. Gigs included an appearance at the Apple Store on London's Regent Street, which featured Mick Quinn's first appearance in the band since his injury. He appeared on stage introduced by Gaz as Biff Hymen.
Goffey and Coombes were members of side-project The Hotrats (originally The Hot Rats). They released an album of covers produced by Nigel Godrich called "Turn Ons" in 25 January 2010. A cover of "Drive My Car" by The Beatles appears in an advert for Orange, a fragrance by Hugo Boss. Since Supergrass announced they were to split, The Hotrats have joined with Air to perform "The Virgin Suicides" live for the first time, over several concert dates.
In May 2010 Mick Quinn formed the DB Band with former Shake Appeal bassist Fab Wilson. The band released their first EP "Stranger In The Alps" on 17 September 2011. They have toured the Netherlands, France and appeared at Oxford's Truck Festival in Steventon. The band plan to record a full-length LP for release in 2012.
Gaz Coombes has completed a solo album at his home studio in Oxford. The album was recorded with Sam Williams, who produced 1995's "I Should Coco" for Supergrass. The album, "Here Come the Bombs", was released on 21 May 2012.

</doc>
<doc id="43014" url="http://en.wikipedia.org/wiki?curid=43014" title="Eunice Kennedy Shriver">
Eunice Kennedy Shriver

Eunice Mary Kennedy Shriver, DSG (July 10, 1921 – August 11, 2009) was a member of the Kennedy family, sister of President John F. Kennedy and Senators Robert F. Kennedy and Ted Kennedy. Eunice Kennedy Shriver was the founder in 1962 of Camp Shriver which started on her Maryland farm known as Timberlawn and, in 1968 evolved into the Special Olympics. Her husband, Sargent Shriver, was United States Ambassador to France and the Democratic vice presidential candidate in the 1972 U.S. presidential election.
Early life.
Born Eunice Mary Kennedy in Brookline, Massachusetts, she was the fifth of nine children of Joseph P. Kennedy, Sr., and Rose Fitzgerald.
She was educated at the Convent of The Sacred Heart, Roehampton, London and at Manhattanville College in Upper Manhattan (the school later moved further North to Purchase, New York). After graduating from Stanford University with a Bachelor of Science degree in sociology in 1943, she worked for the Special War Problems Division of the U.S. State Department. She eventually moved to the U.S. Justice Department as executive secretary for a project dealing
with juvenile delinquency. She served as a social worker at the Federal Industrial Institution for Women for one year before moving to Chicago in 1951 to work with the House of the Good Shepherd women's shelter and the Chicago Juvenile Court.
In 1969, Shriver moved to France and pursued her interest in intellectual disability there. She started organizing small activities with Paris organizations, mostly reaching out to families of kids who had special needs to provide activities for them, laying the foundation for a robust international expansion of the Special Olympics in the late ’70s and ’80s.
Political career.
Shriver actively campaigned for her elder brother, John, during his successful 1960 U.S. presidential election. In 1968, she helped Anne McGlone Burke nationalize the Special Olympics movement and is the only woman to have her portrait appear, during her lifetime, on a U.S. coin – the 1995 commemorative Special Olympics silver dollar.
Although Shriver was a Democrat, she was a vocal supporter of the pro-life movement. In 1990, Shriver wrote a letter to "The New York Times" denouncing the misuse of a quotation by President Kennedy used out of context by a pro-choice group. During Bill Clinton's 1992 Democratic U.S. presidential campaign, she was one of several prominent Democrats – including Governor Robert P. Casey of Pennsylvania, and Bishop Austin Vaughan of New York – who signed a letter to "The New York Times" protesting the Democratic Party's pro-choice plank in its platform. Shriver was a supporter of several pro-life organizations: Feminists for Life of America, the Susan B. Anthony List, and Democrats for Life of America.
A lifelong Democrat, she supported her Republican son-in-law Arnold Schwarzenegger's successful 2003 Governor of California election. On January 28, 2008, Shriver was present at American University in Washington, D.C., when her brother, U.S. Senator Edward M. Kennedy, announced his endorsement of Barack Obama's 2008 Democratic U.S. presidential campaign.
Charity work and awards.
A longtime advocate for children's health and disability issues, Shriver was a key founder of the National Institute of Child Health and Human Development (NICHD), a part of the National Institutes of Health, in 1962, and has also helped to establish numerous other health-care facilities and support networks throughout the country. In 1982, Shriver founded the Eunice Kennedy Shriver National Center for Community of Caring at University of Utah, Salt Lake City. The Community is a "grades K-12, whole school, comprehensive character education program with a focus on disabilities... adopted by almost 1,200 schools nationwide and in Canada."
She was awarded the nation's highest civilian award, the (U.S.) Presidential Medal of Freedom, in 1984 by U.S. President Ronald Reagan, because of her work on behalf of those with intellectual disability. In 1990 Shriver was awarded the Eagle Award from the United States Sports Academy. The Eagle Award is the Academy's highest international honor and was awarded to Shriver for her significant contributions to international sport.
In 1992, Shriver received the Award for Greatest Public Service Benefiting the Disadvantaged, an award given out annually by Jefferson Awards.
For her work in nationalizing the Special Olympics, Shriver received the Civitan International World Citizenship Award. Her advocacy on this issue has also earned her other awards and recognitions, including honorary degrees from numerous universities. She is the second American and only woman to appear on a US coin while still living. Her portrait is on the obverse of the 1995 commemorative silver dollar honoring the Special Olympics. On the reverse is the quotation, "As we hope for the best in them, hope is reborn in us."
Shriver received the 2002 Theodore Roosevelt Award (the Teddy), an annual award given by the National Collegiate Athletic Association to a graduate from an NCAA member institution who earned a varsity letter in college for participation in intercollegiate athletics, and who ultimately became a distinguished citizen of national reputation based on outstanding life accomplishment. In addition to the Teddy recognition, she was selected in 2006 as part of the NCAA Centennial celebration as one of the 100 most-influential individuals in its first century; she was listed ninth.
In 2006, she received a papal knighthood from Pope Benedict XVI, being made a Dame of the Order of St. Gregory the Great (DSG). Her mother had been created a papal countess in 1950 by Pope Pius XII.
In 2008, the U.S. Congress changed the NICHD’s name to the "Eunice Kennedy Shriver" National Institute of Child Health and Human Development. In December 2008, "Sports Illustrated" named her the first recipient of Sportsman of the Year Legacy Award. On May 9, 2009, the Smithsonian Institution's National Portrait Gallery (NPG) in Washington, D.C., unveiled an historic portrait of her, the first portrait the NPG has ever commissioned of an individual who had not served as a U.S. President or First Lady. The portrait depicts her with four Special Olympics athletes (including Loretta Claiborne) and one Best Buddies participant. It was painted by David Lenz, the winner of the Outwin Boochever Portrait Competition in 2006. As part of the Portrait Competition prize, the NPG commissioned a work from the winning artist to depict a living subject for the collection. Lenz, whose son, Sam, has Down syndrome and is an enthusiastic Special Olympics athlete, was inspired by Shriver’s dedication to working with people with intellectual disabilities.
Shriver became involved with Dorothy Hamill's special skating program in the Special Olympics after Hamill's Olympics Games ice-skating win. In September 2010, the State University of New York at Brockport, home of the 1979 Special Olympics, renamed its football stadium after Shriver.
Personal life.
On May 23, 1953, she married Sargent Shriver in a Roman Catholic ceremony at Saint Patrick's Cathedral in New York City. Her husband served as the U.S. Ambassador to France from 1968 to 1970 and was the 1972 Democratic U.S. Vice Presidential candidate (with George McGovern as the candidate for U.S. President). They had five children:
With her husband, she had nineteen grandchildren, the second-most of any of her siblings (her brother Robert had eleven children who have produced thirty-two grandchildren). Her daughter Maria Shriver is separated from actor and former California Governor Arnold Schwarzenegger.
As executive vice president of the Joseph P. Kennedy, Jr. Foundation in the 1950s, she shifted the organization's focus from Catholic charities to research on the causes of intellectual disabilities and humane ways to treat them. This interest eventually culminated in, among other things, the Special Olympics movement.
Upon the death of her sister Rosemary Kennedy on January 7, 2005, Shriver became the eldest of the four then-surviving children of Joseph and Rose Kennedy. Her sister Patricia Kennedy Lawford died on September 17, 2006, and her brother Edward M. Kennedy died on August 25, 2009, leaving former U.S. Ambassador to Ireland Jean Kennedy Smith as the only surviving sibling.
Later years and death.
Shriver, who was believed to have suffered from Addison's disease, suffered a stroke and a broken hip in 2005, and on November 18, 2007, she was admitted to Massachusetts General Hospital in Boston, where she spent several weeks. On August 9, 2009, she was admitted to Cape Cod Hospital in Hyannis, with an undisclosed ailment. On August 10, her relatives were called to the hospital. Early the following morning, Shriver died at the hospital; she was 88 years old. No other Kennedy, with the exception of her mother, Rose, has, to date, lived longer.
Shriver's family issued a statement upon her death, reading in part "Inspired by her love of God, her devotion to her family, and her relentless belief in the dignity and worth of every human life, she worked without ceasing—searching, pushing, demanding, hoping for change. She was a living prayer, a living advocate, a living center of power. She set out to change the world and to change us, and she did that and more. She founded the movement that became Special Olympics, the largest movement for acceptance and inclusion for people with intellectual disabilities in the history of the world. Her work transformed the lives of hundreds of millions of people across the globe, and they in turn are her living legacy."
President Barack Obama remarked after Shriver's death that she was "an extraordinary woman who, as much as anyone, taught our nation—and our world—that no physical or mental barrier can restrain the power of the human spirit."
Funeral and burial.
On August 14, 2009, an invitation-only Requiem Mass was celebrated for Shriver at St. Francis Xavier Roman Catholic Church in Hyannis. Following the Requiem Mass, she was buried at the St. Francis Xavier parish cemetery in nearby Centerville. Pope Benedict XVI sent a letter of condolence to her family. Because her brother Ted had terminal brain cancer, he was unable to attend the funeral, and their sister, Jean Smith (now the sole surviving child of Joseph and Rose Kennedy), stayed with him. Ted died two weeks later.
That same day, U2 performed at Wembley Stadium as part of their U2 360° Tour. As part of the encore, the band performed "With or Without You", which Bono dedicated to Shriver; a recording of the performance is available on their album "U22".

</doc>
<doc id="43015" url="http://en.wikipedia.org/wiki?curid=43015" title="John Lingard">
John Lingard

Rev Dr John Lingard (5 February 1771 – 17 July 1851) was an English historian, the author of "The History of England, From the First Invasion by the Romans to the Accession of Henry VIII", an 8-volume work published in 1819.
Biography.
Born in 1771 in St Thomas Street in Central Winchester to recusant parents, John Lingard entered the English College at Douai, France, to commence training for the Catholic priesthood. In the English College, he excelled in the humanities before beginning the study of theology. Narrowly escaping attacks by mobs at the time of the French Revolution upon war declaration between United Kingdom and France, he returned to England in 1793 where he concluded his theological studies and was ordained. He then taught philosophy and, in 1805, wrote a series of letters which, after their publication in a periodical, were collected as "Catholic Loyalty Vindicated". In 1806 the first edition of "The Antiquities of the Anglo-Saxon Church" appeared, a development of his informal lectures.
The principal object of his major work, "The History of England", is to emphasise the disastrous effects of the Reformation. The book was later expanded by the author and the title changed to reflect the period covered. As each additional volume appeared the "History"'s reputation increased, while Lingard continued to revise and improve the whole work. Most of the earnings from this project and his other writings were directed towards the educating of students to the priesthood.
In his style and presentation of English history, Lingard demonstrates the prevalent manner of Catholic scholarship – he gives, for example, no indication that he is a priest on the title page, and professes emphatically to be writing an impartial history. But in a curious turnaround, his "History" by its very impartiality is a Catholic apologetic, and Lingard's desire for impartiality is a reflection of the Catholic political and intellectual situation in the Emancipation era.
The Catholic position in the early nineteenth century, politically speaking, was that of a minority body, allied to the Whig-Radical-Dissenting political grouping, and seeking religious and political freedom. This alliance encouraged Old Catholic intellectuals to present their arguments in 'liberal' and 'reasonable' form – the argumentative advantage in this being that it presented Catholics as enlightened and tolerant, and their opposition as prejudiced and bigoted.
Lingard himself argued that one of his chief duties as an historian was: "to weigh with care the value of the authorities on which I rely, and to watch with jealousy the secret workings of my own personal feelings and prepossessions. Such vigilance is a matter of necessity to every writer of history ... Otherwise, he will be continually tempted to make an unfair use of the privilege of the historian; he will sacrifice the interests of truth to the interests of party, national, or religious, or political." (J. Lingard, "History of England", vol 1, 6th edition, London: Charles Dolman, 1854, p. 6.)
Lingard adopted a non-controversial and sober approach to history with the emphasis on incontrovertible fact and using primary rather than secondary sources.
In the History, Lingard faces the task of convincing Protestants of the fundamental truths of the Catholic faith, while maintaining an unbiased presentation of historical truth. He possesses little sense of "preaching to the converted" (in a very literal sense), and aims his work more at influencing Protestants than placating his Ultramontane opposition. In a letter of 18 December 1819, Lingard wrote: "... my only chance of being read by Protestants depends upon my having the reputation of a temperate writer. The good to be done is by writing a book which Protestants will read." (Lingard to Kirke, in Haile, Bonney, Life and Letters of John Lingard, 1771–1851, London: 1912, pp 166–67).
Lingard's History is also an apt demonstration of the advantages a Catholic historian of the time may have had, in terms of impartiality. Lingard's religion had to a large extent isolated him from the mainstream nationalism which surrounded Protestant historians, as well as from the growing "providentialist" concept of history. Lingard's strength of argument, however, continued to be popular, and the influence of Protestant animosity for Catholic apologetic also led him to develop a keen critical judgement. He was devoted to absolute accuracy and detail and the History was a groundbreaking work in its use of primary sources. Lingard made extensive use of Vatican archives and French, Italian, Spanish and English dispatches, document collections and state papers – the first British historian to do so. The peripheral nature of English Catholicism put him in a position of "outside observer" to much of English intellectual culture, and this is reflected in his historical works. Despite this distancing effect, however, Lingard maintained an active interest in politics all his life and was a noted pamphleteer.
The edition that is usually seen is a 10 volume set, "to the Accession of William and Mary in 1688." There is an enlarged 13 volume set published just before Lingard's death which was his final revision, "to the Commencement of the Reign of William the Third." The History was abridged and revised adding material to bring its treatment up to the then present and used as a text in English Catholic schools during the nineteenth century.
Lesser known than Lingard's historical works is his anonymously published translation of the Four Gospels in 1836. The title page reads simply that the work is "by a Catholic." Lingard departed from usual Catholic practice by using early Greek manuscripts rather than the Latin Vulgate as the principal basis for the translation. This resulted in such renderings as "repent" rather than "do penance" (Matt 3:2). His willingness to depart from Catholic custom contrasts with his contempt for the Protestant concept of "private interpretation" of Scripture. In a note to John 1:1, he states, "Hence it happens that men of every persuasion find confirmation of their peculiar opinions in the sacred volumes: for, in fact, it is not the Scripture that informs them, but that they affix their own meaning to the language of Scripture."
Lingard's work influenced Francis P. Kenrick (1796–1863), Roman Catholic Bishop of Philadelphia, and later Archbishop of Baltimore, who published his own translation of the Four Gospels in 1849. By 1851, Lingard felt sufficiently confident to publish a new edition of his Four Gospels in his own name.
William Cobbett used Lingards history as an unbiased reference source for his own "History of the Protestant Reformation" in which he puts forward the argument that the reformation had disastrous consequences for the ordinary people of England. His work would in turn be influential in the passage of the Catholic emancipation bill by preparing the way for public acceptance and the minimisation of sectarian violence.
Lingard was accorded no recognition by the British intellectual establishment. "History of England" is a substantial scholarly work which gave full treatment to the history of England. From 1811 until his death in 1851 Lingard spent most of his life in the village of Homby near Lancaster, where he devoted himself to his study and writing. A quiet, gentle man, he was well liked by the residents. Lingard's popularity as an historian had its day, but his contribution to historical method came at a critical point in British intellectual history. That he was also a Catholic priest, in a turbulent time for the Church in England, makes that contribution all the more interesting.
In 1821 Pope Pius VII honoured Lingard with a triple doctorate – in theology, canon law and civil law – and a few years later Leo XII conferred upon him a gold medal generally only given to cardinals and princes. There is even strong evidence that he was made a cardinal "in pectore" ("in the breast") in 1826. This meant that the Pope could have announced the appointment publicly at some future time.
Lingard also authored the very popular Catholic hymn to the Virgin Mary titled Hail Queen of Heaven, the Ocean Star.
In the last few decades a number of books have been devoted to Lingard. They include Peter Phillips, ed., "Lingard Remembered: Essays to Mark the Sesquicentenary of John Lingard’s Death" (London: Catholic Record Society, 2004); Edwin Jones, "John Lingard and the Pursuit of Historical Truth" (Brighton, England, and Portland, Oregon: Sussex Academic Press, 2001); John Trappes-Lomax, ed., "The Letters of Dr. John Lingard to Mrs. Thomas Lomax (1835–51)" (London: Catholic Record Society, 2000); J.A. Hilton, "A Catholic of the Enlightenment: Essays on Lingard’s Work and Times" (Wigan, England: North West Catholic Historical Society, 1999); Joseph P. Chinnici, "The English Catholic Enlightenment: John Lingard and the Cisalpine Movement, 1780–1850" (Shepherdstown, West Virginia: Patmos Press, 1980); Donald F. Shea, "The English Ranke: John Lingard" (New York: Humanities Press, 1969). An older and still indispensable work is Martin Haile [pseud. for Marie Halle] and Edwin Bonney, "Life and Letters of John Lingard, 1771–1851" (1911).

</doc>
<doc id="43017" url="http://en.wikipedia.org/wiki?curid=43017" title="Gruinard Island">
Gruinard Island

Gruinard Island ( ;
Scottish Gaelic: Eilean Ghruinneard) is a small, oval-shaped Scottish island approximately 2 km long by 1 km wide, located in Gruinard Bay, about halfway between Gairloch and Ullapool. At its closest point to the mainland it is just over 1.1 km offshore. The island was made dangerous for all mammals by experiments with the anthrax bacterium, until it was decontaminated in the late 20th century.
Early history.
The island was mentioned by Dean Munro who travelled the area in the mid 16th century. He wrote that it was Clan MacKenzie territory, was "full of woods", (a striking comparison with its treelessness today) and that it was "guid for fostering of thieves and rebellis".
The population was recorded as six in 1881, but Gruinard has been uninhabited since the 1920s.
Biological warfare testing.
Gruinard was the site of a biological warfare test by British military scientists from Porton Down in 1942, during the Second World War. At that time there was an investigation by the British government into the feasibility of an attack using anthrax: to test the vulnerability of Britain against a German attack and the viability of attacking Germany with a British bio-weapon. Given the nature of the weapon which was being developed, it was recognised that tests would cause widespread and long-lasting contamination of the immediate area by anthrax spores. In order to limit contamination a remote and uninhabited island was required. Gruinard was surveyed, deemed suitable and requisitioned from its owners by the British Government.
The anthrax strain chosen for the Gruinard bioweapons trials was a highly virulent type called "Vollum 14578", named after R. L. Vollum, Professor of Bacteriology at the University of Oxford, who supplied it. Eighty sheep were taken to the island and bombs filled with anthrax spores were exploded close to where selected groups were tethered. The sheep became infected with anthrax and began to die within days of exposure. Some of the experiments were recorded on 16 mm colour movie film, which was declassified in 1997. One sequence shows the detonation of an anthrax bomb fixed at the end of a tall pole supported with guy ropes. When the bomb is detonated a brownish aerosol cloud drifts away towards the target animals. A later sequence shows anthrax-infected sheep carcasses being burned in incinerators following the conclusion of the experiment.
Scientists concluded after the tests were completed that a large release of anthrax spores would thoroughly pollute German cities, rendering them uninhabitable for decades afterwards. These conclusions were supported by the discovery that initial efforts to decontaminate the island after the biological warfare trials had ended failed because of the high durability of anthrax spores.
In 1945 when the owner sought the return of Gruinard Island, the Ministry of Supply recognized that the island was contaminated as a result of the wartime experiments and consequently it could not be derequisitioned until it was deemed safe. In 1946, the Crown agreed to acquire the island and to take on the onus of responsibility. The owner or her heirs and beneficiaries would be able to repurchase the island for the sale price of £500 when it was declared "Fit for habitation by man and beast".
For many years it was judged too hazardous and expensive to decontaminate the island sufficiently to allow public access. Gruinard Island was quarantined indefinitely as a result. Visits to the island were prohibited, except periodic checks by Porton Down personnel to determine the level of contamination.
Operation Dark Harvest.
In 1981 newspapers began receiving messages with the heading "Operation Dark Harvest" which demanded that the government decontaminate the island, and reported that a "team of microbiologists from two universities" had landed on the island with the aid of local people and collected 300 lb of soil. The group threatened to leave samples of the soil "at appropriate points that will ensure the rapid loss of indifference of the government and the equally rapid education of the general public". The same day a sealed package of soil was left outside the military research facility at Porton Down; tests revealed that it contained anthrax bacilli. A few days later another sealed package of soil was left in Blackpool, where the ruling Conservative Party was holding its annual conference. The soil did not contain anthrax, but officials said that the soil was similar to that found on the island.
Decontamination.
Starting in 1986 a determined effort was made to decontaminate the island, with 280 tonnes of formaldehyde solution diluted in seawater being sprayed over all 196 hectares of the island and the worst-contaminated topsoil around the dispersal site being removed. A flock of sheep was then placed on the island and remained healthy. On 24 April 1990, after 48 years of quarantine and 4 years after the solution being applied, junior defence minister Michael Neubert visited the island and announced its safety by removing the warning signs. On 1 May 1990, the island was repurchased by the heirs of the original owner for the original sale price of £500.
As of October 2007 there have been no cases of anthrax in the island flock. 
Popular culture references.
The island is mentioned in the novels "The Enemy" by Desmond Bagley (1977), "Sea of Death" by Richard P. Henrick (1992), "The Fist of God" by Frederick Forsyth (1994), "Quantico" by Greg Bear (2005), "The Big Over Easy" by Jasper Fforde (2005), "Forbidden Island" by Malcolm Rose (2009), "And then you die" by Iris Johansen (1998), "The Island" by R J Price (better-known as the poet Richard Price) (2010) and "The Impossible Dead" by Ian Rankin (2011).
In issues 187–188 of the comic book "Hellblazer", in a story titled "Bred in the Bone", the protagonist's niece finds herself on Gruinard surrounded by flesh-eating children. The issues were released in 2003 and were written by Mike Carey and illustrated by Doug Alexander Gregory.
An episode of the British wartime TV series "Foyle's War" entitled "Bad Blood" involved biological testing – a strong reference to the Gruinard testing.
The "Hawaii Five-O" episode "Three Dead Cows at Makapu, Part 2" featured a scientist played by Ed Flanders who threatened to unleash a deadly virus on the island of Oahu. When being interrogated, the scientist briefly mentions Gruinard Island and how it will be uninhabitable for a century due to anthrax experiments.
Outlying Islands, a Fringe First winning play by Scottish dramatist David Greig, is a fictionalised account of two British scientists' visit to an island in Scotland where the government plans to test anthrax inspired by the story of Gruinard.
The 2013 UK TV series Utopia describes the fictional outbreak of a new form of flu. During Episode 3, Dugdale visit the proposed origin of the virus at the, now quarantined, Island of Fetlar. On arrival, personnel at the Island, wearing orange overalls, carry one of numerous covered bodies past on a stretcher in a scene that is nearly identical to that seen in the original test footage from Gruinard Island. In the dramatisation however, the personnel at Fetlar are seen wearing dust masks as opposed to the gas masks seen in the Gruinard footage; likely due to budget constraints (much of Utopia was not filmed where it claims to be).

</doc>
<doc id="43018" url="http://en.wikipedia.org/wiki?curid=43018" title="South-West Africa">
South-West Africa

South-West Africa (Afrikaans: "Suidwes-Afrika"; Dutch: "Zuidwest-Afrika"; German: "Südwestafrika") was the name for modern-day Namibia when it was ruled by the German Empire and later South Africa.
German colony.
As a German colony from 1884, it was known as German South-West Africa ("Deutsch-Südwestafrika"). Germany had a difficult time administering the territory, which experienced many insurrections, especially those led by guerilla leader Jacob Morenga. The main port, Walvis Bay, and the Penguin islands were annexed by Britain as part of the Cape Colony in 1878, and became part of the Union of South Africa in 1910.
As part of the Heligoland-Zanzibar Treaty in 1890, a corridor of land taken from the northern border of Bechuanaland, extending as far as the Zambezi river, were added to the colony. It was named the Caprivi Strip ("Caprivizipfel") after the German Chancellor Leo von Caprivi.
South African rule.
In 1915, during South-West Africa Campaign of the First World War, South Africa captured the German colony. After the war, it was declared a League of Nations Mandate territory under the Treaty of Versailles, with the Union of South Africa responsible for the administration of South West Africa, including Walvis Bay. South West Africa remained a League of Nations Mandate until World War II.
The Mandate was supposed to become a United Nations Trust Territory when League of Nations Mandates were transferred to the United Nations following the Second World War. The Union of South Africa objected to South West Africa coming under UN control and refused to allow the territory's transition to independence, regarding it as a fifth province (even though it was never formally incorporated into South Africa). For example, between 1950 and 1977, whites in the territory were represented in the South African Parliament by four Senators and six MPs.
These South African actions gave rise to several rulings at the International Court of Justice, which in 1950 ruled that South Africa was not obliged to convert South West Africa into a UN trust territory, but was still bound by the League of Nations Mandate with the United Nations General Assembly assuming the supervisory role. The ICJ also clarified that the General Assembly was empowered to receive petitions from the inhabitants of South West Africa and to call for reports from the mandatory nation, South Africa. The General Assembly constituted the Committee on South-West Africa to perform the supervisory functions. In another advisory opinion issued in 1955, the Court further ruled that the General Assembly was not required to follow League of Nations voting procedures in determining questions concerning South-West Africa. In 1956, the Court further ruled that the Committee had the power to grant hearings to petitioners from the mandated territory. In 1960, Ethiopia and Liberia filed a case in the International Court of Justice against South Africa alleging that South Africa had not fulfilled its mandatory duties. This case did not succeed, with the Court ruling in 1966 that they were not the proper parties to bring the case.
UN mandate terminated.
There was a protracted struggle between South Africa and forces fighting for independence, particularly after the formation of the South West Africa People's Organisation (SWAPO) in 1960.
In 1966, the General Assembly passed resolution 2145 (XXI) which declared the Mandate terminated and that the Republic of South Africa had no further right to administer South West Africa. In 1971, acting on a request for an advisory opinion from the United Nations Security Council, the ICJ ruled that the continued presence of South Africa in Namibia was illegal and that South Africa was under an obligation to withdraw from Namibia immediately. It also ruled that all member states of the United Nations were under an obligation not to recognise as valid any act performed by South Africa on behalf of Namibia.
South West Africa became known as Namibia by the UN when the General Assembly changed the territory's name by Resolution 2372 (XXII) of 12 June 1968. SWAPO was recognised as representative of the Namibian people and gained UN observer status when the territory of South West Africa was already removed from the list of Non-Self-Governing Territories.
The territory became the independent Republic of Namibia on 21 March 1990, although Walvis Bay and the Penguin Islands were only incorporated into Namibia in 1994.
Bantustans.
The South African authorities established 10 bantustans in South West Africa in the late 1960s and early 1970s in accordance with the Odendaal Commission, three of which were granted self-rule. These bantustans were replaced with separate ethnicity based governments in 1980.
The bantustans were:

</doc>
<doc id="43021" url="http://en.wikipedia.org/wiki?curid=43021" title="Fata Morgana">
Fata Morgana

Fata Morgana may refer to:

</doc>
<doc id="43023" url="http://en.wikipedia.org/wiki?curid=43023" title="Gulliver's Travels">
Gulliver's Travels

Travels into Several Remote Nations of the World. In Four Parts. By Lemuel Gulliver, First a Surgeon, and then a Captain of Several Ships, commonly known as Gulliver's Travels (1726, amended 1735), is a satire by Anglo-Irish writer and clergyman Jonathan Swift, that is both a satire on human nature and a parody of the "travellers' tales" literary subgenre. It is Swift's best known full-length work, and a classic of English literature.
The book became popular as soon as it was published. John Gay wrote in a 1726 letter to Swift that "It is universally read, from the cabinet council to the nursery." Since then, it has never been out of print.
Cavehill in Belfast is thought to have inspired part of book two of the novel. Swift imagined that the mountain resembled the shape of a sleeping giant safeguarding the city.
Plot summary.
Part I: A Voyage to Lilliput.
The book begins with a short preamble in which Lemuel Gulliver, in the style of books of the time, gives a brief outline of his life and history before his voyages. He enjoys travelling, although it is that love of travel that is his downfall.
During his first voyage, Gulliver is washed ashore after a shipwreck and finds himself a prisoner of a race of tiny people, less than 6 inches tall, who are inhabitants of the island country of Lilliput. After giving assurances of his good behaviour, he is given a residence in Lilliput and becomes a favourite of the court. From there, the book follows Gulliver's observations on the Court of Lilliput. He is also given the permission to roam around the city on a condition that he must not harm their subjects. Gulliver assists the Lilliputians to subdue their neighbours, the Blefuscudians, by stealing their fleet. However, he refuses to reduce the island nation of Blefuscu to a province of Lilliput, displeasing the King and the court. Gulliver is charged with treason for, among other crimes, "making water" (urination) in the capital, though he was putting out a fire and saving countless lives. He is convicted and sentenced to be blinded, but with the assistance of a kind friend, he escapes to Blefuscu. Here he spots and retrieves an abandoned boat and sails out to be rescued by a passing ship, which safely takes him back home.
This book of the "Travels" is a topical political satire.
Part II: A Voyage to Brobdingnag.
When the sailing ship "Adventure" is blown off course by storms and forced to sail for land in search of fresh water, Gulliver is abandoned by his companions and found by a farmer who is 72 ft tall (the scale of Brobdingnag is about 12:1, compared to Lilliput's 1:12, judging from Gulliver estimating a man's step being 10 yd). He brings Gulliver home and his daughter cares for Gulliver. The farmer treats him as a curiosity and exhibits him for money. After a while the constant shows make Lemuel sick, and the farmer sells him to the queen of the realm. The farmer's daughter (who accompanied her father while exhibiting Gulliver) is taken into the queen's service to take care of the tiny man. Since Gulliver is too small to use their huge chairs, beds, knives and forks, the queen commissions a small house to be built for him so that he can be carried around in it; this is referred to as his 'travelling box'. Between small adventures such as fighting giant wasps and being carried to the roof by a monkey, he discusses the state of Europe with the King. The King is not happy with Gulliver's accounts of Europe, especially upon learning of the use of guns and cannons. On a trip to the seaside, his travelling box is seized by a giant eagle which drops Gulliver and his box into the sea, where he is picked up by some sailors, who return him to England.
This book compares the truly moral man to the representative man; the latter is clearly shown to be the lesser of the two. Swift, being in Anglican holy orders, was keen to make such comparisons.
Part III: A Voyage to Laputa, Balnibarbi, Luggnagg, Glubbdubdrib, and Japan.
After Gulliver's ship was attacked by pirates, he is marooned close to a desolate rocky island near India. Fortunately, he is rescued by the flying island of Laputa, a kingdom devoted to the arts of music and mathematics but unable to use them for practical ends.
Laputa's custom of throwing rocks down at rebellious cities on the ground seems the first time that the air strike was conceived as a method of warfare. Gulliver tours Balnibarbi, the kingdom ruled from Laputa, as the guest of a low-ranking courtier and sees the ruin brought about by the blind pursuit of science without practical results, in a satire on bureaucracy and on the Royal Society and its experiments. At the Grand Academy of Lagado, great resources and manpower are employed on researching completely preposterous schemes such as extracting sunbeams from cucumbers, softening marble for use in pillows, learning how to mix paint by smell, and uncovering political conspiracies by examining the excrement of suspicious persons (see muckraking).
Gulliver is then taken to Maldonada, the main port, to await a trader who can take him on to Japan. While waiting for a passage, Gulliver takes a short side-trip to the island of Glubbdubdrib, where he visits a magician's dwelling and discusses history with the ghosts of historical figures, the most obvious restatement of the "ancients versus moderns" theme in the book. In Luggnagg he encounters the "struldbrugs", unfortunates who are immortal. They do not have the gift of eternal youth, but suffer the infirmities of old age and are considered legally dead at the age of eighty. After reaching Japan, Gulliver asks the Emperor "to excuse my performing the ceremony imposed upon my countrymen of trampling upon the crucifix," which the Emperor does. Gulliver returns home, determined to stay there for the rest of his days.
Part IV: A Voyage to the Country of the Houyhnhnms.
Despite his earlier intention of remaining at home, Gulliver returns to the sea as the captain of a merchantman as he is bored with his employment as a surgeon. On this voyage he is forced to find new additions to his crew, whom he believes to have turned the rest of the crew against him. His crew then mutiny, and after keeping him contained for some time resolve to leave him on the first piece of land they come across and continue as pirates. He is abandoned in a landing boat and comes upon a race of hideous, deformed and savage humanoid creatures to which he conceives a violent antipathy. Shortly afterwards he meets a race of horses who call themselves Houyhnhnms (which in their language means "the perfection of nature"); they are the rulers, while the deformed creatures called Yahoos are human beings in their base form.
Gulliver becomes a member of a horse's household, and comes to both admire and emulate the Houyhnhnms and their lifestyle, rejecting his fellow humans as merely Yahoos endowed with some semblance of reason which they only use to exacerbate and add to the vices Nature gave them. However, an Assembly of the Houyhnhnms rules that Gulliver, a Yahoo with some semblance of reason, is a danger to their civilisation, and expels him.
He is then rescued, against his will, by a Portuguese ship, and is surprised to see that Captain Pedro de Mendez, a Yahoo, is a wise, courteous and generous person. He returns to his home in England, but he is unable to reconcile himself to living among 'Yahoos' and becomes a recluse, remaining in his house, largely avoiding his family and his wife, and spending several hours a day speaking with the horses in his stables; in effect becoming insane.
This book uses coarse metaphors to describe human depravity, and the Houyhnhms are symbolised as not only perfected nature but also the emotional barrenness which Swift maintained that devotion to reason brought.
Composition and history.
It is uncertain exactly when Swift started writing "Gulliver's Travels,"(much of the writing was done at Loughry Manor in Cookstown, Co. Tyrone whilst Swift stayed there) but some sources suggest as early as 1713 when Swift, Gay, Pope, Arbuthnot and others formed the Scriblerus Club with the aim of satirising popular literary genres. According to these accounts, Swift was charged with writing the memoirs of the club's imaginary author, Martinus Scriblerus, and also with satirising the "travellers' tales" literary subgenre. It is known from Swift's correspondence that the composition proper began in 1720 with the mirror-themed parts I and II written first, Part IV next in 1723 and Part III written in 1724; but amendments were made even while Swift was writing "Drapier's Letters." By August 1725 the book was complete; and as "Gulliver's Travels" was a transparently anti-Whig satire, it is likely that Swift had the manuscript copied so that his handwriting could not be used as evidence if a prosecution should arise, as had happened in the case of some of his Irish pamphlets (the "Drapier's Letters"). In March 1726 Swift travelled to London to have his work published; the manuscript was secretly delivered to the publisher Benjamin Motte, who used five printing houses to speed production and avoid piracy. Motte, recognising a best-seller but fearing prosecution, cut or altered the worst offending passages (such as the descriptions of the court contests in Lilliput and the rebellion of Lindalino), added some material in defence of Queen Anne to book II, and published it. The first edition was released in two volumes on 26 October 1726, priced at 8"s. "6"d." The book was an instant sensation and sold out its first run in less than a week.
Motte published "Gulliver's Travels" anonymously, and as was often the way with fashionable works, several follow-ups ("Memoirs of the Court of Lilliput"), parodies ("Two Lilliputian Odes, The first on the Famous Engine With Which Captain Gulliver extinguish'd the Palace Fire...") and "keys" ("Gulliver Decipher'd" and "Lemuel Gulliver's Travels into Several Remote Regions of the World Compendiously Methodiz'd", the second by Edmund Curll who had similarly written a "key" to Swift's "Tale of a Tub" in 1705) were swiftly produced. These were mostly printed anonymously (or occasionally pseudonymously) and were quickly forgotten. Swift had nothing to do with them and disavowed them in Faulkner's edition of 1735. Swift's friend Alexander Pope wrote a set of five "Verses on Gulliver's Travels", which Swift liked so much that he added them to the second edition of the book, though they are rarely included.
Faulkner's 1735 edition.
In 1735 an Irish publisher, George Faulkner, printed a set of Swift's works, Volume III of which was "Gulliver's Travels". As revealed in Faulkner's "Advertisement to the Reader", Faulkner had access to an annotated copy of Motte's work by "a friend of the author" (generally believed to be Swift's friend Charles Ford) which reproduced most of the manuscript without Motte's amendments, the original manuscript having been destroyed. It is also believed that Swift at least reviewed proofs of Faulkner's edition before printing, but this cannot be proved. Generally, this is regarded as the "Editio Princeps" of "Gulliver's Travels" with one small exception. This edition had an added piece by Swift, "A letter from Capt. Gulliver to his Cousin Sympson", which complained of Motte's alterations to the original text, saying he had so much altered it that "I do hardly know mine own work" and repudiating all of Motte's changes as well as all the keys, libels, parodies, second parts and continuations that had appeared in the intervening years. This letter now forms part of many standard texts.
Lindalino.
The five-paragraph episode in Part III, telling of the rebellion of the surface city of Lindalino against the flying island of Laputa, was an obvious allegory to the affair of "Drapier's Letters" of which Swift was proud. Lindalino represented Dublin and the impositions of Laputa represented the British imposition of William Wood's poor-quality copper currency. Faulkner had omitted this passage, either because of political sensitivities raised by an Irish publisher printing an anti-British satire, or possibly because the text he worked from did not include the passage. In 1899 the passage was included in a new edition of the "Collected Works". Modern editions derive from the Faulkner edition with the inclusion of this 1899 addendum.
Isaac Asimov notes in "The Annotated Gulliver" that Lindalino is composed of double lins; hence, Dublin.
Major themes.
"Gulliver's Travels" has been the recipient of several designations: from Menippean satire to a children's story, from proto-Science Fiction to a forerunner of the modern novel.
Published seven years after Daniel Defoe's wildly successful "Robinson Crusoe", "Gulliver's Travels" may be read as a systematic rebuttal of Defoe's optimistic account of human capability. In "The Unthinkable Swift: The Spontaneous Philosophy of a Church of England Man", Warren Montag argues that Swift was concerned to refute the notion that the individual precedes society, as Defoe's novel seems to suggest. Swift regarded such thought as a dangerous endorsement of Thomas Hobbes' radical political philosophy and for this reason Gulliver repeatedly encounters established societies rather than desolate islands. The captain who invites Gulliver to serve as a surgeon aboard his ship on the disastrous third voyage is named Robinson.
Scholar Allan Bloom points out that Swift's critique of science (the experiments of Laputa) is the first such questioning by a modern liberal democrat of the effects and cost on a society which embraces and celebrates policies pursuing scientific progress.
A possible reason for the book's classic status is that it can be seen as many things to many different people. Broadly, the book has three themes:
In terms of storytelling and construction the parts follow a pattern:
Of equal interest is the character of Gulliver himself—he progresses from a cheery optimist at the start of the first part to the pompous misanthrope of the book's conclusion and we may well have to filter our understanding of the work if we are to believe the final misanthrope wrote the whole work. In this sense "Gulliver's Travels" is a very modern and complex novel. There are subtle shifts throughout the book, such as when Gulliver begins to see all humans, not just those in Houyhnhnm-land, as Yahoos.
Throughout, Gulliver is presented as being gullible; he believes what he is told, never perceives deeper meanings, is an honest man, and expects others to be honest. This makes for fun and irony; what Gulliver says can be trusted to be accurate, and he does not always understand the meaning of what he perceives.
Also, although Gulliver is presented as a commonplace "everyman", lacking higher education, he possesses a remarkable natural gift for language. He quickly becomes fluent in the native tongue of any strange land in which he finds himself, a literary device that adds much understanding and humour to Swift's work.
Despite the depth and subtlety of the book, it is often classified as a children's story because of the popularity of the Lilliput section (frequently bowdlerised) as a book for children. One can still buy books entitled "Gulliver's Travels" which contain only parts of the Lilliput voyage.
Character analysis.
Pedro de Mendez is the name of the Portuguese captain who rescues Gulliver in Book IV. When Gulliver is forced to leave the Island of the Houyhnhnms, his plan is "to discover some small Island uninhabited" where he can live in solitude. Instead, he is picked up by Don Pedro's crew. Despite Gulliver's appearance—he is dressed in skins and speaks like a horse—Don Pedro treats him compassionately and returns him to Lisbon.
Though Don Pedro appears only briefly, he has become an important figure in the debate between so-called soft school and hard school readers of "Gulliver's Travels". Soft school critics contend that Gulliver is a target of Swift's satire and that Don Pedro represents an ideal of human kindness and generosity. For hard-school critics, Gulliver sees the bleak fallenness at the center of human nature, and Don Pedro is merely a minor character who, in Gulliver's words, is "an Animal which had some little Portion of Reason."
Cultural influences.
From 1738 to 1746, Edward Cave published in occasional issues of "The Gentleman's Magazine" semi-fictionalized accounts of contemporary debates in the two Houses of Parliament under the title of "Debates in the Senate of Lilliput". The names of the speakers in the debates, other individuals mentioned, politicians and monarchs present and past, and most other countries and cities of Europe ("Degulia") and America ("Columbia") were thinly disguised under a variety of Swiftian pseudonyms. The disguised names, and the pretence that the accounts were really translations of speeches by Lilliputian politicians, were a reaction to an Act of Parliament forbidding the publication of accounts of its debates. Cave employed several writers on this series: William Guthrie (June 1738 – November 1740), Samuel Johnson (November 1740 – February 1743), and John Hawkesworth (February 1743 – December 1746).
Voltaire was presumably influenced by Swift: his 1750 short story Micromégas, about an alien visitor to Earth, also refers to two moons of Mars.
Swift crater, a crater on Mars's moon Deimos, is named after Jonathan Swift.
The term "Lilliputian" has entered many languages as an adjective meaning "small and delicate". There is even a brand of small cigar called Lilliput. There is a series of collectable model houses known as "Lilliput Lane". The smallest light bulb fitting (5mm diameter) in the Edison screw series is called the "Lilliput Edison screw". In Dutch and Czech, the words "Lilliputter" and "liliput(á)n" respectively are used for adults shorter than 1.30 meters. Conversely, "Brobdingnagian" appears in the Oxford English Dictionary as a synonym for "very large" or "gigantic".
In like vein, the term "yahoo" is often encountered as a synonym for "ruffian" or "thug".
In the discipline of computer architecture, the terms "big-endian" and "little-endian" are used to describe two possible ways of laying out bytes in memory. The terms derive from one of the satirical conflicts in the book, in which two religious sects of Lilliputians are divided between those who crack open their soft-boiled eggs from the little end, and those who use the big end.
Dostoevsky references "Gulliver's Travels" in his novel Demons (1872): 'In an English satire of the last century, Gulliver, returning from the land of the Lilliputians where the people were only three or four inches high, had grown so accustomed to consider himself a giant among them, that as he walked along the Streets of London he could not help crying out to carriages and passers-by to be careful and get out of his way for fear he should crush them, imagining that they were little and he was still a giant...'
Adaptations.
Film, television and radio.
"Gulliver's Travels" has been adapted several times for film, television and radio. Most film versions avoid the satire completely.

</doc>
<doc id="43024" url="http://en.wikipedia.org/wiki?curid=43024" title="Levee">
Levee

A levee, levée (; ]), dike, dyke, embankment, floodbank or stopbank is an elongated naturally occurring ridge or artificially constructed fill or wall, which regulates water levels. It is usually earthen and often parallel to the course of a river in its floodplain or along low-lying coastlines.
Etymology.
Levee.
The word "levee", from the French word "levée" (from the feminine past participle of the French verb "lever", "to raise"), is used in American English (notably in the Midwest and Deep South). It originated in New Orleans a few years after the city's founding in 1718 and was later adopted by English speakers. The name derives from the trait of the levee's ridges being "raised" higher than both the channel and the surrounding floodplains.
Dike.
The modern word "dike" or "dyke" most likely derives from the Dutch word "dijk", with the construction of dikes in Frisia (now part of the Netherlands and Germany) well attested as early as the 12th century. The 126 km long Westfriese Omringdijk was completed by 1250, and was formed by connecting existing older dikes. The Roman chronicler Tacitus even mentions that the rebellious Batavi pierced dikes to flood their land and to protect their retreat (AD 70). The word "dijk" originally indicated both the trench and the bank. It is closely related to the English verb "to dig".
In Anglo-Saxon, the word "dic" already existed and was pronounced with a hard c in northern England and as "ditch" in the south. Similar to Dutch, the English origins of the word lie in digging a trench and forming the upcast soil into a bank alongside it. This practice has meant that the name may be given to either the excavation or the bank. Thus Offa's Dyke is a combined structure and Car Dyke is a trench though it once had raised banks as well. In the midlands and north of England, and in the United States, a dike is what a ditch is in the south, a property boundary marker or small drainage channel. Where it carries a stream, it may be called a running dike as in "Rippingale Running Dike", which leads water from the catchwater drain, Car Dyke, to the South Forty Foot Drain in Lincolnshire (TF1427). The Weir Dike is a soak dike in Bourne North Fen, near Twenty and alongside the River Glen, Lincolnshire. In the Norfolk and Suffolk Broads, a dyke may be a drainage ditch or a narrow artificial channel off a river or broad for access or mooring, some longer dykes being named, e.g. Candle Dyke.
In parts of Britain, particularly Scotland, a dyke may be a field wall, generally made with dry stone.
Usage.
The main purpose of artificial levees are to prevent flooding of the adjoining countryside and to slow natural course changes in a waterway to provide reliable shipping lanes for maritime commerce over time; they also confine the flow of the river, resulting in higher and faster water flow. Levees can be mainly found along the sea, where dunes are not strong enough, along rivers for protection against high-floods, along lakes or along polders. Furthermore, levees have been built for the purpose of empoldering, or as a boundary for an inundation area. The latter can be a controlled inundation by the military or a measure to prevent inundation of a larger area surrounded by levees. Levees have also been built as field boundaries and as military defences. More on this type of levee can be found in the article on dry-stone walls.
Levees can be permanent earthworks or emergency constructions (often of sandbags) built hastily in a flood emergency. When such an emergency bank is added on top of an existing levee it is known as a "cradge".
Some of the earliest levees were constructed by the Indus Valley Civilization (in Pakistan and North India from circa 2600 BC) on which the agrarian life of the Harappan peoples depended. Levees were also constructed over 3,000 years ago in ancient Egypt, where a system of levees was built along the left bank of the River Nile for more than 600 mi, stretching from modern Aswan to the Nile Delta on the shores of the Mediterranean. The Mesopotamian civilizations and ancient China also built large levee systems. Because a levee is only as strong as its weakest point, the height and standards of construction have to be consistent along its length. Some authorities have argued that this requires a strong governing authority to guide the work, and may have been a catalyst for the development of systems of governance in early civilizations. However, others point to evidence of large scale water-control earthen works such as canals and/or levees dating from before King Scorpion in Predynastic Egypt, during which governance was far less centralized.
Levees are usually built by piling earth on a cleared, level surface. Broad at the base, they taper to a level top, where temporary embankments or sandbags can be placed. Because flood discharge intensity increases in levees on both river banks, and because silt deposits raise the level of riverbeds, planning and auxiliary measures are vital. Sections are often set back from the river to form a wider channel, and flood valley basins are divided by multiple levees to prevent a single breach from flooding a large area. A levee made from stones laid in horizontal rows with a bed of thin turf between each of them is known as a "spetchel".
Artificial levees require substantial engineering. Their surface must be protected from erosion, so they are planted with vegetation such as Bermuda grass in order to bind the earth together. On the land side of high levees, a low terrace of earth known as a "banquette" is usually added as another anti-erosion measure. On the river side, erosion from strong waves or currents presents an even greater threat to the integrity of the levee. The effects of erosion are countered by planting suitable vegetation or installing stones, boulders, weighted matting or concrete revetments. Separate ditches or drainage tiles are constructed to ensure that the foundation does not become waterlogged.
River flood prevention.
Prominent levee systems have been built along the Mississippi River and Sacramento River in the United States, and the Po, Rhine, Meuse River, Rhone, Loire, Vistula, the delta formed by the Rhine, Maas/Meuse and Scheldt in the Netherlands and the Danube in Europe.
The Mississippi levee system represents one of the largest such systems found anywhere in the world. It comprises over 3500 mi of levees extending some 1000 km along the Mississippi, stretching from Cape Girardeau, Missouri, to the Mississippi Delta. They were begun by French settlers in Louisiana in the 18th century to protect the city of New Orleans. The first Louisiana levees were about 3 ft high and covered a distance of about 50 mi along the riverside. The U.S. Army Corps of Engineers, in conjunction with the Mississippi River Commission, extended the levee system beginning in 1882 to cover the riverbanks from Cairo, Illinois to the mouth of the Mississippi delta in Louisiana. By the mid-1980s, they had reached their present extent and averaged 24 ft in height; some Mississippi levees are as high as 50 ft. The Mississippi levees also include some of the longest continuous individual levees in the world. One such levee extends southwards from Pine Bluff, Arkansas, for a distance of some 380 mi.
Soil Reinforcement and Levee Protection – The United States Army Corps of Engineers (USACE) recommends and supports Cellular Confinement technology (geocells) as a best management practice. Particular attention is given to the matter of surface erosion, overtopping prevention and protection of levee crest and downstream slope. Reinforcement with geocells provides tensile force to the soil to better resist instability.
Effects of levees upon the elevation of the river bed.
Artificial levees can lead to an elevation of the natural river bed over time; whether this happens or not and how fast, depends on different factors, one of them being the amount and type of the bed load of a river. Alluvial rivers with intense accumulations of sediment tend to this behavior. Examples of rivers where artificial levees led to an elevation of the river bed, even up to a point where the river bed is higher than the adjacent ground surface behind the levees, are found for the Huang He in China and the Mississippi in the USA.
Coastal flood prevention.
Levees are very common on the flatlands bordering the Bay of Fundy in New Brunswick and Nova Scotia Canada. The Acadians who settled the area can be credited with construction of most of the levees in the area, created for the purpose of farming the fertile tidal flatlands. These levees are referred to as "aboiteau". In the Lower Mainland around the city of Vancouver, British Columbia, there are levees (known locally as dikes, and also referred to as "the sea wall") to protect low-lying land in the Fraser River delta, particularly the city of Richmond on Lulu Island. There are also dikes to protect other locations which have flooded in the past, such as the Pitt Polder, land adjacent to the Pitt River and other tributary rivers.
Coastal flood prevention levees are also common along the inland coastline behind the Wadden Sea, an area devastated by many historic floods. Thus the peoples and governments have erected increasingly large and complex flood protection levee systems to stop the sea even during storm floods. The biggest of these are of course the huge levees in the Netherlands, which have gone beyond just defending against floods, as they have aggressively taken back land that is below mean sea level. 
Spur dykes or groynes.
These typically man-made hydraulic structures are situated to protect against erosion. They are typically placed in alluvial rivers perpendicular, or at an angle, to the bank of the channel or the revetment, and are used widely along coastlines. There are two common types of spur dyke, permeable and impermeable, depending on the materials used to construct them.
Natural levees.
Natural levees commonly form around lowland rivers and creeks without human intervention. They are elongate ridges of mud and/or silt that form on the river floodplains immediately adjacent to the cut banks. Like artificial levees, they act to reduce the likelihood of floodplain inundation.
Deposition of levees is a natural consequence of the flooding of meandering rivers which carry high proportions of suspended sediment in the form of fine sands, silts, and muds. Because the carrying capacity of a river depends in part on its depth, the sediment in the water which is over the flooded banks of the channel is no longer capable of keeping the same amount of fine sediments in suspension as the main thalweg. The extra fine sediments thus settle out quickly on the parts of the floodplain nearest to the channel. Over a significant number of floods, this will eventually result in the building up of ridges in these positions, and reducing the likelihood of further floods and episodes of levee building.
If aggradation continues to occur in the main channel, this will make levee overtopping more likely again, and the levees can continue to build up. In some cases this can result in the channel bed eventually rising above the surrounding floodplains, penned in only by the levees around it; an example is the Yellow River in China near the sea, where oceangoing ships appear to sail high above the plain on the elevated river.
Levees are common in any river with a high suspended sediment fraction, and thus are intimately associated with meandering channels, which also are more likely to occur where a river carries large fractions of suspended sediment. For similar reasons, they are also common in tidal creeks, where tides bring in large amounts of coastal silts and muds. High will cause flooding, and result in the building up of levees.
Levee failures and breaches.
Both natural and man-made levees can fail in a number of ways. Factors that cause levee failure include overtopping, erosion, structural failures, and levee saturation. The most frequent (and dangerous) is a "levee breach". Here, a part of the levee actually breaks or is eroded away, leaving a large opening for water to flood land otherwise protected by the levee. A breach can be a sudden or gradual failure, caused either by surface erosion or by subsurface weakness in the levee. A breach can leave a fan-shaped deposit of sediment radiating away from the breach, described as a crevasse splay. In natural levees, once a breach has occurred, the gap in the levee will remain until it is again filled in by levee building processes. This increases the chances of future breaches occurring in the same location. Breaches can be the location of meander cutoffs if the river flow direction is permanently diverted through the gap.
Sometimes levees are said to fail when water "overtops" the crest of the levee. This will cause flooding on the floodplains, but because it does not damage the levee has fewer consequences for future flooding.

</doc>
